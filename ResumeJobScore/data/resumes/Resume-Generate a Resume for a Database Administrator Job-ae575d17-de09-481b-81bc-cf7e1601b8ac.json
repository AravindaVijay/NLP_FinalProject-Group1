{
    "clean_data": "Data Scientist Machine Learning Data Scientist Machine Learning Data Scientist Machine Learning Rauxa New York NY Professional qualified Data ScientistData Analyst with around 8 years of experience in Data Science and Analytics including Data Mining Deep LearningMachine Learning and Statistical Analysis Involved in the entire data science project life cycle and actively involved in all the phases including data cleaning data extractionand datavisualization with large data sets of structured and unstructured data created ER diagrams and schema Experienced with machine learning algorithm such as logistic regression KNN SVM random forest neural network linear regression lasso regression and kmeans Implemented Bagging and Boosting to enhance the model performance Experience in implementing data analysis with various analytic tools such as Anaconda 40 Jupiter Notebook 4X R 30 ggplot2 dplyr Caret and Excel Solid ability to write and optimize diverse SQL queries working knowledge of RDBMS like SQL Server 12 NoSql databases like MongoDB 32 Excellent understanding Agile and Scrum development methodology Used the version control tools like Git 2X and build tools like Apache MavenAnt Passionate about gleaning insightful information from massive data assets and developing a culture of sound datadriven decision making Ability to maintain a fun casual professional and productive team atmosphere Experienced the full software life cycle in SDLC Agile DevOps and Scrum methodologies including creating requirements test plans Skilled in Advanced Regression Modeling Correlation Multivariate Analysis Model Building Business Intelligence tools and application of Statistical Concepts Developed predictive models using Decision Tree Naive Bayes Logistic Regression Random Forest Social Network Analysis Cluster Analysis and Neural Networks Experienced in Machine Learning and Statistical Analysis with Python ScikitLearn Experienced in Python to manipulate data for data loading and extraction and worked with python libraries like Matplotlib Scipy Numpy and Pandas for data analysis Worked with complex applications such as RR Shiny SAS Plotly ArcGIS Matlab and SPSS to develop neural network cluster analysis Strong SQL programming skills with experience in working with functions packages and triggers Expertise in transforming business requirements into designing algorithms analytical models building models developing data mining and reporting solutions that scales across massive volume of structured and unstructured data Skilled in performing data parsing data manipulation data architecture data ingestion and data preparation with methods including describe data contents compute descriptive statistics of data regex split and combine merge Remap subset reindex melt and reshape Worked with NoSQLDatabase including Hbase Cassandra and MongoDB Experienced in Big Data with Hadoop MapReduce HDFS and Spark Experienced in Data Integration Validation and Data Quality controls for ETL process and Data Warehousing using MS Visual Studio SSAS SSISand SSRS Proficient in Tableau and RShiny data visualization tools to analyze and obtain insights into large datasets create visually powerful and actionable interactive reports and dashboards Automated recurring reports using SQL andPython and visualized them on BI platform like Tableau Worked in development environment like Git and VM Excellent communication skills Successfully working in fastpaced multitasking environment both independently and in collaborative team a selfmotivated enthusiastic learner Authorized to work in the US for any employer Work Experience Data Scientist Machine Learning Rauxa New York NY August 2018 to Present Description Makers of results Rauxa applies data technology and content to create measurable impact at maximum speed for clients that include Gap Inc TGI Fridays and Verizon The countrys largest womanowned independent advertising agency Responsibilities Extracted data from HDFS and prepared data for exploratory analysis using data munging Built models using Statistical techniques like Bayesian HMM and Machine Learning classification models like XGBoost SVM and Random Forest Participated in all phases of data mining data cleaning data collection developing models validation visualization and performed Gap analysis A highly immersive Data Science program involving Data ManipulationVisualization Web Scraping Machine Learning Python programming SQL GIT MongoDB Hadoop Setup storage and data analysis tools in AWS cloud computing infrastructure Installed and used Caffe Deep Learning Framework Worked on different data formats such as JSON XML and performed machine learning algorithms in Python Worked as Data Architects and IT Architects to understand the movement of data and its storage and ER Studio 97 Used pandas numpy seaborn matplotlib scikitlearn scipy NLTK in Python for developing various machine learning algorithms Data Manipulation and Aggregation from different source using Nexus Business Objects Toad Power BI and Smart View Implemented Agile Methodology for building an internal application Focus on integration overlap and Informatica newer commitment to MDM with the acquisition of Identity Systems Coded proprietary packages to analyze and visualize SPCfile data to identify bad spectra and samples to reduce unnecessary procedures and costs Programmed a utility in Python that used multiple packages numpy scipy pandas Implemented Classification using supervised algorithms like Logistic Regression Decision trees Naive Bayes KNN As Architect delivered various complex OLAPdatabasescubes scorecards dashboards and reports Updated Python scripts to match training data with our database stored in AWS Cloud Search so that we would be able to assign each document a response label for further classification Used Teradata utilities such as Fast Export MLOAD for handling various tasks data migrationETL from OLTP Source Systems to OLAP Target Systems Data transformation from various resources data organization features extraction from raw and stored Validated the machine learning classifiers using ROC Curves and Lift Charts Environment Unix Python 352 MLLib SAS regression logistic regression Hadoop 274 NoSQL Teradata OLTP random forest OLAP HDFS ODS NLTK SVM JSON XML and MapReduce Data Scientist Charter communication St Louis MO May 2017 to July 2018 Description Charter Communications Inc is an American telecommunications and mass media company that offers its services to consumers and businesses under the branding of Spectrum Responsibilities Utilized Spark Scala Hadoop HQL VQL oozie pySpark Data Lake TensorFlow HBase Cassandra Redshift MongoDB Kafka Kinesis Spark Streaming Edward CUDA MLLib AWS Python a broad variety of machine learning methods including classifications regressions dimensionally reduction etc Utilized the engine to increase user lifetime by 45 and triple user conversations for target categories Application of various machine learning algorithms and statistical modeling like decision trees text analytics natural language processing NLP supervised and unsupervised regression models social network analysis neural networks deep learning SVM clustering to identify Volume using scikitlearn package in python Matlab Worked onanalyzing data from Google Analytics AdWords Facebook etc Evaluated models using Cross Validation Log loss function ROC curves and used AUC for feature selection and elastic technologies like ElasticSearch Kibana Performed Data Profiling to learn about behavior with various features such as traffic pattern location Date and Time etc Categorized comments into positive and negative clusters from different social networking sites using Sentiment Analysis and Text Analytics Performed Multinomial Logistic Regression Decision Tree Random forest SVM to classify package is going to deliver on time for the new route Performed data analysis by using Hive to retrieve the data from Hadoop cluster Sql to retrieve datafrom Oracle database and used ETL for data transformation Performed Data Cleaning features scaling features engineering using pandas and numpy packages in python Exploring DAGs their dependencies and logs using AirFlow pipelines for automation Performed data cleaning and feature selection using MLlib package in PySpark and working with deep learning frameworks such as Caffe Neon Developed SparkScalaR Python for regular expression regex project in the HadoopHive environment with LinuxWindows for big data resources Used clustering technique KMeans to identify outliers and to classify unlabeled data Tracking operations using sensors until certain criteria is met using AirFlowtechnology Responsible for different Data mapping activities from Source systems to Teradata using utilities like TPump FEXPBTEQ MLOAD FLOAD etc Analyze traffic patterns by calculating autocorrelation with different time lags Ensured that the model has low False Positive Rate and Text classification and sentiment analysis for unstructured and semistructured data Addressed over fitting by implementing of the algorithm regularization methods like L1 and L2 Used Principal Component Analysis in feature engineering to analyze high dimensional data Used MLlib Sparks Machine learning library to build and evaluate different models Implemented rule based expertise system from the results of exploratory analysis and information gathered from the people from different departments Created and designed reports that will use gathered metrics to infer and draw logical conclusions of past and future behavior Developed MapReduce pipeline for feature extraction using Hive and Pig Created Data Quality Scripts using SQL and Hive to validate successful data load and quality of the data Created various types of data visualizations using Python and Tableau Communicated the results with operations team for taking best decisions Collected data needs and requirements by Interacting with the other departments Environment Python 2x CDH5 HDFS Hadoop 23 Hive Impala AWS Linux Spark Tableau Desktop SQL Server 2014 Microsoft Excel Matlab Spark SQL Pyspark Data analyst EDAC Technologies Corp Cheshire CT January 2016 to April 2017 Description EDAC Technologies Corporation provides design manufacturing and services for tooling fixtures molds jet engine components and machine spindles in the aerospace industrial semiconductor and medical device markets Responsibilities Worked with BI team in gathering the report requirements and also Sqoop to export data into HDFS and Hive Involved in the below phases of Analytics using R Python and Jupyter notebook a Data collection and treatment Analysed existing internal data and external data worked on entry errorsclassification errors and defined criteria for missing values b Data Mining Used cluster analysis for identifying customer segments Decision trees used for profitable and nonprofitable customers Market Basket Analysis used for customer purchasing behaviour and partproduct association Developed multiple Map Reduce jobs in Java for data cleaning and preprocessing Assisted with data capacity planning and node forecasting Installed Configured and managed Flume Infrastructure Administrator for Pig Hive and HBase installing updates patches and upgrades Worked closely with the claims processing team to obtain patterns in filing of fraudulent claims Worked on performing major upgrade of cluster from CDH3u6 to CDH440 Developed Map Reduce programs to extract and transform the data sets and results were exported back to RDBMS using Sqoop Patterns were observed in fraudulent claims using text mining in R and Hive Exported the data required information to RDBMS using Sqoop to make the data available for the claims processing team to assist in processing a claim based on the data Developed Map Reduce programs to parse the raw data populate staging tables and store the refined data in partitioned tables in the EDW Created tables in Hive and loaded the structured resulted from Map Reduce jobs data Using HiveQL developed many queries and extracted the required information Created Hive queries that helped market analysts spot emerging trends by comparing fresh data with EDW reference tables and historical metrics Was responsible for importing the data mostly log files from various sources into HDFS using Flume Enabled speedy reviews and first mover advantages by using Oozie to automate data loading into the Hadoop Distributed File System and PIG to preprocess the data Provided design recommendations and thought leadership to sponsorsstakeholders that improved review processes and resolved technical problems Managed and reviewed Hadoop log files Tested raw data and executed performance scripts Environment HDFS PIG HIVE Map Reduce Linux HBase Flume Sqoop R VMware Eclipse Cloudera Python Data ModelerData Analyst Dorman Products Inc Colmar PA March 2014 to December 2015 Description Dorman Products Inc supplies automotive replacement parts automotive hardware and brake products to the automotive aftermarket and mass merchandise markets in the United States Canada Mexico Europe the Middle East and Australia Responsibilities Created and maintained Logical and Physical models for the data mart Created partitions and indexes for the tables in the data mart Performed data profiling and analysis applied various data cleansing rules designed data standards and architecturedesigned the relational models Maintained metadata data definitions of table structures and version controlling for the data model Developed SQL scripts for creating tables Sequences Triggers views and materialized views Worked on query optimization and performance tuning using SQL Profiler and performance monitoring Developed mappings to load Fact and Dimension tables SCD Type 1 and SCD Type 2 dimensions and Incremental loading and unit tested the mappings Utilized Erwins forward reverse engineering tools and target database schema conversion process Worked on creating enterprise wide Model EDM for products and services in Teradata Environment based on the data from PDM Conceived designed developed and implemented this model from the scratch Building publishing customized interactive reports and dashboards report scheduling using Tableau server Write SQL scripts to test the mappings and Developed Traceability Matrix of Business Requirements mapped to Test Scripts to ensure any Change Control in requirements leads to test case update Responsible for development and testing of conversion programs for importing Data from text files into map Oracle Database utilizing PERL shell scripts SQLLoader Involved in extensive DATA validation by writing several complex SQL queries and Involved in backend testing and worked with data quality issues Developed and executed load scripts using Teradata client utilities MULTILOAD FASTLOAD and BTEQ Exporting and importing the data between different platforms such as SAS MSExcel Generated periodic reports based on the statistical analysis of the data using SQL Server Reporting Services SSRS Worked with the ETL team to document the Transformation Rules for Data Migration from OLTP to Warehouse Environment for reporting purposes Created SQL scripts to find data quality issues and to identify keys data anomalies and data validation issues Formatting the data sets read into SAS by using Format statement in the data step as well as Proc Format Applied Business Objects best practices during development with a strong focus on reusability and better performance Developed Tableau visualizations and dashboards using Tableau Desktop Used Graphical Entity Relationship Diagramming to create new database design via easy to use graphical interface Designed different type of STAR schemas for detailed data marts and plan data marts in the OLAP environment Environment Erwin MS SQL Server 2008 DB2 Oracle SQL Developer PLSQL Business Objects Erwin MS office suite Windows XP TOAD SQLPLUS SQLLOADER Teradata Netezza SAS Tableau Business Objects SSRS tableau SQL Assistant Informatica XML Python Developer Dabur India Ltd Ghaziabad Uttar Pradesh December 2012 to February 2014 Description Dabur is one of the Indias largest Ayurvedic medicine natural consumer products manufacturer Dabur demerged its Pharma business in 2003 and hived it off into a separate company Dabur Pharma Ltd German company Fresenius SE bought a 7327 equity stake in Dabur Pharma in June 2008 at Rs 7650 a share Responsibilities Involved in the design development and testing phases of application using AGILE methodology Designed and maintained databases using Python and developed Python based API RESTful Web Service using Flask SQLAlchemy and PostgreSQL Designed and developed the UI of the website using HTML XHTML AJAX CSS and JavaScript Participated in requirement gathering and worked closely with the architect in designing and modeling Worked on Restful web services which enforced a stateless client server and support JSON few changes from SOAP to RESTFUL Technology Involved in detailed analysis based on the requirement documents Involved in writing SQL queries implementing functions triggers cursors object types sequences indexes etc Created and managed all of hosted or local repositories through Source Trees simple interface of GIT client collaborated with GIT command lines and Stash Responsible for setting up Python REST API framework and spring frame work using Django Develope consumer based features and applications using Python Django HTML behavior Driven Development BDD and pair based programming Designed and developed components using Python with Django framework Implemented code in python to retrieve and manipulate data Involved in development of the enterprise social network application using Python Twisted and Cassandra Used Python and Django creating graphics XML processing of documents data exchange and business logic implementation between servers orked closely with backend developer to find ways to push the limits of existing Web technology Designed and developed the UI for the website with HTML XHTML CSS Java Script and AJAX Used AJAXJSON communication for accessing RESTfulweb services data payload Designed dynamic clientside JavaScript codes to build web forms and performed simulations for web application page Created and implemented SQL Queries Stored procedures Functions Packages and Triggers in SQL Server Successfully implemented Auto CompleteAuto Suggest functionality using JQuery Ajax Web Service and JSON Environment Python 25 JavaJ2EE Django10 HTMLCSS Linux Shell Scripting Java Script Ajax JQuery JSON XML PostgreSQL Jenkins ANT Maven Subversion Python Data AnalystData Modeler Kotak Mahindra Bank Mumbai Maharashtra January 2011 to November 2012 Description Kotak Mahindra Bank is an Indian private sector bank headquartered in Mumbai Maharashtra India In February 2003 Reserve Bank of India issued the licence to Kotak Mahindra Finance Ltd the groups flagship company to carry on banking business Responsibilities Analyzed data sources and requirements and business rules to perform logical and physical data modeling Analyzed and designed best fit logical and physical data models and relational database definitions using DB2 Generated reports of data definitions Involved in NormalizationDenormalization Normal Form and database design methodology Maintained existing ETL procedures fixed bugs and restored software to production environment Developed the code as per the clients requirements using SQL PLSQL and Data Warehousing concepts Involved in Dimensional modeling Star Schema of the Data warehouse and used Erwin to design the business process dimensions and measured facts Worked with Data Warehouse Extract and load developers to design mappings for Data Capture Staging Cleansing Loading and Auditing Developed enterprise data model management process to manage multiple data models developed by different groups Designed and created Data Marts as part of a data warehouse Wrote complex SQL queries for validating the data against different kinds of reports generated by Business Objects XIR2 Using Erwin modeling tool publishing of a data dictionary review of the model and dictionary with subject matter experts and generation of data definition language Coordinated with DBA in implementing the Database changes and also updating Data Models with changes implemented in development QA and Production Worked Extensively with DBA and Reporting team for improving the Report Performance with the Use of appropriate indexes and Partitioning Developed Data Mapping Transformation and Cleansing rules for the Master Data Management Architecture involved OLTP ODS and OLAP Tuned and coded optimization using different techniques like dynamic SQL dynamic cursors and tuning SQL queries writing generic procedures functions and packages Experienced in GUI Relational Database Management System RDBMS designing of OLAP system environment as well as Report Development Extensively used SQL TSQL and PLSQL to write stored procedures functions packages and triggers Analyzed of data report were prepared weekly biweekly monthly using MS Excel SQL UNIX Environment ER Studio Informatica Power Center 8191 Power Connect Power exchange Oracle 11g MainframesDB2 MS SQL Server 2008 SQLPLSQL XML Windows NT 40 Tableau Workday SPSS SAS Business Objects XML Tableau Unix Shell Scripting Teradata Netezza Aginity Education Bachelors in Computer Science Royal College of Technology 2011 Skills Linux Solaris Sun Unix Additional Information Operating Systems All versions of Windows UNIX LINUX Macintosh HD Sun Solaris",
    "entities": [
        "MLlib",
        "Informatica XML Python",
        "Python Worked as Data Architects",
        "Informatica",
        "AUC",
        "RESTFUL Technology Involved",
        "SDLC Agile DevOps",
        "BI",
        "MapReduce Data Scientist Charter",
        "HDFS",
        "Ability",
        "OLTP ODS",
        "Write SQL",
        "Driven Development BDD",
        "Responsibilities",
        "Data Scientist Machine Learning Data Scientist Machine Learning Data Scientist Machine Learning Rauxa New York NY Professional",
        "Google Analytics AdWords Facebook etc Evaluated",
        "Data Science",
        "EDAC Technologies Corp Cheshire",
        "Logical",
        "Apache MavenAnt Passionate",
        "GUI Relational Database Management System",
        "Data Marts",
        "ER",
        "Statistical Concepts Developed",
        "Caffe Neon Developed SparkScalaR Python",
        "node",
        "MS Excel SQL UNIX Environment",
        "Hadoop",
        "Kotak Mahindra Finance Ltd",
        "SOAP",
        "XML",
        "Flume Infrastructure Administrator for Pig Hive",
        "Data Science and Analytics",
        "Maintained",
        "kmeans Implemented Bagging and Boosting",
        "SPSS",
        "Reserve Bank of India",
        "Gap Inc",
        "Fresenius",
        "Principal Component Analysis",
        "HBase",
        "Rauxa",
        "Data Models",
        "Oracle SQL Developer PLSQL Business",
        "Dabur India Ltd",
        "Installed Configured",
        "Python",
        "SQL Server",
        "SQL Server Reporting Services",
        "Australia Responsibilities Created",
        "Performed Data Cleaning",
        "Assisted",
        "Generated",
        "Developed",
        "Implemented Classification",
        "Pyspark Data",
        "VM Excellent",
        "Utilized",
        "Data Warehouse Extract",
        "Work Experience Data Scientist Machine Learning Rauxa New York",
        "VMware",
        "Tableau Communicated",
        "Stash Responsible",
        "Tableau Worked",
        "Studio Informatica Power Center 8191 Power Connect Power exchange",
        "ElasticSearch",
        "PDM",
        "AirFlow",
        "Windows XP",
        "AGILE",
        "Developed Traceability Matrix of Business Requirements",
        "API RESTful Web Service",
        "the Hadoop Distributed File System",
        "Built",
        "ER Studio",
        "Python Twisted",
        "DBA",
        "ROC",
        "Developed SQL",
        "RShiny",
        "Spark Experienced",
        "XGBoost SVM",
        "Oracle Database",
        "Data ModelerData Analyst Dorman Products Inc Colmar PA",
        "Description EDAC Technologies Corporation",
        "GIT",
        "linear",
        "Dabur",
        "MULTILOAD",
        "OLTP Source Systems",
        "US",
        "Sqoop",
        "LinuxWindows",
        "the Master Data Management Architecture",
        "Hadoop Setup",
        "Sqoop Patterns",
        "KNN",
        "NoSql",
        "Statistical",
        "Created",
        "Jenkins ANT Maven Subversion Python Data",
        "AWS",
        "Created SQL",
        "Hive and Pig Created Data Quality Scripts",
        "Analyzed",
        "SQL Profiler",
        "Oracle",
        "Data Capture Staging Cleansing Loading and Auditing Developed",
        "PySpark",
        "Developed Tableau",
        "Computer Science Royal College of Technology 2011",
        "PIG",
        "Spectrum Responsibilities Utilized Spark",
        "Sql",
        "Dabur Pharma Ltd German",
        "SAS",
        "Oozie",
        "Data Integration Validation",
        "SSRS",
        "SQLPLSQL XML Windows NT",
        "Market Basket Analysis",
        "CDH3u6",
        "SQL",
        "SCD Type 1",
        "Hive Involved",
        "OLTP",
        "Incremental",
        "BTEQ Exporting",
        "Verizon",
        "NLP",
        "Data Quality",
        "the United States",
        "Anaconda",
        "Big Data",
        "Hive",
        "Macintosh",
        "SQL GIT",
        "SCD Type",
        "Wrote",
        "Sun Solaris",
        "Tableau Desktop Used Graphical Entity Relationship Diagramming",
        "Pandas",
        "Mumbai",
        "MDM",
        "ETL",
        "Random Forest Participated",
        "India",
        "Performed",
        "OLAP Tuned",
        "Description Kotak Mahindra Bank",
        "Partitioning Developed Data Mapping Transformation and Cleansing",
        "JavaScript",
        "TPump",
        "Responsibilities Analyzed",
        "UI",
        "Logistic Regression Decision",
        "Python ScikitLearn Experienced",
        "Microsoft",
        "OLAP Target Systems Data",
        "Dabur Pharma",
        "Data Mining Used",
        "Nexus Business Objects Toad Power BI",
        "EDW",
        "Data",
        "Teradata Environment",
        "the Transformation Rules for Data Migration",
        "Identity Systems Coded",
        "Data ScientistData Analyst",
        "Data Warehousing",
        "NoSQLDatabase",
        "Tableau",
        "JQuery Ajax Web Service and JSON Environment Python",
        "Machine Learning",
        "GIT command lines",
        "AirFlowtechnology Responsible",
        "HMM",
        "Description Dorman Products Inc",
        "SQL Server Successfully",
        "Teradata",
        "Change Control",
        "SVM",
        "SQL andPython",
        "Fact and Dimension",
        "Advanced Regression Modeling Correlation Multivariate Analysis Model Building Business Intelligence",
        "Cross Validation Log",
        "JSON XML"
    ],
    "experience": "Experience in implementing data analysis with various analytic tools such as Anaconda 40 Jupiter Notebook 4X R 30 ggplot2 dplyr Caret and Excel Solid ability to write and optimize diverse SQL queries working knowledge of RDBMS like SQL Server 12 NoSql databases like MongoDB 32 Excellent understanding Agile and Scrum development methodology Used the version control tools like Git 2X and build tools like Apache MavenAnt Passionate about gleaning insightful information from massive data assets and developing a culture of sound datadriven decision making Ability to maintain a fun casual professional and productive team atmosphere Experienced the full software life cycle in SDLC Agile DevOps and Scrum methodologies including creating requirements test plans Skilled in Advanced Regression Modeling Correlation Multivariate Analysis Model Building Business Intelligence tools and application of Statistical Concepts Developed predictive models using Decision Tree Naive Bayes Logistic Regression Random Forest Social Network Analysis Cluster Analysis and Neural Networks Experienced in Machine Learning and Statistical Analysis with Python ScikitLearn Experienced in Python to manipulate data for data loading and extraction and worked with python libraries like Matplotlib Scipy Numpy and Pandas for data analysis Worked with complex applications such as RR Shiny SAS Plotly ArcGIS Matlab and SPSS to develop neural network cluster analysis Strong SQL programming skills with experience in working with functions packages and triggers Expertise in transforming business requirements into designing algorithms analytical models building models developing data mining and reporting solutions that scales across massive volume of structured and unstructured data Skilled in performing data parsing data manipulation data architecture data ingestion and data preparation with methods including describe data contents compute descriptive statistics of data regex split and combine merge Remap subset reindex melt and reshape Worked with NoSQLDatabase including Hbase Cassandra and MongoDB Experienced in Big Data with Hadoop MapReduce HDFS and Spark Experienced in Data Integration Validation and Data Quality controls for ETL process and Data Warehousing using MS Visual Studio SSAS SSISand SSRS Proficient in Tableau and RShiny data visualization tools to analyze and obtain insights into large datasets create visually powerful and actionable interactive reports and dashboards Automated recurring reports using SQL andPython and visualized them on BI platform like Tableau Worked in development environment like Git and VM Excellent communication skills Successfully working in fastpaced multitasking environment both independently and in collaborative team a selfmotivated enthusiastic learner Authorized to work in the US for any employer Work Experience Data Scientist Machine Learning Rauxa New York NY August 2018 to Present Description Makers of results Rauxa applies data technology and content to create measurable impact at maximum speed for clients that include Gap Inc TGI Fridays and Verizon The countrys largest womanowned independent advertising agency Responsibilities Extracted data from HDFS and prepared data for exploratory analysis using data munging Built models using Statistical techniques like Bayesian HMM and Machine Learning classification models like XGBoost SVM and Random Forest Participated in all phases of data mining data cleaning data collection developing models validation visualization and performed Gap analysis A highly immersive Data Science program involving Data ManipulationVisualization Web Scraping Machine Learning Python programming SQL GIT MongoDB Hadoop Setup storage and data analysis tools in AWS cloud computing infrastructure Installed and used Caffe Deep Learning Framework Worked on different data formats such as JSON XML and performed machine learning algorithms in Python Worked as Data Architects and IT Architects to understand the movement of data and its storage and ER Studio 97 Used pandas numpy seaborn matplotlib scikitlearn scipy NLTK in Python for developing various machine learning algorithms Data Manipulation and Aggregation from different source using Nexus Business Objects Toad Power BI and Smart View Implemented Agile Methodology for building an internal application Focus on integration overlap and Informatica newer commitment to MDM with the acquisition of Identity Systems Coded proprietary packages to analyze and visualize SPCfile data to identify bad spectra and samples to reduce unnecessary procedures and costs Programmed a utility in Python that used multiple packages numpy scipy pandas Implemented Classification using supervised algorithms like Logistic Regression Decision trees Naive Bayes KNN As Architect delivered various complex OLAPdatabasescubes scorecards dashboards and reports Updated Python scripts to match training data with our database stored in AWS Cloud Search so that we would be able to assign each document a response label for further classification Used Teradata utilities such as Fast Export MLOAD for handling various tasks data migrationETL from OLTP Source Systems to OLAP Target Systems Data transformation from various resources data organization features extraction from raw and stored Validated the machine learning classifiers using ROC Curves and Lift Charts Environment Unix Python 352 MLLib SAS regression logistic regression Hadoop 274 NoSQL Teradata OLTP random forest OLAP HDFS ODS NLTK SVM JSON XML and MapReduce Data Scientist Charter communication St Louis MO May 2017 to July 2018 Description Charter Communications Inc is an American telecommunications and mass media company that offers its services to consumers and businesses under the branding of Spectrum Responsibilities Utilized Spark Scala Hadoop HQL VQL oozie pySpark Data Lake TensorFlow HBase Cassandra Redshift MongoDB Kafka Kinesis Spark Streaming Edward CUDA MLLib AWS Python a broad variety of machine learning methods including classifications regressions dimensionally reduction etc Utilized the engine to increase user lifetime by 45 and triple user conversations for target categories Application of various machine learning algorithms and statistical modeling like decision trees text analytics natural language processing NLP supervised and unsupervised regression models social network analysis neural networks deep learning SVM clustering to identify Volume using scikitlearn package in python Matlab Worked onanalyzing data from Google Analytics AdWords Facebook etc Evaluated models using Cross Validation Log loss function ROC curves and used AUC for feature selection and elastic technologies like ElasticSearch Kibana Performed Data Profiling to learn about behavior with various features such as traffic pattern location Date and Time etc Categorized comments into positive and negative clusters from different social networking sites using Sentiment Analysis and Text Analytics Performed Multinomial Logistic Regression Decision Tree Random forest SVM to classify package is going to deliver on time for the new route Performed data analysis by using Hive to retrieve the data from Hadoop cluster Sql to retrieve datafrom Oracle database and used ETL for data transformation Performed Data Cleaning features scaling features engineering using pandas and numpy packages in python Exploring DAGs their dependencies and logs using AirFlow pipelines for automation Performed data cleaning and feature selection using MLlib package in PySpark and working with deep learning frameworks such as Caffe Neon Developed SparkScalaR Python for regular expression regex project in the HadoopHive environment with LinuxWindows for big data resources Used clustering technique KMeans to identify outliers and to classify unlabeled data Tracking operations using sensors until certain criteria is met using AirFlowtechnology Responsible for different Data mapping activities from Source systems to Teradata using utilities like TPump FEXPBTEQ MLOAD FLOAD etc Analyze traffic patterns by calculating autocorrelation with different time lags Ensured that the model has low False Positive Rate and Text classification and sentiment analysis for unstructured and semistructured data Addressed over fitting by implementing of the algorithm regularization methods like L1 and L2 Used Principal Component Analysis in feature engineering to analyze high dimensional data Used MLlib Sparks Machine learning library to build and evaluate different models Implemented rule based expertise system from the results of exploratory analysis and information gathered from the people from different departments Created and designed reports that will use gathered metrics to infer and draw logical conclusions of past and future behavior Developed MapReduce pipeline for feature extraction using Hive and Pig Created Data Quality Scripts using SQL and Hive to validate successful data load and quality of the data Created various types of data visualizations using Python and Tableau Communicated the results with operations team for taking best decisions Collected data needs and requirements by Interacting with the other departments Environment Python 2x CDH5 HDFS Hadoop 23 Hive Impala AWS Linux Spark Tableau Desktop SQL Server 2014 Microsoft Excel Matlab Spark SQL Pyspark Data analyst EDAC Technologies Corp Cheshire CT January 2016 to April 2017 Description EDAC Technologies Corporation provides design manufacturing and services for tooling fixtures molds jet engine components and machine spindles in the aerospace industrial semiconductor and medical device markets Responsibilities Worked with BI team in gathering the report requirements and also Sqoop to export data into HDFS and Hive Involved in the below phases of Analytics using R Python and Jupyter notebook a Data collection and treatment Analysed existing internal data and external data worked on entry errorsclassification errors and defined criteria for missing values b Data Mining Used cluster analysis for identifying customer segments Decision trees used for profitable and nonprofitable customers Market Basket Analysis used for customer purchasing behaviour and partproduct association Developed multiple Map Reduce jobs in Java for data cleaning and preprocessing Assisted with data capacity planning and node forecasting Installed Configured and managed Flume Infrastructure Administrator for Pig Hive and HBase installing updates patches and upgrades Worked closely with the claims processing team to obtain patterns in filing of fraudulent claims Worked on performing major upgrade of cluster from CDH3u6 to CDH440 Developed Map Reduce programs to extract and transform the data sets and results were exported back to RDBMS using Sqoop Patterns were observed in fraudulent claims using text mining in R and Hive Exported the data required information to RDBMS using Sqoop to make the data available for the claims processing team to assist in processing a claim based on the data Developed Map Reduce programs to parse the raw data populate staging tables and store the refined data in partitioned tables in the EDW Created tables in Hive and loaded the structured resulted from Map Reduce jobs data Using HiveQL developed many queries and extracted the required information Created Hive queries that helped market analysts spot emerging trends by comparing fresh data with EDW reference tables and historical metrics Was responsible for importing the data mostly log files from various sources into HDFS using Flume Enabled speedy reviews and first mover advantages by using Oozie to automate data loading into the Hadoop Distributed File System and PIG to preprocess the data Provided design recommendations and thought leadership to sponsorsstakeholders that improved review processes and resolved technical problems Managed and reviewed Hadoop log files Tested raw data and executed performance scripts Environment HDFS PIG HIVE Map Reduce Linux HBase Flume Sqoop R VMware Eclipse Cloudera Python Data ModelerData Analyst Dorman Products Inc Colmar PA March 2014 to December 2015 Description Dorman Products Inc supplies automotive replacement parts automotive hardware and brake products to the automotive aftermarket and mass merchandise markets in the United States Canada Mexico Europe the Middle East and Australia Responsibilities Created and maintained Logical and Physical models for the data mart Created partitions and indexes for the tables in the data mart Performed data profiling and analysis applied various data cleansing rules designed data standards and architecturedesigned the relational models Maintained metadata data definitions of table structures and version controlling for the data model Developed SQL scripts for creating tables Sequences Triggers views and materialized views Worked on query optimization and performance tuning using SQL Profiler and performance monitoring Developed mappings to load Fact and Dimension tables SCD Type 1 and SCD Type 2 dimensions and Incremental loading and unit tested the mappings Utilized Erwins forward reverse engineering tools and target database schema conversion process Worked on creating enterprise wide Model EDM for products and services in Teradata Environment based on the data from PDM Conceived designed developed and implemented this model from the scratch Building publishing customized interactive reports and dashboards report scheduling using Tableau server Write SQL scripts to test the mappings and Developed Traceability Matrix of Business Requirements mapped to Test Scripts to ensure any Change Control in requirements leads to test case update Responsible for development and testing of conversion programs for importing Data from text files into map Oracle Database utilizing PERL shell scripts SQLLoader Involved in extensive DATA validation by writing several complex SQL queries and Involved in backend testing and worked with data quality issues Developed and executed load scripts using Teradata client utilities MULTILOAD FASTLOAD and BTEQ Exporting and importing the data between different platforms such as SAS MSExcel Generated periodic reports based on the statistical analysis of the data using SQL Server Reporting Services SSRS Worked with the ETL team to document the Transformation Rules for Data Migration from OLTP to Warehouse Environment for reporting purposes Created SQL scripts to find data quality issues and to identify keys data anomalies and data validation issues Formatting the data sets read into SAS by using Format statement in the data step as well as Proc Format Applied Business Objects best practices during development with a strong focus on reusability and better performance Developed Tableau visualizations and dashboards using Tableau Desktop Used Graphical Entity Relationship Diagramming to create new database design via easy to use graphical interface Designed different type of STAR schemas for detailed data marts and plan data marts in the OLAP environment Environment Erwin MS SQL Server 2008 DB2 Oracle SQL Developer PLSQL Business Objects Erwin MS office suite Windows XP TOAD SQLPLUS SQLLOADER Teradata Netezza SAS Tableau Business Objects SSRS tableau SQL Assistant Informatica XML Python Developer Dabur India Ltd Ghaziabad Uttar Pradesh December 2012 to February 2014 Description Dabur is one of the Indias largest Ayurvedic medicine natural consumer products manufacturer Dabur demerged its Pharma business in 2003 and hived it off into a separate company Dabur Pharma Ltd German company Fresenius SE bought a 7327 equity stake in Dabur Pharma in June 2008 at Rs 7650 a share Responsibilities Involved in the design development and testing phases of application using AGILE methodology Designed and maintained databases using Python and developed Python based API RESTful Web Service using Flask SQLAlchemy and PostgreSQL Designed and developed the UI of the website using HTML XHTML AJAX CSS and JavaScript Participated in requirement gathering and worked closely with the architect in designing and modeling Worked on Restful web services which enforced a stateless client server and support JSON few changes from SOAP to RESTFUL Technology Involved in detailed analysis based on the requirement documents Involved in writing SQL queries implementing functions triggers cursors object types sequences indexes etc Created and managed all of hosted or local repositories through Source Trees simple interface of GIT client collaborated with GIT command lines and Stash Responsible for setting up Python REST API framework and spring frame work using Django Develope consumer based features and applications using Python Django HTML behavior Driven Development BDD and pair based programming Designed and developed components using Python with Django framework Implemented code in python to retrieve and manipulate data Involved in development of the enterprise social network application using Python Twisted and Cassandra Used Python and Django creating graphics XML processing of documents data exchange and business logic implementation between servers orked closely with backend developer to find ways to push the limits of existing Web technology Designed and developed the UI for the website with HTML XHTML CSS Java Script and AJAX Used AJAXJSON communication for accessing RESTfulweb services data payload Designed dynamic clientside JavaScript codes to build web forms and performed simulations for web application page Created and implemented SQL Queries Stored procedures Functions Packages and Triggers in SQL Server Successfully implemented Auto CompleteAuto Suggest functionality using JQuery Ajax Web Service and JSON Environment Python 25 JavaJ2EE Django10 HTMLCSS Linux Shell Scripting Java Script Ajax JQuery JSON XML PostgreSQL Jenkins ANT Maven Subversion Python Data AnalystData Modeler Kotak Mahindra Bank Mumbai Maharashtra January 2011 to November 2012 Description Kotak Mahindra Bank is an Indian private sector bank headquartered in Mumbai Maharashtra India In February 2003 Reserve Bank of India issued the licence to Kotak Mahindra Finance Ltd the groups flagship company to carry on banking business Responsibilities Analyzed data sources and requirements and business rules to perform logical and physical data modeling Analyzed and designed best fit logical and physical data models and relational database definitions using DB2 Generated reports of data definitions Involved in NormalizationDenormalization Normal Form and database design methodology Maintained existing ETL procedures fixed bugs and restored software to production environment Developed the code as per the clients requirements using SQL PLSQL and Data Warehousing concepts Involved in Dimensional modeling Star Schema of the Data warehouse and used Erwin to design the business process dimensions and measured facts Worked with Data Warehouse Extract and load developers to design mappings for Data Capture Staging Cleansing Loading and Auditing Developed enterprise data model management process to manage multiple data models developed by different groups Designed and created Data Marts as part of a data warehouse Wrote complex SQL queries for validating the data against different kinds of reports generated by Business Objects XIR2 Using Erwin modeling tool publishing of a data dictionary review of the model and dictionary with subject matter experts and generation of data definition language Coordinated with DBA in implementing the Database changes and also updating Data Models with changes implemented in development QA and Production Worked Extensively with DBA and Reporting team for improving the Report Performance with the Use of appropriate indexes and Partitioning Developed Data Mapping Transformation and Cleansing rules for the Master Data Management Architecture involved OLTP ODS and OLAP Tuned and coded optimization using different techniques like dynamic SQL dynamic cursors and tuning SQL queries writing generic procedures functions and packages Experienced in GUI Relational Database Management System RDBMS designing of OLAP system environment as well as Report Development Extensively used SQL TSQL and PLSQL to write stored procedures functions packages and triggers Analyzed of data report were prepared weekly biweekly monthly using MS Excel SQL UNIX Environment ER Studio Informatica Power Center 8191 Power Connect Power exchange Oracle 11 g MainframesDB2 MS SQL Server 2008 SQLPLSQL XML Windows NT 40 Tableau Workday SPSS SAS Business Objects XML Tableau Unix Shell Scripting Teradata Netezza Aginity Education Bachelors in Computer Science Royal College of Technology 2011 Skills Linux Solaris Sun Unix Additional Information Operating Systems All versions of Windows UNIX LINUX Macintosh HD Sun Solaris",
    "extracted_keywords": [
        "Data",
        "Scientist",
        "Machine",
        "Learning",
        "Data",
        "Scientist",
        "Machine",
        "Learning",
        "Data",
        "Scientist",
        "Machine",
        "Learning",
        "Rauxa",
        "New",
        "York",
        "NY",
        "Professional",
        "Data",
        "ScientistData",
        "Analyst",
        "years",
        "experience",
        "Data",
        "Science",
        "Analytics",
        "Data",
        "Mining",
        "Deep",
        "LearningMachine",
        "Learning",
        "Statistical",
        "Analysis",
        "data",
        "science",
        "project",
        "life",
        "cycle",
        "phases",
        "data",
        "data",
        "datavisualization",
        "data",
        "sets",
        "data",
        "ER",
        "diagrams",
        "schema",
        "machine",
        "learning",
        "algorithm",
        "regression",
        "KNN",
        "SVM",
        "forest",
        "network",
        "linear",
        "regression",
        "lasso",
        "regression",
        "kmeans",
        "Bagging",
        "model",
        "performance",
        "Experience",
        "data",
        "analysis",
        "tools",
        "Anaconda",
        "Jupiter",
        "Notebook",
        "4X",
        "R",
        "ggplot2",
        "dplyr",
        "Caret",
        "Excel",
        "ability",
        "SQL",
        "knowledge",
        "RDBMS",
        "SQL",
        "Server",
        "NoSql",
        "Excellent",
        "understanding",
        "Agile",
        "Scrum",
        "development",
        "methodology",
        "version",
        "control",
        "tools",
        "Git",
        "2X",
        "tools",
        "Apache",
        "MavenAnt",
        "Passionate",
        "information",
        "data",
        "assets",
        "culture",
        "decision",
        "Ability",
        "team",
        "atmosphere",
        "software",
        "life",
        "cycle",
        "SDLC",
        "Agile",
        "DevOps",
        "Scrum",
        "methodologies",
        "requirements",
        "test",
        "plans",
        "Regression",
        "Modeling",
        "Correlation",
        "Multivariate",
        "Analysis",
        "Model",
        "Building",
        "Business",
        "Intelligence",
        "tools",
        "application",
        "Statistical",
        "Concepts",
        "models",
        "Decision",
        "Tree",
        "Naive",
        "Bayes",
        "Logistic",
        "Regression",
        "Random",
        "Forest",
        "Social",
        "Network",
        "Analysis",
        "Cluster",
        "Analysis",
        "Neural",
        "Networks",
        "Machine",
        "Learning",
        "Statistical",
        "Analysis",
        "Python",
        "ScikitLearn",
        "Python",
        "data",
        "data",
        "loading",
        "extraction",
        "python",
        "libraries",
        "Matplotlib",
        "Scipy",
        "Numpy",
        "Pandas",
        "data",
        "analysis",
        "applications",
        "RR",
        "Shiny",
        "SAS",
        "ArcGIS",
        "Matlab",
        "SPSS",
        "network",
        "cluster",
        "analysis",
        "Strong",
        "SQL",
        "programming",
        "skills",
        "experience",
        "functions",
        "packages",
        "Expertise",
        "business",
        "requirements",
        "algorithms",
        "models",
        "building",
        "models",
        "data",
        "mining",
        "reporting",
        "solutions",
        "volume",
        "data",
        "data",
        "data",
        "manipulation",
        "data",
        "architecture",
        "data",
        "ingestion",
        "data",
        "preparation",
        "methods",
        "describe",
        "data",
        "contents",
        "statistics",
        "data",
        "regex",
        "Remap",
        "subset",
        "reindex",
        "melt",
        "reshape",
        "NoSQLDatabase",
        "Hbase",
        "Cassandra",
        "MongoDB",
        "Big",
        "Data",
        "Hadoop",
        "MapReduce",
        "HDFS",
        "Spark",
        "Data",
        "Integration",
        "Validation",
        "Data",
        "Quality",
        "ETL",
        "process",
        "Data",
        "Warehousing",
        "MS",
        "Visual",
        "Studio",
        "SSAS",
        "SSISand",
        "SSRS",
        "Proficient",
        "Tableau",
        "RShiny",
        "data",
        "visualization",
        "tools",
        "insights",
        "datasets",
        "reports",
        "dashboards",
        "reports",
        "SQL",
        "andPython",
        "BI",
        "platform",
        "Tableau",
        "development",
        "environment",
        "Git",
        "VM",
        "Excellent",
        "communication",
        "skills",
        "environment",
        "team",
        "learner",
        "US",
        "employer",
        "Work",
        "Experience",
        "Data",
        "Scientist",
        "Machine",
        "Learning",
        "Rauxa",
        "New",
        "York",
        "NY",
        "August",
        "Present",
        "Description",
        "Makers",
        "results",
        "Rauxa",
        "data",
        "technology",
        "content",
        "impact",
        "speed",
        "clients",
        "Gap",
        "Inc",
        "TGI",
        "Fridays",
        "Verizon",
        "countrys",
        "advertising",
        "agency",
        "Responsibilities",
        "data",
        "HDFS",
        "data",
        "analysis",
        "data",
        "models",
        "techniques",
        "Bayesian",
        "HMM",
        "Machine",
        "Learning",
        "classification",
        "models",
        "XGBoost",
        "SVM",
        "Random",
        "Forest",
        "phases",
        "data",
        "mining",
        "data",
        "data",
        "collection",
        "models",
        "validation",
        "visualization",
        "analysis",
        "Data",
        "Science",
        "program",
        "Data",
        "ManipulationVisualization",
        "Web",
        "Machine",
        "Learning",
        "Python",
        "programming",
        "SQL",
        "GIT",
        "MongoDB",
        "Hadoop",
        "Setup",
        "storage",
        "data",
        "analysis",
        "tools",
        "AWS",
        "cloud",
        "infrastructure",
        "Caffe",
        "Deep",
        "Learning",
        "Framework",
        "data",
        "formats",
        "JSON",
        "XML",
        "machine",
        "algorithms",
        "Python",
        "Worked",
        "Data",
        "Architects",
        "IT",
        "Architects",
        "movement",
        "data",
        "storage",
        "ER",
        "Studio",
        "pandas",
        "numpy",
        "matplotlib",
        "NLTK",
        "Python",
        "machine",
        "learning",
        "Data",
        "Manipulation",
        "Aggregation",
        "source",
        "Nexus",
        "Business",
        "Toad",
        "Power",
        "BI",
        "Smart",
        "View",
        "Agile",
        "Methodology",
        "application",
        "Focus",
        "integration",
        "overlap",
        "Informatica",
        "commitment",
        "MDM",
        "acquisition",
        "Identity",
        "Systems",
        "packages",
        "SPCfile",
        "data",
        "spectra",
        "samples",
        "procedures",
        "costs",
        "utility",
        "Python",
        "packages",
        "scipy",
        "Implemented",
        "Classification",
        "algorithms",
        "Logistic",
        "Regression",
        "Decision",
        "trees",
        "Bayes",
        "KNN",
        "Architect",
        "OLAPdatabasescubes",
        "scorecards",
        "dashboards",
        "reports",
        "Updated",
        "Python",
        "scripts",
        "training",
        "data",
        "database",
        "AWS",
        "Cloud",
        "Search",
        "document",
        "response",
        "label",
        "classification",
        "Teradata",
        "utilities",
        "Fast",
        "Export",
        "MLOAD",
        "tasks",
        "data",
        "migrationETL",
        "OLTP",
        "Source",
        "Systems",
        "OLAP",
        "Target",
        "Systems",
        "Data",
        "transformation",
        "resources",
        "data",
        "organization",
        "extraction",
        "machine",
        "classifiers",
        "ROC",
        "Curves",
        "Lift",
        "Charts",
        "Environment",
        "Unix",
        "Python",
        "MLLib",
        "SAS",
        "regression",
        "regression",
        "Hadoop",
        "NoSQL",
        "Teradata",
        "OLTP",
        "forest",
        "OLAP",
        "HDFS",
        "ODS",
        "NLTK",
        "SVM",
        "JSON",
        "XML",
        "MapReduce",
        "Data",
        "Scientist",
        "Charter",
        "communication",
        "St",
        "Louis",
        "MO",
        "May",
        "July",
        "Description",
        "Charter",
        "Communications",
        "Inc",
        "telecommunications",
        "media",
        "company",
        "services",
        "consumers",
        "businesses",
        "branding",
        "Spectrum",
        "Responsibilities",
        "Spark",
        "Scala",
        "Hadoop",
        "HQL",
        "VQL",
        "oozie",
        "pySpark",
        "Data",
        "Lake",
        "TensorFlow",
        "HBase",
        "Cassandra",
        "Redshift",
        "MongoDB",
        "Kafka",
        "Kinesis",
        "Spark",
        "Streaming",
        "Edward",
        "CUDA",
        "AWS",
        "Python",
        "variety",
        "machine",
        "learning",
        "methods",
        "classifications",
        "regressions",
        "reduction",
        "engine",
        "user",
        "lifetime",
        "user",
        "conversations",
        "target",
        "categories",
        "Application",
        "machine",
        "algorithms",
        "modeling",
        "decision",
        "trees",
        "text",
        "analytics",
        "language",
        "processing",
        "NLP",
        "regression",
        "models",
        "network",
        "analysis",
        "networks",
        "SVM",
        "Volume",
        "package",
        "python",
        "Matlab",
        "Worked",
        "data",
        "Google",
        "Analytics",
        "AdWords",
        "Facebook",
        "models",
        "Cross",
        "Validation",
        "Log",
        "loss",
        "function",
        "ROC",
        "curves",
        "AUC",
        "feature",
        "selection",
        "technologies",
        "ElasticSearch",
        "Kibana",
        "Performed",
        "Data",
        "Profiling",
        "behavior",
        "features",
        "traffic",
        "pattern",
        "location",
        "Date",
        "Time",
        "comments",
        "clusters",
        "networking",
        "sites",
        "Sentiment",
        "Analysis",
        "Text",
        "Analytics",
        "Performed",
        "Multinomial",
        "Logistic",
        "Regression",
        "Decision",
        "Tree",
        "Random",
        "forest",
        "SVM",
        "package",
        "time",
        "route",
        "Performed",
        "data",
        "analysis",
        "Hive",
        "data",
        "Hadoop",
        "cluster",
        "Sql",
        "Oracle",
        "database",
        "ETL",
        "data",
        "transformation",
        "Performed",
        "Data",
        "Cleaning",
        "features",
        "engineering",
        "pandas",
        "packages",
        "python",
        "DAGs",
        "dependencies",
        "logs",
        "AirFlow",
        "pipelines",
        "automation",
        "Performed",
        "data",
        "cleaning",
        "feature",
        "selection",
        "MLlib",
        "package",
        "PySpark",
        "frameworks",
        "Caffe",
        "Neon",
        "SparkScalaR",
        "Python",
        "expression",
        "regex",
        "project",
        "environment",
        "LinuxWindows",
        "data",
        "resources",
        "clustering",
        "technique",
        "KMeans",
        "outliers",
        "data",
        "Tracking",
        "operations",
        "sensors",
        "criteria",
        "AirFlowtechnology",
        "Data",
        "mapping",
        "activities",
        "Source",
        "systems",
        "Teradata",
        "utilities",
        "TPump",
        "FEXPBTEQ",
        "MLOAD",
        "FLOAD",
        "traffic",
        "patterns",
        "autocorrelation",
        "time",
        "lags",
        "model",
        "False",
        "Positive",
        "Rate",
        "Text",
        "classification",
        "sentiment",
        "analysis",
        "data",
        "algorithm",
        "regularization",
        "methods",
        "L1",
        "L2",
        "Principal",
        "Component",
        "Analysis",
        "feature",
        "engineering",
        "data",
        "MLlib",
        "Sparks",
        "Machine",
        "library",
        "models",
        "rule",
        "expertise",
        "system",
        "results",
        "analysis",
        "information",
        "people",
        "departments",
        "reports",
        "metrics",
        "conclusions",
        "behavior",
        "MapReduce",
        "pipeline",
        "feature",
        "extraction",
        "Hive",
        "Pig",
        "Created",
        "Data",
        "Quality",
        "Scripts",
        "SQL",
        "Hive",
        "data",
        "load",
        "quality",
        "data",
        "types",
        "data",
        "visualizations",
        "Python",
        "Tableau",
        "results",
        "operations",
        "team",
        "decisions",
        "data",
        "needs",
        "requirements",
        "departments",
        "Environment",
        "Python",
        "CDH5",
        "HDFS",
        "Hadoop",
        "Hive",
        "Impala",
        "Linux",
        "Spark",
        "Tableau",
        "Desktop",
        "SQL",
        "Server",
        "Microsoft",
        "Excel",
        "Matlab",
        "Spark",
        "SQL",
        "Pyspark",
        "Data",
        "analyst",
        "EDAC",
        "Technologies",
        "Corp",
        "Cheshire",
        "CT",
        "January",
        "April",
        "Description",
        "EDAC",
        "Technologies",
        "Corporation",
        "design",
        "manufacturing",
        "services",
        "tooling",
        "fixtures",
        "molds",
        "jet",
        "engine",
        "components",
        "machine",
        "spindles",
        "semiconductor",
        "device",
        "markets",
        "Responsibilities",
        "BI",
        "team",
        "report",
        "requirements",
        "Sqoop",
        "data",
        "HDFS",
        "Hive",
        "phases",
        "Analytics",
        "R",
        "Python",
        "Jupyter",
        "notebook",
        "Data",
        "collection",
        "treatment",
        "data",
        "data",
        "entry",
        "errorsclassification",
        "errors",
        "criteria",
        "values",
        "Data",
        "Mining",
        "cluster",
        "analysis",
        "customer",
        "Decision",
        "trees",
        "customers",
        "Market",
        "Basket",
        "Analysis",
        "customer",
        "behaviour",
        "partproduct",
        "association",
        "Map",
        "Reduce",
        "jobs",
        "Java",
        "data",
        "Assisted",
        "data",
        "capacity",
        "planning",
        "node",
        "forecasting",
        "Configured",
        "Flume",
        "Infrastructure",
        "Administrator",
        "Pig",
        "Hive",
        "HBase",
        "updates",
        "patches",
        "upgrades",
        "claims",
        "processing",
        "team",
        "patterns",
        "filing",
        "claims",
        "upgrade",
        "cluster",
        "CDH3u6",
        "CDH440",
        "Developed",
        "Map",
        "programs",
        "data",
        "sets",
        "results",
        "Sqoop",
        "Patterns",
        "claims",
        "text",
        "mining",
        "R",
        "Hive",
        "data",
        "information",
        "Sqoop",
        "data",
        "claims",
        "processing",
        "team",
        "claim",
        "data",
        "Developed",
        "Map",
        "programs",
        "data",
        "staging",
        "tables",
        "data",
        "tables",
        "EDW",
        "Created",
        "tables",
        "Hive",
        "Map",
        "Reduce",
        "jobs",
        "data",
        "HiveQL",
        "queries",
        "information",
        "Hive",
        "queries",
        "market",
        "analysts",
        "trends",
        "data",
        "EDW",
        "reference",
        "tables",
        "metrics",
        "data",
        "files",
        "sources",
        "HDFS",
        "Flume",
        "reviews",
        "advantages",
        "Oozie",
        "data",
        "loading",
        "Hadoop",
        "Distributed",
        "File",
        "System",
        "PIG",
        "data",
        "design",
        "recommendations",
        "leadership",
        "sponsorsstakeholders",
        "review",
        "processes",
        "problems",
        "Hadoop",
        "log",
        "files",
        "data",
        "performance",
        "scripts",
        "Environment",
        "HDFS",
        "PIG",
        "HIVE",
        "Map",
        "Linux",
        "HBase",
        "Flume",
        "Sqoop",
        "R",
        "VMware",
        "Eclipse",
        "Cloudera",
        "Python",
        "Data",
        "ModelerData",
        "Analyst",
        "Dorman",
        "Products",
        "Inc",
        "Colmar",
        "PA",
        "March",
        "December",
        "Description",
        "Dorman",
        "Products",
        "Inc",
        "replacement",
        "parts",
        "hardware",
        "brake",
        "products",
        "aftermarket",
        "merchandise",
        "markets",
        "United",
        "States",
        "Canada",
        "Mexico",
        "Europe",
        "Middle",
        "East",
        "Australia",
        "Responsibilities",
        "models",
        "data",
        "mart",
        "Created",
        "partitions",
        "indexes",
        "tables",
        "data",
        "mart",
        "Performed",
        "data",
        "profiling",
        "analysis",
        "data",
        "cleansing",
        "rules",
        "data",
        "standards",
        "models",
        "metadata",
        "data",
        "definitions",
        "table",
        "structures",
        "version",
        "data",
        "model",
        "Developed",
        "SQL",
        "scripts",
        "tables",
        "Sequences",
        "Triggers",
        "views",
        "views",
        "query",
        "optimization",
        "performance",
        "SQL",
        "Profiler",
        "performance",
        "mappings",
        "Fact",
        "Dimension",
        "SCD",
        "Type",
        "SCD",
        "Type",
        "dimensions",
        "loading",
        "unit",
        "mappings",
        "Erwins",
        "engineering",
        "tools",
        "target",
        "database",
        "schema",
        "conversion",
        "process",
        "enterprise",
        "Model",
        "EDM",
        "products",
        "services",
        "Teradata",
        "Environment",
        "data",
        "PDM",
        "Conceived",
        "model",
        "scratch",
        "Building",
        "reports",
        "dashboards",
        "scheduling",
        "Tableau",
        "server",
        "Write",
        "SQL",
        "scripts",
        "mappings",
        "Traceability",
        "Matrix",
        "Business",
        "Requirements",
        "Scripts",
        "Change",
        "Control",
        "requirements",
        "test",
        "case",
        "development",
        "testing",
        "conversion",
        "programs",
        "Data",
        "text",
        "files",
        "map",
        "Oracle",
        "Database",
        "PERL",
        "shell",
        "scripts",
        "SQLLoader",
        "DATA",
        "validation",
        "SQL",
        "queries",
        "testing",
        "data",
        "quality",
        "issues",
        "load",
        "scripts",
        "Teradata",
        "client",
        "utilities",
        "MULTILOAD",
        "FASTLOAD",
        "BTEQ",
        "Exporting",
        "data",
        "platforms",
        "SAS",
        "MSExcel",
        "reports",
        "analysis",
        "data",
        "SQL",
        "Server",
        "Reporting",
        "Services",
        "SSRS",
        "ETL",
        "team",
        "Transformation",
        "Rules",
        "Data",
        "Migration",
        "OLTP",
        "Warehouse",
        "Environment",
        "purposes",
        "SQL",
        "scripts",
        "data",
        "quality",
        "issues",
        "keys",
        "data",
        "anomalies",
        "data",
        "validation",
        "issues",
        "data",
        "sets",
        "SAS",
        "Format",
        "statement",
        "data",
        "step",
        "Proc",
        "Format",
        "Applied",
        "Business",
        "practices",
        "development",
        "focus",
        "reusability",
        "performance",
        "Developed",
        "Tableau",
        "visualizations",
        "dashboards",
        "Tableau",
        "Desktop",
        "Graphical",
        "Entity",
        "Relationship",
        "database",
        "design",
        "interface",
        "type",
        "STAR",
        "schemas",
        "data",
        "marts",
        "plan",
        "data",
        "marts",
        "OLAP",
        "environment",
        "Environment",
        "Erwin",
        "MS",
        "SQL",
        "Server",
        "DB2",
        "Oracle",
        "SQL",
        "Developer",
        "PLSQL",
        "Business",
        "Erwin",
        "MS",
        "office",
        "suite",
        "Windows",
        "XP",
        "TOAD",
        "SQLPLUS",
        "SQLLOADER",
        "Teradata",
        "Netezza",
        "SAS",
        "Tableau",
        "Business",
        "Objects",
        "SSRS",
        "tableau",
        "SQL",
        "Assistant",
        "Informatica",
        "XML",
        "Python",
        "Developer",
        "Dabur",
        "India",
        "Ltd",
        "Ghaziabad",
        "Uttar",
        "Pradesh",
        "December",
        "February",
        "Description",
        "Dabur",
        "Indias",
        "medicine",
        "consumer",
        "products",
        "manufacturer",
        "Dabur",
        "Pharma",
        "business",
        "company",
        "Dabur",
        "Pharma",
        "Ltd",
        "company",
        "Fresenius",
        "SE",
        "equity",
        "stake",
        "Dabur",
        "Pharma",
        "June",
        "Rs",
        "share",
        "Responsibilities",
        "design",
        "development",
        "phases",
        "application",
        "methodology",
        "databases",
        "Python",
        "Python",
        "API",
        "RESTful",
        "Web",
        "Service",
        "Flask",
        "SQLAlchemy",
        "PostgreSQL",
        "UI",
        "website",
        "HTML",
        "XHTML",
        "AJAX",
        "CSS",
        "JavaScript",
        "requirement",
        "gathering",
        "architect",
        "modeling",
        "Restful",
        "web",
        "services",
        "client",
        "server",
        "changes",
        "SOAP",
        "RESTFUL",
        "Technology",
        "analysis",
        "requirement",
        "documents",
        "SQL",
        "functions",
        "triggers",
        "cursors",
        "types",
        "sequences",
        "indexes",
        "repositories",
        "Source",
        "Trees",
        "interface",
        "GIT",
        "client",
        "GIT",
        "command",
        "lines",
        "Stash",
        "Responsible",
        "Python",
        "REST",
        "API",
        "framework",
        "spring",
        "frame",
        "work",
        "Django",
        "Develope",
        "consumer",
        "features",
        "applications",
        "Python",
        "Django",
        "HTML",
        "behavior",
        "Driven",
        "Development",
        "BDD",
        "programming",
        "components",
        "Python",
        "Django",
        "framework",
        "code",
        "python",
        "manipulate",
        "data",
        "development",
        "enterprise",
        "network",
        "application",
        "Python",
        "Twisted",
        "Cassandra",
        "Python",
        "Django",
        "graphics",
        "XML",
        "processing",
        "documents",
        "data",
        "exchange",
        "business",
        "logic",
        "implementation",
        "servers",
        "developer",
        "ways",
        "limits",
        "Web",
        "technology",
        "UI",
        "website",
        "HTML",
        "XHTML",
        "CSS",
        "Java",
        "Script",
        "AJAX",
        "AJAXJSON",
        "communication",
        "RESTfulweb",
        "services",
        "data",
        "payload",
        "JavaScript",
        "codes",
        "web",
        "forms",
        "simulations",
        "web",
        "application",
        "page",
        "SQL",
        "Queries",
        "procedures",
        "Functions",
        "Packages",
        "Triggers",
        "SQL",
        "Server",
        "Successfully",
        "Auto",
        "CompleteAuto",
        "Suggest",
        "functionality",
        "JQuery",
        "Ajax",
        "Web",
        "Service",
        "JSON",
        "Environment",
        "Python",
        "JavaJ2EE",
        "Django10",
        "HTMLCSS",
        "Linux",
        "Shell",
        "Scripting",
        "Java",
        "Script",
        "Ajax",
        "JQuery",
        "JSON",
        "XML",
        "PostgreSQL",
        "Jenkins",
        "ANT",
        "Maven",
        "Subversion",
        "Python",
        "Data",
        "AnalystData",
        "Modeler",
        "Kotak",
        "Mahindra",
        "Bank",
        "Mumbai",
        "Maharashtra",
        "January",
        "November",
        "Description",
        "Kotak",
        "Mahindra",
        "Bank",
        "sector",
        "bank",
        "Mumbai",
        "Maharashtra",
        "India",
        "February",
        "Reserve",
        "Bank",
        "India",
        "licence",
        "Kotak",
        "Mahindra",
        "Finance",
        "Ltd",
        "groups",
        "flagship",
        "company",
        "banking",
        "business",
        "Responsibilities",
        "data",
        "sources",
        "requirements",
        "business",
        "rules",
        "data",
        "data",
        "models",
        "database",
        "definitions",
        "DB2",
        "Generated",
        "reports",
        "data",
        "definitions",
        "NormalizationDenormalization",
        "Normal",
        "Form",
        "database",
        "design",
        "methodology",
        "ETL",
        "procedures",
        "bugs",
        "software",
        "production",
        "environment",
        "code",
        "clients",
        "requirements",
        "SQL",
        "PLSQL",
        "Data",
        "Warehousing",
        "concepts",
        "modeling",
        "Star",
        "Schema",
        "Data",
        "warehouse",
        "Erwin",
        "business",
        "process",
        "dimensions",
        "facts",
        "Data",
        "Warehouse",
        "Extract",
        "developers",
        "mappings",
        "Data",
        "Capture",
        "Staging",
        "Cleansing",
        "Loading",
        "Auditing",
        "enterprise",
        "data",
        "model",
        "management",
        "process",
        "data",
        "models",
        "groups",
        "Data",
        "Marts",
        "part",
        "data",
        "warehouse",
        "Wrote",
        "SQL",
        "data",
        "kinds",
        "reports",
        "Business",
        "Objects",
        "XIR2",
        "Erwin",
        "modeling",
        "tool",
        "publishing",
        "data",
        "review",
        "model",
        "subject",
        "matter",
        "experts",
        "generation",
        "data",
        "definition",
        "language",
        "DBA",
        "Database",
        "changes",
        "Data",
        "Models",
        "changes",
        "development",
        "QA",
        "Production",
        "DBA",
        "Reporting",
        "team",
        "Report",
        "Performance",
        "Use",
        "indexes",
        "Developed",
        "Data",
        "Mapping",
        "Transformation",
        "Cleansing",
        "rules",
        "Master",
        "Data",
        "Management",
        "Architecture",
        "OLTP",
        "ODS",
        "OLAP",
        "optimization",
        "techniques",
        "SQL",
        "cursors",
        "SQL",
        "queries",
        "procedures",
        "functions",
        "packages",
        "GUI",
        "Relational",
        "Database",
        "Management",
        "System",
        "RDBMS",
        "designing",
        "OLAP",
        "system",
        "environment",
        "Report",
        "Development",
        "SQL",
        "TSQL",
        "PLSQL",
        "procedures",
        "functions",
        "packages",
        "triggers",
        "data",
        "report",
        "MS",
        "Excel",
        "SQL",
        "UNIX",
        "Environment",
        "ER",
        "Studio",
        "Informatica",
        "Power",
        "Center",
        "8191",
        "Power",
        "Connect",
        "Power",
        "exchange",
        "Oracle",
        "g",
        "MainframesDB2",
        "MS",
        "SQL",
        "Server",
        "SQLPLSQL",
        "XML",
        "Windows",
        "NT",
        "Tableau",
        "Workday",
        "SPSS",
        "SAS",
        "Business",
        "XML",
        "Tableau",
        "Unix",
        "Shell",
        "Scripting",
        "Teradata",
        "Netezza",
        "Aginity",
        "Education",
        "Bachelors",
        "Computer",
        "Science",
        "Royal",
        "College",
        "Technology",
        "Skills",
        "Linux",
        "Solaris",
        "Sun",
        "Unix",
        "Additional",
        "Information",
        "Operating",
        "Systems",
        "versions",
        "Windows",
        "UNIX",
        "LINUX",
        "Macintosh",
        "HD",
        "Sun",
        "Solaris"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T19:52:25.735335",
    "resume_data": "Data Scientist Machine Learning Data Scientist Machine Learning Data Scientist Machine Learning Rauxa New York NY Professional qualified Data ScientistData Analyst with around 8 years of experience in Data Science and Analytics including Data Mining Deep LearningMachine Learning and Statistical Analysis Involved in the entire data science project life cycle and actively involved in all the phases including data cleaning data extractionand datavisualization with large data sets of structured and unstructured data created ER diagrams and schema Experienced with machine learning algorithm such as logistic regression KNN SVM random forest neural network linear regression lasso regression and kmeans Implemented Bagging and Boosting to enhance the model performance Experience in implementing data analysis with various analytic tools such as Anaconda 40 Jupiter Notebook 4X R 30 ggplot2 dplyr Caret and Excel Solid ability to write and optimize diverse SQL queries working knowledge of RDBMS like SQL Server 200820102012 NoSql databases like MongoDB 32 Excellent understanding Agile and Scrum development methodology Used the version control tools like Git 2X and build tools like Apache MavenAnt Passionate about gleaning insightful information from massive data assets and developing a culture of sound datadriven decision making Ability to maintain a fun casual professional and productive team atmosphere Experienced the full software life cycle in SDLC Agile DevOps and Scrum methodologies including creating requirements test plans Skilled in Advanced Regression Modeling Correlation Multivariate Analysis Model Building Business Intelligence tools and application of Statistical Concepts Developed predictive models using Decision Tree Naive Bayes Logistic Regression Random Forest Social Network Analysis Cluster Analysis and Neural Networks Experienced in Machine Learning and Statistical Analysis with Python ScikitLearn Experienced in Python to manipulate data for data loading and extraction and worked with python libraries like Matplotlib Scipy Numpy and Pandas for data analysis Worked with complex applications such as RR Shiny SAS Plotly ArcGIS Matlab and SPSS to develop neural network cluster analysis Strong SQL programming skills with experience in working with functions packages and triggers Expertise in transforming business requirements into designing algorithms analytical models building models developing data mining and reporting solutions that scales across massive volume of structured and unstructured data Skilled in performing data parsing data manipulation data architecture data ingestion and data preparation with methods including describe data contents compute descriptive statistics of data regex split and combine merge Remap subset reindex melt and reshape Worked with NoSQLDatabase including Hbase Cassandra and MongoDB Experienced in Big Data with Hadoop MapReduce HDFS and Spark Experienced in Data Integration Validation and Data Quality controls for ETL process and Data Warehousing using MS Visual Studio SSAS SSISand SSRS Proficient in Tableau and RShiny data visualization tools to analyze and obtain insights into large datasets create visually powerful and actionable interactive reports and dashboards Automated recurring reports using SQL andPython and visualized them on BI platform like Tableau Worked in development environment like Git and VM Excellent communication skills Successfully working in fastpaced multitasking environment both independently and in collaborative team a selfmotivated enthusiastic learner Authorized to work in the US for any employer Work Experience Data Scientist Machine Learning Rauxa New York NY August 2018 to Present Description Makers of results Rauxa applies data technology and content to create measurable impact at maximum speed for clients that include Gap Inc TGI Fridays and Verizon The countrys largest womanowned independent advertising agency Responsibilities Extracted data from HDFS and prepared data for exploratory analysis using data munging Built models using Statistical techniques like Bayesian HMM and Machine Learning classification models like XGBoost SVM and Random Forest Participated in all phases of data mining data cleaning data collection developing models validation visualization and performed Gap analysis A highly immersive Data Science program involving Data ManipulationVisualization Web Scraping Machine Learning Python programming SQL GIT MongoDB Hadoop Setup storage and data analysis tools in AWS cloud computing infrastructure Installed and used Caffe Deep Learning Framework Worked on different data formats such as JSON XML and performed machine learning algorithms in Python Worked as Data Architects and IT Architects to understand the movement of data and its storage and ER Studio 97 Used pandas numpy seaborn matplotlib scikitlearn scipy NLTK in Python for developing various machine learning algorithms Data Manipulation and Aggregation from different source using Nexus Business Objects Toad Power BI and Smart View Implemented Agile Methodology for building an internal application Focus on integration overlap and Informatica newer commitment to MDM with the acquisition of Identity Systems Coded proprietary packages to analyze and visualize SPCfile data to identify bad spectra and samples to reduce unnecessary procedures and costs Programmed a utility in Python that used multiple packages numpy scipy pandas Implemented Classification using supervised algorithms like Logistic Regression Decision trees Naive Bayes KNN As Architect delivered various complex OLAPdatabasescubes scorecards dashboards and reports Updated Python scripts to match training data with our database stored in AWS Cloud Search so that we would be able to assign each document a response label for further classification Used Teradata utilities such as Fast Export MLOAD for handling various tasks data migrationETL from OLTP Source Systems to OLAP Target Systems Data transformation from various resources data organization features extraction from raw and stored Validated the machine learning classifiers using ROC Curves and Lift Charts Environment Unix Python 352 MLLib SAS regression logistic regression Hadoop 274 NoSQL Teradata OLTP random forest OLAP HDFS ODS NLTK SVM JSON XML and MapReduce Data Scientist Charter communication St Louis MO May 2017 to July 2018 Description Charter Communications Inc is an American telecommunications and mass media company that offers its services to consumers and businesses under the branding of Spectrum Responsibilities Utilized Spark Scala Hadoop HQL VQL oozie pySpark Data Lake TensorFlow HBase Cassandra Redshift MongoDB Kafka Kinesis Spark Streaming Edward CUDA MLLib AWS Python a broad variety of machine learning methods including classifications regressions dimensionally reduction etc Utilized the engine to increase user lifetime by 45 and triple user conversations for target categories Application of various machine learning algorithms and statistical modeling like decision trees text analytics natural language processing NLP supervised and unsupervised regression models social network analysis neural networks deep learning SVM clustering to identify Volume using scikitlearn package in python Matlab Worked onanalyzing data from Google Analytics AdWords Facebook etc Evaluated models using Cross Validation Log loss function ROC curves and used AUC for feature selection and elastic technologies like ElasticSearch Kibana Performed Data Profiling to learn about behavior with various features such as traffic pattern location Date and Time etc Categorized comments into positive and negative clusters from different social networking sites using Sentiment Analysis and Text Analytics Performed Multinomial Logistic Regression Decision Tree Random forest SVM to classify package is going to deliver on time for the new route Performed data analysis by using Hive to retrieve the data from Hadoop cluster Sql to retrieve datafrom Oracle database and used ETL for data transformation Performed Data Cleaning features scaling features engineering using pandas and numpy packages in python Exploring DAGs their dependencies and logs using AirFlow pipelines for automation Performed data cleaning and feature selection using MLlib package in PySpark and working with deep learning frameworks such as Caffe Neon Developed SparkScalaR Python for regular expression regex project in the HadoopHive environment with LinuxWindows for big data resources Used clustering technique KMeans to identify outliers and to classify unlabeled data Tracking operations using sensors until certain criteria is met using AirFlowtechnology Responsible for different Data mapping activities from Source systems to Teradata using utilities like TPump FEXPBTEQ MLOAD FLOAD etc Analyze traffic patterns by calculating autocorrelation with different time lags Ensured that the model has low False Positive Rate and Text classification and sentiment analysis for unstructured and semistructured data Addressed over fitting by implementing of the algorithm regularization methods like L1 and L2 Used Principal Component Analysis in feature engineering to analyze high dimensional data Used MLlib Sparks Machine learning library to build and evaluate different models Implemented rule based expertise system from the results of exploratory analysis and information gathered from the people from different departments Created and designed reports that will use gathered metrics to infer and draw logical conclusions of past and future behavior Developed MapReduce pipeline for feature extraction using Hive and Pig Created Data Quality Scripts using SQL and Hive to validate successful data load and quality of the data Created various types of data visualizations using Python and Tableau Communicated the results with operations team for taking best decisions Collected data needs and requirements by Interacting with the other departments Environment Python 2x CDH5 HDFS Hadoop 23 Hive Impala AWS Linux Spark Tableau Desktop SQL Server 2014 Microsoft Excel Matlab Spark SQL Pyspark Data analyst EDAC Technologies Corp Cheshire CT January 2016 to April 2017 Description EDAC Technologies Corporation provides design manufacturing and services for tooling fixtures molds jet engine components and machine spindles in the aerospace industrial semiconductor and medical device markets Responsibilities Worked with BI team in gathering the report requirements and also Sqoop to export data into HDFS and Hive Involved in the below phases of Analytics using R Python and Jupyter notebook a Data collection and treatment Analysed existing internal data and external data worked on entry errorsclassification errors and defined criteria for missing values b Data Mining Used cluster analysis for identifying customer segments Decision trees used for profitable and nonprofitable customers Market Basket Analysis used for customer purchasing behaviour and partproduct association Developed multiple Map Reduce jobs in Java for data cleaning and preprocessing Assisted with data capacity planning and node forecasting Installed Configured and managed Flume Infrastructure Administrator for Pig Hive and HBase installing updates patches and upgrades Worked closely with the claims processing team to obtain patterns in filing of fraudulent claims Worked on performing major upgrade of cluster from CDH3u6 to CDH440 Developed Map Reduce programs to extract and transform the data sets and results were exported back to RDBMS using Sqoop Patterns were observed in fraudulent claims using text mining in R and Hive Exported the data required information to RDBMS using Sqoop to make the data available for the claims processing team to assist in processing a claim based on the data Developed Map Reduce programs to parse the raw data populate staging tables and store the refined data in partitioned tables in the EDW Created tables in Hive and loaded the structured resulted from Map Reduce jobs data Using HiveQL developed many queries and extracted the required information Created Hive queries that helped market analysts spot emerging trends by comparing fresh data with EDW reference tables and historical metrics Was responsible for importing the data mostly log files from various sources into HDFS using Flume Enabled speedy reviews and first mover advantages by using Oozie to automate data loading into the Hadoop Distributed File System and PIG to preprocess the data Provided design recommendations and thought leadership to sponsorsstakeholders that improved review processes and resolved technical problems Managed and reviewed Hadoop log files Tested raw data and executed performance scripts Environment HDFS PIG HIVE Map Reduce Linux HBase Flume Sqoop R VMware Eclipse Cloudera Python Data ModelerData Analyst Dorman Products Inc Colmar PA March 2014 to December 2015 Description Dorman Products Inc supplies automotive replacement parts automotive hardware and brake products to the automotive aftermarket and mass merchandise markets in the United States Canada Mexico Europe the Middle East and Australia Responsibilities Created and maintained Logical and Physical models for the data mart Created partitions and indexes for the tables in the data mart Performed data profiling and analysis applied various data cleansing rules designed data standards and architecturedesigned the relational models Maintained metadata data definitions of table structures and version controlling for the data model Developed SQL scripts for creating tables Sequences Triggers views and materialized views Worked on query optimization and performance tuning using SQL Profiler and performance monitoring Developed mappings to load Fact and Dimension tables SCD Type 1 and SCD Type 2 dimensions and Incremental loading and unit tested the mappings Utilized Erwins forward reverse engineering tools and target database schema conversion process Worked on creating enterprise wide Model EDM for products and services in Teradata Environment based on the data from PDM Conceived designed developed and implemented this model from the scratch Building publishing customized interactive reports and dashboards report scheduling using Tableau server Write SQL scripts to test the mappings and Developed Traceability Matrix of Business Requirements mapped to Test Scripts to ensure any Change Control in requirements leads to test case update Responsible for development and testing of conversion programs for importing Data from text files into map Oracle Database utilizing PERL shell scripts SQLLoader Involved in extensive DATA validation by writing several complex SQL queries and Involved in backend testing and worked with data quality issues Developed and executed load scripts using Teradata client utilities MULTILOAD FASTLOAD and BTEQ Exporting and importing the data between different platforms such as SAS MSExcel Generated periodic reports based on the statistical analysis of the data using SQL Server Reporting Services SSRS Worked with the ETL team to document the Transformation Rules for Data Migration from OLTP to Warehouse Environment for reporting purposes Created SQL scripts to find data quality issues and to identify keys data anomalies and data validation issues Formatting the data sets read into SAS by using Format statement in the data step as well as Proc Format Applied Business Objects best practices during development with a strong focus on reusability and better performance Developed Tableau visualizations and dashboards using Tableau Desktop Used Graphical Entity Relationship Diagramming to create new database design via easy to use graphical interface Designed different type of STAR schemas for detailed data marts and plan data marts in the OLAP environment Environment Erwin MS SQL Server 2008 DB2 Oracle SQL Developer PLSQL Business Objects Erwin MS office suite Windows XP TOAD SQLPLUS SQLLOADER Teradata Netezza SAS Tableau Business Objects SSRS tableau SQL Assistant Informatica XML Python Developer Dabur India Ltd Ghaziabad Uttar Pradesh December 2012 to February 2014 Description Dabur is one of the Indias largest Ayurvedic medicine natural consumer products manufacturer Dabur demerged its Pharma business in 2003 and hived it off into a separate company Dabur Pharma Ltd German company Fresenius SE bought a 7327 equity stake in Dabur Pharma in June 2008 at Rs 7650 a share Responsibilities Involved in the design development and testing phases of application using AGILE methodology Designed and maintained databases using Python and developed Python based API RESTful Web Service using Flask SQLAlchemy and PostgreSQL Designed and developed the UI of the website using HTML XHTML AJAX CSS and JavaScript Participated in requirement gathering and worked closely with the architect in designing and modeling Worked on Restful web services which enforced a stateless client server and support JSON few changes from SOAP to RESTFUL Technology Involved in detailed analysis based on the requirement documents Involved in writing SQL queries implementing functions triggers cursors object types sequences indexes etc Created and managed all of hosted or local repositories through Source Trees simple interface of GIT client collaborated with GIT command lines and Stash Responsible for setting up Python REST API framework and spring frame work using Django Develope consumer based features and applications using Python Django HTML behavior Driven Development BDD and pair based programming Designed and developed components using Python with Django framework Implemented code in python to retrieve and manipulate data Involved in development of the enterprise social network application using Python Twisted and Cassandra Used Python and Django creating graphics XML processing of documents data exchange and business logic implementation between servers orked closely with backend developer to find ways to push the limits of existing Web technology Designed and developed the UI for the website with HTML XHTML CSS Java Script and AJAX Used AJAXJSON communication for accessing RESTfulweb services data payload Designed dynamic clientside JavaScript codes to build web forms and performed simulations for web application page Created and implemented SQL Queries Stored procedures Functions Packages and Triggers in SQL Server Successfully implemented Auto CompleteAuto Suggest functionality using JQuery Ajax Web Service and JSON Environment Python 25 JavaJ2EE Django10 HTMLCSS Linux Shell Scripting Java Script Ajax JQuery JSON XML PostgreSQL Jenkins ANT Maven Subversion Python Data AnalystData Modeler Kotak Mahindra Bank Mumbai Maharashtra January 2011 to November 2012 Description Kotak Mahindra Bank is an Indian private sector bank headquartered in Mumbai Maharashtra India In February 2003 Reserve Bank of India issued the licence to Kotak Mahindra Finance Ltd the groups flagship company to carry on banking business Responsibilities Analyzed data sources and requirements and business rules to perform logical and physical data modeling Analyzed and designed best fit logical and physical data models and relational database definitions using DB2 Generated reports of data definitions Involved in NormalizationDenormalization Normal Form and database design methodology Maintained existing ETL procedures fixed bugs and restored software to production environment Developed the code as per the clients requirements using SQL PLSQL and Data Warehousing concepts Involved in Dimensional modeling Star Schema of the Data warehouse and used Erwin to design the business process dimensions and measured facts Worked with Data Warehouse Extract and load developers to design mappings for Data Capture Staging Cleansing Loading and Auditing Developed enterprise data model management process to manage multiple data models developed by different groups Designed and created Data Marts as part of a data warehouse Wrote complex SQL queries for validating the data against different kinds of reports generated by Business Objects XIR2 Using Erwin modeling tool publishing of a data dictionary review of the model and dictionary with subject matter experts and generation of data definition language Coordinated with DBA in implementing the Database changes and also updating Data Models with changes implemented in development QA and Production Worked Extensively with DBA and Reporting team for improving the Report Performance with the Use of appropriate indexes and Partitioning Developed Data Mapping Transformation and Cleansing rules for the Master Data Management Architecture involved OLTP ODS and OLAP Tuned and coded optimization using different techniques like dynamic SQL dynamic cursors and tuning SQL queries writing generic procedures functions and packages Experienced in GUI Relational Database Management System RDBMS designing of OLAP system environment as well as Report Development Extensively used SQL TSQL and PLSQL to write stored procedures functions packages and triggers Analyzed of data report were prepared weekly biweekly monthly using MS Excel SQL UNIX Environment ER Studio Informatica Power Center 8191 Power Connect Power exchange Oracle 11g MainframesDB2 MS SQL Server 2008 SQLPLSQL XML Windows NT 40 Tableau Workday SPSS SAS Business Objects XML Tableau Unix Shell Scripting Teradata Netezza Aginity Education Bachelors in Computer Science Royal College of Technology 2011 Skills Linux Solaris Sun Unix Additional Information Operating Systems All versions of Windows UNIX LINUX Macintosh HD Sun Solaris",
    "unique_id": "ae575d17-de09-481b-81bc-cf7e1601b8ac"
}