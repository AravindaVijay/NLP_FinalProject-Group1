{
    "clean_data": "Data Scientist Data Scientist Data Scientist Capital One Henrico VA Migrating inhouse projects onto cloud like Snowflake Redshift S3 and Databricks Familiarity and comfort with most aspects of Python3 including objectoriented programming and the data science stack wrote Python routines to log into the websites and fetch data for selected options Wrote flexible productionready code in R or Python for different components of the data science process such as data ingestion data manipulation feature selection engineering model training model validation and model deployment Good experience working with Financial data and creating calculated fields extracted from diverse sources Deep understanding of modern machine learning techniques and their mathematical underpinnings such as classification recommendation systems and natural language processing Strong understanding and application of statistical methods and skills like finding Central tendency a measure of dispersion using various distributions experimental design variance analysis AB testing and regression Exposed solving both classification and regression problems from messy data using Machine Learning algorithms Experienced building Natural Language Processing models using Linear classifiers like fastText Delivered business value by translating complex data from BMC remedy tool into meaningful insights using visualization tools Tableau Proficient with data visualization and translating complex problems into actionable insights Implemented python to retrieve and manipulate data visualize the data for key findings and explained to clients Worked on IO operations of different file formats like csv excel json txt using python Good experience with PySpark mainly for getting data from diverse sources for data analysis Worked with business groups and stakeholders to ensure models can be implemented as part of a delivered solution replicable across many clients with no impact on business Presented key findings to stakeholders to drive improvements and solutions from concept through to delivery Displayed strong teamwork and interpersonal skills with the ability to communicate to all levels of management Implemented SQL Scripts Stored Procedures and Triggers in MySQL server to handle user requests and work with the data in the database Experienced documenting all modeling steps in a systematic way including modeling process insights generated model validation results and checklists built in the project Develop software to automate operational processes along with coding for the shared engineering deliverables Experience in configuring deploying the web applications on AWS servers Performed configuration deployment and support of cloud services including Amazon Web Services AWS Created a database using MySQL wrote several queries to extract data from the database Wrote and executed various MYSQL database queries from Python using Python MySQL connector and MySQL Involved in the development of Web Services using SOAP for sending and getting data from the external interface Provide technical guidance recommendations and resolutions to new team members and built a new team Good exposure and business knowledge on AWS services like S3 EC2 Amazon Redshift Kinesis and ML Azure Work Experience Data Scientist Capital One February 2019 to Present Migrating inhouse databases onto AWS cloud Snowflake and running the jobs on Databricks Identify analyze and interpret patterns in complex data sets using Databricks and Tableau Developing python models to identify HighRisk Customers and performing comparative analysis over time to highlight HighRisk Customers variations Creating a Data Pipeline on AWS cloud which allows flexible data movement between different data sources Moving SQL and Oracle compliance data onto AWS redshift and performing scaling and data validity Using python database connectors to connect AWS redshift and perform SQL operations from CLI Creating dashboards connecting DynamoDB to Tableau and analyzing the changes in realtime Good experience in Data Blending Data Joining Applying Filters and proving granularity in Tableau Experience using python Logger module to keep track of different warnings info debug messages when connected to databases like DynamoDB SQL and Redshift This info is basically stored as a log file Data Scientist TAMUC Commerce TX May 2017 to January 2019 Recruited to work as a Data Scientist to do requirement gathering and hypothesis testing for new implementations in websites I was also responsible to perform statistical analysis on the datasets we receive and find the relationship between the features and target variables I also worked on a project NonToxic text classification to stop cyberbullying over the social media network We implemented using fastText linear classifier algorithm and tested its accuracy using other Machine Learning algorithms I also worked with Dr Isaac Gang as a Data Engineer to maintain We Teach CS Collaborative database Key Contributions Scrapped data from various websites cleaned and processed unstructured data to structured data and performed exploratory data analysis using python libraries like Pandas NumPy Matplotlib Seaborn Translated complex unstructured data and analysed the results into compelling visualizations and stories that clearly articulated the strategy and followup actions to the stakeholders Downloaded Twitter tweets using Twitter API trained manually segregated positive and negative tweets Performed hypothesis testing and statistical analysis of various data sets using python libraries and mathematically to find the acceptance and rejection percentage on the null hypothesis and alternate hypothesis Used Supervised Learning Algorithms and Natural Language Processing to analyze the level of toxicity in the text particularly in social networks and stop cyberbullying Worked as Data Engineer for Texas AM UniversityCommerce We Teach_CS Collaborative funded by UTA Created dashboards and reports to regularly communicate results and monitor key metrics Worked closely with various departments to track the survey results and find the insights Responsible to Monitor Computer Science Department Website changes and ICEL Server Analyzed the clickstream data and reported the findings to all the departments in form of charts Conducted text analysis on large datasets of unstructured text and performed statistical analysis using a variety of statistical tools like Minitab and python Data Analyst Python Developer Hewlett Packard Enterprise Bengaluru Karnataka August 2014 to December 2016 Key Contributions Worked independently on complex data analysis business analytics data mining tasks on big datasets Analyzed structured and unstructured data from incidents with complex statistical analysis to find the incident origin and customer behavior towards the incidents Experienced in collecting data from multiple databases and websites validated data integrity and accuracy Leveraged data and performed intensive analysis across all areas of our business to drive growth strategies including product development and rider engagement strategies Communicated with clients and store managers in a clear and concise manner for actionable changes focused on improving velocity predictability and efficiency of each store Developed and implemented a database monitoring system web page automation using FLASK and Django which reduced 4 hours of AMOS team manual work Used legacy mainframes SQL Unix Scripting Autosys BMC remedy as part of project enhancement Responsible for all aspects of the software development life cycle for the applicable projects including gathering requirements design implementation and deployment Automated many AMOS tasks reducing 40 manual effort and 30 ticket count increasing efficiency Experienced using databases such as Postgres and MySQL Wrote Python routines to log into the websites and fetch data for selected options Built SQL queries to perform various CRUD operations with DDL DML DCL and TCL commands HONORS and AWARDS Excellence Award Awarded by Mr Ben Wishart AHOLD CIO for maintaining a good relationship with clients and stakeholders Star Performer Awarded by Ravi Kumar AHOLD Project Manager Hewlett Packard Conduct of Appreciation Awarded by Dr Isaac Gang founder of WeTeachCS Texas AM Collaborative for working as a Data Engineer in this project Texas AM University Computer Science Scholarship Awarded for 2 years of masters education for showing good academic and research work on campus Education MS in Computer Science and Information Systems in Commerce Texas AM University 2018 Bachelors in Electronics and Communication Engineering in Electronics and Communication Engineering Jawaharlal Nehru Technological University 2014 Skills testing Excel Business Intelligence access SQL Additional Information Areas of expertise include Python PySpark SQL Tableau Minitab Statistical Modeling Natural Language Processing Snowflake Pandas NumPy ScikitLearn Keras Matplotlib NLTK PostgreSQL AWS EC2 Redshift S3 etc Machine Learning Algorithms Decision Trees Random Forest RNN Gradient Descent Nave Bayes Logistic Regression Collaborative Filtering",
    "entities": [
        "Henrico VA Migrating",
        "Commerce Texas AM University",
        "linear",
        "MySQL Involved",
        "AMOS",
        "ML Azure Work Experience Data Scientist Capital",
        "Worked as Data Engineer",
        "Conducted",
        "API",
        "Linear",
        "CRUD",
        "Present Migrating",
        "Data Scientist TAMUC Commerce",
        "UTA Created",
        "Snowflake Redshift S3",
        "BMC",
        "ICEL Server Analyzed",
        "Performed",
        "Leveraged",
        "TCL",
        "Tableau Proficient",
        "AWS",
        "WeTeachCS Texas AM Collaborative",
        "Oracle",
        "HighRisk Customers",
        "Databricks Identify",
        "Computer Science and Information Systems",
        "Databricks Familiarity",
        "PySpark",
        "Texas",
        "Postgres",
        "Star Performer Awarded",
        "Communicated",
        "Databricks",
        "Develop",
        "We Teach CS Collaborative",
        "Excel Business Intelligence",
        "Python PySpark SQL Tableau",
        "Dr",
        "Amazon Web Services AWS Created",
        "Pandas NumPy Matplotlib Seaborn Translated",
        "CLI Creating",
        "SQL",
        "Minitab Statistical",
        "Data Scientist",
        "NonToxic",
        "SOAP",
        "Nehru Technological University",
        "Worked",
        "Data Blending Data Joining Applying Filters",
        "Responsible to Monitor Computer Science Department Website",
        "Data Scientist Data Scientist Data Scientist Capital",
        "Tableau",
        "FLASK",
        "Machine Learning",
        "csv",
        "Communication Engineering in",
        "Automated",
        "Texas AM University Computer Science Scholarship Awarded",
        "Developer Hewlett Packard",
        "TX",
        "SQL Additional Information Areas",
        "Amazon Redshift Kinesis",
        "Central"
    ],
    "experience": "Experience in configuring deploying the web applications on AWS servers Performed configuration deployment and support of cloud services including Amazon Web Services AWS Created a database using MySQL wrote several queries to extract data from the database Wrote and executed various MYSQL database queries from Python using Python MySQL connector and MySQL Involved in the development of Web Services using SOAP for sending and getting data from the external interface Provide technical guidance recommendations and resolutions to new team members and built a new team Good exposure and business knowledge on AWS services like S3 EC2 Amazon Redshift Kinesis and ML Azure Work Experience Data Scientist Capital One February 2019 to Present Migrating inhouse databases onto AWS cloud Snowflake and running the jobs on Databricks Identify analyze and interpret patterns in complex data sets using Databricks and Tableau Developing python models to identify HighRisk Customers and performing comparative analysis over time to highlight HighRisk Customers variations Creating a Data Pipeline on AWS cloud which allows flexible data movement between different data sources Moving SQL and Oracle compliance data onto AWS redshift and performing scaling and data validity Using python database connectors to connect AWS redshift and perform SQL operations from CLI Creating dashboards connecting DynamoDB to Tableau and analyzing the changes in realtime Good experience in Data Blending Data Joining Applying Filters and proving granularity in Tableau Experience using python Logger module to keep track of different warnings info debug messages when connected to databases like DynamoDB SQL and Redshift This info is basically stored as a log file Data Scientist TAMUC Commerce TX May 2017 to January 2019 Recruited to work as a Data Scientist to do requirement gathering and hypothesis testing for new implementations in websites I was also responsible to perform statistical analysis on the datasets we receive and find the relationship between the features and target variables I also worked on a project NonToxic text classification to stop cyberbullying over the social media network We implemented using fastText linear classifier algorithm and tested its accuracy using other Machine Learning algorithms I also worked with Dr Isaac Gang as a Data Engineer to maintain We Teach CS Collaborative database Key Contributions Scrapped data from various websites cleaned and processed unstructured data to structured data and performed exploratory data analysis using python libraries like Pandas NumPy Matplotlib Seaborn Translated complex unstructured data and analysed the results into compelling visualizations and stories that clearly articulated the strategy and followup actions to the stakeholders Downloaded Twitter tweets using Twitter API trained manually segregated positive and negative tweets Performed hypothesis testing and statistical analysis of various data sets using python libraries and mathematically to find the acceptance and rejection percentage on the null hypothesis and alternate hypothesis Used Supervised Learning Algorithms and Natural Language Processing to analyze the level of toxicity in the text particularly in social networks and stop cyberbullying Worked as Data Engineer for Texas AM UniversityCommerce We Teach_CS Collaborative funded by UTA Created dashboards and reports to regularly communicate results and monitor key metrics Worked closely with various departments to track the survey results and find the insights Responsible to Monitor Computer Science Department Website changes and ICEL Server Analyzed the clickstream data and reported the findings to all the departments in form of charts Conducted text analysis on large datasets of unstructured text and performed statistical analysis using a variety of statistical tools like Minitab and python Data Analyst Python Developer Hewlett Packard Enterprise Bengaluru Karnataka August 2014 to December 2016 Key Contributions Worked independently on complex data analysis business analytics data mining tasks on big datasets Analyzed structured and unstructured data from incidents with complex statistical analysis to find the incident origin and customer behavior towards the incidents Experienced in collecting data from multiple databases and websites validated data integrity and accuracy Leveraged data and performed intensive analysis across all areas of our business to drive growth strategies including product development and rider engagement strategies Communicated with clients and store managers in a clear and concise manner for actionable changes focused on improving velocity predictability and efficiency of each store Developed and implemented a database monitoring system web page automation using FLASK and Django which reduced 4 hours of AMOS team manual work Used legacy mainframes SQL Unix Scripting Autosys BMC remedy as part of project enhancement Responsible for all aspects of the software development life cycle for the applicable projects including gathering requirements design implementation and deployment Automated many AMOS tasks reducing 40 manual effort and 30 ticket count increasing efficiency Experienced using databases such as Postgres and MySQL Wrote Python routines to log into the websites and fetch data for selected options Built SQL queries to perform various CRUD operations with DDL DML DCL and TCL commands HONORS and AWARDS Excellence Award Awarded by Mr Ben Wishart AHOLD CIO for maintaining a good relationship with clients and stakeholders Star Performer Awarded by Ravi Kumar AHOLD Project Manager Hewlett Packard Conduct of Appreciation Awarded by Dr Isaac Gang founder of WeTeachCS Texas AM Collaborative for working as a Data Engineer in this project Texas AM University Computer Science Scholarship Awarded for 2 years of masters education for showing good academic and research work on campus Education MS in Computer Science and Information Systems in Commerce Texas AM University 2018 Bachelors in Electronics and Communication Engineering in Electronics and Communication Engineering Jawaharlal Nehru Technological University 2014 Skills testing Excel Business Intelligence access SQL Additional Information Areas of expertise include Python PySpark SQL Tableau Minitab Statistical Modeling Natural Language Processing Snowflake Pandas NumPy ScikitLearn Keras Matplotlib NLTK PostgreSQL AWS EC2 Redshift S3 etc Machine Learning Algorithms Decision Trees Random Forest RNN Gradient Descent Nave Bayes Logistic Regression Collaborative Filtering",
    "extracted_keywords": [
        "Data",
        "Scientist",
        "Data",
        "Scientist",
        "Data",
        "Scientist",
        "Capital",
        "One",
        "Henrico",
        "VA",
        "Migrating",
        "inhouse",
        "projects",
        "cloud",
        "Snowflake",
        "Redshift",
        "S3",
        "Databricks",
        "Familiarity",
        "comfort",
        "aspects",
        "Python3",
        "programming",
        "data",
        "science",
        "stack",
        "Python",
        "routines",
        "websites",
        "data",
        "options",
        "Wrote",
        "productionready",
        "code",
        "R",
        "Python",
        "components",
        "data",
        "science",
        "process",
        "data",
        "ingestion",
        "data",
        "manipulation",
        "feature",
        "selection",
        "engineering",
        "model",
        "training",
        "model",
        "validation",
        "model",
        "deployment",
        "experience",
        "data",
        "fields",
        "sources",
        "understanding",
        "machine",
        "techniques",
        "underpinnings",
        "classification",
        "recommendation",
        "systems",
        "language",
        "understanding",
        "application",
        "methods",
        "skills",
        "tendency",
        "measure",
        "dispersion",
        "distributions",
        "design",
        "variance",
        "analysis",
        "AB",
        "testing",
        "regression",
        "classification",
        "regression",
        "problems",
        "data",
        "Machine",
        "Learning",
        "Natural",
        "Language",
        "Processing",
        "models",
        "Linear",
        "classifiers",
        "fastText",
        "business",
        "value",
        "data",
        "BMC",
        "remedy",
        "tool",
        "insights",
        "visualization",
        "tools",
        "Tableau",
        "Proficient",
        "data",
        "visualization",
        "problems",
        "insights",
        "python",
        "manipulate",
        "data",
        "data",
        "findings",
        "clients",
        "IO",
        "operations",
        "file",
        "formats",
        "csv",
        "excel",
        "json",
        "txt",
        "experience",
        "PySpark",
        "data",
        "sources",
        "data",
        "analysis",
        "business",
        "groups",
        "stakeholders",
        "models",
        "part",
        "solution",
        "clients",
        "impact",
        "business",
        "findings",
        "stakeholders",
        "improvements",
        "solutions",
        "concept",
        "delivery",
        "teamwork",
        "skills",
        "ability",
        "levels",
        "management",
        "SQL",
        "Scripts",
        "Stored",
        "Procedures",
        "Triggers",
        "MySQL",
        "server",
        "user",
        "requests",
        "work",
        "data",
        "database",
        "modeling",
        "steps",
        "way",
        "modeling",
        "process",
        "insights",
        "model",
        "validation",
        "results",
        "checklists",
        "project",
        "Develop",
        "software",
        "processes",
        "engineering",
        "deliverables",
        "Experience",
        "web",
        "applications",
        "AWS",
        "servers",
        "configuration",
        "deployment",
        "support",
        "cloud",
        "services",
        "Amazon",
        "Web",
        "Services",
        "AWS",
        "database",
        "MySQL",
        "queries",
        "data",
        "database",
        "Wrote",
        "MYSQL",
        "database",
        "Python",
        "Python",
        "MySQL",
        "connector",
        "MySQL",
        "development",
        "Web",
        "Services",
        "SOAP",
        "data",
        "interface",
        "guidance",
        "recommendations",
        "resolutions",
        "team",
        "members",
        "team",
        "exposure",
        "business",
        "knowledge",
        "AWS",
        "services",
        "S3",
        "EC2",
        "Amazon",
        "Redshift",
        "Kinesis",
        "ML",
        "Azure",
        "Work",
        "Experience",
        "Data",
        "Scientist",
        "Capital",
        "February",
        "Present",
        "Migrating",
        "inhouse",
        "AWS",
        "cloud",
        "Snowflake",
        "jobs",
        "Databricks",
        "patterns",
        "data",
        "sets",
        "Databricks",
        "Tableau",
        "python",
        "models",
        "HighRisk",
        "Customers",
        "analysis",
        "time",
        "HighRisk",
        "Customers",
        "variations",
        "Data",
        "Pipeline",
        "AWS",
        "cloud",
        "data",
        "movement",
        "data",
        "sources",
        "SQL",
        "Oracle",
        "compliance",
        "data",
        "AWS",
        "redshift",
        "scaling",
        "data",
        "validity",
        "python",
        "database",
        "connectors",
        "AWS",
        "redshift",
        "SQL",
        "operations",
        "CLI",
        "dashboards",
        "DynamoDB",
        "Tableau",
        "changes",
        "experience",
        "Data",
        "Blending",
        "Data",
        "Filters",
        "granularity",
        "Tableau",
        "Experience",
        "python",
        "Logger",
        "module",
        "track",
        "warnings",
        "info",
        "debug",
        "messages",
        "databases",
        "DynamoDB",
        "SQL",
        "Redshift",
        "info",
        "log",
        "file",
        "Data",
        "Scientist",
        "TAMUC",
        "Commerce",
        "TX",
        "May",
        "January",
        "Data",
        "Scientist",
        "requirement",
        "gathering",
        "hypothesis",
        "testing",
        "implementations",
        "websites",
        "analysis",
        "datasets",
        "relationship",
        "features",
        "target",
        "variables",
        "project",
        "NonToxic",
        "text",
        "classification",
        "media",
        "network",
        "classifier",
        "algorithm",
        "accuracy",
        "Machine",
        "Learning",
        "Dr",
        "Isaac",
        "Gang",
        "Data",
        "Engineer",
        "CS",
        "Collaborative",
        "database",
        "Key",
        "Contributions",
        "data",
        "websites",
        "data",
        "data",
        "data",
        "analysis",
        "python",
        "libraries",
        "Pandas",
        "NumPy",
        "Matplotlib",
        "Seaborn",
        "Translated",
        "data",
        "results",
        "visualizations",
        "stories",
        "strategy",
        "followup",
        "actions",
        "stakeholders",
        "Downloaded",
        "Twitter",
        "tweets",
        "Twitter",
        "API",
        "tweets",
        "hypothesis",
        "testing",
        "analysis",
        "data",
        "sets",
        "python",
        "libraries",
        "acceptance",
        "rejection",
        "percentage",
        "hypothesis",
        "hypothesis",
        "Supervised",
        "Learning",
        "Algorithms",
        "Natural",
        "Language",
        "Processing",
        "level",
        "toxicity",
        "text",
        "networks",
        "Worked",
        "Data",
        "Engineer",
        "Texas",
        "AM",
        "UniversityCommerce",
        "Teach_CS",
        "Collaborative",
        "UTA",
        "dashboards",
        "reports",
        "results",
        "metrics",
        "departments",
        "survey",
        "insights",
        "Monitor",
        "Computer",
        "Science",
        "Department",
        "Website",
        "ICEL",
        "Server",
        "clickstream",
        "data",
        "findings",
        "departments",
        "form",
        "charts",
        "text",
        "analysis",
        "datasets",
        "text",
        "analysis",
        "variety",
        "tools",
        "Minitab",
        "python",
        "Data",
        "Analyst",
        "Python",
        "Developer",
        "Hewlett",
        "Packard",
        "Enterprise",
        "Bengaluru",
        "Karnataka",
        "August",
        "December",
        "Key",
        "Contributions",
        "data",
        "analysis",
        "business",
        "analytics",
        "data",
        "mining",
        "tasks",
        "datasets",
        "data",
        "incidents",
        "analysis",
        "incident",
        "origin",
        "customer",
        "behavior",
        "incidents",
        "data",
        "databases",
        "websites",
        "data",
        "integrity",
        "accuracy",
        "Leveraged",
        "data",
        "analysis",
        "areas",
        "business",
        "growth",
        "strategies",
        "product",
        "development",
        "rider",
        "engagement",
        "strategies",
        "clients",
        "store",
        "managers",
        "manner",
        "changes",
        "velocity",
        "predictability",
        "efficiency",
        "store",
        "database",
        "system",
        "web",
        "page",
        "automation",
        "FLASK",
        "Django",
        "hours",
        "AMOS",
        "team",
        "work",
        "legacy",
        "mainframes",
        "SQL",
        "Unix",
        "Scripting",
        "Autosys",
        "BMC",
        "remedy",
        "part",
        "project",
        "enhancement",
        "aspects",
        "software",
        "development",
        "life",
        "cycle",
        "projects",
        "gathering",
        "requirements",
        "design",
        "implementation",
        "deployment",
        "Automated",
        "AMOS",
        "tasks",
        "effort",
        "ticket",
        "count",
        "efficiency",
        "databases",
        "Postgres",
        "MySQL",
        "Wrote",
        "Python",
        "websites",
        "data",
        "options",
        "SQL",
        "CRUD",
        "operations",
        "DDL",
        "DML",
        "DCL",
        "TCL",
        "HONORS",
        "AWARDS",
        "Excellence",
        "Award",
        "Mr",
        "Ben",
        "Wishart",
        "CIO",
        "relationship",
        "clients",
        "stakeholders",
        "Star",
        "Performer",
        "Ravi",
        "Kumar",
        "AHOLD",
        "Project",
        "Manager",
        "Hewlett",
        "Packard",
        "Conduct",
        "Appreciation",
        "Dr",
        "Isaac",
        "Gang",
        "founder",
        "WeTeachCS",
        "Texas",
        "AM",
        "Collaborative",
        "Data",
        "Engineer",
        "project",
        "Texas",
        "AM",
        "University",
        "Computer",
        "Science",
        "Scholarship",
        "years",
        "masters",
        "education",
        "research",
        "work",
        "campus",
        "Education",
        "MS",
        "Computer",
        "Science",
        "Information",
        "Systems",
        "Commerce",
        "Texas",
        "AM",
        "University",
        "Bachelors",
        "Electronics",
        "Communication",
        "Engineering",
        "Electronics",
        "Communication",
        "Engineering",
        "Jawaharlal",
        "Nehru",
        "Technological",
        "University",
        "Skills",
        "Excel",
        "Business",
        "Intelligence",
        "access",
        "SQL",
        "Additional",
        "Information",
        "Areas",
        "expertise",
        "Python",
        "PySpark",
        "SQL",
        "Tableau",
        "Minitab",
        "Statistical",
        "Modeling",
        "Natural",
        "Language",
        "Processing",
        "Snowflake",
        "Pandas",
        "NumPy",
        "ScikitLearn",
        "Keras",
        "Matplotlib",
        "NLTK",
        "PostgreSQL",
        "AWS",
        "EC2",
        "Redshift",
        "S3",
        "Machine",
        "Learning",
        "Algorithms",
        "Decision",
        "Trees",
        "Random",
        "Forest",
        "RNN",
        "Gradient",
        "Descent",
        "Nave",
        "Bayes",
        "Logistic",
        "Regression",
        "Collaborative",
        "Filtering"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T19:41:51.945954",
    "resume_data": "Data Scientist Data Scientist Data Scientist Capital One Henrico VA Migrating inhouse projects onto cloud like Snowflake Redshift S3 and Databricks Familiarity and comfort with most aspects of Python3 including objectoriented programming and the data science stack wrote Python routines to log into the websites and fetch data for selected options Wrote flexible productionready code in R or Python for different components of the data science process such as data ingestion data manipulation feature selection engineering model training model validation and model deployment Good experience working with Financial data and creating calculated fields extracted from diverse sources Deep understanding of modern machine learning techniques and their mathematical underpinnings such as classification recommendation systems and natural language processing Strong understanding and application of statistical methods and skills like finding Central tendency a measure of dispersion using various distributions experimental design variance analysis AB testing and regression Exposed solving both classification and regression problems from messy data using Machine Learning algorithms Experienced building Natural Language Processing models using Linear classifiers like fastText Delivered business value by translating complex data from BMC remedy tool into meaningful insights using visualization tools Tableau Proficient with data visualization and translating complex problems into actionable insights Implemented python to retrieve and manipulate data visualize the data for key findings and explained to clients Worked on IO operations of different file formats like csv excel json txt using python Good experience with PySpark mainly for getting data from diverse sources for data analysis Worked with business groups and stakeholders to ensure models can be implemented as part of a delivered solution replicable across many clients with no impact on business Presented key findings to stakeholders to drive improvements and solutions from concept through to delivery Displayed strong teamwork and interpersonal skills with the ability to communicate to all levels of management Implemented SQL Scripts Stored Procedures and Triggers in MySQL server to handle user requests and work with the data in the database Experienced documenting all modeling steps in a systematic way including modeling process insights generated model validation results and checklists built in the project Develop software to automate operational processes along with coding for the shared engineering deliverables Experience in configuring deploying the web applications on AWS servers Performed configuration deployment and support of cloud services including Amazon Web Services AWS Created a database using MySQL wrote several queries to extract data from the database Wrote and executed various MYSQL database queries from Python using Python MySQL connector and MySQL Involved in the development of Web Services using SOAP for sending and getting data from the external interface Provide technical guidance recommendations and resolutions to new team members and built a new team Good exposure and business knowledge on AWS services like S3 EC2 Amazon Redshift Kinesis and ML Azure Work Experience Data Scientist Capital One February 2019 to Present Migrating inhouse databases onto AWS cloud Snowflake and running the jobs on Databricks Identify analyze and interpret patterns in complex data sets using Databricks and Tableau Developing python models to identify HighRisk Customers and performing comparative analysis over time to highlight HighRisk Customers variations Creating a Data Pipeline on AWS cloud which allows flexible data movement between different data sources Moving SQL and Oracle compliance data onto AWS redshift and performing scaling and data validity Using python database connectors to connect AWS redshift and perform SQL operations from CLI Creating dashboards connecting DynamoDB to Tableau and analyzing the changes in realtime Good experience in Data Blending Data Joining Applying Filters and proving granularity in Tableau Experience using python Logger module to keep track of different warnings info debug messages when connected to databases like DynamoDB SQL and Redshift This info is basically stored as a log file Data Scientist TAMUC Commerce TX May 2017 to January 2019 Recruited to work as a Data Scientist to do requirement gathering and hypothesis testing for new implementations in websites I was also responsible to perform statistical analysis on the datasets we receive and find the relationship between the features and target variables I also worked on a project NonToxic text classification to stop cyberbullying over the social media network We implemented using fastText linear classifier algorithm and tested its accuracy using other Machine Learning algorithms I also worked with Dr Isaac Gang as a Data Engineer to maintain We Teach CS Collaborative database Key Contributions Scrapped data from various websites cleaned and processed unstructured data to structured data and performed exploratory data analysis using python libraries like Pandas NumPy Matplotlib Seaborn Translated complex unstructured data and analysed the results into compelling visualizations and stories that clearly articulated the strategy and followup actions to the stakeholders Downloaded Twitter tweets using Twitter API trained manually segregated positive and negative tweets Performed hypothesis testing and statistical analysis of various data sets using python libraries and mathematically to find the acceptance and rejection percentage on the null hypothesis and alternate hypothesis Used Supervised Learning Algorithms and Natural Language Processing to analyze the level of toxicity in the text particularly in social networks and stop cyberbullying Worked as Data Engineer for Texas AM UniversityCommerce We Teach_CS Collaborative funded by UTA Created dashboards and reports to regularly communicate results and monitor key metrics Worked closely with various departments to track the survey results and find the insights Responsible to Monitor Computer Science Department Website changes and ICEL Server Analyzed the clickstream data and reported the findings to all the departments in form of charts Conducted text analysis on large datasets of unstructured text and performed statistical analysis using a variety of statistical tools like Minitab and python Data Analyst Python Developer Hewlett Packard Enterprise Bengaluru Karnataka August 2014 to December 2016 Key Contributions Worked independently on complex data analysis business analytics data mining tasks on big datasets Analyzed structured and unstructured data from incidents with complex statistical analysis to find the incident origin and customer behavior towards the incidents Experienced in collecting data from multiple databases and websites validated data integrity and accuracy Leveraged data and performed intensive analysis across all areas of our business to drive growth strategies including product development and rider engagement strategies Communicated with clients and store managers in a clear and concise manner for actionable changes focused on improving velocity predictability and efficiency of each store Developed and implemented a database monitoring system web page automation using FLASK and Django which reduced 4 hours of AMOS team manual work Used legacy mainframes SQL Unix Scripting Autosys BMC remedy as part of project enhancement Responsible for all aspects of the software development life cycle for the applicable projects including gathering requirements design implementation and deployment Automated many AMOS tasks reducing 40 manual effort and 30 ticket count increasing efficiency Experienced using databases such as Postgres and MySQL Wrote Python routines to log into the websites and fetch data for selected options Built SQL queries to perform various CRUD operations with DDL DML DCL and TCL commands HONORS and AWARDS Excellence Award Awarded by Mr Ben Wishart AHOLD CIO for maintaining a good relationship with clients and stakeholders Star Performer Awarded by Ravi Kumar AHOLD Project Manager Hewlett Packard Conduct of Appreciation Awarded by Dr Isaac Gang founder of WeTeachCS Texas AM Collaborative for working as a Data Engineer in this project Texas AM University Computer Science Scholarship Awarded for 2 years of masters education for showing good academic and research work on campus Education MS in Computer Science and Information Systems in Commerce Texas AM University 2018 Bachelors in Electronics and Communication Engineering in Electronics and Communication Engineering Jawaharlal Nehru Technological University 2014 Skills testing Excel Business Intelligence access SQL Additional Information Areas of expertise include Python PySpark SQL Tableau Minitab Statistical Modeling Natural Language Processing Snowflake Pandas NumPy ScikitLearn Keras Matplotlib NLTK PostgreSQL AWS EC2 Redshift S3 etc Machine Learning Algorithms Decision Trees Random Forest RNN Gradient Descent Nave Bayes Logistic Regression Collaborative Filtering",
    "unique_id": "91da1df7-f805-4e08-928e-5287d482f1b8"
}