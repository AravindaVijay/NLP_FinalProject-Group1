{
    "clean_data": "Lead PythonSparkRedshiftS3 Lead span lPythonspanSparkRedshiftS3 Lead PythonSparkHadoopNeo4j Charlotte NC More than 12 years of experience in Python Hive Sqoop Spark Pig Scripts Big Data Neo4j Neo4j Bloom MS SQL Server MSBI SSIS SSRS SSAS ASPNet with C Implemented Hadoop stack and different big data analytic tools migration from different databases ie Teradata Oracle MYSQL to Hadoop Experience in working with MapReduce programs using Hadoop for working with Big Data Strong Knowledge of Hadoop and Hive and Hives analytical functions Capturing data from existing databases that provide SQL interfaces using Sqoop Efficient in building hive and pig scripts In depth understanding of Spark Architecture including Spark Core Spark Sql Data Frames Experience in usage of Hadoop Distribution like Cloudera 53 Expertise in using SparkSQL with various data sources like JSON and Parquet Having good experience in Hadoop Big Data processing Expertise in developing the queries in Hive Pig Successfully loaded files to Hive and HDFS from Mysql Oracle and Teradata Experience in designing both time driven and data driven automated workflows using Oozie MasteringLeading in the development of applicationstools using Python Worked on several python packages like NumPy SciPy Pandas etc Having Good Experience in Object Oriented Concepts with Python Integrated different data sources data wrangling cleaning transforming merging and reshaping data sets by writing Python scripts Designed and implemented Data Lineage graph from origination to consumption point of view in the form of nodes and edges in Neo4j Created Nodes and Edges in NoSQL Database Neo4j Strong Experience in implementing Data warehouse solutions in Confidential Redshift migrate data from on premise databases to Confidential Redshift RDS and S3 Expertise in writing Spark RDD transformations actions Data Frames case classes for the required input data and performed the data transformations using PySpark Expertise in using SparkSQL with various data sources like JSON and Hive Expert in SSIS Package development deployment Job Scheduling and Error Trapping SSRS Report development deployment subscription SSAS Tabular and Multidimensional Cube development Strong experience as Business Intelligence Developer and Data Analyst in Production Development and Staging Environments Defining data warehouse star and snow flake schema fact table cubes dimensions measures using SQL Server Analysis Services with DAX Authorized to work in the US for any employer Work Experience Lead PythonSparkRedshiftS3 Johnson JohnsonNew Jersey October 2018 to February 2019 Project Description Goal of this project is to create data lake for the marketing and retail data from the multiple pharma vendors This include retrieving huge datasets from vendors create extracts on S3 validate that data using python scripts and migrate data into redshift and then create graph of customers Practioners Facility and their relationshipsMedia Plan and Digital Media Responsibilities Designing and building full endtoend Data Warehouse infrastructure from the ground up on Confidential Redshift for large scale data handling Developed Spark program using PySpark API with Hive and SQL Responsible for ETL and data validation using Python scripts Worked on Quality Check script for matching extract count and trigger error in case of mismatch count in extracts using python Worked on AWS Data Pipeline to configure data loads from S3 to into Redshift Write complex stored procedures to extract data in Redshift Import data from AWS S3 into Sparks RDD and performed transformations and actions on top of that RDD Used AVRO Parquet and ORC file format to store data into HDFS Created Practioners and Facility as nodes and their relationships in neo4j Used Apoc procedures to analyze data Worked on boundary queries for media plan and digital media cypher queries Deploy and execute python scripts using Unix command line Environment Python PySpark Hive HDFS AWS RedshiftPostgres Mysql S3 Neo4j 34 Lead  AIG Charlotte NC October 2017 to September 2018 Project Description Data Management SubsystemsDMS consists of managing different systems workflows processes sub processes applications and servers Source for this subsystem is sql server and hive Data is then integrated and validated by the python and then create nodes and relationships in neo4j graphical database Workflows can be modified through java UI Responsibilities Worked on python files to load the data from csv json MySQL hive files to Neo4j Graphical database Implemented list and dictionary as data structure to manipulate data from sql server csv and hive files Manipulate dataframes using Pandas and Numpy Libraries in Python Performed transformations and actions on PySparks RDD Import csv pyodbc and pyhive to deal with different structured files Import data using Sqoop into Hive and from existing Teradata Export and Import data into HDFS and Hive using Sqoop Used JSON and XML Serdes for serialization and deserialization to load Json and Xml data into Hive tables Worked with AVRO Parquet and ORC file format and various compression formats like Snappy to store data into HDFS Load the data into Spark Dataframe and do in memory data computation to generate output response Store data on HBASE for analysis Wrote different pig scripts to clean up ingested data Load and transform large sets of structured and semi structured data Used sets to compare metadata and differentiate Customized error logging as well as exceptional handling Validate the extracted data and prepare valid files as well as data quality error files Designed and implemented Data Lineage Neo4j graph from origination to consumption point of view Developed Business process hierarchy graph for every process available in the system Created indexs and constraints on nodes and edges of Neo4j Graph Worked on boundary cypher queries using Apoc libraries Create perspectives and parameterized queries in Neo4j Bloom Extensive use of Apoc libraries for data modification and data analysis in Neo4j Graph Design Neo4j architecture for nodes and edges load Created Neo4j Graphical database nodes and relationship Implement cypher queries to manipulate data on Neo4j database Designed and developed a decision tree application using Neo4J graph database to model the nodes and relationships for each decision Implemented data wrangling cleaning transforming merging and reshaping data frames Configure Neo4j on Unix as well as windows system Environment Python Hive HBASE Spark Sqoop OoziePySparkPig Scripts Teradata Neo4j 34Neo4j Bloom Sql Server 2017 Lead  OtsukaLos Angles CA May 2017 to October 2017 Project Description The project goal is to analyze pharma business by combining data from different sources ie csv excel MySQL hive json postgres oracle to one centralize place and then create nodes and relationships in neo4j graphical database Responsibilities Developed a Machine Learning application to predict disease medication based on demographic and medical history in python Explored different implementations in Hadoop environment for data extraction and summarization by using packages like Hive Pig Importing and exporting data into HDFS using Sqoop Sql Server Teradata and Mysql Manipulate dataframes using Pandas and Numpy Libraries in Python Designed and developed a decision tree application using Neo4j graph database to model the nodes and relationships for each decision Wrote python scripts to extract data from diverse sources Worked on python scripts to load the data from csv json mysql hive files to Neo4j Graphical database Created Neo4j Graphical database nodes and edges using neo4jv1 library in Python Created indexes and constraints in Neo4j Implement cypher boundary queries to manipulate data on Neo4j database Data integration and data analysis using APOC procedures and functions in Neo4j Involved in Design analysis Implementation Testing and support of ETL processes for Stage ODS and Mart Used Talend as a Data Cleansing tool to correct the data before loading into the staging area Collect and link metadata from diverse sources including relational databases and flat files Extracted data from different Flat files MS Excel HIve and transformed the data based on user requirement using Talend and loaded data into target by scheduling the sessions Supporting daily loads and work with business users to handle rejected data Implemented data cleansing for files using Talend Created data model in Postgres using dimensional model Performed Unit Testing and tuned for better performance Created Reusable Transformations and multiple Mappings Environment Python Hive Pig Sqoop Oozie Neo4j 32 Talend 96 PGAdmin 14 Data Engineer Florida Power and Light Noida Uttar Pradesh July 2016 to April 2017 Project Description The System will streamline the verification of the sections like revenues billed cash postings refunds issued meters uploaded and cash deposited to banks The System has ability to create dashboard custom reporting and analytics The Revenue balance data can be drilled down by measures and dimensions enabling users to gain access to accurate uptodate Information for better decision making Responsibilities Developed and maintained Python ETL scripts to scrape data from external sources and load cleansed data into a Sql Server The data was used for daily electrical power virtual trading activities in several markets Implemented discretization and binning data wrangling cleaning transforming merging and reshaping data frames using Python Integrated applications with designing database architecture and server scripting studying establishing Designed data visualization to present current impact and growth Loaded the dataset into Hive for ETL Operation Written Hive scripts to extract data from staging tables Building publishing customized interactive reports and dashboards report scheduling using Power BI Desktop Used Power BI Power Pivot to develop data analysis prototype and used Power View and Power Map to visualize reports Used DAX Data Analysis Expressions functions for the creation of calculations and measures in the Power BI Models Designing and apply Microsoft Project Plan Requirement Design Development and Testing Data Modeling and Cube design in SSAS using Tabular Model Monitoring overall progress and use of resources initiating corrective action where necessary Planning and monitoring the project Preparing and maintaining project stage and exception plans as required Environment Hadoop Hive Python SSAS Power BI Data Engineer HMS Irvine Noida Uttar Pradesh March 2014 to July 2016 Project Description Client is an innovative healthcare services company with a Comprehensive patient centered approach to diabetes management SSIS has been used as an ETL tool for building of data warehouse and data consolidation Client provides the claim provider and membership files on daily weekly and monthly basis on FTP or EDI Server Package load the claim data into staging database After loading Medicare and Medicaid data in staging edits Rules provided by State and Federal government for claims applied on staging database and then after finally loaded into the Data Warehouse and then mail delivery report to the client at every step as Sanity Check Return file and Cleanup process Every step maintains in Sql server log table Error handling and monitoring the package is done by the BIxPress tools Responsibilities Designing developing deploying job scheduling reports and successfailure notifications in MS SQL Server environment using SSIS in Sql Server data Tools SSDT Used ETL SSIS tasks to develop jobs for extracting cleaning transforming and loading data into data warehouse Extracted data from various sources like SQL Server 2008 Oracle CSV Excel and Text file from Client servers and through FTP Experience working in a large data environment doing SQL development SQL query optimization Knowledge of Transactions Isolation Levels Indexes Blocks Locks and Database Partitioning etc Implemented SQL constraints Primary Key Foreign Key Index Unique Not Null Default Experience managing large projects with a close attention to detail and highquality performance Data modelling and mapping using UML in Visio Experience in handling Enterprise datasets Identify issues and fix those issues while troubleshooting Experience in writing MDX queries to access cubes data in SSAS Monitor scheduled SSIS Package through the BIxPress tools Error Trapping through BIxPress tool Script Writing in net Transaction Email Alert and some complex Transformation in SSIS Deployment and Scheduling SSIS Package through SQL Server and file system Report development in SSRS Create SSIS packages Email alert for SSIS packages and schedule it through SQL Server and file system Manage the Dev SQA and Production Environments Environment SQL Server 2012 with Integration Reporting Services and Analysis Services Cnet code for SSIS scripting BIxPress Microsoft Business intelligence development studio SQL Server 2012 Data Engineer Sanare Miami FL September 2012 to March 2014 Project Description The system is a database driven online Reporting model and SSIS package for data upload and send reports by the packages which is for Health care domain in USA for this report we have create our own data warehouse and fetch all report from this data warehouse Around 100 business reports have been created using functionality of SSRS SSIS has been used as an ETL tool for building of data warehouse and data consolidation ETL process has been designed on following lines Download daily extracts from SFTP server and upload them into data warehouse after performing various data integrity checks Preparation of data comparison sheet to compare record counts in tables against the counts which are supposed to be them Execution of data maintenance tasks eg Rebuild Indices After successful completion of data upload this package kicks off second package to generate various reports and distribute them in encrypted form to intended recipients Preparation of logs has been maintained during execution of package Notifications to users of failure of package if occurred Notifications to users of success of important tasks to keep them aware during execution At last summary has been sent to show the statistics of package Real time data has been picked from Other production systems for integrated reporting and Dashboards Package is configurable and data driven so it can be changed easily as per the changing business environment Responsibilities Data warehouse architecture designing ETL package Development in SSIS Deployment and Scheduling of SSIS Package in SSIS Populate data marts with the right data at the right time Performance Tuning SQL Query Optimization Complex Query Design Stored Procedure Writing Function Writing Creating Reports in SSRS Project monitoring Status reporting and client interaction Monitoring the critical components of the System Implementation and support of systems in production environment Environment SQL Server 2008 R2 with Integration and Reporting Services SSIS Scripting Microsoft Business intelligence development studio SQL Server 2008 R2 ETL Developer Aspen Noida Uttar Pradesh April 2012 to September 2012 Project Description Data Repository collects all clinical operational and financial extracts across the organization and uploads them in data warehouse after performing various data integrity checks It also integrates real time cloud data with the ware house and provides an open environment for powerful reporting and decision support to healthcare managerial authorities SSIS has been used as an ETL tool with following features Created Complex ETL Packages using SSIS to extract data from staging tables to partitioned tables with incremental load Created SSIS Reusable Packages to extract data from Multi formatted Flat files Excel XML files into Database Billing Systems Created SSIS packages for File Transfer from one location to the other using FTP task Developed deployed and monitored SSIS Packages Transform extract data after performing integrity checks Implement Email notification in case of package failure Integration with another package for cleanup task Develop various script tasks for validating and transform data Designed SSIS Packages to transfer data from flat files to SQL Server using Business Intelligence Development Studio Extensively used SSIS transformations such as Lookup Derived column Data conversion Aggregate Conditional split SQL task Script task and Send Mail task etc Implement configuration files to deploy the SSIS packages across all environments Responsibilities ETL package Development in SSIS Integration of realtime cloud data NetSuite with data Warehouse for critical business requirements Report development in SSRS Project monitoring Status reporting and client interaction Operate as the liaison between Digital Analytics team and IT Development team for all projects and releases including testing prepost deployment of changes to ensure timely and accurate reporting and insights Environment SQL Server 2008 R2 with Integration and Reporting Services Microsoft Business intelligence development studio SSMS SQL Server 2008 R2 ETL and Report Developer Aspen Noida Uttar Pradesh September 2011 to March 2012 Project Description PMIS is an application which stores information about various types of Patients disease and taking input from different chain management solutions around the globe PMIS is used to manage patient historical information Various business reports have been developed for dynamic data analysis Comparison of actual health data with industry standard in graphical analysis has been shown Following features have been implemented in this project Designing developing and deploying reports in MS SQL Server environment using SSRS2008 and SSIS in Business Intelligence Development Studio BIDS Used ETL SSIS to develop jobs for extracting cleaning transforming and loading data into data warehouse Designed SSIS Packages to transfer data from flat files to SQL Server using Business Intelligence Development Studio Creating multiple parameterized stored procedures which were used by the reports to get the data Worked on formatting SSRS reports using the Global variables and expressions Created parameterized reports Drill down and Drill through reports using SSRS Used Execution Plan SQL Profiler and Database Engine Tuning Advisor to optimize queries and enhance the performance of databases Optimized query performance by creating indexes Write TSQL statements for retrieval of data and performance tuning of TSQL Responsibilities ETL package Development in SSIS Report development in SSRS Database Design Understanding the business problem identifying relevant data gathering and summarizing data meaningfully Performance TuningQuery Optimization in SPs Complex Query Design Stored Procedure Writing Function Writing Trigger Writing Environment SQL Server 2008 R2SSISSSRS Microsoft Business intelligence development studio SQL Server 2008 R2 AspNet Developer Oil Field Services Mumbai Maharashtra September 2010 to September 2011 Project Description OFS is webbased application that keeps track of products between storesrigswell OFS application offers clients a secure and efficient management tool in processing their business OFS application is built as an ntier Web based application The Application User interface is built as Web application using ASPNET Master Pages AJAX controls and JavaScript To Support SOA based architecture Business Web services in middle tier is used and it acts as core business engine The web services consume provide the Business Entities Business Entities are used for passing the data across the tiers Database tier is the oracle database To maintain application portability across databases data transactions are handled in business services tier Responsibilities Coordination and handling tasks among team members Designing of HTML pages JavaScript validations Assist Project Manager in Feasibility study and requirement Analysis phase Implement web service as business layer Deploy application on Client Server Designing Database Creation and implementation of View Stored Procedures Documentation for coding Configuring deploying Database and web application for testing environment Code review during different stage of development life cycle Environment Web Framework 20 ASPNet 20 C Web Service TFS Visual Studio 2005 JavaScript Oracle 10G AspNet Developer Insurity Hartford Mumbai Maharashtra January 2010 to August 2010 Project Description NGA Next Generation Application is a webbased insuranceproduct of one of the Famous American productbased company in commercial lines It allows their clients to do policyadministration as Automated rating and issuance support for all major commercial lines of business Full policy lifecycle transaction support Full maintenance of ISO and customerspecific rates forms and rules changes Responsibilities Requirement Gathering of defects Onsite Coordination Estimation of defects Defect Fixing Code review Environment Web Framework 20 VBNet VB6 SQL Server 2005 AspNet Developer Insurity Hartford Mumbai Maharashtra March 2009 to January 2010 Project CHealth Project Description CHealth is a Collaborative Consumer Engagement which is based on Microsoft connected Healthcare framework It empowers consumers through its Wellness module and at the same time provides an integrated platform wherein Payers TPAs Brokers ASOs Providers collaborate seamlessly With an innovative business model cHealth is supported by SOA based architecture To Support SOA based architecture Business Web services in middle tier is used and it acts as core business engine Responsibilities Coordination and handling tasks among team members Designing of HTML pages Assist Project Manager in Feasibility study and requirement Analysis phase Deploy application on Client Server Database Design Creation and implementation of View Stored Procedures Taking Backup and Restore Database Implement Web services as business tier Documentation for coding Configuring deploying Database and web application for testing environment Code review during different stage of development life cycle Developed HL7 translator Write Store Procedure to implement HL7 translator Environment Web Framework 20 AspNet with C SQL Server 2005 AspNet Developer Munich Re Princeton Mumbai Maharashtra August 2007 to February 2009 Project Description AutoFac is a webbased application that allows ReInsurance clients to conduct Property and Commercial Facultative reinsurance business operations with MRAm AutoFac application offers clients a secure and efficient management tool in processing their business AutoFac application is built as an ntier Web based application The Application User interface is built as Web application using ASPNET Master Pages AJAX controls and JavaScript To Support SOA based architecture Business Web services in middle tier is used and it acts as core business engine The web services consume provide the Business Entities Business Entities are used for passing the data across the tiers Database tier is the oracle database To maintain application portability across databases data transactions are handled in business services tier Responsibilities Designing of HTML pages JavaScript validations Programming for Referral People and System admin Organization Policy Quotes Location Upload and Reporting Identifying the Functionality of the Application Designing of framework Designing Database Model Creating Views and Stored Procedure Requirement gathering phase and UAT Extensive work on web services as business tier Deploying application for testing environment Documentation for specific modules Integration and System Testing Programming in XSLT for location upload Environment Web Framework 20 Microsoft Framework 20ASPNet with C Oracle 10G Education Masters Skills APACHE HADOOP SQOOP 1 year ASP 4 years AspNet 4 years Business intelligence 5 years C 5 years database 10 years ETL 6 years EXTRACT TRANSFORM AND LOAD 6 years Hadoop 2 years Hive 2 years HTML 3 years JavaScript 2 years MS ASP 4 years MS SQL Server 8 years Python 2 years R2 2 years SAP 2 years SQL 8 years sql server 8 years SQL Server 2005 1 year",
    "entities": [
        "EDI Server Package",
        "Defect Fixing Code",
        "Responsibilities Coordination",
        "ISO",
        "SSIS Package",
        "Performed Unit Testing",
        "Project Description Client",
        "Created Neo4j",
        "Database Billing Systems Created",
        "Graphical",
        "Identify",
        "DAX Data Analysis Expressions",
        "HDFS",
        "Facility",
        "R2 ETL Developer Aspen Noida Uttar Pradesh",
        "Hive for ETL Operation Written Hive",
        "Developed Spark",
        "Development in SSIS Report",
        "Teradata Oracle",
        "Performance TuningQuery Optimization",
        "OFS",
        "cHealth",
        "Teradata Export and Import",
        "AutoFac",
        "the Power BI Models Designing",
        "Created Complex ETL",
        "Business Intelligence Developer and Data Analyst",
        "Supporting",
        "RDD",
        "Hadoop",
        "Mysql Manipulate",
        "Project Description",
        "Production Environments Environment SQL Server",
        "Integration Reporting Services and Analysis Services Cnet",
        "csv",
        "Transformation in SSIS Deployment and Scheduling SSIS Package",
        "State",
        "Complex Query Design Stored Procedure Writing Function",
        "Neo4j Involved",
        "Medicaid",
        "SSIS",
        "PySparks RDD Import",
        "Responsibilities Data",
        "Payers TPAs Brokers ASOs Providers",
        "SQL Server",
        "Visio",
        "Business Intelligence Development Studio",
        "SparkSQL",
        "Microsoft Business",
        "Data Warehouse",
        "Hive Pig Importing",
        "Digital Media Responsibilities Designing",
        "AWS S3",
        "the System Implementation",
        "NC",
        "Created SSIS Reusable Packages",
        "SSAS Monitor",
        "Redshift Import",
        "Healthcare",
        "Data Lineage",
        "Integration and System Testing Programming",
        "UML",
        "Client",
        "Object Oriented Concepts",
        "Aggregate Conditional",
        "Developed Business",
        "Capturing",
        "TSQL Responsibilities ETL",
        "Comprehensive",
        "SOA",
        "Project Description Data Repository",
        "Spark Dataframe",
        "NetSuite",
        "Talend",
        "ORC",
        "Report Developer Aspen Noida Uttar",
        "Hadoop Big Data",
        "Send Mail",
        "Sql Server",
        "USA",
        "Development in SSIS Deployment and Scheduling of SSIS Package",
        "MS",
        "SSIS Populate",
        "APOC",
        "Validate",
        "PySpark API",
        "sql",
        "VBNet VB6",
        "PySpark Expertise",
        "US",
        "Sqoop",
        "Python Hive Sqoop Spark",
        "Programming for Referral People and System",
        "Lookup Derived",
        "Oozie MasteringLeading",
        "Spark Core Spark",
        "Confidential Redshift",
        "Rebuild Indices",
        "Created Nodes",
        "SQL Server Analysis Services",
        "Redshift Write",
        "Postgres",
        "Mysql Oracle",
        "Project Description Data Management",
        "Organization Policy Quotes Location Upload",
        "Collect",
        "Sql",
        "Microsoft Framework",
        "Wellness",
        "UAT Extensive",
        "Hartford",
        "SSRS",
        "Charlotte",
        "the Business Entities Business Entities",
        "SQL",
        "SSRS Create",
        "Practioners Facility",
        "Spark RDD",
        "Hive Pig Successfully",
        "HDFS Created Practioners",
        "Bloom MS",
        "C Implemented Hadoop",
        "Cleanup",
        "MDX",
        "Production Development and Staging Environments Defining",
        "Hive",
        "Confidential Redshift RDS",
        "Cube",
        "Worked on AWS Data Pipeline",
        "Restore Database Implement Web",
        "FTP",
        "SSAS Tabular",
        "Mappings Environment Python Hive Pig Sqoop Oozie",
        "Pandas",
        "Mumbai",
        "ETL",
        "Talend Created",
        "Medicare",
        "Onsite Coordination Estimation",
        "Data Frames",
        "XSLT",
        "ASPNET Master Pages AJAX",
        "Project Description AutoFac",
        "Status",
        "Project CHealth Project Description CHealth",
        "SQL Responsible",
        "View Stored Procedures Documentation",
        "Microsoft",
        "Development in SSIS Integration",
        "the Data Warehouse",
        "Multi",
        "Created Reusable Transformations",
        "SSAS Power BI Data Engineer HMS",
        "Data",
        "Miami",
        "MapReduce",
        "Multidimensional Cube",
        "Comparison",
        "Spark Architecture",
        "S3 Expertise",
        "the BIxPress tools Responsibilities Designing",
        "Digital Analytics",
        "Numpy Libraries",
        "Oracle CSV Excel",
        "Property",
        "Environment Hadoop Hive Python",
        "Power BI Desktop Used Power BI Power Pivot",
        "NoSQL Database",
        "Python Created"
    ],
    "experience": "Experience in working with MapReduce programs using Hadoop for working with Big Data Strong Knowledge of Hadoop and Hive and Hives analytical functions Capturing data from existing databases that provide SQL interfaces using Sqoop Efficient in building hive and pig scripts In depth understanding of Spark Architecture including Spark Core Spark Sql Data Frames Experience in usage of Hadoop Distribution like Cloudera 53 Expertise in using SparkSQL with various data sources like JSON and Parquet Having good experience in Hadoop Big Data processing Expertise in developing the queries in Hive Pig Successfully loaded files to Hive and HDFS from Mysql Oracle and Teradata Experience in designing both time driven and data driven automated workflows using Oozie MasteringLeading in the development of applicationstools using Python Worked on several python packages like NumPy SciPy Pandas etc Having Good Experience in Object Oriented Concepts with Python Integrated different data sources data wrangling cleaning transforming merging and reshaping data sets by writing Python scripts Designed and implemented Data Lineage graph from origination to consumption point of view in the form of nodes and edges in Neo4j Created Nodes and Edges in NoSQL Database Neo4j Strong Experience in implementing Data warehouse solutions in Confidential Redshift migrate data from on premise databases to Confidential Redshift RDS and S3 Expertise in writing Spark RDD transformations actions Data Frames case classes for the required input data and performed the data transformations using PySpark Expertise in using SparkSQL with various data sources like JSON and Hive Expert in SSIS Package development deployment Job Scheduling and Error Trapping SSRS Report development deployment subscription SSAS Tabular and Multidimensional Cube development Strong experience as Business Intelligence Developer and Data Analyst in Production Development and Staging Environments Defining data warehouse star and snow flake schema fact table cubes dimensions measures using SQL Server Analysis Services with DAX Authorized to work in the US for any employer Work Experience Lead PythonSparkRedshiftS3 Johnson JohnsonNew Jersey October 2018 to February 2019 Project Description Goal of this project is to create data lake for the marketing and retail data from the multiple pharma vendors This include retrieving huge datasets from vendors create extracts on S3 validate that data using python scripts and migrate data into redshift and then create graph of customers Practioners Facility and their relationshipsMedia Plan and Digital Media Responsibilities Designing and building full endtoend Data Warehouse infrastructure from the ground up on Confidential Redshift for large scale data handling Developed Spark program using PySpark API with Hive and SQL Responsible for ETL and data validation using Python scripts Worked on Quality Check script for matching extract count and trigger error in case of mismatch count in extracts using python Worked on AWS Data Pipeline to configure data loads from S3 to into Redshift Write complex stored procedures to extract data in Redshift Import data from AWS S3 into Sparks RDD and performed transformations and actions on top of that RDD Used AVRO Parquet and ORC file format to store data into HDFS Created Practioners and Facility as nodes and their relationships in neo4j Used Apoc procedures to analyze data Worked on boundary queries for media plan and digital media cypher queries Deploy and execute python scripts using Unix command line Environment Python PySpark Hive HDFS AWS RedshiftPostgres Mysql S3 Neo4j 34 Lead   AIG Charlotte NC October 2017 to September 2018 Project Description Data Management SubsystemsDMS consists of managing different systems workflows processes sub processes applications and servers Source for this subsystem is sql server and hive Data is then integrated and validated by the python and then create nodes and relationships in neo4j graphical database Workflows can be modified through java UI Responsibilities Worked on python files to load the data from csv json MySQL hive files to Neo4j Graphical database Implemented list and dictionary as data structure to manipulate data from sql server csv and hive files Manipulate dataframes using Pandas and Numpy Libraries in Python Performed transformations and actions on PySparks RDD Import csv pyodbc and pyhive to deal with different structured files Import data using Sqoop into Hive and from existing Teradata Export and Import data into HDFS and Hive using Sqoop Used JSON and XML Serdes for serialization and deserialization to load Json and Xml data into Hive tables Worked with AVRO Parquet and ORC file format and various compression formats like Snappy to store data into HDFS Load the data into Spark Dataframe and do in memory data computation to generate output response Store data on HBASE for analysis Wrote different pig scripts to clean up ingested data Load and transform large sets of structured and semi structured data Used sets to compare metadata and differentiate Customized error logging as well as exceptional handling Validate the extracted data and prepare valid files as well as data quality error files Designed and implemented Data Lineage Neo4j graph from origination to consumption point of view Developed Business process hierarchy graph for every process available in the system Created indexs and constraints on nodes and edges of Neo4j Graph Worked on boundary cypher queries using Apoc libraries Create perspectives and parameterized queries in Neo4j Bloom Extensive use of Apoc libraries for data modification and data analysis in Neo4j Graph Design Neo4j architecture for nodes and edges load Created Neo4j Graphical database nodes and relationship Implement cypher queries to manipulate data on Neo4j database Designed and developed a decision tree application using Neo4J graph database to model the nodes and relationships for each decision Implemented data wrangling cleaning transforming merging and reshaping data frames Configure Neo4j on Unix as well as windows system Environment Python Hive HBASE Spark Sqoop OoziePySparkPig Scripts Teradata Neo4j 34Neo4j Bloom Sql Server 2017 Lead   OtsukaLos Angles CA May 2017 to October 2017 Project Description The project goal is to analyze pharma business by combining data from different sources ie csv excel MySQL hive json postgres oracle to one centralize place and then create nodes and relationships in neo4j graphical database Responsibilities Developed a Machine Learning application to predict disease medication based on demographic and medical history in python Explored different implementations in Hadoop environment for data extraction and summarization by using packages like Hive Pig Importing and exporting data into HDFS using Sqoop Sql Server Teradata and Mysql Manipulate dataframes using Pandas and Numpy Libraries in Python Designed and developed a decision tree application using Neo4j graph database to model the nodes and relationships for each decision Wrote python scripts to extract data from diverse sources Worked on python scripts to load the data from csv json mysql hive files to Neo4j Graphical database Created Neo4j Graphical database nodes and edges using neo4jv1 library in Python Created indexes and constraints in Neo4j Implement cypher boundary queries to manipulate data on Neo4j database Data integration and data analysis using APOC procedures and functions in Neo4j Involved in Design analysis Implementation Testing and support of ETL processes for Stage ODS and Mart Used Talend as a Data Cleansing tool to correct the data before loading into the staging area Collect and link metadata from diverse sources including relational databases and flat files Extracted data from different Flat files MS Excel HIve and transformed the data based on user requirement using Talend and loaded data into target by scheduling the sessions Supporting daily loads and work with business users to handle rejected data Implemented data cleansing for files using Talend Created data model in Postgres using dimensional model Performed Unit Testing and tuned for better performance Created Reusable Transformations and multiple Mappings Environment Python Hive Pig Sqoop Oozie Neo4j 32 Talend 96 PGAdmin 14 Data Engineer Florida Power and Light Noida Uttar Pradesh July 2016 to April 2017 Project Description The System will streamline the verification of the sections like revenues billed cash postings refunds issued meters uploaded and cash deposited to banks The System has ability to create dashboard custom reporting and analytics The Revenue balance data can be drilled down by measures and dimensions enabling users to gain access to accurate uptodate Information for better decision making Responsibilities Developed and maintained Python ETL scripts to scrape data from external sources and load cleansed data into a Sql Server The data was used for daily electrical power virtual trading activities in several markets Implemented discretization and binning data wrangling cleaning transforming merging and reshaping data frames using Python Integrated applications with designing database architecture and server scripting studying establishing Designed data visualization to present current impact and growth Loaded the dataset into Hive for ETL Operation Written Hive scripts to extract data from staging tables Building publishing customized interactive reports and dashboards report scheduling using Power BI Desktop Used Power BI Power Pivot to develop data analysis prototype and used Power View and Power Map to visualize reports Used DAX Data Analysis Expressions functions for the creation of calculations and measures in the Power BI Models Designing and apply Microsoft Project Plan Requirement Design Development and Testing Data Modeling and Cube design in SSAS using Tabular Model Monitoring overall progress and use of resources initiating corrective action where necessary Planning and monitoring the project Preparing and maintaining project stage and exception plans as required Environment Hadoop Hive Python SSAS Power BI Data Engineer HMS Irvine Noida Uttar Pradesh March 2014 to July 2016 Project Description Client is an innovative healthcare services company with a Comprehensive patient centered approach to diabetes management SSIS has been used as an ETL tool for building of data warehouse and data consolidation Client provides the claim provider and membership files on daily weekly and monthly basis on FTP or EDI Server Package load the claim data into staging database After loading Medicare and Medicaid data in staging edits Rules provided by State and Federal government for claims applied on staging database and then after finally loaded into the Data Warehouse and then mail delivery report to the client at every step as Sanity Check Return file and Cleanup process Every step maintains in Sql server log table Error handling and monitoring the package is done by the BIxPress tools Responsibilities Designing developing deploying job scheduling reports and successfailure notifications in MS SQL Server environment using SSIS in Sql Server data Tools SSDT Used ETL SSIS tasks to develop jobs for extracting cleaning transforming and loading data into data warehouse Extracted data from various sources like SQL Server 2008 Oracle CSV Excel and Text file from Client servers and through FTP Experience working in a large data environment doing SQL development SQL query optimization Knowledge of Transactions Isolation Levels Indexes Blocks Locks and Database Partitioning etc Implemented SQL constraints Primary Key Foreign Key Index Unique Not Null Default Experience managing large projects with a close attention to detail and highquality performance Data modelling and mapping using UML in Visio Experience in handling Enterprise datasets Identify issues and fix those issues while troubleshooting Experience in writing MDX queries to access cubes data in SSAS Monitor scheduled SSIS Package through the BIxPress tools Error Trapping through BIxPress tool Script Writing in net Transaction Email Alert and some complex Transformation in SSIS Deployment and Scheduling SSIS Package through SQL Server and file system Report development in SSRS Create SSIS packages Email alert for SSIS packages and schedule it through SQL Server and file system Manage the Dev SQA and Production Environments Environment SQL Server 2012 with Integration Reporting Services and Analysis Services Cnet code for SSIS scripting BIxPress Microsoft Business intelligence development studio SQL Server 2012 Data Engineer Sanare Miami FL September 2012 to March 2014 Project Description The system is a database driven online Reporting model and SSIS package for data upload and send reports by the packages which is for Health care domain in USA for this report we have create our own data warehouse and fetch all report from this data warehouse Around 100 business reports have been created using functionality of SSRS SSIS has been used as an ETL tool for building of data warehouse and data consolidation ETL process has been designed on following lines Download daily extracts from SFTP server and upload them into data warehouse after performing various data integrity checks Preparation of data comparison sheet to compare record counts in tables against the counts which are supposed to be them Execution of data maintenance tasks eg Rebuild Indices After successful completion of data upload this package kicks off second package to generate various reports and distribute them in encrypted form to intended recipients Preparation of logs has been maintained during execution of package Notifications to users of failure of package if occurred Notifications to users of success of important tasks to keep them aware during execution At last summary has been sent to show the statistics of package Real time data has been picked from Other production systems for integrated reporting and Dashboards Package is configurable and data driven so it can be changed easily as per the changing business environment Responsibilities Data warehouse architecture designing ETL package Development in SSIS Deployment and Scheduling of SSIS Package in SSIS Populate data marts with the right data at the right time Performance Tuning SQL Query Optimization Complex Query Design Stored Procedure Writing Function Writing Creating Reports in SSRS Project monitoring Status reporting and client interaction Monitoring the critical components of the System Implementation and support of systems in production environment Environment SQL Server 2008 R2 with Integration and Reporting Services SSIS Scripting Microsoft Business intelligence development studio SQL Server 2008 R2 ETL Developer Aspen Noida Uttar Pradesh April 2012 to September 2012 Project Description Data Repository collects all clinical operational and financial extracts across the organization and uploads them in data warehouse after performing various data integrity checks It also integrates real time cloud data with the ware house and provides an open environment for powerful reporting and decision support to healthcare managerial authorities SSIS has been used as an ETL tool with following features Created Complex ETL Packages using SSIS to extract data from staging tables to partitioned tables with incremental load Created SSIS Reusable Packages to extract data from Multi formatted Flat files Excel XML files into Database Billing Systems Created SSIS packages for File Transfer from one location to the other using FTP task Developed deployed and monitored SSIS Packages Transform extract data after performing integrity checks Implement Email notification in case of package failure Integration with another package for cleanup task Develop various script tasks for validating and transform data Designed SSIS Packages to transfer data from flat files to SQL Server using Business Intelligence Development Studio Extensively used SSIS transformations such as Lookup Derived column Data conversion Aggregate Conditional split SQL task Script task and Send Mail task etc Implement configuration files to deploy the SSIS packages across all environments Responsibilities ETL package Development in SSIS Integration of realtime cloud data NetSuite with data Warehouse for critical business requirements Report development in SSRS Project monitoring Status reporting and client interaction Operate as the liaison between Digital Analytics team and IT Development team for all projects and releases including testing prepost deployment of changes to ensure timely and accurate reporting and insights Environment SQL Server 2008 R2 with Integration and Reporting Services Microsoft Business intelligence development studio SSMS SQL Server 2008 R2 ETL and Report Developer Aspen Noida Uttar Pradesh September 2011 to March 2012 Project Description PMIS is an application which stores information about various types of Patients disease and taking input from different chain management solutions around the globe PMIS is used to manage patient historical information Various business reports have been developed for dynamic data analysis Comparison of actual health data with industry standard in graphical analysis has been shown Following features have been implemented in this project Designing developing and deploying reports in MS SQL Server environment using SSRS2008 and SSIS in Business Intelligence Development Studio BIDS Used ETL SSIS to develop jobs for extracting cleaning transforming and loading data into data warehouse Designed SSIS Packages to transfer data from flat files to SQL Server using Business Intelligence Development Studio Creating multiple parameterized stored procedures which were used by the reports to get the data Worked on formatting SSRS reports using the Global variables and expressions Created parameterized reports Drill down and Drill through reports using SSRS Used Execution Plan SQL Profiler and Database Engine Tuning Advisor to optimize queries and enhance the performance of databases Optimized query performance by creating indexes Write TSQL statements for retrieval of data and performance tuning of TSQL Responsibilities ETL package Development in SSIS Report development in SSRS Database Design Understanding the business problem identifying relevant data gathering and summarizing data meaningfully Performance TuningQuery Optimization in SPs Complex Query Design Stored Procedure Writing Function Writing Trigger Writing Environment SQL Server 2008 R2SSISSSRS Microsoft Business intelligence development studio SQL Server 2008 R2 AspNet Developer Oil Field Services Mumbai Maharashtra September 2010 to September 2011 Project Description OFS is webbased application that keeps track of products between storesrigswell OFS application offers clients a secure and efficient management tool in processing their business OFS application is built as an ntier Web based application The Application User interface is built as Web application using ASPNET Master Pages AJAX controls and JavaScript To Support SOA based architecture Business Web services in middle tier is used and it acts as core business engine The web services consume provide the Business Entities Business Entities are used for passing the data across the tiers Database tier is the oracle database To maintain application portability across databases data transactions are handled in business services tier Responsibilities Coordination and handling tasks among team members Designing of HTML pages JavaScript validations Assist Project Manager in Feasibility study and requirement Analysis phase Implement web service as business layer Deploy application on Client Server Designing Database Creation and implementation of View Stored Procedures Documentation for coding Configuring deploying Database and web application for testing environment Code review during different stage of development life cycle Environment Web Framework 20 ASPNet 20 C Web Service TFS Visual Studio 2005 JavaScript Oracle 10 G AspNet Developer Insurity Hartford Mumbai Maharashtra January 2010 to August 2010 Project Description NGA Next Generation Application is a webbased insuranceproduct of one of the Famous American productbased company in commercial lines It allows their clients to do policyadministration as Automated rating and issuance support for all major commercial lines of business Full policy lifecycle transaction support Full maintenance of ISO and customerspecific rates forms and rules changes Responsibilities Requirement Gathering of defects Onsite Coordination Estimation of defects Defect Fixing Code review Environment Web Framework 20 VBNet VB6 SQL Server 2005 AspNet Developer Insurity Hartford Mumbai Maharashtra March 2009 to January 2010 Project CHealth Project Description CHealth is a Collaborative Consumer Engagement which is based on Microsoft connected Healthcare framework It empowers consumers through its Wellness module and at the same time provides an integrated platform wherein Payers TPAs Brokers ASOs Providers collaborate seamlessly With an innovative business model cHealth is supported by SOA based architecture To Support SOA based architecture Business Web services in middle tier is used and it acts as core business engine Responsibilities Coordination and handling tasks among team members Designing of HTML pages Assist Project Manager in Feasibility study and requirement Analysis phase Deploy application on Client Server Database Design Creation and implementation of View Stored Procedures Taking Backup and Restore Database Implement Web services as business tier Documentation for coding Configuring deploying Database and web application for testing environment Code review during different stage of development life cycle Developed HL7 translator Write Store Procedure to implement HL7 translator Environment Web Framework 20 AspNet with C SQL Server 2005 AspNet Developer Munich Re Princeton Mumbai Maharashtra August 2007 to February 2009 Project Description AutoFac is a webbased application that allows ReInsurance clients to conduct Property and Commercial Facultative reinsurance business operations with MRAm AutoFac application offers clients a secure and efficient management tool in processing their business AutoFac application is built as an ntier Web based application The Application User interface is built as Web application using ASPNET Master Pages AJAX controls and JavaScript To Support SOA based architecture Business Web services in middle tier is used and it acts as core business engine The web services consume provide the Business Entities Business Entities are used for passing the data across the tiers Database tier is the oracle database To maintain application portability across databases data transactions are handled in business services tier Responsibilities Designing of HTML pages JavaScript validations Programming for Referral People and System admin Organization Policy Quotes Location Upload and Reporting Identifying the Functionality of the Application Designing of framework Designing Database Model Creating Views and Stored Procedure Requirement gathering phase and UAT Extensive work on web services as business tier Deploying application for testing environment Documentation for specific modules Integration and System Testing Programming in XSLT for location upload Environment Web Framework 20 Microsoft Framework 20ASPNet with C Oracle 10 G Education Masters Skills APACHE HADOOP SQOOP 1 year ASP 4 years AspNet 4 years Business intelligence 5 years C 5 years database 10 years ETL 6 years EXTRACT TRANSFORM AND LOAD 6 years Hadoop 2 years Hive 2 years HTML 3 years JavaScript 2 years MS ASP 4 years MS SQL Server 8 years Python 2 years R2 2 years SAP 2 years SQL 8 years sql server 8 years SQL Server 2005 1 year",
    "extracted_keywords": [
        "Lead",
        "Lead",
        "span",
        "Lead",
        "PythonSparkHadoopNeo4j",
        "Charlotte",
        "NC",
        "years",
        "experience",
        "Python",
        "Hive",
        "Sqoop",
        "Spark",
        "Pig",
        "Scripts",
        "Big",
        "Data",
        "Neo4j",
        "Neo4j",
        "Bloom",
        "MS",
        "SQL",
        "Server",
        "MSBI",
        "SSIS",
        "SSRS",
        "SSAS",
        "ASPNet",
        "C",
        "Implemented",
        "Hadoop",
        "stack",
        "data",
        "tools",
        "migration",
        "databases",
        "Teradata",
        "Oracle",
        "MYSQL",
        "Hadoop",
        "Experience",
        "MapReduce",
        "programs",
        "Hadoop",
        "Big",
        "Data",
        "Strong",
        "Knowledge",
        "Hadoop",
        "Hive",
        "Hives",
        "functions",
        "data",
        "databases",
        "SQL",
        "interfaces",
        "Sqoop",
        "Efficient",
        "hive",
        "pig",
        "scripts",
        "depth",
        "understanding",
        "Spark",
        "Architecture",
        "Spark",
        "Core",
        "Spark",
        "Sql",
        "Data",
        "Frames",
        "Experience",
        "usage",
        "Hadoop",
        "Distribution",
        "Cloudera",
        "Expertise",
        "SparkSQL",
        "data",
        "sources",
        "JSON",
        "Parquet",
        "experience",
        "Hadoop",
        "Big",
        "Data",
        "processing",
        "Expertise",
        "queries",
        "Hive",
        "Pig",
        "files",
        "Hive",
        "HDFS",
        "Mysql",
        "Oracle",
        "Teradata",
        "Experience",
        "time",
        "data",
        "workflows",
        "Oozie",
        "MasteringLeading",
        "development",
        "applicationstools",
        "Python",
        "Worked",
        "python",
        "packages",
        "NumPy",
        "SciPy",
        "Pandas",
        "Experience",
        "Object",
        "Oriented",
        "Concepts",
        "Python",
        "Integrated",
        "data",
        "sources",
        "data",
        "data",
        "sets",
        "Python",
        "scripts",
        "Data",
        "Lineage",
        "graph",
        "origination",
        "consumption",
        "point",
        "view",
        "form",
        "nodes",
        "edges",
        "Neo4j",
        "Created",
        "Nodes",
        "Edges",
        "NoSQL",
        "Database",
        "Neo4j",
        "Strong",
        "Experience",
        "Data",
        "warehouse",
        "solutions",
        "Confidential",
        "Redshift",
        "migrate",
        "data",
        "premise",
        "databases",
        "Confidential",
        "Redshift",
        "RDS",
        "S3",
        "Expertise",
        "Spark",
        "RDD",
        "transformations",
        "actions",
        "Data",
        "Frames",
        "case",
        "classes",
        "input",
        "data",
        "data",
        "transformations",
        "PySpark",
        "Expertise",
        "SparkSQL",
        "data",
        "sources",
        "JSON",
        "Hive",
        "Expert",
        "SSIS",
        "Package",
        "development",
        "deployment",
        "Job",
        "Scheduling",
        "Error",
        "SSRS",
        "Report",
        "development",
        "deployment",
        "subscription",
        "SSAS",
        "Tabular",
        "Multidimensional",
        "Cube",
        "development",
        "experience",
        "Business",
        "Intelligence",
        "Developer",
        "Data",
        "Analyst",
        "Production",
        "Development",
        "Staging",
        "Environments",
        "data",
        "warehouse",
        "star",
        "snow",
        "flake",
        "schema",
        "fact",
        "table",
        "cubes",
        "dimensions",
        "measures",
        "SQL",
        "Server",
        "Analysis",
        "Services",
        "DAX",
        "Authorized",
        "US",
        "employer",
        "Work",
        "Experience",
        "Lead",
        "Johnson",
        "JohnsonNew",
        "Jersey",
        "October",
        "February",
        "Project",
        "Description",
        "Goal",
        "project",
        "data",
        "lake",
        "marketing",
        "data",
        "pharma",
        "vendors",
        "datasets",
        "vendors",
        "extracts",
        "S3",
        "validate",
        "data",
        "python",
        "scripts",
        "data",
        "redshift",
        "graph",
        "customers",
        "Practioners",
        "Facility",
        "relationshipsMedia",
        "Plan",
        "Digital",
        "Media",
        "Responsibilities",
        "Designing",
        "endtoend",
        "Data",
        "Warehouse",
        "infrastructure",
        "ground",
        "Confidential",
        "Redshift",
        "scale",
        "data",
        "Developed",
        "Spark",
        "program",
        "PySpark",
        "API",
        "Hive",
        "SQL",
        "Responsible",
        "ETL",
        "data",
        "validation",
        "Python",
        "scripts",
        "Quality",
        "Check",
        "script",
        "extract",
        "error",
        "case",
        "mismatch",
        "count",
        "extracts",
        "python",
        "AWS",
        "Data",
        "Pipeline",
        "data",
        "loads",
        "S3",
        "Redshift",
        "Write",
        "procedures",
        "data",
        "Redshift",
        "Import",
        "data",
        "AWS",
        "S3",
        "Sparks",
        "RDD",
        "transformations",
        "actions",
        "top",
        "RDD",
        "AVRO",
        "Parquet",
        "ORC",
        "file",
        "format",
        "data",
        "HDFS",
        "Created",
        "Practioners",
        "Facility",
        "nodes",
        "relationships",
        "neo4j",
        "Apoc",
        "procedures",
        "data",
        "queries",
        "media",
        "plan",
        "media",
        "cypher",
        "python",
        "scripts",
        "Unix",
        "command",
        "line",
        "Environment",
        "Python",
        "PySpark",
        "Hive",
        "HDFS",
        "AWS",
        "RedshiftPostgres",
        "Mysql",
        "S3",
        "Neo4j",
        "Lead",
        "AIG",
        "Charlotte",
        "NC",
        "October",
        "September",
        "Project",
        "Description",
        "Data",
        "Management",
        "SubsystemsDMS",
        "systems",
        "workflows",
        "sub",
        "applications",
        "servers",
        "Source",
        "subsystem",
        "server",
        "hive",
        "Data",
        "python",
        "nodes",
        "relationships",
        "neo4j",
        "database",
        "Workflows",
        "UI",
        "Responsibilities",
        "files",
        "data",
        "csv",
        "json",
        "MySQL",
        "hive",
        "files",
        "Neo4j",
        "database",
        "list",
        "data",
        "structure",
        "data",
        "sql",
        "server",
        "csv",
        "files",
        "Manipulate",
        "Pandas",
        "Numpy",
        "Libraries",
        "Python",
        "Performed",
        "transformations",
        "actions",
        "PySparks",
        "RDD",
        "Import",
        "pyodbc",
        "pyhive",
        "files",
        "Import",
        "data",
        "Sqoop",
        "Hive",
        "Teradata",
        "Export",
        "Import",
        "data",
        "HDFS",
        "Hive",
        "Sqoop",
        "JSON",
        "XML",
        "Serdes",
        "serialization",
        "deserialization",
        "Json",
        "Xml",
        "data",
        "Hive",
        "tables",
        "AVRO",
        "Parquet",
        "ORC",
        "file",
        "format",
        "compression",
        "formats",
        "Snappy",
        "data",
        "HDFS",
        "Load",
        "data",
        "Spark",
        "Dataframe",
        "memory",
        "data",
        "computation",
        "output",
        "response",
        "Store",
        "data",
        "HBASE",
        "analysis",
        "pig",
        "scripts",
        "data",
        "Load",
        "sets",
        "data",
        "sets",
        "metadata",
        "error",
        "Validate",
        "data",
        "files",
        "data",
        "quality",
        "error",
        "files",
        "Data",
        "Lineage",
        "Neo4j",
        "graph",
        "origination",
        "consumption",
        "point",
        "view",
        "Developed",
        "Business",
        "process",
        "hierarchy",
        "graph",
        "process",
        "system",
        "indexs",
        "constraints",
        "nodes",
        "edges",
        "Neo4j",
        "Graph",
        "cypher",
        "queries",
        "Apoc",
        "libraries",
        "perspectives",
        "queries",
        "Neo4j",
        "Bloom",
        "use",
        "Apoc",
        "libraries",
        "data",
        "modification",
        "data",
        "analysis",
        "Neo4j",
        "Graph",
        "Design",
        "Neo4j",
        "architecture",
        "nodes",
        "edges",
        "load",
        "Neo4j",
        "database",
        "nodes",
        "relationship",
        "Implement",
        "cypher",
        "data",
        "Neo4j",
        "database",
        "decision",
        "tree",
        "application",
        "Neo4J",
        "graph",
        "database",
        "nodes",
        "relationships",
        "decision",
        "data",
        "cleaning",
        "data",
        "frames",
        "Configure",
        "Neo4j",
        "Unix",
        "windows",
        "system",
        "Environment",
        "Python",
        "Hive",
        "HBASE",
        "Spark",
        "Sqoop",
        "Scripts",
        "Teradata",
        "Neo4j",
        "34Neo4j",
        "Bloom",
        "Sql",
        "Server",
        "Lead",
        "OtsukaLos",
        "Angles",
        "CA",
        "May",
        "October",
        "Project",
        "Description",
        "project",
        "goal",
        "pharma",
        "business",
        "data",
        "sources",
        "csv",
        "excel",
        "MySQL",
        "hive",
        "json",
        "postgres",
        "oracle",
        "place",
        "nodes",
        "relationships",
        "neo4j",
        "database",
        "Responsibilities",
        "Machine",
        "Learning",
        "application",
        "disease",
        "medication",
        "history",
        "python",
        "implementations",
        "Hadoop",
        "environment",
        "data",
        "extraction",
        "summarization",
        "packages",
        "Hive",
        "Pig",
        "Importing",
        "data",
        "HDFS",
        "Sqoop",
        "Sql",
        "Server",
        "Teradata",
        "Mysql",
        "Manipulate",
        "Pandas",
        "Numpy",
        "Libraries",
        "Python",
        "decision",
        "tree",
        "application",
        "Neo4j",
        "graph",
        "database",
        "nodes",
        "relationships",
        "decision",
        "Wrote",
        "python",
        "scripts",
        "data",
        "sources",
        "scripts",
        "data",
        "csv",
        "json",
        "mysql",
        "hive",
        "files",
        "Neo4j",
        "database",
        "Neo4j",
        "database",
        "nodes",
        "edges",
        "neo4jv1",
        "library",
        "Python",
        "indexes",
        "constraints",
        "Neo4j",
        "Implement",
        "boundary",
        "queries",
        "data",
        "Neo4j",
        "database",
        "Data",
        "integration",
        "data",
        "analysis",
        "procedures",
        "functions",
        "Neo4j",
        "Design",
        "analysis",
        "Implementation",
        "Testing",
        "support",
        "ETL",
        "processes",
        "Stage",
        "ODS",
        "Mart",
        "Talend",
        "Data",
        "Cleansing",
        "tool",
        "data",
        "staging",
        "area",
        "Collect",
        "metadata",
        "sources",
        "databases",
        "files",
        "data",
        "files",
        "MS",
        "Excel",
        "HIve",
        "data",
        "user",
        "requirement",
        "Talend",
        "data",
        "target",
        "sessions",
        "loads",
        "work",
        "business",
        "users",
        "data",
        "data",
        "cleansing",
        "files",
        "Talend",
        "data",
        "model",
        "Postgres",
        "model",
        "Performed",
        "Unit",
        "Testing",
        "performance",
        "Reusable",
        "Transformations",
        "Mappings",
        "Environment",
        "Python",
        "Hive",
        "Pig",
        "Sqoop",
        "Oozie",
        "Neo4j",
        "Talend",
        "PGAdmin",
        "Data",
        "Engineer",
        "Florida",
        "Power",
        "Light",
        "Noida",
        "Uttar",
        "Pradesh",
        "July",
        "April",
        "Project",
        "Description",
        "System",
        "verification",
        "sections",
        "revenues",
        "cash",
        "postings",
        "refunds",
        "meters",
        "cash",
        "banks",
        "System",
        "ability",
        "dashboard",
        "custom",
        "reporting",
        "Revenue",
        "balance",
        "data",
        "measures",
        "dimensions",
        "users",
        "access",
        "Information",
        "decision",
        "Responsibilities",
        "Python",
        "ETL",
        "scripts",
        "data",
        "sources",
        "load",
        "data",
        "Sql",
        "Server",
        "data",
        "power",
        "trading",
        "activities",
        "markets",
        "discretization",
        "data",
        "cleaning",
        "data",
        "frames",
        "Python",
        "Integrated",
        "applications",
        "database",
        "architecture",
        "server",
        "scripting",
        "data",
        "visualization",
        "impact",
        "growth",
        "dataset",
        "Hive",
        "ETL",
        "Operation",
        "Written",
        "Hive",
        "scripts",
        "data",
        "tables",
        "Building",
        "reports",
        "dashboards",
        "scheduling",
        "Power",
        "BI",
        "Desktop",
        "Used",
        "Power",
        "BI",
        "Power",
        "Pivot",
        "data",
        "analysis",
        "prototype",
        "Power",
        "View",
        "Power",
        "Map",
        "reports",
        "DAX",
        "Data",
        "Analysis",
        "Expressions",
        "functions",
        "creation",
        "calculations",
        "measures",
        "Power",
        "BI",
        "Models",
        "Designing",
        "Microsoft",
        "Project",
        "Plan",
        "Requirement",
        "Design",
        "Development",
        "Testing",
        "Data",
        "Modeling",
        "Cube",
        "design",
        "SSAS",
        "Tabular",
        "Model",
        "Monitoring",
        "progress",
        "use",
        "resources",
        "action",
        "Planning",
        "project",
        "project",
        "stage",
        "exception",
        "plans",
        "Environment",
        "Hadoop",
        "Hive",
        "Python",
        "SSAS",
        "Power",
        "BI",
        "Data",
        "Engineer",
        "HMS",
        "Irvine",
        "Noida",
        "Uttar",
        "Pradesh",
        "March",
        "July",
        "Project",
        "Description",
        "Client",
        "healthcare",
        "services",
        "company",
        "patient",
        "approach",
        "management",
        "SSIS",
        "ETL",
        "tool",
        "building",
        "data",
        "warehouse",
        "data",
        "consolidation",
        "Client",
        "claim",
        "provider",
        "membership",
        "files",
        "basis",
        "FTP",
        "EDI",
        "Server",
        "Package",
        "claim",
        "data",
        "staging",
        "database",
        "Medicare",
        "Medicaid",
        "data",
        "staging",
        "edits",
        "Rules",
        "State",
        "Federal",
        "government",
        "claims",
        "database",
        "Data",
        "Warehouse",
        "mail",
        "delivery",
        "report",
        "client",
        "step",
        "Sanity",
        "Check",
        "Return",
        "file",
        "process",
        "step",
        "Sql",
        "server",
        "log",
        "table",
        "Error",
        "handling",
        "package",
        "BIxPress",
        "tools",
        "Responsibilities",
        "job",
        "scheduling",
        "reports",
        "successfailure",
        "notifications",
        "MS",
        "SQL",
        "Server",
        "environment",
        "SSIS",
        "Sql",
        "Server",
        "data",
        "Tools",
        "SSDT",
        "ETL",
        "SSIS",
        "tasks",
        "jobs",
        "cleaning",
        "transforming",
        "loading",
        "data",
        "data",
        "warehouse",
        "data",
        "sources",
        "SQL",
        "Server",
        "Oracle",
        "CSV",
        "Excel",
        "Text",
        "file",
        "Client",
        "servers",
        "FTP",
        "Experience",
        "data",
        "environment",
        "SQL",
        "development",
        "SQL",
        "query",
        "optimization",
        "Knowledge",
        "Transactions",
        "Isolation",
        "Levels",
        "Indexes",
        "Blocks",
        "Locks",
        "Database",
        "Partitioning",
        "SQL",
        "Primary",
        "Key",
        "Foreign",
        "Key",
        "Index",
        "Unique",
        "Null",
        "Default",
        "Experience",
        "projects",
        "attention",
        "detail",
        "highquality",
        "performance",
        "Data",
        "modelling",
        "mapping",
        "UML",
        "Visio",
        "Experience",
        "Enterprise",
        "datasets",
        "issues",
        "issues",
        "Experience",
        "MDX",
        "access",
        "cubes",
        "data",
        "SSAS",
        "Monitor",
        "SSIS",
        "Package",
        "BIxPress",
        "tools",
        "Error",
        "BIxPress",
        "tool",
        "Script",
        "Writing",
        "Transaction",
        "Email",
        "Alert",
        "Transformation",
        "SSIS",
        "Deployment",
        "Scheduling",
        "SSIS",
        "Package",
        "SQL",
        "Server",
        "file",
        "system",
        "Report",
        "development",
        "SSRS",
        "packages",
        "Email",
        "alert",
        "SSIS",
        "packages",
        "SQL",
        "Server",
        "file",
        "system",
        "Dev",
        "SQA",
        "Production",
        "Environments",
        "Environment",
        "SQL",
        "Server",
        "Integration",
        "Reporting",
        "Services",
        "Analysis",
        "Services",
        "Cnet",
        "code",
        "SSIS",
        "scripting",
        "BIxPress",
        "Microsoft",
        "Business",
        "intelligence",
        "development",
        "studio",
        "SQL",
        "Server",
        "Data",
        "Engineer",
        "Sanare",
        "Miami",
        "FL",
        "September",
        "March",
        "Project",
        "Description",
        "system",
        "database",
        "Reporting",
        "model",
        "package",
        "data",
        "upload",
        "reports",
        "packages",
        "Health",
        "care",
        "domain",
        "USA",
        "report",
        "data",
        "warehouse",
        "report",
        "data",
        "warehouse",
        "business",
        "reports",
        "functionality",
        "SSRS",
        "SSIS",
        "ETL",
        "tool",
        "building",
        "data",
        "warehouse",
        "data",
        "consolidation",
        "ETL",
        "process",
        "lines",
        "Download",
        "extracts",
        "server",
        "data",
        "warehouse",
        "data",
        "integrity",
        "Preparation",
        "data",
        "comparison",
        "sheet",
        "record",
        "counts",
        "tables",
        "counts",
        "Execution",
        "data",
        "maintenance",
        "tasks",
        "Rebuild",
        "Indices",
        "completion",
        "data",
        "package",
        "package",
        "reports",
        "form",
        "recipients",
        "Preparation",
        "logs",
        "execution",
        "package",
        "Notifications",
        "users",
        "failure",
        "package",
        "Notifications",
        "users",
        "success",
        "tasks",
        "execution",
        "summary",
        "statistics",
        "package",
        "time",
        "data",
        "production",
        "systems",
        "reporting",
        "Dashboards",
        "Package",
        "data",
        "business",
        "environment",
        "Responsibilities",
        "Data",
        "warehouse",
        "architecture",
        "ETL",
        "package",
        "Development",
        "SSIS",
        "Deployment",
        "Scheduling",
        "SSIS",
        "Package",
        "SSIS",
        "Populate",
        "data",
        "marts",
        "data",
        "time",
        "Performance",
        "SQL",
        "Query",
        "Optimization",
        "Complex",
        "Query",
        "Design",
        "Stored",
        "Procedure",
        "Function",
        "Creating",
        "Reports",
        "SSRS",
        "Project",
        "Status",
        "reporting",
        "client",
        "interaction",
        "components",
        "System",
        "Implementation",
        "support",
        "systems",
        "production",
        "environment",
        "Environment",
        "SQL",
        "Server",
        "R2",
        "Integration",
        "Reporting",
        "Services",
        "SSIS",
        "Scripting",
        "Microsoft",
        "Business",
        "intelligence",
        "development",
        "studio",
        "SQL",
        "Server",
        "R2",
        "ETL",
        "Developer",
        "Aspen",
        "Noida",
        "Uttar",
        "Pradesh",
        "April",
        "September",
        "Project",
        "Description",
        "Data",
        "Repository",
        "extracts",
        "organization",
        "data",
        "warehouse",
        "data",
        "integrity",
        "checks",
        "time",
        "cloud",
        "data",
        "ware",
        "house",
        "environment",
        "reporting",
        "decision",
        "support",
        "authorities",
        "SSIS",
        "ETL",
        "tool",
        "features",
        "ETL",
        "Packages",
        "SSIS",
        "data",
        "tables",
        "tables",
        "load",
        "SSIS",
        "Reusable",
        "Packages",
        "data",
        "Multi",
        "files",
        "Excel",
        "XML",
        "files",
        "Database",
        "Billing",
        "Systems",
        "SSIS",
        "packages",
        "File",
        "Transfer",
        "location",
        "FTP",
        "task",
        "Developed",
        "SSIS",
        "Packages",
        "Transform",
        "data",
        "integrity",
        "checks",
        "Implement",
        "Email",
        "notification",
        "case",
        "package",
        "failure",
        "Integration",
        "package",
        "task",
        "script",
        "tasks",
        "data",
        "SSIS",
        "Packages",
        "data",
        "files",
        "SQL",
        "Server",
        "Business",
        "Intelligence",
        "Development",
        "Studio",
        "transformations",
        "Lookup",
        "Derived",
        "column",
        "Data",
        "conversion",
        "Aggregate",
        "Conditional",
        "split",
        "SQL",
        "task",
        "Script",
        "task",
        "Mail",
        "task",
        "configuration",
        "files",
        "packages",
        "environments",
        "Responsibilities",
        "ETL",
        "package",
        "Development",
        "SSIS",
        "Integration",
        "cloud",
        "data",
        "NetSuite",
        "data",
        "Warehouse",
        "business",
        "requirements",
        "development",
        "SSRS",
        "Project",
        "Status",
        "reporting",
        "client",
        "interaction",
        "Operate",
        "liaison",
        "Digital",
        "Analytics",
        "team",
        "IT",
        "Development",
        "team",
        "projects",
        "releases",
        "prepost",
        "deployment",
        "changes",
        "reporting",
        "insights",
        "Environment",
        "SQL",
        "Server",
        "R2",
        "Integration",
        "Reporting",
        "Services",
        "Microsoft",
        "Business",
        "intelligence",
        "development",
        "studio",
        "SQL",
        "Server",
        "R2",
        "ETL",
        "Report",
        "Developer",
        "Aspen",
        "Noida",
        "Uttar",
        "Pradesh",
        "September",
        "March",
        "Project",
        "Description",
        "PMIS",
        "application",
        "information",
        "types",
        "Patients",
        "input",
        "chain",
        "management",
        "solutions",
        "globe",
        "PMIS",
        "information",
        "business",
        "reports",
        "data",
        "analysis",
        "Comparison",
        "health",
        "data",
        "industry",
        "standard",
        "analysis",
        "features",
        "project",
        "Designing",
        "reports",
        "MS",
        "SQL",
        "Server",
        "environment",
        "SSRS2008",
        "SSIS",
        "Business",
        "Intelligence",
        "Development",
        "Studio",
        "BIDS",
        "ETL",
        "SSIS",
        "jobs",
        "cleaning",
        "transforming",
        "loading",
        "data",
        "data",
        "warehouse",
        "SSIS",
        "Packages",
        "data",
        "files",
        "SQL",
        "Server",
        "Business",
        "Intelligence",
        "Development",
        "Studio",
        "procedures",
        "reports",
        "data",
        "SSRS",
        "reports",
        "variables",
        "expressions",
        "parameterized",
        "reports",
        "Drill",
        "Drill",
        "reports",
        "SSRS",
        "Used",
        "Execution",
        "Plan",
        "SQL",
        "Profiler",
        "Database",
        "Engine",
        "Tuning",
        "Advisor",
        "queries",
        "performance",
        "databases",
        "query",
        "performance",
        "indexes",
        "TSQL",
        "statements",
        "retrieval",
        "data",
        "performance",
        "tuning",
        "TSQL",
        "Responsibilities",
        "ETL",
        "package",
        "Development",
        "SSIS",
        "Report",
        "development",
        "SSRS",
        "Database",
        "Design",
        "business",
        "problem",
        "data",
        "gathering",
        "data",
        "Performance",
        "TuningQuery",
        "Optimization",
        "SPs",
        "Complex",
        "Query",
        "Design",
        "Stored",
        "Procedure",
        "Function",
        "Trigger",
        "Writing",
        "Environment",
        "SQL",
        "Server",
        "R2SSISSSRS",
        "Microsoft",
        "Business",
        "intelligence",
        "development",
        "studio",
        "SQL",
        "Server",
        "R2",
        "AspNet",
        "Developer",
        "Oil",
        "Field",
        "Services",
        "Mumbai",
        "Maharashtra",
        "September",
        "September",
        "Project",
        "Description",
        "OFS",
        "application",
        "track",
        "products",
        "storesrigswell",
        "OFS",
        "application",
        "clients",
        "management",
        "tool",
        "business",
        "OFS",
        "application",
        "ntier",
        "Web",
        "application",
        "Application",
        "User",
        "interface",
        "Web",
        "application",
        "ASPNET",
        "Master",
        "Pages",
        "AJAX",
        "controls",
        "JavaScript",
        "Support",
        "SOA",
        "architecture",
        "Business",
        "Web",
        "services",
        "tier",
        "core",
        "business",
        "engine",
        "web",
        "services",
        "Business",
        "Entities",
        "Business",
        "Entities",
        "data",
        "tiers",
        "Database",
        "tier",
        "oracle",
        "database",
        "application",
        "portability",
        "databases",
        "data",
        "transactions",
        "business",
        "services",
        "tier",
        "Responsibilities",
        "Coordination",
        "handling",
        "tasks",
        "team",
        "members",
        "Designing",
        "HTML",
        "pages",
        "JavaScript",
        "Assist",
        "Project",
        "Manager",
        "Feasibility",
        "study",
        "requirement",
        "Analysis",
        "phase",
        "Implement",
        "web",
        "service",
        "business",
        "layer",
        "Deploy",
        "application",
        "Client",
        "Server",
        "Designing",
        "Database",
        "Creation",
        "implementation",
        "View",
        "Stored",
        "Procedures",
        "Documentation",
        "Configuring",
        "Database",
        "web",
        "application",
        "testing",
        "environment",
        "Code",
        "review",
        "stage",
        "development",
        "life",
        "cycle",
        "Environment",
        "Web",
        "Framework",
        "C",
        "Web",
        "Service",
        "TFS",
        "Visual",
        "Studio",
        "JavaScript",
        "Oracle",
        "G",
        "AspNet",
        "Developer",
        "Insurity",
        "Hartford",
        "Mumbai",
        "Maharashtra",
        "January",
        "August",
        "Project",
        "Description",
        "NGA",
        "Next",
        "Generation",
        "Application",
        "insuranceproduct",
        "Famous",
        "company",
        "lines",
        "clients",
        "policyadministration",
        "rating",
        "issuance",
        "support",
        "lines",
        "business",
        "policy",
        "lifecycle",
        "transaction",
        "maintenance",
        "ISO",
        "customerspecific",
        "rates",
        "forms",
        "rules",
        "Responsibilities",
        "Requirement",
        "Gathering",
        "defects",
        "Onsite",
        "Coordination",
        "Estimation",
        "defects",
        "Defect",
        "Fixing",
        "Code",
        "review",
        "Environment",
        "Web",
        "Framework",
        "VB6",
        "SQL",
        "Server",
        "AspNet",
        "Developer",
        "Insurity",
        "Hartford",
        "Mumbai",
        "Maharashtra",
        "March",
        "January",
        "Project",
        "CHealth",
        "Project",
        "Description",
        "CHealth",
        "Collaborative",
        "Consumer",
        "Engagement",
        "Microsoft",
        "Healthcare",
        "framework",
        "consumers",
        "Wellness",
        "module",
        "time",
        "platform",
        "Payers",
        "TPAs",
        "Brokers",
        "ASOs",
        "Providers",
        "business",
        "model",
        "cHealth",
        "SOA",
        "architecture",
        "Support",
        "SOA",
        "architecture",
        "Business",
        "Web",
        "services",
        "tier",
        "core",
        "business",
        "engine",
        "Responsibilities",
        "Coordination",
        "handling",
        "tasks",
        "team",
        "members",
        "Designing",
        "HTML",
        "pages",
        "Assist",
        "Project",
        "Manager",
        "Feasibility",
        "study",
        "requirement",
        "Analysis",
        "phase",
        "Deploy",
        "application",
        "Client",
        "Server",
        "Database",
        "Design",
        "Creation",
        "implementation",
        "View",
        "Stored",
        "Procedures",
        "Backup",
        "Restore",
        "Database",
        "Implement",
        "Web",
        "services",
        "business",
        "tier",
        "Documentation",
        "Configuring",
        "Database",
        "web",
        "application",
        "testing",
        "environment",
        "Code",
        "review",
        "stage",
        "development",
        "life",
        "cycle",
        "Developed",
        "HL7",
        "translator",
        "Store",
        "Procedure",
        "HL7",
        "translator",
        "Environment",
        "Web",
        "Framework",
        "AspNet",
        "C",
        "SQL",
        "Server",
        "AspNet",
        "Developer",
        "Munich",
        "Princeton",
        "Mumbai",
        "Maharashtra",
        "August",
        "February",
        "Project",
        "Description",
        "AutoFac",
        "application",
        "ReInsurance",
        "clients",
        "Property",
        "Commercial",
        "reinsurance",
        "business",
        "operations",
        "MRAm",
        "AutoFac",
        "application",
        "clients",
        "management",
        "tool",
        "business",
        "AutoFac",
        "application",
        "ntier",
        "Web",
        "application",
        "Application",
        "User",
        "interface",
        "Web",
        "application",
        "ASPNET",
        "Master",
        "Pages",
        "AJAX",
        "controls",
        "JavaScript",
        "Support",
        "SOA",
        "architecture",
        "Business",
        "Web",
        "services",
        "tier",
        "core",
        "business",
        "engine",
        "web",
        "services",
        "Business",
        "Entities",
        "Business",
        "Entities",
        "data",
        "tiers",
        "Database",
        "tier",
        "oracle",
        "database",
        "application",
        "portability",
        "databases",
        "data",
        "transactions",
        "business",
        "services",
        "tier",
        "Responsibilities",
        "Designing",
        "HTML",
        "pages",
        "JavaScript",
        "Programming",
        "Referral",
        "People",
        "System",
        "admin",
        "Organization",
        "Policy",
        "Quotes",
        "Location",
        "Upload",
        "Functionality",
        "Application",
        "Designing",
        "framework",
        "Designing",
        "Database",
        "Model",
        "Creating",
        "Views",
        "Procedure",
        "Requirement",
        "gathering",
        "phase",
        "work",
        "web",
        "services",
        "business",
        "tier",
        "Deploying",
        "application",
        "testing",
        "environment",
        "Documentation",
        "modules",
        "Integration",
        "System",
        "Testing",
        "Programming",
        "XSLT",
        "location",
        "upload",
        "Environment",
        "Web",
        "Framework",
        "Microsoft",
        "Framework",
        "20ASPNet",
        "C",
        "Oracle",
        "G",
        "Education",
        "Masters",
        "Skills",
        "APACHE",
        "HADOOP",
        "SQOOP",
        "year",
        "ASP",
        "years",
        "AspNet",
        "years",
        "Business",
        "intelligence",
        "years",
        "C",
        "years",
        "database",
        "years",
        "ETL",
        "years",
        "EXTRACT",
        "TRANSFORM",
        "years",
        "Hadoop",
        "years",
        "Hive",
        "years",
        "HTML",
        "years",
        "JavaScript",
        "years",
        "MS",
        "ASP",
        "years",
        "MS",
        "SQL",
        "Server",
        "years",
        "Python",
        "years",
        "R2",
        "years",
        "SAP",
        "years",
        "SQL",
        "years",
        "server",
        "years",
        "SQL",
        "Server",
        "year"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T22:02:03.676696",
    "resume_data": "Lead PythonSparkRedshiftS3 Lead span lPythonspanSparkRedshiftS3 Lead PythonSparkHadoopNeo4j Charlotte NC More than 12 years of experience in Python Hive Sqoop Spark Pig Scripts Big Data Neo4j Neo4j Bloom MS SQL Server MSBI SSIS SSRS SSAS ASPNet with C Implemented Hadoop stack and different big data analytic tools migration from different databases ie Teradata Oracle MYSQL to Hadoop Experience in working with MapReduce programs using Hadoop for working with Big Data Strong Knowledge of Hadoop and Hive and Hives analytical functions Capturing data from existing databases that provide SQL interfaces using Sqoop Efficient in building hive and pig scripts In depth understanding of Spark Architecture including Spark Core Spark Sql Data Frames Experience in usage of Hadoop Distribution like Cloudera 53 Expertise in using SparkSQL with various data sources like JSON and Parquet Having good experience in Hadoop Big Data processing Expertise in developing the queries in Hive Pig Successfully loaded files to Hive and HDFS from Mysql Oracle and Teradata Experience in designing both time driven and data driven automated workflows using Oozie MasteringLeading in the development of applicationstools using Python Worked on several python packages like NumPy SciPy Pandas etc Having Good Experience in Object Oriented Concepts with Python Integrated different data sources data wrangling cleaning transforming merging and reshaping data sets by writing Python scripts Designed and implemented Data Lineage graph from origination to consumption point of view in the form of nodes and edges in Neo4j Created Nodes and Edges in NoSQL Database Neo4j Strong Experience in implementing Data warehouse solutions in Confidential Redshift migrate data from on premise databases to Confidential Redshift RDS and S3 Expertise in writing Spark RDD transformations actions Data Frames case classes for the required input data and performed the data transformations using PySpark Expertise in using SparkSQL with various data sources like JSON and Hive Expert in SSIS Package development deployment Job Scheduling and Error Trapping SSRS Report development deployment subscription SSAS Tabular and Multidimensional Cube development Strong experience as Business Intelligence Developer and Data Analyst in Production Development and Staging Environments Defining data warehouse star and snow flake schema fact table cubes dimensions measures using SQL Server Analysis Services with DAX Authorized to work in the US for any employer Work Experience Lead PythonSparkRedshiftS3 Johnson JohnsonNew Jersey October 2018 to February 2019 Project Description Goal of this project is to create data lake for the marketing and retail data from the multiple pharma vendors This include retrieving huge datasets from vendors create extracts on S3 validate that data using python scripts and migrate data into redshift and then create graph of customers Practioners Facility and their relationshipsMedia Plan and Digital Media Responsibilities Designing and building full endtoend Data Warehouse infrastructure from the ground up on Confidential Redshift for large scale data handling Developed Spark program using PySpark API with Hive and SQL Responsible for ETL and data validation using Python scripts Worked on Quality Check script for matching extract count and trigger error in case of mismatch count in extracts using python Worked on AWS Data Pipeline to configure data loads from S3 to into Redshift Write complex stored procedures to extract data in Redshift Import data from AWS S3 into Sparks RDD and performed transformations and actions on top of that RDD Used AVRO Parquet and ORC file format to store data into HDFS Created Practioners and Facility as nodes and their relationships in neo4j Used Apoc procedures to analyze data Worked on boundary queries for media plan and digital media cypher queries Deploy and execute python scripts using Unix command line Environment Python PySpark Hive HDFS AWS RedshiftPostgres Mysql S3 Neo4j 34 Lead PythonHadoopNeo4j AIG Charlotte NC October 2017 to September 2018 Project Description Data Management SubsystemsDMS consists of managing different systems workflows processes sub processes applications and servers Source for this subsystem is sql server and hive Data is then integrated and validated by the python and then create nodes and relationships in neo4j graphical database Workflows can be modified through java UI Responsibilities Worked on python files to load the data from csv json MySQL hive files to Neo4j Graphical database Implemented list and dictionary as data structure to manipulate data from sql server csv and hive files Manipulate dataframes using Pandas and Numpy Libraries in Python Performed transformations and actions on PySparks RDD Import csv pyodbc and pyhive to deal with different structured files Import data using Sqoop into Hive and from existing Teradata Export and Import data into HDFS and Hive using Sqoop Used JSON and XML Serdes for serialization and deserialization to load Json and Xml data into Hive tables Worked with AVRO Parquet and ORC file format and various compression formats like Snappy to store data into HDFS Load the data into Spark Dataframe and do in memory data computation to generate output response Store data on HBASE for analysis Wrote different pig scripts to clean up ingested data Load and transform large sets of structured and semi structured data Used sets to compare metadata and differentiate Customized error logging as well as exceptional handling Validate the extracted data and prepare valid files as well as data quality error files Designed and implemented Data Lineage Neo4j graph from origination to consumption point of view Developed Business process hierarchy graph for every process available in the system Created indexs and constraints on nodes and edges of Neo4j Graph Worked on boundary cypher queries using Apoc libraries Create perspectives and parameterized queries in Neo4j Bloom Extensive use of Apoc libraries for data modification and data analysis in Neo4j Graph Design Neo4j architecture for nodes and edges load Created Neo4j Graphical database nodes and relationship Implement cypher queries to manipulate data on Neo4j database Designed and developed a decision tree application using Neo4J graph database to model the nodes and relationships for each decision Implemented data wrangling cleaning transforming merging and reshaping data frames Configure Neo4j on Unix as well as windows system Environment Python Hive HBASE Spark Sqoop OoziePySparkPig Scripts Teradata Neo4j 34Neo4j Bloom Sql Server 2017 Lead PythonHadoopNeo4j OtsukaLos Angles CA May 2017 to October 2017 Project Description The project goal is to analyze pharma business by combining data from different sources ie csv excel MySQL hive json postgres oracle to one centralize place and then create nodes and relationships in neo4j graphical database Responsibilities Developed a Machine Learning application to predict disease medication based on demographic and medical history in python Explored different implementations in Hadoop environment for data extraction and summarization by using packages like Hive Pig Importing and exporting data into HDFS using Sqoop Sql Server Teradata and Mysql Manipulate dataframes using Pandas and Numpy Libraries in Python Designed and developed a decision tree application using Neo4j graph database to model the nodes and relationships for each decision Wrote python scripts to extract data from diverse sources Worked on python scripts to load the data from csv json mysql hive files to Neo4j Graphical database Created Neo4j Graphical database nodes and edges using neo4jv1 library in Python Created indexes and constraints in Neo4j Implement cypher boundary queries to manipulate data on Neo4j database Data integration and data analysis using APOC procedures and functions in Neo4j Involved in Design analysis Implementation Testing and support of ETL processes for Stage ODS and Mart Used Talend as a Data Cleansing tool to correct the data before loading into the staging area Collect and link metadata from diverse sources including relational databases and flat files Extracted data from different Flat files MS Excel HIve and transformed the data based on user requirement using Talend and loaded data into target by scheduling the sessions Supporting daily loads and work with business users to handle rejected data Implemented data cleansing for files using Talend Created data model in Postgres using dimensional model Performed Unit Testing and tuned for better performance Created Reusable Transformations and multiple Mappings Environment Python Hive Pig Sqoop Oozie Neo4j 32 Talend 96 PGAdmin 14 Data Engineer Florida Power and Light Noida Uttar Pradesh July 2016 to April 2017 Project Description The System will streamline the verification of the sections like revenues billed cash postings refunds issued meters uploaded and cash deposited to banks The System has ability to create dashboard custom reporting and analytics The Revenue balance data can be drilled down by measures and dimensions enabling users to gain access to accurate uptodate Information for better decision making Responsibilities Developed and maintained Python ETL scripts to scrape data from external sources and load cleansed data into a Sql Server The data was used for daily electrical power virtual trading activities in several markets Implemented discretization and binning data wrangling cleaning transforming merging and reshaping data frames using Python Integrated applications with designing database architecture and server scripting studying establishing Designed data visualization to present current impact and growth Loaded the dataset into Hive for ETL Operation Written Hive scripts to extract data from staging tables Building publishing customized interactive reports and dashboards report scheduling using Power BI Desktop Used Power BI Power Pivot to develop data analysis prototype and used Power View and Power Map to visualize reports Used DAX Data Analysis Expressions functions for the creation of calculations and measures in the Power BI Models Designing and apply Microsoft Project Plan Requirement Design Development and Testing Data Modeling and Cube design in SSAS using Tabular Model Monitoring overall progress and use of resources initiating corrective action where necessary Planning and monitoring the project Preparing and maintaining project stage and exception plans as required Environment Hadoop Hive Python SSAS Power BI Data Engineer HMS Irvine Noida Uttar Pradesh March 2014 to July 2016 Project Description Client is an innovative healthcare services company with a Comprehensive patient centered approach to diabetes management SSIS has been used as an ETL tool for building of data warehouse and data consolidation Client provides the claim provider and membership files on daily weekly and monthly basis on FTP or EDI Server Package load the claim data into staging database After loading Medicare and Medicaid data in staging edits Rules provided by State and Federal government for claims applied on staging database and then after finally loaded into the Data Warehouse and then mail delivery report to the client at every step as Sanity Check Return file and Cleanup process Every step maintains in Sql server log table Error handling and monitoring the package is done by the BIxPress tools Responsibilities Designing developing deploying job scheduling reports and successfailure notifications in MS SQL Server environment using SSIS in Sql Server data Tools SSDT Used ETL SSIS tasks to develop jobs for extracting cleaning transforming and loading data into data warehouse Extracted data from various sources like SQL Server 2008 Oracle CSV Excel and Text file from Client servers and through FTP Experience working in a large data environment doing SQL development SQL query optimization Knowledge of Transactions Isolation Levels Indexes Blocks Locks and Database Partitioning etc Implemented SQL constraints Primary Key Foreign Key Index Unique Not Null Default Experience managing large projects with a close attention to detail and highquality performance Data modelling and mapping using UML in Visio Experience in handling Enterprise datasets Identify issues and fix those issues while troubleshooting Experience in writing MDX queries to access cubes data in SSAS Monitor scheduled SSIS Package through the BIxPress tools Error Trapping through BIxPress tool Script Writing in net Transaction Email Alert and some complex Transformation in SSIS Deployment and Scheduling SSIS Package through SQL Server and file system Report development in SSRS Create SSIS packages Email alert for SSIS packages and schedule it through SQL Server and file system Manage the Dev SQA and Production Environments Environment SQL Server 2012 with Integration Reporting Services and Analysis Services Cnet code for SSIS scripting BIxPress Microsoft Business intelligence development studio SQL Server 2012 Data Engineer Sanare Miami FL September 2012 to March 2014 Project Description The system is a database driven online Reporting model and SSIS package for data upload and send reports by the packages which is for Health care domain in USA for this report we have create our own data warehouse and fetch all report from this data warehouse Around 100 business reports have been created using functionality of SSRS SSIS has been used as an ETL tool for building of data warehouse and data consolidation ETL process has been designed on following lines Download daily extracts from SFTP server and upload them into data warehouse after performing various data integrity checks Preparation of data comparison sheet to compare record counts in tables against the counts which are supposed to be them Execution of data maintenance tasks eg Rebuild Indices After successful completion of data upload this package kicks off second package to generate various reports and distribute them in encrypted form to intended recipients Preparation of logs has been maintained during execution of package Notifications to users of failure of package if occurred Notifications to users of success of important tasks to keep them aware during execution At last summary has been sent to show the statistics of package Real time data has been picked from Other production systems for integrated reporting and Dashboards Package is configurable and data driven so it can be changed easily as per the changing business environment Responsibilities Data warehouse architecture designing ETL package Development in SSIS Deployment and Scheduling of SSIS Package in SSIS Populate data marts with the right data at the right time Performance Tuning SQL Query Optimization Complex Query Design Stored Procedure Writing Function Writing Creating Reports in SSRS Project monitoring Status reporting and client interaction Monitoring the critical components of the System Implementation and support of systems in production environment Environment SQL Server 2008 R2 with Integration and Reporting Services SSIS Scripting Microsoft Business intelligence development studio SQL Server 2008 R2 ETL Developer Aspen Noida Uttar Pradesh April 2012 to September 2012 Project Description Data Repository collects all clinical operational and financial extracts across the organization and uploads them in data warehouse after performing various data integrity checks It also integrates real time cloud data with the ware house and provides an open environment for powerful reporting and decision support to healthcare managerial authorities SSIS has been used as an ETL tool with following features Created Complex ETL Packages using SSIS to extract data from staging tables to partitioned tables with incremental load Created SSIS Reusable Packages to extract data from Multi formatted Flat files Excel XML files into Database Billing Systems Created SSIS packages for File Transfer from one location to the other using FTP task Developed deployed and monitored SSIS Packages Transform extract data after performing integrity checks Implement Email notification in case of package failure Integration with another package for cleanup task Develop various script tasks for validating and transform data Designed SSIS Packages to transfer data from flat files to SQL Server using Business Intelligence Development Studio Extensively used SSIS transformations such as Lookup Derived column Data conversion Aggregate Conditional split SQL task Script task and Send Mail task etc Implement configuration files to deploy the SSIS packages across all environments Responsibilities ETL package Development in SSIS Integration of realtime cloud data NetSuite with data Warehouse for critical business requirements Report development in SSRS Project monitoring Status reporting and client interaction Operate as the liaison between Digital Analytics team and IT Development team for all projects and releases including testing prepost deployment of changes to ensure timely and accurate reporting and insights Environment SQL Server 2008 R2 with Integration and Reporting Services Microsoft Business intelligence development studio SSMS SQL Server 2008 R2 ETL and Report Developer Aspen Noida Uttar Pradesh September 2011 to March 2012 Project Description PMIS is an application which stores information about various types of Patients disease and taking input from different chain management solutions around the globe PMIS is used to manage patient historical information Various business reports have been developed for dynamic data analysis Comparison of actual health data with industry standard in graphical analysis has been shown Following features have been implemented in this project Designing developing and deploying reports in MS SQL Server environment using SSRS2008 and SSIS in Business Intelligence Development Studio BIDS Used ETL SSIS to develop jobs for extracting cleaning transforming and loading data into data warehouse Designed SSIS Packages to transfer data from flat files to SQL Server using Business Intelligence Development Studio Creating multiple parameterized stored procedures which were used by the reports to get the data Worked on formatting SSRS reports using the Global variables and expressions Created parameterized reports Drill down and Drill through reports using SSRS Used Execution Plan SQL Profiler and Database Engine Tuning Advisor to optimize queries and enhance the performance of databases Optimized query performance by creating indexes Write TSQL statements for retrieval of data and performance tuning of TSQL Responsibilities ETL package Development in SSIS Report development in SSRS Database Design Understanding the business problem identifying relevant data gathering and summarizing data meaningfully Performance TuningQuery Optimization in SPs Complex Query Design Stored Procedure Writing Function Writing Trigger Writing Environment SQL Server 2008 R2SSISSSRS Microsoft Business intelligence development studio SQL Server 2008 R2 AspNet Developer Oil Field Services Mumbai Maharashtra September 2010 to September 2011 Project Description OFS is webbased application that keeps track of products between storesrigswell OFS application offers clients a secure and efficient management tool in processing their business OFS application is built as an ntier Web based application The Application User interface is built as Web application using ASPNET Master Pages AJAX controls and JavaScript To Support SOA based architecture Business Web services in middle tier is used and it acts as core business engine The web services consume provide the Business Entities Business Entities are used for passing the data across the tiers Database tier is the oracle database To maintain application portability across databases data transactions are handled in business services tier Responsibilities Coordination and handling tasks among team members Designing of HTML pages JavaScript validations Assist Project Manager in Feasibility study and requirement Analysis phase Implement web service as business layer Deploy application on Client Server Designing Database Creation and implementation of View Stored Procedures Documentation for coding Configuring deploying Database and web application for testing environment Code review during different stage of development life cycle Environment Web Framework 20 ASPNet 20 C Web Service TFS Visual Studio 2005 JavaScript Oracle 10G AspNet Developer Insurity Hartford Mumbai Maharashtra January 2010 to August 2010 Project Description NGA Next Generation Application is a webbased insuranceproduct of one of the Famous American productbased company in commercial lines It allows their clients to do policyadministration as Automated rating and issuance support for all major commercial lines of business Full policy lifecycle transaction support Full maintenance of ISO and customerspecific rates forms and rules changes Responsibilities Requirement Gathering of defects Onsite Coordination Estimation of defects Defect Fixing Code review Environment Web Framework 20 VBNet VB6 SQL Server 2005 AspNet Developer Insurity Hartford Mumbai Maharashtra March 2009 to January 2010 Project CHealth Project Description CHealth is a Collaborative Consumer Engagement which is based on Microsoft connected Healthcare framework It empowers consumers through its Wellness module and at the same time provides an integrated platform wherein Payers TPAs Brokers ASOs Providers collaborate seamlessly With an innovative business model cHealth is supported by SOA based architecture To Support SOA based architecture Business Web services in middle tier is used and it acts as core business engine Responsibilities Coordination and handling tasks among team members Designing of HTML pages Assist Project Manager in Feasibility study and requirement Analysis phase Deploy application on Client Server Database Design Creation and implementation of View Stored Procedures Taking Backup and Restore Database Implement Web services as business tier Documentation for coding Configuring deploying Database and web application for testing environment Code review during different stage of development life cycle Developed HL7 translator Write Store Procedure to implement HL7 translator Environment Web Framework 20 AspNet with C SQL Server 2005 AspNet Developer Munich Re Princeton Mumbai Maharashtra August 2007 to February 2009 Project Description AutoFac is a webbased application that allows ReInsurance clients to conduct Property and Commercial Facultative reinsurance business operations with MRAm AutoFac application offers clients a secure and efficient management tool in processing their business AutoFac application is built as an ntier Web based application The Application User interface is built as Web application using ASPNET Master Pages AJAX controls and JavaScript To Support SOA based architecture Business Web services in middle tier is used and it acts as core business engine The web services consume provide the Business Entities Business Entities are used for passing the data across the tiers Database tier is the oracle database To maintain application portability across databases data transactions are handled in business services tier Responsibilities Designing of HTML pages JavaScript validations Programming for Referral People and System admin Organization Policy Quotes Location Upload and Reporting Identifying the Functionality of the Application Designing of framework Designing Database Model Creating Views and Stored Procedure Requirement gathering phase and UAT Extensive work on web services as business tier Deploying application for testing environment Documentation for specific modules Integration and System Testing Programming in XSLT for location upload Environment Web Framework 20 Microsoft Framework 20ASPNet with C Oracle 10G Education Masters Skills APACHE HADOOP SQOOP 1 year ASP 4 years AspNet 4 years Business intelligence 5 years C 5 years database 10 years ETL 6 years EXTRACT TRANSFORM AND LOAD 6 years Hadoop 2 years Hive 2 years HTML 3 years JavaScript 2 years MS ASP 4 years MS SQL Server 8 years Python 2 years R2 2 years SAP 2 years SQL 8 years sql server 8 years SQL Server 2005 1 year",
    "unique_id": "3e234097-a302-469c-9a8c-f2433b4429ee"
}