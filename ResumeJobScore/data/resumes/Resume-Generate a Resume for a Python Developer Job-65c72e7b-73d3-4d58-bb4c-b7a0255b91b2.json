{
    "clean_data": "Spark Scala Developer Spark Scala span lDeveloperspan Spark Scala Developer Intel Folsom CA Around 10 years of experience in Design Analysis and Development of software application using Big Data Hadoop Spark and JavaJEE Technologies Knowledge in Spark Core SparkSQLSpark Streaming and machine learning using Scala and Python Programming languages Worked on Open Source Apache Hadoop Cloudera Enterprise CDH and Hortonworks Data Platform HDP Hands on experience on major components in Hadoop Ecosystem like Hadoop Map Reduce HDFS HIVE PIG Pentaho HBase Zookeeper Task Tracker Name Node Data NodeSqoop Oozie Cassandra Flume and Avro Developed various Map Reduce applications to perform ETL workloads on terabytes of data Expertise in working with HIVE data warehouse infrastructurecreating tables data distribution by implementing partitioning and bucketing writing and optimizing the HQL queries Used Spark Streaming to divide streaming data into batches as an input to Spark engine for batch processing Exploring with the Spark improving the performance and optimization of the existing algorithms in Hadoop Experience in working with flume to load the log data from multiple sources directly into HDFS Responsible to manage data coming from different sources and involved in HDFS maintenance and loading of structured and unstructured data Good understanding of RDD operations in Apache Spark like TransformationsActions Persistence Caching Accumulators Broadcast Variables Optimising Broadcasts Hands on experience in performing aggregations on data using Hive Query Language HQL Developed MapReduce programs in java Good experience in extending the core functionality of Hive and Pig by developing userdefined functions to provide custom capabilities to these languages Proficient in designing and querying the NoSQL databases like HBase and MongoDB Performed Importing and exporting data into HDFS and Hive using Sqoop Experience in scheduling time driven and data driven Oozie workflows Hands on experience in working with input file formats like parquet json Avro Knowledge on Elastic Search Concepts knowledge on visualizing the NOSQL data from elastic search using Kibana Worked on Extraction Transformation and Loading ETL of data from multiple sources like Flat files XML filesand Databases Handson experience in J2EE technologies such as Servlets JSP EJB JDBC and developing Web Services providers and consumers using SOAP REST Used Agile Development Methodology and Scrum for the development process Good Knowledge in HTML CSS JavaScript and web based applications Excellent analytical problem solving and interpersonal skills Ability to learn new concepts fast consistent team player with excellent communication skills Work Experience Spark Scala Developer Intel Folsom CA June 2018 to Present Responsibilities Developed a spark data ingestion framework to ingest data from various sources like SMB servers FTP servers and Web to HDFS CDR ingestion framework cleanses the metadata files related to actual data files before proceeding for the ingestion CDR ingestion framework will only ingest the data based on the consent from the metadata to a appropriate folder in HDFS Current scope of metadata collection using Rest Client by consuming the service which is created at PEGA platform Worked on Storing the Output of ingestion json in csv format Worked on storing the copy of metadata to HBase Worked on updating the records to Kibana Developed Shell script code to trigger the spark job Developed file watcher using shell script to perform the spark job as the data gets in the input path Developed Error mail code in Scala which triggers mail to specified group when error occurs in processing the json file Created logs in client and cluster mode Environment HDFS Spark Scala cloudera Intellij Hue Shell script HBase Kibana Spark Developer Charter Communications September 2017 to May 2018 Responsibilities Developed Apache spark jobs using Scala in test environment for faster data processing and used spark SQL for querying Developed Scala code using specific monad pattern for different calculations based on the requirement Developed and executed shell scripts to automate the jobs Written complex hive queries and automated using Azkaban for analyzing hourly calculations Analyzed large amounts of data sets using Pig scripts and Hive scripts Worked with Hue manager for developing hive queries and checking data in both development and production environments Developed Pig Latin scripts for extracting data Used Pig for data loading filtering and storing the data Worked on Data Integration from different source systems Used Mongo DB for storing the data Worked on retrieving data from Amazon kinesis streams and Amazon s3 Worked on creating topics in Kafka and integrating kinesis streams to Kafka and storing the data in the HDFS using gobblin ScheduledAutomate jobs using Azkaban Developed and executed shell scripts to automate the jobs Developed python code for creating fields in mongo Used Jenkins for continuous automation of code Complete endtoend design of Apache NiFi to get connected to AWS and store the final output in HDFS Environment HDFS Hive Pig Spark RDD SparkSQL Kafka Spark Scala Python MongoDB Horton works Intellij Azkaban AmbariHue Jenkins Apache NiFi Agile Spark Developer T Rowe Price Owings Mills MD September 2016 to August 2017 Responsibilities Import the data from different sources like HDFSHBase into Spark RDD Importing and exporting data into HDFS and HIVE using Sqoop Involved in gathering the requirements designing development and testing Developed PIG scripts for source data validation and transformation Designing and developing tables in HBase and storing aggregating data from Hive Developed Spark scripts by using Scala shell commands as per the requirement Developed Spark core and SparkSQL scripts using Scala for faster data processing Involved in code review and bug fixing for improving the performance Implemented the workflows using Apache Oozie framework to automate tasks Developing design documents considering all possible approaches and identifying best of them Implemented Partitioning Bucketing in Hive for better organization of the data Optimized Hive queries for performance tuning Used SparkStreaming APIs to perform necessary transformations and actions on the fly for building the common learner data model which gets the data from Kafka in near real time and Persists into Cassandra Responsible for developing data pipeline with Amazon AWS to extract the data from weblogs and store in HDFS Populated HDFS and Cassandra with huge data using Apache Kafka Basic knowledge on Machine Learning and Predictive Analysis Data analysis using Spark with Scala Analyze and report the data using Tableau Create dashboards in Tableau Environment HDFS Hive Pig Spark RDD Spark Streaming SparkSQL HBase Sqoop Oozie Kafka Cassandra Scala TableauAgile Hadoop Developer CareFirst Owings Mills MD January 2015 to September 2016 Responsibilities Worked on importing data from various sources and performed transformations using MapReduce hive to load data into HDFS Worked on compression mechanisms to optimize MapReduce Jobs Developed Big Data Solutions that enabled the business and technology teams to make datadriven decisions on the best ways to acquire customers and provide them business solutions Created scripts to automate the process of Data Ingestion Performed joins group by and other operations in MapReduce by using Java and PIG Configured Sqoop jobs to import data from RDBMS into HDFS using Oozie workflows Worked on setting up Pig Hive and HBase on multiple nodes and developed using Pig Hive HBase and MapReduce Worked on the conversion of existing MapReduce batch applications for better performance Created HBase tables to store variable data formats coming from different portfolios Performed real time analytics on HBase using Java API and Rest API Implemented HBase Coprocessors to notify Support team when inserting data into HBase Tables Worked on compression mechanisms to optimize MapReduce Jobs Analyzed the customer behavior by performing click stream analysis and to ingest the data used flume Experienced with working on Avro Data files using Avro Serialization system Implemented business logic by writing UDFs in Java and used various UDFs from Piggybanks and other sources Continuous monitoring and managing the Hadoop cluster using Cloudera Manager Exported the analyzed data to the relational databases using Sqoop for visualization and to generate reports for the BI team Environment HDFS Hive MapReduce Pig Sqoop RDBMS HBase Java API REST API Cloudera AVRO Flume Hadoop Developer EOG Resources Houston TX June 2013 to December 2014 Responsibilities Installed and configured Apache Hadoop to test the maintenance of log files in Hadoop cluster Installed and configured Hive Pig Sqoop and Oozie on the Hadoop cluster Installed Oozie Workflow engine to run multiple Hive and Pig Jobs Developed multiple MapReduce jobs in Java for data cleansing and preprocessing Developed Simple to complex MapReduce Jobs using Hive and Pig Involved in loading data from UNIX file system to HDFS Evaluated business requirements and prepared detailed specifications that follow project guidelines required to develop written programs Analyzed large amounts of data sets to determine optimal way to aggregate and report on it Provided quick response to ad hoc internal and external client requests for data and experienced in creating ad hoc reports Responsible for building scalable distributed data solutions using Hadoop Migration of ETL processes from Oracle to Hive to test the easy data manipulation Performed optimization on Pig scripts and Hive queries increase efficiency and add new features to existing code Developed PIG Latin scripts for the analysis of semi structured data Developed Hive queries to process the data Used Hive and created Hive tables and involved in data loading and writing Hive UDFs Used Sqoop to import data into HDFS and Hive from other data systems Installed Oozie workflow engine to run multiple Hive Continuous monitoring and managing the Hadoop cluster using Cloudera Manager Conducted some unit testing for the development team within the sandbox environment Environment Hadoop Cluster Hive Pig Sqoop Oozie Oracle Cloudera Manager UNIX ETL Mongo DB Agile Hadoop Developer Merck Bluebell PA April 2012 to May 2013 Responsibilities Installed and configured Hadoop MapReduce HDFS developed multiple MapReduce jobs in Java for data cleaning and preprocessing Good understanding and related experience with Hadoop stack internals Hive Pig and MapReduce The system was initially developed using Java The Java filtering program was restructured to have business rule engine in a jar that can be called from both java and Hadoop Wrote MapReduce jobs to discover trends in data usage by users Involved in defining job flows Involved in managing and reviewing Hadoop log files Load and transform large sets of structured semi structured and unstructured data Responsible to manage data coming from different sources Supported Map Reduce Programs those are running on the cluster Involved in loading data from UNIX file system to HDFS Responsible to manage data coming from different sources Installed and configured Hive and developed Hive UDFs to extend core functionality of hive Involved in creating Hive tables loading with data and writing hive queries which will run internally in map reduce way Implemented Partitioning Dynamic Partitions Buckets in HIVE Monitor System health and logs and respond accordingly to any warning or failure conditions Environment Apache Hadoop HDFS Map Reduce Pig Hive tables Hive UDFs UNIX Java ETL Eclipse Java Developer Gimmal New York NY December 2010 to March 2012 Responsibilities Designed the application using Agile Methodology Developed Maven based project structure having data layer ORM and Web module Developed MVC framework based website using JSF and spring Designed and Developed HTML pages and JSP pages Developed Business components using spring framework and database connections using JDBC Responsible for creating tables of clients information in and writing Hibernate mapping files to manage onetoone and onetomany mapping relationships Implemented data reading saving and modification by stored procedures in MySQL database and Hibernate criteria Developed Graphical User Interfaces by using JSF JSP HTML CSS and JavaScript Installation and configuration of Development Environment using Eclipse with WebLogic Application server On the server side post the access to the application and provided result on the network using RESTfulweb service Developed the XMLGateway to help the ordering process system communicate with the Order Execution Tool and different online tools such as Line Qualification Billing Information and Credit Card Validation Systems Used NodeJS to develop scalable web application Development TDD Approach environment using Agile methodologies Used JUnit to test debugged and implemented the application Implemented payment gateway using PayPal Developed build script using MAVEN to build package test and deploy application in application server Auditing tool is implemented by using log4j Environment MVC HTML JSP XML Maven JavaScript Nodejs TDD Junit log4j JDBC MYSQL Hibernate WebLogic JSF Spring Eclipse Java Developer Metlife Bridgewater NJ October 2009 to November 2010 Responsibilities Designing the Use Case Diagrams Class Model Sequence diagrams for SDLC process of the application Implemented GUI pages by using JavaScript HTML JSP CSS and AJAX Designed and developed UI components using JSP JMS JSTL Deployed project on Web Sphere application server in Linux environment Implemented the online application by using Web Services SOAP JSP Servlets JDBC and Core Java Implemented SingletonDAO Design Patterns factory design pattern based on the application requirements Used DOM and SAX parsers to parse the raw XML documents Tested the web services with SOAP UI tool Developed back end interfaces using PLSQL packages stored procedures Functions Procedure Anonymous PLSQL programs Cursor management Exception Handling in PLSQL programs Tuning complex database queries and joining the tables to improve the performance of the application Used Eclipse as Development IDE for web applications Environment JDBC HTML CSS JSP AJAX XML SOAP DOM SAX PLSQL Eclipse SOAP Servlets Java Developer HLine Soft Hyderabad Telangana June 2008 to September 2009 Responsibilities Involved in the complete SDLC software development life cycle of the application from requirement gathering and analysis to testing and maintenance Worked with the business community to define business requirements and analyze the possible technical solutions Requirement gathering Business Process flow Business Process Modeling and Business Analysis Implemented the User Login logic using Spring MVC framework encouraging application architectures based on the Model View Controller design paradigm Used various Java J2EE APIs including JDBC XML Servlets and JSP Generated Hibernate Mapping files and created the data model using mapping files Developed UI using JavaScript JSP HTML and CSS for interactive cross browser functionality and complex user interface Developed action classes and form beans and configured the strutsconfigxml Provided clientside validations using Struts Validator framework and JavaScript Created business logic using servlets and session beans and deployed them on Apache Tomcat server Created complex SQL Queries PLSQL Stored procedures and functions for back end Prepared the functional design and test case specifications Performed unit testing system testing and integration testing Used JUnit for unit testing of the application Provided Technical support for production environments resolving the issues analyzing the defects providing and implementing the solution defects Resolved more priority defects as per the schedule Environment SDLC Spring MVC JSP Servlets JavaScript SQL HTML CSS PLSQL Hibernate Junit Education Bachelors in Computer Science in Computer Science JNTU 2008 Skills HDFS 6 years Hive 6 years Pig 6 years JAVA 8 years APACHE 5 years Drupal Servicenow Hadoop AEM Cobol Additional Information TECHNICAL SKILLS HadoopBig Data Hadoop MapReduce HDFS Hive Pig Sqoop Flume Oozie Zookeeper Spark Kafka Languages and Web Technologies C C C Scala XML HTML CSS JavaScript J2EE Java Python JSP Frameworks Spring struts Hibernate Servlets Web Services REST SOAP Databases MY SQL Oracle DB2 MongoDB SparkSQL HBase Tools Tableau Weka Application servers Apache Tomcat WebLogic 80 IDEModelling Tools Eclipse Intellij Development Methodologies Agile Waterfall Logging tools Log4j Operating Systems Windows 7810 LINUX",
    "entities": [
        "Installed Oozie",
        "Credit Card Validation Systems Used",
        "Horton",
        "MY SQL Oracle",
        "ORM",
        "Hadoop Experience",
        "Worked on Data Integration",
        "BI",
        "HDFS",
        "UNIX",
        "Agile Methodology Developed",
        "Developed Spark",
        "CDR",
        "Hadoop Ecosystem",
        "Developed Error",
        "Computer Science in Computer",
        "RDD",
        "Sqoop Involved",
        "Hadoop",
        "XML",
        "Hive Query Language HQL Developed MapReduce",
        "Hibernate Servlets Web Services REST SOAP Databases",
        "NOSQL",
        "WebLogic",
        "JUnit",
        "Kibana Worked on Extraction Transformation",
        "Tableau Weka Application",
        "HBase",
        "Apache Spark",
        "Amazon",
        "Piggybanks",
        "HDFS Environment HDFS Hive Pig Spark",
        "Developed",
        "SparkSQL",
        "Web Technologies C C C",
        "MapReduce Worked",
        "Avro Data",
        "Hadoop Migration of ETL",
        "Node Data",
        "MapReduce Jobs Analyzed",
        "Provided Technical",
        "Responsibilities Import",
        "Present Responsibilities Developed",
        "ScheduledAutomate",
        "Pig Hive HBase",
        "JSP Generated Hibernate Mapping",
        "Developed Business",
        "Big Data Hadoop Spark",
        "Linux",
        "JSP",
        "Rest Client",
        "DOM",
        "CareFirst",
        "MapReduce Jobs",
        "MVC",
        "Spark",
        "MapReduce Jobs Developed Big Data Solutions",
        "SMB",
        "HTML CSS JavaScript",
        "Sqoop",
        "HIVE",
        "Intel",
        "Predictive Analysis Data",
        "Intellij Azkaban AmbariHue Jenkins Apache",
        "Created",
        "Hadoop MapReduce HDFS",
        "Data Ingestion Performed",
        "JSF",
        "PIG",
        "HDFS Responsible",
        "Oracle to Hive",
        "SAX",
        "HIVE Monitor System",
        "Oozie",
        "RESTfulweb",
        "SQL Queries PLSQL Stored",
        "Azkaban Developed",
        "SQL",
        "MD",
        "MAVEN",
        "Azkaban",
        "Servicenow Hadoop AEM Cobol Additional Information TECHNICAL SKILLS HadoopBig Data Hadoop MapReduce HDFS Hive Pig",
        "Hive",
        "Python Programming",
        "WebLogic Application",
        "Amazon AWS",
        "FTP",
        "Web Services SOAP JSP Servlets JDBC",
        "the Order Execution Tool",
        "HBase Worked",
        "Environment Apache Hadoop",
        "Avro Serialization",
        "Hadoop Wrote MapReduce",
        "Design Analysis and Development",
        "ETL",
        "Apache Hadoop",
        "JavaScript Created",
        "Performed",
        "AJAX Designed",
        "Tableau Create",
        "UI",
        "Development Environment",
        "Hive Developed Spark",
        "SOAP UI",
        "Cursor",
        "JDBC XML Servlets",
        "CSS",
        "Created HBase",
        "MapReduce",
        "Developed UI",
        "Line Qualification Billing Information",
        "NoSQL",
        "Tableau",
        "Machine Learning",
        "Spark Scala Developer",
        "JSP JMS JSTL Deployed"
    ],
    "experience": "Experience in working with flume to load the log data from multiple sources directly into HDFS Responsible to manage data coming from different sources and involved in HDFS maintenance and loading of structured and unstructured data Good understanding of RDD operations in Apache Spark like TransformationsActions Persistence Caching Accumulators Broadcast Variables Optimising Broadcasts Hands on experience in performing aggregations on data using Hive Query Language HQL Developed MapReduce programs in java Good experience in extending the core functionality of Hive and Pig by developing userdefined functions to provide custom capabilities to these languages Proficient in designing and querying the NoSQL databases like HBase and MongoDB Performed Importing and exporting data into HDFS and Hive using Sqoop Experience in scheduling time driven and data driven Oozie workflows Hands on experience in working with input file formats like parquet json Avro Knowledge on Elastic Search Concepts knowledge on visualizing the NOSQL data from elastic search using Kibana Worked on Extraction Transformation and Loading ETL of data from multiple sources like Flat files XML filesand Databases Handson experience in J2EE technologies such as Servlets JSP EJB JDBC and developing Web Services providers and consumers using SOAP REST Used Agile Development Methodology and Scrum for the development process Good Knowledge in HTML CSS JavaScript and web based applications Excellent analytical problem solving and interpersonal skills Ability to learn new concepts fast consistent team player with excellent communication skills Work Experience Spark Scala Developer Intel Folsom CA June 2018 to Present Responsibilities Developed a spark data ingestion framework to ingest data from various sources like SMB servers FTP servers and Web to HDFS CDR ingestion framework cleanses the metadata files related to actual data files before proceeding for the ingestion CDR ingestion framework will only ingest the data based on the consent from the metadata to a appropriate folder in HDFS Current scope of metadata collection using Rest Client by consuming the service which is created at PEGA platform Worked on Storing the Output of ingestion json in csv format Worked on storing the copy of metadata to HBase Worked on updating the records to Kibana Developed Shell script code to trigger the spark job Developed file watcher using shell script to perform the spark job as the data gets in the input path Developed Error mail code in Scala which triggers mail to specified group when error occurs in processing the json file Created logs in client and cluster mode Environment HDFS Spark Scala cloudera Intellij Hue Shell script HBase Kibana Spark Developer Charter Communications September 2017 to May 2018 Responsibilities Developed Apache spark jobs using Scala in test environment for faster data processing and used spark SQL for querying Developed Scala code using specific monad pattern for different calculations based on the requirement Developed and executed shell scripts to automate the jobs Written complex hive queries and automated using Azkaban for analyzing hourly calculations Analyzed large amounts of data sets using Pig scripts and Hive scripts Worked with Hue manager for developing hive queries and checking data in both development and production environments Developed Pig Latin scripts for extracting data Used Pig for data loading filtering and storing the data Worked on Data Integration from different source systems Used Mongo DB for storing the data Worked on retrieving data from Amazon kinesis streams and Amazon s3 Worked on creating topics in Kafka and integrating kinesis streams to Kafka and storing the data in the HDFS using gobblin ScheduledAutomate jobs using Azkaban Developed and executed shell scripts to automate the jobs Developed python code for creating fields in mongo Used Jenkins for continuous automation of code Complete endtoend design of Apache NiFi to get connected to AWS and store the final output in HDFS Environment HDFS Hive Pig Spark RDD SparkSQL Kafka Spark Scala Python MongoDB Horton works Intellij Azkaban AmbariHue Jenkins Apache NiFi Agile Spark Developer T Rowe Price Owings Mills MD September 2016 to August 2017 Responsibilities Import the data from different sources like HDFSHBase into Spark RDD Importing and exporting data into HDFS and HIVE using Sqoop Involved in gathering the requirements designing development and testing Developed PIG scripts for source data validation and transformation Designing and developing tables in HBase and storing aggregating data from Hive Developed Spark scripts by using Scala shell commands as per the requirement Developed Spark core and SparkSQL scripts using Scala for faster data processing Involved in code review and bug fixing for improving the performance Implemented the workflows using Apache Oozie framework to automate tasks Developing design documents considering all possible approaches and identifying best of them Implemented Partitioning Bucketing in Hive for better organization of the data Optimized Hive queries for performance tuning Used SparkStreaming APIs to perform necessary transformations and actions on the fly for building the common learner data model which gets the data from Kafka in near real time and Persists into Cassandra Responsible for developing data pipeline with Amazon AWS to extract the data from weblogs and store in HDFS Populated HDFS and Cassandra with huge data using Apache Kafka Basic knowledge on Machine Learning and Predictive Analysis Data analysis using Spark with Scala Analyze and report the data using Tableau Create dashboards in Tableau Environment HDFS Hive Pig Spark RDD Spark Streaming SparkSQL HBase Sqoop Oozie Kafka Cassandra Scala TableauAgile Hadoop Developer CareFirst Owings Mills MD January 2015 to September 2016 Responsibilities Worked on importing data from various sources and performed transformations using MapReduce hive to load data into HDFS Worked on compression mechanisms to optimize MapReduce Jobs Developed Big Data Solutions that enabled the business and technology teams to make datadriven decisions on the best ways to acquire customers and provide them business solutions Created scripts to automate the process of Data Ingestion Performed joins group by and other operations in MapReduce by using Java and PIG Configured Sqoop jobs to import data from RDBMS into HDFS using Oozie workflows Worked on setting up Pig Hive and HBase on multiple nodes and developed using Pig Hive HBase and MapReduce Worked on the conversion of existing MapReduce batch applications for better performance Created HBase tables to store variable data formats coming from different portfolios Performed real time analytics on HBase using Java API and Rest API Implemented HBase Coprocessors to notify Support team when inserting data into HBase Tables Worked on compression mechanisms to optimize MapReduce Jobs Analyzed the customer behavior by performing click stream analysis and to ingest the data used flume Experienced with working on Avro Data files using Avro Serialization system Implemented business logic by writing UDFs in Java and used various UDFs from Piggybanks and other sources Continuous monitoring and managing the Hadoop cluster using Cloudera Manager Exported the analyzed data to the relational databases using Sqoop for visualization and to generate reports for the BI team Environment HDFS Hive MapReduce Pig Sqoop RDBMS HBase Java API REST API Cloudera AVRO Flume Hadoop Developer EOG Resources Houston TX June 2013 to December 2014 Responsibilities Installed and configured Apache Hadoop to test the maintenance of log files in Hadoop cluster Installed and configured Hive Pig Sqoop and Oozie on the Hadoop cluster Installed Oozie Workflow engine to run multiple Hive and Pig Jobs Developed multiple MapReduce jobs in Java for data cleansing and preprocessing Developed Simple to complex MapReduce Jobs using Hive and Pig Involved in loading data from UNIX file system to HDFS Evaluated business requirements and prepared detailed specifications that follow project guidelines required to develop written programs Analyzed large amounts of data sets to determine optimal way to aggregate and report on it Provided quick response to ad hoc internal and external client requests for data and experienced in creating ad hoc reports Responsible for building scalable distributed data solutions using Hadoop Migration of ETL processes from Oracle to Hive to test the easy data manipulation Performed optimization on Pig scripts and Hive queries increase efficiency and add new features to existing code Developed PIG Latin scripts for the analysis of semi structured data Developed Hive queries to process the data Used Hive and created Hive tables and involved in data loading and writing Hive UDFs Used Sqoop to import data into HDFS and Hive from other data systems Installed Oozie workflow engine to run multiple Hive Continuous monitoring and managing the Hadoop cluster using Cloudera Manager Conducted some unit testing for the development team within the sandbox environment Environment Hadoop Cluster Hive Pig Sqoop Oozie Oracle Cloudera Manager UNIX ETL Mongo DB Agile Hadoop Developer Merck Bluebell PA April 2012 to May 2013 Responsibilities Installed and configured Hadoop MapReduce HDFS developed multiple MapReduce jobs in Java for data cleaning and preprocessing Good understanding and related experience with Hadoop stack internals Hive Pig and MapReduce The system was initially developed using Java The Java filtering program was restructured to have business rule engine in a jar that can be called from both java and Hadoop Wrote MapReduce jobs to discover trends in data usage by users Involved in defining job flows Involved in managing and reviewing Hadoop log files Load and transform large sets of structured semi structured and unstructured data Responsible to manage data coming from different sources Supported Map Reduce Programs those are running on the cluster Involved in loading data from UNIX file system to HDFS Responsible to manage data coming from different sources Installed and configured Hive and developed Hive UDFs to extend core functionality of hive Involved in creating Hive tables loading with data and writing hive queries which will run internally in map reduce way Implemented Partitioning Dynamic Partitions Buckets in HIVE Monitor System health and logs and respond accordingly to any warning or failure conditions Environment Apache Hadoop HDFS Map Reduce Pig Hive tables Hive UDFs UNIX Java ETL Eclipse Java Developer Gimmal New York NY December 2010 to March 2012 Responsibilities Designed the application using Agile Methodology Developed Maven based project structure having data layer ORM and Web module Developed MVC framework based website using JSF and spring Designed and Developed HTML pages and JSP pages Developed Business components using spring framework and database connections using JDBC Responsible for creating tables of clients information in and writing Hibernate mapping files to manage onetoone and onetomany mapping relationships Implemented data reading saving and modification by stored procedures in MySQL database and Hibernate criteria Developed Graphical User Interfaces by using JSF JSP HTML CSS and JavaScript Installation and configuration of Development Environment using Eclipse with WebLogic Application server On the server side post the access to the application and provided result on the network using RESTfulweb service Developed the XMLGateway to help the ordering process system communicate with the Order Execution Tool and different online tools such as Line Qualification Billing Information and Credit Card Validation Systems Used NodeJS to develop scalable web application Development TDD Approach environment using Agile methodologies Used JUnit to test debugged and implemented the application Implemented payment gateway using PayPal Developed build script using MAVEN to build package test and deploy application in application server Auditing tool is implemented by using log4j Environment MVC HTML JSP XML Maven JavaScript Nodejs TDD Junit log4j JDBC MYSQL Hibernate WebLogic JSF Spring Eclipse Java Developer Metlife Bridgewater NJ October 2009 to November 2010 Responsibilities Designing the Use Case Diagrams Class Model Sequence diagrams for SDLC process of the application Implemented GUI pages by using JavaScript HTML JSP CSS and AJAX Designed and developed UI components using JSP JMS JSTL Deployed project on Web Sphere application server in Linux environment Implemented the online application by using Web Services SOAP JSP Servlets JDBC and Core Java Implemented SingletonDAO Design Patterns factory design pattern based on the application requirements Used DOM and SAX parsers to parse the raw XML documents Tested the web services with SOAP UI tool Developed back end interfaces using PLSQL packages stored procedures Functions Procedure Anonymous PLSQL programs Cursor management Exception Handling in PLSQL programs Tuning complex database queries and joining the tables to improve the performance of the application Used Eclipse as Development IDE for web applications Environment JDBC HTML CSS JSP AJAX XML SOAP DOM SAX PLSQL Eclipse SOAP Servlets Java Developer HLine Soft Hyderabad Telangana June 2008 to September 2009 Responsibilities Involved in the complete SDLC software development life cycle of the application from requirement gathering and analysis to testing and maintenance Worked with the business community to define business requirements and analyze the possible technical solutions Requirement gathering Business Process flow Business Process Modeling and Business Analysis Implemented the User Login logic using Spring MVC framework encouraging application architectures based on the Model View Controller design paradigm Used various Java J2EE APIs including JDBC XML Servlets and JSP Generated Hibernate Mapping files and created the data model using mapping files Developed UI using JavaScript JSP HTML and CSS for interactive cross browser functionality and complex user interface Developed action classes and form beans and configured the strutsconfigxml Provided clientside validations using Struts Validator framework and JavaScript Created business logic using servlets and session beans and deployed them on Apache Tomcat server Created complex SQL Queries PLSQL Stored procedures and functions for back end Prepared the functional design and test case specifications Performed unit testing system testing and integration testing Used JUnit for unit testing of the application Provided Technical support for production environments resolving the issues analyzing the defects providing and implementing the solution defects Resolved more priority defects as per the schedule Environment SDLC Spring MVC JSP Servlets JavaScript SQL HTML CSS PLSQL Hibernate Junit Education Bachelors in Computer Science in Computer Science JNTU 2008 Skills HDFS 6 years Hive 6 years Pig 6 years JAVA 8 years APACHE 5 years Drupal Servicenow Hadoop AEM Cobol Additional Information TECHNICAL SKILLS HadoopBig Data Hadoop MapReduce HDFS Hive Pig Sqoop Flume Oozie Zookeeper Spark Kafka Languages and Web Technologies C C C Scala XML HTML CSS JavaScript J2EE Java Python JSP Frameworks Spring struts Hibernate Servlets Web Services REST SOAP Databases MY SQL Oracle DB2 MongoDB SparkSQL HBase Tools Tableau Weka Application servers Apache Tomcat WebLogic 80 IDEModelling Tools Eclipse Intellij Development Methodologies Agile Waterfall Logging tools Log4j Operating Systems Windows 7810 LINUX",
    "extracted_keywords": [
        "Spark",
        "Scala",
        "Developer",
        "Spark",
        "Scala",
        "span",
        "lDeveloperspan",
        "Spark",
        "Scala",
        "Developer",
        "Intel",
        "Folsom",
        "CA",
        "years",
        "experience",
        "Design",
        "Analysis",
        "Development",
        "software",
        "application",
        "Big",
        "Data",
        "Hadoop",
        "Spark",
        "JavaJEE",
        "Technologies",
        "Knowledge",
        "Spark",
        "Core",
        "SparkSQLSpark",
        "Streaming",
        "machine",
        "learning",
        "Scala",
        "Python",
        "Programming",
        "languages",
        "Open",
        "Source",
        "Apache",
        "Hadoop",
        "Cloudera",
        "Enterprise",
        "CDH",
        "Hortonworks",
        "Data",
        "Platform",
        "HDP",
        "Hands",
        "experience",
        "components",
        "Hadoop",
        "Ecosystem",
        "Hadoop",
        "Map",
        "HDFS",
        "HIVE",
        "PIG",
        "Pentaho",
        "HBase",
        "Zookeeper",
        "Task",
        "Tracker",
        "Name",
        "Node",
        "Data",
        "NodeSqoop",
        "Oozie",
        "Cassandra",
        "Flume",
        "Avro",
        "Map",
        "Reduce",
        "applications",
        "ETL",
        "workloads",
        "terabytes",
        "data",
        "Expertise",
        "HIVE",
        "data",
        "tables",
        "data",
        "distribution",
        "bucketing",
        "writing",
        "HQL",
        "queries",
        "Spark",
        "Streaming",
        "streaming",
        "data",
        "batches",
        "input",
        "Spark",
        "engine",
        "batch",
        "processing",
        "Spark",
        "performance",
        "optimization",
        "algorithms",
        "Hadoop",
        "Experience",
        "flume",
        "log",
        "data",
        "sources",
        "HDFS",
        "data",
        "sources",
        "HDFS",
        "maintenance",
        "loading",
        "data",
        "understanding",
        "RDD",
        "operations",
        "Apache",
        "Spark",
        "TransformationsActions",
        "Persistence",
        "Caching",
        "Accumulators",
        "Broadcast",
        "Variables",
        "Optimising",
        "Hands",
        "experience",
        "aggregations",
        "data",
        "Hive",
        "Query",
        "Language",
        "HQL",
        "MapReduce",
        "programs",
        "experience",
        "functionality",
        "Hive",
        "Pig",
        "functions",
        "custom",
        "capabilities",
        "languages",
        "NoSQL",
        "databases",
        "HBase",
        "MongoDB",
        "Performed",
        "Importing",
        "data",
        "HDFS",
        "Hive",
        "Sqoop",
        "Experience",
        "scheduling",
        "time",
        "data",
        "Oozie",
        "Hands",
        "experience",
        "input",
        "file",
        "formats",
        "parquet",
        "json",
        "Avro",
        "Knowledge",
        "Elastic",
        "Search",
        "Concepts",
        "knowledge",
        "NOSQL",
        "data",
        "search",
        "Kibana",
        "Worked",
        "Extraction",
        "Transformation",
        "Loading",
        "ETL",
        "data",
        "sources",
        "files",
        "XML",
        "filesand",
        "Databases",
        "Handson",
        "experience",
        "J2EE",
        "technologies",
        "Servlets",
        "JSP",
        "EJB",
        "JDBC",
        "Web",
        "Services",
        "providers",
        "consumers",
        "SOAP",
        "REST",
        "Agile",
        "Development",
        "Methodology",
        "Scrum",
        "development",
        "process",
        "Good",
        "Knowledge",
        "HTML",
        "CSS",
        "JavaScript",
        "web",
        "applications",
        "problem",
        "skills",
        "Ability",
        "concepts",
        "team",
        "player",
        "communication",
        "skills",
        "Work",
        "Experience",
        "Spark",
        "Scala",
        "Developer",
        "Intel",
        "Folsom",
        "CA",
        "June",
        "Present",
        "Responsibilities",
        "spark",
        "data",
        "ingestion",
        "framework",
        "data",
        "sources",
        "SMB",
        "FTP",
        "servers",
        "Web",
        "HDFS",
        "CDR",
        "ingestion",
        "framework",
        "metadata",
        "files",
        "data",
        "files",
        "ingestion",
        "CDR",
        "ingestion",
        "framework",
        "data",
        "consent",
        "metadata",
        "folder",
        "HDFS",
        "scope",
        "metadata",
        "collection",
        "Rest",
        "Client",
        "service",
        "PEGA",
        "platform",
        "Output",
        "ingestion",
        "json",
        "csv",
        "format",
        "copy",
        "metadata",
        "HBase",
        "records",
        "Kibana",
        "Developed",
        "Shell",
        "script",
        "code",
        "spark",
        "job",
        "file",
        "watcher",
        "shell",
        "script",
        "spark",
        "job",
        "data",
        "input",
        "path",
        "Developed",
        "Error",
        "mail",
        "code",
        "Scala",
        "mail",
        "group",
        "error",
        "json",
        "file",
        "logs",
        "client",
        "cluster",
        "mode",
        "Environment",
        "HDFS",
        "Spark",
        "Scala",
        "cloudera",
        "Intellij",
        "Hue",
        "Shell",
        "script",
        "HBase",
        "Kibana",
        "Spark",
        "Developer",
        "Charter",
        "Communications",
        "September",
        "May",
        "Responsibilities",
        "Apache",
        "spark",
        "jobs",
        "Scala",
        "test",
        "environment",
        "data",
        "processing",
        "spark",
        "SQL",
        "Developed",
        "Scala",
        "code",
        "monad",
        "pattern",
        "calculations",
        "requirement",
        "shell",
        "scripts",
        "jobs",
        "hive",
        "queries",
        "Azkaban",
        "calculations",
        "amounts",
        "data",
        "sets",
        "Pig",
        "scripts",
        "Hive",
        "scripts",
        "Hue",
        "manager",
        "hive",
        "queries",
        "data",
        "development",
        "production",
        "environments",
        "Developed",
        "Pig",
        "Latin",
        "scripts",
        "data",
        "Pig",
        "data",
        "loading",
        "filtering",
        "data",
        "Data",
        "Integration",
        "source",
        "systems",
        "Mongo",
        "DB",
        "data",
        "data",
        "Amazon",
        "kinesis",
        "streams",
        "Amazon",
        "s3",
        "topics",
        "Kafka",
        "kinesis",
        "streams",
        "Kafka",
        "data",
        "HDFS",
        "gobblin",
        "ScheduledAutomate",
        "jobs",
        "Azkaban",
        "Developed",
        "shell",
        "scripts",
        "jobs",
        "code",
        "fields",
        "mongo",
        "Jenkins",
        "automation",
        "code",
        "Complete",
        "design",
        "Apache",
        "NiFi",
        "AWS",
        "output",
        "HDFS",
        "Environment",
        "HDFS",
        "Hive",
        "Pig",
        "Spark",
        "RDD",
        "SparkSQL",
        "Kafka",
        "Spark",
        "Scala",
        "Python",
        "Horton",
        "Intellij",
        "Azkaban",
        "AmbariHue",
        "Jenkins",
        "Apache",
        "NiFi",
        "Agile",
        "Spark",
        "Developer",
        "T",
        "Rowe",
        "Price",
        "Owings",
        "Mills",
        "MD",
        "September",
        "August",
        "Responsibilities",
        "data",
        "sources",
        "HDFSHBase",
        "Spark",
        "RDD",
        "Importing",
        "data",
        "HDFS",
        "HIVE",
        "Sqoop",
        "requirements",
        "development",
        "PIG",
        "scripts",
        "source",
        "data",
        "validation",
        "transformation",
        "Designing",
        "tables",
        "HBase",
        "data",
        "Hive",
        "Developed",
        "Spark",
        "scripts",
        "Scala",
        "shell",
        "commands",
        "requirement",
        "Developed",
        "Spark",
        "core",
        "SparkSQL",
        "scripts",
        "Scala",
        "data",
        "processing",
        "code",
        "review",
        "bug",
        "performance",
        "workflows",
        "Apache",
        "Oozie",
        "framework",
        "tasks",
        "design",
        "documents",
        "approaches",
        "Partitioning",
        "Bucketing",
        "Hive",
        "organization",
        "data",
        "Hive",
        "queries",
        "performance",
        "SparkStreaming",
        "APIs",
        "transformations",
        "actions",
        "fly",
        "learner",
        "data",
        "model",
        "data",
        "Kafka",
        "time",
        "Persists",
        "Cassandra",
        "Responsible",
        "data",
        "pipeline",
        "Amazon",
        "AWS",
        "data",
        "weblogs",
        "HDFS",
        "Populated",
        "HDFS",
        "Cassandra",
        "data",
        "Apache",
        "Kafka",
        "Basic",
        "knowledge",
        "Machine",
        "Learning",
        "Predictive",
        "Analysis",
        "Data",
        "analysis",
        "Spark",
        "Scala",
        "Analyze",
        "data",
        "Tableau",
        "dashboards",
        "Tableau",
        "Environment",
        "HDFS",
        "Hive",
        "Pig",
        "Spark",
        "RDD",
        "Spark",
        "Streaming",
        "SparkSQL",
        "HBase",
        "Sqoop",
        "Oozie",
        "Kafka",
        "Cassandra",
        "Scala",
        "TableauAgile",
        "Hadoop",
        "Developer",
        "CareFirst",
        "Owings",
        "Mills",
        "MD",
        "January",
        "September",
        "Responsibilities",
        "data",
        "sources",
        "transformations",
        "MapReduce",
        "hive",
        "data",
        "HDFS",
        "compression",
        "mechanisms",
        "MapReduce",
        "Jobs",
        "Big",
        "Data",
        "Solutions",
        "business",
        "technology",
        "teams",
        "decisions",
        "ways",
        "customers",
        "business",
        "solutions",
        "scripts",
        "process",
        "Data",
        "Ingestion",
        "Performed",
        "group",
        "operations",
        "MapReduce",
        "Java",
        "PIG",
        "Configured",
        "Sqoop",
        "jobs",
        "data",
        "RDBMS",
        "HDFS",
        "Oozie",
        "workflows",
        "Pig",
        "Hive",
        "HBase",
        "nodes",
        "Pig",
        "Hive",
        "HBase",
        "MapReduce",
        "conversion",
        "MapReduce",
        "batch",
        "applications",
        "performance",
        "Created",
        "HBase",
        "data",
        "formats",
        "portfolios",
        "time",
        "analytics",
        "HBase",
        "Java",
        "API",
        "Rest",
        "API",
        "HBase",
        "Coprocessors",
        "Support",
        "team",
        "data",
        "HBase",
        "Tables",
        "compression",
        "mechanisms",
        "MapReduce",
        "Jobs",
        "customer",
        "behavior",
        "stream",
        "analysis",
        "data",
        "flume",
        "Avro",
        "Data",
        "files",
        "Avro",
        "Serialization",
        "system",
        "business",
        "logic",
        "UDFs",
        "Java",
        "UDFs",
        "Piggybanks",
        "sources",
        "monitoring",
        "Hadoop",
        "cluster",
        "Cloudera",
        "Manager",
        "data",
        "databases",
        "Sqoop",
        "visualization",
        "reports",
        "BI",
        "team",
        "Environment",
        "HDFS",
        "Hive",
        "MapReduce",
        "Pig",
        "Sqoop",
        "RDBMS",
        "HBase",
        "Java",
        "API",
        "REST",
        "API",
        "Cloudera",
        "AVRO",
        "Flume",
        "Hadoop",
        "Developer",
        "EOG",
        "Resources",
        "Houston",
        "TX",
        "June",
        "December",
        "Responsibilities",
        "Apache",
        "Hadoop",
        "maintenance",
        "log",
        "files",
        "Hadoop",
        "cluster",
        "Installed",
        "Hive",
        "Pig",
        "Sqoop",
        "Oozie",
        "Hadoop",
        "cluster",
        "Installed",
        "Oozie",
        "Workflow",
        "engine",
        "Hive",
        "Pig",
        "Jobs",
        "MapReduce",
        "jobs",
        "Java",
        "data",
        "cleansing",
        "Developed",
        "Simple",
        "MapReduce",
        "Jobs",
        "Hive",
        "Pig",
        "loading",
        "data",
        "UNIX",
        "file",
        "system",
        "business",
        "requirements",
        "specifications",
        "project",
        "guidelines",
        "programs",
        "amounts",
        "data",
        "sets",
        "way",
        "response",
        "client",
        "requests",
        "data",
        "ad",
        "data",
        "solutions",
        "Hadoop",
        "Migration",
        "ETL",
        "processes",
        "Oracle",
        "Hive",
        "data",
        "manipulation",
        "Performed",
        "optimization",
        "Pig",
        "scripts",
        "Hive",
        "queries",
        "efficiency",
        "features",
        "code",
        "PIG",
        "Latin",
        "scripts",
        "analysis",
        "data",
        "Developed",
        "Hive",
        "data",
        "Hive",
        "Hive",
        "tables",
        "data",
        "loading",
        "Hive",
        "UDFs",
        "Sqoop",
        "data",
        "HDFS",
        "Hive",
        "data",
        "systems",
        "Oozie",
        "workflow",
        "engine",
        "Hive",
        "monitoring",
        "Hadoop",
        "cluster",
        "Cloudera",
        "Manager",
        "unit",
        "development",
        "team",
        "sandbox",
        "environment",
        "Environment",
        "Hadoop",
        "Cluster",
        "Hive",
        "Pig",
        "Sqoop",
        "Oozie",
        "Oracle",
        "Cloudera",
        "Manager",
        "UNIX",
        "ETL",
        "Mongo",
        "DB",
        "Agile",
        "Hadoop",
        "Developer",
        "Merck",
        "Bluebell",
        "PA",
        "April",
        "May",
        "Responsibilities",
        "Hadoop",
        "MapReduce",
        "MapReduce",
        "jobs",
        "Java",
        "data",
        "understanding",
        "experience",
        "Hadoop",
        "stack",
        "internals",
        "Hive",
        "Pig",
        "MapReduce",
        "system",
        "Java",
        "Java",
        "filtering",
        "program",
        "business",
        "rule",
        "engine",
        "jar",
        "java",
        "Hadoop",
        "Wrote",
        "MapReduce",
        "jobs",
        "trends",
        "data",
        "usage",
        "users",
        "job",
        "flows",
        "Hadoop",
        "log",
        "Load",
        "sets",
        "data",
        "data",
        "sources",
        "Map",
        "Reduce",
        "Programs",
        "cluster",
        "loading",
        "data",
        "UNIX",
        "file",
        "system",
        "data",
        "sources",
        "Hive",
        "Hive",
        "UDFs",
        "functionality",
        "hive",
        "Hive",
        "tables",
        "data",
        "hive",
        "queries",
        "map",
        "way",
        "Partitioning",
        "Dynamic",
        "Partitions",
        "Buckets",
        "HIVE",
        "Monitor",
        "System",
        "health",
        "logs",
        "warning",
        "failure",
        "conditions",
        "Environment",
        "Apache",
        "Hadoop",
        "HDFS",
        "Map",
        "Reduce",
        "Pig",
        "Hive",
        "Hive",
        "UDFs",
        "UNIX",
        "Java",
        "ETL",
        "Eclipse",
        "Java",
        "Developer",
        "Gimmal",
        "New",
        "York",
        "NY",
        "December",
        "March",
        "Responsibilities",
        "application",
        "Agile",
        "Methodology",
        "Developed",
        "Maven",
        "project",
        "structure",
        "data",
        "layer",
        "ORM",
        "Web",
        "module",
        "Developed",
        "MVC",
        "framework",
        "website",
        "JSF",
        "spring",
        "Developed",
        "HTML",
        "pages",
        "JSP",
        "pages",
        "Business",
        "components",
        "spring",
        "framework",
        "database",
        "connections",
        "JDBC",
        "Responsible",
        "tables",
        "clients",
        "information",
        "Hibernate",
        "mapping",
        "files",
        "onetoone",
        "mapping",
        "relationships",
        "data",
        "saving",
        "modification",
        "procedures",
        "MySQL",
        "database",
        "Hibernate",
        "criteria",
        "Graphical",
        "User",
        "Interfaces",
        "JSF",
        "JSP",
        "HTML",
        "CSS",
        "JavaScript",
        "Installation",
        "configuration",
        "Development",
        "Environment",
        "Eclipse",
        "WebLogic",
        "Application",
        "server",
        "server",
        "side",
        "post",
        "access",
        "application",
        "result",
        "network",
        "RESTfulweb",
        "service",
        "XMLGateway",
        "ordering",
        "process",
        "system",
        "Order",
        "Execution",
        "Tool",
        "tools",
        "Line",
        "Qualification",
        "Billing",
        "Information",
        "Credit",
        "Card",
        "Validation",
        "Systems",
        "NodeJS",
        "web",
        "application",
        "Development",
        "TDD",
        "Approach",
        "environment",
        "methodologies",
        "JUnit",
        "application",
        "payment",
        "gateway",
        "PayPal",
        "Developed",
        "build",
        "script",
        "MAVEN",
        "package",
        "test",
        "application",
        "application",
        "server",
        "Auditing",
        "tool",
        "log4j",
        "Environment",
        "MVC",
        "HTML",
        "JSP",
        "XML",
        "Maven",
        "JavaScript",
        "Nodejs",
        "TDD",
        "Junit",
        "log4j",
        "JDBC",
        "MYSQL",
        "Hibernate",
        "WebLogic",
        "JSF",
        "Spring",
        "Eclipse",
        "Java",
        "Developer",
        "Metlife",
        "Bridgewater",
        "NJ",
        "October",
        "November",
        "Responsibilities",
        "Use",
        "Case",
        "Diagrams",
        "Class",
        "Model",
        "Sequence",
        "diagrams",
        "SDLC",
        "process",
        "application",
        "GUI",
        "pages",
        "JavaScript",
        "HTML",
        "JSP",
        "CSS",
        "AJAX",
        "UI",
        "components",
        "JSP",
        "JMS",
        "JSTL",
        "Deployed",
        "project",
        "Web",
        "Sphere",
        "application",
        "server",
        "Linux",
        "environment",
        "application",
        "Web",
        "Services",
        "SOAP",
        "JSP",
        "Servlets",
        "JDBC",
        "Core",
        "Java",
        "SingletonDAO",
        "Design",
        "Patterns",
        "factory",
        "design",
        "pattern",
        "application",
        "requirements",
        "DOM",
        "SAX",
        "parsers",
        "XML",
        "documents",
        "web",
        "services",
        "SOAP",
        "UI",
        "tool",
        "end",
        "interfaces",
        "PLSQL",
        "packages",
        "procedures",
        "Functions",
        "Procedure",
        "Anonymous",
        "PLSQL",
        "Cursor",
        "management",
        "Exception",
        "Handling",
        "PLSQL",
        "programs",
        "database",
        "queries",
        "tables",
        "performance",
        "application",
        "Eclipse",
        "Development",
        "IDE",
        "web",
        "applications",
        "Environment",
        "JDBC",
        "HTML",
        "CSS",
        "JSP",
        "AJAX",
        "XML",
        "SOAP",
        "DOM",
        "SAX",
        "PLSQL",
        "Eclipse",
        "SOAP",
        "Servlets",
        "Java",
        "Developer",
        "HLine",
        "Soft",
        "Hyderabad",
        "Telangana",
        "June",
        "September",
        "Responsibilities",
        "SDLC",
        "software",
        "development",
        "life",
        "cycle",
        "application",
        "requirement",
        "gathering",
        "analysis",
        "testing",
        "maintenance",
        "business",
        "community",
        "business",
        "requirements",
        "solutions",
        "Requirement",
        "Business",
        "Process",
        "Business",
        "Process",
        "Modeling",
        "Business",
        "Analysis",
        "User",
        "Login",
        "logic",
        "Spring",
        "MVC",
        "framework",
        "application",
        "architectures",
        "Model",
        "View",
        "Controller",
        "design",
        "paradigm",
        "Java",
        "J2EE",
        "APIs",
        "JDBC",
        "XML",
        "Servlets",
        "JSP",
        "Generated",
        "Hibernate",
        "Mapping",
        "files",
        "data",
        "model",
        "mapping",
        "files",
        "UI",
        "JavaScript",
        "JSP",
        "HTML",
        "CSS",
        "cross",
        "browser",
        "functionality",
        "user",
        "interface",
        "action",
        "classes",
        "beans",
        "strutsconfigxml",
        "validations",
        "Struts",
        "Validator",
        "framework",
        "JavaScript",
        "business",
        "logic",
        "servlets",
        "session",
        "beans",
        "Apache",
        "Tomcat",
        "server",
        "SQL",
        "Queries",
        "PLSQL",
        "procedures",
        "functions",
        "end",
        "design",
        "test",
        "case",
        "specifications",
        "Performed",
        "unit",
        "testing",
        "system",
        "testing",
        "integration",
        "testing",
        "JUnit",
        "unit",
        "testing",
        "application",
        "support",
        "production",
        "environments",
        "issues",
        "defects",
        "solution",
        "defects",
        "priority",
        "defects",
        "schedule",
        "Environment",
        "SDLC",
        "Spring",
        "MVC",
        "JSP",
        "Servlets",
        "JavaScript",
        "SQL",
        "HTML",
        "CSS",
        "PLSQL",
        "Hibernate",
        "Junit",
        "Education",
        "Bachelors",
        "Computer",
        "Science",
        "Computer",
        "Science",
        "JNTU",
        "Skills",
        "HDFS",
        "years",
        "Hive",
        "years",
        "Pig",
        "years",
        "years",
        "APACHE",
        "years",
        "Drupal",
        "Servicenow",
        "Hadoop",
        "AEM",
        "Cobol",
        "Additional",
        "Information",
        "TECHNICAL",
        "SKILLS",
        "HadoopBig",
        "Data",
        "Hadoop",
        "MapReduce",
        "HDFS",
        "Hive",
        "Pig",
        "Sqoop",
        "Flume",
        "Oozie",
        "Zookeeper",
        "Spark",
        "Kafka",
        "Languages",
        "Web",
        "Technologies",
        "C",
        "C",
        "C",
        "Scala",
        "XML",
        "HTML",
        "CSS",
        "JavaScript",
        "J2EE",
        "Java",
        "Python",
        "JSP",
        "Frameworks",
        "Spring",
        "Hibernate",
        "Servlets",
        "Web",
        "Services",
        "REST",
        "SOAP",
        "MY",
        "SQL",
        "Oracle",
        "DB2",
        "MongoDB",
        "SparkSQL",
        "HBase",
        "Tools",
        "Tableau",
        "Weka",
        "Application",
        "servers",
        "Apache",
        "Tomcat",
        "WebLogic",
        "IDEModelling",
        "Tools",
        "Eclipse",
        "Intellij",
        "Development",
        "Methodologies",
        "Agile",
        "Waterfall",
        "Logging",
        "tools",
        "Log4j",
        "Operating",
        "Systems",
        "Windows",
        "LINUX"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T21:06:39.308312",
    "resume_data": "Spark Scala Developer Spark Scala span lDeveloperspan Spark Scala Developer Intel Folsom CA Around 10 years of experience in Design Analysis and Development of software application using Big Data Hadoop Spark and JavaJEE Technologies Knowledge in Spark Core SparkSQLSpark Streaming and machine learning using Scala and Python Programming languages Worked on Open Source Apache Hadoop Cloudera Enterprise CDH and Hortonworks Data Platform HDP Hands on experience on major components in Hadoop Ecosystem like Hadoop Map Reduce HDFS HIVE PIG Pentaho HBase Zookeeper Task Tracker Name Node Data NodeSqoop Oozie Cassandra Flume and Avro Developed various Map Reduce applications to perform ETL workloads on terabytes of data Expertise in working with HIVE data warehouse infrastructurecreating tables data distribution by implementing partitioning and bucketing writing and optimizing the HQL queries Used Spark Streaming to divide streaming data into batches as an input to Spark engine for batch processing Exploring with the Spark improving the performance and optimization of the existing algorithms in Hadoop Experience in working with flume to load the log data from multiple sources directly into HDFS Responsible to manage data coming from different sources and involved in HDFS maintenance and loading of structured and unstructured data Good understanding of RDD operations in Apache Spark like TransformationsActions Persistence Caching Accumulators Broadcast Variables Optimising Broadcasts Hands on experience in performing aggregations on data using Hive Query Language HQL Developed MapReduce programs in java Good experience in extending the core functionality of Hive and Pig by developing userdefined functions to provide custom capabilities to these languages Proficient in designing and querying the NoSQL databases like HBase and MongoDB Performed Importing and exporting data into HDFS and Hive using Sqoop Experience in scheduling time driven and data driven Oozie workflows Hands on experience in working with input file formats like parquet json Avro Knowledge on Elastic Search Concepts knowledge on visualizing the NOSQL data from elastic search using Kibana Worked on Extraction Transformation and Loading ETL of data from multiple sources like Flat files XML filesand Databases Handson experience in J2EE technologies such as Servlets JSP EJB JDBC and developing Web Services providers and consumers using SOAP REST Used Agile Development Methodology and Scrum for the development process Good Knowledge in HTML CSS JavaScript and web based applications Excellent analytical problem solving and interpersonal skills Ability to learn new concepts fast consistent team player with excellent communication skills Work Experience Spark Scala Developer Intel Folsom CA June 2018 to Present Responsibilities Developed a spark data ingestion framework to ingest data from various sources like SMB servers FTP servers and Web to HDFS CDR ingestion framework cleanses the metadata files related to actual data files before proceeding for the ingestion CDR ingestion framework will only ingest the data based on the consent from the metadata to a appropriate folder in HDFS Current scope of metadata collection using Rest Client by consuming the service which is created at PEGA platform Worked on Storing the Output of ingestion json in csv format Worked on storing the copy of metadata to HBase Worked on updating the records to Kibana Developed Shell script code to trigger the spark job Developed file watcher using shell script to perform the spark job as the data gets in the input path Developed Error mail code in Scala which triggers mail to specified group when error occurs in processing the json file Created logs in client and cluster mode Environment HDFS Spark Scala cloudera Intellij Hue Shell script HBase Kibana Spark Developer Charter Communications September 2017 to May 2018 Responsibilities Developed Apache spark jobs using Scala in test environment for faster data processing and used spark SQL for querying Developed Scala code using specific monad pattern for different calculations based on the requirement Developed and executed shell scripts to automate the jobs Written complex hive queries and automated using Azkaban for analyzing hourly calculations Analyzed large amounts of data sets using Pig scripts and Hive scripts Worked with Hue manager for developing hive queries and checking data in both development and production environments Developed Pig Latin scripts for extracting data Used Pig for data loading filtering and storing the data Worked on Data Integration from different source systems Used Mongo DB for storing the data Worked on retrieving data from Amazon kinesis streams and Amazon s3 Worked on creating topics in Kafka and integrating kinesis streams to Kafka and storing the data in the HDFS using gobblin ScheduledAutomate jobs using Azkaban Developed and executed shell scripts to automate the jobs Developed python code for creating fields in mongo Used Jenkins for continuous automation of code Complete endtoend design of Apache NiFi to get connected to AWS and store the final output in HDFS Environment HDFS Hive Pig Spark RDD SparkSQL Kafka Spark Scala Python MongoDB Horton works Intellij Azkaban AmbariHue Jenkins Apache NiFi Agile Spark Developer T Rowe Price Owings Mills MD September 2016 to August 2017 Responsibilities Import the data from different sources like HDFSHBase into Spark RDD Importing and exporting data into HDFS and HIVE using Sqoop Involved in gathering the requirements designing development and testing Developed PIG scripts for source data validation and transformation Designing and developing tables in HBase and storing aggregating data from Hive Developed Spark scripts by using Scala shell commands as per the requirement Developed Spark core and SparkSQL scripts using Scala for faster data processing Involved in code review and bug fixing for improving the performance Implemented the workflows using Apache Oozie framework to automate tasks Developing design documents considering all possible approaches and identifying best of them Implemented Partitioning Bucketing in Hive for better organization of the data Optimized Hive queries for performance tuning Used SparkStreaming APIs to perform necessary transformations and actions on the fly for building the common learner data model which gets the data from Kafka in near real time and Persists into Cassandra Responsible for developing data pipeline with Amazon AWS to extract the data from weblogs and store in HDFS Populated HDFS and Cassandra with huge data using Apache Kafka Basic knowledge on Machine Learning and Predictive Analysis Data analysis using Spark with Scala Analyze and report the data using Tableau Create dashboards in Tableau Environment HDFS Hive Pig Spark RDD Spark Streaming SparkSQL HBase Sqoop Oozie Kafka Cassandra Scala TableauAgile Hadoop Developer CareFirst Owings Mills MD January 2015 to September 2016 Responsibilities Worked on importing data from various sources and performed transformations using MapReduce hive to load data into HDFS Worked on compression mechanisms to optimize MapReduce Jobs Developed Big Data Solutions that enabled the business and technology teams to make datadriven decisions on the best ways to acquire customers and provide them business solutions Created scripts to automate the process of Data Ingestion Performed joins group by and other operations in MapReduce by using Java and PIG Configured Sqoop jobs to import data from RDBMS into HDFS using Oozie workflows Worked on setting up Pig Hive and HBase on multiple nodes and developed using Pig Hive HBase and MapReduce Worked on the conversion of existing MapReduce batch applications for better performance Created HBase tables to store variable data formats coming from different portfolios Performed real time analytics on HBase using Java API and Rest API Implemented HBase Coprocessors to notify Support team when inserting data into HBase Tables Worked on compression mechanisms to optimize MapReduce Jobs Analyzed the customer behavior by performing click stream analysis and to ingest the data used flume Experienced with working on Avro Data files using Avro Serialization system Implemented business logic by writing UDFs in Java and used various UDFs from Piggybanks and other sources Continuous monitoring and managing the Hadoop cluster using Cloudera Manager Exported the analyzed data to the relational databases using Sqoop for visualization and to generate reports for the BI team Environment HDFS Hive MapReduce Pig Sqoop RDBMS HBase Java API REST API Cloudera AVRO Flume Hadoop Developer EOG Resources Houston TX June 2013 to December 2014 Responsibilities Installed and configured Apache Hadoop to test the maintenance of log files in Hadoop cluster Installed and configured Hive Pig Sqoop and Oozie on the Hadoop cluster Installed Oozie Workflow engine to run multiple Hive and Pig Jobs Developed multiple MapReduce jobs in Java for data cleansing and preprocessing Developed Simple to complex MapReduce Jobs using Hive and Pig Involved in loading data from UNIX file system to HDFS Evaluated business requirements and prepared detailed specifications that follow project guidelines required to develop written programs Analyzed large amounts of data sets to determine optimal way to aggregate and report on it Provided quick response to ad hoc internal and external client requests for data and experienced in creating ad hoc reports Responsible for building scalable distributed data solutions using Hadoop Migration of ETL processes from Oracle to Hive to test the easy data manipulation Performed optimization on Pig scripts and Hive queries increase efficiency and add new features to existing code Developed PIG Latin scripts for the analysis of semi structured data Developed Hive queries to process the data Used Hive and created Hive tables and involved in data loading and writing Hive UDFs Used Sqoop to import data into HDFS and Hive from other data systems Installed Oozie workflow engine to run multiple Hive Continuous monitoring and managing the Hadoop cluster using Cloudera Manager Conducted some unit testing for the development team within the sandbox environment Environment Hadoop Cluster Hive Pig Sqoop Oozie Oracle Cloudera Manager UNIX ETL Mongo DB Agile Hadoop Developer Merck Bluebell PA April 2012 to May 2013 Responsibilities Installed and configured Hadoop MapReduce HDFS developed multiple MapReduce jobs in Java for data cleaning and preprocessing Good understanding and related experience with Hadoop stack internals Hive Pig and MapReduce The system was initially developed using Java The Java filtering program was restructured to have business rule engine in a jar that can be called from both java and Hadoop Wrote MapReduce jobs to discover trends in data usage by users Involved in defining job flows Involved in managing and reviewing Hadoop log files Load and transform large sets of structured semi structured and unstructured data Responsible to manage data coming from different sources Supported Map Reduce Programs those are running on the cluster Involved in loading data from UNIX file system to HDFS Responsible to manage data coming from different sources Installed and configured Hive and developed Hive UDFs to extend core functionality of hive Involved in creating Hive tables loading with data and writing hive queries which will run internally in map reduce way Implemented Partitioning Dynamic Partitions Buckets in HIVE Monitor System health and logs and respond accordingly to any warning or failure conditions Environment Apache Hadoop HDFS Map Reduce Pig Hive tables Hive UDFs UNIX Java ETL Eclipse Java Developer Gimmal New York NY December 2010 to March 2012 Responsibilities Designed the application using Agile Methodology Developed Maven based project structure having data layer ORM and Web module Developed MVC framework based website using JSF and spring Designed and Developed HTML pages and JSP pages Developed Business components using spring framework and database connections using JDBC Responsible for creating tables of clients information in and writing Hibernate mapping files to manage onetoone and onetomany mapping relationships Implemented data reading saving and modification by stored procedures in MySQL database and Hibernate criteria Developed Graphical User Interfaces by using JSF JSP HTML CSS and JavaScript Installation and configuration of Development Environment using Eclipse with WebLogic Application server On the server side post the access to the application and provided result on the network using RESTfulweb service Developed the XMLGateway to help the ordering process system communicate with the Order Execution Tool and different online tools such as Line Qualification Billing Information and Credit Card Validation Systems Used NodeJS to develop scalable web application Development TDD Approach environment using Agile methodologies Used JUnit to test debugged and implemented the application Implemented payment gateway using PayPal Developed build script using MAVEN to build package test and deploy application in application server Auditing tool is implemented by using log4j Environment MVC HTML JSP XML Maven JavaScript Nodejs TDD Junit log4j JDBC MYSQL Hibernate WebLogic JSF Spring Eclipse Java Developer Metlife Bridgewater NJ October 2009 to November 2010 Responsibilities Designing the Use Case Diagrams Class Model Sequence diagrams for SDLC process of the application Implemented GUI pages by using JavaScript HTML JSP CSS and AJAX Designed and developed UI components using JSP JMS JSTL Deployed project on Web Sphere application server in Linux environment Implemented the online application by using Web Services SOAP JSP Servlets JDBC and Core Java Implemented SingletonDAO Design Patterns factory design pattern based on the application requirements Used DOM and SAX parsers to parse the raw XML documents Tested the web services with SOAP UI tool Developed back end interfaces using PLSQL packages stored procedures Functions Procedure Anonymous PLSQL programs Cursor management Exception Handling in PLSQL programs Tuning complex database queries and joining the tables to improve the performance of the application Used Eclipse as Development IDE for web applications Environment JDBC HTML CSS JSP AJAX XML SOAP DOM SAX PLSQL Eclipse SOAP Servlets Java Developer HLine Soft Hyderabad Telangana June 2008 to September 2009 Responsibilities Involved in the complete SDLC software development life cycle of the application from requirement gathering and analysis to testing and maintenance Worked with the business community to define business requirements and analyze the possible technical solutions Requirement gathering Business Process flow Business Process Modeling and Business Analysis Implemented the User Login logic using Spring MVC framework encouraging application architectures based on the Model View Controller design paradigm Used various Java J2EE APIs including JDBC XML Servlets and JSP Generated Hibernate Mapping files and created the data model using mapping files Developed UI using JavaScript JSP HTML and CSS for interactive cross browser functionality and complex user interface Developed action classes and form beans and configured the strutsconfigxml Provided clientside validations using Struts Validator framework and JavaScript Created business logic using servlets and session beans and deployed them on Apache Tomcat server Created complex SQL Queries PLSQL Stored procedures and functions for back end Prepared the functional design and test case specifications Performed unit testing system testing and integration testing Used JUnit for unit testing of the application Provided Technical support for production environments resolving the issues analyzing the defects providing and implementing the solution defects Resolved more priority defects as per the schedule Environment SDLC Spring MVC JSP Servlets JavaScript SQL HTML CSS PLSQL Hibernate Junit Education Bachelors in Computer Science in Computer Science JNTU 2008 Skills HDFS 6 years Hive 6 years Pig 6 years JAVA 8 years APACHE 5 years Drupal Servicenow Hadoop AEM Cobol Additional Information TECHNICAL SKILLS HadoopBig Data Hadoop MapReduce HDFS Hive Pig Sqoop Flume Oozie Zookeeper Spark Kafka Languages and Web Technologies C C C Scala XML HTML CSS JavaScript J2EE Java Python JSP Frameworks Spring struts Hibernate Servlets Web Services REST SOAP Databases MY SQL Oracle DB2 MongoDB SparkSQL HBase Tools Tableau Weka Application servers Apache Tomcat WebLogic 80 IDEModelling Tools Eclipse Intellij Development Methodologies Agile Waterfall Logging tools Log4j Operating Systems Windows 7810 LINUX",
    "unique_id": "65c72e7b-73d3-4d58-bb4c-b7a0255b91b2"
}