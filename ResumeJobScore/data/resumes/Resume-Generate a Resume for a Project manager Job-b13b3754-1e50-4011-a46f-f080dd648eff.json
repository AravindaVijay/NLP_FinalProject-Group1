{
    "clean_data": "Hadoop Developer Hadoop span lDeveloperspan Hadoop Developer Overall 5 years of IT experience result oriented Hadoop Developer possessing a proven track record of effectively administering Hadoop ecosystem components architecture and managing file distribution systems in the Big Data arena Proficient in collaborating with key stakeholders to conceptualize execute solutions for resolving systems architecturebased technical issues Highly skilled in processing complex data designing Machine Learning modules for effective data mining modeling Adept at Hadoop cluster management capacity planning for endtoend data management performance optimization Hadoop Developer with work experience in using HDFS MapReduce Hive Pig Spark Sqoop Oozie Kafka zookeeper and HBase Experienced working with various Hadoop Distributions Cloudera Hortonworks Map R Amazon EMR Microsoft Azure HDInsight to fully implement and leverage new Hadoop features Proficient in using Unix based Command Line Interface Experience in moving data into and out of the HDFS and Relational Database Systems RDBMS using Apache Sqoop Expertise in working with Hive data warehouse infrastructurecreating tables data distribution by implementing Partitioning and Bucketing developing and tuning the HQL queries Involved in creating Hive tables loading with data and writing Hive Adhoc queries that will run internally in MapReduce and Spark Significant experience writing custom UDFs in Hive and custom Input Formats in MapReduce Experience in managing and reviewing Hadoop log files Knowledge of job work flow management and monitoring tools like Oozie and zookeeper Experience working with NoSQL database technologies including MongoDB Cassandra and HBase Strong experience building end to end data pipelines on Hadoop platform Experience in developing Spark Applications using Spark RDD Spark SQL and Data frame APIs Replaced existing MR jobs and Hive scripts with Spark SQL Spark data transformations for efficient data processing Deep knowledge of troubleshooting and tuning Spark applications and Hive scripts to achieve optimal performance Strong understanding of real time streaming technologies Spark and Kafka Worked with realtime data processing and streaming techniques using Spark streaming and Kafka Good understanding on Machine Learning Methodologies to uncover the hidden patterns user behaviour and modeling using Spark MLlib Java Developer with a year of diversified experience in analysis design implementation integration testing and maintenance of applications using JavaJ2EE and ObjectOriented ClientServer technologies Strong understanding of Java Virtual Machines and multithreading process Expert in Core Java with strong understanding of Collections Multithreading Event handling Exception handling and Generics Strong experience with Software methodologies like Agile Scrum Waterfall and Test Drive Development Significant experience in UI frame works such as JSP HTML5 XML CSS3 JavaScript Angular JS JQuery Extensive experience in developing web applications using Java JEE Spring Servlets Hibernate JDBC Eclipse Handson experience working with Continuous Integration CI buildautomation tools such as Maven SVN CVS Jenkins and Apache Ant Experienced in generating logging by Log4j to identify the errors in production test environment and experienced in Ant and Maven tools Database design modeling migration and development experience in using stored procedures triggers cursor constraints and functions Used My SQL MS SQL Server DB2 and Oracle Effective communication and interpersonal Skills an excellent team player work towards the growth of an organization Work Experience Hadoop Developer Express Scripts Franklin Lakes NJ August 2015 to May 2017 Description Express Scripts is one of the most significant pharmacy benefit management PBM organization in the United States produces an incredible amount of data Using big data to generate actionable insights helps to improve healthcare by making it more accessible affordable and effective The scope of the project was to address the opioid epidemic through proactive intervention by advanced analytics using big data avoiding downstream costs and adverse health events using machine learning algorithms and predicting personalized risk to influence better health using advanced analytics and artificial intelligence provides the cuttingedge capability to determine the health risk of individuals Responsibilities Utilized Sqoop Kafka Flume and Hadoop File System APIs for implementing data ingestion pipelines Worked on real time streaming performed transformations on the data using Kafka and Spark Streaming Created storage with Amazon S3 for storing data Worked on transferring data from Kafka topic into AWS S3 storage Created Hive tables loaded with data and wrote Hive queries to process the data Created Partitions and used Bucketing on Hive tables and used required parameters to improve performance and developed Hive UDFs as per business usecases Developed Hive scripts for source data validation and transformation Automated data loading into HDFS and Hive for preprocessing the data using Oozie Designed and implemented an ETL framework using Java and Pig to load data from multiple sources into Hive and from Hive into Vertica Collaborated in data modeling data mining Machine Learning methodologies advanced data processing ETL optimization Worked on various data formats like Avro Sequence File JSON Map File Parquet and XML Worked extensively on AWS Components such as Airflow Elastic Map Reduce EMR Athena Snowflake Used Apache NiFi to automate data movement between different Hadoop components Used NiFi to perform conversion of raw XML data into JSON Avro Experienced in working with Hadoop from Cloudera Data Platform and running services through Cloudera manager Assisted in Hadoop administration and support activities for installations and configuring Apache Big Data Tools and Hadoop clusters using Cloudera Manager Experienced in Hadoop Production support tasks by analysing the Application and cluster logs Used Agile Scrum methodology Scrum Alliance for development Environment Hadoop HDFS AWS Vertica Scala Kafka MapReduce YARN Drill Spark Pig Hive Scala Java NiFi HBase MySQL Kerberos Maven Java Developer Hadoop Hyderabad Telangana June 2014 to June 2015 Indus Group Hyderabad India Description Indus Group is a technology consulting firm founded by proficient performers in the technical solutions with a mission to provide ontime onbudget and quality service to the clients across globe and consistently meet their expectations As a consulting partner Indus group also suggest best practices appropriate frameworks and optimal solutions to the clients The project is responsible for design development management of javabased application and had deep insight about using Big Data technologies Responsibilities Standardized practices for data acquisition analysis to deliver new products using Big Data technologies Directed data streaming in Kafka deployed Scrum methodologies for data management and analytics in Jira Involved in complete project Life Cycle ie Design Implementation Unit Testing Extensively used agile development methodology and involved in sprint planning Designed and modified User Interfaces using JSP JavaScript HTML5 Angular JS and jQuery with the help of several design patterns like Singleton Factory and MVC Involved in migrating legacy projects to latest versions of spring and hibernate Used DAO to handle connection and to retrieve data from data storage elements Written Microservices to exportimport data and task scheduling using Spring Boot Spring and Hibernate Also Used Swagger API tools while developing the microservices Implemented Hibernate to persist the data into Database and wrote HQL based queries to implement CRUD operations on the data Maintained Sessions using Spring MVC session management tools Annotated POJOs are created using Hibernate annotations Familiarized with Named Queries and Parameterized Queries in Hibernate Also Worked on SQL PLSQL using SQL Developer for Oracle database Involved in deploying the application under Apache Tomcat and maintained application logs Using Log4j Involved in unit testing using JUnit Used MAVEN to define the dependencies plugin and build the application Used SVN version Control tools Used Jenkins for deploying the application to test and production environments Designed and Developed Web services using SOAP to make submissions Created and maintained various Message Queues and Message brokers that were a part of the application JMS is used extensively in the application for sending budget related alerts through SMS email etc Environment Hadoop Kafka Java 7 Spring 4 Spring MVC Spring AOP Spring Data JPA Hibernate 3 SQL Microservices Spring Boot RESTful web services JSON JUnit 4 SVN Java script Log4J Jenkins Tomcat Jira Education Masters Skills APACHE KAFKA 2 years JAVA 2 years JSON 2 years KAFKA 2 years APACHE HADOOP HDFS 1 year Additional Information TECHNICAL SKILLS Operating Systems Linux Ubuntu CentOS Windows Mac OS Hadoop Ecosystem Hadoop MapReduce Yarn HDFS Pig Oozie Zookeeper Big Data Ecosystem Spark Spark SQL Spark Streaming Spark MLlib Hive Impala Hue Data Ingestion Sqoop Flume NiFi Kafka NOSQL Databases HBase Cassandra MongoDB CouchDB Programming Languages C C Scala Core Java J2EE Frameworks Model View Controller MVC Spring Hibernate Web Technologies Spring 4 Spring MVC Hibernate 3 JSP JavaScript AngularJS HTML 5 CSS 3 AJAX JQuery XML XSD WSDL JSON Web services Scripting Languages Java Script UNIX Python R Language Databases Oracle 10g11g PostgreSQL 93 MySQL SQLServer Teradata IDE IntelliJ Eclipse Visual Studio IDLE Web Services Restful SOAP Tools Ant Tortoise SVN Putty Win SCP Maven log4j JUnit SOAPUI Git Jasper reports Jenkins Tableau Mahout Methodologies SDLC Agile Scrum Iterative Development Waterfall Model image1emf",
    "entities": [
        "ObjectOriented ClientServer",
        "Additional Information TECHNICAL SKILLS Operating Systems",
        "HDFS",
        "Hadoop Developer Hadoop",
        "Jira Involved",
        "User Interfaces",
        "NiFi",
        "Maintained Sessions",
        "Continuous Integration CI",
        "Hadoop",
        "Apache Sqoop Expertise",
        "XML",
        "SOAP",
        "JUnit",
        "Life Cycle ie Design Implementation Unit Testing",
        "Automated",
        "Avro Sequence File JSON Map File Parquet",
        "Named Queries and Parameterized Queries in Hibernate Also Worked",
        "Amazon S3",
        "JavaJ2EE",
        "Amazon",
        "Hadoop Production",
        "Command Line Interface Experience",
        "Machine Learning Methodologies",
        "Spark SQL Spark",
        "Spark MLlib Java Developer",
        "Skills",
        "Spark Streaming Created",
        "Written Microservices",
        "Java JEE Spring",
        "Oracle Effective",
        "Spark Applications",
        "MVC Involved",
        "AJAX JQuery XML XSD",
        "Exception",
        "Hadoop File System",
        "Developed Hive",
        "SQL Developer for Oracle database Involved",
        "AWS Components",
        "Hadoop Developer",
        "JSP",
        "Created Partitions",
        "MVC",
        "Cloudera Data Platform",
        "Spark",
        "Java Developer Hadoop Hyderabad",
        "Created Hive",
        "MVC Hibernate",
        "Description Indus Group",
        "Database",
        "Adept",
        "Indus",
        "Created",
        "Apache Big Data Tools",
        "Windows Mac",
        "Annotated",
        "Implemented Hibernate",
        "Oozie Designed",
        "Spark RDD Spark",
        "Responsibilities Utilized Sqoop",
        "Java Virtual Machines",
        "SQL",
        "Log4j",
        "MAVEN",
        "Description Express Scripts",
        "SMS",
        "Relational Database Systems",
        "HDInsight",
        "Ant",
        "AOP Spring Data",
        "MapReduce Experience",
        "the United States",
        "Big Data",
        "Hive",
        "Bucketing on Hive",
        "Jenkins Tableau Mahout Methodologies",
        "JSON Avro Experienced",
        "Directed",
        "ETL",
        "Singleton Factory",
        "CRUD",
        "Maven",
        "UI",
        "HBase Experienced",
        "Work Experience Hadoop Developer Express",
        "Indus Group Hyderabad",
        "SVN",
        "Vertica Collaborated",
        "HDFS MapReduce Hive Pig Spark",
        "Control tools Used Jenkins",
        "jQuery",
        "Data",
        "MapReduce",
        "NoSQL",
        "Machine Learning",
        "Application",
        "KAFKA",
        "Responsibilities Standardized"
    ],
    "experience": "Experience in moving data into and out of the HDFS and Relational Database Systems RDBMS using Apache Sqoop Expertise in working with Hive data warehouse infrastructurecreating tables data distribution by implementing Partitioning and Bucketing developing and tuning the HQL queries Involved in creating Hive tables loading with data and writing Hive Adhoc queries that will run internally in MapReduce and Spark Significant experience writing custom UDFs in Hive and custom Input Formats in MapReduce Experience in managing and reviewing Hadoop log files Knowledge of job work flow management and monitoring tools like Oozie and zookeeper Experience working with NoSQL database technologies including MongoDB Cassandra and HBase Strong experience building end to end data pipelines on Hadoop platform Experience in developing Spark Applications using Spark RDD Spark SQL and Data frame APIs Replaced existing MR jobs and Hive scripts with Spark SQL Spark data transformations for efficient data processing Deep knowledge of troubleshooting and tuning Spark applications and Hive scripts to achieve optimal performance Strong understanding of real time streaming technologies Spark and Kafka Worked with realtime data processing and streaming techniques using Spark streaming and Kafka Good understanding on Machine Learning Methodologies to uncover the hidden patterns user behaviour and modeling using Spark MLlib Java Developer with a year of diversified experience in analysis design implementation integration testing and maintenance of applications using JavaJ2EE and ObjectOriented ClientServer technologies Strong understanding of Java Virtual Machines and multithreading process Expert in Core Java with strong understanding of Collections Multithreading Event handling Exception handling and Generics Strong experience with Software methodologies like Agile Scrum Waterfall and Test Drive Development Significant experience in UI frame works such as JSP HTML5 XML CSS3 JavaScript Angular JS JQuery Extensive experience in developing web applications using Java JEE Spring Servlets Hibernate JDBC Eclipse Handson experience working with Continuous Integration CI buildautomation tools such as Maven SVN CVS Jenkins and Apache Ant Experienced in generating logging by Log4j to identify the errors in production test environment and experienced in Ant and Maven tools Database design modeling migration and development experience in using stored procedures triggers cursor constraints and functions Used My SQL MS SQL Server DB2 and Oracle Effective communication and interpersonal Skills an excellent team player work towards the growth of an organization Work Experience Hadoop Developer Express Scripts Franklin Lakes NJ August 2015 to May 2017 Description Express Scripts is one of the most significant pharmacy benefit management PBM organization in the United States produces an incredible amount of data Using big data to generate actionable insights helps to improve healthcare by making it more accessible affordable and effective The scope of the project was to address the opioid epidemic through proactive intervention by advanced analytics using big data avoiding downstream costs and adverse health events using machine learning algorithms and predicting personalized risk to influence better health using advanced analytics and artificial intelligence provides the cuttingedge capability to determine the health risk of individuals Responsibilities Utilized Sqoop Kafka Flume and Hadoop File System APIs for implementing data ingestion pipelines Worked on real time streaming performed transformations on the data using Kafka and Spark Streaming Created storage with Amazon S3 for storing data Worked on transferring data from Kafka topic into AWS S3 storage Created Hive tables loaded with data and wrote Hive queries to process the data Created Partitions and used Bucketing on Hive tables and used required parameters to improve performance and developed Hive UDFs as per business usecases Developed Hive scripts for source data validation and transformation Automated data loading into HDFS and Hive for preprocessing the data using Oozie Designed and implemented an ETL framework using Java and Pig to load data from multiple sources into Hive and from Hive into Vertica Collaborated in data modeling data mining Machine Learning methodologies advanced data processing ETL optimization Worked on various data formats like Avro Sequence File JSON Map File Parquet and XML Worked extensively on AWS Components such as Airflow Elastic Map Reduce EMR Athena Snowflake Used Apache NiFi to automate data movement between different Hadoop components Used NiFi to perform conversion of raw XML data into JSON Avro Experienced in working with Hadoop from Cloudera Data Platform and running services through Cloudera manager Assisted in Hadoop administration and support activities for installations and configuring Apache Big Data Tools and Hadoop clusters using Cloudera Manager Experienced in Hadoop Production support tasks by analysing the Application and cluster logs Used Agile Scrum methodology Scrum Alliance for development Environment Hadoop HDFS AWS Vertica Scala Kafka MapReduce YARN Drill Spark Pig Hive Scala Java NiFi HBase MySQL Kerberos Maven Java Developer Hadoop Hyderabad Telangana June 2014 to June 2015 Indus Group Hyderabad India Description Indus Group is a technology consulting firm founded by proficient performers in the technical solutions with a mission to provide ontime onbudget and quality service to the clients across globe and consistently meet their expectations As a consulting partner Indus group also suggest best practices appropriate frameworks and optimal solutions to the clients The project is responsible for design development management of javabased application and had deep insight about using Big Data technologies Responsibilities Standardized practices for data acquisition analysis to deliver new products using Big Data technologies Directed data streaming in Kafka deployed Scrum methodologies for data management and analytics in Jira Involved in complete project Life Cycle ie Design Implementation Unit Testing Extensively used agile development methodology and involved in sprint planning Designed and modified User Interfaces using JSP JavaScript HTML5 Angular JS and jQuery with the help of several design patterns like Singleton Factory and MVC Involved in migrating legacy projects to latest versions of spring and hibernate Used DAO to handle connection and to retrieve data from data storage elements Written Microservices to exportimport data and task scheduling using Spring Boot Spring and Hibernate Also Used Swagger API tools while developing the microservices Implemented Hibernate to persist the data into Database and wrote HQL based queries to implement CRUD operations on the data Maintained Sessions using Spring MVC session management tools Annotated POJOs are created using Hibernate annotations Familiarized with Named Queries and Parameterized Queries in Hibernate Also Worked on SQL PLSQL using SQL Developer for Oracle database Involved in deploying the application under Apache Tomcat and maintained application logs Using Log4j Involved in unit testing using JUnit Used MAVEN to define the dependencies plugin and build the application Used SVN version Control tools Used Jenkins for deploying the application to test and production environments Designed and Developed Web services using SOAP to make submissions Created and maintained various Message Queues and Message brokers that were a part of the application JMS is used extensively in the application for sending budget related alerts through SMS email etc Environment Hadoop Kafka Java 7 Spring 4 Spring MVC Spring AOP Spring Data JPA Hibernate 3 SQL Microservices Spring Boot RESTful web services JSON JUnit 4 SVN Java script Log4J Jenkins Tomcat Jira Education Masters Skills APACHE KAFKA 2 years JAVA 2 years JSON 2 years KAFKA 2 years APACHE HADOOP HDFS 1 year Additional Information TECHNICAL SKILLS Operating Systems Linux Ubuntu CentOS Windows Mac OS Hadoop Ecosystem Hadoop MapReduce Yarn HDFS Pig Oozie Zookeeper Big Data Ecosystem Spark Spark SQL Spark Streaming Spark MLlib Hive Impala Hue Data Ingestion Sqoop Flume NiFi Kafka NOSQL Databases HBase Cassandra MongoDB CouchDB Programming Languages C C Scala Core Java J2EE Frameworks Model View Controller MVC Spring Hibernate Web Technologies Spring 4 Spring MVC Hibernate 3 JSP JavaScript AngularJS HTML 5 CSS 3 AJAX JQuery XML XSD WSDL JSON Web services Scripting Languages Java Script UNIX Python R Language Databases Oracle 10g11 g PostgreSQL 93 MySQL SQLServer Teradata IDE IntelliJ Eclipse Visual Studio IDLE Web Services Restful SOAP Tools Ant Tortoise SVN Putty Win SCP Maven log4j JUnit SOAPUI Git Jasper reports Jenkins Tableau Mahout Methodologies SDLC Agile Scrum Iterative Development Waterfall Model image1emf",
    "extracted_keywords": [
        "Hadoop",
        "Developer",
        "Hadoop",
        "span",
        "lDeveloperspan",
        "Hadoop",
        "Developer",
        "Overall",
        "years",
        "IT",
        "experience",
        "Hadoop",
        "Developer",
        "track",
        "record",
        "Hadoop",
        "ecosystem",
        "components",
        "architecture",
        "file",
        "distribution",
        "systems",
        "Big",
        "Data",
        "arena",
        "stakeholders",
        "execute",
        "solutions",
        "systems",
        "issues",
        "data",
        "Machine",
        "Learning",
        "modules",
        "data",
        "mining",
        "Adept",
        "Hadoop",
        "cluster",
        "management",
        "capacity",
        "endtoend",
        "data",
        "management",
        "performance",
        "optimization",
        "Hadoop",
        "Developer",
        "work",
        "experience",
        "HDFS",
        "MapReduce",
        "Hive",
        "Pig",
        "Spark",
        "Sqoop",
        "Oozie",
        "Kafka",
        "zookeeper",
        "HBase",
        "Hadoop",
        "Distributions",
        "Cloudera",
        "Hortonworks",
        "Map",
        "R",
        "Amazon",
        "EMR",
        "Microsoft",
        "Azure",
        "HDInsight",
        "Hadoop",
        "Proficient",
        "Unix",
        "Command",
        "Line",
        "Interface",
        "Experience",
        "data",
        "HDFS",
        "Relational",
        "Database",
        "Systems",
        "RDBMS",
        "Apache",
        "Sqoop",
        "Expertise",
        "Hive",
        "data",
        "warehouse",
        "tables",
        "data",
        "distribution",
        "Partitioning",
        "HQL",
        "queries",
        "Hive",
        "tables",
        "data",
        "Hive",
        "Adhoc",
        "queries",
        "MapReduce",
        "Spark",
        "Significant",
        "experience",
        "custom",
        "UDFs",
        "Hive",
        "custom",
        "Input",
        "Formats",
        "MapReduce",
        "Experience",
        "Hadoop",
        "log",
        "Knowledge",
        "job",
        "work",
        "flow",
        "management",
        "monitoring",
        "tools",
        "Oozie",
        "zookeeper",
        "Experience",
        "NoSQL",
        "database",
        "technologies",
        "Cassandra",
        "HBase",
        "Strong",
        "experience",
        "building",
        "end",
        "data",
        "pipelines",
        "Hadoop",
        "platform",
        "Experience",
        "Spark",
        "Applications",
        "Spark",
        "RDD",
        "Spark",
        "SQL",
        "Data",
        "frame",
        "APIs",
        "MR",
        "jobs",
        "Hive",
        "scripts",
        "Spark",
        "SQL",
        "Spark",
        "data",
        "transformations",
        "data",
        "knowledge",
        "troubleshooting",
        "Spark",
        "applications",
        "Hive",
        "scripts",
        "performance",
        "understanding",
        "time",
        "streaming",
        "technologies",
        "Spark",
        "Kafka",
        "data",
        "processing",
        "streaming",
        "techniques",
        "Spark",
        "streaming",
        "Kafka",
        "understanding",
        "Machine",
        "Learning",
        "Methodologies",
        "patterns",
        "user",
        "behaviour",
        "modeling",
        "Spark",
        "MLlib",
        "Java",
        "Developer",
        "year",
        "experience",
        "analysis",
        "design",
        "implementation",
        "integration",
        "testing",
        "maintenance",
        "applications",
        "JavaJ2EE",
        "ObjectOriented",
        "ClientServer",
        "understanding",
        "Java",
        "Virtual",
        "Machines",
        "multithreading",
        "process",
        "Expert",
        "Core",
        "Java",
        "understanding",
        "Collections",
        "Multithreading",
        "Event",
        "Exception",
        "handling",
        "Generics",
        "Strong",
        "experience",
        "Software",
        "methodologies",
        "Agile",
        "Scrum",
        "Waterfall",
        "Test",
        "Drive",
        "Development",
        "Significant",
        "experience",
        "UI",
        "frame",
        "JSP",
        "HTML5",
        "XML",
        "CSS3",
        "JavaScript",
        "Angular",
        "JS",
        "JQuery",
        "experience",
        "web",
        "applications",
        "Java",
        "JEE",
        "Spring",
        "Servlets",
        "Hibernate",
        "JDBC",
        "Eclipse",
        "Handson",
        "experience",
        "Continuous",
        "Integration",
        "CI",
        "buildautomation",
        "tools",
        "Maven",
        "SVN",
        "CVS",
        "Jenkins",
        "Apache",
        "Ant",
        "Log4j",
        "errors",
        "production",
        "test",
        "environment",
        "Ant",
        "Maven",
        "tools",
        "Database",
        "design",
        "modeling",
        "migration",
        "development",
        "experience",
        "procedures",
        "cursor",
        "constraints",
        "functions",
        "SQL",
        "MS",
        "SQL",
        "Server",
        "DB2",
        "Oracle",
        "communication",
        "Skills",
        "team",
        "player",
        "growth",
        "organization",
        "Work",
        "Experience",
        "Hadoop",
        "Developer",
        "Express",
        "Scripts",
        "Franklin",
        "Lakes",
        "NJ",
        "August",
        "May",
        "Description",
        "Express",
        "Scripts",
        "pharmacy",
        "benefit",
        "management",
        "PBM",
        "organization",
        "United",
        "States",
        "amount",
        "data",
        "data",
        "insights",
        "healthcare",
        "scope",
        "project",
        "epidemic",
        "intervention",
        "analytics",
        "data",
        "costs",
        "health",
        "events",
        "machine",
        "algorithms",
        "risk",
        "health",
        "analytics",
        "intelligence",
        "cuttingedge",
        "capability",
        "health",
        "risk",
        "individuals",
        "Responsibilities",
        "Sqoop",
        "Kafka",
        "Flume",
        "Hadoop",
        "File",
        "System",
        "APIs",
        "data",
        "ingestion",
        "pipelines",
        "time",
        "transformations",
        "data",
        "Kafka",
        "Spark",
        "Streaming",
        "storage",
        "Amazon",
        "S3",
        "data",
        "data",
        "Kafka",
        "topic",
        "AWS",
        "S3",
        "storage",
        "Hive",
        "tables",
        "data",
        "Hive",
        "queries",
        "data",
        "Partitions",
        "Bucketing",
        "Hive",
        "tables",
        "parameters",
        "performance",
        "Hive",
        "UDFs",
        "business",
        "usecases",
        "Hive",
        "scripts",
        "source",
        "data",
        "validation",
        "transformation",
        "Automated",
        "data",
        "HDFS",
        "Hive",
        "data",
        "Oozie",
        "Designed",
        "ETL",
        "framework",
        "Java",
        "Pig",
        "data",
        "sources",
        "Hive",
        "Hive",
        "Vertica",
        "Collaborated",
        "data",
        "data",
        "mining",
        "Machine",
        "Learning",
        "data",
        "processing",
        "ETL",
        "optimization",
        "data",
        "formats",
        "Avro",
        "Sequence",
        "File",
        "JSON",
        "Map",
        "File",
        "Parquet",
        "XML",
        "AWS",
        "Components",
        "Airflow",
        "Elastic",
        "Map",
        "EMR",
        "Athena",
        "Snowflake",
        "Apache",
        "NiFi",
        "data",
        "movement",
        "Hadoop",
        "components",
        "NiFi",
        "conversion",
        "XML",
        "data",
        "JSON",
        "Avro",
        "Hadoop",
        "Cloudera",
        "Data",
        "Platform",
        "running",
        "services",
        "Cloudera",
        "manager",
        "Assisted",
        "Hadoop",
        "administration",
        "support",
        "activities",
        "installations",
        "Apache",
        "Big",
        "Data",
        "Tools",
        "Hadoop",
        "clusters",
        "Cloudera",
        "Manager",
        "Experienced",
        "Hadoop",
        "Production",
        "support",
        "tasks",
        "Application",
        "cluster",
        "logs",
        "Agile",
        "Scrum",
        "methodology",
        "Scrum",
        "Alliance",
        "development",
        "Environment",
        "Hadoop",
        "HDFS",
        "AWS",
        "Vertica",
        "Scala",
        "Kafka",
        "MapReduce",
        "YARN",
        "Drill",
        "Spark",
        "Pig",
        "Hive",
        "Scala",
        "Java",
        "NiFi",
        "HBase",
        "MySQL",
        "Kerberos",
        "Maven",
        "Java",
        "Developer",
        "Hadoop",
        "Hyderabad",
        "Telangana",
        "June",
        "June",
        "Indus",
        "Group",
        "Hyderabad",
        "India",
        "Description",
        "Indus",
        "Group",
        "technology",
        "consulting",
        "firm",
        "performers",
        "solutions",
        "mission",
        "ontime",
        "onbudget",
        "quality",
        "service",
        "clients",
        "globe",
        "expectations",
        "consulting",
        "partner",
        "Indus",
        "group",
        "practices",
        "frameworks",
        "solutions",
        "clients",
        "project",
        "design",
        "development",
        "management",
        "application",
        "insight",
        "Big",
        "Data",
        "technologies",
        "Responsibilities",
        "practices",
        "data",
        "acquisition",
        "analysis",
        "products",
        "Big",
        "Data",
        "technologies",
        "data",
        "streaming",
        "Kafka",
        "Scrum",
        "methodologies",
        "data",
        "management",
        "analytics",
        "Jira",
        "project",
        "Life",
        "Cycle",
        "Design",
        "Implementation",
        "Unit",
        "Testing",
        "development",
        "methodology",
        "sprint",
        "planning",
        "User",
        "Interfaces",
        "JSP",
        "JavaScript",
        "HTML5",
        "Angular",
        "JS",
        "jQuery",
        "help",
        "design",
        "patterns",
        "Singleton",
        "Factory",
        "MVC",
        "legacy",
        "projects",
        "versions",
        "spring",
        "hibernate",
        "Used",
        "DAO",
        "connection",
        "data",
        "data",
        "storage",
        "elements",
        "Written",
        "Microservices",
        "data",
        "task",
        "scheduling",
        "Spring",
        "Boot",
        "Spring",
        "Hibernate",
        "API",
        "tools",
        "microservices",
        "Hibernate",
        "data",
        "Database",
        "HQL",
        "queries",
        "CRUD",
        "operations",
        "data",
        "Sessions",
        "Spring",
        "MVC",
        "session",
        "management",
        "tools",
        "Annotated",
        "POJOs",
        "Hibernate",
        "annotations",
        "Named",
        "Queries",
        "Parameterized",
        "Queries",
        "Hibernate",
        "SQL",
        "PLSQL",
        "SQL",
        "Developer",
        "Oracle",
        "database",
        "application",
        "Apache",
        "Tomcat",
        "application",
        "logs",
        "Log4j",
        "unit",
        "testing",
        "JUnit",
        "MAVEN",
        "dependencies",
        "application",
        "SVN",
        "version",
        "Control",
        "tools",
        "Jenkins",
        "application",
        "production",
        "environments",
        "Web",
        "services",
        "SOAP",
        "submissions",
        "Message",
        "Queues",
        "Message",
        "brokers",
        "part",
        "application",
        "JMS",
        "application",
        "budget",
        "alerts",
        "SMS",
        "email",
        "Environment",
        "Hadoop",
        "Kafka",
        "Java",
        "Spring",
        "Spring",
        "MVC",
        "Spring",
        "AOP",
        "Spring",
        "Data",
        "JPA",
        "Hibernate",
        "SQL",
        "Microservices",
        "Spring",
        "Boot",
        "web",
        "services",
        "JSON",
        "JUnit",
        "SVN",
        "Java",
        "script",
        "Log4J",
        "Jenkins",
        "Tomcat",
        "Jira",
        "Education",
        "Masters",
        "Skills",
        "APACHE",
        "KAFKA",
        "years",
        "years",
        "JSON",
        "years",
        "KAFKA",
        "years",
        "APACHE",
        "HADOOP",
        "HDFS",
        "year",
        "Additional",
        "Information",
        "TECHNICAL",
        "SKILLS",
        "Operating",
        "Systems",
        "Linux",
        "Ubuntu",
        "CentOS",
        "Windows",
        "Mac",
        "OS",
        "Hadoop",
        "Ecosystem",
        "Hadoop",
        "MapReduce",
        "Yarn",
        "HDFS",
        "Pig",
        "Oozie",
        "Zookeeper",
        "Big",
        "Data",
        "Ecosystem",
        "Spark",
        "Spark",
        "SQL",
        "Spark",
        "Streaming",
        "Spark",
        "MLlib",
        "Hive",
        "Impala",
        "Hue",
        "Data",
        "Ingestion",
        "Sqoop",
        "Flume",
        "NiFi",
        "Kafka",
        "NOSQL",
        "HBase",
        "Cassandra",
        "MongoDB",
        "CouchDB",
        "Programming",
        "Languages",
        "C",
        "C",
        "Scala",
        "Core",
        "Java",
        "J2EE",
        "Frameworks",
        "Model",
        "View",
        "Controller",
        "MVC",
        "Spring",
        "Hibernate",
        "Web",
        "Technologies",
        "Spring",
        "Spring",
        "MVC",
        "Hibernate",
        "JSP",
        "JavaScript",
        "HTML",
        "CSS",
        "AJAX",
        "JQuery",
        "XML",
        "XSD",
        "WSDL",
        "JSON",
        "Web",
        "services",
        "Scripting",
        "Languages",
        "Java",
        "Script",
        "UNIX",
        "Python",
        "R",
        "Language",
        "Oracle",
        "g",
        "PostgreSQL",
        "MySQL",
        "SQLServer",
        "Teradata",
        "IDE",
        "IntelliJ",
        "Eclipse",
        "Visual",
        "Studio",
        "IDLE",
        "Web",
        "Services",
        "Restful",
        "SOAP",
        "Tools",
        "Ant",
        "Tortoise",
        "SVN",
        "Putty",
        "Win",
        "SCP",
        "Maven",
        "log4j",
        "JUnit",
        "SOAPUI",
        "Git",
        "Jasper",
        "Jenkins",
        "Tableau",
        "Mahout",
        "Methodologies",
        "SDLC",
        "Agile",
        "Scrum",
        "Iterative",
        "Development",
        "Waterfall",
        "Model",
        "image1emf"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T20:51:59.233709",
    "resume_data": "Hadoop Developer Hadoop span lDeveloperspan Hadoop Developer Overall 5 years of IT experience result oriented Hadoop Developer possessing a proven track record of effectively administering Hadoop ecosystem components architecture and managing file distribution systems in the Big Data arena Proficient in collaborating with key stakeholders to conceptualize execute solutions for resolving systems architecturebased technical issues Highly skilled in processing complex data designing Machine Learning modules for effective data mining modeling Adept at Hadoop cluster management capacity planning for endtoend data management performance optimization Hadoop Developer with work experience in using HDFS MapReduce Hive Pig Spark Sqoop Oozie Kafka zookeeper and HBase Experienced working with various Hadoop Distributions Cloudera Hortonworks Map R Amazon EMR Microsoft Azure HDInsight to fully implement and leverage new Hadoop features Proficient in using Unix based Command Line Interface Experience in moving data into and out of the HDFS and Relational Database Systems RDBMS using Apache Sqoop Expertise in working with Hive data warehouse infrastructurecreating tables data distribution by implementing Partitioning and Bucketing developing and tuning the HQL queries Involved in creating Hive tables loading with data and writing Hive Adhoc queries that will run internally in MapReduce and Spark Significant experience writing custom UDFs in Hive and custom Input Formats in MapReduce Experience in managing and reviewing Hadoop log files Knowledge of job work flow management and monitoring tools like Oozie and zookeeper Experience working with NoSQL database technologies including MongoDB Cassandra and HBase Strong experience building end to end data pipelines on Hadoop platform Experience in developing Spark Applications using Spark RDD Spark SQL and Data frame APIs Replaced existing MR jobs and Hive scripts with Spark SQL Spark data transformations for efficient data processing Deep knowledge of troubleshooting and tuning Spark applications and Hive scripts to achieve optimal performance Strong understanding of real time streaming technologies Spark and Kafka Worked with realtime data processing and streaming techniques using Spark streaming and Kafka Good understanding on Machine Learning Methodologies to uncover the hidden patterns user behaviour and modeling using Spark MLlib Java Developer with a year of diversified experience in analysis design implementation integration testing and maintenance of applications using JavaJ2EE and ObjectOriented ClientServer technologies Strong understanding of Java Virtual Machines and multithreading process Expert in Core Java with strong understanding of Collections Multithreading Event handling Exception handling and Generics Strong experience with Software methodologies like Agile Scrum Waterfall and Test Drive Development Significant experience in UI frame works such as JSP HTML5 XML CSS3 JavaScript Angular JS JQuery Extensive experience in developing web applications using Java JEE Spring Servlets Hibernate JDBC Eclipse Handson experience working with Continuous Integration CI buildautomation tools such as Maven SVN CVS Jenkins and Apache Ant Experienced in generating logging by Log4j to identify the errors in production test environment and experienced in Ant and Maven tools Database design modeling migration and development experience in using stored procedures triggers cursor constraints and functions Used My SQL MS SQL Server DB2 and Oracle Effective communication and interpersonal Skills an excellent team player work towards the growth of an organization Work Experience Hadoop Developer Express Scripts Franklin Lakes NJ August 2015 to May 2017 Description Express Scripts is one of the most significant pharmacy benefit management PBM organization in the United States produces an incredible amount of data Using big data to generate actionable insights helps to improve healthcare by making it more accessible affordable and effective The scope of the project was to address the opioid epidemic through proactive intervention by advanced analytics using big data avoiding downstream costs and adverse health events using machine learning algorithms and predicting personalized risk to influence better health using advanced analytics and artificial intelligence provides the cuttingedge capability to determine the health risk of individuals Responsibilities Utilized Sqoop Kafka Flume and Hadoop File System APIs for implementing data ingestion pipelines Worked on real time streaming performed transformations on the data using Kafka and Spark Streaming Created storage with Amazon S3 for storing data Worked on transferring data from Kafka topic into AWS S3 storage Created Hive tables loaded with data and wrote Hive queries to process the data Created Partitions and used Bucketing on Hive tables and used required parameters to improve performance and developed Hive UDFs as per business usecases Developed Hive scripts for source data validation and transformation Automated data loading into HDFS and Hive for preprocessing the data using Oozie Designed and implemented an ETL framework using Java and Pig to load data from multiple sources into Hive and from Hive into Vertica Collaborated in data modeling data mining Machine Learning methodologies advanced data processing ETL optimization Worked on various data formats like Avro Sequence File JSON Map File Parquet and XML Worked extensively on AWS Components such as Airflow Elastic Map Reduce EMR Athena Snowflake Used Apache NiFi to automate data movement between different Hadoop components Used NiFi to perform conversion of raw XML data into JSON Avro Experienced in working with Hadoop from Cloudera Data Platform and running services through Cloudera manager Assisted in Hadoop administration and support activities for installations and configuring Apache Big Data Tools and Hadoop clusters using Cloudera Manager Experienced in Hadoop Production support tasks by analysing the Application and cluster logs Used Agile Scrum methodology Scrum Alliance for development Environment Hadoop HDFS AWS Vertica Scala Kafka MapReduce YARN Drill Spark Pig Hive Scala Java NiFi HBase MySQL Kerberos Maven Java Developer Hadoop Hyderabad Telangana June 2014 to June 2015 Indus Group Hyderabad India Description Indus Group is a technology consulting firm founded by proficient performers in the technical solutions with a mission to provide ontime onbudget and quality service to the clients across globe and consistently meet their expectations As a consulting partner Indus group also suggest best practices appropriate frameworks and optimal solutions to the clients The project is responsible for design development management of javabased application and had deep insight about using Big Data technologies Responsibilities Standardized practices for data acquisition analysis to deliver new products using Big Data technologies Directed data streaming in Kafka deployed Scrum methodologies for data management and analytics in Jira Involved in complete project Life Cycle ie Design Implementation Unit Testing Extensively used agile development methodology and involved in sprint planning Designed and modified User Interfaces using JSP JavaScript HTML5 Angular JS and jQuery with the help of several design patterns like Singleton Factory and MVC Involved in migrating legacy projects to latest versions of spring and hibernate Used DAO to handle connection and to retrieve data from data storage elements Written Microservices to exportimport data and task scheduling using Spring Boot Spring and Hibernate Also Used Swagger API tools while developing the microservices Implemented Hibernate to persist the data into Database and wrote HQL based queries to implement CRUD operations on the data Maintained Sessions using Spring MVC session management tools Annotated POJOs are created using Hibernate annotations Familiarized with Named Queries and Parameterized Queries in Hibernate Also Worked on SQL PLSQL using SQL Developer for Oracle database Involved in deploying the application under Apache Tomcat and maintained application logs Using Log4j Involved in unit testing using JUnit Used MAVEN to define the dependencies plugin and build the application Used SVN version Control tools Used Jenkins for deploying the application to test and production environments Designed and Developed Web services using SOAP to make submissions Created and maintained various Message Queues and Message brokers that were a part of the application JMS is used extensively in the application for sending budget related alerts through SMS email etc Environment Hadoop Kafka Java 7 Spring 4 Spring MVC Spring AOP Spring Data JPA Hibernate 3 SQL Microservices Spring Boot RESTful web services JSON JUnit 4 SVN Java script Log4J Jenkins Tomcat Jira Education Masters Skills APACHE KAFKA 2 years JAVA 2 years JSON 2 years KAFKA 2 years APACHE HADOOP HDFS 1 year Additional Information TECHNICAL SKILLS Operating Systems Linux Ubuntu CentOS Windows Mac OS Hadoop Ecosystem Hadoop MapReduce Yarn HDFS Pig Oozie Zookeeper Big Data Ecosystem Spark Spark SQL Spark Streaming Spark MLlib Hive Impala Hue Data Ingestion Sqoop Flume NiFi Kafka NOSQL Databases HBase Cassandra MongoDB CouchDB Programming Languages C C Scala Core Java J2EE Frameworks Model View Controller MVC Spring Hibernate Web Technologies Spring 4 Spring MVC Hibernate 3 JSP JavaScript AngularJS HTML 5 CSS 3 AJAX JQuery XML XSD WSDL JSON Web services Scripting Languages Java Script UNIX Python R Language Databases Oracle 10g11g PostgreSQL 93 MySQL SQLServer Teradata IDE IntelliJ Eclipse Visual Studio IDLE Web Services Restful SOAP Tools Ant Tortoise SVN Putty Win SCP Maven log4j JUnit SOAPUI Git Jasper reports Jenkins Tableau Mahout Methodologies SDLC Agile Scrum Iterative Development Waterfall Model image1emf",
    "unique_id": "b13b3754-1e50-4011-a46f-f080dd648eff"
}