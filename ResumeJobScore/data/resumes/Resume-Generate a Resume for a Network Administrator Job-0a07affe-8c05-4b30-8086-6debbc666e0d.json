{
    "clean_data": "Hadoop Developer Hadoop span lDeveloperspan Hadoop Developer Deutsche BankExchange Pl NJ Bloomfield NJ Highly selfmotivatedgood technical communications and interpersonal skills Able to work reliably under pressure Committed team player with strong analytical and problem solving skills ability to quickly adapt to new environments technologies Over eight years of IT experience in analysis design and development using Hadoop Java and J2EE 4 years of experience with Hadoop HDFS MapReduce and Hadoop Ecosystem Pig Hive HBase Hadoop Eco System Excellent understandingknowledge of Hadoop architecture and various components such as HDFS JobTracker TaskTracker Name Node Data Node and MapReduce programming paradigm Hadoop AdministrationCoordinated with administrators in installing configuring and using Hadoop ecosystem components like Hadoop MapReduce HDFS HBase Oozie Sqoop Flume Pig Hive and Spark Hadoop Enterprise Distributions Experience in installing maintaining and upgrading Hadoop distributions like Cloudera CDH 5x4x Hortonworks 2x MapR 1x and DataStax 4x Hadoop Tools Experience in analyzing data using HiveQL HBase Pig and custom MapReduce programs in Java Handson experience in creating scripts for performing dataanalysis with Pig Hive and Impala Experience in migrating applications from MapReduce to Spark using Scala Cassandra Developer and Modeling Configuring and setting up Cassandra Cluster Expertise in data modeling and analysis of Cassandra and Cassandra Query Language HBase Ingested Data from RDBMS and Hive to HBase Based on the requirements implemented custom coded MapReduce for HBase and used the Java client API Data Ingestion Using Flume designed the flow and configured the individual components Efficiently transferred bulk data from and to traditional databases with Sqoop Data Storage Experience in maintaining distributed storage likeHDFS Hbaseand Cassandra SQL to NoSQL Experience in migrating data from traditional database to NoSQL As well included migration tools such as Sqoop and Flume while ingesting data Management and Monitoring Maintained and coordinated centralized services usingZookeeper Messaging System Good knowledge of Kafka for fast messaging transfer across systems Cloud PlatformsExperience in using cloud based Hadoop clusters likeOpenStack and Amazon Web Services AWS Job Schedulers Experience in scheduling jobs using tools like Apache Oozie Scripting Experienced in Hive Pig and shell scripting Java and J2EE Experience in developing applications using JavaJ2E Developed web applications in springframework from the scratch Worked with Object Relational Mapping ORM persistence technologies like Hibernate Experiences in working with annotation in Spring andHibernateExtensive experience and actively involved in various phases of SDLC Experience in design patterns like Singleton pattern Factory pattern Data Access Objectspattern and ModelViewController pattern WebApplication Servers Experience in the functional usage and deployment of applications on web servers or application servers  Tomcat 807060WebLogic 12c11g10xWebSphere 8x 70 JBoss6x andIIS 76 servers UI Design Comprehensive knowledge in HTML5 CSS3 JavaScript and Bootstrap Project Management Experience in Agile andScrum project management Authorized to work in the US for any employer Work Experience Hadoop Developer Deutsche BankExchange Pl NJ March 2017 to Present Genre Risk Management and estatements Project Description The Risk Based Capital group RBC within regulatory reporting is responsible for the preparation calculation and reporting of capital components and capital ratios for Basel III an international standard that banking regulators can use when creating regulations about how much capital banks need to put aside to guard against the types of financial and operational risks banks face RBC reports the capital components and capital ratios to the Corporate Controller Treasurer and Chief Financial Officer of bank and the regulatory agencies In order to calculate the correct amount of capital required to remain Basel III compliant an existing application was required to get upgradedthat will collect transform and calculate finance data and assign the appropriate capital calculation and risk weighting The Deutsche bank has enabledeStatement through email service from Business Credit Card Account The banks existing systems collect the account holder data like account details transaction details and other account related information which is extracted from various sources and stored into distributed file system It processes the data to generate the statement file which is utilized by email sending system to send out emails Approximately 7 Million estatements were required to be processed by the existing system Role and Responsibilities Handling raw data from various subsystems and load the data from different subsystems into to HDFS for further processing Used Flume to collect aggregate and store the web log data onto HDFS Used Sqoop to import data from Oracle database to HDFS Developed UDFs in Pig Scriptsfor datacleaning and preprocessing Used Hive to do analysis on the data and identify different correlations Development and maintenance of the HiveQL Pig Scripts andMapReduce Implemented performancetuning techniques in Hive Facilitating testing in different dimensions Coordinated and modified businesslogicas per the business requirements Used Crontab for automation of scripts Implemented POC for using Apache Impala for real time query processing over Hive Moved all the required data to NoSQL database Cassandra from HDFSfor visualization to generate reports for the BI team ENVIRONMENT Cloudera CDH 4 Hadoop 20 Spark Scala Pig 0110 Hive 0100 Sqoop 143 Flume 140 ZooKeeper 345 Cassandra Spark SparkQLCron JavaJ2EE Oracle 11g WebLogic Server 11g Senior Hadoop Developer Fifth Third Bank Cincinnati OH October 2015 to February 2017 Genre Platform rearchitecture and Risk Management Project DescriptionFifth Third Bank wants to provide risk managementservices for the entire corporate bank the current data platform is undergoing a rearchitecture in order to meet the changing data storage processing and maintainability requirements for risk management The primary objective of this project is to prove that a pivotal based solution will provide a central data platform for management and access to the various risk management datasets while meeting the throughput metrics for rapid data ingestion and processing of this data set with very fast turnaround results Themodule of this project was about decisionmaking churn analysis and sentiment analysis To achieve the project objective Hadoop platform DataStax Enterprise 47 and related tools were used Role and Responsibilities Coordinated with Administrators in setting up configuring initializing and troubleshooting DS Enterprise 47 Involved in loading data from UNIX file system to HDFS Involved in defining job flows and running data streaming jobs to process terabytes of text data Developed multiple MapReduce jobs in Java for data cleaning and preprocessing Wrote MapReduce jobs to discovertrends in data usage by users Involved in managing andreviewing Hadoop log files Installed and configured Hiveand written HiveQL scripts Involved in loading and transforming large sets of structured semi structured and unstructured data Involved in creating Hive tables loading data and writing Hive queries as per business requirements Implemented static partitioningdynamic partitioning and bucketing of data in Hive for improving the performance Supported Map Reduce programs those are running on the cluster Involved in writing both DML and DDL operations in NoSQL database Cassandra Responsible to manage data coming from different sources Implemented POC on writing programs in Scala using Spark Worked on migrating MapReduce programs into Spark using Scala Assisted the team in their developmentdeployment activities Used Web services concepts like SOAP to interact with other project within organization and for sharing information Involved in developing database access components using Spring DAO integrated with Hibernate for accessing the data Followed Hybrid Waterfall Scrumprinciples in developing the project ENVIRONMENTDataStax Enterprise 47 Linux Hadoop240 MapReduce Hive 0120 Pig 0101 Impala Hbase 0961 Sqoop 145 Flume 140 ZooKeeper 345 Cassandra 215 Spark 121 Spark Cassandra Connector 121 SparkQLSolr 410 SOAP Spring 41 Hibernate 43 Oracle 12c Hadoop Developer Comcast Philadelphia PA January 2015 to September 2015 Genre Marketing Research Project DescriptionThe idea of this project isdistributed batch processing infrastructure system based on Hadoop Distributed File System to access and query largescale data It is a high throughput realtime data collection and reporting system used by higher management decision makers in Marketing Research group to analyze future trends for their upcoming product development marketing campaigns and to enhance business continuity by anticipating changes in customer behavior Role and Responsibilities DevelopedMapReduce layers using Javato support SequenceandAvroformatted input data Used PigLatin scripts to process huge datasets in parallel for advancedtransformations Used HiveQL scripts and Hive User Defined Functionsto extract and analyze data over HBase input format Implemented partitioning andbucketing in Hive Used Sqoop data import and export scripts between various RDBMS structuredsources to HDFSand NoSQL database HBase Imported data usingFlume data transfer between various data sources to HDFS DevelopedOozieworkflows that chain HiveMapReduce modules for ingesting periodichourly input data Monitoring Hadoop scripts which take the input from HDFS and load the data into Hive Monitored Systemhealth logs and responds accordingly to any warning or failure conditions using Zookeeper ENVIRONMENT MapR 12 Hadoop 103 Hive 071 Hbase 0904 ZooKeeper 334 Pig 090 Sqoop 130 Oozie 300 Flume 094 Java Developer ICICI Bank Hyderabad Telangana February 2013 to December 2014 Genre Fund Transfer Integration Project Description FTI is fund transfer Integration which is an application enabling users to transfer fund from ownaccount to third party accounts The fund transfer can be to any account of same bank account transfer third party account transfer within same country and Cross border fund transfer FTI is used almost across 16 APAC countries Corporate Consumer segments both uses FTI FTI process starts with any fund transfer instruction feeds to it from Internal or external systems Internal Systems like BSI or FC whereas external instructions coming via SWIFT to CMG to FTI Role and Responsibilities Used Spring Framework for dependency injectionusingspring configuration files Developed the presentation layer using JSP HTML CSS and client validations using JavaScript Involved in the installation and configuration of Tomcat Server Involved in dynamic form generation auto completion of forms and user validation functionalities using AJAX Designed developed and maintained the data layer using Hibernate and performed configuration of Spring application framework Created stored procedures using PLSQL for data access layer Worked on tuning of backend Oracle stored procedures using TOAD Developed test cases for unit testing using JUnit Developed stored procedures to extract data from Oracle database ENVIRONMENT JavaJ2EE JSP Servlets Spring Framework Hibernate SQLPLSQL Web Services WSDL JUnit Tomcat Oracle 9i and Windows Java Developer Next Step Solutions Hyderabad Telangana January 2012 to January 2013 Genre Payroll System Project Description The goal of the project was to design an online payroll system which enabled organizations to maintain the payroll information of their employees The principle attributes of this payroll system included web based timesheets that recorded working hoursgraphical absence calendar system payroll generationand the ability to rerun the payroll for every single employee Another prominent feature was the provision to view historical payroll information via the historical pay slip production functionality Role and Responsibilities Involved in designing screen using HTML and CSS Used JavaScript to perform checking and validations at clientside Worked with SpringDependencyInjectionDI Involved in implementation of Spring Beans DAO Controller and Service layers for fast and efficient processing Implementedpersistence application layer usingObject Relational Model ORM Hibernate HQL and annotations Involved in writingHQL and SQL Queries for Oracle database Used log4j for logging messages Developed the classes for Unit Testing by using JUnit ENVIRONMENT JavaJ2EE Servlets JSP Spring MVC Hibernate Eclipse JavaScript AJAX XML Log4j Oracle 9i Web Logic Jr Java Developer Bank of India Hyderabad Telangana May 2009 to December 2011 Genre Bank Portal Project Description Bank of India BoI is an Indian stateowned commercial bank with headquarters in Mumbai Maharashtra India BoI is a founder member of Society for Worldwide Inter Bank Financial Telecommunications which facilitates provision of costeffective financial processing and communication services The goal of this project was to improve the UI design and improve the overall performance of the system Role and Responsibilities Worked with the frontend applications using HTML CSS and Java Script Developed the business components used in theJSPscreen Implemented DAO patterns for building the application Used JUnit for unit testing Involved in the design of JSP screens for the Public Provident Fund and Bond modules Participated in testing phases and provided UAT support Created jar files and deployed in WebLogicApplicationServer Involved in writing SQL queries to create tables and stored procedure to fulfill the requirements and accommodate the business rules in Oracle database ENVIRONMENT JavaJ2EE JDBC Servlets JSP HTML CSS JavaScript SQL PLSQL Web Logic 61 Eclipse Education Bachelors of Technology in Technology JNTU Hyderabad Telangana Skills JAVA 8 years ORACLE 8 years TESTING 6 years CSS 5 years HTML 5 years Cloudera CDH 4 Hadoop 20 Spark Scala Pig 0110 Hive 0100 Sqoop 143 Flume 140 ZooKeeper 345 Cassandra Spark SparkQLCronJavaJ2EE Oracle 11g WebLogic Server 11g 8 years Additional Information TECHNICAL SKILLS Big Data Technologies Apache Hadoop MapReduce HDFS Pig Hive HBase ZooKeeper Sqoop Flume Oozie HCatalog Hue Tez Avro YARN Ambari Spark Scala Solr Kafka Programming Languages Java Net C VbNet SQL HQLHiveQL CQL Scala Unix Shell Scripting Java Technologies Core Java Servlets JDBC JSP Spring Hibernate Web Technologies HTML 5 CSS 3 Java Script Bootstrap Servlets JSP ASP XML JSON WebApp Servers Tomcat 807060 WebLogic 12c11g10x WebSphere 8x 70 JBoss 6x and IIS 76 servers Databases RDBMSOracle 12c11g SQL Server 201208 MySQL5x andNoSQLHBase Cassandra Operating Systems Windows Unix and Linux IDE EditPlus Eclipse NetBeans TOAD SQL Server Management Studio MySQL Workbench Visual Studio Version Control Git Subversion CVS Testing Technologies JUnit Reporting ETL Tools Tableau Informatica SSIS Pentaho",
    "entities": [
        "Monitoring Hadoop",
        "Sqoop Data Storage Experience",
        "Work Experience Hadoop Developer Deutsche",
        "Apache Oozie Scripting Experienced",
        "CDH 4 Hadoop",
        "Hive Moved",
        "Cassandra and Cassandra Query Language HBase Ingested Data",
        "BI",
        "HDFS",
        "UNIX",
        "BSI",
        "Hadoop Developer Hadoop",
        "APAC",
        "FTI",
        "Hadoop HDFS MapReduce and Hadoop Ecosystem Pig Hive HBase Hadoop Eco System",
        "Genre Marketing Research Project",
        "Data Access Objectspattern",
        "SequenceandAvroformatted",
        "Hadoop",
        "ENVIRONMENT JavaJ2EE",
        "HDFS Involved",
        "SOAP",
        "Telangana",
        "Cross",
        "Hadoop MapReduce HDFS HBase Oozie",
        "HDFS Used Sqoop",
        "Hadoop Distributed File System",
        "Project Description",
        "Cincinnati",
        "WebLogic",
        "Corporate Consumer",
        "JUnit",
        "Hiveand",
        "Hive User",
        "HBase",
        "Oracle database ENVIRONMENT JavaJ2EE JSP Servlets",
        "UAT",
        "PigLatin",
        "WebSphere",
        "Bootstrap Project Management Experience",
        "Node Data",
        "Maharashtra India BoI",
        "Amazon Web Services AWS Job Schedulers",
        "DDL",
        "SWIFT",
        "Internal",
        "WebApplication Servers",
        "Agile andScrum",
        "JSP",
        "Oracle 12c Hadoop Developer Comcast Philadelphia",
        "Workbench Visual Studio",
        "Marketing Research",
        "SQL Queries",
        "JBoss",
        "Efficiently",
        "Scala Assisted",
        "MVC",
        "Present Genre Risk Management",
        "Spark",
        "RBC",
        "HTML CSS",
        "Hive Monitored Systemhealth",
        "US",
        "Sqoop",
        "Hibernate Experiences",
        "Created",
        "Java Script Bootstrap Servlets JSP ASP XML",
        "Additional Information TECHNICAL SKILLS Big Data Technologies Apache Hadoop MapReduce HDFS Pig Hive HBase ZooKeeper",
        "Oracle",
        "Singleton",
        "Spark Hadoop",
        "Cassandra Operating Systems Windows Unix",
        "Fifth Third Bank",
        "Hive Used Sqoop",
        "Scala Cassandra Developer",
        "HTML",
        "Oozie",
        "SQL",
        "DML",
        "the Public Provident Fund",
        "HBase Imported",
        "DescriptionThe",
        "API Data Ingestion Using Flume",
        "Genre Bank Portal Project Description Bank of India BoI",
        "DS Enterprise",
        "Hive",
        "Deutsche",
        "SQL Server Management Studio",
        "Committed",
        "ZooKeeper",
        "Mumbai",
        "FTI Role",
        "JSP Spring",
        "UI Design Comprehensive",
        "Bond",
        "Risk Management Project DescriptionFifth Third Bank",
        "Impala",
        "Zookeeper ENVIRONMENT",
        "JavaScript",
        "Society for Worldwide Inter Bank Financial Telecommunications",
        "Business Credit Card",
        "UI",
        "Monitoring Maintained",
        "CMG",
        "Hadoop Java",
        "Basel III",
        "DataStax 4x Hadoop Tools",
        "Tomcat Server Involved",
        "CSS",
        "Internal Systems",
        "MapReduce",
        "Spring DAO",
        "DataStax Enterprise",
        "NoSQL",
        "JUnit ENVIRONMENT JavaJ2EE Servlets",
        "FTI FTI",
        "JSP Spring Hibernate Web Technologies",
        "FC"
    ],
    "experience": "Experience in installing maintaining and upgrading Hadoop distributions like Cloudera CDH 5x4x Hortonworks 2x MapR 1x and DataStax 4x Hadoop Tools Experience in analyzing data using HiveQL HBase Pig and custom MapReduce programs in Java Handson experience in creating scripts for performing dataanalysis with Pig Hive and Impala Experience in migrating applications from MapReduce to Spark using Scala Cassandra Developer and Modeling Configuring and setting up Cassandra Cluster Expertise in data modeling and analysis of Cassandra and Cassandra Query Language HBase Ingested Data from RDBMS and Hive to HBase Based on the requirements implemented custom coded MapReduce for HBase and used the Java client API Data Ingestion Using Flume designed the flow and configured the individual components Efficiently transferred bulk data from and to traditional databases with Sqoop Data Storage Experience in maintaining distributed storage likeHDFS Hbaseand Cassandra SQL to NoSQL Experience in migrating data from traditional database to NoSQL As well included migration tools such as Sqoop and Flume while ingesting data Management and Monitoring Maintained and coordinated centralized services usingZookeeper Messaging System Good knowledge of Kafka for fast messaging transfer across systems Cloud PlatformsExperience in using cloud based Hadoop clusters likeOpenStack and Amazon Web Services AWS Job Schedulers Experience in scheduling jobs using tools like Apache Oozie Scripting Experienced in Hive Pig and shell scripting Java and J2EE Experience in developing applications using JavaJ2E Developed web applications in springframework from the scratch Worked with Object Relational Mapping ORM persistence technologies like Hibernate Experiences in working with annotation in Spring andHibernateExtensive experience and actively involved in various phases of SDLC Experience in design patterns like Singleton pattern Factory pattern Data Access Objectspattern and ModelViewController pattern WebApplication Servers Experience in the functional usage and deployment of applications on web servers or application servers   Tomcat 807060WebLogic 12c11g10xWebSphere 8x 70 JBoss6x andIIS 76 servers UI Design Comprehensive knowledge in HTML5 CSS3 JavaScript and Bootstrap Project Management Experience in Agile andScrum project management Authorized to work in the US for any employer Work Experience Hadoop Developer Deutsche BankExchange Pl NJ March 2017 to Present Genre Risk Management and estatements Project Description The Risk Based Capital group RBC within regulatory reporting is responsible for the preparation calculation and reporting of capital components and capital ratios for Basel III an international standard that banking regulators can use when creating regulations about how much capital banks need to put aside to guard against the types of financial and operational risks banks face RBC reports the capital components and capital ratios to the Corporate Controller Treasurer and Chief Financial Officer of bank and the regulatory agencies In order to calculate the correct amount of capital required to remain Basel III compliant an existing application was required to get upgradedthat will collect transform and calculate finance data and assign the appropriate capital calculation and risk weighting The Deutsche bank has enabledeStatement through email service from Business Credit Card Account The banks existing systems collect the account holder data like account details transaction details and other account related information which is extracted from various sources and stored into distributed file system It processes the data to generate the statement file which is utilized by email sending system to send out emails Approximately 7 Million estatements were required to be processed by the existing system Role and Responsibilities Handling raw data from various subsystems and load the data from different subsystems into to HDFS for further processing Used Flume to collect aggregate and store the web log data onto HDFS Used Sqoop to import data from Oracle database to HDFS Developed UDFs in Pig Scriptsfor datacleaning and preprocessing Used Hive to do analysis on the data and identify different correlations Development and maintenance of the HiveQL Pig Scripts andMapReduce Implemented performancetuning techniques in Hive Facilitating testing in different dimensions Coordinated and modified businesslogicas per the business requirements Used Crontab for automation of scripts Implemented POC for using Apache Impala for real time query processing over Hive Moved all the required data to NoSQL database Cassandra from HDFSfor visualization to generate reports for the BI team ENVIRONMENT Cloudera CDH 4 Hadoop 20 Spark Scala Pig 0110 Hive 0100 Sqoop 143 Flume 140 ZooKeeper 345 Cassandra Spark SparkQLCron JavaJ2EE Oracle 11 g WebLogic Server 11 g Senior Hadoop Developer Fifth Third Bank Cincinnati OH October 2015 to February 2017 Genre Platform rearchitecture and Risk Management Project DescriptionFifth Third Bank wants to provide risk managementservices for the entire corporate bank the current data platform is undergoing a rearchitecture in order to meet the changing data storage processing and maintainability requirements for risk management The primary objective of this project is to prove that a pivotal based solution will provide a central data platform for management and access to the various risk management datasets while meeting the throughput metrics for rapid data ingestion and processing of this data set with very fast turnaround results Themodule of this project was about decisionmaking churn analysis and sentiment analysis To achieve the project objective Hadoop platform DataStax Enterprise 47 and related tools were used Role and Responsibilities Coordinated with Administrators in setting up configuring initializing and troubleshooting DS Enterprise 47 Involved in loading data from UNIX file system to HDFS Involved in defining job flows and running data streaming jobs to process terabytes of text data Developed multiple MapReduce jobs in Java for data cleaning and preprocessing Wrote MapReduce jobs to discovertrends in data usage by users Involved in managing andreviewing Hadoop log files Installed and configured Hiveand written HiveQL scripts Involved in loading and transforming large sets of structured semi structured and unstructured data Involved in creating Hive tables loading data and writing Hive queries as per business requirements Implemented static partitioningdynamic partitioning and bucketing of data in Hive for improving the performance Supported Map Reduce programs those are running on the cluster Involved in writing both DML and DDL operations in NoSQL database Cassandra Responsible to manage data coming from different sources Implemented POC on writing programs in Scala using Spark Worked on migrating MapReduce programs into Spark using Scala Assisted the team in their developmentdeployment activities Used Web services concepts like SOAP to interact with other project within organization and for sharing information Involved in developing database access components using Spring DAO integrated with Hibernate for accessing the data Followed Hybrid Waterfall Scrumprinciples in developing the project ENVIRONMENTDataStax Enterprise 47 Linux Hadoop240 MapReduce Hive 0120 Pig 0101 Impala Hbase 0961 Sqoop 145 Flume 140 ZooKeeper 345 Cassandra 215 Spark 121 Spark Cassandra Connector 121 SparkQLSolr 410 SOAP Spring 41 Hibernate 43 Oracle 12c Hadoop Developer Comcast Philadelphia PA January 2015 to September 2015 Genre Marketing Research Project DescriptionThe idea of this project isdistributed batch processing infrastructure system based on Hadoop Distributed File System to access and query largescale data It is a high throughput realtime data collection and reporting system used by higher management decision makers in Marketing Research group to analyze future trends for their upcoming product development marketing campaigns and to enhance business continuity by anticipating changes in customer behavior Role and Responsibilities DevelopedMapReduce layers using Javato support SequenceandAvroformatted input data Used PigLatin scripts to process huge datasets in parallel for advancedtransformations Used HiveQL scripts and Hive User Defined Functionsto extract and analyze data over HBase input format Implemented partitioning andbucketing in Hive Used Sqoop data import and export scripts between various RDBMS structuredsources to HDFSand NoSQL database HBase Imported data usingFlume data transfer between various data sources to HDFS DevelopedOozieworkflows that chain HiveMapReduce modules for ingesting periodichourly input data Monitoring Hadoop scripts which take the input from HDFS and load the data into Hive Monitored Systemhealth logs and responds accordingly to any warning or failure conditions using Zookeeper ENVIRONMENT MapR 12 Hadoop 103 Hive 071 Hbase 0904 ZooKeeper 334 Pig 090 Sqoop 130 Oozie 300 Flume 094 Java Developer ICICI Bank Hyderabad Telangana February 2013 to December 2014 Genre Fund Transfer Integration Project Description FTI is fund transfer Integration which is an application enabling users to transfer fund from ownaccount to third party accounts The fund transfer can be to any account of same bank account transfer third party account transfer within same country and Cross border fund transfer FTI is used almost across 16 APAC countries Corporate Consumer segments both uses FTI FTI process starts with any fund transfer instruction feeds to it from Internal or external systems Internal Systems like BSI or FC whereas external instructions coming via SWIFT to CMG to FTI Role and Responsibilities Used Spring Framework for dependency injectionusingspring configuration files Developed the presentation layer using JSP HTML CSS and client validations using JavaScript Involved in the installation and configuration of Tomcat Server Involved in dynamic form generation auto completion of forms and user validation functionalities using AJAX Designed developed and maintained the data layer using Hibernate and performed configuration of Spring application framework Created stored procedures using PLSQL for data access layer Worked on tuning of backend Oracle stored procedures using TOAD Developed test cases for unit testing using JUnit Developed stored procedures to extract data from Oracle database ENVIRONMENT JavaJ2EE JSP Servlets Spring Framework Hibernate SQLPLSQL Web Services WSDL JUnit Tomcat Oracle 9i and Windows Java Developer Next Step Solutions Hyderabad Telangana January 2012 to January 2013 Genre Payroll System Project Description The goal of the project was to design an online payroll system which enabled organizations to maintain the payroll information of their employees The principle attributes of this payroll system included web based timesheets that recorded working hoursgraphical absence calendar system payroll generationand the ability to rerun the payroll for every single employee Another prominent feature was the provision to view historical payroll information via the historical pay slip production functionality Role and Responsibilities Involved in designing screen using HTML and CSS Used JavaScript to perform checking and validations at clientside Worked with SpringDependencyInjectionDI Involved in implementation of Spring Beans DAO Controller and Service layers for fast and efficient processing Implementedpersistence application layer usingObject Relational Model ORM Hibernate HQL and annotations Involved in writingHQL and SQL Queries for Oracle database Used log4j for logging messages Developed the classes for Unit Testing by using JUnit ENVIRONMENT JavaJ2EE Servlets JSP Spring MVC Hibernate Eclipse JavaScript AJAX XML Log4j Oracle 9i Web Logic Jr Java Developer Bank of India Hyderabad Telangana May 2009 to December 2011 Genre Bank Portal Project Description Bank of India BoI is an Indian stateowned commercial bank with headquarters in Mumbai Maharashtra India BoI is a founder member of Society for Worldwide Inter Bank Financial Telecommunications which facilitates provision of costeffective financial processing and communication services The goal of this project was to improve the UI design and improve the overall performance of the system Role and Responsibilities Worked with the frontend applications using HTML CSS and Java Script Developed the business components used in theJSPscreen Implemented DAO patterns for building the application Used JUnit for unit testing Involved in the design of JSP screens for the Public Provident Fund and Bond modules Participated in testing phases and provided UAT support Created jar files and deployed in WebLogicApplicationServer Involved in writing SQL queries to create tables and stored procedure to fulfill the requirements and accommodate the business rules in Oracle database ENVIRONMENT JavaJ2EE JDBC Servlets JSP HTML CSS JavaScript SQL PLSQL Web Logic 61 Eclipse Education Bachelors of Technology in Technology JNTU Hyderabad Telangana Skills JAVA 8 years ORACLE 8 years TESTING 6 years CSS 5 years HTML 5 years Cloudera CDH 4 Hadoop 20 Spark Scala Pig 0110 Hive 0100 Sqoop 143 Flume 140 ZooKeeper 345 Cassandra Spark SparkQLCronJavaJ2EE Oracle 11 g WebLogic Server 11 g 8 years Additional Information TECHNICAL SKILLS Big Data Technologies Apache Hadoop MapReduce HDFS Pig Hive HBase ZooKeeper Sqoop Flume Oozie HCatalog Hue Tez Avro YARN Ambari Spark Scala Solr Kafka Programming Languages Java Net C VbNet SQL HQLHiveQL CQL Scala Unix Shell Scripting Java Technologies Core Java Servlets JDBC JSP Spring Hibernate Web Technologies HTML 5 CSS 3 Java Script Bootstrap Servlets JSP ASP XML JSON WebApp Servers Tomcat 807060 WebLogic 12c11g10x WebSphere 8x 70 JBoss 6x and IIS 76 servers Databases RDBMSOracle 12c11 g SQL Server 201208 MySQL5x andNoSQLHBase Cassandra Operating Systems Windows Unix and Linux IDE EditPlus Eclipse NetBeans TOAD SQL Server Management Studio MySQL Workbench Visual Studio Version Control Git Subversion CVS Testing Technologies JUnit Reporting ETL Tools Tableau Informatica SSIS Pentaho",
    "extracted_keywords": [
        "Hadoop",
        "Developer",
        "Hadoop",
        "span",
        "lDeveloperspan",
        "Hadoop",
        "Developer",
        "Deutsche",
        "BankExchange",
        "Pl",
        "NJ",
        "Bloomfield",
        "NJ",
        "communications",
        "skills",
        "pressure",
        "team",
        "player",
        "problem",
        "skills",
        "ability",
        "environments",
        "technologies",
        "years",
        "IT",
        "experience",
        "analysis",
        "design",
        "development",
        "Hadoop",
        "Java",
        "J2EE",
        "years",
        "experience",
        "Hadoop",
        "HDFS",
        "MapReduce",
        "Hadoop",
        "Ecosystem",
        "Pig",
        "Hive",
        "HBase",
        "Hadoop",
        "Eco",
        "System",
        "Excellent",
        "understandingknowledge",
        "Hadoop",
        "architecture",
        "components",
        "HDFS",
        "JobTracker",
        "TaskTracker",
        "Name",
        "Node",
        "Data",
        "Node",
        "MapReduce",
        "programming",
        "paradigm",
        "Hadoop",
        "AdministrationCoordinated",
        "administrators",
        "configuring",
        "Hadoop",
        "ecosystem",
        "components",
        "Hadoop",
        "MapReduce",
        "HDFS",
        "HBase",
        "Oozie",
        "Sqoop",
        "Flume",
        "Pig",
        "Hive",
        "Spark",
        "Hadoop",
        "Enterprise",
        "Distributions",
        "Experience",
        "Hadoop",
        "distributions",
        "Cloudera",
        "CDH",
        "5x4x",
        "Hortonworks",
        "MapR",
        "1x",
        "DataStax",
        "4x",
        "Hadoop",
        "Tools",
        "Experience",
        "data",
        "HiveQL",
        "HBase",
        "Pig",
        "custom",
        "MapReduce",
        "programs",
        "Java",
        "Handson",
        "experience",
        "scripts",
        "dataanalysis",
        "Pig",
        "Hive",
        "Impala",
        "Experience",
        "migrating",
        "applications",
        "MapReduce",
        "Spark",
        "Scala",
        "Cassandra",
        "Developer",
        "Configuring",
        "Cassandra",
        "Cluster",
        "Expertise",
        "data",
        "modeling",
        "analysis",
        "Cassandra",
        "Cassandra",
        "Query",
        "Language",
        "HBase",
        "Data",
        "RDBMS",
        "Hive",
        "HBase",
        "requirements",
        "custom",
        "MapReduce",
        "HBase",
        "Java",
        "client",
        "API",
        "Data",
        "Ingestion",
        "Flume",
        "flow",
        "components",
        "data",
        "databases",
        "Sqoop",
        "Data",
        "Storage",
        "Experience",
        "storage",
        "likeHDFS",
        "Hbaseand",
        "Cassandra",
        "SQL",
        "NoSQL",
        "Experience",
        "data",
        "database",
        "NoSQL",
        "migration",
        "tools",
        "Sqoop",
        "Flume",
        "data",
        "Management",
        "Monitoring",
        "services",
        "System",
        "knowledge",
        "Kafka",
        "transfer",
        "systems",
        "Cloud",
        "PlatformsExperience",
        "cloud",
        "Hadoop",
        "likeOpenStack",
        "Amazon",
        "Web",
        "Services",
        "AWS",
        "Job",
        "Schedulers",
        "Experience",
        "scheduling",
        "jobs",
        "tools",
        "Apache",
        "Oozie",
        "Scripting",
        "Hive",
        "Pig",
        "shell",
        "scripting",
        "Java",
        "J2EE",
        "Experience",
        "applications",
        "JavaJ2E",
        "Developed",
        "web",
        "applications",
        "springframework",
        "scratch",
        "Object",
        "Relational",
        "Mapping",
        "ORM",
        "persistence",
        "technologies",
        "Hibernate",
        "Experiences",
        "annotation",
        "Spring",
        "andHibernateExtensive",
        "experience",
        "phases",
        "SDLC",
        "Experience",
        "design",
        "patterns",
        "Singleton",
        "pattern",
        "Factory",
        "pattern",
        "Data",
        "Access",
        "Objectspattern",
        "ModelViewController",
        "pattern",
        "WebApplication",
        "Servers",
        "Experience",
        "usage",
        "deployment",
        "applications",
        "web",
        "servers",
        "application",
        "servers",
        "Tomcat",
        "8x",
        "andIIS",
        "servers",
        "UI",
        "Design",
        "Comprehensive",
        "knowledge",
        "HTML5",
        "CSS3",
        "JavaScript",
        "Bootstrap",
        "Project",
        "Management",
        "Experience",
        "Agile",
        "andScrum",
        "project",
        "management",
        "US",
        "employer",
        "Work",
        "Experience",
        "Hadoop",
        "Developer",
        "Deutsche",
        "BankExchange",
        "Pl",
        "NJ",
        "March",
        "Present",
        "Genre",
        "Risk",
        "Management",
        "estatements",
        "Project",
        "Description",
        "Risk",
        "Based",
        "Capital",
        "group",
        "RBC",
        "reporting",
        "preparation",
        "calculation",
        "reporting",
        "capital",
        "components",
        "capital",
        "ratios",
        "Basel",
        "III",
        "standard",
        "banking",
        "regulators",
        "regulations",
        "capital",
        "banks",
        "types",
        "risks",
        "banks",
        "RBC",
        "capital",
        "components",
        "capital",
        "ratios",
        "Controller",
        "Treasurer",
        "Chief",
        "Financial",
        "Officer",
        "bank",
        "agencies",
        "order",
        "amount",
        "capital",
        "Basel",
        "III",
        "compliant",
        "application",
        "transform",
        "finance",
        "data",
        "capital",
        "calculation",
        "risk",
        "Deutsche",
        "bank",
        "email",
        "service",
        "Business",
        "Credit",
        "Card",
        "Account",
        "banks",
        "systems",
        "account",
        "holder",
        "data",
        "account",
        "details",
        "details",
        "account",
        "information",
        "sources",
        "file",
        "system",
        "data",
        "statement",
        "file",
        "email",
        "system",
        "emails",
        "estatements",
        "system",
        "Role",
        "Responsibilities",
        "data",
        "subsystems",
        "data",
        "subsystems",
        "HDFS",
        "processing",
        "Flume",
        "aggregate",
        "web",
        "log",
        "data",
        "HDFS",
        "Sqoop",
        "data",
        "Oracle",
        "database",
        "HDFS",
        "UDFs",
        "Pig",
        "Scriptsfor",
        "Hive",
        "analysis",
        "data",
        "correlations",
        "Development",
        "maintenance",
        "HiveQL",
        "Pig",
        "Scripts",
        "techniques",
        "Hive",
        "Facilitating",
        "testing",
        "dimensions",
        "businesslogicas",
        "business",
        "requirements",
        "Crontab",
        "automation",
        "scripts",
        "POC",
        "Apache",
        "Impala",
        "time",
        "query",
        "Hive",
        "Moved",
        "data",
        "NoSQL",
        "database",
        "Cassandra",
        "HDFSfor",
        "visualization",
        "reports",
        "BI",
        "team",
        "ENVIRONMENT",
        "Cloudera",
        "CDH",
        "Hadoop",
        "Spark",
        "Scala",
        "Pig",
        "Hive",
        "Sqoop",
        "143",
        "Flume",
        "ZooKeeper",
        "Cassandra",
        "Spark",
        "SparkQLCron",
        "JavaJ2EE",
        "Oracle",
        "g",
        "WebLogic",
        "Server",
        "g",
        "Senior",
        "Hadoop",
        "Developer",
        "Fifth",
        "Third",
        "Bank",
        "Cincinnati",
        "OH",
        "October",
        "February",
        "Genre",
        "Platform",
        "rearchitecture",
        "Risk",
        "Management",
        "Project",
        "DescriptionFifth",
        "Third",
        "Bank",
        "risk",
        "managementservices",
        "bank",
        "data",
        "platform",
        "rearchitecture",
        "order",
        "data",
        "storage",
        "processing",
        "maintainability",
        "requirements",
        "risk",
        "management",
        "objective",
        "project",
        "solution",
        "data",
        "platform",
        "management",
        "access",
        "risk",
        "management",
        "datasets",
        "throughput",
        "metrics",
        "data",
        "ingestion",
        "processing",
        "data",
        "turnaround",
        "results",
        "Themodule",
        "project",
        "churn",
        "analysis",
        "analysis",
        "project",
        "Hadoop",
        "platform",
        "DataStax",
        "Enterprise",
        "tools",
        "Role",
        "Responsibilities",
        "Administrators",
        "DS",
        "Enterprise",
        "loading",
        "data",
        "UNIX",
        "file",
        "system",
        "HDFS",
        "job",
        "flows",
        "data",
        "streaming",
        "jobs",
        "terabytes",
        "text",
        "data",
        "MapReduce",
        "jobs",
        "Java",
        "data",
        "cleaning",
        "Wrote",
        "MapReduce",
        "jobs",
        "discovertrends",
        "data",
        "usage",
        "users",
        "Hadoop",
        "log",
        "files",
        "Hiveand",
        "scripts",
        "loading",
        "sets",
        "data",
        "Hive",
        "tables",
        "loading",
        "data",
        "Hive",
        "queries",
        "business",
        "requirements",
        "partitioningdynamic",
        "partitioning",
        "bucketing",
        "data",
        "Hive",
        "performance",
        "Map",
        "Reduce",
        "programs",
        "cluster",
        "DML",
        "DDL",
        "operations",
        "NoSQL",
        "database",
        "Cassandra",
        "data",
        "sources",
        "POC",
        "programs",
        "Scala",
        "Spark",
        "Worked",
        "MapReduce",
        "programs",
        "Spark",
        "Scala",
        "Assisted",
        "team",
        "activities",
        "Web",
        "services",
        "concepts",
        "SOAP",
        "project",
        "organization",
        "information",
        "database",
        "access",
        "components",
        "Spring",
        "DAO",
        "Hibernate",
        "data",
        "Hybrid",
        "Waterfall",
        "Scrumprinciples",
        "project",
        "ENVIRONMENTDataStax",
        "Enterprise",
        "Linux",
        "Hadoop240",
        "MapReduce",
        "Hive",
        "Pig",
        "Impala",
        "Hbase",
        "Sqoop",
        "Flume",
        "ZooKeeper",
        "Cassandra",
        "Spark",
        "Spark",
        "Cassandra",
        "Connector",
        "SparkQLSolr",
        "SOAP",
        "Spring",
        "Hibernate",
        "Oracle",
        "12c",
        "Hadoop",
        "Developer",
        "Comcast",
        "Philadelphia",
        "PA",
        "January",
        "September",
        "Genre",
        "Marketing",
        "Research",
        "Project",
        "DescriptionThe",
        "idea",
        "project",
        "batch",
        "processing",
        "infrastructure",
        "system",
        "Hadoop",
        "Distributed",
        "File",
        "System",
        "query",
        "largescale",
        "data",
        "data",
        "collection",
        "reporting",
        "system",
        "management",
        "decision",
        "makers",
        "Marketing",
        "Research",
        "group",
        "trends",
        "product",
        "development",
        "marketing",
        "campaigns",
        "business",
        "continuity",
        "changes",
        "customer",
        "behavior",
        "Role",
        "Responsibilities",
        "layers",
        "Javato",
        "support",
        "SequenceandAvroformatted",
        "input",
        "data",
        "PigLatin",
        "scripts",
        "datasets",
        "parallel",
        "advancedtransformations",
        "scripts",
        "Hive",
        "User",
        "Defined",
        "Functionsto",
        "extract",
        "data",
        "HBase",
        "input",
        "format",
        "Hive",
        "Used",
        "Sqoop",
        "data",
        "import",
        "export",
        "scripts",
        "RDBMS",
        "structuredsources",
        "HDFSand",
        "NoSQL",
        "database",
        "HBase",
        "data",
        "data",
        "transfer",
        "data",
        "sources",
        "HDFS",
        "DevelopedOozieworkflows",
        "chain",
        "modules",
        "input",
        "data",
        "Monitoring",
        "Hadoop",
        "scripts",
        "input",
        "HDFS",
        "data",
        "Hive",
        "Monitored",
        "Systemhealth",
        "logs",
        "warning",
        "failure",
        "conditions",
        "Zookeeper",
        "ENVIRONMENT",
        "MapR",
        "Hadoop",
        "Hive",
        "071",
        "Hbase",
        "ZooKeeper",
        "Pig",
        "Sqoop",
        "Oozie",
        "Flume",
        "Java",
        "Developer",
        "ICICI",
        "Bank",
        "Hyderabad",
        "Telangana",
        "February",
        "December",
        "Genre",
        "Fund",
        "Transfer",
        "Integration",
        "Project",
        "Description",
        "FTI",
        "fund",
        "transfer",
        "Integration",
        "application",
        "users",
        "fund",
        "ownaccount",
        "party",
        "fund",
        "transfer",
        "account",
        "bank",
        "account",
        "transfer",
        "party",
        "account",
        "transfer",
        "country",
        "border",
        "fund",
        "transfer",
        "FTI",
        "APAC",
        "countries",
        "Corporate",
        "Consumer",
        "segments",
        "FTI",
        "FTI",
        "process",
        "fund",
        "transfer",
        "instruction",
        "systems",
        "Internal",
        "Systems",
        "BSI",
        "FC",
        "instructions",
        "SWIFT",
        "CMG",
        "FTI",
        "Role",
        "Responsibilities",
        "Spring",
        "Framework",
        "dependency",
        "configuration",
        "files",
        "presentation",
        "layer",
        "JSP",
        "HTML",
        "CSS",
        "client",
        "validations",
        "JavaScript",
        "installation",
        "configuration",
        "Tomcat",
        "Server",
        "form",
        "generation",
        "auto",
        "completion",
        "forms",
        "user",
        "validation",
        "functionalities",
        "AJAX",
        "data",
        "layer",
        "Hibernate",
        "configuration",
        "Spring",
        "application",
        "framework",
        "procedures",
        "PLSQL",
        "data",
        "access",
        "layer",
        "tuning",
        "Oracle",
        "procedures",
        "TOAD",
        "test",
        "cases",
        "unit",
        "testing",
        "JUnit",
        "Developed",
        "procedures",
        "data",
        "Oracle",
        "database",
        "ENVIRONMENT",
        "JavaJ2EE",
        "JSP",
        "Servlets",
        "Spring",
        "Framework",
        "Hibernate",
        "SQLPLSQL",
        "Web",
        "Services",
        "WSDL",
        "JUnit",
        "Tomcat",
        "Oracle",
        "9i",
        "Windows",
        "Java",
        "Developer",
        "Next",
        "Step",
        "Solutions",
        "Hyderabad",
        "Telangana",
        "January",
        "January",
        "Genre",
        "Payroll",
        "System",
        "Project",
        "Description",
        "goal",
        "project",
        "payroll",
        "system",
        "organizations",
        "payroll",
        "information",
        "employees",
        "attributes",
        "payroll",
        "system",
        "web",
        "timesheets",
        "absence",
        "calendar",
        "system",
        "payroll",
        "ability",
        "payroll",
        "employee",
        "feature",
        "provision",
        "payroll",
        "information",
        "pay",
        "slip",
        "production",
        "functionality",
        "Role",
        "Responsibilities",
        "screen",
        "HTML",
        "CSS",
        "JavaScript",
        "checking",
        "validations",
        "clientside",
        "implementation",
        "Spring",
        "Beans",
        "DAO",
        "Controller",
        "Service",
        "layers",
        "processing",
        "Implementedpersistence",
        "application",
        "layer",
        "Relational",
        "Model",
        "ORM",
        "Hibernate",
        "HQL",
        "annotations",
        "writingHQL",
        "SQL",
        "Queries",
        "Oracle",
        "database",
        "log4j",
        "messages",
        "classes",
        "Unit",
        "Testing",
        "JUnit",
        "ENVIRONMENT",
        "JavaJ2EE",
        "Servlets",
        "JSP",
        "Spring",
        "MVC",
        "Hibernate",
        "Eclipse",
        "JavaScript",
        "AJAX",
        "XML",
        "Log4j",
        "Oracle",
        "9i",
        "Web",
        "Logic",
        "Jr",
        "Java",
        "Developer",
        "Bank",
        "India",
        "Hyderabad",
        "Telangana",
        "May",
        "December",
        "Genre",
        "Bank",
        "Portal",
        "Project",
        "Description",
        "Bank",
        "India",
        "BoI",
        "bank",
        "headquarters",
        "Mumbai",
        "Maharashtra",
        "India",
        "BoI",
        "founder",
        "member",
        "Society",
        "Worldwide",
        "Inter",
        "Bank",
        "Financial",
        "Telecommunications",
        "provision",
        "processing",
        "communication",
        "services",
        "goal",
        "project",
        "UI",
        "design",
        "performance",
        "system",
        "Role",
        "Responsibilities",
        "frontend",
        "applications",
        "HTML",
        "CSS",
        "Java",
        "Script",
        "business",
        "components",
        "Implemented",
        "DAO",
        "patterns",
        "application",
        "JUnit",
        "unit",
        "testing",
        "design",
        "JSP",
        "screens",
        "Public",
        "Provident",
        "Fund",
        "Bond",
        "modules",
        "phases",
        "support",
        "jar",
        "files",
        "WebLogicApplicationServer",
        "SQL",
        "queries",
        "tables",
        "procedure",
        "requirements",
        "business",
        "rules",
        "Oracle",
        "database",
        "ENVIRONMENT",
        "JavaJ2EE",
        "JDBC",
        "Servlets",
        "JSP",
        "HTML",
        "CSS",
        "JavaScript",
        "SQL",
        "PLSQL",
        "Web",
        "Logic",
        "Eclipse",
        "Education",
        "Bachelors",
        "Technology",
        "Technology",
        "JNTU",
        "Hyderabad",
        "Telangana",
        "Skills",
        "JAVA",
        "years",
        "ORACLE",
        "years",
        "years",
        "CSS",
        "years",
        "HTML",
        "years",
        "Cloudera",
        "CDH",
        "Hadoop",
        "Spark",
        "Scala",
        "Pig",
        "Hive",
        "Sqoop",
        "143",
        "Flume",
        "ZooKeeper",
        "Cassandra",
        "Spark",
        "SparkQLCronJavaJ2EE",
        "Oracle",
        "g",
        "WebLogic",
        "Server",
        "g",
        "years",
        "Information",
        "TECHNICAL",
        "SKILLS",
        "Big",
        "Data",
        "Technologies",
        "Apache",
        "Hadoop",
        "MapReduce",
        "HDFS",
        "Pig",
        "Hive",
        "HBase",
        "ZooKeeper",
        "Sqoop",
        "Flume",
        "Oozie",
        "HCatalog",
        "Hue",
        "Tez",
        "Avro",
        "YARN",
        "Ambari",
        "Spark",
        "Scala",
        "Solr",
        "Kafka",
        "Programming",
        "Languages",
        "Java",
        "Net",
        "C",
        "VbNet",
        "SQL",
        "HQLHiveQL",
        "CQL",
        "Scala",
        "Unix",
        "Shell",
        "Scripting",
        "Java",
        "Technologies",
        "Core",
        "Java",
        "Servlets",
        "JDBC",
        "JSP",
        "Spring",
        "Hibernate",
        "Web",
        "Technologies",
        "HTML",
        "CSS",
        "Java",
        "Script",
        "Bootstrap",
        "Servlets",
        "JSP",
        "ASP",
        "XML",
        "JSON",
        "WebApp",
        "Servers",
        "Tomcat",
        "WebLogic",
        "WebSphere",
        "8x",
        "JBoss",
        "6x",
        "IIS",
        "servers",
        "RDBMSOracle",
        "g",
        "SQL",
        "Server",
        "MySQL5x",
        "andNoSQLHBase",
        "Cassandra",
        "Operating",
        "Systems",
        "Windows",
        "Unix",
        "Linux",
        "IDE",
        "EditPlus",
        "Eclipse",
        "NetBeans",
        "SQL",
        "Server",
        "Management",
        "Studio",
        "MySQL",
        "Workbench",
        "Visual",
        "Studio",
        "Version",
        "Control",
        "Git",
        "Subversion",
        "CVS",
        "Testing",
        "Technologies",
        "JUnit",
        "Reporting",
        "ETL",
        "Tools",
        "Tableau",
        "Informatica",
        "SSIS",
        "Pentaho"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T20:25:40.449137",
    "resume_data": "Hadoop Developer Hadoop span lDeveloperspan Hadoop Developer Deutsche BankExchange Pl NJ Bloomfield NJ Highly selfmotivatedgood technical communications and interpersonal skills Able to work reliably under pressure Committed team player with strong analytical and problem solving skills ability to quickly adapt to new environments technologies Over eight years of IT experience in analysis design and development using Hadoop Java and J2EE 4 years of experience with Hadoop HDFS MapReduce and Hadoop Ecosystem Pig Hive HBase Hadoop Eco System Excellent understandingknowledge of Hadoop architecture and various components such as HDFS JobTracker TaskTracker Name Node Data Node and MapReduce programming paradigm Hadoop AdministrationCoordinated with administrators in installing configuring and using Hadoop ecosystem components like Hadoop MapReduce HDFS HBase Oozie Sqoop Flume Pig Hive and Spark Hadoop Enterprise Distributions Experience in installing maintaining and upgrading Hadoop distributions like Cloudera CDH 5x4x Hortonworks 2x MapR 1x and DataStax 4x Hadoop Tools Experience in analyzing data using HiveQL HBase Pig and custom MapReduce programs in Java Handson experience in creating scripts for performing dataanalysis with Pig Hive and Impala Experience in migrating applications from MapReduce to Spark using Scala Cassandra Developer and Modeling Configuring and setting up Cassandra Cluster Expertise in data modeling and analysis of Cassandra and Cassandra Query Language HBase Ingested Data from RDBMS and Hive to HBase Based on the requirements implemented custom coded MapReduce for HBase and used the Java client API Data Ingestion Using Flume designed the flow and configured the individual components Efficiently transferred bulk data from and to traditional databases with Sqoop Data Storage Experience in maintaining distributed storage likeHDFS Hbaseand Cassandra SQL to NoSQL Experience in migrating data from traditional database to NoSQL As well included migration tools such as Sqoop and Flume while ingesting data Management and Monitoring Maintained and coordinated centralized services usingZookeeper Messaging System Good knowledge of Kafka for fast messaging transfer across systems Cloud PlatformsExperience in using cloud based Hadoop clusters likeOpenStack and Amazon Web Services AWS Job Schedulers Experience in scheduling jobs using tools like Apache Oozie Scripting Experienced in Hive Pig and shell scripting Java and J2EE Experience in developing applications using JavaJ2E Developed web applications in springframework from the scratch Worked with Object Relational Mapping ORM persistence technologies like Hibernate Experiences in working with annotation in Spring andHibernateExtensive experience and actively involved in various phases of SDLC Experience in design patterns like Singleton pattern Factory pattern Data Access Objectspattern and ModelViewController pattern WebApplication Servers Experience in the functional usage and deployment of applications on web servers or application servers likeApache Tomcat 807060WebLogic 12c11g10xWebSphere 8x 70 JBoss6x andIIS 76 servers UI Design Comprehensive knowledge in HTML5 CSS3 JavaScript and Bootstrap Project Management Experience in Agile andScrum project management Authorized to work in the US for any employer Work Experience Hadoop Developer Deutsche BankExchange Pl NJ March 2017 to Present Genre Risk Management and estatements Project Description The Risk Based Capital group RBC within regulatory reporting is responsible for the preparation calculation and reporting of capital components and capital ratios for Basel III an international standard that banking regulators can use when creating regulations about how much capital banks need to put aside to guard against the types of financial and operational risks banks face RBC reports the capital components and capital ratios to the Corporate Controller Treasurer and Chief Financial Officer of bank and the regulatory agencies In order to calculate the correct amount of capital required to remain Basel III compliant an existing application was required to get upgradedthat will collect transform and calculate finance data and assign the appropriate capital calculation and risk weighting The Deutsche bank has enabledeStatement through email service from Business Credit Card Account The banks existing systems collect the account holder data like account details transaction details and other account related information which is extracted from various sources and stored into distributed file system It processes the data to generate the statement file which is utilized by email sending system to send out emails Approximately 7 Million estatements were required to be processed by the existing system Role and Responsibilities Handling raw data from various subsystems and load the data from different subsystems into to HDFS for further processing Used Flume to collect aggregate and store the web log data onto HDFS Used Sqoop to import data from Oracle database to HDFS Developed UDFs in Pig Scriptsfor datacleaning and preprocessing Used Hive to do analysis on the data and identify different correlations Development and maintenance of the HiveQL Pig Scripts andMapReduce Implemented performancetuning techniques in Hive Facilitating testing in different dimensions Coordinated and modified businesslogicas per the business requirements Used Crontab for automation of scripts Implemented POC for using Apache Impala for real time query processing over Hive Moved all the required data to NoSQL database Cassandra from HDFSfor visualization to generate reports for the BI team ENVIRONMENT Cloudera CDH 4 Hadoop 20 Spark Scala Pig 0110 Hive 0100 Sqoop 143 Flume 140 ZooKeeper 345 Cassandra Spark SparkQLCron JavaJ2EE Oracle 11g WebLogic Server 11g Senior Hadoop Developer Fifth Third Bank Cincinnati OH October 2015 to February 2017 Genre Platform rearchitecture and Risk Management Project DescriptionFifth Third Bank wants to provide risk managementservices for the entire corporate bank the current data platform is undergoing a rearchitecture in order to meet the changing data storage processing and maintainability requirements for risk management The primary objective of this project is to prove that a pivotal based solution will provide a central data platform for management and access to the various risk management datasets while meeting the throughput metrics for rapid data ingestion and processing of this data set with very fast turnaround results Themodule of this project was about decisionmaking churn analysis and sentiment analysis To achieve the project objective Hadoop platform DataStax Enterprise 47 and related tools were used Role and Responsibilities Coordinated with Administrators in setting up configuring initializing and troubleshooting DS Enterprise 47 Involved in loading data from UNIX file system to HDFS Involved in defining job flows and running data streaming jobs to process terabytes of text data Developed multiple MapReduce jobs in Java for data cleaning and preprocessing Wrote MapReduce jobs to discovertrends in data usage by users Involved in managing andreviewing Hadoop log files Installed and configured Hiveand written HiveQL scripts Involved in loading and transforming large sets of structured semi structured and unstructured data Involved in creating Hive tables loading data and writing Hive queries as per business requirements Implemented static partitioningdynamic partitioning and bucketing of data in Hive for improving the performance Supported Map Reduce programs those are running on the cluster Involved in writing both DML and DDL operations in NoSQL database Cassandra Responsible to manage data coming from different sources Implemented POC on writing programs in Scala using Spark Worked on migrating MapReduce programs into Spark using Scala Assisted the team in their developmentdeployment activities Used Web services concepts like SOAP to interact with other project within organization and for sharing information Involved in developing database access components using Spring DAO integrated with Hibernate for accessing the data Followed Hybrid Waterfall Scrumprinciples in developing the project ENVIRONMENTDataStax Enterprise 47 Linux Hadoop240 MapReduce Hive 0120 Pig 0101 Impala Hbase 0961 Sqoop 145 Flume 140 ZooKeeper 345 Cassandra 215 Spark 121 Spark Cassandra Connector 121 SparkQLSolr 410 SOAP Spring 41 Hibernate 43 Oracle 12c Hadoop Developer Comcast Philadelphia PA January 2015 to September 2015 Genre Marketing Research Project DescriptionThe idea of this project isdistributed batch processing infrastructure system based on Hadoop Distributed File System to access and query largescale data It is a high throughput realtime data collection and reporting system used by higher management decision makers in Marketing Research group to analyze future trends for their upcoming product development marketing campaigns and to enhance business continuity by anticipating changes in customer behavior Role and Responsibilities DevelopedMapReduce layers using Javato support SequenceandAvroformatted input data Used PigLatin scripts to process huge datasets in parallel for advancedtransformations Used HiveQL scripts and Hive User Defined Functionsto extract and analyze data over HBase input format Implemented partitioning andbucketing in Hive Used Sqoop data import and export scripts between various RDBMS structuredsources to HDFSand NoSQL database HBase Imported data usingFlume data transfer between various data sources to HDFS DevelopedOozieworkflows that chain HiveMapReduce modules for ingesting periodichourly input data Monitoring Hadoop scripts which take the input from HDFS and load the data into Hive Monitored Systemhealth logs and responds accordingly to any warning or failure conditions using Zookeeper ENVIRONMENT MapR 12 Hadoop 103 Hive 071 Hbase 0904 ZooKeeper 334 Pig 090 Sqoop 130 Oozie 300 Flume 094 Java Developer ICICI Bank Hyderabad Telangana February 2013 to December 2014 Genre Fund Transfer Integration Project Description FTI is fund transfer Integration which is an application enabling users to transfer fund from ownaccount to third party accounts The fund transfer can be to any account of same bank account transfer third party account transfer within same country and Cross border fund transfer FTI is used almost across 16 APAC countries Corporate Consumer segments both uses FTI FTI process starts with any fund transfer instruction feeds to it from Internal or external systems Internal Systems like BSI or FC whereas external instructions coming via SWIFT to CMG to FTI Role and Responsibilities Used Spring Framework for dependency injectionusingspring configuration files Developed the presentation layer using JSP HTML CSS and client validations using JavaScript Involved in the installation and configuration of Tomcat Server Involved in dynamic form generation auto completion of forms and user validation functionalities using AJAX Designed developed and maintained the data layer using Hibernate and performed configuration of Spring application framework Created stored procedures using PLSQL for data access layer Worked on tuning of backend Oracle stored procedures using TOAD Developed test cases for unit testing using JUnit Developed stored procedures to extract data from Oracle database ENVIRONMENT JavaJ2EE JSP Servlets Spring Framework Hibernate SQLPLSQL Web Services WSDL JUnit Tomcat Oracle 9i and Windows Java Developer Next Step Solutions Hyderabad Telangana January 2012 to January 2013 Genre Payroll System Project Description The goal of the project was to design an online payroll system which enabled organizations to maintain the payroll information of their employees The principle attributes of this payroll system included web based timesheets that recorded working hoursgraphical absence calendar system payroll generationand the ability to rerun the payroll for every single employee Another prominent feature was the provision to view historical payroll information via the historical pay slip production functionality Role and Responsibilities Involved in designing screen using HTML and CSS Used JavaScript to perform checking and validations at clientside Worked with SpringDependencyInjectionDI Involved in implementation of Spring Beans DAO Controller and Service layers for fast and efficient processing Implementedpersistence application layer usingObject Relational Model ORM Hibernate HQL and annotations Involved in writingHQL and SQL Queries for Oracle database Used log4j for logging messages Developed the classes for Unit Testing by using JUnit ENVIRONMENT JavaJ2EE Servlets JSP Spring MVC Hibernate Eclipse JavaScript AJAX XML Log4j Oracle 9i Web Logic Jr Java Developer Bank of India Hyderabad Telangana May 2009 to December 2011 Genre Bank Portal Project Description Bank of India BoI is an Indian stateowned commercial bank with headquarters in Mumbai Maharashtra India BoI is a founder member of Society for Worldwide Inter Bank Financial Telecommunications which facilitates provision of costeffective financial processing and communication services The goal of this project was to improve the UI design and improve the overall performance of the system Role and Responsibilities Worked with the frontend applications using HTML CSS and Java Script Developed the business components used in theJSPscreen Implemented DAO patterns for building the application Used JUnit for unit testing Involved in the design of JSP screens for the Public Provident Fund and Bond modules Participated in testing phases and provided UAT support Created jar files and deployed in WebLogicApplicationServer Involved in writing SQL queries to create tables and stored procedure to fulfill the requirements and accommodate the business rules in Oracle database ENVIRONMENT JavaJ2EE JDBC Servlets JSP HTML CSS JavaScript SQL PLSQL Web Logic 61 Eclipse Education Bachelors of Technology in Technology JNTU Hyderabad Telangana Skills JAVA 8 years ORACLE 8 years TESTING 6 years CSS 5 years HTML 5 years Cloudera CDH 4 Hadoop 20 Spark Scala Pig 0110 Hive 0100 Sqoop 143 Flume 140 ZooKeeper 345 Cassandra Spark SparkQLCronJavaJ2EE Oracle 11g WebLogic Server 11g 8 years Additional Information TECHNICAL SKILLS Big Data Technologies Apache Hadoop MapReduce HDFS Pig Hive HBase ZooKeeper Sqoop Flume Oozie HCatalog Hue Tez Avro YARN Ambari Spark Scala Solr Kafka Programming Languages Java Net C VbNet SQL HQLHiveQL CQL Scala Unix Shell Scripting Java Technologies Core Java Servlets JDBC JSP Spring Hibernate Web Technologies HTML 5 CSS 3 Java Script Bootstrap Servlets JSP ASP XML JSON WebApp Servers Tomcat 807060 WebLogic 12c11g10x WebSphere 8x 70 JBoss 6x and IIS 76 servers Databases RDBMSOracle 12c11g SQL Server 201208 MySQL5x andNoSQLHBase Cassandra Operating Systems Windows Unix and Linux IDE EditPlus Eclipse NetBeans TOAD SQL Server Management Studio MySQL Workbench Visual Studio Version Control Git Subversion CVS Testing Technologies JUnit Reporting ETL Tools Tableau Informatica SSIS Pentaho",
    "unique_id": "0a07affe-8c05-4b30-8086-6debbc666e0d"
}