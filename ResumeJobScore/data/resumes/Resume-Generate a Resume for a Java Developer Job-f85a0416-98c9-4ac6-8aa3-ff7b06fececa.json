{
    "clean_data": "Sr Hadoop Spark Developer Sr Hadoop Spark span lDeveloperspan Sr Hadoop Spark Developer Citi Bank Jacksonville FL 9 years of total IT experience which includes Java Application Development Database Management on Big Data technologies using Hadoop Ecosystem 4 years of experience in Big Data technologies and Hadoop ecosystem components like Spark HDFS MapReduce Pig Hive YARN Sqoop Flume Kafka and NoSQL systems like HBase Cassandra Expertise in writing MapReduce Jobs in Java for processing large sets of structured semistructured and store them in HDFS Good Knowledge on Spark framework for batch and realtime data processing Proficient in developing data transformation and other analytical applications in Spark SparkSQL using Scala programming language Profound experience in creating real time data streaming solutions using Apache SparkSpark Streaming Kafka Performed importing and exporting data to from traditional RDBMS into HDFS and Hive using Sqoop Experience in developing Custom UDFs using Java to use in Pig and Hive Have a great understanding on concepts like Partitions Bucketing in Hive to optimize the performance and designed both Managed and External tables in Hive Experience in job workflow scheduling and monitoring tools like Oozie Airflow Proficient in designing and querying the NoSQL databases like HBase Cassandra Experience in designing and developing tables in HBase and storing aggregated data from Hive Table Worked with Java API Rest API to handle real time analytics on HBase data Experience on streaming data using Apache Kafka Storm and Flume to an extent Worked on various Hadoop distributions like Cloudera Hortonworks Hands on experience in installing configuring and using Hadoop components like Map Reduce MR HDFS HBase Hive Sqoop Pig and Flume Have worked on a 40 nodes live Hadoop cluster running on Cloudera CDH4 and CDH5 Familiar working on various formats of files like delimited text files click stream log files Apache log files Avro files JSON files XML Files Good understanding on compression techniques used in Hadoop processing like Gzip SNAPPY LZO Experience in writing build scripts using Maven ANT and Gradle Experienced with different scripting language like Python and shell scripts Good knowledge of BI Tools like Tableau and Plotly Worked on version control tools like BitBucket GIT SVN CVS Good experience in working with cloud environment like Amazon Web Services AWS EMR EC2 ES and S3 Proficient in creating data ingestion pipelines data transformations data management and real time streaming at an enterprise level Experienced in working with Machine learning librariessparkMLlib and implementing ML algorithms for clustering regression filtering and dimensional reduction Experienced in writing spark jobs for Data clustering and data processing using sparkMLlib and cluster algorithms as per functional requirements along with data scientists Involved in requirement analysis reviews and working sessions to understand the requirements and system design Experienced in using Agile methodologies including extreme programming Scrum Process and TestDriven Development TDD Good experience with SQL PLSQL and database concepts Good Experience as a Java Developer in Web ClientServer technologies using Java J2EE Servlets JSP EJB JDBC Have a good Knowledge working with mesos Used Scala to write the code for all the use cases in spark Proactive and well organized with effective time management skills and problemsolving skills Authorized to work in the US for any employer Work Experience Sr Hadoop Spark Developer Citi Bank Jacksonville FL January 2016 to Present Citi Bank is one of the leading banking sectors in America providing Banking Mortgage Credit Card services and many commercial financial services to its clients Scope of this project is to develop spark applications to access the data of their customers and developed several customer usage patterns Responsibilities Worked on cloud platform which was built with a scalable distributed data solution using Hadoop on a 40node cluster using AWS cloud to run analysis on 25 Terabytes of customer usage data Involved in creating end to end spark applications for various data transformation activities Performed series of ingestion jobs using Sqoop Kafka and custom Input adapter to move data from various sources to HDFS Developed a data pipeline using Kafka Spark and Hive to ingest transform and analyzing customer behavioral data Created Spark jobs to see trends in data usage by users Implemented Spark using Scala and utilizing Data frames and Spark SQL API for faster processing of data Used Spark for interactive queries processing of streaming data and integration with popular NoSQL database for huge volume of data Configured Kafka to read and write messages from external programs Converted Hive queries into Spark transformations using Spark RDDs Exploring with the Spark improving the performance and optimization of the existing algorithms in Hadoop using Spark Context SparkSQL Data Frame Pair RDDs Spark YARN Implemented various machine learning techniques like Random Forest KMeans Logistic Regression for predictions and pattern identification using SparkMLib Worked extensively with importing metadata into Hive using Scala and migrated existing tables and applications to work on AWS cloudS3 Data processing and hosting Real time data streaming using Spark with Kafka Collecting and aggregating large amounts of log data using Kafka and staging data in HDFS for further analysis Involved in performing the Linear Regression using Scala API and Spark Working on custom portions and configuration to cluster nodes with the Zookeeper Created HBase tables to store user data and Written automated HBase test cases for data quality check coming from different portfolios Data processing using SPARK Developed Hive scripts in spark SQL to denormalize and aggregating the data Working on NoSQL databases like HBase Cassandra for internal data storage and test validations Used Hive to analyze the partitioned and bucketed data and compute various metrics for reporting Developed Hive scripts in Hive QL to denormalize and aggregate the data Created HBase tables and column families to store the user event data Written automated HBase test cases for data quality checks using HBase command line tools Scheduled and executed workflows in Oozie to run Hive and Spark jobs Have a handson experience working with Airflow replaced the work of oozie Used to monitor and manage the Hadoop cluster using Cloudera Manager Developed interactive shell scripts for scheduling various data cleansing and data loading process Environment Hadoop Distribution of Cloudera AWS Service Clusters on cloud HDFS Map Reduce Sqoop Spark SparkSQL Hive HBase LINUX Java Scala Eclipse JIRA GIT Oracle Toad 96 Tableau UNIX Shell Scripting Hadoop Developer Liberty Mutual Insurance MA June 2014 to December 2015 Liberty Mutual Insurance Company is a 100 billion capital worth insurance company It is well known for its Auto Home and life insurance products in USA This company offers financial services fixed indexed and variable annuities and life insurance products through banks financial planners regional dealers and independent agents The BRP Solution Center provides services and project support for a wide variety of business partners and applications Responsibilities Understanding business needs analyzing functional specifications and map those to develop and designing end to end data transformation pipelines Created Hive Tables loaded data from Teradata using Sqoop Importing and exporting data into HDFS from Relational Databases and vice versa using Sqoop Worked extensively with importing metadata into Hive and migrated existing tables and applications to work on Hive and AWS cloud Developed MR jobs for cleaning validating and transforming the data Implemented Hive Generic UDFs to incorporate business logic into Hive Queries Extensively worked on HiveQL join operations writing custom UDFs and having good experience in optimizing Hive Queries Written Hive jobs to parse the logs and structure them in tabular format to facilitate effective querying on the log data Worked on debugging performance tuning PIG and HIVE scripts by understanding the joins group and aggregation between them Wrote Pig scripts to transform raw data from several data sources Experience of using different columnar file formats like RCFile Parquet and ORC formats Monitored workload job performance and capacity planning using Cloudera Manager Involved in build applications using Maven and integrated with Continuous Integration servers like Jenkins to build jobs Performing data migration from Legacy Databases RDBMS to HDFS using Sqoop Got good experience with NOSQL database HBase MongoDB and Hybrid implementations Hands on experience on whole ETL Extract Transformation Load process Worked with BI teams in generating the reports and designing ETL workflows on Tableau ETL development to normalize this data and publish it in IMPALA Used IMPALA to analyze data ingested into HBase and compute various metrics for reporting on the dashboard Designed and Maintained Oozie workflows to manage the flow of jobs in the cluster Worked with the testing teams to fix bugs and ensure smooth and errorfree code Involved in Agile methodologies daily Scrum meetings Sprint planning Environment Hadoop MapReduce HDFS Hive Java SQL Cloudera Manager Pig Sqoop Oozie HBase Zookeeper PLSQL MySQL DB2 Teradata Hadoop Developer Xerox Jersey City NJ May 2013 to June 2014 Xerox is the enterprise for business process and document management Its administration innovation and aptitude empower working environments from small organization to substantial worldwide undertaking improved the way work completes so they work more adequately The idea of Integrated Data Ware House Project is to ingest data from different multiple sources to Hadoop Data Lake perform transformations on it as per business requirements and exporting the data to external systems Responsibilities Involved in design low level design documents for functional and nonfunctional requirements Developed MapReduce programs in Java and Sqoop the data from ORACLE database Involved in writing complex MapReduce programs that work with different file formats like Text Sequence Xml and Avro Have solid understanding of REST architecture style and its application to well performing web sites for global usage Experienced in creating data model and implement queries to handle time series data with HBase data Integrated HBase with Map Reduce to move bulk amount of data into HBase Imported data using Sqoop to load data from Oracle to HDFS on regular basis Developed multiple MapReduce jobs in Pig for data cleaning and processing Developed Pig scripts and UDFs extensively for Value added Processing VAPS Used Avro Storage to use in Pig Latin to load and Store data Installed and configured Hive and wrote Hive UDFs to implement the business requirement Developed Pig scripts to convert the data from Avro to Text file format Worked on partitioning the HIVE table and running the scripts in parallel to reduce the run time of the scripts Written Hive queries for data analysis to meet the business requirements Creating Hive tables and working on them using HiveQL Developing Scripts and Batch Job to schedule various Hadoop Program using Oozie Importing and exporting data into HDFS from Oracle Database and vice versa using Sqoop Involved in preparation of docs like Functional Specification document and Deployment Instruction documents Fix defects as needed during the QA phase support QA testing troubleshoot defects and identify the source of defects Environment Hadoop Map Reduce HDFS Hive Pig HBase Linux XML Java Eclipse Oracle JIRA GIT Hub CDH4 Rest API Sr Java Developer Fifth Third Bank Charlotte NC November 2012 to April 2013 Online Account Opening OAO portal which will be used by applicants to open new Deposit Accounts Checking Saving Money Market And Customer Care representatives and branch personnel shall be provided access to allow them to complete an application on behalf of a customer as well as search for and finish incomplete applications Responsibilities Participated in requirement gathering and converting the requirements into technical specifications Developed UI using HTML JavaScript and JSP and developed Business Logic and Interfacing components using Business Objects XML and JDBC Designed userinterface and checking validations using JavaScript Involved in design of JSPs and Servlets for navigation among the modules Developed various EJBs for handling business logic and data manipulations from database Managed connectivity using JDBC for queryinginserting data management including triggers and stored procedures Developed SQL queries and Stored Procedures using PLSQL to retrieve and insert into multiple database schemas Developed the XML Schema and Web services for the data maintenance and structures Wrote test cases in JUnit for unit testing of classes Provided Technical support for production environments resolving the issues analysing the defects providing and implementing the solution defects Built and deployed Java applications into multiple UNIX based environments and produced both unit and functional test results along with release notes Developed the presentation layer using CSS and HTML taken from bootstrap to develop for browsers Environment Java Spring JSP Hibernate XML HTML JavaScript JDBC CSS SOAP Web services JIRA SVN Java Developer Magnaquest Hyderabad Telangana July 2010 to August 2012 Magnaquest Technologies Limited Magnaquest is an enterprise solutions and products company with offices in India Malaysia and USA and partners spread all over the globe The project is to build a retail applications related by using Java and J2EE Roles Responsibilities Assisted in designing and programming for the system which includes development of Process Flow Diagram Entity Relationship Diagram Data Flow Diagram and Database Design Designed front end components using JSF Involved in developing Java APIs which communicates with the Java Beans Implemented MVC architecture using Java Custom and JSTL tag libraries Involved in development of POJO classes and writing Hibernate query language HQL queries Implemented MVC architecture and DAO design pattern for maximum abstraction of the application and code reusability Created Stored Procedures using SQLPLSQL for data modification Used XML XSL for Data presentation Report generation and customer feedback documents Used Java Beans to automate the generation of Dynamic Reports and for customer transactions Developed JUnit test cases for regression testing and integrated with ANT build Implemented Logging framework using Log4J Involved in code review and documentation review of technical artifacts Environment Java Spring JSP Hibernate XML HTML JavaScript JDBC CSS SOAP Web services CVS JIRA Java Developer Apollo Microsystems Hyderabad Telangana June 2008 to June 2010 Developed the frontend featuring rich web interface using HTML45 CSS23 JavaScript jQuery and implemented the Responsive browser compatible tableless and w3c compliant standards Responsibilities Worked on gathering the requirements and for the development of the code for the entire project Designed UI using JSP pages HTML CSS Implemented the objectoriented concepts in programming Implemented validations using java script Implemented the application using JAVA and JDBC connectivity was used to integrate with the database Developed and deployed Enterprise Web Services SOAP and RESTFUL and generated client using Jersey and Axis Frameworks using Eclipse Developed the code which will create XML files and Flat files with the data retrieved from Databases and XML files Extensively used Core Java concepts like Multithreading Collections Framework File Io and concurrency Used Log4j for the logging the output to the files Utilize Struts MVC2 framework and developed JSP pages Action Servlets and XML based actionmapping files for web tier Environment Eclipse JIRA SVN version control MYSQL HTML CSS JAVASCRIPT JAVA JQuery MVC architecture Skills DATABASE 9 years JAVA 9 years SQL 6 years XML 5 years ECLIPSE 5 years Additional Information Technical Skills Big Data Ecosystem Hadoop HDFS Spark Hive Kafka Hue MapReduce YARN Pig Sqoop HBase Couchbase Cassandra Oozie Airflow Elastic Search Storm Flume Talend AWS Hortonworks and Cloudera distributions Programming Languages Scala Java C Unix Shell Scripting AngularJS PLSQL Python Java Tools Web Technologies J2EE JSF EJB HTML XHTML AngularJS Servlets JSP CSS XML Ajax Java script SOAP RESTful Database Couchbase Cassandra HBase Oracle 10g MySQL Teradata SQL Frameworks MVC Spring Struts JDBC Visualization Tableau Plotly Kibana MS Excel Development Tools Eclipse NetBeans ANT Maven and SBT",
    "entities": [
        "Implemented Spark",
        "Spark Context",
        "Amazon Web Services AWS EMR EC2 ES",
        "BI Tools",
        "BI",
        "Implemented Hive Generic",
        "SPARK Developed Hive",
        "IMPALA",
        "UNIX",
        "Spark HDFS MapReduce Pig Hive YARN",
        "America",
        "Partitions Bucketing",
        "Jersey",
        "the Java Beans Implemented",
        "Java Application Development Database Management on Big Data",
        "Hadoop Ecosystem",
        "Converted Hive",
        "Hadoop",
        "Sqoop Involved",
        "XML",
        "Integrated Data Ware House Project",
        "Telangana",
        "NOSQL",
        "Oozie Airflow Proficient",
        "Sr Hadoop Spark Developer Sr Hadoop Spark",
        "JUnit",
        "Multithreading Collections Framework File Io",
        "Gzip SNAPPY LZO Experience",
        "HBase",
        "Avro",
        "Hive Queries Written Hive",
        "S3 Proficient",
        "Functional Specification",
        "Zookeeper Created HBase",
        "SparkSQL",
        "Developed",
        "Skills",
        "Programming Languages Scala Java C",
        "Hadoop Program",
        "Provided Technical",
        "Created Hive Tables",
        "SQLPLSQL",
        "Responsibilities Involved",
        "Relational Databases",
        "Magnaquest Technologies Limited Magnaquest",
        "Custom",
        "ORACLE",
        "Hadoop Data Lake",
        "The BRP Solution Center",
        "JSP",
        "Airflow",
        "Hive Queries",
        "Built",
        "Present Citi Bank",
        "ORC",
        "Java J2EE Servlets JSP",
        "Spark SQL API",
        "USA",
        "Developed SQL",
        "Maven ANT",
        "Oracle Database",
        "MapReduce Jobs",
        "Random Forest KMeans Logistic Regression",
        "MVC",
        "Responsibilities Understanding",
        "Spark",
        "Agile",
        "Business Logic and Interfacing",
        "BitBucket GIT",
        "US",
        "Sqoop",
        "QA",
        "HIVE",
        "POJO",
        "Kibana MS Excel",
        "Liberty Mutual Insurance Company",
        "ETL Extract Transformation Load",
        "Process Flow Diagram Entity Relationship Diagram Data Flow Diagram",
        "AWS",
        "Oozie Importing",
        "Oracle",
        "PIG",
        "Developed JUnit",
        "log data",
        "Oozie",
        "RESTFUL",
        "SQL",
        "Malaysia",
        "HBase Imported",
        "Big Data",
        "Hive",
        "CDH5",
        "Profound",
        "Environment Hadoop MapReduce HDFS",
        "Store",
        "Designed UI",
        "Business Objects XML",
        "ETL",
        "FL",
        "India",
        "Work Experience Sr Hadoop Spark Developer Citi Bank",
        "Maven",
        "Performed",
        "Maintained Oozie",
        "Responsive",
        "ANT",
        "Spark Working",
        "SBT",
        "Cloudera AWS",
        "Continuous Integration",
        "ML",
        "CSS",
        "Auto Home",
        "Created HBase",
        "Developed MapReduce",
        "Shell Scripting Hadoop Developer Liberty Mutual Insurance",
        "Deployment Instruction",
        "the Linear Regression",
        "REST",
        "HTML CSS Implemented",
        "MapReduce",
        "NoSQL",
        "Tableau",
        "JSF Involved",
        "Created Spark",
        "Hive Table Worked",
        "NetBeans ANT Maven",
        "Teradata",
        "Value",
        "Additional Information Technical Skills Big Data Ecosystem Hadoop"
    ],
    "experience": "Experience in developing Custom UDFs using Java to use in Pig and Hive Have a great understanding on concepts like Partitions Bucketing in Hive to optimize the performance and designed both Managed and External tables in Hive Experience in job workflow scheduling and monitoring tools like Oozie Airflow Proficient in designing and querying the NoSQL databases like HBase Cassandra Experience in designing and developing tables in HBase and storing aggregated data from Hive Table Worked with Java API Rest API to handle real time analytics on HBase data Experience on streaming data using Apache Kafka Storm and Flume to an extent Worked on various Hadoop distributions like Cloudera Hortonworks Hands on experience in installing configuring and using Hadoop components like Map Reduce MR HDFS HBase Hive Sqoop Pig and Flume Have worked on a 40 nodes live Hadoop cluster running on Cloudera CDH4 and CDH5 Familiar working on various formats of files like delimited text files click stream log files Apache log files Avro files JSON files XML Files Good understanding on compression techniques used in Hadoop processing like Gzip SNAPPY LZO Experience in writing build scripts using Maven ANT and Gradle Experienced with different scripting language like Python and shell scripts Good knowledge of BI Tools like Tableau and Plotly Worked on version control tools like BitBucket GIT SVN CVS Good experience in working with cloud environment like Amazon Web Services AWS EMR EC2 ES and S3 Proficient in creating data ingestion pipelines data transformations data management and real time streaming at an enterprise level Experienced in working with Machine learning librariessparkMLlib and implementing ML algorithms for clustering regression filtering and dimensional reduction Experienced in writing spark jobs for Data clustering and data processing using sparkMLlib and cluster algorithms as per functional requirements along with data scientists Involved in requirement analysis reviews and working sessions to understand the requirements and system design Experienced in using Agile methodologies including extreme programming Scrum Process and TestDriven Development TDD Good experience with SQL PLSQL and database concepts Good Experience as a Java Developer in Web ClientServer technologies using Java J2EE Servlets JSP EJB JDBC Have a good Knowledge working with mesos Used Scala to write the code for all the use cases in spark Proactive and well organized with effective time management skills and problemsolving skills Authorized to work in the US for any employer Work Experience Sr Hadoop Spark Developer Citi Bank Jacksonville FL January 2016 to Present Citi Bank is one of the leading banking sectors in America providing Banking Mortgage Credit Card services and many commercial financial services to its clients Scope of this project is to develop spark applications to access the data of their customers and developed several customer usage patterns Responsibilities Worked on cloud platform which was built with a scalable distributed data solution using Hadoop on a 40node cluster using AWS cloud to run analysis on 25 Terabytes of customer usage data Involved in creating end to end spark applications for various data transformation activities Performed series of ingestion jobs using Sqoop Kafka and custom Input adapter to move data from various sources to HDFS Developed a data pipeline using Kafka Spark and Hive to ingest transform and analyzing customer behavioral data Created Spark jobs to see trends in data usage by users Implemented Spark using Scala and utilizing Data frames and Spark SQL API for faster processing of data Used Spark for interactive queries processing of streaming data and integration with popular NoSQL database for huge volume of data Configured Kafka to read and write messages from external programs Converted Hive queries into Spark transformations using Spark RDDs Exploring with the Spark improving the performance and optimization of the existing algorithms in Hadoop using Spark Context SparkSQL Data Frame Pair RDDs Spark YARN Implemented various machine learning techniques like Random Forest KMeans Logistic Regression for predictions and pattern identification using SparkMLib Worked extensively with importing metadata into Hive using Scala and migrated existing tables and applications to work on AWS cloudS3 Data processing and hosting Real time data streaming using Spark with Kafka Collecting and aggregating large amounts of log data using Kafka and staging data in HDFS for further analysis Involved in performing the Linear Regression using Scala API and Spark Working on custom portions and configuration to cluster nodes with the Zookeeper Created HBase tables to store user data and Written automated HBase test cases for data quality check coming from different portfolios Data processing using SPARK Developed Hive scripts in spark SQL to denormalize and aggregating the data Working on NoSQL databases like HBase Cassandra for internal data storage and test validations Used Hive to analyze the partitioned and bucketed data and compute various metrics for reporting Developed Hive scripts in Hive QL to denormalize and aggregate the data Created HBase tables and column families to store the user event data Written automated HBase test cases for data quality checks using HBase command line tools Scheduled and executed workflows in Oozie to run Hive and Spark jobs Have a handson experience working with Airflow replaced the work of oozie Used to monitor and manage the Hadoop cluster using Cloudera Manager Developed interactive shell scripts for scheduling various data cleansing and data loading process Environment Hadoop Distribution of Cloudera AWS Service Clusters on cloud HDFS Map Reduce Sqoop Spark SparkSQL Hive HBase LINUX Java Scala Eclipse JIRA GIT Oracle Toad 96 Tableau UNIX Shell Scripting Hadoop Developer Liberty Mutual Insurance MA June 2014 to December 2015 Liberty Mutual Insurance Company is a 100 billion capital worth insurance company It is well known for its Auto Home and life insurance products in USA This company offers financial services fixed indexed and variable annuities and life insurance products through banks financial planners regional dealers and independent agents The BRP Solution Center provides services and project support for a wide variety of business partners and applications Responsibilities Understanding business needs analyzing functional specifications and map those to develop and designing end to end data transformation pipelines Created Hive Tables loaded data from Teradata using Sqoop Importing and exporting data into HDFS from Relational Databases and vice versa using Sqoop Worked extensively with importing metadata into Hive and migrated existing tables and applications to work on Hive and AWS cloud Developed MR jobs for cleaning validating and transforming the data Implemented Hive Generic UDFs to incorporate business logic into Hive Queries Extensively worked on HiveQL join operations writing custom UDFs and having good experience in optimizing Hive Queries Written Hive jobs to parse the logs and structure them in tabular format to facilitate effective querying on the log data Worked on debugging performance tuning PIG and HIVE scripts by understanding the joins group and aggregation between them Wrote Pig scripts to transform raw data from several data sources Experience of using different columnar file formats like RCFile Parquet and ORC formats Monitored workload job performance and capacity planning using Cloudera Manager Involved in build applications using Maven and integrated with Continuous Integration servers like Jenkins to build jobs Performing data migration from Legacy Databases RDBMS to HDFS using Sqoop Got good experience with NOSQL database HBase MongoDB and Hybrid implementations Hands on experience on whole ETL Extract Transformation Load process Worked with BI teams in generating the reports and designing ETL workflows on Tableau ETL development to normalize this data and publish it in IMPALA Used IMPALA to analyze data ingested into HBase and compute various metrics for reporting on the dashboard Designed and Maintained Oozie workflows to manage the flow of jobs in the cluster Worked with the testing teams to fix bugs and ensure smooth and errorfree code Involved in Agile methodologies daily Scrum meetings Sprint planning Environment Hadoop MapReduce HDFS Hive Java SQL Cloudera Manager Pig Sqoop Oozie HBase Zookeeper PLSQL MySQL DB2 Teradata Hadoop Developer Xerox Jersey City NJ May 2013 to June 2014 Xerox is the enterprise for business process and document management Its administration innovation and aptitude empower working environments from small organization to substantial worldwide undertaking improved the way work completes so they work more adequately The idea of Integrated Data Ware House Project is to ingest data from different multiple sources to Hadoop Data Lake perform transformations on it as per business requirements and exporting the data to external systems Responsibilities Involved in design low level design documents for functional and nonfunctional requirements Developed MapReduce programs in Java and Sqoop the data from ORACLE database Involved in writing complex MapReduce programs that work with different file formats like Text Sequence Xml and Avro Have solid understanding of REST architecture style and its application to well performing web sites for global usage Experienced in creating data model and implement queries to handle time series data with HBase data Integrated HBase with Map Reduce to move bulk amount of data into HBase Imported data using Sqoop to load data from Oracle to HDFS on regular basis Developed multiple MapReduce jobs in Pig for data cleaning and processing Developed Pig scripts and UDFs extensively for Value added Processing VAPS Used Avro Storage to use in Pig Latin to load and Store data Installed and configured Hive and wrote Hive UDFs to implement the business requirement Developed Pig scripts to convert the data from Avro to Text file format Worked on partitioning the HIVE table and running the scripts in parallel to reduce the run time of the scripts Written Hive queries for data analysis to meet the business requirements Creating Hive tables and working on them using HiveQL Developing Scripts and Batch Job to schedule various Hadoop Program using Oozie Importing and exporting data into HDFS from Oracle Database and vice versa using Sqoop Involved in preparation of docs like Functional Specification document and Deployment Instruction documents Fix defects as needed during the QA phase support QA testing troubleshoot defects and identify the source of defects Environment Hadoop Map Reduce HDFS Hive Pig HBase Linux XML Java Eclipse Oracle JIRA GIT Hub CDH4 Rest API Sr Java Developer Fifth Third Bank Charlotte NC November 2012 to April 2013 Online Account Opening OAO portal which will be used by applicants to open new Deposit Accounts Checking Saving Money Market And Customer Care representatives and branch personnel shall be provided access to allow them to complete an application on behalf of a customer as well as search for and finish incomplete applications Responsibilities Participated in requirement gathering and converting the requirements into technical specifications Developed UI using HTML JavaScript and JSP and developed Business Logic and Interfacing components using Business Objects XML and JDBC Designed userinterface and checking validations using JavaScript Involved in design of JSPs and Servlets for navigation among the modules Developed various EJBs for handling business logic and data manipulations from database Managed connectivity using JDBC for queryinginserting data management including triggers and stored procedures Developed SQL queries and Stored Procedures using PLSQL to retrieve and insert into multiple database schemas Developed the XML Schema and Web services for the data maintenance and structures Wrote test cases in JUnit for unit testing of classes Provided Technical support for production environments resolving the issues analysing the defects providing and implementing the solution defects Built and deployed Java applications into multiple UNIX based environments and produced both unit and functional test results along with release notes Developed the presentation layer using CSS and HTML taken from bootstrap to develop for browsers Environment Java Spring JSP Hibernate XML HTML JavaScript JDBC CSS SOAP Web services JIRA SVN Java Developer Magnaquest Hyderabad Telangana July 2010 to August 2012 Magnaquest Technologies Limited Magnaquest is an enterprise solutions and products company with offices in India Malaysia and USA and partners spread all over the globe The project is to build a retail applications related by using Java and J2EE Roles Responsibilities Assisted in designing and programming for the system which includes development of Process Flow Diagram Entity Relationship Diagram Data Flow Diagram and Database Design Designed front end components using JSF Involved in developing Java APIs which communicates with the Java Beans Implemented MVC architecture using Java Custom and JSTL tag libraries Involved in development of POJO classes and writing Hibernate query language HQL queries Implemented MVC architecture and DAO design pattern for maximum abstraction of the application and code reusability Created Stored Procedures using SQLPLSQL for data modification Used XML XSL for Data presentation Report generation and customer feedback documents Used Java Beans to automate the generation of Dynamic Reports and for customer transactions Developed JUnit test cases for regression testing and integrated with ANT build Implemented Logging framework using Log4J Involved in code review and documentation review of technical artifacts Environment Java Spring JSP Hibernate XML HTML JavaScript JDBC CSS SOAP Web services CVS JIRA Java Developer Apollo Microsystems Hyderabad Telangana June 2008 to June 2010 Developed the frontend featuring rich web interface using HTML45 CSS23 JavaScript jQuery and implemented the Responsive browser compatible tableless and w3c compliant standards Responsibilities Worked on gathering the requirements and for the development of the code for the entire project Designed UI using JSP pages HTML CSS Implemented the objectoriented concepts in programming Implemented validations using java script Implemented the application using JAVA and JDBC connectivity was used to integrate with the database Developed and deployed Enterprise Web Services SOAP and RESTFUL and generated client using Jersey and Axis Frameworks using Eclipse Developed the code which will create XML files and Flat files with the data retrieved from Databases and XML files Extensively used Core Java concepts like Multithreading Collections Framework File Io and concurrency Used Log4j for the logging the output to the files Utilize Struts MVC2 framework and developed JSP pages Action Servlets and XML based actionmapping files for web tier Environment Eclipse JIRA SVN version control MYSQL HTML CSS JAVASCRIPT JAVA JQuery MVC architecture Skills DATABASE 9 years JAVA 9 years SQL 6 years XML 5 years ECLIPSE 5 years Additional Information Technical Skills Big Data Ecosystem Hadoop HDFS Spark Hive Kafka Hue MapReduce YARN Pig Sqoop HBase Couchbase Cassandra Oozie Airflow Elastic Search Storm Flume Talend AWS Hortonworks and Cloudera distributions Programming Languages Scala Java C Unix Shell Scripting AngularJS PLSQL Python Java Tools Web Technologies J2EE JSF EJB HTML XHTML AngularJS Servlets JSP CSS XML Ajax Java script SOAP RESTful Database Couchbase Cassandra HBase Oracle 10 g MySQL Teradata SQL Frameworks MVC Spring Struts JDBC Visualization Tableau Plotly Kibana MS Excel Development Tools Eclipse NetBeans ANT Maven and SBT",
    "extracted_keywords": [
        "Sr",
        "Hadoop",
        "Spark",
        "Developer",
        "Sr",
        "Hadoop",
        "Spark",
        "span",
        "lDeveloperspan",
        "Sr",
        "Hadoop",
        "Spark",
        "Developer",
        "Citi",
        "Bank",
        "Jacksonville",
        "FL",
        "years",
        "IT",
        "experience",
        "Java",
        "Application",
        "Development",
        "Database",
        "Management",
        "Big",
        "Data",
        "technologies",
        "Hadoop",
        "Ecosystem",
        "years",
        "experience",
        "Big",
        "Data",
        "technologies",
        "Hadoop",
        "ecosystem",
        "components",
        "Spark",
        "HDFS",
        "MapReduce",
        "Pig",
        "Hive",
        "YARN",
        "Sqoop",
        "Flume",
        "Kafka",
        "NoSQL",
        "systems",
        "HBase",
        "Cassandra",
        "Expertise",
        "MapReduce",
        "Jobs",
        "Java",
        "sets",
        "semistructured",
        "HDFS",
        "Good",
        "Knowledge",
        "Spark",
        "framework",
        "batch",
        "data",
        "Proficient",
        "data",
        "transformation",
        "applications",
        "Spark",
        "SparkSQL",
        "Scala",
        "programming",
        "language",
        "Profound",
        "experience",
        "time",
        "data",
        "streaming",
        "solutions",
        "Apache",
        "SparkSpark",
        "Streaming",
        "Kafka",
        "data",
        "RDBMS",
        "HDFS",
        "Hive",
        "Sqoop",
        "Experience",
        "Custom",
        "UDFs",
        "Java",
        "Pig",
        "Hive",
        "understanding",
        "concepts",
        "Partitions",
        "Bucketing",
        "Hive",
        "performance",
        "Managed",
        "tables",
        "Hive",
        "Experience",
        "job",
        "workflow",
        "scheduling",
        "monitoring",
        "tools",
        "Oozie",
        "Airflow",
        "Proficient",
        "NoSQL",
        "databases",
        "HBase",
        "Cassandra",
        "Experience",
        "tables",
        "HBase",
        "data",
        "Hive",
        "Table",
        "Java",
        "API",
        "Rest",
        "API",
        "time",
        "analytics",
        "HBase",
        "data",
        "Experience",
        "data",
        "Apache",
        "Kafka",
        "Storm",
        "Flume",
        "extent",
        "Hadoop",
        "distributions",
        "Cloudera",
        "Hortonworks",
        "Hands",
        "experience",
        "configuring",
        "Hadoop",
        "components",
        "Map",
        "MR",
        "HDFS",
        "HBase",
        "Hive",
        "Sqoop",
        "Pig",
        "Flume",
        "nodes",
        "Hadoop",
        "cluster",
        "Cloudera",
        "CDH4",
        "CDH5",
        "Familiar",
        "formats",
        "files",
        "text",
        "files",
        "stream",
        "log",
        "files",
        "Apache",
        "log",
        "Avro",
        "files",
        "XML",
        "Files",
        "understanding",
        "compression",
        "techniques",
        "Hadoop",
        "processing",
        "Gzip",
        "LZO",
        "Experience",
        "build",
        "scripts",
        "Maven",
        "ANT",
        "Gradle",
        "scripting",
        "language",
        "Python",
        "shell",
        "scripts",
        "knowledge",
        "BI",
        "Tools",
        "Tableau",
        "version",
        "control",
        "tools",
        "BitBucket",
        "GIT",
        "SVN",
        "CVS",
        "experience",
        "cloud",
        "environment",
        "Amazon",
        "Web",
        "Services",
        "AWS",
        "EMR",
        "EC2",
        "ES",
        "S3",
        "Proficient",
        "data",
        "ingestion",
        "pipelines",
        "data",
        "transformations",
        "data",
        "management",
        "time",
        "streaming",
        "enterprise",
        "level",
        "Machine",
        "learning",
        "ML",
        "algorithms",
        "regression",
        "filtering",
        "reduction",
        "spark",
        "jobs",
        "Data",
        "clustering",
        "data",
        "processing",
        "sparkMLlib",
        "cluster",
        "algorithms",
        "requirements",
        "data",
        "scientists",
        "requirement",
        "analysis",
        "reviews",
        "sessions",
        "requirements",
        "system",
        "design",
        "methodologies",
        "programming",
        "Scrum",
        "Process",
        "TestDriven",
        "Development",
        "TDD",
        "Good",
        "experience",
        "SQL",
        "PLSQL",
        "database",
        "Good",
        "Experience",
        "Java",
        "Developer",
        "Web",
        "ClientServer",
        "technologies",
        "Java",
        "J2EE",
        "Servlets",
        "JSP",
        "EJB",
        "JDBC",
        "Knowledge",
        "mesos",
        "Scala",
        "code",
        "use",
        "cases",
        "spark",
        "Proactive",
        "time",
        "management",
        "skills",
        "skills",
        "US",
        "employer",
        "Work",
        "Experience",
        "Sr",
        "Hadoop",
        "Spark",
        "Developer",
        "Citi",
        "Bank",
        "Jacksonville",
        "FL",
        "January",
        "Present",
        "Citi",
        "Bank",
        "banking",
        "sectors",
        "America",
        "Banking",
        "Mortgage",
        "Credit",
        "Card",
        "services",
        "services",
        "clients",
        "Scope",
        "project",
        "spark",
        "applications",
        "data",
        "customers",
        "customer",
        "usage",
        "patterns",
        "Responsibilities",
        "cloud",
        "platform",
        "data",
        "solution",
        "Hadoop",
        "40node",
        "cluster",
        "AWS",
        "cloud",
        "analysis",
        "Terabytes",
        "customer",
        "usage",
        "data",
        "end",
        "spark",
        "applications",
        "data",
        "transformation",
        "activities",
        "Performed",
        "series",
        "ingestion",
        "jobs",
        "Sqoop",
        "Kafka",
        "Input",
        "adapter",
        "data",
        "sources",
        "data",
        "pipeline",
        "Kafka",
        "Spark",
        "Hive",
        "transform",
        "customer",
        "data",
        "Spark",
        "jobs",
        "trends",
        "data",
        "usage",
        "users",
        "Spark",
        "Scala",
        "Data",
        "frames",
        "Spark",
        "SQL",
        "API",
        "processing",
        "data",
        "Spark",
        "queries",
        "processing",
        "data",
        "integration",
        "NoSQL",
        "database",
        "volume",
        "data",
        "Configured",
        "Kafka",
        "messages",
        "programs",
        "Hive",
        "Spark",
        "transformations",
        "Spark",
        "RDDs",
        "Spark",
        "performance",
        "optimization",
        "algorithms",
        "Hadoop",
        "Spark",
        "Context",
        "SparkSQL",
        "Data",
        "Frame",
        "Pair",
        "RDDs",
        "Spark",
        "YARN",
        "machine",
        "techniques",
        "Random",
        "Forest",
        "KMeans",
        "Logistic",
        "Regression",
        "predictions",
        "pattern",
        "identification",
        "SparkMLib",
        "metadata",
        "Hive",
        "Scala",
        "tables",
        "applications",
        "AWS",
        "Data",
        "processing",
        "time",
        "data",
        "streaming",
        "Spark",
        "Kafka",
        "Collecting",
        "amounts",
        "log",
        "data",
        "Kafka",
        "data",
        "HDFS",
        "analysis",
        "Linear",
        "Regression",
        "Scala",
        "API",
        "Spark",
        "Working",
        "custom",
        "portions",
        "configuration",
        "cluster",
        "nodes",
        "Zookeeper",
        "Created",
        "HBase",
        "user",
        "data",
        "Written",
        "HBase",
        "test",
        "cases",
        "data",
        "quality",
        "check",
        "portfolios",
        "Data",
        "processing",
        "SPARK",
        "Developed",
        "Hive",
        "scripts",
        "spark",
        "SQL",
        "data",
        "NoSQL",
        "databases",
        "HBase",
        "Cassandra",
        "data",
        "storage",
        "test",
        "validations",
        "Hive",
        "data",
        "metrics",
        "Developed",
        "Hive",
        "scripts",
        "Hive",
        "QL",
        "data",
        "Created",
        "HBase",
        "tables",
        "column",
        "families",
        "user",
        "event",
        "data",
        "HBase",
        "test",
        "cases",
        "data",
        "quality",
        "checks",
        "HBase",
        "command",
        "line",
        "tools",
        "workflows",
        "Oozie",
        "Hive",
        "Spark",
        "jobs",
        "handson",
        "experience",
        "Airflow",
        "work",
        "oozie",
        "Hadoop",
        "cluster",
        "Cloudera",
        "Manager",
        "shell",
        "scripts",
        "data",
        "cleansing",
        "data",
        "loading",
        "process",
        "Environment",
        "Hadoop",
        "Distribution",
        "Cloudera",
        "AWS",
        "Service",
        "Clusters",
        "cloud",
        "HDFS",
        "Map",
        "Reduce",
        "Sqoop",
        "Spark",
        "SparkSQL",
        "Hive",
        "HBase",
        "LINUX",
        "Java",
        "Scala",
        "Eclipse",
        "JIRA",
        "GIT",
        "Oracle",
        "Toad",
        "Tableau",
        "UNIX",
        "Shell",
        "Scripting",
        "Hadoop",
        "Developer",
        "Liberty",
        "Mutual",
        "Insurance",
        "MA",
        "June",
        "December",
        "Liberty",
        "Mutual",
        "Insurance",
        "Company",
        "capital",
        "insurance",
        "company",
        "Auto",
        "Home",
        "life",
        "insurance",
        "products",
        "USA",
        "company",
        "services",
        "annuities",
        "life",
        "insurance",
        "products",
        "banks",
        "planners",
        "dealers",
        "agents",
        "BRP",
        "Solution",
        "Center",
        "services",
        "project",
        "support",
        "variety",
        "business",
        "partners",
        "applications",
        "Responsibilities",
        "business",
        "needs",
        "specifications",
        "end",
        "data",
        "transformation",
        "pipelines",
        "Hive",
        "Tables",
        "data",
        "Teradata",
        "Sqoop",
        "Importing",
        "data",
        "HDFS",
        "Relational",
        "Databases",
        "Sqoop",
        "Worked",
        "metadata",
        "Hive",
        "tables",
        "applications",
        "Hive",
        "AWS",
        "MR",
        "jobs",
        "data",
        "Hive",
        "Generic",
        "UDFs",
        "business",
        "logic",
        "Hive",
        "Queries",
        "HiveQL",
        "join",
        "operations",
        "custom",
        "UDFs",
        "experience",
        "Hive",
        "Queries",
        "Written",
        "Hive",
        "jobs",
        "logs",
        "format",
        "querying",
        "log",
        "data",
        "performance",
        "PIG",
        "HIVE",
        "scripts",
        "joins",
        "group",
        "aggregation",
        "Wrote",
        "Pig",
        "scripts",
        "data",
        "data",
        "sources",
        "Experience",
        "file",
        "formats",
        "RCFile",
        "Parquet",
        "ORC",
        "formats",
        "workload",
        "job",
        "performance",
        "capacity",
        "planning",
        "Cloudera",
        "Manager",
        "build",
        "applications",
        "Maven",
        "Continuous",
        "Integration",
        "servers",
        "Jenkins",
        "jobs",
        "data",
        "migration",
        "Legacy",
        "Databases",
        "RDBMS",
        "HDFS",
        "Sqoop",
        "experience",
        "NOSQL",
        "database",
        "HBase",
        "MongoDB",
        "Hybrid",
        "implementations",
        "Hands",
        "experience",
        "ETL",
        "Extract",
        "Transformation",
        "Load",
        "process",
        "BI",
        "teams",
        "reports",
        "ETL",
        "workflows",
        "Tableau",
        "ETL",
        "development",
        "data",
        "IMPALA",
        "IMPALA",
        "data",
        "HBase",
        "metrics",
        "dashboard",
        "Oozie",
        "workflows",
        "flow",
        "jobs",
        "cluster",
        "testing",
        "teams",
        "bugs",
        "errorfree",
        "code",
        "methodologies",
        "Scrum",
        "meetings",
        "Sprint",
        "planning",
        "Environment",
        "Hadoop",
        "MapReduce",
        "HDFS",
        "Hive",
        "Java",
        "SQL",
        "Cloudera",
        "Manager",
        "Pig",
        "Sqoop",
        "Oozie",
        "HBase",
        "Zookeeper",
        "PLSQL",
        "MySQL",
        "DB2",
        "Teradata",
        "Hadoop",
        "Developer",
        "Xerox",
        "Jersey",
        "City",
        "NJ",
        "May",
        "June",
        "Xerox",
        "enterprise",
        "business",
        "process",
        "document",
        "management",
        "administration",
        "innovation",
        "empower",
        "environments",
        "organization",
        "undertaking",
        "way",
        "work",
        "idea",
        "Integrated",
        "Data",
        "Ware",
        "House",
        "Project",
        "data",
        "sources",
        "Hadoop",
        "Data",
        "Lake",
        "transformations",
        "business",
        "requirements",
        "data",
        "systems",
        "Responsibilities",
        "design",
        "level",
        "design",
        "documents",
        "requirements",
        "MapReduce",
        "programs",
        "Java",
        "Sqoop",
        "data",
        "database",
        "MapReduce",
        "programs",
        "file",
        "formats",
        "Text",
        "Sequence",
        "Xml",
        "Avro",
        "understanding",
        "REST",
        "architecture",
        "style",
        "application",
        "web",
        "sites",
        "usage",
        "data",
        "model",
        "queries",
        "time",
        "series",
        "data",
        "HBase",
        "data",
        "Integrated",
        "HBase",
        "Map",
        "Reduce",
        "amount",
        "data",
        "HBase",
        "data",
        "Sqoop",
        "data",
        "Oracle",
        "HDFS",
        "basis",
        "MapReduce",
        "jobs",
        "Pig",
        "data",
        "Developed",
        "Pig",
        "scripts",
        "UDFs",
        "Value",
        "Processing",
        "VAPS",
        "Avro",
        "Storage",
        "Pig",
        "Latin",
        "Store",
        "data",
        "Hive",
        "Hive",
        "UDFs",
        "business",
        "requirement",
        "Developed",
        "Pig",
        "scripts",
        "data",
        "Avro",
        "Text",
        "file",
        "format",
        "HIVE",
        "table",
        "scripts",
        "parallel",
        "time",
        "scripts",
        "Written",
        "Hive",
        "data",
        "analysis",
        "business",
        "requirements",
        "Hive",
        "tables",
        "HiveQL",
        "Developing",
        "Scripts",
        "Batch",
        "Job",
        "Hadoop",
        "Program",
        "Oozie",
        "Importing",
        "data",
        "HDFS",
        "Oracle",
        "Database",
        "vice",
        "Sqoop",
        "preparation",
        "docs",
        "Functional",
        "Specification",
        "document",
        "Deployment",
        "Instruction",
        "documents",
        "Fix",
        "defects",
        "QA",
        "phase",
        "support",
        "QA",
        "troubleshoot",
        "defects",
        "source",
        "defects",
        "Environment",
        "Hadoop",
        "Map",
        "HDFS",
        "Hive",
        "Pig",
        "HBase",
        "Linux",
        "XML",
        "Java",
        "Eclipse",
        "Oracle",
        "JIRA",
        "GIT",
        "Hub",
        "CDH4",
        "Rest",
        "API",
        "Sr",
        "Java",
        "Developer",
        "Fifth",
        "Third",
        "Bank",
        "Charlotte",
        "NC",
        "November",
        "April",
        "Online",
        "Account",
        "Opening",
        "OAO",
        "portal",
        "applicants",
        "Deposit",
        "Accounts",
        "Saving",
        "Money",
        "Market",
        "Customer",
        "Care",
        "representatives",
        "branch",
        "personnel",
        "access",
        "application",
        "behalf",
        "customer",
        "search",
        "applications",
        "Responsibilities",
        "requirement",
        "gathering",
        "requirements",
        "specifications",
        "UI",
        "HTML",
        "JavaScript",
        "JSP",
        "Business",
        "Logic",
        "components",
        "Business",
        "Objects",
        "XML",
        "JDBC",
        "userinterface",
        "validations",
        "JavaScript",
        "design",
        "JSPs",
        "Servlets",
        "navigation",
        "modules",
        "EJBs",
        "business",
        "logic",
        "data",
        "manipulations",
        "database",
        "Managed",
        "connectivity",
        "JDBC",
        "data",
        "management",
        "triggers",
        "procedures",
        "SQL",
        "queries",
        "Procedures",
        "PLSQL",
        "database",
        "schemas",
        "XML",
        "Schema",
        "Web",
        "services",
        "data",
        "maintenance",
        "structures",
        "Wrote",
        "test",
        "cases",
        "JUnit",
        "unit",
        "testing",
        "classes",
        "support",
        "production",
        "environments",
        "issues",
        "defects",
        "solution",
        "defects",
        "Java",
        "applications",
        "UNIX",
        "environments",
        "unit",
        "test",
        "results",
        "release",
        "notes",
        "presentation",
        "layer",
        "CSS",
        "HTML",
        "bootstrap",
        "browsers",
        "Environment",
        "Java",
        "Spring",
        "JSP",
        "Hibernate",
        "XML",
        "HTML",
        "JavaScript",
        "JDBC",
        "CSS",
        "SOAP",
        "Web",
        "services",
        "JIRA",
        "SVN",
        "Java",
        "Developer",
        "Magnaquest",
        "Hyderabad",
        "Telangana",
        "July",
        "August",
        "Magnaquest",
        "Technologies",
        "Limited",
        "Magnaquest",
        "enterprise",
        "solutions",
        "products",
        "company",
        "offices",
        "India",
        "Malaysia",
        "USA",
        "partners",
        "globe",
        "project",
        "applications",
        "Java",
        "J2EE",
        "Roles",
        "Responsibilities",
        "designing",
        "programming",
        "system",
        "development",
        "Process",
        "Flow",
        "Diagram",
        "Entity",
        "Relationship",
        "Diagram",
        "Data",
        "Flow",
        "Diagram",
        "Database",
        "Design",
        "end",
        "components",
        "JSF",
        "Java",
        "APIs",
        "Java",
        "Beans",
        "MVC",
        "architecture",
        "Java",
        "Custom",
        "JSTL",
        "tag",
        "libraries",
        "development",
        "POJO",
        "classes",
        "Hibernate",
        "query",
        "language",
        "HQL",
        "MVC",
        "architecture",
        "DAO",
        "design",
        "pattern",
        "abstraction",
        "application",
        "code",
        "reusability",
        "Stored",
        "Procedures",
        "SQLPLSQL",
        "data",
        "modification",
        "XML",
        "XSL",
        "Data",
        "presentation",
        "Report",
        "generation",
        "customer",
        "feedback",
        "documents",
        "Java",
        "Beans",
        "generation",
        "Dynamic",
        "Reports",
        "customer",
        "transactions",
        "JUnit",
        "test",
        "cases",
        "regression",
        "testing",
        "ANT",
        "Logging",
        "framework",
        "Log4J",
        "code",
        "review",
        "documentation",
        "review",
        "artifacts",
        "Environment",
        "Java",
        "Spring",
        "JSP",
        "Hibernate",
        "XML",
        "HTML",
        "JavaScript",
        "JDBC",
        "CSS",
        "SOAP",
        "Web",
        "services",
        "CVS",
        "JIRA",
        "Java",
        "Developer",
        "Apollo",
        "Microsystems",
        "Hyderabad",
        "Telangana",
        "June",
        "June",
        "frontend",
        "web",
        "interface",
        "HTML45",
        "CSS23",
        "JavaScript",
        "jQuery",
        "Responsive",
        "browser",
        "tableless",
        "w3c",
        "standards",
        "Responsibilities",
        "requirements",
        "development",
        "code",
        "project",
        "UI",
        "JSP",
        "pages",
        "HTML",
        "CSS",
        "concepts",
        "programming",
        "validations",
        "java",
        "script",
        "application",
        "JAVA",
        "JDBC",
        "connectivity",
        "database",
        "Enterprise",
        "Web",
        "Services",
        "SOAP",
        "RESTFUL",
        "client",
        "Jersey",
        "Axis",
        "Frameworks",
        "Eclipse",
        "code",
        "XML",
        "files",
        "files",
        "data",
        "Databases",
        "XML",
        "files",
        "Core",
        "Java",
        "concepts",
        "Multithreading",
        "Collections",
        "Framework",
        "File",
        "Io",
        "concurrency",
        "Log4j",
        "output",
        "files",
        "Utilize",
        "Struts",
        "MVC2",
        "framework",
        "JSP",
        "pages",
        "Action",
        "Servlets",
        "XML",
        "files",
        "web",
        "tier",
        "Environment",
        "Eclipse",
        "JIRA",
        "SVN",
        "version",
        "control",
        "MYSQL",
        "HTML",
        "CSS",
        "JAVASCRIPT",
        "JQuery",
        "MVC",
        "architecture",
        "Skills",
        "DATABASE",
        "years",
        "years",
        "SQL",
        "years",
        "XML",
        "years",
        "years",
        "Additional",
        "Information",
        "Technical",
        "Skills",
        "Big",
        "Data",
        "Ecosystem",
        "Hadoop",
        "HDFS",
        "Spark",
        "Hive",
        "Kafka",
        "Hue",
        "MapReduce",
        "YARN",
        "Pig",
        "Sqoop",
        "HBase",
        "Couchbase",
        "Cassandra",
        "Oozie",
        "Airflow",
        "Elastic",
        "Search",
        "Storm",
        "Flume",
        "Talend",
        "AWS",
        "Hortonworks",
        "Cloudera",
        "distributions",
        "Programming",
        "Languages",
        "Scala",
        "Java",
        "C",
        "Unix",
        "Shell",
        "Scripting",
        "PLSQL",
        "Python",
        "Java",
        "Tools",
        "Web",
        "Technologies",
        "J2EE",
        "JSF",
        "EJB",
        "HTML",
        "Servlets",
        "JSP",
        "CSS",
        "XML",
        "Ajax",
        "Java",
        "script",
        "SOAP",
        "RESTful",
        "Database",
        "Couchbase",
        "Cassandra",
        "HBase",
        "Oracle",
        "g",
        "MySQL",
        "Teradata",
        "SQL",
        "Frameworks",
        "MVC",
        "Spring",
        "Struts",
        "JDBC",
        "Visualization",
        "Tableau",
        "Kibana",
        "MS",
        "Excel",
        "Development",
        "Tools",
        "Eclipse",
        "NetBeans",
        "ANT",
        "Maven",
        "SBT"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T20:20:39.052495",
    "resume_data": "Sr Hadoop Spark Developer Sr Hadoop Spark span lDeveloperspan Sr Hadoop Spark Developer Citi Bank Jacksonville FL 9 years of total IT experience which includes Java Application Development Database Management on Big Data technologies using Hadoop Ecosystem 4 years of experience in Big Data technologies and Hadoop ecosystem components like Spark HDFS MapReduce Pig Hive YARN Sqoop Flume Kafka and NoSQL systems like HBase Cassandra Expertise in writing MapReduce Jobs in Java for processing large sets of structured semistructured and store them in HDFS Good Knowledge on Spark framework for batch and realtime data processing Proficient in developing data transformation and other analytical applications in Spark SparkSQL using Scala programming language Profound experience in creating real time data streaming solutions using Apache SparkSpark Streaming Kafka Performed importing and exporting data to from traditional RDBMS into HDFS and Hive using Sqoop Experience in developing Custom UDFs using Java to use in Pig and Hive Have a great understanding on concepts like Partitions Bucketing in Hive to optimize the performance and designed both Managed and External tables in Hive Experience in job workflow scheduling and monitoring tools like Oozie Airflow Proficient in designing and querying the NoSQL databases like HBase Cassandra Experience in designing and developing tables in HBase and storing aggregated data from Hive Table Worked with Java API Rest API to handle real time analytics on HBase data Experience on streaming data using Apache Kafka Storm and Flume to an extent Worked on various Hadoop distributions like Cloudera Hortonworks Hands on experience in installing configuring and using Hadoop components like Map Reduce MR HDFS HBase Hive Sqoop Pig and Flume Have worked on a 40 nodes live Hadoop cluster running on Cloudera CDH4 and CDH5 Familiar working on various formats of files like delimited text files click stream log files Apache log files Avro files JSON files XML Files Good understanding on compression techniques used in Hadoop processing like Gzip SNAPPY LZO Experience in writing build scripts using Maven ANT and Gradle Experienced with different scripting language like Python and shell scripts Good knowledge of BI Tools like Tableau and Plotly Worked on version control tools like BitBucket GIT SVN CVS Good experience in working with cloud environment like Amazon Web Services AWS EMR EC2 ES and S3 Proficient in creating data ingestion pipelines data transformations data management and real time streaming at an enterprise level Experienced in working with Machine learning librariessparkMLlib and implementing ML algorithms for clustering regression filtering and dimensional reduction Experienced in writing spark jobs for Data clustering and data processing using sparkMLlib and cluster algorithms as per functional requirements along with data scientists Involved in requirement analysis reviews and working sessions to understand the requirements and system design Experienced in using Agile methodologies including extreme programming Scrum Process and TestDriven Development TDD Good experience with SQL PLSQL and database concepts Good Experience as a Java Developer in Web ClientServer technologies using Java J2EE Servlets JSP EJB JDBC Have a good Knowledge working with mesos Used Scala to write the code for all the use cases in spark Proactive and well organized with effective time management skills and problemsolving skills Authorized to work in the US for any employer Work Experience Sr Hadoop Spark Developer Citi Bank Jacksonville FL January 2016 to Present Citi Bank is one of the leading banking sectors in America providing Banking Mortgage Credit Card services and many commercial financial services to its clients Scope of this project is to develop spark applications to access the data of their customers and developed several customer usage patterns Responsibilities Worked on cloud platform which was built with a scalable distributed data solution using Hadoop on a 40node cluster using AWS cloud to run analysis on 25 Terabytes of customer usage data Involved in creating end to end spark applications for various data transformation activities Performed series of ingestion jobs using Sqoop Kafka and custom Input adapter to move data from various sources to HDFS Developed a data pipeline using Kafka Spark and Hive to ingest transform and analyzing customer behavioral data Created Spark jobs to see trends in data usage by users Implemented Spark using Scala and utilizing Data frames and Spark SQL API for faster processing of data Used Spark for interactive queries processing of streaming data and integration with popular NoSQL database for huge volume of data Configured Kafka to read and write messages from external programs Converted Hive queries into Spark transformations using Spark RDDs Exploring with the Spark improving the performance and optimization of the existing algorithms in Hadoop using Spark Context SparkSQL Data Frame Pair RDDs Spark YARN Implemented various machine learning techniques like Random Forest KMeans Logistic Regression for predictions and pattern identification using SparkMLib Worked extensively with importing metadata into Hive using Scala and migrated existing tables and applications to work on AWS cloudS3 Data processing and hosting Real time data streaming using Spark with Kafka Collecting and aggregating large amounts of log data using Kafka and staging data in HDFS for further analysis Involved in performing the Linear Regression using Scala API and Spark Working on custom portions and configuration to cluster nodes with the Zookeeper Created HBase tables to store user data and Written automated HBase test cases for data quality check coming from different portfolios Data processing using SPARK Developed Hive scripts in spark SQL to denormalize and aggregating the data Working on NoSQL databases like HBase Cassandra for internal data storage and test validations Used Hive to analyze the partitioned and bucketed data and compute various metrics for reporting Developed Hive scripts in Hive QL to denormalize and aggregate the data Created HBase tables and column families to store the user event data Written automated HBase test cases for data quality checks using HBase command line tools Scheduled and executed workflows in Oozie to run Hive and Spark jobs Have a handson experience working with Airflow replaced the work of oozie Used to monitor and manage the Hadoop cluster using Cloudera Manager Developed interactive shell scripts for scheduling various data cleansing and data loading process Environment Hadoop Distribution of Cloudera AWS Service Clusters on cloud HDFS Map Reduce Sqoop Spark SparkSQL Hive HBase LINUX Java Scala Eclipse JIRA GIT Oracle Toad 96 Tableau UNIX Shell Scripting Hadoop Developer Liberty Mutual Insurance MA June 2014 to December 2015 Liberty Mutual Insurance Company is a 100 billion capital worth insurance company It is well known for its Auto Home and life insurance products in USA This company offers financial services fixed indexed and variable annuities and life insurance products through banks financial planners regional dealers and independent agents The BRP Solution Center provides services and project support for a wide variety of business partners and applications Responsibilities Understanding business needs analyzing functional specifications and map those to develop and designing end to end data transformation pipelines Created Hive Tables loaded data from Teradata using Sqoop Importing and exporting data into HDFS from Relational Databases and vice versa using Sqoop Worked extensively with importing metadata into Hive and migrated existing tables and applications to work on Hive and AWS cloud Developed MR jobs for cleaning validating and transforming the data Implemented Hive Generic UDFs to incorporate business logic into Hive Queries Extensively worked on HiveQL join operations writing custom UDFs and having good experience in optimizing Hive Queries Written Hive jobs to parse the logs and structure them in tabular format to facilitate effective querying on the log data Worked on debugging performance tuning PIG and HIVE scripts by understanding the joins group and aggregation between them Wrote Pig scripts to transform raw data from several data sources Experience of using different columnar file formats like RCFile Parquet and ORC formats Monitored workload job performance and capacity planning using Cloudera Manager Involved in build applications using Maven and integrated with Continuous Integration servers like Jenkins to build jobs Performing data migration from Legacy Databases RDBMS to HDFS using Sqoop Got good experience with NOSQL database HBase MongoDB and Hybrid implementations Hands on experience on whole ETL Extract Transformation Load process Worked with BI teams in generating the reports and designing ETL workflows on Tableau ETL development to normalize this data and publish it in IMPALA Used IMPALA to analyze data ingested into HBase and compute various metrics for reporting on the dashboard Designed and Maintained Oozie workflows to manage the flow of jobs in the cluster Worked with the testing teams to fix bugs and ensure smooth and errorfree code Involved in Agile methodologies daily Scrum meetings Sprint planning Environment Hadoop MapReduce HDFS Hive Java SQL Cloudera Manager Pig Sqoop Oozie HBase Zookeeper PLSQL MySQL DB2 Teradata Hadoop Developer Xerox Jersey City NJ May 2013 to June 2014 Xerox is the enterprise for business process and document management Its administration innovation and aptitude empower working environments from small organization to substantial worldwide undertaking improved the way work completes so they work more adequately The idea of Integrated Data Ware House Project is to ingest data from different multiple sources to Hadoop Data Lake perform transformations on it as per business requirements and exporting the data to external systems Responsibilities Involved in design low level design documents for functional and nonfunctional requirements Developed MapReduce programs in Java and Sqoop the data from ORACLE database Involved in writing complex MapReduce programs that work with different file formats like Text Sequence Xml and Avro Have solid understanding of REST architecture style and its application to well performing web sites for global usage Experienced in creating data model and implement queries to handle time series data with HBase data Integrated HBase with Map Reduce to move bulk amount of data into HBase Imported data using Sqoop to load data from Oracle to HDFS on regular basis Developed multiple MapReduce jobs in Pig for data cleaning and processing Developed Pig scripts and UDFs extensively for Value added Processing VAPS Used Avro Storage to use in Pig Latin to load and Store data Installed and configured Hive and wrote Hive UDFs to implement the business requirement Developed Pig scripts to convert the data from Avro to Text file format Worked on partitioning the HIVE table and running the scripts in parallel to reduce the run time of the scripts Written Hive queries for data analysis to meet the business requirements Creating Hive tables and working on them using HiveQL Developing Scripts and Batch Job to schedule various Hadoop Program using Oozie Importing and exporting data into HDFS from Oracle Database and vice versa using Sqoop Involved in preparation of docs like Functional Specification document and Deployment Instruction documents Fix defects as needed during the QA phase support QA testing troubleshoot defects and identify the source of defects Environment Hadoop Map Reduce HDFS Hive Pig HBase Linux XML Java Eclipse Oracle JIRA GIT Hub CDH4 Rest API Sr Java Developer Fifth Third Bank Charlotte NC November 2012 to April 2013 Online Account Opening OAO portal which will be used by applicants to open new Deposit Accounts Checking Saving Money Market And Customer Care representatives and branch personnel shall be provided access to allow them to complete an application on behalf of a customer as well as search for and finish incomplete applications Responsibilities Participated in requirement gathering and converting the requirements into technical specifications Developed UI using HTML JavaScript and JSP and developed Business Logic and Interfacing components using Business Objects XML and JDBC Designed userinterface and checking validations using JavaScript Involved in design of JSPs and Servlets for navigation among the modules Developed various EJBs for handling business logic and data manipulations from database Managed connectivity using JDBC for queryinginserting data management including triggers and stored procedures Developed SQL queries and Stored Procedures using PLSQL to retrieve and insert into multiple database schemas Developed the XML Schema and Web services for the data maintenance and structures Wrote test cases in JUnit for unit testing of classes Provided Technical support for production environments resolving the issues analysing the defects providing and implementing the solution defects Built and deployed Java applications into multiple UNIX based environments and produced both unit and functional test results along with release notes Developed the presentation layer using CSS and HTML taken from bootstrap to develop for browsers Environment Java Spring JSP Hibernate XML HTML JavaScript JDBC CSS SOAP Web services JIRA SVN Java Developer Magnaquest Hyderabad Telangana July 2010 to August 2012 Magnaquest Technologies Limited Magnaquest is an enterprise solutions and products company with offices in India Malaysia and USA and partners spread all over the globe The project is to build a retail applications related by using Java and J2EE Roles Responsibilities Assisted in designing and programming for the system which includes development of Process Flow Diagram Entity Relationship Diagram Data Flow Diagram and Database Design Designed front end components using JSF Involved in developing Java APIs which communicates with the Java Beans Implemented MVC architecture using Java Custom and JSTL tag libraries Involved in development of POJO classes and writing Hibernate query language HQL queries Implemented MVC architecture and DAO design pattern for maximum abstraction of the application and code reusability Created Stored Procedures using SQLPLSQL for data modification Used XML XSL for Data presentation Report generation and customer feedback documents Used Java Beans to automate the generation of Dynamic Reports and for customer transactions Developed JUnit test cases for regression testing and integrated with ANT build Implemented Logging framework using Log4J Involved in code review and documentation review of technical artifacts Environment Java Spring JSP Hibernate XML HTML JavaScript JDBC CSS SOAP Web services CVS JIRA Java Developer Apollo Microsystems Hyderabad Telangana June 2008 to June 2010 Developed the frontend featuring rich web interface using HTML45 CSS23 JavaScript jQuery and implemented the Responsive browser compatible tableless and w3c compliant standards Responsibilities Worked on gathering the requirements and for the development of the code for the entire project Designed UI using JSP pages HTML CSS Implemented the objectoriented concepts in programming Implemented validations using java script Implemented the application using JAVA and JDBC connectivity was used to integrate with the database Developed and deployed Enterprise Web Services SOAP and RESTFUL and generated client using Jersey and Axis Frameworks using Eclipse Developed the code which will create XML files and Flat files with the data retrieved from Databases and XML files Extensively used Core Java concepts like Multithreading Collections Framework File Io and concurrency Used Log4j for the logging the output to the files Utilize Struts MVC2 framework and developed JSP pages Action Servlets and XML based actionmapping files for web tier Environment Eclipse JIRA SVN version control MYSQL HTML CSS JAVASCRIPT JAVA JQuery MVC architecture Skills DATABASE 9 years JAVA 9 years SQL 6 years XML 5 years ECLIPSE 5 years Additional Information Technical Skills Big Data Ecosystem Hadoop HDFS Spark Hive Kafka Hue MapReduce YARN Pig Sqoop HBase Couchbase Cassandra Oozie Airflow Elastic Search Storm Flume Talend AWS Hortonworks and Cloudera distributions Programming Languages Scala Java C Unix Shell Scripting AngularJS PLSQL Python Java Tools Web Technologies J2EE JSF EJB HTML XHTML AngularJS Servlets JSP CSS XML Ajax Java script SOAP RESTful Database Couchbase Cassandra HBase Oracle 10g MySQL Teradata SQL Frameworks MVC Spring Struts JDBC Visualization Tableau Plotly Kibana MS Excel Development Tools Eclipse NetBeans ANT Maven and SBT",
    "unique_id": "f85a0416-98c9-4ac6-8aa3-ff7b06fececa"
}