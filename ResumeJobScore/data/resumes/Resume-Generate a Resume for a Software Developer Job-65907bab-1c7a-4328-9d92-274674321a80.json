{
    "clean_data": "Sr SCALA Developer Sr SCALA span lDeveloperspan Sr SCALA Developer Instacart Inc Palo Alto CA Authorized to work in the US for any employer Work Experience Sr SCALA Developer Instacart Inc San Francisco CA December 2016 to Present operates as a sameday grocery delivery service company The Company allows customers to select groceries through an online application platform from various retailers and have them delivered by personal shoppers Worked on combining a personal touch and cuttingedge technology to offer customers a simple solution to save time and eat fresh food from the most trusted grocery brands Developed Spark Applications by using Scala Java and Implemented Apache Spark data processing project to handle data from various RDBMS and Streaming sources Developed Spark Programs using Scala and Java APIs and performed transformations and actions on RDDs Involved in converting HiveSQL queries into Spark transformations using Spark RDD Scala and Python Develop ETL Process usingSPARK SCALA HIVE and HBASE Developed REST APIs using Scala Play framework and Akka Used ScalaTest for writing test cases and coordinated with QA team on end to end testing Developed REST APIs using Scala and Play framework to retrieve processed data from Cassandra database Developing UDFs in java for hive and pig and worked on reading multiple data formats on HDFS using Scala Involved in converting HiveSQL queries into Spark transformations using Spark RDDs and Scala Used Scala collection framework to store and process the complex consumer information Used Scala functional programming concepts to develop business logic Developed programs in JAVA ScalaSpark for data reformation after extraction from HDFS for analysis Developed Spark scripts by using Scala shell commands as per the requirement Processed the schema oriented and nonschemaoriented data using Scala and Spark Developed Scala scripts UDFFs using both Data framesSQLData sets and RDDMapReduce in Spark 16 for Data Aggregation queries and writing data back into OLTP system through Sqoop Provided architecture and design as product is migrated to Scala Play framework and Sencha UI Implemented applications with Scala along with Akka and Play framework Expert in implementing advanced procedures like text analytics and processing using the inmemory computing capabilities like Apache Spark written in Scala Auction web app calculated bids for energy auctions utilizing Scala JPA and Oracle Built KafkaSparkCassandra Scala simulator for MetiStream a big data consultancy KafkaSparkCassandra prototypes Developed a Restful API using Scala for tracking open source projects in Github and computing the inprocess metrics information for those projects Developed analytical components using Scala Spark Apache Mesos and Spark Stream Experience in using the Docker container system with the Kubernetes integration Developed a Web Application using Java with the Google Web Toolkit API with PostgreSql RedisCreating a dashboard using Flask Python libraries and AngularJS to visualize their progress Improve site performance by making better use of caches via MemCachedon Amazon Web Services Used R for prototype on a sample data exploration to identify the best algorithimic approach and then wrote scala scripts using spark machine learning module Developed MapReduceSpark Python modules for machine learning predictive analytics in Hadoop on AWS Implemented a Pythonbased distributed random forest via Python streaming Utilized Spark Scala Hadoop HBase Kafka Spark Streaming Caffe TensorFlow  Python a broad variety of machine learning methods including classifications regressions dimensionally reduction etc Excellent understanding knowledge of Hadoop architecture and various components such as HDFSHbase Hive Job Tracker Task Tracker Name Node Data Node and Map Reduce programming paradigm Built KafkaSparkCassandra ScalaFX simulator for MetiStream a big data consultancy KafkaSparkCassandra prototypes Designed a data analysis pipeline in Python using Amazon Web Services such as S3 EC2 and Elastic Map Reduce Implemented applications with Scala along with Akka and Play framework Expert in implementing advanced procedures like text analytics and processing using the inmemory computing capabilities like Apache Spark written in Scala Designing a highly scalable highly available minimum TCO for maximum ROI using big data components like kafkasparkcassandramongoDB and API It is python and scala based analytic system with ML Libraries Worked with NoSQL Platforms and Extensive understanding on relational databases versus NoSQL platforms Background Skills Scala Play Akka Java Python Kafka Scala Spark Hadoop Hive Big Data HBase HDFS Sqoop MapReduce Pig Docker PostgreSQL JSON Redis Memcache AWS Sr SCALA Developer JobNimbus American Fork UT January 2014 to November 2016 The project was to work on JobNimbus CRM Customer Relationship Management and Project Management software all wrapped into one JobNimbus goal was to offer clients the easiest way to manage projects contacts and tasks With detailed reporting and employee management toolsI have worked on customizing workflow and define the sequence of work With prebuilt custom forms users can create and share invoices Managed all their contacts at one central place Contacts can be imported from multiple sales channels including emails and phone calls that allows project managers to assign tasks to their teams monitor progress and send notifications to clients and stakeholders Involved in converting HiveSQL queries into Spark transformations using Spark RDDs and Scala Developed multiple POCs using Scala and deployed on the Yarn cluster compared the performance of Spark with Hive and SQLTeradata Analysed the SQL scripts and designed the solution to implement using Scala Developed analytical component using Scala Spark and Spark Stream Involved in performing the Linear Regression using Scala API and Spark Experience in developing and designing POCs using Scala and deployed on the Yarn cluster compared the performance of Spark with Hive and SQLTeradata Performed advanced procedures like text analytics and processing using the inmemory computing capabilities of Spark using Scala Created various Parser programs to extract data from Business Objects XML Informatica Java and database views using Scala Developed Spark code to using Scala and SparkSQL for faster processing and testing Designed scalable scala Web Architecture hosting reports for the entire application Wrote entities in Scala and Java along with named queries to interact with database Implementing a search microservice Scala REST PlayFramework ElasticSearch Designed a distributed system using Scala and the AKKA Actor Model that runs on multicore machines The server and clients are designed to share the work Server can work independent of a client Wrote a tool for extracting and reconciling trading data from Warehouses The tool is used for data consistency and data quality monitoring The tool is written using Java Scala Play and Akka Errors flagged by the tool are picked up in Splunk and Sitescope Wrote coding highly flexible scalable distributed applications using Scala Created and consuming RESTful Web services in Scala Play using AKKA Automated Compute Engine and Docker Image Builds with Jenkins Worked with various schema for Application Data Processing and Data warehouse that resides in AWS RDS database  Dynamo DB Interacted with DB sharding Redis Jenkins SOLR GraphQL Grafana Click Tracking for analytics Designed a persistent versus transient architecture raw Linux server with Spark ML algorithm jobs test Spark Worked in Big Data technologies Hadoop HDFS Map Reduce Kafka Sqoop Flume BigSQL Hive Pig Hbase and Apache Spark Spark ML MLlib as developer and analyst Wrote SparkML Map Reduce programs HiveQL and PigLatin scripts leading to good understanding in Map Reduce design patterns data analysis using Hive and Pig Utilized Apache Spark with Python to develop and execute Big Data Analytics and Machine learning applications executed machine Learning use cases under Spark ML and Mllib Developed different Statistical Machine Learning Data Mining solutions to various business problems and generating data visualizations using R SAS and Python and creating dashboards using tools like Tableau Explored MLlib algorithms in Spark to understand the possible Machine Learning functionalities that can be used for our use case Utilized Spark Scala Hadoop HBase Kafka Spark Streaming  Python a broad variety of machine learning methods including classifications regressions dimensionally reduction etc Worked on different data formats such as JSON XML and performed machine learning algorithms in R and used Spark for test data analytics using  and Analyzed the performance to identify bottlenecks Imported data using Sqoop to load data from MySQL to HDFS on regular basis Developed Scripts and Batch Job to schedule various Hadoop Program Used Spark API over Hortonworks Hadoop YARN to perform analytics on data in Hive Developed Kafka producer and consumers for message handling Used Amazon CLI for data transfers to and from Amazon S3 buckets Executed HadoopSpark jobs on AWS EMR using programs data stored in S3 Buckets Exploring with Spark improving performance and optimization of the existing algorithms in Hadoop MapReduce using Spark Context SparkSQL Data Frames Pair RDDs and Spark YARN Deployed MapReduce and Spark jobs on Amazon Elastic MapReduce using datasets stored on S3 Used Amazon Cloudwatch to monitor and track resources on AWS Knowledge of designing and deployment of Hadoop cluster and different Big Data analytic tools including Hive HBase Oozie Sqoop Flume Spark Impala Cassandra Responsible for developing data pipeline using Flume Sqoop and Pig to extract the data from weblogs and store in HDFS Involved in converting HiveSQL queries into Spark transformations using Spark RDDs Scala Involved in importing the realtime data to Hadoop using Kafka and implemented Oozie jobs for daily imports Automated the process for extraction of data from warehouses and weblogs by developing workflows and coordinator jobs in Oozie Background Skills Scala Akka Play Java Python Spark Hadoop Hive Big Data Hbase HDFS Oozie Hibernate Spring Angularjs Nodejs bootstrapjs backbonejs JSP Struts JDBC HTML CSS JQuery Javascript XML Sr ML Developer Homedepotcom Atlanta GA March 2011 to December 2013 Replatform project is a complete rewrite of the existing site that runs in a Broadvision environment This site allows customers to browse online and instore products add products to a shopping cart wish list or gift registry create a user account opt for email subscriptions place orders compare and search for products I have also worked on Tax Exempt application that consists of two main parts Registration and Maintenance Tax Exempt Registration is a customer facing application where customers can register for taxexempt status for their eligible taxexempt purchases Maintenance is being used by the Tax Department at the home depot to review search and maintain tax exemption reasons and other activities Worked in loading and analyzing large datasets with Hadoop framework MapReduce HDFS PIG HIVE Flume Sqoop SPARK Impala Scala NoSQL databases like MongoDB HBase Cassandra Involved in start to end process of Hadoop jobs that used various technologies such as Sqoop PIG Hive MapReduce Spark and Shellscripts for scheduling of few jobs extracted and loaded data into DataLake environment AmazonS3 by using Sqoop which was accessed by business users and data scientists Manage and support of enterprise Data Warehouse operation big data advanced predictive application development using Cloudera Hortonworks HDP Developed PIG scripts to transform the raw data into intelligent data as specified by business users Utilized Apache Spark with Python to develop and execute Big Data Analytics and Machine learning applications executed machine Learning use cases under Spark ML and Mllib Involved in designing and deployment of Hadoop cluster and different Big Data analytic tools including Pig Hive HBase Oozie ZooKeeper SQOOP flume Spark Impala and Cassandra with Horton work Distribution Installed Hadoop Map Reduce HDFS and AWS and developed multiple MapReduce jobs in PIG and Hive for data cleaning and preprocessing Developed SparkScala Python for regular expression regex project in the HadoopHive environment with LinuxWindows for big data resources Assisted in upgrading configuration and maintenance of various Hadoop infrastructures like Pig Hive and Hbase Used SparkAPI over Hortonworks Hadoop YARN to perform analytics on data in Hive Improved the performance and optimization of the existing algorithms in Hadoop using Spark Context SparkSQL Data Frame Pair RDDs Spark YARN Deployed this application which uses J2EE architecture model and Struts Framework first on Weblogic and helped in migrating to JBoss Application server Worked in Java J2EE XSL XML Oracle DB2 Struts spring Hibernate REST Web services Model driven architecture and software configuration management tools Developed Application based on J2EE using Hibernate Spring JSF frameworks and SOAPREST web services Web Sphere Integration Developer WID Tool to develop WPS components Used Spring Framework for Dependency injection and integrated with the EJB using annotations Responsible for analysis design development and integration of UI components with backend using J2EE technologies such as Servlets Java Beans and JSP Design and development of the exception management workflow using Oracle BPM Deployed the applications in Linux servers using deployment scripts Designed and developed programs in C to integrate as per the users requirements Createdtranslated PLI programming into SAS which were used as part of the process used to standardize military personnel records Created PLSQL stored procedures for new Oracle Forms and Reports development Background Skills Machine Learning MapReduce HDFS PIG SPARK Impala Scala Java J2EE Spring Struts JSF JSP EJB DOJO JQuery Sencha ExtJS JavaScript Java Developer WellsFargo San Francisco CA July 2009 to February 2011 Wells Fargo Company I have worked on its application that primarily helps the OperationalRiskCompliance team to report analyze operational loss events and their impacts across the bank operations globally Loss events are either grouped as linked losses or nonlinked losses according to the nature of the events Also worked on its features to record Root causes which resulted in the loss event and come up with Action plans to address such loss events The loss events are then associated with multiple Processes Risk Control measures based on the loss and insurance recovery amounts Integrated Hibernate ORM with SpringHibernate framework to facilitate DML and DQL queries and represent objectdatabase mapping Involved in transforming the Use Cases into Class Diagrams Sequence Diagrams and State diagrams Involved in development of Web Services creation of WSDL and schemas Extensively participated on working with Spring framework Involved in writing JSP and Servlets Involved in development of Web Services were developed to receive client requests Implemented Spring JDBC template Spring Exception Strategy and AOP Involved in setting up WebSphere Application server and using Ant tool to build the application and deploy the application in WebSphere Application server Worked with the creation of Store Procedures Involved in writing SQL queries Stored Procedures to accomplish complex functionalities Part of team creating quality working J2EE code to design schedule and cost to implement use cases Developed Reusable classes in the middleware using Hibernate Involved in writing lots of JSP for maintains and enhancements of the application Worked on Front End using Servlets and also backend using EJB and Hibernate Worked on Presentation Layer using Struts Tiles JSPs and Servlets Created quality working J2EE code to design schedule and cost to implement use cases Setting of DB2 build settings in RAD application development server Involved in writing the database integration code using Hibernate Creation of managed server and JDBC connections Worked on the application using Rational Application Developer Designed Developed Application flow UML diagrams of the application using Rational Rose Background Skills J2ee Java Jsp Servlet Jdbc Struts Junit Log4j Javascript Websphere Application Server Axis Wsad Xml Xslt Ant Sql Sql Query Analyzer Jprobe Cvs Opprox Reports Windowsxp UnixIbm Aix Java Developer OxfordHealthcare Broken Arrow OK February 2007 to June 2009 OxfordHealthcare The Project was to enhance and integrate Commercial off the shelf COTS web based Electronic Health Record EHR NextGen Software system and provide a better approach to documentation than conventional paperbased system The project involved building a webbased application and was created to provide online access to the components of disease management programs and staging process Provides direction necessary to clinical and support staff to improve financial revenue performance and the overall patient experience Worked with Spring Batch Used Spring ORM module to integrate with Hibernate Developed the web pages using JSP CSS and HTML Developed the RTM interface module to map the requirements to the testcase and Test design modules Used several J2EE Design Patterns Session Faade Aggregate Entity for the Middle Tier development Developed EJBS Session and MessageDriven Beans in RAD for handling business processing database access and asynchronous messaging Made extensive use of Java Naming and Directory Interface JNDI for looking up enterprise beans Developed MessageDriven beans in collaboration with Java Messaging Service JMS Involved in writing JSPHTMLJavaScript and Servlets to generate dynamic web pages and web content Wrote various stored procedures in PLSQL and JDBC routines to update tables Wrote various SQL queries for data retrieval using JDBC Involved in building and parsing XML documents using SAX parser Exposed business logic as a web service and developed WSDL files for describing these web services Extensively used SOAP formatted messages for communication between web services Developed the application on IBM WebSphere Application Server Developed the plugin interfaces for the TMS features TEE Requirements Version Control Developed Form Beans which are used to store data when the user submits the HTML form Coded various Java beans to implement the business logic Development of GUI using AWT Involved in creating the tables using SQL and connectivity is done by JDBC Involved in generating the reports regarding the marks they secured in the online test once they press the submit button in the test using HTML and JSP Apache Tomcat is used as an Application Server Background Skills J2ee Java Jsp Servlet Jdbc Struts Junit Log4j Javascript Dhtml Websphere Application Server Axis Xml Xslt Ant Sql Opprox Reports Windowsxp Education Bachelors Skills Apache hadoop hdfs 8 years Apache hbase 8 years Aws 8 years Hadoop 8 years Java 10 years Scala 6 years Play 4 years Python 6 years Kafka 3 years Spark 4 years Hive 4 years BigData 5 years Hbase 4 years HDFS 3 years Sqoop 4 years MapReduce 4 years Pig 4 years Docker 3 years PostgreSQL 4 years JSON 3 years Redis 5 years AWS 6 years MapR 3 years MongoDB 4 years Spark 3 years Scala R 3 years Redis 4 years HDFS 3 years Apache Mesos 3 years PMML 4 years Spring Security 2 years Spring Batch 4 years Solaris 4 years AWS EMR 4 years Azure 3 years AWS Aurora 3 years Apache mesos 4 years Spring batch 4 years Spring Security 3 years SpringNet 3 years S3 2 years Microservices 4 years Scala 3 years Akka 3 years Puppet 3 years Chef 3 years Ansible 2 years AspNet 3 years Mongodb 3 years AkkaHttp 3 years AWS EMR 3 years Kafka 2 years Python 3 years AspNet 3 years Additional Information Areas of Expertise include Scala Akka HiveBig DataHbase Java Play AWS Spark Hadoop HDFS",
    "entities": [
        "Statistical Machine Learning Data Mining",
        "Spark Context",
        "JobNimbus",
        "RTM",
        "WPS",
        "WebSphere Application",
        "Hortonworks Hadoop",
        "Cassandra",
        "HDFS",
        "Electronic Health Record EHR",
        "Developed Spark Applications",
        "Broadvision",
        "Developed Reusable",
        "Spark Stream Involved",
        "Distribution Installed Hadoop",
        "NoSQL Platforms",
        "HadoopSpark",
        "AWS EMR",
        "Hadoop",
        "HDFS Involved",
        "XML",
        "Atlanta",
        "Scala Created",
        "Business Objects XML Informatica Java",
        "Integrated Hibernate",
        "Amazon CLI",
        "Amazon Elastic MapReduce",
        "State",
        "Automated",
        "Developed Spark Programs",
        "Implemented Apache Spark",
        "Apache Spark",
        "Amazon",
        "Spark ML",
        "PigLatin",
        "AWT Involved",
        "Rational Rose Background Skills",
        "Used Spring Framework for Dependency",
        "JSP Apache Tomcat",
        "Assisted",
        "JSP Design",
        "SparkSQL",
        "Hibernate Involved",
        "Data Warehouse",
        "COTS",
        "Oracle Forms and Reports",
        "Node Data",
        "Hadoop Program",
        "Scala Developed",
        "HDP Developed",
        "San Francisco",
        "RAD",
        "Hadoop MapReduce",
        "UML",
        "HTML Developed",
        "Wells Fargo Company",
        "Servlets",
        "ML Libraries Worked",
        "DQL",
        "Spark Stream",
        "JBoss Application",
        "Processed",
        "Linux",
        "JSP",
        "Hibernate Developed",
        "Warehouses",
        "HDFSHbase Hive Job Tracker Task Tracker",
        "MemCachedon Amazon Web Services",
        "Servlets Created",
        "Utilized Apache Spark",
        "Sr SCALA Developer Sr SCALA",
        "MetiStream",
        "Flask Python",
        "Utilized Spark",
        "RDS",
        "Flume Sqoop",
        "Mllib Involved",
        "Oracle BPM Deployed",
        "Spark",
        "EJB",
        "Rational Application Developer Designed Developed Application",
        "TMS",
        "Hibernate Worked on Presentation Layer",
        "the Tax Department",
        "US",
        "Scala Involved",
        "Customer Relationship Management",
        "QA",
        "Sqoop",
        "LinuxWindows",
        "Skills Apache",
        "Background Skills Machine Learning MapReduce HDFS PIG SPARK",
        "Created",
        "the AKKA Actor Model",
        "Scala",
        "Server",
        "Data Aggregation",
        "AWS",
        "Implemented",
        "Oracle",
        "Coded",
        "Project Management",
        "Big Data Analytics",
        "Apache",
        "PIG",
        "Development of GUI",
        "SAX",
        "HTML",
        "SAS",
        "Scala Spark",
        "Test",
        "Sr SCALA",
        "Servlets Involved",
        "Hive HBase Oozie",
        "SQL",
        "OLTP",
        "Amazon Web Services",
        "toolsI",
        "AKKA",
        "Oozie Background Skills",
        "DML",
        "Tax Exempt",
        "Kubernetes",
        "Github",
        "Spark Impala",
        "Akka Errors",
        "TEE Requirements Version",
        "Big Data",
        "Hive",
        "Tableau Explored MLlib",
        "Java Messaging Service JMS Involved",
        "Hive Improved",
        "IBM WebSphere Application",
        "PostgreSql RedisCreating",
        "Instacart Inc",
        "Python Develop ETL",
        "Developed Application",
        "Front End",
        "UI",
        "Splunk and Sitescope",
        "J2EE using Hibernate Spring JSF",
        "Java J2EE XSL XML Oracle",
        "Expertise",
        "SpringHibernate",
        "AWS Knowledge",
        "Weblogic",
        "Scala Developed Spark",
        "the Linear Regression",
        "MapReduce",
        "SCALA",
        "NoSQL",
        "Processes Risk Control",
        "Web Sphere Integration Developer WID Tool",
        "Application Data Processing and Data",
        "SOAPREST",
        "JSON XML"
    ],
    "experience": "Experience Sr SCALA Developer Instacart Inc San Francisco CA December 2016 to Present operates as a sameday grocery delivery service company The Company allows customers to select groceries through an online application platform from various retailers and have them delivered by personal shoppers Worked on combining a personal touch and cuttingedge technology to offer customers a simple solution to save time and eat fresh food from the most trusted grocery brands Developed Spark Applications by using Scala Java and Implemented Apache Spark data processing project to handle data from various RDBMS and Streaming sources Developed Spark Programs using Scala and Java APIs and performed transformations and actions on RDDs Involved in converting HiveSQL queries into Spark transformations using Spark RDD Scala and Python Develop ETL Process usingSPARK SCALA HIVE and HBASE Developed REST APIs using Scala Play framework and Akka Used ScalaTest for writing test cases and coordinated with QA team on end to end testing Developed REST APIs using Scala and Play framework to retrieve processed data from Cassandra database Developing UDFs in java for hive and pig and worked on reading multiple data formats on HDFS using Scala Involved in converting HiveSQL queries into Spark transformations using Spark RDDs and Scala Used Scala collection framework to store and process the complex consumer information Used Scala functional programming concepts to develop business logic Developed programs in JAVA ScalaSpark for data reformation after extraction from HDFS for analysis Developed Spark scripts by using Scala shell commands as per the requirement Processed the schema oriented and nonschemaoriented data using Scala and Spark Developed Scala scripts UDFFs using both Data framesSQLData sets and RDDMapReduce in Spark 16 for Data Aggregation queries and writing data back into OLTP system through Sqoop Provided architecture and design as product is migrated to Scala Play framework and Sencha UI Implemented applications with Scala along with Akka and Play framework Expert in implementing advanced procedures like text analytics and processing using the inmemory computing capabilities like Apache Spark written in Scala Auction web app calculated bids for energy auctions utilizing Scala JPA and Oracle Built KafkaSparkCassandra Scala simulator for MetiStream a big data consultancy KafkaSparkCassandra prototypes Developed a Restful API using Scala for tracking open source projects in Github and computing the inprocess metrics information for those projects Developed analytical components using Scala Spark Apache Mesos and Spark Stream Experience in using the Docker container system with the Kubernetes integration Developed a Web Application using Java with the Google Web Toolkit API with PostgreSql RedisCreating a dashboard using Flask Python libraries and AngularJS to visualize their progress Improve site performance by making better use of caches via MemCachedon Amazon Web Services Used R for prototype on a sample data exploration to identify the best algorithimic approach and then wrote scala scripts using spark machine learning module Developed MapReduceSpark Python modules for machine learning predictive analytics in Hadoop on AWS Implemented a Pythonbased distributed random forest via Python streaming Utilized Spark Scala Hadoop HBase Kafka Spark Streaming Caffe TensorFlow   Python a broad variety of machine learning methods including classifications regressions dimensionally reduction etc Excellent understanding knowledge of Hadoop architecture and various components such as HDFSHbase Hive Job Tracker Task Tracker Name Node Data Node and Map Reduce programming paradigm Built KafkaSparkCassandra ScalaFX simulator for MetiStream a big data consultancy KafkaSparkCassandra prototypes Designed a data analysis pipeline in Python using Amazon Web Services such as S3 EC2 and Elastic Map Reduce Implemented applications with Scala along with Akka and Play framework Expert in implementing advanced procedures like text analytics and processing using the inmemory computing capabilities like Apache Spark written in Scala Designing a highly scalable highly available minimum TCO for maximum ROI using big data components like kafkasparkcassandramongoDB and API It is python and scala based analytic system with ML Libraries Worked with NoSQL Platforms and Extensive understanding on relational databases versus NoSQL platforms Background Skills Scala Play Akka Java Python Kafka Scala Spark Hadoop Hive Big Data HBase HDFS Sqoop MapReduce Pig Docker PostgreSQL JSON Redis Memcache AWS Sr SCALA Developer JobNimbus American Fork UT January 2014 to November 2016 The project was to work on JobNimbus CRM Customer Relationship Management and Project Management software all wrapped into one JobNimbus goal was to offer clients the easiest way to manage projects contacts and tasks With detailed reporting and employee management toolsI have worked on customizing workflow and define the sequence of work With prebuilt custom forms users can create and share invoices Managed all their contacts at one central place Contacts can be imported from multiple sales channels including emails and phone calls that allows project managers to assign tasks to their teams monitor progress and send notifications to clients and stakeholders Involved in converting HiveSQL queries into Spark transformations using Spark RDDs and Scala Developed multiple POCs using Scala and deployed on the Yarn cluster compared the performance of Spark with Hive and SQLTeradata Analysed the SQL scripts and designed the solution to implement using Scala Developed analytical component using Scala Spark and Spark Stream Involved in performing the Linear Regression using Scala API and Spark Experience in developing and designing POCs using Scala and deployed on the Yarn cluster compared the performance of Spark with Hive and SQLTeradata Performed advanced procedures like text analytics and processing using the inmemory computing capabilities of Spark using Scala Created various Parser programs to extract data from Business Objects XML Informatica Java and database views using Scala Developed Spark code to using Scala and SparkSQL for faster processing and testing Designed scalable scala Web Architecture hosting reports for the entire application Wrote entities in Scala and Java along with named queries to interact with database Implementing a search microservice Scala REST PlayFramework ElasticSearch Designed a distributed system using Scala and the AKKA Actor Model that runs on multicore machines The server and clients are designed to share the work Server can work independent of a client Wrote a tool for extracting and reconciling trading data from Warehouses The tool is used for data consistency and data quality monitoring The tool is written using Java Scala Play and Akka Errors flagged by the tool are picked up in Splunk and Sitescope Wrote coding highly flexible scalable distributed applications using Scala Created and consuming RESTful Web services in Scala Play using AKKA Automated Compute Engine and Docker Image Builds with Jenkins Worked with various schema for Application Data Processing and Data warehouse that resides in AWS RDS database   Dynamo DB Interacted with DB sharding Redis Jenkins SOLR GraphQL Grafana Click Tracking for analytics Designed a persistent versus transient architecture raw Linux server with Spark ML algorithm jobs test Spark Worked in Big Data technologies Hadoop HDFS Map Reduce Kafka Sqoop Flume BigSQL Hive Pig Hbase and Apache Spark Spark ML MLlib as developer and analyst Wrote SparkML Map Reduce programs HiveQL and PigLatin scripts leading to good understanding in Map Reduce design patterns data analysis using Hive and Pig Utilized Apache Spark with Python to develop and execute Big Data Analytics and Machine learning applications executed machine Learning use cases under Spark ML and Mllib Developed different Statistical Machine Learning Data Mining solutions to various business problems and generating data visualizations using R SAS and Python and creating dashboards using tools like Tableau Explored MLlib algorithms in Spark to understand the possible Machine Learning functionalities that can be used for our use case Utilized Spark Scala Hadoop HBase Kafka Spark Streaming   Python a broad variety of machine learning methods including classifications regressions dimensionally reduction etc Worked on different data formats such as JSON XML and performed machine learning algorithms in R and used Spark for test data analytics using   and Analyzed the performance to identify bottlenecks Imported data using Sqoop to load data from MySQL to HDFS on regular basis Developed Scripts and Batch Job to schedule various Hadoop Program Used Spark API over Hortonworks Hadoop YARN to perform analytics on data in Hive Developed Kafka producer and consumers for message handling Used Amazon CLI for data transfers to and from Amazon S3 buckets Executed HadoopSpark jobs on AWS EMR using programs data stored in S3 Buckets Exploring with Spark improving performance and optimization of the existing algorithms in Hadoop MapReduce using Spark Context SparkSQL Data Frames Pair RDDs and Spark YARN Deployed MapReduce and Spark jobs on Amazon Elastic MapReduce using datasets stored on S3 Used Amazon Cloudwatch to monitor and track resources on AWS Knowledge of designing and deployment of Hadoop cluster and different Big Data analytic tools including Hive HBase Oozie Sqoop Flume Spark Impala Cassandra Responsible for developing data pipeline using Flume Sqoop and Pig to extract the data from weblogs and store in HDFS Involved in converting HiveSQL queries into Spark transformations using Spark RDDs Scala Involved in importing the realtime data to Hadoop using Kafka and implemented Oozie jobs for daily imports Automated the process for extraction of data from warehouses and weblogs by developing workflows and coordinator jobs in Oozie Background Skills Scala Akka Play Java Python Spark Hadoop Hive Big Data Hbase HDFS Oozie Hibernate Spring Angularjs Nodejs bootstrapjs backbonejs JSP Struts JDBC HTML CSS JQuery Javascript XML Sr ML Developer Homedepotcom Atlanta GA March 2011 to December 2013 Replatform project is a complete rewrite of the existing site that runs in a Broadvision environment This site allows customers to browse online and instore products add products to a shopping cart wish list or gift registry create a user account opt for email subscriptions place orders compare and search for products I have also worked on Tax Exempt application that consists of two main parts Registration and Maintenance Tax Exempt Registration is a customer facing application where customers can register for taxexempt status for their eligible taxexempt purchases Maintenance is being used by the Tax Department at the home depot to review search and maintain tax exemption reasons and other activities Worked in loading and analyzing large datasets with Hadoop framework MapReduce HDFS PIG HIVE Flume Sqoop SPARK Impala Scala NoSQL databases like MongoDB HBase Cassandra Involved in start to end process of Hadoop jobs that used various technologies such as Sqoop PIG Hive MapReduce Spark and Shellscripts for scheduling of few jobs extracted and loaded data into DataLake environment AmazonS3 by using Sqoop which was accessed by business users and data scientists Manage and support of enterprise Data Warehouse operation big data advanced predictive application development using Cloudera Hortonworks HDP Developed PIG scripts to transform the raw data into intelligent data as specified by business users Utilized Apache Spark with Python to develop and execute Big Data Analytics and Machine learning applications executed machine Learning use cases under Spark ML and Mllib Involved in designing and deployment of Hadoop cluster and different Big Data analytic tools including Pig Hive HBase Oozie ZooKeeper SQOOP flume Spark Impala and Cassandra with Horton work Distribution Installed Hadoop Map Reduce HDFS and AWS and developed multiple MapReduce jobs in PIG and Hive for data cleaning and preprocessing Developed SparkScala Python for regular expression regex project in the HadoopHive environment with LinuxWindows for big data resources Assisted in upgrading configuration and maintenance of various Hadoop infrastructures like Pig Hive and Hbase Used SparkAPI over Hortonworks Hadoop YARN to perform analytics on data in Hive Improved the performance and optimization of the existing algorithms in Hadoop using Spark Context SparkSQL Data Frame Pair RDDs Spark YARN Deployed this application which uses J2EE architecture model and Struts Framework first on Weblogic and helped in migrating to JBoss Application server Worked in Java J2EE XSL XML Oracle DB2 Struts spring Hibernate REST Web services Model driven architecture and software configuration management tools Developed Application based on J2EE using Hibernate Spring JSF frameworks and SOAPREST web services Web Sphere Integration Developer WID Tool to develop WPS components Used Spring Framework for Dependency injection and integrated with the EJB using annotations Responsible for analysis design development and integration of UI components with backend using J2EE technologies such as Servlets Java Beans and JSP Design and development of the exception management workflow using Oracle BPM Deployed the applications in Linux servers using deployment scripts Designed and developed programs in C to integrate as per the users requirements Createdtranslated PLI programming into SAS which were used as part of the process used to standardize military personnel records Created PLSQL stored procedures for new Oracle Forms and Reports development Background Skills Machine Learning MapReduce HDFS PIG SPARK Impala Scala Java J2EE Spring Struts JSF JSP EJB DOJO JQuery Sencha ExtJS JavaScript Java Developer WellsFargo San Francisco CA July 2009 to February 2011 Wells Fargo Company I have worked on its application that primarily helps the OperationalRiskCompliance team to report analyze operational loss events and their impacts across the bank operations globally Loss events are either grouped as linked losses or nonlinked losses according to the nature of the events Also worked on its features to record Root causes which resulted in the loss event and come up with Action plans to address such loss events The loss events are then associated with multiple Processes Risk Control measures based on the loss and insurance recovery amounts Integrated Hibernate ORM with SpringHibernate framework to facilitate DML and DQL queries and represent objectdatabase mapping Involved in transforming the Use Cases into Class Diagrams Sequence Diagrams and State diagrams Involved in development of Web Services creation of WSDL and schemas Extensively participated on working with Spring framework Involved in writing JSP and Servlets Involved in development of Web Services were developed to receive client requests Implemented Spring JDBC template Spring Exception Strategy and AOP Involved in setting up WebSphere Application server and using Ant tool to build the application and deploy the application in WebSphere Application server Worked with the creation of Store Procedures Involved in writing SQL queries Stored Procedures to accomplish complex functionalities Part of team creating quality working J2EE code to design schedule and cost to implement use cases Developed Reusable classes in the middleware using Hibernate Involved in writing lots of JSP for maintains and enhancements of the application Worked on Front End using Servlets and also backend using EJB and Hibernate Worked on Presentation Layer using Struts Tiles JSPs and Servlets Created quality working J2EE code to design schedule and cost to implement use cases Setting of DB2 build settings in RAD application development server Involved in writing the database integration code using Hibernate Creation of managed server and JDBC connections Worked on the application using Rational Application Developer Designed Developed Application flow UML diagrams of the application using Rational Rose Background Skills J2ee Java Jsp Servlet Jdbc Struts Junit Log4j Javascript Websphere Application Server Axis Wsad Xml Xslt Ant Sql Sql Query Analyzer Jprobe Cvs Opprox Reports Windowsxp UnixIbm Aix Java Developer OxfordHealthcare Broken Arrow OK February 2007 to June 2009 OxfordHealthcare The Project was to enhance and integrate Commercial off the shelf COTS web based Electronic Health Record EHR NextGen Software system and provide a better approach to documentation than conventional paperbased system The project involved building a webbased application and was created to provide online access to the components of disease management programs and staging process Provides direction necessary to clinical and support staff to improve financial revenue performance and the overall patient experience Worked with Spring Batch Used Spring ORM module to integrate with Hibernate Developed the web pages using JSP CSS and HTML Developed the RTM interface module to map the requirements to the testcase and Test design modules Used several J2EE Design Patterns Session Faade Aggregate Entity for the Middle Tier development Developed EJBS Session and MessageDriven Beans in RAD for handling business processing database access and asynchronous messaging Made extensive use of Java Naming and Directory Interface JNDI for looking up enterprise beans Developed MessageDriven beans in collaboration with Java Messaging Service JMS Involved in writing JSPHTMLJavaScript and Servlets to generate dynamic web pages and web content Wrote various stored procedures in PLSQL and JDBC routines to update tables Wrote various SQL queries for data retrieval using JDBC Involved in building and parsing XML documents using SAX parser Exposed business logic as a web service and developed WSDL files for describing these web services Extensively used SOAP formatted messages for communication between web services Developed the application on IBM WebSphere Application Server Developed the plugin interfaces for the TMS features TEE Requirements Version Control Developed Form Beans which are used to store data when the user submits the HTML form Coded various Java beans to implement the business logic Development of GUI using AWT Involved in creating the tables using SQL and connectivity is done by JDBC Involved in generating the reports regarding the marks they secured in the online test once they press the submit button in the test using HTML and JSP Apache Tomcat is used as an Application Server Background Skills J2ee Java Jsp Servlet Jdbc Struts Junit Log4j Javascript Dhtml Websphere Application Server Axis Xml Xslt Ant Sql Opprox Reports Windowsxp Education Bachelors Skills Apache hadoop hdfs 8 years Apache hbase 8 years Aws 8 years Hadoop 8 years Java 10 years Scala 6 years Play 4 years Python 6 years Kafka 3 years Spark 4 years Hive 4 years BigData 5 years Hbase 4 years HDFS 3 years Sqoop 4 years MapReduce 4 years Pig 4 years Docker 3 years PostgreSQL 4 years JSON 3 years Redis 5 years AWS 6 years MapR 3 years MongoDB 4 years Spark 3 years Scala R 3 years Redis 4 years HDFS 3 years Apache Mesos 3 years PMML 4 years Spring Security 2 years Spring Batch 4 years Solaris 4 years AWS EMR 4 years Azure 3 years AWS Aurora 3 years Apache mesos 4 years Spring batch 4 years Spring Security 3 years SpringNet 3 years S3 2 years Microservices 4 years Scala 3 years Akka 3 years Puppet 3 years Chef 3 years Ansible 2 years AspNet 3 years Mongodb 3 years AkkaHttp 3 years AWS EMR 3 years Kafka 2 years Python 3 years AspNet 3 years Additional Information Areas of Expertise include Scala Akka HiveBig DataHbase Java Play AWS Spark Hadoop HDFS",
    "extracted_keywords": [
        "Sr",
        "SCALA",
        "Developer",
        "Sr",
        "SCALA",
        "span",
        "lDeveloperspan",
        "Sr",
        "SCALA",
        "Developer",
        "Instacart",
        "Inc",
        "Palo",
        "Alto",
        "CA",
        "Authorized",
        "US",
        "employer",
        "Work",
        "Experience",
        "Sr",
        "Developer",
        "Instacart",
        "Inc",
        "San",
        "Francisco",
        "CA",
        "December",
        "Present",
        "sameday",
        "grocery",
        "delivery",
        "service",
        "company",
        "Company",
        "customers",
        "groceries",
        "application",
        "platform",
        "retailers",
        "shoppers",
        "touch",
        "cuttingedge",
        "technology",
        "customers",
        "solution",
        "time",
        "food",
        "grocery",
        "brands",
        "Spark",
        "Applications",
        "Scala",
        "Java",
        "Apache",
        "Spark",
        "data",
        "processing",
        "project",
        "data",
        "RDBMS",
        "sources",
        "Spark",
        "Programs",
        "Scala",
        "Java",
        "APIs",
        "transformations",
        "actions",
        "RDDs",
        "HiveSQL",
        "queries",
        "Spark",
        "transformations",
        "Spark",
        "RDD",
        "Scala",
        "Python",
        "Develop",
        "ETL",
        "Process",
        "SCALA",
        "HIVE",
        "HBASE",
        "REST",
        "APIs",
        "Scala",
        "Play",
        "framework",
        "Akka",
        "ScalaTest",
        "test",
        "cases",
        "QA",
        "team",
        "end",
        "end",
        "REST",
        "APIs",
        "Scala",
        "framework",
        "data",
        "Cassandra",
        "database",
        "UDFs",
        "java",
        "hive",
        "pig",
        "data",
        "formats",
        "HDFS",
        "Scala",
        "HiveSQL",
        "queries",
        "Spark",
        "transformations",
        "Spark",
        "RDDs",
        "Scala",
        "Scala",
        "collection",
        "framework",
        "consumer",
        "information",
        "Scala",
        "programming",
        "concepts",
        "business",
        "logic",
        "programs",
        "JAVA",
        "ScalaSpark",
        "data",
        "reformation",
        "extraction",
        "HDFS",
        "analysis",
        "Spark",
        "scripts",
        "Scala",
        "shell",
        "commands",
        "requirement",
        "schema",
        "data",
        "Scala",
        "Spark",
        "Developed",
        "Scala",
        "UDFFs",
        "Data",
        "framesSQLData",
        "sets",
        "RDDMapReduce",
        "Spark",
        "Data",
        "Aggregation",
        "data",
        "OLTP",
        "system",
        "Sqoop",
        "architecture",
        "design",
        "product",
        "Scala",
        "framework",
        "Sencha",
        "UI",
        "applications",
        "Scala",
        "Akka",
        "Play",
        "framework",
        "Expert",
        "procedures",
        "text",
        "analytics",
        "processing",
        "inmemory",
        "computing",
        "capabilities",
        "Apache",
        "Spark",
        "Scala",
        "Auction",
        "web",
        "app",
        "bids",
        "energy",
        "auctions",
        "Scala",
        "JPA",
        "Oracle",
        "KafkaSparkCassandra",
        "Scala",
        "simulator",
        "MetiStream",
        "data",
        "consultancy",
        "KafkaSparkCassandra",
        "API",
        "Scala",
        "source",
        "projects",
        "Github",
        "metrics",
        "information",
        "projects",
        "components",
        "Scala",
        "Spark",
        "Apache",
        "Mesos",
        "Spark",
        "Stream",
        "Experience",
        "Docker",
        "container",
        "system",
        "Kubernetes",
        "integration",
        "Web",
        "Application",
        "Java",
        "Google",
        "Web",
        "Toolkit",
        "API",
        "PostgreSql",
        "RedisCreating",
        "dashboard",
        "Flask",
        "Python",
        "libraries",
        "progress",
        "site",
        "performance",
        "use",
        "caches",
        "MemCachedon",
        "Amazon",
        "Web",
        "Services",
        "R",
        "prototype",
        "sample",
        "data",
        "exploration",
        "algorithimic",
        "approach",
        "scala",
        "scripts",
        "spark",
        "machine",
        "module",
        "Developed",
        "MapReduceSpark",
        "Python",
        "modules",
        "machine",
        "analytics",
        "Hadoop",
        "AWS",
        "Pythonbased",
        "forest",
        "Python",
        "Spark",
        "Scala",
        "Hadoop",
        "HBase",
        "Kafka",
        "Spark",
        "Streaming",
        "Caffe",
        "TensorFlow",
        "Python",
        "variety",
        "machine",
        "learning",
        "methods",
        "classifications",
        "regressions",
        "reduction",
        "knowledge",
        "Hadoop",
        "architecture",
        "components",
        "HDFSHbase",
        "Hive",
        "Job",
        "Tracker",
        "Task",
        "Tracker",
        "Name",
        "Node",
        "Data",
        "Node",
        "Map",
        "Reduce",
        "programming",
        "paradigm",
        "KafkaSparkCassandra",
        "ScalaFX",
        "simulator",
        "MetiStream",
        "data",
        "consultancy",
        "KafkaSparkCassandra",
        "prototypes",
        "data",
        "analysis",
        "pipeline",
        "Python",
        "Amazon",
        "Web",
        "Services",
        "S3",
        "EC2",
        "Elastic",
        "Map",
        "applications",
        "Scala",
        "Akka",
        "Play",
        "framework",
        "Expert",
        "procedures",
        "text",
        "analytics",
        "processing",
        "inmemory",
        "computing",
        "capabilities",
        "Apache",
        "Spark",
        "Scala",
        "TCO",
        "ROI",
        "data",
        "components",
        "kafkasparkcassandramongoDB",
        "API",
        "system",
        "ML",
        "Libraries",
        "NoSQL",
        "Platforms",
        "understanding",
        "databases",
        "NoSQL",
        "Background",
        "Skills",
        "Scala",
        "Akka",
        "Java",
        "Python",
        "Kafka",
        "Scala",
        "Spark",
        "Hadoop",
        "Hive",
        "Big",
        "Data",
        "HBase",
        "HDFS",
        "Sqoop",
        "MapReduce",
        "Pig",
        "Docker",
        "PostgreSQL",
        "JSON",
        "Redis",
        "Memcache",
        "AWS",
        "Sr",
        "Developer",
        "JobNimbus",
        "American",
        "Fork",
        "UT",
        "January",
        "November",
        "project",
        "JobNimbus",
        "CRM",
        "Customer",
        "Relationship",
        "Management",
        "Project",
        "Management",
        "software",
        "JobNimbus",
        "goal",
        "clients",
        "way",
        "projects",
        "contacts",
        "tasks",
        "reporting",
        "employee",
        "management",
        "toolsI",
        "sequence",
        "work",
        "prebuilt",
        "custom",
        "forms",
        "users",
        "invoices",
        "contacts",
        "place",
        "Contacts",
        "sales",
        "channels",
        "emails",
        "phone",
        "calls",
        "project",
        "managers",
        "tasks",
        "teams",
        "progress",
        "notifications",
        "clients",
        "stakeholders",
        "HiveSQL",
        "queries",
        "Spark",
        "transformations",
        "Spark",
        "RDDs",
        "Scala",
        "POCs",
        "Scala",
        "Yarn",
        "cluster",
        "performance",
        "Spark",
        "Hive",
        "SQLTeradata",
        "SQL",
        "scripts",
        "solution",
        "Scala",
        "component",
        "Scala",
        "Spark",
        "Spark",
        "Stream",
        "Linear",
        "Regression",
        "Scala",
        "API",
        "Spark",
        "Experience",
        "designing",
        "POCs",
        "Scala",
        "Yarn",
        "cluster",
        "performance",
        "Spark",
        "Hive",
        "SQLTeradata",
        "procedures",
        "text",
        "analytics",
        "processing",
        "inmemory",
        "computing",
        "capabilities",
        "Spark",
        "Scala",
        "Parser",
        "programs",
        "data",
        "Business",
        "Objects",
        "XML",
        "Informatica",
        "Java",
        "database",
        "views",
        "Scala",
        "Developed",
        "Spark",
        "code",
        "Scala",
        "SparkSQL",
        "processing",
        "testing",
        "scala",
        "Web",
        "Architecture",
        "reports",
        "application",
        "Wrote",
        "entities",
        "Scala",
        "Java",
        "queries",
        "database",
        "search",
        "microservice",
        "Scala",
        "REST",
        "PlayFramework",
        "ElasticSearch",
        "system",
        "Scala",
        "AKKA",
        "Actor",
        "Model",
        "machines",
        "server",
        "clients",
        "work",
        "Server",
        "client",
        "tool",
        "trading",
        "data",
        "Warehouses",
        "tool",
        "data",
        "consistency",
        "data",
        "quality",
        "tool",
        "Java",
        "Scala",
        "Play",
        "Akka",
        "Errors",
        "tool",
        "Splunk",
        "Sitescope",
        "applications",
        "Scala",
        "Created",
        "Web",
        "services",
        "Scala",
        "AKKA",
        "Automated",
        "Compute",
        "Engine",
        "Docker",
        "Image",
        "Builds",
        "Jenkins",
        "Worked",
        "schema",
        "Application",
        "Data",
        "Processing",
        "Data",
        "warehouse",
        "AWS",
        "RDS",
        "database",
        "Dynamo",
        "DB",
        "DB",
        "Redis",
        "Jenkins",
        "SOLR",
        "GraphQL",
        "Grafana",
        "Click",
        "Tracking",
        "analytics",
        "architecture",
        "Linux",
        "server",
        "Spark",
        "ML",
        "algorithm",
        "jobs",
        "Spark",
        "Worked",
        "Big",
        "Data",
        "technologies",
        "Hadoop",
        "HDFS",
        "Map",
        "Reduce",
        "Kafka",
        "Sqoop",
        "Flume",
        "BigSQL",
        "Hive",
        "Pig",
        "Hbase",
        "Apache",
        "Spark",
        "Spark",
        "ML",
        "MLlib",
        "developer",
        "analyst",
        "Wrote",
        "SparkML",
        "Map",
        "Reduce",
        "programs",
        "HiveQL",
        "PigLatin",
        "scripts",
        "understanding",
        "Map",
        "Reduce",
        "design",
        "patterns",
        "data",
        "analysis",
        "Hive",
        "Pig",
        "Apache",
        "Spark",
        "Python",
        "Big",
        "Data",
        "Analytics",
        "Machine",
        "learning",
        "applications",
        "machine",
        "Learning",
        "use",
        "cases",
        "Spark",
        "ML",
        "Mllib",
        "Machine",
        "Learning",
        "Data",
        "Mining",
        "solutions",
        "business",
        "problems",
        "data",
        "visualizations",
        "R",
        "SAS",
        "Python",
        "dashboards",
        "tools",
        "Tableau",
        "MLlib",
        "algorithms",
        "Spark",
        "Machine",
        "Learning",
        "functionalities",
        "use",
        "case",
        "Spark",
        "Scala",
        "Hadoop",
        "HBase",
        "Kafka",
        "Spark",
        "Streaming",
        "Python",
        "variety",
        "machine",
        "learning",
        "methods",
        "classifications",
        "regressions",
        "reduction",
        "data",
        "formats",
        "JSON",
        "XML",
        "machine",
        "algorithms",
        "R",
        "Spark",
        "test",
        "data",
        "analytics",
        "performance",
        "bottlenecks",
        "data",
        "Sqoop",
        "data",
        "MySQL",
        "HDFS",
        "basis",
        "Scripts",
        "Batch",
        "Job",
        "Hadoop",
        "Program",
        "Spark",
        "API",
        "Hortonworks",
        "Hadoop",
        "YARN",
        "analytics",
        "data",
        "Hive",
        "Developed",
        "Kafka",
        "producer",
        "consumers",
        "message",
        "Amazon",
        "CLI",
        "data",
        "transfers",
        "Amazon",
        "S3",
        "buckets",
        "HadoopSpark",
        "jobs",
        "AWS",
        "EMR",
        "programs",
        "data",
        "S3",
        "Buckets",
        "Spark",
        "performance",
        "optimization",
        "algorithms",
        "Hadoop",
        "MapReduce",
        "Spark",
        "Context",
        "SparkSQL",
        "Data",
        "Frames",
        "Pair",
        "RDDs",
        "Spark",
        "YARN",
        "MapReduce",
        "Spark",
        "jobs",
        "Amazon",
        "Elastic",
        "MapReduce",
        "datasets",
        "S3",
        "Amazon",
        "Cloudwatch",
        "resources",
        "AWS",
        "Knowledge",
        "designing",
        "deployment",
        "Hadoop",
        "cluster",
        "Big",
        "Data",
        "tools",
        "Hive",
        "HBase",
        "Oozie",
        "Sqoop",
        "Flume",
        "Spark",
        "Impala",
        "Cassandra",
        "Responsible",
        "data",
        "pipeline",
        "Flume",
        "Sqoop",
        "Pig",
        "data",
        "weblogs",
        "HDFS",
        "HiveSQL",
        "queries",
        "Spark",
        "transformations",
        "Spark",
        "RDDs",
        "Scala",
        "data",
        "Hadoop",
        "Kafka",
        "Oozie",
        "jobs",
        "imports",
        "process",
        "extraction",
        "data",
        "warehouses",
        "weblogs",
        "workflows",
        "coordinator",
        "jobs",
        "Oozie",
        "Background",
        "Skills",
        "Scala",
        "Akka",
        "Java",
        "Python",
        "Spark",
        "Hadoop",
        "Hive",
        "Big",
        "Data",
        "Hbase",
        "HDFS",
        "Oozie",
        "Hibernate",
        "Spring",
        "Angularjs",
        "Nodejs",
        "bootstrapjs",
        "backbonejs",
        "JSP",
        "Struts",
        "JDBC",
        "HTML",
        "CSS",
        "JQuery",
        "Javascript",
        "XML",
        "Sr",
        "ML",
        "Developer",
        "Homedepotcom",
        "Atlanta",
        "GA",
        "March",
        "December",
        "Replatform",
        "project",
        "rewrite",
        "site",
        "Broadvision",
        "environment",
        "site",
        "customers",
        "products",
        "products",
        "shopping",
        "cart",
        "wish",
        "list",
        "gift",
        "registry",
        "user",
        "account",
        "opt",
        "email",
        "subscriptions",
        "place",
        "orders",
        "products",
        "Tax",
        "Exempt",
        "application",
        "parts",
        "Registration",
        "Maintenance",
        "Tax",
        "Exempt",
        "Registration",
        "customer",
        "application",
        "customers",
        "status",
        "purchases",
        "Maintenance",
        "Tax",
        "Department",
        "home",
        "depot",
        "search",
        "tax",
        "exemption",
        "reasons",
        "activities",
        "loading",
        "datasets",
        "Hadoop",
        "framework",
        "MapReduce",
        "HDFS",
        "PIG",
        "HIVE",
        "Flume",
        "Sqoop",
        "SPARK",
        "Impala",
        "Scala",
        "NoSQL",
        "MongoDB",
        "HBase",
        "Cassandra",
        "start",
        "end",
        "process",
        "Hadoop",
        "jobs",
        "technologies",
        "Sqoop",
        "PIG",
        "Hive",
        "MapReduce",
        "Spark",
        "Shellscripts",
        "scheduling",
        "jobs",
        "data",
        "DataLake",
        "environment",
        "AmazonS3",
        "Sqoop",
        "business",
        "users",
        "data",
        "scientists",
        "support",
        "enterprise",
        "Data",
        "Warehouse",
        "operation",
        "data",
        "application",
        "development",
        "Cloudera",
        "Hortonworks",
        "HDP",
        "PIG",
        "scripts",
        "data",
        "data",
        "business",
        "users",
        "Apache",
        "Spark",
        "Python",
        "Big",
        "Data",
        "Analytics",
        "Machine",
        "learning",
        "applications",
        "machine",
        "Learning",
        "use",
        "cases",
        "Spark",
        "ML",
        "Mllib",
        "designing",
        "deployment",
        "Hadoop",
        "cluster",
        "Big",
        "Data",
        "tools",
        "Pig",
        "Hive",
        "HBase",
        "Oozie",
        "ZooKeeper",
        "SQOOP",
        "Spark",
        "Impala",
        "Cassandra",
        "Horton",
        "work",
        "Distribution",
        "Installed",
        "Hadoop",
        "Map",
        "HDFS",
        "AWS",
        "MapReduce",
        "jobs",
        "PIG",
        "Hive",
        "data",
        "Developed",
        "SparkScala",
        "Python",
        "expression",
        "regex",
        "project",
        "environment",
        "LinuxWindows",
        "data",
        "resources",
        "configuration",
        "maintenance",
        "Hadoop",
        "infrastructures",
        "Pig",
        "Hive",
        "Hbase",
        "SparkAPI",
        "Hortonworks",
        "Hadoop",
        "YARN",
        "analytics",
        "data",
        "Hive",
        "performance",
        "optimization",
        "algorithms",
        "Hadoop",
        "Spark",
        "Context",
        "SparkSQL",
        "Data",
        "Frame",
        "Pair",
        "Spark",
        "YARN",
        "application",
        "J2EE",
        "architecture",
        "model",
        "Struts",
        "Framework",
        "Weblogic",
        "JBoss",
        "Application",
        "server",
        "Java",
        "J2EE",
        "XSL",
        "XML",
        "Oracle",
        "DB2",
        "Struts",
        "spring",
        "Hibernate",
        "REST",
        "Web",
        "services",
        "architecture",
        "software",
        "configuration",
        "management",
        "tools",
        "Developed",
        "Application",
        "J2EE",
        "Hibernate",
        "Spring",
        "JSF",
        "frameworks",
        "SOAPREST",
        "web",
        "services",
        "Web",
        "Sphere",
        "Integration",
        "Developer",
        "WID",
        "Tool",
        "WPS",
        "components",
        "Spring",
        "Framework",
        "Dependency",
        "injection",
        "EJB",
        "annotations",
        "analysis",
        "design",
        "development",
        "integration",
        "UI",
        "components",
        "backend",
        "J2EE",
        "technologies",
        "Servlets",
        "Java",
        "Beans",
        "JSP",
        "Design",
        "development",
        "exception",
        "management",
        "workflow",
        "Oracle",
        "BPM",
        "applications",
        "Linux",
        "servers",
        "deployment",
        "scripts",
        "programs",
        "C",
        "users",
        "Createdtranslated",
        "PLI",
        "programming",
        "SAS",
        "part",
        "process",
        "personnel",
        "records",
        "procedures",
        "Oracle",
        "Forms",
        "Reports",
        "development",
        "Background",
        "Skills",
        "Machine",
        "Learning",
        "MapReduce",
        "HDFS",
        "PIG",
        "SPARK",
        "Impala",
        "Scala",
        "Java",
        "J2EE",
        "Spring",
        "Struts",
        "JSF",
        "JSP",
        "EJB",
        "DOJO",
        "JQuery",
        "Sencha",
        "ExtJS",
        "JavaScript",
        "Java",
        "Developer",
        "WellsFargo",
        "San",
        "Francisco",
        "CA",
        "July",
        "February",
        "Wells",
        "Fargo",
        "Company",
        "application",
        "OperationalRiskCompliance",
        "team",
        "loss",
        "events",
        "impacts",
        "bank",
        "operations",
        "Loss",
        "events",
        "losses",
        "losses",
        "nature",
        "events",
        "features",
        "Root",
        "causes",
        "loss",
        "event",
        "Action",
        "loss",
        "events",
        "loss",
        "events",
        "Processes",
        "Risk",
        "Control",
        "measures",
        "loss",
        "insurance",
        "recovery",
        "Integrated",
        "Hibernate",
        "ORM",
        "SpringHibernate",
        "framework",
        "DML",
        "DQL",
        "queries",
        "objectdatabase",
        "mapping",
        "Use",
        "Cases",
        "Class",
        "Diagrams",
        "Sequence",
        "Diagrams",
        "State",
        "diagrams",
        "development",
        "Web",
        "Services",
        "creation",
        "WSDL",
        "schemas",
        "Spring",
        "framework",
        "JSP",
        "Servlets",
        "development",
        "Web",
        "Services",
        "client",
        "requests",
        "Spring",
        "JDBC",
        "template",
        "Spring",
        "Exception",
        "Strategy",
        "AOP",
        "WebSphere",
        "Application",
        "server",
        "Ant",
        "tool",
        "application",
        "application",
        "WebSphere",
        "Application",
        "server",
        "creation",
        "Store",
        "Procedures",
        "SQL",
        "Procedures",
        "functionalities",
        "Part",
        "team",
        "quality",
        "J2EE",
        "code",
        "schedule",
        "cost",
        "use",
        "cases",
        "Reusable",
        "classes",
        "middleware",
        "Hibernate",
        "lots",
        "JSP",
        "maintains",
        "enhancements",
        "application",
        "Front",
        "End",
        "Servlets",
        "EJB",
        "Hibernate",
        "Presentation",
        "Layer",
        "Struts",
        "Tiles",
        "JSPs",
        "Servlets",
        "Created",
        "quality",
        "J2EE",
        "code",
        "schedule",
        "cost",
        "use",
        "cases",
        "DB2",
        "settings",
        "RAD",
        "application",
        "development",
        "server",
        "database",
        "integration",
        "code",
        "Hibernate",
        "Creation",
        "server",
        "JDBC",
        "connections",
        "application",
        "Rational",
        "Application",
        "Developer",
        "Developed",
        "Application",
        "flow",
        "UML",
        "diagrams",
        "application",
        "Rational",
        "Rose",
        "Background",
        "Skills",
        "J2ee",
        "Java",
        "Jsp",
        "Servlet",
        "Jdbc",
        "Struts",
        "Junit",
        "Log4j",
        "Javascript",
        "Websphere",
        "Application",
        "Server",
        "Axis",
        "Wsad",
        "Xml",
        "Xslt",
        "Ant",
        "Sql",
        "Sql",
        "Query",
        "Analyzer",
        "Jprobe",
        "Cvs",
        "Opprox",
        "Reports",
        "Windowsxp",
        "UnixIbm",
        "Aix",
        "Java",
        "Developer",
        "OxfordHealthcare",
        "Broken",
        "Arrow",
        "February",
        "June",
        "OxfordHealthcare",
        "Project",
        "Commercial",
        "shelf",
        "COTS",
        "web",
        "Electronic",
        "Health",
        "Record",
        "EHR",
        "NextGen",
        "Software",
        "system",
        "approach",
        "documentation",
        "system",
        "project",
        "application",
        "access",
        "components",
        "disease",
        "management",
        "programs",
        "staging",
        "process",
        "direction",
        "support",
        "staff",
        "revenue",
        "performance",
        "experience",
        "Spring",
        "Batch",
        "Spring",
        "ORM",
        "module",
        "Hibernate",
        "web",
        "pages",
        "JSP",
        "CSS",
        "HTML",
        "RTM",
        "interface",
        "module",
        "requirements",
        "testcase",
        "Test",
        "design",
        "modules",
        "J2EE",
        "Design",
        "Patterns",
        "Session",
        "Faade",
        "Aggregate",
        "Entity",
        "Middle",
        "Tier",
        "development",
        "Developed",
        "EJBS",
        "Session",
        "MessageDriven",
        "Beans",
        "RAD",
        "business",
        "processing",
        "database",
        "access",
        "messaging",
        "use",
        "Java",
        "Naming",
        "Directory",
        "Interface",
        "JNDI",
        "enterprise",
        "beans",
        "MessageDriven",
        "beans",
        "collaboration",
        "Java",
        "Messaging",
        "Service",
        "JMS",
        "JSPHTMLJavaScript",
        "Servlets",
        "web",
        "pages",
        "web",
        "content",
        "procedures",
        "PLSQL",
        "JDBC",
        "routines",
        "tables",
        "Wrote",
        "SQL",
        "queries",
        "data",
        "retrieval",
        "JDBC",
        "XML",
        "documents",
        "SAX",
        "parser",
        "business",
        "logic",
        "web",
        "service",
        "WSDL",
        "files",
        "web",
        "services",
        "SOAP",
        "messages",
        "communication",
        "web",
        "services",
        "application",
        "IBM",
        "WebSphere",
        "Application",
        "Server",
        "interfaces",
        "TMS",
        "TEE",
        "Requirements",
        "Version",
        "Control",
        "Developed",
        "Form",
        "Beans",
        "data",
        "user",
        "HTML",
        "form",
        "Java",
        "beans",
        "business",
        "logic",
        "Development",
        "GUI",
        "AWT",
        "tables",
        "SQL",
        "connectivity",
        "JDBC",
        "reports",
        "marks",
        "test",
        "submit",
        "button",
        "test",
        "HTML",
        "JSP",
        "Apache",
        "Tomcat",
        "Application",
        "Server",
        "Background",
        "Skills",
        "J2ee",
        "Java",
        "Jsp",
        "Servlet",
        "Jdbc",
        "Struts",
        "Junit",
        "Log4j",
        "Javascript",
        "Dhtml",
        "Websphere",
        "Application",
        "Server",
        "Axis",
        "Xml",
        "Xslt",
        "Ant",
        "Sql",
        "Opprox",
        "Reports",
        "Windowsxp",
        "Education",
        "Bachelors",
        "Skills",
        "Apache",
        "hadoop",
        "hdfs",
        "years",
        "Apache",
        "hbase",
        "years",
        "Aws",
        "years",
        "Hadoop",
        "years",
        "Java",
        "years",
        "Scala",
        "years",
        "years",
        "Python",
        "years",
        "Kafka",
        "years",
        "Spark",
        "years",
        "Hive",
        "years",
        "BigData",
        "years",
        "Hbase",
        "years",
        "HDFS",
        "years",
        "Sqoop",
        "years",
        "MapReduce",
        "years",
        "Pig",
        "years",
        "Docker",
        "years",
        "PostgreSQL",
        "years",
        "JSON",
        "years",
        "Redis",
        "years",
        "AWS",
        "years",
        "MapR",
        "years",
        "MongoDB",
        "years",
        "Spark",
        "years",
        "Scala",
        "R",
        "years",
        "Redis",
        "years",
        "HDFS",
        "years",
        "Apache",
        "Mesos",
        "years",
        "PMML",
        "years",
        "Spring",
        "Security",
        "years",
        "Spring",
        "Batch",
        "years",
        "Solaris",
        "years",
        "AWS",
        "EMR",
        "years",
        "years",
        "AWS",
        "Aurora",
        "years",
        "Apache",
        "mesos",
        "years",
        "Spring",
        "batch",
        "years",
        "Spring",
        "Security",
        "years",
        "SpringNet",
        "years",
        "S3",
        "years",
        "Microservices",
        "years",
        "Scala",
        "years",
        "Akka",
        "years",
        "Puppet",
        "years",
        "Chef",
        "years",
        "years",
        "AspNet",
        "years",
        "Mongodb",
        "years",
        "AkkaHttp",
        "years",
        "AWS",
        "EMR",
        "years",
        "Kafka",
        "years",
        "Python",
        "years",
        "AspNet",
        "years",
        "Additional",
        "Information",
        "Areas",
        "Expertise",
        "Scala",
        "Akka",
        "HiveBig",
        "DataHbase",
        "Java",
        "AWS",
        "Spark",
        "Hadoop",
        "HDFS"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T22:16:27.128080",
    "resume_data": "Sr SCALA Developer Sr SCALA span lDeveloperspan Sr SCALA Developer Instacart Inc Palo Alto CA Authorized to work in the US for any employer Work Experience Sr SCALA Developer Instacart Inc San Francisco CA December 2016 to Present operates as a sameday grocery delivery service company The Company allows customers to select groceries through an online application platform from various retailers and have them delivered by personal shoppers Worked on combining a personal touch and cuttingedge technology to offer customers a simple solution to save time and eat fresh food from the most trusted grocery brands Developed Spark Applications by using Scala Java and Implemented Apache Spark data processing project to handle data from various RDBMS and Streaming sources Developed Spark Programs using Scala and Java APIs and performed transformations and actions on RDDs Involved in converting HiveSQL queries into Spark transformations using Spark RDD Scala and Python Develop ETL Process usingSPARK SCALA HIVE and HBASE Developed REST APIs using Scala Play framework and Akka Used ScalaTest for writing test cases and coordinated with QA team on end to end testing Developed REST APIs using Scala and Play framework to retrieve processed data from Cassandra database Developing UDFs in java for hive and pig and worked on reading multiple data formats on HDFS using Scala Involved in converting HiveSQL queries into Spark transformations using Spark RDDs and Scala Used Scala collection framework to store and process the complex consumer information Used Scala functional programming concepts to develop business logic Developed programs in JAVA ScalaSpark for data reformation after extraction from HDFS for analysis Developed Spark scripts by using Scala shell commands as per the requirement Processed the schema oriented and nonschemaoriented data using Scala and Spark Developed Scala scripts UDFFs using both Data framesSQLData sets and RDDMapReduce in Spark 16 for Data Aggregation queries and writing data back into OLTP system through Sqoop Provided architecture and design as product is migrated to Scala Play framework and Sencha UI Implemented applications with Scala along with Akka and Play framework Expert in implementing advanced procedures like text analytics and processing using the inmemory computing capabilities like Apache Spark written in Scala Auction web app calculated bids for energy auctions utilizing Scala JPA and Oracle Built KafkaSparkCassandra Scala simulator for MetiStream a big data consultancy KafkaSparkCassandra prototypes Developed a Restful API using Scala for tracking open source projects in Github and computing the inprocess metrics information for those projects Developed analytical components using Scala Spark Apache Mesos and Spark Stream Experience in using the Docker container system with the Kubernetes integration Developed a Web Application using Java with the Google Web Toolkit API with PostgreSql RedisCreating a dashboard using Flask Python libraries and AngularJS to visualize their progress Improve site performance by making better use of caches via MemCachedon Amazon Web Services Used R for prototype on a sample data exploration to identify the best algorithimic approach and then wrote scala scripts using spark machine learning module Developed MapReduceSpark Python modules for machine learning predictive analytics in Hadoop on AWS Implemented a Pythonbased distributed random forest via Python streaming Utilized Spark Scala Hadoop HBase Kafka Spark Streaming Caffe TensorFlow MLLib Python a broad variety of machine learning methods including classifications regressions dimensionally reduction etc Excellent understanding knowledge of Hadoop architecture and various components such as HDFSHbase Hive Job Tracker Task Tracker Name Node Data Node and Map Reduce programming paradigm Built KafkaSparkCassandra ScalaFX simulator for MetiStream a big data consultancy KafkaSparkCassandra prototypes Designed a data analysis pipeline in Python using Amazon Web Services such as S3 EC2 and Elastic Map Reduce Implemented applications with Scala along with Akka and Play framework Expert in implementing advanced procedures like text analytics and processing using the inmemory computing capabilities like Apache Spark written in Scala Designing a highly scalable highly available minimum TCO for maximum ROI using big data components like kafkasparkcassandramongoDB and API It is python and scala based analytic system with ML Libraries Worked with NoSQL Platforms and Extensive understanding on relational databases versus NoSQL platforms Background Skills Scala Play Akka Java Python Kafka Scala Spark Hadoop Hive Big Data HBase HDFS Sqoop MapReduce Pig Docker PostgreSQL JSON Redis Memcache AWS Sr SCALA Developer JobNimbus American Fork UT January 2014 to November 2016 The project was to work on JobNimbus CRM Customer Relationship Management and Project Management software all wrapped into one JobNimbus goal was to offer clients the easiest way to manage projects contacts and tasks With detailed reporting and employee management toolsI have worked on customizing workflow and define the sequence of work With prebuilt custom forms users can create and share invoices Managed all their contacts at one central place Contacts can be imported from multiple sales channels including emails and phone calls that allows project managers to assign tasks to their teams monitor progress and send notifications to clients and stakeholders Involved in converting HiveSQL queries into Spark transformations using Spark RDDs and Scala Developed multiple POCs using Scala and deployed on the Yarn cluster compared the performance of Spark with Hive and SQLTeradata Analysed the SQL scripts and designed the solution to implement using Scala Developed analytical component using Scala Spark and Spark Stream Involved in performing the Linear Regression using Scala API and Spark Experience in developing and designing POCs using Scala and deployed on the Yarn cluster compared the performance of Spark with Hive and SQLTeradata Performed advanced procedures like text analytics and processing using the inmemory computing capabilities of Spark using Scala Created various Parser programs to extract data from Business Objects XML Informatica Java and database views using Scala Developed Spark code to using Scala and SparkSQL for faster processing and testing Designed scalable scala Web Architecture hosting reports for the entire application Wrote entities in Scala and Java along with named queries to interact with database Implementing a search microservice Scala REST PlayFramework ElasticSearch Designed a distributed system using Scala and the AKKA Actor Model that runs on multicore machines The server and clients are designed to share the work Server can work independent of a client Wrote a tool for extracting and reconciling trading data from Warehouses The tool is used for data consistency and data quality monitoring The tool is written using Java Scala Play and Akka Errors flagged by the tool are picked up in Splunk and Sitescope Wrote coding highly flexible scalable distributed applications using Scala Created and consuming RESTful Web services in Scala Play using AKKA Automated Compute Engine and Docker Image Builds with Jenkins Worked with various schema for Application Data Processing and Data warehouse that resides in AWS RDS database PostGreSql Dynamo DB Interacted with DB sharding Redis Jenkins SOLR GraphQL Grafana Click Tracking for analytics Designed a persistent versus transient architecture raw Linux server with Spark ML algorithm jobs test Spark Worked in Big Data technologies Hadoop HDFS Map Reduce Kafka Sqoop Flume BigSQL Hive Pig Hbase and Apache Spark Spark ML MLlib as developer and analyst Wrote SparkML Map Reduce programs HiveQL and PigLatin scripts leading to good understanding in Map Reduce design patterns data analysis using Hive and Pig Utilized Apache Spark with Python to develop and execute Big Data Analytics and Machine learning applications executed machine Learning use cases under Spark ML and Mllib Developed different Statistical Machine Learning Data Mining solutions to various business problems and generating data visualizations using R SAS and Python and creating dashboards using tools like Tableau Explored MLlib algorithms in Spark to understand the possible Machine Learning functionalities that can be used for our use case Utilized Spark Scala Hadoop HBase Kafka Spark Streaming MLLib Python a broad variety of machine learning methods including classifications regressions dimensionally reduction etc Worked on different data formats such as JSON XML and performed machine learning algorithms in R and used Spark for test data analytics using MLLib and Analyzed the performance to identify bottlenecks Imported data using Sqoop to load data from MySQL to HDFS on regular basis Developed Scripts and Batch Job to schedule various Hadoop Program Used Spark API over Hortonworks Hadoop YARN to perform analytics on data in Hive Developed Kafka producer and consumers for message handling Used Amazon CLI for data transfers to and from Amazon S3 buckets Executed HadoopSpark jobs on AWS EMR using programs data stored in S3 Buckets Exploring with Spark improving performance and optimization of the existing algorithms in Hadoop MapReduce using Spark Context SparkSQL Data Frames Pair RDDs and Spark YARN Deployed MapReduce and Spark jobs on Amazon Elastic MapReduce using datasets stored on S3 Used Amazon Cloudwatch to monitor and track resources on AWS Knowledge of designing and deployment of Hadoop cluster and different Big Data analytic tools including Hive HBase Oozie Sqoop Flume Spark Impala Cassandra Responsible for developing data pipeline using Flume Sqoop and Pig to extract the data from weblogs and store in HDFS Involved in converting HiveSQL queries into Spark transformations using Spark RDDs Scala Involved in importing the realtime data to Hadoop using Kafka and implemented Oozie jobs for daily imports Automated the process for extraction of data from warehouses and weblogs by developing workflows and coordinator jobs in Oozie Background Skills Scala Akka Play Java Python Spark Hadoop Hive Big Data Hbase HDFS Oozie Hibernate Spring Angularjs Nodejs bootstrapjs backbonejs JSP Struts JDBC HTML CSS JQuery Javascript XML Sr ML Developer Homedepotcom Atlanta GA March 2011 to December 2013 Replatform project is a complete rewrite of the existing site that runs in a Broadvision environment This site allows customers to browse online and instore products add products to a shopping cart wish list or gift registry create a user account opt for email subscriptions place orders compare and search for products I have also worked on Tax Exempt application that consists of two main parts Registration and Maintenance Tax Exempt Registration is a customer facing application where customers can register for taxexempt status for their eligible taxexempt purchases Maintenance is being used by the Tax Department at the home depot to review search and maintain tax exemption reasons and other activities Worked in loading and analyzing large datasets with Hadoop framework MapReduce HDFS PIG HIVE Flume Sqoop SPARK Impala Scala NoSQL databases like MongoDB HBase Cassandra Involved in start to end process of Hadoop jobs that used various technologies such as Sqoop PIG Hive MapReduce Spark and Shellscripts for scheduling of few jobs extracted and loaded data into DataLake environment AmazonS3 by using Sqoop which was accessed by business users and data scientists Manage and support of enterprise Data Warehouse operation big data advanced predictive application development using Cloudera Hortonworks HDP Developed PIG scripts to transform the raw data into intelligent data as specified by business users Utilized Apache Spark with Python to develop and execute Big Data Analytics and Machine learning applications executed machine Learning use cases under Spark ML and Mllib Involved in designing and deployment of Hadoop cluster and different Big Data analytic tools including Pig Hive HBase Oozie ZooKeeper SQOOP flume Spark Impala and Cassandra with Horton work Distribution Installed Hadoop Map Reduce HDFS and AWS and developed multiple MapReduce jobs in PIG and Hive for data cleaning and preprocessing Developed SparkScala Python for regular expression regex project in the HadoopHive environment with LinuxWindows for big data resources Assisted in upgrading configuration and maintenance of various Hadoop infrastructures like Pig Hive and Hbase Used SparkAPI over Hortonworks Hadoop YARN to perform analytics on data in Hive Improved the performance and optimization of the existing algorithms in Hadoop using Spark Context SparkSQL Data Frame Pair RDDs Spark YARN Deployed this application which uses J2EE architecture model and Struts Framework first on Weblogic and helped in migrating to JBoss Application server Worked in Java J2EE XSL XML Oracle DB2 Struts spring Hibernate REST Web services Model driven architecture and software configuration management tools Developed Application based on J2EE using Hibernate Spring JSF frameworks and SOAPREST web services Web Sphere Integration Developer WID Tool to develop WPS components Used Spring Framework for Dependency injection and integrated with the EJB using annotations Responsible for analysis design development and integration of UI components with backend using J2EE technologies such as Servlets Java Beans and JSP Design and development of the exception management workflow using Oracle BPM Deployed the applications in Linux servers using deployment scripts Designed and developed programs in C to integrate as per the users requirements Createdtranslated PLI programming into SAS which were used as part of the process used to standardize military personnel records Created PLSQL stored procedures for new Oracle Forms and Reports development Background Skills Machine Learning MapReduce HDFS PIG SPARK Impala Scala Java J2EE Spring Struts JSF JSP EJB DOJO JQuery Sencha ExtJS JavaScript Java Developer WellsFargo San Francisco CA July 2009 to February 2011 Wells Fargo Company I have worked on its application that primarily helps the OperationalRiskCompliance team to report analyze operational loss events and their impacts across the bank operations globally Loss events are either grouped as linked losses or nonlinked losses according to the nature of the events Also worked on its features to record Root causes which resulted in the loss event and come up with Action plans to address such loss events The loss events are then associated with multiple Processes Risk Control measures based on the loss and insurance recovery amounts Integrated Hibernate ORM with SpringHibernate framework to facilitate DML and DQL queries and represent objectdatabase mapping Involved in transforming the Use Cases into Class Diagrams Sequence Diagrams and State diagrams Involved in development of Web Services creation of WSDL and schemas Extensively participated on working with Spring framework Involved in writing JSP and Servlets Involved in development of Web Services were developed to receive client requests Implemented Spring JDBC template Spring Exception Strategy and AOP Involved in setting up WebSphere Application server and using Ant tool to build the application and deploy the application in WebSphere Application server Worked with the creation of Store Procedures Involved in writing SQL queries Stored Procedures to accomplish complex functionalities Part of team creating quality working J2EE code to design schedule and cost to implement use cases Developed Reusable classes in the middleware using Hibernate Involved in writing lots of JSP for maintains and enhancements of the application Worked on Front End using Servlets and also backend using EJB and Hibernate Worked on Presentation Layer using Struts Tiles JSPs and Servlets Created quality working J2EE code to design schedule and cost to implement use cases Setting of DB2 build settings in RAD application development server Involved in writing the database integration code using Hibernate Creation of managed server and JDBC connections Worked on the application using Rational Application Developer Designed Developed Application flow UML diagrams of the application using Rational Rose Background Skills J2ee Java Jsp Servlet Jdbc Struts Junit Log4j Javascript Websphere Application Server Axis Wsad Xml Xslt Ant Sql Sql Query Analyzer Jprobe Cvs Opprox Reports Windowsxp UnixIbm Aix Java Developer OxfordHealthcare Broken Arrow OK February 2007 to June 2009 OxfordHealthcare The Project was to enhance and integrate Commercial off the shelf COTS web based Electronic Health Record EHR NextGen Software system and provide a better approach to documentation than conventional paperbased system The project involved building a webbased application and was created to provide online access to the components of disease management programs and staging process Provides direction necessary to clinical and support staff to improve financial revenue performance and the overall patient experience Worked with Spring Batch Used Spring ORM module to integrate with Hibernate Developed the web pages using JSP CSS and HTML Developed the RTM interface module to map the requirements to the testcase and Test design modules Used several J2EE Design Patterns Session Faade Aggregate Entity for the Middle Tier development Developed EJBS Session and MessageDriven Beans in RAD for handling business processing database access and asynchronous messaging Made extensive use of Java Naming and Directory Interface JNDI for looking up enterprise beans Developed MessageDriven beans in collaboration with Java Messaging Service JMS Involved in writing JSPHTMLJavaScript and Servlets to generate dynamic web pages and web content Wrote various stored procedures in PLSQL and JDBC routines to update tables Wrote various SQL queries for data retrieval using JDBC Involved in building and parsing XML documents using SAX parser Exposed business logic as a web service and developed WSDL files for describing these web services Extensively used SOAP formatted messages for communication between web services Developed the application on IBM WebSphere Application Server Developed the plugin interfaces for the TMS features TEE Requirements Version Control Developed Form Beans which are used to store data when the user submits the HTML form Coded various Java beans to implement the business logic Development of GUI using AWT Involved in creating the tables using SQL and connectivity is done by JDBC Involved in generating the reports regarding the marks they secured in the online test once they press the submit button in the test using HTML and JSP Apache Tomcat is used as an Application Server Background Skills J2ee Java Jsp Servlet Jdbc Struts Junit Log4j Javascript Dhtml Websphere Application Server Axis Xml Xslt Ant Sql Opprox Reports Windowsxp Education Bachelors Skills Apache hadoop hdfs 8 years Apache hbase 8 years Aws 8 years Hadoop 8 years Java 10 years Scala 6 years Play 4 years Python 6 years Kafka 3 years Spark 4 years Hive 4 years BigData 5 years Hbase 4 years HDFS 3 years Sqoop 4 years MapReduce 4 years Pig 4 years Docker 3 years PostgreSQL 4 years JSON 3 years Redis 5 years AWS 6 years MapR 3 years MongoDB 4 years Spark 3 years Scala R 3 years Redis 4 years HDFS 3 years Apache Mesos 3 years PMML 4 years Spring Security 2 years Spring Batch 4 years Solaris 4 years AWS EMR 4 years Azure 3 years AWS Aurora 3 years Apache mesos 4 years Spring batch 4 years Spring Security 3 years SpringNet 3 years S3 2 years Microservices 4 years Scala 3 years Akka 3 years Puppet 3 years Chef 3 years Ansible 2 years AspNet 3 years Mongodb 3 years AkkaHttp 3 years AWS EMR 3 years Kafka 2 years Python 3 years AspNet 3 years Additional Information Areas of Expertise include Scala Akka HiveBig DataHbase Java Play AWS Spark Hadoop HDFS",
    "unique_id": "65907bab-1c7a-4328-9d92-274674321a80"
}