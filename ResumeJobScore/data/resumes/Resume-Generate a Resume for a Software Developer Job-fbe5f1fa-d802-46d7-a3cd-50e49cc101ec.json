{
    "clean_data": "Sr Hadoop Developer Sr Hadoop span lDeveloperspan Sr Hadoop Developer Provish Consulting Atlanta GA Over 9 years of professional IT experience that includes of BigDataHadoop and of web and Windows application development with net and Java Hands on experience with the Hadoop stack MapReduce HDFS Sqoop Pig Hive HBase SPARK KafkaControlM Oozie ZooKeeper and Talend Have experience in configuring and administrating the Hadoop Cluster using major Hadoop Distributions like Hortonworks and Cloudera Excellent experience in developing different components using Apache Hadoop ecosystem components like Map Reduce Hive HBase PIG Sqoop Spark Kafka Flume Zookeeper Oozie and Storm Expertise in depth understandingknowledge of Hadoop Architecture and various components such as HDFS Job Tracker Task Tracker Name Node Data Node MRv1 andMRv2 YARN Experienced with data architecture including data ingestion pipeline design Hadoop information architecture data modeling and data mining machine learning and advanced data processing Expertise in writing Apache Spark streaming API on Big Data distribution in the active cluster environment Experienced on implementation of a log producer in Scala that watches for application logs transform incremental log and sends them to a Kafka and Zookeeper based log collection platform Experienced in working with Flume to load the log data from multiple sources directly intoHDFS Excellent knowledge in building and scheduling Big Data workflows with the help of OOZIEand Autosys Experienced in importing and exporting data from the different Data sources like Teradata and DB2 using Sqoop from HDFS to Relational Database Systems RDBMS and viceversa and load into partitioned Hive tables Proven Expertise in performing analytics on Big Data using Map Reduce Hive Pig and Talend Experienced with performing real time analytics on NoSQL databases like HBase and Cassandra Experienced with ETL to load data into HadoopNoSQL Experienced with Dimensional modeling Data migration Data Masking Data cleansing Data profiling and ETL Processes features for data warehouses Experience in integration of various data sources like SQL Server Oracle Vertica MySQL Flat files Authorized to work in the US for any employer Work Experience Sr Hadoop Developer Provish Consulting Charlotte NC September 2018 to Present Responsibilities Worked on WEB and Windows Application development Worked on the application which talks to different technological platforms such as Share point SAP Oracle and SQL Worked with BI teams in generating the reports and designing ETL workflows on SAP BI Worked on 3Tier architecture of application development Used OOPS concepts in application development Worked as coOrdinator between Onshore and Offshore teams Worked closely with business and gather the requirements Worked on importing data from various sources and performed transformations using MapReduce Hive to load data into HDFS Involved in complete implementation lifecycle specialized in writing custom MapReduce Pig and Hive programs Handled importing of data from RDBMS into HDFS using Sqoop Managing data flow into Pivotal HAWQ Internal External tables Experienced in data cleansing processing using Pig Latin operations and UDFs Experienced in writing Hive Queries for analyzing data in Hive warehouse using Hive Query Language Involved in creating Hive tables loading with data and writing hive queries to process the data Created scripts to automate the process of Data Ingestion Developed PIG scripts for source data validation and transformation Performed various performance optimizations like using distributed cache for small datasets Partitioning Bucketing in Hive and Map Side Joins Managing and scheduling jobs to remove the duplicate log data files in HDFS using Oozie Extensively used HiveHQL or Hive queries to query or search for particular string in Hive tables in HDFS Experience in developing customized UDFs in java to extend Hive and Pig Latin functionality Created HBase tables to store various data formats for data coming from different portfolios Environment Horton works Map Reduce HBase HDFS Hive Pig Java SQL Cloudera Manager Sqoop Flume Oozie Sr Hadoop Developer Trimble Navigation Limited Chantilly VA May 2016 to August 2018 Responsibilities Worked as a part RD team in Zeta experimenting with emerging technologies Involved in Installing Configuring Hadoop Eco System Cloudera Manager using CDH54 and Horton works Distribution Worked on POC to upgrade the tradition relational data process to Hadoop Used Talend as ETL tool with Hadoop and other database components Used ControlM was workflow job scheduler for all the bigdata Talend jobs Played a key role in installation and configuration of the various Hadoop ecosystem tools such asSolr Kafka Pig HBase and Cassandra Implemented multiple Map Reduce Jobs in java for data cleansing and preprocessing Wrote complex Hive queries and UDFs in Java and Python Involved in implementing an HDInsight version 33 clusters which is based on spark version 151 Responsible for data extraction and data ingestion from different data sources into Hadoop Data Lake by creating ETL pipelines using Pig and Hive Job duties involved the design development of various modules in Hadoop Big Data Platform and processing data using Map Reduce Hive Pig Sqoop and Oozie Design developed and tested Map Reduce programs on Mobile Offers Redemptions and Send it to the downstream applications like HAVI Extract transform and load ETL data from multiple federated data sources JSON relational database etc with DataFrames in Spark Used Sqoop to import data from Vertica and Oracle Worked extensively on Talend Pig and Hive for ETL processing Responsible for importing log files from various sources into HDFS using Kafka Used Hbase and Cassandra for data store for Microstrategy and Jasper IReport BI reporting Did Experiment with HAWQ to speed up the operation between microstrategy and Hbase Optimizing the Hive queries using Partitioning and Bucketing techniques for controlling the data Developed Unit test cases using Junit testing frameworks Used Importtsv to create dynamin columns in Hbase Experienced in Monitoring Cluster using Cloudera Manager and Ambari Environment Hadoop HDFS HBase Talend MapReduce Kafka Java Hive Pig Sqoop Oozie SQL ETL Cloudera Manager Ambari MySQL Oracle Vertica Senior Software Engineer LendingTree Charlotte NC June 2012 to April 2016 Responsibilities Responsible to manage data coming from different sources and involved in HDFS maintenance and loading of structured and unstructured data Integrated Quartz scheduler with Oozie work flows to get data from multiple data sources parallel using fork Processed Multiple Data sources input to same Reducer using Generic Writable and Multi Input format Created Data Pipeline of Map Reduce programs using Chained Mappers Visualize the HDFS data to customer using BI tool with the help of Hive ODBC Driver Worked on Big Data processing of clinical and nonclinical data using MapReduce Implemented complex MapReduce programs to perform joins on the Map side using Distributed Cache in Java Responsible for importing log files from various sources into HDFS using Flume Created customized BI tool for manager team that perform Query analytics using HiveQL Used Hive and Pig to generate BI reports Imported data using Sqoop to load data from MySQL to HDFS on regular basis Created Partitions Buckets based on State to further process using Bucket based Hive joins Created Hive Generic UDFs to process business logic that varies based on policy Moved Relational Data base data using Sqoop into Hive Dynamic partition tables using staging tables Optimizing the Hive queries using Partitioning and Bucketing techniques for controlling the data Worked on custom Pig Loaders and storage classes to work with variety of data formats such as JSON and XML file formats Used Oozie workflow engine to manage interdependent Hadoop jobs and to automate several types of Hadoop jobs such as Java mapreduce Hive Pig and Sqoop Developed Unit test cases using Junit testing frameworks Experienced in Monitoring Cluster using Cloudera Manager Software Engineer Matlen Silver Charlotte NC February 2010 to May 2012 Responsibilities Used Rational Rose for Use Case Diagram Class Diagrams Sequence diagrams and Object diagrams in design phase Involved in creation of UML diagrams like Class Activity and Sequence Diagrams using modeling tools of IBM Rational Rose Involved in the full life cycle development of the modules for the project Created EC2 instances and implemented large multi node Hadoop clusters in AWS cloud from scratch Configured AWS IAM and Security Groups Responsible for implementing Kerberos creating service principals user accounts keytabs syncing with AD Developed terraform template to deploy Cloudera Manager on AWS Configured different Notifications on AWS Services Installed configured Hadoop Cluster using Puppet MR2 Batch job was written to fetch required data from DB and store the same in CSV static file Spark job to process the files from Vision EMS and AMN Cache to identify the violations and sending the same to Smarts as SNMP traps Automated workflows using shell scripting to schedulecrontab Spark jobs Installed Oozie workflow engine to run multiple Map Reduce Hive HQL and Pig jobs Implemented best income logic using Pig scripts and UDFs Used Eclipse and Visual Studio IDE for application development Used Spring and Net framework for dependency injection Worked with back end database such as Oracle and MS SQL Developed Web application and services using C and ASP net Used Struts MVC for developing presentation layer Used IIS application server for deploying applications Used SOAP XML WCF Web services for transferring data between different applications Used MVC design pattern for designing application Persistence layer was implemented using Hibernate Framework Integrated Hibernate with Spring framework Worked with complex SQL queries SQL Joins and Stored Procedures using TOAD for data retrieval and update Used JUnit and NUnit for performing Unit Testing Used Log4J to capture the logs that included runtime exceptions Environment EclipseMicrosoft Visual Studio Web Services UML MVC NHibernate JSP WSDL JMS AJAX JavaScript JunitNUnit PLSQL Oracle 10G SVN TFS Education Bachelors Skills Apache hadoop oozie 9 years Hadoop 9 years Hive 9 years Mapreduce 9 years Oozie 9 years Additional Information Technical Skill Hadoop Technologies Apache Hadoop Cloud era Hadoop Distribution HDFS and Map Reduce Technologies HDFS YARN Map Reduce Hive Pig Sqoop Flume Spark Kafka Zookeeper and Oozie CDH 4 CDH 5 HDP 242 Hadoop Ecosystem Hive Pig Sqoop Flume Zookeeper Oozie Streaming Technologies Spark Kafka Storm AWS S3 EC2 JavaJ2EE Technologies Core Java Data Structures Multithreading NOSQL Databases Hbase Cassandra MongoDB Programming Languages Java Linux shell scripting Scala Python Web Technologies HTML CSS JavaScript AJAX JSP DOM XML Databases MySQL SQL Oracle SQL Server DB2 PLSQL Application Servers Web Logic Web Sphere JBoss Software Engineering Scrum Agile methodologies ETL Talend Operating Systems Windows MAC OS UNIX LINUX IDE Tools Eclipse IntelliJ IDEA",
    "entities": [
        "Node",
        "Installed Oozie",
        "MapReduce Hive",
        "Hive ODBC Driver Worked",
        "BI",
        "Oracle Worked",
        "Partitioning",
        "ETL Processes",
        "JSON",
        "Map Reduce Hive Pig Sqoop",
        "Hibernate Framework Integrated Hibernate",
        "Query",
        "MapReduce Implemented",
        "Notifications on AWS Services Installed",
        "Hadoop",
        "HDFS Involved",
        "XML",
        "Atlanta",
        "Created Partitions Buckets",
        "the Hadoop Cluster",
        "Moved Relational Data",
        "JUnit",
        "State",
        "HBase",
        "Automated",
        "Apache Spark",
        "Sr Hadoop Developer Sr Hadoop",
        "Oozie Design",
        "Chained Mappers Visualize",
        "SAP BI Worked",
        "Multi Input",
        "NUnit",
        "Hive Query Language Involved",
        "NC",
        "UML",
        "Hbase Experienced",
        "Hadoop Distributions",
        "Hadoop Data Lake",
        "RD",
        "Importtsv",
        "Created EC2",
        "Hive Queries",
        "Talend",
        "Hadoop Cluster",
        "Hadoop Used Talend",
        "Mobile Offers Redemptions and Send",
        "Multiple Data",
        "Hadoop Big Data",
        "HDP",
        "Data Masking Data",
        "BigDataHadoop",
        "MVC",
        "Spark",
        "Proven Expertise",
        "CSV",
        "Data Ingestion Developed PIG",
        "US",
        "Zeta",
        "Sqoop",
        "Vertica",
        "Spark Used",
        "Created",
        "Pivotal HAWQ Internal External",
        "AWS",
        "Hadoop Architecture",
        "Jasper IReport BI",
        "Oracle",
        "Ambari Environment Hadoop HDFS HBase Talend MapReduce Kafka",
        "ControlM",
        "IDEA",
        "node Hadoop",
        "Created Hive Generic",
        "HDFS Job Tracker Task Tracker",
        "Monitoring Cluster",
        "Talend Pig and",
        "Oozie",
        "SAP Oracle",
        "SQL",
        "DataFrames",
        "Autosys Experienced",
        "IBM Rational Rose Involved",
        "Relational Database Systems",
        "Cassandra Implemented",
        "ETL Talend Operating Systems Windows MAC",
        "HDInsight",
        "MapReduce Pig",
        "DB2 PLSQL Application Servers Web Logic Web Sphere JBoss Software Engineering",
        "Big Data",
        "Hive",
        "HAVI Extract",
        "Wrote",
        "ETL",
        "DB",
        "Apache Hadoop",
        "Performed",
        "Developed Unit",
        "AWS Configured",
        "Additional Information Technical Skill Hadoop Technologies Apache Hadoop Cloud",
        "Work Experience Sr Hadoop Developer Provish Consulting",
        "Onshore",
        "Vision EMS",
        "Windows Application",
        "Flume Created",
        "Oozie Extensively",
        "ASP",
        "Expertise",
        "Present Responsibilities Worked",
        "MapReduce",
        "AD Developed",
        "NoSQL",
        "TOAD",
        "Scala Python Web Technologies HTML CSS",
        "Teradata",
        "Created Data Pipeline",
        "AMN Cache",
        "Bucketing",
        "Map Reduce Technologies",
        "Share"
    ],
    "experience": "Experience in integration of various data sources like SQL Server Oracle Vertica MySQL Flat files Authorized to work in the US for any employer Work Experience Sr Hadoop Developer Provish Consulting Charlotte NC September 2018 to Present Responsibilities Worked on WEB and Windows Application development Worked on the application which talks to different technological platforms such as Share point SAP Oracle and SQL Worked with BI teams in generating the reports and designing ETL workflows on SAP BI Worked on 3Tier architecture of application development Used OOPS concepts in application development Worked as coOrdinator between Onshore and Offshore teams Worked closely with business and gather the requirements Worked on importing data from various sources and performed transformations using MapReduce Hive to load data into HDFS Involved in complete implementation lifecycle specialized in writing custom MapReduce Pig and Hive programs Handled importing of data from RDBMS into HDFS using Sqoop Managing data flow into Pivotal HAWQ Internal External tables Experienced in data cleansing processing using Pig Latin operations and UDFs Experienced in writing Hive Queries for analyzing data in Hive warehouse using Hive Query Language Involved in creating Hive tables loading with data and writing hive queries to process the data Created scripts to automate the process of Data Ingestion Developed PIG scripts for source data validation and transformation Performed various performance optimizations like using distributed cache for small datasets Partitioning Bucketing in Hive and Map Side Joins Managing and scheduling jobs to remove the duplicate log data files in HDFS using Oozie Extensively used HiveHQL or Hive queries to query or search for particular string in Hive tables in HDFS Experience in developing customized UDFs in java to extend Hive and Pig Latin functionality Created HBase tables to store various data formats for data coming from different portfolios Environment Horton works Map Reduce HBase HDFS Hive Pig Java SQL Cloudera Manager Sqoop Flume Oozie Sr Hadoop Developer Trimble Navigation Limited Chantilly VA May 2016 to August 2018 Responsibilities Worked as a part RD team in Zeta experimenting with emerging technologies Involved in Installing Configuring Hadoop Eco System Cloudera Manager using CDH54 and Horton works Distribution Worked on POC to upgrade the tradition relational data process to Hadoop Used Talend as ETL tool with Hadoop and other database components Used ControlM was workflow job scheduler for all the bigdata Talend jobs Played a key role in installation and configuration of the various Hadoop ecosystem tools such asSolr Kafka Pig HBase and Cassandra Implemented multiple Map Reduce Jobs in java for data cleansing and preprocessing Wrote complex Hive queries and UDFs in Java and Python Involved in implementing an HDInsight version 33 clusters which is based on spark version 151 Responsible for data extraction and data ingestion from different data sources into Hadoop Data Lake by creating ETL pipelines using Pig and Hive Job duties involved the design development of various modules in Hadoop Big Data Platform and processing data using Map Reduce Hive Pig Sqoop and Oozie Design developed and tested Map Reduce programs on Mobile Offers Redemptions and Send it to the downstream applications like HAVI Extract transform and load ETL data from multiple federated data sources JSON relational database etc with DataFrames in Spark Used Sqoop to import data from Vertica and Oracle Worked extensively on Talend Pig and Hive for ETL processing Responsible for importing log files from various sources into HDFS using Kafka Used Hbase and Cassandra for data store for Microstrategy and Jasper IReport BI reporting Did Experiment with HAWQ to speed up the operation between microstrategy and Hbase Optimizing the Hive queries using Partitioning and Bucketing techniques for controlling the data Developed Unit test cases using Junit testing frameworks Used Importtsv to create dynamin columns in Hbase Experienced in Monitoring Cluster using Cloudera Manager and Ambari Environment Hadoop HDFS HBase Talend MapReduce Kafka Java Hive Pig Sqoop Oozie SQL ETL Cloudera Manager Ambari MySQL Oracle Vertica Senior Software Engineer LendingTree Charlotte NC June 2012 to April 2016 Responsibilities Responsible to manage data coming from different sources and involved in HDFS maintenance and loading of structured and unstructured data Integrated Quartz scheduler with Oozie work flows to get data from multiple data sources parallel using fork Processed Multiple Data sources input to same Reducer using Generic Writable and Multi Input format Created Data Pipeline of Map Reduce programs using Chained Mappers Visualize the HDFS data to customer using BI tool with the help of Hive ODBC Driver Worked on Big Data processing of clinical and nonclinical data using MapReduce Implemented complex MapReduce programs to perform joins on the Map side using Distributed Cache in Java Responsible for importing log files from various sources into HDFS using Flume Created customized BI tool for manager team that perform Query analytics using HiveQL Used Hive and Pig to generate BI reports Imported data using Sqoop to load data from MySQL to HDFS on regular basis Created Partitions Buckets based on State to further process using Bucket based Hive joins Created Hive Generic UDFs to process business logic that varies based on policy Moved Relational Data base data using Sqoop into Hive Dynamic partition tables using staging tables Optimizing the Hive queries using Partitioning and Bucketing techniques for controlling the data Worked on custom Pig Loaders and storage classes to work with variety of data formats such as JSON and XML file formats Used Oozie workflow engine to manage interdependent Hadoop jobs and to automate several types of Hadoop jobs such as Java mapreduce Hive Pig and Sqoop Developed Unit test cases using Junit testing frameworks Experienced in Monitoring Cluster using Cloudera Manager Software Engineer Matlen Silver Charlotte NC February 2010 to May 2012 Responsibilities Used Rational Rose for Use Case Diagram Class Diagrams Sequence diagrams and Object diagrams in design phase Involved in creation of UML diagrams like Class Activity and Sequence Diagrams using modeling tools of IBM Rational Rose Involved in the full life cycle development of the modules for the project Created EC2 instances and implemented large multi node Hadoop clusters in AWS cloud from scratch Configured AWS IAM and Security Groups Responsible for implementing Kerberos creating service principals user accounts keytabs syncing with AD Developed terraform template to deploy Cloudera Manager on AWS Configured different Notifications on AWS Services Installed configured Hadoop Cluster using Puppet MR2 Batch job was written to fetch required data from DB and store the same in CSV static file Spark job to process the files from Vision EMS and AMN Cache to identify the violations and sending the same to Smarts as SNMP traps Automated workflows using shell scripting to schedulecrontab Spark jobs Installed Oozie workflow engine to run multiple Map Reduce Hive HQL and Pig jobs Implemented best income logic using Pig scripts and UDFs Used Eclipse and Visual Studio IDE for application development Used Spring and Net framework for dependency injection Worked with back end database such as Oracle and MS SQL Developed Web application and services using C and ASP net Used Struts MVC for developing presentation layer Used IIS application server for deploying applications Used SOAP XML WCF Web services for transferring data between different applications Used MVC design pattern for designing application Persistence layer was implemented using Hibernate Framework Integrated Hibernate with Spring framework Worked with complex SQL queries SQL Joins and Stored Procedures using TOAD for data retrieval and update Used JUnit and NUnit for performing Unit Testing Used Log4J to capture the logs that included runtime exceptions Environment EclipseMicrosoft Visual Studio Web Services UML MVC NHibernate JSP WSDL JMS AJAX JavaScript JunitNUnit PLSQL Oracle 10 G SVN TFS Education Bachelors Skills Apache hadoop oozie 9 years Hadoop 9 years Hive 9 years Mapreduce 9 years Oozie 9 years Additional Information Technical Skill Hadoop Technologies Apache Hadoop Cloud era Hadoop Distribution HDFS and Map Reduce Technologies HDFS YARN Map Reduce Hive Pig Sqoop Flume Spark Kafka Zookeeper and Oozie CDH 4 CDH 5 HDP 242 Hadoop Ecosystem Hive Pig Sqoop Flume Zookeeper Oozie Streaming Technologies Spark Kafka Storm AWS S3 EC2 JavaJ2EE Technologies Core Java Data Structures Multithreading NOSQL Databases Hbase Cassandra MongoDB Programming Languages Java Linux shell scripting Scala Python Web Technologies HTML CSS JavaScript AJAX JSP DOM XML Databases MySQL SQL Oracle SQL Server DB2 PLSQL Application Servers Web Logic Web Sphere JBoss Software Engineering Scrum Agile methodologies ETL Talend Operating Systems Windows MAC OS UNIX LINUX IDE Tools Eclipse IntelliJ IDEA",
    "extracted_keywords": [
        "Sr",
        "Hadoop",
        "Developer",
        "Sr",
        "Hadoop",
        "span",
        "lDeveloperspan",
        "Sr",
        "Hadoop",
        "Developer",
        "Provish",
        "Consulting",
        "Atlanta",
        "GA",
        "years",
        "IT",
        "experience",
        "BigDataHadoop",
        "web",
        "Windows",
        "application",
        "development",
        "net",
        "Java",
        "Hands",
        "experience",
        "Hadoop",
        "stack",
        "MapReduce",
        "HDFS",
        "Sqoop",
        "Pig",
        "Hive",
        "HBase",
        "SPARK",
        "KafkaControlM",
        "Oozie",
        "ZooKeeper",
        "Talend",
        "experience",
        "configuring",
        "Hadoop",
        "Cluster",
        "Hadoop",
        "Distributions",
        "Hortonworks",
        "Cloudera",
        "Excellent",
        "experience",
        "components",
        "Apache",
        "Hadoop",
        "ecosystem",
        "components",
        "Map",
        "Reduce",
        "Hive",
        "HBase",
        "PIG",
        "Sqoop",
        "Spark",
        "Kafka",
        "Flume",
        "Zookeeper",
        "Oozie",
        "Storm",
        "Expertise",
        "depth",
        "understandingknowledge",
        "Hadoop",
        "Architecture",
        "components",
        "HDFS",
        "Job",
        "Tracker",
        "Task",
        "Tracker",
        "Name",
        "Node",
        "Data",
        "Node",
        "MRv1",
        "andMRv2",
        "YARN",
        "data",
        "architecture",
        "data",
        "ingestion",
        "pipeline",
        "design",
        "Hadoop",
        "information",
        "architecture",
        "data",
        "modeling",
        "data",
        "mining",
        "machine",
        "learning",
        "data",
        "Expertise",
        "Apache",
        "Spark",
        "API",
        "Big",
        "Data",
        "distribution",
        "cluster",
        "environment",
        "implementation",
        "log",
        "producer",
        "Scala",
        "application",
        "logs",
        "log",
        "Kafka",
        "Zookeeper",
        "log",
        "collection",
        "platform",
        "Flume",
        "log",
        "data",
        "sources",
        "knowledge",
        "building",
        "scheduling",
        "Big",
        "Data",
        "help",
        "OOZIEand",
        "Autosys",
        "data",
        "Data",
        "sources",
        "Teradata",
        "DB2",
        "Sqoop",
        "HDFS",
        "Relational",
        "Database",
        "Systems",
        "RDBMS",
        "viceversa",
        "load",
        "Hive",
        "tables",
        "Proven",
        "Expertise",
        "analytics",
        "Big",
        "Data",
        "Map",
        "Reduce",
        "Hive",
        "Pig",
        "Talend",
        "time",
        "analytics",
        "databases",
        "HBase",
        "Cassandra",
        "ETL",
        "data",
        "HadoopNoSQL",
        "modeling",
        "Data",
        "migration",
        "Data",
        "Masking",
        "Data",
        "Data",
        "profiling",
        "ETL",
        "Processes",
        "features",
        "data",
        "warehouses",
        "Experience",
        "integration",
        "data",
        "sources",
        "SQL",
        "Server",
        "Oracle",
        "Vertica",
        "MySQL",
        "files",
        "US",
        "employer",
        "Work",
        "Experience",
        "Sr",
        "Hadoop",
        "Developer",
        "Provish",
        "Consulting",
        "Charlotte",
        "NC",
        "September",
        "Present",
        "Responsibilities",
        "WEB",
        "Windows",
        "Application",
        "development",
        "application",
        "platforms",
        "Share",
        "point",
        "SAP",
        "Oracle",
        "SQL",
        "BI",
        "teams",
        "reports",
        "ETL",
        "workflows",
        "SAP",
        "BI",
        "architecture",
        "application",
        "development",
        "OOPS",
        "concepts",
        "application",
        "development",
        "coOrdinator",
        "Onshore",
        "Offshore",
        "teams",
        "business",
        "requirements",
        "data",
        "sources",
        "transformations",
        "MapReduce",
        "Hive",
        "data",
        "HDFS",
        "implementation",
        "lifecycle",
        "custom",
        "MapReduce",
        "Pig",
        "Hive",
        "programs",
        "importing",
        "data",
        "RDBMS",
        "HDFS",
        "Sqoop",
        "Managing",
        "data",
        "flow",
        "HAWQ",
        "Internal",
        "External",
        "tables",
        "data",
        "cleansing",
        "processing",
        "Pig",
        "Latin",
        "operations",
        "UDFs",
        "Hive",
        "Queries",
        "data",
        "Hive",
        "warehouse",
        "Hive",
        "Query",
        "Language",
        "Hive",
        "tables",
        "data",
        "hive",
        "queries",
        "data",
        "scripts",
        "process",
        "Data",
        "Ingestion",
        "Developed",
        "PIG",
        "scripts",
        "source",
        "data",
        "validation",
        "transformation",
        "performance",
        "optimizations",
        "cache",
        "datasets",
        "Bucketing",
        "Hive",
        "Map",
        "Side",
        "Managing",
        "scheduling",
        "jobs",
        "log",
        "data",
        "files",
        "HDFS",
        "Oozie",
        "HiveHQL",
        "Hive",
        "queries",
        "query",
        "search",
        "string",
        "Hive",
        "tables",
        "HDFS",
        "Experience",
        "UDFs",
        "Hive",
        "Pig",
        "Latin",
        "functionality",
        "Created",
        "HBase",
        "data",
        "formats",
        "data",
        "portfolios",
        "Environment",
        "Horton",
        "Map",
        "Reduce",
        "HBase",
        "HDFS",
        "Hive",
        "Pig",
        "Java",
        "SQL",
        "Cloudera",
        "Manager",
        "Sqoop",
        "Flume",
        "Oozie",
        "Sr",
        "Hadoop",
        "Developer",
        "Trimble",
        "Navigation",
        "Limited",
        "Chantilly",
        "VA",
        "May",
        "August",
        "Responsibilities",
        "part",
        "RD",
        "team",
        "Zeta",
        "technologies",
        "Configuring",
        "Hadoop",
        "Eco",
        "System",
        "Cloudera",
        "Manager",
        "CDH54",
        "Horton",
        "Distribution",
        "Worked",
        "POC",
        "tradition",
        "data",
        "process",
        "Hadoop",
        "Talend",
        "ETL",
        "tool",
        "Hadoop",
        "database",
        "components",
        "ControlM",
        "job",
        "scheduler",
        "bigdata",
        "Talend",
        "jobs",
        "role",
        "installation",
        "configuration",
        "Hadoop",
        "ecosystem",
        "tools",
        "asSolr",
        "Kafka",
        "Pig",
        "HBase",
        "Cassandra",
        "Map",
        "Reduce",
        "Jobs",
        "java",
        "data",
        "cleansing",
        "Wrote",
        "Hive",
        "queries",
        "UDFs",
        "Java",
        "Python",
        "HDInsight",
        "version",
        "clusters",
        "spark",
        "version",
        "data",
        "extraction",
        "data",
        "ingestion",
        "data",
        "sources",
        "Hadoop",
        "Data",
        "Lake",
        "ETL",
        "pipelines",
        "Pig",
        "Hive",
        "Job",
        "duties",
        "design",
        "development",
        "modules",
        "Hadoop",
        "Big",
        "Data",
        "Platform",
        "processing",
        "data",
        "Map",
        "Reduce",
        "Hive",
        "Pig",
        "Sqoop",
        "Oozie",
        "Design",
        "Map",
        "Reduce",
        "programs",
        "Mobile",
        "Offers",
        "Redemptions",
        "applications",
        "HAVI",
        "Extract",
        "transform",
        "load",
        "ETL",
        "data",
        "data",
        "sources",
        "database",
        "DataFrames",
        "Spark",
        "Sqoop",
        "data",
        "Vertica",
        "Oracle",
        "Talend",
        "Pig",
        "Hive",
        "ETL",
        "processing",
        "log",
        "files",
        "sources",
        "HDFS",
        "Kafka",
        "Used",
        "Hbase",
        "Cassandra",
        "data",
        "store",
        "Microstrategy",
        "Jasper",
        "IReport",
        "BI",
        "reporting",
        "Experiment",
        "HAWQ",
        "operation",
        "microstrategy",
        "Hbase",
        "Hive",
        "queries",
        "Partitioning",
        "Bucketing",
        "techniques",
        "data",
        "Developed",
        "Unit",
        "test",
        "cases",
        "Junit",
        "testing",
        "frameworks",
        "Importtsv",
        "columns",
        "Hbase",
        "Monitoring",
        "Cluster",
        "Cloudera",
        "Manager",
        "Ambari",
        "Environment",
        "Hadoop",
        "HDFS",
        "HBase",
        "Talend",
        "MapReduce",
        "Kafka",
        "Java",
        "Hive",
        "Pig",
        "Sqoop",
        "Oozie",
        "SQL",
        "ETL",
        "Cloudera",
        "Manager",
        "Ambari",
        "MySQL",
        "Oracle",
        "Vertica",
        "Senior",
        "Software",
        "Engineer",
        "LendingTree",
        "Charlotte",
        "NC",
        "June",
        "April",
        "Responsibilities",
        "data",
        "sources",
        "HDFS",
        "maintenance",
        "loading",
        "data",
        "Integrated",
        "Quartz",
        "scheduler",
        "Oozie",
        "work",
        "data",
        "data",
        "sources",
        "fork",
        "Processed",
        "Multiple",
        "Data",
        "sources",
        "input",
        "Reducer",
        "Generic",
        "Writable",
        "Multi",
        "Input",
        "format",
        "Created",
        "Data",
        "Pipeline",
        "Map",
        "Reduce",
        "programs",
        "Mappers",
        "Visualize",
        "HDFS",
        "data",
        "customer",
        "BI",
        "tool",
        "help",
        "Hive",
        "ODBC",
        "Driver",
        "Big",
        "Data",
        "processing",
        "data",
        "MapReduce",
        "MapReduce",
        "programs",
        "joins",
        "Map",
        "side",
        "Cache",
        "Java",
        "Responsible",
        "log",
        "files",
        "sources",
        "HDFS",
        "Flume",
        "Created",
        "BI",
        "tool",
        "manager",
        "team",
        "Query",
        "analytics",
        "HiveQL",
        "Used",
        "Hive",
        "Pig",
        "BI",
        "reports",
        "data",
        "Sqoop",
        "data",
        "MySQL",
        "HDFS",
        "basis",
        "Partitions",
        "Buckets",
        "State",
        "process",
        "Bucket",
        "Hive",
        "Hive",
        "Generic",
        "UDFs",
        "business",
        "logic",
        "policy",
        "Moved",
        "Relational",
        "Data",
        "base",
        "data",
        "Sqoop",
        "Hive",
        "partition",
        "tables",
        "staging",
        "tables",
        "Hive",
        "queries",
        "Partitioning",
        "Bucketing",
        "techniques",
        "data",
        "custom",
        "Pig",
        "Loaders",
        "storage",
        "classes",
        "variety",
        "data",
        "formats",
        "JSON",
        "XML",
        "file",
        "formats",
        "Oozie",
        "workflow",
        "engine",
        "Hadoop",
        "jobs",
        "types",
        "Hadoop",
        "jobs",
        "Java",
        "mapreduce",
        "Hive",
        "Pig",
        "Sqoop",
        "Developed",
        "Unit",
        "test",
        "cases",
        "Junit",
        "testing",
        "frameworks",
        "Monitoring",
        "Cluster",
        "Cloudera",
        "Manager",
        "Software",
        "Engineer",
        "Matlen",
        "Silver",
        "Charlotte",
        "NC",
        "February",
        "May",
        "Responsibilities",
        "Rational",
        "Rose",
        "Use",
        "Case",
        "Diagram",
        "Class",
        "Diagrams",
        "Sequence",
        "diagrams",
        "Object",
        "diagrams",
        "design",
        "phase",
        "creation",
        "UML",
        "diagrams",
        "Class",
        "Activity",
        "Sequence",
        "Diagrams",
        "modeling",
        "tools",
        "IBM",
        "Rational",
        "Rose",
        "life",
        "cycle",
        "development",
        "modules",
        "project",
        "EC2",
        "instances",
        "multi",
        "node",
        "Hadoop",
        "clusters",
        "AWS",
        "cloud",
        "scratch",
        "Configured",
        "AWS",
        "IAM",
        "Security",
        "Groups",
        "Kerberos",
        "service",
        "principals",
        "user",
        "keytabs",
        "AD",
        "terraform",
        "template",
        "Cloudera",
        "Manager",
        "AWS",
        "Notifications",
        "AWS",
        "Services",
        "Installed",
        "Hadoop",
        "Cluster",
        "Puppet",
        "MR2",
        "Batch",
        "job",
        "data",
        "DB",
        "CSV",
        "file",
        "Spark",
        "job",
        "files",
        "Vision",
        "EMS",
        "AMN",
        "Cache",
        "violations",
        "Smarts",
        "SNMP",
        "workflows",
        "shell",
        "scripting",
        "Spark",
        "jobs",
        "Installed",
        "Oozie",
        "workflow",
        "engine",
        "Map",
        "Reduce",
        "Hive",
        "HQL",
        "Pig",
        "jobs",
        "income",
        "logic",
        "Pig",
        "scripts",
        "UDFs",
        "Eclipse",
        "Visual",
        "Studio",
        "IDE",
        "application",
        "development",
        "Spring",
        "framework",
        "dependency",
        "injection",
        "end",
        "database",
        "Oracle",
        "MS",
        "SQL",
        "Developed",
        "Web",
        "application",
        "services",
        "C",
        "ASP",
        "net",
        "Struts",
        "MVC",
        "presentation",
        "layer",
        "IIS",
        "application",
        "server",
        "applications",
        "SOAP",
        "XML",
        "WCF",
        "Web",
        "services",
        "data",
        "applications",
        "MVC",
        "design",
        "pattern",
        "application",
        "Persistence",
        "layer",
        "Hibernate",
        "Framework",
        "Integrated",
        "Hibernate",
        "Spring",
        "framework",
        "SQL",
        "SQL",
        "Joins",
        "Stored",
        "Procedures",
        "TOAD",
        "data",
        "retrieval",
        "JUnit",
        "NUnit",
        "Unit",
        "Testing",
        "Log4J",
        "logs",
        "runtime",
        "exceptions",
        "Environment",
        "EclipseMicrosoft",
        "Visual",
        "Studio",
        "Web",
        "Services",
        "UML",
        "MVC",
        "NHibernate",
        "JSP",
        "WSDL",
        "JMS",
        "AJAX",
        "JavaScript",
        "JunitNUnit",
        "PLSQL",
        "Oracle",
        "G",
        "SVN",
        "TFS",
        "Education",
        "Bachelors",
        "Skills",
        "Apache",
        "hadoop",
        "oozie",
        "years",
        "Hadoop",
        "years",
        "Hive",
        "years",
        "years",
        "Oozie",
        "years",
        "Additional",
        "Information",
        "Technical",
        "Skill",
        "Hadoop",
        "Technologies",
        "Apache",
        "Hadoop",
        "Cloud",
        "era",
        "Hadoop",
        "Distribution",
        "HDFS",
        "Map",
        "Reduce",
        "Technologies",
        "HDFS",
        "YARN",
        "Map",
        "Reduce",
        "Hive",
        "Pig",
        "Sqoop",
        "Flume",
        "Spark",
        "Kafka",
        "Zookeeper",
        "Oozie",
        "CDH",
        "CDH",
        "HDP",
        "Hadoop",
        "Ecosystem",
        "Hive",
        "Pig",
        "Sqoop",
        "Flume",
        "Zookeeper",
        "Oozie",
        "Streaming",
        "Technologies",
        "Spark",
        "Kafka",
        "Storm",
        "S3",
        "EC2",
        "JavaJ2EE",
        "Technologies",
        "Core",
        "Java",
        "Data",
        "Structures",
        "Multithreading",
        "NOSQL",
        "Hbase",
        "Cassandra",
        "MongoDB",
        "Programming",
        "Languages",
        "Java",
        "Linux",
        "shell",
        "Scala",
        "Python",
        "Web",
        "Technologies",
        "HTML",
        "CSS",
        "JavaScript",
        "AJAX",
        "JSP",
        "DOM",
        "XML",
        "MySQL",
        "SQL",
        "Oracle",
        "SQL",
        "Server",
        "DB2",
        "PLSQL",
        "Application",
        "Servers",
        "Web",
        "Logic",
        "Web",
        "Sphere",
        "JBoss",
        "Software",
        "Engineering",
        "Scrum",
        "Agile",
        "methodologies",
        "ETL",
        "Talend",
        "Operating",
        "Systems",
        "Windows",
        "MAC",
        "UNIX",
        "LINUX",
        "IDE",
        "Tools",
        "Eclipse",
        "IntelliJ",
        "IDEA"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T21:58:58.653942",
    "resume_data": "Sr Hadoop Developer Sr Hadoop span lDeveloperspan Sr Hadoop Developer Provish Consulting Atlanta GA Over 9 years of professional IT experience that includes of BigDataHadoop and of web and Windows application development with net and Java Hands on experience with the Hadoop stack MapReduce HDFS Sqoop Pig Hive HBase SPARK KafkaControlM Oozie ZooKeeper and Talend Have experience in configuring and administrating the Hadoop Cluster using major Hadoop Distributions like Hortonworks and Cloudera Excellent experience in developing different components using Apache Hadoop ecosystem components like Map Reduce Hive HBase PIG Sqoop Spark Kafka Flume Zookeeper Oozie and Storm Expertise in depth understandingknowledge of Hadoop Architecture and various components such as HDFS Job Tracker Task Tracker Name Node Data Node MRv1 andMRv2 YARN Experienced with data architecture including data ingestion pipeline design Hadoop information architecture data modeling and data mining machine learning and advanced data processing Expertise in writing Apache Spark streaming API on Big Data distribution in the active cluster environment Experienced on implementation of a log producer in Scala that watches for application logs transform incremental log and sends them to a Kafka and Zookeeper based log collection platform Experienced in working with Flume to load the log data from multiple sources directly intoHDFS Excellent knowledge in building and scheduling Big Data workflows with the help of OOZIEand Autosys Experienced in importing and exporting data from the different Data sources like Teradata and DB2 using Sqoop from HDFS to Relational Database Systems RDBMS and viceversa and load into partitioned Hive tables Proven Expertise in performing analytics on Big Data using Map Reduce Hive Pig and Talend Experienced with performing real time analytics on NoSQL databases like HBase and Cassandra Experienced with ETL to load data into HadoopNoSQL Experienced with Dimensional modeling Data migration Data Masking Data cleansing Data profiling and ETL Processes features for data warehouses Experience in integration of various data sources like SQL Server Oracle Vertica MySQL Flat files Authorized to work in the US for any employer Work Experience Sr Hadoop Developer Provish Consulting Charlotte NC September 2018 to Present Responsibilities Worked on WEB and Windows Application development Worked on the application which talks to different technological platforms such as Share point SAP Oracle and SQL Worked with BI teams in generating the reports and designing ETL workflows on SAP BI Worked on 3Tier architecture of application development Used OOPS concepts in application development Worked as coOrdinator between Onshore and Offshore teams Worked closely with business and gather the requirements Worked on importing data from various sources and performed transformations using MapReduce Hive to load data into HDFS Involved in complete implementation lifecycle specialized in writing custom MapReduce Pig and Hive programs Handled importing of data from RDBMS into HDFS using Sqoop Managing data flow into Pivotal HAWQ Internal External tables Experienced in data cleansing processing using Pig Latin operations and UDFs Experienced in writing Hive Queries for analyzing data in Hive warehouse using Hive Query Language Involved in creating Hive tables loading with data and writing hive queries to process the data Created scripts to automate the process of Data Ingestion Developed PIG scripts for source data validation and transformation Performed various performance optimizations like using distributed cache for small datasets Partitioning Bucketing in Hive and Map Side Joins Managing and scheduling jobs to remove the duplicate log data files in HDFS using Oozie Extensively used HiveHQL or Hive queries to query or search for particular string in Hive tables in HDFS Experience in developing customized UDFs in java to extend Hive and Pig Latin functionality Created HBase tables to store various data formats for data coming from different portfolios Environment Horton works Map Reduce HBase HDFS Hive Pig Java SQL Cloudera Manager Sqoop Flume Oozie Sr Hadoop Developer Trimble Navigation Limited Chantilly VA May 2016 to August 2018 Responsibilities Worked as a part RD team in Zeta experimenting with emerging technologies Involved in Installing Configuring Hadoop Eco System Cloudera Manager using CDH54 and Horton works Distribution Worked on POC to upgrade the tradition relational data process to Hadoop Used Talend as ETL tool with Hadoop and other database components Used ControlM was workflow job scheduler for all the bigdata Talend jobs Played a key role in installation and configuration of the various Hadoop ecosystem tools such asSolr Kafka Pig HBase and Cassandra Implemented multiple Map Reduce Jobs in java for data cleansing and preprocessing Wrote complex Hive queries and UDFs in Java and Python Involved in implementing an HDInsight version 33 clusters which is based on spark version 151 Responsible for data extraction and data ingestion from different data sources into Hadoop Data Lake by creating ETL pipelines using Pig and Hive Job duties involved the design development of various modules in Hadoop Big Data Platform and processing data using Map Reduce Hive Pig Sqoop and Oozie Design developed and tested Map Reduce programs on Mobile Offers Redemptions and Send it to the downstream applications like HAVI Extract transform and load ETL data from multiple federated data sources JSON relational database etc with DataFrames in Spark Used Sqoop to import data from Vertica and Oracle Worked extensively on Talend Pig and Hive for ETL processing Responsible for importing log files from various sources into HDFS using Kafka Used Hbase and Cassandra for data store for Microstrategy and Jasper IReport BI reporting Did Experiment with HAWQ to speed up the operation between microstrategy and Hbase Optimizing the Hive queries using Partitioning and Bucketing techniques for controlling the data Developed Unit test cases using Junit testing frameworks Used Importtsv to create dynamin columns in Hbase Experienced in Monitoring Cluster using Cloudera Manager and Ambari Environment Hadoop HDFS HBase Talend MapReduce Kafka Java Hive Pig Sqoop Oozie SQL ETL Cloudera Manager Ambari MySQL Oracle Vertica Senior Software Engineer LendingTree Charlotte NC June 2012 to April 2016 Responsibilities Responsible to manage data coming from different sources and involved in HDFS maintenance and loading of structured and unstructured data Integrated Quartz scheduler with Oozie work flows to get data from multiple data sources parallel using fork Processed Multiple Data sources input to same Reducer using Generic Writable and Multi Input format Created Data Pipeline of Map Reduce programs using Chained Mappers Visualize the HDFS data to customer using BI tool with the help of Hive ODBC Driver Worked on Big Data processing of clinical and nonclinical data using MapReduce Implemented complex MapReduce programs to perform joins on the Map side using Distributed Cache in Java Responsible for importing log files from various sources into HDFS using Flume Created customized BI tool for manager team that perform Query analytics using HiveQL Used Hive and Pig to generate BI reports Imported data using Sqoop to load data from MySQL to HDFS on regular basis Created Partitions Buckets based on State to further process using Bucket based Hive joins Created Hive Generic UDFs to process business logic that varies based on policy Moved Relational Data base data using Sqoop into Hive Dynamic partition tables using staging tables Optimizing the Hive queries using Partitioning and Bucketing techniques for controlling the data Worked on custom Pig Loaders and storage classes to work with variety of data formats such as JSON and XML file formats Used Oozie workflow engine to manage interdependent Hadoop jobs and to automate several types of Hadoop jobs such as Java mapreduce Hive Pig and Sqoop Developed Unit test cases using Junit testing frameworks Experienced in Monitoring Cluster using Cloudera Manager Software Engineer Matlen Silver Charlotte NC February 2010 to May 2012 Responsibilities Used Rational Rose for Use Case Diagram Class Diagrams Sequence diagrams and Object diagrams in design phase Involved in creation of UML diagrams like Class Activity and Sequence Diagrams using modeling tools of IBM Rational Rose Involved in the full life cycle development of the modules for the project Created EC2 instances and implemented large multi node Hadoop clusters in AWS cloud from scratch Configured AWS IAM and Security Groups Responsible for implementing Kerberos creating service principals user accounts keytabs syncing with AD Developed terraform template to deploy Cloudera Manager on AWS Configured different Notifications on AWS Services Installed configured Hadoop Cluster using Puppet MR2 Batch job was written to fetch required data from DB and store the same in CSV static file Spark job to process the files from Vision EMS and AMN Cache to identify the violations and sending the same to Smarts as SNMP traps Automated workflows using shell scripting to schedulecrontab Spark jobs Installed Oozie workflow engine to run multiple Map Reduce Hive HQL and Pig jobs Implemented best income logic using Pig scripts and UDFs Used Eclipse and Visual Studio IDE for application development Used Spring and Net framework for dependency injection Worked with back end database such as Oracle and MS SQL Developed Web application and services using C and ASP net Used Struts MVC for developing presentation layer Used IIS application server for deploying applications Used SOAP XML WCF Web services for transferring data between different applications Used MVC design pattern for designing application Persistence layer was implemented using Hibernate Framework Integrated Hibernate with Spring framework Worked with complex SQL queries SQL Joins and Stored Procedures using TOAD for data retrieval and update Used JUnit and NUnit for performing Unit Testing Used Log4J to capture the logs that included runtime exceptions Environment EclipseMicrosoft Visual Studio Web Services UML MVC NHibernate JSP WSDL JMS AJAX JavaScript JunitNUnit PLSQL Oracle 10G SVN TFS Education Bachelors Skills Apache hadoop oozie 9 years Hadoop 9 years Hive 9 years Mapreduce 9 years Oozie 9 years Additional Information Technical Skill Hadoop Technologies Apache Hadoop Cloud era Hadoop Distribution HDFS and Map Reduce Technologies HDFS YARN Map Reduce Hive Pig Sqoop Flume Spark Kafka Zookeeper and Oozie CDH 4 CDH 5 HDP 242 Hadoop Ecosystem Hive Pig Sqoop Flume Zookeeper Oozie Streaming Technologies Spark Kafka Storm AWS S3 EC2 JavaJ2EE Technologies Core Java Data Structures Multithreading NOSQL Databases Hbase Cassandra MongoDB Programming Languages Java Linux shell scripting Scala Python Web Technologies HTML CSS JavaScript AJAX JSP DOM XML Databases MySQL SQL Oracle SQL Server DB2 PLSQL Application Servers Web Logic Web Sphere JBoss Software Engineering Scrum Agile methodologies ETL Talend Operating Systems Windows MAC OS UNIX LINUX IDE Tools Eclipse IntelliJ IDEA",
    "unique_id": "fbe5f1fa-d802-46d7-a3cd-50e49cc101ec"
}