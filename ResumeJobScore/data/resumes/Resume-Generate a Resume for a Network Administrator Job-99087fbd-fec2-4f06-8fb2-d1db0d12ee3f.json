{
    "clean_data": "JavaHadoop Developer span lJavaspanHadoop span lDeveloperspan JavaHadoop Developer TMobile Bellevue WA Certified Apache Spark Developer Over 7 years of experience in software architecture design development testing and maintenance of web and enterprise applications with 3 years of experience in using Hadoop and Hadoop Ecosystems HDFS YARN MAPREDUCE PIG HIVE HBASE SQOOP FLUME ZOOKEEPER OOZIE and 4 years of experience as Java developer Excellent understanding of Hadoop architecture and various components such as HDFS YARN Resource Manager Node Manager Name Node Data Node and Map Reduce programming paradigm Hands on experience using Cloudera Horton works and BigInsights Quick start Hadoop Distributions Hands on experience on Scalaand Python Strong experience in developing Storm topology and ActiveMQ using Java Good understanding how Lambda architectureand Kappa architecture Responsible for writing MapReduce programsHadoop pig and Hive Experience in importing and exporting data using Sqoop from HDFS to Relational Database Systems and viceversa Experience loading data to Hive partitions and creating buckets in Hive Experience developing custom SerDes in HIVE Experience in extending Hive and Pig core functionality by writing custom UDFs Knowledge in extending Hive core functionality by writing custom UDFs like UDAFs and UDTFs Expertise in analysis using PIG HIVE and MapReduce Logical Implementation and interaction with HBase Developed MapReduce jobs to automate transfer the data from HBase Experience in developing ETL process using MapReduce Framework in java Knowledge in efficiently using Sparks memory caching for iterative processing using Scala Good understanding how Spark instructions are translated into jobs Strong Knowledge developing web service using Playframeworkand Akka Actors Good Knowledge on Hadoop Cluster architecture and monitoring the cluster Knowledge in jobworkflow scheduling and monitoring tools like Oozie and Zookeeper Strong understanding of NoSQL databases like HBase MongoDB Cassandra Good working knowledge with Agile for SDLC Knowledge in Amazon AWS concepts like EMR S3 and EC2 web services Efficient team player having strong desire and capabilities to efficiently convert Customers requirements into applications Authorized to work in the US for any employer Work Experience JavaHadoop Developer TMobile Bellevue WA January 2019 to Present Project BEAM Description The transformation program of our billing systems is the main objective of uprising project It will provide new capabilities for flexibility efficiency and new business models Uprising project covers various domains like Business Intelligence and Engineering etc Integrated Data warehouse IDW is TMobile central repository of data in Hadoop and Teradata ecosystems and IDW is part of BI As part of data ingestion team we will bring incremental data from various source systems into Hadoop to maintain one central repository Responsibilities Developed Storm topology to ingest data from various source into hadoop data lake Configured ActiveMQ for enterprise and resolved ActiveMQ issues Developed web application using HBase and Hive API to compare schema between HBase and Hive tables Used JVM monitor to monitor threads and memory usage of HBase and Hive schema check web application Developed code to generate Hive DDLs from source DDLs Created HBase tables using Sqoop from Relational Database Oracle Developed code to import JSON data into MYSQL database Developed code to import data SQLServer into HDFS and created Hive views on data in HDFS using Spark Created scripts to append data from temporary HBase table to target HBase table in Spark Developed Hive queries for the analysis Handling structured and unstructured data and applying ETL processes Handled importing of data from various data sources performance optimization of Sqoop Developed Oozie workflow to compare data in Hive and Source Assessed existing and EDW technologies and methods to ensure our EDWBI architecture meet the needs of the business and enterprise and allows for business growth Assisted with data capacity planning and node forecasting Collaborated with the infrastructure network database application and BI teams to ensure data quality and availability Environment Hadoop Map Reduce HDFS Hive HBase Pig Java Hadoop distribution of Hortonworks SQLUNIX Shell Scripting Oracle SQLServer MySQL Big Data Developer KCPL Kansas City MO August 2018 to January 2019 Project Meter Data Management Description Collected managed and distributed information to the users of databases applications and platforms that are supported by the KCPL team Key areas were leveraging this capability to drive financial benefit to the company in Information Management Marketing and Risk Capabilities Used substantial technical infrastructure to support the data management model and operational philosophy which provided the flexibility and reliability needed for inclass production analytics and online capability Responsibilities Worked on Big Data Hadoop cluster implementation and data integration in developing largescale system software Developed MapReduce programs to parse the raw data populate staging tables and store the refined data in partitioned tables in the EDW Capturing data from existing databases that provide SQL interfaces using Sqoop Worked extensively with Sqoop for importing and exporting the data from HDFS to Relational Database system Oracle and viceversa Loading data into HDFS Develop and maintains complex outbound notification applications that run on custom architectures using diverse technologies including Java XML JMS and Web Services Created Hive queries that helped market analysts spot emerging trends by comparing fresh data with EDW reference tables and historical metrics Enabled speedy reviews and first mover advantages by using Oozie to automate data loading into the Hadoop Distributed File System and PIG to preprocess the data Provided design recommendations and thought leadership to sponsorsstakeholders that improved review processes and resolved technical problems Shared responsibility for administration of Hadoop Hive and Pig Helped business processes by developing and configuring Hadoop ecosystem components that moved data from individual servers to HDFS Created HBase tables to load large sets of structured semistructured and unstructured data coming from UNIX NoSQL and a variety of portfolios Developed multiple MapReduce jobs in Java for data cleaning and preprocessing Developed workflow in Oozie to automate the tasks of loading the data into HDFS and preprocessing with Pig Production Rollout Support and resolving any issues that are discovered by the client and client services teams Environment Hadoop Map Reduce HDFS Hive Java jdk16 Hadoop distribution of Hortonworks Cloudera PLSQL SQL Toad 96 Windows NT UNIX Shell Scripting Software EngineerBig Data Experian Costa Mesa CA December 2017 to July 2018 Project Precise ID for Customer Management Description This project involves setting up a data repository and use it for search processing for analytical and research purposes This application provides the capability for large batch processing using Hadoop map reduce jobs using Java runtime environment as well as real time search capabilities using Solr cloud environment Responsibilities Involved in designing and developing Hadoop MapReduce jobs Using Java Runtime Environment for the batch processing to search and match the scores Used Rational Rose for developing Use case diagrams Activity flow diagrams Class diagrams and Object diagrams in the design phase Used Struts with Tiles in the MVC framework for the application Experiencing in running MapReduce programs on 20 node cluster Amazon EC2 spot instances with Apache Hadoop Developed java web services to upload data from local to Amazon S3 listing S3 objects and file manipulation operations Successfully ran all Hadoop MapReduce programs on Amazon Elastic MapReduce framework by using Amazon S3 for Input and Output Experience in Amazon S3 for storing objects and sharing data between Hadoop Involved in developing Hadoop MapReduce jobs for merging and appending the repository data Used Sqoop to pull the data from Teradata Hands on experience in setting up HBase Column based storage repository for archiving and retro data Used Java Message Service JMS 11 for reliable and asynchronous communication Involved in integration of Legacy Scoring and Analytical Models like SMG3 into the new application using Web Services Responsible for writing Pig UDFs and Hive UDFs Handled importing of data from various data sources performance transformation using Hive Experience in optimization of Map reduce algorithm using combiners and partitions to deliver the best results and worked on Application performance optimization for a HDFS cluster Experience working with offshore teams and communicating daily status on issues roadblocks Environment Java Hadoop HBase Zookeeper Solr cloud Pig Latin Oozie scheduler JavaBeans Eclipse Rational Clear case Servlet 23 JUnit Maven XML Web services JDBC Unix Windows NT2000 Hadoop Developer Deutsche Bank Cary NC September 2015 to November 2017 Project Marketing and Securities FDR Description Designed and developed big data solutions involving Terabytes of data The big data solution consists of collecting large amounts oflog data from distributed sources transformations and standardizations analysis statistics aggregations and reporting etc Built an on demand elastic Hadoop cluster infrastructure to cater the needs of various Big Data projects automated various Big Data workflows to process and extracts analytics out of the data using MapReduce Pig and Hive Responsibilities Installed configured and maintained Apache Hadoop clusters for application development and Hadoop tools like Hive Pig HBase Zookeeper and Sqoop for POC Created HDFS Hadoop Distributed File System and MapReduce jobs in java Implemented NameNode backup using NFS for High availability Used Pig as ETL tool to do transformations event joins and some preaggregations before storing the data onto HDFS Responsible for developing data pipeline using flume Sqoop and pig to extract the data from weblogs and store in HDFS Used Sqoop to import and export data from HDFS to RDBMS and viceversa Created Hive tables and involved in data loading and writing Hive UDFs Exported the analyzed data to the relational database MySQL using Sqoop for visualization and to generate reports Used Hive to analyze the partitioned and bucketed data and compute various metrics for reporting Automated workflows using shell scripts to pull data from various databases into Hadoop Responsible for creating a Solr schema from the Indexer settings Written Solr queries for various search documents EnvironmentHadoop MapReduce Hive HDFS PIG Sqoop Oozie Solr Cloudera Flume HBase Zookeeper Oracle NoSQL and UnixLinux JavaJ2ee Developer Cognolabs Systems Pvt Ltd Hyderabad Telangana October 2013 to August 2015 DescriptionThis project was to develop an interface between the Claims Service Records CSR and a newly developed website for the Insurance Bureau Purpose of creating the interface was to minimize the changes to the existing Claim Service Records On request by a Claims Agent data was requested for CSR which was in a flat file format This was converted to XML by the application and sent to Insurance Bureau website where it was displayed Responsibilities Involved in the software development life cycle coding testing and implementation Worked in the healthcare domain Involved in Using Java Message Service JMS for loosely coupled reliable and asynchronous exchange of patient treatment information among J2EE components and legacy system Developed MDBs using JMS to exchange messages between different applications using MQ Series Involved in working with J2EE Design patterns Singleton Factory DAO and Business Delegate and Model View Controller Architecture with JSF and Spring DI Involved in Content Management using XML Developed a standalone module transforming XML 837 module to database using SAX parser Installed Configured and administered WebSphere ESB v6x Worked on Performance tuning of WebSphere ESB in different environments on different platforms Configured and Implemented web services specifications in collaboration with offshore team Involved in Creating dash board charts business charts using fusion charts Involved in creating reports for the most of the business criteria Environment Java Servlets JSP Vignette Tool XML Eclipse 330 Tomcat 5x Java Developer Skdotcom Technologies Hyderabad Telangana June 2011 to September 2013 DescriptionSpeedTrial is a webbased enterprise software solution for managing multisite clinical research studies It helps the research team to manage clinical trials in an easier faster and more efficient way SpeedTrial is designed for pharmaceutical biotech medical companies contract research organization CROs specialized research organization and academic medical centers AMCs to manage thousands of trials in a variety of therapeutic areas SpeedTrial facilitates creation of study protocol designing the Case Report Forms CRFs by dragging and dropping the sections and questions Electronic Data Capture EDC and exhaustive clinical data management and reduces administration and monitoring costs Responsibilities Designed and Developed application using EJB 20 and Struts framework Developed POJOs for Data Model to map the Java Objects with Relational database tables Designed and developed Service layer using Struts framework Used MVC based Struts framework to develop the multitier web application presentation layer components Involved in Integration of Struts with Database Implemented Struts tag libraries like html logic tab bean etc in the JSP pages Used Struts tiles libraries for layout of web page and performed struts validations using Struts validation framework Implemented Oracle database and JDBC drivers to access the data Involved in design analysis and architectural meetings Created Architecture Diagrams and Flow Charts using Rational Rose Followed agile software development practice paired programming test driven development and scrum status meetings Developed use case diagrams class diagrams database tables and mapping between relational database tables Developed Unit test cases using JUnit Maintained the application configuration information in various properties file Performed unit testing system testing and integration testing Environment Java Struts Framework log4j Servlets JSP JSTL I18N JDBC HTML Java Script CSS UML Oracle Windows NT Education Bachelor of Technology in Electronics and Communication Engineering in Electronics and Communication Engineering Jawaharlal Nehru Technological University Skills ECLIPSE J2EE JAVA SPRING JMS JSP JVM HDFS OOZIE SQOOP HBASE DB2 FLUME HADOOP MAP REDUCE AMAZON WEB SERVICES Git GRADLE Hadoop HBase",
    "entities": [
        "Spark Created",
        "Work Experience JavaHadoop Developer TMobile Bellevue",
        "Legacy Scoring",
        "Relational",
        "BI",
        "Customer Management Description",
        "HDFS",
        "Sparks",
        "UNIX",
        "JMS",
        "Kappa architecture Responsible",
        "UDAFs",
        "Hortonworks Cloudera",
        "Hadoop",
        "XML",
        "Nehru Technological University",
        "Created Architecture Diagrams",
        "HDFS Used Sqoop",
        "Information Management Marketing and Risk Capabilities Used",
        "J2EE Design",
        "Shared",
        "JUnit",
        "Amazon Elastic MapReduce",
        "Integrated Data",
        "HBase",
        "Automated",
        "Amazon S3",
        "Amazon",
        "Amazon S3 for Input and Output Experience",
        "WebSphere",
        "Project Marketing and Securities FDR Description Designed",
        "Git GRADLE Hadoop HBase",
        "Assisted",
        "the Case Report Forms",
        "Project Meter Data Management Description Collected",
        "Developed",
        "NameNode",
        "JavaHadoop Developer",
        "TMobile",
        "EnvironmentHadoop MapReduce Hive HDFS PIG",
        "Insurance Bureau",
        "Responsibilities Involved",
        "Hadoop MapReduce",
        "SpeedTrial",
        "Apache Hadoop Developed",
        "POC Created HDFS Hadoop Distributed File System",
        "Business Intelligence and Engineering etc",
        "the Hadoop Distributed File System",
        "Hadoop Involved",
        "JSP",
        "CSR",
        "Hive Experience",
        "Collaborated",
        "OOZIE SQOOP HBASE",
        "MVC",
        "Hive Pig HBase Zookeeper",
        "Spark",
        "EJB",
        "Created Hive",
        "HDFS Created HBase",
        "Web Services Responsible",
        "Implemented Oracle",
        "EDW Capturing",
        "Scalaand Python Strong",
        "Hortonworks SQLUNIX Shell Scripting Oracle SQLServer MySQL Big Data Developer KCPL Kansas City",
        "US",
        "Hadoop Hive",
        "Sqoop",
        "HDFS Develop",
        "Storm",
        "Terabytes",
        "Present Project BEAM Description",
        "JSF",
        "PIG",
        "HDFS Responsible",
        "MapReduce Logical Implementation",
        "SAX",
        "Oozie",
        "SQL",
        "Business Delegate and",
        "HBase Developed MapReduce",
        "the Claims Service Records",
        "BigInsights Quick",
        "Relational Database Systems",
        "node forecasting",
        "MapReduce Pig",
        "WebSphere ESB",
        "Big Data",
        "Hive",
        "Amazon AWS",
        "OOZIE",
        "EDWBI",
        "Teradata Hands",
        "ETL",
        "Playframeworkand Akka Actors Good Knowledge on Hadoop Cluster",
        "java Implemented",
        "Apache Hadoop",
        "Spark Developed Hive",
        "Performed",
        "MapReduce Framework",
        "Relational Database Oracle Developed",
        "Developed Unit",
        "Hadoop Responsible",
        "Content Management",
        "Created HBase",
        "IDW",
        "Java Developer Skdotcom Technologies Hyderabad",
        "Java Message Service",
        "EDW",
        "Relational Database",
        "MapReduce",
        "NFS",
        "NoSQL",
        "UnixLinux JavaJ2ee Developer Cognolabs Systems Pvt Ltd Hyderabad",
        "the Insurance Bureau Purpose",
        "Application",
        "Communication Engineering in",
        "Teradata",
        "Node",
        "Using Java Runtime Environment"
    ],
    "experience": "Experience in importing and exporting data using Sqoop from HDFS to Relational Database Systems and viceversa Experience loading data to Hive partitions and creating buckets in Hive Experience developing custom SerDes in HIVE Experience in extending Hive and Pig core functionality by writing custom UDFs Knowledge in extending Hive core functionality by writing custom UDFs like UDAFs and UDTFs Expertise in analysis using PIG HIVE and MapReduce Logical Implementation and interaction with HBase Developed MapReduce jobs to automate transfer the data from HBase Experience in developing ETL process using MapReduce Framework in java Knowledge in efficiently using Sparks memory caching for iterative processing using Scala Good understanding how Spark instructions are translated into jobs Strong Knowledge developing web service using Playframeworkand Akka Actors Good Knowledge on Hadoop Cluster architecture and monitoring the cluster Knowledge in jobworkflow scheduling and monitoring tools like Oozie and Zookeeper Strong understanding of NoSQL databases like HBase MongoDB Cassandra Good working knowledge with Agile for SDLC Knowledge in Amazon AWS concepts like EMR S3 and EC2 web services Efficient team player having strong desire and capabilities to efficiently convert Customers requirements into applications Authorized to work in the US for any employer Work Experience JavaHadoop Developer TMobile Bellevue WA January 2019 to Present Project BEAM Description The transformation program of our billing systems is the main objective of uprising project It will provide new capabilities for flexibility efficiency and new business models Uprising project covers various domains like Business Intelligence and Engineering etc Integrated Data warehouse IDW is TMobile central repository of data in Hadoop and Teradata ecosystems and IDW is part of BI As part of data ingestion team we will bring incremental data from various source systems into Hadoop to maintain one central repository Responsibilities Developed Storm topology to ingest data from various source into hadoop data lake Configured ActiveMQ for enterprise and resolved ActiveMQ issues Developed web application using HBase and Hive API to compare schema between HBase and Hive tables Used JVM monitor to monitor threads and memory usage of HBase and Hive schema check web application Developed code to generate Hive DDLs from source DDLs Created HBase tables using Sqoop from Relational Database Oracle Developed code to import JSON data into MYSQL database Developed code to import data SQLServer into HDFS and created Hive views on data in HDFS using Spark Created scripts to append data from temporary HBase table to target HBase table in Spark Developed Hive queries for the analysis Handling structured and unstructured data and applying ETL processes Handled importing of data from various data sources performance optimization of Sqoop Developed Oozie workflow to compare data in Hive and Source Assessed existing and EDW technologies and methods to ensure our EDWBI architecture meet the needs of the business and enterprise and allows for business growth Assisted with data capacity planning and node forecasting Collaborated with the infrastructure network database application and BI teams to ensure data quality and availability Environment Hadoop Map Reduce HDFS Hive HBase Pig Java Hadoop distribution of Hortonworks SQLUNIX Shell Scripting Oracle SQLServer MySQL Big Data Developer KCPL Kansas City MO August 2018 to January 2019 Project Meter Data Management Description Collected managed and distributed information to the users of databases applications and platforms that are supported by the KCPL team Key areas were leveraging this capability to drive financial benefit to the company in Information Management Marketing and Risk Capabilities Used substantial technical infrastructure to support the data management model and operational philosophy which provided the flexibility and reliability needed for inclass production analytics and online capability Responsibilities Worked on Big Data Hadoop cluster implementation and data integration in developing largescale system software Developed MapReduce programs to parse the raw data populate staging tables and store the refined data in partitioned tables in the EDW Capturing data from existing databases that provide SQL interfaces using Sqoop Worked extensively with Sqoop for importing and exporting the data from HDFS to Relational Database system Oracle and viceversa Loading data into HDFS Develop and maintains complex outbound notification applications that run on custom architectures using diverse technologies including Java XML JMS and Web Services Created Hive queries that helped market analysts spot emerging trends by comparing fresh data with EDW reference tables and historical metrics Enabled speedy reviews and first mover advantages by using Oozie to automate data loading into the Hadoop Distributed File System and PIG to preprocess the data Provided design recommendations and thought leadership to sponsorsstakeholders that improved review processes and resolved technical problems Shared responsibility for administration of Hadoop Hive and Pig Helped business processes by developing and configuring Hadoop ecosystem components that moved data from individual servers to HDFS Created HBase tables to load large sets of structured semistructured and unstructured data coming from UNIX NoSQL and a variety of portfolios Developed multiple MapReduce jobs in Java for data cleaning and preprocessing Developed workflow in Oozie to automate the tasks of loading the data into HDFS and preprocessing with Pig Production Rollout Support and resolving any issues that are discovered by the client and client services teams Environment Hadoop Map Reduce HDFS Hive Java jdk16 Hadoop distribution of Hortonworks Cloudera PLSQL SQL Toad 96 Windows NT UNIX Shell Scripting Software EngineerBig Data Experian Costa Mesa CA December 2017 to July 2018 Project Precise ID for Customer Management Description This project involves setting up a data repository and use it for search processing for analytical and research purposes This application provides the capability for large batch processing using Hadoop map reduce jobs using Java runtime environment as well as real time search capabilities using Solr cloud environment Responsibilities Involved in designing and developing Hadoop MapReduce jobs Using Java Runtime Environment for the batch processing to search and match the scores Used Rational Rose for developing Use case diagrams Activity flow diagrams Class diagrams and Object diagrams in the design phase Used Struts with Tiles in the MVC framework for the application Experiencing in running MapReduce programs on 20 node cluster Amazon EC2 spot instances with Apache Hadoop Developed java web services to upload data from local to Amazon S3 listing S3 objects and file manipulation operations Successfully ran all Hadoop MapReduce programs on Amazon Elastic MapReduce framework by using Amazon S3 for Input and Output Experience in Amazon S3 for storing objects and sharing data between Hadoop Involved in developing Hadoop MapReduce jobs for merging and appending the repository data Used Sqoop to pull the data from Teradata Hands on experience in setting up HBase Column based storage repository for archiving and retro data Used Java Message Service JMS 11 for reliable and asynchronous communication Involved in integration of Legacy Scoring and Analytical Models like SMG3 into the new application using Web Services Responsible for writing Pig UDFs and Hive UDFs Handled importing of data from various data sources performance transformation using Hive Experience in optimization of Map reduce algorithm using combiners and partitions to deliver the best results and worked on Application performance optimization for a HDFS cluster Experience working with offshore teams and communicating daily status on issues roadblocks Environment Java Hadoop HBase Zookeeper Solr cloud Pig Latin Oozie scheduler JavaBeans Eclipse Rational Clear case Servlet 23 JUnit Maven XML Web services JDBC Unix Windows NT2000 Hadoop Developer Deutsche Bank Cary NC September 2015 to November 2017 Project Marketing and Securities FDR Description Designed and developed big data solutions involving Terabytes of data The big data solution consists of collecting large amounts oflog data from distributed sources transformations and standardizations analysis statistics aggregations and reporting etc Built an on demand elastic Hadoop cluster infrastructure to cater the needs of various Big Data projects automated various Big Data workflows to process and extracts analytics out of the data using MapReduce Pig and Hive Responsibilities Installed configured and maintained Apache Hadoop clusters for application development and Hadoop tools like Hive Pig HBase Zookeeper and Sqoop for POC Created HDFS Hadoop Distributed File System and MapReduce jobs in java Implemented NameNode backup using NFS for High availability Used Pig as ETL tool to do transformations event joins and some preaggregations before storing the data onto HDFS Responsible for developing data pipeline using flume Sqoop and pig to extract the data from weblogs and store in HDFS Used Sqoop to import and export data from HDFS to RDBMS and viceversa Created Hive tables and involved in data loading and writing Hive UDFs Exported the analyzed data to the relational database MySQL using Sqoop for visualization and to generate reports Used Hive to analyze the partitioned and bucketed data and compute various metrics for reporting Automated workflows using shell scripts to pull data from various databases into Hadoop Responsible for creating a Solr schema from the Indexer settings Written Solr queries for various search documents EnvironmentHadoop MapReduce Hive HDFS PIG Sqoop Oozie Solr Cloudera Flume HBase Zookeeper Oracle NoSQL and UnixLinux JavaJ2ee Developer Cognolabs Systems Pvt Ltd Hyderabad Telangana October 2013 to August 2015 DescriptionThis project was to develop an interface between the Claims Service Records CSR and a newly developed website for the Insurance Bureau Purpose of creating the interface was to minimize the changes to the existing Claim Service Records On request by a Claims Agent data was requested for CSR which was in a flat file format This was converted to XML by the application and sent to Insurance Bureau website where it was displayed Responsibilities Involved in the software development life cycle coding testing and implementation Worked in the healthcare domain Involved in Using Java Message Service JMS for loosely coupled reliable and asynchronous exchange of patient treatment information among J2EE components and legacy system Developed MDBs using JMS to exchange messages between different applications using MQ Series Involved in working with J2EE Design patterns Singleton Factory DAO and Business Delegate and Model View Controller Architecture with JSF and Spring DI Involved in Content Management using XML Developed a standalone module transforming XML 837 module to database using SAX parser Installed Configured and administered WebSphere ESB v6x Worked on Performance tuning of WebSphere ESB in different environments on different platforms Configured and Implemented web services specifications in collaboration with offshore team Involved in Creating dash board charts business charts using fusion charts Involved in creating reports for the most of the business criteria Environment Java Servlets JSP Vignette Tool XML Eclipse 330 Tomcat 5x Java Developer Skdotcom Technologies Hyderabad Telangana June 2011 to September 2013 DescriptionSpeedTrial is a webbased enterprise software solution for managing multisite clinical research studies It helps the research team to manage clinical trials in an easier faster and more efficient way SpeedTrial is designed for pharmaceutical biotech medical companies contract research organization CROs specialized research organization and academic medical centers AMCs to manage thousands of trials in a variety of therapeutic areas SpeedTrial facilitates creation of study protocol designing the Case Report Forms CRFs by dragging and dropping the sections and questions Electronic Data Capture EDC and exhaustive clinical data management and reduces administration and monitoring costs Responsibilities Designed and Developed application using EJB 20 and Struts framework Developed POJOs for Data Model to map the Java Objects with Relational database tables Designed and developed Service layer using Struts framework Used MVC based Struts framework to develop the multitier web application presentation layer components Involved in Integration of Struts with Database Implemented Struts tag libraries like html logic tab bean etc in the JSP pages Used Struts tiles libraries for layout of web page and performed struts validations using Struts validation framework Implemented Oracle database and JDBC drivers to access the data Involved in design analysis and architectural meetings Created Architecture Diagrams and Flow Charts using Rational Rose Followed agile software development practice paired programming test driven development and scrum status meetings Developed use case diagrams class diagrams database tables and mapping between relational database tables Developed Unit test cases using JUnit Maintained the application configuration information in various properties file Performed unit testing system testing and integration testing Environment Java Struts Framework log4j Servlets JSP JSTL I18N JDBC HTML Java Script CSS UML Oracle Windows NT Education Bachelor of Technology in Electronics and Communication Engineering in Electronics and Communication Engineering Jawaharlal Nehru Technological University Skills ECLIPSE J2EE JAVA SPRING JMS JSP JVM HDFS OOZIE SQOOP HBASE DB2 FLUME HADOOP MAP REDUCE AMAZON WEB SERVICES Git GRADLE Hadoop HBase",
    "extracted_keywords": [
        "JavaHadoop",
        "Developer",
        "span",
        "lJavaspanHadoop",
        "span",
        "lDeveloperspan",
        "JavaHadoop",
        "Developer",
        "TMobile",
        "Bellevue",
        "WA",
        "Certified",
        "Apache",
        "Spark",
        "Developer",
        "years",
        "experience",
        "software",
        "architecture",
        "design",
        "development",
        "testing",
        "maintenance",
        "web",
        "enterprise",
        "applications",
        "years",
        "experience",
        "Hadoop",
        "Hadoop",
        "Ecosystems",
        "HDFS",
        "YARN",
        "MAPREDUCE",
        "PIG",
        "HIVE",
        "HBASE",
        "FLUME",
        "ZOOKEEPER",
        "OOZIE",
        "years",
        "experience",
        "Java",
        "developer",
        "understanding",
        "Hadoop",
        "architecture",
        "components",
        "HDFS",
        "YARN",
        "Resource",
        "Manager",
        "Node",
        "Manager",
        "Name",
        "Node",
        "Data",
        "Node",
        "Map",
        "Reduce",
        "programming",
        "paradigm",
        "Hands",
        "experience",
        "Cloudera",
        "Horton",
        "BigInsights",
        "Quick",
        "Hadoop",
        "Distributions",
        "Hands",
        "experience",
        "Scalaand",
        "Python",
        "experience",
        "Storm",
        "topology",
        "ActiveMQ",
        "Java",
        "Good",
        "understanding",
        "Lambda",
        "Kappa",
        "architecture",
        "MapReduce",
        "programsHadoop",
        "pig",
        "Hive",
        "Experience",
        "data",
        "Sqoop",
        "HDFS",
        "Relational",
        "Database",
        "Systems",
        "viceversa",
        "Experience",
        "loading",
        "data",
        "Hive",
        "partitions",
        "buckets",
        "Hive",
        "Experience",
        "custom",
        "SerDes",
        "HIVE",
        "Experience",
        "Hive",
        "Pig",
        "core",
        "functionality",
        "custom",
        "UDFs",
        "Knowledge",
        "Hive",
        "core",
        "functionality",
        "custom",
        "UDFs",
        "UDAFs",
        "UDTFs",
        "Expertise",
        "analysis",
        "PIG",
        "HIVE",
        "MapReduce",
        "Implementation",
        "interaction",
        "HBase",
        "MapReduce",
        "jobs",
        "data",
        "HBase",
        "Experience",
        "ETL",
        "process",
        "MapReduce",
        "Framework",
        "Knowledge",
        "Sparks",
        "memory",
        "processing",
        "Scala",
        "Good",
        "understanding",
        "Spark",
        "instructions",
        "jobs",
        "Strong",
        "Knowledge",
        "web",
        "service",
        "Playframeworkand",
        "Akka",
        "Actors",
        "Good",
        "Knowledge",
        "Hadoop",
        "Cluster",
        "architecture",
        "cluster",
        "Knowledge",
        "jobworkflow",
        "scheduling",
        "monitoring",
        "tools",
        "Oozie",
        "Zookeeper",
        "Strong",
        "understanding",
        "HBase",
        "MongoDB",
        "Cassandra",
        "Good",
        "knowledge",
        "Agile",
        "SDLC",
        "Knowledge",
        "Amazon",
        "AWS",
        "concepts",
        "EMR",
        "S3",
        "EC2",
        "web",
        "services",
        "team",
        "player",
        "desire",
        "capabilities",
        "Customers",
        "requirements",
        "applications",
        "US",
        "employer",
        "Work",
        "Experience",
        "JavaHadoop",
        "Developer",
        "TMobile",
        "Bellevue",
        "WA",
        "January",
        "Present",
        "Project",
        "BEAM",
        "Description",
        "transformation",
        "program",
        "billing",
        "systems",
        "objective",
        "uprising",
        "project",
        "capabilities",
        "flexibility",
        "efficiency",
        "business",
        "models",
        "Uprising",
        "project",
        "domains",
        "Business",
        "Intelligence",
        "Engineering",
        "Integrated",
        "Data",
        "warehouse",
        "IDW",
        "repository",
        "data",
        "Hadoop",
        "Teradata",
        "ecosystems",
        "IDW",
        "part",
        "BI",
        "part",
        "data",
        "ingestion",
        "team",
        "data",
        "source",
        "systems",
        "Hadoop",
        "repository",
        "Responsibilities",
        "Storm",
        "topology",
        "data",
        "source",
        "hadoop",
        "data",
        "lake",
        "Configured",
        "ActiveMQ",
        "enterprise",
        "ActiveMQ",
        "issues",
        "web",
        "application",
        "HBase",
        "Hive",
        "API",
        "schema",
        "HBase",
        "Hive",
        "tables",
        "JVM",
        "monitor",
        "threads",
        "memory",
        "usage",
        "HBase",
        "Hive",
        "schema",
        "web",
        "application",
        "code",
        "Hive",
        "DDLs",
        "source",
        "Created",
        "HBase",
        "tables",
        "Sqoop",
        "Relational",
        "Database",
        "Oracle",
        "Developed",
        "code",
        "data",
        "MYSQL",
        "database",
        "code",
        "data",
        "HDFS",
        "Hive",
        "views",
        "data",
        "HDFS",
        "Spark",
        "scripts",
        "data",
        "HBase",
        "table",
        "HBase",
        "table",
        "Spark",
        "Developed",
        "Hive",
        "analysis",
        "data",
        "ETL",
        "processes",
        "importing",
        "data",
        "data",
        "sources",
        "optimization",
        "Sqoop",
        "Developed",
        "Oozie",
        "workflow",
        "data",
        "Hive",
        "Source",
        "EDW",
        "technologies",
        "methods",
        "EDWBI",
        "architecture",
        "needs",
        "business",
        "enterprise",
        "business",
        "growth",
        "data",
        "capacity",
        "planning",
        "node",
        "forecasting",
        "Collaborated",
        "infrastructure",
        "network",
        "database",
        "application",
        "BI",
        "teams",
        "data",
        "quality",
        "availability",
        "Environment",
        "Hadoop",
        "Map",
        "HDFS",
        "Hive",
        "HBase",
        "Pig",
        "Java",
        "Hadoop",
        "distribution",
        "Hortonworks",
        "SQLUNIX",
        "Shell",
        "Scripting",
        "Oracle",
        "SQLServer",
        "MySQL",
        "Big",
        "Data",
        "Developer",
        "KCPL",
        "Kansas",
        "City",
        "MO",
        "August",
        "January",
        "Project",
        "Meter",
        "Data",
        "Management",
        "Description",
        "Collected",
        "information",
        "users",
        "databases",
        "applications",
        "platforms",
        "KCPL",
        "team",
        "Key",
        "areas",
        "capability",
        "benefit",
        "company",
        "Information",
        "Management",
        "Marketing",
        "Risk",
        "Capabilities",
        "infrastructure",
        "data",
        "management",
        "model",
        "philosophy",
        "flexibility",
        "reliability",
        "production",
        "analytics",
        "capability",
        "Responsibilities",
        "Big",
        "Data",
        "Hadoop",
        "cluster",
        "implementation",
        "data",
        "integration",
        "largescale",
        "system",
        "software",
        "Developed",
        "MapReduce",
        "programs",
        "data",
        "staging",
        "tables",
        "data",
        "tables",
        "EDW",
        "Capturing",
        "data",
        "databases",
        "SQL",
        "interfaces",
        "Sqoop",
        "Worked",
        "Sqoop",
        "data",
        "HDFS",
        "Relational",
        "Database",
        "system",
        "Oracle",
        "viceversa",
        "Loading",
        "data",
        "HDFS",
        "Develop",
        "outbound",
        "notification",
        "applications",
        "custom",
        "architectures",
        "technologies",
        "Java",
        "XML",
        "JMS",
        "Web",
        "Services",
        "Hive",
        "queries",
        "market",
        "analysts",
        "trends",
        "data",
        "EDW",
        "reference",
        "tables",
        "metrics",
        "reviews",
        "advantages",
        "Oozie",
        "data",
        "loading",
        "Hadoop",
        "Distributed",
        "File",
        "System",
        "PIG",
        "data",
        "design",
        "recommendations",
        "leadership",
        "sponsorsstakeholders",
        "review",
        "processes",
        "problems",
        "responsibility",
        "administration",
        "Hadoop",
        "Hive",
        "Pig",
        "business",
        "processes",
        "Hadoop",
        "ecosystem",
        "components",
        "data",
        "servers",
        "HDFS",
        "Created",
        "HBase",
        "sets",
        "data",
        "UNIX",
        "NoSQL",
        "variety",
        "portfolios",
        "MapReduce",
        "jobs",
        "Java",
        "data",
        "Developed",
        "workflow",
        "Oozie",
        "tasks",
        "data",
        "HDFS",
        "Pig",
        "Production",
        "Rollout",
        "Support",
        "issues",
        "client",
        "client",
        "services",
        "teams",
        "Environment",
        "Hadoop",
        "Map",
        "HDFS",
        "Hive",
        "Java",
        "jdk16",
        "Hadoop",
        "distribution",
        "Hortonworks",
        "Cloudera",
        "PLSQL",
        "SQL",
        "Toad",
        "Windows",
        "NT",
        "UNIX",
        "Shell",
        "Scripting",
        "Software",
        "EngineerBig",
        "Data",
        "Experian",
        "Costa",
        "Mesa",
        "CA",
        "December",
        "July",
        "Project",
        "Precise",
        "ID",
        "Customer",
        "Management",
        "Description",
        "project",
        "data",
        "repository",
        "search",
        "processing",
        "research",
        "purposes",
        "application",
        "capability",
        "batch",
        "processing",
        "Hadoop",
        "map",
        "jobs",
        "Java",
        "runtime",
        "environment",
        "time",
        "search",
        "capabilities",
        "Solr",
        "cloud",
        "environment",
        "Responsibilities",
        "Hadoop",
        "MapReduce",
        "jobs",
        "Java",
        "Runtime",
        "Environment",
        "batch",
        "processing",
        "search",
        "scores",
        "Rational",
        "Rose",
        "Use",
        "case",
        "diagrams",
        "Activity",
        "flow",
        "diagrams",
        "Class",
        "diagrams",
        "Object",
        "diagrams",
        "design",
        "phase",
        "Struts",
        "Tiles",
        "MVC",
        "framework",
        "application",
        "MapReduce",
        "programs",
        "node",
        "cluster",
        "Amazon",
        "EC2",
        "spot",
        "instances",
        "Apache",
        "Hadoop",
        "Developed",
        "web",
        "services",
        "data",
        "Amazon",
        "S3",
        "S3",
        "objects",
        "file",
        "manipulation",
        "operations",
        "Hadoop",
        "MapReduce",
        "programs",
        "Amazon",
        "Elastic",
        "MapReduce",
        "framework",
        "Amazon",
        "S3",
        "Input",
        "Output",
        "Experience",
        "Amazon",
        "S3",
        "objects",
        "data",
        "Hadoop",
        "Hadoop",
        "MapReduce",
        "jobs",
        "repository",
        "data",
        "Sqoop",
        "data",
        "Teradata",
        "Hands",
        "experience",
        "HBase",
        "Column",
        "storage",
        "repository",
        "retro",
        "data",
        "Java",
        "Message",
        "Service",
        "JMS",
        "communication",
        "integration",
        "Legacy",
        "Scoring",
        "Analytical",
        "Models",
        "SMG3",
        "application",
        "Web",
        "Services",
        "Pig",
        "UDFs",
        "Hive",
        "UDFs",
        "importing",
        "data",
        "data",
        "sources",
        "transformation",
        "Hive",
        "Experience",
        "optimization",
        "Map",
        "algorithm",
        "combiners",
        "partitions",
        "results",
        "Application",
        "performance",
        "optimization",
        "HDFS",
        "cluster",
        "Experience",
        "teams",
        "status",
        "issues",
        "roadblocks",
        "Environment",
        "Java",
        "Hadoop",
        "HBase",
        "Zookeeper",
        "Solr",
        "cloud",
        "Pig",
        "Latin",
        "Oozie",
        "JavaBeans",
        "Eclipse",
        "Rational",
        "Clear",
        "case",
        "Servlet",
        "JUnit",
        "Maven",
        "XML",
        "Web",
        "services",
        "JDBC",
        "Unix",
        "Windows",
        "NT2000",
        "Hadoop",
        "Developer",
        "Deutsche",
        "Bank",
        "Cary",
        "NC",
        "September",
        "November",
        "Project",
        "Marketing",
        "Securities",
        "FDR",
        "Description",
        "data",
        "solutions",
        "Terabytes",
        "data",
        "data",
        "solution",
        "amounts",
        "data",
        "sources",
        "transformations",
        "standardizations",
        "analysis",
        "statistics",
        "aggregations",
        "demand",
        "Hadoop",
        "cluster",
        "infrastructure",
        "needs",
        "Big",
        "Data",
        "projects",
        "Big",
        "Data",
        "workflows",
        "analytics",
        "data",
        "MapReduce",
        "Pig",
        "Hive",
        "Responsibilities",
        "Apache",
        "Hadoop",
        "clusters",
        "application",
        "development",
        "Hadoop",
        "tools",
        "Hive",
        "Pig",
        "HBase",
        "Zookeeper",
        "Sqoop",
        "POC",
        "Created",
        "HDFS",
        "Hadoop",
        "Distributed",
        "File",
        "System",
        "MapReduce",
        "jobs",
        "NameNode",
        "backup",
        "NFS",
        "availability",
        "Pig",
        "ETL",
        "tool",
        "transformations",
        "event",
        "preaggregations",
        "data",
        "data",
        "pipeline",
        "flume",
        "Sqoop",
        "pig",
        "data",
        "weblogs",
        "HDFS",
        "Sqoop",
        "export",
        "data",
        "HDFS",
        "viceversa",
        "Created",
        "Hive",
        "tables",
        "data",
        "loading",
        "Hive",
        "UDFs",
        "data",
        "database",
        "MySQL",
        "Sqoop",
        "visualization",
        "reports",
        "Hive",
        "data",
        "metrics",
        "Automated",
        "workflows",
        "scripts",
        "data",
        "databases",
        "Hadoop",
        "Responsible",
        "Solr",
        "schema",
        "Indexer",
        "settings",
        "Written",
        "Solr",
        "search",
        "documents",
        "EnvironmentHadoop",
        "MapReduce",
        "Hive",
        "HDFS",
        "PIG",
        "Sqoop",
        "Oozie",
        "Solr",
        "Cloudera",
        "Flume",
        "HBase",
        "Zookeeper",
        "Oracle",
        "NoSQL",
        "UnixLinux",
        "JavaJ2ee",
        "Developer",
        "Cognolabs",
        "Systems",
        "Pvt",
        "Ltd",
        "Hyderabad",
        "Telangana",
        "October",
        "August",
        "DescriptionThis",
        "project",
        "interface",
        "Claims",
        "Service",
        "Records",
        "CSR",
        "website",
        "Insurance",
        "Bureau",
        "Purpose",
        "interface",
        "changes",
        "Claim",
        "Service",
        "Records",
        "request",
        "Claims",
        "Agent",
        "data",
        "CSR",
        "file",
        "format",
        "XML",
        "application",
        "Insurance",
        "Bureau",
        "website",
        "Responsibilities",
        "software",
        "development",
        "life",
        "cycle",
        "testing",
        "implementation",
        "healthcare",
        "domain",
        "Java",
        "Message",
        "Service",
        "JMS",
        "exchange",
        "treatment",
        "information",
        "J2EE",
        "components",
        "legacy",
        "system",
        "MDBs",
        "JMS",
        "messages",
        "applications",
        "MQ",
        "Series",
        "J2EE",
        "Design",
        "patterns",
        "Singleton",
        "Factory",
        "DAO",
        "Business",
        "Delegate",
        "Model",
        "View",
        "Controller",
        "Architecture",
        "JSF",
        "Spring",
        "DI",
        "Content",
        "Management",
        "XML",
        "module",
        "XML",
        "module",
        "database",
        "SAX",
        "parser",
        "Installed",
        "Configured",
        "WebSphere",
        "ESB",
        "v6x",
        "Performance",
        "tuning",
        "WebSphere",
        "ESB",
        "environments",
        "platforms",
        "web",
        "services",
        "specifications",
        "collaboration",
        "team",
        "dash",
        "board",
        "charts",
        "business",
        "charts",
        "fusion",
        "charts",
        "reports",
        "business",
        "criteria",
        "Environment",
        "Java",
        "Servlets",
        "JSP",
        "Vignette",
        "Tool",
        "XML",
        "Eclipse",
        "Tomcat",
        "Java",
        "Developer",
        "Skdotcom",
        "Technologies",
        "Hyderabad",
        "Telangana",
        "June",
        "September",
        "enterprise",
        "software",
        "solution",
        "multisite",
        "research",
        "studies",
        "research",
        "team",
        "trials",
        "way",
        "SpeedTrial",
        "biotech",
        "companies",
        "research",
        "organization",
        "CROs",
        "specialized",
        "research",
        "organization",
        "centers",
        "AMCs",
        "thousands",
        "trials",
        "variety",
        "areas",
        "SpeedTrial",
        "creation",
        "study",
        "protocol",
        "Case",
        "Report",
        "CRFs",
        "sections",
        "questions",
        "Electronic",
        "Data",
        "Capture",
        "EDC",
        "data",
        "management",
        "administration",
        "monitoring",
        "costs",
        "Responsibilities",
        "application",
        "EJB",
        "Struts",
        "POJOs",
        "Data",
        "Model",
        "Java",
        "Objects",
        "database",
        "tables",
        "Service",
        "layer",
        "Struts",
        "framework",
        "Used",
        "MVC",
        "Struts",
        "framework",
        "web",
        "application",
        "presentation",
        "layer",
        "components",
        "Integration",
        "Struts",
        "Database",
        "Struts",
        "tag",
        "html",
        "logic",
        "tab",
        "bean",
        "JSP",
        "pages",
        "Struts",
        "libraries",
        "layout",
        "web",
        "page",
        "struts",
        "validations",
        "Struts",
        "validation",
        "framework",
        "Oracle",
        "database",
        "JDBC",
        "drivers",
        "data",
        "design",
        "analysis",
        "meetings",
        "Created",
        "Architecture",
        "Diagrams",
        "Flow",
        "Charts",
        "Rational",
        "Rose",
        "software",
        "development",
        "practice",
        "programming",
        "test",
        "development",
        "status",
        "meetings",
        "use",
        "case",
        "diagrams",
        "class",
        "diagrams",
        "database",
        "tables",
        "mapping",
        "database",
        "tables",
        "Developed",
        "Unit",
        "test",
        "cases",
        "JUnit",
        "application",
        "configuration",
        "information",
        "properties",
        "Performed",
        "unit",
        "testing",
        "system",
        "testing",
        "integration",
        "testing",
        "Environment",
        "Java",
        "Struts",
        "Framework",
        "log4j",
        "Servlets",
        "JSP",
        "JSTL",
        "I18N",
        "JDBC",
        "HTML",
        "Java",
        "Script",
        "CSS",
        "UML",
        "Oracle",
        "Windows",
        "NT",
        "Education",
        "Bachelor",
        "Technology",
        "Electronics",
        "Communication",
        "Engineering",
        "Electronics",
        "Communication",
        "Engineering",
        "Jawaharlal",
        "Nehru",
        "Technological",
        "University",
        "Skills",
        "ECLIPSE",
        "J2EE",
        "JAVA",
        "SPRING",
        "JMS",
        "JSP",
        "JVM",
        "HDFS",
        "OOZIE",
        "SQOOP",
        "HBASE",
        "DB2",
        "FLUME",
        "HADOOP",
        "MAP",
        "REDUCE",
        "AMAZON",
        "WEB",
        "SERVICES",
        "Git",
        "GRADLE",
        "Hadoop",
        "HBase"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T20:24:28.049263",
    "resume_data": "JavaHadoop Developer span lJavaspanHadoop span lDeveloperspan JavaHadoop Developer TMobile Bellevue WA Certified Apache Spark Developer Over 7 years of experience in software architecture design development testing and maintenance of web and enterprise applications with 3 years of experience in using Hadoop and Hadoop Ecosystems HDFS YARN MAPREDUCE PIG HIVE HBASE SQOOP FLUME ZOOKEEPER OOZIE and 4 years of experience as Java developer Excellent understanding of Hadoop architecture and various components such as HDFS YARN Resource Manager Node Manager Name Node Data Node and Map Reduce programming paradigm Hands on experience using Cloudera Horton works and BigInsights Quick start Hadoop Distributions Hands on experience on Scalaand Python Strong experience in developing Storm topology and ActiveMQ using Java Good understanding how Lambda architectureand Kappa architecture Responsible for writing MapReduce programsHadoop pig and Hive Experience in importing and exporting data using Sqoop from HDFS to Relational Database Systems and viceversa Experience loading data to Hive partitions and creating buckets in Hive Experience developing custom SerDes in HIVE Experience in extending Hive and Pig core functionality by writing custom UDFs Knowledge in extending Hive core functionality by writing custom UDFs like UDAFs and UDTFs Expertise in analysis using PIG HIVE and MapReduce Logical Implementation and interaction with HBase Developed MapReduce jobs to automate transfer the data from HBase Experience in developing ETL process using MapReduce Framework in java Knowledge in efficiently using Sparks memory caching for iterative processing using Scala Good understanding how Spark instructions are translated into jobs Strong Knowledge developing web service using Playframeworkand Akka Actors Good Knowledge on Hadoop Cluster architecture and monitoring the cluster Knowledge in jobworkflow scheduling and monitoring tools like Oozie and Zookeeper Strong understanding of NoSQL databases like HBase MongoDB Cassandra Good working knowledge with Agile for SDLC Knowledge in Amazon AWS concepts like EMR S3 and EC2 web services Efficient team player having strong desire and capabilities to efficiently convert Customers requirements into applications Authorized to work in the US for any employer Work Experience JavaHadoop Developer TMobile Bellevue WA January 2019 to Present Project BEAM Description The transformation program of our billing systems is the main objective of uprising project It will provide new capabilities for flexibility efficiency and new business models Uprising project covers various domains like Business Intelligence and Engineering etc Integrated Data warehouse IDW is TMobile central repository of data in Hadoop and Teradata ecosystems and IDW is part of BI As part of data ingestion team we will bring incremental data from various source systems into Hadoop to maintain one central repository Responsibilities Developed Storm topology to ingest data from various source into hadoop data lake Configured ActiveMQ for enterprise and resolved ActiveMQ issues Developed web application using HBase and Hive API to compare schema between HBase and Hive tables Used JVM monitor to monitor threads and memory usage of HBase and Hive schema check web application Developed code to generate Hive DDLs from source DDLs Created HBase tables using Sqoop from Relational Database Oracle Developed code to import JSON data into MYSQL database Developed code to import data SQLServer into HDFS and created Hive views on data in HDFS using Spark Created scripts to append data from temporary HBase table to target HBase table in Spark Developed Hive queries for the analysis Handling structured and unstructured data and applying ETL processes Handled importing of data from various data sources performance optimization of Sqoop Developed Oozie workflow to compare data in Hive and Source Assessed existing and EDW technologies and methods to ensure our EDWBI architecture meet the needs of the business and enterprise and allows for business growth Assisted with data capacity planning and node forecasting Collaborated with the infrastructure network database application and BI teams to ensure data quality and availability Environment Hadoop Map Reduce HDFS Hive HBase Pig Java Hadoop distribution of Hortonworks SQLUNIX Shell Scripting Oracle SQLServer MySQL Big Data Developer KCPL Kansas City MO August 2018 to January 2019 Project Meter Data Management Description Collected managed and distributed information to the users of databases applications and platforms that are supported by the KCPL team Key areas were leveraging this capability to drive financial benefit to the company in Information Management Marketing and Risk Capabilities Used substantial technical infrastructure to support the data management model and operational philosophy which provided the flexibility and reliability needed for inclass production analytics and online capability Responsibilities Worked on Big Data Hadoop cluster implementation and data integration in developing largescale system software Developed MapReduce programs to parse the raw data populate staging tables and store the refined data in partitioned tables in the EDW Capturing data from existing databases that provide SQL interfaces using Sqoop Worked extensively with Sqoop for importing and exporting the data from HDFS to Relational Database system Oracle and viceversa Loading data into HDFS Develop and maintains complex outbound notification applications that run on custom architectures using diverse technologies including Java XML JMS and Web Services Created Hive queries that helped market analysts spot emerging trends by comparing fresh data with EDW reference tables and historical metrics Enabled speedy reviews and first mover advantages by using Oozie to automate data loading into the Hadoop Distributed File System and PIG to preprocess the data Provided design recommendations and thought leadership to sponsorsstakeholders that improved review processes and resolved technical problems Shared responsibility for administration of Hadoop Hive and Pig Helped business processes by developing and configuring Hadoop ecosystem components that moved data from individual servers to HDFS Created HBase tables to load large sets of structured semistructured and unstructured data coming from UNIX NoSQL and a variety of portfolios Developed multiple MapReduce jobs in Java for data cleaning and preprocessing Developed workflow in Oozie to automate the tasks of loading the data into HDFS and preprocessing with Pig Production Rollout Support and resolving any issues that are discovered by the client and client services teams Environment Hadoop Map Reduce HDFS Hive Java jdk16 Hadoop distribution of Hortonworks Cloudera PLSQL SQL Toad 96 Windows NT UNIX Shell Scripting Software EngineerBig Data Experian Costa Mesa CA December 2017 to July 2018 Project Precise ID for Customer Management Description This project involves setting up a data repository and use it for search processing for analytical and research purposes This application provides the capability for large batch processing using Hadoop map reduce jobs using Java runtime environment as well as real time search capabilities using Solr cloud environment Responsibilities Involved in designing and developing Hadoop MapReduce jobs Using Java Runtime Environment for the batch processing to search and match the scores Used Rational Rose for developing Use case diagrams Activity flow diagrams Class diagrams and Object diagrams in the design phase Used Struts with Tiles in the MVC framework for the application Experiencing in running MapReduce programs on 20 node cluster Amazon EC2 spot instances with Apache Hadoop Developed java web services to upload data from local to Amazon S3 listing S3 objects and file manipulation operations Successfully ran all Hadoop MapReduce programs on Amazon Elastic MapReduce framework by using Amazon S3 for Input and Output Experience in Amazon S3 for storing objects and sharing data between Hadoop Involved in developing Hadoop MapReduce jobs for merging and appending the repository data Used Sqoop to pull the data from Teradata Hands on experience in setting up HBase Column based storage repository for archiving and retro data Used Java Message Service JMS 11 for reliable and asynchronous communication Involved in integration of Legacy Scoring and Analytical Models like SMG3 into the new application using Web Services Responsible for writing Pig UDFs and Hive UDFs Handled importing of data from various data sources performance transformation using Hive Experience in optimization of Map reduce algorithm using combiners and partitions to deliver the best results and worked on Application performance optimization for a HDFS cluster Experience working with offshore teams and communicating daily status on issues roadblocks Environment Java Hadoop HBase Zookeeper Solr cloud Pig Latin Oozie scheduler JavaBeans Eclipse Rational Clear case Servlet 23 JUnit Maven XML Web services JDBC Unix Windows NT2000 Hadoop Developer Deutsche Bank Cary NC September 2015 to November 2017 Project Marketing and Securities FDR Description Designed and developed big data solutions involving Terabytes of data The big data solution consists of collecting large amounts oflog data from distributed sources transformations and standardizations analysis statistics aggregations and reporting etc Built an on demand elastic Hadoop cluster infrastructure to cater the needs of various Big Data projects automated various Big Data workflows to process and extracts analytics out of the data using MapReduce Pig and Hive Responsibilities Installed configured and maintained Apache Hadoop clusters for application development and Hadoop tools like Hive Pig HBase Zookeeper and Sqoop for POC Created HDFS Hadoop Distributed File System and MapReduce jobs in java Implemented NameNode backup using NFS for High availability Used Pig as ETL tool to do transformations event joins and some preaggregations before storing the data onto HDFS Responsible for developing data pipeline using flume Sqoop and pig to extract the data from weblogs and store in HDFS Used Sqoop to import and export data from HDFS to RDBMS and viceversa Created Hive tables and involved in data loading and writing Hive UDFs Exported the analyzed data to the relational database MySQL using Sqoop for visualization and to generate reports Used Hive to analyze the partitioned and bucketed data and compute various metrics for reporting Automated workflows using shell scripts to pull data from various databases into Hadoop Responsible for creating a Solr schema from the Indexer settings Written Solr queries for various search documents EnvironmentHadoop MapReduce Hive HDFS PIG Sqoop Oozie Solr Cloudera Flume HBase Zookeeper Oracle NoSQL and UnixLinux JavaJ2ee Developer Cognolabs Systems Pvt Ltd Hyderabad Telangana October 2013 to August 2015 DescriptionThis project was to develop an interface between the Claims Service Records CSR and a newly developed website for the Insurance Bureau Purpose of creating the interface was to minimize the changes to the existing Claim Service Records On request by a Claims Agent data was requested for CSR which was in a flat file format This was converted to XML by the application and sent to Insurance Bureau website where it was displayed Responsibilities Involved in the software development life cycle coding testing and implementation Worked in the healthcare domain Involved in Using Java Message Service JMS for loosely coupled reliable and asynchronous exchange of patient treatment information among J2EE components and legacy system Developed MDBs using JMS to exchange messages between different applications using MQ Series Involved in working with J2EE Design patterns Singleton Factory DAO and Business Delegate and Model View Controller Architecture with JSF and Spring DI Involved in Content Management using XML Developed a standalone module transforming XML 837 module to database using SAX parser Installed Configured and administered WebSphere ESB v6x Worked on Performance tuning of WebSphere ESB in different environments on different platforms Configured and Implemented web services specifications in collaboration with offshore team Involved in Creating dash board charts business charts using fusion charts Involved in creating reports for the most of the business criteria Environment Java Servlets JSP Vignette Tool XML Eclipse 330 Tomcat 5x Java Developer Skdotcom Technologies Hyderabad Telangana June 2011 to September 2013 DescriptionSpeedTrial is a webbased enterprise software solution for managing multisite clinical research studies It helps the research team to manage clinical trials in an easier faster and more efficient way SpeedTrial is designed for pharmaceutical biotech medical companies contract research organization CROs specialized research organization and academic medical centers AMCs to manage thousands of trials in a variety of therapeutic areas SpeedTrial facilitates creation of study protocol designing the Case Report Forms CRFs by dragging and dropping the sections and questions Electronic Data Capture EDC and exhaustive clinical data management and reduces administration and monitoring costs Responsibilities Designed and Developed application using EJB 20 and Struts framework Developed POJOs for Data Model to map the Java Objects with Relational database tables Designed and developed Service layer using Struts framework Used MVC based Struts framework to develop the multitier web application presentation layer components Involved in Integration of Struts with Database Implemented Struts tag libraries like html logic tab bean etc in the JSP pages Used Struts tiles libraries for layout of web page and performed struts validations using Struts validation framework Implemented Oracle database and JDBC drivers to access the data Involved in design analysis and architectural meetings Created Architecture Diagrams and Flow Charts using Rational Rose Followed agile software development practice paired programming test driven development and scrum status meetings Developed use case diagrams class diagrams database tables and mapping between relational database tables Developed Unit test cases using JUnit Maintained the application configuration information in various properties file Performed unit testing system testing and integration testing Environment Java Struts Framework log4j Servlets JSP JSTL I18N JDBC HTML Java Script CSS UML Oracle Windows NT Education Bachelor of Technology in Electronics and Communication Engineering in Electronics and Communication Engineering Jawaharlal Nehru Technological University Skills ECLIPSE J2EE JAVA SPRING JMS JSP JVM HDFS OOZIE SQOOP HBASE DB2 FLUME HADOOP MAP REDUCE AMAZON WEB SERVICES Git GRADLE Hadoop HBase",
    "unique_id": "99087fbd-fec2-4f06-8fb2-d1db0d12ee3f"
}