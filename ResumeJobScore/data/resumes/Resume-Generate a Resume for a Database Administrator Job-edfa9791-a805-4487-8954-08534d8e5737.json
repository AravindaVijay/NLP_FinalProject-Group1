{
    "clean_data": "Python Developer span lPythonspan span lDeveloperspan Python Developer Infor Inc New York NY Over 10 years of IT industry experience encompassing in Machine Learning Datamining with large datasets of Structured and Unstructured data Data Acquisition Data Validation Predictive modeling DataVisualization Extensive experience in Text Analytics developing different statistical machine learning Datamining solutions to various business problems and generating data visualizations using R Python and Tableau Experience objectoriented programming OOP concepts using Python C and PHP Experience on advanced SAS programming techniques such as PROC SQL JOIN UNION PROC APPEND PROC DATASETS and PROC TRANSPOSE Integration Architect Data Scientist experience in Analytics Bigdata BPM SOA ETL and Cloud technologies Highly skilled in using visualization tools like Tableau ggplot2 and d3js for creating dashboards Tagging of experience in foundational machine learning models and concepts regression random forest boosting GBM NNs HMMs CRFs MRFs deep learning Proficiency in understanding statistical and other toolslanguages R Python C C Java SQL UNIX QlikView data visualization tool and Anaplan forecasting tool Proficient in the Integration of various data sources with multiple relational databases like Oracle MS SQL Server DB2 Teradata and Flat Files into the staging area ODS Data Warehouse and DataMart Experience in Extracting data for creating Value Added Datasets using Python R SAS Azure and SQL to analyze the behavior to target a specific set of customers to obtain hidden insights within the data to effectively implement the project Objectives Worked with NoSQL Database including HBase Cassandra and MongoDB Extensively worked on statistical analysis tools and adept at writing code in Advanced Excel R MATLAB Python Implemented deep learning models and numerical Computation with the help of data flow graphs using Tensor Flow Machine Learning Worked with complex applications such as R Stata Scala Perl Linear SAS and SPSS to develop a neural network cluster analysis Experienced the full software lifecycle in SDLC Agile and Scrum methodologies Experience in designing stunning visualizations using Tableau software and publishing and presenting dashboards Storyline on web and desktop platforms Hands on experience in implementing LDA Naive Bayes and skilled in RandomForests DecisionTrees Linear and LogisticRegression SVM Clustering neural networks Principle Component Analysis and good knowledge of Recommender Systems Experienced with machine learning algorithms such as logistic regression random forest XP boost KNN SVM neural network linear regression lasso regression and kmeans Strong experience in Software Development Life Cycle SDLC including Requirements Analysis Design Specification and Testing as per Cycle in both Waterfall and Agile methodologies Skilled in using dplyr and pandas in R and python for performing Exploratory data analysis Experience working with data modeling tools like Erwin Power Designer and ERStudio Experience with data analytics data reporting Adhoc reporting Graphs Scales PivotTables and OLAP reporting Highly skilled in using visualization tools like Tableau ggplot2 and d3js for creating dashboards Worked and extracted data from various database sources like Oracle SQL Server DB2 and Teradata Proficient knowledge of statistics mathematics machine learning recommendation algorithms and analytics with an excellent understanding of business operations and analytics tools for effective analysis of data Authorized to work in the US for any employer Work Experience Python Developer Infor Inc New York NY July 2017 to Present Description Myntra is an Indian fashion ecommerce marketplace company headquartered in Bengaluru Karnataka India The focus of the company is the online retailing of branded apparel The main goal of the project is to develop an application to generate the tickets for the orders of the customers Responsibilities Worked on the project from gathering requirements to developing the entire application Worked on AnacondaPython Environment Created activated and programmed in Anaconda environment Wrote programs for performance calculations using NumPy and SQLAlchemy Wrote python routines to log into the websites and fetch data for selected options Used python modules of Urllib urllib2 Requests for web crawling Extensive experience in Text Analytics developing different Statistical Machine Learning Data mining solutions to various business problems and generating data visualizations using R Python and Tableau Used with other packages such as Beautiful Soup for data parsing Involved in development of Web Services using SOAP for sending and getting data from the external interface in the XML format Used with other packages such as Beautiful Soup for data parsing Worked on development of SQL and stored procedures on MYSQL Analyzed the code completely and have reduced the code redundancy to the optimal level Design and build a text classification application using different text classification models Used Jira for defect tracking and project management Worked on writing and as well as read data from CSV and excel file formats Involved in Sprintplanning sessions and participated in the daily AgileSCRUM meetings Conducted every day scrum as part of the SCRUM Master role Developed the project in Linux environment Worked on resulting reports of the application Performed QA testing on the application Held meetings with client and worked for the entire project with limited help from the client Environment Python Anaconda Spyder IDE Windows 7 Teradata Requests urllib urllib2 Beautiful Soup Tableau python libraries such as NumPy SQLAlchemy MySQL Data ScientistMachine Learning Engineer AT T Dallas TX February 2015 to July 2017 Description ATT Inc provides telecommunications and digital entertainment services The company operates through four segments Business Solutions Entertainment Group Consumer Mobility and International Responsibilities Used Pandas NumPy Seaborn SciPy Matplotlib Scikitlearn and NLTK in Python for developing various machine learning algorithms Worked on different data formats such as JSON XML and performed machine learning algorithms in Python Setup storage and data analysis tools in Amazon Web Services cloud computing infrastructure Implemented endtoend systems for Data Analytics Data Automation and integrated with custom visualization tools using R Mahout Hadoop and MongoDB Utilized Spark Scala Hadoop HBase Cassandra MongoDB Kafka Spark Streaming  Python a broad variety of machine learning methods including classifications dimensionality reduction etc and utilized the engine to increase user lifetime by 45 and triple user conversations for target categories Used Spark Data frames SparkSQL Spark  extensively and developing and designing POCs using Scala Spark SQL and MLlib libraries Used Data Quality Validation techniques to validate Critical Data Elements CDE and identified various anomalies Participated in all phases of DataMining Datacollection DataCleaning DevelopingModels Validation Visualization and Performed Gap Analysis Data Manipulation and Aggregation from different source using Nexus Toad Business Objects and SmartView Implemented Agile Methodology for building an internal application Focus on integration overlap and Informatica newer commitment to MDM with the acquisition of Identity Systems Good knowledge of Hadoop Architecture and various components such as HDFS Job Tracker Task Tracker Name node Data node Secondary Name node and MapReduce concepts Programmed a utility in Python that used multiple packages SciPy NumPy Pandas Implemented Classification using supervised algorithms like Logistic Regression Decision trees KNN Naive Bayes Updated Python scripts to match training data with our database stored in AWS Cloud Search so that we would be able to assign each document a response label for further classification Created SQL tables with referential integrity and developed queries using SQL SQL PLUS and PLSQL Interaction with Business Analyst SMEs and other Data Architects to understand Business needs and functionality for various project solutions Identifying and executing process improvements handson in various technologies such as Oracle Informatica and BusinessObjects Environment AWS Informatica Python HDFS ODS OLTP Oracle 10g Hive OLAP DB2 Metadata MS Excel Mainframes MS Vision MapReduce Rational Rose SQL and MongoDB Sr Python developer UnitedHealth Group Minneapolis MN November 2013 to February 2015 Description Cisco is an American multinational technology that develops manufactures and sells networking hardware telecommunications equipment and other hightechnology services and products The main goal of the project is to report IT security mobility problems faced by the customers and suggest solutions to them We developed a website for reporting issues and finding solutions to that problem easily Responsibilities Analyzed the requirements and designed the flow of task using flow charts and designed flow between pages of the UI Documented the design solutions and created stories for client requirements Written REST services using python and Apollo internal to Cisco Written Python Scripts to establish continuous workflows from different teams providing data Written unit and integration tests in python to test the code Implemented LDAP authentication to authenticate and authorize the Customers using python Rest Services Generated client certificates in both pem and pfx formats using M2Crypto python module Used  database for caching the client requests Written LDAP search filters for both single level and multilevel Complete UI development using AngularJS CSS and HTML5 Dashboards with quick filters parameters and sets to handle views more efficiently Performed user validations on client side as well as server side Improved code reuse and performance by making effective use of various design patterns Efficient delivered code based on principles of Test Driven Development TDD and continuous integration to keep in line with Agile Software Methodology principles Participated with QA to develop test plans from highlevel design documentation Used Rally for Agile software management Primary contact for all issues in both development and production environments Implemented the Longterm fix for incidents that are happened in production environment by finding the root cause Completed the cisco white belt in security which helps to develop the applications in a secured manner to protect from the threats Environment Python 27 Cassandra MySQL LDAP Git Linux Windows JSON JQuery HTML XML CSS REST Rally Bootstrap JavaScript Angular JS Agile Bitbucket Py Unit PyCharm Microsoft SQL server management studio DataStax DevCenter Apache Directory Studio Ansible Jenkins Matplotlib MOCK Beautiful Soup PyTest Machine Learning EngineerData Scientist Regions Bank Birmingham AL August 2011 to November 2013 Description Regions Financial Corporation is a bank and financial services company headquartered in the Regions Center in Birmingham Alabama The company provides retail and commercial banking trust securities brokerage mortgage and insurance products and services Responsibilities Worked with several R packages including knit dplyr SparkR Causal Infer SpaceTime Coded R functions to interface with Caffe Deep Learning Framework Used Pandas NumPy Seaborn SciPy Matplotlib Scikitlearn and NLTK in Python for developing various machine learning algorithms Installed and used Caffe Deep Learning Framework Worked on different data formats such as JSON XML and performed machine learning algorithms in Python Setup storage and data analysis tools in Amazon Web Services cloud computing infrastructure Implemented endtoend systems for Data Analytics Data Automation and integrated with custom visualization tools using R Mahout Hadoop and MongoDB Worked as Data Architects and IT Architects to understand the movement of data and its storage and ERStudio97 Utilized Spark Scala Hadoop HBase Cassandra MongoDB Kafka Spark Streaming  Python a broad variety of machine learning methods including classifications regressions dimensionally reduction etc and Utilized the engine to increase user lifetime by 45 and triple user conversations for target categories Used Spark Data frames SparkSQL Spark  extensively and developing and designing POCs using Scala Spark SQL and MLlib libraries Used Data Quality Validation techniques to validate Critical Data Elements CDE and identified various anomalies Extensively worked on Data Modeling tools Erwin Data Modeler to design the Data Models Developed various QlikView Data Models by extracting and using the data from various sources files DB2 Excel Flat Files and Bigdata Participated in all phases of DataMining Datacollection DataCleaning DevelopingModels Validation Visualization and Performed Gap Analysis Data Manipulation and Aggregation from a different source using Nexus Toad BusinessObjects Power BI and SmartView Implemented Agile Methodology for building an internal application Focus on integration overlap and Informatica newer commitment to MDM with the acquisition of Identity Systems Good knowledge of Hadoop Architecture and various components such as HDFS Job Tracker Task Tracker Name Node Data Node Secondary Name Node and MapReduce concepts As Architect delivered various complex OLAP DatabasesCubes Scorecards Dashboards and Reports Programmed a utility in Python that used multiple packages SciPy NumPy Pandas Implemented Classification using supervised algorithms like Logistic Regression Decision Trees KNN Naive Bayes Designed both 3NF data models for ODS OLTP systems and Dimensional Data Models using Star and Snowflake Schemas Updated Python scripts to match training data with our database stored in AWS Cloud Search so that we would be able to assign each document a response label for further classification Created SQL tables with referential integrity and developed queries using SQL SQLPLUS and PLSQL Designed and developed Use Case Activity Diagrams Sequence Diagrams OOD Object oriented Design using UML and Visio Interaction with Business Analyst SMEs and other Data Architects to understand Business needs and functionality for various project solutions Identifying and executing process improvements handson in various technologies such as Oracle Informatica and BusinessObjects Environment AWS R Informatica Python HDFS ODS OLTP Oracle 10g Hive OLAP DB2 Metadata MS Excel Mainframes MS Vision MapReduce Rational Rose SQL and MongoDB Data Scientist Cisco San Jose CA May 2009 to August 2011 Description UnitedHealth Group Inc is an American forprofit managed health care company based in Minnetonka Minnesota It is sixth in the United States on the Fortune 500UnitedHealth Group offers health care products and insurance services UnitedHealth Group is the largest healthcare company in the world by revenue 184 billion in 2016 UnitedHealth Group subsidiaries companies together serve approximately 115 million individuals in 2016 Responsibilities Provided the architectural leadership in shaping strategic business technology projects with an emphasis on application architecture Utilized domain knowledge and application portfolio knowledge to play a key role in defining the future state of large business technology programs Participated in all phases of data mining data collection data cleaning developing models validation and visualization and performed Gap analysis Developed MapReduceSpark Python modules for machine learning predictive analytics in Hadoop on AWS Implemented a Pythonbased distributed random forest via Python streaming Created ecosystem models eg conceptual logical physical canonical that are required for supporting services within the enterprise data architecture conceptual data model for defining the major subject areas used ecosystem logical model for defining standard business meaning for entities and fields and an ecosystem canonical model for defining the standard messages and formats to be used in data integration services throughout the ecosystem Used PandasNumPy seaborn SciPy Matplotlib Scikitlearn NLTK in Python for developing various machine learning algorithms and utilized machine learning algorithms such as linear regression multivariate regression naive Bayes Random Forests Kmeans KNN for data analysis Conducted studies rapid plots and using advance data mining and statistical modelling techniques to build a solution that optimize the quality and performance of data Demonstrated experience in design and implementation of Statistical models Predictive models enterprise data model metadata solution and data life cycle management in both RDBMS Big Data environments Analyzed large data sets apply machine learning techniques and develop predictive models statistical models and developing and enhancing statistical models by leveraging bestinclass modeling techniques Worked on database design relational integrity constraints OLAP OLTP Cubes and Normalization 3NF and Denormalization of the database Developed MapReduceSpark Python modules for machine learning predictive analytics in Hadoop on AWS Worked on customer segmentation using an unsupervised learning technique clustering Worked with various Teradata15 tools and utilities like TeradataViewpoint Multi Load ARC Teradata Administrator BTEQ and other Teradata Utilities Utilized Spark Scala Hadoop HBase Kafka Spark Streaming MLlib Python a broad variety of machine learning methods including classifications regressions dimensionally reduction etc Developed LINUX Shell scripts by using NZSQLNZLOAD utilities to load data from flat files to Netezza database Designed and implemented system architecture for Amazon EC2 based cloudhosted solution for the client Tested Complex ETL Mappings and Sessions based on business user requirements and business rules to load data from source flat files and RDBMS tables to target tables Environment Erwin r96 Python SQL Oracle 12c Netezza SQL Server Informatica Java SSRS PLSQL TSQL Tableau MLlib regression Cluster analysis Scala NLP Spark Kafka MongoDB logistic regression Hadoop Hive Teradata random forest OLAP Azure MariaDB SAP CRM HDFS ODS NLTK SVM JSON Tableau XML Cassandra MapReduce AWS Education Bachelors Skills Analysis services Application development Hadoop Hbase Hdfs Javascript MySQL Android ASP PHP",
    "entities": [
        "MLlib",
        "Use Case Activity",
        "Description Regions Financial Corporation",
        "Oracle SQL Server",
        "Informatica",
        "Test Driven Development TDD",
        "MN",
        "New York",
        "XP",
        "Karnataka",
        "Description ATT Inc",
        "Agile Software Methodology",
        "Tensor Flow Machine Learning Worked",
        "SQL Oracle",
        "Denormalization of the database",
        "Requirements Analysis Design Specification",
        "Beautiful Soup",
        "Critical Data Elements CDE",
        "Erwin Data Modeler",
        "Worked on AnacondaPython Environment Created",
        "Identity Systems Good",
        "Hadoop",
        "SOAP",
        "XML",
        "DataMining Datacollection DataCleaning DevelopingModels Validation Visualization",
        "Rest Services Generated",
        "Objectives Worked",
        "SPSS",
        "Software Development Life Cycle SDLC",
        "HDFS Job Tracker Task Tracker Name",
        "Oracle Informatica",
        "Used Rally for Agile",
        "TX",
        "Amazon",
        "Extracting",
        "Python C",
        "DataVisualization Extensive",
        "DataStax DevCenter",
        "Conducted",
        "Spark Data",
        "Dallas",
        "ODS NLTK",
        "Dimensional Data Models using Star and Snowflake Schemas Updated Python",
        "Bengaluru",
        "Utilized",
        "UML",
        "Waterfall",
        "Visio Interaction with Business Analyst",
        "San Jose",
        "Value Added Datasets",
        "LDA Naive Bayes",
        "Used Data Quality Validation",
        "KNN Naive Bayes Updated Python",
        "Linux",
        "Unstructured",
        "Cloud technologies Highly",
        "Minneapolis",
        "Teradata Utilities Utilized",
        "the Regions Center",
        "APPEND",
        "Logistic Regression Decision Trees KNN Naive Bayes Designed",
        "NZSQLNZLOAD",
        "Mainframes MS Vision MapReduce Rational Rose SQL",
        "PROC",
        "linear",
        "CSV",
        "Oracle MS SQL Server",
        "SDLC Agile",
        "US",
        "QA",
        "Graphs Scales PivotTables",
        "Nexus Toad Business Objects",
        "ODS Data Warehouse",
        "Caffe Deep Learning Framework",
        "KNN",
        "Data Architects",
        "Sr Python",
        "Created SQL",
        "Hadoop Architecture",
        "Text Analytics",
        "Nexus Toad BusinessObjects Power BI",
        "the Fortune 500UnitedHealth Group",
        "Apollo",
        "HDFS Job Tracker Task Tracker",
        "SAS",
        "highlevel",
        "Completed",
        "Data Acquisition Data Validation Predictive",
        "SQL",
        "ODS OLTP",
        "Metadata MS",
        "Amazon Web Services",
        "GBM",
        "Exploratory",
        "lPythonspan",
        "Responsibilities Worked",
        "Bayes Random Forests Kmeans",
        "the United States",
        "SQL SQL PLUS",
        "Anaconda",
        "Description UnitedHealth Group Inc",
        "Statistical Machine Learning Data",
        "MDM",
        "SmartView Implemented Agile Methodology",
        "Bigdata Participated",
        "the UI Documented",
        "India",
        "Anaplan",
        "Performed",
        "OLAP",
        "Cisco",
        "Responsibilities Analyzed",
        "TeradataViewpoint Multi Load",
        "Android ASP PHP",
        "ERStudio",
        "Logistic Regression Decision",
        "Principle Component Analysis",
        "Data Analytics Data Automation",
        "Recommender Systems Experienced",
        "CSS",
        "Data Modeling",
        "R Mahout Hadoop",
        "Structured",
        "Python Setup",
        "SQL JOIN UNION",
        "MapReduce",
        "Present Description Myntra",
        "UnitedHealth Group",
        "Tableau",
        "Business Solutions Entertainment Group",
        "Birmingham",
        "SAP CRM",
        "Python R SAS Azure",
        "NoSQL Database",
        "SVM",
        "Node",
        "JSON XML"
    ],
    "experience": "Experience objectoriented programming OOP concepts using Python C and PHP Experience on advanced SAS programming techniques such as PROC SQL JOIN UNION PROC APPEND PROC DATASETS and PROC TRANSPOSE Integration Architect Data Scientist experience in Analytics Bigdata BPM SOA ETL and Cloud technologies Highly skilled in using visualization tools like Tableau ggplot2 and d3js for creating dashboards Tagging of experience in foundational machine learning models and concepts regression random forest boosting GBM NNs HMMs CRFs MRFs deep learning Proficiency in understanding statistical and other toolslanguages R Python C C Java SQL UNIX QlikView data visualization tool and Anaplan forecasting tool Proficient in the Integration of various data sources with multiple relational databases like Oracle MS SQL Server DB2 Teradata and Flat Files into the staging area ODS Data Warehouse and DataMart Experience in Extracting data for creating Value Added Datasets using Python R SAS Azure and SQL to analyze the behavior to target a specific set of customers to obtain hidden insights within the data to effectively implement the project Objectives Worked with NoSQL Database including HBase Cassandra and MongoDB Extensively worked on statistical analysis tools and adept at writing code in Advanced Excel R MATLAB Python Implemented deep learning models and numerical Computation with the help of data flow graphs using Tensor Flow Machine Learning Worked with complex applications such as R Stata Scala Perl Linear SAS and SPSS to develop a neural network cluster analysis Experienced the full software lifecycle in SDLC Agile and Scrum methodologies Experience in designing stunning visualizations using Tableau software and publishing and presenting dashboards Storyline on web and desktop platforms Hands on experience in implementing LDA Naive Bayes and skilled in RandomForests DecisionTrees Linear and LogisticRegression SVM Clustering neural networks Principle Component Analysis and good knowledge of Recommender Systems Experienced with machine learning algorithms such as logistic regression random forest XP boost KNN SVM neural network linear regression lasso regression and kmeans Strong experience in Software Development Life Cycle SDLC including Requirements Analysis Design Specification and Testing as per Cycle in both Waterfall and Agile methodologies Skilled in using dplyr and pandas in R and python for performing Exploratory data analysis Experience working with data modeling tools like Erwin Power Designer and ERStudio Experience with data analytics data reporting Adhoc reporting Graphs Scales PivotTables and OLAP reporting Highly skilled in using visualization tools like Tableau ggplot2 and d3js for creating dashboards Worked and extracted data from various database sources like Oracle SQL Server DB2 and Teradata Proficient knowledge of statistics mathematics machine learning recommendation algorithms and analytics with an excellent understanding of business operations and analytics tools for effective analysis of data Authorized to work in the US for any employer Work Experience Python Developer Infor Inc New York NY July 2017 to Present Description Myntra is an Indian fashion ecommerce marketplace company headquartered in Bengaluru Karnataka India The focus of the company is the online retailing of branded apparel The main goal of the project is to develop an application to generate the tickets for the orders of the customers Responsibilities Worked on the project from gathering requirements to developing the entire application Worked on AnacondaPython Environment Created activated and programmed in Anaconda environment Wrote programs for performance calculations using NumPy and SQLAlchemy Wrote python routines to log into the websites and fetch data for selected options Used python modules of Urllib urllib2 Requests for web crawling Extensive experience in Text Analytics developing different Statistical Machine Learning Data mining solutions to various business problems and generating data visualizations using R Python and Tableau Used with other packages such as Beautiful Soup for data parsing Involved in development of Web Services using SOAP for sending and getting data from the external interface in the XML format Used with other packages such as Beautiful Soup for data parsing Worked on development of SQL and stored procedures on MYSQL Analyzed the code completely and have reduced the code redundancy to the optimal level Design and build a text classification application using different text classification models Used Jira for defect tracking and project management Worked on writing and as well as read data from CSV and excel file formats Involved in Sprintplanning sessions and participated in the daily AgileSCRUM meetings Conducted every day scrum as part of the SCRUM Master role Developed the project in Linux environment Worked on resulting reports of the application Performed QA testing on the application Held meetings with client and worked for the entire project with limited help from the client Environment Python Anaconda Spyder IDE Windows 7 Teradata Requests urllib urllib2 Beautiful Soup Tableau python libraries such as NumPy SQLAlchemy MySQL Data ScientistMachine Learning Engineer AT T Dallas TX February 2015 to July 2017 Description ATT Inc provides telecommunications and digital entertainment services The company operates through four segments Business Solutions Entertainment Group Consumer Mobility and International Responsibilities Used Pandas NumPy Seaborn SciPy Matplotlib Scikitlearn and NLTK in Python for developing various machine learning algorithms Worked on different data formats such as JSON XML and performed machine learning algorithms in Python Setup storage and data analysis tools in Amazon Web Services cloud computing infrastructure Implemented endtoend systems for Data Analytics Data Automation and integrated with custom visualization tools using R Mahout Hadoop and MongoDB Utilized Spark Scala Hadoop HBase Cassandra MongoDB Kafka Spark Streaming   Python a broad variety of machine learning methods including classifications dimensionality reduction etc and utilized the engine to increase user lifetime by 45 and triple user conversations for target categories Used Spark Data frames SparkSQL Spark   extensively and developing and designing POCs using Scala Spark SQL and MLlib libraries Used Data Quality Validation techniques to validate Critical Data Elements CDE and identified various anomalies Participated in all phases of DataMining Datacollection DataCleaning DevelopingModels Validation Visualization and Performed Gap Analysis Data Manipulation and Aggregation from different source using Nexus Toad Business Objects and SmartView Implemented Agile Methodology for building an internal application Focus on integration overlap and Informatica newer commitment to MDM with the acquisition of Identity Systems Good knowledge of Hadoop Architecture and various components such as HDFS Job Tracker Task Tracker Name node Data node Secondary Name node and MapReduce concepts Programmed a utility in Python that used multiple packages SciPy NumPy Pandas Implemented Classification using supervised algorithms like Logistic Regression Decision trees KNN Naive Bayes Updated Python scripts to match training data with our database stored in AWS Cloud Search so that we would be able to assign each document a response label for further classification Created SQL tables with referential integrity and developed queries using SQL SQL PLUS and PLSQL Interaction with Business Analyst SMEs and other Data Architects to understand Business needs and functionality for various project solutions Identifying and executing process improvements handson in various technologies such as Oracle Informatica and BusinessObjects Environment AWS Informatica Python HDFS ODS OLTP Oracle 10 g Hive OLAP DB2 Metadata MS Excel Mainframes MS Vision MapReduce Rational Rose SQL and MongoDB Sr Python developer UnitedHealth Group Minneapolis MN November 2013 to February 2015 Description Cisco is an American multinational technology that develops manufactures and sells networking hardware telecommunications equipment and other hightechnology services and products The main goal of the project is to report IT security mobility problems faced by the customers and suggest solutions to them We developed a website for reporting issues and finding solutions to that problem easily Responsibilities Analyzed the requirements and designed the flow of task using flow charts and designed flow between pages of the UI Documented the design solutions and created stories for client requirements Written REST services using python and Apollo internal to Cisco Written Python Scripts to establish continuous workflows from different teams providing data Written unit and integration tests in python to test the code Implemented LDAP authentication to authenticate and authorize the Customers using python Rest Services Generated client certificates in both pem and pfx formats using M2Crypto python module Used   database for caching the client requests Written LDAP search filters for both single level and multilevel Complete UI development using AngularJS CSS and HTML5 Dashboards with quick filters parameters and sets to handle views more efficiently Performed user validations on client side as well as server side Improved code reuse and performance by making effective use of various design patterns Efficient delivered code based on principles of Test Driven Development TDD and continuous integration to keep in line with Agile Software Methodology principles Participated with QA to develop test plans from highlevel design documentation Used Rally for Agile software management Primary contact for all issues in both development and production environments Implemented the Longterm fix for incidents that are happened in production environment by finding the root cause Completed the cisco white belt in security which helps to develop the applications in a secured manner to protect from the threats Environment Python 27 Cassandra MySQL LDAP Git Linux Windows JSON JQuery HTML XML CSS REST Rally Bootstrap JavaScript Angular JS Agile Bitbucket Py Unit PyCharm Microsoft SQL server management studio DataStax DevCenter Apache Directory Studio Ansible Jenkins Matplotlib MOCK Beautiful Soup PyTest Machine Learning EngineerData Scientist Regions Bank Birmingham AL August 2011 to November 2013 Description Regions Financial Corporation is a bank and financial services company headquartered in the Regions Center in Birmingham Alabama The company provides retail and commercial banking trust securities brokerage mortgage and insurance products and services Responsibilities Worked with several R packages including knit dplyr SparkR Causal Infer SpaceTime Coded R functions to interface with Caffe Deep Learning Framework Used Pandas NumPy Seaborn SciPy Matplotlib Scikitlearn and NLTK in Python for developing various machine learning algorithms Installed and used Caffe Deep Learning Framework Worked on different data formats such as JSON XML and performed machine learning algorithms in Python Setup storage and data analysis tools in Amazon Web Services cloud computing infrastructure Implemented endtoend systems for Data Analytics Data Automation and integrated with custom visualization tools using R Mahout Hadoop and MongoDB Worked as Data Architects and IT Architects to understand the movement of data and its storage and ERStudio97 Utilized Spark Scala Hadoop HBase Cassandra MongoDB Kafka Spark Streaming   Python a broad variety of machine learning methods including classifications regressions dimensionally reduction etc and Utilized the engine to increase user lifetime by 45 and triple user conversations for target categories Used Spark Data frames SparkSQL Spark   extensively and developing and designing POCs using Scala Spark SQL and MLlib libraries Used Data Quality Validation techniques to validate Critical Data Elements CDE and identified various anomalies Extensively worked on Data Modeling tools Erwin Data Modeler to design the Data Models Developed various QlikView Data Models by extracting and using the data from various sources files DB2 Excel Flat Files and Bigdata Participated in all phases of DataMining Datacollection DataCleaning DevelopingModels Validation Visualization and Performed Gap Analysis Data Manipulation and Aggregation from a different source using Nexus Toad BusinessObjects Power BI and SmartView Implemented Agile Methodology for building an internal application Focus on integration overlap and Informatica newer commitment to MDM with the acquisition of Identity Systems Good knowledge of Hadoop Architecture and various components such as HDFS Job Tracker Task Tracker Name Node Data Node Secondary Name Node and MapReduce concepts As Architect delivered various complex OLAP DatabasesCubes Scorecards Dashboards and Reports Programmed a utility in Python that used multiple packages SciPy NumPy Pandas Implemented Classification using supervised algorithms like Logistic Regression Decision Trees KNN Naive Bayes Designed both 3NF data models for ODS OLTP systems and Dimensional Data Models using Star and Snowflake Schemas Updated Python scripts to match training data with our database stored in AWS Cloud Search so that we would be able to assign each document a response label for further classification Created SQL tables with referential integrity and developed queries using SQL SQLPLUS and PLSQL Designed and developed Use Case Activity Diagrams Sequence Diagrams OOD Object oriented Design using UML and Visio Interaction with Business Analyst SMEs and other Data Architects to understand Business needs and functionality for various project solutions Identifying and executing process improvements handson in various technologies such as Oracle Informatica and BusinessObjects Environment AWS R Informatica Python HDFS ODS OLTP Oracle 10 g Hive OLAP DB2 Metadata MS Excel Mainframes MS Vision MapReduce Rational Rose SQL and MongoDB Data Scientist Cisco San Jose CA May 2009 to August 2011 Description UnitedHealth Group Inc is an American forprofit managed health care company based in Minnetonka Minnesota It is sixth in the United States on the Fortune 500UnitedHealth Group offers health care products and insurance services UnitedHealth Group is the largest healthcare company in the world by revenue 184 billion in 2016 UnitedHealth Group subsidiaries companies together serve approximately 115 million individuals in 2016 Responsibilities Provided the architectural leadership in shaping strategic business technology projects with an emphasis on application architecture Utilized domain knowledge and application portfolio knowledge to play a key role in defining the future state of large business technology programs Participated in all phases of data mining data collection data cleaning developing models validation and visualization and performed Gap analysis Developed MapReduceSpark Python modules for machine learning predictive analytics in Hadoop on AWS Implemented a Pythonbased distributed random forest via Python streaming Created ecosystem models eg conceptual logical physical canonical that are required for supporting services within the enterprise data architecture conceptual data model for defining the major subject areas used ecosystem logical model for defining standard business meaning for entities and fields and an ecosystem canonical model for defining the standard messages and formats to be used in data integration services throughout the ecosystem Used PandasNumPy seaborn SciPy Matplotlib Scikitlearn NLTK in Python for developing various machine learning algorithms and utilized machine learning algorithms such as linear regression multivariate regression naive Bayes Random Forests Kmeans KNN for data analysis Conducted studies rapid plots and using advance data mining and statistical modelling techniques to build a solution that optimize the quality and performance of data Demonstrated experience in design and implementation of Statistical models Predictive models enterprise data model metadata solution and data life cycle management in both RDBMS Big Data environments Analyzed large data sets apply machine learning techniques and develop predictive models statistical models and developing and enhancing statistical models by leveraging bestinclass modeling techniques Worked on database design relational integrity constraints OLAP OLTP Cubes and Normalization 3NF and Denormalization of the database Developed MapReduceSpark Python modules for machine learning predictive analytics in Hadoop on AWS Worked on customer segmentation using an unsupervised learning technique clustering Worked with various Teradata15 tools and utilities like TeradataViewpoint Multi Load ARC Teradata Administrator BTEQ and other Teradata Utilities Utilized Spark Scala Hadoop HBase Kafka Spark Streaming MLlib Python a broad variety of machine learning methods including classifications regressions dimensionally reduction etc Developed LINUX Shell scripts by using NZSQLNZLOAD utilities to load data from flat files to Netezza database Designed and implemented system architecture for Amazon EC2 based cloudhosted solution for the client Tested Complex ETL Mappings and Sessions based on business user requirements and business rules to load data from source flat files and RDBMS tables to target tables Environment Erwin r96 Python SQL Oracle 12c Netezza SQL Server Informatica Java SSRS PLSQL TSQL Tableau MLlib regression Cluster analysis Scala NLP Spark Kafka MongoDB logistic regression Hadoop Hive Teradata random forest OLAP Azure MariaDB SAP CRM HDFS ODS NLTK SVM JSON Tableau XML Cassandra MapReduce AWS Education Bachelors Skills Analysis services Application development Hadoop Hbase Hdfs Javascript MySQL Android ASP PHP",
    "extracted_keywords": [
        "Python",
        "Developer",
        "lPythonspan",
        "span",
        "lDeveloperspan",
        "Python",
        "Developer",
        "Infor",
        "Inc",
        "New",
        "York",
        "NY",
        "years",
        "IT",
        "industry",
        "experience",
        "Machine",
        "Learning",
        "Datamining",
        "datasets",
        "Structured",
        "Unstructured",
        "data",
        "Data",
        "Acquisition",
        "Data",
        "Validation",
        "Predictive",
        "DataVisualization",
        "experience",
        "Text",
        "Analytics",
        "machine",
        "Datamining",
        "solutions",
        "business",
        "problems",
        "data",
        "visualizations",
        "R",
        "Python",
        "Tableau",
        "Experience",
        "programming",
        "OOP",
        "concepts",
        "Python",
        "C",
        "PHP",
        "Experience",
        "SAS",
        "programming",
        "techniques",
        "PROC",
        "SQL",
        "JOIN",
        "UNION",
        "PROC",
        "APPEND",
        "PROC",
        "DATASETS",
        "PROC",
        "TRANSPOSE",
        "Integration",
        "Architect",
        "Data",
        "Scientist",
        "experience",
        "Analytics",
        "Bigdata",
        "BPM",
        "SOA",
        "ETL",
        "Cloud",
        "technologies",
        "visualization",
        "tools",
        "Tableau",
        "ggplot2",
        "d3js",
        "dashboards",
        "Tagging",
        "experience",
        "machine",
        "learning",
        "models",
        "concepts",
        "forest",
        "GBM",
        "NNs",
        "HMMs",
        "CRFs",
        "MRFs",
        "Proficiency",
        "toolslanguages",
        "R",
        "Python",
        "C",
        "C",
        "Java",
        "SQL",
        "UNIX",
        "QlikView",
        "data",
        "visualization",
        "tool",
        "Anaplan",
        "forecasting",
        "tool",
        "Proficient",
        "Integration",
        "data",
        "sources",
        "databases",
        "Oracle",
        "MS",
        "SQL",
        "Server",
        "DB2",
        "Teradata",
        "Flat",
        "Files",
        "staging",
        "area",
        "ODS",
        "Data",
        "Warehouse",
        "DataMart",
        "Experience",
        "data",
        "Value",
        "Added",
        "Datasets",
        "Python",
        "R",
        "SAS",
        "Azure",
        "SQL",
        "behavior",
        "set",
        "customers",
        "insights",
        "data",
        "project",
        "Objectives",
        "NoSQL",
        "Database",
        "HBase",
        "Cassandra",
        "MongoDB",
        "analysis",
        "tools",
        "code",
        "Advanced",
        "Excel",
        "R",
        "MATLAB",
        "Python",
        "learning",
        "models",
        "numerical",
        "Computation",
        "help",
        "data",
        "flow",
        "graphs",
        "Tensor",
        "Flow",
        "Machine",
        "Learning",
        "applications",
        "R",
        "Stata",
        "Scala",
        "Perl",
        "Linear",
        "SAS",
        "SPSS",
        "network",
        "cluster",
        "analysis",
        "software",
        "lifecycle",
        "SDLC",
        "Agile",
        "Scrum",
        "methodologies",
        "Experience",
        "visualizations",
        "Tableau",
        "software",
        "publishing",
        "dashboards",
        "Storyline",
        "web",
        "desktop",
        "platforms",
        "Hands",
        "experience",
        "LDA",
        "Naive",
        "Bayes",
        "RandomForests",
        "DecisionTrees",
        "Linear",
        "LogisticRegression",
        "SVM",
        "networks",
        "Principle",
        "Component",
        "Analysis",
        "knowledge",
        "Recommender",
        "Systems",
        "machine",
        "learning",
        "algorithms",
        "regression",
        "forest",
        "XP",
        "KNN",
        "SVM",
        "network",
        "linear",
        "regression",
        "lasso",
        "regression",
        "experience",
        "Software",
        "Development",
        "Life",
        "Cycle",
        "SDLC",
        "Requirements",
        "Analysis",
        "Design",
        "Specification",
        "Testing",
        "Cycle",
        "Waterfall",
        "methodologies",
        "dplyr",
        "pandas",
        "R",
        "data",
        "analysis",
        "Experience",
        "data",
        "modeling",
        "tools",
        "Erwin",
        "Power",
        "Designer",
        "ERStudio",
        "Experience",
        "data",
        "analytics",
        "data",
        "Adhoc",
        "Graphs",
        "Scales",
        "PivotTables",
        "OLAP",
        "visualization",
        "tools",
        "Tableau",
        "ggplot2",
        "d3js",
        "dashboards",
        "data",
        "database",
        "sources",
        "Oracle",
        "SQL",
        "Server",
        "DB2",
        "Teradata",
        "Proficient",
        "knowledge",
        "statistics",
        "mathematics",
        "machine",
        "recommendation",
        "algorithms",
        "analytics",
        "understanding",
        "business",
        "operations",
        "analytics",
        "tools",
        "analysis",
        "data",
        "US",
        "employer",
        "Work",
        "Experience",
        "Python",
        "Developer",
        "Infor",
        "Inc",
        "New",
        "York",
        "NY",
        "July",
        "Present",
        "Description",
        "Myntra",
        "fashion",
        "ecommerce",
        "marketplace",
        "company",
        "Bengaluru",
        "Karnataka",
        "India",
        "focus",
        "company",
        "retailing",
        "apparel",
        "goal",
        "project",
        "application",
        "tickets",
        "orders",
        "customers",
        "Responsibilities",
        "project",
        "requirements",
        "application",
        "AnacondaPython",
        "Environment",
        "Anaconda",
        "environment",
        "Wrote",
        "programs",
        "performance",
        "calculations",
        "NumPy",
        "SQLAlchemy",
        "python",
        "routines",
        "websites",
        "data",
        "options",
        "modules",
        "Requests",
        "web",
        "experience",
        "Text",
        "Analytics",
        "Machine",
        "Learning",
        "Data",
        "mining",
        "solutions",
        "business",
        "problems",
        "data",
        "visualizations",
        "R",
        "Python",
        "Tableau",
        "packages",
        "Beautiful",
        "Soup",
        "data",
        "development",
        "Web",
        "Services",
        "SOAP",
        "data",
        "interface",
        "XML",
        "format",
        "packages",
        "Beautiful",
        "Soup",
        "data",
        "development",
        "SQL",
        "procedures",
        "MYSQL",
        "code",
        "code",
        "redundancy",
        "level",
        "Design",
        "text",
        "classification",
        "application",
        "text",
        "classification",
        "models",
        "Jira",
        "tracking",
        "project",
        "management",
        "writing",
        "data",
        "CSV",
        "file",
        "formats",
        "Sprintplanning",
        "sessions",
        "AgileSCRUM",
        "meetings",
        "day",
        "part",
        "SCRUM",
        "Master",
        "role",
        "project",
        "Linux",
        "environment",
        "reports",
        "application",
        "Performed",
        "QA",
        "testing",
        "application",
        "meetings",
        "client",
        "project",
        "help",
        "client",
        "Environment",
        "Python",
        "Anaconda",
        "Spyder",
        "IDE",
        "Windows",
        "Teradata",
        "Requests",
        "Beautiful",
        "Soup",
        "Tableau",
        "NumPy",
        "SQLAlchemy",
        "MySQL",
        "Data",
        "ScientistMachine",
        "Learning",
        "Engineer",
        "AT",
        "T",
        "Dallas",
        "TX",
        "February",
        "July",
        "Description",
        "ATT",
        "Inc",
        "telecommunications",
        "entertainment",
        "services",
        "company",
        "segments",
        "Business",
        "Solutions",
        "Entertainment",
        "Group",
        "Consumer",
        "Mobility",
        "International",
        "Responsibilities",
        "Pandas",
        "NumPy",
        "Seaborn",
        "SciPy",
        "Matplotlib",
        "Scikitlearn",
        "NLTK",
        "Python",
        "machine",
        "algorithms",
        "data",
        "formats",
        "JSON",
        "XML",
        "machine",
        "algorithms",
        "Python",
        "Setup",
        "storage",
        "data",
        "analysis",
        "tools",
        "Amazon",
        "Web",
        "Services",
        "cloud",
        "infrastructure",
        "systems",
        "Data",
        "Analytics",
        "Data",
        "Automation",
        "custom",
        "visualization",
        "tools",
        "R",
        "Mahout",
        "Hadoop",
        "Spark",
        "Scala",
        "Hadoop",
        "HBase",
        "Cassandra",
        "MongoDB",
        "Kafka",
        "Spark",
        "Streaming",
        "Python",
        "variety",
        "machine",
        "learning",
        "methods",
        "classifications",
        "dimensionality",
        "reduction",
        "engine",
        "user",
        "lifetime",
        "user",
        "conversations",
        "target",
        "categories",
        "Spark",
        "Data",
        "SparkSQL",
        "Spark",
        "designing",
        "POCs",
        "Scala",
        "Spark",
        "SQL",
        "MLlib",
        "libraries",
        "Data",
        "Quality",
        "Validation",
        "techniques",
        "Critical",
        "Data",
        "Elements",
        "CDE",
        "anomalies",
        "phases",
        "DataMining",
        "Datacollection",
        "DataCleaning",
        "DevelopingModels",
        "Validation",
        "Visualization",
        "Performed",
        "Gap",
        "Analysis",
        "Data",
        "Manipulation",
        "Aggregation",
        "source",
        "Nexus",
        "Toad",
        "Business",
        "Objects",
        "SmartView",
        "Agile",
        "Methodology",
        "application",
        "Focus",
        "integration",
        "overlap",
        "Informatica",
        "commitment",
        "MDM",
        "acquisition",
        "Identity",
        "Systems",
        "knowledge",
        "Hadoop",
        "Architecture",
        "components",
        "HDFS",
        "Job",
        "Tracker",
        "Task",
        "Tracker",
        "Name",
        "node",
        "Data",
        "node",
        "Secondary",
        "Name",
        "node",
        "MapReduce",
        "concepts",
        "utility",
        "Python",
        "packages",
        "SciPy",
        "NumPy",
        "Pandas",
        "Classification",
        "algorithms",
        "Logistic",
        "Regression",
        "Decision",
        "KNN",
        "Naive",
        "Bayes",
        "Python",
        "scripts",
        "training",
        "data",
        "database",
        "AWS",
        "Cloud",
        "Search",
        "document",
        "response",
        "label",
        "classification",
        "SQL",
        "tables",
        "integrity",
        "queries",
        "SQL",
        "SQL",
        "PLUS",
        "PLSQL",
        "Interaction",
        "Business",
        "Analyst",
        "SMEs",
        "Data",
        "Architects",
        "Business",
        "needs",
        "functionality",
        "project",
        "solutions",
        "process",
        "improvements",
        "handson",
        "technologies",
        "Oracle",
        "Informatica",
        "BusinessObjects",
        "Environment",
        "AWS",
        "Informatica",
        "Python",
        "HDFS",
        "ODS",
        "OLTP",
        "Oracle",
        "g",
        "Hive",
        "OLAP",
        "DB2",
        "Metadata",
        "MS",
        "Excel",
        "Mainframes",
        "MS",
        "Vision",
        "MapReduce",
        "Rational",
        "Rose",
        "SQL",
        "MongoDB",
        "Sr",
        "Python",
        "developer",
        "UnitedHealth",
        "Group",
        "Minneapolis",
        "MN",
        "November",
        "February",
        "Description",
        "Cisco",
        "technology",
        "manufactures",
        "networking",
        "hardware",
        "telecommunications",
        "equipment",
        "hightechnology",
        "services",
        "products",
        "goal",
        "project",
        "IT",
        "security",
        "mobility",
        "problems",
        "customers",
        "solutions",
        "website",
        "issues",
        "solutions",
        "problem",
        "Responsibilities",
        "requirements",
        "flow",
        "task",
        "flow",
        "charts",
        "flow",
        "pages",
        "UI",
        "design",
        "solutions",
        "stories",
        "client",
        "requirements",
        "Written",
        "REST",
        "services",
        "python",
        "Apollo",
        "Cisco",
        "Python",
        "Scripts",
        "workflows",
        "teams",
        "data",
        "unit",
        "integration",
        "tests",
        "python",
        "code",
        "LDAP",
        "authentication",
        "Customers",
        "python",
        "Rest",
        "Services",
        "client",
        "certificates",
        "pem",
        "pfx",
        "formats",
        "python",
        "module",
        "database",
        "client",
        "requests",
        "Written",
        "LDAP",
        "search",
        "filters",
        "level",
        "Complete",
        "UI",
        "development",
        "CSS",
        "HTML5",
        "Dashboards",
        "filters",
        "parameters",
        "sets",
        "views",
        "user",
        "validations",
        "client",
        "side",
        "server",
        "side",
        "code",
        "reuse",
        "performance",
        "use",
        "design",
        "patterns",
        "Efficient",
        "code",
        "principles",
        "Test",
        "Driven",
        "Development",
        "TDD",
        "integration",
        "line",
        "Agile",
        "Software",
        "Methodology",
        "principles",
        "QA",
        "test",
        "plans",
        "highlevel",
        "design",
        "documentation",
        "Rally",
        "software",
        "management",
        "Primary",
        "contact",
        "issues",
        "development",
        "production",
        "environments",
        "Longterm",
        "fix",
        "incidents",
        "production",
        "environment",
        "root",
        "cause",
        "cisco",
        "belt",
        "security",
        "applications",
        "manner",
        "threats",
        "Environment",
        "Python",
        "Cassandra",
        "MySQL",
        "LDAP",
        "Git",
        "Linux",
        "Windows",
        "JSON",
        "JQuery",
        "HTML",
        "XML",
        "CSS",
        "REST",
        "Rally",
        "Bootstrap",
        "JavaScript",
        "Angular",
        "JS",
        "Agile",
        "Bitbucket",
        "Py",
        "Unit",
        "PyCharm",
        "Microsoft",
        "SQL",
        "server",
        "management",
        "studio",
        "DataStax",
        "DevCenter",
        "Apache",
        "Directory",
        "Studio",
        "Ansible",
        "Jenkins",
        "Matplotlib",
        "MOCK",
        "Beautiful",
        "Soup",
        "PyTest",
        "Machine",
        "Learning",
        "EngineerData",
        "Scientist",
        "Regions",
        "Bank",
        "Birmingham",
        "AL",
        "August",
        "November",
        "Description",
        "Regions",
        "Financial",
        "Corporation",
        "bank",
        "services",
        "company",
        "Regions",
        "Center",
        "Birmingham",
        "Alabama",
        "company",
        "banking",
        "trust",
        "securities",
        "brokerage",
        "mortgage",
        "insurance",
        "products",
        "services",
        "Responsibilities",
        "R",
        "packages",
        "dplyr",
        "Causal",
        "Infer",
        "SpaceTime",
        "Coded",
        "R",
        "functions",
        "Caffe",
        "Deep",
        "Learning",
        "Framework",
        "Pandas",
        "NumPy",
        "Seaborn",
        "SciPy",
        "Matplotlib",
        "Scikitlearn",
        "NLTK",
        "Python",
        "machine",
        "learning",
        "algorithms",
        "Caffe",
        "Deep",
        "Learning",
        "Framework",
        "data",
        "formats",
        "JSON",
        "XML",
        "machine",
        "algorithms",
        "Python",
        "Setup",
        "storage",
        "data",
        "analysis",
        "tools",
        "Amazon",
        "Web",
        "Services",
        "cloud",
        "infrastructure",
        "systems",
        "Data",
        "Analytics",
        "Data",
        "Automation",
        "custom",
        "visualization",
        "tools",
        "R",
        "Mahout",
        "Hadoop",
        "MongoDB",
        "Data",
        "Architects",
        "IT",
        "Architects",
        "movement",
        "data",
        "storage",
        "ERStudio97",
        "Spark",
        "Scala",
        "Hadoop",
        "HBase",
        "Cassandra",
        "MongoDB",
        "Kafka",
        "Spark",
        "Streaming",
        "Python",
        "variety",
        "machine",
        "learning",
        "methods",
        "classifications",
        "regressions",
        "engine",
        "user",
        "lifetime",
        "user",
        "conversations",
        "target",
        "categories",
        "Spark",
        "Data",
        "SparkSQL",
        "Spark",
        "designing",
        "POCs",
        "Scala",
        "Spark",
        "SQL",
        "MLlib",
        "libraries",
        "Data",
        "Quality",
        "Validation",
        "techniques",
        "Critical",
        "Data",
        "Elements",
        "CDE",
        "anomalies",
        "Data",
        "Modeling",
        "tools",
        "Erwin",
        "Data",
        "Modeler",
        "Data",
        "Models",
        "QlikView",
        "Data",
        "Models",
        "data",
        "sources",
        "DB2",
        "Excel",
        "Flat",
        "Files",
        "Bigdata",
        "phases",
        "DataMining",
        "Datacollection",
        "DataCleaning",
        "DevelopingModels",
        "Validation",
        "Visualization",
        "Performed",
        "Gap",
        "Analysis",
        "Data",
        "Manipulation",
        "Aggregation",
        "source",
        "Nexus",
        "Toad",
        "BusinessObjects",
        "Power",
        "BI",
        "SmartView",
        "Agile",
        "Methodology",
        "application",
        "Focus",
        "integration",
        "overlap",
        "Informatica",
        "commitment",
        "MDM",
        "acquisition",
        "Identity",
        "Systems",
        "knowledge",
        "Hadoop",
        "Architecture",
        "components",
        "HDFS",
        "Job",
        "Tracker",
        "Task",
        "Tracker",
        "Name",
        "Node",
        "Data",
        "Node",
        "Secondary",
        "Name",
        "Node",
        "MapReduce",
        "concepts",
        "Architect",
        "OLAP",
        "DatabasesCubes",
        "Scorecards",
        "Dashboards",
        "Reports",
        "utility",
        "Python",
        "packages",
        "SciPy",
        "NumPy",
        "Pandas",
        "Classification",
        "algorithms",
        "Logistic",
        "Regression",
        "Decision",
        "Trees",
        "KNN",
        "Naive",
        "Bayes",
        "data",
        "models",
        "ODS",
        "OLTP",
        "systems",
        "Dimensional",
        "Data",
        "Models",
        "Star",
        "Snowflake",
        "Schemas",
        "Updated",
        "Python",
        "scripts",
        "training",
        "data",
        "database",
        "AWS",
        "Cloud",
        "Search",
        "document",
        "response",
        "label",
        "classification",
        "SQL",
        "tables",
        "integrity",
        "queries",
        "SQL",
        "SQLPLUS",
        "PLSQL",
        "Use",
        "Case",
        "Activity",
        "Diagrams",
        "Sequence",
        "Diagrams",
        "OOD",
        "Object",
        "Design",
        "UML",
        "Visio",
        "Interaction",
        "Business",
        "Analyst",
        "SMEs",
        "Data",
        "Architects",
        "Business",
        "needs",
        "functionality",
        "project",
        "solutions",
        "process",
        "improvements",
        "handson",
        "technologies",
        "Oracle",
        "Informatica",
        "BusinessObjects",
        "Environment",
        "AWS",
        "R",
        "Informatica",
        "Python",
        "HDFS",
        "ODS",
        "OLTP",
        "Oracle",
        "g",
        "Hive",
        "OLAP",
        "DB2",
        "Metadata",
        "MS",
        "Excel",
        "Mainframes",
        "MS",
        "Vision",
        "MapReduce",
        "Rational",
        "Rose",
        "SQL",
        "MongoDB",
        "Data",
        "Scientist",
        "Cisco",
        "San",
        "Jose",
        "CA",
        "May",
        "August",
        "Description",
        "UnitedHealth",
        "Group",
        "Inc",
        "forprofit",
        "health",
        "care",
        "company",
        "Minnetonka",
        "Minnesota",
        "United",
        "States",
        "Fortune",
        "500UnitedHealth",
        "Group",
        "health",
        "care",
        "products",
        "insurance",
        "services",
        "UnitedHealth",
        "Group",
        "healthcare",
        "company",
        "world",
        "revenue",
        "UnitedHealth",
        "Group",
        "companies",
        "individuals",
        "Responsibilities",
        "leadership",
        "business",
        "technology",
        "projects",
        "emphasis",
        "application",
        "architecture",
        "domain",
        "knowledge",
        "application",
        "portfolio",
        "knowledge",
        "role",
        "state",
        "business",
        "technology",
        "programs",
        "phases",
        "data",
        "mining",
        "data",
        "collection",
        "data",
        "models",
        "validation",
        "visualization",
        "analysis",
        "MapReduceSpark",
        "Python",
        "modules",
        "machine",
        "analytics",
        "Hadoop",
        "AWS",
        "Pythonbased",
        "forest",
        "Python",
        "ecosystem",
        "models",
        "canonical",
        "services",
        "enterprise",
        "data",
        "data",
        "model",
        "subject",
        "areas",
        "ecosystem",
        "model",
        "business",
        "meaning",
        "entities",
        "fields",
        "model",
        "messages",
        "formats",
        "data",
        "integration",
        "services",
        "ecosystem",
        "PandasNumPy",
        "SciPy",
        "Matplotlib",
        "Scikitlearn",
        "NLTK",
        "Python",
        "machine",
        "algorithms",
        "machine",
        "learning",
        "algorithms",
        "linear",
        "regression",
        "multivariate",
        "regression",
        "Bayes",
        "Random",
        "Forests",
        "Kmeans",
        "KNN",
        "data",
        "analysis",
        "studies",
        "plots",
        "advance",
        "data",
        "mining",
        "modelling",
        "techniques",
        "solution",
        "quality",
        "performance",
        "data",
        "experience",
        "design",
        "implementation",
        "models",
        "models",
        "enterprise",
        "data",
        "model",
        "metadata",
        "solution",
        "data",
        "life",
        "cycle",
        "management",
        "Big",
        "Data",
        "environments",
        "data",
        "sets",
        "machine",
        "techniques",
        "models",
        "models",
        "models",
        "bestinclass",
        "modeling",
        "techniques",
        "database",
        "design",
        "integrity",
        "constraints",
        "OLTP",
        "Cubes",
        "Normalization",
        "Denormalization",
        "database",
        "MapReduceSpark",
        "Python",
        "modules",
        "machine",
        "analytics",
        "Hadoop",
        "AWS",
        "customer",
        "segmentation",
        "learning",
        "technique",
        "clustering",
        "Teradata15",
        "tools",
        "utilities",
        "TeradataViewpoint",
        "Multi",
        "Load",
        "ARC",
        "Teradata",
        "Administrator",
        "BTEQ",
        "Teradata",
        "Utilities",
        "Spark",
        "Scala",
        "Hadoop",
        "HBase",
        "Kafka",
        "Spark",
        "Streaming",
        "MLlib",
        "Python",
        "variety",
        "machine",
        "learning",
        "methods",
        "classifications",
        "regressions",
        "reduction",
        "LINUX",
        "Shell",
        "scripts",
        "NZSQLNZLOAD",
        "utilities",
        "data",
        "files",
        "Netezza",
        "database",
        "system",
        "architecture",
        "Amazon",
        "EC2",
        "solution",
        "client",
        "Complex",
        "ETL",
        "Mappings",
        "Sessions",
        "business",
        "user",
        "requirements",
        "business",
        "rules",
        "data",
        "source",
        "files",
        "RDBMS",
        "tables",
        "tables",
        "Environment",
        "Erwin",
        "Python",
        "SQL",
        "Oracle",
        "12c",
        "Netezza",
        "SQL",
        "Server",
        "Informatica",
        "Java",
        "SSRS",
        "PLSQL",
        "TSQL",
        "Tableau",
        "MLlib",
        "regression",
        "Cluster",
        "analysis",
        "Scala",
        "NLP",
        "Spark",
        "Kafka",
        "MongoDB",
        "regression",
        "Hadoop",
        "Hive",
        "Teradata",
        "forest",
        "OLAP",
        "Azure",
        "SAP",
        "CRM",
        "HDFS",
        "ODS",
        "NLTK",
        "SVM",
        "JSON",
        "Tableau",
        "XML",
        "Cassandra",
        "MapReduce",
        "AWS",
        "Education",
        "Bachelors",
        "Skills",
        "Analysis",
        "services",
        "Application",
        "development",
        "Hadoop",
        "Hbase",
        "Hdfs",
        "Javascript",
        "MySQL",
        "Android",
        "ASP",
        "PHP"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T19:41:04.061586",
    "resume_data": "Python Developer span lPythonspan span lDeveloperspan Python Developer Infor Inc New York NY Over 10 years of IT industry experience encompassing in Machine Learning Datamining with large datasets of Structured and Unstructured data Data Acquisition Data Validation Predictive modeling DataVisualization Extensive experience in Text Analytics developing different statistical machine learning Datamining solutions to various business problems and generating data visualizations using R Python and Tableau Experience objectoriented programming OOP concepts using Python C and PHP Experience on advanced SAS programming techniques such as PROC SQL JOIN UNION PROC APPEND PROC DATASETS and PROC TRANSPOSE Integration Architect Data Scientist experience in Analytics Bigdata BPM SOA ETL and Cloud technologies Highly skilled in using visualization tools like Tableau ggplot2 and d3js for creating dashboards Tagging of experience in foundational machine learning models and concepts regression random forest boosting GBM NNs HMMs CRFs MRFs deep learning Proficiency in understanding statistical and other toolslanguages R Python C C Java SQL UNIX QlikView data visualization tool and Anaplan forecasting tool Proficient in the Integration of various data sources with multiple relational databases like Oracle MS SQL Server DB2 Teradata and Flat Files into the staging area ODS Data Warehouse and DataMart Experience in Extracting data for creating Value Added Datasets using Python R SAS Azure and SQL to analyze the behavior to target a specific set of customers to obtain hidden insights within the data to effectively implement the project Objectives Worked with NoSQL Database including HBase Cassandra and MongoDB Extensively worked on statistical analysis tools and adept at writing code in Advanced Excel R MATLAB Python Implemented deep learning models and numerical Computation with the help of data flow graphs using Tensor Flow Machine Learning Worked with complex applications such as R Stata Scala Perl Linear SAS and SPSS to develop a neural network cluster analysis Experienced the full software lifecycle in SDLC Agile and Scrum methodologies Experience in designing stunning visualizations using Tableau software and publishing and presenting dashboards Storyline on web and desktop platforms Hands on experience in implementing LDA Naive Bayes and skilled in RandomForests DecisionTrees Linear and LogisticRegression SVM Clustering neural networks Principle Component Analysis and good knowledge of Recommender Systems Experienced with machine learning algorithms such as logistic regression random forest XP boost KNN SVM neural network linear regression lasso regression and kmeans Strong experience in Software Development Life Cycle SDLC including Requirements Analysis Design Specification and Testing as per Cycle in both Waterfall and Agile methodologies Skilled in using dplyr and pandas in R and python for performing Exploratory data analysis Experience working with data modeling tools like Erwin Power Designer and ERStudio Experience with data analytics data reporting Adhoc reporting Graphs Scales PivotTables and OLAP reporting Highly skilled in using visualization tools like Tableau ggplot2 and d3js for creating dashboards Worked and extracted data from various database sources like Oracle SQL Server DB2 and Teradata Proficient knowledge of statistics mathematics machine learning recommendation algorithms and analytics with an excellent understanding of business operations and analytics tools for effective analysis of data Authorized to work in the US for any employer Work Experience Python Developer Infor Inc New York NY July 2017 to Present Description Myntra is an Indian fashion ecommerce marketplace company headquartered in Bengaluru Karnataka India The focus of the company is the online retailing of branded apparel The main goal of the project is to develop an application to generate the tickets for the orders of the customers Responsibilities Worked on the project from gathering requirements to developing the entire application Worked on AnacondaPython Environment Created activated and programmed in Anaconda environment Wrote programs for performance calculations using NumPy and SQLAlchemy Wrote python routines to log into the websites and fetch data for selected options Used python modules of Urllib urllib2 Requests for web crawling Extensive experience in Text Analytics developing different Statistical Machine Learning Data mining solutions to various business problems and generating data visualizations using R Python and Tableau Used with other packages such as Beautiful Soup for data parsing Involved in development of Web Services using SOAP for sending and getting data from the external interface in the XML format Used with other packages such as Beautiful Soup for data parsing Worked on development of SQL and stored procedures on MYSQL Analyzed the code completely and have reduced the code redundancy to the optimal level Design and build a text classification application using different text classification models Used Jira for defect tracking and project management Worked on writing and as well as read data from CSV and excel file formats Involved in Sprintplanning sessions and participated in the daily AgileSCRUM meetings Conducted every day scrum as part of the SCRUM Master role Developed the project in Linux environment Worked on resulting reports of the application Performed QA testing on the application Held meetings with client and worked for the entire project with limited help from the client Environment Python Anaconda Spyder IDE Windows 7 Teradata Requests urllib urllib2 Beautiful Soup Tableau python libraries such as NumPy SQLAlchemy MySQL Data ScientistMachine Learning Engineer AT T Dallas TX February 2015 to July 2017 Description ATT Inc provides telecommunications and digital entertainment services The company operates through four segments Business Solutions Entertainment Group Consumer Mobility and International Responsibilities Used Pandas NumPy Seaborn SciPy Matplotlib Scikitlearn and NLTK in Python for developing various machine learning algorithms Worked on different data formats such as JSON XML and performed machine learning algorithms in Python Setup storage and data analysis tools in Amazon Web Services cloud computing infrastructure Implemented endtoend systems for Data Analytics Data Automation and integrated with custom visualization tools using R Mahout Hadoop and MongoDB Utilized Spark Scala Hadoop HBase Cassandra MongoDB Kafka Spark Streaming MLLib Python a broad variety of machine learning methods including classifications dimensionality reduction etc and utilized the engine to increase user lifetime by 45 and triple user conversations for target categories Used Spark Data frames SparkSQL Spark MLLib extensively and developing and designing POCs using Scala Spark SQL and MLlib libraries Used Data Quality Validation techniques to validate Critical Data Elements CDE and identified various anomalies Participated in all phases of DataMining Datacollection DataCleaning DevelopingModels Validation Visualization and Performed Gap Analysis Data Manipulation and Aggregation from different source using Nexus Toad Business Objects and SmartView Implemented Agile Methodology for building an internal application Focus on integration overlap and Informatica newer commitment to MDM with the acquisition of Identity Systems Good knowledge of Hadoop Architecture and various components such as HDFS Job Tracker Task Tracker Name node Data node Secondary Name node and MapReduce concepts Programmed a utility in Python that used multiple packages SciPy NumPy Pandas Implemented Classification using supervised algorithms like Logistic Regression Decision trees KNN Naive Bayes Updated Python scripts to match training data with our database stored in AWS Cloud Search so that we would be able to assign each document a response label for further classification Created SQL tables with referential integrity and developed queries using SQL SQL PLUS and PLSQL Interaction with Business Analyst SMEs and other Data Architects to understand Business needs and functionality for various project solutions Identifying and executing process improvements handson in various technologies such as Oracle Informatica and BusinessObjects Environment AWS Informatica Python HDFS ODS OLTP Oracle 10g Hive OLAP DB2 Metadata MS Excel Mainframes MS Vision MapReduce Rational Rose SQL and MongoDB Sr Python developer UnitedHealth Group Minneapolis MN November 2013 to February 2015 Description Cisco is an American multinational technology that develops manufactures and sells networking hardware telecommunications equipment and other hightechnology services and products The main goal of the project is to report IT security mobility problems faced by the customers and suggest solutions to them We developed a website for reporting issues and finding solutions to that problem easily Responsibilities Analyzed the requirements and designed the flow of task using flow charts and designed flow between pages of the UI Documented the design solutions and created stories for client requirements Written REST services using python and Apollo internal to Cisco Written Python Scripts to establish continuous workflows from different teams providing data Written unit and integration tests in python to test the code Implemented LDAP authentication to authenticate and authorize the Customers using python Rest Services Generated client certificates in both pem and pfx formats using M2Crypto python module Used SQLite3 database for caching the client requests Written LDAP search filters for both single level and multilevel Complete UI development using AngularJS CSS and HTML5 Dashboards with quick filters parameters and sets to handle views more efficiently Performed user validations on client side as well as server side Improved code reuse and performance by making effective use of various design patterns Efficient delivered code based on principles of Test Driven Development TDD and continuous integration to keep in line with Agile Software Methodology principles Participated with QA to develop test plans from highlevel design documentation Used Rally for Agile software management Primary contact for all issues in both development and production environments Implemented the Longterm fix for incidents that are happened in production environment by finding the root cause Completed the cisco white belt in security which helps to develop the applications in a secured manner to protect from the threats Environment Python 27 Cassandra MySQL LDAP Git Linux Windows JSON JQuery HTML XML CSS REST Rally Bootstrap JavaScript Angular JS Agile Bitbucket Py Unit PyCharm Microsoft SQL server management studio DataStax DevCenter Apache Directory Studio Ansible Jenkins Matplotlib MOCK Beautiful Soup PyTest Machine Learning EngineerData Scientist Regions Bank Birmingham AL August 2011 to November 2013 Description Regions Financial Corporation is a bank and financial services company headquartered in the Regions Center in Birmingham Alabama The company provides retail and commercial banking trust securities brokerage mortgage and insurance products and services Responsibilities Worked with several R packages including knit dplyr SparkR Causal Infer SpaceTime Coded R functions to interface with Caffe Deep Learning Framework Used Pandas NumPy Seaborn SciPy Matplotlib Scikitlearn and NLTK in Python for developing various machine learning algorithms Installed and used Caffe Deep Learning Framework Worked on different data formats such as JSON XML and performed machine learning algorithms in Python Setup storage and data analysis tools in Amazon Web Services cloud computing infrastructure Implemented endtoend systems for Data Analytics Data Automation and integrated with custom visualization tools using R Mahout Hadoop and MongoDB Worked as Data Architects and IT Architects to understand the movement of data and its storage and ERStudio97 Utilized Spark Scala Hadoop HBase Cassandra MongoDB Kafka Spark Streaming MLLib Python a broad variety of machine learning methods including classifications regressions dimensionally reduction etc and Utilized the engine to increase user lifetime by 45 and triple user conversations for target categories Used Spark Data frames SparkSQL Spark MLLib extensively and developing and designing POCs using Scala Spark SQL and MLlib libraries Used Data Quality Validation techniques to validate Critical Data Elements CDE and identified various anomalies Extensively worked on Data Modeling tools Erwin Data Modeler to design the Data Models Developed various QlikView Data Models by extracting and using the data from various sources files DB2 Excel Flat Files and Bigdata Participated in all phases of DataMining Datacollection DataCleaning DevelopingModels Validation Visualization and Performed Gap Analysis Data Manipulation and Aggregation from a different source using Nexus Toad BusinessObjects Power BI and SmartView Implemented Agile Methodology for building an internal application Focus on integration overlap and Informatica newer commitment to MDM with the acquisition of Identity Systems Good knowledge of Hadoop Architecture and various components such as HDFS Job Tracker Task Tracker Name Node Data Node Secondary Name Node and MapReduce concepts As Architect delivered various complex OLAP DatabasesCubes Scorecards Dashboards and Reports Programmed a utility in Python that used multiple packages SciPy NumPy Pandas Implemented Classification using supervised algorithms like Logistic Regression Decision Trees KNN Naive Bayes Designed both 3NF data models for ODS OLTP systems and Dimensional Data Models using Star and Snowflake Schemas Updated Python scripts to match training data with our database stored in AWS Cloud Search so that we would be able to assign each document a response label for further classification Created SQL tables with referential integrity and developed queries using SQL SQLPLUS and PLSQL Designed and developed Use Case Activity Diagrams Sequence Diagrams OOD Object oriented Design using UML and Visio Interaction with Business Analyst SMEs and other Data Architects to understand Business needs and functionality for various project solutions Identifying and executing process improvements handson in various technologies such as Oracle Informatica and BusinessObjects Environment AWS R Informatica Python HDFS ODS OLTP Oracle 10g Hive OLAP DB2 Metadata MS Excel Mainframes MS Vision MapReduce Rational Rose SQL and MongoDB Data Scientist Cisco San Jose CA May 2009 to August 2011 Description UnitedHealth Group Inc is an American forprofit managed health care company based in Minnetonka Minnesota It is sixth in the United States on the Fortune 500UnitedHealth Group offers health care products and insurance services UnitedHealth Group is the largest healthcare company in the world by revenue 184 billion in 2016 UnitedHealth Group subsidiaries companies together serve approximately 115 million individuals in 2016 Responsibilities Provided the architectural leadership in shaping strategic business technology projects with an emphasis on application architecture Utilized domain knowledge and application portfolio knowledge to play a key role in defining the future state of large business technology programs Participated in all phases of data mining data collection data cleaning developing models validation and visualization and performed Gap analysis Developed MapReduceSpark Python modules for machine learning predictive analytics in Hadoop on AWS Implemented a Pythonbased distributed random forest via Python streaming Created ecosystem models eg conceptual logical physical canonical that are required for supporting services within the enterprise data architecture conceptual data model for defining the major subject areas used ecosystem logical model for defining standard business meaning for entities and fields and an ecosystem canonical model for defining the standard messages and formats to be used in data integration services throughout the ecosystem Used PandasNumPy seaborn SciPy Matplotlib Scikitlearn NLTK in Python for developing various machine learning algorithms and utilized machine learning algorithms such as linear regression multivariate regression naive Bayes Random Forests Kmeans KNN for data analysis Conducted studies rapid plots and using advance data mining and statistical modelling techniques to build a solution that optimize the quality and performance of data Demonstrated experience in design and implementation of Statistical models Predictive models enterprise data model metadata solution and data life cycle management in both RDBMS Big Data environments Analyzed large data sets apply machine learning techniques and develop predictive models statistical models and developing and enhancing statistical models by leveraging bestinclass modeling techniques Worked on database design relational integrity constraints OLAP OLTP Cubes and Normalization 3NF and Denormalization of the database Developed MapReduceSpark Python modules for machine learning predictive analytics in Hadoop on AWS Worked on customer segmentation using an unsupervised learning technique clustering Worked with various Teradata15 tools and utilities like TeradataViewpoint Multi Load ARC Teradata Administrator BTEQ and other Teradata Utilities Utilized Spark Scala Hadoop HBase Kafka Spark Streaming MLlib Python a broad variety of machine learning methods including classifications regressions dimensionally reduction etc Developed LINUX Shell scripts by using NZSQLNZLOAD utilities to load data from flat files to Netezza database Designed and implemented system architecture for Amazon EC2 based cloudhosted solution for the client Tested Complex ETL Mappings and Sessions based on business user requirements and business rules to load data from source flat files and RDBMS tables to target tables Environment Erwin r96 Python SQL Oracle 12c Netezza SQL Server Informatica Java SSRS PLSQL TSQL Tableau MLlib regression Cluster analysis Scala NLP Spark Kafka MongoDB logistic regression Hadoop Hive Teradata random forest OLAP Azure MariaDB SAP CRM HDFS ODS NLTK SVM JSON Tableau XML Cassandra MapReduce AWS Education Bachelors Skills Analysis services Application development Hadoop Hbase Hdfs Javascript MySQL Android ASP PHP",
    "unique_id": "edfa9791-a805-4487-8954-08534d8e5737"
}