{
    "clean_data": "Sr Big DataData Engineer Sr Big DataData Engineer Sr Big DataData Engineer AT T Jersey City NJ Having 10 years of overall IT experience in a variety of industries which includes hands on experience of 7 years in Big Data AnalyticsData Engineer and development Expertise with the tools in Hadoop Ecosystem including Pig Hive HDFS MapReduce Sqoop Storm Spark Kafka Yarn Oozie and Zookeeper Excellent knowledge on Hadoop ecosystems such as HDFS Job Tracker Task Tracker Name Node Data Node and Map Reduce programming paradigm Experience in manipulatinganalysing large datasets and finding patterns and insights within structured and unstructured data Strong experience on Hadoop distributions like Cloudera MapR and Horton works Good understanding of NoSQL databases and hands on work experience in writing applications on NoSQL databases like HBase Cassandra and MongoDB Experience in search engine Technologies SOLR Elastic Search etc Worked with various HDFS file formats like Avro Sequence File and various compression formats like Snappy bzip2 Developed Simple to complex MapReduce streaming jobs using Python language that are implemented using Hive and Pig Skilled in developing applications in Python language for multiple platforms Hands on experience in application development using Java Linux Shell Scripting Experience in Oozie and workflow scheduler to manage Hadoop jobs by Direct Acyclic Graph DAG of actions with control flows Experience in migrating the data using Sqoop from HDFS to Relational Database System and viceversa according to clients requirement Extensive Experience on importing and exporting data using stream processing platforms like Flume and Kafka Strong Knowledge on Apache Spark with Scala Environment Developed Spark scripts by using Scala shell commands as per the requirement Good hands on experience in creating the RDDs Data frames for the required input data and performed the data transformations using Spark Scala Good knowledge on real time data streaming solutions using Apache Spark Streaming Kafka and Flume Experience in designing and developing applications in Spark using Scala to compare the performance of Spark with Hive and SQLOracle Very good experience in complete project life cycle design development testing and implementation of Client Server and Web applications Excellent Java development skills using J2EE J2SE Servlets JSP EJB JDBC SOAP and Restful web services Experience in MicroServices Architecture with Spring Boot and Docker Strong Experience of Data Warehousing ETL concepts using Informatica Power Centre OLAP OLTP and Autosys Experienced in working with Amazon Web Services AWS using EC2 for computing and S3 as storage mechanism Strong experience in ObjectOriented Design Analysis Development Testing and Maintenance Excellent implementation knowledge of EnterpriseWebClient Server using Java J2EE Experienced in using agile approaches including Extreme Programming TestDriven Development and Agile Scrum Worked in large and small teams for systems requirement design development Key participant in all phases of software development life cycle with Analysis Design Development Integration Implementation Debugging and Testing of Software Applications in client server environment Object Oriented Technology and Web based applications Experience in using various IDEs Eclipse IntelliJ and repositories SVN and Git Experience of using build tools Ant Maven Preparation of Standard Code guidelines analysis and testing documentations Good interpersonal skills committed result oriented hard working with a quest and deal to learn new technologies Work Experience Sr Big DataData Engineer AT T Middletown NJ April 2018 to Present ATT Inc is an American multinational conglomerate holding company headquartered at Whit acre Tower in Downtown Dallas Texas It is the worlds largest telecommunications company the second largest provider of mobile telephone services and the largest provider of fixed telephone services in the United States through ATT Communications Responsibilities Configured Spark Streaming to receive real time data from the Kafka and store the stream data to Cassandra utilizing Scala Developed Spark code to read data from Hdfs and write to Cassandra Configured Spark Streaming to receive real time data from the Kafka and store the stream data to Cassandra utilizing Scala Performed realtime analysis of the incoming data using Kafka consumer API Kafka topics Spark Streaming utilizing Scala Integrated Apache Storm with Kafka to perform web analytics Uploaded click stream data from Kafka to HDFS HBase and Hive by integrating with Storm Performed realtime analysis of the incoming data using Kafka consumer API Kafka topics Spark Streaming utilizing Scala Developed Kafka producer and consumers Cassandra clients and Spark along with components on HDFS Used the Spark Cassandra Connector to load data to and from Cassandra Experienced in designing and deployment of Hadoop cluster and various Big Data components including HDFS Map Reduce and Zookeeper in Cloudera distribution Implemented advanced procedures like text analytics and processing using the inmemory computing capabilities like Apache Spark written in Scala Implemented Kafka Custom partitions to send data to different categorized topics Implemented messaging system for different data sources using apache Kafka and configuring High level consumers for online and offline processing Used Pig as ETL tool to do transformations event joins filter and some preaggregations Migrated ETL jobs to Pig scripts do Transformations even joins and some preaggregations before storing the data onto HDFS Loaded and transformed large sets of structured unstructured data Was responsible to handle a Team of 4 members at Offshore Involved in daily SCRUM meetings to discuss the developmentprocess Worked with data delivery teams to setup new Hadoop users This job included setting up Linux users setting up Kerberos principals and testing HDFS Hive Worked with data Ingestion Team have Good understanding of Apache Nifi and its transformations Done Scaling Cassandra cluster based on lead patterns Good understanding of Cassandra Data Modelling based on applications Create Solution Architecture based upon Microsoft Azure PaaS Services Design solution for various system components using Microsoft Azure Configure Setup Azure Hybrid Connection to pull data from SAP Systems Involved in code deployment to Production and providing support to App support team Good understanding of Hadoop admin work maintained Hadoop cluster using Ambari Environment Hadoop Spark Scala Java Map Reduce HDFS Cassandra Ambari Hive Pig Sqoop Flume Linux Python Kafka Storm Shell Scripting XML ETL Eclipse Cloudera DB2 SQL Server MySQL AWS HBase Sr Big Data Hadoop Developer Scripps Health San Diego CA February 2015 to March 2018 Scripps Health is a nonprofit health care system based in San Diego California The system includes five hospitals and 19 outpatient facilities and treats a halfmillion patients annually through 2600 affiliated physicians The system also includes clinical research and medical education programs The organization has a number of projects planned including the Scripps Prebys Cardiovascular Institute which will serve as a centre for heart disease treatment research and graduate medical education Responsibilities Responsible for building scalable distributed data solutions using Hadoop Developed Pyspark code to read data from Hive group the fields and generate XML files Enhanced the Pyspark code to write the generated XML files to a directory to zip them to CDAs Implemented REST call to submit the generated CDAs to vendor website Implemented Impala to support JDBCODBC connections for Hiveserver2 Configured Spark Streaming to receive real time data from the Kafka and store the stream data to HDFS Integrated Apache Storm with Kafka to perform web analytics Uploaded click stream data from Kafka to HDFS HBase and Hive by integrating with Storm Performed realtime analysis of the incoming data using Kafka consumer API Kafka topics Spark Streaming utilizing Scala Developed Kafka producer and consumers Cassandra clients and Spark along with components on HDFS Hive Good Understanding of DAG cycle for entire Spark application flow on Spark application WebUI Used the Spark Cassandra Connector to load data to and from Cassandra Written Storm topology to emit data into Cassandra DB Experience with developing and maintaining Applications written for Amazon Simple Storage AWS Elastic Beanstalk and AWS Cloud Formation Implemented Python script to call the Cassandra Rest API performed transformations and loaded the data into Hive Experienced in designing and deployment of Hadoop cluster and various Big Data components including HDFS MapReduce Hive Sqoop Pig Oozie and Zookeeper in Cloudera distribution Implemented advanced procedures like text analytics and processing using the inmemory computing capabilities like Apache Spark written in Scala Implemented Kafka Custom partitioning to send data to different categorized topics Developed end to end data processing pipelines that begin with receiving data using distributed messaging systems Kafka through persistence of data into HBase Implemented messaging system for different data sources using apache Kafka and configuring High level consumers for online and offline processing Written Shell scripts that run multiple Hive jobs which helps to automate different hive tables incrementally which are used to generate different reports using Tableau for the Business use Worked with data delivery teams to setup new Hadoop users This job included setting up Linux users setting up Kerberos principals and testing HDFS Hive Done Scaling Cassandra cluster based on lead patterns Worked with Kafka for the proof of concept for carrying out log processing on a distributed system Performed realtime analysis of the incoming data using Kafka consumer API Kafka topics Spark Streaming utilizing Scala Good understanding of Cassandra Data Modelling based on applications Created PIG script jobs in maintaining minimal query optimization Environment Hadoop Java MapReduce HDFS Hive Pig Sqoop Flume Linux Python Spark Impala Scala Kafka Storm Shell Scripting XML Eclipse Cloudera DB2 SQL Server MySQL Autosys Talend AWS HBase Hadoop Developer Merck Pharmaceuticals Ltd Kenilworth NJ October 2013 to January 2015 Merck is adopting Hadoop to overcome three data challenges to its goal of improving yields in its manufacturing process First it needed to combine years of data from multiple data silos within its organization Secondly Merck needs to extend both the amount of data it can capture and its ability to retain that data for longer Thirdly the Merck team wants to test new hypotheses virtually at a far lower cost than testing those ideas with realworld material and equipment With Hadoop Merck plans to overcame these challenges to combine 10 years of vaccine manufacturing data and conduct 55 million crossbatch comparisons over 10 billion records The resulting yield improvement could grow profits by 10 million dollars for just one vaccine Responsibilities Installed and configured Hadoop MapReduce HDFS developed multiple MapReduce jobs in Java for data cleaning and preprocessing Importing and exporting data into HDFS from Oracle 102 database and vice versa using SQOOP Experienced in defining and coordination of job flows Gained experience in reviewing and managing Hadoop log files Extracted files from NoSQL database MongoDB HBase through Sqoop and placed in HDFS for processing Involved in Writing Data Refinement Pig Scripts and Hive Queries Good knowledge in running Hadoop streaming jobs to process terabytes of xml format data Load and transform large sets of structured semi structured and unstructured data Coordinated cluster services using Zookeeper Used XML Technologies like DOM for transferring data Object relational mapping and Persistence mechanism is executed using Hibernate ORM Developed custom validator in Struts and implemented server side validations using annotations Used Oracle for the database and Web Logic as the application server Used Flume to transport logs to HDFS Experienced in moving data from Hive tables into Cassandra for real time analytics on hive tables Organize documents in more useable clusters using Mahout Configured connection between HDFS and Tableau using Impala for Tableau developer team Responsible to manage data coming from different sources Got good experience with various NoSQL databases Experienced with handling administration activations using Cloudera manager Supported MapReduce programs those are running on the cluster Involved in loading data from UNIX file system to HDFS Installed and configured Hive and also written Hive UDFs Involved in creating Hive tables loading with data and writing Hive queries which will run internally in MapReduce way Worked on Talend ETL tool developed and scheduled jobs in Talend integration suite Modified reports and Talend ETL jobs based on the feedback from QA testers and Users in development and staging environments Environment Apache Hadoop Java JDK16 J2EE JDBC Servlets JSP Linux XML Web Logic SOAP WSDL HBase Hive Pig Sqoop ZooKeeper NoSQL HBase R MAHOUT MapReduce Cloudera HDFS Flume Impala Tableau Talend MySQL HTML5 CSS MongoDB Java developer Dedicated Tech Services Inc Columbus OH November 2011 to September 2013 The Fuse Box Electronic Cash Register shall provide basic set of services to allow the retail merchant to perform with great ease any function of a standard electronic cash register on the market today This set of services shall provide a powerful web enabled cash management tool for any merchantss Responsibilities Developed JSP for UI and Java classes for business logic Used XSLT for UI to display XML Data Utilized JavaScript for client side validation Utilized Oracle PLSQL for database operations using JDBC API Implemented DAO for Oracle 8i for DML Operations like Insert Update Delete the records VSS is used for Software Configuration Management Involved in the design development and deployment of the Application using JavaJ2EE Technologies Used IDE tool WSAD for development of the application Developed Application in Jakarta Struts Framework using MVC architecture Customizing all the JSP pages with same look and feel using Tiles CSS Cascading Style Sheets Involved in coding for the presentation layer using Apache Struts XML and JavaScript Created Action Forms and Action classes for the modules Developed JSPs to validate the information automatically using Ajax Implemented J2EE design patterns viz Faade pattern Singleton Pattern Created strutsconfigxml and tilesdefxml files Developed Ant script to create warear file and deploy the application to application server Extensively involved in database activities like designing tables SQL queries in the application and maintained complex reusable methods which implements stored procedures to fetch data from the database Used CVS for version control Also involved in testing and deployment of the application on Web Logic Application Server during integration Environment JavaJ2EE JSP Servlets Struts 11 Spring JUnit Eclipse Apache Ant JSP JavaBeans JavaScript Tomcat 41 Oracle 9i XML XSLT HTMLDHTMLXHTML CSS Tiles Ajax DB2 UDB PLSQL XML SPY Java Developer General Dynamics Information Technology May 2009 to October 2011 Apollo Munich is an insurance company intended to offer medical and health insurance plans that caters to individuals families and corporate houses as well Played the role of a Java developer for customer registration plans and claims plans Responsibilities Involved in projects utilizing Java Java EE web applications to create fullyintegrated client management systems Developed UI using HTML Java Script JSP and developed business Logic and interfacing components using Business Objects JDBC and XML Participated in user requirement sessions to analysis and gather Business requirements Development of user visible site using Perl back end admin sites using Python and big data using core java Involved in development of the application using Spring Web MVC and other components of the Elaborated Use Cases based on business requirements and was responsible for creation of class Diagrams Sequence Diagrams Implemented Objectrelation mapping in the persistence layer using Hibernate ORM framework Implemented REST Web Services with Jersey API to deal with customer requests Experienced in developing Restful web services consumed and also produced Used Hibernate for the Database connection and Hibernate Query Language HQL to add and retrieve the information from the Database Implemented Spring JDBC for connecting Oracle database Designed the application using MVC framework for easy maintainability Provided bug fixing and testing for existing web applications Involved in full system life cycle and responsible for Developing Testing Implementing Involved in Unit Testing Integration Testing and System Testing Implemented Form Beans and their Validations Written Hibernate components Developed client side validations with Java script Environment Spring JSP Servlets REST Oracle AJAX Java Script JQuery Hibernate Web Logic Log4j HTML XML CVS Eclipse SOAP Web Services XSLT XSD UNIX Maven Jenkins shell scripting MVS ISPF Education Bachelors Skills Amazon web services Git Hbase Hdfs Hive Javascript Jenkins Json Pig Python Scripting Svn Zookeeper Cassandra Impala Oozie Sqoop Hbase Kafka Db2",
    "entities": [
        "Secondly Merck",
        "HDFS",
        "UNIX",
        "Avro Sequence File",
        "Cassandra DB",
        "Hadoop Developed Pyspark",
        "HDFS Experienced",
        "Migrated ETL",
        "Hadoop Ecosystem",
        "CVS",
        "MVS ISPF",
        "ObjectOriented Design Analysis Development Testing",
        "Amazon Web Services AWS",
        "Web Logic Application Server",
        "Hadoop",
        "Storm Performed",
        "XML",
        "Created PIG",
        "Extreme Programming TestDriven Development",
        "San Diego",
        "HBase",
        "Apache Spark",
        "Present ATT Inc",
        "Amazon",
        "Spark with",
        "Offshore Involved",
        "SQL Server",
        "Apollo Munich",
        "Scala Integrated Apache Storm",
        "Kerberos",
        "Object Oriented Technology",
        "Dallas",
        "Node Data",
        "Zookeeper Used XML Technologies",
        "Mahout Configured",
        "Singleton Pattern Created",
        "Oracle 8i",
        "Responsibilities Involved",
        "Cassandra Configured Spark",
        "Git",
        "Scripps Prebys Cardiovascular Institute",
        "Restful",
        "SQOOP Experienced",
        "Cassandra Written Storm",
        "Responsibilities Developed JSP",
        "the Elaborated Use Cases",
        "DML Operations",
        "Ant Maven Preparation of Standard Code",
        "JSP",
        "Technologies SOLR Elastic Search etc Worked",
        "Logic",
        "Talend",
        "Cassandra Data Modelling",
        "Sr Big DataData",
        "Ingestion Team",
        "Hdfs",
        "HDFS Hive Good Understanding",
        "MVC",
        "HDFS MapReduce Hive Sqoop",
        "MicroServices Architecture",
        "Cassandra utilizing Scala Developed Spark",
        "Spark",
        "J2EE J2SE Servlets JSP EJB",
        "The Fuse Box Electronic Cash Register",
        "Direct Acyclic Graph DAG",
        "HDFS Integrated Apache Storm",
        "Informatica Power Centre OLAP OLTP",
        "Sqoop",
        "QA",
        "the Application using JavaJ2EE Technologies Used",
        "Hadoop MapReduce HDFS",
        "Responsibilities Installed",
        "Scala Environment Developed Spark",
        "ATT Communications Responsibilities Configured Spark",
        "Create Solution Architecture",
        "HDFS Job Tracker Task Tracker",
        "Responsibilities Responsible",
        "California",
        "SQL",
        "HTML Java Script JSP",
        "Analysis Design Development Integration Implementation Debugging",
        "Ambari Environment Hadoop",
        "Autosys Experienced",
        "JavaScript Created Action Forms and Action",
        "SAP Systems Involved",
        "Java Developer General Dynamics Information Technology",
        "the United States",
        "Big Data",
        "Hive",
        "DAG",
        "Jakarta",
        "Hadoop Merck",
        "Dedicated Tech Services Inc",
        "Hive Experienced",
        "ETL",
        "Hibernate Query Language",
        "Microsoft Azure PaaS Services Design",
        "Zookeeper",
        "XSLT",
        "Software Configuration Management Involved",
        "Relational Database System",
        "Impala",
        "Zookeeper Excellent",
        "Talend ETL",
        "Developed Application",
        "Data Warehousing ETL",
        "UI",
        "Ajax Implemented",
        "Microsoft",
        "JSP Servlets",
        "Texas",
        "Merck",
        "Amazon Simple Storage AWS Elastic Beanstalk",
        "XSD",
        "SVN",
        "HDFS Used the Spark Cassandra Connector",
        "REST Oracle",
        "Struts 11 Spring",
        "MapReduce",
        "Developed UI",
        "NoSQL",
        "Tableau",
        "Oracle 102",
        "the Database Implemented Spring",
        "Big Data AnalyticsData Engineer",
        "Tiles CSS Cascading Style Sheets Involved",
        "Cloudera"
    ],
    "experience": "Experience in manipulatinganalysing large datasets and finding patterns and insights within structured and unstructured data Strong experience on Hadoop distributions like Cloudera MapR and Horton works Good understanding of NoSQL databases and hands on work experience in writing applications on NoSQL databases like HBase Cassandra and MongoDB Experience in search engine Technologies SOLR Elastic Search etc Worked with various HDFS file formats like Avro Sequence File and various compression formats like Snappy bzip2 Developed Simple to complex MapReduce streaming jobs using Python language that are implemented using Hive and Pig Skilled in developing applications in Python language for multiple platforms Hands on experience in application development using Java Linux Shell Scripting Experience in Oozie and workflow scheduler to manage Hadoop jobs by Direct Acyclic Graph DAG of actions with control flows Experience in migrating the data using Sqoop from HDFS to Relational Database System and viceversa according to clients requirement Extensive Experience on importing and exporting data using stream processing platforms like Flume and Kafka Strong Knowledge on Apache Spark with Scala Environment Developed Spark scripts by using Scala shell commands as per the requirement Good hands on experience in creating the RDDs Data frames for the required input data and performed the data transformations using Spark Scala Good knowledge on real time data streaming solutions using Apache Spark Streaming Kafka and Flume Experience in designing and developing applications in Spark using Scala to compare the performance of Spark with Hive and SQLOracle Very good experience in complete project life cycle design development testing and implementation of Client Server and Web applications Excellent Java development skills using J2EE J2SE Servlets JSP EJB JDBC SOAP and Restful web services Experience in MicroServices Architecture with Spring Boot and Docker Strong Experience of Data Warehousing ETL concepts using Informatica Power Centre OLAP OLTP and Autosys Experienced in working with Amazon Web Services AWS using EC2 for computing and S3 as storage mechanism Strong experience in ObjectOriented Design Analysis Development Testing and Maintenance Excellent implementation knowledge of EnterpriseWebClient Server using Java J2EE Experienced in using agile approaches including Extreme Programming TestDriven Development and Agile Scrum Worked in large and small teams for systems requirement design development Key participant in all phases of software development life cycle with Analysis Design Development Integration Implementation Debugging and Testing of Software Applications in client server environment Object Oriented Technology and Web based applications Experience in using various IDEs Eclipse IntelliJ and repositories SVN and Git Experience of using build tools Ant Maven Preparation of Standard Code guidelines analysis and testing documentations Good interpersonal skills committed result oriented hard working with a quest and deal to learn new technologies Work Experience Sr Big DataData Engineer AT T Middletown NJ April 2018 to Present ATT Inc is an American multinational conglomerate holding company headquartered at Whit acre Tower in Downtown Dallas Texas It is the worlds largest telecommunications company the second largest provider of mobile telephone services and the largest provider of fixed telephone services in the United States through ATT Communications Responsibilities Configured Spark Streaming to receive real time data from the Kafka and store the stream data to Cassandra utilizing Scala Developed Spark code to read data from Hdfs and write to Cassandra Configured Spark Streaming to receive real time data from the Kafka and store the stream data to Cassandra utilizing Scala Performed realtime analysis of the incoming data using Kafka consumer API Kafka topics Spark Streaming utilizing Scala Integrated Apache Storm with Kafka to perform web analytics Uploaded click stream data from Kafka to HDFS HBase and Hive by integrating with Storm Performed realtime analysis of the incoming data using Kafka consumer API Kafka topics Spark Streaming utilizing Scala Developed Kafka producer and consumers Cassandra clients and Spark along with components on HDFS Used the Spark Cassandra Connector to load data to and from Cassandra Experienced in designing and deployment of Hadoop cluster and various Big Data components including HDFS Map Reduce and Zookeeper in Cloudera distribution Implemented advanced procedures like text analytics and processing using the inmemory computing capabilities like Apache Spark written in Scala Implemented Kafka Custom partitions to send data to different categorized topics Implemented messaging system for different data sources using apache Kafka and configuring High level consumers for online and offline processing Used Pig as ETL tool to do transformations event joins filter and some preaggregations Migrated ETL jobs to Pig scripts do Transformations even joins and some preaggregations before storing the data onto HDFS Loaded and transformed large sets of structured unstructured data Was responsible to handle a Team of 4 members at Offshore Involved in daily SCRUM meetings to discuss the developmentprocess Worked with data delivery teams to setup new Hadoop users This job included setting up Linux users setting up Kerberos principals and testing HDFS Hive Worked with data Ingestion Team have Good understanding of Apache Nifi and its transformations Done Scaling Cassandra cluster based on lead patterns Good understanding of Cassandra Data Modelling based on applications Create Solution Architecture based upon Microsoft Azure PaaS Services Design solution for various system components using Microsoft Azure Configure Setup Azure Hybrid Connection to pull data from SAP Systems Involved in code deployment to Production and providing support to App support team Good understanding of Hadoop admin work maintained Hadoop cluster using Ambari Environment Hadoop Spark Scala Java Map Reduce HDFS Cassandra Ambari Hive Pig Sqoop Flume Linux Python Kafka Storm Shell Scripting XML ETL Eclipse Cloudera DB2 SQL Server MySQL AWS HBase Sr Big Data Hadoop Developer Scripps Health San Diego CA February 2015 to March 2018 Scripps Health is a nonprofit health care system based in San Diego California The system includes five hospitals and 19 outpatient facilities and treats a halfmillion patients annually through 2600 affiliated physicians The system also includes clinical research and medical education programs The organization has a number of projects planned including the Scripps Prebys Cardiovascular Institute which will serve as a centre for heart disease treatment research and graduate medical education Responsibilities Responsible for building scalable distributed data solutions using Hadoop Developed Pyspark code to read data from Hive group the fields and generate XML files Enhanced the Pyspark code to write the generated XML files to a directory to zip them to CDAs Implemented REST call to submit the generated CDAs to vendor website Implemented Impala to support JDBCODBC connections for Hiveserver2 Configured Spark Streaming to receive real time data from the Kafka and store the stream data to HDFS Integrated Apache Storm with Kafka to perform web analytics Uploaded click stream data from Kafka to HDFS HBase and Hive by integrating with Storm Performed realtime analysis of the incoming data using Kafka consumer API Kafka topics Spark Streaming utilizing Scala Developed Kafka producer and consumers Cassandra clients and Spark along with components on HDFS Hive Good Understanding of DAG cycle for entire Spark application flow on Spark application WebUI Used the Spark Cassandra Connector to load data to and from Cassandra Written Storm topology to emit data into Cassandra DB Experience with developing and maintaining Applications written for Amazon Simple Storage AWS Elastic Beanstalk and AWS Cloud Formation Implemented Python script to call the Cassandra Rest API performed transformations and loaded the data into Hive Experienced in designing and deployment of Hadoop cluster and various Big Data components including HDFS MapReduce Hive Sqoop Pig Oozie and Zookeeper in Cloudera distribution Implemented advanced procedures like text analytics and processing using the inmemory computing capabilities like Apache Spark written in Scala Implemented Kafka Custom partitioning to send data to different categorized topics Developed end to end data processing pipelines that begin with receiving data using distributed messaging systems Kafka through persistence of data into HBase Implemented messaging system for different data sources using apache Kafka and configuring High level consumers for online and offline processing Written Shell scripts that run multiple Hive jobs which helps to automate different hive tables incrementally which are used to generate different reports using Tableau for the Business use Worked with data delivery teams to setup new Hadoop users This job included setting up Linux users setting up Kerberos principals and testing HDFS Hive Done Scaling Cassandra cluster based on lead patterns Worked with Kafka for the proof of concept for carrying out log processing on a distributed system Performed realtime analysis of the incoming data using Kafka consumer API Kafka topics Spark Streaming utilizing Scala Good understanding of Cassandra Data Modelling based on applications Created PIG script jobs in maintaining minimal query optimization Environment Hadoop Java MapReduce HDFS Hive Pig Sqoop Flume Linux Python Spark Impala Scala Kafka Storm Shell Scripting XML Eclipse Cloudera DB2 SQL Server MySQL Autosys Talend AWS HBase Hadoop Developer Merck Pharmaceuticals Ltd Kenilworth NJ October 2013 to January 2015 Merck is adopting Hadoop to overcome three data challenges to its goal of improving yields in its manufacturing process First it needed to combine years of data from multiple data silos within its organization Secondly Merck needs to extend both the amount of data it can capture and its ability to retain that data for longer Thirdly the Merck team wants to test new hypotheses virtually at a far lower cost than testing those ideas with realworld material and equipment With Hadoop Merck plans to overcame these challenges to combine 10 years of vaccine manufacturing data and conduct 55 million crossbatch comparisons over 10 billion records The resulting yield improvement could grow profits by 10 million dollars for just one vaccine Responsibilities Installed and configured Hadoop MapReduce HDFS developed multiple MapReduce jobs in Java for data cleaning and preprocessing Importing and exporting data into HDFS from Oracle 102 database and vice versa using SQOOP Experienced in defining and coordination of job flows Gained experience in reviewing and managing Hadoop log files Extracted files from NoSQL database MongoDB HBase through Sqoop and placed in HDFS for processing Involved in Writing Data Refinement Pig Scripts and Hive Queries Good knowledge in running Hadoop streaming jobs to process terabytes of xml format data Load and transform large sets of structured semi structured and unstructured data Coordinated cluster services using Zookeeper Used XML Technologies like DOM for transferring data Object relational mapping and Persistence mechanism is executed using Hibernate ORM Developed custom validator in Struts and implemented server side validations using annotations Used Oracle for the database and Web Logic as the application server Used Flume to transport logs to HDFS Experienced in moving data from Hive tables into Cassandra for real time analytics on hive tables Organize documents in more useable clusters using Mahout Configured connection between HDFS and Tableau using Impala for Tableau developer team Responsible to manage data coming from different sources Got good experience with various NoSQL databases Experienced with handling administration activations using Cloudera manager Supported MapReduce programs those are running on the cluster Involved in loading data from UNIX file system to HDFS Installed and configured Hive and also written Hive UDFs Involved in creating Hive tables loading with data and writing Hive queries which will run internally in MapReduce way Worked on Talend ETL tool developed and scheduled jobs in Talend integration suite Modified reports and Talend ETL jobs based on the feedback from QA testers and Users in development and staging environments Environment Apache Hadoop Java JDK16 J2EE JDBC Servlets JSP Linux XML Web Logic SOAP WSDL HBase Hive Pig Sqoop ZooKeeper NoSQL HBase R MAHOUT MapReduce Cloudera HDFS Flume Impala Tableau Talend MySQL HTML5 CSS MongoDB Java developer Dedicated Tech Services Inc Columbus OH November 2011 to September 2013 The Fuse Box Electronic Cash Register shall provide basic set of services to allow the retail merchant to perform with great ease any function of a standard electronic cash register on the market today This set of services shall provide a powerful web enabled cash management tool for any merchantss Responsibilities Developed JSP for UI and Java classes for business logic Used XSLT for UI to display XML Data Utilized JavaScript for client side validation Utilized Oracle PLSQL for database operations using JDBC API Implemented DAO for Oracle 8i for DML Operations like Insert Update Delete the records VSS is used for Software Configuration Management Involved in the design development and deployment of the Application using JavaJ2EE Technologies Used IDE tool WSAD for development of the application Developed Application in Jakarta Struts Framework using MVC architecture Customizing all the JSP pages with same look and feel using Tiles CSS Cascading Style Sheets Involved in coding for the presentation layer using Apache Struts XML and JavaScript Created Action Forms and Action classes for the modules Developed JSPs to validate the information automatically using Ajax Implemented J2EE design patterns viz Faade pattern Singleton Pattern Created strutsconfigxml and tilesdefxml files Developed Ant script to create warear file and deploy the application to application server Extensively involved in database activities like designing tables SQL queries in the application and maintained complex reusable methods which implements stored procedures to fetch data from the database Used CVS for version control Also involved in testing and deployment of the application on Web Logic Application Server during integration Environment JavaJ2EE JSP Servlets Struts 11 Spring JUnit Eclipse Apache Ant JSP JavaBeans JavaScript Tomcat 41 Oracle 9i XML XSLT HTMLDHTMLXHTML CSS Tiles Ajax DB2 UDB PLSQL XML SPY Java Developer General Dynamics Information Technology May 2009 to October 2011 Apollo Munich is an insurance company intended to offer medical and health insurance plans that caters to individuals families and corporate houses as well Played the role of a Java developer for customer registration plans and claims plans Responsibilities Involved in projects utilizing Java Java EE web applications to create fullyintegrated client management systems Developed UI using HTML Java Script JSP and developed business Logic and interfacing components using Business Objects JDBC and XML Participated in user requirement sessions to analysis and gather Business requirements Development of user visible site using Perl back end admin sites using Python and big data using core java Involved in development of the application using Spring Web MVC and other components of the Elaborated Use Cases based on business requirements and was responsible for creation of class Diagrams Sequence Diagrams Implemented Objectrelation mapping in the persistence layer using Hibernate ORM framework Implemented REST Web Services with Jersey API to deal with customer requests Experienced in developing Restful web services consumed and also produced Used Hibernate for the Database connection and Hibernate Query Language HQL to add and retrieve the information from the Database Implemented Spring JDBC for connecting Oracle database Designed the application using MVC framework for easy maintainability Provided bug fixing and testing for existing web applications Involved in full system life cycle and responsible for Developing Testing Implementing Involved in Unit Testing Integration Testing and System Testing Implemented Form Beans and their Validations Written Hibernate components Developed client side validations with Java script Environment Spring JSP Servlets REST Oracle AJAX Java Script JQuery Hibernate Web Logic Log4j HTML XML CVS Eclipse SOAP Web Services XSLT XSD UNIX Maven Jenkins shell scripting MVS ISPF Education Bachelors Skills Amazon web services Git Hbase Hdfs Hive Javascript Jenkins Json Pig Python Scripting Svn Zookeeper Cassandra Impala Oozie Sqoop Hbase Kafka Db2",
    "extracted_keywords": [
        "Sr",
        "Big",
        "DataData",
        "Engineer",
        "Sr",
        "Big",
        "DataData",
        "Engineer",
        "Sr",
        "Big",
        "DataData",
        "Engineer",
        "AT",
        "T",
        "Jersey",
        "City",
        "NJ",
        "years",
        "IT",
        "experience",
        "variety",
        "industries",
        "hands",
        "experience",
        "years",
        "Big",
        "Data",
        "AnalyticsData",
        "Engineer",
        "development",
        "Expertise",
        "tools",
        "Hadoop",
        "Ecosystem",
        "Pig",
        "Hive",
        "HDFS",
        "MapReduce",
        "Sqoop",
        "Storm",
        "Spark",
        "Kafka",
        "Yarn",
        "Oozie",
        "Zookeeper",
        "Excellent",
        "knowledge",
        "Hadoop",
        "ecosystems",
        "HDFS",
        "Job",
        "Tracker",
        "Task",
        "Tracker",
        "Name",
        "Node",
        "Data",
        "Node",
        "Map",
        "Reduce",
        "programming",
        "paradigm",
        "Experience",
        "datasets",
        "patterns",
        "insights",
        "data",
        "experience",
        "Hadoop",
        "distributions",
        "Cloudera",
        "MapR",
        "Horton",
        "understanding",
        "NoSQL",
        "databases",
        "hands",
        "work",
        "experience",
        "applications",
        "NoSQL",
        "databases",
        "HBase",
        "Cassandra",
        "Experience",
        "search",
        "engine",
        "Technologies",
        "SOLR",
        "Elastic",
        "Search",
        "HDFS",
        "file",
        "formats",
        "Avro",
        "Sequence",
        "File",
        "compression",
        "formats",
        "bzip2",
        "Simple",
        "MapReduce",
        "streaming",
        "jobs",
        "Python",
        "language",
        "Hive",
        "Pig",
        "Skilled",
        "applications",
        "Python",
        "language",
        "platforms",
        "Hands",
        "experience",
        "application",
        "development",
        "Java",
        "Linux",
        "Shell",
        "Scripting",
        "Experience",
        "Oozie",
        "workflow",
        "scheduler",
        "Hadoop",
        "jobs",
        "Direct",
        "Acyclic",
        "Graph",
        "DAG",
        "actions",
        "control",
        "Experience",
        "data",
        "Sqoop",
        "HDFS",
        "Relational",
        "Database",
        "System",
        "viceversa",
        "clients",
        "requirement",
        "Experience",
        "data",
        "stream",
        "processing",
        "platforms",
        "Flume",
        "Kafka",
        "Strong",
        "Knowledge",
        "Apache",
        "Spark",
        "Scala",
        "Environment",
        "Developed",
        "Spark",
        "scripts",
        "Scala",
        "shell",
        "commands",
        "requirement",
        "hands",
        "experience",
        "RDDs",
        "Data",
        "frames",
        "input",
        "data",
        "data",
        "transformations",
        "Spark",
        "Scala",
        "knowledge",
        "time",
        "data",
        "streaming",
        "solutions",
        "Apache",
        "Spark",
        "Streaming",
        "Kafka",
        "Flume",
        "Experience",
        "applications",
        "Spark",
        "Scala",
        "performance",
        "Spark",
        "Hive",
        "SQLOracle",
        "experience",
        "project",
        "life",
        "cycle",
        "design",
        "development",
        "testing",
        "implementation",
        "Client",
        "Server",
        "Web",
        "applications",
        "Excellent",
        "Java",
        "development",
        "skills",
        "J2EE",
        "J2SE",
        "Servlets",
        "JSP",
        "EJB",
        "JDBC",
        "SOAP",
        "web",
        "services",
        "Experience",
        "MicroServices",
        "Architecture",
        "Spring",
        "Boot",
        "Docker",
        "Strong",
        "Experience",
        "Data",
        "Warehousing",
        "ETL",
        "concepts",
        "Informatica",
        "Power",
        "Centre",
        "OLTP",
        "Autosys",
        "Amazon",
        "Web",
        "Services",
        "AWS",
        "EC2",
        "computing",
        "S3",
        "storage",
        "mechanism",
        "experience",
        "ObjectOriented",
        "Design",
        "Analysis",
        "Development",
        "Testing",
        "Maintenance",
        "Excellent",
        "implementation",
        "knowledge",
        "EnterpriseWebClient",
        "Server",
        "Java",
        "J2EE",
        "approaches",
        "Extreme",
        "Programming",
        "TestDriven",
        "Development",
        "Agile",
        "Scrum",
        "teams",
        "systems",
        "requirement",
        "design",
        "development",
        "participant",
        "phases",
        "software",
        "development",
        "life",
        "cycle",
        "Analysis",
        "Design",
        "Development",
        "Integration",
        "Implementation",
        "Debugging",
        "Testing",
        "Software",
        "Applications",
        "client",
        "server",
        "environment",
        "Object",
        "Oriented",
        "Technology",
        "Web",
        "applications",
        "Experience",
        "IDEs",
        "Eclipse",
        "IntelliJ",
        "repositories",
        "SVN",
        "Git",
        "Experience",
        "build",
        "tools",
        "Ant",
        "Maven",
        "Preparation",
        "Standard",
        "Code",
        "guidelines",
        "analysis",
        "testing",
        "documentations",
        "skills",
        "result",
        "quest",
        "technologies",
        "Work",
        "Experience",
        "Sr",
        "Big",
        "DataData",
        "Engineer",
        "AT",
        "T",
        "Middletown",
        "NJ",
        "April",
        "Present",
        "ATT",
        "Inc",
        "conglomerate",
        "company",
        "Whit",
        "acre",
        "Tower",
        "Downtown",
        "Dallas",
        "Texas",
        "worlds",
        "telecommunications",
        "company",
        "provider",
        "telephone",
        "services",
        "provider",
        "telephone",
        "services",
        "United",
        "States",
        "ATT",
        "Communications",
        "Responsibilities",
        "Spark",
        "Streaming",
        "time",
        "data",
        "Kafka",
        "stream",
        "data",
        "Cassandra",
        "Scala",
        "Developed",
        "Spark",
        "code",
        "data",
        "Hdfs",
        "Cassandra",
        "Configured",
        "Spark",
        "Streaming",
        "time",
        "data",
        "Kafka",
        "stream",
        "data",
        "Cassandra",
        "Scala",
        "analysis",
        "data",
        "Kafka",
        "consumer",
        "API",
        "Kafka",
        "Spark",
        "Streaming",
        "Scala",
        "Integrated",
        "Apache",
        "Storm",
        "Kafka",
        "web",
        "analytics",
        "stream",
        "data",
        "Kafka",
        "HDFS",
        "HBase",
        "Hive",
        "Storm",
        "analysis",
        "data",
        "Kafka",
        "consumer",
        "API",
        "Kafka",
        "Spark",
        "Streaming",
        "Scala",
        "Developed",
        "Kafka",
        "producer",
        "consumers",
        "Cassandra",
        "clients",
        "Spark",
        "components",
        "HDFS",
        "Spark",
        "Cassandra",
        "Connector",
        "data",
        "Cassandra",
        "Experienced",
        "designing",
        "deployment",
        "Hadoop",
        "cluster",
        "Big",
        "Data",
        "components",
        "Map",
        "Reduce",
        "Zookeeper",
        "Cloudera",
        "distribution",
        "procedures",
        "text",
        "analytics",
        "processing",
        "inmemory",
        "computing",
        "capabilities",
        "Apache",
        "Spark",
        "Scala",
        "Kafka",
        "Custom",
        "data",
        "topics",
        "system",
        "data",
        "sources",
        "apache",
        "Kafka",
        "level",
        "consumers",
        "Pig",
        "ETL",
        "tool",
        "transformations",
        "event",
        "filter",
        "preaggregations",
        "ETL",
        "jobs",
        "scripts",
        "Transformations",
        "preaggregations",
        "data",
        "Loaded",
        "sets",
        "data",
        "Team",
        "members",
        "Offshore",
        "SCRUM",
        "meetings",
        "developmentprocess",
        "data",
        "delivery",
        "teams",
        "Hadoop",
        "users",
        "job",
        "Linux",
        "users",
        "Kerberos",
        "principals",
        "HDFS",
        "Hive",
        "Worked",
        "data",
        "Ingestion",
        "Team",
        "understanding",
        "Apache",
        "Nifi",
        "transformations",
        "Scaling",
        "Cassandra",
        "cluster",
        "lead",
        "patterns",
        "understanding",
        "Cassandra",
        "Data",
        "Modelling",
        "applications",
        "Create",
        "Solution",
        "Architecture",
        "Microsoft",
        "Azure",
        "PaaS",
        "Services",
        "Design",
        "solution",
        "system",
        "components",
        "Microsoft",
        "Azure",
        "Configure",
        "Setup",
        "Azure",
        "Hybrid",
        "Connection",
        "data",
        "SAP",
        "Systems",
        "code",
        "deployment",
        "Production",
        "support",
        "App",
        "support",
        "team",
        "understanding",
        "Hadoop",
        "admin",
        "work",
        "Hadoop",
        "cluster",
        "Ambari",
        "Environment",
        "Hadoop",
        "Spark",
        "Scala",
        "Java",
        "Map",
        "HDFS",
        "Cassandra",
        "Ambari",
        "Hive",
        "Pig",
        "Sqoop",
        "Flume",
        "Linux",
        "Python",
        "Kafka",
        "Storm",
        "Shell",
        "Scripting",
        "XML",
        "ETL",
        "Eclipse",
        "Cloudera",
        "DB2",
        "SQL",
        "Server",
        "MySQL",
        "AWS",
        "HBase",
        "Sr",
        "Big",
        "Data",
        "Hadoop",
        "Developer",
        "Scripps",
        "Health",
        "San",
        "Diego",
        "CA",
        "February",
        "March",
        "Scripps",
        "Health",
        "health",
        "care",
        "system",
        "San",
        "Diego",
        "California",
        "system",
        "hospitals",
        "outpatient",
        "facilities",
        "halfmillion",
        "patients",
        "physicians",
        "system",
        "research",
        "education",
        "programs",
        "organization",
        "number",
        "projects",
        "Scripps",
        "Prebys",
        "Cardiovascular",
        "Institute",
        "centre",
        "heart",
        "disease",
        "treatment",
        "research",
        "graduate",
        "education",
        "Responsibilities",
        "data",
        "solutions",
        "Hadoop",
        "Developed",
        "Pyspark",
        "code",
        "data",
        "Hive",
        "group",
        "fields",
        "XML",
        "files",
        "Pyspark",
        "code",
        "XML",
        "files",
        "directory",
        "CDAs",
        "REST",
        "call",
        "CDAs",
        "vendor",
        "website",
        "Impala",
        "JDBCODBC",
        "connections",
        "Hiveserver2",
        "Configured",
        "Spark",
        "Streaming",
        "time",
        "data",
        "Kafka",
        "stream",
        "data",
        "HDFS",
        "Integrated",
        "Apache",
        "Storm",
        "Kafka",
        "web",
        "analytics",
        "stream",
        "data",
        "Kafka",
        "HDFS",
        "HBase",
        "Hive",
        "Storm",
        "analysis",
        "data",
        "Kafka",
        "consumer",
        "API",
        "Kafka",
        "Spark",
        "Streaming",
        "Scala",
        "Developed",
        "Kafka",
        "producer",
        "consumers",
        "Cassandra",
        "clients",
        "Spark",
        "components",
        "HDFS",
        "Hive",
        "Good",
        "Understanding",
        "DAG",
        "cycle",
        "Spark",
        "application",
        "flow",
        "Spark",
        "application",
        "WebUI",
        "Spark",
        "Cassandra",
        "Connector",
        "data",
        "Cassandra",
        "Written",
        "Storm",
        "topology",
        "data",
        "Cassandra",
        "DB",
        "Experience",
        "Applications",
        "Amazon",
        "Simple",
        "Storage",
        "AWS",
        "Elastic",
        "Beanstalk",
        "AWS",
        "Cloud",
        "Formation",
        "Python",
        "script",
        "Cassandra",
        "Rest",
        "API",
        "transformations",
        "data",
        "Hive",
        "Experienced",
        "designing",
        "deployment",
        "Hadoop",
        "cluster",
        "Big",
        "Data",
        "components",
        "HDFS",
        "MapReduce",
        "Hive",
        "Sqoop",
        "Pig",
        "Oozie",
        "Zookeeper",
        "Cloudera",
        "distribution",
        "procedures",
        "text",
        "analytics",
        "processing",
        "inmemory",
        "computing",
        "capabilities",
        "Apache",
        "Spark",
        "Scala",
        "Kafka",
        "Custom",
        "data",
        "topics",
        "end",
        "data",
        "processing",
        "pipelines",
        "data",
        "systems",
        "Kafka",
        "persistence",
        "data",
        "HBase",
        "system",
        "data",
        "sources",
        "apache",
        "Kafka",
        "level",
        "consumers",
        "processing",
        "Written",
        "Shell",
        "scripts",
        "Hive",
        "jobs",
        "hive",
        "tables",
        "reports",
        "Tableau",
        "Business",
        "use",
        "data",
        "delivery",
        "teams",
        "Hadoop",
        "users",
        "job",
        "Linux",
        "users",
        "Kerberos",
        "principals",
        "HDFS",
        "Hive",
        "Scaling",
        "Cassandra",
        "cluster",
        "lead",
        "patterns",
        "Kafka",
        "proof",
        "concept",
        "log",
        "processing",
        "system",
        "analysis",
        "data",
        "Kafka",
        "consumer",
        "API",
        "Kafka",
        "Spark",
        "Streaming",
        "Scala",
        "understanding",
        "Cassandra",
        "Data",
        "Modelling",
        "applications",
        "PIG",
        "script",
        "jobs",
        "query",
        "optimization",
        "Environment",
        "Hadoop",
        "Java",
        "MapReduce",
        "HDFS",
        "Hive",
        "Pig",
        "Sqoop",
        "Flume",
        "Linux",
        "Python",
        "Spark",
        "Impala",
        "Scala",
        "Kafka",
        "Storm",
        "Shell",
        "Scripting",
        "XML",
        "Eclipse",
        "Cloudera",
        "DB2",
        "SQL",
        "Server",
        "MySQL",
        "Autosys",
        "Talend",
        "AWS",
        "HBase",
        "Hadoop",
        "Developer",
        "Merck",
        "Pharmaceuticals",
        "Ltd",
        "Kenilworth",
        "NJ",
        "October",
        "January",
        "Merck",
        "Hadoop",
        "data",
        "challenges",
        "goal",
        "yields",
        "manufacturing",
        "process",
        "First",
        "years",
        "data",
        "data",
        "silos",
        "organization",
        "Merck",
        "amount",
        "data",
        "ability",
        "data",
        "Merck",
        "team",
        "hypotheses",
        "cost",
        "ideas",
        "realworld",
        "material",
        "equipment",
        "Hadoop",
        "Merck",
        "challenges",
        "years",
        "vaccine",
        "manufacturing",
        "data",
        "crossbatch",
        "comparisons",
        "records",
        "yield",
        "improvement",
        "profits",
        "dollars",
        "vaccine",
        "Responsibilities",
        "Hadoop",
        "MapReduce",
        "MapReduce",
        "jobs",
        "Java",
        "data",
        "Importing",
        "data",
        "HDFS",
        "Oracle",
        "database",
        "vice",
        "SQOOP",
        "coordination",
        "job",
        "experience",
        "Hadoop",
        "log",
        "files",
        "NoSQL",
        "database",
        "MongoDB",
        "HBase",
        "Sqoop",
        "HDFS",
        "processing",
        "Writing",
        "Data",
        "Refinement",
        "Pig",
        "Scripts",
        "Hive",
        "Queries",
        "knowledge",
        "Hadoop",
        "streaming",
        "jobs",
        "terabytes",
        "xml",
        "format",
        "data",
        "Load",
        "sets",
        "data",
        "Coordinated",
        "cluster",
        "services",
        "Zookeeper",
        "XML",
        "Technologies",
        "DOM",
        "data",
        "mapping",
        "Persistence",
        "mechanism",
        "Hibernate",
        "ORM",
        "custom",
        "validator",
        "Struts",
        "server",
        "side",
        "validations",
        "annotations",
        "Oracle",
        "database",
        "Web",
        "Logic",
        "application",
        "server",
        "Flume",
        "logs",
        "data",
        "Hive",
        "tables",
        "Cassandra",
        "time",
        "analytics",
        "tables",
        "documents",
        "clusters",
        "Mahout",
        "Configured",
        "connection",
        "HDFS",
        "Tableau",
        "Impala",
        "Tableau",
        "developer",
        "team",
        "data",
        "sources",
        "experience",
        "NoSQL",
        "administration",
        "activations",
        "Cloudera",
        "manager",
        "Supported",
        "MapReduce",
        "programs",
        "cluster",
        "loading",
        "data",
        "UNIX",
        "file",
        "system",
        "HDFS",
        "Installed",
        "Hive",
        "Hive",
        "UDFs",
        "Hive",
        "tables",
        "data",
        "Hive",
        "queries",
        "MapReduce",
        "way",
        "Talend",
        "ETL",
        "tool",
        "jobs",
        "Talend",
        "integration",
        "suite",
        "Modified",
        "reports",
        "Talend",
        "ETL",
        "jobs",
        "feedback",
        "QA",
        "testers",
        "Users",
        "development",
        "staging",
        "environments",
        "Environment",
        "Apache",
        "Hadoop",
        "Java",
        "JDK16",
        "J2EE",
        "JDBC",
        "Servlets",
        "JSP",
        "Linux",
        "XML",
        "Web",
        "Logic",
        "SOAP",
        "WSDL",
        "HBase",
        "Hive",
        "Pig",
        "Sqoop",
        "ZooKeeper",
        "NoSQL",
        "HBase",
        "R",
        "MAHOUT",
        "MapReduce",
        "Cloudera",
        "HDFS",
        "Flume",
        "Impala",
        "Tableau",
        "Talend",
        "MySQL",
        "HTML5",
        "CSS",
        "MongoDB",
        "Java",
        "developer",
        "Dedicated",
        "Tech",
        "Services",
        "Inc",
        "Columbus",
        "OH",
        "November",
        "September",
        "Fuse",
        "Box",
        "Electronic",
        "Cash",
        "Register",
        "set",
        "services",
        "merchant",
        "ease",
        "function",
        "cash",
        "register",
        "market",
        "today",
        "set",
        "services",
        "web",
        "cash",
        "management",
        "tool",
        "Responsibilities",
        "JSP",
        "UI",
        "Java",
        "classes",
        "business",
        "logic",
        "XSLT",
        "UI",
        "XML",
        "Data",
        "JavaScript",
        "client",
        "side",
        "validation",
        "Oracle",
        "PLSQL",
        "database",
        "operations",
        "JDBC",
        "API",
        "DAO",
        "Oracle",
        "DML",
        "Operations",
        "Insert",
        "Update",
        "Delete",
        "records",
        "VSS",
        "Software",
        "Configuration",
        "Management",
        "design",
        "development",
        "deployment",
        "Application",
        "JavaJ2EE",
        "Technologies",
        "IDE",
        "tool",
        "WSAD",
        "development",
        "application",
        "Developed",
        "Application",
        "Jakarta",
        "Struts",
        "Framework",
        "MVC",
        "architecture",
        "Customizing",
        "JSP",
        "pages",
        "look",
        "Tiles",
        "CSS",
        "Cascading",
        "Style",
        "Sheets",
        "presentation",
        "layer",
        "Apache",
        "Struts",
        "XML",
        "JavaScript",
        "Created",
        "Action",
        "Forms",
        "Action",
        "classes",
        "modules",
        "JSPs",
        "information",
        "Ajax",
        "J2EE",
        "design",
        "patterns",
        "Faade",
        "pattern",
        "Singleton",
        "Pattern",
        "strutsconfigxml",
        "tilesdefxml",
        "Developed",
        "Ant",
        "script",
        "warear",
        "file",
        "application",
        "application",
        "server",
        "database",
        "activities",
        "tables",
        "SQL",
        "application",
        "methods",
        "procedures",
        "data",
        "database",
        "CVS",
        "version",
        "control",
        "testing",
        "deployment",
        "application",
        "Web",
        "Logic",
        "Application",
        "Server",
        "integration",
        "Environment",
        "JavaJ2EE",
        "JSP",
        "Servlets",
        "Struts",
        "Spring",
        "JUnit",
        "Eclipse",
        "Apache",
        "Ant",
        "JSP",
        "JavaBeans",
        "JavaScript",
        "Tomcat",
        "Oracle",
        "9i",
        "XML",
        "XSLT",
        "CSS",
        "Tiles",
        "Ajax",
        "DB2",
        "UDB",
        "PLSQL",
        "XML",
        "SPY",
        "Java",
        "Developer",
        "General",
        "Dynamics",
        "Information",
        "Technology",
        "May",
        "October",
        "Apollo",
        "Munich",
        "insurance",
        "company",
        "health",
        "insurance",
        "individuals",
        "families",
        "houses",
        "role",
        "Java",
        "developer",
        "customer",
        "registration",
        "plans",
        "claims",
        "plans",
        "Responsibilities",
        "projects",
        "Java",
        "Java",
        "EE",
        "web",
        "applications",
        "client",
        "management",
        "systems",
        "UI",
        "HTML",
        "Java",
        "Script",
        "JSP",
        "business",
        "Logic",
        "components",
        "Business",
        "Objects",
        "JDBC",
        "XML",
        "user",
        "requirement",
        "sessions",
        "Business",
        "requirements",
        "Development",
        "user",
        "site",
        "Perl",
        "end",
        "admin",
        "sites",
        "Python",
        "data",
        "core",
        "development",
        "application",
        "Spring",
        "Web",
        "MVC",
        "components",
        "Elaborated",
        "Use",
        "Cases",
        "business",
        "requirements",
        "creation",
        "class",
        "Diagrams",
        "Sequence",
        "Diagrams",
        "Objectrelation",
        "mapping",
        "persistence",
        "layer",
        "Hibernate",
        "ORM",
        "framework",
        "REST",
        "Web",
        "Services",
        "Jersey",
        "API",
        "customer",
        "requests",
        "Restful",
        "web",
        "services",
        "Hibernate",
        "Database",
        "connection",
        "Hibernate",
        "Query",
        "Language",
        "HQL",
        "information",
        "Database",
        "Spring",
        "JDBC",
        "Oracle",
        "database",
        "application",
        "MVC",
        "framework",
        "maintainability",
        "bug",
        "web",
        "applications",
        "system",
        "life",
        "cycle",
        "Testing",
        "Implementing",
        "Unit",
        "Testing",
        "Integration",
        "Testing",
        "System",
        "Testing",
        "Form",
        "Beans",
        "Validations",
        "Written",
        "Hibernate",
        "components",
        "client",
        "side",
        "validations",
        "Java",
        "script",
        "Environment",
        "Spring",
        "JSP",
        "Servlets",
        "REST",
        "Oracle",
        "AJAX",
        "Java",
        "Script",
        "JQuery",
        "Hibernate",
        "Web",
        "Logic",
        "Log4j",
        "HTML",
        "XML",
        "CVS",
        "Eclipse",
        "SOAP",
        "Web",
        "Services",
        "XSLT",
        "XSD",
        "UNIX",
        "Maven",
        "Jenkins",
        "shell",
        "MVS",
        "ISPF",
        "Education",
        "Bachelors",
        "Skills",
        "Amazon",
        "web",
        "services",
        "Git",
        "Hbase",
        "Hdfs",
        "Hive",
        "Javascript",
        "Jenkins",
        "Json",
        "Pig",
        "Python",
        "Scripting",
        "Svn",
        "Zookeeper",
        "Cassandra",
        "Impala",
        "Oozie",
        "Sqoop",
        "Hbase",
        "Kafka"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T21:16:07.594759",
    "resume_data": "Sr Big DataData Engineer Sr Big DataData Engineer Sr Big DataData Engineer AT T Jersey City NJ Having 10 years of overall IT experience in a variety of industries which includes hands on experience of 7 years in Big Data AnalyticsData Engineer and development Expertise with the tools in Hadoop Ecosystem including Pig Hive HDFS MapReduce Sqoop Storm Spark Kafka Yarn Oozie and Zookeeper Excellent knowledge on Hadoop ecosystems such as HDFS Job Tracker Task Tracker Name Node Data Node and Map Reduce programming paradigm Experience in manipulatinganalysing large datasets and finding patterns and insights within structured and unstructured data Strong experience on Hadoop distributions like Cloudera MapR and Horton works Good understanding of NoSQL databases and hands on work experience in writing applications on NoSQL databases like HBase Cassandra and MongoDB Experience in search engine Technologies SOLR Elastic Search etc Worked with various HDFS file formats like Avro Sequence File and various compression formats like Snappy bzip2 Developed Simple to complex MapReduce streaming jobs using Python language that are implemented using Hive and Pig Skilled in developing applications in Python language for multiple platforms Hands on experience in application development using Java Linux Shell Scripting Experience in Oozie and workflow scheduler to manage Hadoop jobs by Direct Acyclic Graph DAG of actions with control flows Experience in migrating the data using Sqoop from HDFS to Relational Database System and viceversa according to clients requirement Extensive Experience on importing and exporting data using stream processing platforms like Flume and Kafka Strong Knowledge on Apache Spark with Scala Environment Developed Spark scripts by using Scala shell commands as per the requirement Good hands on experience in creating the RDDs Data frames for the required input data and performed the data transformations using Spark Scala Good knowledge on real time data streaming solutions using Apache Spark Streaming Kafka and Flume Experience in designing and developing applications in Spark using Scala to compare the performance of Spark with Hive and SQLOracle Very good experience in complete project life cycle design development testing and implementation of Client Server and Web applications Excellent Java development skills using J2EE J2SE Servlets JSP EJB JDBC SOAP and Restful web services Experience in MicroServices Architecture with Spring Boot and Docker Strong Experience of Data Warehousing ETL concepts using Informatica Power Centre OLAP OLTP and Autosys Experienced in working with Amazon Web Services AWS using EC2 for computing and S3 as storage mechanism Strong experience in ObjectOriented Design Analysis Development Testing and Maintenance Excellent implementation knowledge of EnterpriseWebClient Server using Java J2EE Experienced in using agile approaches including Extreme Programming TestDriven Development and Agile Scrum Worked in large and small teams for systems requirement design development Key participant in all phases of software development life cycle with Analysis Design Development Integration Implementation Debugging and Testing of Software Applications in client server environment Object Oriented Technology and Web based applications Experience in using various IDEs Eclipse IntelliJ and repositories SVN and Git Experience of using build tools Ant Maven Preparation of Standard Code guidelines analysis and testing documentations Good interpersonal skills committed result oriented hard working with a quest and deal to learn new technologies Work Experience Sr Big DataData Engineer AT T Middletown NJ April 2018 to Present ATT Inc is an American multinational conglomerate holding company headquartered at Whit acre Tower in Downtown Dallas Texas It is the worlds largest telecommunications company the second largest provider of mobile telephone services and the largest provider of fixed telephone services in the United States through ATT Communications Responsibilities Configured Spark Streaming to receive real time data from the Kafka and store the stream data to Cassandra utilizing Scala Developed Spark code to read data from Hdfs and write to Cassandra Configured Spark Streaming to receive real time data from the Kafka and store the stream data to Cassandra utilizing Scala Performed realtime analysis of the incoming data using Kafka consumer API Kafka topics Spark Streaming utilizing Scala Integrated Apache Storm with Kafka to perform web analytics Uploaded click stream data from Kafka to HDFS HBase and Hive by integrating with Storm Performed realtime analysis of the incoming data using Kafka consumer API Kafka topics Spark Streaming utilizing Scala Developed Kafka producer and consumers Cassandra clients and Spark along with components on HDFS Used the Spark Cassandra Connector to load data to and from Cassandra Experienced in designing and deployment of Hadoop cluster and various Big Data components including HDFS Map Reduce and Zookeeper in Cloudera distribution Implemented advanced procedures like text analytics and processing using the inmemory computing capabilities like Apache Spark written in Scala Implemented Kafka Custom partitions to send data to different categorized topics Implemented messaging system for different data sources using apache Kafka and configuring High level consumers for online and offline processing Used Pig as ETL tool to do transformations event joins filter and some preaggregations Migrated ETL jobs to Pig scripts do Transformations even joins and some preaggregations before storing the data onto HDFS Loaded and transformed large sets of structured unstructured data Was responsible to handle a Team of 4 members at Offshore Involved in daily SCRUM meetings to discuss the developmentprocess Worked with data delivery teams to setup new Hadoop users This job included setting up Linux users setting up Kerberos principals and testing HDFS Hive Worked with data Ingestion Team have Good understanding of Apache Nifi and its transformations Done Scaling Cassandra cluster based on lead patterns Good understanding of Cassandra Data Modelling based on applications Create Solution Architecture based upon Microsoft Azure PaaS Services Design solution for various system components using Microsoft Azure Configure Setup Azure Hybrid Connection to pull data from SAP Systems Involved in code deployment to Production and providing support to App support team Good understanding of Hadoop admin work maintained Hadoop cluster using Ambari Environment Hadoop Spark Scala Java Map Reduce HDFS Cassandra Ambari Hive Pig Sqoop Flume Linux Python Kafka Storm Shell Scripting XML ETL Eclipse Cloudera DB2 SQL Server MySQL AWS HBase Sr Big Data Hadoop Developer Scripps Health San Diego CA February 2015 to March 2018 Scripps Health is a nonprofit health care system based in San Diego California The system includes five hospitals and 19 outpatient facilities and treats a halfmillion patients annually through 2600 affiliated physicians The system also includes clinical research and medical education programs The organization has a number of projects planned including the Scripps Prebys Cardiovascular Institute which will serve as a centre for heart disease treatment research and graduate medical education Responsibilities Responsible for building scalable distributed data solutions using Hadoop Developed Pyspark code to read data from Hive group the fields and generate XML files Enhanced the Pyspark code to write the generated XML files to a directory to zip them to CDAs Implemented REST call to submit the generated CDAs to vendor website Implemented Impala to support JDBCODBC connections for Hiveserver2 Configured Spark Streaming to receive real time data from the Kafka and store the stream data to HDFS Integrated Apache Storm with Kafka to perform web analytics Uploaded click stream data from Kafka to HDFS HBase and Hive by integrating with Storm Performed realtime analysis of the incoming data using Kafka consumer API Kafka topics Spark Streaming utilizing Scala Developed Kafka producer and consumers Cassandra clients and Spark along with components on HDFS Hive Good Understanding of DAG cycle for entire Spark application flow on Spark application WebUI Used the Spark Cassandra Connector to load data to and from Cassandra Written Storm topology to emit data into Cassandra DB Experience with developing and maintaining Applications written for Amazon Simple Storage AWS Elastic Beanstalk and AWS Cloud Formation Implemented Python script to call the Cassandra Rest API performed transformations and loaded the data into Hive Experienced in designing and deployment of Hadoop cluster and various Big Data components including HDFS MapReduce Hive Sqoop Pig Oozie and Zookeeper in Cloudera distribution Implemented advanced procedures like text analytics and processing using the inmemory computing capabilities like Apache Spark written in Scala Implemented Kafka Custom partitioning to send data to different categorized topics Developed end to end data processing pipelines that begin with receiving data using distributed messaging systems Kafka through persistence of data into HBase Implemented messaging system for different data sources using apache Kafka and configuring High level consumers for online and offline processing Written Shell scripts that run multiple Hive jobs which helps to automate different hive tables incrementally which are used to generate different reports using Tableau for the Business use Worked with data delivery teams to setup new Hadoop users This job included setting up Linux users setting up Kerberos principals and testing HDFS Hive Done Scaling Cassandra cluster based on lead patterns Worked with Kafka for the proof of concept for carrying out log processing on a distributed system Performed realtime analysis of the incoming data using Kafka consumer API Kafka topics Spark Streaming utilizing Scala Good understanding of Cassandra Data Modelling based on applications Created PIG script jobs in maintaining minimal query optimization Environment Hadoop Java MapReduce HDFS Hive Pig Sqoop Flume Linux Python Spark Impala Scala Kafka Storm Shell Scripting XML Eclipse Cloudera DB2 SQL Server MySQL Autosys Talend AWS HBase Hadoop Developer Merck Pharmaceuticals Ltd Kenilworth NJ October 2013 to January 2015 Merck is adopting Hadoop to overcome three data challenges to its goal of improving yields in its manufacturing process First it needed to combine years of data from multiple data silos within its organization Secondly Merck needs to extend both the amount of data it can capture and its ability to retain that data for longer Thirdly the Merck team wants to test new hypotheses virtually at a far lower cost than testing those ideas with realworld material and equipment With Hadoop Merck plans to overcame these challenges to combine 10 years of vaccine manufacturing data and conduct 55 million crossbatch comparisons over 10 billion records The resulting yield improvement could grow profits by 10 million dollars for just one vaccine Responsibilities Installed and configured Hadoop MapReduce HDFS developed multiple MapReduce jobs in Java for data cleaning and preprocessing Importing and exporting data into HDFS from Oracle 102 database and vice versa using SQOOP Experienced in defining and coordination of job flows Gained experience in reviewing and managing Hadoop log files Extracted files from NoSQL database MongoDB HBase through Sqoop and placed in HDFS for processing Involved in Writing Data Refinement Pig Scripts and Hive Queries Good knowledge in running Hadoop streaming jobs to process terabytes of xml format data Load and transform large sets of structured semi structured and unstructured data Coordinated cluster services using Zookeeper Used XML Technologies like DOM for transferring data Object relational mapping and Persistence mechanism is executed using Hibernate ORM Developed custom validator in Struts and implemented server side validations using annotations Used Oracle for the database and Web Logic as the application server Used Flume to transport logs to HDFS Experienced in moving data from Hive tables into Cassandra for real time analytics on hive tables Organize documents in more useable clusters using Mahout Configured connection between HDFS and Tableau using Impala for Tableau developer team Responsible to manage data coming from different sources Got good experience with various NoSQL databases Experienced with handling administration activations using Cloudera manager Supported MapReduce programs those are running on the cluster Involved in loading data from UNIX file system to HDFS Installed and configured Hive and also written Hive UDFs Involved in creating Hive tables loading with data and writing Hive queries which will run internally in MapReduce way Worked on Talend ETL tool developed and scheduled jobs in Talend integration suite Modified reports and Talend ETL jobs based on the feedback from QA testers and Users in development and staging environments Environment Apache Hadoop Java JDK16 J2EE JDBC Servlets JSP Linux XML Web Logic SOAP WSDL HBase Hive Pig Sqoop ZooKeeper NoSQL HBase R MAHOUT MapReduce Cloudera HDFS Flume Impala Tableau Talend MySQL HTML5 CSS MongoDB Java developer Dedicated Tech Services Inc Columbus OH November 2011 to September 2013 The Fuse Box Electronic Cash Register shall provide basic set of services to allow the retail merchant to perform with great ease any function of a standard electronic cash register on the market today This set of services shall provide a powerful web enabled cash management tool for any merchantss Responsibilities Developed JSP for UI and Java classes for business logic Used XSLT for UI to display XML Data Utilized JavaScript for client side validation Utilized Oracle PLSQL for database operations using JDBC API Implemented DAO for Oracle 8i for DML Operations like Insert Update Delete the records VSS is used for Software Configuration Management Involved in the design development and deployment of the Application using JavaJ2EE Technologies Used IDE tool WSAD for development of the application Developed Application in Jakarta Struts Framework using MVC architecture Customizing all the JSP pages with same look and feel using Tiles CSS Cascading Style Sheets Involved in coding for the presentation layer using Apache Struts XML and JavaScript Created Action Forms and Action classes for the modules Developed JSPs to validate the information automatically using Ajax Implemented J2EE design patterns viz Faade pattern Singleton Pattern Created strutsconfigxml and tilesdefxml files Developed Ant script to create warear file and deploy the application to application server Extensively involved in database activities like designing tables SQL queries in the application and maintained complex reusable methods which implements stored procedures to fetch data from the database Used CVS for version control Also involved in testing and deployment of the application on Web Logic Application Server during integration Environment JavaJ2EE JSP Servlets Struts 11 Spring JUnit Eclipse Apache Ant JSP JavaBeans JavaScript Tomcat 41 Oracle 9i XML XSLT HTMLDHTMLXHTML CSS Tiles Ajax DB2 UDB PLSQL XML SPY Java Developer General Dynamics Information Technology May 2009 to October 2011 Apollo Munich is an insurance company intended to offer medical and health insurance plans that caters to individuals families and corporate houses as well Played the role of a Java developer for customer registration plans and claims plans Responsibilities Involved in projects utilizing Java Java EE web applications to create fullyintegrated client management systems Developed UI using HTML Java Script JSP and developed business Logic and interfacing components using Business Objects JDBC and XML Participated in user requirement sessions to analysis and gather Business requirements Development of user visible site using Perl back end admin sites using Python and big data using core java Involved in development of the application using Spring Web MVC and other components of the Elaborated Use Cases based on business requirements and was responsible for creation of class Diagrams Sequence Diagrams Implemented Objectrelation mapping in the persistence layer using Hibernate ORM framework Implemented REST Web Services with Jersey API to deal with customer requests Experienced in developing Restful web services consumed and also produced Used Hibernate for the Database connection and Hibernate Query Language HQL to add and retrieve the information from the Database Implemented Spring JDBC for connecting Oracle database Designed the application using MVC framework for easy maintainability Provided bug fixing and testing for existing web applications Involved in full system life cycle and responsible for Developing Testing Implementing Involved in Unit Testing Integration Testing and System Testing Implemented Form Beans and their Validations Written Hibernate components Developed client side validations with Java script Environment Spring JSP Servlets REST Oracle AJAX Java Script JQuery Hibernate Web Logic Log4j HTML XML CVS Eclipse SOAP Web Services XSLT XSD UNIX Maven Jenkins shell scripting MVS ISPF Education Bachelors Skills Amazon web services Git Hbase Hdfs Hive Javascript Jenkins Json Pig Python Scripting Svn Zookeeper Cassandra Impala Oozie Sqoop Hbase Kafka Db2",
    "unique_id": "693d260b-b922-4aa4-a298-1b0814366095"
}