{
    "clean_data": "Run Lead Big Data Platform Run Lead Big Data Platform Run Lead Intel Salt Lake City UT Over 5 years of comprehensive IT experience in BigData and BigData Analytics Hadoop HDFS MapReduce YARN Hadoop Ecosystem and Shell Scripting 5 years of development experience using Java J2EE JSP and Servlets Highly capable for processing large sets of Structured Semistructured and Unstructured datasets and supporting BigData applications Hands on experience with Hadoop Ecosystem components like Map Reduce Processing HDFS Storage YARN Sqoop Pig Hive HBase Oozie ZooKeeper and Spark for data storage and analysis Expertise in transferring data between a Hadoop ecosystem and structured data storage in a RDBMS such as MY SQL Oracle Teradata and DB2 using Sqoop Experience in NoSQL databases like Mongo DB HBase and Cassandra Have excellent knowledge on Python Collections and MultiThreading Skilled experience in Python with proven expertise in using new tools and technical developments Experience in Apache Spark cluster and streams processing using Spark Streaming Worked on several python packages like numpy scipy pytables etc Expertise in moving large amounts of log streaming event data and Transactional data using Flume Experience in developing MapReduce jobs in Java for data cleaning and preprocessing Expertise in writing Pig Latin HiveScripts and extended their functionality using User Defined Functions UDFs Expertise in handling structured arrangement of data within certain limits Data Layouts using Partitions and Bucketing in Hive Expertise in preparing interactive Data Visualizations using Tableau Software from different sources Hands on experience in developing workflows that execute MapReduce Sqoop Pig Hive and Shell scripts using Oozie Experience working with Cloudera Hue Interface and Impala Hands on experience developing Solr Indexes using MapReduceIndexer Tool Expertise in Objectoriented analysis and design OOAD like UML and use of various design patterns Experience in Java JSP Servlets EJB Web Logic Web Sphere Hibernate SpringJBoss JDBC RMI Java Script Ajax Jquery XML and HTML Fluent with the core Java concepts like IO Multithreading Exceptions RegEx Data Structures and Serialization Performed unit testing using Junit Testing Framework and Log4J to monitor the error logs Good Knowledge of Python and Python Web Framework Django Experienced with Python frameworks like Webapp2 and Flask Experience in process improvement normalizationdenormalization data extraction cleansing and manipulation Converting requirement specification Source system understanding into Conceptual Logical and physical Data Model Data flow DFD Expertise in working with transactional databases like Oracle SQL server My SQL and Db2 Expertise in developing SQL queries Stored Procedures and excellent development experience with Agile Methodology Ability to adapt to evolving technology strong sense of responsibility and accomplishment Excellent leadership interpersonal problem solving and time management skills Excellent communication skills both Written documentation and Verbal presentation Authorized to work in the US for any employer Work Experience Run Lead Big Data Platform Intel Austin TX June 2015 to Present Manage onshore and offshore Hadoop teams consisting of Developers Architects Data Engineers and Administrators Report to stakeholders on biweekly basis on the overall health of the Enterprise Big Data platform Oversee Synchrony core 5 Hadoop environments significantly ingesting large datasets sourced from realtime feeds variety user interactions with mobile apps and internal transactions Single point of contact for all platform operations Run issues from immediate response coordination escalation root cause analysis and resolutionEnsuring SLAs for availability performance security maintenance upgrades installation and user administration across all Data Lake environments Administer and maintain PivotalHortonworks Hadoop Greenplum and GemFire clusters across all environments Installationconfiguration of the Hadoop ecosystem tools and continuous enhancement and expansion of the enterprise data lake Collaborate with development teams to assist in code promotion across environments and deployments in production including CMDB CI creation and updates Proactively monitor cluster health and perform performance tuning activities Perform capacity planning and expansion activities working across Infrastructure and other enterprise service teams Perform cluster maintenance with patchingupgradesmigration user provisioning automation of routine tasks reprocessing of failed jobs configure and maintain security policies Ensure the Enterprise Data Lake initiative is continually providing unprecedented customer 360 capabilities and associated services at all time Certified HDP Hadoop Administrator HealthScape Advisors LLC Chicago IL February 2013 to May 2015 Cluster planning and engineering of POC and Production Clusters Strong experience on Hadoop distributions Hortonworks Cloudera Administer troubleshoot and debug cluster problems on RHELUbuntu Troubleshoot and debugs cluster problems on RHEL based Linux infrastructure Kerberos AdministratorKerberize cluster and ensure sound domain name torealm mapping Periodically Check HDFS health Configure YARN Capacity Scheduler based on infrastructure needsYARN tuning Perform upgrades patches and fixes using effective rollout method Ensure HDFS is Balanced and performing optimally at all times CommissionDecommission Hadoop cluster nodes Review namenode WebUI for information concerning Datanode volume failures Backup existing Hadoop databases such as oozie hive metastore HDFS Metadata backup etc Active DirectoryLDAPS integration and management Purge older log files Build cluster according to workload patterncluster type Configure Cluster services for High Availability Ensure volume and hdfs encryptionData at rest encryption Experienced in AWS configuration optimization for Hadoop Backup Procedures and Disaster Recovery ACLs and audits to meet compliance specifications using Ranger Open tickets and troubleshoot cluster problems with support Experienced in AWS Storage methodologies Expertise in cloudbreak setupconfiguration and blueprint creation Engineer data pipeline management using Falcon Definingexecuting feeds processes data pipelines jobs mirroring between production and testing cluster using Falcon ETL offload using Atlas Securing hadoop services Webhdfshiveyarn using Knox for REST API calls Trains and leads teams in understanding and implementing hadoop solutions in organization Monitor clusterorganization servers for intrusionsdetection log file reviews and threats using SIEM solution Alien vault Documents and reports findings to compliance and audit teams Perform Active directorywindows server administration Big Data Analyst Lorven Technologies August 2010 to January 2013 Worked with most Hadoop distributions Hortonworks Developed data governance process and controls and ensured compliance with enterprise data architecture principles and standards for the various systems and components Analyzing profiling data for quality and reconciling data issues Build test and deploy Hadoop solutions using most Hadoop ecosystem components Administer Hadoop cluster monitor performance with Ganglia Design robust Hadoop solutions for complex business problems Utilize new and latest Open Source tools for addressing business challenges Work with multiple customer teams and support teams to execute Hadoop engagements Integrated Hadoop into traditional ETL accelerating the extraction transformation and loading of massive structured and unstructured data Python developer GAD GROUP TECHNOLOGY INC Chicago IL June 2009 to July 2010 Build commandline app with Restful API layers that cater for end users who are mostly kids Pull data out of HTML and XML file with ScrapyBeautiful soup Interacting with webapp using Flask Performed Data analysis using Python Pandas Processing Data records and returning computed results using Mongo DB Aggregation framework Parse aggregated data into Apache Solr and graph database Orient DB IT Analyst Programmer Logos Infotech Inc St Louis MO April 2007 to May 2009 Worked both independently and in a teamoriented collaborative environment Worked with Microsoft SQL server Documented and provided status of project and technical information related to the applicationsoftware supported  NET Supported remote users at their home office hotel or customer site utilizing remote tools and troubleshooting over phone using VPN Knowledge of DSLCable Modem Routers Windows Server 2012 and 2008 CISCO Switches and Routers and VOIP ie Cisco Call Manager Education Bachelors Skills APACHE HADOOP HDFS 2 years APACHE HADOOP OOZIE 2 years ETL 4 years EXTRACT TRANSFORM AND LOAD 4 years Hadoop 7 years Additional Information Technical Skills Operating Systems Linux windows Mac OS Unix Cloud Techs AWS EC2 Elasticsearch Cloudwatch EMR Hadoop Eco System HDFS MapReduce Pig Hive SparkSQL HBase Apache Crunch Solr Sqoop Spark Streaming Spark Oozie Zookeeper Hue AVR0 Cassandra MongoDB Database Servers HBAse MongoDB DynamoDB Cassandra DB2 Teradata MYSQL Oracle MS SQL Server 12 ETL Design Tools Teradata Load Utilities BTEQ Fast Multi load SSIS Informatica Report DesignTool SSRS Tableau Application Servers JBoss 7x WebSphere 6x WebLogic 11g JBoss 50 Programming Languages Java Shell Scripts Scala Python and R Web Technologies Servlets JSP JSTL JDBC IDEs Eclipse Netbeans RAD Jdeveloper TOAD SQL Developer Frameworks Spring Struts Hibernate 4x3x Hadoop Impala",
    "entities": [
        "Infrastructure",
        "User Defined Functions UDFs Expertise",
        "Falcon",
        "Lake City UT",
        "ScrapyBeautiful",
        "Tableau Software",
        "Conceptual Logical",
        "Infotech Inc",
        "Active DirectoryLDAPS",
        "Data Layouts using Partitions",
        "Solr Sqoop Spark",
        "Configure Cluster",
        "Data Lake",
        "Certified HDP Hadoop Administrator HealthScape Advisors",
        "Periodically Check",
        "Engineer",
        "Hadoop Ecosystem",
        "Oracle MS",
        "Hadoop",
        "XML",
        "IO Multithreading Exceptions RegEx Data Structures",
        "Additional Information Technical Skills Operating Systems Linux",
        "apps",
        "WebLogic",
        "Flask Performed Data",
        "Shell",
        "Apache Spark",
        "NET Supported",
        "TX",
        "SSIS",
        "HTML Fluent",
        "WebSphere",
        "Junit Testing Framework",
        "Data Model Data",
        "UML",
        "Structured Semistructured",
        "Ranger Open",
        "the Enterprise Big Data",
        "Review",
        "Linux",
        "Falcon Definingexecuting",
        "Unstructured",
        "Flask",
        "GAD GROUP TECHNOLOGY INC",
        "Python Pandas Processing Data",
        "BigData",
        "Developers Architects Data Engineers and Administrators Report",
        "Spark",
        "Restful API",
        "US",
        "Perform",
        "Sqoop",
        "Purge",
        "Oozie Experience",
        "AWS",
        "Verbal",
        "MY SQL Oracle Teradata",
        "Elasticsearch Cloudwatch EMR Hadoop Eco System",
        "VOIP",
        "CommissionDecommission Hadoop",
        "Serialization Performed",
        "HTML",
        "SQL",
        "Oracle SQL",
        "Transactional",
        "Atlas Securing",
        "Hortonworks Developed",
        "Administer Hadoop",
        "Chicago",
        "Mongo DB Aggregation",
        "Big Data",
        "Integrated Hadoop",
        "ETL",
        "Hortonworks Cloudera Administer",
        "Build",
        "Data Visualizations",
        "PivotalHortonworks Hadoop Greenplum",
        "Impala",
        "Cisco",
        "Microsoft",
        "Agile Methodology Ability",
        "GemFire",
        "Perform Active",
        "MapReduce Sqoop",
        "Informatica Report DesignTool SSRS Tableau Application Servers",
        "Parse",
        "Java JSP Servlets",
        "Servlets Highly",
        "BigData Analytics Hadoop",
        "MapReduce",
        "Impala Hands",
        "NoSQL",
        "Hadoop Backup Procedures",
        "Ganglia Design"
    ],
    "experience": "Experience in NoSQL databases like Mongo DB HBase and Cassandra Have excellent knowledge on Python Collections and MultiThreading Skilled experience in Python with proven expertise in using new tools and technical developments Experience in Apache Spark cluster and streams processing using Spark Streaming Worked on several python packages like numpy scipy pytables etc Expertise in moving large amounts of log streaming event data and Transactional data using Flume Experience in developing MapReduce jobs in Java for data cleaning and preprocessing Expertise in writing Pig Latin HiveScripts and extended their functionality using User Defined Functions UDFs Expertise in handling structured arrangement of data within certain limits Data Layouts using Partitions and Bucketing in Hive Expertise in preparing interactive Data Visualizations using Tableau Software from different sources Hands on experience in developing workflows that execute MapReduce Sqoop Pig Hive and Shell scripts using Oozie Experience working with Cloudera Hue Interface and Impala Hands on experience developing Solr Indexes using MapReduceIndexer Tool Expertise in Objectoriented analysis and design OOAD like UML and use of various design patterns Experience in Java JSP Servlets EJB Web Logic Web Sphere Hibernate SpringJBoss JDBC RMI Java Script Ajax Jquery XML and HTML Fluent with the core Java concepts like IO Multithreading Exceptions RegEx Data Structures and Serialization Performed unit testing using Junit Testing Framework and Log4J to monitor the error logs Good Knowledge of Python and Python Web Framework Django Experienced with Python frameworks like Webapp2 and Flask Experience in process improvement normalizationdenormalization data extraction cleansing and manipulation Converting requirement specification Source system understanding into Conceptual Logical and physical Data Model Data flow DFD Expertise in working with transactional databases like Oracle SQL server My SQL and Db2 Expertise in developing SQL queries Stored Procedures and excellent development experience with Agile Methodology Ability to adapt to evolving technology strong sense of responsibility and accomplishment Excellent leadership interpersonal problem solving and time management skills Excellent communication skills both Written documentation and Verbal presentation Authorized to work in the US for any employer Work Experience Run Lead Big Data Platform Intel Austin TX June 2015 to Present Manage onshore and offshore Hadoop teams consisting of Developers Architects Data Engineers and Administrators Report to stakeholders on biweekly basis on the overall health of the Enterprise Big Data platform Oversee Synchrony core 5 Hadoop environments significantly ingesting large datasets sourced from realtime feeds variety user interactions with mobile apps and internal transactions Single point of contact for all platform operations Run issues from immediate response coordination escalation root cause analysis and resolutionEnsuring SLAs for availability performance security maintenance upgrades installation and user administration across all Data Lake environments Administer and maintain PivotalHortonworks Hadoop Greenplum and GemFire clusters across all environments Installationconfiguration of the Hadoop ecosystem tools and continuous enhancement and expansion of the enterprise data lake Collaborate with development teams to assist in code promotion across environments and deployments in production including CMDB CI creation and updates Proactively monitor cluster health and perform performance tuning activities Perform capacity planning and expansion activities working across Infrastructure and other enterprise service teams Perform cluster maintenance with patchingupgradesmigration user provisioning automation of routine tasks reprocessing of failed jobs configure and maintain security policies Ensure the Enterprise Data Lake initiative is continually providing unprecedented customer 360 capabilities and associated services at all time Certified HDP Hadoop Administrator HealthScape Advisors LLC Chicago IL February 2013 to May 2015 Cluster planning and engineering of POC and Production Clusters Strong experience on Hadoop distributions Hortonworks Cloudera Administer troubleshoot and debug cluster problems on RHELUbuntu Troubleshoot and debugs cluster problems on RHEL based Linux infrastructure Kerberos AdministratorKerberize cluster and ensure sound domain name torealm mapping Periodically Check HDFS health Configure YARN Capacity Scheduler based on infrastructure needsYARN tuning Perform upgrades patches and fixes using effective rollout method Ensure HDFS is Balanced and performing optimally at all times CommissionDecommission Hadoop cluster nodes Review namenode WebUI for information concerning Datanode volume failures Backup existing Hadoop databases such as oozie hive metastore HDFS Metadata backup etc Active DirectoryLDAPS integration and management Purge older log files Build cluster according to workload patterncluster type Configure Cluster services for High Availability Ensure volume and hdfs encryptionData at rest encryption Experienced in AWS configuration optimization for Hadoop Backup Procedures and Disaster Recovery ACLs and audits to meet compliance specifications using Ranger Open tickets and troubleshoot cluster problems with support Experienced in AWS Storage methodologies Expertise in cloudbreak setupconfiguration and blueprint creation Engineer data pipeline management using Falcon Definingexecuting feeds processes data pipelines jobs mirroring between production and testing cluster using Falcon ETL offload using Atlas Securing hadoop services Webhdfshiveyarn using Knox for REST API calls Trains and leads teams in understanding and implementing hadoop solutions in organization Monitor clusterorganization servers for intrusionsdetection log file reviews and threats using SIEM solution Alien vault Documents and reports findings to compliance and audit teams Perform Active directorywindows server administration Big Data Analyst Lorven Technologies August 2010 to January 2013 Worked with most Hadoop distributions Hortonworks Developed data governance process and controls and ensured compliance with enterprise data architecture principles and standards for the various systems and components Analyzing profiling data for quality and reconciling data issues Build test and deploy Hadoop solutions using most Hadoop ecosystem components Administer Hadoop cluster monitor performance with Ganglia Design robust Hadoop solutions for complex business problems Utilize new and latest Open Source tools for addressing business challenges Work with multiple customer teams and support teams to execute Hadoop engagements Integrated Hadoop into traditional ETL accelerating the extraction transformation and loading of massive structured and unstructured data Python developer GAD GROUP TECHNOLOGY INC Chicago IL June 2009 to July 2010 Build commandline app with Restful API layers that cater for end users who are mostly kids Pull data out of HTML and XML file with ScrapyBeautiful soup Interacting with webapp using Flask Performed Data analysis using Python Pandas Processing Data records and returning computed results using Mongo DB Aggregation framework Parse aggregated data into Apache Solr and graph database Orient DB IT Analyst Programmer Logos Infotech Inc St Louis MO April 2007 to May 2009 Worked both independently and in a teamoriented collaborative environment Worked with Microsoft SQL server Documented and provided status of project and technical information related to the applicationsoftware supported   NET Supported remote users at their home office hotel or customer site utilizing remote tools and troubleshooting over phone using VPN Knowledge of DSLCable Modem Routers Windows Server 2012 and 2008 CISCO Switches and Routers and VOIP ie Cisco Call Manager Education Bachelors Skills APACHE HADOOP HDFS 2 years APACHE HADOOP OOZIE 2 years ETL 4 years EXTRACT TRANSFORM AND LOAD 4 years Hadoop 7 years Additional Information Technical Skills Operating Systems Linux windows Mac OS Unix Cloud Techs AWS EC2 Elasticsearch Cloudwatch EMR Hadoop Eco System HDFS MapReduce Pig Hive SparkSQL HBase Apache Crunch Solr Sqoop Spark Streaming Spark Oozie Zookeeper Hue AVR0 Cassandra MongoDB Database Servers HBAse MongoDB DynamoDB Cassandra DB2 Teradata MYSQL Oracle MS SQL Server 12 ETL Design Tools Teradata Load Utilities BTEQ Fast Multi load SSIS Informatica Report DesignTool SSRS Tableau Application Servers JBoss 7x WebSphere 6x WebLogic 11 g JBoss 50 Programming Languages Java Shell Scripts Scala Python and R Web Technologies Servlets JSP JSTL JDBC IDEs Eclipse Netbeans RAD Jdeveloper TOAD SQL Developer Frameworks Spring Struts Hibernate 4x3x Hadoop Impala",
    "extracted_keywords": [
        "Lead",
        "Big",
        "Data",
        "Platform",
        "Run",
        "Lead",
        "Big",
        "Data",
        "Platform",
        "Run",
        "Lead",
        "Intel",
        "Salt",
        "Lake",
        "City",
        "UT",
        "years",
        "IT",
        "experience",
        "BigData",
        "BigData",
        "Analytics",
        "Hadoop",
        "HDFS",
        "MapReduce",
        "YARN",
        "Hadoop",
        "Ecosystem",
        "Shell",
        "Scripting",
        "years",
        "development",
        "experience",
        "Java",
        "J2EE",
        "JSP",
        "Servlets",
        "sets",
        "Structured",
        "Semistructured",
        "datasets",
        "BigData",
        "applications",
        "Hands",
        "experience",
        "Hadoop",
        "Ecosystem",
        "components",
        "Map",
        "Reduce",
        "Processing",
        "HDFS",
        "Storage",
        "YARN",
        "Sqoop",
        "Pig",
        "Hive",
        "HBase",
        "Oozie",
        "ZooKeeper",
        "Spark",
        "data",
        "storage",
        "analysis",
        "Expertise",
        "data",
        "Hadoop",
        "ecosystem",
        "data",
        "storage",
        "RDBMS",
        "MY",
        "SQL",
        "Oracle",
        "Teradata",
        "DB2",
        "Sqoop",
        "Experience",
        "NoSQL",
        "databases",
        "Mongo",
        "DB",
        "HBase",
        "Cassandra",
        "knowledge",
        "Python",
        "Collections",
        "MultiThreading",
        "experience",
        "Python",
        "expertise",
        "tools",
        "developments",
        "Experience",
        "Apache",
        "Spark",
        "cluster",
        "streams",
        "processing",
        "Spark",
        "Streaming",
        "python",
        "packages",
        "pytables",
        "Expertise",
        "amounts",
        "log",
        "event",
        "data",
        "data",
        "Flume",
        "Experience",
        "MapReduce",
        "jobs",
        "Java",
        "data",
        "Expertise",
        "Pig",
        "Latin",
        "HiveScripts",
        "functionality",
        "User",
        "Defined",
        "Functions",
        "UDFs",
        "Expertise",
        "arrangement",
        "data",
        "limits",
        "Data",
        "Layouts",
        "Partitions",
        "Bucketing",
        "Hive",
        "Expertise",
        "Data",
        "Visualizations",
        "Tableau",
        "Software",
        "sources",
        "Hands",
        "experience",
        "workflows",
        "MapReduce",
        "Sqoop",
        "Pig",
        "Hive",
        "Shell",
        "scripts",
        "Oozie",
        "Experience",
        "Cloudera",
        "Hue",
        "Interface",
        "Impala",
        "Hands",
        "experience",
        "Solr",
        "Indexes",
        "MapReduceIndexer",
        "Tool",
        "Expertise",
        "analysis",
        "design",
        "OOAD",
        "UML",
        "use",
        "design",
        "patterns",
        "Experience",
        "Java",
        "JSP",
        "Servlets",
        "EJB",
        "Web",
        "Logic",
        "Web",
        "Sphere",
        "Hibernate",
        "SpringJBoss",
        "JDBC",
        "RMI",
        "Java",
        "Script",
        "Ajax",
        "Jquery",
        "XML",
        "HTML",
        "Fluent",
        "core",
        "Java",
        "concepts",
        "IO",
        "Multithreading",
        "Exceptions",
        "RegEx",
        "Data",
        "Structures",
        "Serialization",
        "Performed",
        "unit",
        "testing",
        "Junit",
        "Testing",
        "Framework",
        "Log4J",
        "error",
        "Good",
        "Knowledge",
        "Python",
        "Python",
        "Web",
        "Framework",
        "Django",
        "Python",
        "frameworks",
        "Webapp2",
        "Flask",
        "Experience",
        "process",
        "improvement",
        "normalizationdenormalization",
        "data",
        "extraction",
        "cleansing",
        "manipulation",
        "requirement",
        "specification",
        "Source",
        "system",
        "Conceptual",
        "Logical",
        "physical",
        "Data",
        "Model",
        "Data",
        "DFD",
        "Expertise",
        "databases",
        "Oracle",
        "SQL",
        "server",
        "SQL",
        "Db2",
        "Expertise",
        "SQL",
        "Procedures",
        "development",
        "experience",
        "Agile",
        "Methodology",
        "Ability",
        "technology",
        "sense",
        "responsibility",
        "accomplishment",
        "Excellent",
        "leadership",
        "problem",
        "time",
        "management",
        "communication",
        "skills",
        "documentation",
        "presentation",
        "US",
        "employer",
        "Work",
        "Experience",
        "Run",
        "Lead",
        "Big",
        "Data",
        "Platform",
        "Intel",
        "Austin",
        "TX",
        "June",
        "Present",
        "Manage",
        "Hadoop",
        "teams",
        "Developers",
        "Architects",
        "Data",
        "Engineers",
        "Administrators",
        "Report",
        "stakeholders",
        "basis",
        "health",
        "Enterprise",
        "Big",
        "Data",
        "platform",
        "Oversee",
        "Synchrony",
        "core",
        "Hadoop",
        "environments",
        "datasets",
        "feeds",
        "variety",
        "user",
        "interactions",
        "apps",
        "transactions",
        "point",
        "contact",
        "platform",
        "operations",
        "Run",
        "issues",
        "response",
        "coordination",
        "escalation",
        "root",
        "analysis",
        "SLAs",
        "availability",
        "performance",
        "security",
        "maintenance",
        "upgrades",
        "installation",
        "user",
        "administration",
        "Data",
        "Lake",
        "PivotalHortonworks",
        "Hadoop",
        "Greenplum",
        "GemFire",
        "clusters",
        "environments",
        "Installationconfiguration",
        "Hadoop",
        "ecosystem",
        "tools",
        "enhancement",
        "expansion",
        "enterprise",
        "data",
        "lake",
        "Collaborate",
        "development",
        "teams",
        "code",
        "promotion",
        "environments",
        "deployments",
        "production",
        "CMDB",
        "CI",
        "creation",
        "updates",
        "cluster",
        "health",
        "performance",
        "tuning",
        "activities",
        "capacity",
        "planning",
        "expansion",
        "activities",
        "Infrastructure",
        "enterprise",
        "service",
        "teams",
        "cluster",
        "maintenance",
        "patchingupgradesmigration",
        "user",
        "automation",
        "tasks",
        "jobs",
        "security",
        "policies",
        "Enterprise",
        "Data",
        "Lake",
        "initiative",
        "customer",
        "capabilities",
        "services",
        "time",
        "HDP",
        "Hadoop",
        "Administrator",
        "HealthScape",
        "Advisors",
        "LLC",
        "Chicago",
        "IL",
        "February",
        "May",
        "Cluster",
        "planning",
        "engineering",
        "POC",
        "Production",
        "Clusters",
        "Strong",
        "experience",
        "Hadoop",
        "distributions",
        "Hortonworks",
        "Cloudera",
        "Administer",
        "troubleshoot",
        "cluster",
        "problems",
        "RHELUbuntu",
        "Troubleshoot",
        "debugs",
        "cluster",
        "problems",
        "RHEL",
        "Linux",
        "infrastructure",
        "Kerberos",
        "AdministratorKerberize",
        "cluster",
        "domain",
        "name",
        "torealm",
        "mapping",
        "HDFS",
        "health",
        "Configure",
        "YARN",
        "Capacity",
        "Scheduler",
        "infrastructure",
        "Perform",
        "upgrades",
        "patches",
        "fixes",
        "rollout",
        "method",
        "Ensure",
        "HDFS",
        "times",
        "CommissionDecommission",
        "Hadoop",
        "cluster",
        "nodes",
        "Review",
        "WebUI",
        "information",
        "Datanode",
        "volume",
        "Backup",
        "Hadoop",
        "databases",
        "oozie",
        "hive",
        "metastore",
        "HDFS",
        "Metadata",
        "backup",
        "DirectoryLDAPS",
        "integration",
        "management",
        "Purge",
        "log",
        "Build",
        "cluster",
        "workload",
        "patterncluster",
        "type",
        "Configure",
        "Cluster",
        "services",
        "High",
        "Availability",
        "Ensure",
        "volume",
        "hdfs",
        "encryptionData",
        "rest",
        "encryption",
        "AWS",
        "configuration",
        "optimization",
        "Hadoop",
        "Backup",
        "Procedures",
        "Disaster",
        "Recovery",
        "ACLs",
        "audits",
        "compliance",
        "specifications",
        "Ranger",
        "Open",
        "tickets",
        "troubleshoot",
        "cluster",
        "problems",
        "support",
        "AWS",
        "Storage",
        "methodologies",
        "Expertise",
        "cloudbreak",
        "setupconfiguration",
        "blueprint",
        "creation",
        "Engineer",
        "data",
        "pipeline",
        "management",
        "Falcon",
        "Definingexecuting",
        "feeds",
        "data",
        "pipelines",
        "jobs",
        "production",
        "testing",
        "cluster",
        "Falcon",
        "ETL",
        "offload",
        "Atlas",
        "Securing",
        "hadoop",
        "services",
        "Webhdfshiveyarn",
        "Knox",
        "REST",
        "API",
        "Trains",
        "teams",
        "understanding",
        "hadoop",
        "solutions",
        "organization",
        "Monitor",
        "clusterorganization",
        "servers",
        "intrusionsdetection",
        "log",
        "file",
        "reviews",
        "threats",
        "SIEM",
        "solution",
        "Alien",
        "vault",
        "Documents",
        "findings",
        "audit",
        "teams",
        "directorywindows",
        "server",
        "administration",
        "Big",
        "Data",
        "Analyst",
        "Lorven",
        "Technologies",
        "August",
        "January",
        "Hadoop",
        "distributions",
        "Hortonworks",
        "data",
        "governance",
        "process",
        "controls",
        "compliance",
        "enterprise",
        "data",
        "architecture",
        "principles",
        "standards",
        "systems",
        "components",
        "profiling",
        "data",
        "quality",
        "reconciling",
        "data",
        "issues",
        "test",
        "Hadoop",
        "solutions",
        "Hadoop",
        "ecosystem",
        "components",
        "Administer",
        "Hadoop",
        "cluster",
        "monitor",
        "performance",
        "Ganglia",
        "Design",
        "Hadoop",
        "solutions",
        "business",
        "problems",
        "Utilize",
        "Open",
        "Source",
        "tools",
        "business",
        "challenges",
        "customer",
        "teams",
        "support",
        "teams",
        "Hadoop",
        "engagements",
        "Integrated",
        "Hadoop",
        "ETL",
        "extraction",
        "transformation",
        "loading",
        "data",
        "Python",
        "developer",
        "GAD",
        "GROUP",
        "TECHNOLOGY",
        "INC",
        "Chicago",
        "IL",
        "June",
        "July",
        "Build",
        "app",
        "API",
        "layers",
        "end",
        "users",
        "kids",
        "data",
        "HTML",
        "XML",
        "file",
        "ScrapyBeautiful",
        "soup",
        "webapp",
        "Flask",
        "Performed",
        "Data",
        "analysis",
        "Python",
        "Pandas",
        "Processing",
        "Data",
        "records",
        "results",
        "Mongo",
        "DB",
        "Aggregation",
        "framework",
        "Parse",
        "data",
        "Apache",
        "Solr",
        "graph",
        "database",
        "Orient",
        "DB",
        "IT",
        "Analyst",
        "Programmer",
        "Logos",
        "Infotech",
        "Inc",
        "St",
        "Louis",
        "MO",
        "April",
        "May",
        "collaborative",
        "environment",
        "Microsoft",
        "SQL",
        "server",
        "status",
        "project",
        "information",
        "applicationsoftware",
        "NET",
        "users",
        "home",
        "office",
        "hotel",
        "customer",
        "site",
        "tools",
        "troubleshooting",
        "phone",
        "VPN",
        "Knowledge",
        "DSLCable",
        "Modem",
        "Routers",
        "Windows",
        "Server",
        "CISCO",
        "Switches",
        "Routers",
        "VOIP",
        "Cisco",
        "Call",
        "Manager",
        "Education",
        "Bachelors",
        "Skills",
        "APACHE",
        "HADOOP",
        "HDFS",
        "years",
        "APACHE",
        "HADOOP",
        "OOZIE",
        "years",
        "ETL",
        "years",
        "EXTRACT",
        "TRANSFORM",
        "years",
        "Hadoop",
        "years",
        "Additional",
        "Information",
        "Technical",
        "Skills",
        "Operating",
        "Systems",
        "Linux",
        "Mac",
        "OS",
        "Unix",
        "Cloud",
        "Techs",
        "AWS",
        "EC2",
        "Elasticsearch",
        "Cloudwatch",
        "EMR",
        "Hadoop",
        "Eco",
        "System",
        "HDFS",
        "MapReduce",
        "Pig",
        "Hive",
        "SparkSQL",
        "HBase",
        "Apache",
        "Crunch",
        "Solr",
        "Sqoop",
        "Spark",
        "Streaming",
        "Spark",
        "Oozie",
        "Zookeeper",
        "Hue",
        "AVR0",
        "Cassandra",
        "MongoDB",
        "Database",
        "Servers",
        "HBAse",
        "DynamoDB",
        "Cassandra",
        "DB2",
        "Teradata",
        "MYSQL",
        "Oracle",
        "MS",
        "SQL",
        "Server",
        "ETL",
        "Design",
        "Tools",
        "Teradata",
        "Load",
        "Utilities",
        "BTEQ",
        "Multi",
        "load",
        "SSIS",
        "Informatica",
        "Report",
        "DesignTool",
        "SSRS",
        "Tableau",
        "Application",
        "Servers",
        "JBoss",
        "7x",
        "WebSphere",
        "6x",
        "WebLogic",
        "g",
        "JBoss",
        "Programming",
        "Languages",
        "Java",
        "Shell",
        "Scripts",
        "Scala",
        "Python",
        "R",
        "Web",
        "Technologies",
        "Servlets",
        "JSP",
        "JSTL",
        "JDBC",
        "IDEs",
        "Eclipse",
        "Netbeans",
        "RAD",
        "Jdeveloper",
        "TOAD",
        "SQL",
        "Developer",
        "Frameworks",
        "Spring",
        "Struts",
        "Hibernate",
        "4x3x",
        "Hadoop",
        "Impala"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T21:11:14.036095",
    "resume_data": "Run Lead Big Data Platform Run Lead Big Data Platform Run Lead Intel Salt Lake City UT Over 5 years of comprehensive IT experience in BigData and BigData Analytics Hadoop HDFS MapReduce YARN Hadoop Ecosystem and Shell Scripting 5 years of development experience using Java J2EE JSP and Servlets Highly capable for processing large sets of Structured Semistructured and Unstructured datasets and supporting BigData applications Hands on experience with Hadoop Ecosystem components like Map Reduce Processing HDFS Storage YARN Sqoop Pig Hive HBase Oozie ZooKeeper and Spark for data storage and analysis Expertise in transferring data between a Hadoop ecosystem and structured data storage in a RDBMS such as MY SQL Oracle Teradata and DB2 using Sqoop Experience in NoSQL databases like Mongo DB HBase and Cassandra Have excellent knowledge on Python Collections and MultiThreading Skilled experience in Python with proven expertise in using new tools and technical developments Experience in Apache Spark cluster and streams processing using Spark Streaming Worked on several python packages like numpy scipy pytables etc Expertise in moving large amounts of log streaming event data and Transactional data using Flume Experience in developing MapReduce jobs in Java for data cleaning and preprocessing Expertise in writing Pig Latin HiveScripts and extended their functionality using User Defined Functions UDFs Expertise in handling structured arrangement of data within certain limits Data Layouts using Partitions and Bucketing in Hive Expertise in preparing interactive Data Visualizations using Tableau Software from different sources Hands on experience in developing workflows that execute MapReduce Sqoop Pig Hive and Shell scripts using Oozie Experience working with Cloudera Hue Interface and Impala Hands on experience developing Solr Indexes using MapReduceIndexer Tool Expertise in Objectoriented analysis and design OOAD like UML and use of various design patterns Experience in Java JSP Servlets EJB Web Logic Web Sphere Hibernate SpringJBoss JDBC RMI Java Script Ajax Jquery XML and HTML Fluent with the core Java concepts like IO Multithreading Exceptions RegEx Data Structures and Serialization Performed unit testing using Junit Testing Framework and Log4J to monitor the error logs Good Knowledge of Python and Python Web Framework Django Experienced with Python frameworks like Webapp2 and Flask Experience in process improvement normalizationdenormalization data extraction cleansing and manipulation Converting requirement specification Source system understanding into Conceptual Logical and physical Data Model Data flow DFD Expertise in working with transactional databases like Oracle SQL server My SQL and Db2 Expertise in developing SQL queries Stored Procedures and excellent development experience with Agile Methodology Ability to adapt to evolving technology strong sense of responsibility and accomplishment Excellent leadership interpersonal problem solving and time management skills Excellent communication skills both Written documentation and Verbal presentation Authorized to work in the US for any employer Work Experience Run Lead Big Data Platform Intel Austin TX June 2015 to Present Manage onshore and offshore Hadoop teams consisting of Developers Architects Data Engineers and Administrators Report to stakeholders on biweekly basis on the overall health of the Enterprise Big Data platform Oversee Synchrony core 5 Hadoop environments significantly ingesting large datasets sourced from realtime feeds variety user interactions with mobile apps and internal transactions Single point of contact for all platform operations Run issues from immediate response coordination escalation root cause analysis and resolutionEnsuring SLAs for availability performance security maintenance upgrades installation and user administration across all Data Lake environments Administer and maintain PivotalHortonworks Hadoop Greenplum and GemFire clusters across all environments Installationconfiguration of the Hadoop ecosystem tools and continuous enhancement and expansion of the enterprise data lake Collaborate with development teams to assist in code promotion across environments and deployments in production including CMDB CI creation and updates Proactively monitor cluster health and perform performance tuning activities Perform capacity planning and expansion activities working across Infrastructure and other enterprise service teams Perform cluster maintenance with patchingupgradesmigration user provisioning automation of routine tasks reprocessing of failed jobs configure and maintain security policies Ensure the Enterprise Data Lake initiative is continually providing unprecedented customer 360 capabilities and associated services at all time Certified HDP Hadoop Administrator HealthScape Advisors LLC Chicago IL February 2013 to May 2015 Cluster planning and engineering of POC and Production Clusters Strong experience on Hadoop distributions Hortonworks Cloudera Administer troubleshoot and debug cluster problems on RHELUbuntu Troubleshoot and debugs cluster problems on RHEL based Linux infrastructure Kerberos AdministratorKerberize cluster and ensure sound domain name torealm mapping Periodically Check HDFS health Configure YARN Capacity Scheduler based on infrastructure needsYARN tuning Perform upgrades patches and fixes using effective rollout method Ensure HDFS is Balanced and performing optimally at all times CommissionDecommission Hadoop cluster nodes Review namenode WebUI for information concerning Datanode volume failures Backup existing Hadoop databases such as oozie hive metastore HDFS Metadata backup etc Active DirectoryLDAPS integration and management Purge older log files Build cluster according to workload patterncluster type Configure Cluster services for High Availability Ensure volume and hdfs encryptionData at rest encryption Experienced in AWS configuration optimization for Hadoop Backup Procedures and Disaster Recovery ACLs and audits to meet compliance specifications using Ranger Open tickets and troubleshoot cluster problems with support Experienced in AWS Storage methodologies Expertise in cloudbreak setupconfiguration and blueprint creation Engineer data pipeline management using Falcon Definingexecuting feeds processes data pipelines jobs mirroring between production and testing cluster using Falcon ETL offload using Atlas Securing hadoop services Webhdfshiveyarn using Knox for REST API calls Trains and leads teams in understanding and implementing hadoop solutions in organization Monitor clusterorganization servers for intrusionsdetection log file reviews and threats using SIEM solution Alien vault Documents and reports findings to compliance and audit teams Perform Active directorywindows server administration Big Data Analyst Lorven Technologies August 2010 to January 2013 Worked with most Hadoop distributions Hortonworks Developed data governance process and controls and ensured compliance with enterprise data architecture principles and standards for the various systems and components Analyzing profiling data for quality and reconciling data issues Build test and deploy Hadoop solutions using most Hadoop ecosystem components Administer Hadoop cluster monitor performance with Ganglia Design robust Hadoop solutions for complex business problems Utilize new and latest Open Source tools for addressing business challenges Work with multiple customer teams and support teams to execute Hadoop engagements Integrated Hadoop into traditional ETL accelerating the extraction transformation and loading of massive structured and unstructured data Python developer GAD GROUP TECHNOLOGY INC Chicago IL June 2009 to July 2010 Build commandline app with Restful API layers that cater for end users who are mostly kids Pull data out of HTML and XML file with ScrapyBeautiful soup Interacting with webapp using Flask Performed Data analysis using Python Pandas Processing Data records and returning computed results using Mongo DB Aggregation framework Parse aggregated data into Apache Solr and graph database Orient DB IT Analyst Programmer Logos Infotech Inc St Louis MO April 2007 to May 2009 Worked both independently and in a teamoriented collaborative environment Worked with Microsoft SQL server Documented and provided status of project and technical information related to the applicationsoftware supported Web2Py NET Supported remote users at their home office hotel or customer site utilizing remote tools and troubleshooting over phone using VPN Knowledge of DSLCable Modem Routers Windows Server 2012 and 2008 CISCO Switches and Routers and VOIP ie Cisco Call Manager Education Bachelors Skills APACHE HADOOP HDFS 2 years APACHE HADOOP OOZIE 2 years ETL 4 years EXTRACT TRANSFORM AND LOAD 4 years Hadoop 7 years Additional Information Technical Skills Operating Systems Linux windows Mac OS Unix Cloud Techs AWS EC2 Elasticsearch Cloudwatch EMR Hadoop Eco System HDFS MapReduce Pig Hive SparkSQL HBase Apache Crunch Solr Sqoop Spark Streaming Spark Oozie Zookeeper Hue AVR0 Cassandra MongoDB Database Servers HBAse MongoDB DynamoDB Cassandra DB2 Teradata MYSQL Oracle MS SQL Server 200520082012 ETL Design Tools Teradata Load Utilities BTEQ Fast Multi load SSIS Informatica Report DesignTool SSRS Tableau Application Servers JBoss 7x WebSphere 6x WebLogic 11g JBoss 50 Programming Languages Java Shell Scripts Scala Python and R Web Technologies Servlets JSP JSTL JDBC IDEs Eclipse Netbeans RAD Jdeveloper TOAD SQL Developer Frameworks Spring Struts Hibernate 4x3x Hadoop Impala",
    "unique_id": "561bb8b8-ac00-4177-be57-690a31fba995"
}