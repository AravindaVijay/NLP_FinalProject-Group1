{
    "clean_data": "Sr Hadoop Developer Sr Hadoop span lDeveloperspan Sr Hadoop Developer Syniverse Technologies Tempe  Overall 10 years of IT industry experience in product Development Implementation and Maintenance of various cloudbased web applications using Java J2EE technologies and Big Data ecosystems on Linux environment Overall 5 years of experience working with analytics using Big Data technologies Have handson experience in Storing Querying Processing and Data Analysis Experience in importing and exporting data using Sqoop between HDFS and Relational Database Management Systems Populated HDFS with huge amounts of data using Apache Kafka and Flume Excellent knowledge of data mapping extracting transforming and loading from different data sources Worked with different File Formats like TEXTFILE SEQUENCE FILE AVROFILE ORC and PARQUET for Hive querying and processing Experience in developing custom MapReduce Programs in Java using Apache Hadoop for analyzing Big Data as per the requirement Comprehensive work experience in implementing Big Data projects using Apache Hadoop Pig Hive HBase Spark Sqoop Flume Zookeeper Oozie Experience with distributed systems largescale nonrelational data stores and multiterabyte data warehouses Excellent knowledge on Hadoop architecture Hadoop Distributed File system HDFS Job Tracker Task Tracker Name Node Data Node and Map Reduce programming paradigm Handson experience building data pipelines using Hadoop components Sqoop Hive Pig MapReduce Spark Spark SQL Hands on experience in various Big Data application phases like Data Ingestion Data Analytics and Data Visualization Experience working on Hortonworks Cloudera MapR distributions Extensively worked on MRV1 and MRV2 Hadoop architectures Experience working on Spark RDDs DAGs Spark SQL and Spark Streaming Well experienced in data transformation using custom MapReduce Hive and Pig scripts for different types of file formats Expertise in extending Hive and Pig core functionality by writing custom UDFs and UDAFs Designing and creating Hive external tables using shared metastore instead of derby with partitioning dynamic partitioning and buckets Experience building solutions with NoSQL databases such as HBase Cassandra MongoDB Firm grip on data modeling data mapping database performance tuning and NoSQL mapreduce systems Indepth understanding of Spark architecture including Spark Core Spark SQL Data Frames and Spark Streaming Hands on experience migrating complex MapReduce programs into Apache Spark RDD transformations Experienced in Apache Spark for implementing advanced procedures like text analytics and processing using the inmemory computing capabilities written in Scala Experience in Kafka installation integration with Spark Streaming Used Spark Streaming to divide streaming data into batches as an input to Spark engine for batch processing Good Experience in Linux Bash scripting and following PEP Guidelines in Python Involved in converting HiveSQL queries into Spark transformations using Spark RDD in Scala and Python Experienced with different scripting language like Python and shell scripts Handson experience in using python Scripts to handle data manipulation Worked well with Python R scripts for statistics analytics for generating reports for Data Quality Experience in designing both time driven and data driven automated workflows using Oozie Good understanding of ZooKeeper for monitoring and managing Hadoop jobs Good understanding of ETL tools and how they can be applied in a Big Data environment Monitoring Map Reduce Jobs and YARN Applications Handson experience with Amazon Elastic MapReduce EMR Storage S3 EC2 instances and Data Warehousing Experience with RDBMS and writing SQL and PLSQL scripts used in stored procedures Used Git for source code and version control management Proficient in Java J2EE JDBC Collection Framework Servlets JSP Spring Hibernate JSON XML REST SOAP Web Services Strong understanding in Agile and Waterfall SDLC methodologies Excellent problem solving proactive thinking analytical programming and communication skills Ability to learn new technologies and work effectively in crossfunctional team environments Authorized to work in the US for any employer Work Experience Sr Hadoop Developer Syniverse Technologies Tampa FL September 2018 to Present Responsibilities Experience with complete SDLC process staging code reviews source code management and build process Implemented Big Data platforms as data storage retrieval and processing systems Developed data pipeline using Kafka Sqoop Hive and Java MapReduce to ingest customer behavioral data and financial histories into HDFS for analysis Wrote Sqoop scripts for importing and exporting data into HDFS and Hive Wrote MapReduce jobs to discover trends in data usage by the users Load and transform large sets of structured semi structured and unstructured data Pig Experienced working on Pig to do transformations event joins filtering and some preaggregations before storing the data onto HDFS Involved in developing Hive UDFs for the needed functionality that is not available out of the box from Hive Created SubQueries for filtering and faster execution of data Experienced in migrating Hive QL into Impala to minimize query response time Used HCATALOG to access the Hive table metadata from MapReduce and Pig scripts Experience loading and transforming large amounts of structured and unstructured data into HBase and exposure handling Automatic failover in HBase Ran POCs in Spark to take the benchmarking of the implementation Developed Spark jobs using Scala in test environment for faster data processing and querying Worked on migrating MapReduce programs into Spark transformations using Spark and Scala Used Python for pattern matching in build logs to format warnings and errors Configured big data workflows to run on the top of Hadoop using Oozie and these workflows comprises of heterogeneous jobs like Pig Hive Sqoop Cluster coordination services through ZooKeeper Hands on experience in Tableau for Data Visualization and analysis on large data sets drawing various conclusions Involved in developing test framework for data profiling and validation using interactive queries and collected all the test results into audit tables for comparing the results over the period Documented all the requirements code and implementation methodologies for reviewing and analyzation purposes Extensively used GitHub as a code repository and Phabricator for managing day to day development process and to keep track of the issues Environment Java Scala Hadoop Spark HDFS MapReduce Yarn Hive Pig Impala Oozie Sqoop Flume Kafka Teradata SQL GitHub Phabricator Amazon Web Services Hadoop Developer Amtrak Washington DC May 2016 to August 2018 Responsibilities Involved in complete project life cycle starting from design discussion to production deployment Worked closely with the business team to gather their requirements and new support features Involved in running POCs on different use cases of the application and maintained a standard document for best coding practices Developed a 16node cluster in designing the Data Lake with the Hortonworks distribution Responsible for building scalable distributed data solutions using Hadoop Installed configured and implemented high availability Hadoop Clusters with required services HDFS Hive HBase Spark ZooKeeper Implemented Kerberos for authenticating all the services in Hadoop Cluster Configured ZooKeeper to coordinate the servers in clusters to maintain the data consistency Involved in designing the Data pipeline from endtoend to ingest data into the Data Lake Wrote scripts to automate application deployments and configurations monitoring YARN Configured and developed Sqoop scripts to migrate the data from relational databases like Oracle Teradata to HDFS Used Flume for collecting and aggregating large amounts of streaming data into HDFS Wrote MapReduce jobs in Java to parse the raw data populate staging tables and store the refined data Developed Map Reduce programs as a part of predictive analytical model development Built reusable Hive UDF libraries for business requirements which enabled various business analysts to use these UDFs in Hive querying Created different staging tables like ingestion tables and preparation tables in Hive environment Optimized Hive queries and used Hive on top of Spark engine Worked on Sequence files Map side joins Bucketing Static and Dynamic Partitioning for Hive performance enhancement and storage improvement Tested Apache TEZ an extensible framework for building high performance batch and interactive data processing applications on Pig and Hive jobs Involved in converting HiveSQL queries into Spark transformations using Spark RDDs Spark SQL Scala Worked on the Spark core and Spark SQL modules of Spark extensively Developed Python scripts to find vulnerabilities with SQL Queries by doing SQL injection Wrote Python UDFs to process the RegEx and return the valid names using streaming Created tables in HBase to store the variable data formats of data coming from different upstream sources Leveraged AWS cloud services such as EC2 autoscaling and VPC Virtual Private Cloud to build secure highly scalable and flexible systems that handled expected and unexpected load bursts and can quickly evolve during development iterations Developed the batch scripts to fetch the data from AWS S3 storage and do required transformations in Spark framework using Scala Configured various workflows to run on top of Hadoop using Oozie and these workflows comprises of heterogeneous jobs like Pig Hive Sqoop and MapReduce Experience in managing and reviewing Hadoop log files Utilized capabilities of Tableau such as Data extracts Data blending Forecasting Dashboard actions and table calculations to build dashboards Followed Agile Methodologies while working on the project Performed bug fixing and 24X7 production support for running the processes Environment Java Scala Hadoop Hortonworks AWS HDFS YARN Map Reduce Hive Pig Spark Flume Kafka Sqoop Oozie Zookeeper Oracle Teradata MySQL Hadoop Developer Tiffany Co NJ August 2014 to April 2016 Responsibilities Worked on Hortonworks cluster which is responsible for providing open source platform based on Apache Hadoop for analyzing storing and managing big data Worked with analyst to determine and understand business requirements Load and transform large datasets of structured semi structured and unstructured data using HadoopBig Data concepts Developed data pipeline using Flume Sqoop Pig and MapReduce to ingest customer data and financial histories into HDFS for analysis Used MapReduce and Flume to load aggregate store and analyze web log data from different web servers Created MapReduce programs to handle semiunstructured data like XML JSON AVRO data files and sequence files for log files Involved in submitting and tracking MapReduce jobs using Job Tracker Experience writing Pig Latin scripts for Data Cleansing ETL operations and query optimizations of exists scripts Written Hive UDF to sort Structure fields and return complex data types Created Hive tables from JSON data using data serialization framework like AVRO Experience writing reusable custom Hive and Pig UDFs in Java and using existing UDFs from Piggybank and other sources Experience in working with NoSQL database HBase in getting real time data analytics Integrated Hive tables to HBase to perform row level analytics Developed Oozie workflows for daily incremental loads which Sqoops data from Teradata Netezza and then imported into Hive tables Involved in performance tuning by using different service engines like TEZ etc Performed performance tuning and troubleshooting of MapReduce jobs by analyzing and reviewing Hadoop log files Implemented Daily Cron jobs that automate parallel tasks of loading the data into HDFS using AutoSys and Oozie coordinator jobs Developed suit of Unit Test Cases for Mapper Reducer and Driver classes using MR Testing library Environment Hortonworks Java Hadoop HDFS MapReduce Tez Hive Pig Oozie Sqoop Flume Teradata Netezza Tableau Hadoop Developer LakeLand Bank West Milford NJ September 2013 to July 2014 Responsibilities Installed Cloudera distribution of Hadoop Cluster and services HDFS Pig Hive Sqoop Flume and MapReduce Responsible for providing open source platform based on Apache Hadoop for analyzing storing and managing big data Loaded and transformed large sets of structured semistructured and unstructured data Responsible for managing data coming from different sources Imported and exported data into HDFS and Hive using Sqoop Wrote Hive queries Involved in loading data from UNIX file system to HDFS Created Hive tables loaded with data and wrote queries which will run internally in MapReduce and performed data analysis as per the business requirements Worked with analysts to determine and understand business requirements Loaded and transformed large datasets of structured semi structured and unstructured data using HadoopBig Data concepts Developed data pipeline using Flume Sqoop Pig and MapReduce to ingest customer data and financial histories into HDFS for analysis Used MapReduce and Flume to load aggregate store and analyze web log data from different web servers Created MapReduce programs to handle semiunstructured data like XML JSON AVRO data files and sequence files for log files Involved in submitting and tracking MapReduce jobs using Job Tracker Experience writing Pig Latin scripts for Data Cleansing ETL operations and query optimizations of exists scripts Written Hive UDF to sort Structure fields and return complex data types Created Hive tables from JSON data using data serialization framework like AVRO Experience writing reusable custom Hive and Pig UDFs in Java and using existing UDFs from Piggybank and other sources Experience in working with NoSQL database HBase in getting real time data analytics Integrated Hive tables to HBase to perform row level analytics Performed performance tuning and troubleshooting of MapReduce jobs by analyzing and reviewing Hadoop log files Developed Unit Test Cases for Mapper Reducer and Driver classes using MR Testing library Supported operations team in Hadoop cluster maintenance including commissioning and decommissioning nodes and upgrades Provided technical assistance to all development projects Handson experience with Qlik Sense for Data Visualization and Analysis on large data sets drawing various insights Created dashboards using Qlik Sense and performed Data extracts Data blending Forecasting and table calculations Environment Hortonworks Java Hadoop HDFS MapReduce Hive Pig Oozie Sqoop Flume Netezza Qlik Sense Java Developer Trimble Navigation Limited Centreville VA March 2011 to August 2013 Responsibilities Built the application based on Rational Unified Process RUP Analyzed and developed UMLs with Rational Rose including development of class diagrams sequence diagrams use case diagrams and activity diagrams Implemented the MiddleTier employing design patterns like MVC Business Delegate Service Locator Session Faade Data Access Objects DAOs Developed using MVC architecture and employed the Struts Framework and used Validator Framework and Tiles Framework as a plugin with struts Developed user interface using JSP JSP Tag libraries JSTL and Struts Tag Libraries Used Hibernate in data access layer to access and update the information in database Used JSON to pass objects between web pages and serverside application Used XSLFO to generate PDF reports Extensively worked on XML parsers SAXDOM Used WSDL and SOAP protocol for Web Services implementation Used JDBC to access DB2 UDB database for accessing customer information Developed application level logging using Log4J Used CVS for version controlling and Junit for unit testing Involved in development of Tables Indices Stored procedures Database Triggers and Functions Environment J2EE 17 WebSphere Application Server v80 RAD JSP 20 EJB 31 Struts 20 JMS JSON JDBC JNDI XML XSL XSLT XSLFO WSDL SOAP Hibernate 40 RUP Rational Rose 2000 Log4J Junit CVS IBM DB2 v82 Red Hat LINUX RESTful web services Java Developer United Health Group Sacramento CA June 2008 to February 2011 Responsibilities Designed and Developed application using EJB 20 and Struts framework Developed POJOs for Data Model to map the Java Objects with Relational Database tables Designed and developed Service layer using Struts framework Used MVC based Struts framework to develop the multitier web application presentation layer components Implemented Struts tag libraries like HTML logic tab bean etc in the JSP pages Used Struts tiles library for layout of web page and performed struts validations using Struts validation framework Implemented Oracle database and JDBC drivers to access the data Involved in design analysis and architectural meetings created Architecture Diagrams and Flow Charts using Rational Rose Followed Agile software development practice paired with programming test driven development and scrum status meetings Developed use case diagrams class diagrams database tables and mapping between relational database tables Maintained the application configuration information in various properties file Performed unit testing system testing and integration testing Environment Java Struts Framework log4j JBoss Servlets JSP JSTL I18N JDBC HTML JavaScript CSS Rational Rose UML Junit Oracle Windows NT Education Bachelors",
    "entities": [
        "Kafka Sqoop Hive",
        "Hadoop Clusters",
        "MapReduce Hive",
        "Hadoop Cluster Configured ZooKeeper",
        "JBoss Servlets JSP JSTL",
        "multiterabyte",
        "HDFS",
        "UNIX",
        "Developed Spark",
        "Spark Streaming Hands",
        "XSLT XSLFO WSDL SOAP Hibernate",
        "Flume Sqoop Pig",
        "CVS",
        "Indepth",
        "Spark Streaming Used Spark Streaming",
        "RDD",
        "Hadoop",
        "HDFS Involved",
        "Pig Hive Sqoop Cluster",
        "XML",
        "Maintained",
        "Present Responsibilities",
        "Hive UDF",
        "Written Hive UDF",
        "HBase",
        "Apache Spark",
        "Sr Hadoop Developer Sr Hadoop",
        "Implemented Daily Cron",
        "MapReduce Responsible",
        "the Data Lake Wrote",
        "RUP Rational",
        "Rational Unified Process RUP Analyzed",
        "Developed",
        "AWS S3",
        "Node Data",
        "Junit CVS IBM",
        "Integrated Hive",
        "Python Experienced",
        "Teradata Netezza",
        "Work Experience Sr Hadoop Developer",
        "Amazon Elastic MapReduce EMR Storage S3 EC2",
        "PARQUET",
        "Relational Database Management Systems Populated",
        "TEXTFILE SEQUENCE FILE AVROFILE",
        "Storing Querying Processing and Data Analysis",
        "Java J2EE JDBC Collection Framework Servlets",
        "Scala Configured",
        "Linux",
        "HadoopBig Data",
        "Data blending Forecasting",
        "JSP",
        "Rational Rose",
        "Worked",
        "Struts Tag Libraries Used Hibernate",
        "SQL Queries",
        "ORC",
        "Hadoop Cluster",
        "Created MapReduce",
        "Driver",
        "MVC",
        "Spark Streaming Well",
        "Spark",
        "File Formats",
        "Development Implementation and Maintenance",
        "Hadoop Installed",
        "Data Visualization",
        "Created Hive",
        "XSLFO",
        "EJB",
        "Implemented Oracle",
        "US",
        "Sqoop",
        "Oracle Teradata",
        "JSP JSP Tag",
        "PDF",
        "YARN Configured",
        "JSP Spring Hibernate JSON XML REST SOAP Web Services Strong",
        "Created",
        "MVC Business Delegate Service",
        "Spark Core Spark",
        "Developed Unit Test Cases for Mapper Reducer",
        "UDAFs Designing",
        "Oozie",
        "Oozie Good",
        "SQL",
        "ZooKeeper Hands",
        "Spark RDD",
        "GitHub",
        "Job Tracker",
        "Java Developer United Health Group",
        "Pig Hive Sqoop",
        "Architecture Diagrams",
        "Hive Created SubQueries",
        "MapReduce Experience",
        "Big Data",
        "Hive",
        "Handson",
        "Data Quality Experience",
        "ZooKeeper",
        "TEZ",
        "Unit Test Cases for Mapper Reducer",
        "MapReduce Programs",
        "ETL",
        "Leveraged AWS",
        "Apache Hadoop",
        "Performed",
        "Impala",
        "PEP Guidelines",
        "Spark SQL",
        "HBase Ran",
        "AutoSys",
        "RegEx",
        "Piggybank",
        "Bucketing Static and Dynamic Partitioning for Hive",
        "Tables Indices Stored",
        "Qlik Sense for Data Visualization and Analysis",
        "Data",
        "Amazon Web Services Hadoop Developer Amtrak Washington DC",
        "Relational Database",
        "Data Cleansing ETL",
        "MapReduce",
        "MiddleTier",
        "Data Warehousing",
        "NoSQL",
        "Tableau",
        "WebSphere Application Server",
        "Data Ingestion Data Analytics and Data Visualization",
        "HDFS Created Hive"
    ],
    "experience": "Experience in importing and exporting data using Sqoop between HDFS and Relational Database Management Systems Populated HDFS with huge amounts of data using Apache Kafka and Flume Excellent knowledge of data mapping extracting transforming and loading from different data sources Worked with different File Formats like TEXTFILE SEQUENCE FILE AVROFILE ORC and PARQUET for Hive querying and processing Experience in developing custom MapReduce Programs in Java using Apache Hadoop for analyzing Big Data as per the requirement Comprehensive work experience in implementing Big Data projects using Apache Hadoop Pig Hive HBase Spark Sqoop Flume Zookeeper Oozie Experience with distributed systems largescale nonrelational data stores and multiterabyte data warehouses Excellent knowledge on Hadoop architecture Hadoop Distributed File system HDFS Job Tracker Task Tracker Name Node Data Node and Map Reduce programming paradigm Handson experience building data pipelines using Hadoop components Sqoop Hive Pig MapReduce Spark Spark SQL Hands on experience in various Big Data application phases like Data Ingestion Data Analytics and Data Visualization Experience working on Hortonworks Cloudera MapR distributions Extensively worked on MRV1 and MRV2 Hadoop architectures Experience working on Spark RDDs DAGs Spark SQL and Spark Streaming Well experienced in data transformation using custom MapReduce Hive and Pig scripts for different types of file formats Expertise in extending Hive and Pig core functionality by writing custom UDFs and UDAFs Designing and creating Hive external tables using shared metastore instead of derby with partitioning dynamic partitioning and buckets Experience building solutions with NoSQL databases such as HBase Cassandra MongoDB Firm grip on data modeling data mapping database performance tuning and NoSQL mapreduce systems Indepth understanding of Spark architecture including Spark Core Spark SQL Data Frames and Spark Streaming Hands on experience migrating complex MapReduce programs into Apache Spark RDD transformations Experienced in Apache Spark for implementing advanced procedures like text analytics and processing using the inmemory computing capabilities written in Scala Experience in Kafka installation integration with Spark Streaming Used Spark Streaming to divide streaming data into batches as an input to Spark engine for batch processing Good Experience in Linux Bash scripting and following PEP Guidelines in Python Involved in converting HiveSQL queries into Spark transformations using Spark RDD in Scala and Python Experienced with different scripting language like Python and shell scripts Handson experience in using python Scripts to handle data manipulation Worked well with Python R scripts for statistics analytics for generating reports for Data Quality Experience in designing both time driven and data driven automated workflows using Oozie Good understanding of ZooKeeper for monitoring and managing Hadoop jobs Good understanding of ETL tools and how they can be applied in a Big Data environment Monitoring Map Reduce Jobs and YARN Applications Handson experience with Amazon Elastic MapReduce EMR Storage S3 EC2 instances and Data Warehousing Experience with RDBMS and writing SQL and PLSQL scripts used in stored procedures Used Git for source code and version control management Proficient in Java J2EE JDBC Collection Framework Servlets JSP Spring Hibernate JSON XML REST SOAP Web Services Strong understanding in Agile and Waterfall SDLC methodologies Excellent problem solving proactive thinking analytical programming and communication skills Ability to learn new technologies and work effectively in crossfunctional team environments Authorized to work in the US for any employer Work Experience Sr Hadoop Developer Syniverse Technologies Tampa FL September 2018 to Present Responsibilities Experience with complete SDLC process staging code reviews source code management and build process Implemented Big Data platforms as data storage retrieval and processing systems Developed data pipeline using Kafka Sqoop Hive and Java MapReduce to ingest customer behavioral data and financial histories into HDFS for analysis Wrote Sqoop scripts for importing and exporting data into HDFS and Hive Wrote MapReduce jobs to discover trends in data usage by the users Load and transform large sets of structured semi structured and unstructured data Pig Experienced working on Pig to do transformations event joins filtering and some preaggregations before storing the data onto HDFS Involved in developing Hive UDFs for the needed functionality that is not available out of the box from Hive Created SubQueries for filtering and faster execution of data Experienced in migrating Hive QL into Impala to minimize query response time Used HCATALOG to access the Hive table metadata from MapReduce and Pig scripts Experience loading and transforming large amounts of structured and unstructured data into HBase and exposure handling Automatic failover in HBase Ran POCs in Spark to take the benchmarking of the implementation Developed Spark jobs using Scala in test environment for faster data processing and querying Worked on migrating MapReduce programs into Spark transformations using Spark and Scala Used Python for pattern matching in build logs to format warnings and errors Configured big data workflows to run on the top of Hadoop using Oozie and these workflows comprises of heterogeneous jobs like Pig Hive Sqoop Cluster coordination services through ZooKeeper Hands on experience in Tableau for Data Visualization and analysis on large data sets drawing various conclusions Involved in developing test framework for data profiling and validation using interactive queries and collected all the test results into audit tables for comparing the results over the period Documented all the requirements code and implementation methodologies for reviewing and analyzation purposes Extensively used GitHub as a code repository and Phabricator for managing day to day development process and to keep track of the issues Environment Java Scala Hadoop Spark HDFS MapReduce Yarn Hive Pig Impala Oozie Sqoop Flume Kafka Teradata SQL GitHub Phabricator Amazon Web Services Hadoop Developer Amtrak Washington DC May 2016 to August 2018 Responsibilities Involved in complete project life cycle starting from design discussion to production deployment Worked closely with the business team to gather their requirements and new support features Involved in running POCs on different use cases of the application and maintained a standard document for best coding practices Developed a 16node cluster in designing the Data Lake with the Hortonworks distribution Responsible for building scalable distributed data solutions using Hadoop Installed configured and implemented high availability Hadoop Clusters with required services HDFS Hive HBase Spark ZooKeeper Implemented Kerberos for authenticating all the services in Hadoop Cluster Configured ZooKeeper to coordinate the servers in clusters to maintain the data consistency Involved in designing the Data pipeline from endtoend to ingest data into the Data Lake Wrote scripts to automate application deployments and configurations monitoring YARN Configured and developed Sqoop scripts to migrate the data from relational databases like Oracle Teradata to HDFS Used Flume for collecting and aggregating large amounts of streaming data into HDFS Wrote MapReduce jobs in Java to parse the raw data populate staging tables and store the refined data Developed Map Reduce programs as a part of predictive analytical model development Built reusable Hive UDF libraries for business requirements which enabled various business analysts to use these UDFs in Hive querying Created different staging tables like ingestion tables and preparation tables in Hive environment Optimized Hive queries and used Hive on top of Spark engine Worked on Sequence files Map side joins Bucketing Static and Dynamic Partitioning for Hive performance enhancement and storage improvement Tested Apache TEZ an extensible framework for building high performance batch and interactive data processing applications on Pig and Hive jobs Involved in converting HiveSQL queries into Spark transformations using Spark RDDs Spark SQL Scala Worked on the Spark core and Spark SQL modules of Spark extensively Developed Python scripts to find vulnerabilities with SQL Queries by doing SQL injection Wrote Python UDFs to process the RegEx and return the valid names using streaming Created tables in HBase to store the variable data formats of data coming from different upstream sources Leveraged AWS cloud services such as EC2 autoscaling and VPC Virtual Private Cloud to build secure highly scalable and flexible systems that handled expected and unexpected load bursts and can quickly evolve during development iterations Developed the batch scripts to fetch the data from AWS S3 storage and do required transformations in Spark framework using Scala Configured various workflows to run on top of Hadoop using Oozie and these workflows comprises of heterogeneous jobs like Pig Hive Sqoop and MapReduce Experience in managing and reviewing Hadoop log files Utilized capabilities of Tableau such as Data extracts Data blending Forecasting Dashboard actions and table calculations to build dashboards Followed Agile Methodologies while working on the project Performed bug fixing and 24X7 production support for running the processes Environment Java Scala Hadoop Hortonworks AWS HDFS YARN Map Reduce Hive Pig Spark Flume Kafka Sqoop Oozie Zookeeper Oracle Teradata MySQL Hadoop Developer Tiffany Co NJ August 2014 to April 2016 Responsibilities Worked on Hortonworks cluster which is responsible for providing open source platform based on Apache Hadoop for analyzing storing and managing big data Worked with analyst to determine and understand business requirements Load and transform large datasets of structured semi structured and unstructured data using HadoopBig Data concepts Developed data pipeline using Flume Sqoop Pig and MapReduce to ingest customer data and financial histories into HDFS for analysis Used MapReduce and Flume to load aggregate store and analyze web log data from different web servers Created MapReduce programs to handle semiunstructured data like XML JSON AVRO data files and sequence files for log files Involved in submitting and tracking MapReduce jobs using Job Tracker Experience writing Pig Latin scripts for Data Cleansing ETL operations and query optimizations of exists scripts Written Hive UDF to sort Structure fields and return complex data types Created Hive tables from JSON data using data serialization framework like AVRO Experience writing reusable custom Hive and Pig UDFs in Java and using existing UDFs from Piggybank and other sources Experience in working with NoSQL database HBase in getting real time data analytics Integrated Hive tables to HBase to perform row level analytics Developed Oozie workflows for daily incremental loads which Sqoops data from Teradata Netezza and then imported into Hive tables Involved in performance tuning by using different service engines like TEZ etc Performed performance tuning and troubleshooting of MapReduce jobs by analyzing and reviewing Hadoop log files Implemented Daily Cron jobs that automate parallel tasks of loading the data into HDFS using AutoSys and Oozie coordinator jobs Developed suit of Unit Test Cases for Mapper Reducer and Driver classes using MR Testing library Environment Hortonworks Java Hadoop HDFS MapReduce Tez Hive Pig Oozie Sqoop Flume Teradata Netezza Tableau Hadoop Developer LakeLand Bank West Milford NJ September 2013 to July 2014 Responsibilities Installed Cloudera distribution of Hadoop Cluster and services HDFS Pig Hive Sqoop Flume and MapReduce Responsible for providing open source platform based on Apache Hadoop for analyzing storing and managing big data Loaded and transformed large sets of structured semistructured and unstructured data Responsible for managing data coming from different sources Imported and exported data into HDFS and Hive using Sqoop Wrote Hive queries Involved in loading data from UNIX file system to HDFS Created Hive tables loaded with data and wrote queries which will run internally in MapReduce and performed data analysis as per the business requirements Worked with analysts to determine and understand business requirements Loaded and transformed large datasets of structured semi structured and unstructured data using HadoopBig Data concepts Developed data pipeline using Flume Sqoop Pig and MapReduce to ingest customer data and financial histories into HDFS for analysis Used MapReduce and Flume to load aggregate store and analyze web log data from different web servers Created MapReduce programs to handle semiunstructured data like XML JSON AVRO data files and sequence files for log files Involved in submitting and tracking MapReduce jobs using Job Tracker Experience writing Pig Latin scripts for Data Cleansing ETL operations and query optimizations of exists scripts Written Hive UDF to sort Structure fields and return complex data types Created Hive tables from JSON data using data serialization framework like AVRO Experience writing reusable custom Hive and Pig UDFs in Java and using existing UDFs from Piggybank and other sources Experience in working with NoSQL database HBase in getting real time data analytics Integrated Hive tables to HBase to perform row level analytics Performed performance tuning and troubleshooting of MapReduce jobs by analyzing and reviewing Hadoop log files Developed Unit Test Cases for Mapper Reducer and Driver classes using MR Testing library Supported operations team in Hadoop cluster maintenance including commissioning and decommissioning nodes and upgrades Provided technical assistance to all development projects Handson experience with Qlik Sense for Data Visualization and Analysis on large data sets drawing various insights Created dashboards using Qlik Sense and performed Data extracts Data blending Forecasting and table calculations Environment Hortonworks Java Hadoop HDFS MapReduce Hive Pig Oozie Sqoop Flume Netezza Qlik Sense Java Developer Trimble Navigation Limited Centreville VA March 2011 to August 2013 Responsibilities Built the application based on Rational Unified Process RUP Analyzed and developed UMLs with Rational Rose including development of class diagrams sequence diagrams use case diagrams and activity diagrams Implemented the MiddleTier employing design patterns like MVC Business Delegate Service Locator Session Faade Data Access Objects DAOs Developed using MVC architecture and employed the Struts Framework and used Validator Framework and Tiles Framework as a plugin with struts Developed user interface using JSP JSP Tag libraries JSTL and Struts Tag Libraries Used Hibernate in data access layer to access and update the information in database Used JSON to pass objects between web pages and serverside application Used XSLFO to generate PDF reports Extensively worked on XML parsers SAXDOM Used WSDL and SOAP protocol for Web Services implementation Used JDBC to access DB2 UDB database for accessing customer information Developed application level logging using Log4J Used CVS for version controlling and Junit for unit testing Involved in development of Tables Indices Stored procedures Database Triggers and Functions Environment J2EE 17 WebSphere Application Server v80 RAD JSP 20 EJB 31 Struts 20 JMS JSON JDBC JNDI XML XSL XSLT XSLFO WSDL SOAP Hibernate 40 RUP Rational Rose 2000 Log4J Junit CVS IBM DB2 v82 Red Hat LINUX RESTful web services Java Developer United Health Group Sacramento CA June 2008 to February 2011 Responsibilities Designed and Developed application using EJB 20 and Struts framework Developed POJOs for Data Model to map the Java Objects with Relational Database tables Designed and developed Service layer using Struts framework Used MVC based Struts framework to develop the multitier web application presentation layer components Implemented Struts tag libraries like HTML logic tab bean etc in the JSP pages Used Struts tiles library for layout of web page and performed struts validations using Struts validation framework Implemented Oracle database and JDBC drivers to access the data Involved in design analysis and architectural meetings created Architecture Diagrams and Flow Charts using Rational Rose Followed Agile software development practice paired with programming test driven development and scrum status meetings Developed use case diagrams class diagrams database tables and mapping between relational database tables Maintained the application configuration information in various properties file Performed unit testing system testing and integration testing Environment Java Struts Framework log4j JBoss Servlets JSP JSTL I18N JDBC HTML JavaScript CSS Rational Rose UML Junit Oracle Windows NT Education Bachelors",
    "extracted_keywords": [
        "Sr",
        "Hadoop",
        "Developer",
        "Sr",
        "Hadoop",
        "span",
        "lDeveloperspan",
        "Sr",
        "Hadoop",
        "Developer",
        "Syniverse",
        "Technologies",
        "Tempe",
        "years",
        "IT",
        "industry",
        "experience",
        "product",
        "Development",
        "Implementation",
        "Maintenance",
        "web",
        "applications",
        "Java",
        "J2EE",
        "technologies",
        "Big",
        "Data",
        "ecosystems",
        "Linux",
        "environment",
        "Overall",
        "years",
        "experience",
        "analytics",
        "Big",
        "Data",
        "technologies",
        "handson",
        "experience",
        "Querying",
        "Processing",
        "Data",
        "Analysis",
        "Experience",
        "data",
        "Sqoop",
        "HDFS",
        "Relational",
        "Database",
        "Management",
        "Systems",
        "Populated",
        "HDFS",
        "amounts",
        "data",
        "Apache",
        "Kafka",
        "Flume",
        "Excellent",
        "knowledge",
        "data",
        "mapping",
        "transforming",
        "loading",
        "data",
        "sources",
        "File",
        "Formats",
        "TEXTFILE",
        "SEQUENCE",
        "AVROFILE",
        "ORC",
        "PARQUET",
        "Hive",
        "Experience",
        "custom",
        "MapReduce",
        "Programs",
        "Java",
        "Apache",
        "Hadoop",
        "Big",
        "Data",
        "requirement",
        "Comprehensive",
        "work",
        "experience",
        "Big",
        "Data",
        "projects",
        "Apache",
        "Hadoop",
        "Pig",
        "Hive",
        "HBase",
        "Spark",
        "Sqoop",
        "Flume",
        "Zookeeper",
        "Oozie",
        "Experience",
        "systems",
        "largescale",
        "data",
        "stores",
        "multiterabyte",
        "data",
        "knowledge",
        "Hadoop",
        "architecture",
        "Hadoop",
        "Distributed",
        "File",
        "system",
        "HDFS",
        "Job",
        "Tracker",
        "Task",
        "Tracker",
        "Name",
        "Node",
        "Data",
        "Node",
        "Map",
        "Reduce",
        "programming",
        "paradigm",
        "Handson",
        "experience",
        "building",
        "data",
        "pipelines",
        "Hadoop",
        "components",
        "Sqoop",
        "Hive",
        "Pig",
        "MapReduce",
        "Spark",
        "Spark",
        "SQL",
        "Hands",
        "experience",
        "Big",
        "Data",
        "application",
        "phases",
        "Data",
        "Ingestion",
        "Data",
        "Analytics",
        "Data",
        "Visualization",
        "Experience",
        "Hortonworks",
        "Cloudera",
        "MapR",
        "distributions",
        "MRV1",
        "MRV2",
        "Hadoop",
        "Experience",
        "Spark",
        "RDDs",
        "Spark",
        "SQL",
        "Spark",
        "Streaming",
        "Well",
        "data",
        "transformation",
        "custom",
        "MapReduce",
        "Hive",
        "Pig",
        "scripts",
        "types",
        "file",
        "formats",
        "Expertise",
        "Hive",
        "Pig",
        "core",
        "functionality",
        "custom",
        "UDFs",
        "UDAFs",
        "Designing",
        "Hive",
        "tables",
        "metastore",
        "derby",
        "partitioning",
        "building",
        "solutions",
        "NoSQL",
        "databases",
        "HBase",
        "Cassandra",
        "MongoDB",
        "grip",
        "data",
        "data",
        "mapping",
        "database",
        "performance",
        "tuning",
        "NoSQL",
        "systems",
        "understanding",
        "Spark",
        "architecture",
        "Spark",
        "Core",
        "Spark",
        "SQL",
        "Data",
        "Frames",
        "Spark",
        "Streaming",
        "Hands",
        "experience",
        "MapReduce",
        "programs",
        "Apache",
        "Spark",
        "RDD",
        "transformations",
        "Apache",
        "Spark",
        "procedures",
        "text",
        "analytics",
        "processing",
        "inmemory",
        "computing",
        "capabilities",
        "Scala",
        "Experience",
        "Kafka",
        "installation",
        "integration",
        "Spark",
        "Streaming",
        "Spark",
        "Streaming",
        "streaming",
        "data",
        "batches",
        "input",
        "Spark",
        "engine",
        "batch",
        "Good",
        "Experience",
        "Linux",
        "Bash",
        "PEP",
        "Guidelines",
        "Python",
        "HiveSQL",
        "queries",
        "Spark",
        "transformations",
        "Spark",
        "RDD",
        "Scala",
        "Python",
        "scripting",
        "language",
        "Python",
        "shell",
        "scripts",
        "Handson",
        "experience",
        "python",
        "Scripts",
        "data",
        "manipulation",
        "Python",
        "R",
        "scripts",
        "statistics",
        "analytics",
        "generating",
        "reports",
        "Data",
        "Quality",
        "Experience",
        "time",
        "data",
        "workflows",
        "Oozie",
        "understanding",
        "ZooKeeper",
        "Hadoop",
        "jobs",
        "understanding",
        "ETL",
        "tools",
        "Big",
        "Data",
        "environment",
        "Monitoring",
        "Map",
        "Jobs",
        "YARN",
        "Applications",
        "Handson",
        "experience",
        "Amazon",
        "Elastic",
        "MapReduce",
        "EMR",
        "Storage",
        "S3",
        "EC2",
        "instances",
        "Data",
        "Warehousing",
        "Experience",
        "RDBMS",
        "SQL",
        "PLSQL",
        "scripts",
        "procedures",
        "Git",
        "source",
        "code",
        "version",
        "control",
        "management",
        "Proficient",
        "Java",
        "J2EE",
        "JDBC",
        "Collection",
        "Framework",
        "Servlets",
        "JSP",
        "Spring",
        "Hibernate",
        "XML",
        "REST",
        "SOAP",
        "Web",
        "Services",
        "Strong",
        "understanding",
        "Agile",
        "Waterfall",
        "SDLC",
        "methodologies",
        "Excellent",
        "problem",
        "thinking",
        "programming",
        "communication",
        "Ability",
        "technologies",
        "work",
        "team",
        "environments",
        "US",
        "employer",
        "Work",
        "Experience",
        "Sr",
        "Hadoop",
        "Developer",
        "Syniverse",
        "Technologies",
        "Tampa",
        "FL",
        "September",
        "Present",
        "Responsibilities",
        "Experience",
        "SDLC",
        "process",
        "code",
        "reviews",
        "source",
        "code",
        "management",
        "build",
        "process",
        "Big",
        "Data",
        "platforms",
        "data",
        "storage",
        "retrieval",
        "processing",
        "systems",
        "data",
        "pipeline",
        "Kafka",
        "Sqoop",
        "Hive",
        "Java",
        "MapReduce",
        "customer",
        "data",
        "histories",
        "HDFS",
        "analysis",
        "Wrote",
        "Sqoop",
        "data",
        "HDFS",
        "Hive",
        "Wrote",
        "MapReduce",
        "jobs",
        "trends",
        "data",
        "usage",
        "users",
        "sets",
        "data",
        "Pig",
        "Pig",
        "transformations",
        "event",
        "preaggregations",
        "data",
        "HDFS",
        "Hive",
        "UDFs",
        "functionality",
        "box",
        "Hive",
        "Created",
        "SubQueries",
        "filtering",
        "execution",
        "data",
        "Hive",
        "QL",
        "Impala",
        "query",
        "response",
        "time",
        "HCATALOG",
        "Hive",
        "table",
        "metadata",
        "MapReduce",
        "Pig",
        "scripts",
        "Experience",
        "loading",
        "amounts",
        "data",
        "HBase",
        "exposure",
        "failover",
        "HBase",
        "Ran",
        "POCs",
        "Spark",
        "benchmarking",
        "implementation",
        "Spark",
        "jobs",
        "Scala",
        "test",
        "environment",
        "data",
        "processing",
        "Worked",
        "MapReduce",
        "programs",
        "Spark",
        "transformations",
        "Spark",
        "Scala",
        "Python",
        "pattern",
        "build",
        "logs",
        "warnings",
        "errors",
        "data",
        "workflows",
        "top",
        "Hadoop",
        "Oozie",
        "workflows",
        "comprises",
        "jobs",
        "Pig",
        "Hive",
        "Sqoop",
        "Cluster",
        "coordination",
        "services",
        "ZooKeeper",
        "Hands",
        "experience",
        "Tableau",
        "Data",
        "Visualization",
        "analysis",
        "data",
        "sets",
        "conclusions",
        "test",
        "framework",
        "data",
        "profiling",
        "validation",
        "queries",
        "test",
        "results",
        "audit",
        "tables",
        "results",
        "period",
        "requirements",
        "code",
        "implementation",
        "methodologies",
        "analyzation",
        "purposes",
        "GitHub",
        "code",
        "repository",
        "Phabricator",
        "day",
        "day",
        "development",
        "process",
        "track",
        "issues",
        "Environment",
        "Java",
        "Scala",
        "Hadoop",
        "Spark",
        "HDFS",
        "MapReduce",
        "Yarn",
        "Hive",
        "Pig",
        "Impala",
        "Oozie",
        "Sqoop",
        "Flume",
        "Kafka",
        "Teradata",
        "SQL",
        "GitHub",
        "Phabricator",
        "Amazon",
        "Web",
        "Services",
        "Hadoop",
        "Developer",
        "Amtrak",
        "Washington",
        "DC",
        "May",
        "August",
        "Responsibilities",
        "project",
        "life",
        "cycle",
        "design",
        "discussion",
        "production",
        "deployment",
        "business",
        "team",
        "requirements",
        "support",
        "features",
        "POCs",
        "use",
        "cases",
        "application",
        "document",
        "practices",
        "cluster",
        "Data",
        "Lake",
        "Hortonworks",
        "distribution",
        "data",
        "solutions",
        "Hadoop",
        "Installed",
        "availability",
        "Hadoop",
        "Clusters",
        "services",
        "HDFS",
        "Hive",
        "HBase",
        "Spark",
        "ZooKeeper",
        "Kerberos",
        "services",
        "Hadoop",
        "Cluster",
        "Configured",
        "ZooKeeper",
        "servers",
        "clusters",
        "data",
        "consistency",
        "Data",
        "pipeline",
        "endtoend",
        "data",
        "Data",
        "Lake",
        "Wrote",
        "scripts",
        "application",
        "deployments",
        "configurations",
        "YARN",
        "Sqoop",
        "scripts",
        "data",
        "databases",
        "Oracle",
        "Teradata",
        "HDFS",
        "Flume",
        "amounts",
        "data",
        "HDFS",
        "Wrote",
        "MapReduce",
        "jobs",
        "Java",
        "data",
        "staging",
        "tables",
        "data",
        "Developed",
        "Map",
        "programs",
        "part",
        "model",
        "development",
        "Hive",
        "UDF",
        "business",
        "requirements",
        "business",
        "analysts",
        "UDFs",
        "Hive",
        "staging",
        "tables",
        "ingestion",
        "tables",
        "preparation",
        "tables",
        "Hive",
        "environment",
        "Hive",
        "queries",
        "Hive",
        "top",
        "Spark",
        "engine",
        "Sequence",
        "files",
        "Map",
        "side",
        "Bucketing",
        "Static",
        "Dynamic",
        "Partitioning",
        "Hive",
        "performance",
        "enhancement",
        "storage",
        "improvement",
        "Tested",
        "Apache",
        "TEZ",
        "framework",
        "performance",
        "batch",
        "data",
        "processing",
        "applications",
        "Pig",
        "Hive",
        "jobs",
        "HiveSQL",
        "queries",
        "Spark",
        "transformations",
        "Spark",
        "RDDs",
        "Spark",
        "SQL",
        "Scala",
        "Spark",
        "core",
        "Spark",
        "SQL",
        "modules",
        "Spark",
        "Python",
        "scripts",
        "vulnerabilities",
        "SQL",
        "Queries",
        "SQL",
        "injection",
        "Wrote",
        "Python",
        "UDFs",
        "RegEx",
        "names",
        "tables",
        "HBase",
        "data",
        "formats",
        "data",
        "sources",
        "AWS",
        "cloud",
        "services",
        "EC2",
        "autoscaling",
        "VPC",
        "Virtual",
        "Cloud",
        "systems",
        "load",
        "bursts",
        "development",
        "iterations",
        "batch",
        "scripts",
        "data",
        "AWS",
        "S3",
        "storage",
        "transformations",
        "Spark",
        "framework",
        "Scala",
        "Configured",
        "workflows",
        "top",
        "Hadoop",
        "Oozie",
        "workflows",
        "comprises",
        "jobs",
        "Pig",
        "Hive",
        "Sqoop",
        "MapReduce",
        "Experience",
        "Hadoop",
        "log",
        "capabilities",
        "Tableau",
        "Data",
        "Data",
        "Forecasting",
        "Dashboard",
        "actions",
        "table",
        "calculations",
        "dashboards",
        "Agile",
        "Methodologies",
        "project",
        "Performed",
        "bug",
        "fixing",
        "production",
        "support",
        "processes",
        "Environment",
        "Java",
        "Scala",
        "Hadoop",
        "Hortonworks",
        "AWS",
        "HDFS",
        "YARN",
        "Map",
        "Reduce",
        "Hive",
        "Pig",
        "Spark",
        "Flume",
        "Kafka",
        "Sqoop",
        "Oozie",
        "Zookeeper",
        "Oracle",
        "Teradata",
        "MySQL",
        "Hadoop",
        "Developer",
        "Tiffany",
        "Co",
        "NJ",
        "August",
        "April",
        "Responsibilities",
        "Hortonworks",
        "cluster",
        "source",
        "platform",
        "Apache",
        "Hadoop",
        "data",
        "analyst",
        "business",
        "requirements",
        "datasets",
        "data",
        "HadoopBig",
        "Data",
        "data",
        "pipeline",
        "Flume",
        "Sqoop",
        "Pig",
        "MapReduce",
        "customer",
        "data",
        "histories",
        "HDFS",
        "analysis",
        "MapReduce",
        "Flume",
        "store",
        "analyze",
        "web",
        "log",
        "data",
        "web",
        "servers",
        "MapReduce",
        "programs",
        "data",
        "XML",
        "AVRO",
        "data",
        "files",
        "sequence",
        "files",
        "log",
        "files",
        "MapReduce",
        "jobs",
        "Job",
        "Tracker",
        "Experience",
        "Pig",
        "Latin",
        "scripts",
        "Data",
        "Cleansing",
        "ETL",
        "operations",
        "query",
        "optimizations",
        "scripts",
        "Hive",
        "UDF",
        "Structure",
        "fields",
        "data",
        "types",
        "Hive",
        "tables",
        "data",
        "data",
        "serialization",
        "framework",
        "AVRO",
        "Experience",
        "custom",
        "Hive",
        "Pig",
        "UDFs",
        "Java",
        "UDFs",
        "Piggybank",
        "sources",
        "NoSQL",
        "database",
        "HBase",
        "time",
        "data",
        "Integrated",
        "Hive",
        "tables",
        "HBase",
        "row",
        "level",
        "analytics",
        "Oozie",
        "workflows",
        "loads",
        "data",
        "Teradata",
        "Netezza",
        "Hive",
        "tables",
        "performance",
        "service",
        "engines",
        "TEZ",
        "Performed",
        "performance",
        "tuning",
        "troubleshooting",
        "MapReduce",
        "jobs",
        "Hadoop",
        "log",
        "files",
        "Daily",
        "Cron",
        "jobs",
        "tasks",
        "data",
        "HDFS",
        "AutoSys",
        "Oozie",
        "coordinator",
        "jobs",
        "suit",
        "Unit",
        "Test",
        "Cases",
        "Mapper",
        "Reducer",
        "Driver",
        "classes",
        "MR",
        "Testing",
        "library",
        "Environment",
        "Hortonworks",
        "Java",
        "Hadoop",
        "HDFS",
        "MapReduce",
        "Tez",
        "Hive",
        "Pig",
        "Oozie",
        "Sqoop",
        "Flume",
        "Teradata",
        "Netezza",
        "Tableau",
        "Hadoop",
        "Developer",
        "LakeLand",
        "Bank",
        "West",
        "Milford",
        "NJ",
        "September",
        "July",
        "Responsibilities",
        "Cloudera",
        "distribution",
        "Hadoop",
        "Cluster",
        "services",
        "HDFS",
        "Pig",
        "Hive",
        "Sqoop",
        "Flume",
        "MapReduce",
        "Responsible",
        "source",
        "platform",
        "Apache",
        "Hadoop",
        "data",
        "sets",
        "data",
        "data",
        "sources",
        "data",
        "HDFS",
        "Hive",
        "Sqoop",
        "Wrote",
        "Hive",
        "queries",
        "loading",
        "data",
        "UNIX",
        "file",
        "system",
        "Hive",
        "tables",
        "data",
        "queries",
        "MapReduce",
        "data",
        "analysis",
        "business",
        "requirements",
        "analysts",
        "business",
        "requirements",
        "datasets",
        "data",
        "HadoopBig",
        "Data",
        "data",
        "pipeline",
        "Flume",
        "Sqoop",
        "Pig",
        "MapReduce",
        "customer",
        "data",
        "histories",
        "HDFS",
        "analysis",
        "MapReduce",
        "Flume",
        "store",
        "analyze",
        "web",
        "log",
        "data",
        "web",
        "servers",
        "MapReduce",
        "programs",
        "data",
        "XML",
        "AVRO",
        "data",
        "files",
        "sequence",
        "files",
        "log",
        "files",
        "MapReduce",
        "jobs",
        "Job",
        "Tracker",
        "Experience",
        "Pig",
        "Latin",
        "scripts",
        "Data",
        "Cleansing",
        "ETL",
        "operations",
        "query",
        "optimizations",
        "scripts",
        "Hive",
        "UDF",
        "Structure",
        "fields",
        "data",
        "types",
        "Hive",
        "tables",
        "data",
        "data",
        "serialization",
        "framework",
        "AVRO",
        "Experience",
        "custom",
        "Hive",
        "Pig",
        "UDFs",
        "Java",
        "UDFs",
        "Piggybank",
        "sources",
        "NoSQL",
        "database",
        "HBase",
        "time",
        "data",
        "Integrated",
        "Hive",
        "tables",
        "HBase",
        "row",
        "level",
        "analytics",
        "performance",
        "tuning",
        "troubleshooting",
        "MapReduce",
        "jobs",
        "Hadoop",
        "log",
        "Developed",
        "Unit",
        "Test",
        "Cases",
        "Mapper",
        "Reducer",
        "Driver",
        "classes",
        "MR",
        "Testing",
        "library",
        "operations",
        "team",
        "Hadoop",
        "cluster",
        "maintenance",
        "decommissioning",
        "nodes",
        "upgrades",
        "assistance",
        "development",
        "projects",
        "Handson",
        "experience",
        "Qlik",
        "Sense",
        "Data",
        "Visualization",
        "Analysis",
        "data",
        "sets",
        "insights",
        "dashboards",
        "Qlik",
        "Sense",
        "Data",
        "Data",
        "Forecasting",
        "table",
        "calculations",
        "Environment",
        "Hortonworks",
        "Java",
        "Hadoop",
        "HDFS",
        "MapReduce",
        "Hive",
        "Pig",
        "Oozie",
        "Sqoop",
        "Flume",
        "Netezza",
        "Qlik",
        "Sense",
        "Java",
        "Developer",
        "Trimble",
        "Navigation",
        "Limited",
        "Centreville",
        "VA",
        "March",
        "August",
        "Responsibilities",
        "application",
        "Rational",
        "Unified",
        "Process",
        "RUP",
        "Analyzed",
        "UMLs",
        "Rational",
        "Rose",
        "development",
        "class",
        "diagrams",
        "sequence",
        "diagrams",
        "case",
        "diagrams",
        "activity",
        "diagrams",
        "MiddleTier",
        "design",
        "patterns",
        "MVC",
        "Business",
        "Delegate",
        "Service",
        "Locator",
        "Session",
        "Faade",
        "Data",
        "Access",
        "DAOs",
        "MVC",
        "architecture",
        "Struts",
        "Framework",
        "Validator",
        "Framework",
        "Tiles",
        "Framework",
        "plugin",
        "struts",
        "user",
        "interface",
        "JSP",
        "JSP",
        "Tag",
        "JSTL",
        "Struts",
        "Tag",
        "Libraries",
        "Hibernate",
        "data",
        "access",
        "layer",
        "information",
        "database",
        "JSON",
        "objects",
        "web",
        "pages",
        "serverside",
        "application",
        "XSLFO",
        "PDF",
        "reports",
        "XML",
        "parsers",
        "SAXDOM",
        "WSDL",
        "protocol",
        "Web",
        "Services",
        "implementation",
        "JDBC",
        "DB2",
        "UDB",
        "database",
        "customer",
        "information",
        "application",
        "level",
        "Log4J",
        "Used",
        "CVS",
        "version",
        "Junit",
        "unit",
        "testing",
        "development",
        "Tables",
        "Indices",
        "procedures",
        "Database",
        "Triggers",
        "Functions",
        "Environment",
        "J2EE",
        "WebSphere",
        "Application",
        "Server",
        "v80",
        "RAD",
        "JSP",
        "EJB",
        "Struts",
        "JMS",
        "JSON",
        "JDBC",
        "JNDI",
        "XML",
        "XSL",
        "XSLT",
        "XSLFO",
        "WSDL",
        "Hibernate",
        "RUP",
        "Rational",
        "Rose",
        "Log4J",
        "Junit",
        "CVS",
        "IBM",
        "DB2",
        "v82",
        "Red",
        "Hat",
        "LINUX",
        "web",
        "services",
        "Java",
        "Developer",
        "United",
        "Health",
        "Group",
        "Sacramento",
        "CA",
        "June",
        "February",
        "Responsibilities",
        "application",
        "EJB",
        "Struts",
        "POJOs",
        "Data",
        "Model",
        "Java",
        "Objects",
        "Relational",
        "Database",
        "tables",
        "Service",
        "layer",
        "Struts",
        "framework",
        "Used",
        "MVC",
        "Struts",
        "framework",
        "web",
        "application",
        "presentation",
        "layer",
        "components",
        "Struts",
        "tag",
        "HTML",
        "logic",
        "tab",
        "bean",
        "JSP",
        "pages",
        "Struts",
        "library",
        "layout",
        "web",
        "page",
        "struts",
        "validations",
        "Struts",
        "validation",
        "framework",
        "Oracle",
        "database",
        "JDBC",
        "drivers",
        "data",
        "design",
        "analysis",
        "meetings",
        "Architecture",
        "Diagrams",
        "Flow",
        "Charts",
        "Rational",
        "Rose",
        "software",
        "development",
        "practice",
        "programming",
        "test",
        "development",
        "status",
        "meetings",
        "use",
        "case",
        "diagrams",
        "class",
        "diagrams",
        "database",
        "tables",
        "mapping",
        "database",
        "tables",
        "application",
        "configuration",
        "information",
        "properties",
        "Performed",
        "unit",
        "testing",
        "system",
        "testing",
        "integration",
        "testing",
        "Environment",
        "Java",
        "Struts",
        "Framework",
        "log4j",
        "JBoss",
        "Servlets",
        "JSP",
        "JSTL",
        "I18N",
        "JDBC",
        "HTML",
        "JavaScript",
        "CSS",
        "Rational",
        "Rose",
        "UML",
        "Junit",
        "Oracle",
        "Windows",
        "NT",
        "Education",
        "Bachelors"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T22:59:00.374145",
    "resume_data": "Sr Hadoop Developer Sr Hadoop span lDeveloperspan Sr Hadoop Developer Syniverse Technologies Tempe AZ Overall 10 years of IT industry experience in product Development Implementation and Maintenance of various cloudbased web applications using Java J2EE technologies and Big Data ecosystems on Linux environment Overall 5 years of experience working with analytics using Big Data technologies Have handson experience in Storing Querying Processing and Data Analysis Experience in importing and exporting data using Sqoop between HDFS and Relational Database Management Systems Populated HDFS with huge amounts of data using Apache Kafka and Flume Excellent knowledge of data mapping extracting transforming and loading from different data sources Worked with different File Formats like TEXTFILE SEQUENCE FILE AVROFILE ORC and PARQUET for Hive querying and processing Experience in developing custom MapReduce Programs in Java using Apache Hadoop for analyzing Big Data as per the requirement Comprehensive work experience in implementing Big Data projects using Apache Hadoop Pig Hive HBase Spark Sqoop Flume Zookeeper Oozie Experience with distributed systems largescale nonrelational data stores and multiterabyte data warehouses Excellent knowledge on Hadoop architecture Hadoop Distributed File system HDFS Job Tracker Task Tracker Name Node Data Node and Map Reduce programming paradigm Handson experience building data pipelines using Hadoop components Sqoop Hive Pig MapReduce Spark Spark SQL Hands on experience in various Big Data application phases like Data Ingestion Data Analytics and Data Visualization Experience working on Hortonworks Cloudera MapR distributions Extensively worked on MRV1 and MRV2 Hadoop architectures Experience working on Spark RDDs DAGs Spark SQL and Spark Streaming Well experienced in data transformation using custom MapReduce Hive and Pig scripts for different types of file formats Expertise in extending Hive and Pig core functionality by writing custom UDFs and UDAFs Designing and creating Hive external tables using shared metastore instead of derby with partitioning dynamic partitioning and buckets Experience building solutions with NoSQL databases such as HBase Cassandra MongoDB Firm grip on data modeling data mapping database performance tuning and NoSQL mapreduce systems Indepth understanding of Spark architecture including Spark Core Spark SQL Data Frames and Spark Streaming Hands on experience migrating complex MapReduce programs into Apache Spark RDD transformations Experienced in Apache Spark for implementing advanced procedures like text analytics and processing using the inmemory computing capabilities written in Scala Experience in Kafka installation integration with Spark Streaming Used Spark Streaming to divide streaming data into batches as an input to Spark engine for batch processing Good Experience in Linux Bash scripting and following PEP Guidelines in Python Involved in converting HiveSQL queries into Spark transformations using Spark RDD in Scala and Python Experienced with different scripting language like Python and shell scripts Handson experience in using python Scripts to handle data manipulation Worked well with Python R scripts for statistics analytics for generating reports for Data Quality Experience in designing both time driven and data driven automated workflows using Oozie Good understanding of ZooKeeper for monitoring and managing Hadoop jobs Good understanding of ETL tools and how they can be applied in a Big Data environment Monitoring Map Reduce Jobs and YARN Applications Handson experience with Amazon Elastic MapReduce EMR Storage S3 EC2 instances and Data Warehousing Experience with RDBMS and writing SQL and PLSQL scripts used in stored procedures Used Git for source code and version control management Proficient in Java J2EE JDBC Collection Framework Servlets JSP Spring Hibernate JSON XML REST SOAP Web Services Strong understanding in Agile and Waterfall SDLC methodologies Excellent problem solving proactive thinking analytical programming and communication skills Ability to learn new technologies and work effectively in crossfunctional team environments Authorized to work in the US for any employer Work Experience Sr Hadoop Developer Syniverse Technologies Tampa FL September 2018 to Present Responsibilities Experience with complete SDLC process staging code reviews source code management and build process Implemented Big Data platforms as data storage retrieval and processing systems Developed data pipeline using Kafka Sqoop Hive and Java MapReduce to ingest customer behavioral data and financial histories into HDFS for analysis Wrote Sqoop scripts for importing and exporting data into HDFS and Hive Wrote MapReduce jobs to discover trends in data usage by the users Load and transform large sets of structured semi structured and unstructured data Pig Experienced working on Pig to do transformations event joins filtering and some preaggregations before storing the data onto HDFS Involved in developing Hive UDFs for the needed functionality that is not available out of the box from Hive Created SubQueries for filtering and faster execution of data Experienced in migrating Hive QL into Impala to minimize query response time Used HCATALOG to access the Hive table metadata from MapReduce and Pig scripts Experience loading and transforming large amounts of structured and unstructured data into HBase and exposure handling Automatic failover in HBase Ran POCs in Spark to take the benchmarking of the implementation Developed Spark jobs using Scala in test environment for faster data processing and querying Worked on migrating MapReduce programs into Spark transformations using Spark and Scala Used Python for pattern matching in build logs to format warnings and errors Configured big data workflows to run on the top of Hadoop using Oozie and these workflows comprises of heterogeneous jobs like Pig Hive Sqoop Cluster coordination services through ZooKeeper Hands on experience in Tableau for Data Visualization and analysis on large data sets drawing various conclusions Involved in developing test framework for data profiling and validation using interactive queries and collected all the test results into audit tables for comparing the results over the period Documented all the requirements code and implementation methodologies for reviewing and analyzation purposes Extensively used GitHub as a code repository and Phabricator for managing day to day development process and to keep track of the issues Environment Java Scala Hadoop Spark HDFS MapReduce Yarn Hive Pig Impala Oozie Sqoop Flume Kafka Teradata SQL GitHub Phabricator Amazon Web Services Hadoop Developer Amtrak Washington DC May 2016 to August 2018 Responsibilities Involved in complete project life cycle starting from design discussion to production deployment Worked closely with the business team to gather their requirements and new support features Involved in running POCs on different use cases of the application and maintained a standard document for best coding practices Developed a 16node cluster in designing the Data Lake with the Hortonworks distribution Responsible for building scalable distributed data solutions using Hadoop Installed configured and implemented high availability Hadoop Clusters with required services HDFS Hive HBase Spark ZooKeeper Implemented Kerberos for authenticating all the services in Hadoop Cluster Configured ZooKeeper to coordinate the servers in clusters to maintain the data consistency Involved in designing the Data pipeline from endtoend to ingest data into the Data Lake Wrote scripts to automate application deployments and configurations monitoring YARN Configured and developed Sqoop scripts to migrate the data from relational databases like Oracle Teradata to HDFS Used Flume for collecting and aggregating large amounts of streaming data into HDFS Wrote MapReduce jobs in Java to parse the raw data populate staging tables and store the refined data Developed Map Reduce programs as a part of predictive analytical model development Built reusable Hive UDF libraries for business requirements which enabled various business analysts to use these UDFs in Hive querying Created different staging tables like ingestion tables and preparation tables in Hive environment Optimized Hive queries and used Hive on top of Spark engine Worked on Sequence files Map side joins Bucketing Static and Dynamic Partitioning for Hive performance enhancement and storage improvement Tested Apache TEZ an extensible framework for building high performance batch and interactive data processing applications on Pig and Hive jobs Involved in converting HiveSQL queries into Spark transformations using Spark RDDs Spark SQL Scala Worked on the Spark core and Spark SQL modules of Spark extensively Developed Python scripts to find vulnerabilities with SQL Queries by doing SQL injection Wrote Python UDFs to process the RegEx and return the valid names using streaming Created tables in HBase to store the variable data formats of data coming from different upstream sources Leveraged AWS cloud services such as EC2 autoscaling and VPC Virtual Private Cloud to build secure highly scalable and flexible systems that handled expected and unexpected load bursts and can quickly evolve during development iterations Developed the batch scripts to fetch the data from AWS S3 storage and do required transformations in Spark framework using Scala Configured various workflows to run on top of Hadoop using Oozie and these workflows comprises of heterogeneous jobs like Pig Hive Sqoop and MapReduce Experience in managing and reviewing Hadoop log files Utilized capabilities of Tableau such as Data extracts Data blending Forecasting Dashboard actions and table calculations to build dashboards Followed Agile Methodologies while working on the project Performed bug fixing and 24X7 production support for running the processes Environment Java Scala Hadoop Hortonworks AWS HDFS YARN Map Reduce Hive Pig Spark Flume Kafka Sqoop Oozie Zookeeper Oracle Teradata MySQL Hadoop Developer Tiffany Co NJ August 2014 to April 2016 Responsibilities Worked on Hortonworks cluster which is responsible for providing open source platform based on Apache Hadoop for analyzing storing and managing big data Worked with analyst to determine and understand business requirements Load and transform large datasets of structured semi structured and unstructured data using HadoopBig Data concepts Developed data pipeline using Flume Sqoop Pig and MapReduce to ingest customer data and financial histories into HDFS for analysis Used MapReduce and Flume to load aggregate store and analyze web log data from different web servers Created MapReduce programs to handle semiunstructured data like XML JSON AVRO data files and sequence files for log files Involved in submitting and tracking MapReduce jobs using Job Tracker Experience writing Pig Latin scripts for Data Cleansing ETL operations and query optimizations of exists scripts Written Hive UDF to sort Structure fields and return complex data types Created Hive tables from JSON data using data serialization framework like AVRO Experience writing reusable custom Hive and Pig UDFs in Java and using existing UDFs from Piggybank and other sources Experience in working with NoSQL database HBase in getting real time data analytics Integrated Hive tables to HBase to perform row level analytics Developed Oozie workflows for daily incremental loads which Sqoops data from Teradata Netezza and then imported into Hive tables Involved in performance tuning by using different service engines like TEZ etc Performed performance tuning and troubleshooting of MapReduce jobs by analyzing and reviewing Hadoop log files Implemented Daily Cron jobs that automate parallel tasks of loading the data into HDFS using AutoSys and Oozie coordinator jobs Developed suit of Unit Test Cases for Mapper Reducer and Driver classes using MR Testing library Environment Hortonworks Java Hadoop HDFS MapReduce Tez Hive Pig Oozie Sqoop Flume Teradata Netezza Tableau Hadoop Developer LakeLand Bank West Milford NJ September 2013 to July 2014 Responsibilities Installed Cloudera distribution of Hadoop Cluster and services HDFS Pig Hive Sqoop Flume and MapReduce Responsible for providing open source platform based on Apache Hadoop for analyzing storing and managing big data Loaded and transformed large sets of structured semistructured and unstructured data Responsible for managing data coming from different sources Imported and exported data into HDFS and Hive using Sqoop Wrote Hive queries Involved in loading data from UNIX file system to HDFS Created Hive tables loaded with data and wrote queries which will run internally in MapReduce and performed data analysis as per the business requirements Worked with analysts to determine and understand business requirements Loaded and transformed large datasets of structured semi structured and unstructured data using HadoopBig Data concepts Developed data pipeline using Flume Sqoop Pig and MapReduce to ingest customer data and financial histories into HDFS for analysis Used MapReduce and Flume to load aggregate store and analyze web log data from different web servers Created MapReduce programs to handle semiunstructured data like XML JSON AVRO data files and sequence files for log files Involved in submitting and tracking MapReduce jobs using Job Tracker Experience writing Pig Latin scripts for Data Cleansing ETL operations and query optimizations of exists scripts Written Hive UDF to sort Structure fields and return complex data types Created Hive tables from JSON data using data serialization framework like AVRO Experience writing reusable custom Hive and Pig UDFs in Java and using existing UDFs from Piggybank and other sources Experience in working with NoSQL database HBase in getting real time data analytics Integrated Hive tables to HBase to perform row level analytics Performed performance tuning and troubleshooting of MapReduce jobs by analyzing and reviewing Hadoop log files Developed Unit Test Cases for Mapper Reducer and Driver classes using MR Testing library Supported operations team in Hadoop cluster maintenance including commissioning and decommissioning nodes and upgrades Provided technical assistance to all development projects Handson experience with Qlik Sense for Data Visualization and Analysis on large data sets drawing various insights Created dashboards using Qlik Sense and performed Data extracts Data blending Forecasting and table calculations Environment Hortonworks Java Hadoop HDFS MapReduce Hive Pig Oozie Sqoop Flume Netezza Qlik Sense Java Developer Trimble Navigation Limited Centreville VA March 2011 to August 2013 Responsibilities Built the application based on Rational Unified Process RUP Analyzed and developed UMLs with Rational Rose including development of class diagrams sequence diagrams use case diagrams and activity diagrams Implemented the MiddleTier employing design patterns like MVC Business Delegate Service Locator Session Faade Data Access Objects DAOs Developed using MVC architecture and employed the Struts Framework and used Validator Framework and Tiles Framework as a plugin with struts Developed user interface using JSP JSP Tag libraries JSTL and Struts Tag Libraries Used Hibernate in data access layer to access and update the information in database Used JSON to pass objects between web pages and serverside application Used XSLFO to generate PDF reports Extensively worked on XML parsers SAXDOM Used WSDL and SOAP protocol for Web Services implementation Used JDBC to access DB2 UDB database for accessing customer information Developed application level logging using Log4J Used CVS for version controlling and Junit for unit testing Involved in development of Tables Indices Stored procedures Database Triggers and Functions Environment J2EE 17 WebSphere Application Server v80 RAD JSP 20 EJB 31 Struts 20 JMS JSON JDBC JNDI XML XSL XSLT XSLFO WSDL SOAP Hibernate 40 RUP Rational Rose 2000 Log4J Junit CVS IBM DB2 v82 Red Hat LINUX RESTful web services Java Developer United Health Group Sacramento CA June 2008 to February 2011 Responsibilities Designed and Developed application using EJB 20 and Struts framework Developed POJOs for Data Model to map the Java Objects with Relational Database tables Designed and developed Service layer using Struts framework Used MVC based Struts framework to develop the multitier web application presentation layer components Implemented Struts tag libraries like HTML logic tab bean etc in the JSP pages Used Struts tiles library for layout of web page and performed struts validations using Struts validation framework Implemented Oracle database and JDBC drivers to access the data Involved in design analysis and architectural meetings created Architecture Diagrams and Flow Charts using Rational Rose Followed Agile software development practice paired with programming test driven development and scrum status meetings Developed use case diagrams class diagrams database tables and mapping between relational database tables Maintained the application configuration information in various properties file Performed unit testing system testing and integration testing Environment Java Struts Framework log4j JBoss Servlets JSP JSTL I18N JDBC HTML JavaScript CSS Rational Rose UML Junit Oracle Windows NT Education Bachelors",
    "unique_id": "9c993463-14f8-441e-bd18-985c54a8de3a"
}