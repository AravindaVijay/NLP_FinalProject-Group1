{
    "clean_data": "Hadoop Developer Hadoop span lDeveloperspan Hadoop Developer Having 3 years of overall IT experience diversified exposure in Software Process Engineering developing building enterprise applications using Java SQL Big Data Technologies in various domains like Financial sector and Telecom Strong working experience on different Big Data Platforms like Cloudera CDH Hortonworks HDP and MapR Distribution Strong working experience with Ingestion Storage Querying Processing and Analysis of Big data Integration and Talend Integration Worked on developing SPARK Applications using Scala and Java Spark Core Spark Data Frames Spark SQL and Spark Streaming APIs for Fast processing of Data Experience in developing MapReduce programs and custom UDFs for data processing using Java and Scala Hands on experience working on different Hadoop components like Big Data Talend Kafka Spark Hive HBase Sqoop Cassandra and Zookeeper and used tHDFSInput Thdfs Output TPigLoad tPigFilterRow and tsqoopImport and export Involved in Creating Hive Tables and load processed data into tables and finetuned performance using different optimization techniques Worked on importing and exporting data using Apache Sqoop from RDBMS to Hadoop Platform and vice versa Knowledge on troubleshooting failures in spark applications and finetuning for better performance Hands on experience working with Dimensional ModellingStar Schema and Snowflake Schema Experience in using Zookeeper for coordinating the distributed applications Good experience with NoSQL databases like HBase and Cassandra Experience in Object Oriented Analysis Design OOAD and development of software using UML Methodology good knowledge of J2EE design patterns and Core Java design patterns Expertise in various JAVAJ2EE technologies such as JSP 20 Servlets 2x Struts 1220 Hibernate 2030 ORM Spring 2030 JDBC Very good experience in complete project life cycle design development testing and implementation of Client Server and Web applications Possess excellent communication and analytical skills along with a cando attitude Experience in developing Web Applications with various Open Source frameworks Spring Hibernate 2030 ORM Frameworks Proficient in Core Java J2EE JDBC Servlets JSP Exception Handling Multithreading Concepts Experience in Testing and documenting software for client applications Good experience in using Data Modelling techniques to find the results based on SQL and PLSQL queries Worked and learned a great deal from Amazon Webservices AWS Cloud Services like EC2 S3 and EMR Experience working with different databases such as Oracle SQL Server and MySQL writing stored procedures functions joins and triggers for different Data Models Experience in using version control tools like GITHUB BitBucket share the code snippet among the team members Experience in working in Agile project management Strong motivational skills familiarity with various technologies ability to learn quickly dealing with people commitment to work and believes in hardworking Work Experience Hadoop Developer Cisco September 2018 to March 2019 Project Name Marketing IT Project description The primary goal of this project is to develop its enterprise data hub for storing accessing and analyzing Marketting data At Cisco company stores large volumes of customer data from different sources like web pages transactions and SFDC This will be core to its overall information delivery strategy and allows team comprehensive view and study of customer behaviour and target the customer based on their intrests Responsibilities Responsible for ingesting large volumes of data into Hadoop Data Lake Pipeline on daily basis Read data from different source like Oracle and Hive using Apache Spark and process consumed data using Spark and load the processed to HIVE and downstream applications for different purposes Developed Apache spark application using Java to perform different kind of validations and standardization on fields based on certain validations rules on incoming Data Designed and Implemented ETL Process to load data from Source to Target Tables Once data processed loaded data into different Hive Tables based on the derived fields Used Java based Spark RDDs Data Frames and Datasets Spark SQL faster processing of ETL workloads For performance optimization implemented different optimization techniques optimizations like Partition Bucketing and Map Side joins for fast query processing Excellent working Knowledge in Spark Core Spark SQL Spark Streaming Implemented Spark using Scala and Spark SQL for faster testing and processing of data Involved in converting HiveSQL queries into Spark transformations using Spark Data Frames and Scala Design data ingestion and integration process using SQOOP Shell Scripts Pig with Hive Optimizing existing algorithms in Hadoop using Spark Context SparkSQL Data Frames and Pair RDDs Worked on Error Handling techniques and tuning ETL flow for better performance Generated Java APIs for retrieval and analysis on NoSQL database such as HBase Worked on troubleshooting and Performance optimization on Spark Application to improve the overall processing time and make more error tolerant for the pipeline Collaborated with the infrastructure network database application and BA teams to ensure data quality and availability Used NOSQL HBase for storing batching information Used Rally tracking tool for assigning and defect management Worked in Agile development environment and actively involved in daily Scrum and other design related meetings Used BitBucket to share the code snippet among the team members Environment Spark 211 Oracle 11203 Apache Solar 665 Datastax Cassandra 300 Hbase118 Talend Bitbucket Rally Hadoop Developer T Mobile Atlanta GA June 2017 to August 2018 Project Name Continuous Service Providing T Mobile is one of the top Telecommunication company in US providing cell phone services Its the second Biggest mobile network operator serving clients with high quality mobile networks solutions For Continuous Service Providing and detecting the towers cables and routers before getting failed T mobile stores large volumes of customer usage data which provides company an accurate data driven pictures of user experience and the areas experienced by outage Mainly useful to estimate and setup repairs to cell towers based on how it impacts customer experience By predicting and identifying the tower behavior which tower needs maintenance and fixing the repairs before they become inactive which helped TMobile in improving customer experience and saved costs Responsibilities Responsible for ingesting large volumes of data into Hadoop Data Lake Pipeline on daily basis Developed Spark programs to Transform and Analyze the data Build data pipeline using Kafka and developed Kafka producers to stream data from external source to Kafka Topics Developed Kafka Consumer APIs in Scala for consuming data from Kafka topics and process consumed data using Spark and load the processed streams to HIVE for analytics Developed Spark applications using Scala for performing data cleansing event enrichment data aggregation and data preparation needed for reporting teams to consume Experienced in handling large datasets using Spark in Memory capabilities using Broadcast variables in Spark efficient Joins transformations and other capabilities Used Scala based Spark RDDs and Spark SQL for faster testing and processing of ETL workloads and loaded the results into HIVE Designed and Developed endtoend process from various source systems to staging area from staging to Data Marts Transformed RDDs into Data Frames using CaseClass and toDF also published the end reports to HDFS Involved in creating HIVE tables imported processed data into tables and done analytics as per business developments For performance optimization implemented partitions bucketing and compression techniques and Hive on Tez for fast query processing Worked on troubleshooting and Performance optimization on Spark Application to improve the overall processing time and make more error tolerant for the pipeline Used Zookeeper to provide coordination services to the cluster Used MySQL for storing customer data moved data to HDFS when needed and run Spark jobs for analyzing customer behavior Worked on NOSQL database HBase for Further analysis Run Sqoop jobs to migrate data from MySQL Database to Hadoop File System Generated daily final reports on processed data using Tableau Used JIRA tracking tool for assigning and defect management Worked in Agile development environment and actively involved in daily Scrum and other design related meetings Environment Hortonworks HDP Hadoop HDFS Hive Spark Scala Kafka Sqoop Oozie Zookeeper MySQL HBase Jira Tableau Java Developer Mahindra Convivia Technologies Bangalore KARNATAKA IN May 2014 to November 2015 Bangalore India May 2014 Nov 2015 Project Name Integrated Messaging IM Role Java Developer Mahindra Convivia is a valueadded service provider for mobile operations and sells solutions in Consumer value management Business and Financial Solutions It provides mobile apps and a variety of voice SMS and Internet Services Developed application which provides all the VAS services like SMS MMS Data using Integrated Messaging Platform which enables the user to configure services based on their requirements Application also provides insight of the traffic and load on the application with statistics Responsibilities Involved in all phases of project development requirements gathering analysis design development coding and testing Developed the application using Spring MVC and involved in setting up the Spring Bean Profiling Developed the data layer for the applications using Spring Hibernate ORM and developed various business logic and services using HQL Used Hibernate named queries concept to retrieve data from the database and integrate with Spring MVC to interact with back end persistence system SQL Server Used Maven script for building and deploying the application Strong experience in SQL and knowledge of MySQL Server database Wrote SQL queries to fetch the statistics from the database and displayed the statistics in the application UI Experience in ETL software development Involved in code review and documentation review of technical artifacts Testing of the product Unit Testing Regression Testing and Integration Testing Used Tortoise SVN for source code versioning and code repository Used Jenkins for Continuous Integration Builds and Deployments CICD Used JIRA to track bugs Environment Java Hibernate Spring MVC MySQL Maven 32 JIRA Jenkins Agile Eclipse IDE Additional Information Technical Skills BigData Technologies HDFS Spark Hive MapReduce Sqoop Oozie HBase and Streaming KafkaConnect Processing Spark Spark Streaming Spark SQL Hive Big Data Platforms Cloudera Hortonworks MapR Databases RDBMS MySQL SQLPLSQL MSSQL Server 2005 Oracle 9i10g11g Language Java Scala PLSQL ETL Tools Kafka Sqoop ETL Talend Data Integration Big Data Integration Informatica Power Center Software Life Cycles SDLC Agile models Cloud Platforms Amazon EC2 S3 EMR Bug Tracking Tools Jira Rally",
    "entities": [
        "Oracle SQL Server",
        "Spark Context",
        "Streaming KafkaConnect Processing Spark Spark",
        "the Spring",
        "Hadoop Developer Hadoop",
        "Apache Sqoop",
        "Developed Spark",
        "Software Process Engineering",
        "tsqoopImport",
        "Hadoop",
        "Oracle 11203",
        "HDFS Involved",
        "Atlanta",
        "NOSQL",
        "apps",
        "HBase",
        "Apache Spark",
        "MySQL Database",
        "Data Models",
        "JAVAJ2EE",
        "Testing",
        "SPARK Applications",
        "Developed",
        "Big Data Platforms",
        "TMobile",
        "Possess",
        "MMS Data",
        "SQLPLSQL",
        "Spark Application",
        "Marketting",
        "Business and Financial Solutions",
        "Ingestion Storage Querying Processing and Analysis of Big data Integration and Talend Integration Worked",
        "JSP",
        "Collaborated",
        "UI Experience",
        "Spark Streaming",
        "HDP",
        "MVC",
        "Spark",
        "Agile",
        "Scala Design",
        "Hive Tables",
        "BitBucket",
        "US",
        "Sqoop",
        "HIVE",
        "Amazon Webservices AWS Cloud Services",
        "Hadoop Data Lake Pipeline",
        "Hortonworks HDP Hadoop",
        "GITHUB BitBucket",
        "Responsibilities Responsible",
        "SQL",
        "Data Designed and Implemented ETL Process",
        "Spark Data Frames",
        "Object Oriented Analysis Design OOAD",
        "Big Data",
        "Target Tables Once",
        "HBase Worked",
        "Telecom Strong",
        "ETL",
        "Java SQL Big Data Technologies",
        "India",
        "Data Frames",
        "Maven",
        "Data Modelling",
        "Cassandra Experience",
        "Spark SQL",
        "Cisco",
        "BA",
        "Work Experience Hadoop Developer",
        "Data Experience",
        "Oracle and Hive",
        "Dimensional ModellingStar Schema",
        "Talend Data Integration Big Data Integration Informatica Power Center Software Life Cycles SDLC",
        "MapReduce",
        "Financial",
        "UML Methodology",
        "NoSQL",
        "CaseClass",
        "Developer Mahindra Convivia",
        "Application",
        "IDE Additional Information Technical Skills BigData Technologies HDFS Spark Hive MapReduce Sqoop Oozie HBase",
        "Project Name Continuous Service"
    ],
    "experience": "Experience in developing MapReduce programs and custom UDFs for data processing using Java and Scala Hands on experience working on different Hadoop components like Big Data Talend Kafka Spark Hive HBase Sqoop Cassandra and Zookeeper and used tHDFSInput Thdfs Output TPigLoad tPigFilterRow and tsqoopImport and export Involved in Creating Hive Tables and load processed data into tables and finetuned performance using different optimization techniques Worked on importing and exporting data using Apache Sqoop from RDBMS to Hadoop Platform and vice versa Knowledge on troubleshooting failures in spark applications and finetuning for better performance Hands on experience working with Dimensional ModellingStar Schema and Snowflake Schema Experience in using Zookeeper for coordinating the distributed applications Good experience with NoSQL databases like HBase and Cassandra Experience in Object Oriented Analysis Design OOAD and development of software using UML Methodology good knowledge of J2EE design patterns and Core Java design patterns Expertise in various JAVAJ2EE technologies such as JSP 20 Servlets 2x Struts 1220 Hibernate 2030 ORM Spring 2030 JDBC Very good experience in complete project life cycle design development testing and implementation of Client Server and Web applications Possess excellent communication and analytical skills along with a cando attitude Experience in developing Web Applications with various Open Source frameworks Spring Hibernate 2030 ORM Frameworks Proficient in Core Java J2EE JDBC Servlets JSP Exception Handling Multithreading Concepts Experience in Testing and documenting software for client applications Good experience in using Data Modelling techniques to find the results based on SQL and PLSQL queries Worked and learned a great deal from Amazon Webservices AWS Cloud Services like EC2 S3 and EMR Experience working with different databases such as Oracle SQL Server and MySQL writing stored procedures functions joins and triggers for different Data Models Experience in using version control tools like GITHUB BitBucket share the code snippet among the team members Experience in working in Agile project management Strong motivational skills familiarity with various technologies ability to learn quickly dealing with people commitment to work and believes in hardworking Work Experience Hadoop Developer Cisco September 2018 to March 2019 Project Name Marketing IT Project description The primary goal of this project is to develop its enterprise data hub for storing accessing and analyzing Marketting data At Cisco company stores large volumes of customer data from different sources like web pages transactions and SFDC This will be core to its overall information delivery strategy and allows team comprehensive view and study of customer behaviour and target the customer based on their intrests Responsibilities Responsible for ingesting large volumes of data into Hadoop Data Lake Pipeline on daily basis Read data from different source like Oracle and Hive using Apache Spark and process consumed data using Spark and load the processed to HIVE and downstream applications for different purposes Developed Apache spark application using Java to perform different kind of validations and standardization on fields based on certain validations rules on incoming Data Designed and Implemented ETL Process to load data from Source to Target Tables Once data processed loaded data into different Hive Tables based on the derived fields Used Java based Spark RDDs Data Frames and Datasets Spark SQL faster processing of ETL workloads For performance optimization implemented different optimization techniques optimizations like Partition Bucketing and Map Side joins for fast query processing Excellent working Knowledge in Spark Core Spark SQL Spark Streaming Implemented Spark using Scala and Spark SQL for faster testing and processing of data Involved in converting HiveSQL queries into Spark transformations using Spark Data Frames and Scala Design data ingestion and integration process using SQOOP Shell Scripts Pig with Hive Optimizing existing algorithms in Hadoop using Spark Context SparkSQL Data Frames and Pair RDDs Worked on Error Handling techniques and tuning ETL flow for better performance Generated Java APIs for retrieval and analysis on NoSQL database such as HBase Worked on troubleshooting and Performance optimization on Spark Application to improve the overall processing time and make more error tolerant for the pipeline Collaborated with the infrastructure network database application and BA teams to ensure data quality and availability Used NOSQL HBase for storing batching information Used Rally tracking tool for assigning and defect management Worked in Agile development environment and actively involved in daily Scrum and other design related meetings Used BitBucket to share the code snippet among the team members Environment Spark 211 Oracle 11203 Apache Solar 665 Datastax Cassandra 300 Hbase118 Talend Bitbucket Rally Hadoop Developer T Mobile Atlanta GA June 2017 to August 2018 Project Name Continuous Service Providing T Mobile is one of the top Telecommunication company in US providing cell phone services Its the second Biggest mobile network operator serving clients with high quality mobile networks solutions For Continuous Service Providing and detecting the towers cables and routers before getting failed T mobile stores large volumes of customer usage data which provides company an accurate data driven pictures of user experience and the areas experienced by outage Mainly useful to estimate and setup repairs to cell towers based on how it impacts customer experience By predicting and identifying the tower behavior which tower needs maintenance and fixing the repairs before they become inactive which helped TMobile in improving customer experience and saved costs Responsibilities Responsible for ingesting large volumes of data into Hadoop Data Lake Pipeline on daily basis Developed Spark programs to Transform and Analyze the data Build data pipeline using Kafka and developed Kafka producers to stream data from external source to Kafka Topics Developed Kafka Consumer APIs in Scala for consuming data from Kafka topics and process consumed data using Spark and load the processed streams to HIVE for analytics Developed Spark applications using Scala for performing data cleansing event enrichment data aggregation and data preparation needed for reporting teams to consume Experienced in handling large datasets using Spark in Memory capabilities using Broadcast variables in Spark efficient Joins transformations and other capabilities Used Scala based Spark RDDs and Spark SQL for faster testing and processing of ETL workloads and loaded the results into HIVE Designed and Developed endtoend process from various source systems to staging area from staging to Data Marts Transformed RDDs into Data Frames using CaseClass and toDF also published the end reports to HDFS Involved in creating HIVE tables imported processed data into tables and done analytics as per business developments For performance optimization implemented partitions bucketing and compression techniques and Hive on Tez for fast query processing Worked on troubleshooting and Performance optimization on Spark Application to improve the overall processing time and make more error tolerant for the pipeline Used Zookeeper to provide coordination services to the cluster Used MySQL for storing customer data moved data to HDFS when needed and run Spark jobs for analyzing customer behavior Worked on NOSQL database HBase for Further analysis Run Sqoop jobs to migrate data from MySQL Database to Hadoop File System Generated daily final reports on processed data using Tableau Used JIRA tracking tool for assigning and defect management Worked in Agile development environment and actively involved in daily Scrum and other design related meetings Environment Hortonworks HDP Hadoop HDFS Hive Spark Scala Kafka Sqoop Oozie Zookeeper MySQL HBase Jira Tableau Java Developer Mahindra Convivia Technologies Bangalore KARNATAKA IN May 2014 to November 2015 Bangalore India May 2014 Nov 2015 Project Name Integrated Messaging IM Role Java Developer Mahindra Convivia is a valueadded service provider for mobile operations and sells solutions in Consumer value management Business and Financial Solutions It provides mobile apps and a variety of voice SMS and Internet Services Developed application which provides all the VAS services like SMS MMS Data using Integrated Messaging Platform which enables the user to configure services based on their requirements Application also provides insight of the traffic and load on the application with statistics Responsibilities Involved in all phases of project development requirements gathering analysis design development coding and testing Developed the application using Spring MVC and involved in setting up the Spring Bean Profiling Developed the data layer for the applications using Spring Hibernate ORM and developed various business logic and services using HQL Used Hibernate named queries concept to retrieve data from the database and integrate with Spring MVC to interact with back end persistence system SQL Server Used Maven script for building and deploying the application Strong experience in SQL and knowledge of MySQL Server database Wrote SQL queries to fetch the statistics from the database and displayed the statistics in the application UI Experience in ETL software development Involved in code review and documentation review of technical artifacts Testing of the product Unit Testing Regression Testing and Integration Testing Used Tortoise SVN for source code versioning and code repository Used Jenkins for Continuous Integration Builds and Deployments CICD Used JIRA to track bugs Environment Java Hibernate Spring MVC MySQL Maven 32 JIRA Jenkins Agile Eclipse IDE Additional Information Technical Skills BigData Technologies HDFS Spark Hive MapReduce Sqoop Oozie HBase and Streaming KafkaConnect Processing Spark Spark Streaming Spark SQL Hive Big Data Platforms Cloudera Hortonworks MapR Databases RDBMS MySQL SQLPLSQL MSSQL Server 2005 Oracle 9i10g11 g Language Java Scala PLSQL ETL Tools Kafka Sqoop ETL Talend Data Integration Big Data Integration Informatica Power Center Software Life Cycles SDLC Agile models Cloud Platforms Amazon EC2 S3 EMR Bug Tracking Tools Jira Rally",
    "extracted_keywords": [
        "Hadoop",
        "Developer",
        "Hadoop",
        "span",
        "lDeveloperspan",
        "Hadoop",
        "Developer",
        "years",
        "IT",
        "experience",
        "exposure",
        "Software",
        "Process",
        "Engineering",
        "building",
        "enterprise",
        "applications",
        "Java",
        "SQL",
        "Big",
        "Data",
        "Technologies",
        "domains",
        "sector",
        "Telecom",
        "Strong",
        "working",
        "experience",
        "Big",
        "Data",
        "Platforms",
        "Cloudera",
        "CDH",
        "Hortonworks",
        "HDP",
        "MapR",
        "Distribution",
        "Strong",
        "working",
        "experience",
        "Ingestion",
        "Storage",
        "Querying",
        "Processing",
        "Analysis",
        "data",
        "Integration",
        "Talend",
        "Integration",
        "SPARK",
        "Applications",
        "Scala",
        "Java",
        "Spark",
        "Core",
        "Spark",
        "Data",
        "Frames",
        "Spark",
        "SQL",
        "Spark",
        "Streaming",
        "APIs",
        "processing",
        "Data",
        "Experience",
        "MapReduce",
        "programs",
        "custom",
        "UDFs",
        "data",
        "processing",
        "Java",
        "Scala",
        "Hands",
        "experience",
        "Hadoop",
        "components",
        "Big",
        "Data",
        "Talend",
        "Kafka",
        "Spark",
        "Hive",
        "HBase",
        "Sqoop",
        "Cassandra",
        "Zookeeper",
        "tHDFSInput",
        "Thdfs",
        "Output",
        "TPigLoad",
        "tPigFilterRow",
        "tsqoopImport",
        "export",
        "Hive",
        "Tables",
        "data",
        "tables",
        "performance",
        "optimization",
        "techniques",
        "data",
        "Apache",
        "Sqoop",
        "RDBMS",
        "Hadoop",
        "Platform",
        "vice",
        "Knowledge",
        "troubleshooting",
        "failures",
        "spark",
        "applications",
        "performance",
        "Hands",
        "experience",
        "Dimensional",
        "ModellingStar",
        "Schema",
        "Snowflake",
        "Schema",
        "Experience",
        "Zookeeper",
        "applications",
        "experience",
        "databases",
        "HBase",
        "Cassandra",
        "Experience",
        "Object",
        "Oriented",
        "Analysis",
        "Design",
        "OOAD",
        "development",
        "software",
        "UML",
        "Methodology",
        "knowledge",
        "J2EE",
        "design",
        "patterns",
        "Core",
        "Java",
        "design",
        "patterns",
        "Expertise",
        "JAVAJ2EE",
        "technologies",
        "JSP",
        "Servlets",
        "Struts",
        "Hibernate",
        "ORM",
        "Spring",
        "JDBC",
        "experience",
        "project",
        "life",
        "cycle",
        "design",
        "development",
        "testing",
        "implementation",
        "Client",
        "Server",
        "Web",
        "applications",
        "communication",
        "skills",
        "cando",
        "attitude",
        "Experience",
        "Web",
        "Applications",
        "Open",
        "Source",
        "frameworks",
        "Spring",
        "Hibernate",
        "ORM",
        "Frameworks",
        "Proficient",
        "Core",
        "Java",
        "J2EE",
        "JDBC",
        "Servlets",
        "JSP",
        "Exception",
        "Handling",
        "Multithreading",
        "Concepts",
        "Experience",
        "Testing",
        "documenting",
        "software",
        "client",
        "applications",
        "experience",
        "Data",
        "Modelling",
        "techniques",
        "results",
        "SQL",
        "PLSQL",
        "deal",
        "Amazon",
        "Webservices",
        "AWS",
        "Cloud",
        "Services",
        "EC2",
        "S3",
        "EMR",
        "Experience",
        "databases",
        "Oracle",
        "SQL",
        "Server",
        "MySQL",
        "procedures",
        "functions",
        "triggers",
        "Data",
        "Models",
        "Experience",
        "version",
        "control",
        "tools",
        "GITHUB",
        "BitBucket",
        "code",
        "snippet",
        "team",
        "members",
        "project",
        "management",
        "motivational",
        "skills",
        "familiarity",
        "technologies",
        "ability",
        "people",
        "commitment",
        "Work",
        "Experience",
        "Hadoop",
        "Developer",
        "Cisco",
        "September",
        "March",
        "Project",
        "Name",
        "Marketing",
        "IT",
        "Project",
        "description",
        "goal",
        "project",
        "enterprise",
        "data",
        "hub",
        "accessing",
        "Marketting",
        "data",
        "Cisco",
        "company",
        "volumes",
        "customer",
        "data",
        "sources",
        "web",
        "pages",
        "transactions",
        "SFDC",
        "information",
        "delivery",
        "strategy",
        "team",
        "view",
        "study",
        "customer",
        "behaviour",
        "customer",
        "intrests",
        "Responsibilities",
        "volumes",
        "data",
        "Hadoop",
        "Data",
        "Lake",
        "Pipeline",
        "basis",
        "Read",
        "data",
        "source",
        "Oracle",
        "Hive",
        "Apache",
        "Spark",
        "process",
        "data",
        "Spark",
        "HIVE",
        "applications",
        "purposes",
        "Apache",
        "spark",
        "application",
        "Java",
        "kind",
        "validations",
        "standardization",
        "fields",
        "validations",
        "Data",
        "ETL",
        "Process",
        "data",
        "Source",
        "Target",
        "Tables",
        "data",
        "data",
        "Hive",
        "Tables",
        "fields",
        "Java",
        "Spark",
        "RDDs",
        "Data",
        "Frames",
        "Datasets",
        "Spark",
        "SQL",
        "processing",
        "ETL",
        "workloads",
        "performance",
        "optimization",
        "optimization",
        "techniques",
        "optimizations",
        "Partition",
        "Bucketing",
        "Map",
        "Side",
        "query",
        "Excellent",
        "Knowledge",
        "Spark",
        "Core",
        "Spark",
        "SQL",
        "Spark",
        "Streaming",
        "Spark",
        "Scala",
        "Spark",
        "SQL",
        "testing",
        "processing",
        "data",
        "HiveSQL",
        "queries",
        "Spark",
        "transformations",
        "Spark",
        "Data",
        "Frames",
        "Scala",
        "Design",
        "data",
        "ingestion",
        "integration",
        "process",
        "SQOOP",
        "Shell",
        "Scripts",
        "Pig",
        "Hive",
        "Optimizing",
        "algorithms",
        "Hadoop",
        "Spark",
        "Context",
        "SparkSQL",
        "Data",
        "Frames",
        "Pair",
        "RDDs",
        "Error",
        "Handling",
        "techniques",
        "ETL",
        "flow",
        "performance",
        "Java",
        "APIs",
        "retrieval",
        "analysis",
        "NoSQL",
        "database",
        "HBase",
        "troubleshooting",
        "Performance",
        "optimization",
        "Spark",
        "Application",
        "processing",
        "time",
        "error",
        "pipeline",
        "infrastructure",
        "network",
        "database",
        "application",
        "BA",
        "teams",
        "data",
        "quality",
        "availability",
        "NOSQL",
        "HBase",
        "information",
        "Rally",
        "tool",
        "management",
        "Agile",
        "development",
        "environment",
        "Scrum",
        "design",
        "meetings",
        "BitBucket",
        "code",
        "snippet",
        "team",
        "members",
        "Environment",
        "Spark",
        "Oracle",
        "Apache",
        "Solar",
        "Datastax",
        "Cassandra",
        "Hbase118",
        "Talend",
        "Bitbucket",
        "Rally",
        "Hadoop",
        "Developer",
        "T",
        "Mobile",
        "Atlanta",
        "GA",
        "June",
        "August",
        "Project",
        "Name",
        "Continuous",
        "Service",
        "Providing",
        "T",
        "Mobile",
        "Telecommunication",
        "company",
        "US",
        "cell",
        "phone",
        "services",
        "network",
        "operator",
        "clients",
        "quality",
        "networks",
        "solutions",
        "Continuous",
        "Service",
        "Providing",
        "towers",
        "cables",
        "routers",
        "T",
        "mobile",
        "stores",
        "volumes",
        "customer",
        "usage",
        "data",
        "company",
        "data",
        "pictures",
        "user",
        "experience",
        "areas",
        "outage",
        "repairs",
        "cell",
        "towers",
        "customer",
        "experience",
        "tower",
        "behavior",
        "tower",
        "maintenance",
        "repairs",
        "TMobile",
        "customer",
        "experience",
        "costs",
        "Responsibilities",
        "volumes",
        "data",
        "Hadoop",
        "Data",
        "Lake",
        "Pipeline",
        "basis",
        "Spark",
        "programs",
        "Transform",
        "data",
        "Build",
        "data",
        "pipeline",
        "Kafka",
        "Kafka",
        "producers",
        "data",
        "source",
        "Kafka",
        "Topics",
        "Kafka",
        "Consumer",
        "APIs",
        "Scala",
        "data",
        "Kafka",
        "topics",
        "process",
        "data",
        "Spark",
        "streams",
        "analytics",
        "Spark",
        "applications",
        "Scala",
        "data",
        "cleansing",
        "event",
        "enrichment",
        "data",
        "aggregation",
        "data",
        "preparation",
        "teams",
        "datasets",
        "Spark",
        "Memory",
        "capabilities",
        "Broadcast",
        "variables",
        "Spark",
        "Joins",
        "transformations",
        "capabilities",
        "Scala",
        "Spark",
        "RDDs",
        "Spark",
        "SQL",
        "testing",
        "processing",
        "ETL",
        "workloads",
        "results",
        "HIVE",
        "endtoend",
        "process",
        "source",
        "systems",
        "area",
        "Data",
        "Marts",
        "Transformed",
        "RDDs",
        "Data",
        "Frames",
        "CaseClass",
        "toDF",
        "end",
        "reports",
        "HDFS",
        "HIVE",
        "tables",
        "data",
        "tables",
        "analytics",
        "business",
        "developments",
        "performance",
        "optimization",
        "partitions",
        "bucketing",
        "compression",
        "techniques",
        "Hive",
        "Tez",
        "query",
        "troubleshooting",
        "Performance",
        "optimization",
        "Spark",
        "Application",
        "processing",
        "time",
        "error",
        "pipeline",
        "Zookeeper",
        "coordination",
        "services",
        "cluster",
        "MySQL",
        "customer",
        "data",
        "data",
        "HDFS",
        "Spark",
        "jobs",
        "customer",
        "behavior",
        "NOSQL",
        "database",
        "HBase",
        "analysis",
        "Run",
        "Sqoop",
        "jobs",
        "data",
        "MySQL",
        "Database",
        "Hadoop",
        "File",
        "System",
        "reports",
        "data",
        "Tableau",
        "JIRA",
        "tool",
        "management",
        "Agile",
        "development",
        "environment",
        "Scrum",
        "design",
        "meetings",
        "Environment",
        "Hortonworks",
        "HDP",
        "Hadoop",
        "HDFS",
        "Hive",
        "Spark",
        "Scala",
        "Kafka",
        "Sqoop",
        "Oozie",
        "Zookeeper",
        "MySQL",
        "HBase",
        "Jira",
        "Tableau",
        "Java",
        "Developer",
        "Mahindra",
        "Convivia",
        "Technologies",
        "Bangalore",
        "KARNATAKA",
        "May",
        "November",
        "Bangalore",
        "India",
        "May",
        "Nov",
        "Project",
        "Name",
        "Integrated",
        "Messaging",
        "IM",
        "Role",
        "Java",
        "Developer",
        "Mahindra",
        "Convivia",
        "service",
        "provider",
        "operations",
        "solutions",
        "Consumer",
        "value",
        "management",
        "Business",
        "Financial",
        "Solutions",
        "apps",
        "variety",
        "voice",
        "SMS",
        "Internet",
        "Services",
        "application",
        "VAS",
        "services",
        "SMS",
        "MMS",
        "Data",
        "Integrated",
        "Messaging",
        "Platform",
        "user",
        "configure",
        "services",
        "requirements",
        "Application",
        "insight",
        "traffic",
        "load",
        "application",
        "statistics",
        "Responsibilities",
        "phases",
        "project",
        "development",
        "requirements",
        "analysis",
        "design",
        "development",
        "application",
        "Spring",
        "MVC",
        "Spring",
        "Bean",
        "Profiling",
        "data",
        "layer",
        "applications",
        "Spring",
        "Hibernate",
        "ORM",
        "business",
        "logic",
        "services",
        "HQL",
        "Hibernate",
        "queries",
        "concept",
        "data",
        "database",
        "Spring",
        "MVC",
        "end",
        "persistence",
        "system",
        "SQL",
        "Server",
        "Maven",
        "script",
        "building",
        "application",
        "experience",
        "SQL",
        "knowledge",
        "MySQL",
        "Server",
        "database",
        "Wrote",
        "SQL",
        "statistics",
        "database",
        "statistics",
        "application",
        "UI",
        "Experience",
        "ETL",
        "software",
        "development",
        "code",
        "review",
        "documentation",
        "review",
        "artifacts",
        "product",
        "Unit",
        "Testing",
        "Regression",
        "Testing",
        "Integration",
        "Testing",
        "Tortoise",
        "SVN",
        "source",
        "code",
        "versioning",
        "code",
        "repository",
        "Jenkins",
        "Continuous",
        "Integration",
        "Builds",
        "Deployments",
        "CICD",
        "JIRA",
        "bugs",
        "Environment",
        "Java",
        "Hibernate",
        "Spring",
        "MVC",
        "MySQL",
        "Maven",
        "JIRA",
        "Jenkins",
        "Agile",
        "Eclipse",
        "IDE",
        "Additional",
        "Information",
        "Technical",
        "Skills",
        "BigData",
        "Technologies",
        "HDFS",
        "Spark",
        "Hive",
        "MapReduce",
        "Sqoop",
        "Oozie",
        "HBase",
        "Streaming",
        "KafkaConnect",
        "Processing",
        "Spark",
        "Spark",
        "Streaming",
        "Spark",
        "SQL",
        "Hive",
        "Big",
        "Data",
        "Platforms",
        "Cloudera",
        "Hortonworks",
        "MapR",
        "Databases",
        "MySQL",
        "SQLPLSQL",
        "MSSQL",
        "Server",
        "Oracle",
        "9i10g11",
        "g",
        "Language",
        "Java",
        "Scala",
        "PLSQL",
        "ETL",
        "Tools",
        "Kafka",
        "Sqoop",
        "ETL",
        "Talend",
        "Data",
        "Integration",
        "Big",
        "Data",
        "Integration",
        "Informatica",
        "Power",
        "Center",
        "Software",
        "Life",
        "Cycles",
        "SDLC",
        "models",
        "Cloud",
        "Amazon",
        "EC2",
        "S3",
        "EMR",
        "Bug",
        "Tracking",
        "Tools",
        "Jira",
        "Rally"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T22:09:59.556539",
    "resume_data": "Hadoop Developer Hadoop span lDeveloperspan Hadoop Developer Having 3 years of overall IT experience diversified exposure in Software Process Engineering developing building enterprise applications using Java SQL Big Data Technologies in various domains like Financial sector and Telecom Strong working experience on different Big Data Platforms like Cloudera CDH Hortonworks HDP and MapR Distribution Strong working experience with Ingestion Storage Querying Processing and Analysis of Big data Integration and Talend Integration Worked on developing SPARK Applications using Scala and Java Spark Core Spark Data Frames Spark SQL and Spark Streaming APIs for Fast processing of Data Experience in developing MapReduce programs and custom UDFs for data processing using Java and Scala Hands on experience working on different Hadoop components like Big Data Talend Kafka Spark Hive HBase Sqoop Cassandra and Zookeeper and used tHDFSInput Thdfs Output TPigLoad tPigFilterRow and tsqoopImport and export Involved in Creating Hive Tables and load processed data into tables and finetuned performance using different optimization techniques Worked on importing and exporting data using Apache Sqoop from RDBMS to Hadoop Platform and vice versa Knowledge on troubleshooting failures in spark applications and finetuning for better performance Hands on experience working with Dimensional ModellingStar Schema and Snowflake Schema Experience in using Zookeeper for coordinating the distributed applications Good experience with NoSQL databases like HBase and Cassandra Experience in Object Oriented Analysis Design OOAD and development of software using UML Methodology good knowledge of J2EE design patterns and Core Java design patterns Expertise in various JAVAJ2EE technologies such as JSP 20 Servlets 2x Struts 1220 Hibernate 2030 ORM Spring 2030 JDBC Very good experience in complete project life cycle design development testing and implementation of Client Server and Web applications Possess excellent communication and analytical skills along with a cando attitude Experience in developing Web Applications with various Open Source frameworks Spring Hibernate 2030 ORM Frameworks Proficient in Core Java J2EE JDBC Servlets JSP Exception Handling Multithreading Concepts Experience in Testing and documenting software for client applications Good experience in using Data Modelling techniques to find the results based on SQL and PLSQL queries Worked and learned a great deal from Amazon Webservices AWS Cloud Services like EC2 S3 and EMR Experience working with different databases such as Oracle SQL Server and MySQL writing stored procedures functions joins and triggers for different Data Models Experience in using version control tools like GITHUB BitBucket share the code snippet among the team members Experience in working in Agile project management Strong motivational skills familiarity with various technologies ability to learn quickly dealing with people commitment to work and believes in hardworking Work Experience Hadoop Developer Cisco September 2018 to March 2019 Project Name Marketing IT Project description The primary goal of this project is to develop its enterprise data hub for storing accessing and analyzing Marketting data At Cisco company stores large volumes of customer data from different sources like web pages transactions and SFDC This will be core to its overall information delivery strategy and allows team comprehensive view and study of customer behaviour and target the customer based on their intrests Responsibilities Responsible for ingesting large volumes of data into Hadoop Data Lake Pipeline on daily basis Read data from different source like Oracle and Hive using Apache Spark and process consumed data using Spark and load the processed to HIVE and downstream applications for different purposes Developed Apache spark application using Java to perform different kind of validations and standardization on fields based on certain validations rules on incoming Data Designed and Implemented ETL Process to load data from Source to Target Tables Once data processed loaded data into different Hive Tables based on the derived fields Used Java based Spark RDDs Data Frames and Datasets Spark SQL faster processing of ETL workloads For performance optimization implemented different optimization techniques optimizations like Partition Bucketing and Map Side joins for fast query processing Excellent working Knowledge in Spark Core Spark SQL Spark Streaming Implemented Spark using Scala and Spark SQL for faster testing and processing of data Involved in converting HiveSQL queries into Spark transformations using Spark Data Frames and Scala Design data ingestion and integration process using SQOOP Shell Scripts Pig with Hive Optimizing existing algorithms in Hadoop using Spark Context SparkSQL Data Frames and Pair RDDs Worked on Error Handling techniques and tuning ETL flow for better performance Generated Java APIs for retrieval and analysis on NoSQL database such as HBase Worked on troubleshooting and Performance optimization on Spark Application to improve the overall processing time and make more error tolerant for the pipeline Collaborated with the infrastructure network database application and BA teams to ensure data quality and availability Used NOSQL HBase for storing batching information Used Rally tracking tool for assigning and defect management Worked in Agile development environment and actively involved in daily Scrum and other design related meetings Used BitBucket to share the code snippet among the team members Environment Spark 211 Oracle 11203 Apache Solar 665 Datastax Cassandra 300 Hbase118 Talend Bitbucket Rally Hadoop Developer T Mobile Atlanta GA June 2017 to August 2018 Project Name Continuous Service Providing T Mobile is one of the top Telecommunication company in US providing cell phone services Its the second Biggest mobile network operator serving clients with high quality mobile networks solutions For Continuous Service Providing and detecting the towers cables and routers before getting failed T mobile stores large volumes of customer usage data which provides company an accurate data driven pictures of user experience and the areas experienced by outage Mainly useful to estimate and setup repairs to cell towers based on how it impacts customer experience By predicting and identifying the tower behavior which tower needs maintenance and fixing the repairs before they become inactive which helped TMobile in improving customer experience and saved costs Responsibilities Responsible for ingesting large volumes of data into Hadoop Data Lake Pipeline on daily basis Developed Spark programs to Transform and Analyze the data Build data pipeline using Kafka and developed Kafka producers to stream data from external source to Kafka Topics Developed Kafka Consumer APIs in Scala for consuming data from Kafka topics and process consumed data using Spark and load the processed streams to HIVE for analytics Developed Spark applications using Scala for performing data cleansing event enrichment data aggregation and data preparation needed for reporting teams to consume Experienced in handling large datasets using Spark in Memory capabilities using Broadcast variables in Spark efficient Joins transformations and other capabilities Used Scala based Spark RDDs and Spark SQL for faster testing and processing of ETL workloads and loaded the results into HIVE Designed and Developed endtoend process from various source systems to staging area from staging to Data Marts Transformed RDDs into Data Frames using CaseClass and toDF also published the end reports to HDFS Involved in creating HIVE tables imported processed data into tables and done analytics as per business developments For performance optimization implemented partitions bucketing and compression techniques and Hive on Tez for fast query processing Worked on troubleshooting and Performance optimization on Spark Application to improve the overall processing time and make more error tolerant for the pipeline Used Zookeeper to provide coordination services to the cluster Used MySQL for storing customer data moved data to HDFS when needed and run Spark jobs for analyzing customer behavior Worked on NOSQL database HBase for Further analysis Run Sqoop jobs to migrate data from MySQL Database to Hadoop File System Generated daily final reports on processed data using Tableau Used JIRA tracking tool for assigning and defect management Worked in Agile development environment and actively involved in daily Scrum and other design related meetings Environment Hortonworks HDP Hadoop HDFS Hive Spark Scala Kafka Sqoop Oozie Zookeeper MySQL HBase Jira Tableau Java Developer Mahindra Convivia Technologies Bangalore KARNATAKA IN May 2014 to November 2015 Bangalore India May 2014 Nov 2015 Project Name Integrated Messaging IM Role Java Developer Mahindra Convivia is a valueadded service provider for mobile operations and sells solutions in Consumer value management Business and Financial Solutions It provides mobile apps and a variety of voice SMS and Internet Services Developed application which provides all the VAS services like SMS MMS Data using Integrated Messaging Platform which enables the user to configure services based on their requirements Application also provides insight of the traffic and load on the application with statistics Responsibilities Involved in all phases of project development requirements gathering analysis design development coding and testing Developed the application using Spring MVC and involved in setting up the Spring Bean Profiling Developed the data layer for the applications using Spring Hibernate ORM and developed various business logic and services using HQL Used Hibernate named queries concept to retrieve data from the database and integrate with Spring MVC to interact with back end persistence system SQL Server Used Maven script for building and deploying the application Strong experience in SQL and knowledge of MySQL Server database Wrote SQL queries to fetch the statistics from the database and displayed the statistics in the application UI Experience in ETL software development Involved in code review and documentation review of technical artifacts Testing of the product Unit Testing Regression Testing and Integration Testing Used Tortoise SVN for source code versioning and code repository Used Jenkins for Continuous Integration Builds and Deployments CICD Used JIRA to track bugs Environment Java Hibernate Spring MVC MySQL Maven 32 JIRA Jenkins Agile Eclipse IDE Additional Information Technical Skills BigData Technologies HDFS Spark Hive MapReduce Sqoop Oozie HBase and Streaming KafkaConnect Processing Spark Spark Streaming Spark SQL Hive Big Data Platforms Cloudera Hortonworks MapR Databases RDBMS MySQL SQLPLSQL MSSQL Server 2005 Oracle 9i10g11g Language Java Scala PLSQL ETL Tools Kafka Sqoop ETL Talend Data Integration Big Data Integration Informatica Power Center Software Life Cycles SDLC Agile models Cloud Platforms Amazon EC2 S3 EMR Bug Tracking Tools Jira Rally",
    "unique_id": "39de77f3-e52b-4da5-8c60-92d437b6270c"
}