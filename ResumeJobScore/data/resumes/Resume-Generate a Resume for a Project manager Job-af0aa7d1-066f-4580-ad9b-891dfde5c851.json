{
    "clean_data": "Senior JavaHadoop Python Developer Senior JavaHadoopspan lPythonspan span lDeveloperspan Senior JavaHadoop Python Developer Smith Nephew Minneapolis MN MN Work Experience Senior JavaHadoop Python Developer Smith Nephew Minneapolis MN January 2016 to Present DESCRIPTION Smith Nephew has many data sources in Mainframe and Distributed Applications with Traditional DW with Tera data Big Data Hadoop projects were launched to simplify the Data Pipelines to unify data sources for broader effective insights and actions in Customer Segmentation Product Offering Risk Management Regulatory Reporting and Fraud Detection RESPONSIBILITIES Responsible for developing efficient MapReduce on AWS cloud programs for more than 20 years worth of claim data to detect and separate fraudulent claims Worked with the advanced analytics team to design fraud detection algorithms and then developed MapReduce programs to efficiently run the algorithm on the huge datasets Ran data formatting scripts in python and created terabyte csv files to be consumed by Hadoop MapReduce jobs Performed Kafka analysis feature selection feature extraction using Apache Spark Machine Learning streaming libraries in Python Extensively used Akka actors architecture for scalable hassle free multithreading Experience using Cloudera in an application for Vendors platform Developed Python code using version control tools like GIT hub and SVN on vagrant machines Created Hive tables to store data into HDFS loading data and writing hive queries that will run internally in mapreduce way Involved in building the ETL architecture and Source to Target mapping to load data into Data warehouse Uploaded and processed terabytes of data from various structured and unstructured sources into HDFS AWS cloud using Sqoop and Flume Involved in Cluster coordination services through Zookeeper Using Scala for coding the components in Play and Akka and Used Maven to build and generate code analysis reports Involved in implementing Programmatic transaction management using AOP Exported the analyzed data to the relational databases using Sqoop for visualization and to generate reports for the BI team Played a key role in installation and configuration of the various Hadoop ecosystem tools such as Solr Pig HBase and Cassandra Developed an information pipeline utilizing Kafka and Storm to store data into HDFS Loading spilling data using Kafka Flume and real time Using Spark and Storm Implemented various hive optimization techniques like Dynamic Partitions Buckets Map Joins Parallel executions in Hive Working on handling all the requests to the systems using play framework MVC framework Worked with PreSession and PostSession Linux scripts for automation of ETL jobs and to perform operations like gunzip remove and archive files Created Talend jobs to copy the files from one server to another and utilized Talend FTP components Created Joblets and Parent child jobs in Talend Designed Developed the ETL Jobs using Talend Integration Suite by using various transformations as per the business requirements and based on ETL Mapping Specifications Extracted meaningful data from dealer csv files text files and mainframe files and generated Pythonpandas reports for data analysis Utilized Python to run scripts generate tables and reports Coordinates with Agile team to effectively meet all sprint commitments Worked on JMS like Rabbit MQ Active MQ and used JERSEY framework to implement the JAX RS Designed and Maintained Oozie workflows to manage the flow of jobs in the cluster Parse Json files through Spark core to extract schema for the production data using SparkSQL and Scala Actively updated the upper management with daily updates on the progress of project that include the classification levels that were achieved on the data ENVIRONMENT Scala language with Akka framework Java J2EE Hadoop HDFS Pig Nifi Hive MapReduce Sqoop Kafka CDH3 Cassandra Python Oozie collection Scala AWS cloud storm Ab Initio Apache SQL NoSQL Bitbucket HBase Flume spark Solr Zookeeper ETL Talend Centos Eclipse Agile JavaHadoop Developer Sprint Overland Park KS February 2015 to December 2015 DESCRIPTION Sprint Nextel Corporation is a United Statesbased holding organization that works numerous wire lines and remote systems serving consumer business and government clients I worked as a Hadoop Developer in Data Insights group where I performed analysis on gigantic information sets and helped the association get an upper hand by discovering the client patterns which helped in advertisement focusing on and organize streamlining Developed RESTful API services for postpaid on top of sprint legacy SOAP based services RESPONSIBILITIES Involved in making Hive tables stacking the information and composing Hive queries that will run inside in MapReduce Involved in converting HiveSQL queries into Spark transformations using Spark RDDs Python and Scala Wrote Python modules to view and connect the Apache Cassandra instance Involved in writing MapReduce jobs Developed and implemented two Service Endpoints end to end in Java using Play framework Akka server Hazel cast Developed the GUI module DMT using Struts framework JavaScript DOJO Ajax HTML and DHTML Developed RESTful web services interface to Javabased runtime engine and accounts Customized RESTful Web Service using Spring RESTful API sending JSON format data packets between frontend and middletier controller Realtime streaming the information utilizing Spark with Kafka Responsible for creating information pipeline utilizing flume Sqoop and pig to remove the information from weblogs and store in HDFS Involved in emitting processed information from Hadoop to relational databases or external frameworks utilizing Sqoop HDFS GET or CopyToLocal Used Play logger to run through preload and postload test cycles for application performance and errors Developed data pipeline utilizing Flume Sqoop Pig and Java MapReduce to ingest client behavioral information and money related histories into HDFS for analysis Experienced in managing and assessing Hadoop log records Used Pig to do changes like event joins filter boot traffic and some preaggregations before storing the information onto HDFS Written Hive inquiries for data to meet the business requirements Importing and sending out information into HDFS and Hive utilizing Sqoop and Kafka Created various Parser programs to extract data from Autosys Tibco Business Objects XML Informatica Java and database views using Scala Performed deployment and support of cloud services including AWS Involved in creating Hive Tables loading with data and writing Hive queries which will invoke and run MapReduce jobs in the backend Experience in Message based systems using JMS and MQ Series Implemented the workflows using Apache Oozie framework to automate tasks Developed scripts and automated data management from end to end and sync up between all the clusters ENVIRONMENT Java Hadoop Scala MapReduce MongoDB SQL Apache Yarn Hive Pig HBase Oozie Sqoop Flume Akka Play IBM MQSeries Core Java HDP HDFS Eclipse Kafka Software Engineer Merck co Boston MA January 2014 to January 2015 DESCRIPTION Merck Co is an independent practice association IPA serving some 300 000 health plan members in northern California The company contracts with managed care organizations throughout the region including HMOs belonging to Aetna CIGNA and Health Net to provide care to health plan members through its provider affiliates Its network includes about 3 700 primary care and specialty physicians 36 hospitals and 15 urgent care centers RESPONSIBILITIES Installed and configured Hadoop MapReduce HDFS Developed multiple MapReduce jobs in java for data cleaning and preprocessing Involved in installing and updating and managing Environment Collecting and aggregating large amounts of log data using Apache Flume and staging data in HDFS for further analysis Involved in running Hadoop streaming jobs to process terabytes of XML format data Participated in requirement gathering and analysis phase of the project in documenting the business requirements by conducting workshopsmeetings with various business users Involved in Sqoop HDFS Put or CopyFromLocal to ingest data Used Hive to analyze the partitioned and bucketed data and compute various metrics for reporting Created and maintained technical documentation for launching Hadoop clusters and for executing Hive queries and Pig Scripts Implemented test scripts to support testdriven development and continuous integration Implemented SQL PLSQL Stored Procedures Involved in developing Shell scripts to orchestrate the execution of all other scripts Pig Hive and MapReduce and move the data files within and outside of HDFS Involved in developing Hive UDFs for the needed functionality that is not out of the box available from Apache Hive Actively updated the upper management with daily updates on the progress of a project that include the classification levels that were achieved on the data ENVIRONMENT Core Java J2ee Hadoop MapReduce NoSQL Hive Pig Sqoop Apache HDP HDFS Eclipse Java Developer Emblem Health New York NY January 2011 to December 2013 DESCRIPTION Emblem Health Javelin application program is primarily a batch engine which evaluates many Claims each night after the close of business It works as a decision system to pull Claims data from various sources like underwriting data source run that through various types of common Pre Posteligibility rule sets and check for eligibility of the Claims This process involves Claim adjustments with respects group plan and individual Insurance It has many different programs targeting specific group of Plans RESPONSIBILITIES Involved in Analysis design and coding on J2EE Environment Implemented MVC architecture using Struts JSP and EJBs Used Core Java concepts in an application such as multithreaded programming synchronization of threads used thread to wait notify join methods etc Presentation layer design and programming on HTML XML XSL JSP JSTL and Ajax Creating crossbrowser compatible and standardscompliant CSSbased page layouts Worked on Hibernate objectrelational mapping related to the database schema Designed developed and implemented the business logic required for Security presentation controller Used JSP Servlet coding under J2EE Environment Good Experience in software configuration management using CVS GIT and SVN Designed XML files to implement most of the wiring need for Hibernate annotations and Struts configurations Responsible for developing the forms which contains the details of the employees and generating the reports and bills Developed Web Services for data transfer from client to server and vice versa using Apache Axis SOAP and WSDL Involved in designing of class and data flow diagrams using UML Rational Rose Created and modified Stored Procedures Functions Triggers and Complex SQL Commands using PLSQL Involved in the Design of ERD Entity Relationship Diagrams for the Relational database Developed Shell scripts in UNIX and procedures using SQL and PLSQL to process the data from the input file and load into the database Used CVS for maintaining the Source Code Designed developed and deployed on WebLogic Server Performed Unit Testing on the applications that are developed ENVIRONMENT Java JDK 16 J2EE JSP Servlet Hibernate JavaScript JDBC Oracle 10g UML Rational Rose SOAP Web Logic Server JUnit PLSQL CSS HTML XML Eclipse CVS GIT SVN Java Developer Southtech Limited Dhaka BD June 2007 to December 2011 DESCRIPTION Usage Management System UMS is a turnkey mediation rating and prebilling application that address the need for flexible reliable and costeffective usage processing RESPONSIBILITIES Worked on both WebLogic Portal 92 for Portal development and WebLogic 81 for Data Services Programming Developed the presentation layer using JSP HTML CSS and client validations using JavaScript Developed user interface using JSP Struts Tag Libraries to simplify the complexities of the application Developed business logic using Stateless session beans for calculating asset depreciation on Straight line and written down value approaches Database Modification using SQL PLSQL Stored procedures triggers Views in Oracle Created java classes to communicate with the database using JDBC Responsible for design and implementation of various modules of the application using StrutsSpringHibernate architecture Developed the Web Interface using Servlets Java Server Pages HTML and CSS Extensively used the JDBC Prepared Statement to embed the SQL queries into the java code Developed DAO Data Access Objects using Spring Framework 3 Developed Web applications with Rich Internet applications using Java applets Silverlight Java Deployed this application which uses J2EE architecture model and Struts Framework first on WebLogic and helped in migrating to JBoss Application server Used JavaScript to perform clientside validations and StrutsValidator Framework for serverside validation Designed and developed the application using various design patterns such as session facade business delegate and service locator Involved in designing usecase diagrams class diagrams interaction using UML model with Rational Rose ENVIRONMENT Java Servlets JSP EJB J2EE STRUTS XML XSLT JavaScript HTML CSS Spring 32 SQL PLSQL MS Visio Eclipse JDBC Windows XP WebLogic portal JBoss Education Bachelors Skills Eclipse 10 years HTML 8 years J2EE 10 years Java 10 years JavaScript 8 years Additional Information TECHNICAL SKILLS Languages JavaScript TypeScript ES6 Java J2EE HTML5 CSS3 PHP Design side Adobe Photoshop Adobe Illustrator Adobe Dreamweaver Adobe Flash Frontend jQuery Sass Less AJAX JSON Bootstrap Graphic libraries CreateJS D3 HighCharts jQueryUI Frameworks AngularJS Angular24 AngularCli Backend Nodejs Express Passportjs Promisejs JSP Databases MongoDB MySQL Oracle Servers REST Tomcat Apache AWS IDE Visual Studio Code Eclipse WebStorm Sublime Debug tool Chrome DevTools Firebug Production model Agile TDD BDD Project management Git SVN JIRA Testing Jasmine Mocha Protractor Junit Qunit Operating system Linux Windows XP7vista810 MacOS",
    "entities": [
        "MapReduce Involved",
        "GUI",
        "Pre Posteligibility",
        "MN",
        "New York",
        "BI",
        "Rabbit MQ Active",
        "HDFS",
        "UNIX",
        "Worked on Hibernate",
        "MN Work",
        "JMS",
        "PHP Design",
        "Oracle Created",
        "Big Data Hadoop",
        "Complex SQL Commands",
        "Flume Sqoop Pig",
        "CVS",
        "DESCRIPTION Sprint Nextel Corporation",
        "Customized RESTful Web Service",
        "Hadoop",
        "Database Modification",
        "HDFS Involved",
        "XML",
        "JSP Struts Tag Libraries",
        "Stateless",
        "WebLogic",
        "IPA",
        "Shell",
        "CVS GIT",
        "Target",
        "Created Joblets",
        "Python",
        "Data Services Programming Developed",
        "Apache Hive Actively",
        "SparkSQL",
        "Developed",
        "Customer Segmentation Product Offering Risk Management Regulatory Reporting",
        "Hive Working",
        "Flume Involved",
        "Hadoop MapReduce",
        "UML",
        "AOP Exported",
        "CSSbased",
        "MQ",
        "Health Net",
        "PreSession",
        "Zookeeper Using Scala",
        "JBoss Application",
        "Straight",
        "Developed Web Services",
        "Hadoop Developer",
        "DESCRIPTION Usage Management System UMS",
        "JSP",
        "Minneapolis",
        "DevTools Firebug Production",
        "Rational Rose ENVIRONMENT",
        "JSP JSTL",
        "Views",
        "MVC",
        "Spark",
        "WebLogic Server Performed Unit Testing",
        "Developed DAO Data Access Objects",
        "HighCharts",
        "AWS Involved",
        "GIT",
        "Created Hive",
        "Java J2EE Hadoop HDFS Pig Nifi Hive MapReduce",
        "STRUTS XML",
        "Parent",
        "Storm Implemented",
        "Sqoop",
        "Tera",
        "HTML XML",
        "Storm",
        "Created",
        "AWS",
        "Akka",
        "Aetna",
        "Autosys Tibco Business Objects XML Informatica Java",
        "Using Spark",
        "HMOs",
        "log data",
        "java",
        "California",
        "Created Talend",
        "Ab Initio",
        "SQL",
        "DESCRIPTION Merck Co",
        "Apache Spark Machine Learning",
        "Smith",
        "Hive",
        "JBoss Education",
        "JERSEY",
        "Additional Information TECHNICAL SKILLS Languages",
        "Agile TDD BDD Project",
        "Utilized Python",
        "AngularCli Backend Nodejs Express",
        "ETL",
        "Maintained Oozie",
        "JavaScript",
        "Vendors",
        "SVN",
        "CSS",
        "ETL Mapping Specifications Extracted",
        "Boston",
        "MapReduce",
        "Dynamic Partitions Buckets",
        "the Data Pipelines",
        "IDE Visual Studio",
        "WebLogic Portal",
        "UML Rational Rose Created",
        "Hadoop MapReduce HDFS Developed"
    ],
    "experience": "Experience Senior JavaHadoop Python Developer Smith Nephew Minneapolis MN January 2016 to Present DESCRIPTION Smith Nephew has many data sources in Mainframe and Distributed Applications with Traditional DW with Tera data Big Data Hadoop projects were launched to simplify the Data Pipelines to unify data sources for broader effective insights and actions in Customer Segmentation Product Offering Risk Management Regulatory Reporting and Fraud Detection RESPONSIBILITIES Responsible for developing efficient MapReduce on AWS cloud programs for more than 20 years worth of claim data to detect and separate fraudulent claims Worked with the advanced analytics team to design fraud detection algorithms and then developed MapReduce programs to efficiently run the algorithm on the huge datasets Ran data formatting scripts in python and created terabyte csv files to be consumed by Hadoop MapReduce jobs Performed Kafka analysis feature selection feature extraction using Apache Spark Machine Learning streaming libraries in Python Extensively used Akka actors architecture for scalable hassle free multithreading Experience using Cloudera in an application for Vendors platform Developed Python code using version control tools like GIT hub and SVN on vagrant machines Created Hive tables to store data into HDFS loading data and writing hive queries that will run internally in mapreduce way Involved in building the ETL architecture and Source to Target mapping to load data into Data warehouse Uploaded and processed terabytes of data from various structured and unstructured sources into HDFS AWS cloud using Sqoop and Flume Involved in Cluster coordination services through Zookeeper Using Scala for coding the components in Play and Akka and Used Maven to build and generate code analysis reports Involved in implementing Programmatic transaction management using AOP Exported the analyzed data to the relational databases using Sqoop for visualization and to generate reports for the BI team Played a key role in installation and configuration of the various Hadoop ecosystem tools such as Solr Pig HBase and Cassandra Developed an information pipeline utilizing Kafka and Storm to store data into HDFS Loading spilling data using Kafka Flume and real time Using Spark and Storm Implemented various hive optimization techniques like Dynamic Partitions Buckets Map Joins Parallel executions in Hive Working on handling all the requests to the systems using play framework MVC framework Worked with PreSession and PostSession Linux scripts for automation of ETL jobs and to perform operations like gunzip remove and archive files Created Talend jobs to copy the files from one server to another and utilized Talend FTP components Created Joblets and Parent child jobs in Talend Designed Developed the ETL Jobs using Talend Integration Suite by using various transformations as per the business requirements and based on ETL Mapping Specifications Extracted meaningful data from dealer csv files text files and mainframe files and generated Pythonpandas reports for data analysis Utilized Python to run scripts generate tables and reports Coordinates with Agile team to effectively meet all sprint commitments Worked on JMS like Rabbit MQ Active MQ and used JERSEY framework to implement the JAX RS Designed and Maintained Oozie workflows to manage the flow of jobs in the cluster Parse Json files through Spark core to extract schema for the production data using SparkSQL and Scala Actively updated the upper management with daily updates on the progress of project that include the classification levels that were achieved on the data ENVIRONMENT Scala language with Akka framework Java J2EE Hadoop HDFS Pig Nifi Hive MapReduce Sqoop Kafka CDH3 Cassandra Python Oozie collection Scala AWS cloud storm Ab Initio Apache SQL NoSQL Bitbucket HBase Flume spark Solr Zookeeper ETL Talend Centos Eclipse Agile JavaHadoop Developer Sprint Overland Park KS February 2015 to December 2015 DESCRIPTION Sprint Nextel Corporation is a United Statesbased holding organization that works numerous wire lines and remote systems serving consumer business and government clients I worked as a Hadoop Developer in Data Insights group where I performed analysis on gigantic information sets and helped the association get an upper hand by discovering the client patterns which helped in advertisement focusing on and organize streamlining Developed RESTful API services for postpaid on top of sprint legacy SOAP based services RESPONSIBILITIES Involved in making Hive tables stacking the information and composing Hive queries that will run inside in MapReduce Involved in converting HiveSQL queries into Spark transformations using Spark RDDs Python and Scala Wrote Python modules to view and connect the Apache Cassandra instance Involved in writing MapReduce jobs Developed and implemented two Service Endpoints end to end in Java using Play framework Akka server Hazel cast Developed the GUI module DMT using Struts framework JavaScript DOJO Ajax HTML and DHTML Developed RESTful web services interface to Javabased runtime engine and accounts Customized RESTful Web Service using Spring RESTful API sending JSON format data packets between frontend and middletier controller Realtime streaming the information utilizing Spark with Kafka Responsible for creating information pipeline utilizing flume Sqoop and pig to remove the information from weblogs and store in HDFS Involved in emitting processed information from Hadoop to relational databases or external frameworks utilizing Sqoop HDFS GET or CopyToLocal Used Play logger to run through preload and postload test cycles for application performance and errors Developed data pipeline utilizing Flume Sqoop Pig and Java MapReduce to ingest client behavioral information and money related histories into HDFS for analysis Experienced in managing and assessing Hadoop log records Used Pig to do changes like event joins filter boot traffic and some preaggregations before storing the information onto HDFS Written Hive inquiries for data to meet the business requirements Importing and sending out information into HDFS and Hive utilizing Sqoop and Kafka Created various Parser programs to extract data from Autosys Tibco Business Objects XML Informatica Java and database views using Scala Performed deployment and support of cloud services including AWS Involved in creating Hive Tables loading with data and writing Hive queries which will invoke and run MapReduce jobs in the backend Experience in Message based systems using JMS and MQ Series Implemented the workflows using Apache Oozie framework to automate tasks Developed scripts and automated data management from end to end and sync up between all the clusters ENVIRONMENT Java Hadoop Scala MapReduce MongoDB SQL Apache Yarn Hive Pig HBase Oozie Sqoop Flume Akka Play IBM MQSeries Core Java HDP HDFS Eclipse Kafka Software Engineer Merck co Boston MA January 2014 to January 2015 DESCRIPTION Merck Co is an independent practice association IPA serving some 300 000 health plan members in northern California The company contracts with managed care organizations throughout the region including HMOs belonging to Aetna CIGNA and Health Net to provide care to health plan members through its provider affiliates Its network includes about 3 700 primary care and specialty physicians 36 hospitals and 15 urgent care centers RESPONSIBILITIES Installed and configured Hadoop MapReduce HDFS Developed multiple MapReduce jobs in java for data cleaning and preprocessing Involved in installing and updating and managing Environment Collecting and aggregating large amounts of log data using Apache Flume and staging data in HDFS for further analysis Involved in running Hadoop streaming jobs to process terabytes of XML format data Participated in requirement gathering and analysis phase of the project in documenting the business requirements by conducting workshopsmeetings with various business users Involved in Sqoop HDFS Put or CopyFromLocal to ingest data Used Hive to analyze the partitioned and bucketed data and compute various metrics for reporting Created and maintained technical documentation for launching Hadoop clusters and for executing Hive queries and Pig Scripts Implemented test scripts to support testdriven development and continuous integration Implemented SQL PLSQL Stored Procedures Involved in developing Shell scripts to orchestrate the execution of all other scripts Pig Hive and MapReduce and move the data files within and outside of HDFS Involved in developing Hive UDFs for the needed functionality that is not out of the box available from Apache Hive Actively updated the upper management with daily updates on the progress of a project that include the classification levels that were achieved on the data ENVIRONMENT Core Java J2ee Hadoop MapReduce NoSQL Hive Pig Sqoop Apache HDP HDFS Eclipse Java Developer Emblem Health New York NY January 2011 to December 2013 DESCRIPTION Emblem Health Javelin application program is primarily a batch engine which evaluates many Claims each night after the close of business It works as a decision system to pull Claims data from various sources like underwriting data source run that through various types of common Pre Posteligibility rule sets and check for eligibility of the Claims This process involves Claim adjustments with respects group plan and individual Insurance It has many different programs targeting specific group of Plans RESPONSIBILITIES Involved in Analysis design and coding on J2EE Environment Implemented MVC architecture using Struts JSP and EJBs Used Core Java concepts in an application such as multithreaded programming synchronization of threads used thread to wait notify join methods etc Presentation layer design and programming on HTML XML XSL JSP JSTL and Ajax Creating crossbrowser compatible and standardscompliant CSSbased page layouts Worked on Hibernate objectrelational mapping related to the database schema Designed developed and implemented the business logic required for Security presentation controller Used JSP Servlet coding under J2EE Environment Good Experience in software configuration management using CVS GIT and SVN Designed XML files to implement most of the wiring need for Hibernate annotations and Struts configurations Responsible for developing the forms which contains the details of the employees and generating the reports and bills Developed Web Services for data transfer from client to server and vice versa using Apache Axis SOAP and WSDL Involved in designing of class and data flow diagrams using UML Rational Rose Created and modified Stored Procedures Functions Triggers and Complex SQL Commands using PLSQL Involved in the Design of ERD Entity Relationship Diagrams for the Relational database Developed Shell scripts in UNIX and procedures using SQL and PLSQL to process the data from the input file and load into the database Used CVS for maintaining the Source Code Designed developed and deployed on WebLogic Server Performed Unit Testing on the applications that are developed ENVIRONMENT Java JDK 16 J2EE JSP Servlet Hibernate JavaScript JDBC Oracle 10 g UML Rational Rose SOAP Web Logic Server JUnit PLSQL CSS HTML XML Eclipse CVS GIT SVN Java Developer Southtech Limited Dhaka BD June 2007 to December 2011 DESCRIPTION Usage Management System UMS is a turnkey mediation rating and prebilling application that address the need for flexible reliable and costeffective usage processing RESPONSIBILITIES Worked on both WebLogic Portal 92 for Portal development and WebLogic 81 for Data Services Programming Developed the presentation layer using JSP HTML CSS and client validations using JavaScript Developed user interface using JSP Struts Tag Libraries to simplify the complexities of the application Developed business logic using Stateless session beans for calculating asset depreciation on Straight line and written down value approaches Database Modification using SQL PLSQL Stored procedures triggers Views in Oracle Created java classes to communicate with the database using JDBC Responsible for design and implementation of various modules of the application using StrutsSpringHibernate architecture Developed the Web Interface using Servlets Java Server Pages HTML and CSS Extensively used the JDBC Prepared Statement to embed the SQL queries into the java code Developed DAO Data Access Objects using Spring Framework 3 Developed Web applications with Rich Internet applications using Java applets Silverlight Java Deployed this application which uses J2EE architecture model and Struts Framework first on WebLogic and helped in migrating to JBoss Application server Used JavaScript to perform clientside validations and StrutsValidator Framework for serverside validation Designed and developed the application using various design patterns such as session facade business delegate and service locator Involved in designing usecase diagrams class diagrams interaction using UML model with Rational Rose ENVIRONMENT Java Servlets JSP EJB J2EE STRUTS XML XSLT JavaScript HTML CSS Spring 32 SQL PLSQL MS Visio Eclipse JDBC Windows XP WebLogic portal JBoss Education Bachelors Skills Eclipse 10 years HTML 8 years J2EE 10 years Java 10 years JavaScript 8 years Additional Information TECHNICAL SKILLS Languages JavaScript TypeScript ES6 Java J2EE HTML5 CSS3 PHP Design side Adobe Photoshop Adobe Illustrator Adobe Dreamweaver Adobe Flash Frontend jQuery Sass Less AJAX JSON Bootstrap Graphic libraries CreateJS D3 HighCharts jQueryUI Frameworks AngularJS Angular24 AngularCli Backend Nodejs Express Passportjs Promisejs JSP Databases MongoDB MySQL Oracle Servers REST Tomcat Apache AWS IDE Visual Studio Code Eclipse WebStorm Sublime Debug tool Chrome DevTools Firebug Production model Agile TDD BDD Project management Git SVN JIRA Testing Jasmine Mocha Protractor Junit Qunit Operating system Linux Windows XP7vista810 MacOS",
    "extracted_keywords": [
        "Senior",
        "JavaHadoop",
        "Python",
        "Developer",
        "Senior",
        "JavaHadoopspan",
        "lPythonspan",
        "span",
        "lDeveloperspan",
        "Senior",
        "JavaHadoop",
        "Python",
        "Developer",
        "Smith",
        "Nephew",
        "Minneapolis",
        "MN",
        "MN",
        "Work",
        "Experience",
        "Senior",
        "JavaHadoop",
        "Python",
        "Developer",
        "Smith",
        "Nephew",
        "Minneapolis",
        "MN",
        "January",
        "Present",
        "DESCRIPTION",
        "Smith",
        "Nephew",
        "data",
        "sources",
        "Mainframe",
        "Distributed",
        "Applications",
        "DW",
        "Tera",
        "data",
        "Big",
        "Data",
        "Hadoop",
        "projects",
        "Data",
        "Pipelines",
        "data",
        "sources",
        "insights",
        "actions",
        "Customer",
        "Segmentation",
        "Product",
        "Offering",
        "Risk",
        "Management",
        "Regulatory",
        "Reporting",
        "Fraud",
        "Detection",
        "RESPONSIBILITIES",
        "MapReduce",
        "AWS",
        "cloud",
        "programs",
        "years",
        "claim",
        "data",
        "claims",
        "analytics",
        "team",
        "fraud",
        "detection",
        "algorithms",
        "MapReduce",
        "programs",
        "algorithm",
        "datasets",
        "Ran",
        "data",
        "scripts",
        "python",
        "terabyte",
        "files",
        "Hadoop",
        "MapReduce",
        "jobs",
        "Performed",
        "Kafka",
        "analysis",
        "feature",
        "selection",
        "feature",
        "extraction",
        "Apache",
        "Spark",
        "Machine",
        "Learning",
        "streaming",
        "libraries",
        "Python",
        "Akka",
        "actors",
        "architecture",
        "hassle",
        "Experience",
        "Cloudera",
        "application",
        "Vendors",
        "platform",
        "Python",
        "code",
        "version",
        "control",
        "tools",
        "GIT",
        "hub",
        "SVN",
        "machines",
        "Hive",
        "tables",
        "data",
        "HDFS",
        "loading",
        "data",
        "hive",
        "queries",
        "way",
        "ETL",
        "architecture",
        "Source",
        "mapping",
        "data",
        "Data",
        "warehouse",
        "terabytes",
        "data",
        "sources",
        "HDFS",
        "AWS",
        "cloud",
        "Sqoop",
        "Flume",
        "Cluster",
        "coordination",
        "services",
        "Zookeeper",
        "Scala",
        "components",
        "Play",
        "Akka",
        "Maven",
        "code",
        "analysis",
        "reports",
        "transaction",
        "management",
        "AOP",
        "data",
        "databases",
        "Sqoop",
        "visualization",
        "reports",
        "BI",
        "team",
        "role",
        "installation",
        "configuration",
        "Hadoop",
        "ecosystem",
        "tools",
        "Solr",
        "Pig",
        "HBase",
        "Cassandra",
        "information",
        "pipeline",
        "Kafka",
        "Storm",
        "data",
        "HDFS",
        "Loading",
        "spilling",
        "data",
        "Kafka",
        "Flume",
        "time",
        "Spark",
        "Storm",
        "hive",
        "optimization",
        "techniques",
        "Dynamic",
        "Partitions",
        "Buckets",
        "Map",
        "Joins",
        "executions",
        "Hive",
        "Working",
        "requests",
        "systems",
        "play",
        "framework",
        "MVC",
        "framework",
        "PreSession",
        "PostSession",
        "Linux",
        "scripts",
        "automation",
        "ETL",
        "jobs",
        "operations",
        "gunzip",
        "remove",
        "files",
        "Talend",
        "jobs",
        "files",
        "server",
        "Talend",
        "FTP",
        "components",
        "Joblets",
        "Parent",
        "child",
        "jobs",
        "Talend",
        "ETL",
        "Jobs",
        "Talend",
        "Integration",
        "Suite",
        "transformations",
        "business",
        "requirements",
        "ETL",
        "Mapping",
        "Specifications",
        "data",
        "dealer",
        "csv",
        "files",
        "text",
        "files",
        "mainframe",
        "files",
        "Pythonpandas",
        "data",
        "analysis",
        "Python",
        "scripts",
        "tables",
        "reports",
        "Coordinates",
        "team",
        "sprint",
        "commitments",
        "JMS",
        "Rabbit",
        "MQ",
        "MQ",
        "JERSEY",
        "framework",
        "JAX",
        "RS",
        "Oozie",
        "workflows",
        "flow",
        "jobs",
        "cluster",
        "Parse",
        "Json",
        "files",
        "Spark",
        "core",
        "schema",
        "production",
        "data",
        "SparkSQL",
        "Scala",
        "management",
        "updates",
        "progress",
        "project",
        "classification",
        "levels",
        "data",
        "ENVIRONMENT",
        "Scala",
        "language",
        "Akka",
        "framework",
        "Java",
        "J2EE",
        "Hadoop",
        "HDFS",
        "Pig",
        "Nifi",
        "Hive",
        "MapReduce",
        "Sqoop",
        "Kafka",
        "CDH3",
        "Cassandra",
        "Python",
        "Oozie",
        "collection",
        "Scala",
        "AWS",
        "cloud",
        "storm",
        "Initio",
        "Apache",
        "SQL",
        "NoSQL",
        "Bitbucket",
        "HBase",
        "Flume",
        "spark",
        "Solr",
        "Zookeeper",
        "ETL",
        "Talend",
        "Centos",
        "Eclipse",
        "Agile",
        "JavaHadoop",
        "Developer",
        "Sprint",
        "Overland",
        "Park",
        "KS",
        "February",
        "December",
        "DESCRIPTION",
        "Sprint",
        "Nextel",
        "Corporation",
        "United",
        "Statesbased",
        "organization",
        "wire",
        "lines",
        "systems",
        "consumer",
        "business",
        "government",
        "clients",
        "Hadoop",
        "Developer",
        "Data",
        "Insights",
        "group",
        "analysis",
        "information",
        "sets",
        "association",
        "hand",
        "client",
        "patterns",
        "advertisement",
        "API",
        "services",
        "postpaid",
        "top",
        "sprint",
        "legacy",
        "SOAP",
        "services",
        "RESPONSIBILITIES",
        "Hive",
        "tables",
        "information",
        "Hive",
        "queries",
        "MapReduce",
        "HiveSQL",
        "queries",
        "Spark",
        "transformations",
        "Spark",
        "RDDs",
        "Python",
        "Scala",
        "Wrote",
        "Python",
        "modules",
        "Apache",
        "Cassandra",
        "instance",
        "MapReduce",
        "jobs",
        "Service",
        "Endpoints",
        "Java",
        "Play",
        "framework",
        "Akka",
        "server",
        "Hazel",
        "GUI",
        "module",
        "DMT",
        "Struts",
        "framework",
        "JavaScript",
        "DOJO",
        "Ajax",
        "HTML",
        "DHTML",
        "web",
        "services",
        "interface",
        "runtime",
        "engine",
        "Web",
        "Service",
        "Spring",
        "API",
        "format",
        "data",
        "packets",
        "frontend",
        "controller",
        "Realtime",
        "information",
        "Spark",
        "Kafka",
        "Responsible",
        "information",
        "pipeline",
        "flume",
        "Sqoop",
        "pig",
        "information",
        "weblogs",
        "HDFS",
        "information",
        "Hadoop",
        "databases",
        "frameworks",
        "Sqoop",
        "HDFS",
        "GET",
        "CopyToLocal",
        "Used",
        "Play",
        "logger",
        "preload",
        "postload",
        "test",
        "cycles",
        "application",
        "performance",
        "errors",
        "data",
        "pipeline",
        "Flume",
        "Sqoop",
        "Pig",
        "Java",
        "MapReduce",
        "client",
        "information",
        "money",
        "histories",
        "HDFS",
        "analysis",
        "Hadoop",
        "log",
        "records",
        "Pig",
        "changes",
        "event",
        "filter",
        "boot",
        "traffic",
        "preaggregations",
        "information",
        "HDFS",
        "Written",
        "Hive",
        "inquiries",
        "data",
        "business",
        "requirements",
        "information",
        "HDFS",
        "Hive",
        "Sqoop",
        "Kafka",
        "Parser",
        "programs",
        "data",
        "Autosys",
        "Tibco",
        "Business",
        "XML",
        "Informatica",
        "Java",
        "database",
        "views",
        "Scala",
        "Performed",
        "deployment",
        "support",
        "cloud",
        "services",
        "AWS",
        "Hive",
        "Tables",
        "data",
        "Hive",
        "queries",
        "MapReduce",
        "jobs",
        "Experience",
        "Message",
        "systems",
        "JMS",
        "MQ",
        "Series",
        "workflows",
        "Apache",
        "Oozie",
        "framework",
        "tasks",
        "scripts",
        "data",
        "management",
        "end",
        "end",
        "clusters",
        "Java",
        "Hadoop",
        "Scala",
        "MapReduce",
        "MongoDB",
        "SQL",
        "Apache",
        "Yarn",
        "Hive",
        "Pig",
        "HBase",
        "Oozie",
        "Sqoop",
        "Flume",
        "Akka",
        "IBM",
        "MQSeries",
        "Core",
        "Java",
        "HDP",
        "HDFS",
        "Eclipse",
        "Kafka",
        "Software",
        "Engineer",
        "Merck",
        "co",
        "Boston",
        "MA",
        "January",
        "January",
        "DESCRIPTION",
        "Merck",
        "Co",
        "practice",
        "association",
        "IPA",
        "health",
        "plan",
        "members",
        "California",
        "company",
        "care",
        "organizations",
        "region",
        "HMOs",
        "Aetna",
        "CIGNA",
        "Health",
        "Net",
        "care",
        "health",
        "plan",
        "members",
        "provider",
        "affiliates",
        "network",
        "care",
        "specialty",
        "physicians",
        "hospitals",
        "care",
        "centers",
        "RESPONSIBILITIES",
        "Hadoop",
        "MapReduce",
        "MapReduce",
        "jobs",
        "java",
        "data",
        "cleaning",
        "Environment",
        "Collecting",
        "amounts",
        "log",
        "data",
        "Apache",
        "Flume",
        "data",
        "HDFS",
        "analysis",
        "Hadoop",
        "streaming",
        "jobs",
        "terabytes",
        "XML",
        "format",
        "data",
        "requirement",
        "gathering",
        "analysis",
        "phase",
        "project",
        "business",
        "requirements",
        "workshopsmeetings",
        "business",
        "users",
        "Sqoop",
        "HDFS",
        "Put",
        "data",
        "Hive",
        "data",
        "metrics",
        "documentation",
        "Hadoop",
        "clusters",
        "Hive",
        "queries",
        "Pig",
        "Scripts",
        "test",
        "scripts",
        "testdriven",
        "development",
        "integration",
        "SQL",
        "PLSQL",
        "Stored",
        "Procedures",
        "Shell",
        "scripts",
        "execution",
        "scripts",
        "Pig",
        "Hive",
        "MapReduce",
        "data",
        "files",
        "outside",
        "HDFS",
        "Hive",
        "UDFs",
        "functionality",
        "box",
        "Apache",
        "Hive",
        "management",
        "updates",
        "progress",
        "project",
        "classification",
        "levels",
        "data",
        "ENVIRONMENT",
        "Core",
        "Java",
        "J2ee",
        "Hadoop",
        "MapReduce",
        "NoSQL",
        "Hive",
        "Pig",
        "Sqoop",
        "Apache",
        "HDP",
        "HDFS",
        "Eclipse",
        "Java",
        "Developer",
        "Emblem",
        "Health",
        "New",
        "York",
        "NY",
        "January",
        "December",
        "DESCRIPTION",
        "Emblem",
        "Health",
        "Javelin",
        "application",
        "program",
        "batch",
        "engine",
        "Claims",
        "night",
        "close",
        "business",
        "decision",
        "system",
        "Claims",
        "data",
        "sources",
        "underwriting",
        "data",
        "source",
        "types",
        "Pre",
        "Posteligibility",
        "rule",
        "sets",
        "eligibility",
        "Claims",
        "process",
        "Claim",
        "adjustments",
        "respects",
        "group",
        "plan",
        "Insurance",
        "programs",
        "group",
        "Plans",
        "RESPONSIBILITIES",
        "Analysis",
        "design",
        "J2EE",
        "Environment",
        "MVC",
        "architecture",
        "Struts",
        "JSP",
        "EJBs",
        "Core",
        "Java",
        "concepts",
        "application",
        "programming",
        "synchronization",
        "threads",
        "thread",
        "join",
        "methods",
        "Presentation",
        "layer",
        "design",
        "programming",
        "HTML",
        "XML",
        "XSL",
        "JSP",
        "JSTL",
        "Ajax",
        "Creating",
        "crossbrowser",
        "page",
        "layouts",
        "Hibernate",
        "mapping",
        "database",
        "schema",
        "business",
        "logic",
        "Security",
        "presentation",
        "controller",
        "JSP",
        "Servlet",
        "J2EE",
        "Environment",
        "Good",
        "Experience",
        "software",
        "configuration",
        "management",
        "CVS",
        "GIT",
        "SVN",
        "XML",
        "files",
        "wiring",
        "need",
        "Hibernate",
        "annotations",
        "Struts",
        "configurations",
        "forms",
        "details",
        "employees",
        "reports",
        "bills",
        "Developed",
        "Web",
        "Services",
        "data",
        "transfer",
        "client",
        "Apache",
        "Axis",
        "SOAP",
        "WSDL",
        "designing",
        "class",
        "data",
        "flow",
        "diagrams",
        "UML",
        "Rational",
        "Rose",
        "Stored",
        "Procedures",
        "Functions",
        "Triggers",
        "Complex",
        "SQL",
        "Commands",
        "PLSQL",
        "Design",
        "ERD",
        "Entity",
        "Relationship",
        "Diagrams",
        "database",
        "Developed",
        "Shell",
        "scripts",
        "UNIX",
        "procedures",
        "SQL",
        "PLSQL",
        "data",
        "input",
        "file",
        "load",
        "database",
        "CVS",
        "Source",
        "Code",
        "WebLogic",
        "Server",
        "Performed",
        "Unit",
        "Testing",
        "applications",
        "ENVIRONMENT",
        "Java",
        "JDK",
        "J2EE",
        "JSP",
        "Servlet",
        "Hibernate",
        "JavaScript",
        "JDBC",
        "Oracle",
        "g",
        "UML",
        "Rational",
        "Rose",
        "SOAP",
        "Web",
        "Logic",
        "Server",
        "JUnit",
        "PLSQL",
        "CSS",
        "HTML",
        "XML",
        "Eclipse",
        "CVS",
        "GIT",
        "SVN",
        "Java",
        "Developer",
        "Southtech",
        "Limited",
        "Dhaka",
        "BD",
        "June",
        "December",
        "DESCRIPTION",
        "Usage",
        "Management",
        "System",
        "UMS",
        "turnkey",
        "mediation",
        "rating",
        "prebilling",
        "application",
        "need",
        "usage",
        "processing",
        "RESPONSIBILITIES",
        "WebLogic",
        "Portal",
        "Portal",
        "development",
        "WebLogic",
        "Data",
        "Services",
        "Programming",
        "presentation",
        "layer",
        "JSP",
        "HTML",
        "CSS",
        "client",
        "validations",
        "JavaScript",
        "Developed",
        "user",
        "interface",
        "JSP",
        "Struts",
        "Tag",
        "complexities",
        "application",
        "business",
        "logic",
        "Stateless",
        "session",
        "beans",
        "asset",
        "depreciation",
        "line",
        "value",
        "Database",
        "Modification",
        "SQL",
        "PLSQL",
        "procedures",
        "Views",
        "Oracle",
        "classes",
        "database",
        "JDBC",
        "Responsible",
        "design",
        "implementation",
        "modules",
        "application",
        "StrutsSpringHibernate",
        "architecture",
        "Web",
        "Interface",
        "Servlets",
        "Java",
        "Server",
        "Pages",
        "HTML",
        "CSS",
        "JDBC",
        "Prepared",
        "Statement",
        "SQL",
        "java",
        "code",
        "Developed",
        "DAO",
        "Data",
        "Access",
        "Spring",
        "Framework",
        "Web",
        "applications",
        "Rich",
        "Internet",
        "applications",
        "Java",
        "applets",
        "Silverlight",
        "Java",
        "application",
        "J2EE",
        "architecture",
        "model",
        "Struts",
        "Framework",
        "WebLogic",
        "JBoss",
        "Application",
        "server",
        "JavaScript",
        "validations",
        "StrutsValidator",
        "Framework",
        "serverside",
        "validation",
        "application",
        "design",
        "patterns",
        "session",
        "facade",
        "business",
        "delegate",
        "service",
        "locator",
        "usecase",
        "diagrams",
        "class",
        "diagrams",
        "interaction",
        "UML",
        "model",
        "Rational",
        "Rose",
        "ENVIRONMENT",
        "Java",
        "Servlets",
        "JSP",
        "EJB",
        "J2EE",
        "STRUTS",
        "XML",
        "XSLT",
        "JavaScript",
        "HTML",
        "CSS",
        "Spring",
        "SQL",
        "PLSQL",
        "MS",
        "Visio",
        "Eclipse",
        "JDBC",
        "XP",
        "WebLogic",
        "JBoss",
        "Education",
        "Bachelors",
        "Skills",
        "Eclipse",
        "years",
        "HTML",
        "years",
        "J2EE",
        "years",
        "Java",
        "years",
        "JavaScript",
        "years",
        "Information",
        "TECHNICAL",
        "SKILLS",
        "Languages",
        "JavaScript",
        "TypeScript",
        "ES6",
        "Java",
        "J2EE",
        "HTML5",
        "CSS3",
        "PHP",
        "Design",
        "side",
        "Adobe",
        "Photoshop",
        "Adobe",
        "Illustrator",
        "Adobe",
        "Dreamweaver",
        "Adobe",
        "Flash",
        "Frontend",
        "jQuery",
        "Sass",
        "Less",
        "AJAX",
        "JSON",
        "Bootstrap",
        "Graphic",
        "D3",
        "HighCharts",
        "Frameworks",
        "Angular24",
        "AngularCli",
        "Backend",
        "Nodejs",
        "Express",
        "Passportjs",
        "Promisejs",
        "JSP",
        "MongoDB",
        "MySQL",
        "Oracle",
        "Servers",
        "REST",
        "Tomcat",
        "Apache",
        "AWS",
        "IDE",
        "Visual",
        "Studio",
        "Code",
        "Eclipse",
        "WebStorm",
        "Sublime",
        "Debug",
        "tool",
        "Chrome",
        "DevTools",
        "Firebug",
        "Production",
        "model",
        "Agile",
        "TDD",
        "BDD",
        "Project",
        "management",
        "Git",
        "SVN",
        "JIRA",
        "Testing",
        "Jasmine",
        "Mocha",
        "Protractor",
        "Junit",
        "Qunit",
        "Operating",
        "system",
        "Linux",
        "Windows",
        "XP7vista810",
        "MacOS"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T20:49:31.555204",
    "resume_data": "Senior JavaHadoop Python Developer Senior JavaHadoopspan lPythonspan span lDeveloperspan Senior JavaHadoop Python Developer Smith Nephew Minneapolis MN MN Work Experience Senior JavaHadoop Python Developer Smith Nephew Minneapolis MN January 2016 to Present DESCRIPTION Smith Nephew has many data sources in Mainframe and Distributed Applications with Traditional DW with Tera data Big Data Hadoop projects were launched to simplify the Data Pipelines to unify data sources for broader effective insights and actions in Customer Segmentation Product Offering Risk Management Regulatory Reporting and Fraud Detection RESPONSIBILITIES Responsible for developing efficient MapReduce on AWS cloud programs for more than 20 years worth of claim data to detect and separate fraudulent claims Worked with the advanced analytics team to design fraud detection algorithms and then developed MapReduce programs to efficiently run the algorithm on the huge datasets Ran data formatting scripts in python and created terabyte csv files to be consumed by Hadoop MapReduce jobs Performed Kafka analysis feature selection feature extraction using Apache Spark Machine Learning streaming libraries in Python Extensively used Akka actors architecture for scalable hassle free multithreading Experience using Cloudera in an application for Vendors platform Developed Python code using version control tools like GIT hub and SVN on vagrant machines Created Hive tables to store data into HDFS loading data and writing hive queries that will run internally in mapreduce way Involved in building the ETL architecture and Source to Target mapping to load data into Data warehouse Uploaded and processed terabytes of data from various structured and unstructured sources into HDFS AWS cloud using Sqoop and Flume Involved in Cluster coordination services through Zookeeper Using Scala for coding the components in Play and Akka and Used Maven to build and generate code analysis reports Involved in implementing Programmatic transaction management using AOP Exported the analyzed data to the relational databases using Sqoop for visualization and to generate reports for the BI team Played a key role in installation and configuration of the various Hadoop ecosystem tools such as Solr Pig HBase and Cassandra Developed an information pipeline utilizing Kafka and Storm to store data into HDFS Loading spilling data using Kafka Flume and real time Using Spark and Storm Implemented various hive optimization techniques like Dynamic Partitions Buckets Map Joins Parallel executions in Hive Working on handling all the requests to the systems using play framework MVC framework Worked with PreSession and PostSession Linux scripts for automation of ETL jobs and to perform operations like gunzip remove and archive files Created Talend jobs to copy the files from one server to another and utilized Talend FTP components Created Joblets and Parent child jobs in Talend Designed Developed the ETL Jobs using Talend Integration Suite by using various transformations as per the business requirements and based on ETL Mapping Specifications Extracted meaningful data from dealer csv files text files and mainframe files and generated Pythonpandas reports for data analysis Utilized Python to run scripts generate tables and reports Coordinates with Agile team to effectively meet all sprint commitments Worked on JMS like Rabbit MQ Active MQ and used JERSEY framework to implement the JAX RS Designed and Maintained Oozie workflows to manage the flow of jobs in the cluster Parse Json files through Spark core to extract schema for the production data using SparkSQL and Scala Actively updated the upper management with daily updates on the progress of project that include the classification levels that were achieved on the data ENVIRONMENT Scala language with Akka framework Java J2EE Hadoop HDFS Pig Nifi Hive MapReduce Sqoop Kafka CDH3 Cassandra Python Oozie collection Scala AWS cloud storm Ab Initio Apache SQL NoSQL Bitbucket HBase Flume spark Solr Zookeeper ETL Talend Centos Eclipse Agile JavaHadoop Developer Sprint Overland Park KS February 2015 to December 2015 DESCRIPTION Sprint Nextel Corporation is a United Statesbased holding organization that works numerous wire lines and remote systems serving consumer business and government clients I worked as a Hadoop Developer in Data Insights group where I performed analysis on gigantic information sets and helped the association get an upper hand by discovering the client patterns which helped in advertisement focusing on and organize streamlining Developed RESTful API services for postpaid on top of sprint legacy SOAP based services RESPONSIBILITIES Involved in making Hive tables stacking the information and composing Hive queries that will run inside in MapReduce Involved in converting HiveSQL queries into Spark transformations using Spark RDDs Python and Scala Wrote Python modules to view and connect the Apache Cassandra instance Involved in writing MapReduce jobs Developed and implemented two Service Endpoints end to end in Java using Play framework Akka server Hazel cast Developed the GUI module DMT using Struts framework JavaScript DOJO Ajax HTML and DHTML Developed RESTful web services interface to Javabased runtime engine and accounts Customized RESTful Web Service using Spring RESTful API sending JSON format data packets between frontend and middletier controller Realtime streaming the information utilizing Spark with Kafka Responsible for creating information pipeline utilizing flume Sqoop and pig to remove the information from weblogs and store in HDFS Involved in emitting processed information from Hadoop to relational databases or external frameworks utilizing Sqoop HDFS GET or CopyToLocal Used Play logger to run through preload and postload test cycles for application performance and errors Developed data pipeline utilizing Flume Sqoop Pig and Java MapReduce to ingest client behavioral information and money related histories into HDFS for analysis Experienced in managing and assessing Hadoop log records Used Pig to do changes like event joins filter boot traffic and some preaggregations before storing the information onto HDFS Written Hive inquiries for data to meet the business requirements Importing and sending out information into HDFS and Hive utilizing Sqoop and Kafka Created various Parser programs to extract data from Autosys Tibco Business Objects XML Informatica Java and database views using Scala Performed deployment and support of cloud services including AWS Involved in creating Hive Tables loading with data and writing Hive queries which will invoke and run MapReduce jobs in the backend Experience in Message based systems using JMS and MQ Series Implemented the workflows using Apache Oozie framework to automate tasks Developed scripts and automated data management from end to end and sync up between all the clusters ENVIRONMENT Java Hadoop Scala MapReduce MongoDB SQL Apache Yarn Hive Pig HBase Oozie Sqoop Flume Akka Play IBM MQSeries Core Java HDP HDFS Eclipse Kafka Software Engineer Merck co Boston MA January 2014 to January 2015 DESCRIPTION Merck Co is an independent practice association IPA serving some 300 000 health plan members in northern California The company contracts with managed care organizations throughout the region including HMOs belonging to Aetna CIGNA and Health Net to provide care to health plan members through its provider affiliates Its network includes about 3 700 primary care and specialty physicians 36 hospitals and 15 urgent care centers RESPONSIBILITIES Installed and configured Hadoop MapReduce HDFS Developed multiple MapReduce jobs in java for data cleaning and preprocessing Involved in installing and updating and managing Environment Collecting and aggregating large amounts of log data using Apache Flume and staging data in HDFS for further analysis Involved in running Hadoop streaming jobs to process terabytes of XML format data Participated in requirement gathering and analysis phase of the project in documenting the business requirements by conducting workshopsmeetings with various business users Involved in Sqoop HDFS Put or CopyFromLocal to ingest data Used Hive to analyze the partitioned and bucketed data and compute various metrics for reporting Created and maintained technical documentation for launching Hadoop clusters and for executing Hive queries and Pig Scripts Implemented test scripts to support testdriven development and continuous integration Implemented SQL PLSQL Stored Procedures Involved in developing Shell scripts to orchestrate the execution of all other scripts Pig Hive and MapReduce and move the data files within and outside of HDFS Involved in developing Hive UDFs for the needed functionality that is not out of the box available from Apache Hive Actively updated the upper management with daily updates on the progress of a project that include the classification levels that were achieved on the data ENVIRONMENT Core Java J2ee Hadoop MapReduce NoSQL Hive Pig Sqoop Apache HDP HDFS Eclipse Java Developer Emblem Health New York NY January 2011 to December 2013 DESCRIPTION Emblem Health Javelin application program is primarily a batch engine which evaluates many Claims each night after the close of business It works as a decision system to pull Claims data from various sources like underwriting data source run that through various types of common Pre Posteligibility rule sets and check for eligibility of the Claims This process involves Claim adjustments with respects group plan and individual Insurance It has many different programs targeting specific group of Plans RESPONSIBILITIES Involved in Analysis design and coding on J2EE Environment Implemented MVC architecture using Struts JSP and EJBs Used Core Java concepts in an application such as multithreaded programming synchronization of threads used thread to wait notify join methods etc Presentation layer design and programming on HTML XML XSL JSP JSTL and Ajax Creating crossbrowser compatible and standardscompliant CSSbased page layouts Worked on Hibernate objectrelational mapping related to the database schema Designed developed and implemented the business logic required for Security presentation controller Used JSP Servlet coding under J2EE Environment Good Experience in software configuration management using CVS GIT and SVN Designed XML files to implement most of the wiring need for Hibernate annotations and Struts configurations Responsible for developing the forms which contains the details of the employees and generating the reports and bills Developed Web Services for data transfer from client to server and vice versa using Apache Axis SOAP and WSDL Involved in designing of class and data flow diagrams using UML Rational Rose Created and modified Stored Procedures Functions Triggers and Complex SQL Commands using PLSQL Involved in the Design of ERD Entity Relationship Diagrams for the Relational database Developed Shell scripts in UNIX and procedures using SQL and PLSQL to process the data from the input file and load into the database Used CVS for maintaining the Source Code Designed developed and deployed on WebLogic Server Performed Unit Testing on the applications that are developed ENVIRONMENT Java JDK 16 J2EE JSP Servlet Hibernate JavaScript JDBC Oracle 10g UML Rational Rose SOAP Web Logic Server JUnit PLSQL CSS HTML XML Eclipse CVS GIT SVN Java Developer Southtech Limited Dhaka BD June 2007 to December 2011 DESCRIPTION Usage Management System UMS is a turnkey mediation rating and prebilling application that address the need for flexible reliable and costeffective usage processing RESPONSIBILITIES Worked on both WebLogic Portal 92 for Portal development and WebLogic 81 for Data Services Programming Developed the presentation layer using JSP HTML CSS and client validations using JavaScript Developed user interface using JSP Struts Tag Libraries to simplify the complexities of the application Developed business logic using Stateless session beans for calculating asset depreciation on Straight line and written down value approaches Database Modification using SQL PLSQL Stored procedures triggers Views in Oracle Created java classes to communicate with the database using JDBC Responsible for design and implementation of various modules of the application using StrutsSpringHibernate architecture Developed the Web Interface using Servlets Java Server Pages HTML and CSS Extensively used the JDBC Prepared Statement to embed the SQL queries into the java code Developed DAO Data Access Objects using Spring Framework 3 Developed Web applications with Rich Internet applications using Java applets Silverlight Java Deployed this application which uses J2EE architecture model and Struts Framework first on WebLogic and helped in migrating to JBoss Application server Used JavaScript to perform clientside validations and StrutsValidator Framework for serverside validation Designed and developed the application using various design patterns such as session facade business delegate and service locator Involved in designing usecase diagrams class diagrams interaction using UML model with Rational Rose ENVIRONMENT Java Servlets JSP EJB J2EE STRUTS XML XSLT JavaScript HTML CSS Spring 32 SQL PLSQL MS Visio Eclipse JDBC Windows XP WebLogic portal JBoss Education Bachelors Skills Eclipse 10 years HTML 8 years J2EE 10 years Java 10 years JavaScript 8 years Additional Information TECHNICAL SKILLS Languages JavaScript TypeScript ES6 Java J2EE HTML5 CSS3 PHP Design side Adobe Photoshop Adobe Illustrator Adobe Dreamweaver Adobe Flash Frontend jQuery Sass Less AJAX JSON Bootstrap Graphic libraries CreateJS D3 HighCharts jQueryUI Frameworks AngularJS Angular24 AngularCli Backend Nodejs Express Passportjs Promisejs JSP Databases MongoDB MySQL Oracle Servers REST Tomcat Apache AWS IDE Visual Studio Code Eclipse WebStorm Sublime Debug tool Chrome DevTools Firebug Production model Agile TDD BDD Project management Git SVN JIRA Testing Jasmine Mocha Protractor Junit Qunit Operating system Linux Windows XP7vista810 MacOS",
    "unique_id": "af0aa7d1-066f-4580-ad9b-891dfde5c851"
}