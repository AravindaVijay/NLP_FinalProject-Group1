{
    "clean_data": "Hadoop Administrator Hadoop Administrator Hadoop Administrator Lead IT Corporation Franklin TN 10 years of experience in Information Technology with a strong background in application development and maintenancesupport and full project life cycle using Microsoft NET Technologies including Unit Testing Client Interaction and handling functional technical queries 4 years of experience in BIG DATA Hadoop Administration and Development Handson experience in designing and implementing solutions using Hadoop HDFS Map Reduce HBase Hive Pig Spark Oozie Tez Yarn Sqoop Solr Zookeeper Experience in Configuring Namenode High availability and Namenode Federation Experience in Disaster recovery and Backup activities Experience in Multinode setup of Hadoop cluster Experience in Performance tuning and benchmarking of Hadoop Cluster Experience in Monitoring maintenance and troubleshooting of Hadoop cluster Experience in Security integration of Hadoop Cluster Good knowledge on Kerberos Security Setting up and integrating Hadoop eco system tools HBase Hive Pig Sqoop etc Making Hadoop cluster ready for development team working on POCs Experience in deploying Hadoop 20YARN Enabling and managing various components in Hadoop Ecosystem like HDFS YARN Map Reduce Hive Pig Sqoop Oozie Sentry Spark and Zookeeper Experienced in configuring installing upgrading and managing Cloudera Apache Hortonworks Hadoop Distributions Familiar with writing Oozie workflows and Job Controllers for job automation Hive automation Hands on experience in analyzing log files for Hadoop and ecosystem services and finding root cause Hands on experience in Installing Configuring and managing the Hands on experience in analyzing log files for Hadoop and ecosystem services and finding root cause Hands on experience in Installing Configuring and managing the HCatalog Experience in importing and exporting the data using Sqoop from HDFS to Relational Database systems and viceversa Optimizing performance of HBaseHivePig jobs Experience with Spark Streaming Sql MLib GraphX and integrating Spark with HDFS and HBase Experience in tuning and debugging Spark application running Experience integration of Kafka with Spark for real time data processing Handsonexperience on ZKFC in managing and configuring the Name Node failure scenarios Experience on Commissioning Decommissioning Balancing and Managing Nodes and tuning server for optimal performance of the cluster Experience in importing and exporting the data using Sqoop from HDFS to Relational Database systems and viceversa Optimizing performance of HBaseHivePig jobs Experience in understanding the security requirements for Hadoop and integrating with Kerberos authentication infrastructure KDC server setup creating realm domain managing Good Experience in using SQL Server 20082005 and its tools like SQL Server Reporting Services SSRS SQL Server Integration Services SSIS Well Proficient in writing Views Stored Procedures Functions MS SQL Server and Oracle Balaram Sekuboyina bshadoopadmngmailcom  Work Experience Hadoop Administrator Lead IT Corporation December 2017 to Present Nissan was the sixth largest automaker in the world Nissan leveraged the Austin patents to further develop their own modern engine designs Nissan management realized their Datsun small car line would fill an unmet need in markets Nissan tried to convert the Greek plant into one manufacturing cars for all European countries The RenaultNissan Alliance has evolved over years the alliance itself is incorporated as the RenaultNissan BV Group For many years Nissan used a red wordmark for the company and car badges for the Nissan and Infiniti brands Nissan maintains all the data related to Cars sales features of the cars faults of the cars when occurred using the telematics data and warranty of each vehicle depends of the type of cars All the data will is collected will be stored using using Hadoop environment by using HortonworksThis connects which various sources like Informatica Tableau to the get data maintained in Hadoop Cluster This uses a Hortonworks Hadoop Cluster Responsibilities Upgraded Hortonworks HADOOP Cluster from 25 to 265 for development staging pre prod and Prod with most of the available services in the Hadoop Ecosystem Installed HDF Cluster in the all development staging preprod and Prod with most of the available services in the Hadoop Ecosystem Installed System Security Services Daemon on all the environments to sync the Active Directory directly to the cluster and remove the local users Administered Cluster maintenance commissioning and decommissioning Data nodes Cluster Monitoring Troubleshooting Performed Addingremoving new nodes to an existing Hadoop cluster Defining the cron jobs to run the automated jobs at desired intervals Creating the policies in Ranger to make the cluster robust without any issues Installed different softwares likes subversionVeracryptGit for better performance of the cluster Proposing tools like Pycharm which helps the developers for better debugging of the code bshadoopadmngmailcom  Making the cluster available for various data sources for easy data access and make sure all the connections are reachable Scheduling the batch process jobs in the cluster and distribute the cluster among all jobs Creating alerts on Ambari based on the criticality Creating the coding standards for the developers reviewing the codes and deploying the codes in different enviroments Overseeing the smooth execution of the code into the cluster on the three environemnts of minor and major enhancements Environment HDP 265 Ambari 262 HADOOP HDFS Zookeeper Map Reduce YARN Scala Spark Python HBASE Hive SQOOP OOZIE Linux CENTOS UBUNTU Red Hat Hadoop Administrator CrBard Houston TX December 2016 to August 2017 CRBard is health care manufacturing equipments for patients and healthcare professionals in wellness and prevention early diagnosis treatment and postcare management In fact throughout our history Bard has lead the industry in groundbreaking devices and therapies that continuously seek to set the new standard for excellence and quality A leading multinational developer manufacturer and marketer of innovative lifeenhancing medical technologies in the fields of vascular urology oncology and surgical specialties Developing innovative medical devices for more than 100 years that meet the needs of clinicians and their patients with more than 75 locations worldwide Pioneered the development of single patientuse medical products for hospital procedures today it is dedicated to pursuing technological innovations that offer superior clinical benefits while helping to reduce overall costs Responsibilities Installed and configured Hortonworks HADOOP Cluster from scratch for development staging preprod and Prod with most of the available services in the Hadoop Ecosystem Administered Cluster maintenance commissioning and decommissioning Data nodes Cluster Monitoring Troubleshooting Performed Addingremoving new nodes to an existing Hadoop cluster Implemented Backup configurations and Recoveries from a Name Node failure Monitored systems and services architecture design and implementation of Hadoop deployment configuration management backup and disaster recovery systems and procedures Configured various property files like coresitexml hdfssitexml mapredsitexml based upon the job requirement Performed Importing and exporting data into HDFS using SQOOP Installed and configured HDFS Zookeeper Map Reduce Yarn HBASE Hive SQOOP and OOZIE Integrated Hive and HBASE to perform analysis on data balaramsekuboyinak2gmailcom  Managed and reviewed Hadoop Log files as a part of administration for troubleshooting purposes Communicated and escalated issues appropriately Applied standard Back up policies to make sure the high availability of cluster Involved in analyzing system failures identifying root causes and recommended course of actions Documented the systems processes and procedures for future references Worked with systems engineering team to plan and deploy new Hadoop environments and expand existing Hadoop clusters Support preproduction and production support teams in the analysis of critical services and assists with maintenance operations Improve system performance by working with the development team to analyze identify and resolve issues quickly Automate administration tasks with shell scripting and Job Scheduling using CRON Performed many Proof of Concepts to the client and suggested different tools available in the Hadoop Ecosystem Environment HDP 26 Ambari 25 HADOOP HDFS Zookeeper Map Reduce YARN Scala Spark Python HBASE Hive SQOOP OOZIE Linux CENTOS UBUNTU Red Hat Application Software DeveloperHadoop Administrator Accenture Hyderabad Houston TX January 2011 to November 2016 Shell Oil Company is the United Statesbased subsidiary of Royal Dutch Shell a multinational oil major of AngloDutch origins which is amongst the largest oil companies in the world Shell products include oils fuels and card services as well as exploration production and refining of petroleum products Shell Oil operates as a subsidiary of Royal DutchShell Group the second largest oil company in the world In 1999 Shell Oil and its USbased counterparts secured 22 percent of the Groups income Shell is a global group of energy and petrochemicals companies with around 90000 employees in more than 80 countries and territories Shell innovative approach ensures to help tackle the challenge of new energy future Shell energy business is spread across 3 areas Upstream Midstream and Downstream Shell has requirements to automate many standard manual processes to automate using workflow technology This enables lot of work spread among the teams to get chance to work on multiple workflows and deliverables Shell is recommending to develop workflows in SharePoint K2 SharePoint team handled the following workflows Responsibilities Analyze customer orders set delivery priorities and make schedule adjustments to meet timely delivery goals Worked on Intranet site and prepared plan for migration from SharePoint 2010 to 2013 and SharePoint online365 Installed and configured Hadoop Map Reduce HDFS Developed multiple Map Reduce jobs in java for data cleaning and preprocessing Worked on Installing and configuring the HDP Hortonworks 2x Clusters in Dev and Staging Environments Worked on installing and configuring HDP 23 HDP 24 HDP 25 and HDP 26 Worked on Capacity planning for the Production Cluster Involved in loading data from UNIX file system to HDFS using Sqoop Involved in creating Hive tables loading the data and writing hive queries which will run internally in map reduce way balaramsekuboyinak2gmailcom  Worked on Configuring Oozie Jobs Worked on Configuring High Availability for Name Node in HDP 2x Worked on Configuring Kerberos Authentication in the cluster Worked on cluster upgradation in Hadoop from HDP 21 to HDP 23 Worked on Configuring queues in capacity scheduler Created tables loaded data and wrote queries in Hive Monitored cluster using Ambari and optimize system based on job performance and criteria Managed cluster through performance tuning and enhancements Formulated procedures for installation of Hadoop patches updates and version upgrades Ensured data recoverability by implementing system and application level backups Worked on installing and configuring Solr 521 in Hadoop cluster Worked on taking Snapshot backups for HBase tables Responsible for Cluster maintenance Monitoring commissioning and decommissioning Data nodes Troubleshooting Manage and review data backups Manage review log files Day to day responsibilities includes solving developer issues deployments moving code from one environment to other environment providing access to new users and providing instant solutions to reduce the impact and documenting the same and preventing future issues Addinginstallation of new components and removal of them through Ambari Collaborating with application teams to install operating system and Hadoop updates patches version upgrades Monitored workload job performance and capacity planning Involved in Analyzing system failures identifying root causes and recommended course of actions Responsible for resolving the issues with the specified SLA Involved in Migration of k2 from 45 to 465 Reduced the time out errors in K2 Involved in the K2 Archiving Process Extensively worked on Disaster Recovery Plan in K2 Creating the deployment packages in K2 Handling different cloud machines to maintain K2 the applications Created Stored Procedures to support Daily scheduled Feeds by providing necessary Data for the creation of Data Files Environment HDP 2X CDH Apache Hadoop Hive Pig Flume Oozie Sqoop MS Visual Studio 2012 MS Net Framework 40 C Angularjs K2 workflows Smart Objects SQL Server Oracle web logic Developer Procter and Gamble Cincinnati OH November 2009 to December 2010 Procter Gamble Co also known as PG is an American multinational consumer goods company headquartered in downtown Cincinnati Ohio United States PG announced it was streamlining the company dropping around 100 brands and concentrating on the remaining 80 brands which produced 95 percent of the companys profits The company began to build factories in other locations in the United States because the demand for products had outgrown the capacity of the Cincinnati facilities Procter Gamble acquired a number of other companies that diversified its product line and significantly increased profits balaramsekuboyinak2gmailcom  Procter and Gambles Global Business Services Decision Cockpits is responsible for integrating data from across PG to provide analytic reporting to enable decision making In order to do this in a quick efficient manner PG has put in place a strategy to create Decision Cockpit DC Decision Cockpit 30 35 is to increase adoption among end users and meet the business reporting needs of end users DC 35 will add additional value by providing Personalization Look Feel to employees and services tools such as travel calendar learning and performance resources Responsibilities Involved in Functional Integration Regression Smoke and System testing and logging defects Developed and Deployed InfoPath forms which involve creating InfoPath forms that retrieve data directly from the Monsanto End Client Test SQL Server database Developed Office SharePoint Server 2007 InfoPath Services to enable the interaction of the end users through a Web browser Configured the custom workflows to InfoPath forms Design and Developed the functionality to Add Edit and Delete the existing items in the SharePoint list through a InfoPath form Created SharePoint Features for enabling InfoPath form And Used the SharePoint Solutions for creating the packages and Deployment Analyzed the System Functional Requirement document and written Test Scenarios for the functionality testing of the application Ensured that all the test cases are updated Developed Portlets based on the clients requirements and assigning the security to the portlet based on the availability of the regions and users Involved in doing the Build Reviews and Peer Reviews once the build is completed Worked on SharePoint for security quota to the sites Reviewed tests specifications test cases and performed manual testing Involved in doing Smoke test once the production is completed Providing Test status updates and daily test progress reports Used Team Foundation Server TFS 20052008 as versionchange management control Testing of UAT defects in test environment before the new code gets moved to UAT environment Preparing the Migration Documents with no issues from the production and Involving with the client team during the production Migration Created LLDs HLDs and test case documents Created documents like code review check list Migration check list Involved in all the existing releases and helped the team in resolving critical issues Worked with the QA team to test the code Environment Oracle web Logic SharePoint Vignette Developer Nokia Chennai Tamil Nadu June 2008 to October 2009 The company currently focuses on largescale telecommunications infrastructures and technology development and licensing Nokia is also a major contributor to the mobile telephony industry having assisted in development of the GSM and LTE standards and was for a period the largest vendor of mobile phones in the world Nokias dominance also extended into the smartphone industry through its Symbian platform Nokia eventually entered into a pact with Microsoft in 2011 to exclusively use its Windows Phone platform on future smartphones balaramsekuboyinak2gmailcom  The purpose of this Project is to create Web Sites across 58 countries for every phone they release and for every accessory which are compatible for that phone Creating Nokia maps which has got the highest reputation during the recent times Responsibilities Created the content items in the Content Management System CMS Arranged the contents in the Site ManagementSM Involved in doing the Peer Reviews once the migration is completed Created documents which helped for quick reference for the new joiners Involved in resolving the critical UAT issues Involved in all the existing releases and helped the team in resolving critical issues Worked with the QA team to reduce defects before moving to UAT Environment Vignette Accenture Hyderabad Hyderabad Telangana 2005 to 2008 Education Bachelor of Technology in Computer Science Engineering in Computer Science Engineering PVP Nagarjuna Institute of Technology Vijayawada Andhra Pradesh August 2007 Skills Oozie Sqoop Hbase Hadoop Map reduce CertificationsLicenses Drivers License Additional Information TECHNICAL SKILLS Languages CNET VBNET JavaScript JQuery ASPNET Angularjs Silverlight XML HTML CSS Web Technologies SQL Server 05 MS ACCESS Databases Visual Studio 05Sharepoint Online SharePoint IDE 20132010MOSS 2007 3rd Party Toolkits K2 Black Pearl Smart Forms Vignette ALUI Aqua Logic User Interface Reporting Tools SQL Server Reporting Services Version Control Team Foundation Server Distribution Hadoop Frameworks Hadoop Hortonworks HDP 2X Cloudera Distributions Map Reduce HBase 11 Hive12 Sqoop 146 Pig 016 Hadoop Oozie 420 Technologies",
    "entities": [
        "Ranger",
        "Developed Portlets",
        "Test Scenarios",
        "Development Handson",
        "UBUNTU Red Hat Application Software DeveloperHadoop Administrator",
        "Hortonworks Hadoop",
        "CertificationsLicenses Drivers License Additional Information TECHNICAL SKILLS Languages CNET VBNET JavaScript JQuery ASPNET Angularjs Silverlight",
        "SQL Server 20082005",
        "Information Technology",
        "HDFS",
        "UNIX",
        "Applied",
        "Job Scheduling",
        "Royal DutchShell Group",
        "the SharePoint Solutions",
        "Ambari Collaborating",
        "SQOOP Installed",
        "Procter and Gambles Global Business Services Decision Cockpits",
        "Hortonworks Hadoop Cluster Responsibilities Upgraded Hortonworks HADOOP Cluster",
        "Hadoop Ecosystem",
        "Ambari",
        "Hadoop",
        "Dev and Staging Environments Worked",
        "Bard",
        "Skills Oozie Sqoop Hbase Hadoop",
        "Informatica Tableau",
        "Cincinnati",
        "the Active Directory",
        "Shell",
        "Hive Monitored",
        "LTE",
        "Responsibilities Created",
        "AngloDutch",
        "HBase",
        "Formulated",
        "GSM",
        "HBase Hive Pig",
        "TX",
        "the Hadoop Ecosystem Installed System Security Services",
        "SQL Server Reporting Services",
        "Hadoop Cluster Good",
        "Procter Gamble",
        "the Content Management System CMS Arranged",
        "the Hadoop Ecosystem Installed HDF Cluster",
        "Created SharePoint Features",
        "Windows Phone",
        "3rd Party",
        "Kerberos",
        "Monsanto",
        "Commissioning Decommissioning Balancing",
        "Cloudera Apache",
        "Unit Testing Client Interaction",
        "Responsibilities Involved",
        "Hadoop Log",
        "the Production Cluster Involved",
        "the Migration Documents",
        "Used Team Foundation",
        "Monitored",
        "Communicated",
        "Microsoft NET Technologies",
        "the Hadoop Ecosystem Administered Cluster",
        "the RenaultNissan BV Group",
        "Hortonworks HADOOP Cluster",
        "Hadoop Cluster",
        "Nokia",
        "HDP",
        "MS Net Framework",
        "Responsible for Cluster",
        "Royal Dutch Shell",
        "Spark",
        "Procter Gamble Co",
        "Shell Oil Company",
        "Infiniti",
        "PG",
        "Hadoop Cluster Experience",
        "Peer Reviews",
        "Installing Configuring",
        "Improve",
        "SQL Server Integration Services",
        "Handsonexperience",
        "Sqoop",
        "QA",
        "Created",
        "Responsibilities Installed",
        "Multinode",
        "UAT Environment Vignette Accenture Hyderabad Hyderabad",
        "java",
        "Oozie",
        "SharePoint K2 SharePoint",
        "Nissan",
        "Smart Objects",
        "Deployment Analyzed the System Functional Requirement",
        "KDC",
        "The RenaultNissan Alliance",
        "the United States",
        "OOZIE Integrated Hive",
        "United States",
        "SLA Involved",
        "the Hadoop Ecosystem Environment",
        "Implemented Backup",
        "Upstream Midstream",
        "Prod",
        "Oracle Balaram Sekuboyina",
        "Microsoft",
        "Creating Nokia",
        "Hadoop Administrator Hadoop Administrator Hadoop Administrator Lead IT Corporation Franklin",
        "BIG DATA Hadoop Administration",
        "Namenode Federation",
        "Data",
        "SharePoint",
        "Relational Database",
        "Spark Streaming Sql MLib",
        "Version Control Team Foundation Server Distribution Hadoop Frameworks Hadoop",
        "Developed Office SharePoint Server",
        "Migration Created",
        "Cluster Monitoring Troubleshooting Performed Addingremoving",
        "Data Files Environment",
        "Shell Oil",
        "Ohio",
        "InfoPath Services"
    ],
    "experience": "Experience in Configuring Namenode High availability and Namenode Federation Experience in Disaster recovery and Backup activities Experience in Multinode setup of Hadoop cluster Experience in Performance tuning and benchmarking of Hadoop Cluster Experience in Monitoring maintenance and troubleshooting of Hadoop cluster Experience in Security integration of Hadoop Cluster Good knowledge on Kerberos Security Setting up and integrating Hadoop eco system tools HBase Hive Pig Sqoop etc Making Hadoop cluster ready for development team working on POCs Experience in deploying Hadoop 20YARN Enabling and managing various components in Hadoop Ecosystem like HDFS YARN Map Reduce Hive Pig Sqoop Oozie Sentry Spark and Zookeeper Experienced in configuring installing upgrading and managing Cloudera Apache Hortonworks Hadoop Distributions Familiar with writing Oozie workflows and Job Controllers for job automation Hive automation Hands on experience in analyzing log files for Hadoop and ecosystem services and finding root cause Hands on experience in Installing Configuring and managing the Hands on experience in analyzing log files for Hadoop and ecosystem services and finding root cause Hands on experience in Installing Configuring and managing the HCatalog Experience in importing and exporting the data using Sqoop from HDFS to Relational Database systems and viceversa Optimizing performance of HBaseHivePig jobs Experience with Spark Streaming Sql MLib GraphX and integrating Spark with HDFS and HBase Experience in tuning and debugging Spark application running Experience integration of Kafka with Spark for real time data processing Handsonexperience on ZKFC in managing and configuring the Name Node failure scenarios Experience on Commissioning Decommissioning Balancing and Managing Nodes and tuning server for optimal performance of the cluster Experience in importing and exporting the data using Sqoop from HDFS to Relational Database systems and viceversa Optimizing performance of HBaseHivePig jobs Experience in understanding the security requirements for Hadoop and integrating with Kerberos authentication infrastructure KDC server setup creating realm domain managing Good Experience in using SQL Server 20082005 and its tools like SQL Server Reporting Services SSRS SQL Server Integration Services SSIS Well Proficient in writing Views Stored Procedures Functions MS SQL Server and Oracle Balaram Sekuboyina bshadoopadmngmailcom   Work Experience Hadoop Administrator Lead IT Corporation December 2017 to Present Nissan was the sixth largest automaker in the world Nissan leveraged the Austin patents to further develop their own modern engine designs Nissan management realized their Datsun small car line would fill an unmet need in markets Nissan tried to convert the Greek plant into one manufacturing cars for all European countries The RenaultNissan Alliance has evolved over years the alliance itself is incorporated as the RenaultNissan BV Group For many years Nissan used a red wordmark for the company and car badges for the Nissan and Infiniti brands Nissan maintains all the data related to Cars sales features of the cars faults of the cars when occurred using the telematics data and warranty of each vehicle depends of the type of cars All the data will is collected will be stored using using Hadoop environment by using HortonworksThis connects which various sources like Informatica Tableau to the get data maintained in Hadoop Cluster This uses a Hortonworks Hadoop Cluster Responsibilities Upgraded Hortonworks HADOOP Cluster from 25 to 265 for development staging pre prod and Prod with most of the available services in the Hadoop Ecosystem Installed HDF Cluster in the all development staging preprod and Prod with most of the available services in the Hadoop Ecosystem Installed System Security Services Daemon on all the environments to sync the Active Directory directly to the cluster and remove the local users Administered Cluster maintenance commissioning and decommissioning Data nodes Cluster Monitoring Troubleshooting Performed Addingremoving new nodes to an existing Hadoop cluster Defining the cron jobs to run the automated jobs at desired intervals Creating the policies in Ranger to make the cluster robust without any issues Installed different softwares likes subversionVeracryptGit for better performance of the cluster Proposing tools like Pycharm which helps the developers for better debugging of the code bshadoopadmngmailcom   Making the cluster available for various data sources for easy data access and make sure all the connections are reachable Scheduling the batch process jobs in the cluster and distribute the cluster among all jobs Creating alerts on Ambari based on the criticality Creating the coding standards for the developers reviewing the codes and deploying the codes in different enviroments Overseeing the smooth execution of the code into the cluster on the three environemnts of minor and major enhancements Environment HDP 265 Ambari 262 HADOOP HDFS Zookeeper Map Reduce YARN Scala Spark Python HBASE Hive SQOOP OOZIE Linux CENTOS UBUNTU Red Hat Hadoop Administrator CrBard Houston TX December 2016 to August 2017 CRBard is health care manufacturing equipments for patients and healthcare professionals in wellness and prevention early diagnosis treatment and postcare management In fact throughout our history Bard has lead the industry in groundbreaking devices and therapies that continuously seek to set the new standard for excellence and quality A leading multinational developer manufacturer and marketer of innovative lifeenhancing medical technologies in the fields of vascular urology oncology and surgical specialties Developing innovative medical devices for more than 100 years that meet the needs of clinicians and their patients with more than 75 locations worldwide Pioneered the development of single patientuse medical products for hospital procedures today it is dedicated to pursuing technological innovations that offer superior clinical benefits while helping to reduce overall costs Responsibilities Installed and configured Hortonworks HADOOP Cluster from scratch for development staging preprod and Prod with most of the available services in the Hadoop Ecosystem Administered Cluster maintenance commissioning and decommissioning Data nodes Cluster Monitoring Troubleshooting Performed Addingremoving new nodes to an existing Hadoop cluster Implemented Backup configurations and Recoveries from a Name Node failure Monitored systems and services architecture design and implementation of Hadoop deployment configuration management backup and disaster recovery systems and procedures Configured various property files like coresitexml hdfssitexml mapredsitexml based upon the job requirement Performed Importing and exporting data into HDFS using SQOOP Installed and configured HDFS Zookeeper Map Reduce Yarn HBASE Hive SQOOP and OOZIE Integrated Hive and HBASE to perform analysis on data balaramsekuboyinak2gmailcom   Managed and reviewed Hadoop Log files as a part of administration for troubleshooting purposes Communicated and escalated issues appropriately Applied standard Back up policies to make sure the high availability of cluster Involved in analyzing system failures identifying root causes and recommended course of actions Documented the systems processes and procedures for future references Worked with systems engineering team to plan and deploy new Hadoop environments and expand existing Hadoop clusters Support preproduction and production support teams in the analysis of critical services and assists with maintenance operations Improve system performance by working with the development team to analyze identify and resolve issues quickly Automate administration tasks with shell scripting and Job Scheduling using CRON Performed many Proof of Concepts to the client and suggested different tools available in the Hadoop Ecosystem Environment HDP 26 Ambari 25 HADOOP HDFS Zookeeper Map Reduce YARN Scala Spark Python HBASE Hive SQOOP OOZIE Linux CENTOS UBUNTU Red Hat Application Software DeveloperHadoop Administrator Accenture Hyderabad Houston TX January 2011 to November 2016 Shell Oil Company is the United Statesbased subsidiary of Royal Dutch Shell a multinational oil major of AngloDutch origins which is amongst the largest oil companies in the world Shell products include oils fuels and card services as well as exploration production and refining of petroleum products Shell Oil operates as a subsidiary of Royal DutchShell Group the second largest oil company in the world In 1999 Shell Oil and its USbased counterparts secured 22 percent of the Groups income Shell is a global group of energy and petrochemicals companies with around 90000 employees in more than 80 countries and territories Shell innovative approach ensures to help tackle the challenge of new energy future Shell energy business is spread across 3 areas Upstream Midstream and Downstream Shell has requirements to automate many standard manual processes to automate using workflow technology This enables lot of work spread among the teams to get chance to work on multiple workflows and deliverables Shell is recommending to develop workflows in SharePoint K2 SharePoint team handled the following workflows Responsibilities Analyze customer orders set delivery priorities and make schedule adjustments to meet timely delivery goals Worked on Intranet site and prepared plan for migration from SharePoint 2010 to 2013 and SharePoint online365 Installed and configured Hadoop Map Reduce HDFS Developed multiple Map Reduce jobs in java for data cleaning and preprocessing Worked on Installing and configuring the HDP Hortonworks 2x Clusters in Dev and Staging Environments Worked on installing and configuring HDP 23 HDP 24 HDP 25 and HDP 26 Worked on Capacity planning for the Production Cluster Involved in loading data from UNIX file system to HDFS using Sqoop Involved in creating Hive tables loading the data and writing hive queries which will run internally in map reduce way balaramsekuboyinak2gmailcom   Worked on Configuring Oozie Jobs Worked on Configuring High Availability for Name Node in HDP 2x Worked on Configuring Kerberos Authentication in the cluster Worked on cluster upgradation in Hadoop from HDP 21 to HDP 23 Worked on Configuring queues in capacity scheduler Created tables loaded data and wrote queries in Hive Monitored cluster using Ambari and optimize system based on job performance and criteria Managed cluster through performance tuning and enhancements Formulated procedures for installation of Hadoop patches updates and version upgrades Ensured data recoverability by implementing system and application level backups Worked on installing and configuring Solr 521 in Hadoop cluster Worked on taking Snapshot backups for HBase tables Responsible for Cluster maintenance Monitoring commissioning and decommissioning Data nodes Troubleshooting Manage and review data backups Manage review log files Day to day responsibilities includes solving developer issues deployments moving code from one environment to other environment providing access to new users and providing instant solutions to reduce the impact and documenting the same and preventing future issues Addinginstallation of new components and removal of them through Ambari Collaborating with application teams to install operating system and Hadoop updates patches version upgrades Monitored workload job performance and capacity planning Involved in Analyzing system failures identifying root causes and recommended course of actions Responsible for resolving the issues with the specified SLA Involved in Migration of k2 from 45 to 465 Reduced the time out errors in K2 Involved in the K2 Archiving Process Extensively worked on Disaster Recovery Plan in K2 Creating the deployment packages in K2 Handling different cloud machines to maintain K2 the applications Created Stored Procedures to support Daily scheduled Feeds by providing necessary Data for the creation of Data Files Environment HDP 2X CDH Apache Hadoop Hive Pig Flume Oozie Sqoop MS Visual Studio 2012 MS Net Framework 40 C Angularjs K2 workflows Smart Objects SQL Server Oracle web logic Developer Procter and Gamble Cincinnati OH November 2009 to December 2010 Procter Gamble Co also known as PG is an American multinational consumer goods company headquartered in downtown Cincinnati Ohio United States PG announced it was streamlining the company dropping around 100 brands and concentrating on the remaining 80 brands which produced 95 percent of the companys profits The company began to build factories in other locations in the United States because the demand for products had outgrown the capacity of the Cincinnati facilities Procter Gamble acquired a number of other companies that diversified its product line and significantly increased profits balaramsekuboyinak2gmailcom   Procter and Gambles Global Business Services Decision Cockpits is responsible for integrating data from across PG to provide analytic reporting to enable decision making In order to do this in a quick efficient manner PG has put in place a strategy to create Decision Cockpit DC Decision Cockpit 30 35 is to increase adoption among end users and meet the business reporting needs of end users DC 35 will add additional value by providing Personalization Look Feel to employees and services tools such as travel calendar learning and performance resources Responsibilities Involved in Functional Integration Regression Smoke and System testing and logging defects Developed and Deployed InfoPath forms which involve creating InfoPath forms that retrieve data directly from the Monsanto End Client Test SQL Server database Developed Office SharePoint Server 2007 InfoPath Services to enable the interaction of the end users through a Web browser Configured the custom workflows to InfoPath forms Design and Developed the functionality to Add Edit and Delete the existing items in the SharePoint list through a InfoPath form Created SharePoint Features for enabling InfoPath form And Used the SharePoint Solutions for creating the packages and Deployment Analyzed the System Functional Requirement document and written Test Scenarios for the functionality testing of the application Ensured that all the test cases are updated Developed Portlets based on the clients requirements and assigning the security to the portlet based on the availability of the regions and users Involved in doing the Build Reviews and Peer Reviews once the build is completed Worked on SharePoint for security quota to the sites Reviewed tests specifications test cases and performed manual testing Involved in doing Smoke test once the production is completed Providing Test status updates and daily test progress reports Used Team Foundation Server TFS 20052008 as versionchange management control Testing of UAT defects in test environment before the new code gets moved to UAT environment Preparing the Migration Documents with no issues from the production and Involving with the client team during the production Migration Created LLDs HLDs and test case documents Created documents like code review check list Migration check list Involved in all the existing releases and helped the team in resolving critical issues Worked with the QA team to test the code Environment Oracle web Logic SharePoint Vignette Developer Nokia Chennai Tamil Nadu June 2008 to October 2009 The company currently focuses on largescale telecommunications infrastructures and technology development and licensing Nokia is also a major contributor to the mobile telephony industry having assisted in development of the GSM and LTE standards and was for a period the largest vendor of mobile phones in the world Nokias dominance also extended into the smartphone industry through its Symbian platform Nokia eventually entered into a pact with Microsoft in 2011 to exclusively use its Windows Phone platform on future smartphones balaramsekuboyinak2gmailcom   The purpose of this Project is to create Web Sites across 58 countries for every phone they release and for every accessory which are compatible for that phone Creating Nokia maps which has got the highest reputation during the recent times Responsibilities Created the content items in the Content Management System CMS Arranged the contents in the Site ManagementSM Involved in doing the Peer Reviews once the migration is completed Created documents which helped for quick reference for the new joiners Involved in resolving the critical UAT issues Involved in all the existing releases and helped the team in resolving critical issues Worked with the QA team to reduce defects before moving to UAT Environment Vignette Accenture Hyderabad Hyderabad Telangana 2005 to 2008 Education Bachelor of Technology in Computer Science Engineering in Computer Science Engineering PVP Nagarjuna Institute of Technology Vijayawada Andhra Pradesh August 2007 Skills Oozie Sqoop Hbase Hadoop Map reduce CertificationsLicenses Drivers License Additional Information TECHNICAL SKILLS Languages CNET VBNET JavaScript JQuery ASPNET Angularjs Silverlight XML HTML CSS Web Technologies SQL Server 05 MS ACCESS Databases Visual Studio 05Sharepoint Online SharePoint IDE 20132010MOSS 2007 3rd Party Toolkits K2 Black Pearl Smart Forms Vignette ALUI Aqua Logic User Interface Reporting Tools SQL Server Reporting Services Version Control Team Foundation Server Distribution Hadoop Frameworks Hadoop Hortonworks HDP 2X Cloudera Distributions Map Reduce HBase 11 Hive12 Sqoop 146 Pig 016 Hadoop Oozie 420 Technologies",
    "extracted_keywords": [
        "Hadoop",
        "Administrator",
        "Hadoop",
        "Administrator",
        "Hadoop",
        "Administrator",
        "Lead",
        "IT",
        "Corporation",
        "Franklin",
        "TN",
        "years",
        "experience",
        "Information",
        "Technology",
        "background",
        "application",
        "development",
        "maintenancesupport",
        "project",
        "life",
        "cycle",
        "Microsoft",
        "NET",
        "Technologies",
        "Unit",
        "Testing",
        "Client",
        "Interaction",
        "queries",
        "years",
        "experience",
        "BIG",
        "DATA",
        "Hadoop",
        "Administration",
        "Development",
        "Handson",
        "experience",
        "solutions",
        "Hadoop",
        "HDFS",
        "Map",
        "Reduce",
        "HBase",
        "Hive",
        "Pig",
        "Spark",
        "Oozie",
        "Tez",
        "Yarn",
        "Sqoop",
        "Solr",
        "Zookeeper",
        "Experience",
        "Configuring",
        "Namenode",
        "availability",
        "Namenode",
        "Federation",
        "Experience",
        "Disaster",
        "recovery",
        "activities",
        "Experience",
        "Multinode",
        "setup",
        "Hadoop",
        "cluster",
        "Experience",
        "Performance",
        "tuning",
        "benchmarking",
        "Hadoop",
        "Cluster",
        "Experience",
        "maintenance",
        "troubleshooting",
        "Hadoop",
        "cluster",
        "Experience",
        "Security",
        "integration",
        "Hadoop",
        "Cluster",
        "knowledge",
        "Kerberos",
        "Security",
        "Hadoop",
        "eco",
        "system",
        "HBase",
        "Hive",
        "Pig",
        "Sqoop",
        "Hadoop",
        "cluster",
        "development",
        "team",
        "POCs",
        "Experience",
        "Hadoop",
        "Enabling",
        "components",
        "Hadoop",
        "Ecosystem",
        "HDFS",
        "YARN",
        "Map",
        "Reduce",
        "Hive",
        "Pig",
        "Sqoop",
        "Oozie",
        "Sentry",
        "Spark",
        "Zookeeper",
        "upgrading",
        "Cloudera",
        "Apache",
        "Hortonworks",
        "Hadoop",
        "Distributions",
        "Oozie",
        "workflows",
        "Job",
        "Controllers",
        "job",
        "automation",
        "Hive",
        "automation",
        "Hands",
        "experience",
        "log",
        "files",
        "Hadoop",
        "ecosystem",
        "services",
        "root",
        "Hands",
        "experience",
        "Configuring",
        "Hands",
        "experience",
        "log",
        "files",
        "Hadoop",
        "ecosystem",
        "services",
        "root",
        "Hands",
        "experience",
        "Configuring",
        "HCatalog",
        "Experience",
        "data",
        "Sqoop",
        "HDFS",
        "Relational",
        "Database",
        "systems",
        "viceversa",
        "Optimizing",
        "performance",
        "jobs",
        "Experience",
        "Spark",
        "Streaming",
        "Sql",
        "MLib",
        "GraphX",
        "Spark",
        "HDFS",
        "HBase",
        "Experience",
        "Spark",
        "application",
        "Experience",
        "integration",
        "Kafka",
        "Spark",
        "time",
        "data",
        "Handsonexperience",
        "ZKFC",
        "Name",
        "Node",
        "failure",
        "scenarios",
        "Experience",
        "Decommissioning",
        "Balancing",
        "Managing",
        "Nodes",
        "server",
        "performance",
        "cluster",
        "Experience",
        "data",
        "Sqoop",
        "HDFS",
        "Relational",
        "Database",
        "systems",
        "viceversa",
        "Optimizing",
        "performance",
        "jobs",
        "Experience",
        "security",
        "requirements",
        "Hadoop",
        "authentication",
        "infrastructure",
        "KDC",
        "server",
        "setup",
        "realm",
        "domain",
        "Good",
        "Experience",
        "SQL",
        "Server",
        "tools",
        "SQL",
        "Server",
        "Reporting",
        "Services",
        "SSRS",
        "SQL",
        "Server",
        "Integration",
        "Services",
        "SSIS",
        "Views",
        "Stored",
        "Procedures",
        "Functions",
        "MS",
        "SQL",
        "Server",
        "Oracle",
        "Balaram",
        "Sekuboyina",
        "bshadoopadmngmailcom",
        "Work",
        "Experience",
        "Hadoop",
        "Administrator",
        "Lead",
        "IT",
        "Corporation",
        "December",
        "Present",
        "Nissan",
        "automaker",
        "world",
        "Nissan",
        "Austin",
        "patents",
        "engine",
        "Nissan",
        "management",
        "Datsun",
        "car",
        "line",
        "need",
        "markets",
        "Nissan",
        "plant",
        "manufacturing",
        "cars",
        "countries",
        "RenaultNissan",
        "Alliance",
        "years",
        "alliance",
        "RenaultNissan",
        "BV",
        "Group",
        "years",
        "Nissan",
        "wordmark",
        "company",
        "car",
        "badges",
        "Nissan",
        "Infiniti",
        "Nissan",
        "data",
        "Cars",
        "sales",
        "features",
        "cars",
        "faults",
        "cars",
        "telematics",
        "data",
        "warranty",
        "vehicle",
        "type",
        "cars",
        "data",
        "Hadoop",
        "environment",
        "HortonworksThis",
        "connects",
        "sources",
        "Informatica",
        "Tableau",
        "get",
        "data",
        "Hadoop",
        "Cluster",
        "Hortonworks",
        "Hadoop",
        "Cluster",
        "Responsibilities",
        "Hortonworks",
        "HADOOP",
        "Cluster",
        "development",
        "prod",
        "Prod",
        "services",
        "Hadoop",
        "Ecosystem",
        "HDF",
        "Cluster",
        "development",
        "staging",
        "preprod",
        "Prod",
        "services",
        "Hadoop",
        "Ecosystem",
        "Installed",
        "System",
        "Security",
        "Services",
        "Daemon",
        "environments",
        "Active",
        "Directory",
        "cluster",
        "users",
        "Cluster",
        "maintenance",
        "Data",
        "nodes",
        "Cluster",
        "Monitoring",
        "Troubleshooting",
        "Performed",
        "nodes",
        "Hadoop",
        "cluster",
        "cron",
        "jobs",
        "jobs",
        "intervals",
        "policies",
        "Ranger",
        "cluster",
        "robust",
        "issues",
        "softwares",
        "subversionVeracryptGit",
        "performance",
        "cluster",
        "Proposing",
        "tools",
        "Pycharm",
        "developers",
        "debugging",
        "code",
        "bshadoopadmngmailcom",
        "cluster",
        "data",
        "sources",
        "data",
        "access",
        "connections",
        "Scheduling",
        "batch",
        "process",
        "jobs",
        "cluster",
        "cluster",
        "jobs",
        "alerts",
        "Ambari",
        "criticality",
        "standards",
        "developers",
        "codes",
        "codes",
        "enviroments",
        "execution",
        "code",
        "cluster",
        "environemnts",
        "enhancements",
        "Environment",
        "HDP",
        "Ambari",
        "HADOOP",
        "HDFS",
        "Zookeeper",
        "Map",
        "Reduce",
        "YARN",
        "Scala",
        "Spark",
        "Python",
        "HBASE",
        "Hive",
        "SQOOP",
        "OOZIE",
        "Linux",
        "CENTOS",
        "UBUNTU",
        "Red",
        "Hat",
        "Hadoop",
        "Administrator",
        "CrBard",
        "Houston",
        "TX",
        "December",
        "August",
        "CRBard",
        "health",
        "care",
        "manufacturing",
        "equipments",
        "patients",
        "healthcare",
        "professionals",
        "wellness",
        "prevention",
        "diagnosis",
        "treatment",
        "postcare",
        "management",
        "fact",
        "history",
        "Bard",
        "industry",
        "groundbreaking",
        "devices",
        "therapies",
        "standard",
        "excellence",
        "quality",
        "developer",
        "manufacturer",
        "marketer",
        "technologies",
        "fields",
        "urology",
        "oncology",
        "specialties",
        "devices",
        "years",
        "needs",
        "clinicians",
        "patients",
        "locations",
        "development",
        "patientuse",
        "products",
        "hospital",
        "procedures",
        "today",
        "innovations",
        "benefits",
        "costs",
        "Responsibilities",
        "Hortonworks",
        "HADOOP",
        "Cluster",
        "scratch",
        "development",
        "staging",
        "preprod",
        "Prod",
        "services",
        "Hadoop",
        "Ecosystem",
        "Cluster",
        "maintenance",
        "Data",
        "nodes",
        "Cluster",
        "Monitoring",
        "Troubleshooting",
        "Performed",
        "nodes",
        "Hadoop",
        "cluster",
        "Backup",
        "configurations",
        "Recoveries",
        "Name",
        "Node",
        "failure",
        "Monitored",
        "systems",
        "services",
        "architecture",
        "design",
        "implementation",
        "Hadoop",
        "deployment",
        "configuration",
        "management",
        "backup",
        "disaster",
        "recovery",
        "systems",
        "procedures",
        "property",
        "files",
        "coresitexml",
        "hdfssitexml",
        "mapredsitexml",
        "job",
        "requirement",
        "Performed",
        "Importing",
        "data",
        "HDFS",
        "SQOOP",
        "Installed",
        "HDFS",
        "Zookeeper",
        "Map",
        "Reduce",
        "Yarn",
        "HBASE",
        "Hive",
        "SQOOP",
        "OOZIE",
        "Integrated",
        "Hive",
        "HBASE",
        "analysis",
        "data",
        "balaramsekuboyinak2gmailcom",
        "Managed",
        "Hadoop",
        "Log",
        "files",
        "part",
        "administration",
        "troubleshooting",
        "purposes",
        "issues",
        "policies",
        "availability",
        "cluster",
        "system",
        "failures",
        "root",
        "causes",
        "course",
        "actions",
        "systems",
        "processes",
        "procedures",
        "references",
        "systems",
        "engineering",
        "team",
        "Hadoop",
        "environments",
        "Hadoop",
        "clusters",
        "Support",
        "preproduction",
        "production",
        "support",
        "teams",
        "analysis",
        "services",
        "assists",
        "maintenance",
        "operations",
        "system",
        "performance",
        "development",
        "team",
        "issues",
        "administration",
        "tasks",
        "shell",
        "scripting",
        "Job",
        "Scheduling",
        "CRON",
        "Proof",
        "Concepts",
        "client",
        "tools",
        "Hadoop",
        "Ecosystem",
        "Environment",
        "HDP",
        "Ambari",
        "HADOOP",
        "HDFS",
        "Zookeeper",
        "Map",
        "Reduce",
        "YARN",
        "Scala",
        "Spark",
        "Python",
        "HBASE",
        "Hive",
        "SQOOP",
        "OOZIE",
        "Linux",
        "CENTOS",
        "UBUNTU",
        "Red",
        "Hat",
        "Application",
        "Software",
        "DeveloperHadoop",
        "Administrator",
        "Accenture",
        "Hyderabad",
        "Houston",
        "TX",
        "January",
        "November",
        "Shell",
        "Oil",
        "Company",
        "United",
        "Statesbased",
        "subsidiary",
        "Royal",
        "Dutch",
        "Shell",
        "oil",
        "major",
        "AngloDutch",
        "origins",
        "oil",
        "companies",
        "world",
        "Shell",
        "products",
        "oils",
        "fuels",
        "card",
        "services",
        "exploration",
        "production",
        "refining",
        "petroleum",
        "products",
        "Shell",
        "Oil",
        "subsidiary",
        "Royal",
        "DutchShell",
        "Group",
        "oil",
        "company",
        "world",
        "Shell",
        "Oil",
        "USbased",
        "counterparts",
        "percent",
        "Groups",
        "income",
        "Shell",
        "group",
        "energy",
        "petrochemicals",
        "companies",
        "employees",
        "countries",
        "territories",
        "Shell",
        "approach",
        "challenge",
        "energy",
        "Shell",
        "energy",
        "business",
        "areas",
        "Upstream",
        "Midstream",
        "Downstream",
        "Shell",
        "requirements",
        "processes",
        "workflow",
        "technology",
        "lot",
        "work",
        "teams",
        "chance",
        "workflows",
        "deliverables",
        "Shell",
        "workflows",
        "SharePoint",
        "K2",
        "SharePoint",
        "team",
        "workflows",
        "Responsibilities",
        "Analyze",
        "customer",
        "orders",
        "delivery",
        "priorities",
        "schedule",
        "adjustments",
        "delivery",
        "goals",
        "Intranet",
        "site",
        "plan",
        "migration",
        "SharePoint",
        "SharePoint",
        "online365",
        "Installed",
        "Hadoop",
        "Map",
        "Reduce",
        "HDFS",
        "Map",
        "Reduce",
        "jobs",
        "java",
        "data",
        "cleaning",
        "Worked",
        "HDP",
        "Hortonworks",
        "Clusters",
        "Dev",
        "Staging",
        "Environments",
        "HDP",
        "HDP",
        "HDP",
        "HDP",
        "Capacity",
        "Production",
        "Cluster",
        "loading",
        "data",
        "UNIX",
        "file",
        "system",
        "HDFS",
        "Sqoop",
        "Hive",
        "tables",
        "data",
        "hive",
        "queries",
        "map",
        "way",
        "balaramsekuboyinak2gmailcom",
        "Configuring",
        "Oozie",
        "Jobs",
        "Configuring",
        "Availability",
        "Name",
        "Node",
        "HDP",
        "Configuring",
        "Kerberos",
        "Authentication",
        "cluster",
        "cluster",
        "upgradation",
        "Hadoop",
        "HDP",
        "HDP",
        "Configuring",
        "queues",
        "capacity",
        "scheduler",
        "Created",
        "tables",
        "data",
        "queries",
        "Hive",
        "Monitored",
        "cluster",
        "Ambari",
        "optimize",
        "system",
        "job",
        "performance",
        "criteria",
        "Managed",
        "cluster",
        "performance",
        "tuning",
        "procedures",
        "installation",
        "Hadoop",
        "updates",
        "version",
        "upgrades",
        "data",
        "recoverability",
        "system",
        "application",
        "level",
        "backups",
        "Solr",
        "Hadoop",
        "cluster",
        "Snapshot",
        "backups",
        "HBase",
        "tables",
        "Cluster",
        "maintenance",
        "Monitoring",
        "Data",
        "nodes",
        "Troubleshooting",
        "Manage",
        "data",
        "backups",
        "Manage",
        "review",
        "log",
        "files",
        "Day",
        "day",
        "responsibilities",
        "developer",
        "issues",
        "deployments",
        "code",
        "environment",
        "environment",
        "access",
        "users",
        "solutions",
        "impact",
        "issues",
        "Addinginstallation",
        "components",
        "removal",
        "Ambari",
        "Collaborating",
        "application",
        "teams",
        "operating",
        "system",
        "Hadoop",
        "updates",
        "version",
        "upgrades",
        "workload",
        "job",
        "performance",
        "capacity",
        "planning",
        "system",
        "failures",
        "root",
        "causes",
        "course",
        "actions",
        "issues",
        "SLA",
        "Migration",
        "k2",
        "time",
        "errors",
        "Archiving",
        "Process",
        "Disaster",
        "Recovery",
        "Plan",
        "K2",
        "deployment",
        "packages",
        "K2",
        "Handling",
        "cloud",
        "machines",
        "applications",
        "Stored",
        "Procedures",
        "Daily",
        "Feeds",
        "Data",
        "creation",
        "Data",
        "Files",
        "Environment",
        "HDP",
        "2X",
        "CDH",
        "Apache",
        "Hadoop",
        "Hive",
        "Pig",
        "Flume",
        "Oozie",
        "Sqoop",
        "MS",
        "Visual",
        "Studio",
        "MS",
        "Net",
        "Framework",
        "C",
        "Angularjs",
        "K2",
        "Smart",
        "Objects",
        "SQL",
        "Server",
        "Oracle",
        "web",
        "logic",
        "Developer",
        "Procter",
        "Gamble",
        "Cincinnati",
        "OH",
        "November",
        "December",
        "Procter",
        "Gamble",
        "Co",
        "PG",
        "consumer",
        "goods",
        "company",
        "downtown",
        "Cincinnati",
        "Ohio",
        "United",
        "States",
        "PG",
        "company",
        "brands",
        "brands",
        "percent",
        "companys",
        "company",
        "factories",
        "locations",
        "United",
        "States",
        "demand",
        "products",
        "capacity",
        "Cincinnati",
        "facilities",
        "Procter",
        "Gamble",
        "number",
        "companies",
        "product",
        "line",
        "profits",
        "balaramsekuboyinak2gmailcom",
        "Procter",
        "Gambles",
        "Global",
        "Business",
        "Services",
        "Decision",
        "Cockpits",
        "data",
        "PG",
        "reporting",
        "decision",
        "making",
        "order",
        "manner",
        "PG",
        "place",
        "strategy",
        "Decision",
        "Cockpit",
        "DC",
        "Decision",
        "Cockpit",
        "adoption",
        "end",
        "users",
        "business",
        "reporting",
        "needs",
        "end",
        "users",
        "DC",
        "value",
        "Personalization",
        "Look",
        "employees",
        "services",
        "tools",
        "travel",
        "calendar",
        "learning",
        "performance",
        "resources",
        "Responsibilities",
        "Functional",
        "Integration",
        "Regression",
        "Smoke",
        "System",
        "testing",
        "defects",
        "Deployed",
        "InfoPath",
        "forms",
        "InfoPath",
        "forms",
        "data",
        "Monsanto",
        "End",
        "Client",
        "Test",
        "SQL",
        "Server",
        "database",
        "Developed",
        "Office",
        "SharePoint",
        "Server",
        "InfoPath",
        "Services",
        "interaction",
        "end",
        "users",
        "Web",
        "browser",
        "custom",
        "workflows",
        "InfoPath",
        "Design",
        "functionality",
        "Edit",
        "Delete",
        "items",
        "SharePoint",
        "list",
        "InfoPath",
        "form",
        "SharePoint",
        "Features",
        "InfoPath",
        "form",
        "SharePoint",
        "Solutions",
        "packages",
        "Deployment",
        "System",
        "Functional",
        "Requirement",
        "document",
        "Test",
        "Scenarios",
        "functionality",
        "testing",
        "application",
        "test",
        "cases",
        "Developed",
        "Portlets",
        "clients",
        "requirements",
        "security",
        "portlet",
        "availability",
        "regions",
        "users",
        "Build",
        "Reviews",
        "Peer",
        "Reviews",
        "build",
        "Worked",
        "SharePoint",
        "security",
        "quota",
        "sites",
        "tests",
        "specifications",
        "test",
        "cases",
        "testing",
        "Smoke",
        "test",
        "production",
        "Providing",
        "Test",
        "status",
        "updates",
        "test",
        "progress",
        "Team",
        "Foundation",
        "Server",
        "TFS",
        "versionchange",
        "management",
        "control",
        "Testing",
        "defects",
        "test",
        "environment",
        "code",
        "UAT",
        "environment",
        "Migration",
        "Documents",
        "issues",
        "production",
        "client",
        "team",
        "production",
        "Migration",
        "LLDs",
        "HLDs",
        "test",
        "case",
        "documents",
        "documents",
        "code",
        "review",
        "check",
        "list",
        "Migration",
        "check",
        "list",
        "releases",
        "team",
        "issues",
        "QA",
        "team",
        "code",
        "Environment",
        "Oracle",
        "web",
        "Logic",
        "SharePoint",
        "Vignette",
        "Developer",
        "Nokia",
        "Chennai",
        "Tamil",
        "Nadu",
        "June",
        "October",
        "company",
        "largescale",
        "telecommunications",
        "infrastructures",
        "technology",
        "development",
        "licensing",
        "Nokia",
        "contributor",
        "telephony",
        "industry",
        "development",
        "GSM",
        "LTE",
        "standards",
        "period",
        "vendor",
        "phones",
        "world",
        "Nokias",
        "dominance",
        "smartphone",
        "industry",
        "platform",
        "Nokia",
        "pact",
        "Microsoft",
        "Windows",
        "Phone",
        "platform",
        "smartphones",
        "balaramsekuboyinak2gmailcom",
        "purpose",
        "Project",
        "Web",
        "Sites",
        "countries",
        "phone",
        "accessory",
        "phone",
        "Creating",
        "Nokia",
        "maps",
        "reputation",
        "times",
        "Responsibilities",
        "content",
        "items",
        "Content",
        "Management",
        "System",
        "CMS",
        "contents",
        "Site",
        "ManagementSM",
        "Peer",
        "Reviews",
        "migration",
        "documents",
        "reference",
        "joiners",
        "issues",
        "releases",
        "team",
        "issues",
        "QA",
        "team",
        "defects",
        "UAT",
        "Environment",
        "Vignette",
        "Accenture",
        "Hyderabad",
        "Hyderabad",
        "Telangana",
        "Education",
        "Bachelor",
        "Technology",
        "Computer",
        "Science",
        "Engineering",
        "Computer",
        "Science",
        "Engineering",
        "PVP",
        "Nagarjuna",
        "Institute",
        "Technology",
        "Vijayawada",
        "Andhra",
        "Pradesh",
        "August",
        "Skills",
        "Oozie",
        "Sqoop",
        "Hbase",
        "Hadoop",
        "Map",
        "CertificationsLicenses",
        "Drivers",
        "License",
        "Additional",
        "Information",
        "TECHNICAL",
        "SKILLS",
        "Languages",
        "CNET",
        "VBNET",
        "JavaScript",
        "JQuery",
        "ASPNET",
        "Angularjs",
        "Silverlight",
        "XML",
        "HTML",
        "CSS",
        "Web",
        "Technologies",
        "SQL",
        "Server",
        "MS",
        "ACCESS",
        "Databases",
        "Visual",
        "Studio",
        "05Sharepoint",
        "Online",
        "SharePoint",
        "IDE",
        "3rd",
        "Party",
        "Toolkits",
        "K2",
        "Black",
        "Pearl",
        "Smart",
        "Forms",
        "Vignette",
        "ALUI",
        "Aqua",
        "Logic",
        "User",
        "Interface",
        "Reporting",
        "Tools",
        "SQL",
        "Server",
        "Reporting",
        "Services",
        "Version",
        "Control",
        "Team",
        "Foundation",
        "Server",
        "Distribution",
        "Hadoop",
        "Frameworks",
        "Hadoop",
        "Hortonworks",
        "HDP",
        "2X",
        "Cloudera",
        "Distributions",
        "Map",
        "Reduce",
        "HBase",
        "Hive12",
        "Sqoop",
        "Pig",
        "Hadoop",
        "Oozie",
        "Technologies"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T20:05:58.423466",
    "resume_data": "Hadoop Administrator Hadoop Administrator Hadoop Administrator Lead IT Corporation Franklin TN 10 years of experience in Information Technology with a strong background in application development and maintenancesupport and full project life cycle using Microsoft NET Technologies including Unit Testing Client Interaction and handling functional technical queries 4 years of experience in BIG DATA Hadoop Administration and Development Handson experience in designing and implementing solutions using Hadoop HDFS Map Reduce HBase Hive Pig Spark Oozie Tez Yarn Sqoop Solr Zookeeper Experience in Configuring Namenode High availability and Namenode Federation Experience in Disaster recovery and Backup activities Experience in Multinode setup of Hadoop cluster Experience in Performance tuning and benchmarking of Hadoop Cluster Experience in Monitoring maintenance and troubleshooting of Hadoop cluster Experience in Security integration of Hadoop Cluster Good knowledge on Kerberos Security Setting up and integrating Hadoop eco system tools HBase Hive Pig Sqoop etc Making Hadoop cluster ready for development team working on POCs Experience in deploying Hadoop 20YARN Enabling and managing various components in Hadoop Ecosystem like HDFS YARN Map Reduce Hive Pig Sqoop Oozie Sentry Spark and Zookeeper Experienced in configuring installing upgrading and managing Cloudera Apache Hortonworks Hadoop Distributions Familiar with writing Oozie workflows and Job Controllers for job automation Hive automation Hands on experience in analyzing log files for Hadoop and ecosystem services and finding root cause Hands on experience in Installing Configuring and managing the Hands on experience in analyzing log files for Hadoop and ecosystem services and finding root cause Hands on experience in Installing Configuring and managing the HCatalog Experience in importing and exporting the data using Sqoop from HDFS to Relational Database systems and viceversa Optimizing performance of HBaseHivePig jobs Experience with Spark Streaming Sql MLib GraphX and integrating Spark with HDFS and HBase Experience in tuning and debugging Spark application running Experience integration of Kafka with Spark for real time data processing Handsonexperience on ZKFC in managing and configuring the Name Node failure scenarios Experience on Commissioning Decommissioning Balancing and Managing Nodes and tuning server for optimal performance of the cluster Experience in importing and exporting the data using Sqoop from HDFS to Relational Database systems and viceversa Optimizing performance of HBaseHivePig jobs Experience in understanding the security requirements for Hadoop and integrating with Kerberos authentication infrastructure KDC server setup creating realm domain managing Good Experience in using SQL Server 20082005 and its tools like SQL Server Reporting Services SSRS SQL Server Integration Services SSIS Well Proficient in writing Views Stored Procedures Functions MS SQL Server and Oracle Balaram Sekuboyina bshadoopadmngmailcom 5137244666 Work Experience Hadoop Administrator Lead IT Corporation December 2017 to Present Nissan was the sixth largest automaker in the world Nissan leveraged the Austin patents to further develop their own modern engine designs Nissan management realized their Datsun small car line would fill an unmet need in markets Nissan tried to convert the Greek plant into one manufacturing cars for all European countries The RenaultNissan Alliance has evolved over years the alliance itself is incorporated as the RenaultNissan BV Group For many years Nissan used a red wordmark for the company and car badges for the Nissan and Infiniti brands Nissan maintains all the data related to Cars sales features of the cars faults of the cars when occurred using the telematics data and warranty of each vehicle depends of the type of cars All the data will is collected will be stored using using Hadoop environment by using HortonworksThis connects which various sources like Informatica Tableau to the get data maintained in Hadoop Cluster This uses a Hortonworks Hadoop Cluster Responsibilities Upgraded Hortonworks HADOOP Cluster from 25 to 265 for development staging pre prod and Prod with most of the available services in the Hadoop Ecosystem Installed HDF Cluster in the all development staging preprod and Prod with most of the available services in the Hadoop Ecosystem Installed System Security Services Daemon on all the environments to sync the Active Directory directly to the cluster and remove the local users Administered Cluster maintenance commissioning and decommissioning Data nodes Cluster Monitoring Troubleshooting Performed Addingremoving new nodes to an existing Hadoop cluster Defining the cron jobs to run the automated jobs at desired intervals Creating the policies in Ranger to make the cluster robust without any issues Installed different softwares likes subversionVeracryptGit for better performance of the cluster Proposing tools like Pycharm which helps the developers for better debugging of the code bshadoopadmngmailcom 5137244666 Making the cluster available for various data sources for easy data access and make sure all the connections are reachable Scheduling the batch process jobs in the cluster and distribute the cluster among all jobs Creating alerts on Ambari based on the criticality Creating the coding standards for the developers reviewing the codes and deploying the codes in different enviroments Overseeing the smooth execution of the code into the cluster on the three environemnts of minor and major enhancements Environment HDP 265 Ambari 262 HADOOP HDFS Zookeeper Map Reduce YARN Scala Spark Python HBASE Hive SQOOP OOZIE Linux CENTOS UBUNTU Red Hat Hadoop Administrator CrBard Houston TX December 2016 to August 2017 CRBard is health care manufacturing equipments for patients and healthcare professionals in wellness and prevention early diagnosis treatment and postcare management In fact throughout our history Bard has lead the industry in groundbreaking devices and therapies that continuously seek to set the new standard for excellence and quality A leading multinational developer manufacturer and marketer of innovative lifeenhancing medical technologies in the fields of vascular urology oncology and surgical specialties Developing innovative medical devices for more than 100 years that meet the needs of clinicians and their patients with more than 75 locations worldwide Pioneered the development of single patientuse medical products for hospital procedures today it is dedicated to pursuing technological innovations that offer superior clinical benefits while helping to reduce overall costs Responsibilities Installed and configured Hortonworks HADOOP Cluster from scratch for development staging preprod and Prod with most of the available services in the Hadoop Ecosystem Administered Cluster maintenance commissioning and decommissioning Data nodes Cluster Monitoring Troubleshooting Performed Addingremoving new nodes to an existing Hadoop cluster Implemented Backup configurations and Recoveries from a Name Node failure Monitored systems and services architecture design and implementation of Hadoop deployment configuration management backup and disaster recovery systems and procedures Configured various property files like coresitexml hdfssitexml mapredsitexml based upon the job requirement Performed Importing and exporting data into HDFS using SQOOP Installed and configured HDFS Zookeeper Map Reduce Yarn HBASE Hive SQOOP and OOZIE Integrated Hive and HBASE to perform analysis on data balaramsekuboyinak2gmailcom 5137244666 Managed and reviewed Hadoop Log files as a part of administration for troubleshooting purposes Communicated and escalated issues appropriately Applied standard Back up policies to make sure the high availability of cluster Involved in analyzing system failures identifying root causes and recommended course of actions Documented the systems processes and procedures for future references Worked with systems engineering team to plan and deploy new Hadoop environments and expand existing Hadoop clusters Support preproduction and production support teams in the analysis of critical services and assists with maintenance operations Improve system performance by working with the development team to analyze identify and resolve issues quickly Automate administration tasks with shell scripting and Job Scheduling using CRON Performed many Proof of Concepts to the client and suggested different tools available in the Hadoop Ecosystem Environment HDP 26 Ambari 25 HADOOP HDFS Zookeeper Map Reduce YARN Scala Spark Python HBASE Hive SQOOP OOZIE Linux CENTOS UBUNTU Red Hat Application Software DeveloperHadoop Administrator Accenture Hyderabad Houston TX January 2011 to November 2016 Shell Oil Company is the United Statesbased subsidiary of Royal Dutch Shell a multinational oil major of AngloDutch origins which is amongst the largest oil companies in the world Shell products include oils fuels and card services as well as exploration production and refining of petroleum products Shell Oil operates as a subsidiary of Royal DutchShell Group the second largest oil company in the world In 1999 Shell Oil and its USbased counterparts secured 22 percent of the Groups income Shell is a global group of energy and petrochemicals companies with around 90000 employees in more than 80 countries and territories Shell innovative approach ensures to help tackle the challenge of new energy future Shell energy business is spread across 3 areas Upstream Midstream and Downstream Shell has requirements to automate many standard manual processes to automate using workflow technology This enables lot of work spread among the teams to get chance to work on multiple workflows and deliverables Shell is recommending to develop workflows in SharePoint K2 SharePoint team handled the following workflows Responsibilities Analyze customer orders set delivery priorities and make schedule adjustments to meet timely delivery goals Worked on Intranet site and prepared plan for migration from SharePoint 2010 to 2013 and SharePoint online365 Installed and configured Hadoop Map Reduce HDFS Developed multiple Map Reduce jobs in java for data cleaning and preprocessing Worked on Installing and configuring the HDP Hortonworks 2x Clusters in Dev and Staging Environments Worked on installing and configuring HDP 23 HDP 24 HDP 25 and HDP 26 Worked on Capacity planning for the Production Cluster Involved in loading data from UNIX file system to HDFS using Sqoop Involved in creating Hive tables loading the data and writing hive queries which will run internally in map reduce way balaramsekuboyinak2gmailcom 5137244666 Worked on Configuring Oozie Jobs Worked on Configuring High Availability for Name Node in HDP 2x Worked on Configuring Kerberos Authentication in the cluster Worked on cluster upgradation in Hadoop from HDP 21 to HDP 23 Worked on Configuring queues in capacity scheduler Created tables loaded data and wrote queries in Hive Monitored cluster using Ambari and optimize system based on job performance and criteria Managed cluster through performance tuning and enhancements Formulated procedures for installation of Hadoop patches updates and version upgrades Ensured data recoverability by implementing system and application level backups Worked on installing and configuring Solr 521 in Hadoop cluster Worked on taking Snapshot backups for HBase tables Responsible for Cluster maintenance Monitoring commissioning and decommissioning Data nodes Troubleshooting Manage and review data backups Manage review log files Day to day responsibilities includes solving developer issues deployments moving code from one environment to other environment providing access to new users and providing instant solutions to reduce the impact and documenting the same and preventing future issues Addinginstallation of new components and removal of them through Ambari Collaborating with application teams to install operating system and Hadoop updates patches version upgrades Monitored workload job performance and capacity planning Involved in Analyzing system failures identifying root causes and recommended course of actions Responsible for resolving the issues with the specified SLA Involved in Migration of k2 from 45 to 465 Reduced the time out errors in K2 Involved in the K2 Archiving Process Extensively worked on Disaster Recovery Plan in K2 Creating the deployment packages in K2 Handling different cloud machines to maintain K2 the applications Created Stored Procedures to support Daily scheduled Feeds by providing necessary Data for the creation of Data Files Environment HDP 2X CDH Apache Hadoop Hive Pig Flume Oozie Sqoop MS Visual Studio 2012 MS Net Framework 40 C Angularjs K2 workflows Smart Objects SQL Server Oracle web logic Developer Procter and Gamble Cincinnati OH November 2009 to December 2010 Procter Gamble Co also known as PG is an American multinational consumer goods company headquartered in downtown Cincinnati Ohio United States PG announced it was streamlining the company dropping around 100 brands and concentrating on the remaining 80 brands which produced 95 percent of the companys profits The company began to build factories in other locations in the United States because the demand for products had outgrown the capacity of the Cincinnati facilities Procter Gamble acquired a number of other companies that diversified its product line and significantly increased profits balaramsekuboyinak2gmailcom 5137244666 Procter and Gambles Global Business Services Decision Cockpits is responsible for integrating data from across PG to provide analytic reporting to enable decision making In order to do this in a quick efficient manner PG has put in place a strategy to create Decision Cockpit DC Decision Cockpit 30 35 is to increase adoption among end users and meet the business reporting needs of end users DC 35 will add additional value by providing Personalization Look Feel to employees and services tools such as travel calendar learning and performance resources Responsibilities Involved in Functional Integration Regression Smoke and System testing and logging defects Developed and Deployed InfoPath forms which involve creating InfoPath forms that retrieve data directly from the Monsanto End Client Test SQL Server database Developed Office SharePoint Server 2007 InfoPath Services to enable the interaction of the end users through a Web browser Configured the custom workflows to InfoPath forms Design and Developed the functionality to Add Edit and Delete the existing items in the SharePoint list through a InfoPath form Created SharePoint Features for enabling InfoPath form And Used the SharePoint Solutions for creating the packages and Deployment Analyzed the System Functional Requirement document and written Test Scenarios for the functionality testing of the application Ensured that all the test cases are updated Developed Portlets based on the clients requirements and assigning the security to the portlet based on the availability of the regions and users Involved in doing the Build Reviews and Peer Reviews once the build is completed Worked on SharePoint for security quota to the sites Reviewed tests specifications test cases and performed manual testing Involved in doing Smoke test once the production is completed Providing Test status updates and daily test progress reports Used Team Foundation Server TFS 20052008 as versionchange management control Testing of UAT defects in test environment before the new code gets moved to UAT environment Preparing the Migration Documents with no issues from the production and Involving with the client team during the production Migration Created LLDs HLDs and test case documents Created documents like code review check list Migration check list Involved in all the existing releases and helped the team in resolving critical issues Worked with the QA team to test the code Environment Oracle web Logic SharePoint Vignette Developer Nokia Chennai Tamil Nadu June 2008 to October 2009 The company currently focuses on largescale telecommunications infrastructures and technology development and licensing Nokia is also a major contributor to the mobile telephony industry having assisted in development of the GSM and LTE standards and was for a period the largest vendor of mobile phones in the world Nokias dominance also extended into the smartphone industry through its Symbian platform Nokia eventually entered into a pact with Microsoft in 2011 to exclusively use its Windows Phone platform on future smartphones balaramsekuboyinak2gmailcom 5137244666 The purpose of this Project is to create Web Sites across 58 countries for every phone they release and for every accessory which are compatible for that phone Creating Nokia maps which has got the highest reputation during the recent times Responsibilities Created the content items in the Content Management System CMS Arranged the contents in the Site ManagementSM Involved in doing the Peer Reviews once the migration is completed Created documents which helped for quick reference for the new joiners Involved in resolving the critical UAT issues Involved in all the existing releases and helped the team in resolving critical issues Worked with the QA team to reduce defects before moving to UAT Environment Vignette Accenture Hyderabad Hyderabad Telangana 2005 to 2008 Education Bachelor of Technology in Computer Science Engineering in Computer Science Engineering PVP Nagarjuna Institute of Technology Vijayawada Andhra Pradesh August 2007 Skills Oozie Sqoop Hbase Hadoop Map reduce CertificationsLicenses Drivers License Additional Information TECHNICAL SKILLS Languages CNET VBNET JavaScript JQuery ASPNET Angularjs Silverlight XML HTML CSS Web Technologies SQL Server 201220082005 MS ACCESS Databases Visual Studio 201020082005Sharepoint Online SharePoint IDE 20132010MOSS 2007 3rd Party Toolkits K2 Black Pearl Smart Forms Vignette ALUI Aqua Logic User Interface Reporting Tools SQL Server Reporting Services Version Control Team Foundation Server Distribution Hadoop Frameworks Hadoop Hortonworks HDP 2X Cloudera Distributions Map Reduce HBase 11 Hive12 Sqoop 146 Pig 016 Hadoop Oozie 420 Technologies",
    "unique_id": "62830884-ee31-47ab-bc36-94eb9a736000"
}