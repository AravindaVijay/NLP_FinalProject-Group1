{
    "clean_data": "Hadoop Developer Hadoop span lDeveloperspan Hadoop Developer Edison NJ Overall 5 years 10 months of experience in software development life cycle like Analysis Design Implementation Testing and partial support of Core JAVA 7 Big Data eco system and Big Data Analytics Worked as anIT Analyst with Tata Consultancy Services Pune and Hyderabad Over 2 years of experience in web application development using JAVAJ2EE technologies Worked entirely in Banking domain Exposure of endtoend development of software development from system study designing coding testing debugging documentation and implementation Acquires good understanding of JIRA and maintaining JIRA dashboards Experience on working with Amazon Web Services like EC2 Linux operating system Expert level of skills and experience in internet and GUI technologies Web based application development such as JAVA SERVLET JSP JDBC and XML Expertise in Client Side Designing and Validations using HTML5 CSS3 Java Script JSP Expert in using Java IDEs like Eclipse and IntelliJ Used Maven for building projects Over 3 years of experience in Hadoop architecture and various components such as HDFS Namenode Datanode and MapReduce Job Tracker Task Tracker and programming paradigm Experience in using Hadoop Technologies such as HDFS SQOOP HIVE Impala Flume Spark Strong experience in writing Map Reduce jobs in Hive Experience with Databases like SQL MySQL and MongoDB Extensively worked on importing and exporting data from different systems to HDFS using SQOOP Extensively works on using Hadoop ecosystem components for storage and processing data exported data into Tableau using Live connection Experienced in working with Hadoop eco system using HiveQL on different formats like Text file CSV file Good experience on creating databases tables and views in HiveQL Implemented Hadoop stack and different bigdata analytic tools migration from different databases ie SQL MySQL to Hadoop Load and transform large sets of structured semistructured data using Hadoop ecosystem components Worked on building hive and mapreduce scripts Having experience on using OOZIE to define and schedule the jobs Good Knowledge on Zookeeper Sentiment Analysis Having experience on Managing HDFS file system Having knowledge on Python MongoDB Experience and knowledge of real time data analytics using Spark Streaming and Flume Implementing Spark using Python and Spark SQL for faster testing and processing of data responsible to manage data from different sources Good experience on all flavours of HadoopCloudera Hortonworks MapR etc Excellent communication skills interpersonal skills problem solving skills a very good team player along with a cando attitude and ability to effectively communicate with all levels of the organization such as technical management and customers Ability to easily adapt and learn any new technology or software Authorized to work in the US for any employer Work Experience Hadoop Developer Tata Consultancy Services Pune May 2014 to April 2017 Description Creatingdashboard for data visualization based on everyday critical and as well as failed transactions in order to monitor their status rather than checking each and every transaction status using middleware applications status manually GUI provides level of criticality for a particular transaction and charts for number failures It allows the monitoring team take appropriate action within SLA Service level agreement Responsibilities Involved in analyzing the system and business Involved in database connection by using SQOOP Involved in importing data from MySQL to HDFS using SQOOPInvolved in migrating tables from RDBMS into Hive tables using SQOOP Written Hive UDFs to sort Structure fields and return complex data type Writing Hive queries to load and process data in Hadoop File System Creating Hive tables loading with data and writing hive queries which will run internally in map reduce way Involved in loading data from UNIX file system to HDFS Installed and configured Hive and also written Hive UDFs Worked with NoSQL databases like Hbase inmaking Hbase tables to load expansive arrangements of semi structured data Zookeeper Sentiment Analysis on reviews of the products on the clients website Involved in transferring files from OLTP server to Hadoop file system Involved in writing queries with HiveQL and Pig Export and Import data into HDFS HBase and Hive using Sqoop Load and transform large sets of structured and semi structured data Loading data into Hive partitioned tables Create reports for the BI team using Sqoop to export data into HDFS and Hive Importing and Exporting Data from Oracle to HiveQL Importing and Exporting Data from HiveQL to HDFS Process and analyze the data from Hive tables using HiveQL EnvironmentAmazon EC2 HartonWorksAmbari Hadoop HDFS Map Reduce Hive Hbase Sqoop Flume CentOS Linux Spark Core JAVA Developer Tata Consultancy Services Pune October 2011 to April 2014 Worked as Hadoop Developer atTata Consultancy Services Puneand Hyderabad from May2014April2017 Project Responsibilities Java Developer Tata Consultancy Services Pune October 2011 to April 2014 Description Creating web based GUI dashboard for Autosys jobs in order to monitor their status rather than checking each and every jobs status manually GUI helps in tracking filewatcher and all other jobs It allows the monitoring team to contact upstreamdownstream teams in time and to run the jobs complete successfully Responsibilities Utilizing Java Java EE JSP Apache Server and SQL Created roles to the customers based on their designation and teams Responsible for creating front end applications user interactive UI web pages using web technologies like HTML5 CSS3 JavaScript JUnit test cases for the classes post development Using SVN delivered and pushed code to Integration and QA environments on time for BA and QA signoffs Extreme attention to accuracy detail presentation and timeliness of delivery Used PCF for monitoring application stability and continuous integration Object storage service Amazon S3 is used to store and retrieve DB information Debugging production issues root cause analysis and fixing Involved in setting up Maven configuration and helping Continuous Integration CI Issues Extensively used SVN as the version controlling Tool for checkins and checkouts Involved in debugging the defects code review and analysis of Performance issues Involved in Jenkins configuration Environment Java Java EE JSP Apache Server SQL Windows7 SVN GIT Jenkin Builds Education Masters in Computer Science and Engineering New Jersey Institute of Technology Newark NJ September 2007 to May 2011 Skills Hdfs Mapreduce Sqoop Hbase Flume Hadoop Mongodb Hadoop Hbase Hive Javascript Mapreduce Python Scripting Database Mysql Sql Apache Linux Unix Additional Information Technical Expertise Hadoop Framework HDFS MapReduce Hive Hbase Sqoop Flume Database Technology HiveQL SQL MySQL MongoDB Operating Systems Windows 10 Linux CentOS 7 ApplicationWeb servers Apache Tomcat ScriptingUnix Shell Javascript LanguagesCore Java Python",
    "entities": [
        "Flume Implementing Spark",
        "OOZIE",
        "Python",
        "JAVA SERVLET JSP JDBC",
        "EnvironmentAmazon",
        "Writing Hive",
        "CSV",
        "Hive Importing",
        "PCF",
        "GUI",
        "HiveQL",
        "US",
        "JAVA",
        "Acquires",
        "Sqoop",
        "QA",
        "Maven",
        "Big Data Analytics Worked",
        "BI",
        "HDFS",
        "UNIX",
        "Responsibilities Involved",
        "Hadoop Developer Hadoop",
        "HartonWorksAmbari Hadoop",
        "Project Responsibilities Java Developer Tata Consultancy Services",
        "BA",
        "Client",
        "Pig Export and Import",
        "Oracle",
        "ApplicationWeb",
        "SQL Created",
        "Hadoop File System Creating Hive",
        "HiveQL Importing and Exporting Data",
        "Analysis Design Implementation Testing",
        "HTML5",
        "SVN",
        "Unix Additional Information Technical Expertise Hadoop Framework HDFS MapReduce Hive Hbase",
        "HiveQL Implemented Hadoop",
        "JSP",
        "Continuous Integration CI",
        "SQL",
        "Hadoop Load",
        "Hadoop",
        "OLTP",
        "SQOOP Involved",
        "Amazon Web Services",
        "Tata Consultancy Services Pune and Hyderabad",
        "Spark Streaming",
        "NoSQL",
        "Tableau",
        "Hadoop Developer atTata Consultancy Services Puneand Hyderabad",
        "Autosys",
        "Work Experience Hadoop Developer Tata Consultancy Services",
        "Amazon S3",
        "Hive",
        "SQOOP",
        "SLA Service",
        "Hadoop Technologies",
        "JAVAJ2EE",
        "Ability",
        "Core JAVA 7 Big Data"
    ],
    "experience": "Experience on working with Amazon Web Services like EC2 Linux operating system Expert level of skills and experience in internet and GUI technologies Web based application development such as JAVA SERVLET JSP JDBC and XML Expertise in Client Side Designing and Validations using HTML5 CSS3 Java Script JSP Expert in using Java IDEs like Eclipse and IntelliJ Used Maven for building projects Over 3 years of experience in Hadoop architecture and various components such as HDFS Namenode Datanode and MapReduce Job Tracker Task Tracker and programming paradigm Experience in using Hadoop Technologies such as HDFS SQOOP HIVE Impala Flume Spark Strong experience in writing Map Reduce jobs in Hive Experience with Databases like SQL MySQL and MongoDB Extensively worked on importing and exporting data from different systems to HDFS using SQOOP Extensively works on using Hadoop ecosystem components for storage and processing data exported data into Tableau using Live connection Experienced in working with Hadoop eco system using HiveQL on different formats like Text file CSV file Good experience on creating databases tables and views in HiveQL Implemented Hadoop stack and different bigdata analytic tools migration from different databases ie SQL MySQL to Hadoop Load and transform large sets of structured semistructured data using Hadoop ecosystem components Worked on building hive and mapreduce scripts Having experience on using OOZIE to define and schedule the jobs Good Knowledge on Zookeeper Sentiment Analysis Having experience on Managing HDFS file system Having knowledge on Python MongoDB Experience and knowledge of real time data analytics using Spark Streaming and Flume Implementing Spark using Python and Spark SQL for faster testing and processing of data responsible to manage data from different sources Good experience on all flavours of HadoopCloudera Hortonworks MapR etc Excellent communication skills interpersonal skills problem solving skills a very good team player along with a cando attitude and ability to effectively communicate with all levels of the organization such as technical management and customers Ability to easily adapt and learn any new technology or software Authorized to work in the US for any employer Work Experience Hadoop Developer Tata Consultancy Services Pune May 2014 to April 2017 Description Creatingdashboard for data visualization based on everyday critical and as well as failed transactions in order to monitor their status rather than checking each and every transaction status using middleware applications status manually GUI provides level of criticality for a particular transaction and charts for number failures It allows the monitoring team take appropriate action within SLA Service level agreement Responsibilities Involved in analyzing the system and business Involved in database connection by using SQOOP Involved in importing data from MySQL to HDFS using SQOOPInvolved in migrating tables from RDBMS into Hive tables using SQOOP Written Hive UDFs to sort Structure fields and return complex data type Writing Hive queries to load and process data in Hadoop File System Creating Hive tables loading with data and writing hive queries which will run internally in map reduce way Involved in loading data from UNIX file system to HDFS Installed and configured Hive and also written Hive UDFs Worked with NoSQL databases like Hbase inmaking Hbase tables to load expansive arrangements of semi structured data Zookeeper Sentiment Analysis on reviews of the products on the clients website Involved in transferring files from OLTP server to Hadoop file system Involved in writing queries with HiveQL and Pig Export and Import data into HDFS HBase and Hive using Sqoop Load and transform large sets of structured and semi structured data Loading data into Hive partitioned tables Create reports for the BI team using Sqoop to export data into HDFS and Hive Importing and Exporting Data from Oracle to HiveQL Importing and Exporting Data from HiveQL to HDFS Process and analyze the data from Hive tables using HiveQL EnvironmentAmazon EC2 HartonWorksAmbari Hadoop HDFS Map Reduce Hive Hbase Sqoop Flume CentOS Linux Spark Core JAVA Developer Tata Consultancy Services Pune October 2011 to April 2014 Worked as Hadoop Developer atTata Consultancy Services Puneand Hyderabad from May2014April2017 Project Responsibilities Java Developer Tata Consultancy Services Pune October 2011 to April 2014 Description Creating web based GUI dashboard for Autosys jobs in order to monitor their status rather than checking each and every jobs status manually GUI helps in tracking filewatcher and all other jobs It allows the monitoring team to contact upstreamdownstream teams in time and to run the jobs complete successfully Responsibilities Utilizing Java Java EE JSP Apache Server and SQL Created roles to the customers based on their designation and teams Responsible for creating front end applications user interactive UI web pages using web technologies like HTML5 CSS3 JavaScript JUnit test cases for the classes post development Using SVN delivered and pushed code to Integration and QA environments on time for BA and QA signoffs Extreme attention to accuracy detail presentation and timeliness of delivery Used PCF for monitoring application stability and continuous integration Object storage service Amazon S3 is used to store and retrieve DB information Debugging production issues root cause analysis and fixing Involved in setting up Maven configuration and helping Continuous Integration CI Issues Extensively used SVN as the version controlling Tool for checkins and checkouts Involved in debugging the defects code review and analysis of Performance issues Involved in Jenkins configuration Environment Java Java EE JSP Apache Server SQL Windows7 SVN GIT Jenkin Builds Education Masters in Computer Science and Engineering New Jersey Institute of Technology Newark NJ September 2007 to May 2011 Skills Hdfs Mapreduce Sqoop Hbase Flume Hadoop Mongodb Hadoop Hbase Hive Javascript Mapreduce Python Scripting Database Mysql Sql Apache Linux Unix Additional Information Technical Expertise Hadoop Framework HDFS MapReduce Hive Hbase Sqoop Flume Database Technology HiveQL SQL MySQL MongoDB Operating Systems Windows 10 Linux CentOS 7 ApplicationWeb servers Apache Tomcat ScriptingUnix Shell Javascript LanguagesCore Java Python",
    "extracted_keywords": [
        "Hadoop",
        "Developer",
        "Hadoop",
        "span",
        "lDeveloperspan",
        "Hadoop",
        "Developer",
        "Edison",
        "NJ",
        "Overall",
        "years",
        "months",
        "experience",
        "software",
        "development",
        "life",
        "cycle",
        "Analysis",
        "Design",
        "Implementation",
        "Testing",
        "support",
        "Core",
        "JAVA",
        "Big",
        "Data",
        "eco",
        "system",
        "Big",
        "Data",
        "Analytics",
        "anIT",
        "Analyst",
        "Tata",
        "Consultancy",
        "Services",
        "Pune",
        "Hyderabad",
        "years",
        "experience",
        "web",
        "application",
        "development",
        "JAVAJ2EE",
        "technologies",
        "Banking",
        "domain",
        "Exposure",
        "endtoend",
        "development",
        "software",
        "development",
        "system",
        "study",
        "testing",
        "documentation",
        "implementation",
        "Acquires",
        "understanding",
        "JIRA",
        "JIRA",
        "dashboards",
        "Experience",
        "Amazon",
        "Web",
        "Services",
        "EC2",
        "Linux",
        "operating",
        "system",
        "Expert",
        "level",
        "skills",
        "experience",
        "internet",
        "GUI",
        "technologies",
        "Web",
        "application",
        "development",
        "JAVA",
        "JSP",
        "JDBC",
        "XML",
        "Expertise",
        "Client",
        "Side",
        "Designing",
        "Validations",
        "HTML5",
        "CSS3",
        "Java",
        "Script",
        "JSP",
        "Expert",
        "Java",
        "IDEs",
        "Eclipse",
        "IntelliJ",
        "Maven",
        "building",
        "projects",
        "years",
        "experience",
        "Hadoop",
        "architecture",
        "components",
        "HDFS",
        "Namenode",
        "Datanode",
        "MapReduce",
        "Job",
        "Tracker",
        "Task",
        "Tracker",
        "programming",
        "paradigm",
        "Experience",
        "Hadoop",
        "Technologies",
        "HDFS",
        "SQOOP",
        "HIVE",
        "Impala",
        "Flume",
        "Spark",
        "Strong",
        "experience",
        "Map",
        "Reduce",
        "jobs",
        "Hive",
        "Experience",
        "Databases",
        "SQL",
        "MySQL",
        "MongoDB",
        "data",
        "systems",
        "HDFS",
        "SQOOP",
        "Hadoop",
        "ecosystem",
        "components",
        "storage",
        "processing",
        "data",
        "data",
        "Tableau",
        "connection",
        "Hadoop",
        "eco",
        "system",
        "HiveQL",
        "formats",
        "Text",
        "file",
        "CSV",
        "file",
        "experience",
        "databases",
        "tables",
        "views",
        "HiveQL",
        "Implemented",
        "Hadoop",
        "stack",
        "bigdata",
        "tools",
        "migration",
        "databases",
        "SQL",
        "MySQL",
        "Hadoop",
        "Load",
        "sets",
        "data",
        "Hadoop",
        "ecosystem",
        "components",
        "hive",
        "scripts",
        "experience",
        "OOZIE",
        "jobs",
        "Good",
        "Knowledge",
        "Zookeeper",
        "Sentiment",
        "Analysis",
        "experience",
        "HDFS",
        "file",
        "system",
        "knowledge",
        "Python",
        "MongoDB",
        "Experience",
        "knowledge",
        "time",
        "data",
        "analytics",
        "Spark",
        "Streaming",
        "Flume",
        "Implementing",
        "Spark",
        "Python",
        "Spark",
        "SQL",
        "testing",
        "processing",
        "data",
        "data",
        "sources",
        "experience",
        "flavours",
        "HadoopCloudera",
        "Hortonworks",
        "MapR",
        "communication",
        "skills",
        "problem",
        "skills",
        "team",
        "player",
        "cando",
        "attitude",
        "ability",
        "levels",
        "organization",
        "management",
        "Ability",
        "technology",
        "software",
        "US",
        "employer",
        "Work",
        "Experience",
        "Hadoop",
        "Developer",
        "Tata",
        "Consultancy",
        "Services",
        "Pune",
        "May",
        "April",
        "Description",
        "Creatingdashboard",
        "data",
        "visualization",
        "transactions",
        "order",
        "status",
        "transaction",
        "status",
        "middleware",
        "applications",
        "status",
        "GUI",
        "level",
        "criticality",
        "transaction",
        "charts",
        "number",
        "failures",
        "monitoring",
        "team",
        "action",
        "SLA",
        "Service",
        "level",
        "agreement",
        "Responsibilities",
        "system",
        "business",
        "database",
        "connection",
        "SQOOP",
        "data",
        "MySQL",
        "HDFS",
        "tables",
        "RDBMS",
        "Hive",
        "tables",
        "SQOOP",
        "Written",
        "Hive",
        "UDFs",
        "Structure",
        "fields",
        "data",
        "type",
        "Writing",
        "Hive",
        "process",
        "data",
        "Hadoop",
        "File",
        "System",
        "Hive",
        "tables",
        "data",
        "hive",
        "queries",
        "map",
        "way",
        "loading",
        "data",
        "UNIX",
        "file",
        "system",
        "HDFS",
        "Installed",
        "Hive",
        "Hive",
        "UDFs",
        "databases",
        "Hbase",
        "Hbase",
        "tables",
        "arrangements",
        "data",
        "Zookeeper",
        "Sentiment",
        "Analysis",
        "reviews",
        "products",
        "clients",
        "website",
        "files",
        "OLTP",
        "server",
        "Hadoop",
        "file",
        "system",
        "queries",
        "HiveQL",
        "Pig",
        "Export",
        "Import",
        "data",
        "HDFS",
        "HBase",
        "Hive",
        "Sqoop",
        "Load",
        "sets",
        "data",
        "Loading",
        "data",
        "Hive",
        "tables",
        "reports",
        "BI",
        "team",
        "Sqoop",
        "data",
        "HDFS",
        "Hive",
        "Importing",
        "Exporting",
        "Data",
        "Oracle",
        "HiveQL",
        "Importing",
        "Exporting",
        "Data",
        "HiveQL",
        "HDFS",
        "Process",
        "data",
        "Hive",
        "tables",
        "HiveQL",
        "EnvironmentAmazon",
        "EC2",
        "HartonWorksAmbari",
        "Hadoop",
        "HDFS",
        "Map",
        "Reduce",
        "Hive",
        "Hbase",
        "Sqoop",
        "Flume",
        "CentOS",
        "Linux",
        "Spark",
        "Core",
        "JAVA",
        "Developer",
        "Tata",
        "Consultancy",
        "Services",
        "Pune",
        "October",
        "April",
        "Hadoop",
        "Developer",
        "atTata",
        "Consultancy",
        "Services",
        "Puneand",
        "Hyderabad",
        "May2014April2017",
        "Project",
        "Responsibilities",
        "Java",
        "Developer",
        "Tata",
        "Consultancy",
        "Services",
        "Pune",
        "October",
        "April",
        "Description",
        "Creating",
        "web",
        "GUI",
        "dashboard",
        "Autosys",
        "jobs",
        "order",
        "status",
        "jobs",
        "status",
        "GUI",
        "filewatcher",
        "jobs",
        "monitoring",
        "team",
        "teams",
        "time",
        "jobs",
        "Responsibilities",
        "Java",
        "Java",
        "EE",
        "JSP",
        "Apache",
        "Server",
        "SQL",
        "roles",
        "customers",
        "designation",
        "teams",
        "end",
        "applications",
        "user",
        "UI",
        "web",
        "pages",
        "web",
        "technologies",
        "HTML5",
        "CSS3",
        "JavaScript",
        "JUnit",
        "test",
        "cases",
        "classes",
        "development",
        "SVN",
        "code",
        "Integration",
        "QA",
        "environments",
        "time",
        "BA",
        "QA",
        "signoffs",
        "Extreme",
        "attention",
        "accuracy",
        "detail",
        "presentation",
        "timeliness",
        "delivery",
        "PCF",
        "application",
        "stability",
        "integration",
        "Object",
        "storage",
        "service",
        "Amazon",
        "S3",
        "DB",
        "information",
        "production",
        "issues",
        "root",
        "analysis",
        "Maven",
        "configuration",
        "Continuous",
        "Integration",
        "CI",
        "Issues",
        "SVN",
        "version",
        "Tool",
        "checkins",
        "checkouts",
        "defects",
        "code",
        "review",
        "analysis",
        "Performance",
        "issues",
        "Jenkins",
        "configuration",
        "Environment",
        "Java",
        "Java",
        "EE",
        "JSP",
        "Apache",
        "Server",
        "SQL",
        "Windows7",
        "SVN",
        "GIT",
        "Jenkin",
        "Builds",
        "Education",
        "Masters",
        "Computer",
        "Science",
        "Engineering",
        "New",
        "Jersey",
        "Institute",
        "Technology",
        "Newark",
        "NJ",
        "September",
        "May",
        "Skills",
        "Hdfs",
        "Mapreduce",
        "Sqoop",
        "Hbase",
        "Flume",
        "Hadoop",
        "Mongodb",
        "Hadoop",
        "Hbase",
        "Hive",
        "Javascript",
        "Mapreduce",
        "Python",
        "Scripting",
        "Database",
        "Mysql",
        "Sql",
        "Apache",
        "Linux",
        "Unix",
        "Additional",
        "Information",
        "Technical",
        "Expertise",
        "Hadoop",
        "Framework",
        "HDFS",
        "MapReduce",
        "Hive",
        "Hbase",
        "Sqoop",
        "Flume",
        "Database",
        "Technology",
        "HiveQL",
        "SQL",
        "MySQL",
        "MongoDB",
        "Operating",
        "Systems",
        "Windows",
        "Linux",
        "CentOS",
        "ApplicationWeb",
        "servers",
        "Apache",
        "Tomcat",
        "ScriptingUnix",
        "Shell",
        "Javascript",
        "LanguagesCore",
        "Java",
        "Python"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T19:44:07.531972",
    "resume_data": "Hadoop Developer Hadoop span lDeveloperspan Hadoop Developer Edison NJ Overall 5 years 10 months of experience in software development life cycle like Analysis Design Implementation Testing and partial support of Core JAVA 7 Big Data eco system and Big Data Analytics Worked as anIT Analyst with Tata Consultancy Services Pune and Hyderabad Over 2 years of experience in web application development using JAVAJ2EE technologies Worked entirely in Banking domain Exposure of endtoend development of software development from system study designing coding testing debugging documentation and implementation Acquires good understanding of JIRA and maintaining JIRA dashboards Experience on working with Amazon Web Services like EC2 Linux operating system Expert level of skills and experience in internet and GUI technologies Web based application development such as JAVA SERVLET JSP JDBC and XML Expertise in Client Side Designing and Validations using HTML5 CSS3 Java Script JSP Expert in using Java IDEs like Eclipse and IntelliJ Used Maven for building projects Over 3 years of experience in Hadoop architecture and various components such as HDFS Namenode Datanode and MapReduce Job Tracker Task Tracker and programming paradigm Experience in using Hadoop Technologies such as HDFS SQOOP HIVE Impala Flume Spark Strong experience in writing Map Reduce jobs in Hive Experience with Databases like SQL MySQL and MongoDB Extensively worked on importing and exporting data from different systems to HDFS using SQOOP Extensively works on using Hadoop ecosystem components for storage and processing data exported data into Tableau using Live connection Experienced in working with Hadoop eco system using HiveQL on different formats like Text file CSV file Good experience on creating databases tables and views in HiveQL Implemented Hadoop stack and different bigdata analytic tools migration from different databases ie SQL MySQL to Hadoop Load and transform large sets of structured semistructured data using Hadoop ecosystem components Worked on building hive and mapreduce scripts Having experience on using OOZIE to define and schedule the jobs Good Knowledge on Zookeeper Sentiment Analysis Having experience on Managing HDFS file system Having knowledge on Python MongoDB Experience and knowledge of real time data analytics using Spark Streaming and Flume Implementing Spark using Python and Spark SQL for faster testing and processing of data responsible to manage data from different sources Good experience on all flavours of HadoopCloudera Hortonworks MapR etc Excellent communication skills interpersonal skills problem solving skills a very good team player along with a cando attitude and ability to effectively communicate with all levels of the organization such as technical management and customers Ability to easily adapt and learn any new technology or software Authorized to work in the US for any employer Work Experience Hadoop Developer Tata Consultancy Services Pune May 2014 to April 2017 Description Creatingdashboard for data visualization based on everyday critical and as well as failed transactions in order to monitor their status rather than checking each and every transaction status using middleware applications status manually GUI provides level of criticality for a particular transaction and charts for number failures It allows the monitoring team take appropriate action within SLA Service level agreement Responsibilities Involved in analyzing the system and business Involved in database connection by using SQOOP Involved in importing data from MySQL to HDFS using SQOOPInvolved in migrating tables from RDBMS into Hive tables using SQOOP Written Hive UDFs to sort Structure fields and return complex data type Writing Hive queries to load and process data in Hadoop File System Creating Hive tables loading with data and writing hive queries which will run internally in map reduce way Involved in loading data from UNIX file system to HDFS Installed and configured Hive and also written Hive UDFs Worked with NoSQL databases like Hbase inmaking Hbase tables to load expansive arrangements of semi structured data Zookeeper Sentiment Analysis on reviews of the products on the clients website Involved in transferring files from OLTP server to Hadoop file system Involved in writing queries with HiveQL and Pig Export and Import data into HDFS HBase and Hive using Sqoop Load and transform large sets of structured and semi structured data Loading data into Hive partitioned tables Create reports for the BI team using Sqoop to export data into HDFS and Hive Importing and Exporting Data from Oracle to HiveQL Importing and Exporting Data from HiveQL to HDFS Process and analyze the data from Hive tables using HiveQL EnvironmentAmazon EC2 HartonWorksAmbari Hadoop HDFS Map Reduce Hive Hbase Sqoop Flume CentOS Linux Spark Core JAVA Developer Tata Consultancy Services Pune October 2011 to April 2014 Worked as Hadoop Developer atTata Consultancy Services Puneand Hyderabad from May2014April2017 Project Responsibilities Java Developer Tata Consultancy Services Pune October 2011 to April 2014 Description Creating web based GUI dashboard for Autosys jobs in order to monitor their status rather than checking each and every jobs status manually GUI helps in tracking filewatcher and all other jobs It allows the monitoring team to contact upstreamdownstream teams in time and to run the jobs complete successfully Responsibilities Utilizing Java Java EE JSP Apache Server and SQL Created roles to the customers based on their designation and teams Responsible for creating front end applications user interactive UI web pages using web technologies like HTML5 CSS3 JavaScript JUnit test cases for the classes post development Using SVN delivered and pushed code to Integration and QA environments on time for BA and QA signoffs Extreme attention to accuracy detail presentation and timeliness of delivery Used PCF for monitoring application stability and continuous integration Object storage service Amazon S3 is used to store and retrieve DB information Debugging production issues root cause analysis and fixing Involved in setting up Maven configuration and helping Continuous Integration CI Issues Extensively used SVN as the version controlling Tool for checkins and checkouts Involved in debugging the defects code review and analysis of Performance issues Involved in Jenkins configuration Environment Java Java EE JSP Apache Server SQL Windows7 SVN GIT Jenkin Builds Education Masters in Computer Science and Engineering New Jersey Institute of Technology Newark NJ September 2007 to May 2011 Skills Hdfs Mapreduce Sqoop Hbase Flume Hadoop Mongodb Hadoop Hbase Hive Javascript Mapreduce Python Scripting Database Mysql Sql Apache Linux Unix Additional Information Technical Expertise Hadoop Framework HDFS MapReduce Hive Hbase Sqoop Flume Database Technology HiveQL SQL MySQL MongoDB Operating Systems Windows 10 Linux CentOS 7 ApplicationWeb servers Apache Tomcat ScriptingUnix Shell Javascript LanguagesCore Java Python",
    "unique_id": "86e217f5-795c-4a0d-9fe8-f5d1a718798e"
}