{
    "clean_data": "Jr Hadoop Developer Jr Hadoop span lDeveloperspan Jr Hadoop Developer New Orleans LA 3 years of professional experience in designing development and deployment of big data projects Hadoop written jobs to analyze data using HDFS Map reduce Hive Impala Spark Hbase Kafka Flume and Oozie Good Knowledge on 10 Architecture which include HDFS Hadoop 20 Name Node Data Node Task Tracker Job Tracker Map Reduce YARN Experience in Data load management importing exporting data using SQOOP FLUME Experience in analyzing data using Hive Pig and custom MR programs in Java Experience in integrating Hive and Hbase for effective operations Experience in scheduling and monitoring jobs using Oozie and Zookeeper Experienced in writing Map Reduce programs UDFs for both Pig Hive in java Experience in dealing with log files to extract data and to copy into HDFS using flume Analyzed the SQL scripts and designed the solution to implement using Scala Good Knowledge on Spark Architecture which include RDDs APIs of transformations and actions Experience in writing Hive UDFs queries Experience in developing Applications in Python and Scala for Spark Experience in writing Map reduce Applications using Java Using Sqoop imported data from RDBMS to HDFS Authorized to work in the US for any employer Authorized to work in the US for any employer Work Experience Jr Hadoop Developer Sixgen technology Hyderabad Hyderabad Telangana May 2014 to June 2015 Responsibilities Worked closely with team in converting Business requirements to Technical requirements Imported required tables from RDBMS to HDFS using Sqoop Transformed data using Spark based on requirement Written spark applications using Python and Scala Created H base tables to load large sets of structured data Managed and reviewed Hadoop log files Involved in providing inputs for estimate preparation for the new proposal Worked extensively with HIVE DDLs and Hive Query language HQLs Developed UDF UDAF UDTF functions and implemented it in HIVE Queries Implemented SQOOP for large dataset transfer between Hadoop and RDBMs Used spark SQL for Analytics Experience in Spark streaming Scheduled automated jobs with the help of Oozie Environment HDFS Sqoop Hadoop Flume Kafka Map reduce Hive PIG Oozie HBase Zookeeper Cloudera Oracle NoSQL and UnixLinu Jr Hadoop Developer ISPACE Hyderabad Telangana April 2013 to May 2014 Responsibilities Imported required tables from RDBMS to HDFS using Sqoop Transformed data using Map Reduce on requirement Used Java for writing Map Reduce Jobs Analyzed data using Hive and Impala Worked with Hive Tables Hive queries Partitioning Bucketing Developed Data ingestion platform using Sqoop and Flume to ingest Twitter and Facebook data for Marketing Offers platform Developed and designed automate process using shell scripting for data movement and purging Installation Configuration Management of a small multi node Hadoop cluster Installation and configuration of other open source software like Pig Hive Flume Sqoop Developed programs in JAVA ScalaSpark for data reformation after extraction from HDFS for analysis Written Hive jobs to parse the logs and structure them in tabular format to facilitate effective querying on the log data Importing and exporting data into Impala HDFS and Hive using Sqoop Environment HDFS Sqoop Map Reduce Hive PIG Oozie Cloudera CDH5 Oracle NoSQL and UnixLinux Hadoop Python Developer Glance IT Solutions Hyderabad Telangana May 2012 to March 2013 Responsibilities Designed views and templates with Djangos view controller and templating language to create a web interface Used on Djangos APIs to access database objects Developed user interface using HTML CSS AJAX and JavaScript Worked on APIs deploying queries status codes requests to retrieve data in json format Worked on data exchange from website using xml java and web services Performed on GUI programming using Tkinter Worked onc scripts to parse XML documents and load the data in database Coordinated with team members to improve quality assurance and mitigate risks Wrote stored procedures in MySQL Environment PyQT Python 27 NET PyQuery MVW HTML5 Shell Scripting JSON Rest Apache Web Server SQL UNIX Windows PostgreSQL libraries such as Numpy SQLAlchemy Python etc Education Masters Southern University at New Orleans 2017 Skills APACHE HADOOP SQOOP 2 years Hadoop 2 years HADOOP 2 years Hive 2 years Pig 2 years Additional Information Skills APACHE HADOOP 3 YEARS APACHE HADOOP SQOOP 3 Years HADOOP3YEARS Hive 3 YEARS PIG 3 YEARS",
    "entities": [
        "Installation Configuration Management",
        "Tkinter Worked",
        "Skills APACHE HADOOP",
        "HDFS Hadoop",
        "Developed",
        "US",
        "Sqoop",
        "HIVE",
        "Impala",
        "Djangos",
        "Written Hive",
        "UnixLinu Jr Hadoop Developer ISPACE Hyderabad",
        "Oozie Environment HDFS Sqoop Hadoop",
        "Java Using Sqoop",
        "New Orleans",
        "SQL for Analytics Experience",
        "Sixgen technology Hyderabad Hyderabad",
        "SQL",
        "Hadoop",
        "Data",
        "XML",
        "Telangana",
        "Hive Pig",
        "Marketing Offers",
        "HTML CSS AJAX",
        "Developed UDF UDAF UDTF",
        "Sqoop Transformed",
        "UnixLinux Hadoop Python Developer Glance IT Solutions Hyderabad",
        "Hive",
        "SQOOP",
        "Spark"
    ],
    "experience": "Experience in Data load management importing exporting data using SQOOP FLUME Experience in analyzing data using Hive Pig and custom MR programs in Java Experience in integrating Hive and Hbase for effective operations Experience in scheduling and monitoring jobs using Oozie and Zookeeper Experienced in writing Map Reduce programs UDFs for both Pig Hive in java Experience in dealing with log files to extract data and to copy into HDFS using flume Analyzed the SQL scripts and designed the solution to implement using Scala Good Knowledge on Spark Architecture which include RDDs APIs of transformations and actions Experience in writing Hive UDFs queries Experience in developing Applications in Python and Scala for Spark Experience in writing Map reduce Applications using Java Using Sqoop imported data from RDBMS to HDFS Authorized to work in the US for any employer Authorized to work in the US for any employer Work Experience Jr Hadoop Developer Sixgen technology Hyderabad Hyderabad Telangana May 2014 to June 2015 Responsibilities Worked closely with team in converting Business requirements to Technical requirements Imported required tables from RDBMS to HDFS using Sqoop Transformed data using Spark based on requirement Written spark applications using Python and Scala Created H base tables to load large sets of structured data Managed and reviewed Hadoop log files Involved in providing inputs for estimate preparation for the new proposal Worked extensively with HIVE DDLs and Hive Query language HQLs Developed UDF UDAF UDTF functions and implemented it in HIVE Queries Implemented SQOOP for large dataset transfer between Hadoop and RDBMs Used spark SQL for Analytics Experience in Spark streaming Scheduled automated jobs with the help of Oozie Environment HDFS Sqoop Hadoop Flume Kafka Map reduce Hive PIG Oozie HBase Zookeeper Cloudera Oracle NoSQL and UnixLinu Jr Hadoop Developer ISPACE Hyderabad Telangana April 2013 to May 2014 Responsibilities Imported required tables from RDBMS to HDFS using Sqoop Transformed data using Map Reduce on requirement Used Java for writing Map Reduce Jobs Analyzed data using Hive and Impala Worked with Hive Tables Hive queries Partitioning Bucketing Developed Data ingestion platform using Sqoop and Flume to ingest Twitter and Facebook data for Marketing Offers platform Developed and designed automate process using shell scripting for data movement and purging Installation Configuration Management of a small multi node Hadoop cluster Installation and configuration of other open source software like Pig Hive Flume Sqoop Developed programs in JAVA ScalaSpark for data reformation after extraction from HDFS for analysis Written Hive jobs to parse the logs and structure them in tabular format to facilitate effective querying on the log data Importing and exporting data into Impala HDFS and Hive using Sqoop Environment HDFS Sqoop Map Reduce Hive PIG Oozie Cloudera CDH5 Oracle NoSQL and UnixLinux Hadoop Python Developer Glance IT Solutions Hyderabad Telangana May 2012 to March 2013 Responsibilities Designed views and templates with Djangos view controller and templating language to create a web interface Used on Djangos APIs to access database objects Developed user interface using HTML CSS AJAX and JavaScript Worked on APIs deploying queries status codes requests to retrieve data in json format Worked on data exchange from website using xml java and web services Performed on GUI programming using Tkinter Worked onc scripts to parse XML documents and load the data in database Coordinated with team members to improve quality assurance and mitigate risks Wrote stored procedures in MySQL Environment PyQT Python 27 NET PyQuery MVW HTML5 Shell Scripting JSON Rest Apache Web Server SQL UNIX Windows PostgreSQL libraries such as Numpy SQLAlchemy Python etc Education Masters Southern University at New Orleans 2017 Skills APACHE HADOOP SQOOP 2 years Hadoop 2 years HADOOP 2 years Hive 2 years Pig 2 years Additional Information Skills APACHE HADOOP 3 YEARS APACHE HADOOP SQOOP 3 Years HADOOP3YEARS Hive 3 YEARS PIG 3 YEARS",
    "extracted_keywords": [
        "Jr",
        "Hadoop",
        "Developer",
        "Jr",
        "Hadoop",
        "span",
        "lDeveloperspan",
        "Jr",
        "Hadoop",
        "Developer",
        "New",
        "Orleans",
        "LA",
        "years",
        "experience",
        "development",
        "deployment",
        "data",
        "projects",
        "Hadoop",
        "jobs",
        "data",
        "HDFS",
        "Map",
        "Hive",
        "Impala",
        "Spark",
        "Hbase",
        "Kafka",
        "Flume",
        "Oozie",
        "Good",
        "Knowledge",
        "Architecture",
        "HDFS",
        "Hadoop",
        "Name",
        "Node",
        "Data",
        "Node",
        "Task",
        "Tracker",
        "Job",
        "Tracker",
        "Map",
        "Reduce",
        "YARN",
        "Experience",
        "Data",
        "load",
        "management",
        "data",
        "SQOOP",
        "FLUME",
        "Experience",
        "data",
        "Hive",
        "Pig",
        "custom",
        "MR",
        "programs",
        "Java",
        "Experience",
        "Hive",
        "Hbase",
        "operations",
        "Experience",
        "scheduling",
        "monitoring",
        "jobs",
        "Oozie",
        "Zookeeper",
        "Map",
        "Reduce",
        "programs",
        "UDFs",
        "Pig",
        "Hive",
        "Experience",
        "log",
        "files",
        "data",
        "HDFS",
        "flume",
        "SQL",
        "scripts",
        "solution",
        "Scala",
        "Good",
        "Knowledge",
        "Spark",
        "Architecture",
        "RDDs",
        "APIs",
        "transformations",
        "actions",
        "Hive",
        "UDFs",
        "Experience",
        "Applications",
        "Python",
        "Scala",
        "Spark",
        "Experience",
        "Map",
        "Applications",
        "Java",
        "Using",
        "Sqoop",
        "data",
        "RDBMS",
        "HDFS",
        "US",
        "employer",
        "US",
        "employer",
        "Work",
        "Experience",
        "Jr",
        "Hadoop",
        "Developer",
        "Sixgen",
        "technology",
        "Hyderabad",
        "Hyderabad",
        "Telangana",
        "May",
        "June",
        "Responsibilities",
        "team",
        "Business",
        "requirements",
        "requirements",
        "tables",
        "RDBMS",
        "HDFS",
        "Sqoop",
        "Transformed",
        "data",
        "Spark",
        "requirement",
        "Written",
        "spark",
        "applications",
        "Python",
        "Scala",
        "Created",
        "H",
        "base",
        "tables",
        "sets",
        "data",
        "Hadoop",
        "log",
        "files",
        "inputs",
        "estimate",
        "preparation",
        "proposal",
        "HIVE",
        "DDLs",
        "Hive",
        "Query",
        "language",
        "HQLs",
        "UDF",
        "UDTF",
        "functions",
        "HIVE",
        "Queries",
        "SQOOP",
        "transfer",
        "Hadoop",
        "spark",
        "SQL",
        "Analytics",
        "Experience",
        "Spark",
        "jobs",
        "help",
        "Oozie",
        "Environment",
        "HDFS",
        "Sqoop",
        "Hadoop",
        "Flume",
        "Kafka",
        "Map",
        "Hive",
        "PIG",
        "Oozie",
        "HBase",
        "Zookeeper",
        "Cloudera",
        "Oracle",
        "NoSQL",
        "UnixLinu",
        "Jr",
        "Hadoop",
        "Developer",
        "ISPACE",
        "Hyderabad",
        "Telangana",
        "April",
        "May",
        "Responsibilities",
        "tables",
        "RDBMS",
        "HDFS",
        "Sqoop",
        "Transformed",
        "data",
        "Map",
        "Reduce",
        "requirement",
        "Java",
        "Map",
        "Reduce",
        "Jobs",
        "data",
        "Hive",
        "Impala",
        "Hive",
        "Tables",
        "Hive",
        "Partitioning",
        "Bucketing",
        "Developed",
        "Data",
        "ingestion",
        "platform",
        "Sqoop",
        "Flume",
        "Twitter",
        "Facebook",
        "data",
        "Marketing",
        "Offers",
        "platform",
        "process",
        "shell",
        "scripting",
        "data",
        "movement",
        "Installation",
        "Configuration",
        "Management",
        "multi",
        "node",
        "Hadoop",
        "cluster",
        "Installation",
        "configuration",
        "source",
        "software",
        "Pig",
        "Hive",
        "Flume",
        "Sqoop",
        "programs",
        "JAVA",
        "ScalaSpark",
        "data",
        "reformation",
        "extraction",
        "HDFS",
        "analysis",
        "Written",
        "Hive",
        "jobs",
        "logs",
        "format",
        "querying",
        "log",
        "data",
        "data",
        "Impala",
        "HDFS",
        "Hive",
        "Sqoop",
        "Environment",
        "HDFS",
        "Sqoop",
        "Map",
        "Reduce",
        "Hive",
        "PIG",
        "Oozie",
        "Cloudera",
        "CDH5",
        "Oracle",
        "NoSQL",
        "UnixLinux",
        "Hadoop",
        "Python",
        "Developer",
        "Glance",
        "IT",
        "Solutions",
        "Hyderabad",
        "Telangana",
        "May",
        "March",
        "Responsibilities",
        "views",
        "templates",
        "Djangos",
        "controller",
        "templating",
        "language",
        "web",
        "interface",
        "Djangos",
        "APIs",
        "database",
        "user",
        "interface",
        "HTML",
        "CSS",
        "AJAX",
        "JavaScript",
        "APIs",
        "queries",
        "status",
        "requests",
        "data",
        "json",
        "format",
        "data",
        "exchange",
        "website",
        "xml",
        "java",
        "web",
        "services",
        "GUI",
        "programming",
        "Tkinter",
        "Worked",
        "onc",
        "scripts",
        "XML",
        "documents",
        "data",
        "database",
        "team",
        "members",
        "quality",
        "assurance",
        "mitigate",
        "risks",
        "Wrote",
        "procedures",
        "MySQL",
        "Environment",
        "PyQT",
        "Python",
        "NET",
        "PyQuery",
        "MVW",
        "HTML5",
        "Shell",
        "Scripting",
        "JSON",
        "Rest",
        "Apache",
        "Web",
        "Server",
        "SQL",
        "UNIX",
        "Windows",
        "PostgreSQL",
        "libraries",
        "Numpy",
        "SQLAlchemy",
        "Python",
        "Education",
        "Masters",
        "Southern",
        "University",
        "New",
        "Orleans",
        "Skills",
        "APACHE",
        "HADOOP",
        "SQOOP",
        "years",
        "Hadoop",
        "years",
        "HADOOP",
        "years",
        "Hive",
        "years",
        "Pig",
        "years",
        "Additional",
        "Information",
        "Skills",
        "APACHE",
        "HADOOP",
        "YEARS",
        "APACHE",
        "HADOOP",
        "Years",
        "Hive",
        "YEARS",
        "PIG",
        "YEARS"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T22:48:59.011385",
    "resume_data": "Jr Hadoop Developer Jr Hadoop span lDeveloperspan Jr Hadoop Developer New Orleans LA 3 years of professional experience in designing development and deployment of big data projects Hadoop written jobs to analyze data using HDFS Map reduce Hive Impala Spark Hbase Kafka Flume and Oozie Good Knowledge on 10 Architecture which include HDFS Hadoop 20 Name Node Data Node Task Tracker Job Tracker Map Reduce YARN Experience in Data load management importing exporting data using SQOOP FLUME Experience in analyzing data using Hive Pig and custom MR programs in Java Experience in integrating Hive and Hbase for effective operations Experience in scheduling and monitoring jobs using Oozie and Zookeeper Experienced in writing Map Reduce programs UDFs for both Pig Hive in java Experience in dealing with log files to extract data and to copy into HDFS using flume Analyzed the SQL scripts and designed the solution to implement using Scala Good Knowledge on Spark Architecture which include RDDs APIs of transformations and actions Experience in writing Hive UDFs queries Experience in developing Applications in Python and Scala for Spark Experience in writing Map reduce Applications using Java Using Sqoop imported data from RDBMS to HDFS Authorized to work in the US for any employer Authorized to work in the US for any employer Work Experience Jr Hadoop Developer Sixgen technology Hyderabad Hyderabad Telangana May 2014 to June 2015 Responsibilities Worked closely with team in converting Business requirements to Technical requirements Imported required tables from RDBMS to HDFS using Sqoop Transformed data using Spark based on requirement Written spark applications using Python and Scala Created H base tables to load large sets of structured data Managed and reviewed Hadoop log files Involved in providing inputs for estimate preparation for the new proposal Worked extensively with HIVE DDLs and Hive Query language HQLs Developed UDF UDAF UDTF functions and implemented it in HIVE Queries Implemented SQOOP for large dataset transfer between Hadoop and RDBMs Used spark SQL for Analytics Experience in Spark streaming Scheduled automated jobs with the help of Oozie Environment HDFS Sqoop Hadoop Flume Kafka Map reduce Hive PIG Oozie HBase Zookeeper Cloudera Oracle NoSQL and UnixLinu Jr Hadoop Developer ISPACE Hyderabad Telangana April 2013 to May 2014 Responsibilities Imported required tables from RDBMS to HDFS using Sqoop Transformed data using Map Reduce on requirement Used Java for writing Map Reduce Jobs Analyzed data using Hive and Impala Worked with Hive Tables Hive queries Partitioning Bucketing Developed Data ingestion platform using Sqoop and Flume to ingest Twitter and Facebook data for Marketing Offers platform Developed and designed automate process using shell scripting for data movement and purging Installation Configuration Management of a small multi node Hadoop cluster Installation and configuration of other open source software like Pig Hive Flume Sqoop Developed programs in JAVA ScalaSpark for data reformation after extraction from HDFS for analysis Written Hive jobs to parse the logs and structure them in tabular format to facilitate effective querying on the log data Importing and exporting data into Impala HDFS and Hive using Sqoop Environment HDFS Sqoop Map Reduce Hive PIG Oozie Cloudera CDH5 Oracle NoSQL and UnixLinux Hadoop Python Developer Glance IT Solutions Hyderabad Telangana May 2012 to March 2013 Responsibilities Designed views and templates with Djangos view controller and templating language to create a web interface Used on Djangos APIs to access database objects Developed user interface using HTML CSS AJAX and JavaScript Worked on APIs deploying queries status codes requests to retrieve data in json format Worked on data exchange from website using xml java and web services Performed on GUI programming using Tkinter Worked onc scripts to parse XML documents and load the data in database Coordinated with team members to improve quality assurance and mitigate risks Wrote stored procedures in MySQL Environment PyQT Python 27 NET PyQuery MVW HTML5 Shell Scripting JSON Rest Apache Web Server SQL UNIX Windows PostgreSQL libraries such as Numpy SQLAlchemy Python etc Education Masters Southern University at New Orleans 2017 Skills APACHE HADOOP SQOOP 2 years Hadoop 2 years HADOOP 2 years Hive 2 years Pig 2 years Additional Information Skills APACHE HADOOP 3 YEARS APACHE HADOOP SQOOP 3 Years HADOOP3YEARS Hive 3 YEARS PIG 3 YEARS",
    "unique_id": "85e47dd4-6c24-40af-a54a-672c0a0871e6"
}