{
    "clean_data": "Talend Developer Talend span lDeveloperspan Talend Developer Big Data Over 8 years of IT industry experience in all aspects of Analysis Design Testing Development Implementation and Support of Relational Database OLTP Data Warehousing Systems OLAP and Data Marts in various domains Around 4 years of experience with Talend Open Studio Talend Enterprise platform for Data Management Experience in working with Data Warehousing Concepts like OLAP OLTP Star Schema Snow Flake Schema Logical Data Modeling Physical Modeling and Dimension Data Modeling Utilized tStats Catcher tDie tLog Row to create a generic job to store processing stats Involved in extracting users data from various data sources into Hadoop Distributed File Systems HDFS Automated all the jobs for pulling data from FTP server to load data into Hive tables using Oozy workflows Experience in managing and reviewing Hadoop log files Excellent understanding and knowledge of NOSQL databases like MongoDB HBase Cassandra Experience in Hadoop administration activities such as installation and configuration of clusters using Apache Cloudera and AWS Experience in importing and exporting data using Sqoop and writing custom shell scripts from HDFS to Relational Database Systems and viceversa Experience with MapReduce Pig Programming Model Installation and Configuration of Hadoop HBase Hive Pig Sqoop and Flume using Linux commands Experienced in creating Generic schemas and creating Context Groups and Variables to run jobs against different environments like Dev Test and Prod Thorough knowledge of addressing Performance Issues and Involved in query tuning Index tuning Data profiling and other database related activities Extensively created mappings in Talend using tMap tJoin tReplicate tParallelize t Java t Java row tDie tAggregate Row tWarn tLog Catcher tFilter tGlobal map etc Wrote Hive and Pig queries for data analysis to meet the business requirements Experienced in scheduling Talend jobs using Talend Administration Console TAC Experience with Talend DI Installation Administration and development for data warehouse and application integration Expertise in Data modeling techniques like Data Modeling Dimensional Star Schema and Snowflake modeling Slowly Changing Dimensions SCD Type 1 Type 2 and Type 3 Tracking Daily data load Monthly data extracts and send to client for their verification Strong experience in designing and developing Business Intelligence solutions in Data Warehousing using ETL Tools Excellent understanding and best practice of Data Warehousing Concepts involved in Full Development life cycle of Data Warehousing Experienced in working with different data sources like Flat files Spreadsheet files log files and Databases Worked extensively with slowly changing dimensions Handson experience across all stages of Software Development Life Cycle SDLC including business requirement analysis data mapping build unit testing systems integration and user acceptance testing Excellent interpersonal and communication skills and is experienced in working with senior level managers business people and developers across multiple discipline Work Experience Talend Developer Big Data October 2015 to Present Bank Of New York Mellon New Jersey Responsibilities Worked closely with Business Analysts to review the business specifications of the project and to gather the ETL requirements Closely worked with Data Architects in designing of tables and even involved in modifying technical Specifications Since this is Migration project we are doing migrating from data stage to Talend Using Big data Components We are working sprint wise Using Talend big data components like Hadoop and S3 Buckets and AWS Services for redshift Involved in Extraction Transformation and Loading of data Utilized Big Data components like tHDFSInput tHDFSOutput tHiveLoad tHiveInput tHbaseInput tHbaseOutput tSqoopImport and tSqoopExport Work with the offshore team for the day to day work and review the tasks done by them get the status updates in the daily meetings Data ingestion with different data sources and load into redshift Developed jobs to send and read data from AWS S3 buckets using components like tS3Connection tS3BucketExist tS3Get tS3Put Designed and Implemented the ETL process using Talend Enterprise Big Data Edition to load the data from Source to Target Database Involved in Data Extraction from Flat files and XML files using Talend by using Java as Backend Language Using Talend to load the data into our warehouse systems Used over 20 Components in Talend Like tMap Tfilelist Tjava Tlogrow ToracleInput ToracleOutput tsendEmail etc Used debugger and breakpoints to view transformations output and debug mappings Load and transform data into HDFS from large set of structured data OracleSql server using Talend Big data studio Used Big Data components Hive components for extracting data from hive sources Wrote HiveQL queries using joins and implemented in tHiveInput component Utilized Big Data components like tHiveInput tHiveOutput tHDFSOutput tHiveRow tHiveLoad tHiveConnection tOracleInput tOracleOutput tPreJob tPostJob tLogRow Develop ETL mappings for various Sources TXT CSV XML and also load the data from these sources into relational tables with Talend Enterprise Edition Worked on Global Context variables Context variables and extensively used over 70components in Talend to create jobs Extracting transformed data from Hadoop to destination systems as a oneoff job batch process or Hadoop streaming process Worked on Error handling techniques and tuning the ETL flow for better performance Worked Extensively TAC Admin Console where we Schedule Jobs in Job Conductor Extensively Used Talend components tMap tDie tConvertType tFlowMeter tLogCatcher tRowGenerator tOracleInput tOracleOutput tfileList tDelimited etc Migrated the code and release documents from DEV to QA UAT and to Production Design and Implemented ETL for data load from heterogeneous Sources to SQL Server and Oracle as target databases and for Fact and Slowly Changing Dimensions SCDType1 and SCDType2 Created complex ETL jobs for data exchange from and to Database Server and various other systems including RDBMS XML CSV and Flat file structures Environment Talend Data Integration 64631 Talend Enterprise Big Data Edition 51 Talend Administrator Console MS SQL Server 20122008 Oracle 11g HDFS Hive Impala Sqoop Cloudera CDH 5 TOAD UNIX Sr Talend Developer Intercontinental Hotels Group Atlanta GA August 2013 to September 2015 Responsibilities Participated in all phases of development lifecycle with extensive involvement in the definition and design meetings functional and technical walkthroughs Created Talend jobs to copy the files from one server to another and utilized Talend FTP components Created and managed Source to Target mapping documents for all Facts and Dimension tables Used ETL methodologies and best practices to create Talend ETL jobs Followed and enhanced programming and naming standards Created and deployed physical objects including custom tables custom views stored procedures and Indexes to SQL Server for Staging and DataMart environment Utilized Big Data components like tHDFSInput tHDFSOutput tPigLoad tPigFilterRow   tHiveLoad tHiveInput tHbaseInput tHbaseOutput tSqoopImport and tSqoopExport Extensively used tMap component which does lookup Joiner Functions tjava tOracle txml tdelimtedfiles tlogrow tlogback components etc in many of my Jobs Created and worked on over 100components to use in my jobs Used Talend most used components tMap tDie tConvertType tFlowMeter tLogCatcher tRowGenerator tSetGlobalVar tHashInput tHashOutput and many more Created many complex ETL jobs for data exchange from and to Database Server and various other systems including RDBMS XML CSV and Flat file structures Created Implicit local and global Context variables in the job Worked on Talend Administration Console TAC for scheduling jobs and adding users Worked on various Talend components such as tMap tFilterRow tAggregateRow tFileExist tFileCopy tFileList tDie etc Developed stored procedure to automate the testing process to ease QA efforts and reduced the test timelines for data comparison on tables Automated SFTP process by exchanging SSH keys between UNIX servers Worked Extensively on Talend Admin Console and Schedule Jobs in Job Conductor Involved in production n deployment activities creation of the deployment guide for migration of the code to production also prepared production run books Environment Talend Data Integration 61551 Talend Enterprise Big Data Edition 551 Talend Administrator Console Oracle 11g Hive HDFS Sqoop Netezza SQL Navigator Toad Control M Putty Winscp ETL Developer Talend Dallas TX November 2012 to July 2013 Responsibilities Coordinated with Business Users for requirement gathering business analysis to understand the business requirement and to prepare Technical Specification documents TSD to code ETL Mappings for new requirement changes Analyze and create low level design document LLD and mapping document Created Hadoop cluster connections to access HDFS Extensively used components like tWaitForFile tIterateToFlow tFlowToIterate tHashoutput tHashInput tMap tRunjob tJava tNormalize and tfile components to create Talend jobs Used Talend components such as tmap tFileExit tFileCompare tETLAggregate tOracleinput tOracleOutput etc Developed jobs to move inbound files to HDFS file location based on monthly weekly daily and hourly partitioning Developed jobs to expose HDFS files to Hive tables and Views depending up on the schema versions Worked on Talend Administrator Console TAC for scheduling jobs and adding users Created jobs to perform record count validation and schema validation Created contexts to use the values throughout the process to pass from parent to child jobs and child to parent jobs Developed joblets that are reused in different processes in the flow Developed error logging module to capture both system errors and logical errors that contains Email notification updating tables and moving files to error directories Performed unit testing and integration testing after the development and got the code reviewed Extensively worked on AutoSys to schedule the jobs for loading data Implemented Error handling in Talend to validate the data integrity and data completeness for the data from the flat file Designed and Developed Oracle PLSQL and UNIX Shell Scripts Data ImportExport Used mapping parameters and variables for pulling incremental loads from source Identified and fixed the Bottle Necks and tuned the Mappings and Sessions for improving performance Tuned both ETL process as well as Databases Environment Talend Studio 522 56 Talend Studio Big Data Platform HBase XML files Flat files HL7 files HDFS Hive Oracle 11g Business Objects UNIX WinSCP Clear Case Clear Quest Erwin PLSQL Toad Windows 7 Pro TFS JIRA Java Developer Wisdom ITS Hyderabad Telangana June 2009 to October 2012 Responsibilities Worked on both WebLogic Portal for Portal development and WebLogic for Data Services Programming Developed the presentation layer using JSP HTML CSS and client validations using JavaScript Used GWT to send AJAX request to the server and updating data in the UI dynamically Developed Hibernate 30 in Data Access Layer to access and update information in the database Used JDBC SQL and PLSQL programming for storing retrieving manipulating the data Involved in designing and development of the ecommerce site using JSP Servlet EJBs JavaScript and JDBC Used Eclipse 60 as IDE for application development Configured Struts framework to implement MVC design patterns Validated all forms using Struts validation framework and implemented Tiles framework in the presentation layer Designed and developed GUI using JSP HTML DHTML and CSS Worked with JMS for messaging interface Used Hibernate for handling database transactions and persisting objects deployed the entire project on WebLogic application server Used AJAX for interactive user operations and clientside validations Used XSL transforms on certain XML data Used XML for ORM mapping relations with the java classes and the database Developed ANT script for compiling and deployment Performed unit testing using Junit Environment JavaJ2EE Oracle 10g SQL PLSQL JSP EJB Struts Hibernate WebLogic 80 HTML AJAX Java Script JDBC XML JMS XSLT UML JUnit Log4j Eclipse 60",
    "entities": [
        "Jobs Created",
        "Talend Studio Big Data",
        "Talend Enterprise Edition Worked",
        "Used Hibernate",
        "AJAX",
        "Environment Talend Data Integration",
        "GUI",
        "QA UAT",
        "Worked Extensively",
        "Analysis Design Testing Development Implementation",
        "HDFS",
        "UNIX",
        "Database Server",
        "Worked on Talend Administration Console",
        "Technical Specification",
        "Data Marts",
        "tOracle",
        "Present Bank Of New York Mellon",
        "Target Database Involved",
        "SQL Server for Staging and DataMart",
        "Hadoop",
        "XML",
        "DHTML",
        "Atlanta",
        "NOSQL",
        "Talend Developer Talend span lDeveloperspan Talend Developer Big Data",
        "WebLogic",
        "Dimension Data Modeling Utilized",
        "Data Modeling Dimensional Star Schema and Snowflake",
        "MapReduce Pig Programming Model Installation and Configuration of Hadoop HBase Hive Pig",
        "Modeling",
        "AWS Services",
        "TX",
        "Junit Environment JavaJ2EE Oracle",
        "Target",
        "Talend Enterprise Big Data Edition",
        "ETL Tools Excellent",
        "Data Services Programming Developed",
        "SQL Server",
        "Utilized Big Data",
        "Production Design and Implemented ETL",
        "Developed",
        "S3 Buckets",
        "Created Hadoop",
        "AWS S3",
        "Dallas",
        "tMap",
        "Talend Big",
        "Business Analysts",
        "ETL Mappings",
        "Talend Admin Console",
        "Hadoop Distributed File Systems",
        "Develop",
        "JSP",
        "Talend Open Studio Talend Enterprise",
        "Talend",
        "Views",
        "TAC Admin Console",
        "MVC",
        "Talend Using Big data",
        "Data Extraction",
        "TAC",
        "Talend Administration Console",
        "Sqoop",
        "QA",
        "Data Architects",
        "OracleSql",
        "Created",
        "Oracle",
        "Business Intelligence",
        "Created Talend",
        "Data Warehousing Experienced",
        "Sr Talend Developer Intercontinental Hotels Group",
        "Relational Database Systems",
        "Data Management",
        "SSH",
        "Big Data",
        "Hive",
        "Data Access Layer",
        "Handson",
        "FTP",
        "Talend DI Installation Administration",
        "Mappings",
        "Using Talend big data",
        "ETL",
        "Followed",
        "Performed",
        "XSLT UML JUnit",
        "AutoSys",
        "GWT",
        "Talend ETL",
        "DEV",
        "UI",
        "CDH 5",
        "Business Objects",
        "Facts",
        "Data Warehousing",
        "Created Implicit",
        "TOAD",
        "Software Development Life Cycle",
        "WebLogic Portal",
        "Data Warehousing Concepts"
    ],
    "experience": "Experience in working with Data Warehousing Concepts like OLAP OLTP Star Schema Snow Flake Schema Logical Data Modeling Physical Modeling and Dimension Data Modeling Utilized tStats Catcher tDie tLog Row to create a generic job to store processing stats Involved in extracting users data from various data sources into Hadoop Distributed File Systems HDFS Automated all the jobs for pulling data from FTP server to load data into Hive tables using Oozy workflows Experience in managing and reviewing Hadoop log files Excellent understanding and knowledge of NOSQL databases like MongoDB HBase Cassandra Experience in Hadoop administration activities such as installation and configuration of clusters using Apache Cloudera and AWS Experience in importing and exporting data using Sqoop and writing custom shell scripts from HDFS to Relational Database Systems and viceversa Experience with MapReduce Pig Programming Model Installation and Configuration of Hadoop HBase Hive Pig Sqoop and Flume using Linux commands Experienced in creating Generic schemas and creating Context Groups and Variables to run jobs against different environments like Dev Test and Prod Thorough knowledge of addressing Performance Issues and Involved in query tuning Index tuning Data profiling and other database related activities Extensively created mappings in Talend using tMap tJoin tReplicate tParallelize t Java t Java row tDie tAggregate Row tWarn tLog Catcher tFilter tGlobal map etc Wrote Hive and Pig queries for data analysis to meet the business requirements Experienced in scheduling Talend jobs using Talend Administration Console TAC Experience with Talend DI Installation Administration and development for data warehouse and application integration Expertise in Data modeling techniques like Data Modeling Dimensional Star Schema and Snowflake modeling Slowly Changing Dimensions SCD Type 1 Type 2 and Type 3 Tracking Daily data load Monthly data extracts and send to client for their verification Strong experience in designing and developing Business Intelligence solutions in Data Warehousing using ETL Tools Excellent understanding and best practice of Data Warehousing Concepts involved in Full Development life cycle of Data Warehousing Experienced in working with different data sources like Flat files Spreadsheet files log files and Databases Worked extensively with slowly changing dimensions Handson experience across all stages of Software Development Life Cycle SDLC including business requirement analysis data mapping build unit testing systems integration and user acceptance testing Excellent interpersonal and communication skills and is experienced in working with senior level managers business people and developers across multiple discipline Work Experience Talend Developer Big Data October 2015 to Present Bank Of New York Mellon New Jersey Responsibilities Worked closely with Business Analysts to review the business specifications of the project and to gather the ETL requirements Closely worked with Data Architects in designing of tables and even involved in modifying technical Specifications Since this is Migration project we are doing migrating from data stage to Talend Using Big data Components We are working sprint wise Using Talend big data components like Hadoop and S3 Buckets and AWS Services for redshift Involved in Extraction Transformation and Loading of data Utilized Big Data components like tHDFSInput tHDFSOutput tHiveLoad tHiveInput tHbaseInput tHbaseOutput tSqoopImport and tSqoopExport Work with the offshore team for the day to day work and review the tasks done by them get the status updates in the daily meetings Data ingestion with different data sources and load into redshift Developed jobs to send and read data from AWS S3 buckets using components like tS3Connection tS3BucketExist tS3Get tS3Put Designed and Implemented the ETL process using Talend Enterprise Big Data Edition to load the data from Source to Target Database Involved in Data Extraction from Flat files and XML files using Talend by using Java as Backend Language Using Talend to load the data into our warehouse systems Used over 20 Components in Talend Like tMap Tfilelist Tjava Tlogrow ToracleInput ToracleOutput tsendEmail etc Used debugger and breakpoints to view transformations output and debug mappings Load and transform data into HDFS from large set of structured data OracleSql server using Talend Big data studio Used Big Data components Hive components for extracting data from hive sources Wrote HiveQL queries using joins and implemented in tHiveInput component Utilized Big Data components like tHiveInput tHiveOutput tHDFSOutput tHiveRow tHiveLoad tHiveConnection tOracleInput tOracleOutput tPreJob tPostJob tLogRow Develop ETL mappings for various Sources TXT CSV XML and also load the data from these sources into relational tables with Talend Enterprise Edition Worked on Global Context variables Context variables and extensively used over 70components in Talend to create jobs Extracting transformed data from Hadoop to destination systems as a oneoff job batch process or Hadoop streaming process Worked on Error handling techniques and tuning the ETL flow for better performance Worked Extensively TAC Admin Console where we Schedule Jobs in Job Conductor Extensively Used Talend components tMap tDie tConvertType tFlowMeter tLogCatcher tRowGenerator tOracleInput tOracleOutput tfileList tDelimited etc Migrated the code and release documents from DEV to QA UAT and to Production Design and Implemented ETL for data load from heterogeneous Sources to SQL Server and Oracle as target databases and for Fact and Slowly Changing Dimensions SCDType1 and SCDType2 Created complex ETL jobs for data exchange from and to Database Server and various other systems including RDBMS XML CSV and Flat file structures Environment Talend Data Integration 64631 Talend Enterprise Big Data Edition 51 Talend Administrator Console MS SQL Server 20122008 Oracle 11 g HDFS Hive Impala Sqoop Cloudera CDH 5 TOAD UNIX Sr Talend Developer Intercontinental Hotels Group Atlanta GA August 2013 to September 2015 Responsibilities Participated in all phases of development lifecycle with extensive involvement in the definition and design meetings functional and technical walkthroughs Created Talend jobs to copy the files from one server to another and utilized Talend FTP components Created and managed Source to Target mapping documents for all Facts and Dimension tables Used ETL methodologies and best practices to create Talend ETL jobs Followed and enhanced programming and naming standards Created and deployed physical objects including custom tables custom views stored procedures and Indexes to SQL Server for Staging and DataMart environment Utilized Big Data components like tHDFSInput tHDFSOutput tPigLoad tPigFilterRow    tHiveLoad tHiveInput tHbaseInput tHbaseOutput tSqoopImport and tSqoopExport Extensively used tMap component which does lookup Joiner Functions tjava tOracle txml tdelimtedfiles tlogrow tlogback components etc in many of my Jobs Created and worked on over 100components to use in my jobs Used Talend most used components tMap tDie tConvertType tFlowMeter tLogCatcher tRowGenerator tSetGlobalVar tHashInput tHashOutput and many more Created many complex ETL jobs for data exchange from and to Database Server and various other systems including RDBMS XML CSV and Flat file structures Created Implicit local and global Context variables in the job Worked on Talend Administration Console TAC for scheduling jobs and adding users Worked on various Talend components such as tMap tFilterRow tAggregateRow tFileExist tFileCopy tFileList tDie etc Developed stored procedure to automate the testing process to ease QA efforts and reduced the test timelines for data comparison on tables Automated SFTP process by exchanging SSH keys between UNIX servers Worked Extensively on Talend Admin Console and Schedule Jobs in Job Conductor Involved in production n deployment activities creation of the deployment guide for migration of the code to production also prepared production run books Environment Talend Data Integration 61551 Talend Enterprise Big Data Edition 551 Talend Administrator Console Oracle 11 g Hive HDFS Sqoop Netezza SQL Navigator Toad Control M Putty Winscp ETL Developer Talend Dallas TX November 2012 to July 2013 Responsibilities Coordinated with Business Users for requirement gathering business analysis to understand the business requirement and to prepare Technical Specification documents TSD to code ETL Mappings for new requirement changes Analyze and create low level design document LLD and mapping document Created Hadoop cluster connections to access HDFS Extensively used components like tWaitForFile tIterateToFlow tFlowToIterate tHashoutput tHashInput tMap tRunjob tJava tNormalize and tfile components to create Talend jobs Used Talend components such as tmap tFileExit tFileCompare tETLAggregate tOracleinput tOracleOutput etc Developed jobs to move inbound files to HDFS file location based on monthly weekly daily and hourly partitioning Developed jobs to expose HDFS files to Hive tables and Views depending up on the schema versions Worked on Talend Administrator Console TAC for scheduling jobs and adding users Created jobs to perform record count validation and schema validation Created contexts to use the values throughout the process to pass from parent to child jobs and child to parent jobs Developed joblets that are reused in different processes in the flow Developed error logging module to capture both system errors and logical errors that contains Email notification updating tables and moving files to error directories Performed unit testing and integration testing after the development and got the code reviewed Extensively worked on AutoSys to schedule the jobs for loading data Implemented Error handling in Talend to validate the data integrity and data completeness for the data from the flat file Designed and Developed Oracle PLSQL and UNIX Shell Scripts Data ImportExport Used mapping parameters and variables for pulling incremental loads from source Identified and fixed the Bottle Necks and tuned the Mappings and Sessions for improving performance Tuned both ETL process as well as Databases Environment Talend Studio 522 56 Talend Studio Big Data Platform HBase XML files Flat files HL7 files HDFS Hive Oracle 11 g Business Objects UNIX WinSCP Clear Case Clear Quest Erwin PLSQL Toad Windows 7 Pro TFS JIRA Java Developer Wisdom ITS Hyderabad Telangana June 2009 to October 2012 Responsibilities Worked on both WebLogic Portal for Portal development and WebLogic for Data Services Programming Developed the presentation layer using JSP HTML CSS and client validations using JavaScript Used GWT to send AJAX request to the server and updating data in the UI dynamically Developed Hibernate 30 in Data Access Layer to access and update information in the database Used JDBC SQL and PLSQL programming for storing retrieving manipulating the data Involved in designing and development of the ecommerce site using JSP Servlet EJBs JavaScript and JDBC Used Eclipse 60 as IDE for application development Configured Struts framework to implement MVC design patterns Validated all forms using Struts validation framework and implemented Tiles framework in the presentation layer Designed and developed GUI using JSP HTML DHTML and CSS Worked with JMS for messaging interface Used Hibernate for handling database transactions and persisting objects deployed the entire project on WebLogic application server Used AJAX for interactive user operations and clientside validations Used XSL transforms on certain XML data Used XML for ORM mapping relations with the java classes and the database Developed ANT script for compiling and deployment Performed unit testing using Junit Environment JavaJ2EE Oracle 10 g SQL PLSQL JSP EJB Struts Hibernate WebLogic 80 HTML AJAX Java Script JDBC XML JMS XSLT UML JUnit Log4j Eclipse 60",
    "extracted_keywords": [
        "Talend",
        "Developer",
        "Talend",
        "span",
        "lDeveloperspan",
        "Talend",
        "Developer",
        "Big",
        "Data",
        "years",
        "IT",
        "industry",
        "experience",
        "aspects",
        "Analysis",
        "Design",
        "Testing",
        "Development",
        "Implementation",
        "Support",
        "Relational",
        "Database",
        "OLTP",
        "Data",
        "Warehousing",
        "Systems",
        "OLAP",
        "Data",
        "Marts",
        "domains",
        "years",
        "experience",
        "Talend",
        "Open",
        "Studio",
        "Talend",
        "Enterprise",
        "platform",
        "Data",
        "Management",
        "Experience",
        "Data",
        "Warehousing",
        "Concepts",
        "OLAP",
        "OLTP",
        "Star",
        "Schema",
        "Snow",
        "Flake",
        "Schema",
        "Logical",
        "Data",
        "Modeling",
        "Physical",
        "Modeling",
        "Dimension",
        "Data",
        "Modeling",
        "tStats",
        "Catcher",
        "tDie",
        "tLog",
        "Row",
        "job",
        "processing",
        "stats",
        "users",
        "data",
        "data",
        "sources",
        "Hadoop",
        "Distributed",
        "File",
        "Systems",
        "HDFS",
        "jobs",
        "data",
        "FTP",
        "server",
        "data",
        "Hive",
        "tables",
        "Oozy",
        "workflows",
        "Experience",
        "Hadoop",
        "log",
        "understanding",
        "knowledge",
        "NOSQL",
        "MongoDB",
        "HBase",
        "Cassandra",
        "Experience",
        "Hadoop",
        "administration",
        "activities",
        "installation",
        "configuration",
        "clusters",
        "Apache",
        "Cloudera",
        "AWS",
        "Experience",
        "data",
        "Sqoop",
        "custom",
        "shell",
        "scripts",
        "HDFS",
        "Relational",
        "Database",
        "Systems",
        "viceversa",
        "Experience",
        "MapReduce",
        "Pig",
        "Programming",
        "Model",
        "Installation",
        "Configuration",
        "Hadoop",
        "HBase",
        "Hive",
        "Pig",
        "Sqoop",
        "Flume",
        "Linux",
        "commands",
        "Generic",
        "schemas",
        "Context",
        "Groups",
        "Variables",
        "jobs",
        "environments",
        "Dev",
        "Test",
        "Prod",
        "Thorough",
        "knowledge",
        "Performance",
        "Issues",
        "query",
        "Index",
        "Data",
        "profiling",
        "database",
        "activities",
        "mappings",
        "Talend",
        "tMap",
        "tJoin",
        "tReplicate",
        "tParallelize",
        "t",
        "Java",
        "t",
        "Java",
        "row",
        "tDie",
        "tAggregate",
        "Row",
        "tWarn",
        "tLog",
        "Catcher",
        "tFilter",
        "tGlobal",
        "map",
        "Wrote",
        "Hive",
        "Pig",
        "queries",
        "data",
        "analysis",
        "business",
        "requirements",
        "scheduling",
        "Talend",
        "jobs",
        "Talend",
        "Administration",
        "Console",
        "TAC",
        "Experience",
        "Talend",
        "DI",
        "Installation",
        "Administration",
        "development",
        "data",
        "warehouse",
        "application",
        "integration",
        "Expertise",
        "Data",
        "techniques",
        "Data",
        "Dimensional",
        "Star",
        "Schema",
        "Snowflake",
        "Dimensions",
        "SCD",
        "Type",
        "Type",
        "Type",
        "Tracking",
        "Daily",
        "data",
        "data",
        "extracts",
        "client",
        "verification",
        "experience",
        "Business",
        "Intelligence",
        "solutions",
        "Data",
        "Warehousing",
        "ETL",
        "Tools",
        "Excellent",
        "understanding",
        "practice",
        "Data",
        "Warehousing",
        "Concepts",
        "Full",
        "Development",
        "life",
        "cycle",
        "Data",
        "Warehousing",
        "data",
        "sources",
        "files",
        "Spreadsheet",
        "files",
        "files",
        "Databases",
        "dimensions",
        "Handson",
        "experience",
        "stages",
        "Software",
        "Development",
        "Life",
        "Cycle",
        "SDLC",
        "business",
        "requirement",
        "analysis",
        "data",
        "mapping",
        "build",
        "unit",
        "testing",
        "systems",
        "integration",
        "user",
        "acceptance",
        "testing",
        "Excellent",
        "communication",
        "skills",
        "level",
        "managers",
        "business",
        "people",
        "developers",
        "discipline",
        "Work",
        "Experience",
        "Talend",
        "Developer",
        "Big",
        "Data",
        "October",
        "Present",
        "Bank",
        "New",
        "York",
        "Mellon",
        "New",
        "Jersey",
        "Responsibilities",
        "Business",
        "Analysts",
        "business",
        "specifications",
        "project",
        "ETL",
        "requirements",
        "Data",
        "Architects",
        "designing",
        "tables",
        "Specifications",
        "Migration",
        "project",
        "migrating",
        "data",
        "stage",
        "data",
        "Components",
        "sprint",
        "Talend",
        "data",
        "components",
        "Hadoop",
        "S3",
        "Buckets",
        "AWS",
        "Services",
        "redshift",
        "Extraction",
        "Transformation",
        "Loading",
        "data",
        "Big",
        "Data",
        "components",
        "tHDFSInput",
        "tHDFSOutput",
        "tHiveLoad",
        "tHiveInput",
        "tHbaseInput",
        "tHbaseOutput",
        "tSqoopImport",
        "tSqoopExport",
        "Work",
        "team",
        "day",
        "day",
        "work",
        "tasks",
        "status",
        "updates",
        "meetings",
        "Data",
        "ingestion",
        "data",
        "sources",
        "load",
        "redshift",
        "jobs",
        "data",
        "AWS",
        "S3",
        "buckets",
        "components",
        "ETL",
        "process",
        "Talend",
        "Enterprise",
        "Big",
        "Data",
        "Edition",
        "data",
        "Source",
        "Target",
        "Database",
        "Data",
        "Extraction",
        "files",
        "XML",
        "files",
        "Talend",
        "Java",
        "Backend",
        "Language",
        "Talend",
        "data",
        "warehouse",
        "systems",
        "Components",
        "Talend",
        "tMap",
        "Tfilelist",
        "Tjava",
        "Tlogrow",
        "ToracleInput",
        "ToracleOutput",
        "debugger",
        "breakpoints",
        "transformations",
        "output",
        "debug",
        "mappings",
        "Load",
        "data",
        "HDFS",
        "set",
        "data",
        "OracleSql",
        "server",
        "data",
        "studio",
        "Big",
        "Data",
        "components",
        "Hive",
        "components",
        "data",
        "hive",
        "sources",
        "Wrote",
        "HiveQL",
        "queries",
        "joins",
        "tHiveInput",
        "component",
        "Big",
        "Data",
        "components",
        "tHiveInput",
        "tHiveOutput",
        "tHDFSOutput",
        "tHiveRow",
        "tHiveLoad",
        "tHiveConnection",
        "tOracleInput",
        "tOracleOutput",
        "tPreJob",
        "tPostJob",
        "tLogRow",
        "Develop",
        "ETL",
        "mappings",
        "Sources",
        "CSV",
        "XML",
        "data",
        "sources",
        "tables",
        "Talend",
        "Enterprise",
        "Edition",
        "Global",
        "Context",
        "Context",
        "variables",
        "70components",
        "Talend",
        "jobs",
        "data",
        "Hadoop",
        "destination",
        "systems",
        "oneoff",
        "job",
        "batch",
        "process",
        "Hadoop",
        "streaming",
        "process",
        "Error",
        "techniques",
        "ETL",
        "flow",
        "performance",
        "TAC",
        "Admin",
        "Console",
        "Jobs",
        "Job",
        "Conductor",
        "Talend",
        "components",
        "tMap",
        "tDie",
        "tConvertType",
        "tFlowMeter",
        "tLogCatcher",
        "tRowGenerator",
        "tOracleInput",
        "tOracleOutput",
        "tfileList",
        "tDelimited",
        "code",
        "documents",
        "DEV",
        "UAT",
        "Production",
        "Design",
        "ETL",
        "data",
        "load",
        "Sources",
        "SQL",
        "Server",
        "Oracle",
        "target",
        "databases",
        "Fact",
        "Dimensions",
        "SCDType1",
        "SCDType2",
        "ETL",
        "jobs",
        "data",
        "exchange",
        "Database",
        "Server",
        "systems",
        "XML",
        "CSV",
        "file",
        "structures",
        "Environment",
        "Talend",
        "Data",
        "Integration",
        "Talend",
        "Enterprise",
        "Big",
        "Data",
        "Edition",
        "Talend",
        "Administrator",
        "Console",
        "MS",
        "SQL",
        "Server",
        "Oracle",
        "g",
        "HDFS",
        "Hive",
        "Impala",
        "Sqoop",
        "Cloudera",
        "CDH",
        "TOAD",
        "UNIX",
        "Sr",
        "Talend",
        "Developer",
        "Intercontinental",
        "Hotels",
        "Group",
        "Atlanta",
        "GA",
        "August",
        "September",
        "Responsibilities",
        "phases",
        "development",
        "lifecycle",
        "involvement",
        "definition",
        "design",
        "meetings",
        "walkthroughs",
        "Created",
        "Talend",
        "jobs",
        "files",
        "server",
        "Talend",
        "FTP",
        "components",
        "Source",
        "mapping",
        "documents",
        "Facts",
        "Dimension",
        "ETL",
        "methodologies",
        "practices",
        "Talend",
        "ETL",
        "jobs",
        "programming",
        "naming",
        "standards",
        "objects",
        "custom",
        "tables",
        "custom",
        "views",
        "procedures",
        "Indexes",
        "SQL",
        "Server",
        "Staging",
        "DataMart",
        "environment",
        "Big",
        "Data",
        "components",
        "tHDFSInput",
        "tHDFSOutput",
        "tPigFilterRow",
        "tHiveLoad",
        "tHiveInput",
        "tHbaseInput",
        "tHbaseOutput",
        "tSqoopImport",
        "tSqoopExport",
        "tMap",
        "component",
        "Joiner",
        "Functions",
        "tjava",
        "tOracle",
        "txml",
        "tlogback",
        "components",
        "Jobs",
        "100components",
        "jobs",
        "Talend",
        "components",
        "tMap",
        "tDie",
        "tConvertType",
        "tFlowMeter",
        "tLogCatcher",
        "tRowGenerator",
        "tSetGlobalVar",
        "tHashInput",
        "tHashOutput",
        "ETL",
        "jobs",
        "data",
        "exchange",
        "Database",
        "Server",
        "systems",
        "XML",
        "CSV",
        "file",
        "structures",
        "Implicit",
        "Context",
        "variables",
        "job",
        "Talend",
        "Administration",
        "Console",
        "TAC",
        "scheduling",
        "jobs",
        "users",
        "Talend",
        "components",
        "tMap",
        "tFilterRow",
        "tAggregateRow",
        "tFileExist",
        "tFileCopy",
        "tFileList",
        "tDie",
        "procedure",
        "testing",
        "process",
        "QA",
        "efforts",
        "test",
        "timelines",
        "data",
        "comparison",
        "tables",
        "process",
        "SSH",
        "keys",
        "UNIX",
        "servers",
        "Talend",
        "Admin",
        "Console",
        "Schedule",
        "Jobs",
        "Job",
        "Conductor",
        "production",
        "deployment",
        "activities",
        "creation",
        "deployment",
        "guide",
        "migration",
        "code",
        "production",
        "production",
        "run",
        "books",
        "Environment",
        "Talend",
        "Data",
        "Integration",
        "Talend",
        "Enterprise",
        "Big",
        "Data",
        "Edition",
        "Talend",
        "Administrator",
        "Console",
        "Oracle",
        "g",
        "Hive",
        "HDFS",
        "Sqoop",
        "Netezza",
        "SQL",
        "Navigator",
        "Toad",
        "Control",
        "M",
        "Putty",
        "Winscp",
        "ETL",
        "Developer",
        "Talend",
        "Dallas",
        "TX",
        "November",
        "July",
        "Responsibilities",
        "Business",
        "Users",
        "requirement",
        "business",
        "analysis",
        "business",
        "requirement",
        "Technical",
        "Specification",
        "documents",
        "TSD",
        "ETL",
        "Mappings",
        "requirement",
        "changes",
        "Analyze",
        "level",
        "design",
        "document",
        "LLD",
        "mapping",
        "document",
        "Created",
        "Hadoop",
        "cluster",
        "connections",
        "access",
        "components",
        "tWaitForFile",
        "tFlowToIterate",
        "tHashoutput",
        "tHashInput",
        "tMap",
        "tRunjob",
        "tJava",
        "tNormalize",
        "components",
        "jobs",
        "Talend",
        "components",
        "tFileExit",
        "tFileCompare",
        "tOracleinput",
        "jobs",
        "files",
        "HDFS",
        "file",
        "location",
        "jobs",
        "HDFS",
        "files",
        "Hive",
        "tables",
        "Views",
        "schema",
        "versions",
        "Talend",
        "Administrator",
        "Console",
        "TAC",
        "scheduling",
        "jobs",
        "users",
        "jobs",
        "record",
        "count",
        "validation",
        "schema",
        "validation",
        "contexts",
        "values",
        "process",
        "parent",
        "child",
        "jobs",
        "child",
        "parent",
        "jobs",
        "joblets",
        "processes",
        "flow",
        "error",
        "module",
        "system",
        "errors",
        "errors",
        "Email",
        "notification",
        "tables",
        "files",
        "error",
        "directories",
        "Performed",
        "unit",
        "testing",
        "integration",
        "testing",
        "development",
        "code",
        "AutoSys",
        "jobs",
        "loading",
        "data",
        "Error",
        "handling",
        "Talend",
        "data",
        "integrity",
        "data",
        "completeness",
        "data",
        "file",
        "Oracle",
        "PLSQL",
        "UNIX",
        "Shell",
        "Scripts",
        "Data",
        "ImportExport",
        "mapping",
        "parameters",
        "variables",
        "loads",
        "source",
        "Bottle",
        "Necks",
        "Mappings",
        "Sessions",
        "performance",
        "ETL",
        "process",
        "Databases",
        "Environment",
        "Talend",
        "Studio",
        "Talend",
        "Studio",
        "Big",
        "Data",
        "Platform",
        "HBase",
        "XML",
        "files",
        "HL7",
        "HDFS",
        "Hive",
        "Oracle",
        "g",
        "Business",
        "UNIX",
        "WinSCP",
        "Clear",
        "Case",
        "Clear",
        "Quest",
        "Erwin",
        "PLSQL",
        "Toad",
        "Windows",
        "Pro",
        "TFS",
        "JIRA",
        "Java",
        "Developer",
        "Wisdom",
        "Hyderabad",
        "Telangana",
        "June",
        "October",
        "Responsibilities",
        "WebLogic",
        "Portal",
        "Portal",
        "development",
        "WebLogic",
        "Data",
        "Services",
        "Programming",
        "presentation",
        "layer",
        "JSP",
        "HTML",
        "CSS",
        "client",
        "validations",
        "JavaScript",
        "GWT",
        "AJAX",
        "request",
        "server",
        "data",
        "UI",
        "Hibernate",
        "Data",
        "Access",
        "Layer",
        "information",
        "database",
        "JDBC",
        "SQL",
        "PLSQL",
        "programming",
        "retrieving",
        "data",
        "designing",
        "development",
        "ecommerce",
        "site",
        "JSP",
        "Servlet",
        "EJBs",
        "JavaScript",
        "JDBC",
        "Eclipse",
        "IDE",
        "application",
        "development",
        "Configured",
        "Struts",
        "framework",
        "MVC",
        "design",
        "patterns",
        "forms",
        "Struts",
        "validation",
        "framework",
        "Tiles",
        "framework",
        "presentation",
        "layer",
        "GUI",
        "JSP",
        "HTML",
        "DHTML",
        "CSS",
        "JMS",
        "interface",
        "Hibernate",
        "database",
        "transactions",
        "objects",
        "project",
        "WebLogic",
        "application",
        "server",
        "AJAX",
        "user",
        "operations",
        "validations",
        "XSL",
        "XML",
        "data",
        "XML",
        "ORM",
        "mapping",
        "relations",
        "classes",
        "database",
        "ANT",
        "script",
        "deployment",
        "Performed",
        "unit",
        "testing",
        "Junit",
        "Environment",
        "JavaJ2EE",
        "Oracle",
        "g",
        "SQL",
        "PLSQL",
        "JSP",
        "EJB",
        "Struts",
        "Hibernate",
        "WebLogic",
        "HTML",
        "AJAX",
        "Java",
        "Script",
        "JDBC",
        "XML",
        "JMS",
        "UML",
        "JUnit",
        "Log4j",
        "Eclipse"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T21:49:04.772102",
    "resume_data": "Talend Developer Talend span lDeveloperspan Talend Developer Big Data Over 8 years of IT industry experience in all aspects of Analysis Design Testing Development Implementation and Support of Relational Database OLTP Data Warehousing Systems OLAP and Data Marts in various domains Around 4 years of experience with Talend Open Studio Talend Enterprise platform for Data Management Experience in working with Data Warehousing Concepts like OLAP OLTP Star Schema Snow Flake Schema Logical Data Modeling Physical Modeling and Dimension Data Modeling Utilized tStats Catcher tDie tLog Row to create a generic job to store processing stats Involved in extracting users data from various data sources into Hadoop Distributed File Systems HDFS Automated all the jobs for pulling data from FTP server to load data into Hive tables using Oozy workflows Experience in managing and reviewing Hadoop log files Excellent understanding and knowledge of NOSQL databases like MongoDB HBase Cassandra Experience in Hadoop administration activities such as installation and configuration of clusters using Apache Cloudera and AWS Experience in importing and exporting data using Sqoop and writing custom shell scripts from HDFS to Relational Database Systems and viceversa Experience with MapReduce Pig Programming Model Installation and Configuration of Hadoop HBase Hive Pig Sqoop and Flume using Linux commands Experienced in creating Generic schemas and creating Context Groups and Variables to run jobs against different environments like Dev Test and Prod Thorough knowledge of addressing Performance Issues and Involved in query tuning Index tuning Data profiling and other database related activities Extensively created mappings in Talend using tMap tJoin tReplicate tParallelize t Java t Java row tDie tAggregate Row tWarn tLog Catcher tFilter tGlobal map etc Wrote Hive and Pig queries for data analysis to meet the business requirements Experienced in scheduling Talend jobs using Talend Administration Console TAC Experience with Talend DI Installation Administration and development for data warehouse and application integration Expertise in Data modeling techniques like Data Modeling Dimensional Star Schema and Snowflake modeling Slowly Changing Dimensions SCD Type 1 Type 2 and Type 3 Tracking Daily data load Monthly data extracts and send to client for their verification Strong experience in designing and developing Business Intelligence solutions in Data Warehousing using ETL Tools Excellent understanding and best practice of Data Warehousing Concepts involved in Full Development life cycle of Data Warehousing Experienced in working with different data sources like Flat files Spreadsheet files log files and Databases Worked extensively with slowly changing dimensions Handson experience across all stages of Software Development Life Cycle SDLC including business requirement analysis data mapping build unit testing systems integration and user acceptance testing Excellent interpersonal and communication skills and is experienced in working with senior level managers business people and developers across multiple discipline Work Experience Talend Developer Big Data October 2015 to Present Bank Of New York Mellon New Jersey Responsibilities Worked closely with Business Analysts to review the business specifications of the project and to gather the ETL requirements Closely worked with Data Architects in designing of tables and even involved in modifying technical Specifications Since this is Migration project we are doing migrating from data stage to Talend Using Big data Components We are working sprint wise Using Talend big data components like Hadoop and S3 Buckets and AWS Services for redshift Involved in Extraction Transformation and Loading of data Utilized Big Data components like tHDFSInput tHDFSOutput tHiveLoad tHiveInput tHbaseInput tHbaseOutput tSqoopImport and tSqoopExport Work with the offshore team for the day to day work and review the tasks done by them get the status updates in the daily meetings Data ingestion with different data sources and load into redshift Developed jobs to send and read data from AWS S3 buckets using components like tS3Connection tS3BucketExist tS3Get tS3Put Designed and Implemented the ETL process using Talend Enterprise Big Data Edition to load the data from Source to Target Database Involved in Data Extraction from Flat files and XML files using Talend by using Java as Backend Language Using Talend to load the data into our warehouse systems Used over 20 Components in Talend Like tMap Tfilelist Tjava Tlogrow ToracleInput ToracleOutput tsendEmail etc Used debugger and breakpoints to view transformations output and debug mappings Load and transform data into HDFS from large set of structured data OracleSql server using Talend Big data studio Used Big Data components Hive components for extracting data from hive sources Wrote HiveQL queries using joins and implemented in tHiveInput component Utilized Big Data components like tHiveInput tHiveOutput tHDFSOutput tHiveRow tHiveLoad tHiveConnection tOracleInput tOracleOutput tPreJob tPostJob tLogRow Develop ETL mappings for various Sources TXT CSV XML and also load the data from these sources into relational tables with Talend Enterprise Edition Worked on Global Context variables Context variables and extensively used over 70components in Talend to create jobs Extracting transformed data from Hadoop to destination systems as a oneoff job batch process or Hadoop streaming process Worked on Error handling techniques and tuning the ETL flow for better performance Worked Extensively TAC Admin Console where we Schedule Jobs in Job Conductor Extensively Used Talend components tMap tDie tConvertType tFlowMeter tLogCatcher tRowGenerator tOracleInput tOracleOutput tfileList tDelimited etc Migrated the code and release documents from DEV to QA UAT and to Production Design and Implemented ETL for data load from heterogeneous Sources to SQL Server and Oracle as target databases and for Fact and Slowly Changing Dimensions SCDType1 and SCDType2 Created complex ETL jobs for data exchange from and to Database Server and various other systems including RDBMS XML CSV and Flat file structures Environment Talend Data Integration 64631 Talend Enterprise Big Data Edition 51 Talend Administrator Console MS SQL Server 20122008 Oracle 11g HDFS Hive Impala Sqoop Cloudera CDH 5 TOAD UNIX Sr Talend Developer Intercontinental Hotels Group Atlanta GA August 2013 to September 2015 Responsibilities Participated in all phases of development lifecycle with extensive involvement in the definition and design meetings functional and technical walkthroughs Created Talend jobs to copy the files from one server to another and utilized Talend FTP components Created and managed Source to Target mapping documents for all Facts and Dimension tables Used ETL methodologies and best practices to create Talend ETL jobs Followed and enhanced programming and naming standards Created and deployed physical objects including custom tables custom views stored procedures and Indexes to SQL Server for Staging and DataMart environment Utilized Big Data components like tHDFSInput tHDFSOutput tPigLoad tPigFilterRow tPigFilterColumn tPigStoreResult tHiveLoad tHiveInput tHbaseInput tHbaseOutput tSqoopImport and tSqoopExport Extensively used tMap component which does lookup Joiner Functions tjava tOracle txml tdelimtedfiles tlogrow tlogback components etc in many of my Jobs Created and worked on over 100components to use in my jobs Used Talend most used components tMap tDie tConvertType tFlowMeter tLogCatcher tRowGenerator tSetGlobalVar tHashInput tHashOutput and many more Created many complex ETL jobs for data exchange from and to Database Server and various other systems including RDBMS XML CSV and Flat file structures Created Implicit local and global Context variables in the job Worked on Talend Administration Console TAC for scheduling jobs and adding users Worked on various Talend components such as tMap tFilterRow tAggregateRow tFileExist tFileCopy tFileList tDie etc Developed stored procedure to automate the testing process to ease QA efforts and reduced the test timelines for data comparison on tables Automated SFTP process by exchanging SSH keys between UNIX servers Worked Extensively on Talend Admin Console and Schedule Jobs in Job Conductor Involved in production n deployment activities creation of the deployment guide for migration of the code to production also prepared production run books Environment Talend Data Integration 61551 Talend Enterprise Big Data Edition 551 Talend Administrator Console Oracle 11g Hive HDFS Sqoop Netezza SQL Navigator Toad Control M Putty Winscp ETL Developer Talend Dallas TX November 2012 to July 2013 Responsibilities Coordinated with Business Users for requirement gathering business analysis to understand the business requirement and to prepare Technical Specification documents TSD to code ETL Mappings for new requirement changes Analyze and create low level design document LLD and mapping document Created Hadoop cluster connections to access HDFS Extensively used components like tWaitForFile tIterateToFlow tFlowToIterate tHashoutput tHashInput tMap tRunjob tJava tNormalize and tfile components to create Talend jobs Used Talend components such as tmap tFileExit tFileCompare tETLAggregate tOracleinput tOracleOutput etc Developed jobs to move inbound files to HDFS file location based on monthly weekly daily and hourly partitioning Developed jobs to expose HDFS files to Hive tables and Views depending up on the schema versions Worked on Talend Administrator Console TAC for scheduling jobs and adding users Created jobs to perform record count validation and schema validation Created contexts to use the values throughout the process to pass from parent to child jobs and child to parent jobs Developed joblets that are reused in different processes in the flow Developed error logging module to capture both system errors and logical errors that contains Email notification updating tables and moving files to error directories Performed unit testing and integration testing after the development and got the code reviewed Extensively worked on AutoSys to schedule the jobs for loading data Implemented Error handling in Talend to validate the data integrity and data completeness for the data from the flat file Designed and Developed Oracle PLSQL and UNIX Shell Scripts Data ImportExport Used mapping parameters and variables for pulling incremental loads from source Identified and fixed the Bottle Necks and tuned the Mappings and Sessions for improving performance Tuned both ETL process as well as Databases Environment Talend Studio 522 56 Talend Studio Big Data Platform HBase XML files Flat files HL7 files HDFS Hive Oracle 11g Business Objects UNIX WinSCP Clear Case Clear Quest Erwin PLSQL Toad Windows 7 Pro TFS JIRA Java Developer Wisdom ITS Hyderabad Telangana June 2009 to October 2012 Responsibilities Worked on both WebLogic Portal for Portal development and WebLogic for Data Services Programming Developed the presentation layer using JSP HTML CSS and client validations using JavaScript Used GWT to send AJAX request to the server and updating data in the UI dynamically Developed Hibernate 30 in Data Access Layer to access and update information in the database Used JDBC SQL and PLSQL programming for storing retrieving manipulating the data Involved in designing and development of the ecommerce site using JSP Servlet EJBs JavaScript and JDBC Used Eclipse 60 as IDE for application development Configured Struts framework to implement MVC design patterns Validated all forms using Struts validation framework and implemented Tiles framework in the presentation layer Designed and developed GUI using JSP HTML DHTML and CSS Worked with JMS for messaging interface Used Hibernate for handling database transactions and persisting objects deployed the entire project on WebLogic application server Used AJAX for interactive user operations and clientside validations Used XSL transforms on certain XML data Used XML for ORM mapping relations with the java classes and the database Developed ANT script for compiling and deployment Performed unit testing using Junit Environment JavaJ2EE Oracle 10g SQL PLSQL JSP EJB Struts Hibernate WebLogic 80 HTML AJAX Java Script JDBC XML JMS XSLT UML JUnit Log4j Eclipse 60",
    "unique_id": "3bb6c2bf-3161-4af3-96a7-29293f9d5350"
}