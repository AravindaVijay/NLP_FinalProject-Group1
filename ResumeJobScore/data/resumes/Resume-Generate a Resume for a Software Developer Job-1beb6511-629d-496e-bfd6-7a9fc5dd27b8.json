{
    "clean_data": "Sr Big Data Developer Sr Big Data span lDeveloperspan Sr Big Data Developer JPMorgan Chase Texas City TX 6 years of IT experience including 4 years of working with Big Data and Cloudera Involved in various SDLC methods from analysis design development testing implementation and maintenance with timely delivery against aggressive deadlines in both AgileScrum and Waterfall methodology Good experience in installing configuring and leveraging the Hadoop ecosystem to glean meaningful insights from semistructured and unstructured data Excellent Communication Management and Presentation skills Strong working experience with ingestion storage processing and analysis of big data Successfully loaded files to HDFS from Oracle Sql Server and Teradata using Sqoop Extensive experience with ETL and Query tools for Big Data like Pig Latin and HiveQL Experience in developing data pipeline using Kafka Spark and Hive to ingest transform and analyzing data Experience in Data modeling and connecting Cassandra from Spark and saving summarized data frame to Cassandra Exploring with Spark for improving the performance and optimization of the existing algorithms in Hadoop using Spark Context SparkSQL Data Frame Pair RDDs Spark Yarn Developing applications using Scala Spark SQL and MLlib along with Kafka and other tools as per requirement then deployed on the Yarn cluster Experience in building high performance and scalable solutions using various Hadoop ecosystem tools like Pig Hive Sqoop Spark Solr and Kafka Good knowledge in distributed coordination system ZooKeeper and experience with Data Warehousing and ETL Good knowledge in job workflow scheduling and monitoring tools like Oozie and Zookeeper Involved in converting HiveSQL queries into Spark transformations using Spark RDDs Spark SQL using Scala Very good understanding of Partitions bucketing concepts in Hive and designed both Managed and External tables in Hive to optimize performance Experience in developing ETL scripts for data acquisition and transformation using Informatica and Talend Experience in working with databases such as Oracle SQL Server My SQL Good understanding of Software Development Life Cycle SDLC and sound knowledge of project implementation methodologies including Waterfall and Agile Proficient in Java J2EE Servlets JSP spring Hibernate Strong Experience on ApplicationPlatform Consolidation and Rehosting Legacy ConversionRetirement ETL ELT data pipeline development for operational data stores and analytical warehouses Well verse and hands on experience in Version control tools like GIT and SVN Experience on working structured unstructured data with various file formats such as Avro data files xml files JSON files sequence files ORC and Parquet Experience in job workflow scheduling and monitoring tools like Oozie Experience in Amazon AWS to spin up the EMR cluster to process the data which is stored in Amazon S3 Work Experience Sr Big Data Developer JPMorgan Chase Plano TX February 2018 to Present Responsibilities Involved in project Life Cycle from analysis to production implementation with emphasis on identifying the source and source data validation developing logic and transformation as per the requirement and creating mappings and loading the data into different targets Loaded periodic incremental imports of structured batch data from various RDBMS to HDFS using Sqoop Implemented Kafka consumers for HDFS and Spark Streaming Used Spark Streaming to preprocess the data for realtime data analysis Worked on basic Shell Scripting to put data from sources to HDFS and S3 Scheduled the scripts using cron tab Involved in writing query using Impala for better and faster processing of data Implemented Partitioning in Impala for faster and efficient data access Worked on reading multiple data formats such as Avro Parquet ORC JSON including Text Spark transformation scripts using APIs like Spark Core and Spark SQL in Scala Worked on writing custom Spark Streaming APIs to ingest the data to Elastic Search post the data enrichment in Spark Worked on Apache NiFi in implementing basic workflows using prebuilt processors Worked with the team in visualizing data using Tableau Experienced as Senior ETL Developer Hadoop ETL Teradata Vertica Informatica Datastage Mainframe Subject Matter Expertise SME Production Support Analyst QA Teste Environment Spark Streaming Spark SQL Spark Core HDFS S3 EMR Impala Kafka Sqoop Oozie Cloudera Manager Apache NiFi Zoo Keeper Sr Hadoop Developer Allergan Chicago IL June 2016 to December 2017 Responsibilities Extracted the data from Oracle Teradata into HDFS using Sqoop Created and worked Sqoop version 143 jobs with incremental load to populate Hive External tables Used Flume to collect aggregate and store the log data from web servers Developed UDFs in Java as and when necessary to use in Hive queries Used Spark Core and Spark SQL for transformations in python based of the business requirements Developed Oozie workflow for scheduling and orchestrating the ETL process Used JIRA for incident creation bug tracking and change management process Worked on developing automation scripts using python Good Working knowledge of Tableau Environment HDFS PySpark Core Spark Spark SQL Sqoop Flume Oozie WinSCP UNIX Shell Scripting HIVE Cloudera Hadoop distribution AWS EC2 EMR S3 JIRA Oracle etc Hadoop Developer Sabre Dallas TX May 2015 to May 2016 Responsibilities Ingested Batch Files into HDFS using shell scripting Used flume to ingest nearrealtime data and perform necessary transformations and aggregations on the fly and persisted the data in Hive Used Hadoops Pig Hive and Map Reduce for analyzing the data and to help by extract data sets for meaningful information Building high resource intensive low latency and SLA stringent processes by establishing Data Ingestion from Data lake in Hadoop ETLELT jobs development using Scala API in Spark Spark SQl Hive QLs for few ETL activities Hive as operational data store HbaseTeradata as historicalproduction data Developed workflow in Oozie to orchestrate a series of Pig scripts to cleanse data such as removing irrelevant information or merging many small files into a handful of very large compressed files using pig pipelines in the data preparation stage Extensively used PIG to communicate with Hive using HCatalog Implemented exception tracking logic using Pig scripts Saved the analyzed data to the Hive Tables for visualization and to generate reports for the BI team Environment Hadoop Map Reduce HDFS Hive Pig Oozie Core Java Python Eclipse Flume Cloudera Oracle UNIX Shell Scripting JavaJ2EE Developer Bajaj Allianz Mumbai Maharashtra January 2014 to March 2015 Responsibilities Worked in SDLC methodology followed Waterfall environment including Acceptance Test Driven Design and Continuous IntegrationDelivery Responsible for analyzing designing developing coordinating and deploying web based application Developed the application using Spring MVC Framework that uses Model View Controller MVC architecture with JSP as the view Used Spring MVC for the management of application flow by developing configurable handler mappings view resolution Used Spring Framework to inject the DAO and Bean objects by auto wiring the components Developed front end applications using the HTML CSS JavaScript and JQuery Designed and developed XSLT transformation of components to convert data from XML to HTML Implemented the project using JAXWS based Web Services using WSDL UDDI and SOAP to communicate with other systems Monitored the error logs using Log4J Maven is used as a build tool and continuous integration is done using Jenkins Used complex queries like SQL statements and procedures to fetch the data from the database Used version control repository GIT and Service now for issue tracking Developed test cases and performed unit testing using Junit Test cases Used ANT as build tool and developed build file for compiling the code of creating WAR files Used Tortoise SVN for Source Control and Version Management Environment Java J2EE MVC JUnit JavaBeans HTML CSS JavaScript JQuery Oracle Hibernate SQL Soap Eclipse ANT Maven Software Developer Infor Hyderabad Telangana May 2013 to December 2014 Responsibilities Involved in daily scrums and weekly meeting with the project sponsors Designed and developed abstract classes interfaces classes to construct the business logic using Object Oriented Concepts Developed the classes using Java which incorporate N tier architecture and database connectivity Developed and tested userfriendly navigators by utilizing JavaScript and JQuery Performed Manual Unit Testing for all units in developed pages Designed and developed rich user interfaces with HTMLCSSJavaScriptBootstrap Involved in writing Stored procedures and Views as per the requirement Environment Java Spring Hibernate JSP JavaScript JQuery Tomcat Apache MySQL HTML CSS Bootstrap Eclipse IDE Maven MySQL workbench Eclipse IDE Apache SubversionSVN Junit JIRA Education Bachelors in information Technology in information Technology Gurunanak Institute of Technology Skills Dynamodb Cassandra Ambari Hdfs Impala Sqoop Hbase Kafka Data visualization Etl Flume Hadoop Map reduce Mongodb Nosql Avro Git Gradle Hadoop Hbase",
    "entities": [
        "MLlib",
        "Oracle SQL Server",
        "Spark Context",
        "Tableau Experienced",
        "Informatica",
        "Tableau Environment HDFS PySpark Core Spark",
        "Spark Core",
        "Partitions",
        "BI",
        "HDFS",
        "Texas City",
        "ConversionRetirement",
        "Sqoop Created",
        "Query",
        "Present Responsibilities Involved",
        "Hive External",
        "Hadoop",
        "Spring MVC Framework",
        "SOAP",
        "ETL Developer Hadoop",
        "XML",
        "Spark Worked",
        "Telangana",
        "JAXWS",
        "Software Development Life Cycle SDLC",
        "Life Cycle",
        "JQuery Performed Manual Unit Testing",
        "ELT",
        "Avro",
        "Developed Oozie",
        "TX",
        "Object Oriented Concepts Developed",
        "Cloudera Hadoop",
        "Developed",
        "DAO",
        "Excellent Communication Management",
        "HbaseTeradata",
        "Dallas",
        "Waterfall",
        "Amazon S3 Work Experience Sr Big Data Developer",
        "Text Spark",
        "Monitored",
        "JSP",
        "Sr Big Data Developer Sr Big Data",
        "Acceptance Test Driven Design",
        "Version",
        "SDLC",
        "Spark Streaming",
        "ORC",
        "Parquet",
        "Java J2EE Servlets JSP",
        "Views",
        "the Hive Tables",
        "Spark",
        "GIT",
        "Web Services",
        "HTML CSS JavaScript",
        "Oracle Teradata",
        "Sqoop",
        "Oozie Experience",
        "HTML Implemented",
        "N",
        "AWS",
        "PIG",
        "Oozie",
        "SQL",
        "Spark Spark",
        "ApplicationPlatform Consolidation and",
        "Hive Used Hadoops Pig Hive",
        "IDE Apache SubversionSVN Junit JIRA Education Bachelors",
        "Big Data",
        "Hive",
        "Amazon AWS",
        "IDE Maven MySQL",
        "Pig Hive Sqoop Spark",
        "ZooKeeper",
        "Mumbai",
        "ETL",
        "Maven",
        "XSLT",
        "Impala",
        "ANT",
        "Technology Gurunanak Institute of Technology",
        "SVN",
        "Source Control",
        "Data",
        "Data Ingestion from Data",
        "Data Warehousing",
        "HCatalog Implemented",
        "Version Management",
        "Teradata",
        "Oracle Sql Server"
    ],
    "experience": "Experience in developing data pipeline using Kafka Spark and Hive to ingest transform and analyzing data Experience in Data modeling and connecting Cassandra from Spark and saving summarized data frame to Cassandra Exploring with Spark for improving the performance and optimization of the existing algorithms in Hadoop using Spark Context SparkSQL Data Frame Pair RDDs Spark Yarn Developing applications using Scala Spark SQL and MLlib along with Kafka and other tools as per requirement then deployed on the Yarn cluster Experience in building high performance and scalable solutions using various Hadoop ecosystem tools like Pig Hive Sqoop Spark Solr and Kafka Good knowledge in distributed coordination system ZooKeeper and experience with Data Warehousing and ETL Good knowledge in job workflow scheduling and monitoring tools like Oozie and Zookeeper Involved in converting HiveSQL queries into Spark transformations using Spark RDDs Spark SQL using Scala Very good understanding of Partitions bucketing concepts in Hive and designed both Managed and External tables in Hive to optimize performance Experience in developing ETL scripts for data acquisition and transformation using Informatica and Talend Experience in working with databases such as Oracle SQL Server My SQL Good understanding of Software Development Life Cycle SDLC and sound knowledge of project implementation methodologies including Waterfall and Agile Proficient in Java J2EE Servlets JSP spring Hibernate Strong Experience on ApplicationPlatform Consolidation and Rehosting Legacy ConversionRetirement ETL ELT data pipeline development for operational data stores and analytical warehouses Well verse and hands on experience in Version control tools like GIT and SVN Experience on working structured unstructured data with various file formats such as Avro data files xml files JSON files sequence files ORC and Parquet Experience in job workflow scheduling and monitoring tools like Oozie Experience in Amazon AWS to spin up the EMR cluster to process the data which is stored in Amazon S3 Work Experience Sr Big Data Developer JPMorgan Chase Plano TX February 2018 to Present Responsibilities Involved in project Life Cycle from analysis to production implementation with emphasis on identifying the source and source data validation developing logic and transformation as per the requirement and creating mappings and loading the data into different targets Loaded periodic incremental imports of structured batch data from various RDBMS to HDFS using Sqoop Implemented Kafka consumers for HDFS and Spark Streaming Used Spark Streaming to preprocess the data for realtime data analysis Worked on basic Shell Scripting to put data from sources to HDFS and S3 Scheduled the scripts using cron tab Involved in writing query using Impala for better and faster processing of data Implemented Partitioning in Impala for faster and efficient data access Worked on reading multiple data formats such as Avro Parquet ORC JSON including Text Spark transformation scripts using APIs like Spark Core and Spark SQL in Scala Worked on writing custom Spark Streaming APIs to ingest the data to Elastic Search post the data enrichment in Spark Worked on Apache NiFi in implementing basic workflows using prebuilt processors Worked with the team in visualizing data using Tableau Experienced as Senior ETL Developer Hadoop ETL Teradata Vertica Informatica Datastage Mainframe Subject Matter Expertise SME Production Support Analyst QA Teste Environment Spark Streaming Spark SQL Spark Core HDFS S3 EMR Impala Kafka Sqoop Oozie Cloudera Manager Apache NiFi Zoo Keeper Sr Hadoop Developer Allergan Chicago IL June 2016 to December 2017 Responsibilities Extracted the data from Oracle Teradata into HDFS using Sqoop Created and worked Sqoop version 143 jobs with incremental load to populate Hive External tables Used Flume to collect aggregate and store the log data from web servers Developed UDFs in Java as and when necessary to use in Hive queries Used Spark Core and Spark SQL for transformations in python based of the business requirements Developed Oozie workflow for scheduling and orchestrating the ETL process Used JIRA for incident creation bug tracking and change management process Worked on developing automation scripts using python Good Working knowledge of Tableau Environment HDFS PySpark Core Spark Spark SQL Sqoop Flume Oozie WinSCP UNIX Shell Scripting HIVE Cloudera Hadoop distribution AWS EC2 EMR S3 JIRA Oracle etc Hadoop Developer Sabre Dallas TX May 2015 to May 2016 Responsibilities Ingested Batch Files into HDFS using shell scripting Used flume to ingest nearrealtime data and perform necessary transformations and aggregations on the fly and persisted the data in Hive Used Hadoops Pig Hive and Map Reduce for analyzing the data and to help by extract data sets for meaningful information Building high resource intensive low latency and SLA stringent processes by establishing Data Ingestion from Data lake in Hadoop ETLELT jobs development using Scala API in Spark Spark SQl Hive QLs for few ETL activities Hive as operational data store HbaseTeradata as historicalproduction data Developed workflow in Oozie to orchestrate a series of Pig scripts to cleanse data such as removing irrelevant information or merging many small files into a handful of very large compressed files using pig pipelines in the data preparation stage Extensively used PIG to communicate with Hive using HCatalog Implemented exception tracking logic using Pig scripts Saved the analyzed data to the Hive Tables for visualization and to generate reports for the BI team Environment Hadoop Map Reduce HDFS Hive Pig Oozie Core Java Python Eclipse Flume Cloudera Oracle UNIX Shell Scripting JavaJ2EE Developer Bajaj Allianz Mumbai Maharashtra January 2014 to March 2015 Responsibilities Worked in SDLC methodology followed Waterfall environment including Acceptance Test Driven Design and Continuous IntegrationDelivery Responsible for analyzing designing developing coordinating and deploying web based application Developed the application using Spring MVC Framework that uses Model View Controller MVC architecture with JSP as the view Used Spring MVC for the management of application flow by developing configurable handler mappings view resolution Used Spring Framework to inject the DAO and Bean objects by auto wiring the components Developed front end applications using the HTML CSS JavaScript and JQuery Designed and developed XSLT transformation of components to convert data from XML to HTML Implemented the project using JAXWS based Web Services using WSDL UDDI and SOAP to communicate with other systems Monitored the error logs using Log4J Maven is used as a build tool and continuous integration is done using Jenkins Used complex queries like SQL statements and procedures to fetch the data from the database Used version control repository GIT and Service now for issue tracking Developed test cases and performed unit testing using Junit Test cases Used ANT as build tool and developed build file for compiling the code of creating WAR files Used Tortoise SVN for Source Control and Version Management Environment Java J2EE MVC JUnit JavaBeans HTML CSS JavaScript JQuery Oracle Hibernate SQL Soap Eclipse ANT Maven Software Developer Infor Hyderabad Telangana May 2013 to December 2014 Responsibilities Involved in daily scrums and weekly meeting with the project sponsors Designed and developed abstract classes interfaces classes to construct the business logic using Object Oriented Concepts Developed the classes using Java which incorporate N tier architecture and database connectivity Developed and tested userfriendly navigators by utilizing JavaScript and JQuery Performed Manual Unit Testing for all units in developed pages Designed and developed rich user interfaces with HTMLCSSJavaScriptBootstrap Involved in writing Stored procedures and Views as per the requirement Environment Java Spring Hibernate JSP JavaScript JQuery Tomcat Apache MySQL HTML CSS Bootstrap Eclipse IDE Maven MySQL workbench Eclipse IDE Apache SubversionSVN Junit JIRA Education Bachelors in information Technology in information Technology Gurunanak Institute of Technology Skills Dynamodb Cassandra Ambari Hdfs Impala Sqoop Hbase Kafka Data visualization Etl Flume Hadoop Map reduce Mongodb Nosql Avro Git Gradle Hadoop Hbase",
    "extracted_keywords": [
        "Sr",
        "Big",
        "Data",
        "Developer",
        "Sr",
        "Big",
        "Data",
        "span",
        "lDeveloperspan",
        "Sr",
        "Big",
        "Data",
        "Developer",
        "JPMorgan",
        "Chase",
        "Texas",
        "City",
        "TX",
        "years",
        "IT",
        "experience",
        "years",
        "Big",
        "Data",
        "Cloudera",
        "SDLC",
        "methods",
        "analysis",
        "design",
        "development",
        "testing",
        "implementation",
        "maintenance",
        "delivery",
        "deadlines",
        "AgileScrum",
        "Waterfall",
        "methodology",
        "experience",
        "configuring",
        "Hadoop",
        "ecosystem",
        "insights",
        "data",
        "Excellent",
        "Communication",
        "Management",
        "Presentation",
        "working",
        "experience",
        "ingestion",
        "storage",
        "processing",
        "analysis",
        "data",
        "files",
        "HDFS",
        "Oracle",
        "Sql",
        "Server",
        "Teradata",
        "Sqoop",
        "experience",
        "ETL",
        "Query",
        "tools",
        "Big",
        "Data",
        "Pig",
        "Latin",
        "HiveQL",
        "Experience",
        "data",
        "pipeline",
        "Kafka",
        "Spark",
        "Hive",
        "transform",
        "data",
        "Experience",
        "Data",
        "modeling",
        "Cassandra",
        "Spark",
        "data",
        "frame",
        "Cassandra",
        "Exploring",
        "Spark",
        "performance",
        "optimization",
        "algorithms",
        "Hadoop",
        "Spark",
        "Context",
        "SparkSQL",
        "Data",
        "Frame",
        "Pair",
        "RDDs",
        "Spark",
        "Yarn",
        "applications",
        "Scala",
        "Spark",
        "SQL",
        "MLlib",
        "Kafka",
        "tools",
        "requirement",
        "Yarn",
        "cluster",
        "Experience",
        "performance",
        "solutions",
        "Hadoop",
        "ecosystem",
        "tools",
        "Pig",
        "Hive",
        "Sqoop",
        "Spark",
        "Solr",
        "Kafka",
        "knowledge",
        "coordination",
        "system",
        "ZooKeeper",
        "experience",
        "Data",
        "Warehousing",
        "ETL",
        "knowledge",
        "job",
        "workflow",
        "scheduling",
        "monitoring",
        "tools",
        "Oozie",
        "Zookeeper",
        "HiveSQL",
        "queries",
        "Spark",
        "transformations",
        "Spark",
        "RDDs",
        "Spark",
        "SQL",
        "Scala",
        "understanding",
        "Partitions",
        "bucketing",
        "concepts",
        "Hive",
        "Managed",
        "tables",
        "Hive",
        "performance",
        "Experience",
        "ETL",
        "scripts",
        "data",
        "acquisition",
        "transformation",
        "Informatica",
        "Talend",
        "Experience",
        "databases",
        "Oracle",
        "SQL",
        "Server",
        "SQL",
        "understanding",
        "Software",
        "Development",
        "Life",
        "Cycle",
        "SDLC",
        "knowledge",
        "project",
        "implementation",
        "methodologies",
        "Waterfall",
        "Agile",
        "Proficient",
        "Java",
        "J2EE",
        "Servlets",
        "JSP",
        "spring",
        "Hibernate",
        "Strong",
        "Experience",
        "ApplicationPlatform",
        "Consolidation",
        "Rehosting",
        "Legacy",
        "ConversionRetirement",
        "ETL",
        "ELT",
        "data",
        "pipeline",
        "development",
        "data",
        "stores",
        "warehouses",
        "verse",
        "hands",
        "experience",
        "Version",
        "control",
        "tools",
        "GIT",
        "SVN",
        "Experience",
        "data",
        "file",
        "formats",
        "Avro",
        "data",
        "files",
        "xml",
        "files",
        "sequence",
        "ORC",
        "Parquet",
        "Experience",
        "job",
        "workflow",
        "scheduling",
        "monitoring",
        "tools",
        "Oozie",
        "Experience",
        "Amazon",
        "AWS",
        "EMR",
        "cluster",
        "data",
        "Amazon",
        "S3",
        "Work",
        "Experience",
        "Sr",
        "Big",
        "Data",
        "Developer",
        "JPMorgan",
        "Chase",
        "Plano",
        "TX",
        "February",
        "Present",
        "Responsibilities",
        "project",
        "Life",
        "Cycle",
        "analysis",
        "production",
        "implementation",
        "emphasis",
        "source",
        "source",
        "data",
        "validation",
        "logic",
        "transformation",
        "requirement",
        "mappings",
        "data",
        "targets",
        "imports",
        "batch",
        "data",
        "RDBMS",
        "HDFS",
        "Sqoop",
        "Kafka",
        "consumers",
        "HDFS",
        "Spark",
        "Streaming",
        "Spark",
        "Streaming",
        "data",
        "data",
        "analysis",
        "Shell",
        "Scripting",
        "data",
        "sources",
        "HDFS",
        "S3",
        "scripts",
        "cron",
        "tab",
        "query",
        "Impala",
        "processing",
        "data",
        "Partitioning",
        "Impala",
        "data",
        "access",
        "data",
        "formats",
        "Avro",
        "Parquet",
        "ORC",
        "JSON",
        "Text",
        "Spark",
        "transformation",
        "scripts",
        "APIs",
        "Spark",
        "Core",
        "Spark",
        "SQL",
        "Scala",
        "custom",
        "Spark",
        "Streaming",
        "APIs",
        "data",
        "Elastic",
        "Search",
        "data",
        "enrichment",
        "Spark",
        "Worked",
        "Apache",
        "NiFi",
        "workflows",
        "prebuilt",
        "processors",
        "team",
        "data",
        "Tableau",
        "ETL",
        "Developer",
        "Hadoop",
        "ETL",
        "Teradata",
        "Vertica",
        "Informatica",
        "Datastage",
        "Mainframe",
        "Subject",
        "Matter",
        "Expertise",
        "SME",
        "Production",
        "Support",
        "Analyst",
        "QA",
        "Teste",
        "Environment",
        "Spark",
        "Streaming",
        "Spark",
        "SQL",
        "Spark",
        "Core",
        "HDFS",
        "S3",
        "EMR",
        "Impala",
        "Kafka",
        "Sqoop",
        "Oozie",
        "Cloudera",
        "Manager",
        "Apache",
        "NiFi",
        "Zoo",
        "Keeper",
        "Sr",
        "Hadoop",
        "Developer",
        "Allergan",
        "Chicago",
        "IL",
        "June",
        "December",
        "Responsibilities",
        "data",
        "Oracle",
        "Teradata",
        "HDFS",
        "Sqoop",
        "Created",
        "Sqoop",
        "version",
        "jobs",
        "load",
        "Hive",
        "External",
        "tables",
        "Flume",
        "aggregate",
        "log",
        "data",
        "web",
        "servers",
        "UDFs",
        "Java",
        "Hive",
        "queries",
        "Spark",
        "Core",
        "Spark",
        "SQL",
        "transformations",
        "python",
        "business",
        "requirements",
        "Oozie",
        "workflow",
        "scheduling",
        "ETL",
        "process",
        "JIRA",
        "incident",
        "creation",
        "bug",
        "tracking",
        "management",
        "process",
        "automation",
        "scripts",
        "python",
        "Working",
        "knowledge",
        "Tableau",
        "Environment",
        "HDFS",
        "PySpark",
        "Core",
        "Spark",
        "Spark",
        "SQL",
        "Sqoop",
        "Flume",
        "Oozie",
        "WinSCP",
        "UNIX",
        "Shell",
        "Scripting",
        "HIVE",
        "Cloudera",
        "Hadoop",
        "distribution",
        "AWS",
        "EC2",
        "EMR",
        "S3",
        "JIRA",
        "Oracle",
        "Hadoop",
        "Developer",
        "Sabre",
        "Dallas",
        "TX",
        "May",
        "May",
        "Responsibilities",
        "Batch",
        "Files",
        "HDFS",
        "shell",
        "scripting",
        "flume",
        "nearrealtime",
        "data",
        "transformations",
        "aggregations",
        "fly",
        "data",
        "Hive",
        "Used",
        "Hadoops",
        "Pig",
        "Hive",
        "Map",
        "Reduce",
        "data",
        "data",
        "sets",
        "information",
        "resource",
        "latency",
        "SLA",
        "processes",
        "Data",
        "Ingestion",
        "Data",
        "lake",
        "Hadoop",
        "ETLELT",
        "jobs",
        "development",
        "Scala",
        "API",
        "Spark",
        "Spark",
        "SQl",
        "Hive",
        "QLs",
        "ETL",
        "activities",
        "Hive",
        "data",
        "store",
        "HbaseTeradata",
        "historicalproduction",
        "data",
        "workflow",
        "Oozie",
        "series",
        "Pig",
        "scripts",
        "cleanse",
        "data",
        "information",
        "files",
        "handful",
        "files",
        "pig",
        "pipelines",
        "data",
        "preparation",
        "stage",
        "PIG",
        "Hive",
        "HCatalog",
        "exception",
        "tracking",
        "logic",
        "Pig",
        "scripts",
        "data",
        "Hive",
        "Tables",
        "visualization",
        "reports",
        "BI",
        "team",
        "Environment",
        "Hadoop",
        "Map",
        "HDFS",
        "Hive",
        "Pig",
        "Oozie",
        "Core",
        "Java",
        "Python",
        "Eclipse",
        "Flume",
        "Cloudera",
        "Oracle",
        "UNIX",
        "Shell",
        "Scripting",
        "JavaJ2EE",
        "Developer",
        "Bajaj",
        "Allianz",
        "Mumbai",
        "Maharashtra",
        "January",
        "March",
        "Responsibilities",
        "SDLC",
        "methodology",
        "Waterfall",
        "environment",
        "Acceptance",
        "Test",
        "Driven",
        "Design",
        "Continuous",
        "IntegrationDelivery",
        "Responsible",
        "web",
        "application",
        "application",
        "Spring",
        "MVC",
        "Framework",
        "Model",
        "View",
        "Controller",
        "MVC",
        "architecture",
        "JSP",
        "view",
        "Spring",
        "MVC",
        "management",
        "application",
        "flow",
        "handler",
        "mappings",
        "resolution",
        "Spring",
        "Framework",
        "DAO",
        "Bean",
        "auto",
        "wiring",
        "components",
        "end",
        "applications",
        "HTML",
        "CSS",
        "JavaScript",
        "JQuery",
        "XSLT",
        "transformation",
        "components",
        "data",
        "XML",
        "HTML",
        "project",
        "JAXWS",
        "Web",
        "Services",
        "WSDL",
        "UDDI",
        "SOAP",
        "systems",
        "error",
        "logs",
        "Maven",
        "build",
        "tool",
        "integration",
        "Jenkins",
        "queries",
        "SQL",
        "statements",
        "procedures",
        "data",
        "database",
        "version",
        "control",
        "repository",
        "GIT",
        "Service",
        "issue",
        "test",
        "cases",
        "unit",
        "testing",
        "Junit",
        "Test",
        "cases",
        "ANT",
        "build",
        "tool",
        "build",
        "file",
        "code",
        "WAR",
        "files",
        "Used",
        "Tortoise",
        "SVN",
        "Source",
        "Control",
        "Version",
        "Management",
        "Environment",
        "Java",
        "J2EE",
        "MVC",
        "JUnit",
        "JavaBeans",
        "HTML",
        "CSS",
        "JavaScript",
        "JQuery",
        "Oracle",
        "Hibernate",
        "SQL",
        "Soap",
        "Eclipse",
        "ANT",
        "Maven",
        "Software",
        "Developer",
        "Infor",
        "Hyderabad",
        "Telangana",
        "May",
        "December",
        "Responsibilities",
        "scrums",
        "meeting",
        "project",
        "sponsors",
        "classes",
        "classes",
        "business",
        "logic",
        "Object",
        "Oriented",
        "Concepts",
        "classes",
        "Java",
        "tier",
        "architecture",
        "database",
        "connectivity",
        "Developed",
        "navigators",
        "JavaScript",
        "JQuery",
        "Performed",
        "Manual",
        "Unit",
        "Testing",
        "units",
        "pages",
        "user",
        "interfaces",
        "HTMLCSSJavaScriptBootstrap",
        "procedures",
        "Views",
        "requirement",
        "Environment",
        "Java",
        "Spring",
        "Hibernate",
        "JSP",
        "JavaScript",
        "JQuery",
        "Tomcat",
        "Apache",
        "MySQL",
        "HTML",
        "CSS",
        "Bootstrap",
        "Eclipse",
        "IDE",
        "Maven",
        "MySQL",
        "workbench",
        "Eclipse",
        "IDE",
        "Apache",
        "SubversionSVN",
        "Junit",
        "JIRA",
        "Education",
        "Bachelors",
        "information",
        "Technology",
        "information",
        "Technology",
        "Gurunanak",
        "Institute",
        "Technology",
        "Skills",
        "Dynamodb",
        "Cassandra",
        "Ambari",
        "Hdfs",
        "Impala",
        "Sqoop",
        "Hbase",
        "Kafka",
        "Data",
        "visualization",
        "Etl",
        "Flume",
        "Hadoop",
        "Map",
        "Mongodb",
        "Nosql",
        "Avro",
        "Git",
        "Gradle",
        "Hadoop",
        "Hbase"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T22:17:38.466234",
    "resume_data": "Sr Big Data Developer Sr Big Data span lDeveloperspan Sr Big Data Developer JPMorgan Chase Texas City TX 6 years of IT experience including 4 years of working with Big Data and Cloudera Involved in various SDLC methods from analysis design development testing implementation and maintenance with timely delivery against aggressive deadlines in both AgileScrum and Waterfall methodology Good experience in installing configuring and leveraging the Hadoop ecosystem to glean meaningful insights from semistructured and unstructured data Excellent Communication Management and Presentation skills Strong working experience with ingestion storage processing and analysis of big data Successfully loaded files to HDFS from Oracle Sql Server and Teradata using Sqoop Extensive experience with ETL and Query tools for Big Data like Pig Latin and HiveQL Experience in developing data pipeline using Kafka Spark and Hive to ingest transform and analyzing data Experience in Data modeling and connecting Cassandra from Spark and saving summarized data frame to Cassandra Exploring with Spark for improving the performance and optimization of the existing algorithms in Hadoop using Spark Context SparkSQL Data Frame Pair RDDs Spark Yarn Developing applications using Scala Spark SQL and MLlib along with Kafka and other tools as per requirement then deployed on the Yarn cluster Experience in building high performance and scalable solutions using various Hadoop ecosystem tools like Pig Hive Sqoop Spark Solr and Kafka Good knowledge in distributed coordination system ZooKeeper and experience with Data Warehousing and ETL Good knowledge in job workflow scheduling and monitoring tools like Oozie and Zookeeper Involved in converting HiveSQL queries into Spark transformations using Spark RDDs Spark SQL using Scala Very good understanding of Partitions bucketing concepts in Hive and designed both Managed and External tables in Hive to optimize performance Experience in developing ETL scripts for data acquisition and transformation using Informatica and Talend Experience in working with databases such as Oracle SQL Server My SQL Good understanding of Software Development Life Cycle SDLC and sound knowledge of project implementation methodologies including Waterfall and Agile Proficient in Java J2EE Servlets JSP spring Hibernate Strong Experience on ApplicationPlatform Consolidation and Rehosting Legacy ConversionRetirement ETL ELT data pipeline development for operational data stores and analytical warehouses Well verse and hands on experience in Version control tools like GIT and SVN Experience on working structured unstructured data with various file formats such as Avro data files xml files JSON files sequence files ORC and Parquet Experience in job workflow scheduling and monitoring tools like Oozie Experience in Amazon AWS to spin up the EMR cluster to process the data which is stored in Amazon S3 Work Experience Sr Big Data Developer JPMorgan Chase Plano TX February 2018 to Present Responsibilities Involved in project Life Cycle from analysis to production implementation with emphasis on identifying the source and source data validation developing logic and transformation as per the requirement and creating mappings and loading the data into different targets Loaded periodic incremental imports of structured batch data from various RDBMS to HDFS using Sqoop Implemented Kafka consumers for HDFS and Spark Streaming Used Spark Streaming to preprocess the data for realtime data analysis Worked on basic Shell Scripting to put data from sources to HDFS and S3 Scheduled the scripts using cron tab Involved in writing query using Impala for better and faster processing of data Implemented Partitioning in Impala for faster and efficient data access Worked on reading multiple data formats such as Avro Parquet ORC JSON including Text Spark transformation scripts using APIs like Spark Core and Spark SQL in Scala Worked on writing custom Spark Streaming APIs to ingest the data to Elastic Search post the data enrichment in Spark Worked on Apache NiFi in implementing basic workflows using prebuilt processors Worked with the team in visualizing data using Tableau Experienced as Senior ETL Developer Hadoop ETL Teradata Vertica Informatica Datastage Mainframe Subject Matter Expertise SME Production Support Analyst QA Teste Environment Spark Streaming Spark SQL Spark Core HDFS S3 EMR Impala Kafka Sqoop Oozie Cloudera Manager Apache NiFi Zoo Keeper Sr Hadoop Developer Allergan Chicago IL June 2016 to December 2017 Responsibilities Extracted the data from Oracle Teradata into HDFS using Sqoop Created and worked Sqoop version 143 jobs with incremental load to populate Hive External tables Used Flume to collect aggregate and store the log data from web servers Developed UDFs in Java as and when necessary to use in Hive queries Used Spark Core and Spark SQL for transformations in python based of the business requirements Developed Oozie workflow for scheduling and orchestrating the ETL process Used JIRA for incident creation bug tracking and change management process Worked on developing automation scripts using python Good Working knowledge of Tableau Environment HDFS PySpark Core Spark Spark SQL Sqoop Flume Oozie WinSCP UNIX Shell Scripting HIVE Cloudera Hadoop distribution AWS EC2 EMR S3 JIRA Oracle etc Hadoop Developer Sabre Dallas TX May 2015 to May 2016 Responsibilities Ingested Batch Files into HDFS using shell scripting Used flume to ingest nearrealtime data and perform necessary transformations and aggregations on the fly and persisted the data in Hive Used Hadoops Pig Hive and Map Reduce for analyzing the data and to help by extract data sets for meaningful information Building high resource intensive low latency and SLA stringent processes by establishing Data Ingestion from Data lake in Hadoop ETLELT jobs development using Scala API in Spark Spark SQl Hive QLs for few ETL activities Hive as operational data store HbaseTeradata as historicalproduction data Developed workflow in Oozie to orchestrate a series of Pig scripts to cleanse data such as removing irrelevant information or merging many small files into a handful of very large compressed files using pig pipelines in the data preparation stage Extensively used PIG to communicate with Hive using HCatalog Implemented exception tracking logic using Pig scripts Saved the analyzed data to the Hive Tables for visualization and to generate reports for the BI team Environment Hadoop Map Reduce HDFS Hive Pig Oozie Core Java Python Eclipse Flume Cloudera Oracle UNIX Shell Scripting JavaJ2EE Developer Bajaj Allianz Mumbai Maharashtra January 2014 to March 2015 Responsibilities Worked in SDLC methodology followed Waterfall environment including Acceptance Test Driven Design and Continuous IntegrationDelivery Responsible for analyzing designing developing coordinating and deploying web based application Developed the application using Spring MVC Framework that uses Model View Controller MVC architecture with JSP as the view Used Spring MVC for the management of application flow by developing configurable handler mappings view resolution Used Spring Framework to inject the DAO and Bean objects by auto wiring the components Developed front end applications using the HTML CSS JavaScript and JQuery Designed and developed XSLT transformation of components to convert data from XML to HTML Implemented the project using JAXWS based Web Services using WSDL UDDI and SOAP to communicate with other systems Monitored the error logs using Log4J Maven is used as a build tool and continuous integration is done using Jenkins Used complex queries like SQL statements and procedures to fetch the data from the database Used version control repository GIT and Service now for issue tracking Developed test cases and performed unit testing using Junit Test cases Used ANT as build tool and developed build file for compiling the code of creating WAR files Used Tortoise SVN for Source Control and Version Management Environment Java J2EE MVC JUnit JavaBeans HTML CSS JavaScript JQuery Oracle Hibernate SQL Soap Eclipse ANT Maven Software Developer Infor Hyderabad Telangana May 2013 to December 2014 Responsibilities Involved in daily scrums and weekly meeting with the project sponsors Designed and developed abstract classes interfaces classes to construct the business logic using Object Oriented Concepts Developed the classes using Java which incorporate N tier architecture and database connectivity Developed and tested userfriendly navigators by utilizing JavaScript and JQuery Performed Manual Unit Testing for all units in developed pages Designed and developed rich user interfaces with HTMLCSSJavaScriptBootstrap Involved in writing Stored procedures and Views as per the requirement Environment Java Spring Hibernate JSP JavaScript JQuery Tomcat Apache MySQL HTML CSS Bootstrap Eclipse IDE Maven MySQL workbench Eclipse IDE Apache SubversionSVN Junit JIRA Education Bachelors in information Technology in information Technology Gurunanak Institute of Technology Skills Dynamodb Cassandra Ambari Hdfs Impala Sqoop Hbase Kafka Data visualization Etl Flume Hadoop Map reduce Mongodb Nosql Avro Git Gradle Hadoop Hbase",
    "unique_id": "1beb6511-629d-496e-bfd6-7a9fc5dd27b8"
}