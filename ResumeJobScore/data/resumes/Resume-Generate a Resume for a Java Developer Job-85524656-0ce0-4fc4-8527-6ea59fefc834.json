{
    "clean_data": "Sr Hadoop Developer Sr Hadoop span lDeveloperspan Sr Hadoop Developer Directv Miami  Overall 8 years of professional IT experience in Software Development This also includes Extensive years of experience in Ingestion Storage Querying Processing and Analysis of Big Data using Hadooptechnologies and solutions Excellent understanding knowledge of Hadoop architecture and various components of Hadoopecosystem such as HDFS Job Tracker Task Tracker Name Node Data Node Map ReduceYARN Good understandingknowledge of Hadoop Architecture and various components such as HDFS JobTracker TaskTracker NameNode DataNode Secondry Namenode and MapReduce concepts Experienced managing NoSQL DB on large Hadoop distribution Systems such as Cloudera Hortonworks HDP MapR M series etc Experienced developing Hadoop integration for data ingestion data mapping and data process capabilities Experienced in building analytics for structured and unstructured data and managing large data ingestion using technologies like KafkaAvroThift Software development in Java Application Development ClientServer Applications InternetIntranet based database applications and developing testing and implementing application environment using C J2EE JDBC JSP Servlets Web Services Oracle PLSQL and Relational Databases Exceptional ability to quickly master new concepts and capable of working in groups as well as independently Excellent interpersonal skills and the ability to work as a part of a team Experience in debugging troubleshooting production systems profiling and identifying performance bottlenecks Has good knowledge of virtualization and worked on VMware Virtual Center Hands on experience in installing configuring and using Hadoop ecosystem components like Hadoop MapReduce HDFS HBase Hive Sqoop Pig Zookeeper Storm Spark Kafka and Flume Good Knowledge on Hadoop Cluster architecture and monitoring the cluster Indepth understanding of Data Structure and Algorithms Experience in managing and troubleshooting Hadoop related issues Expertise in setting up standards and processes for Hadoop based application design and implementation Experience in importing and exporting data using Sqoop from Relational Database Systems to HDFS and viceversa Experience in managing Hadoop clusters using Cloudera Manager Experience in using the Impala usage for the high performance SQL queries Very good experience in complete project life cycle design development testing and implementation of Client Server and Web applications Hands on experience in VPN Putty winSCP VNCviewer etc Hands on experience in application development using Java RDBMS and Linux shell scripting Performed data analysis using MySQL SQL Server Management Studio and Oracle Expertise in creating Conceptual Data Models ProcessData Flow Diagram Use Case Diagrams and State Diagrams Experience with cloud computing platforms like Amazon Web ServicesAWS Ability to adapt to evolving technology strong sense of responsibility and accomplishment Authorized to work in the US for any employer Work Experience Sr Hadoop Developer Directv Los Angeles CA May 2017 to Present Responsibilities Data Ingestion implemented using SQOOP SPARK loading data from various RDBMS CSV XML files Data cleansing transformations tasks are handled using SPARK using SCALA and HIVE Data Consolidation was implemented using SPARK HIVE to generate data in the required formats by applying various ETL tasks for data repair massaging data to identify source for audit purpose data filtering and store back to HDFS Responsible for design development of Spark SQL Scripts based on Functional Specifications Exploring with the Spark improving the Performance and Optimization of the existing algorithms in Hadoop ETL development to normalize this data and publish it in IMPALA Involved in converting HiveSQL queries into Spark RDD using Scala Responsible for Job management using Fair scheduler and Developed Job Processing scripts using Oozie Workflow Responsible for Performance Tuning of Spark Applications for setting right Batch Interval time correct level of Parallelism and Memory tuning Optimizing of existing algorithms in Hadoop using Spark Context SparkSQL and Pair RDDs Responsible in handling large datasets using Partitions Spark in Memory capabilities Broadcasts in Spark Effective efficient Joins Transformations and other during Ingestion process itself Importing and exporting data into HDFS and HIVE PIG using Sqoop Involved in creating Hive Tables loading with data and writing Hive queries which will invoke and run Map Reduce jobs in the backend Implemented the workflows using Apache Oozie framework to automate tasks Worked with No SQL databases like HBase Creating HBase tables to load large sets of semi structured data coming from various sources Worked with different file formats such as Text Sequence files Avro ORC and Parquet Responsible to manage data coming from different sources Responsible on loading and transforming of large sets of structured semi structured and unstructured data Analyzed large amounts of data sets to determine optimal way to aggregate and report on it Environment Scala Hive HBase Flume Java Impala Pig Spark Oozie Oracle Yarn Junit Unix Cloudera Flume Sqoop HDFS Java Python Hadoop Developer Market St San Francisco CA July 2015 to April 2017 Responsibilities Involved in file movements between HDFS and AWSS3 and extensively worked with S3 bucket inAWS Developing use cases for processing real time streaming data using tools like Spark Streaming Handled large datasets using Partitions Spark in Memory capabilities Broadcasts in Spark Effective efficient Joins Transformations Imported required tables from RDBMS to HDFS using Sqoop and used Spark and Kafka to get real time streaming of data into HBase Enhanced and optimized product Spark code to aggregate group and run data mining tasks using the Spark framework and handled Json Data Developed Spark code using Scala and SparkSQL for faster testing and data processing Responsible for batch processing of data sources using Apache Spark Developed predictive analytic using Apache Spark Scala APIs Developed MapReduce jobs in Java API to parse the raw data and store the refined data Developed Kafka producer and consumers Hbase clients Spark and Hadoop MapReduce jobs along with components on HDFS Hive Involved in identifying job dependencies to design workflow for Oozie YARN resource management Worked on a product team using Agile Scrum methodology to Design Develop Deploy and support solutions that leverage the Client big data platform Integrated Apache Storm with Kafka to perform web analytics Uploaded click stream data from Kafka to Hdfs Hbase and Hive by integrating with Storm Design and code from specifications Analyzes Evaluates Tests Debugs Documents and Implements Complex Software Apps Worked in tuning Hive Pig to improve performance and solved performance issues in both scripts with understanding of Joins Group and aggregation and how does it translate to Map Reduce jobs Created Partitions Buckets based on State to further process using Bucket based Hive joins Implemented Cloudera Manager on existing cluster Extensively worked with Cloudera Distribution Hadoop CDH 5x CDH4x Responsible for troubleshooting debugging and fixing the wrong data or data missing problem for Oracle Database Mysql Environment HDFS MapReduce JavaAPI JSP JavaBean Pig Hive Sqoop Flume Oozie HBase KafkaImpala Spark Streaming Storm Yarn Eclipse Unix Shell Scripting Cloudera Hadoop Developer ATT Atlanta GA April 2013 to June 2015 Responsibilities Involved in review of functional and nonfunctional requirements Facilitated knowledge transfer sessions Installed and configured Hadoop Mapreduce HDFS Developed multiple MapReduce jobs in java for data cleaning and preprocessing Importing and exporting data into HDFS and Hive using Sqoop Experienced in defining job flows Experienced in managing and reviewing Hadoop log files Extracted files from CouchDB through Sqoop and placed in HDFS and processed Experienced in running Hadoop streaming jobs to process terabytes of xml format data Load and transform large sets of structured semi structured and unstructured data Responsible to manage data coming from different sources Exploring with Spark improving the performance and optimization of the existing algorithms in Hadoop using Spark contextSparkSQL Data Frame pair RDDs Spark YARN Used Coalesce and repartition on data frames while optimizing the Spark jobs Developed Spark scripts by using Scala shell commands as per the requirement Experience in using Spark API over Cloudera Hadoop YARN to perform analytics on data in Hive Developed Scala scripts UDFFs using both Data framesSQL and RDDMapReduce in Spark 16 for Data Aggregation queries and writing data back into OLTP system through Sqoop and Developed enterprise application using scala as well Involved in creating Hive tables loading with data and writing hive queries which will run internally in map reduce way Performed advanced procedures like text analytics and processing using the inmemory computing capabilities of Spark using Scala Gained very good business knowledge on health insurance claim processing fraud suspect identification appeals process etc Developed a custom File System plug in for Hadoop so it can access files on Data Platform This plugin allows Hadoop MapReduce programs HBase Pig and Hive to work unmodified and access files directly Designed and implemented Mapreducebased largescale parallel relationlearning system Extracted feeds form social media sites such as Facebook Twitter using Python scripts Setup and benchmarked HadoopHBase clusters for internal use Environment Hadoop MapReduce HDFS Hive Sqoop HBase UNIX Shell ScriptingScala Java Developer Paycom Oklahoma City OK February 2011 to March 2013 Responsibilities Developed Map Reduce programs in Java for parsing the raw data and populating staging Worked on both WebLogic Portal 92 for Portal development and WebLogic 81 for Data Services Programming Used Eclipse 60 as IDE for application development Involved in writing test cases by using set of conditions to test the application Configured Struts framework to implement MVC design patterns Build sql queries for fetching the required columns and data from database Used Subversion as the version control system Managed the SVN related responsibilities and maintained the versions accordingly Done SVN check in and check outs Used Hibernate for handling database transactions and persisting objects Used AJAX for interactive user operations and client side validations Developed ANT script for compiling and deployment Performed unit testing using Junit Extensively used Log4j for logging the log files Environment JavaJ2EE SQL PLSQL JSP EJB Struts SVN JDBC XML XSLT UML JUnit System Engineer Morgan Stanley New York NY July 2009 to January 2011 Responsibilities Involved in Requirement Analysis Development and Documentation Participation in developing formbeans and action mappings required for struts implementation and validation framework using struts Development of frontend screens with JSP Using Eclipse Involved in Development of Medical Records module XML and XSDs are used to define data formats Involved in Bug fixing and functionality enhancements Designed and developed excellent Logging Mechanism for each order process using Log4J Involved in writing Oracle SQL Queries Involved in Checkin and Checkout process using CVS Developed additional functionality in the software as per business requirements Involved in requirement analysis and complete development of client side code Followed Sun standard coding and documentation standards Participation in project planning with business analysts and team members to analyze the Business requirements and translated business requirements into working software Developed software application modules using disciplined software development process Environment Java J2EE JSP EJB ANT STRUTS12 Log4J Web logic 70 JDBC MyEclipse Windows XP CVS Oracle Education Bachelor of Science in Information Systems Management University of Maryland Baltimore County Catonsville MD Skills Hadoop 5 years XML 7 years SQL 9 years ECLIPSE 5 years JAVA 9 years Additional Information Technical Skills Hadoop ECO Systems HDFS Map Reducing HDFS Oozie Hive Pig Sqoop Flume Zookeeper and HBase Cassandra NO SQL HBase Cassandra MongoDB Data Bases MS SQL Server 082012 MY SQL Oracle 9i10g Languages Languages Java JDK14 15 16 JDK 5 JDK 6 CC SQL PLSQL Operating Systems Windows Server 08 Windows XPVista Mac OS UNIX LINUX Java Technologies Servlets JavaBeans JDBC JNDI Frame Works JUnit and JTest IDEs Utilities Eclipse Maven NetBeans SQL Server Tools SQL Server Management Studio Enterprise Manager QueryAnalyser Profiler Export Import DTS WebDev Technologies ASPNET HTMLXML",
    "entities": [
        "Conceptual Data Models",
        "JobTracker",
        "Joins Group",
        "Used Hibernate",
        "Spark Context",
        "Spark SQL Scripts",
        "SPARK",
        "Oozie Workflow Responsible for Performance Tuning of Spark Applications",
        "Spark Effective",
        "HDFS Hive Involved",
        "Json Data Developed Spark",
        "Work Experience Sr Hadoop Developer Directv Los Angeles",
        "HDFS",
        "Developed Spark",
        "MySQL SQL Server Management Studio",
        "Data Structure and Algorithms",
        "QueryAnalyser Profiler Export Import DTS WebDev Technologies",
        "Functional Specifications Exploring",
        "Scala Gained",
        "Indepth",
        "Hadoop",
        "Sqoop Involved",
        "XML",
        "Hadoop MapReduce HDFS HBase Hive",
        "Integrated Apache Storm",
        "Created Partitions Buckets",
        "Data Services Programming Used",
        "WebLogic",
        "Hive Developed",
        "Shell",
        "CVS Developed",
        "Fair",
        "State",
        "Sr Hadoop Developer Sr Hadoop",
        "File System",
        "Amazon",
        "Cloudera Hadoop",
        "HBase Pig",
        "Participation",
        "SparkSQL",
        "Developed",
        "Developed Job Processing",
        "the Performance and Optimization",
        "Data Bases MS",
        "Spark Streaming Handled",
        "VMware Virtual Center Hands",
        "Hadoop MapReduce",
        "Relational Databases",
        "Client",
        "Parquet Responsible",
        "Utilities",
        "Hadoop Mapreduce HDFS Developed",
        "JSP",
        "IMPALA Involved",
        "Spark contextSparkSQL Data Frame",
        "HDP",
        "Oracle SQL Queries Involved",
        "MVC",
        "Oklahoma City",
        "Spark",
        "Java Application Development ClientServer Applications InternetIntranet",
        "US",
        "Sqoop",
        "UML JUnit System Engineer Morgan Stanley New York",
        "Oozie YARN",
        "Software Development This",
        "Data Aggregation",
        "Hadoop Architecture",
        "JSP JavaBean Pig Hive Sqoop",
        "HIVE Data Consolidation",
        "Checkin",
        "Design Develop",
        "HadoopHBase",
        "HDFS Job Tracker Task Tracker",
        "java",
        "HBase Creating HBase",
        "SQL",
        "CC SQL PLSQL",
        "Facilitated",
        "JTest",
        "OLTP",
        "Relational Database Systems",
        "Java Technologies",
        "Storm Design",
        "Hive",
        "Present Responsibilities Data Ingestion",
        "Oracle Database Mysql Environment HDFS MapReduce",
        "SQL Server Management Studio",
        "State Diagrams",
        "Partitions Spark",
        "SQOOP SPARK",
        "ETL",
        "Maven",
        "Development of Medical Records",
        "Performed",
        "Developed ANT",
        "Impala",
        "Windows XPVista Mac",
        "Cloudera Distribution Hadoop",
        "Skills Hadoop",
        "Additional Information Technical Skills Hadoop ECO Systems",
        "SVN",
        "Expertise",
        "ATT",
        "Data",
        "Environment Hadoop MapReduce HDFS Hive Sqoop HBase",
        "MapReduce",
        "NetBeans",
        "SCALA",
        "Scala Responsible for Job",
        "WebLogic Portal",
        "Oracle Expertise",
        "Node",
        "HBase Enhanced",
        "Followed Sun"
    ],
    "experience": "Experience in debugging troubleshooting production systems profiling and identifying performance bottlenecks Has good knowledge of virtualization and worked on VMware Virtual Center Hands on experience in installing configuring and using Hadoop ecosystem components like Hadoop MapReduce HDFS HBase Hive Sqoop Pig Zookeeper Storm Spark Kafka and Flume Good Knowledge on Hadoop Cluster architecture and monitoring the cluster Indepth understanding of Data Structure and Algorithms Experience in managing and troubleshooting Hadoop related issues Expertise in setting up standards and processes for Hadoop based application design and implementation Experience in importing and exporting data using Sqoop from Relational Database Systems to HDFS and viceversa Experience in managing Hadoop clusters using Cloudera Manager Experience in using the Impala usage for the high performance SQL queries Very good experience in complete project life cycle design development testing and implementation of Client Server and Web applications Hands on experience in VPN Putty winSCP VNCviewer etc Hands on experience in application development using Java RDBMS and Linux shell scripting Performed data analysis using MySQL SQL Server Management Studio and Oracle Expertise in creating Conceptual Data Models ProcessData Flow Diagram Use Case Diagrams and State Diagrams Experience with cloud computing platforms like Amazon Web ServicesAWS Ability to adapt to evolving technology strong sense of responsibility and accomplishment Authorized to work in the US for any employer Work Experience Sr Hadoop Developer Directv Los Angeles CA May 2017 to Present Responsibilities Data Ingestion implemented using SQOOP SPARK loading data from various RDBMS CSV XML files Data cleansing transformations tasks are handled using SPARK using SCALA and HIVE Data Consolidation was implemented using SPARK HIVE to generate data in the required formats by applying various ETL tasks for data repair massaging data to identify source for audit purpose data filtering and store back to HDFS Responsible for design development of Spark SQL Scripts based on Functional Specifications Exploring with the Spark improving the Performance and Optimization of the existing algorithms in Hadoop ETL development to normalize this data and publish it in IMPALA Involved in converting HiveSQL queries into Spark RDD using Scala Responsible for Job management using Fair scheduler and Developed Job Processing scripts using Oozie Workflow Responsible for Performance Tuning of Spark Applications for setting right Batch Interval time correct level of Parallelism and Memory tuning Optimizing of existing algorithms in Hadoop using Spark Context SparkSQL and Pair RDDs Responsible in handling large datasets using Partitions Spark in Memory capabilities Broadcasts in Spark Effective efficient Joins Transformations and other during Ingestion process itself Importing and exporting data into HDFS and HIVE PIG using Sqoop Involved in creating Hive Tables loading with data and writing Hive queries which will invoke and run Map Reduce jobs in the backend Implemented the workflows using Apache Oozie framework to automate tasks Worked with No SQL databases like HBase Creating HBase tables to load large sets of semi structured data coming from various sources Worked with different file formats such as Text Sequence files Avro ORC and Parquet Responsible to manage data coming from different sources Responsible on loading and transforming of large sets of structured semi structured and unstructured data Analyzed large amounts of data sets to determine optimal way to aggregate and report on it Environment Scala Hive HBase Flume Java Impala Pig Spark Oozie Oracle Yarn Junit Unix Cloudera Flume Sqoop HDFS Java Python Hadoop Developer Market St San Francisco CA July 2015 to April 2017 Responsibilities Involved in file movements between HDFS and AWSS3 and extensively worked with S3 bucket inAWS Developing use cases for processing real time streaming data using tools like Spark Streaming Handled large datasets using Partitions Spark in Memory capabilities Broadcasts in Spark Effective efficient Joins Transformations Imported required tables from RDBMS to HDFS using Sqoop and used Spark and Kafka to get real time streaming of data into HBase Enhanced and optimized product Spark code to aggregate group and run data mining tasks using the Spark framework and handled Json Data Developed Spark code using Scala and SparkSQL for faster testing and data processing Responsible for batch processing of data sources using Apache Spark Developed predictive analytic using Apache Spark Scala APIs Developed MapReduce jobs in Java API to parse the raw data and store the refined data Developed Kafka producer and consumers Hbase clients Spark and Hadoop MapReduce jobs along with components on HDFS Hive Involved in identifying job dependencies to design workflow for Oozie YARN resource management Worked on a product team using Agile Scrum methodology to Design Develop Deploy and support solutions that leverage the Client big data platform Integrated Apache Storm with Kafka to perform web analytics Uploaded click stream data from Kafka to Hdfs Hbase and Hive by integrating with Storm Design and code from specifications Analyzes Evaluates Tests Debugs Documents and Implements Complex Software Apps Worked in tuning Hive Pig to improve performance and solved performance issues in both scripts with understanding of Joins Group and aggregation and how does it translate to Map Reduce jobs Created Partitions Buckets based on State to further process using Bucket based Hive joins Implemented Cloudera Manager on existing cluster Extensively worked with Cloudera Distribution Hadoop CDH 5x CDH4x Responsible for troubleshooting debugging and fixing the wrong data or data missing problem for Oracle Database Mysql Environment HDFS MapReduce JavaAPI JSP JavaBean Pig Hive Sqoop Flume Oozie HBase KafkaImpala Spark Streaming Storm Yarn Eclipse Unix Shell Scripting Cloudera Hadoop Developer ATT Atlanta GA April 2013 to June 2015 Responsibilities Involved in review of functional and nonfunctional requirements Facilitated knowledge transfer sessions Installed and configured Hadoop Mapreduce HDFS Developed multiple MapReduce jobs in java for data cleaning and preprocessing Importing and exporting data into HDFS and Hive using Sqoop Experienced in defining job flows Experienced in managing and reviewing Hadoop log files Extracted files from CouchDB through Sqoop and placed in HDFS and processed Experienced in running Hadoop streaming jobs to process terabytes of xml format data Load and transform large sets of structured semi structured and unstructured data Responsible to manage data coming from different sources Exploring with Spark improving the performance and optimization of the existing algorithms in Hadoop using Spark contextSparkSQL Data Frame pair RDDs Spark YARN Used Coalesce and repartition on data frames while optimizing the Spark jobs Developed Spark scripts by using Scala shell commands as per the requirement Experience in using Spark API over Cloudera Hadoop YARN to perform analytics on data in Hive Developed Scala scripts UDFFs using both Data framesSQL and RDDMapReduce in Spark 16 for Data Aggregation queries and writing data back into OLTP system through Sqoop and Developed enterprise application using scala as well Involved in creating Hive tables loading with data and writing hive queries which will run internally in map reduce way Performed advanced procedures like text analytics and processing using the inmemory computing capabilities of Spark using Scala Gained very good business knowledge on health insurance claim processing fraud suspect identification appeals process etc Developed a custom File System plug in for Hadoop so it can access files on Data Platform This plugin allows Hadoop MapReduce programs HBase Pig and Hive to work unmodified and access files directly Designed and implemented Mapreducebased largescale parallel relationlearning system Extracted feeds form social media sites such as Facebook Twitter using Python scripts Setup and benchmarked HadoopHBase clusters for internal use Environment Hadoop MapReduce HDFS Hive Sqoop HBase UNIX Shell ScriptingScala Java Developer Paycom Oklahoma City OK February 2011 to March 2013 Responsibilities Developed Map Reduce programs in Java for parsing the raw data and populating staging Worked on both WebLogic Portal 92 for Portal development and WebLogic 81 for Data Services Programming Used Eclipse 60 as IDE for application development Involved in writing test cases by using set of conditions to test the application Configured Struts framework to implement MVC design patterns Build sql queries for fetching the required columns and data from database Used Subversion as the version control system Managed the SVN related responsibilities and maintained the versions accordingly Done SVN check in and check outs Used Hibernate for handling database transactions and persisting objects Used AJAX for interactive user operations and client side validations Developed ANT script for compiling and deployment Performed unit testing using Junit Extensively used Log4j for logging the log files Environment JavaJ2EE SQL PLSQL JSP EJB Struts SVN JDBC XML XSLT UML JUnit System Engineer Morgan Stanley New York NY July 2009 to January 2011 Responsibilities Involved in Requirement Analysis Development and Documentation Participation in developing formbeans and action mappings required for struts implementation and validation framework using struts Development of frontend screens with JSP Using Eclipse Involved in Development of Medical Records module XML and XSDs are used to define data formats Involved in Bug fixing and functionality enhancements Designed and developed excellent Logging Mechanism for each order process using Log4J Involved in writing Oracle SQL Queries Involved in Checkin and Checkout process using CVS Developed additional functionality in the software as per business requirements Involved in requirement analysis and complete development of client side code Followed Sun standard coding and documentation standards Participation in project planning with business analysts and team members to analyze the Business requirements and translated business requirements into working software Developed software application modules using disciplined software development process Environment Java J2EE JSP EJB ANT STRUTS12 Log4J Web logic 70 JDBC MyEclipse Windows XP CVS Oracle Education Bachelor of Science in Information Systems Management University of Maryland Baltimore County Catonsville MD Skills Hadoop 5 years XML 7 years SQL 9 years ECLIPSE 5 years JAVA 9 years Additional Information Technical Skills Hadoop ECO Systems HDFS Map Reducing HDFS Oozie Hive Pig Sqoop Flume Zookeeper and HBase Cassandra NO SQL HBase Cassandra MongoDB Data Bases MS SQL Server 082012 MY SQL Oracle 9i10 g Languages Languages Java JDK14 15 16 JDK 5 JDK 6 CC SQL PLSQL Operating Systems Windows Server 08 Windows XPVista Mac OS UNIX LINUX Java Technologies Servlets JavaBeans JDBC JNDI Frame Works JUnit and JTest IDEs Utilities Eclipse Maven NetBeans SQL Server Tools SQL Server Management Studio Enterprise Manager QueryAnalyser Profiler Export Import DTS WebDev Technologies ASPNET HTMLXML",
    "extracted_keywords": [
        "Sr",
        "Hadoop",
        "Developer",
        "Sr",
        "Hadoop",
        "span",
        "lDeveloperspan",
        "Sr",
        "Hadoop",
        "Developer",
        "Directv",
        "Miami",
        "Overall",
        "years",
        "IT",
        "experience",
        "Software",
        "Development",
        "This",
        "years",
        "experience",
        "Ingestion",
        "Storage",
        "Querying",
        "Processing",
        "Analysis",
        "Big",
        "Data",
        "Hadooptechnologies",
        "solutions",
        "Excellent",
        "knowledge",
        "Hadoop",
        "architecture",
        "components",
        "Hadoopecosystem",
        "HDFS",
        "Job",
        "Tracker",
        "Task",
        "Tracker",
        "Name",
        "Node",
        "Data",
        "Node",
        "Map",
        "ReduceYARN",
        "understandingknowledge",
        "Hadoop",
        "Architecture",
        "components",
        "HDFS",
        "JobTracker",
        "TaskTracker",
        "NameNode",
        "DataNode",
        "Secondry",
        "Namenode",
        "MapReduce",
        "concepts",
        "NoSQL",
        "DB",
        "Hadoop",
        "distribution",
        "Systems",
        "Cloudera",
        "Hortonworks",
        "HDP",
        "MapR",
        "M",
        "series",
        "Hadoop",
        "integration",
        "data",
        "ingestion",
        "data",
        "mapping",
        "data",
        "process",
        "capabilities",
        "building",
        "analytics",
        "data",
        "data",
        "ingestion",
        "technologies",
        "KafkaAvroThift",
        "Software",
        "development",
        "Java",
        "Application",
        "Development",
        "ClientServer",
        "Applications",
        "InternetIntranet",
        "database",
        "applications",
        "testing",
        "application",
        "environment",
        "C",
        "J2EE",
        "JDBC",
        "JSP",
        "Servlets",
        "Web",
        "Services",
        "Oracle",
        "PLSQL",
        "Relational",
        "ability",
        "concepts",
        "groups",
        "skills",
        "ability",
        "part",
        "team",
        "Experience",
        "troubleshooting",
        "production",
        "systems",
        "profiling",
        "performance",
        "bottlenecks",
        "knowledge",
        "virtualization",
        "VMware",
        "Virtual",
        "Center",
        "Hands",
        "experience",
        "configuring",
        "Hadoop",
        "ecosystem",
        "components",
        "Hadoop",
        "MapReduce",
        "HDFS",
        "HBase",
        "Hive",
        "Sqoop",
        "Pig",
        "Zookeeper",
        "Storm",
        "Spark",
        "Kafka",
        "Flume",
        "Good",
        "Knowledge",
        "Hadoop",
        "Cluster",
        "architecture",
        "cluster",
        "understanding",
        "Data",
        "Structure",
        "Algorithms",
        "Experience",
        "Hadoop",
        "issues",
        "Expertise",
        "standards",
        "processes",
        "Hadoop",
        "application",
        "design",
        "implementation",
        "Experience",
        "data",
        "Sqoop",
        "Relational",
        "Database",
        "Systems",
        "HDFS",
        "viceversa",
        "Experience",
        "Hadoop",
        "clusters",
        "Cloudera",
        "Manager",
        "Experience",
        "Impala",
        "usage",
        "performance",
        "SQL",
        "experience",
        "project",
        "life",
        "cycle",
        "design",
        "development",
        "testing",
        "implementation",
        "Client",
        "Server",
        "Web",
        "applications",
        "Hands",
        "experience",
        "VPN",
        "Putty",
        "winSCP",
        "VNCviewer",
        "Hands",
        "experience",
        "application",
        "development",
        "Java",
        "RDBMS",
        "Linux",
        "shell",
        "Performed",
        "data",
        "analysis",
        "MySQL",
        "SQL",
        "Server",
        "Management",
        "Studio",
        "Oracle",
        "Expertise",
        "Conceptual",
        "Data",
        "Models",
        "ProcessData",
        "Flow",
        "Diagram",
        "Use",
        "Case",
        "Diagrams",
        "State",
        "Diagrams",
        "Experience",
        "cloud",
        "computing",
        "platforms",
        "Amazon",
        "Web",
        "ServicesAWS",
        "Ability",
        "technology",
        "sense",
        "responsibility",
        "accomplishment",
        "US",
        "employer",
        "Work",
        "Experience",
        "Sr",
        "Hadoop",
        "Developer",
        "Directv",
        "Los",
        "Angeles",
        "CA",
        "May",
        "Present",
        "Responsibilities",
        "Data",
        "Ingestion",
        "SPARK",
        "loading",
        "data",
        "CSV",
        "XML",
        "Data",
        "cleansing",
        "transformations",
        "tasks",
        "SPARK",
        "SCALA",
        "HIVE",
        "Data",
        "Consolidation",
        "SPARK",
        "HIVE",
        "data",
        "formats",
        "ETL",
        "tasks",
        "data",
        "repair",
        "data",
        "source",
        "audit",
        "purpose",
        "data",
        "filtering",
        "store",
        "design",
        "development",
        "Spark",
        "SQL",
        "Scripts",
        "Functional",
        "Specifications",
        "Spark",
        "Performance",
        "Optimization",
        "algorithms",
        "Hadoop",
        "ETL",
        "development",
        "data",
        "IMPALA",
        "HiveSQL",
        "queries",
        "Spark",
        "RDD",
        "Scala",
        "Responsible",
        "Job",
        "management",
        "Fair",
        "scheduler",
        "Developed",
        "Job",
        "Processing",
        "scripts",
        "Oozie",
        "Workflow",
        "Performance",
        "Tuning",
        "Spark",
        "Applications",
        "Batch",
        "Interval",
        "time",
        "level",
        "Parallelism",
        "Memory",
        "tuning",
        "Optimizing",
        "algorithms",
        "Hadoop",
        "Spark",
        "Context",
        "SparkSQL",
        "Pair",
        "RDDs",
        "datasets",
        "Partitions",
        "Spark",
        "Memory",
        "capabilities",
        "Broadcasts",
        "Spark",
        "Effective",
        "Joins",
        "Transformations",
        "Ingestion",
        "process",
        "data",
        "HDFS",
        "HIVE",
        "PIG",
        "Sqoop",
        "Hive",
        "Tables",
        "data",
        "Hive",
        "queries",
        "Map",
        "Reduce",
        "jobs",
        "backend",
        "workflows",
        "Apache",
        "Oozie",
        "framework",
        "tasks",
        "SQL",
        "HBase",
        "HBase",
        "tables",
        "sets",
        "data",
        "sources",
        "file",
        "formats",
        "Text",
        "Sequence",
        "Avro",
        "ORC",
        "Parquet",
        "Responsible",
        "data",
        "sources",
        "loading",
        "transforming",
        "sets",
        "data",
        "amounts",
        "data",
        "sets",
        "way",
        "Environment",
        "Scala",
        "Hive",
        "HBase",
        "Flume",
        "Java",
        "Impala",
        "Pig",
        "Spark",
        "Oozie",
        "Oracle",
        "Yarn",
        "Junit",
        "Unix",
        "Cloudera",
        "Flume",
        "Sqoop",
        "HDFS",
        "Java",
        "Python",
        "Hadoop",
        "Developer",
        "Market",
        "St",
        "San",
        "Francisco",
        "CA",
        "July",
        "April",
        "Responsibilities",
        "file",
        "movements",
        "HDFS",
        "AWSS3",
        "S3",
        "bucket",
        "use",
        "cases",
        "time",
        "data",
        "tools",
        "Spark",
        "Streaming",
        "datasets",
        "Partitions",
        "Spark",
        "Memory",
        "capabilities",
        "Broadcasts",
        "Spark",
        "Effective",
        "Joins",
        "Transformations",
        "tables",
        "RDBMS",
        "HDFS",
        "Sqoop",
        "Spark",
        "Kafka",
        "time",
        "streaming",
        "data",
        "HBase",
        "Enhanced",
        "product",
        "Spark",
        "code",
        "group",
        "data",
        "mining",
        "tasks",
        "Spark",
        "framework",
        "Json",
        "Data",
        "Developed",
        "Spark",
        "code",
        "Scala",
        "SparkSQL",
        "testing",
        "data",
        "batch",
        "processing",
        "data",
        "sources",
        "Apache",
        "Spark",
        "analytic",
        "Apache",
        "Spark",
        "Scala",
        "APIs",
        "MapReduce",
        "jobs",
        "Java",
        "API",
        "data",
        "data",
        "Kafka",
        "producer",
        "consumers",
        "Hbase",
        "Spark",
        "Hadoop",
        "MapReduce",
        "jobs",
        "components",
        "HDFS",
        "Hive",
        "job",
        "dependencies",
        "workflow",
        "Oozie",
        "YARN",
        "resource",
        "management",
        "product",
        "team",
        "Agile",
        "Scrum",
        "methodology",
        "Design",
        "Develop",
        "Deploy",
        "solutions",
        "Client",
        "data",
        "platform",
        "Integrated",
        "Apache",
        "Storm",
        "Kafka",
        "web",
        "analytics",
        "stream",
        "data",
        "Kafka",
        "Hdfs",
        "Hbase",
        "Hive",
        "Storm",
        "Design",
        "code",
        "specifications",
        "Analyzes",
        "Evaluates",
        "Tests",
        "Debugs",
        "Documents",
        "Implements",
        "Complex",
        "Software",
        "Apps",
        "Hive",
        "Pig",
        "performance",
        "performance",
        "issues",
        "scripts",
        "understanding",
        "Joins",
        "Group",
        "aggregation",
        "Map",
        "jobs",
        "Partitions",
        "Buckets",
        "State",
        "process",
        "Bucket",
        "Hive",
        "Cloudera",
        "Manager",
        "cluster",
        "Cloudera",
        "Distribution",
        "Hadoop",
        "CDH",
        "data",
        "data",
        "problem",
        "Oracle",
        "Database",
        "Mysql",
        "Environment",
        "HDFS",
        "MapReduce",
        "JavaAPI",
        "JSP",
        "JavaBean",
        "Pig",
        "Hive",
        "Sqoop",
        "Flume",
        "Oozie",
        "HBase",
        "KafkaImpala",
        "Spark",
        "Streaming",
        "Storm",
        "Yarn",
        "Eclipse",
        "Unix",
        "Shell",
        "Scripting",
        "Cloudera",
        "Hadoop",
        "Developer",
        "ATT",
        "Atlanta",
        "GA",
        "April",
        "June",
        "Responsibilities",
        "review",
        "requirements",
        "knowledge",
        "transfer",
        "sessions",
        "Hadoop",
        "Mapreduce",
        "HDFS",
        "MapReduce",
        "jobs",
        "java",
        "data",
        "cleaning",
        "Importing",
        "data",
        "HDFS",
        "Hive",
        "Sqoop",
        "Experienced",
        "job",
        "flows",
        "Hadoop",
        "log",
        "files",
        "CouchDB",
        "Sqoop",
        "HDFS",
        "Hadoop",
        "streaming",
        "jobs",
        "terabytes",
        "xml",
        "format",
        "data",
        "Load",
        "sets",
        "data",
        "data",
        "sources",
        "Spark",
        "performance",
        "optimization",
        "algorithms",
        "Hadoop",
        "Spark",
        "contextSparkSQL",
        "Data",
        "Frame",
        "pair",
        "RDDs",
        "Spark",
        "YARN",
        "Coalesce",
        "repartition",
        "data",
        "frames",
        "Spark",
        "jobs",
        "Developed",
        "Spark",
        "scripts",
        "Scala",
        "shell",
        "commands",
        "requirement",
        "Experience",
        "Spark",
        "API",
        "Cloudera",
        "Hadoop",
        "YARN",
        "analytics",
        "data",
        "Hive",
        "Developed",
        "Scala",
        "UDFFs",
        "Data",
        "framesSQL",
        "RDDMapReduce",
        "Spark",
        "Data",
        "Aggregation",
        "data",
        "OLTP",
        "system",
        "Sqoop",
        "enterprise",
        "application",
        "scala",
        "Hive",
        "tables",
        "data",
        "hive",
        "queries",
        "map",
        "way",
        "Performed",
        "procedures",
        "text",
        "analytics",
        "processing",
        "inmemory",
        "computing",
        "capabilities",
        "Spark",
        "Scala",
        "business",
        "knowledge",
        "health",
        "insurance",
        "claim",
        "processing",
        "fraud",
        "suspect",
        "identification",
        "appeals",
        "process",
        "custom",
        "File",
        "System",
        "plug",
        "Hadoop",
        "files",
        "Data",
        "Platform",
        "plugin",
        "Hadoop",
        "MapReduce",
        "programs",
        "HBase",
        "Pig",
        "Hive",
        "access",
        "files",
        "Mapreducebased",
        "largescale",
        "system",
        "media",
        "sites",
        "Facebook",
        "Twitter",
        "Python",
        "scripts",
        "Setup",
        "HadoopHBase",
        "clusters",
        "use",
        "Environment",
        "Hadoop",
        "MapReduce",
        "HDFS",
        "Hive",
        "Sqoop",
        "HBase",
        "UNIX",
        "Shell",
        "ScriptingScala",
        "Java",
        "Developer",
        "Paycom",
        "Oklahoma",
        "City",
        "OK",
        "February",
        "March",
        "Responsibilities",
        "Map",
        "programs",
        "Java",
        "data",
        "staging",
        "WebLogic",
        "Portal",
        "Portal",
        "development",
        "WebLogic",
        "Data",
        "Services",
        "Programming",
        "Eclipse",
        "IDE",
        "application",
        "development",
        "test",
        "cases",
        "set",
        "conditions",
        "application",
        "Configured",
        "Struts",
        "framework",
        "MVC",
        "design",
        "patterns",
        "Build",
        "sql",
        "queries",
        "columns",
        "data",
        "database",
        "Subversion",
        "version",
        "control",
        "system",
        "SVN",
        "responsibilities",
        "versions",
        "SVN",
        "check",
        "outs",
        "Hibernate",
        "database",
        "transactions",
        "objects",
        "AJAX",
        "user",
        "operations",
        "client",
        "side",
        "ANT",
        "script",
        "deployment",
        "Performed",
        "unit",
        "testing",
        "Junit",
        "Log4j",
        "log",
        "files",
        "Environment",
        "JavaJ2EE",
        "SQL",
        "PLSQL",
        "JSP",
        "EJB",
        "Struts",
        "SVN",
        "JDBC",
        "XML",
        "XSLT",
        "UML",
        "JUnit",
        "System",
        "Engineer",
        "Morgan",
        "Stanley",
        "New",
        "York",
        "NY",
        "July",
        "January",
        "Responsibilities",
        "Requirement",
        "Analysis",
        "Development",
        "Documentation",
        "Participation",
        "formbeans",
        "action",
        "mappings",
        "struts",
        "implementation",
        "validation",
        "framework",
        "struts",
        "Development",
        "frontend",
        "screens",
        "JSP",
        "Eclipse",
        "Development",
        "Medical",
        "Records",
        "module",
        "XML",
        "XSDs",
        "data",
        "formats",
        "Bug",
        "fixing",
        "functionality",
        "enhancements",
        "Logging",
        "Mechanism",
        "order",
        "process",
        "Log4J",
        "Oracle",
        "SQL",
        "Queries",
        "Checkin",
        "Checkout",
        "process",
        "CVS",
        "functionality",
        "software",
        "business",
        "requirements",
        "requirement",
        "analysis",
        "development",
        "client",
        "side",
        "code",
        "Sun",
        "coding",
        "documentation",
        "standards",
        "Participation",
        "project",
        "business",
        "analysts",
        "team",
        "members",
        "Business",
        "requirements",
        "business",
        "requirements",
        "software",
        "software",
        "application",
        "modules",
        "software",
        "development",
        "process",
        "Environment",
        "Java",
        "J2EE",
        "JSP",
        "EJB",
        "ANT",
        "STRUTS12",
        "Log4J",
        "Web",
        "logic",
        "JDBC",
        "MyEclipse",
        "Windows",
        "XP",
        "CVS",
        "Oracle",
        "Education",
        "Bachelor",
        "Science",
        "Information",
        "Systems",
        "Management",
        "University",
        "Maryland",
        "Baltimore",
        "County",
        "Catonsville",
        "MD",
        "Skills",
        "Hadoop",
        "years",
        "XML",
        "years",
        "SQL",
        "years",
        "years",
        "years",
        "Additional",
        "Information",
        "Technical",
        "Skills",
        "Hadoop",
        "ECO",
        "Systems",
        "HDFS",
        "Map",
        "HDFS",
        "Oozie",
        "Hive",
        "Pig",
        "Sqoop",
        "Flume",
        "Zookeeper",
        "HBase",
        "Cassandra",
        "SQL",
        "HBase",
        "Cassandra",
        "MongoDB",
        "Data",
        "Bases",
        "MS",
        "SQL",
        "Server",
        "MY",
        "SQL",
        "Oracle",
        "9i10",
        "g",
        "Languages",
        "Languages",
        "Java",
        "JDK14",
        "JDK",
        "JDK",
        "CC",
        "SQL",
        "PLSQL",
        "Operating",
        "Systems",
        "Windows",
        "Server",
        "Windows",
        "XPVista",
        "Mac",
        "OS",
        "UNIX",
        "LINUX",
        "Java",
        "Technologies",
        "Servlets",
        "JavaBeans",
        "JDBC",
        "JNDI",
        "Frame",
        "JUnit",
        "JTest",
        "IDEs",
        "Utilities",
        "Eclipse",
        "Maven",
        "NetBeans",
        "SQL",
        "Server",
        "Tools",
        "SQL",
        "Server",
        "Management",
        "Studio",
        "Enterprise",
        "Manager",
        "QueryAnalyser",
        "Profiler",
        "Export",
        "Import",
        "DTS",
        "WebDev",
        "Technologies",
        "ASPNET",
        "HTMLXML"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T20:12:23.690367",
    "resume_data": "Sr Hadoop Developer Sr Hadoop span lDeveloperspan Sr Hadoop Developer Directv Miami FL Overall 8 years of professional IT experience in Software Development This also includes Extensive years of experience in Ingestion Storage Querying Processing and Analysis of Big Data using Hadooptechnologies and solutions Excellent understanding knowledge of Hadoop architecture and various components of Hadoopecosystem such as HDFS Job Tracker Task Tracker Name Node Data Node Map ReduceYARN Good understandingknowledge of Hadoop Architecture and various components such as HDFS JobTracker TaskTracker NameNode DataNode Secondry Namenode and MapReduce concepts Experienced managing NoSQL DB on large Hadoop distribution Systems such as Cloudera Hortonworks HDP MapR M series etc Experienced developing Hadoop integration for data ingestion data mapping and data process capabilities Experienced in building analytics for structured and unstructured data and managing large data ingestion using technologies like KafkaAvroThift Software development in Java Application Development ClientServer Applications InternetIntranet based database applications and developing testing and implementing application environment using C J2EE JDBC JSP Servlets Web Services Oracle PLSQL and Relational Databases Exceptional ability to quickly master new concepts and capable of working in groups as well as independently Excellent interpersonal skills and the ability to work as a part of a team Experience in debugging troubleshooting production systems profiling and identifying performance bottlenecks Has good knowledge of virtualization and worked on VMware Virtual Center Hands on experience in installing configuring and using Hadoop ecosystem components like Hadoop MapReduce HDFS HBase Hive Sqoop Pig Zookeeper Storm Spark Kafka and Flume Good Knowledge on Hadoop Cluster architecture and monitoring the cluster Indepth understanding of Data Structure and Algorithms Experience in managing and troubleshooting Hadoop related issues Expertise in setting up standards and processes for Hadoop based application design and implementation Experience in importing and exporting data using Sqoop from Relational Database Systems to HDFS and viceversa Experience in managing Hadoop clusters using Cloudera Manager Experience in using the Impala usage for the high performance SQL queries Very good experience in complete project life cycle design development testing and implementation of Client Server and Web applications Hands on experience in VPN Putty winSCP VNCviewer etc Hands on experience in application development using Java RDBMS and Linux shell scripting Performed data analysis using MySQL SQL Server Management Studio and Oracle Expertise in creating Conceptual Data Models ProcessData Flow Diagram Use Case Diagrams and State Diagrams Experience with cloud computing platforms like Amazon Web ServicesAWS Ability to adapt to evolving technology strong sense of responsibility and accomplishment Authorized to work in the US for any employer Work Experience Sr Hadoop Developer Directv Los Angeles CA May 2017 to Present Responsibilities Data Ingestion implemented using SQOOP SPARK loading data from various RDBMS CSV XML files Data cleansing transformations tasks are handled using SPARK using SCALA and HIVE Data Consolidation was implemented using SPARK HIVE to generate data in the required formats by applying various ETL tasks for data repair massaging data to identify source for audit purpose data filtering and store back to HDFS Responsible for design development of Spark SQL Scripts based on Functional Specifications Exploring with the Spark improving the Performance and Optimization of the existing algorithms in Hadoop ETL development to normalize this data and publish it in IMPALA Involved in converting HiveSQL queries into Spark RDD using Scala Responsible for Job management using Fair scheduler and Developed Job Processing scripts using Oozie Workflow Responsible for Performance Tuning of Spark Applications for setting right Batch Interval time correct level of Parallelism and Memory tuning Optimizing of existing algorithms in Hadoop using Spark Context SparkSQL and Pair RDDs Responsible in handling large datasets using Partitions Spark in Memory capabilities Broadcasts in Spark Effective efficient Joins Transformations and other during Ingestion process itself Importing and exporting data into HDFS and HIVE PIG using Sqoop Involved in creating Hive Tables loading with data and writing Hive queries which will invoke and run Map Reduce jobs in the backend Implemented the workflows using Apache Oozie framework to automate tasks Worked with No SQL databases like HBase Creating HBase tables to load large sets of semi structured data coming from various sources Worked with different file formats such as Text Sequence files Avro ORC and Parquet Responsible to manage data coming from different sources Responsible on loading and transforming of large sets of structured semi structured and unstructured data Analyzed large amounts of data sets to determine optimal way to aggregate and report on it Environment Scala Hive HBase Flume Java Impala Pig Spark Oozie Oracle Yarn Junit Unix Cloudera Flume Sqoop HDFS Java Python Hadoop Developer Market St San Francisco CA July 2015 to April 2017 Responsibilities Involved in file movements between HDFS and AWSS3 and extensively worked with S3 bucket inAWS Developing use cases for processing real time streaming data using tools like Spark Streaming Handled large datasets using Partitions Spark in Memory capabilities Broadcasts in Spark Effective efficient Joins Transformations Imported required tables from RDBMS to HDFS using Sqoop and used Spark and Kafka to get real time streaming of data into HBase Enhanced and optimized product Spark code to aggregate group and run data mining tasks using the Spark framework and handled Json Data Developed Spark code using Scala and SparkSQL for faster testing and data processing Responsible for batch processing of data sources using Apache Spark Developed predictive analytic using Apache Spark Scala APIs Developed MapReduce jobs in Java API to parse the raw data and store the refined data Developed Kafka producer and consumers Hbase clients Spark and Hadoop MapReduce jobs along with components on HDFS Hive Involved in identifying job dependencies to design workflow for Oozie YARN resource management Worked on a product team using Agile Scrum methodology to Design Develop Deploy and support solutions that leverage the Client big data platform Integrated Apache Storm with Kafka to perform web analytics Uploaded click stream data from Kafka to Hdfs Hbase and Hive by integrating with Storm Design and code from specifications Analyzes Evaluates Tests Debugs Documents and Implements Complex Software Apps Worked in tuning Hive Pig to improve performance and solved performance issues in both scripts with understanding of Joins Group and aggregation and how does it translate to Map Reduce jobs Created Partitions Buckets based on State to further process using Bucket based Hive joins Implemented Cloudera Manager on existing cluster Extensively worked with Cloudera Distribution Hadoop CDH 5x CDH4x Responsible for troubleshooting debugging and fixing the wrong data or data missing problem for Oracle Database Mysql Environment HDFS MapReduce JavaAPI JSP JavaBean Pig Hive Sqoop Flume Oozie HBase KafkaImpala Spark Streaming Storm Yarn Eclipse Unix Shell Scripting Cloudera Hadoop Developer ATT Atlanta GA April 2013 to June 2015 Responsibilities Involved in review of functional and nonfunctional requirements Facilitated knowledge transfer sessions Installed and configured Hadoop Mapreduce HDFS Developed multiple MapReduce jobs in java for data cleaning and preprocessing Importing and exporting data into HDFS and Hive using Sqoop Experienced in defining job flows Experienced in managing and reviewing Hadoop log files Extracted files from CouchDB through Sqoop and placed in HDFS and processed Experienced in running Hadoop streaming jobs to process terabytes of xml format data Load and transform large sets of structured semi structured and unstructured data Responsible to manage data coming from different sources Exploring with Spark improving the performance and optimization of the existing algorithms in Hadoop using Spark contextSparkSQL Data Frame pair RDDs Spark YARN Used Coalesce and repartition on data frames while optimizing the Spark jobs Developed Spark scripts by using Scala shell commands as per the requirement Experience in using Spark API over Cloudera Hadoop YARN to perform analytics on data in Hive Developed Scala scripts UDFFs using both Data framesSQL and RDDMapReduce in Spark 16 for Data Aggregation queries and writing data back into OLTP system through Sqoop and Developed enterprise application using scala as well Involved in creating Hive tables loading with data and writing hive queries which will run internally in map reduce way Performed advanced procedures like text analytics and processing using the inmemory computing capabilities of Spark using Scala Gained very good business knowledge on health insurance claim processing fraud suspect identification appeals process etc Developed a custom File System plug in for Hadoop so it can access files on Data Platform This plugin allows Hadoop MapReduce programs HBase Pig and Hive to work unmodified and access files directly Designed and implemented Mapreducebased largescale parallel relationlearning system Extracted feeds form social media sites such as Facebook Twitter using Python scripts Setup and benchmarked HadoopHBase clusters for internal use Environment Hadoop MapReduce HDFS Hive Sqoop HBase UNIX Shell ScriptingScala Java Developer Paycom Oklahoma City OK February 2011 to March 2013 Responsibilities Developed Map Reduce programs in Java for parsing the raw data and populating staging Worked on both WebLogic Portal 92 for Portal development and WebLogic 81 for Data Services Programming Used Eclipse 60 as IDE for application development Involved in writing test cases by using set of conditions to test the application Configured Struts framework to implement MVC design patterns Build sql queries for fetching the required columns and data from database Used Subversion as the version control system Managed the SVN related responsibilities and maintained the versions accordingly Done SVN check in and check outs Used Hibernate for handling database transactions and persisting objects Used AJAX for interactive user operations and client side validations Developed ANT script for compiling and deployment Performed unit testing using Junit Extensively used Log4j for logging the log files Environment JavaJ2EE SQL PLSQL JSP EJB Struts SVN JDBC XML XSLT UML JUnit System Engineer Morgan Stanley New York NY July 2009 to January 2011 Responsibilities Involved in Requirement Analysis Development and Documentation Participation in developing formbeans and action mappings required for struts implementation and validation framework using struts Development of frontend screens with JSP Using Eclipse Involved in Development of Medical Records module XML and XSDs are used to define data formats Involved in Bug fixing and functionality enhancements Designed and developed excellent Logging Mechanism for each order process using Log4J Involved in writing Oracle SQL Queries Involved in Checkin and Checkout process using CVS Developed additional functionality in the software as per business requirements Involved in requirement analysis and complete development of client side code Followed Sun standard coding and documentation standards Participation in project planning with business analysts and team members to analyze the Business requirements and translated business requirements into working software Developed software application modules using disciplined software development process Environment Java J2EE JSP EJB ANT STRUTS12 Log4J Web logic 70 JDBC MyEclipse Windows XP CVS Oracle Education Bachelor of Science in Information Systems Management University of Maryland Baltimore County Catonsville MD Skills Hadoop 5 years XML 7 years SQL 9 years ECLIPSE 5 years JAVA 9 years Additional Information Technical Skills Hadoop ECO Systems HDFS Map Reducing HDFS Oozie Hive Pig Sqoop Flume Zookeeper and HBase Cassandra NO SQL HBase Cassandra MongoDB Data Bases MS SQL Server 2000200520082012 MY SQL Oracle 9i10g Languages Languages Java JDK14 15 16 JDK 5 JDK 6 CC SQL PLSQL Operating Systems Windows Server 200020032008 Windows XPVista Mac OS UNIX LINUX Java Technologies Servlets JavaBeans JDBC JNDI Frame Works JUnit and JTest IDEs Utilities Eclipse Maven NetBeans SQL Server Tools SQL Server Management Studio Enterprise Manager QueryAnalyser Profiler Export Import DTS WebDev Technologies ASPNET HTMLXML",
    "unique_id": "85524656-0ce0-4fc4-8527-6ea59fefc834"
}