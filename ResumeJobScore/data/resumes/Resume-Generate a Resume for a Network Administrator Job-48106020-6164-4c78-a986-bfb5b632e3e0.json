{
    "clean_data": "Sr Big Data Hadoop Developer Sr Big Data Hadoop span lDeveloperspan Sr Big Data Hadoop Developer ADP Florhan Edison NJ Above 9 years of experience as Big Data Engineer Analysis Design Development Deployment and Maintenance of software in JavaJ2EE technologies and BigData applications Expertise in Data Development in Hortonworks HDP platform Hadoop ecosystem tools like Hadoop HDFS Spark Zeppelin Hive HBase Sqoop flume Atlas SOLR Pig Falcon Oozie Hue Tez Apache NiFi Kafka Built streaming applications using SPARK Streaming Knowledge on bigdata database HBase and NoSQL databases Mongo DB and Cassandra Expertise in Java Script JavaScript MVC patterns Object Oriented JavaScript Design Patterns and AJAX calls Experience includes Requirements Gathering Design Development Integration Documentation Testing and Build Experience in working with MapReduce programs Pig Scripts and Hive commands to deliver the best results Good Knowledge in Amazon Web Service AWS concepts like EMR and EC2 web services which provides fast and efficient processing of Teradata Big Data Analytics Experienced in collection of Log Data and JSON data into HDFS using Flume and processed the data using HivePig Strong knowledge and experience in Object Oriented Programming using Java Extensively worked on development and optimization of MapReduce programs PIG Scripts and HIVE queries to create structured data for data mining Expertise in developing the presentation layer components like HTML CSS JavaScript JQuery XML JSON AJAX and D3 Good knowledge of coding using SQL SQL Plus TSQL PLSQL Stored ProceduresFunctions Worked on Bootstrap Angular JS and Node JS knockout ember Java Persistence Architecture JPA Hands on experience in advanced BigData technologies like Spark Ecosystem Spark SQL MLlib SparkR and Spark Streaming Kafka and Predictive analytics Knowledge of the software Development Life Cycle SDLC Agile and Waterfall Methodologies Strong experience in developing Enterprise and Web applications on ntier architecture using Java J2EE based technologies such as Servlets JSP Spring Hibernate Struts EJBs Web Services XML JPA JMS JNDI and JDBC Developed applications based on ModelViewController MVC Working knowledge on Oozie a workflow scheduler system to manage the jobs that run on PIG HIVE and SQOOP Expertise in developing test cases for Unit testing Integration testing and System testing Extensively development experience in different IDE like Eclipse Net Beans IntelliJ and STS Experienced with programming language such as C C Xpath Core Java and JavaScript Good experience in Installing Upgrading and Configuring Redhat Linux using Kickstart Servers and Interactive Installation Good experience in Tableau for Data Visualization and analysis on large data sets drawing various conclusions Extensive experience in building and deploying applications on WebApplication Servers like Web logic Web sphere and Tomcat Expertise in core Java J2EE Multithreading JDBC Hibernate Spring Shell Scripting and proficient in using Java APIs for application development Good at problemsolving skills to identify areas of improvement and incorporating best practices for delivering quality deliverables Have good experience excellent communication and interpersonal skills which contribute to timely completion of project deliverable well ahead of schedule Authorized to work in the US for any employer Work Experience Sr Big Data Hadoop Developer ADP Florhan Park NJ US July 2018 to Present Responsibilities Coordinated with business customers to gather business requirements And also interact with other technical peers to derive Technical requirements and delivered the BRD and TDD documents Implement Big Data systems in distributed cloud environment AWS using Amazon EMR Extensively involved in Design phase and delivered Design documents Experience in Hadoop eco system with HDFS HIVE PIG SQOOP and SPARK with SCALA Worked on analyzing Hadoop cluster and different Big Data Components including Pig Hive Spark HBase Kafka Elastic Search database and SQOOP Installed Hadoop Map Reduce HDFS and developed multiple MapReduce jobs in PIG and Hive for data cleaning and preprocessing Importing and exporting data into HDFS and Hive using SQOOP Migration of 100 TBs of data from different databases ie Netezza Oracle SQL Server to Hadoop Wring code in different applications of Hadoop Ecosystem to achieve the required output in a sprint time period Written Hive jobs to parse the logs and structure them in tabular format to facilitate effective querying on the log data Involved in creating Hive tables loading with data and writing Hive queries that will run internally in map reduce way Generate OBIEE reports to verify the Hive tables data Experienced in defining job flows Used Hive to analyze the partitioned and bucketed data and compute various metrics for reporting Experienced in managing and reviewing the Hadoop log files Used Pig as ETL tool to do Transformations with joins and preaggregations before storing the dataonto HDFS Responsible to develop data pipelines from different sources Wrote Hive and Impala queries to load and processing data in Hadoop File system Utilized Apache Hadoop environment by Cloud era Distribution Exported data from HDFS environment into RDBMS using Sqoop for report generation and visualization purpose Worked on Oozie workflow engine for job scheduling Involved in Unit testing and delivered Unit test plans and results documents Environment Hadoop MapReduce HDFS Hive Pig Hue Ganglia Nagios Java Kafka Elastic Search SQL Scala Oracle Netezza Ambari Sqoop Flume Oozie Java jdk 16 Eclipse Service Controller Disney Orlando FL June 2016 to July 2018 Orlando FL Jun2016Jul2018 Full Stack Java Devoloper Responsibilities Done the design development and testing phases of Software Development using AGILE methodology and TestDriven Development TDD Designed the application using Front Controller Service Controller MVC Spring DAO Factory Data Access Object Service Locator and Session Facade Design Patterns Involved in development of the applications using Spring Web MVC and other components of the Spring Framework the controller being Spring 30 Core Dispatcher Servlet Used Hibernate for Object relational model for handling server side database object data Work closely with our partners and clients to develop and support ongoing API integrations Used SOAP and REST based web service protocol Bootstrap used along with Angular 2 and EcmaScript7 with TypeScript in creating the Application Provide leadership in developing the companys IT software and ongoing strategy for internal and external purposes Ensure Client Subsidiaries are in compliance with all domestic and international data privacy lAWS ensuring that the flow of confidential data is secured and in compliance with local jurisdictions Developed Linux bash MS DOS Scripts for internal use Used PostgreSQL as back end and developed Stored procedures Batch jobs triggers Used Jenkins to perform software build with Gradle run shell Scripts and worked on integration tests Develop customized reports for clients or internal customers using Pentaho ETL tools Utilized React for its efficient data flow architecture to create a lightweight and render efficient web app that searched projects via the GitHub API through keywords Designed Frontend with in object oriented JavaScript Framework like Bootstrap Nodejs Expressjs and Angularjs Redux Wrote a Python module to connect and view the status of an Apache Cassandra instance Implemented log4j by enabling logging at runtime without modifying the application binary Developed Mean Stack from scratch including Mongo DB server setup and Express JS server development Worked in using React JS components Forms Events Keys Router Animations and Flux concept Used popular Nodejs frameworks like Express and RESTify to create a RESTful Mock API Provide estimates designs and specifications for AEM templates components and workflows Environment MVC Factory Session Facade Design Patterns Spring SOAP RESTful web services Angularjs Linux bash MS DOS Hibernate PostgreSQL Dynatrace Git Github Bootstrap Nodejs log4j Rally AWS Big data architect Turner Broadcasting Atlanta GA May 2014 to June 2016 Responsibilities Provide technical leadership and contribute to the definition development integration test documentation and support across multiple platforms Design architected and implemented complex projects dealing with the considerable data size GB PB and with high complexity Provide deployment solutions based on customer needs with Sound knowledge about the clustered deployment architecture Able to guide partner with VP Directors for architecting solutions for the Big data Organization Created detailed AWS Security groups which behaved as virtual firewalls that controlled the traffic allowed reaching one or more AWS EC2 instances Data modeling Design implement and deploy highperformance custom applications at scale on Hadoop Spark Data processing with MapReduce and Spark Stream processing on Sparkstorm thru Kafka message broker Review and audit of existing solution design and system architecture Perform profiling troubleshooting of existing solutions Create technical and designing documentation Creation of a User Interface to search andor view content within the cluster by using solar cloud Worked on AWS provisioning EC2 Infrastructure and deploying applications in Elastic load balancing Cluster management and analytic in Cloudera and Horton work Distributed database Design Data modeling Development and Support in Datastax Cassandra distribution Cassandra products strengths and weakness to produce efficient schema designs that serves effective and high performance queries Maintain and work with our data pipeline that transfers and processes several terabytes of data using Spark Scala Python Apache Kafka Pig Hive Impala Apply data analysis data mining and data engineering to present data clearly Ensure highquality data and understand how data is generated out experimental design and how these experiments can produce actionable trustworthy conclusions Full life cycle of Data Lake Data Warehouse with Big data technologies like Spark Hadoop Cassandra Working with Spark RDD Data Frames Data Pipelines Building complex ETLs Data Warehousing or custom pipelines from multiple data sources Setting up connector for security logs and Splunk data use cases Building the Hadoop cluster MTS to host the three use cases Analyzing the data using Tableau Extract and analysis the data before load into cluster Review and understand data architecture data models Source to target mapping rules and Match and merge rules Evaluate Hadoop infrastructure requirements and designdeploy solutions high availability big data clusters elastic load tolerance etc Hadoop ecosystem components in our open source infrastructure stack specifically HBase HDFS MapReduce Yarn Oozie Pig Hive Kafka Storm Spark SparkSQL and Flume Estimate and obtain management support for the time resources and budget required to perform in different projects Keep track of the new requirements change in requirements of the Project Understand Inbound and outbound data flow requirements data models for Landing Staging and base objects Mapping documents Match and Merge rules Proof of Concept POC and Proof of TechnologyPOT execution and evaluation on MTS platforms Installing and Configuring required ecosystem tools for each use case Environment Big Data Spark YARN HIVE Pig Scala Python Hadoop AWS Dynamo DB Kibana Cloudera EMR JDBC Redshift NOSQL Sqoop MYSQL Sr Java Developer Actionet Germantown MD April 2011 to May 2014 Responsibilities Worked on the existing application wireframes FDN and BRD documents to get the requirements and analyzed Handson Experience with Cassandra to provide Scalability along with NoSQL Developed Agile processes using Groovy JUnit to use continuous integration Integrated Automated functional tests Groovy with ContinuousIntegration in Jenkins Parse requests and built response data using Groovys JSON tools and Grails web services Imported data from various resources to the Cassandra cluster using Java APIs Used Eclipse SWT for developing the applications Involved in preparation of TSD documents using UML diagrams Class Sequence and Use case diagrams using Microsoft VISIO tool Wrote RESTful services on the server in NodeJS to listen to requests from devices Built a Grails web application that allows admin users to manage detailed data for all types of Target locations Have worked with Standard Widget Toolkit SWT Conversion of major Openworks components in to Eclipse RCPSWT platform along with support of SwingSWT components Involved in to develop view pages of desktop portal using HTML Java Script JSP Struts Tag libraries AJAX JQUERY GWT DOJO XML and XSLT Developed and deployed Web services to interact with partner interfaces and client interfaces to consume the web services using CXF WSDL SOAP AXIS and JAXWS technologies Integrating third party libraries to augment those lacking or inefficient in ExtJS Used RESTful web services using JERSEY tool to develop web services easily and to be invoked by different channels Developed service objects as beans by using Spring IOCDI Developed Web API using NodeJS and hosted on multiple load balanced API instances Implementation of enterprise application with jQuery angularJS nodejs and SpringMVC Used Spring Beans to encapsulate business logic and Implemented Application MVC Architecture using Spring MVC framework Implemented Hibernate ORM Mapping tool framework to interact with the database to update retrieve insert and delete values effectively Used Java Swing for few components in accordance with SWT application with multithreading environment with Concurrency and Java Collections Used EH Cache for second level cache in Hibernate for the application Involved in to pass messages like payload to track different statuses and milestones using EJB JMS Involved in unit testing integration testing SOAP UI testing smoketesting system testing and user acceptance testing of the application Used Spring programmatic transaction management for Java Persistence Involved in integration of Spring and Hibernate frameworks Involved in setting server properties DSs JNDI queues deploying app in WebSphere Application Server Followed the test driven development using the JUNIT and Mockito framework Created continuous integration builds using Maven Involved in fixing QAUATProduction issues and tracked them using QC Environment Java JSP Servlets JavaScript Spring DI Spring IOC Spring AOP Hibernate 30 AJAX XML XSLT JAXP JAXB AXIS CSS CXF WSDL Java Developer Capital One Bank Plano TX March 2010 to April 2011 Responsabilites Developed Controller Servlets and Action Servlets to handle the request and responses Developed and coordinated complex high quality solutions to clients using IBM ProductsTools Apache Tools J2SE J2EE EJB Servlets JSP HTML JavaScript JQuery JSON and XML Developing the web applications using Spring Framework Hibernate Applying Spring Framework for transaction Management and Spring JDBC for building ORM and for AOP and Dependency Injection Responsible for using AJAX framework with JQuery Dojo ExtJs implementation for Widgets and Events handling Customizing Log4J for maintaining information and debugging Customizing third party vendor information using Web services SOAP and WSDL Developed Request Cash Message and get Cash plugins using Java Beans Worked with development of data access beans using Hibernate middle ware web service components Develop the GUI using JSP Spring web flow following Spring web MVC pattern Implemented persistence layer using Hibernate that use the POJOs to represent the persistence database tables Used SVN for version control across common source code used by developers Written the JUNIT test cases for the functionalities Environment J2EEJ2SE Java15 JSP Ajax4JSF JSF 12 Spring Frame Work 3 Hibernate JMS CSS3 Apache CXF XML HTML Oracle Education Bachelors Skills Apache 6 years Java 7 years MODEL VIEW CONTROLLER 6 years ModelViewController 6 years CertificationsLicenses A valid CPA CMA or other related accounting designation",
    "entities": [
        "Spark RDD Data",
        "STS Experienced",
        "Infrastructure",
        "Object Oriented Programming",
        "AJAX",
        "GUI",
        "Merge",
        "ORM",
        "Orlando",
        "SPARK",
        "Standard Widget Toolkit SWT Conversion",
        "HDFS",
        "MTS",
        "XSLT Developed",
        "Sr Big Data Hadoop Developer Sr Big Data Hadoop",
        "Pentaho ETL",
        "AXIS",
        "Java Persistence Involved",
        "Java Persistence Architecture JPA Hands",
        "Hadoop Ecosystem",
        "IBM",
        "Spark Hadoop Cassandra Working",
        "Hadoop HDFS Spark",
        "EJB Servlets",
        "Hadoop",
        "Express",
        "Spark Ecosystem Spark",
        "Atlanta",
        "Frames Data Pipelines Building",
        "WebSphere Application Server Followed",
        "MVC Working",
        "Spark Scala Python",
        "TechnologyPOT",
        "NodeJS",
        "Implemented Hibernate ORM Mapping",
        "Landing Staging",
        "HBase",
        "JavaJ2EE",
        "Amazon",
        "TX",
        "AJAX XML XSLT JAXP JAXB AXIS CSS",
        "Hadoop File",
        "Sparkstorm",
        "Implemented Application MVC Architecture",
        "Tomcat Expertise",
        "Evaluate Hadoop",
        "Creation of a User Interface",
        "SparkSQL",
        "HTML Java Script JSP Struts Tag",
        "Developed",
        "Widgets",
        "DAO Factory Data Access Object Service Locator",
        "SWT",
        "SCALA Worked",
        "Mockito",
        "UML",
        "Spring and Hibernate frameworks Involved",
        "Spark Stream",
        "WebApplication Servers",
        "Environment Hadoop MapReduce HDFS Hive Pig Hue Ganglia Nagios",
        "Develop",
        "AGILE",
        "JNDI",
        "Review",
        "JSP",
        "HTML CSS JavaScript JQuery XML JSON AJAX",
        "Data Lake Data Warehouse",
        "Front Controller Service",
        "TestDriven Development TDD Designed",
        "BigData",
        "FDN",
        "MVC",
        "Flume Estimate",
        "Log Data",
        "Data Visualization",
        "Work Experience Sr Big Data Hadoop Developer ADP Florhan Park NJ",
        "Hadoop Spark Data",
        "API",
        "US",
        "Sqoop",
        "HIVE",
        "DOS Hibernate PostgreSQL Dynatrace",
        "Big Data Components",
        "Groovy JUnit",
        "AOP",
        "AWS",
        "the Project Understand Inbound",
        "Teradata Big Data Analytics Experienced",
        "the Big data",
        "JSF",
        "Utilized React",
        "Action Servlets",
        "PIG",
        "React JS",
        "TypeScript",
        "Servlets JSP Spring Hibernate Struts EJBs Web Services",
        "CXF",
        "BRD",
        "SQOOP Expertise",
        "Design",
        "AEM",
        "Development Life Cycle",
        "Organization Created",
        "JQuery Dojo ExtJs",
        "Rally AWS Big",
        "JAXWS technologies Integrating third party",
        "Maven Involved",
        "Big Data Engineer Analysis Design Development Deployment and Maintenance",
        "Software Development",
        "DI Spring",
        "Present Responsibilities Coordinated",
        "Netezza Oracle SQL Server",
        "Hive",
        "SQOOP",
        "Built a Grails",
        "Requirements Gathering Design Development Integration Documentation Testing",
        "JUNIT",
        "Handson",
        "Installing Upgrading and Configuring Redhat Linux",
        "ETL",
        "the Spring Framework",
        "SQL SQL Plus",
        "Hibernate",
        "Impala",
        "Cluster",
        "ADP Florhan Edison NJ",
        "Microsoft",
        "Amazon Web Service AWS",
        "SOAP UI",
        "SVN",
        "Spark Streaming Kafka",
        "Tableau Extract",
        "jQuery",
        "REST",
        "Turner Broadcasting",
        "MapReduce",
        "AWS Security",
        "TDD",
        "NoSQL",
        "Tableau",
        "Data Warehousing",
        "Bootstrap Nodejs Expressjs",
        "Utilized Apache Hadoop",
        "Node",
        "Cloudera",
        "Java Developer Capital One Bank"
    ],
    "experience": "Experience includes Requirements Gathering Design Development Integration Documentation Testing and Build Experience in working with MapReduce programs Pig Scripts and Hive commands to deliver the best results Good Knowledge in Amazon Web Service AWS concepts like EMR and EC2 web services which provides fast and efficient processing of Teradata Big Data Analytics Experienced in collection of Log Data and JSON data into HDFS using Flume and processed the data using HivePig Strong knowledge and experience in Object Oriented Programming using Java Extensively worked on development and optimization of MapReduce programs PIG Scripts and HIVE queries to create structured data for data mining Expertise in developing the presentation layer components like HTML CSS JavaScript JQuery XML JSON AJAX and D3 Good knowledge of coding using SQL SQL Plus TSQL PLSQL Stored ProceduresFunctions Worked on Bootstrap Angular JS and Node JS knockout ember Java Persistence Architecture JPA Hands on experience in advanced BigData technologies like Spark Ecosystem Spark SQL MLlib SparkR and Spark Streaming Kafka and Predictive analytics Knowledge of the software Development Life Cycle SDLC Agile and Waterfall Methodologies Strong experience in developing Enterprise and Web applications on ntier architecture using Java J2EE based technologies such as Servlets JSP Spring Hibernate Struts EJBs Web Services XML JPA JMS JNDI and JDBC Developed applications based on ModelViewController MVC Working knowledge on Oozie a workflow scheduler system to manage the jobs that run on PIG HIVE and SQOOP Expertise in developing test cases for Unit testing Integration testing and System testing Extensively development experience in different IDE like Eclipse Net Beans IntelliJ and STS Experienced with programming language such as C C Xpath Core Java and JavaScript Good experience in Installing Upgrading and Configuring Redhat Linux using Kickstart Servers and Interactive Installation Good experience in Tableau for Data Visualization and analysis on large data sets drawing various conclusions Extensive experience in building and deploying applications on WebApplication Servers like Web logic Web sphere and Tomcat Expertise in core Java J2EE Multithreading JDBC Hibernate Spring Shell Scripting and proficient in using Java APIs for application development Good at problemsolving skills to identify areas of improvement and incorporating best practices for delivering quality deliverables Have good experience excellent communication and interpersonal skills which contribute to timely completion of project deliverable well ahead of schedule Authorized to work in the US for any employer Work Experience Sr Big Data Hadoop Developer ADP Florhan Park NJ US July 2018 to Present Responsibilities Coordinated with business customers to gather business requirements And also interact with other technical peers to derive Technical requirements and delivered the BRD and TDD documents Implement Big Data systems in distributed cloud environment AWS using Amazon EMR Extensively involved in Design phase and delivered Design documents Experience in Hadoop eco system with HDFS HIVE PIG SQOOP and SPARK with SCALA Worked on analyzing Hadoop cluster and different Big Data Components including Pig Hive Spark HBase Kafka Elastic Search database and SQOOP Installed Hadoop Map Reduce HDFS and developed multiple MapReduce jobs in PIG and Hive for data cleaning and preprocessing Importing and exporting data into HDFS and Hive using SQOOP Migration of 100 TBs of data from different databases ie Netezza Oracle SQL Server to Hadoop Wring code in different applications of Hadoop Ecosystem to achieve the required output in a sprint time period Written Hive jobs to parse the logs and structure them in tabular format to facilitate effective querying on the log data Involved in creating Hive tables loading with data and writing Hive queries that will run internally in map reduce way Generate OBIEE reports to verify the Hive tables data Experienced in defining job flows Used Hive to analyze the partitioned and bucketed data and compute various metrics for reporting Experienced in managing and reviewing the Hadoop log files Used Pig as ETL tool to do Transformations with joins and preaggregations before storing the dataonto HDFS Responsible to develop data pipelines from different sources Wrote Hive and Impala queries to load and processing data in Hadoop File system Utilized Apache Hadoop environment by Cloud era Distribution Exported data from HDFS environment into RDBMS using Sqoop for report generation and visualization purpose Worked on Oozie workflow engine for job scheduling Involved in Unit testing and delivered Unit test plans and results documents Environment Hadoop MapReduce HDFS Hive Pig Hue Ganglia Nagios Java Kafka Elastic Search SQL Scala Oracle Netezza Ambari Sqoop Flume Oozie Java jdk 16 Eclipse Service Controller Disney Orlando FL June 2016 to July 2018 Orlando FL Jun2016Jul2018 Full Stack Java Devoloper Responsibilities Done the design development and testing phases of Software Development using AGILE methodology and TestDriven Development TDD Designed the application using Front Controller Service Controller MVC Spring DAO Factory Data Access Object Service Locator and Session Facade Design Patterns Involved in development of the applications using Spring Web MVC and other components of the Spring Framework the controller being Spring 30 Core Dispatcher Servlet Used Hibernate for Object relational model for handling server side database object data Work closely with our partners and clients to develop and support ongoing API integrations Used SOAP and REST based web service protocol Bootstrap used along with Angular 2 and EcmaScript7 with TypeScript in creating the Application Provide leadership in developing the companys IT software and ongoing strategy for internal and external purposes Ensure Client Subsidiaries are in compliance with all domestic and international data privacy lAWS ensuring that the flow of confidential data is secured and in compliance with local jurisdictions Developed Linux bash MS DOS Scripts for internal use Used PostgreSQL as back end and developed Stored procedures Batch jobs triggers Used Jenkins to perform software build with Gradle run shell Scripts and worked on integration tests Develop customized reports for clients or internal customers using Pentaho ETL tools Utilized React for its efficient data flow architecture to create a lightweight and render efficient web app that searched projects via the GitHub API through keywords Designed Frontend with in object oriented JavaScript Framework like Bootstrap Nodejs Expressjs and Angularjs Redux Wrote a Python module to connect and view the status of an Apache Cassandra instance Implemented log4j by enabling logging at runtime without modifying the application binary Developed Mean Stack from scratch including Mongo DB server setup and Express JS server development Worked in using React JS components Forms Events Keys Router Animations and Flux concept Used popular Nodejs frameworks like Express and RESTify to create a RESTful Mock API Provide estimates designs and specifications for AEM templates components and workflows Environment MVC Factory Session Facade Design Patterns Spring SOAP RESTful web services Angularjs Linux bash MS DOS Hibernate PostgreSQL Dynatrace Git Github Bootstrap Nodejs log4j Rally AWS Big data architect Turner Broadcasting Atlanta GA May 2014 to June 2016 Responsibilities Provide technical leadership and contribute to the definition development integration test documentation and support across multiple platforms Design architected and implemented complex projects dealing with the considerable data size GB PB and with high complexity Provide deployment solutions based on customer needs with Sound knowledge about the clustered deployment architecture Able to guide partner with VP Directors for architecting solutions for the Big data Organization Created detailed AWS Security groups which behaved as virtual firewalls that controlled the traffic allowed reaching one or more AWS EC2 instances Data modeling Design implement and deploy highperformance custom applications at scale on Hadoop Spark Data processing with MapReduce and Spark Stream processing on Sparkstorm thru Kafka message broker Review and audit of existing solution design and system architecture Perform profiling troubleshooting of existing solutions Create technical and designing documentation Creation of a User Interface to search andor view content within the cluster by using solar cloud Worked on AWS provisioning EC2 Infrastructure and deploying applications in Elastic load balancing Cluster management and analytic in Cloudera and Horton work Distributed database Design Data modeling Development and Support in Datastax Cassandra distribution Cassandra products strengths and weakness to produce efficient schema designs that serves effective and high performance queries Maintain and work with our data pipeline that transfers and processes several terabytes of data using Spark Scala Python Apache Kafka Pig Hive Impala Apply data analysis data mining and data engineering to present data clearly Ensure highquality data and understand how data is generated out experimental design and how these experiments can produce actionable trustworthy conclusions Full life cycle of Data Lake Data Warehouse with Big data technologies like Spark Hadoop Cassandra Working with Spark RDD Data Frames Data Pipelines Building complex ETLs Data Warehousing or custom pipelines from multiple data sources Setting up connector for security logs and Splunk data use cases Building the Hadoop cluster MTS to host the three use cases Analyzing the data using Tableau Extract and analysis the data before load into cluster Review and understand data architecture data models Source to target mapping rules and Match and merge rules Evaluate Hadoop infrastructure requirements and designdeploy solutions high availability big data clusters elastic load tolerance etc Hadoop ecosystem components in our open source infrastructure stack specifically HBase HDFS MapReduce Yarn Oozie Pig Hive Kafka Storm Spark SparkSQL and Flume Estimate and obtain management support for the time resources and budget required to perform in different projects Keep track of the new requirements change in requirements of the Project Understand Inbound and outbound data flow requirements data models for Landing Staging and base objects Mapping documents Match and Merge rules Proof of Concept POC and Proof of TechnologyPOT execution and evaluation on MTS platforms Installing and Configuring required ecosystem tools for each use case Environment Big Data Spark YARN HIVE Pig Scala Python Hadoop AWS Dynamo DB Kibana Cloudera EMR JDBC Redshift NOSQL Sqoop MYSQL Sr Java Developer Actionet Germantown MD April 2011 to May 2014 Responsibilities Worked on the existing application wireframes FDN and BRD documents to get the requirements and analyzed Handson Experience with Cassandra to provide Scalability along with NoSQL Developed Agile processes using Groovy JUnit to use continuous integration Integrated Automated functional tests Groovy with ContinuousIntegration in Jenkins Parse requests and built response data using Groovys JSON tools and Grails web services Imported data from various resources to the Cassandra cluster using Java APIs Used Eclipse SWT for developing the applications Involved in preparation of TSD documents using UML diagrams Class Sequence and Use case diagrams using Microsoft VISIO tool Wrote RESTful services on the server in NodeJS to listen to requests from devices Built a Grails web application that allows admin users to manage detailed data for all types of Target locations Have worked with Standard Widget Toolkit SWT Conversion of major Openworks components in to Eclipse RCPSWT platform along with support of SwingSWT components Involved in to develop view pages of desktop portal using HTML Java Script JSP Struts Tag libraries AJAX JQUERY GWT DOJO XML and XSLT Developed and deployed Web services to interact with partner interfaces and client interfaces to consume the web services using CXF WSDL SOAP AXIS and JAXWS technologies Integrating third party libraries to augment those lacking or inefficient in ExtJS Used RESTful web services using JERSEY tool to develop web services easily and to be invoked by different channels Developed service objects as beans by using Spring IOCDI Developed Web API using NodeJS and hosted on multiple load balanced API instances Implementation of enterprise application with jQuery angularJS nodejs and SpringMVC Used Spring Beans to encapsulate business logic and Implemented Application MVC Architecture using Spring MVC framework Implemented Hibernate ORM Mapping tool framework to interact with the database to update retrieve insert and delete values effectively Used Java Swing for few components in accordance with SWT application with multithreading environment with Concurrency and Java Collections Used EH Cache for second level cache in Hibernate for the application Involved in to pass messages like payload to track different statuses and milestones using EJB JMS Involved in unit testing integration testing SOAP UI testing smoketesting system testing and user acceptance testing of the application Used Spring programmatic transaction management for Java Persistence Involved in integration of Spring and Hibernate frameworks Involved in setting server properties DSs JNDI queues deploying app in WebSphere Application Server Followed the test driven development using the JUNIT and Mockito framework Created continuous integration builds using Maven Involved in fixing QAUATProduction issues and tracked them using QC Environment Java JSP Servlets JavaScript Spring DI Spring IOC Spring AOP Hibernate 30 AJAX XML XSLT JAXP JAXB AXIS CSS CXF WSDL Java Developer Capital One Bank Plano TX March 2010 to April 2011 Responsabilites Developed Controller Servlets and Action Servlets to handle the request and responses Developed and coordinated complex high quality solutions to clients using IBM ProductsTools Apache Tools J2SE J2EE EJB Servlets JSP HTML JavaScript JQuery JSON and XML Developing the web applications using Spring Framework Hibernate Applying Spring Framework for transaction Management and Spring JDBC for building ORM and for AOP and Dependency Injection Responsible for using AJAX framework with JQuery Dojo ExtJs implementation for Widgets and Events handling Customizing Log4J for maintaining information and debugging Customizing third party vendor information using Web services SOAP and WSDL Developed Request Cash Message and get Cash plugins using Java Beans Worked with development of data access beans using Hibernate middle ware web service components Develop the GUI using JSP Spring web flow following Spring web MVC pattern Implemented persistence layer using Hibernate that use the POJOs to represent the persistence database tables Used SVN for version control across common source code used by developers Written the JUNIT test cases for the functionalities Environment J2EEJ2SE Java15 JSP Ajax4JSF JSF 12 Spring Frame Work 3 Hibernate JMS CSS3 Apache CXF XML HTML Oracle Education Bachelors Skills Apache 6 years Java 7 years MODEL VIEW CONTROLLER 6 years ModelViewController 6 years CertificationsLicenses A valid CPA CMA or other related accounting designation",
    "extracted_keywords": [
        "Sr",
        "Big",
        "Data",
        "Hadoop",
        "Developer",
        "Sr",
        "Big",
        "Data",
        "Hadoop",
        "span",
        "lDeveloperspan",
        "Sr",
        "Big",
        "Data",
        "Hadoop",
        "Developer",
        "ADP",
        "Florhan",
        "Edison",
        "NJ",
        "years",
        "experience",
        "Big",
        "Data",
        "Engineer",
        "Analysis",
        "Design",
        "Development",
        "Deployment",
        "Maintenance",
        "software",
        "JavaJ2EE",
        "technologies",
        "BigData",
        "applications",
        "Expertise",
        "Data",
        "Development",
        "Hortonworks",
        "HDP",
        "platform",
        "Hadoop",
        "ecosystem",
        "tools",
        "Hadoop",
        "HDFS",
        "Spark",
        "Zeppelin",
        "Hive",
        "HBase",
        "Sqoop",
        "Atlas",
        "SOLR",
        "Pig",
        "Falcon",
        "Oozie",
        "Hue",
        "Tez",
        "Apache",
        "NiFi",
        "Kafka",
        "streaming",
        "applications",
        "SPARK",
        "Streaming",
        "Knowledge",
        "bigdata",
        "database",
        "HBase",
        "NoSQL",
        "Mongo",
        "DB",
        "Cassandra",
        "Expertise",
        "Java",
        "Script",
        "JavaScript",
        "MVC",
        "Object",
        "Oriented",
        "JavaScript",
        "Design",
        "Patterns",
        "AJAX",
        "Experience",
        "Requirements",
        "Design",
        "Development",
        "Integration",
        "Documentation",
        "Testing",
        "Build",
        "Experience",
        "MapReduce",
        "programs",
        "Pig",
        "Scripts",
        "Hive",
        "commands",
        "results",
        "Good",
        "Knowledge",
        "Amazon",
        "Web",
        "Service",
        "AWS",
        "concepts",
        "EMR",
        "EC2",
        "web",
        "services",
        "processing",
        "Teradata",
        "Big",
        "Data",
        "Analytics",
        "collection",
        "Log",
        "Data",
        "data",
        "HDFS",
        "Flume",
        "data",
        "HivePig",
        "knowledge",
        "experience",
        "Object",
        "Oriented",
        "Programming",
        "Java",
        "Extensively",
        "development",
        "optimization",
        "MapReduce",
        "programs",
        "Scripts",
        "HIVE",
        "queries",
        "data",
        "data",
        "mining",
        "Expertise",
        "presentation",
        "layer",
        "components",
        "HTML",
        "CSS",
        "JavaScript",
        "JQuery",
        "XML",
        "JSON",
        "AJAX",
        "D3",
        "knowledge",
        "SQL",
        "SQL",
        "TSQL",
        "PLSQL",
        "Stored",
        "ProceduresFunctions",
        "Worked",
        "Bootstrap",
        "Angular",
        "JS",
        "Node",
        "JS",
        "knockout",
        "ember",
        "Java",
        "Persistence",
        "Architecture",
        "JPA",
        "Hands",
        "experience",
        "BigData",
        "technologies",
        "Spark",
        "Ecosystem",
        "Spark",
        "SQL",
        "MLlib",
        "SparkR",
        "Spark",
        "Streaming",
        "Kafka",
        "Predictive",
        "analytics",
        "Knowledge",
        "software",
        "Development",
        "Life",
        "Cycle",
        "SDLC",
        "Agile",
        "Waterfall",
        "Methodologies",
        "Strong",
        "experience",
        "Enterprise",
        "Web",
        "applications",
        "ntier",
        "architecture",
        "Java",
        "J2EE",
        "technologies",
        "Servlets",
        "JSP",
        "Spring",
        "Hibernate",
        "Struts",
        "EJBs",
        "Web",
        "Services",
        "XML",
        "JPA",
        "JMS",
        "JNDI",
        "JDBC",
        "applications",
        "ModelViewController",
        "MVC",
        "Working",
        "knowledge",
        "Oozie",
        "scheduler",
        "system",
        "jobs",
        "PIG",
        "HIVE",
        "SQOOP",
        "Expertise",
        "test",
        "cases",
        "Unit",
        "testing",
        "Integration",
        "testing",
        "System",
        "development",
        "experience",
        "IDE",
        "Eclipse",
        "Net",
        "Beans",
        "IntelliJ",
        "STS",
        "programming",
        "language",
        "C",
        "C",
        "Xpath",
        "Core",
        "Java",
        "JavaScript",
        "Good",
        "experience",
        "Installing",
        "Upgrading",
        "Redhat",
        "Linux",
        "Kickstart",
        "Servers",
        "Interactive",
        "Installation",
        "Good",
        "experience",
        "Tableau",
        "Data",
        "Visualization",
        "analysis",
        "data",
        "sets",
        "conclusions",
        "experience",
        "applications",
        "WebApplication",
        "Servers",
        "Web",
        "logic",
        "Web",
        "sphere",
        "Tomcat",
        "Expertise",
        "core",
        "Java",
        "J2EE",
        "Multithreading",
        "JDBC",
        "Hibernate",
        "Spring",
        "Shell",
        "Scripting",
        "Java",
        "APIs",
        "application",
        "development",
        "Good",
        "skills",
        "areas",
        "improvement",
        "practices",
        "quality",
        "deliverables",
        "experience",
        "communication",
        "skills",
        "completion",
        "project",
        "schedule",
        "US",
        "employer",
        "Work",
        "Experience",
        "Sr",
        "Big",
        "Data",
        "Hadoop",
        "Developer",
        "ADP",
        "Florhan",
        "Park",
        "NJ",
        "US",
        "July",
        "Present",
        "Responsibilities",
        "business",
        "customers",
        "business",
        "requirements",
        "peers",
        "requirements",
        "BRD",
        "TDD",
        "documents",
        "Implement",
        "Big",
        "Data",
        "systems",
        "cloud",
        "environment",
        "AWS",
        "Amazon",
        "EMR",
        "Design",
        "phase",
        "Design",
        "documents",
        "Experience",
        "Hadoop",
        "eco",
        "system",
        "HDFS",
        "HIVE",
        "PIG",
        "SQOOP",
        "SPARK",
        "SCALA",
        "Hadoop",
        "cluster",
        "Big",
        "Data",
        "Components",
        "Pig",
        "Hive",
        "Spark",
        "HBase",
        "Kafka",
        "Elastic",
        "Search",
        "database",
        "SQOOP",
        "Installed",
        "Hadoop",
        "Map",
        "HDFS",
        "MapReduce",
        "jobs",
        "PIG",
        "Hive",
        "data",
        "Importing",
        "data",
        "HDFS",
        "Hive",
        "SQOOP",
        "Migration",
        "TBs",
        "data",
        "databases",
        "Netezza",
        "Oracle",
        "SQL",
        "Server",
        "Hadoop",
        "Wring",
        "code",
        "applications",
        "Hadoop",
        "Ecosystem",
        "output",
        "time",
        "period",
        "Written",
        "Hive",
        "jobs",
        "logs",
        "format",
        "querying",
        "log",
        "data",
        "Hive",
        "tables",
        "data",
        "Hive",
        "queries",
        "map",
        "way",
        "Generate",
        "OBIEE",
        "Hive",
        "tables",
        "data",
        "job",
        "flows",
        "Hive",
        "data",
        "metrics",
        "Hadoop",
        "log",
        "Pig",
        "ETL",
        "tool",
        "Transformations",
        "joins",
        "preaggregations",
        "dataonto",
        "HDFS",
        "data",
        "pipelines",
        "sources",
        "Wrote",
        "Hive",
        "Impala",
        "processing",
        "data",
        "Hadoop",
        "File",
        "system",
        "Apache",
        "Hadoop",
        "environment",
        "Cloud",
        "era",
        "Distribution",
        "data",
        "HDFS",
        "environment",
        "RDBMS",
        "Sqoop",
        "report",
        "generation",
        "visualization",
        "purpose",
        "Oozie",
        "workflow",
        "engine",
        "job",
        "scheduling",
        "Unit",
        "testing",
        "Unit",
        "test",
        "plans",
        "documents",
        "Environment",
        "Hadoop",
        "MapReduce",
        "HDFS",
        "Hive",
        "Pig",
        "Hue",
        "Ganglia",
        "Nagios",
        "Java",
        "Kafka",
        "Elastic",
        "Search",
        "SQL",
        "Scala",
        "Oracle",
        "Netezza",
        "Ambari",
        "Sqoop",
        "Flume",
        "Oozie",
        "Java",
        "jdk",
        "Eclipse",
        "Service",
        "Controller",
        "Disney",
        "Orlando",
        "FL",
        "June",
        "July",
        "Orlando",
        "FL",
        "Jun2016Jul2018",
        "Full",
        "Stack",
        "Java",
        "Devoloper",
        "Responsibilities",
        "design",
        "development",
        "phases",
        "Software",
        "Development",
        "methodology",
        "TestDriven",
        "Development",
        "TDD",
        "application",
        "Front",
        "Controller",
        "Service",
        "Controller",
        "MVC",
        "Spring",
        "DAO",
        "Factory",
        "Data",
        "Access",
        "Object",
        "Service",
        "Locator",
        "Session",
        "Facade",
        "Design",
        "Patterns",
        "development",
        "applications",
        "Spring",
        "Web",
        "MVC",
        "components",
        "Spring",
        "Framework",
        "controller",
        "Spring",
        "Core",
        "Dispatcher",
        "Servlet",
        "Hibernate",
        "Object",
        "model",
        "server",
        "side",
        "database",
        "object",
        "data",
        "partners",
        "clients",
        "API",
        "integrations",
        "SOAP",
        "REST",
        "web",
        "service",
        "protocol",
        "Bootstrap",
        "Angular",
        "EcmaScript7",
        "TypeScript",
        "Application",
        "Provide",
        "leadership",
        "companys",
        "IT",
        "software",
        "strategy",
        "purposes",
        "Ensure",
        "Client",
        "Subsidiaries",
        "compliance",
        "data",
        "privacy",
        "flow",
        "data",
        "compliance",
        "jurisdictions",
        "Linux",
        "bash",
        "MS",
        "DOS",
        "Scripts",
        "use",
        "PostgreSQL",
        "end",
        "procedures",
        "Batch",
        "jobs",
        "Jenkins",
        "software",
        "build",
        "Gradle",
        "shell",
        "Scripts",
        "integration",
        "tests",
        "Develop",
        "reports",
        "clients",
        "customers",
        "Pentaho",
        "ETL",
        "tools",
        "React",
        "data",
        "flow",
        "architecture",
        "lightweight",
        "web",
        "app",
        "projects",
        "GitHub",
        "API",
        "keywords",
        "Frontend",
        "object",
        "JavaScript",
        "Framework",
        "Bootstrap",
        "Nodejs",
        "Expressjs",
        "Angularjs",
        "Redux",
        "Python",
        "module",
        "status",
        "Apache",
        "Cassandra",
        "instance",
        "log4j",
        "runtime",
        "application",
        "Developed",
        "Mean",
        "Stack",
        "scratch",
        "Mongo",
        "DB",
        "server",
        "setup",
        "Express",
        "JS",
        "server",
        "development",
        "React",
        "JS",
        "components",
        "Forms",
        "Events",
        "Keys",
        "Router",
        "Animations",
        "Flux",
        "concept",
        "Nodejs",
        "frameworks",
        "Express",
        "API",
        "Provide",
        "designs",
        "specifications",
        "AEM",
        "templates",
        "components",
        "workflows",
        "Environment",
        "MVC",
        "Factory",
        "Session",
        "Facade",
        "Design",
        "Patterns",
        "Spring",
        "SOAP",
        "web",
        "services",
        "Angularjs",
        "Linux",
        "MS",
        "DOS",
        "Hibernate",
        "PostgreSQL",
        "Dynatrace",
        "Git",
        "Github",
        "Bootstrap",
        "Nodejs",
        "log4j",
        "Rally",
        "data",
        "Turner",
        "Broadcasting",
        "Atlanta",
        "GA",
        "May",
        "June",
        "Responsibilities",
        "leadership",
        "definition",
        "development",
        "integration",
        "test",
        "documentation",
        "support",
        "platforms",
        "Design",
        "projects",
        "data",
        "size",
        "GB",
        "PB",
        "complexity",
        "deployment",
        "solutions",
        "customer",
        "needs",
        "knowledge",
        "deployment",
        "architecture",
        "partner",
        "VP",
        "Directors",
        "solutions",
        "data",
        "Organization",
        "AWS",
        "Security",
        "groups",
        "firewalls",
        "traffic",
        "AWS",
        "EC2",
        "Data",
        "Design",
        "highperformance",
        "custom",
        "applications",
        "scale",
        "Hadoop",
        "Spark",
        "Data",
        "processing",
        "MapReduce",
        "Spark",
        "Stream",
        "processing",
        "Sparkstorm",
        "Kafka",
        "message",
        "broker",
        "Review",
        "audit",
        "solution",
        "design",
        "system",
        "architecture",
        "Perform",
        "profiling",
        "troubleshooting",
        "solutions",
        "documentation",
        "Creation",
        "User",
        "Interface",
        "view",
        "content",
        "cluster",
        "cloud",
        "AWS",
        "EC2",
        "Infrastructure",
        "deploying",
        "applications",
        "load",
        "Cluster",
        "management",
        "Cloudera",
        "Horton",
        "work",
        "database",
        "Design",
        "Data",
        "Development",
        "Support",
        "Datastax",
        "Cassandra",
        "distribution",
        "Cassandra",
        "products",
        "strengths",
        "weakness",
        "schema",
        "designs",
        "performance",
        "work",
        "data",
        "pipeline",
        "transfers",
        "terabytes",
        "data",
        "Spark",
        "Scala",
        "Python",
        "Apache",
        "Kafka",
        "Pig",
        "Hive",
        "Impala",
        "data",
        "analysis",
        "data",
        "mining",
        "data",
        "engineering",
        "data",
        "highquality",
        "data",
        "data",
        "design",
        "experiments",
        "conclusions",
        "life",
        "cycle",
        "Data",
        "Lake",
        "Data",
        "Warehouse",
        "data",
        "technologies",
        "Spark",
        "Hadoop",
        "Cassandra",
        "Working",
        "Spark",
        "RDD",
        "Data",
        "Frames",
        "Data",
        "Pipelines",
        "Building",
        "ETLs",
        "Data",
        "Warehousing",
        "custom",
        "pipelines",
        "data",
        "sources",
        "connector",
        "security",
        "logs",
        "Splunk",
        "data",
        "cases",
        "Hadoop",
        "cluster",
        "MTS",
        "use",
        "cases",
        "data",
        "Tableau",
        "Extract",
        "data",
        "load",
        "cluster",
        "Review",
        "data",
        "architecture",
        "data",
        "models",
        "Source",
        "mapping",
        "rules",
        "rules",
        "Evaluate",
        "Hadoop",
        "infrastructure",
        "requirements",
        "designdeploy",
        "solutions",
        "availability",
        "data",
        "clusters",
        "load",
        "tolerance",
        "Hadoop",
        "ecosystem",
        "components",
        "source",
        "infrastructure",
        "stack",
        "HBase",
        "HDFS",
        "MapReduce",
        "Yarn",
        "Oozie",
        "Pig",
        "Hive",
        "Kafka",
        "Storm",
        "Spark",
        "SparkSQL",
        "Flume",
        "Estimate",
        "management",
        "support",
        "time",
        "resources",
        "budget",
        "projects",
        "track",
        "requirements",
        "requirements",
        "Project",
        "Understand",
        "Inbound",
        "data",
        "flow",
        "requirements",
        "data",
        "models",
        "Landing",
        "Staging",
        "base",
        "Mapping",
        "documents",
        "Match",
        "Merge",
        "Proof",
        "Concept",
        "POC",
        "Proof",
        "execution",
        "evaluation",
        "MTS",
        "platforms",
        "Configuring",
        "ecosystem",
        "tools",
        "use",
        "case",
        "Environment",
        "Big",
        "Data",
        "Spark",
        "YARN",
        "HIVE",
        "Pig",
        "Scala",
        "Python",
        "Hadoop",
        "Dynamo",
        "DB",
        "Kibana",
        "Cloudera",
        "EMR",
        "JDBC",
        "Redshift",
        "NOSQL",
        "Sqoop",
        "MYSQL",
        "Sr",
        "Java",
        "Developer",
        "Actionet",
        "Germantown",
        "MD",
        "April",
        "May",
        "Responsibilities",
        "application",
        "FDN",
        "BRD",
        "documents",
        "requirements",
        "Handson",
        "Experience",
        "Cassandra",
        "Scalability",
        "NoSQL",
        "processes",
        "Groovy",
        "JUnit",
        "integration",
        "Integrated",
        "tests",
        "Groovy",
        "ContinuousIntegration",
        "Jenkins",
        "Parse",
        "requests",
        "response",
        "data",
        "Groovys",
        "JSON",
        "tools",
        "Grails",
        "web",
        "services",
        "data",
        "resources",
        "Cassandra",
        "cluster",
        "Java",
        "APIs",
        "Eclipse",
        "SWT",
        "applications",
        "preparation",
        "TSD",
        "documents",
        "UML",
        "diagrams",
        "Class",
        "Sequence",
        "Use",
        "case",
        "diagrams",
        "Microsoft",
        "VISIO",
        "tool",
        "services",
        "server",
        "NodeJS",
        "requests",
        "devices",
        "Grails",
        "web",
        "application",
        "users",
        "data",
        "types",
        "Target",
        "locations",
        "Standard",
        "Widget",
        "Toolkit",
        "SWT",
        "Conversion",
        "Openworks",
        "components",
        "Eclipse",
        "RCPSWT",
        "platform",
        "support",
        "SwingSWT",
        "components",
        "view",
        "pages",
        "portal",
        "HTML",
        "Java",
        "Script",
        "JSP",
        "Struts",
        "Tag",
        "AJAX",
        "JQUERY",
        "GWT",
        "DOJO",
        "XML",
        "XSLT",
        "Developed",
        "Web",
        "services",
        "partner",
        "interfaces",
        "client",
        "interfaces",
        "web",
        "services",
        "CXF",
        "WSDL",
        "SOAP",
        "AXIS",
        "JAXWS",
        "technologies",
        "party",
        "ExtJS",
        "web",
        "services",
        "JERSEY",
        "tool",
        "web",
        "services",
        "channels",
        "service",
        "objects",
        "beans",
        "Spring",
        "IOCDI",
        "Web",
        "API",
        "NodeJS",
        "load",
        "API",
        "instances",
        "Implementation",
        "enterprise",
        "application",
        "jQuery",
        "angularJS",
        "SpringMVC",
        "Spring",
        "Beans",
        "business",
        "logic",
        "Implemented",
        "Application",
        "MVC",
        "Architecture",
        "Spring",
        "MVC",
        "framework",
        "Hibernate",
        "ORM",
        "Mapping",
        "tool",
        "framework",
        "database",
        "retrieve",
        "values",
        "Java",
        "Swing",
        "components",
        "accordance",
        "SWT",
        "application",
        "multithreading",
        "environment",
        "Concurrency",
        "Java",
        "Collections",
        "EH",
        "Cache",
        "level",
        "cache",
        "Hibernate",
        "application",
        "messages",
        "payload",
        "statuses",
        "milestones",
        "EJB",
        "JMS",
        "unit",
        "testing",
        "integration",
        "testing",
        "SOAP",
        "UI",
        "system",
        "testing",
        "user",
        "acceptance",
        "testing",
        "application",
        "Spring",
        "transaction",
        "management",
        "Java",
        "Persistence",
        "integration",
        "Spring",
        "Hibernate",
        "frameworks",
        "server",
        "properties",
        "JNDI",
        "queues",
        "app",
        "WebSphere",
        "Application",
        "Server",
        "test",
        "development",
        "JUNIT",
        "Mockito",
        "framework",
        "integration",
        "Maven",
        "QAUATProduction",
        "issues",
        "QC",
        "Environment",
        "Java",
        "JSP",
        "Servlets",
        "JavaScript",
        "Spring",
        "DI",
        "Spring",
        "IOC",
        "Spring",
        "AOP",
        "Hibernate",
        "AJAX",
        "XML",
        "XSLT",
        "JAXP",
        "JAXB",
        "CSS",
        "CXF",
        "WSDL",
        "Java",
        "Developer",
        "Capital",
        "One",
        "Bank",
        "Plano",
        "TX",
        "March",
        "April",
        "Controller",
        "Servlets",
        "Action",
        "Servlets",
        "request",
        "responses",
        "quality",
        "solutions",
        "clients",
        "IBM",
        "ProductsTools",
        "Apache",
        "Tools",
        "J2SE",
        "J2EE",
        "EJB",
        "Servlets",
        "JSP",
        "HTML",
        "JavaScript",
        "JQuery",
        "JSON",
        "XML",
        "web",
        "applications",
        "Spring",
        "Framework",
        "Hibernate",
        "Applying",
        "Spring",
        "Framework",
        "transaction",
        "Management",
        "Spring",
        "JDBC",
        "ORM",
        "AOP",
        "Dependency",
        "Injection",
        "Responsible",
        "AJAX",
        "framework",
        "JQuery",
        "Dojo",
        "ExtJs",
        "implementation",
        "Widgets",
        "Events",
        "Customizing",
        "information",
        "Customizing",
        "party",
        "vendor",
        "information",
        "Web",
        "services",
        "SOAP",
        "WSDL",
        "Request",
        "Cash",
        "Message",
        "Cash",
        "plugins",
        "Java",
        "Beans",
        "development",
        "data",
        "access",
        "beans",
        "Hibernate",
        "ware",
        "web",
        "service",
        "components",
        "GUI",
        "JSP",
        "Spring",
        "web",
        "flow",
        "Spring",
        "web",
        "MVC",
        "pattern",
        "persistence",
        "layer",
        "Hibernate",
        "POJOs",
        "persistence",
        "database",
        "SVN",
        "version",
        "control",
        "source",
        "code",
        "developers",
        "JUNIT",
        "test",
        "cases",
        "functionalities",
        "Environment",
        "J2EEJ2SE",
        "Java15",
        "JSP",
        "JSF",
        "Spring",
        "Frame",
        "Work",
        "Hibernate",
        "JMS",
        "CSS3",
        "Apache",
        "CXF",
        "XML",
        "HTML",
        "Oracle",
        "Education",
        "Bachelors",
        "Skills",
        "Apache",
        "years",
        "Java",
        "years",
        "MODEL",
        "VIEW",
        "years",
        "ModelViewController",
        "years",
        "CertificationsLicenses",
        "CPA",
        "CMA",
        "accounting",
        "designation"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T20:27:38.785120",
    "resume_data": "Sr Big Data Hadoop Developer Sr Big Data Hadoop span lDeveloperspan Sr Big Data Hadoop Developer ADP Florhan Edison NJ Above 9 years of experience as Big Data Engineer Analysis Design Development Deployment and Maintenance of software in JavaJ2EE technologies and BigData applications Expertise in Data Development in Hortonworks HDP platform Hadoop ecosystem tools like Hadoop HDFS Spark Zeppelin Hive HBase Sqoop flume Atlas SOLR Pig Falcon Oozie Hue Tez Apache NiFi Kafka Built streaming applications using SPARK Streaming Knowledge on bigdata database HBase and NoSQL databases Mongo DB and Cassandra Expertise in Java Script JavaScript MVC patterns Object Oriented JavaScript Design Patterns and AJAX calls Experience includes Requirements Gathering Design Development Integration Documentation Testing and Build Experience in working with MapReduce programs Pig Scripts and Hive commands to deliver the best results Good Knowledge in Amazon Web Service AWS concepts like EMR and EC2 web services which provides fast and efficient processing of Teradata Big Data Analytics Experienced in collection of Log Data and JSON data into HDFS using Flume and processed the data using HivePig Strong knowledge and experience in Object Oriented Programming using Java Extensively worked on development and optimization of MapReduce programs PIG Scripts and HIVE queries to create structured data for data mining Expertise in developing the presentation layer components like HTML CSS JavaScript JQuery XML JSON AJAX and D3 Good knowledge of coding using SQL SQL Plus TSQL PLSQL Stored ProceduresFunctions Worked on Bootstrap Angular JS and Node JS knockout ember Java Persistence Architecture JPA Hands on experience in advanced BigData technologies like Spark Ecosystem Spark SQL MLlib SparkR and Spark Streaming Kafka and Predictive analytics Knowledge of the software Development Life Cycle SDLC Agile and Waterfall Methodologies Strong experience in developing Enterprise and Web applications on ntier architecture using Java J2EE based technologies such as Servlets JSP Spring Hibernate Struts EJBs Web Services XML JPA JMS JNDI and JDBC Developed applications based on ModelViewController MVC Working knowledge on Oozie a workflow scheduler system to manage the jobs that run on PIG HIVE and SQOOP Expertise in developing test cases for Unit testing Integration testing and System testing Extensively development experience in different IDE like Eclipse Net Beans IntelliJ and STS Experienced with programming language such as C C Xpath Core Java and JavaScript Good experience in Installing Upgrading and Configuring Redhat Linux using Kickstart Servers and Interactive Installation Good experience in Tableau for Data Visualization and analysis on large data sets drawing various conclusions Extensive experience in building and deploying applications on WebApplication Servers like Web logic Web sphere and Tomcat Expertise in core Java J2EE Multithreading JDBC Hibernate Spring Shell Scripting and proficient in using Java APIs for application development Good at problemsolving skills to identify areas of improvement and incorporating best practices for delivering quality deliverables Have good experience excellent communication and interpersonal skills which contribute to timely completion of project deliverable well ahead of schedule Authorized to work in the US for any employer Work Experience Sr Big Data Hadoop Developer ADP Florhan Park NJ US July 2018 to Present Responsibilities Coordinated with business customers to gather business requirements And also interact with other technical peers to derive Technical requirements and delivered the BRD and TDD documents Implement Big Data systems in distributed cloud environment AWS using Amazon EMR Extensively involved in Design phase and delivered Design documents Experience in Hadoop eco system with HDFS HIVE PIG SQOOP and SPARK with SCALA Worked on analyzing Hadoop cluster and different Big Data Components including Pig Hive Spark HBase Kafka Elastic Search database and SQOOP Installed Hadoop Map Reduce HDFS and developed multiple MapReduce jobs in PIG and Hive for data cleaning and preprocessing Importing and exporting data into HDFS and Hive using SQOOP Migration of 100 TBs of data from different databases ie Netezza Oracle SQL Server to Hadoop Wring code in different applications of Hadoop Ecosystem to achieve the required output in a sprint time period Written Hive jobs to parse the logs and structure them in tabular format to facilitate effective querying on the log data Involved in creating Hive tables loading with data and writing Hive queries that will run internally in map reduce way Generate OBIEE reports to verify the Hive tables data Experienced in defining job flows Used Hive to analyze the partitioned and bucketed data and compute various metrics for reporting Experienced in managing and reviewing the Hadoop log files Used Pig as ETL tool to do Transformations with joins and preaggregations before storing the dataonto HDFS Responsible to develop data pipelines from different sources Wrote Hive and Impala queries to load and processing data in Hadoop File system Utilized Apache Hadoop environment by Cloud era Distribution Exported data from HDFS environment into RDBMS using Sqoop for report generation and visualization purpose Worked on Oozie workflow engine for job scheduling Involved in Unit testing and delivered Unit test plans and results documents Environment Hadoop MapReduce HDFS Hive Pig Hue Ganglia Nagios Java Kafka Elastic Search SQL Scala Oracle Netezza Ambari Sqoop Flume Oozie Java jdk 16 Eclipse Service Controller Disney Orlando FL June 2016 to July 2018 Orlando FL Jun2016Jul2018 Full Stack Java Devoloper Responsibilities Done the design development and testing phases of Software Development using AGILE methodology and TestDriven Development TDD Designed the application using Front Controller Service Controller MVC Spring DAO Factory Data Access Object Service Locator and Session Facade Design Patterns Involved in development of the applications using Spring Web MVC and other components of the Spring Framework the controller being Spring 30 Core Dispatcher Servlet Used Hibernate for Object relational model for handling server side database object data Work closely with our partners and clients to develop and support ongoing API integrations Used SOAP and REST based web service protocol Bootstrap used along with Angular 2 and EcmaScript7 with TypeScript in creating the Application Provide leadership in developing the companys IT software and ongoing strategy for internal and external purposes Ensure Client Subsidiaries are in compliance with all domestic and international data privacy lAWS ensuring that the flow of confidential data is secured and in compliance with local jurisdictions Developed Linux bash MS DOS Scripts for internal use Used PostgreSQL as back end and developed Stored procedures Batch jobs triggers Used Jenkins to perform software build with Gradle run shell Scripts and worked on integration tests Develop customized reports for clients or internal customers using Pentaho ETL tools Utilized React for its efficient data flow architecture to create a lightweight and render efficient web app that searched projects via the GitHub API through keywords Designed Frontend with in object oriented JavaScript Framework like Bootstrap Nodejs Expressjs and Angularjs Redux Wrote a Python module to connect and view the status of an Apache Cassandra instance Implemented log4j by enabling logging at runtime without modifying the application binary Developed Mean Stack from scratch including Mongo DB server setup and Express JS server development Worked in using React JS components Forms Events Keys Router Animations and Flux concept Used popular Nodejs frameworks like Express and RESTify to create a RESTful Mock API Provide estimates designs and specifications for AEM templates components and workflows Environment MVC Factory Session Facade Design Patterns Spring SOAP RESTful web services Angularjs Linux bash MS DOS Hibernate PostgreSQL Dynatrace Git Github Bootstrap Nodejs log4j Rally AWS Big data architect Turner Broadcasting Atlanta GA May 2014 to June 2016 Responsibilities Provide technical leadership and contribute to the definition development integration test documentation and support across multiple platforms Design architected and implemented complex projects dealing with the considerable data size GB PB and with high complexity Provide deployment solutions based on customer needs with Sound knowledge about the clustered deployment architecture Able to guide partner with VP Directors for architecting solutions for the Big data Organization Created detailed AWS Security groups which behaved as virtual firewalls that controlled the traffic allowed reaching one or more AWS EC2 instances Data modeling Design implement and deploy highperformance custom applications at scale on Hadoop Spark Data processing with MapReduce and Spark Stream processing on Sparkstorm thru Kafka message broker Review and audit of existing solution design and system architecture Perform profiling troubleshooting of existing solutions Create technical and designing documentation Creation of a User Interface to search andor view content within the cluster by using solar cloud Worked on AWS provisioning EC2 Infrastructure and deploying applications in Elastic load balancing Cluster management and analytic in Cloudera and Horton work Distributed database Design Data modeling Development and Support in Datastax Cassandra distribution Cassandra products strengths and weakness to produce efficient schema designs that serves effective and high performance queries Maintain and work with our data pipeline that transfers and processes several terabytes of data using Spark Scala Python Apache Kafka Pig Hive Impala Apply data analysis data mining and data engineering to present data clearly Ensure highquality data and understand how data is generated out experimental design and how these experiments can produce actionable trustworthy conclusions Full life cycle of Data Lake Data Warehouse with Big data technologies like Spark Hadoop Cassandra Working with Spark RDD Data Frames Data Pipelines Building complex ETLs Data Warehousing or custom pipelines from multiple data sources Setting up connector for security logs and Splunk data use cases Building the Hadoop cluster MTS to host the three use cases Analyzing the data using Tableau Extract and analysis the data before load into cluster Review and understand data architecture data models Source to target mapping rules and Match and merge rules Evaluate Hadoop infrastructure requirements and designdeploy solutions high availability big data clusters elastic load tolerance etc Hadoop ecosystem components in our open source infrastructure stack specifically HBase HDFS MapReduce Yarn Oozie Pig Hive Kafka Storm Spark SparkSQL and Flume Estimate and obtain management support for the time resources and budget required to perform in different projects Keep track of the new requirements change in requirements of the Project Understand Inbound and outbound data flow requirements data models for Landing Staging and base objects Mapping documents Match and Merge rules Proof of Concept POC and Proof of TechnologyPOT execution and evaluation on MTS platforms Installing and Configuring required ecosystem tools for each use case Environment Big Data Spark YARN HIVE Pig Scala Python Hadoop AWS Dynamo DB Kibana Cloudera EMR JDBC Redshift NOSQL Sqoop MYSQL Sr Java Developer Actionet Germantown MD April 2011 to May 2014 Responsibilities Worked on the existing application wireframes FDN and BRD documents to get the requirements and analyzed Handson Experience with Cassandra to provide Scalability along with NoSQL Developed Agile processes using Groovy JUnit to use continuous integration Integrated Automated functional tests Groovy with ContinuousIntegration in Jenkins Parse requests and built response data using Groovys JSON tools and Grails web services Imported data from various resources to the Cassandra cluster using Java APIs Used Eclipse SWT for developing the applications Involved in preparation of TSD documents using UML diagrams Class Sequence and Use case diagrams using Microsoft VISIO tool Wrote RESTful services on the server in NodeJS to listen to requests from devices Built a Grails web application that allows admin users to manage detailed data for all types of Target locations Have worked with Standard Widget Toolkit SWT Conversion of major Openworks components in to Eclipse RCPSWT platform along with support of SwingSWT components Involved in to develop view pages of desktop portal using HTML Java Script JSP Struts Tag libraries AJAX JQUERY GWT DOJO XML and XSLT Developed and deployed Web services to interact with partner interfaces and client interfaces to consume the web services using CXF WSDL SOAP AXIS and JAXWS technologies Integrating third party libraries to augment those lacking or inefficient in ExtJS Used RESTful web services using JERSEY tool to develop web services easily and to be invoked by different channels Developed service objects as beans by using Spring IOCDI Developed Web API using NodeJS and hosted on multiple load balanced API instances Implementation of enterprise application with jQuery angularJS nodejs and SpringMVC Used Spring Beans to encapsulate business logic and Implemented Application MVC Architecture using Spring MVC framework Implemented Hibernate ORM Mapping tool framework to interact with the database to update retrieve insert and delete values effectively Used Java Swing for few components in accordance with SWT application with multithreading environment with Concurrency and Java Collections Used EH Cache for second level cache in Hibernate for the application Involved in to pass messages like payload to track different statuses and milestones using EJB JMS Involved in unit testing integration testing SOAP UI testing smoketesting system testing and user acceptance testing of the application Used Spring programmatic transaction management for Java Persistence Involved in integration of Spring and Hibernate frameworks Involved in setting server properties DSs JNDI queues deploying app in WebSphere Application Server Followed the test driven development using the JUNIT and Mockito framework Created continuous integration builds using Maven Involved in fixing QAUATProduction issues and tracked them using QC Environment Java JSP Servlets JavaScript Spring DI Spring IOC Spring AOP Hibernate 30 AJAX XML XSLT JAXP JAXB AXIS CSS CXF WSDL Java Developer Capital One Bank Plano TX March 2010 to April 2011 Responsabilites Developed Controller Servlets and Action Servlets to handle the request and responses Developed and coordinated complex high quality solutions to clients using IBM ProductsTools Apache Tools J2SE J2EE EJB Servlets JSP HTML JavaScript JQuery JSON and XML Developing the web applications using Spring Framework Hibernate Applying Spring Framework for transaction Management and Spring JDBC for building ORM and for AOP and Dependency Injection Responsible for using AJAX framework with JQuery Dojo ExtJs implementation for Widgets and Events handling Customizing Log4J for maintaining information and debugging Customizing third party vendor information using Web services SOAP and WSDL Developed Request Cash Message and get Cash plugins using Java Beans Worked with development of data access beans using Hibernate middle ware web service components Develop the GUI using JSP Spring web flow following Spring web MVC pattern Implemented persistence layer using Hibernate that use the POJOs to represent the persistence database tables Used SVN for version control across common source code used by developers Written the JUNIT test cases for the functionalities Environment J2EEJ2SE Java15 JSP Ajax4JSF JSF 12 Spring Frame Work 3 Hibernate JMS CSS3 Apache CXF XML HTML Oracle Education Bachelors Skills Apache 6 years Java 7 years MODEL VIEW CONTROLLER 6 years ModelViewController 6 years CertificationsLicenses A valid CPA CMA or other related accounting designation",
    "unique_id": "48106020-6164-4c78-a986-bfb5b632e3e0"
}