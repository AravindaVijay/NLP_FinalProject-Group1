{
    "clean_data": "Data Scientist Data Scientist Data Scientist Palo Alto Networks Dallas TX Dallas TX Above 8 years of experience in large Unstructured data Datasets of Structured Data Visualization Data Acquisition Predictive modeling Data Validation Develop maintain and teach new tools and methodologies related to data science and high performance computing Data Scientist with proven expertise in Data Analysis Machine Learning and Modeling Experience in Machine Learning algorithms such as Linear Regression Logistic Regression Naive Bayes Decision Trees KMeans Clustering and Association Rules Experience in applying predictive modeling and machine learning algorithms for analytical reports Experience using technology to work efficiently with datasets such as scripting data cleaning tools statistical software packages Developed predictive models using Decision Tree Random Forest Nave Bayes Logistic Regression Cluster Analysis and Neural Networks Very Strong in Python statistical analysis tools and modeling Experienced in Machine Learning and Statistical Analysis with Python ScikitLearn Excellent Knowledge in Relational Data WarehouseOLAP concepts Database Design and methodologies Strong SQL programming skills with experience in working with functions packages and triggers Expertise in transforming business requirements into designing algorithms analytical models building models developing data mining and reporting solutions that scales across massive volume of structured and unstructured data Skilled in performing data parsing data manipulation data architecture data ingestion and data preparation with methods including describe data contents compute descriptive statistics of data regex split and combine merge Remap subset reindex melt and reshape Worked with NoSQLDatabase including Hbase Cassandra and MongoDB Experienced in Big Data with Hadoop MapReduce HDFS and Spark Experienced in Data Integration Validation and Data Quality controls for ETL process and Data Warehousing using MS Visual Studio SSAS SSISand SSRS Proficient in Tableau and RShiny data visualization tools to analyze and obtain insights into large datasets create visually powerful and actionable interactive reports and dashboards Automated recurring reports using SQL andPython and visualized them on BI platform like Tableau Worked in development environment like Git and VM Excellent communication skills Successfully working in fastpaced multitasking environment both independently and in collaborative team a selfmotivated enthusiastic learner Experience with Big Data technologies like Hadoop and Spark would be a plus Worked and extracted data from various database sources like SQL Server Oracle and DB2 Experience working at Pricing andor Revenue Management would be a plus Familiarity with agile principles eg Scrum facilitating workshops and prototyping Hands on experience with RStudio for doing data preprocessing and building machine learning algorithms on different datasets Good Knowledge in NoSQL databases like HBase and Mongo DB Time Series Analysis ARIMA Neural Networks Sentiment Analysis Forecasting and Text Mining Cluster Analysis Principal Component Analysis Association Rules Recommender Systems Inferential Statistics Hypothesis Testing Descriptive and Sampling Work Experience Data Scientist Palo Alto Networks Dallas TX Dallas TX August 2018 to Present DescriptionPalo Alto Networks Inc an American multinational cyber security company Its core products are a platform that includes advanced firewalls and cloudbased offerings that extend those firewalls to cover other aspects of security Responsibilities Extracted data from HDFS and prepared data for exploratory analysis using data munging Built models using Statistical techniques like Bayesian HMM and Machine Learning classification models like XGBoost SVM and Random Forest Participated in all phases of data mining data cleaning data collection developing models validation and visualization and performed Gap analysis A highly immersive Data Science program involving Data Manipulation Visualization Web Scraping Machine Learning Python programming SQL GIT MongoDB Hadoop Setup storage and data analysis tools in AWS cloud computing infrastructure Installed and used Caffe Deep Learning Framework Worked on different data formats such as JSON XML and performed machine learning algorithms in Python Worked as Data Architects and IT Architects to understand the movement of data and its storage and ER Studio 97 Used pandas numpy seaborn matplotlib scikitlearn scipy NLTK in Python for developing various machine learning algorithms Data Manipulation and Aggregation from different source using Nexus Business Objects Toad Power BI and Smart View Implemented Agile Methodology for building an internal application Focus on integration overlap and Informatica newer commitment to MDM with the acquisition of Identity Systems Coded proprietary packages to analyze and visualize SPCfile data to identify bad spectra and samples to reduce unnecessary procedures and costs Programmed a utility in Python that used multiple packages numpy scipy pandas Implemented Classification using supervised algorithms like Logistic Regression Decision trees Naive Bayes KNN As Architect delivered various complex OLAPdatabasescubes scorecards dashboards and reports Updated Python scripts to match training data with our database stored in AWS Cloud Search so that we would be able to assign each document a response label for further classification Used Teradata utilities such as Fast Export MLOAD for handling various tasks data migrationETL from OLTP Source Systems to OLAP Target Systems Data transformation from various resources data organization features extraction from raw and stored Validated the machine learning classifiers using ROC Curves and Lift Charts Environment Unix Python 352 MLLib SAS regression logistic regression Hadoop 274 NoSQL Teradata OLTP random forest OLAP HDFS ODS NLTK SVM JSON XML and MapReduce Data Scientist Cisco Dallas TX May 2017 to July 2018 DescriptionCisco Systems Inc is an American multinational technology conglomerate that develops manufactures and sells networking hardware telecommunications equipment and other hightechnology services and products Cisco helps seize the opportunities of tomorrow by proving that amazing things can happen when you connect the unconnected An integral part of our DNA is creating longlasting customer partnerships working together to identify our customers needs and provide solutions that fuel their success Responsibilities Utilized Spark Scala Hadoop HQL VQL oozie pySpark Data Lake Tensor Flow HBase Cassandra Redshift MongoDB Kafka Kinesis Spark Streaming Edward CUDA MLLib AWS Python a broad variety of machine learning methods including classifications regressions dimensionally reduction etc Utilized the engine to increase user lifetime by 45 and triple user conversations for target categories Application of various machine learning algorithms and statistical modeling like decision trees text analytics natural language processing NLP supervised and unsupervised regression models social network analysis neural networks deep learning SVM clustering to identify Volume using scikitlearn package in python Matlab Worked onanalyzing data from Google Analytics AdWords Facebook etc Evaluated models using Cross Validation Log loss function ROC curves and used AUC for feature selection and elastic technologies like ElasticSearch Kibana Performed Multinomial Logistic Regression Decision Tree Random forest SVM to classify package is going to deliver on time for the new route Performed data analysis by using Hive to retrieve the data from Hadoop cluster Sql to retrieve data from Oracle database and used ETL for data transformation Performed Data Cleaning features scaling features engineering using pandas and numpy packages in python Performed data cleaning and feature selection using MLlib package in PySpark and working with deep learning frameworks such as Caffe Neon Developed SparkScalaR Python for regular expression regex project in the HadoopHive environment with LinuxWindows for big data resources Used clustering technique KMeans to identify outliers and to classify unlabeled data Tracking operations using sensors until certain criteria is met using Air Flow technology Responsible for different Data mapping activities from Source systems to Teradata using utilities like TPump FEXPBTEQ MLOAD FLOAD etc Addressed over fitting by implementing of the algorithm regularization methods like L1 and L2 Used Principal Component Analysis in feature engineering to analyze high dimensional data Used MLlib Sparks Machine learning library to build and evaluate different models Created and designed reports that will use gathered metrics to infer and draw logical conclusions of past and future behavior Developed Map Reduce pipeline for feature extraction using Hive and Pig Created Data Quality Scripts using SQL and Hive to validate successful data load and quality of the data Created various types of data visualizations using Python and Tableau Environment Python 2x CDH5 HDFS Hadoop 23 Hive Impala AWS Linux Spark Tableau Desktop SQL Server 2014 Microsoft Excel Matlab Spark SQL Pyspark Data Scientist Cardlytics Inc Atlanta GA January 2016 to April 2017 Description Cardlytics uses purchasebased intelligence to make marketing more relevant and measurable It help marketers identify reach and influence likely buyers at scale as well as measure the true sales impact of marketing campaigns Responsibilities Used SSIS to create ETL packages to Validate Extract Transform and Load data into Data Warehouse and Data Mart Maintained and developed complex SQL queries stored procedures views functions and reports that meet customer requirements using Microsoft SQL Server 2008 R2 Created Views and Tablevalued Functions Common Table Expression CTE joins complex sub queries to provide the reporting solutions Optimized the performance of queries with modification in TSQL queries removed the unnecessary columns and redundant data normalized tables established joins and created index Created SSIS packages using Pivot Transformation Fuzzy Lookup Derived Columns Condition Split Aggregate Execute SQL Task Data Flow Task and Execute Package Task Migrated data from SAS environment to SQL Server 2008 via SQL Integration Services SSIS Developed and implemented several types of Financial Reports Income Statement Profit Loss Statement EBIT ROIC Reports by using SSRS Developed parameterized dynamic performance Reports Gross Margin Revenue base on geographic regions Profitability based on web sales and smartphone app sales and ran the reports every month and distributed them to respective departments through mailing server subscriptions and SharePoint server Designed and developed new reports and maintained existing reports using Microsoft SQL Reporting Services SSRS and Microsoft Excel to support the firms strategy and management Created subreports drill down reports summary reports parameterized reports and adhoc reports using SSRS Used SASSQL to pull data out from databases and aggregate to provide detailed reporting based on the user requirements Used SAS for preprocessing data SQL queries data analysis generating reports graphics and statistical analyses Provided statistical research analyses and data modeling support for mortgage product Perform analyses such as regression analysis logistic regression discriminant analysis cluster analysis using SAS programming Environment SQL Server 2008 R2 DB2 Oracle SQL Server Management Studio SAS BASE SASSQL SASEnterprise Guide MS BI Suite SSISSSRS TSQL SharePoint 2010 Visual Studio 2010 AgileSCRUM Data Analyst Beacon Healthcare Communications Bedminster NJ March 2014 to December 2015 Description At Beacon were all Engagement Architects people with significant industry experience engaging the 3 primary healthcare customers consumers providers and payersAnd because we also engage with one another were able to provide a more efficient integration of thinking right from the start One that looks at all the stakeholders regardless of what you hire us for Responsibilities Used SSIS to create ETL packages to Validate Extract Transform and Load data into Data Warehouse and Data Mart Collaborating with business and technology teams Data AnalysisData collection data transformation and data loading the data using different ETL systems like SSIS and Informatica Performed source to target mapping as part of data migration from JD Edwards system to Agile PDM system Data Migration testing and implementation activities using SSIS and SSRS tools of Microsoft SQL Server 2008 Involved in construction of data flow diagrams and documentation of the processes Interacted with end users for requirements study and analysis by JAD Joint Application Development Participated in system and use case modeling like activity and use case diagrams Analyzed user requirements worked with data modelers to identify entities and relationship for data modeling Actively participated in the design of data model like conceptual logical models using Erwin Used Exception handling application block for checking errorsexceptions across the website Developed Report Component so that it retrieves the data by executing Stored Procedures throw Data Access component Environment Windows Oracle MS Excel SSIS Informatica GAP Analysis ERWIN Data Analyst CybermateInfotek Limited Hyderabad Telangana December 2012 to February 2014 Description CybermateInfotek Ltd CIL is a Software solutions and IT services company was founded in May 1994 at Hyderabad INDIA and is a CIL is an offshore software development company executing projects on Web Web related technologies Both Microsoft Open Sources Responsibilities Involved in Data mapping specifications to create and execute detailed system test plans The data mapping specifies what data will be extracted from an internal data warehouse transformed and sent to an external entity Worked closely with stakeholders to understand define document business questions needed Review systemapplication requirements functional specifications test results and metrics for quality and completeness Designed and Developed Oracle PLSQL Procedures and UNIX Shell Scripts for Data ImportExport and Data Conversions Analyzed the source data coming from different sources SQL Server Oracle and also from flat files like Access and Excel and working with business users and developers to develop the Model Have Used Informatica Data Quality IDQ and Informatica Power Center as ETL tools to extract the data from various sources systems and transform them into one common format and load them into target database for the analysis purpose from Data Warehouse Accomplished data analysis statistical reports and graphs based on the business requirement using SASBase SASMacro and SASGraph SASSQL SASAccess SASODS and SASConnect Worked on Predictive Modeling using SASSQL Executed SQL queries to validate actual test results and match expected results as per financial rules Responsible for maintaining the integrity of the SQL database and reporting any issues to the database architect Design and model the reporting data warehouse considering current and future reporting requirement Involved in the daily maintenance of the database that involved monitoring the daily run of the scripts as well as troubleshooting in the event of any errors in the entire process Involved with statistical domain experts to understand the data and worked with data management team on data quality assurance Environment SQL Server Oracle PLSQL Informatics Data Quality IDQ Informatics PowerCenter Designer Workflow Manager Workflow Monitor UNIX SASBase SASMacro and SASGraph SASSQL SASAccess SASODS and SASConnect Data Analyst Rsoft India Pvtltd Bengaluru Karnataka January 2011 to November 2012 Description RSoft is a leading software product development company in India We develop Mobile Cloud based Customer Relationship Management CRM Software Solution for all size of businesses Such as Small Medium Large Size of businesses B2B and B2C enterprises to increase leads and sales opportunities Responsibilities Understanding the requirements and develop various packages in SSIS Gathered requirements from JADJAR sections with developers and business clients Designed the business requirement collection approach based on the project scope and SDLC methodology Designs and develops the logical and physical data models to support the Data Marts and the Data Warehouse Create SQL queries for product components to update FACETS backend tables and create product prefixes Involved in formatting data stores and generate UML diagrams of logical and physical data Developed project plans and manage project scope Identifieddocumented data sources and transformation rules required to populate and maintain data warehouse content Write and execute positive and negative test cases to ensure the data originating from the data warehouse Oracle  is accurate through to the SQL  in the applications Work and triage Facets configuration issues and route work back for correct processing Document step by step Facets configuration steps for the Quality Assurance team Assisted in building a Business Analysis Process Model using Rational Rose and Visio Created adhoc and custom reports using Microsoft Access Cognos BI and Crystal reports Performed extensive Requirement analysis and developed use cases and workflows Designed and developed Use Cases Activity Diagrams Sequence Diagrams and OOD Played a key role in the planning User Accepted Testing and Implementation of system enhancements and conversions Updated provider tables diagnosis tables fee schedules and service is contained in Facetsbackend tables Environment MS SQL Server 2008R2 BIDS 2008 SSIS JADJAR SSIS Cognos BI Use Cases Activity Diagrams Sequence Diagrams Education Bachelors Skills Db2 Microsoft sql server Microsoft sql server 2008 Sql server Sql server 2008 Mysql Oracle Sql Cassandra Hdfs Impala Mapreduce Oozie Sqoop Hbase Kafka Flume Hadoop Mongodb Splunk Additional Information TECHNICAL SKILLS BigDataHadoop Technologies Hadoop HDFS YARN MapReduce Hive Pig Impala Sqoop Flume Spark Kafka Storm Drill Zookeeper and Oozie Languages HTML5DHTML WSDL CSS3 C C XMLRR Studio SAS Enterprise Guide SAS R Caret Weka ggplot Perl MATLAB Mathematica FORTRAN DTD Schemas Json Ajax Java Scala Python NumPy SciPy Pandas Gensim Keras Java Script Shell Scripting NO SQL Databases Cassandra HBase MongoDB MariaDB Business Intelligence Tools Tableau server Tableau Reader Tableau Splunk SAP Business Objects OBIEE SAP Business Intelligence QlikView Amazon Redshift or Azure Data Warehouse Development Tools Microsoft SQL Studio IntelliJ Eclipse NetBeans Development Methodologies AgileScrum UML Design Patterns Waterfall Build Tools Jenkins Toad SQL Loader Maven ANT RTC RSA ControlM Oziee Hue SOAP UI Reporting Tools MS Office WordExcelPower Point VisioOutlook Crystal reports XI SSRS cognos 7060 Databases Microsoft SQL Server 12 MySQL 4x5x Oracle 11g 12c DB2 Teradata Netezza Operating Systems All versions of Windows UNIX LINUX Macintosh HD Sun Solaris",
    "entities": [
        "MLlib",
        "SSIS Gathered",
        "Beacon Healthcare Communications",
        "Python Worked as Data Architects",
        "Informatica",
        "Informatica Power Center",
        "JAD Joint Application Development Participated",
        "AUC",
        "Data Mart",
        "Karnataka",
        "BI",
        "HDFS",
        "UNIX",
        "Oozie Languages HTML5DHTML",
        "Pyspark Data Scientist Cardlytics Inc",
        "Responsibilities",
        "Google Analytics AdWords Facebook etc Evaluated",
        "Data Science",
        "Interacted",
        "Data Validation Develop",
        "CybermateInfotek Limited Hyderabad",
        "SSIS JADJAR SSIS Cognos",
        "Microsoft Access Cognos BI",
        "Caffe Neon Developed SparkScalaR Python",
        "Hadoop",
        "JADJAR",
        "Atlanta",
        "DescriptionCisco Systems Inc",
        "Modeling",
        "HBase",
        "Principal Component Analysis",
        "Access and Excel",
        "TX",
        "Created SSIS",
        "the Model Have Used Informatica Data Quality IDQ",
        "Maven ANT RTC",
        "SSIS",
        "SASConnect Worked on Predictive Modeling",
        "Air Flow technology Responsible",
        "Python",
        "SQL Server",
        "Performed Data Cleaning",
        "Data Mart Collaborating",
        "CIL",
        "Developed",
        "Data Warehouse",
        "Data Access",
        "Oracle PLSQL Informatics Data Quality IDQ Informatics",
        "Implemented Classification",
        "Dallas",
        "VM Excellent",
        "Present DescriptionPalo Alto Networks Inc",
        "Informatica Performed",
        "Financial Reports Income Statement Profit Loss Statement",
        "FACETS",
        "UML",
        "Tableau Worked",
        "Description Cardlytics",
        "ElasticSearch",
        "Data Analysis Machine Learning",
        "Mysql Oracle Sql Cassandra Hdfs",
        "Python ScikitLearn Excellent Knowledge",
        "Quality Assurance",
        "Unstructured",
        "Data Scientist",
        "Built",
        "ER Studio",
        "SDLC",
        "ROC",
        "RShiny",
        "Database Design",
        "Spark Experienced",
        "XGBoost SVM",
        "Responsibilities Understanding",
        "Spark",
        "Use Cases Activity Diagrams",
        "Palo Alto Networks",
        "Text Mining Cluster Analysis Principal Component Analysis Association",
        "OLTP Source Systems",
        "Microsoft SQL Reporting Services",
        "Perform",
        "LinuxWindows",
        "Hadoop Setup",
        "SASConnect Data",
        "SASEnterprise Guide",
        "Statistical",
        "Created",
        "Data AnalysisData collection",
        "AWS",
        "Hive and Pig Created Data Quality Scripts",
        "Oracle",
        "PySpark",
        "Sql",
        "Decision Tree Random Forest Nave Bayes Logistic Regression Cluster Analysis",
        "R2 Created Views",
        "SAS",
        "Intelligence Tools Tableau",
        "Data Integration Validation",
        "SSRS",
        "SQL",
        "OLTP",
        "Beacon",
        "NLP",
        "Bedminster NJ",
        "Data Quality",
        "RStudio",
        "OOD Played",
        "Big Data",
        "Hive",
        "Macintosh",
        "Tableau Reader Tableau Splunk SAP Business Objects OBIEE SAP Business Intelligence QlikView Amazon Redshift or Azure Data Warehouse Development Tools",
        "SQL GIT",
        "Rsoft India Pvtltd",
        "Sun Solaris",
        "Relational Data",
        "MDM",
        "GAP Analysis ERWIN",
        "ETL",
        "Validate Extract Transform",
        "User Accepted Testing and Implementation",
        "Structured Data Visualization Data Acquisition Predictive",
        "Reporting Tools MS Office",
        "Hyderabad INDIA",
        "Profitability",
        "SQL Server Oracle",
        "Random Forest Participated",
        "India",
        "Performed",
        "Data Migration",
        "Impala",
        "Customer Relationship Management CRM Software Solution",
        "Cisco",
        "TPump",
        "Description CybermateInfotek Ltd CIL",
        "MapReduce Data",
        "Linear Regression Logistic Regression Naive Bayes Decision Trees KMeans Clustering and Association Rules Experience",
        "Logistic Regression Decision",
        "Microsoft",
        "OLAP Target Systems Data",
        "Oracle SQL Server Management Studio",
        "NetBeans Development Methodologies AgileScrum UML Design",
        "Data Warehouse Accomplished",
        "Data Scientist Data Scientist Data",
        "Responsibilities Utilized Spark",
        "Nexus Business Objects Toad Power BI",
        "TSQL",
        "XI SSRS",
        "SQL Integration Services",
        "SharePoint",
        "Developed Report Component",
        "Identity Systems Coded",
        "Data Warehousing",
        "NoSQL",
        "NoSQLDatabase",
        "Tableau",
        "Machine Learning",
        "HMM",
        "SVM",
        "SQL andPython",
        "Cross Validation Log",
        "JSON XML"
    ],
    "experience": "Experience in Machine Learning algorithms such as Linear Regression Logistic Regression Naive Bayes Decision Trees KMeans Clustering and Association Rules Experience in applying predictive modeling and machine learning algorithms for analytical reports Experience using technology to work efficiently with datasets such as scripting data cleaning tools statistical software packages Developed predictive models using Decision Tree Random Forest Nave Bayes Logistic Regression Cluster Analysis and Neural Networks Very Strong in Python statistical analysis tools and modeling Experienced in Machine Learning and Statistical Analysis with Python ScikitLearn Excellent Knowledge in Relational Data WarehouseOLAP concepts Database Design and methodologies Strong SQL programming skills with experience in working with functions packages and triggers Expertise in transforming business requirements into designing algorithms analytical models building models developing data mining and reporting solutions that scales across massive volume of structured and unstructured data Skilled in performing data parsing data manipulation data architecture data ingestion and data preparation with methods including describe data contents compute descriptive statistics of data regex split and combine merge Remap subset reindex melt and reshape Worked with NoSQLDatabase including Hbase Cassandra and MongoDB Experienced in Big Data with Hadoop MapReduce HDFS and Spark Experienced in Data Integration Validation and Data Quality controls for ETL process and Data Warehousing using MS Visual Studio SSAS SSISand SSRS Proficient in Tableau and RShiny data visualization tools to analyze and obtain insights into large datasets create visually powerful and actionable interactive reports and dashboards Automated recurring reports using SQL andPython and visualized them on BI platform like Tableau Worked in development environment like Git and VM Excellent communication skills Successfully working in fastpaced multitasking environment both independently and in collaborative team a selfmotivated enthusiastic learner Experience with Big Data technologies like Hadoop and Spark would be a plus Worked and extracted data from various database sources like SQL Server Oracle and DB2 Experience working at Pricing andor Revenue Management would be a plus Familiarity with agile principles eg Scrum facilitating workshops and prototyping Hands on experience with RStudio for doing data preprocessing and building machine learning algorithms on different datasets Good Knowledge in NoSQL databases like HBase and Mongo DB Time Series Analysis ARIMA Neural Networks Sentiment Analysis Forecasting and Text Mining Cluster Analysis Principal Component Analysis Association Rules Recommender Systems Inferential Statistics Hypothesis Testing Descriptive and Sampling Work Experience Data Scientist Palo Alto Networks Dallas TX Dallas TX August 2018 to Present DescriptionPalo Alto Networks Inc an American multinational cyber security company Its core products are a platform that includes advanced firewalls and cloudbased offerings that extend those firewalls to cover other aspects of security Responsibilities Extracted data from HDFS and prepared data for exploratory analysis using data munging Built models using Statistical techniques like Bayesian HMM and Machine Learning classification models like XGBoost SVM and Random Forest Participated in all phases of data mining data cleaning data collection developing models validation and visualization and performed Gap analysis A highly immersive Data Science program involving Data Manipulation Visualization Web Scraping Machine Learning Python programming SQL GIT MongoDB Hadoop Setup storage and data analysis tools in AWS cloud computing infrastructure Installed and used Caffe Deep Learning Framework Worked on different data formats such as JSON XML and performed machine learning algorithms in Python Worked as Data Architects and IT Architects to understand the movement of data and its storage and ER Studio 97 Used pandas numpy seaborn matplotlib scikitlearn scipy NLTK in Python for developing various machine learning algorithms Data Manipulation and Aggregation from different source using Nexus Business Objects Toad Power BI and Smart View Implemented Agile Methodology for building an internal application Focus on integration overlap and Informatica newer commitment to MDM with the acquisition of Identity Systems Coded proprietary packages to analyze and visualize SPCfile data to identify bad spectra and samples to reduce unnecessary procedures and costs Programmed a utility in Python that used multiple packages numpy scipy pandas Implemented Classification using supervised algorithms like Logistic Regression Decision trees Naive Bayes KNN As Architect delivered various complex OLAPdatabasescubes scorecards dashboards and reports Updated Python scripts to match training data with our database stored in AWS Cloud Search so that we would be able to assign each document a response label for further classification Used Teradata utilities such as Fast Export MLOAD for handling various tasks data migrationETL from OLTP Source Systems to OLAP Target Systems Data transformation from various resources data organization features extraction from raw and stored Validated the machine learning classifiers using ROC Curves and Lift Charts Environment Unix Python 352 MLLib SAS regression logistic regression Hadoop 274 NoSQL Teradata OLTP random forest OLAP HDFS ODS NLTK SVM JSON XML and MapReduce Data Scientist Cisco Dallas TX May 2017 to July 2018 DescriptionCisco Systems Inc is an American multinational technology conglomerate that develops manufactures and sells networking hardware telecommunications equipment and other hightechnology services and products Cisco helps seize the opportunities of tomorrow by proving that amazing things can happen when you connect the unconnected An integral part of our DNA is creating longlasting customer partnerships working together to identify our customers needs and provide solutions that fuel their success Responsibilities Utilized Spark Scala Hadoop HQL VQL oozie pySpark Data Lake Tensor Flow HBase Cassandra Redshift MongoDB Kafka Kinesis Spark Streaming Edward CUDA MLLib AWS Python a broad variety of machine learning methods including classifications regressions dimensionally reduction etc Utilized the engine to increase user lifetime by 45 and triple user conversations for target categories Application of various machine learning algorithms and statistical modeling like decision trees text analytics natural language processing NLP supervised and unsupervised regression models social network analysis neural networks deep learning SVM clustering to identify Volume using scikitlearn package in python Matlab Worked onanalyzing data from Google Analytics AdWords Facebook etc Evaluated models using Cross Validation Log loss function ROC curves and used AUC for feature selection and elastic technologies like ElasticSearch Kibana Performed Multinomial Logistic Regression Decision Tree Random forest SVM to classify package is going to deliver on time for the new route Performed data analysis by using Hive to retrieve the data from Hadoop cluster Sql to retrieve data from Oracle database and used ETL for data transformation Performed Data Cleaning features scaling features engineering using pandas and numpy packages in python Performed data cleaning and feature selection using MLlib package in PySpark and working with deep learning frameworks such as Caffe Neon Developed SparkScalaR Python for regular expression regex project in the HadoopHive environment with LinuxWindows for big data resources Used clustering technique KMeans to identify outliers and to classify unlabeled data Tracking operations using sensors until certain criteria is met using Air Flow technology Responsible for different Data mapping activities from Source systems to Teradata using utilities like TPump FEXPBTEQ MLOAD FLOAD etc Addressed over fitting by implementing of the algorithm regularization methods like L1 and L2 Used Principal Component Analysis in feature engineering to analyze high dimensional data Used MLlib Sparks Machine learning library to build and evaluate different models Created and designed reports that will use gathered metrics to infer and draw logical conclusions of past and future behavior Developed Map Reduce pipeline for feature extraction using Hive and Pig Created Data Quality Scripts using SQL and Hive to validate successful data load and quality of the data Created various types of data visualizations using Python and Tableau Environment Python 2x CDH5 HDFS Hadoop 23 Hive Impala AWS Linux Spark Tableau Desktop SQL Server 2014 Microsoft Excel Matlab Spark SQL Pyspark Data Scientist Cardlytics Inc Atlanta GA January 2016 to April 2017 Description Cardlytics uses purchasebased intelligence to make marketing more relevant and measurable It help marketers identify reach and influence likely buyers at scale as well as measure the true sales impact of marketing campaigns Responsibilities Used SSIS to create ETL packages to Validate Extract Transform and Load data into Data Warehouse and Data Mart Maintained and developed complex SQL queries stored procedures views functions and reports that meet customer requirements using Microsoft SQL Server 2008 R2 Created Views and Tablevalued Functions Common Table Expression CTE joins complex sub queries to provide the reporting solutions Optimized the performance of queries with modification in TSQL queries removed the unnecessary columns and redundant data normalized tables established joins and created index Created SSIS packages using Pivot Transformation Fuzzy Lookup Derived Columns Condition Split Aggregate Execute SQL Task Data Flow Task and Execute Package Task Migrated data from SAS environment to SQL Server 2008 via SQL Integration Services SSIS Developed and implemented several types of Financial Reports Income Statement Profit Loss Statement EBIT ROIC Reports by using SSRS Developed parameterized dynamic performance Reports Gross Margin Revenue base on geographic regions Profitability based on web sales and smartphone app sales and ran the reports every month and distributed them to respective departments through mailing server subscriptions and SharePoint server Designed and developed new reports and maintained existing reports using Microsoft SQL Reporting Services SSRS and Microsoft Excel to support the firms strategy and management Created subreports drill down reports summary reports parameterized reports and adhoc reports using SSRS Used SASSQL to pull data out from databases and aggregate to provide detailed reporting based on the user requirements Used SAS for preprocessing data SQL queries data analysis generating reports graphics and statistical analyses Provided statistical research analyses and data modeling support for mortgage product Perform analyses such as regression analysis logistic regression discriminant analysis cluster analysis using SAS programming Environment SQL Server 2008 R2 DB2 Oracle SQL Server Management Studio SAS BASE SASSQL SASEnterprise Guide MS BI Suite SSISSSRS TSQL SharePoint 2010 Visual Studio 2010 AgileSCRUM Data Analyst Beacon Healthcare Communications Bedminster NJ March 2014 to December 2015 Description At Beacon were all Engagement Architects people with significant industry experience engaging the 3 primary healthcare customers consumers providers and payersAnd because we also engage with one another were able to provide a more efficient integration of thinking right from the start One that looks at all the stakeholders regardless of what you hire us for Responsibilities Used SSIS to create ETL packages to Validate Extract Transform and Load data into Data Warehouse and Data Mart Collaborating with business and technology teams Data AnalysisData collection data transformation and data loading the data using different ETL systems like SSIS and Informatica Performed source to target mapping as part of data migration from JD Edwards system to Agile PDM system Data Migration testing and implementation activities using SSIS and SSRS tools of Microsoft SQL Server 2008 Involved in construction of data flow diagrams and documentation of the processes Interacted with end users for requirements study and analysis by JAD Joint Application Development Participated in system and use case modeling like activity and use case diagrams Analyzed user requirements worked with data modelers to identify entities and relationship for data modeling Actively participated in the design of data model like conceptual logical models using Erwin Used Exception handling application block for checking errorsexceptions across the website Developed Report Component so that it retrieves the data by executing Stored Procedures throw Data Access component Environment Windows Oracle MS Excel SSIS Informatica GAP Analysis ERWIN Data Analyst CybermateInfotek Limited Hyderabad Telangana December 2012 to February 2014 Description CybermateInfotek Ltd CIL is a Software solutions and IT services company was founded in May 1994 at Hyderabad INDIA and is a CIL is an offshore software development company executing projects on Web Web related technologies Both Microsoft Open Sources Responsibilities Involved in Data mapping specifications to create and execute detailed system test plans The data mapping specifies what data will be extracted from an internal data warehouse transformed and sent to an external entity Worked closely with stakeholders to understand define document business questions needed Review systemapplication requirements functional specifications test results and metrics for quality and completeness Designed and Developed Oracle PLSQL Procedures and UNIX Shell Scripts for Data ImportExport and Data Conversions Analyzed the source data coming from different sources SQL Server Oracle and also from flat files like Access and Excel and working with business users and developers to develop the Model Have Used Informatica Data Quality IDQ and Informatica Power Center as ETL tools to extract the data from various sources systems and transform them into one common format and load them into target database for the analysis purpose from Data Warehouse Accomplished data analysis statistical reports and graphs based on the business requirement using SASBase SASMacro and SASGraph SASSQL SASAccess SASODS and SASConnect Worked on Predictive Modeling using SASSQL Executed SQL queries to validate actual test results and match expected results as per financial rules Responsible for maintaining the integrity of the SQL database and reporting any issues to the database architect Design and model the reporting data warehouse considering current and future reporting requirement Involved in the daily maintenance of the database that involved monitoring the daily run of the scripts as well as troubleshooting in the event of any errors in the entire process Involved with statistical domain experts to understand the data and worked with data management team on data quality assurance Environment SQL Server Oracle PLSQL Informatics Data Quality IDQ Informatics PowerCenter Designer Workflow Manager Workflow Monitor UNIX SASBase SASMacro and SASGraph SASSQL SASAccess SASODS and SASConnect Data Analyst Rsoft India Pvtltd Bengaluru Karnataka January 2011 to November 2012 Description RSoft is a leading software product development company in India We develop Mobile Cloud based Customer Relationship Management CRM Software Solution for all size of businesses Such as Small Medium Large Size of businesses B2B and B2C enterprises to increase leads and sales opportunities Responsibilities Understanding the requirements and develop various packages in SSIS Gathered requirements from JADJAR sections with developers and business clients Designed the business requirement collection approach based on the project scope and SDLC methodology Designs and develops the logical and physical data models to support the Data Marts and the Data Warehouse Create SQL queries for product components to update FACETS backend tables and create product prefixes Involved in formatting data stores and generate UML diagrams of logical and physical data Developed project plans and manage project scope Identifieddocumented data sources and transformation rules required to populate and maintain data warehouse content Write and execute positive and negative test cases to ensure the data originating from the data warehouse Oracle   is accurate through to the SQL   in the applications Work and triage Facets configuration issues and route work back for correct processing Document step by step Facets configuration steps for the Quality Assurance team Assisted in building a Business Analysis Process Model using Rational Rose and Visio Created adhoc and custom reports using Microsoft Access Cognos BI and Crystal reports Performed extensive Requirement analysis and developed use cases and workflows Designed and developed Use Cases Activity Diagrams Sequence Diagrams and OOD Played a key role in the planning User Accepted Testing and Implementation of system enhancements and conversions Updated provider tables diagnosis tables fee schedules and service is contained in Facetsbackend tables Environment MS SQL Server 2008R2 BIDS 2008 SSIS JADJAR SSIS Cognos BI Use Cases Activity Diagrams Sequence Diagrams Education Bachelors Skills Db2 Microsoft sql server Microsoft sql server 2008 Sql server Sql server 2008 Mysql Oracle Sql Cassandra Hdfs Impala Mapreduce Oozie Sqoop Hbase Kafka Flume Hadoop Mongodb Splunk Additional Information TECHNICAL SKILLS BigDataHadoop Technologies Hadoop HDFS YARN MapReduce Hive Pig Impala Sqoop Flume Spark Kafka Storm Drill Zookeeper and Oozie Languages HTML5DHTML WSDL CSS3 C C XMLRR Studio SAS Enterprise Guide SAS R Caret Weka ggplot Perl MATLAB Mathematica FORTRAN DTD Schemas Json Ajax Java Scala Python NumPy SciPy Pandas Gensim Keras Java Script Shell Scripting NO SQL Databases Cassandra HBase MongoDB MariaDB Business Intelligence Tools Tableau server Tableau Reader Tableau Splunk SAP Business Objects OBIEE SAP Business Intelligence QlikView Amazon Redshift or Azure Data Warehouse Development Tools Microsoft SQL Studio IntelliJ Eclipse NetBeans Development Methodologies AgileScrum UML Design Patterns Waterfall Build Tools Jenkins Toad SQL Loader Maven ANT RTC RSA ControlM Oziee Hue SOAP UI Reporting Tools MS Office WordExcelPower Point VisioOutlook Crystal reports XI SSRS cognos 7060 Databases Microsoft SQL Server 12 MySQL 4x5x Oracle 11 g 12c DB2 Teradata Netezza Operating Systems All versions of Windows UNIX LINUX Macintosh HD Sun Solaris",
    "extracted_keywords": [
        "Data",
        "Scientist",
        "Data",
        "Scientist",
        "Data",
        "Scientist",
        "Palo",
        "Alto",
        "Networks",
        "Dallas",
        "TX",
        "Dallas",
        "TX",
        "Above",
        "years",
        "experience",
        "data",
        "Datasets",
        "Structured",
        "Data",
        "Visualization",
        "Data",
        "Acquisition",
        "Predictive",
        "Data",
        "Validation",
        "Develop",
        "tools",
        "methodologies",
        "data",
        "science",
        "performance",
        "Data",
        "Scientist",
        "expertise",
        "Data",
        "Analysis",
        "Machine",
        "Learning",
        "Modeling",
        "Experience",
        "Machine",
        "Learning",
        "Linear",
        "Regression",
        "Logistic",
        "Regression",
        "Naive",
        "Bayes",
        "Decision",
        "Trees",
        "KMeans",
        "Clustering",
        "Association",
        "Rules",
        "Experience",
        "modeling",
        "machine",
        "algorithms",
        "reports",
        "Experience",
        "technology",
        "datasets",
        "data",
        "cleaning",
        "tools",
        "software",
        "packages",
        "models",
        "Decision",
        "Tree",
        "Random",
        "Forest",
        "Nave",
        "Bayes",
        "Logistic",
        "Regression",
        "Cluster",
        "Analysis",
        "Neural",
        "Networks",
        "Strong",
        "Python",
        "analysis",
        "tools",
        "modeling",
        "Machine",
        "Learning",
        "Statistical",
        "Analysis",
        "Python",
        "ScikitLearn",
        "Excellent",
        "Knowledge",
        "Relational",
        "Data",
        "WarehouseOLAP",
        "concepts",
        "Database",
        "Design",
        "Strong",
        "SQL",
        "programming",
        "skills",
        "experience",
        "functions",
        "packages",
        "Expertise",
        "business",
        "requirements",
        "algorithms",
        "models",
        "building",
        "models",
        "data",
        "mining",
        "reporting",
        "solutions",
        "volume",
        "data",
        "data",
        "data",
        "manipulation",
        "data",
        "architecture",
        "data",
        "ingestion",
        "data",
        "preparation",
        "methods",
        "describe",
        "data",
        "contents",
        "statistics",
        "data",
        "regex",
        "Remap",
        "subset",
        "reindex",
        "melt",
        "reshape",
        "NoSQLDatabase",
        "Hbase",
        "Cassandra",
        "MongoDB",
        "Big",
        "Data",
        "Hadoop",
        "MapReduce",
        "HDFS",
        "Spark",
        "Data",
        "Integration",
        "Validation",
        "Data",
        "Quality",
        "ETL",
        "process",
        "Data",
        "Warehousing",
        "MS",
        "Visual",
        "Studio",
        "SSAS",
        "SSISand",
        "SSRS",
        "Proficient",
        "Tableau",
        "RShiny",
        "data",
        "visualization",
        "tools",
        "insights",
        "datasets",
        "reports",
        "dashboards",
        "reports",
        "SQL",
        "andPython",
        "BI",
        "platform",
        "Tableau",
        "development",
        "environment",
        "Git",
        "VM",
        "Excellent",
        "communication",
        "skills",
        "environment",
        "team",
        "learner",
        "Experience",
        "Big",
        "Data",
        "technologies",
        "Hadoop",
        "Spark",
        "Worked",
        "data",
        "database",
        "sources",
        "SQL",
        "Server",
        "Oracle",
        "DB2",
        "Experience",
        "Pricing",
        "andor",
        "Revenue",
        "Management",
        "Familiarity",
        "principles",
        "eg",
        "Scrum",
        "workshops",
        "Hands",
        "experience",
        "RStudio",
        "data",
        "machine",
        "algorithms",
        "datasets",
        "Good",
        "Knowledge",
        "NoSQL",
        "HBase",
        "Mongo",
        "DB",
        "Time",
        "Series",
        "Analysis",
        "ARIMA",
        "Neural",
        "Networks",
        "Sentiment",
        "Analysis",
        "Forecasting",
        "Text",
        "Mining",
        "Cluster",
        "Analysis",
        "Principal",
        "Component",
        "Analysis",
        "Association",
        "Rules",
        "Recommender",
        "Systems",
        "Inferential",
        "Statistics",
        "Hypothesis",
        "Testing",
        "Descriptive",
        "Sampling",
        "Work",
        "Experience",
        "Data",
        "Scientist",
        "Palo",
        "Alto",
        "Networks",
        "Dallas",
        "TX",
        "Dallas",
        "TX",
        "August",
        "Present",
        "DescriptionPalo",
        "Alto",
        "Networks",
        "Inc",
        "cyber",
        "security",
        "company",
        "core",
        "products",
        "platform",
        "firewalls",
        "offerings",
        "firewalls",
        "aspects",
        "security",
        "Responsibilities",
        "data",
        "HDFS",
        "data",
        "analysis",
        "data",
        "models",
        "techniques",
        "Bayesian",
        "HMM",
        "Machine",
        "Learning",
        "classification",
        "models",
        "XGBoost",
        "SVM",
        "Random",
        "Forest",
        "phases",
        "data",
        "mining",
        "data",
        "data",
        "collection",
        "models",
        "validation",
        "visualization",
        "analysis",
        "Data",
        "Science",
        "program",
        "Data",
        "Manipulation",
        "Visualization",
        "Web",
        "Machine",
        "Learning",
        "Python",
        "programming",
        "SQL",
        "GIT",
        "MongoDB",
        "Hadoop",
        "Setup",
        "storage",
        "data",
        "analysis",
        "tools",
        "AWS",
        "cloud",
        "infrastructure",
        "Caffe",
        "Deep",
        "Learning",
        "Framework",
        "data",
        "formats",
        "JSON",
        "XML",
        "machine",
        "algorithms",
        "Python",
        "Worked",
        "Data",
        "Architects",
        "IT",
        "Architects",
        "movement",
        "data",
        "storage",
        "ER",
        "Studio",
        "pandas",
        "numpy",
        "matplotlib",
        "NLTK",
        "Python",
        "machine",
        "learning",
        "Data",
        "Manipulation",
        "Aggregation",
        "source",
        "Nexus",
        "Business",
        "Toad",
        "Power",
        "BI",
        "Smart",
        "View",
        "Agile",
        "Methodology",
        "application",
        "Focus",
        "integration",
        "overlap",
        "Informatica",
        "commitment",
        "MDM",
        "acquisition",
        "Identity",
        "Systems",
        "packages",
        "SPCfile",
        "data",
        "spectra",
        "samples",
        "procedures",
        "costs",
        "utility",
        "Python",
        "packages",
        "scipy",
        "Implemented",
        "Classification",
        "algorithms",
        "Logistic",
        "Regression",
        "Decision",
        "trees",
        "Bayes",
        "KNN",
        "Architect",
        "OLAPdatabasescubes",
        "scorecards",
        "dashboards",
        "reports",
        "Updated",
        "Python",
        "scripts",
        "training",
        "data",
        "database",
        "AWS",
        "Cloud",
        "Search",
        "document",
        "response",
        "label",
        "classification",
        "Teradata",
        "utilities",
        "Fast",
        "Export",
        "MLOAD",
        "tasks",
        "data",
        "migrationETL",
        "OLTP",
        "Source",
        "Systems",
        "OLAP",
        "Target",
        "Systems",
        "Data",
        "transformation",
        "resources",
        "data",
        "organization",
        "extraction",
        "machine",
        "classifiers",
        "ROC",
        "Curves",
        "Lift",
        "Charts",
        "Environment",
        "Unix",
        "Python",
        "MLLib",
        "SAS",
        "regression",
        "regression",
        "Hadoop",
        "NoSQL",
        "Teradata",
        "OLTP",
        "forest",
        "OLAP",
        "HDFS",
        "ODS",
        "NLTK",
        "SVM",
        "JSON",
        "XML",
        "MapReduce",
        "Data",
        "Scientist",
        "Cisco",
        "Dallas",
        "TX",
        "May",
        "July",
        "DescriptionCisco",
        "Systems",
        "Inc",
        "technology",
        "conglomerate",
        "manufactures",
        "networking",
        "hardware",
        "telecommunications",
        "equipment",
        "hightechnology",
        "services",
        "products",
        "Cisco",
        "opportunities",
        "tomorrow",
        "things",
        "part",
        "DNA",
        "customer",
        "partnerships",
        "customers",
        "solutions",
        "success",
        "Responsibilities",
        "Spark",
        "Scala",
        "Hadoop",
        "HQL",
        "VQL",
        "oozie",
        "pySpark",
        "Data",
        "Lake",
        "Tensor",
        "Flow",
        "HBase",
        "Cassandra",
        "Redshift",
        "MongoDB",
        "Kafka",
        "Kinesis",
        "Spark",
        "Streaming",
        "Edward",
        "CUDA",
        "AWS",
        "Python",
        "variety",
        "machine",
        "learning",
        "methods",
        "classifications",
        "regressions",
        "reduction",
        "engine",
        "user",
        "lifetime",
        "user",
        "conversations",
        "target",
        "categories",
        "Application",
        "machine",
        "algorithms",
        "modeling",
        "decision",
        "trees",
        "text",
        "analytics",
        "language",
        "processing",
        "NLP",
        "regression",
        "models",
        "network",
        "analysis",
        "networks",
        "SVM",
        "Volume",
        "package",
        "python",
        "Matlab",
        "Worked",
        "data",
        "Google",
        "Analytics",
        "AdWords",
        "Facebook",
        "models",
        "Cross",
        "Validation",
        "Log",
        "loss",
        "function",
        "ROC",
        "curves",
        "AUC",
        "feature",
        "selection",
        "technologies",
        "ElasticSearch",
        "Kibana",
        "Performed",
        "Multinomial",
        "Logistic",
        "Regression",
        "Decision",
        "Tree",
        "Random",
        "forest",
        "SVM",
        "package",
        "time",
        "route",
        "Performed",
        "data",
        "analysis",
        "Hive",
        "data",
        "Hadoop",
        "cluster",
        "Sql",
        "data",
        "Oracle",
        "database",
        "ETL",
        "data",
        "transformation",
        "Performed",
        "Data",
        "Cleaning",
        "features",
        "engineering",
        "pandas",
        "packages",
        "python",
        "Performed",
        "data",
        "cleaning",
        "feature",
        "selection",
        "MLlib",
        "package",
        "PySpark",
        "frameworks",
        "Caffe",
        "Neon",
        "SparkScalaR",
        "Python",
        "expression",
        "regex",
        "project",
        "environment",
        "LinuxWindows",
        "data",
        "resources",
        "clustering",
        "technique",
        "KMeans",
        "outliers",
        "data",
        "Tracking",
        "operations",
        "sensors",
        "criteria",
        "Air",
        "Flow",
        "technology",
        "Data",
        "mapping",
        "activities",
        "Source",
        "systems",
        "Teradata",
        "utilities",
        "TPump",
        "FEXPBTEQ",
        "MLOAD",
        "FLOAD",
        "algorithm",
        "regularization",
        "methods",
        "L1",
        "L2",
        "Principal",
        "Component",
        "Analysis",
        "feature",
        "engineering",
        "data",
        "MLlib",
        "Sparks",
        "Machine",
        "library",
        "models",
        "reports",
        "metrics",
        "conclusions",
        "behavior",
        "Developed",
        "Map",
        "pipeline",
        "feature",
        "extraction",
        "Hive",
        "Pig",
        "Created",
        "Data",
        "Quality",
        "Scripts",
        "SQL",
        "Hive",
        "data",
        "load",
        "quality",
        "data",
        "types",
        "data",
        "visualizations",
        "Python",
        "Tableau",
        "Environment",
        "Python",
        "CDH5",
        "HDFS",
        "Hadoop",
        "Hive",
        "Impala",
        "Linux",
        "Spark",
        "Tableau",
        "Desktop",
        "SQL",
        "Server",
        "Microsoft",
        "Excel",
        "Matlab",
        "Spark",
        "SQL",
        "Pyspark",
        "Data",
        "Scientist",
        "Cardlytics",
        "Inc",
        "Atlanta",
        "GA",
        "January",
        "April",
        "Description",
        "Cardlytics",
        "intelligence",
        "marketing",
        "marketers",
        "buyers",
        "scale",
        "sales",
        "impact",
        "marketing",
        "campaigns",
        "Responsibilities",
        "SSIS",
        "ETL",
        "packages",
        "Validate",
        "Extract",
        "Transform",
        "Load",
        "data",
        "Data",
        "Warehouse",
        "Data",
        "Mart",
        "SQL",
        "queries",
        "procedures",
        "views",
        "functions",
        "reports",
        "customer",
        "requirements",
        "Microsoft",
        "SQL",
        "Server",
        "R2",
        "Views",
        "Tablevalued",
        "Functions",
        "Common",
        "Table",
        "Expression",
        "CTE",
        "sub",
        "reporting",
        "solutions",
        "performance",
        "queries",
        "modification",
        "TSQL",
        "queries",
        "columns",
        "data",
        "tables",
        "joins",
        "index",
        "SSIS",
        "packages",
        "Pivot",
        "Transformation",
        "Fuzzy",
        "Lookup",
        "Derived",
        "Columns",
        "Condition",
        "Split",
        "Aggregate",
        "Execute",
        "SQL",
        "Task",
        "Data",
        "Flow",
        "Task",
        "Execute",
        "Package",
        "Task",
        "data",
        "SAS",
        "environment",
        "SQL",
        "Server",
        "SQL",
        "Integration",
        "Services",
        "SSIS",
        "Developed",
        "types",
        "Financial",
        "Reports",
        "Income",
        "Statement",
        "Profit",
        "Loss",
        "Statement",
        "ROIC",
        "Reports",
        "SSRS",
        "Developed",
        "performance",
        "Reports",
        "Gross",
        "Margin",
        "Revenue",
        "base",
        "regions",
        "Profitability",
        "web",
        "sales",
        "app",
        "sales",
        "reports",
        "month",
        "departments",
        "server",
        "subscriptions",
        "SharePoint",
        "server",
        "reports",
        "reports",
        "Microsoft",
        "SQL",
        "Reporting",
        "Services",
        "SSRS",
        "Microsoft",
        "Excel",
        "firms",
        "strategy",
        "management",
        "Created",
        "subreports",
        "reports",
        "summary",
        "reports",
        "parameterized",
        "reports",
        "reports",
        "SSRS",
        "SASSQL",
        "data",
        "databases",
        "reporting",
        "user",
        "requirements",
        "SAS",
        "data",
        "SQL",
        "data",
        "analysis",
        "generating",
        "reports",
        "graphics",
        "analyses",
        "research",
        "analyses",
        "data",
        "modeling",
        "support",
        "mortgage",
        "product",
        "Perform",
        "analyses",
        "regression",
        "analysis",
        "regression",
        "discriminant",
        "analysis",
        "cluster",
        "analysis",
        "SAS",
        "programming",
        "Environment",
        "SQL",
        "Server",
        "R2",
        "DB2",
        "Oracle",
        "SQL",
        "Server",
        "Management",
        "Studio",
        "SAS",
        "BASE",
        "SASSQL",
        "SASEnterprise",
        "Guide",
        "MS",
        "BI",
        "Suite",
        "SSISSSRS",
        "TSQL",
        "SharePoint",
        "Visual",
        "Studio",
        "AgileSCRUM",
        "Data",
        "Analyst",
        "Beacon",
        "Healthcare",
        "Communications",
        "Bedminster",
        "NJ",
        "March",
        "December",
        "Description",
        "Beacon",
        "Engagement",
        "Architects",
        "people",
        "industry",
        "experience",
        "healthcare",
        "customers",
        "consumers",
        "providers",
        "payersAnd",
        "integration",
        "start",
        "stakeholders",
        "Responsibilities",
        "SSIS",
        "ETL",
        "packages",
        "Validate",
        "Extract",
        "Transform",
        "Load",
        "data",
        "Data",
        "Warehouse",
        "Data",
        "Mart",
        "business",
        "technology",
        "teams",
        "Data",
        "AnalysisData",
        "collection",
        "data",
        "transformation",
        "data",
        "data",
        "ETL",
        "systems",
        "SSIS",
        "Informatica",
        "Performed",
        "source",
        "mapping",
        "part",
        "data",
        "migration",
        "JD",
        "Edwards",
        "system",
        "PDM",
        "system",
        "Data",
        "Migration",
        "testing",
        "implementation",
        "activities",
        "SSIS",
        "SSRS",
        "tools",
        "Microsoft",
        "SQL",
        "Server",
        "construction",
        "data",
        "flow",
        "diagrams",
        "documentation",
        "processes",
        "end",
        "users",
        "requirements",
        "study",
        "analysis",
        "JAD",
        "Joint",
        "Application",
        "Development",
        "system",
        "case",
        "activity",
        "use",
        "case",
        "diagrams",
        "user",
        "requirements",
        "data",
        "modelers",
        "entities",
        "relationship",
        "data",
        "modeling",
        "design",
        "data",
        "model",
        "models",
        "Erwin",
        "Exception",
        "handling",
        "application",
        "block",
        "errorsexceptions",
        "website",
        "Developed",
        "Report",
        "Component",
        "data",
        "Stored",
        "Procedures",
        "Data",
        "Access",
        "component",
        "Environment",
        "Windows",
        "Oracle",
        "MS",
        "Excel",
        "SSIS",
        "Informatica",
        "GAP",
        "Analysis",
        "ERWIN",
        "Data",
        "Analyst",
        "CybermateInfotek",
        "Limited",
        "Hyderabad",
        "Telangana",
        "December",
        "February",
        "Description",
        "CybermateInfotek",
        "Ltd",
        "CIL",
        "Software",
        "solutions",
        "IT",
        "services",
        "company",
        "May",
        "Hyderabad",
        "INDIA",
        "CIL",
        "software",
        "development",
        "company",
        "projects",
        "Web",
        "Web",
        "technologies",
        "Microsoft",
        "Open",
        "Sources",
        "Responsibilities",
        "Data",
        "mapping",
        "specifications",
        "system",
        "test",
        "data",
        "mapping",
        "specifies",
        "data",
        "data",
        "warehouse",
        "entity",
        "stakeholders",
        "document",
        "business",
        "questions",
        "Review",
        "systemapplication",
        "requirements",
        "specifications",
        "results",
        "metrics",
        "quality",
        "completeness",
        "Developed",
        "Oracle",
        "PLSQL",
        "Procedures",
        "UNIX",
        "Shell",
        "Scripts",
        "Data",
        "ImportExport",
        "Data",
        "Conversions",
        "source",
        "data",
        "sources",
        "SQL",
        "Server",
        "Oracle",
        "files",
        "Access",
        "Excel",
        "business",
        "users",
        "developers",
        "Model",
        "Informatica",
        "Data",
        "Quality",
        "IDQ",
        "Informatica",
        "Power",
        "Center",
        "ETL",
        "tools",
        "data",
        "sources",
        "systems",
        "format",
        "target",
        "database",
        "analysis",
        "purpose",
        "Data",
        "Warehouse",
        "data",
        "analysis",
        "reports",
        "graphs",
        "business",
        "requirement",
        "SASBase",
        "SASMacro",
        "SASGraph",
        "SASAccess",
        "SASODS",
        "SASConnect",
        "Predictive",
        "Modeling",
        "SQL",
        "test",
        "results",
        "match",
        "results",
        "rules",
        "integrity",
        "SQL",
        "database",
        "issues",
        "database",
        "architect",
        "Design",
        "reporting",
        "data",
        "warehouse",
        "reporting",
        "requirement",
        "maintenance",
        "database",
        "run",
        "scripts",
        "event",
        "errors",
        "process",
        "domain",
        "experts",
        "data",
        "data",
        "management",
        "team",
        "data",
        "quality",
        "assurance",
        "Environment",
        "SQL",
        "Server",
        "Oracle",
        "PLSQL",
        "Informatics",
        "Data",
        "Quality",
        "IDQ",
        "Informatics",
        "PowerCenter",
        "Designer",
        "Workflow",
        "Manager",
        "Workflow",
        "Monitor",
        "UNIX",
        "SASMacro",
        "SASGraph",
        "SASAccess",
        "SASODS",
        "SASConnect",
        "Data",
        "Analyst",
        "Rsoft",
        "India",
        "Pvtltd",
        "Bengaluru",
        "Karnataka",
        "January",
        "November",
        "Description",
        "RSoft",
        "software",
        "product",
        "development",
        "company",
        "India",
        "Mobile",
        "Cloud",
        "Customer",
        "Relationship",
        "Management",
        "CRM",
        "Software",
        "Solution",
        "size",
        "businesses",
        "Small",
        "Medium",
        "Large",
        "Size",
        "businesses",
        "B2B",
        "B2C",
        "enterprises",
        "leads",
        "sales",
        "opportunities",
        "Responsibilities",
        "requirements",
        "packages",
        "SSIS",
        "Gathered",
        "requirements",
        "JADJAR",
        "sections",
        "developers",
        "business",
        "clients",
        "business",
        "requirement",
        "collection",
        "approach",
        "project",
        "scope",
        "SDLC",
        "methodology",
        "data",
        "models",
        "Data",
        "Marts",
        "Data",
        "Warehouse",
        "SQL",
        "product",
        "components",
        "FACETS",
        "backend",
        "tables",
        "product",
        "prefixes",
        "data",
        "stores",
        "UML",
        "diagrams",
        "data",
        "project",
        "plans",
        "project",
        "scope",
        "Identifieddocumented",
        "data",
        "sources",
        "transformation",
        "rules",
        "data",
        "warehouse",
        "content",
        "test",
        "cases",
        "data",
        "data",
        "warehouse",
        "Oracle",
        "SQL",
        "applications",
        "Work",
        "triage",
        "Facets",
        "configuration",
        "issues",
        "route",
        "processing",
        "Document",
        "step",
        "step",
        "Facets",
        "configuration",
        "steps",
        "Quality",
        "Assurance",
        "team",
        "Business",
        "Analysis",
        "Process",
        "Model",
        "Rational",
        "Rose",
        "Visio",
        "adhoc",
        "custom",
        "reports",
        "Microsoft",
        "Access",
        "Cognos",
        "BI",
        "Crystal",
        "reports",
        "Requirement",
        "analysis",
        "use",
        "cases",
        "workflows",
        "Use",
        "Cases",
        "Activity",
        "Diagrams",
        "Sequence",
        "Diagrams",
        "OOD",
        "role",
        "planning",
        "User",
        "Accepted",
        "Testing",
        "Implementation",
        "system",
        "enhancements",
        "conversions",
        "provider",
        "tables",
        "diagnosis",
        "tables",
        "fee",
        "schedules",
        "service",
        "Facetsbackend",
        "tables",
        "Environment",
        "MS",
        "SQL",
        "Server",
        "2008R2",
        "BIDS",
        "SSIS",
        "JADJAR",
        "SSIS",
        "Cognos",
        "BI",
        "Use",
        "Cases",
        "Activity",
        "Diagrams",
        "Sequence",
        "Diagrams",
        "Education",
        "Bachelors",
        "Skills",
        "Microsoft",
        "server",
        "Microsoft",
        "server",
        "Sql",
        "server",
        "Sql",
        "server",
        "Mysql",
        "Oracle",
        "Sql",
        "Cassandra",
        "Hdfs",
        "Impala",
        "Mapreduce",
        "Oozie",
        "Sqoop",
        "Hbase",
        "Kafka",
        "Flume",
        "Hadoop",
        "Mongodb",
        "Splunk",
        "Additional",
        "Information",
        "TECHNICAL",
        "SKILLS",
        "BigDataHadoop",
        "Technologies",
        "Hadoop",
        "HDFS",
        "YARN",
        "MapReduce",
        "Hive",
        "Pig",
        "Impala",
        "Sqoop",
        "Flume",
        "Spark",
        "Kafka",
        "Storm",
        "Drill",
        "Zookeeper",
        "Oozie",
        "Languages",
        "WSDL",
        "CSS3",
        "C",
        "C",
        "XMLRR",
        "Studio",
        "SAS",
        "Enterprise",
        "Guide",
        "SAS",
        "R",
        "Caret",
        "Weka",
        "ggplot",
        "Perl",
        "MATLAB",
        "Mathematica",
        "FORTRAN",
        "DTD",
        "Schemas",
        "Json",
        "Ajax",
        "Java",
        "Scala",
        "Python",
        "NumPy",
        "SciPy",
        "Pandas",
        "Gensim",
        "Keras",
        "Java",
        "Script",
        "Shell",
        "Scripting",
        "SQL",
        "Databases",
        "Cassandra",
        "HBase",
        "MongoDB",
        "MariaDB",
        "Business",
        "Intelligence",
        "Tools",
        "Tableau",
        "server",
        "Tableau",
        "Reader",
        "Tableau",
        "Splunk",
        "SAP",
        "Business",
        "OBIEE",
        "SAP",
        "Business",
        "Intelligence",
        "QlikView",
        "Amazon",
        "Redshift",
        "Azure",
        "Data",
        "Warehouse",
        "Development",
        "Tools",
        "Microsoft",
        "SQL",
        "Studio",
        "Eclipse",
        "NetBeans",
        "Development",
        "Methodologies",
        "AgileScrum",
        "UML",
        "Design",
        "Patterns",
        "Waterfall",
        "Build",
        "Tools",
        "Jenkins",
        "Toad",
        "SQL",
        "Loader",
        "Maven",
        "ANT",
        "RTC",
        "RSA",
        "ControlM",
        "Oziee",
        "Hue",
        "SOAP",
        "UI",
        "Reporting",
        "Tools",
        "MS",
        "Office",
        "WordExcelPower",
        "Point",
        "VisioOutlook",
        "Crystal",
        "XI",
        "SSRS",
        "cognos",
        "Microsoft",
        "SQL",
        "Server",
        "MySQL",
        "Oracle",
        "g",
        "DB2",
        "Teradata",
        "Netezza",
        "Operating",
        "Systems",
        "versions",
        "Windows",
        "UNIX",
        "LINUX",
        "Macintosh",
        "HD",
        "Sun",
        "Solaris"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T20:14:01.575143",
    "resume_data": "Data Scientist Data Scientist Data Scientist Palo Alto Networks Dallas TX Dallas TX Above 8 years of experience in large Unstructured data Datasets of Structured Data Visualization Data Acquisition Predictive modeling Data Validation Develop maintain and teach new tools and methodologies related to data science and high performance computing Data Scientist with proven expertise in Data Analysis Machine Learning and Modeling Experience in Machine Learning algorithms such as Linear Regression Logistic Regression Naive Bayes Decision Trees KMeans Clustering and Association Rules Experience in applying predictive modeling and machine learning algorithms for analytical reports Experience using technology to work efficiently with datasets such as scripting data cleaning tools statistical software packages Developed predictive models using Decision Tree Random Forest Nave Bayes Logistic Regression Cluster Analysis and Neural Networks Very Strong in Python statistical analysis tools and modeling Experienced in Machine Learning and Statistical Analysis with Python ScikitLearn Excellent Knowledge in Relational Data WarehouseOLAP concepts Database Design and methodologies Strong SQL programming skills with experience in working with functions packages and triggers Expertise in transforming business requirements into designing algorithms analytical models building models developing data mining and reporting solutions that scales across massive volume of structured and unstructured data Skilled in performing data parsing data manipulation data architecture data ingestion and data preparation with methods including describe data contents compute descriptive statistics of data regex split and combine merge Remap subset reindex melt and reshape Worked with NoSQLDatabase including Hbase Cassandra and MongoDB Experienced in Big Data with Hadoop MapReduce HDFS and Spark Experienced in Data Integration Validation and Data Quality controls for ETL process and Data Warehousing using MS Visual Studio SSAS SSISand SSRS Proficient in Tableau and RShiny data visualization tools to analyze and obtain insights into large datasets create visually powerful and actionable interactive reports and dashboards Automated recurring reports using SQL andPython and visualized them on BI platform like Tableau Worked in development environment like Git and VM Excellent communication skills Successfully working in fastpaced multitasking environment both independently and in collaborative team a selfmotivated enthusiastic learner Experience with Big Data technologies like Hadoop and Spark would be a plus Worked and extracted data from various database sources like SQL Server Oracle and DB2 Experience working at Pricing andor Revenue Management would be a plus Familiarity with agile principles eg Scrum facilitating workshops and prototyping Hands on experience with RStudio for doing data preprocessing and building machine learning algorithms on different datasets Good Knowledge in NoSQL databases like HBase and Mongo DB Time Series Analysis ARIMA Neural Networks Sentiment Analysis Forecasting and Text Mining Cluster Analysis Principal Component Analysis Association Rules Recommender Systems Inferential Statistics Hypothesis Testing Descriptive and Sampling Work Experience Data Scientist Palo Alto Networks Dallas TX Dallas TX August 2018 to Present DescriptionPalo Alto Networks Inc an American multinational cyber security company Its core products are a platform that includes advanced firewalls and cloudbased offerings that extend those firewalls to cover other aspects of security Responsibilities Extracted data from HDFS and prepared data for exploratory analysis using data munging Built models using Statistical techniques like Bayesian HMM and Machine Learning classification models like XGBoost SVM and Random Forest Participated in all phases of data mining data cleaning data collection developing models validation and visualization and performed Gap analysis A highly immersive Data Science program involving Data Manipulation Visualization Web Scraping Machine Learning Python programming SQL GIT MongoDB Hadoop Setup storage and data analysis tools in AWS cloud computing infrastructure Installed and used Caffe Deep Learning Framework Worked on different data formats such as JSON XML and performed machine learning algorithms in Python Worked as Data Architects and IT Architects to understand the movement of data and its storage and ER Studio 97 Used pandas numpy seaborn matplotlib scikitlearn scipy NLTK in Python for developing various machine learning algorithms Data Manipulation and Aggregation from different source using Nexus Business Objects Toad Power BI and Smart View Implemented Agile Methodology for building an internal application Focus on integration overlap and Informatica newer commitment to MDM with the acquisition of Identity Systems Coded proprietary packages to analyze and visualize SPCfile data to identify bad spectra and samples to reduce unnecessary procedures and costs Programmed a utility in Python that used multiple packages numpy scipy pandas Implemented Classification using supervised algorithms like Logistic Regression Decision trees Naive Bayes KNN As Architect delivered various complex OLAPdatabasescubes scorecards dashboards and reports Updated Python scripts to match training data with our database stored in AWS Cloud Search so that we would be able to assign each document a response label for further classification Used Teradata utilities such as Fast Export MLOAD for handling various tasks data migrationETL from OLTP Source Systems to OLAP Target Systems Data transformation from various resources data organization features extraction from raw and stored Validated the machine learning classifiers using ROC Curves and Lift Charts Environment Unix Python 352 MLLib SAS regression logistic regression Hadoop 274 NoSQL Teradata OLTP random forest OLAP HDFS ODS NLTK SVM JSON XML and MapReduce Data Scientist Cisco Dallas TX May 2017 to July 2018 DescriptionCisco Systems Inc is an American multinational technology conglomerate that develops manufactures and sells networking hardware telecommunications equipment and other hightechnology services and products Cisco helps seize the opportunities of tomorrow by proving that amazing things can happen when you connect the unconnected An integral part of our DNA is creating longlasting customer partnerships working together to identify our customers needs and provide solutions that fuel their success Responsibilities Utilized Spark Scala Hadoop HQL VQL oozie pySpark Data Lake Tensor Flow HBase Cassandra Redshift MongoDB Kafka Kinesis Spark Streaming Edward CUDA MLLib AWS Python a broad variety of machine learning methods including classifications regressions dimensionally reduction etc Utilized the engine to increase user lifetime by 45 and triple user conversations for target categories Application of various machine learning algorithms and statistical modeling like decision trees text analytics natural language processing NLP supervised and unsupervised regression models social network analysis neural networks deep learning SVM clustering to identify Volume using scikitlearn package in python Matlab Worked onanalyzing data from Google Analytics AdWords Facebook etc Evaluated models using Cross Validation Log loss function ROC curves and used AUC for feature selection and elastic technologies like ElasticSearch Kibana Performed Multinomial Logistic Regression Decision Tree Random forest SVM to classify package is going to deliver on time for the new route Performed data analysis by using Hive to retrieve the data from Hadoop cluster Sql to retrieve data from Oracle database and used ETL for data transformation Performed Data Cleaning features scaling features engineering using pandas and numpy packages in python Performed data cleaning and feature selection using MLlib package in PySpark and working with deep learning frameworks such as Caffe Neon Developed SparkScalaR Python for regular expression regex project in the HadoopHive environment with LinuxWindows for big data resources Used clustering technique KMeans to identify outliers and to classify unlabeled data Tracking operations using sensors until certain criteria is met using Air Flow technology Responsible for different Data mapping activities from Source systems to Teradata using utilities like TPump FEXPBTEQ MLOAD FLOAD etc Addressed over fitting by implementing of the algorithm regularization methods like L1 and L2 Used Principal Component Analysis in feature engineering to analyze high dimensional data Used MLlib Sparks Machine learning library to build and evaluate different models Created and designed reports that will use gathered metrics to infer and draw logical conclusions of past and future behavior Developed Map Reduce pipeline for feature extraction using Hive and Pig Created Data Quality Scripts using SQL and Hive to validate successful data load and quality of the data Created various types of data visualizations using Python and Tableau Environment Python 2x CDH5 HDFS Hadoop 23 Hive Impala AWS Linux Spark Tableau Desktop SQL Server 2014 Microsoft Excel Matlab Spark SQL Pyspark Data Scientist Cardlytics Inc Atlanta GA January 2016 to April 2017 Description Cardlytics uses purchasebased intelligence to make marketing more relevant and measurable It help marketers identify reach and influence likely buyers at scale as well as measure the true sales impact of marketing campaigns Responsibilities Used SSIS to create ETL packages to Validate Extract Transform and Load data into Data Warehouse and Data Mart Maintained and developed complex SQL queries stored procedures views functions and reports that meet customer requirements using Microsoft SQL Server 2008 R2 Created Views and Tablevalued Functions Common Table Expression CTE joins complex sub queries to provide the reporting solutions Optimized the performance of queries with modification in TSQL queries removed the unnecessary columns and redundant data normalized tables established joins and created index Created SSIS packages using Pivot Transformation Fuzzy Lookup Derived Columns Condition Split Aggregate Execute SQL Task Data Flow Task and Execute Package Task Migrated data from SAS environment to SQL Server 2008 via SQL Integration Services SSIS Developed and implemented several types of Financial Reports Income Statement Profit Loss Statement EBIT ROIC Reports by using SSRS Developed parameterized dynamic performance Reports Gross Margin Revenue base on geographic regions Profitability based on web sales and smartphone app sales and ran the reports every month and distributed them to respective departments through mailing server subscriptions and SharePoint server Designed and developed new reports and maintained existing reports using Microsoft SQL Reporting Services SSRS and Microsoft Excel to support the firms strategy and management Created subreports drill down reports summary reports parameterized reports and adhoc reports using SSRS Used SASSQL to pull data out from databases and aggregate to provide detailed reporting based on the user requirements Used SAS for preprocessing data SQL queries data analysis generating reports graphics and statistical analyses Provided statistical research analyses and data modeling support for mortgage product Perform analyses such as regression analysis logistic regression discriminant analysis cluster analysis using SAS programming Environment SQL Server 2008 R2 DB2 Oracle SQL Server Management Studio SAS BASE SASSQL SASEnterprise Guide MS BI Suite SSISSSRS TSQL SharePoint 2010 Visual Studio 2010 AgileSCRUM Data Analyst Beacon Healthcare Communications Bedminster NJ March 2014 to December 2015 Description At Beacon were all Engagement Architects people with significant industry experience engaging the 3 primary healthcare customers consumers providers and payersAnd because we also engage with one another were able to provide a more efficient integration of thinking right from the start One that looks at all the stakeholders regardless of what you hire us for Responsibilities Used SSIS to create ETL packages to Validate Extract Transform and Load data into Data Warehouse and Data Mart Collaborating with business and technology teams Data AnalysisData collection data transformation and data loading the data using different ETL systems like SSIS and Informatica Performed source to target mapping as part of data migration from JD Edwards system to Agile PDM system Data Migration testing and implementation activities using SSIS and SSRS tools of Microsoft SQL Server 2008 Involved in construction of data flow diagrams and documentation of the processes Interacted with end users for requirements study and analysis by JAD Joint Application Development Participated in system and use case modeling like activity and use case diagrams Analyzed user requirements worked with data modelers to identify entities and relationship for data modeling Actively participated in the design of data model like conceptual logical models using Erwin Used Exception handling application block for checking errorsexceptions across the website Developed Report Component so that it retrieves the data by executing Stored Procedures throw Data Access component Environment Windows Oracle MS Excel SSIS Informatica GAP Analysis ERWIN Data Analyst CybermateInfotek Limited Hyderabad Telangana December 2012 to February 2014 Description CybermateInfotek Ltd CIL is a Software solutions and IT services company was founded in May 1994 at Hyderabad INDIA and is a CIL is an offshore software development company executing projects on Web Web related technologies Both Microsoft Open Sources Responsibilities Involved in Data mapping specifications to create and execute detailed system test plans The data mapping specifies what data will be extracted from an internal data warehouse transformed and sent to an external entity Worked closely with stakeholders to understand define document business questions needed Review systemapplication requirements functional specifications test results and metrics for quality and completeness Designed and Developed Oracle PLSQL Procedures and UNIX Shell Scripts for Data ImportExport and Data Conversions Analyzed the source data coming from different sources SQL Server Oracle and also from flat files like Access and Excel and working with business users and developers to develop the Model Have Used Informatica Data Quality IDQ and Informatica Power Center as ETL tools to extract the data from various sources systems and transform them into one common format and load them into target database for the analysis purpose from Data Warehouse Accomplished data analysis statistical reports and graphs based on the business requirement using SASBase SASMacro and SASGraph SASSQL SASAccess SASODS and SASConnect Worked on Predictive Modeling using SASSQL Executed SQL queries to validate actual test results and match expected results as per financial rules Responsible for maintaining the integrity of the SQL database and reporting any issues to the database architect Design and model the reporting data warehouse considering current and future reporting requirement Involved in the daily maintenance of the database that involved monitoring the daily run of the scripts as well as troubleshooting in the event of any errors in the entire process Involved with statistical domain experts to understand the data and worked with data management team on data quality assurance Environment SQL Server Oracle PLSQL Informatics Data Quality IDQ Informatics PowerCenter Designer Workflow Manager Workflow Monitor UNIX SASBase SASMacro and SASGraph SASSQL SASAccess SASODS and SASConnect Data Analyst Rsoft India Pvtltd Bengaluru Karnataka January 2011 to November 2012 Description RSoft is a leading software product development company in India We develop Mobile Cloud based Customer Relationship Management CRM Software Solution for all size of businesses Such as Small Medium Large Size of businesses B2B and B2C enterprises to increase leads and sales opportunities Responsibilities Understanding the requirements and develop various packages in SSIS Gathered requirements from JADJAR sections with developers and business clients Designed the business requirement collection approach based on the project scope and SDLC methodology Designs and develops the logical and physical data models to support the Data Marts and the Data Warehouse Create SQL queries for product components to update FACETS backend tables and create product prefixes Involved in formatting data stores and generate UML diagrams of logical and physical data Developed project plans and manage project scope Identifieddocumented data sources and transformation rules required to populate and maintain data warehouse content Write and execute positive and negative test cases to ensure the data originating from the data warehouse Oracle dB is accurate through to the SQL dB in the applications Work and triage Facets configuration issues and route work back for correct processing Document step by step Facets configuration steps for the Quality Assurance team Assisted in building a Business Analysis Process Model using Rational Rose and Visio Created adhoc and custom reports using Microsoft Access Cognos BI and Crystal reports Performed extensive Requirement analysis and developed use cases and workflows Designed and developed Use Cases Activity Diagrams Sequence Diagrams and OOD Played a key role in the planning User Accepted Testing and Implementation of system enhancements and conversions Updated provider tables diagnosis tables fee schedules and service is contained in Facetsbackend tables Environment MS SQL Server 2008R2 BIDS 2008 SSIS JADJAR SSIS Cognos BI Use Cases Activity Diagrams Sequence Diagrams Education Bachelors Skills Db2 Microsoft sql server Microsoft sql server 2008 Sql server Sql server 2008 Mysql Oracle Sql Cassandra Hdfs Impala Mapreduce Oozie Sqoop Hbase Kafka Flume Hadoop Mongodb Splunk Additional Information TECHNICAL SKILLS BigDataHadoop Technologies Hadoop HDFS YARN MapReduce Hive Pig Impala Sqoop Flume Spark Kafka Storm Drill Zookeeper and Oozie Languages HTML5DHTML WSDL CSS3 C C XMLRR Studio SAS Enterprise Guide SAS R Caret Weka ggplot Perl MATLAB Mathematica FORTRAN DTD Schemas Json Ajax Java Scala Python NumPy SciPy Pandas Gensim Keras Java Script Shell Scripting NO SQL Databases Cassandra HBase MongoDB MariaDB Business Intelligence Tools Tableau server Tableau Reader Tableau Splunk SAP Business Objects OBIEE SAP Business Intelligence QlikView Amazon Redshift or Azure Data Warehouse Development Tools Microsoft SQL Studio IntelliJ Eclipse NetBeans Development Methodologies AgileScrum UML Design Patterns Waterfall Build Tools Jenkins Toad SQL Loader Maven ANT RTC RSA ControlM Oziee Hue SOAP UI Reporting Tools MS Office WordExcelPower Point VisioOutlook Crystal reports XI SSRS cognos 7060 Databases Microsoft SQL Server 200820102012 MySQL 4x5x Oracle 11g 12c DB2 Teradata Netezza Operating Systems All versions of Windows UNIX LINUX Macintosh HD Sun Solaris",
    "unique_id": "25779e96-571e-4815-837b-030506c16b2a"
}