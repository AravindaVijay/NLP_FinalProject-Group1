{
    "clean_data": "Big Data Spark Engineer Big Data Spark Engineer Big Data Spark Engineer The Hartford South Portland ME 7 years of professional experience as a software professional industry comprising of Big Data Hadoop development design deployment Expertise in designing scalable Big Data solutions data warehouse models on largescale distributed data performing wide range of analytics Experience in building data pipelines using Bigdata tools like HDFS Sqoop Flume Kafka Spark Scala Hive Impala Oozie YARN Experience in building data ingestion data processing pipelines using On Prem and Cloud services Experience in working with Spark API Core Sql Streaming using Scala Experience in building streaming pipelines using Spark and Kafka Experience in working with different file formats and compressions like Parquet Avro ORC Snappy LZO etc Experience working with AWS services like S3 EMR Data Pipeline Step Functions Athena and Redshift Experience in working with NoSQL data stores like HBase Experience in manipulatinganalyzing large datasets within structured and semi structured JSON XML data Experience in writing testcases static code analysis and CICD process using Git Jenkins Experience working in agile environment with tools like Rally Clear Case and Jira Experience in Object Oriented Analysis Design OOAD and development Experience in working with Onshore Offshore model code reviews and solving the defects Strong team player with good communication analytical presentation and interpersonal skills Work Experience Big Data Spark Engineer The Hartford South Portland ME December 2018 to Present Responsibilities Design and Document the new architecture and development process to convert existing ETL pipeline in to Hadoop based systems Extensively worked on Spark Scala to prepare data for building Prediction model which will be consumed by Data Science team Developed Streaming data pipelines using Spark Scala and Kafka Developed a generic framework using Spark for processingFlatten JSON data that is reused by various applications within the enterprise Performance tuning of Spark Applications from code resource and data point views Expertise in performance tuning of Spark Streaming Applications for setting right Batch Interval time correct level of Parallelism and memory tuning Developed a common framework to prepare the data to feed for the machine learning models Developed Oozie Workflows for daily incremental loads which gets data from Teradata and then imported into hive tables Design and performance tuning hive tables and queries from storage file formats and query levels Used Hive to analyses the partitioned and bucketed data and compute various metrics for reporting on the dashboard Automated the deployment process using Git Jenkins and IBM UDeploy Environment Spark Scala Cloudera HDFS Hive Sqoop Python Agile YARN Teradata Shell Scripting Autosys Bit Bucket and JIRA Hadoop Developer Paypal San Jose CA March 2017 to November 2018 Responsibilities Responsible for designing implementing and testing data pipelines on the cloud using AWS Services Extensively used Spark to read data from S3 and preprocess it and to store in back S3 again for creating tables using Athena Designed and developed Spark application to read data json data from REST APIs Extensively used EMR S3 Data pipeline and Step Functions for building data pipelines Created partitioned tables in Athena also designed a data warehouse using Athena external tables and also created Athena queries for analysis Responsible for designing and implementing the data pipeline using Big Data tools including Spark and Sqoop Worked with different source file formats and destination source file formats like Parquet and ORC Experience in performance tuning of long running spark applications by looking into Spark UI Implemented the Spark Best practices to efficiently process data to meet ETAs by utilizing features like partitioning resource tuning memory management and Check pointing features Used versions controls tools such as GitHub to pull data from Upstream to local branch check conflict cleaning also reviewing the codes of other developers Worked on POC for exploring cuttingedge technologies in Big Data open source tools to make existing process in efficient manner Environment Athena EMR S3 Data pipeline Step Functions Sqoop Spark Scala Linux SQL Server Data Warehouse and Tableau Hadoop Developer Signa Bloomfield CT April 2016 to February 2017 Responsibilities Responsible for developing solutions by working closely with Solution Architects and Business teams Document technical and business requirements and develop architectural diagrams Developed the code for Importing and exporting data into HDFS and Hive and Impala using Sqoop Extensive experience in working with Hive and Impala for designing tables Worked on data science project life cycle and actively involved in phases data acquisition data cleansing and data preparation Developed an ingestion module to ingest data into HDFS from heterogeneous data sources Built distributed inmemory applications using Spark and Spark SQL to do analytics efficiently on huge data sets Developed HIVE and Impala queries for the Data Transformation and Data analysis Developed Oozie workflows and sub workflows to orchestrate the Sqoop scripts hive queries and the Oozie workflows are scheduled through Autosys Environment Hive Sqoop Spark Python Scala Linux Impala SQL Server Research Graduate Assistant North Western University Fremont CA August 2014 to December 2015 Responsibilities Handle the installation and configuration of a Hadoop cluster Responsible for analyzing and cleansing raw data by performing Hive queries Created Hive tables loaded data and wrote hive queries that run within the map Extracted the data from RDBMS into HDFS using Sqoop and vice versa Implemented Partitioning Dynamic Partitioning and Bucketing in Hive Software Engineer Python Developer Infosys Hyderabad Telangana May 2012 to July 2014 Responsibilities Involved in Requirements gathering Requirement analysis Design Development Integration and Deployment Developed entire frontend and backend modules using Python on Django Web Framework Developed Python batch processors to consume and produce various feeds Wrote and executed various MYSQL database queries from python using PythonMySQL connector and MySQL dB package Utilized PyUnit the Python unit test framework for testing the functionality of the application Environment Python Django MySQL PyUnit Git and Linux Education Masters in Computer Science in Computer Science Northwestern University December 2015 Bachelor of Engineering in Computer Science in Computer Science JNTU Hyderabad Telangana May 2012 Skills Impala Mapreduce Oozie Sqoop Hbase",
    "entities": [
        "Present Responsibilities Design",
        "Created Hive",
        "Parquet Avro ORC Snappy LZO",
        "SQL Server",
        "Impala Mapreduce Oozie Sqoop Hbase",
        "Rally Clear Case",
        "ETL",
        "Sqoop Extensive",
        "Sqoop",
        "The Hartford South Portland",
        "Tableau Hadoop Developer",
        "Prem and Cloud",
        "HDFS",
        "Created",
        "Impala",
        "CA",
        "North Western University",
        "Linux Education Masters",
        "Upstream",
        "ETAs",
        "Data Science",
        "Bigdata",
        "Solution Architects and Business",
        "Big Data Hadoop",
        "Hive",
        "Developed Oozie Workflows",
        "IBM",
        "Utilized PyUnit",
        "Spark Streaming Applications",
        "Oozie",
        "Big Data Spark Engineer Big Data Spark Engineer Big Data Spark",
        "Hadoop",
        "Work Experience Big Data Spark",
        "REST",
        "GitHub",
        "Built",
        "Telangana",
        "the Data Transformation and Data",
        "NoSQL",
        "Parquet",
        "AWS Services",
        "Object Oriented Analysis Design OOAD",
        "HBase",
        "Developed Oozie",
        "Autosys Environment Hive Sqoop Spark",
        "Teradata",
        "Spark API Core Sql Streaming",
        "Big Data",
        "Athena",
        "CICD",
        "Redshift Experience",
        "Spark",
        "Fremont",
        "JSON XML"
    ],
    "experience": "Experience in building data pipelines using Bigdata tools like HDFS Sqoop Flume Kafka Spark Scala Hive Impala Oozie YARN Experience in building data ingestion data processing pipelines using On Prem and Cloud services Experience in working with Spark API Core Sql Streaming using Scala Experience in building streaming pipelines using Spark and Kafka Experience in working with different file formats and compressions like Parquet Avro ORC Snappy LZO etc Experience working with AWS services like S3 EMR Data Pipeline Step Functions Athena and Redshift Experience in working with NoSQL data stores like HBase Experience in manipulatinganalyzing large datasets within structured and semi structured JSON XML data Experience in writing testcases static code analysis and CICD process using Git Jenkins Experience working in agile environment with tools like Rally Clear Case and Jira Experience in Object Oriented Analysis Design OOAD and development Experience in working with Onshore Offshore model code reviews and solving the defects Strong team player with good communication analytical presentation and interpersonal skills Work Experience Big Data Spark Engineer The Hartford South Portland ME December 2018 to Present Responsibilities Design and Document the new architecture and development process to convert existing ETL pipeline in to Hadoop based systems Extensively worked on Spark Scala to prepare data for building Prediction model which will be consumed by Data Science team Developed Streaming data pipelines using Spark Scala and Kafka Developed a generic framework using Spark for processingFlatten JSON data that is reused by various applications within the enterprise Performance tuning of Spark Applications from code resource and data point views Expertise in performance tuning of Spark Streaming Applications for setting right Batch Interval time correct level of Parallelism and memory tuning Developed a common framework to prepare the data to feed for the machine learning models Developed Oozie Workflows for daily incremental loads which gets data from Teradata and then imported into hive tables Design and performance tuning hive tables and queries from storage file formats and query levels Used Hive to analyses the partitioned and bucketed data and compute various metrics for reporting on the dashboard Automated the deployment process using Git Jenkins and IBM UDeploy Environment Spark Scala Cloudera HDFS Hive Sqoop Python Agile YARN Teradata Shell Scripting Autosys Bit Bucket and JIRA Hadoop Developer Paypal San Jose CA March 2017 to November 2018 Responsibilities Responsible for designing implementing and testing data pipelines on the cloud using AWS Services Extensively used Spark to read data from S3 and preprocess it and to store in back S3 again for creating tables using Athena Designed and developed Spark application to read data json data from REST APIs Extensively used EMR S3 Data pipeline and Step Functions for building data pipelines Created partitioned tables in Athena also designed a data warehouse using Athena external tables and also created Athena queries for analysis Responsible for designing and implementing the data pipeline using Big Data tools including Spark and Sqoop Worked with different source file formats and destination source file formats like Parquet and ORC Experience in performance tuning of long running spark applications by looking into Spark UI Implemented the Spark Best practices to efficiently process data to meet ETAs by utilizing features like partitioning resource tuning memory management and Check pointing features Used versions controls tools such as GitHub to pull data from Upstream to local branch check conflict cleaning also reviewing the codes of other developers Worked on POC for exploring cuttingedge technologies in Big Data open source tools to make existing process in efficient manner Environment Athena EMR S3 Data pipeline Step Functions Sqoop Spark Scala Linux SQL Server Data Warehouse and Tableau Hadoop Developer Signa Bloomfield CT April 2016 to February 2017 Responsibilities Responsible for developing solutions by working closely with Solution Architects and Business teams Document technical and business requirements and develop architectural diagrams Developed the code for Importing and exporting data into HDFS and Hive and Impala using Sqoop Extensive experience in working with Hive and Impala for designing tables Worked on data science project life cycle and actively involved in phases data acquisition data cleansing and data preparation Developed an ingestion module to ingest data into HDFS from heterogeneous data sources Built distributed inmemory applications using Spark and Spark SQL to do analytics efficiently on huge data sets Developed HIVE and Impala queries for the Data Transformation and Data analysis Developed Oozie workflows and sub workflows to orchestrate the Sqoop scripts hive queries and the Oozie workflows are scheduled through Autosys Environment Hive Sqoop Spark Python Scala Linux Impala SQL Server Research Graduate Assistant North Western University Fremont CA August 2014 to December 2015 Responsibilities Handle the installation and configuration of a Hadoop cluster Responsible for analyzing and cleansing raw data by performing Hive queries Created Hive tables loaded data and wrote hive queries that run within the map Extracted the data from RDBMS into HDFS using Sqoop and vice versa Implemented Partitioning Dynamic Partitioning and Bucketing in Hive Software Engineer Python Developer Infosys Hyderabad Telangana May 2012 to July 2014 Responsibilities Involved in Requirements gathering Requirement analysis Design Development Integration and Deployment Developed entire frontend and backend modules using Python on Django Web Framework Developed Python batch processors to consume and produce various feeds Wrote and executed various MYSQL database queries from python using PythonMySQL connector and MySQL dB package Utilized PyUnit the Python unit test framework for testing the functionality of the application Environment Python Django MySQL PyUnit Git and Linux Education Masters in Computer Science in Computer Science Northwestern University December 2015 Bachelor of Engineering in Computer Science in Computer Science JNTU Hyderabad Telangana May 2012 Skills Impala Mapreduce Oozie Sqoop Hbase",
    "extracted_keywords": [
        "Big",
        "Data",
        "Spark",
        "Engineer",
        "Big",
        "Data",
        "Spark",
        "Engineer",
        "Big",
        "Data",
        "Spark",
        "Engineer",
        "Hartford",
        "South",
        "Portland",
        "ME",
        "years",
        "experience",
        "software",
        "industry",
        "comprising",
        "Big",
        "Data",
        "Hadoop",
        "development",
        "design",
        "deployment",
        "Expertise",
        "Big",
        "Data",
        "solutions",
        "data",
        "warehouse",
        "models",
        "largescale",
        "data",
        "range",
        "analytics",
        "Experience",
        "data",
        "pipelines",
        "Bigdata",
        "tools",
        "HDFS",
        "Sqoop",
        "Flume",
        "Kafka",
        "Spark",
        "Scala",
        "Hive",
        "Impala",
        "Oozie",
        "YARN",
        "Experience",
        "data",
        "ingestion",
        "data",
        "pipelines",
        "Prem",
        "Cloud",
        "services",
        "Experience",
        "Spark",
        "API",
        "Core",
        "Sql",
        "Streaming",
        "Scala",
        "Experience",
        "pipelines",
        "Spark",
        "Kafka",
        "Experience",
        "file",
        "formats",
        "compressions",
        "Parquet",
        "Avro",
        "ORC",
        "Snappy",
        "LZO",
        "Experience",
        "AWS",
        "services",
        "S3",
        "EMR",
        "Data",
        "Pipeline",
        "Step",
        "Functions",
        "Athena",
        "Redshift",
        "Experience",
        "NoSQL",
        "data",
        "stores",
        "HBase",
        "Experience",
        "datasets",
        "JSON",
        "XML",
        "data",
        "Experience",
        "writing",
        "testcases",
        "code",
        "analysis",
        "CICD",
        "process",
        "Git",
        "Jenkins",
        "Experience",
        "environment",
        "tools",
        "Rally",
        "Clear",
        "Case",
        "Jira",
        "Experience",
        "Object",
        "Oriented",
        "Analysis",
        "Design",
        "OOAD",
        "development",
        "Experience",
        "Onshore",
        "Offshore",
        "model",
        "code",
        "reviews",
        "defects",
        "team",
        "player",
        "communication",
        "presentation",
        "skills",
        "Work",
        "Experience",
        "Big",
        "Data",
        "Spark",
        "Engineer",
        "Hartford",
        "South",
        "Portland",
        "ME",
        "December",
        "Present",
        "Responsibilities",
        "Design",
        "Document",
        "architecture",
        "development",
        "process",
        "ETL",
        "pipeline",
        "Hadoop",
        "systems",
        "Spark",
        "Scala",
        "data",
        "Prediction",
        "model",
        "Data",
        "Science",
        "team",
        "Streaming",
        "data",
        "pipelines",
        "Spark",
        "Scala",
        "Kafka",
        "framework",
        "Spark",
        "JSON",
        "data",
        "applications",
        "enterprise",
        "Performance",
        "tuning",
        "Spark",
        "Applications",
        "code",
        "resource",
        "data",
        "point",
        "Expertise",
        "performance",
        "tuning",
        "Spark",
        "Streaming",
        "Applications",
        "Batch",
        "Interval",
        "time",
        "level",
        "Parallelism",
        "memory",
        "tuning",
        "framework",
        "data",
        "machine",
        "learning",
        "models",
        "Oozie",
        "Workflows",
        "loads",
        "data",
        "Teradata",
        "tables",
        "Design",
        "performance",
        "hive",
        "tables",
        "queries",
        "storage",
        "file",
        "formats",
        "query",
        "levels",
        "Hive",
        "data",
        "metrics",
        "dashboard",
        "deployment",
        "process",
        "Git",
        "Jenkins",
        "IBM",
        "UDeploy",
        "Environment",
        "Spark",
        "Scala",
        "Cloudera",
        "HDFS",
        "Hive",
        "Sqoop",
        "Python",
        "YARN",
        "Teradata",
        "Shell",
        "Scripting",
        "Autosys",
        "Bit",
        "Bucket",
        "JIRA",
        "Hadoop",
        "Developer",
        "Paypal",
        "San",
        "Jose",
        "CA",
        "March",
        "November",
        "Responsibilities",
        "testing",
        "data",
        "pipelines",
        "cloud",
        "AWS",
        "Services",
        "Spark",
        "data",
        "S3",
        "S3",
        "tables",
        "Athena",
        "Spark",
        "application",
        "data",
        "json",
        "data",
        "REST",
        "APIs",
        "EMR",
        "S3",
        "Data",
        "pipeline",
        "Step",
        "Functions",
        "data",
        "pipelines",
        "tables",
        "Athena",
        "data",
        "warehouse",
        "Athena",
        "tables",
        "Athena",
        "queries",
        "analysis",
        "data",
        "pipeline",
        "Big",
        "Data",
        "tools",
        "Spark",
        "Sqoop",
        "source",
        "file",
        "formats",
        "destination",
        "source",
        "file",
        "formats",
        "Parquet",
        "ORC",
        "Experience",
        "performance",
        "tuning",
        "spark",
        "applications",
        "Spark",
        "UI",
        "Spark",
        "practices",
        "data",
        "ETAs",
        "features",
        "resource",
        "memory",
        "management",
        "features",
        "versions",
        "controls",
        "tools",
        "GitHub",
        "data",
        "Upstream",
        "branch",
        "check",
        "conflict",
        "cleaning",
        "codes",
        "developers",
        "POC",
        "cuttingedge",
        "technologies",
        "Big",
        "Data",
        "source",
        "tools",
        "process",
        "manner",
        "Environment",
        "Athena",
        "EMR",
        "S3",
        "Data",
        "pipeline",
        "Step",
        "Functions",
        "Sqoop",
        "Spark",
        "Scala",
        "Linux",
        "SQL",
        "Server",
        "Data",
        "Warehouse",
        "Tableau",
        "Hadoop",
        "Developer",
        "Signa",
        "Bloomfield",
        "CT",
        "April",
        "February",
        "Responsibilities",
        "solutions",
        "Solution",
        "Architects",
        "Business",
        "teams",
        "Document",
        "business",
        "requirements",
        "diagrams",
        "code",
        "data",
        "HDFS",
        "Hive",
        "Impala",
        "Sqoop",
        "experience",
        "Hive",
        "Impala",
        "tables",
        "data",
        "science",
        "project",
        "life",
        "cycle",
        "phases",
        "data",
        "acquisition",
        "data",
        "cleansing",
        "data",
        "preparation",
        "ingestion",
        "module",
        "data",
        "HDFS",
        "data",
        "sources",
        "applications",
        "Spark",
        "Spark",
        "SQL",
        "analytics",
        "data",
        "sets",
        "HIVE",
        "Impala",
        "Data",
        "Transformation",
        "Data",
        "analysis",
        "Oozie",
        "workflows",
        "sub",
        "workflows",
        "Sqoop",
        "hive",
        "queries",
        "Oozie",
        "workflows",
        "Autosys",
        "Environment",
        "Hive",
        "Sqoop",
        "Spark",
        "Python",
        "Scala",
        "Linux",
        "Impala",
        "SQL",
        "Server",
        "Research",
        "Graduate",
        "Assistant",
        "North",
        "Western",
        "University",
        "Fremont",
        "CA",
        "August",
        "December",
        "Responsibilities",
        "installation",
        "configuration",
        "Hadoop",
        "cluster",
        "data",
        "Hive",
        "queries",
        "Hive",
        "tables",
        "data",
        "hive",
        "queries",
        "map",
        "data",
        "RDBMS",
        "HDFS",
        "Sqoop",
        "vice",
        "Partitioning",
        "Dynamic",
        "Partitioning",
        "Bucketing",
        "Hive",
        "Software",
        "Engineer",
        "Python",
        "Developer",
        "Infosys",
        "Hyderabad",
        "Telangana",
        "May",
        "July",
        "Responsibilities",
        "Requirements",
        "Requirement",
        "analysis",
        "Design",
        "Development",
        "Integration",
        "Deployment",
        "frontend",
        "modules",
        "Python",
        "Django",
        "Web",
        "Framework",
        "Python",
        "batch",
        "processors",
        "feeds",
        "Wrote",
        "MYSQL",
        "database",
        "python",
        "PythonMySQL",
        "connector",
        "MySQL",
        "package",
        "PyUnit",
        "Python",
        "unit",
        "test",
        "framework",
        "functionality",
        "application",
        "Environment",
        "Python",
        "Django",
        "MySQL",
        "PyUnit",
        "Git",
        "Linux",
        "Education",
        "Masters",
        "Computer",
        "Science",
        "Computer",
        "Science",
        "Northwestern",
        "University",
        "December",
        "Bachelor",
        "Engineering",
        "Computer",
        "Science",
        "Computer",
        "Science",
        "JNTU",
        "Hyderabad",
        "Telangana",
        "May",
        "Skills",
        "Impala",
        "Mapreduce",
        "Oozie",
        "Sqoop",
        "Hbase"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T21:53:56.161274",
    "resume_data": "Big Data Spark Engineer Big Data Spark Engineer Big Data Spark Engineer The Hartford South Portland ME 7 years of professional experience as a software professional industry comprising of Big Data Hadoop development design deployment Expertise in designing scalable Big Data solutions data warehouse models on largescale distributed data performing wide range of analytics Experience in building data pipelines using Bigdata tools like HDFS Sqoop Flume Kafka Spark Scala Hive Impala Oozie YARN Experience in building data ingestion data processing pipelines using On Prem and Cloud services Experience in working with Spark API Core Sql Streaming using Scala Experience in building streaming pipelines using Spark and Kafka Experience in working with different file formats and compressions like Parquet Avro ORC Snappy LZO etc Experience working with AWS services like S3 EMR Data Pipeline Step Functions Athena and Redshift Experience in working with NoSQL data stores like HBase Experience in manipulatinganalyzing large datasets within structured and semi structured JSON XML data Experience in writing testcases static code analysis and CICD process using Git Jenkins Experience working in agile environment with tools like Rally Clear Case and Jira Experience in Object Oriented Analysis Design OOAD and development Experience in working with Onshore Offshore model code reviews and solving the defects Strong team player with good communication analytical presentation and interpersonal skills Work Experience Big Data Spark Engineer The Hartford South Portland ME December 2018 to Present Responsibilities Design and Document the new architecture and development process to convert existing ETL pipeline in to Hadoop based systems Extensively worked on Spark Scala to prepare data for building Prediction model which will be consumed by Data Science team Developed Streaming data pipelines using Spark Scala and Kafka Developed a generic framework using Spark for processingFlatten JSON data that is reused by various applications within the enterprise Performance tuning of Spark Applications from code resource and data point views Expertise in performance tuning of Spark Streaming Applications for setting right Batch Interval time correct level of Parallelism and memory tuning Developed a common framework to prepare the data to feed for the machine learning models Developed Oozie Workflows for daily incremental loads which gets data from Teradata and then imported into hive tables Design and performance tuning hive tables and queries from storage file formats and query levels Used Hive to analyses the partitioned and bucketed data and compute various metrics for reporting on the dashboard Automated the deployment process using Git Jenkins and IBM UDeploy Environment Spark Scala Cloudera HDFS Hive Sqoop Python Agile YARN Teradata Shell Scripting Autosys Bit Bucket and JIRA Hadoop Developer Paypal San Jose CA March 2017 to November 2018 Responsibilities Responsible for designing implementing and testing data pipelines on the cloud using AWS Services Extensively used Spark to read data from S3 and preprocess it and to store in back S3 again for creating tables using Athena Designed and developed Spark application to read data json data from REST APIs Extensively used EMR S3 Data pipeline and Step Functions for building data pipelines Created partitioned tables in Athena also designed a data warehouse using Athena external tables and also created Athena queries for analysis Responsible for designing and implementing the data pipeline using Big Data tools including Spark and Sqoop Worked with different source file formats and destination source file formats like Parquet and ORC Experience in performance tuning of long running spark applications by looking into Spark UI Implemented the Spark Best practices to efficiently process data to meet ETAs by utilizing features like partitioning resource tuning memory management and Check pointing features Used versions controls tools such as GitHub to pull data from Upstream to local branch check conflict cleaning also reviewing the codes of other developers Worked on POC for exploring cuttingedge technologies in Big Data open source tools to make existing process in efficient manner Environment Athena EMR S3 Data pipeline Step Functions Sqoop Spark Scala Linux SQL Server Data Warehouse and Tableau Hadoop Developer Signa Bloomfield CT April 2016 to February 2017 Responsibilities Responsible for developing solutions by working closely with Solution Architects and Business teams Document technical and business requirements and develop architectural diagrams Developed the code for Importing and exporting data into HDFS and Hive and Impala using Sqoop Extensive experience in working with Hive and Impala for designing tables Worked on data science project life cycle and actively involved in phases data acquisition data cleansing and data preparation Developed an ingestion module to ingest data into HDFS from heterogeneous data sources Built distributed inmemory applications using Spark and Spark SQL to do analytics efficiently on huge data sets Developed HIVE and Impala queries for the Data Transformation and Data analysis Developed Oozie workflows and sub workflows to orchestrate the Sqoop scripts hive queries and the Oozie workflows are scheduled through Autosys Environment Hive Sqoop Spark Python Scala Linux Impala SQL Server Research Graduate Assistant North Western University Fremont CA August 2014 to December 2015 Responsibilities Handle the installation and configuration of a Hadoop cluster Responsible for analyzing and cleansing raw data by performing Hive queries Created Hive tables loaded data and wrote hive queries that run within the map Extracted the data from RDBMS into HDFS using Sqoop and vice versa Implemented Partitioning Dynamic Partitioning and Bucketing in Hive Software Engineer Python Developer Infosys Hyderabad Telangana May 2012 to July 2014 Responsibilities Involved in Requirements gathering Requirement analysis Design Development Integration and Deployment Developed entire frontend and backend modules using Python on Django Web Framework Developed Python batch processors to consume and produce various feeds Wrote and executed various MYSQL database queries from python using PythonMySQL connector and MySQL dB package Utilized PyUnit the Python unit test framework for testing the functionality of the application Environment Python Django MySQL PyUnit Git and Linux Education Masters in Computer Science in Computer Science Northwestern University December 2015 Bachelor of Engineering in Computer Science in Computer Science JNTU Hyderabad Telangana May 2012 Skills Impala Mapreduce Oozie Sqoop Hbase",
    "unique_id": "bdfffbdd-e98f-4beb-9428-7de31af5c31e"
}