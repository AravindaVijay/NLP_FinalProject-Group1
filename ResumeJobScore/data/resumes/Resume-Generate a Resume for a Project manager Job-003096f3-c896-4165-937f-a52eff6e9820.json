{
    "clean_data": "Python Big Data Developer span lPythonspan Big Data span lDeveloperspan Python Big Data Developer VALIC Group AIG AIG Global Data Services Currently working in AIG Global Data Services as Data Science Python developer Worked extensively on Big Data analytical models developed in Python Worked on regression models Random forest algorithm Regular expressions andparallel processing Worked on various Python data structures including list dictionaries comprehensions dataframes vectors Experience of working in live production environment and Unix platforms Capable of working efficiently in aggressive time bound schedules and high stress environment Excellent team player and strong individual ownership Experience in building a Regression and Classification models using machine learning techniques Implemented many codes in python to automate the intermediate process while building the models Active participate on kaggle Tata Consultancy Service In Tata Consultancy Service worked as Assistant System EngineerTraineefor one year Worked on Web Development frameworkSpring Hibernet Work Experience Python Big Data Developer VALIC Group AIG New York NY April 2016 to Present Technologies Python os sys time ConfigParser subprocess shlex Big Data Technologies PySpark Sqoop DB Oracle Responsibilities Involved in model development and optimization Implemented Autosys for automatic frequency base run of model Implemented wrapper in Python which handles entire execution flow of model Used Pythonsubprocess module to call the PySpark job SFTP and Oracle store procedure Developed a scoring calculation algorithmin PySpark on AIGs Hadoop cluster Used Sqoop to fetch the data from Oracle database and also send it back Implemented file parser to read the data from configuration files Automate the whole process in a single run using shell scripting Responsible in restructuring and finalizing architecture for the overall process Logistic regression used for scoring probability of surrendering an account Extensively used python list dictionaries comprehensions and generators for various data manipulations Python Big Data Developer AIG Internal Tool Building Bangalore Karnataka January 2016 to August 2016 Technologies Python os sys re time csv numpy math subprocess Big Data Technology PySpark Responsibilities Worked extensively in the complete end to end development of the tasks involved right from the Ingestion till the end to get a summary files Implemented PySpark applicationto create schema file from the data set Perform profiling task on categorical features to identify the junk value count null and empty values pattern of different categories etc Calculation of statistics on numeric features such as mean median percentile skewness etc using PySpark Automate the whole process in a single run using shell scripting Implemented a code to generate report files from the Profiling task Iterators and foreach used vastly to make code efficient Used Python subprocess module to invoke Spark job via python wrapper Developed Python text analytics using re regular expressions to find pattern and generate the schema file Education MTECH in Computer Science International Institute of Information Technology Bangalore Karnataka 2016 BTECH in Electronics Gujarat Technological University 2012 Additional Information Technical Skills Python Libraries used os csv re numpy pandas sklearn collections sys PDFMiner Scrapy Beautifulsoup subprocess zipfile ConfigParser time Hadoop Spark Sqoop Hive DB and Data Formats Oracle PLSQL MySQL SQL Server SQLite JSON Tools Eclipse Android Studio Tortoise SVN AncondaPython",
    "entities": [
        "Present Technologies Python",
        "Sqoop",
        "New York",
        "PySpark Automate",
        "Random",
        "Tata Consultancy Service",
        "Python Big Data Developer AIG Internal Tool Building Bangalore",
        "Oracle",
        "Implemented Autosys",
        "ConfigParser",
        "Data Formats Oracle",
        "Big Data Technology PySpark Responsibilities Worked",
        "Electronics Gujarat Technological University",
        "Implemented PySpark",
        "Used Pythonsubprocess",
        "Hadoop Spark",
        "Hadoop",
        "Automate",
        "Big Data Technologies PySpark Sqoop DB",
        "Global Data Services",
        "Profiling",
        "2012 Additional Information Technical Skills Python Libraries",
        "Regression and Classification",
        "Computer Science International Institute of Information Technology",
        "Big Data",
        "Developed Python",
        "Spark",
        "Oracle Responsibilities Involved"
    ],
    "experience": "Experience of working in live production environment and Unix platforms Capable of working efficiently in aggressive time bound schedules and high stress environment Excellent team player and strong individual ownership Experience in building a Regression and Classification models using machine learning techniques Implemented many codes in python to automate the intermediate process while building the models Active participate on kaggle Tata Consultancy Service In Tata Consultancy Service worked as Assistant System EngineerTraineefor one year Worked on Web Development frameworkSpring Hibernet Work Experience Python Big Data Developer VALIC Group AIG New York NY April 2016 to Present Technologies Python os sys time ConfigParser subprocess shlex Big Data Technologies PySpark Sqoop DB Oracle Responsibilities Involved in model development and optimization Implemented Autosys for automatic frequency base run of model Implemented wrapper in Python which handles entire execution flow of model Used Pythonsubprocess module to call the PySpark job SFTP and Oracle store procedure Developed a scoring calculation algorithmin PySpark on AIGs Hadoop cluster Used Sqoop to fetch the data from Oracle database and also send it back Implemented file parser to read the data from configuration files Automate the whole process in a single run using shell scripting Responsible in restructuring and finalizing architecture for the overall process Logistic regression used for scoring probability of surrendering an account Extensively used python list dictionaries comprehensions and generators for various data manipulations Python Big Data Developer AIG Internal Tool Building Bangalore Karnataka January 2016 to August 2016 Technologies Python os sys re time csv numpy math subprocess Big Data Technology PySpark Responsibilities Worked extensively in the complete end to end development of the tasks involved right from the Ingestion till the end to get a summary files Implemented PySpark applicationto create schema file from the data set Perform profiling task on categorical features to identify the junk value count null and empty values pattern of different categories etc Calculation of statistics on numeric features such as mean median percentile skewness etc using PySpark Automate the whole process in a single run using shell scripting Implemented a code to generate report files from the Profiling task Iterators and foreach used vastly to make code efficient Used Python subprocess module to invoke Spark job via python wrapper Developed Python text analytics using re regular expressions to find pattern and generate the schema file Education MTECH in Computer Science International Institute of Information Technology Bangalore Karnataka 2016 BTECH in Electronics Gujarat Technological University 2012 Additional Information Technical Skills Python Libraries used os csv re numpy pandas sklearn collections sys PDFMiner Scrapy Beautifulsoup subprocess zipfile ConfigParser time Hadoop Spark Sqoop Hive DB and Data Formats Oracle PLSQL MySQL SQL Server SQLite JSON Tools Eclipse Android Studio Tortoise SVN AncondaPython",
    "extracted_keywords": [
        "Python",
        "Big",
        "Data",
        "Developer",
        "lPythonspan",
        "Big",
        "Data",
        "span",
        "lDeveloperspan",
        "Python",
        "Big",
        "Data",
        "Developer",
        "VALIC",
        "Group",
        "AIG",
        "AIG",
        "Global",
        "Data",
        "Services",
        "AIG",
        "Global",
        "Data",
        "Services",
        "Data",
        "Science",
        "Python",
        "developer",
        "Data",
        "models",
        "Python",
        "regression",
        "models",
        "Random",
        "forest",
        "algorithm",
        "expressions",
        "andparallel",
        "processing",
        "Python",
        "data",
        "structures",
        "list",
        "dictionaries",
        "comprehensions",
        "vectors",
        "Experience",
        "production",
        "environment",
        "Unix",
        "time",
        "schedules",
        "stress",
        "environment",
        "Excellent",
        "team",
        "player",
        "ownership",
        "Experience",
        "Regression",
        "Classification",
        "models",
        "machine",
        "learning",
        "techniques",
        "codes",
        "python",
        "process",
        "models",
        "participate",
        "kaggle",
        "Tata",
        "Consultancy",
        "Service",
        "Tata",
        "Consultancy",
        "Service",
        "Assistant",
        "System",
        "EngineerTraineefor",
        "year",
        "Web",
        "Development",
        "Hibernet",
        "Work",
        "Experience",
        "Python",
        "Big",
        "Data",
        "Developer",
        "VALIC",
        "Group",
        "AIG",
        "New",
        "York",
        "NY",
        "April",
        "Present",
        "Technologies",
        "Python",
        "sys",
        "time",
        "ConfigParser",
        "shlex",
        "Big",
        "Data",
        "Technologies",
        "PySpark",
        "Sqoop",
        "DB",
        "Oracle",
        "Responsibilities",
        "model",
        "development",
        "optimization",
        "Autosys",
        "base",
        "run",
        "model",
        "wrapper",
        "Python",
        "execution",
        "flow",
        "model",
        "Pythonsubprocess",
        "module",
        "PySpark",
        "job",
        "SFTP",
        "Oracle",
        "store",
        "procedure",
        "scoring",
        "calculation",
        "algorithmin",
        "PySpark",
        "AIGs",
        "Hadoop",
        "cluster",
        "Sqoop",
        "data",
        "Oracle",
        "database",
        "file",
        "parser",
        "data",
        "configuration",
        "files",
        "Automate",
        "process",
        "run",
        "shell",
        "restructuring",
        "architecture",
        "process",
        "regression",
        "probability",
        "account",
        "python",
        "list",
        "comprehensions",
        "generators",
        "data",
        "manipulations",
        "Python",
        "Big",
        "Data",
        "Developer",
        "AIG",
        "Internal",
        "Tool",
        "Building",
        "Bangalore",
        "Karnataka",
        "January",
        "August",
        "Technologies",
        "Python",
        "sys",
        "time",
        "csv",
        "math",
        "subprocess",
        "Big",
        "Data",
        "Technology",
        "PySpark",
        "Responsibilities",
        "end",
        "development",
        "tasks",
        "Ingestion",
        "end",
        "summary",
        "files",
        "PySpark",
        "applicationto",
        "schema",
        "file",
        "data",
        "Perform",
        "task",
        "features",
        "junk",
        "value",
        "count",
        "values",
        "pattern",
        "categories",
        "Calculation",
        "statistics",
        "features",
        "percentile",
        "skewness",
        "PySpark",
        "Automate",
        "process",
        "run",
        "shell",
        "scripting",
        "code",
        "report",
        "files",
        "Profiling",
        "task",
        "Iterators",
        "foreach",
        "code",
        "Python",
        "module",
        "Spark",
        "job",
        "python",
        "wrapper",
        "Developed",
        "Python",
        "text",
        "analytics",
        "expressions",
        "pattern",
        "schema",
        "file",
        "Education",
        "MTECH",
        "Computer",
        "Science",
        "International",
        "Institute",
        "Information",
        "Technology",
        "Bangalore",
        "Karnataka",
        "BTECH",
        "Electronics",
        "Gujarat",
        "Technological",
        "University",
        "Additional",
        "Information",
        "Technical",
        "Skills",
        "Python",
        "Libraries",
        "csv",
        "numpy",
        "pandas",
        "collections",
        "sys",
        "PDFMiner",
        "Scrapy",
        "Beautifulsoup",
        "subprocess",
        "zipfile",
        "ConfigParser",
        "time",
        "Hadoop",
        "Spark",
        "Sqoop",
        "Hive",
        "DB",
        "Data",
        "Formats",
        "Oracle",
        "PLSQL",
        "MySQL",
        "SQL",
        "Server",
        "SQLite",
        "JSON",
        "Tools",
        "Eclipse",
        "Android",
        "Studio",
        "Tortoise",
        "SVN",
        "AncondaPython"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T20:59:44.816352",
    "resume_data": "Python Big Data Developer span lPythonspan Big Data span lDeveloperspan Python Big Data Developer VALIC Group AIG AIG Global Data Services Currently working in AIG Global Data Services as Data Science Python developer Worked extensively on Big Data analytical models developed in Python Worked on regression models Random forest algorithm Regular expressions andparallel processing Worked on various Python data structures including list dictionaries comprehensions dataframes vectors Experience of working in live production environment and Unix platforms Capable of working efficiently in aggressive time bound schedules and high stress environment Excellent team player and strong individual ownership Experience in building a Regression and Classification models using machine learning techniques Implemented many codes in python to automate the intermediate process while building the models Active participate on kaggle Tata Consultancy Service In Tata Consultancy Service worked as Assistant System EngineerTraineefor one year Worked on Web Development frameworkSpring Hibernet Work Experience Python Big Data Developer VALIC Group AIG New York NY April 2016 to Present Technologies Python os sys time ConfigParser subprocess shlex Big Data Technologies PySpark Sqoop DB Oracle Responsibilities Involved in model development and optimization Implemented Autosys for automatic frequency base run of model Implemented wrapper in Python which handles entire execution flow of model Used Pythonsubprocess module to call the PySpark job SFTP and Oracle store procedure Developed a scoring calculation algorithmin PySpark on AIGs Hadoop cluster Used Sqoop to fetch the data from Oracle database and also send it back Implemented file parser to read the data from configuration files Automate the whole process in a single run using shell scripting Responsible in restructuring and finalizing architecture for the overall process Logistic regression used for scoring probability of surrendering an account Extensively used python list dictionaries comprehensions and generators for various data manipulations Python Big Data Developer AIG Internal Tool Building Bangalore Karnataka January 2016 to August 2016 Technologies Python os sys re time csv numpy math subprocess Big Data Technology PySpark Responsibilities Worked extensively in the complete end to end development of the tasks involved right from the Ingestion till the end to get a summary files Implemented PySpark applicationto create schema file from the data set Perform profiling task on categorical features to identify the junk value count null and empty values pattern of different categories etc Calculation of statistics on numeric features such as mean median percentile skewness etc using PySpark Automate the whole process in a single run using shell scripting Implemented a code to generate report files from the Profiling task Iterators and foreach used vastly to make code efficient Used Python subprocess module to invoke Spark job via python wrapper Developed Python text analytics using re regular expressions to find pattern and generate the schema file Education MTECH in Computer Science International Institute of Information Technology Bangalore Karnataka 2016 BTECH in Electronics Gujarat Technological University 2012 Additional Information Technical Skills Python Libraries used os csv re numpy pandas sklearn collections sys PDFMiner Scrapy Beautifulsoup subprocess zipfile ConfigParser time Hadoop Spark Sqoop Hive DB and Data Formats Oracle PLSQL MySQL SQL Server SQLite JSON Tools Eclipse Android Studio Tortoise SVN AncondaPython",
    "unique_id": "003096f3-c896-4165-937f-a52eff6e9820"
}