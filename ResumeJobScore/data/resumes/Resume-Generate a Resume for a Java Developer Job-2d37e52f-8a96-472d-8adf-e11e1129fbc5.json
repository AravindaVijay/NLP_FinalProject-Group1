{
    "clean_data": "Senior Hadoop Developer Senior Hadoop span lDeveloperspan Senior Hadoop Developer MoneyGram International Location Dallas TX 8 years of experience in IT which includes experience in Hadoop ecosystem Bigdata Technologies QA Automation Tools SQL related technologies in Retail Manufacturing Financial and Communication sectors 4 Years of experience as Hadoop Developer using Various Hadoop ecosystems tools and Spark Framework and Currently working on Spark and Spark Streaming frameworks extensively using Scala as the main programming dialect Experience installingconfiguringmaintaining Apache Hadoop clusters for application development and Hadoop tools like Sqoop Hive PIG Flume Hbase Kafka Hue Storm Zoo Keeper Oozie MangoDB Sqoop Python Worked with major distributions like Cloudera CDH 34 Horton works Distributions and AWS Also worked on Unix and DWH in support for various Distributions Hands on experience in developing and deploying enterprise based applications using major components in Hadoop ecosystem like Hadoop 2X YARN Hive Pig MapReduce Spark Kafka Storm Oozie HBase Flume Sqoop and ZooKeeper Experience in handling large datasets using Partitions Spark in memory capabilities Broadcasts in Spark with Scala and python Effective and efficient Joins Transformations and other during ingestion process itself Experience in developing data pipeline using Pig Sqoop and Flume to extract the data from weblogs and store in HDFS and accomplished developing Pig Latin Scripts and using HiveQL for data analytics Extensively dealt with Spark Streaming and Apache Kafka to fetch live stream data Experience in converting HiveSQL queries into Spark transformations using Java and experience in ETL development using Kafka Flume and Sqoop Good experience in writing Spark applications using Scala and Java and used Scala sbt to develop Scala projects and executed using SparkSubmit Experience working on NoSQL databases including Hbase MangoDB and experience using Sqoop to import data into HDFS from RDBMS and viceversa Developed Spark scripts by using Scala shell commands as per the requirement Good experience in writing Sqoop queries for transferring bulk data between Apache Hadoop and structured data stores Substantial experience in writing Map Reduce jobs in Java PIG Flume Zookeeper Hive and Storm Created multiple MapReduce Jobs using Java API Pig and Hive for data extraction Strong expertise in troubleshooting and performance finetuning Spark MapReduce and Hive applications Good experience on working with Amazon EMR framework for processing data on EMR and EC2 instances Created AWS VPC network for the installed Instances and configured security groups and Elastic IPs Accordingly Developed AWS Cloud formation templates to create custom sized VPC subnets EC2 instances ELB and security groups Extensive experience in developing applications that perform Data Processing tasks using Teradata Oracle SQL Server and MySQL database Worked on data warehousing and ETL tools like Informatica Tableau and Pentaho Experience in understanding the security requirements for Hadoop and integrate with Kerberos authentication and authorization infrastructure Acquaintance with Agile and Waterfall methodologies Responsible for handling several clients facing meetings with great communication skills Work Experience Senior Hadoop Developer MoneyGram International Location Dallas TX May 2018 to Present Responsibilities Worked on analysingHadoop cluster and different big data analytical and processing tools including Pig Hive Sqoopand Spark with Scala java Spark Streaming Wrote SparkStreaming applications to consume the data from Kafka topics and wrote processed streams to HBase and steamed data using Spark with Kafka Worked on the largescale Hadoop YARN cluster for distributed data processing and analysis using Spark Hive and HBase Involved in creating datalake by extracting customers data from various data sources to HDFS which include data from Excel databases and log data from servers Developed Apache Spark applications by using Scala for data processing from various streaming sources Used Scala to convert HiveSQL queries into RDD transformations in Apache Spark Implemented Spark solutions to generate reports fetch and load data in Hive Experienced in writing realtime processing and core jobs using Spark Streaming with Kafka as a data pipeline system Written HiveQL to analyse the number of unique visitors and their visit information such as views most visited pages etc Configured Spark Streaming to receive real time data from the Apache Kafka and store the stream data to HDFS using Scala Monitored workload job performance and capacity planning using Cloudera Manager Experienced on working with Amazon EMR framework for processing data on EMR and EC2 instances Designing and implementing complete endtoend Hadoop Infrastructure including Pig Hive Sqoop Oozie Flume and Zookeeper Further used pig to do transformations event joins elephant bird API and pre aggregations performed before loading JSON files format onto HDFS Involved in resolving performance issues in Pig and Hive with understanding of Map Reduce physical plan execution and using debugging commands to run code in optimized way Used Spark to perform analytics on data in Hive and experienced with ETL working with Hive and MapReduce Environments Hdp 260 HDFS MapReduce Spark Streaming SparkCore Spark SQL Scala Pig 014 Hive 121 Sqoop 144 Flume 160 Kafka JSON HBase Hadoop Developer New York Times New York NY August 2016 to May 2018 Responsibilities Installed and configured Hadoop MapReduce HDFS and developed multiple MapReduce jobs in Java for data cleansing and prep Worked on Spark streaming to collect TBs of data for every hour from connected cars Worked on Spark Batch Processing to load the processed data into MongoDB Manage migration of onperm servers to AWS by creating golden images for upload and deployment Manage multiple AWS accounts with multiple VPCs for both production and nonproduction where primary objectives are automation build out integration and cost control Moving log data periodically into HDFS using Flume and building multihop flows fanout flows and failover mechanism Developed MapReduce jobs to automate transfer of data from Hbase and to read data files and scrub the data Experience in Spark programming using Scala Experienced in performing ETL using Spark Spark SQL Transferring data between MySQL and HDFS using Sqoop with connectors Creating and populating Hive tables and writing Hive queries for data analysis to meet the business requirements Transformed Kafka loaded data using Sparkstreaming with Scala Installed and configured Pig and written Pig Latin scripts Migrating data from MySQL database to HBase Running MapReduce jobs to access HBase data from application using Java Client APIs Installed and configured Hive Pig Sqoop Flume and Oozie on the Hadoop cluster and automating the jobs using Oozie Actively participated in software development lifecycle including design and code reviews test development test automation Involved in solutiondriven agile development methodology and actively participated in daily scrum meetings Monitoring Hadoop cluster using tools like Cloudera Manager Automation script to monitor HDFS and HBase through Cron jobs Create a complete processing engine based on Clouderas distribution enhanced to performance Environments Hadoop MapReduce HDFS Sqoop Hbase Oozie SQL Pig Flume HiveJava Big Data Developer Wells Forgo Location Dallas TX December 2014 to July 2016 Responsibilities Installed configured and job creation in Hadoop MapReduce Pig Hive HBase Spark RDD Pair RDD Flume Oozie Sqoop environment Involved in application migration from Hadoop to Spark for the fast processing Extracted data from Oracle database into HDFS using Sqoop Developed Oozie workflows to schedule and manage Sqoop Hive Pig jobs to ExtractTransformLoad process Used Flume and configured it to use multiplexing replicating multisource interceptors selectors to import log files from Web Servers in to HDFSHive Managed and scheduled Jobs on a Hadoop cluster using Shell Scripts Maintained Cluster coordination services through Zookeeper for system Involved in filter the partition data based on different year range different format using Hive functions Defining a schema creating new relations performing PigJoin sorting and filtering using PigGroup on large data sets Performed MapSide joins and ReduceSide joins for large tables Involved in filtering the partition data based on different year range different format using with Hive functions Creating HBase tables for random readwrites by the map reduce programs Designed and developed entire pipeline from data ingestion to reporting tables Performed data cleaning integration transformation reduction by developing MapReduce jobs in java for data mining Creating Hive tables loading data into it and customizing hive queries internally operating in MapReduce way Performed MapSide joins and ReduceSide joins for large tables Used Cloudera Manager to monitor and manage Hadoop Cluster Environments HDFS CDH BigInsights Apache Spark Flume Hive Pig Scala Java Sqoop SQL Perl Shell scripting C C Java Oracle WebSphere Application Server Spring Hibernate Struts JMS SQLJava Developer Staples Location Framingham MA September 2012 to November 2014 Responsibilities Responsible for the analysis documenting the requirements and architecting the application based on J2EE standards Attended Scrum meetings daily as a part of Agile Methodology Involved in complete Software Development Life Cycle SDLC with Object Oriented Approach of clients business process and continuous client feedback Implementing MVC Architecture using Spring Framework customized user interfaces Used Core Java and Spring Aspect Oriented programming concepts for logging security error handling mechanism Developed application modules using Spring MVC Spring Annotations Spring Beans Dependency Injection with database interfaceusing Hibernate Used the Java Collections API extensively in the application as security protection for XML SOAP REST and JSON to make a secure Web Deployment Developed serverside services using Java Spring Web Services SOAP Restful WSDL JAXB JAXRPC Built Web pages that are more userinteractive using JQuery plugins for Drag and Drop AutoComplete AJAX JSON Angular JS JavaScript and Bootstrap Used XSL to transform XML data structure into HTML pages Used Struts as the framework in this project and developed struts action classes form beans Created dispatch Action classes and Validation plugin using Struts framework DB2 was used as the database and wrote queries to extract data from the database Developed SQL queries and stored procedures Designed Developed whitebox test cases using JUnit Git JMeter Mockito Framework Environment Core Java AgileScrum XML HTML Jmeter SOAP REST JDK JSP Servlets JDBC HTML CSS JUnit SQL MySQL Windows Oracle Eclipse Python Developer Autodesk San Rafael CA February 2011 to August 2012 Responsibilities Involved in the analysis and development of Software Development Life Cycle SDLC Contributed in developing a web services middletier in Pythonto integrate with an existing MySQL backend Wrote several internal API utilities and micro services to carry out specific tasks Wrote and executed exhaustive SQL queries using Pythonwith help from various query builders inPythonto frame the queries Migrated MySQL to NoSQL data store using ETL processes using Python Tasks included CRUD elaborate cleansing of data and packing it into the expected format JSON Developed Pythonscripts to perform auditing tasks and generating Excel reports to support engineering a logistics sales and inventory management system Worked as assistant to Web Programmer in creating DjangoFlask Web apps Generated sample JSON format queries for testing REST endpoints Frequently performed ad hoc File IO tasks in python Dealt with CSV JSON Text XML and XLSX files Used shell scripting to automate repetitive tasks Followed a testdriven approach closely Used Pythons Unit Testing extensively Environment Python MySQL PostgreSQL Jira Flask REST JSON CSV Excel Eclipse Education Bachelors Skills HDFS IMPALA MAPREDUCE OOZIE SQOOP",
    "entities": [
        "Monitoring Hadoop",
        "Created dispatch Action",
        "Spring MVC Spring",
        "Distributions and AWS Also",
        "Hadoop MapReduce Pig Hive HBase Spark",
        "HDFS",
        "SparkSubmit",
        "Developed Spark",
        "Pythonwith",
        "Implementing MVC Architecture",
        "ZooKeeper Experience",
        "Software Development Life Cycle SDLC Contributed",
        "Sparkstreaming",
        "RDD",
        "Hadoop",
        "Oozie Sqoop",
        "XML",
        "Informatica Tableau",
        "Software Development Life Cycle SDLC",
        "JUnit",
        "HBase Involved",
        "HBase",
        "Created AWS VPC",
        "PigJoin",
        "Apache Spark",
        "TX",
        "Spark MapReduce",
        "Pig Hive Sqoopand Spark",
        "Developed",
        "Pythonto",
        "Agile Methodology Involved",
        "Environments Hadoop MapReduce HDFS Sqoop Hbase Oozie",
        "Kerberos",
        "Dallas",
        "Validation",
        "Spark Framework",
        "Java Spring Web Services SOAP Restful WSDL",
        "Hadoop Developer",
        "Spark Streaming",
        "Bigdata Technologies QA Automation Tools SQL",
        "Spark",
        "Hadoop Infrastructure",
        "New York Times",
        "Agile",
        "Amazon EMR",
        "API",
        "Sqoop",
        "AWS",
        "CA",
        "Hadoop MapReduce HDFS",
        "Sqoop Developed Oozie",
        "Oracle",
        "Developed Apache Spark",
        "Teradata Oracle SQL Server",
        "Distributions Hands",
        "ELB",
        "Instances",
        "java",
        "Cron",
        "SQL",
        "Spark Spark",
        "Creating HBase",
        "Hive",
        "REST JDK JSP",
        "PigGroup",
        "Partitions Spark",
        "API utilities",
        "Hive Experienced",
        "Spark Hive",
        "Moving",
        "ETL",
        "CRUD",
        "Apache Hadoop",
        "Zookeeper",
        "Performed",
        "Shell Scripts Maintained Cluster",
        "Present Responsibilities Worked",
        "Developed MapReduce",
        "Oozie Actively",
        "REST",
        "Storm Created",
        "Retail Manufacturing Financial and Communication",
        "MapReduce",
        "Object Oriented Approach",
        "NoSQL",
        "HBase Running MapReduce",
        "HDFSHive Managed",
        "Used Pythons Unit Testing extensively",
        "Bootstrap Used",
        "JQuery",
        "DWH",
        "Data Processing"
    ],
    "experience": "Experience installingconfiguringmaintaining Apache Hadoop clusters for application development and Hadoop tools like Sqoop Hive PIG Flume Hbase Kafka Hue Storm Zoo Keeper Oozie MangoDB Sqoop Python Worked with major distributions like Cloudera CDH 34 Horton works Distributions and AWS Also worked on Unix and DWH in support for various Distributions Hands on experience in developing and deploying enterprise based applications using major components in Hadoop ecosystem like Hadoop 2X YARN Hive Pig MapReduce Spark Kafka Storm Oozie HBase Flume Sqoop and ZooKeeper Experience in handling large datasets using Partitions Spark in memory capabilities Broadcasts in Spark with Scala and python Effective and efficient Joins Transformations and other during ingestion process itself Experience in developing data pipeline using Pig Sqoop and Flume to extract the data from weblogs and store in HDFS and accomplished developing Pig Latin Scripts and using HiveQL for data analytics Extensively dealt with Spark Streaming and Apache Kafka to fetch live stream data Experience in converting HiveSQL queries into Spark transformations using Java and experience in ETL development using Kafka Flume and Sqoop Good experience in writing Spark applications using Scala and Java and used Scala sbt to develop Scala projects and executed using SparkSubmit Experience working on NoSQL databases including Hbase MangoDB and experience using Sqoop to import data into HDFS from RDBMS and viceversa Developed Spark scripts by using Scala shell commands as per the requirement Good experience in writing Sqoop queries for transferring bulk data between Apache Hadoop and structured data stores Substantial experience in writing Map Reduce jobs in Java PIG Flume Zookeeper Hive and Storm Created multiple MapReduce Jobs using Java API Pig and Hive for data extraction Strong expertise in troubleshooting and performance finetuning Spark MapReduce and Hive applications Good experience on working with Amazon EMR framework for processing data on EMR and EC2 instances Created AWS VPC network for the installed Instances and configured security groups and Elastic IPs Accordingly Developed AWS Cloud formation templates to create custom sized VPC subnets EC2 instances ELB and security groups Extensive experience in developing applications that perform Data Processing tasks using Teradata Oracle SQL Server and MySQL database Worked on data warehousing and ETL tools like Informatica Tableau and Pentaho Experience in understanding the security requirements for Hadoop and integrate with Kerberos authentication and authorization infrastructure Acquaintance with Agile and Waterfall methodologies Responsible for handling several clients facing meetings with great communication skills Work Experience Senior Hadoop Developer MoneyGram International Location Dallas TX May 2018 to Present Responsibilities Worked on analysingHadoop cluster and different big data analytical and processing tools including Pig Hive Sqoopand Spark with Scala java Spark Streaming Wrote SparkStreaming applications to consume the data from Kafka topics and wrote processed streams to HBase and steamed data using Spark with Kafka Worked on the largescale Hadoop YARN cluster for distributed data processing and analysis using Spark Hive and HBase Involved in creating datalake by extracting customers data from various data sources to HDFS which include data from Excel databases and log data from servers Developed Apache Spark applications by using Scala for data processing from various streaming sources Used Scala to convert HiveSQL queries into RDD transformations in Apache Spark Implemented Spark solutions to generate reports fetch and load data in Hive Experienced in writing realtime processing and core jobs using Spark Streaming with Kafka as a data pipeline system Written HiveQL to analyse the number of unique visitors and their visit information such as views most visited pages etc Configured Spark Streaming to receive real time data from the Apache Kafka and store the stream data to HDFS using Scala Monitored workload job performance and capacity planning using Cloudera Manager Experienced on working with Amazon EMR framework for processing data on EMR and EC2 instances Designing and implementing complete endtoend Hadoop Infrastructure including Pig Hive Sqoop Oozie Flume and Zookeeper Further used pig to do transformations event joins elephant bird API and pre aggregations performed before loading JSON files format onto HDFS Involved in resolving performance issues in Pig and Hive with understanding of Map Reduce physical plan execution and using debugging commands to run code in optimized way Used Spark to perform analytics on data in Hive and experienced with ETL working with Hive and MapReduce Environments Hdp 260 HDFS MapReduce Spark Streaming SparkCore Spark SQL Scala Pig 014 Hive 121 Sqoop 144 Flume 160 Kafka JSON HBase Hadoop Developer New York Times New York NY August 2016 to May 2018 Responsibilities Installed and configured Hadoop MapReduce HDFS and developed multiple MapReduce jobs in Java for data cleansing and prep Worked on Spark streaming to collect TBs of data for every hour from connected cars Worked on Spark Batch Processing to load the processed data into MongoDB Manage migration of onperm servers to AWS by creating golden images for upload and deployment Manage multiple AWS accounts with multiple VPCs for both production and nonproduction where primary objectives are automation build out integration and cost control Moving log data periodically into HDFS using Flume and building multihop flows fanout flows and failover mechanism Developed MapReduce jobs to automate transfer of data from Hbase and to read data files and scrub the data Experience in Spark programming using Scala Experienced in performing ETL using Spark Spark SQL Transferring data between MySQL and HDFS using Sqoop with connectors Creating and populating Hive tables and writing Hive queries for data analysis to meet the business requirements Transformed Kafka loaded data using Sparkstreaming with Scala Installed and configured Pig and written Pig Latin scripts Migrating data from MySQL database to HBase Running MapReduce jobs to access HBase data from application using Java Client APIs Installed and configured Hive Pig Sqoop Flume and Oozie on the Hadoop cluster and automating the jobs using Oozie Actively participated in software development lifecycle including design and code reviews test development test automation Involved in solutiondriven agile development methodology and actively participated in daily scrum meetings Monitoring Hadoop cluster using tools like Cloudera Manager Automation script to monitor HDFS and HBase through Cron jobs Create a complete processing engine based on Clouderas distribution enhanced to performance Environments Hadoop MapReduce HDFS Sqoop Hbase Oozie SQL Pig Flume HiveJava Big Data Developer Wells Forgo Location Dallas TX December 2014 to July 2016 Responsibilities Installed configured and job creation in Hadoop MapReduce Pig Hive HBase Spark RDD Pair RDD Flume Oozie Sqoop environment Involved in application migration from Hadoop to Spark for the fast processing Extracted data from Oracle database into HDFS using Sqoop Developed Oozie workflows to schedule and manage Sqoop Hive Pig jobs to ExtractTransformLoad process Used Flume and configured it to use multiplexing replicating multisource interceptors selectors to import log files from Web Servers in to HDFSHive Managed and scheduled Jobs on a Hadoop cluster using Shell Scripts Maintained Cluster coordination services through Zookeeper for system Involved in filter the partition data based on different year range different format using Hive functions Defining a schema creating new relations performing PigJoin sorting and filtering using PigGroup on large data sets Performed MapSide joins and ReduceSide joins for large tables Involved in filtering the partition data based on different year range different format using with Hive functions Creating HBase tables for random readwrites by the map reduce programs Designed and developed entire pipeline from data ingestion to reporting tables Performed data cleaning integration transformation reduction by developing MapReduce jobs in java for data mining Creating Hive tables loading data into it and customizing hive queries internally operating in MapReduce way Performed MapSide joins and ReduceSide joins for large tables Used Cloudera Manager to monitor and manage Hadoop Cluster Environments HDFS CDH BigInsights Apache Spark Flume Hive Pig Scala Java Sqoop SQL Perl Shell scripting C C Java Oracle WebSphere Application Server Spring Hibernate Struts JMS SQLJava Developer Staples Location Framingham MA September 2012 to November 2014 Responsibilities Responsible for the analysis documenting the requirements and architecting the application based on J2EE standards Attended Scrum meetings daily as a part of Agile Methodology Involved in complete Software Development Life Cycle SDLC with Object Oriented Approach of clients business process and continuous client feedback Implementing MVC Architecture using Spring Framework customized user interfaces Used Core Java and Spring Aspect Oriented programming concepts for logging security error handling mechanism Developed application modules using Spring MVC Spring Annotations Spring Beans Dependency Injection with database interfaceusing Hibernate Used the Java Collections API extensively in the application as security protection for XML SOAP REST and JSON to make a secure Web Deployment Developed serverside services using Java Spring Web Services SOAP Restful WSDL JAXB JAXRPC Built Web pages that are more userinteractive using JQuery plugins for Drag and Drop AutoComplete AJAX JSON Angular JS JavaScript and Bootstrap Used XSL to transform XML data structure into HTML pages Used Struts as the framework in this project and developed struts action classes form beans Created dispatch Action classes and Validation plugin using Struts framework DB2 was used as the database and wrote queries to extract data from the database Developed SQL queries and stored procedures Designed Developed whitebox test cases using JUnit Git JMeter Mockito Framework Environment Core Java AgileScrum XML HTML Jmeter SOAP REST JDK JSP Servlets JDBC HTML CSS JUnit SQL MySQL Windows Oracle Eclipse Python Developer Autodesk San Rafael CA February 2011 to August 2012 Responsibilities Involved in the analysis and development of Software Development Life Cycle SDLC Contributed in developing a web services middletier in Pythonto integrate with an existing MySQL backend Wrote several internal API utilities and micro services to carry out specific tasks Wrote and executed exhaustive SQL queries using Pythonwith help from various query builders inPythonto frame the queries Migrated MySQL to NoSQL data store using ETL processes using Python Tasks included CRUD elaborate cleansing of data and packing it into the expected format JSON Developed Pythonscripts to perform auditing tasks and generating Excel reports to support engineering a logistics sales and inventory management system Worked as assistant to Web Programmer in creating DjangoFlask Web apps Generated sample JSON format queries for testing REST endpoints Frequently performed ad hoc File IO tasks in python Dealt with CSV JSON Text XML and XLSX files Used shell scripting to automate repetitive tasks Followed a testdriven approach closely Used Pythons Unit Testing extensively Environment Python MySQL PostgreSQL Jira Flask REST JSON CSV Excel Eclipse Education Bachelors Skills HDFS IMPALA MAPREDUCE OOZIE SQOOP",
    "extracted_keywords": [
        "Senior",
        "Hadoop",
        "Developer",
        "Senior",
        "Hadoop",
        "span",
        "lDeveloperspan",
        "Senior",
        "Hadoop",
        "Developer",
        "MoneyGram",
        "International",
        "Location",
        "Dallas",
        "TX",
        "years",
        "experience",
        "IT",
        "experience",
        "Hadoop",
        "ecosystem",
        "Bigdata",
        "Technologies",
        "QA",
        "Automation",
        "Tools",
        "SQL",
        "technologies",
        "Retail",
        "Manufacturing",
        "Financial",
        "Communication",
        "Years",
        "experience",
        "Hadoop",
        "Developer",
        "Various",
        "Hadoop",
        "tools",
        "Spark",
        "Framework",
        "Spark",
        "Spark",
        "Streaming",
        "frameworks",
        "Scala",
        "programming",
        "dialect",
        "Experience",
        "Apache",
        "Hadoop",
        "clusters",
        "application",
        "development",
        "Hadoop",
        "tools",
        "Sqoop",
        "Hive",
        "PIG",
        "Flume",
        "Hbase",
        "Kafka",
        "Hue",
        "Storm",
        "Zoo",
        "Keeper",
        "Oozie",
        "MangoDB",
        "Sqoop",
        "Python",
        "distributions",
        "Cloudera",
        "CDH",
        "Horton",
        "Distributions",
        "AWS",
        "Unix",
        "DWH",
        "support",
        "Distributions",
        "Hands",
        "experience",
        "enterprise",
        "applications",
        "components",
        "Hadoop",
        "ecosystem",
        "Hadoop",
        "2X",
        "YARN",
        "Hive",
        "Pig",
        "MapReduce",
        "Spark",
        "Kafka",
        "Storm",
        "Oozie",
        "HBase",
        "Flume",
        "Sqoop",
        "ZooKeeper",
        "Experience",
        "datasets",
        "Partitions",
        "Spark",
        "memory",
        "capabilities",
        "Broadcasts",
        "Spark",
        "Scala",
        "python",
        "Joins",
        "Transformations",
        "ingestion",
        "process",
        "Experience",
        "data",
        "pipeline",
        "Pig",
        "Sqoop",
        "Flume",
        "data",
        "weblogs",
        "HDFS",
        "Pig",
        "Latin",
        "Scripts",
        "HiveQL",
        "data",
        "analytics",
        "Spark",
        "Streaming",
        "Apache",
        "Kafka",
        "stream",
        "data",
        "Experience",
        "HiveSQL",
        "queries",
        "Spark",
        "transformations",
        "Java",
        "experience",
        "ETL",
        "development",
        "Kafka",
        "Flume",
        "Sqoop",
        "Good",
        "experience",
        "Spark",
        "applications",
        "Scala",
        "Java",
        "Scala",
        "sbt",
        "Scala",
        "projects",
        "SparkSubmit",
        "Experience",
        "NoSQL",
        "databases",
        "Hbase",
        "MangoDB",
        "experience",
        "Sqoop",
        "data",
        "HDFS",
        "RDBMS",
        "viceversa",
        "Developed",
        "Spark",
        "scripts",
        "Scala",
        "shell",
        "commands",
        "requirement",
        "experience",
        "Sqoop",
        "data",
        "Apache",
        "Hadoop",
        "data",
        "stores",
        "experience",
        "Map",
        "Reduce",
        "jobs",
        "Java",
        "PIG",
        "Flume",
        "Zookeeper",
        "Hive",
        "Storm",
        "MapReduce",
        "Jobs",
        "Java",
        "API",
        "Pig",
        "Hive",
        "data",
        "extraction",
        "expertise",
        "troubleshooting",
        "performance",
        "Spark",
        "MapReduce",
        "Hive",
        "applications",
        "experience",
        "Amazon",
        "EMR",
        "framework",
        "data",
        "EMR",
        "EC2",
        "instances",
        "AWS",
        "network",
        "Instances",
        "security",
        "groups",
        "IPs",
        "AWS",
        "Cloud",
        "formation",
        "templates",
        "custom",
        "VPC",
        "subnets",
        "EC2",
        "ELB",
        "security",
        "groups",
        "experience",
        "applications",
        "Data",
        "Processing",
        "tasks",
        "Teradata",
        "Oracle",
        "SQL",
        "Server",
        "MySQL",
        "database",
        "data",
        "warehousing",
        "ETL",
        "tools",
        "Informatica",
        "Tableau",
        "Pentaho",
        "Experience",
        "security",
        "requirements",
        "Hadoop",
        "authentication",
        "authorization",
        "infrastructure",
        "Acquaintance",
        "Agile",
        "Waterfall",
        "methodologies",
        "clients",
        "meetings",
        "communication",
        "skills",
        "Work",
        "Experience",
        "Senior",
        "Hadoop",
        "Developer",
        "MoneyGram",
        "International",
        "Location",
        "Dallas",
        "TX",
        "May",
        "Present",
        "Responsibilities",
        "analysingHadoop",
        "cluster",
        "data",
        "processing",
        "tools",
        "Pig",
        "Hive",
        "Sqoopand",
        "Spark",
        "Scala",
        "java",
        "Spark",
        "Streaming",
        "Wrote",
        "SparkStreaming",
        "applications",
        "data",
        "Kafka",
        "topics",
        "streams",
        "HBase",
        "data",
        "Spark",
        "Kafka",
        "largescale",
        "Hadoop",
        "YARN",
        "cluster",
        "data",
        "processing",
        "analysis",
        "Spark",
        "Hive",
        "HBase",
        "datalake",
        "customers",
        "data",
        "data",
        "sources",
        "HDFS",
        "data",
        "Excel",
        "databases",
        "data",
        "servers",
        "Developed",
        "Apache",
        "Spark",
        "applications",
        "Scala",
        "data",
        "processing",
        "sources",
        "Scala",
        "HiveSQL",
        "queries",
        "RDD",
        "transformations",
        "Apache",
        "Spark",
        "Spark",
        "solutions",
        "reports",
        "data",
        "Hive",
        "processing",
        "core",
        "jobs",
        "Spark",
        "Streaming",
        "Kafka",
        "data",
        "pipeline",
        "system",
        "HiveQL",
        "number",
        "visitors",
        "visit",
        "information",
        "views",
        "pages",
        "Configured",
        "Spark",
        "Streaming",
        "time",
        "data",
        "Apache",
        "Kafka",
        "stream",
        "data",
        "HDFS",
        "Scala",
        "Monitored",
        "workload",
        "job",
        "performance",
        "capacity",
        "planning",
        "Cloudera",
        "Manager",
        "Amazon",
        "EMR",
        "framework",
        "data",
        "EMR",
        "EC2",
        "instances",
        "endtoend",
        "Hadoop",
        "Infrastructure",
        "Pig",
        "Hive",
        "Sqoop",
        "Oozie",
        "Flume",
        "Zookeeper",
        "Further",
        "pig",
        "transformations",
        "event",
        "elephant",
        "bird",
        "API",
        "aggregations",
        "JSON",
        "files",
        "format",
        "HDFS",
        "performance",
        "issues",
        "Pig",
        "Hive",
        "understanding",
        "Map",
        "plan",
        "execution",
        "commands",
        "code",
        "way",
        "Spark",
        "analytics",
        "data",
        "Hive",
        "ETL",
        "Hive",
        "MapReduce",
        "Environments",
        "Hdp",
        "HDFS",
        "MapReduce",
        "Spark",
        "Streaming",
        "SparkCore",
        "Spark",
        "SQL",
        "Scala",
        "Pig",
        "Hive",
        "Sqoop",
        "Flume",
        "Kafka",
        "JSON",
        "HBase",
        "Hadoop",
        "Developer",
        "New",
        "York",
        "Times",
        "New",
        "York",
        "NY",
        "August",
        "May",
        "Responsibilities",
        "Hadoop",
        "MapReduce",
        "HDFS",
        "MapReduce",
        "jobs",
        "Java",
        "data",
        "cleansing",
        "prep",
        "Spark",
        "streaming",
        "TBs",
        "data",
        "hour",
        "cars",
        "Spark",
        "Batch",
        "Processing",
        "data",
        "MongoDB",
        "Manage",
        "migration",
        "onperm",
        "servers",
        "AWS",
        "images",
        "upload",
        "deployment",
        "Manage",
        "AWS",
        "VPCs",
        "production",
        "nonproduction",
        "objectives",
        "automation",
        "integration",
        "cost",
        "control",
        "log",
        "data",
        "HDFS",
        "Flume",
        "building",
        "multihop",
        "flows",
        "mechanism",
        "MapReduce",
        "jobs",
        "transfer",
        "data",
        "Hbase",
        "data",
        "files",
        "data",
        "Experience",
        "Spark",
        "programming",
        "Scala",
        "ETL",
        "Spark",
        "Spark",
        "SQL",
        "data",
        "MySQL",
        "HDFS",
        "Sqoop",
        "connectors",
        "Hive",
        "tables",
        "Hive",
        "queries",
        "data",
        "analysis",
        "business",
        "requirements",
        "Transformed",
        "Kafka",
        "data",
        "Scala",
        "Installed",
        "Pig",
        "Pig",
        "Latin",
        "Migrating",
        "data",
        "MySQL",
        "database",
        "HBase",
        "MapReduce",
        "jobs",
        "HBase",
        "data",
        "application",
        "Java",
        "Client",
        "APIs",
        "Hive",
        "Pig",
        "Sqoop",
        "Flume",
        "Oozie",
        "Hadoop",
        "cluster",
        "jobs",
        "Oozie",
        "software",
        "development",
        "lifecycle",
        "design",
        "code",
        "reviews",
        "test",
        "development",
        "test",
        "automation",
        "development",
        "methodology",
        "scrum",
        "meetings",
        "Monitoring",
        "Hadoop",
        "cluster",
        "tools",
        "Cloudera",
        "Manager",
        "Automation",
        "script",
        "HDFS",
        "HBase",
        "Cron",
        "jobs",
        "processing",
        "engine",
        "Clouderas",
        "distribution",
        "performance",
        "Environments",
        "Hadoop",
        "MapReduce",
        "HDFS",
        "Sqoop",
        "Hbase",
        "Oozie",
        "SQL",
        "Pig",
        "Flume",
        "HiveJava",
        "Big",
        "Data",
        "Developer",
        "Wells",
        "Forgo",
        "Location",
        "Dallas",
        "TX",
        "December",
        "July",
        "Responsibilities",
        "job",
        "creation",
        "Hadoop",
        "MapReduce",
        "Pig",
        "Hive",
        "HBase",
        "Spark",
        "RDD",
        "Pair",
        "RDD",
        "Flume",
        "Oozie",
        "Sqoop",
        "environment",
        "application",
        "migration",
        "Hadoop",
        "Spark",
        "processing",
        "data",
        "Oracle",
        "database",
        "HDFS",
        "Sqoop",
        "Developed",
        "Oozie",
        "workflows",
        "Sqoop",
        "Hive",
        "Pig",
        "jobs",
        "process",
        "Flume",
        "multiplexing",
        "multisource",
        "interceptors",
        "selectors",
        "log",
        "files",
        "Web",
        "Servers",
        "HDFSHive",
        "Managed",
        "Jobs",
        "Hadoop",
        "cluster",
        "Shell",
        "Scripts",
        "Cluster",
        "coordination",
        "services",
        "Zookeeper",
        "system",
        "filter",
        "partition",
        "data",
        "year",
        "range",
        "format",
        "Hive",
        "functions",
        "schema",
        "relations",
        "PigJoin",
        "sorting",
        "filtering",
        "PigGroup",
        "data",
        "sets",
        "Performed",
        "MapSide",
        "ReduceSide",
        "tables",
        "partition",
        "data",
        "year",
        "range",
        "format",
        "Hive",
        "functions",
        "HBase",
        "tables",
        "readwrites",
        "map",
        "programs",
        "pipeline",
        "data",
        "ingestion",
        "tables",
        "data",
        "integration",
        "transformation",
        "reduction",
        "MapReduce",
        "jobs",
        "java",
        "data",
        "mining",
        "Hive",
        "tables",
        "loading",
        "data",
        "hive",
        "queries",
        "MapReduce",
        "way",
        "Performed",
        "MapSide",
        "ReduceSide",
        "tables",
        "Cloudera",
        "Manager",
        "Hadoop",
        "Cluster",
        "Environments",
        "HDFS",
        "CDH",
        "BigInsights",
        "Apache",
        "Spark",
        "Flume",
        "Hive",
        "Pig",
        "Scala",
        "Java",
        "Sqoop",
        "SQL",
        "Perl",
        "Shell",
        "C",
        "C",
        "Java",
        "Oracle",
        "WebSphere",
        "Application",
        "Server",
        "Spring",
        "Hibernate",
        "Struts",
        "JMS",
        "SQLJava",
        "Developer",
        "Staples",
        "Location",
        "Framingham",
        "MA",
        "September",
        "November",
        "Responsibilities",
        "analysis",
        "requirements",
        "application",
        "J2EE",
        "standards",
        "Scrum",
        "meetings",
        "part",
        "Agile",
        "Methodology",
        "Software",
        "Development",
        "Life",
        "Cycle",
        "SDLC",
        "Object",
        "Oriented",
        "Approach",
        "clients",
        "business",
        "process",
        "client",
        "feedback",
        "Implementing",
        "MVC",
        "Architecture",
        "Spring",
        "Framework",
        "user",
        "interfaces",
        "Core",
        "Java",
        "Spring",
        "Aspect",
        "programming",
        "concepts",
        "security",
        "error",
        "handling",
        "mechanism",
        "application",
        "modules",
        "Spring",
        "MVC",
        "Spring",
        "Annotations",
        "Spring",
        "Beans",
        "Dependency",
        "Injection",
        "database",
        "Hibernate",
        "Java",
        "Collections",
        "API",
        "application",
        "security",
        "protection",
        "XML",
        "REST",
        "JSON",
        "Web",
        "Deployment",
        "serverside",
        "services",
        "Java",
        "Spring",
        "Web",
        "Services",
        "SOAP",
        "Restful",
        "WSDL",
        "JAXB",
        "JAXRPC",
        "Web",
        "pages",
        "JQuery",
        "plugins",
        "Drag",
        "Drop",
        "AutoComplete",
        "AJAX",
        "JSON",
        "Angular",
        "JS",
        "JavaScript",
        "Bootstrap",
        "XSL",
        "XML",
        "data",
        "structure",
        "HTML",
        "pages",
        "Struts",
        "framework",
        "project",
        "struts",
        "action",
        "classes",
        "beans",
        "dispatch",
        "Action",
        "classes",
        "Validation",
        "plugin",
        "Struts",
        "framework",
        "DB2",
        "database",
        "queries",
        "data",
        "database",
        "SQL",
        "queries",
        "procedures",
        "whitebox",
        "test",
        "cases",
        "JUnit",
        "Git",
        "JMeter",
        "Mockito",
        "Framework",
        "Environment",
        "Core",
        "Java",
        "AgileScrum",
        "XML",
        "HTML",
        "Jmeter",
        "SOAP",
        "REST",
        "JDK",
        "JSP",
        "Servlets",
        "JDBC",
        "HTML",
        "CSS",
        "JUnit",
        "SQL",
        "MySQL",
        "Windows",
        "Oracle",
        "Eclipse",
        "Python",
        "Developer",
        "Autodesk",
        "San",
        "Rafael",
        "CA",
        "February",
        "August",
        "Responsibilities",
        "analysis",
        "development",
        "Software",
        "Development",
        "Life",
        "Cycle",
        "SDLC",
        "web",
        "services",
        "middletier",
        "Pythonto",
        "MySQL",
        "backend",
        "API",
        "utilities",
        "micro",
        "services",
        "tasks",
        "Wrote",
        "SQL",
        "queries",
        "Pythonwith",
        "help",
        "query",
        "builders",
        "queries",
        "MySQL",
        "NoSQL",
        "data",
        "store",
        "ETL",
        "processes",
        "Python",
        "Tasks",
        "CRUD",
        "cleansing",
        "data",
        "format",
        "JSON",
        "Developed",
        "Pythonscripts",
        "auditing",
        "tasks",
        "Excel",
        "reports",
        "logistics",
        "sales",
        "inventory",
        "management",
        "system",
        "assistant",
        "Web",
        "Programmer",
        "DjangoFlask",
        "Web",
        "apps",
        "sample",
        "format",
        "testing",
        "REST",
        "File",
        "IO",
        "tasks",
        "python",
        "Dealt",
        "CSV",
        "JSON",
        "Text",
        "XML",
        "XLSX",
        "files",
        "shell",
        "scripting",
        "tasks",
        "testdriven",
        "approach",
        "Pythons",
        "Unit",
        "Testing",
        "Environment",
        "Python",
        "MySQL",
        "PostgreSQL",
        "Jira",
        "Flask",
        "JSON",
        "CSV",
        "Excel",
        "Eclipse",
        "Education",
        "Bachelors",
        "Skills",
        "HDFS",
        "IMPALA",
        "MAPREDUCE",
        "OOZIE",
        "SQOOP"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T20:16:40.806968",
    "resume_data": "Senior Hadoop Developer Senior Hadoop span lDeveloperspan Senior Hadoop Developer MoneyGram International Location Dallas TX 8 years of experience in IT which includes experience in Hadoop ecosystem Bigdata Technologies QA Automation Tools SQL related technologies in Retail Manufacturing Financial and Communication sectors 4 Years of experience as Hadoop Developer using Various Hadoop ecosystems tools and Spark Framework and Currently working on Spark and Spark Streaming frameworks extensively using Scala as the main programming dialect Experience installingconfiguringmaintaining Apache Hadoop clusters for application development and Hadoop tools like Sqoop Hive PIG Flume Hbase Kafka Hue Storm Zoo Keeper Oozie MangoDB Sqoop Python Worked with major distributions like Cloudera CDH 34 Horton works Distributions and AWS Also worked on Unix and DWH in support for various Distributions Hands on experience in developing and deploying enterprise based applications using major components in Hadoop ecosystem like Hadoop 2X YARN Hive Pig MapReduce Spark Kafka Storm Oozie HBase Flume Sqoop and ZooKeeper Experience in handling large datasets using Partitions Spark in memory capabilities Broadcasts in Spark with Scala and python Effective and efficient Joins Transformations and other during ingestion process itself Experience in developing data pipeline using Pig Sqoop and Flume to extract the data from weblogs and store in HDFS and accomplished developing Pig Latin Scripts and using HiveQL for data analytics Extensively dealt with Spark Streaming and Apache Kafka to fetch live stream data Experience in converting HiveSQL queries into Spark transformations using Java and experience in ETL development using Kafka Flume and Sqoop Good experience in writing Spark applications using Scala and Java and used Scala sbt to develop Scala projects and executed using SparkSubmit Experience working on NoSQL databases including Hbase MangoDB and experience using Sqoop to import data into HDFS from RDBMS and viceversa Developed Spark scripts by using Scala shell commands as per the requirement Good experience in writing Sqoop queries for transferring bulk data between Apache Hadoop and structured data stores Substantial experience in writing Map Reduce jobs in Java PIG Flume Zookeeper Hive and Storm Created multiple MapReduce Jobs using Java API Pig and Hive for data extraction Strong expertise in troubleshooting and performance finetuning Spark MapReduce and Hive applications Good experience on working with Amazon EMR framework for processing data on EMR and EC2 instances Created AWS VPC network for the installed Instances and configured security groups and Elastic IPs Accordingly Developed AWS Cloud formation templates to create custom sized VPC subnets EC2 instances ELB and security groups Extensive experience in developing applications that perform Data Processing tasks using Teradata Oracle SQL Server and MySQL database Worked on data warehousing and ETL tools like Informatica Tableau and Pentaho Experience in understanding the security requirements for Hadoop and integrate with Kerberos authentication and authorization infrastructure Acquaintance with Agile and Waterfall methodologies Responsible for handling several clients facing meetings with great communication skills Work Experience Senior Hadoop Developer MoneyGram International Location Dallas TX May 2018 to Present Responsibilities Worked on analysingHadoop cluster and different big data analytical and processing tools including Pig Hive Sqoopand Spark with Scala java Spark Streaming Wrote SparkStreaming applications to consume the data from Kafka topics and wrote processed streams to HBase and steamed data using Spark with Kafka Worked on the largescale Hadoop YARN cluster for distributed data processing and analysis using Spark Hive and HBase Involved in creating datalake by extracting customers data from various data sources to HDFS which include data from Excel databases and log data from servers Developed Apache Spark applications by using Scala for data processing from various streaming sources Used Scala to convert HiveSQL queries into RDD transformations in Apache Spark Implemented Spark solutions to generate reports fetch and load data in Hive Experienced in writing realtime processing and core jobs using Spark Streaming with Kafka as a data pipeline system Written HiveQL to analyse the number of unique visitors and their visit information such as views most visited pages etc Configured Spark Streaming to receive real time data from the Apache Kafka and store the stream data to HDFS using Scala Monitored workload job performance and capacity planning using Cloudera Manager Experienced on working with Amazon EMR framework for processing data on EMR and EC2 instances Designing and implementing complete endtoend Hadoop Infrastructure including Pig Hive Sqoop Oozie Flume and Zookeeper Further used pig to do transformations event joins elephant bird API and pre aggregations performed before loading JSON files format onto HDFS Involved in resolving performance issues in Pig and Hive with understanding of Map Reduce physical plan execution and using debugging commands to run code in optimized way Used Spark to perform analytics on data in Hive and experienced with ETL working with Hive and MapReduce Environments Hdp 260 HDFS MapReduce Spark Streaming SparkCore Spark SQL Scala Pig 014 Hive 121 Sqoop 144 Flume 160 Kafka JSON HBase Hadoop Developer New York Times New York NY August 2016 to May 2018 Responsibilities Installed and configured Hadoop MapReduce HDFS and developed multiple MapReduce jobs in Java for data cleansing and prep Worked on Spark streaming to collect TBs of data for every hour from connected cars Worked on Spark Batch Processing to load the processed data into MongoDB Manage migration of onperm servers to AWS by creating golden images for upload and deployment Manage multiple AWS accounts with multiple VPCs for both production and nonproduction where primary objectives are automation build out integration and cost control Moving log data periodically into HDFS using Flume and building multihop flows fanout flows and failover mechanism Developed MapReduce jobs to automate transfer of data from Hbase and to read data files and scrub the data Experience in Spark programming using Scala Experienced in performing ETL using Spark Spark SQL Transferring data between MySQL and HDFS using Sqoop with connectors Creating and populating Hive tables and writing Hive queries for data analysis to meet the business requirements Transformed Kafka loaded data using Sparkstreaming with Scala Installed and configured Pig and written Pig Latin scripts Migrating data from MySQL database to HBase Running MapReduce jobs to access HBase data from application using Java Client APIs Installed and configured Hive Pig Sqoop Flume and Oozie on the Hadoop cluster and automating the jobs using Oozie Actively participated in software development lifecycle including design and code reviews test development test automation Involved in solutiondriven agile development methodology and actively participated in daily scrum meetings Monitoring Hadoop cluster using tools like Cloudera Manager Automation script to monitor HDFS and HBase through Cron jobs Create a complete processing engine based on Clouderas distribution enhanced to performance Environments Hadoop MapReduce HDFS Sqoop Hbase Oozie SQL Pig Flume HiveJava Big Data Developer Wells Forgo Location Dallas TX December 2014 to July 2016 Responsibilities Installed configured and job creation in Hadoop MapReduce Pig Hive HBase Spark RDD Pair RDD Flume Oozie Sqoop environment Involved in application migration from Hadoop to Spark for the fast processing Extracted data from Oracle database into HDFS using Sqoop Developed Oozie workflows to schedule and manage Sqoop Hive Pig jobs to ExtractTransformLoad process Used Flume and configured it to use multiplexing replicating multisource interceptors selectors to import log files from Web Servers in to HDFSHive Managed and scheduled Jobs on a Hadoop cluster using Shell Scripts Maintained Cluster coordination services through Zookeeper for system Involved in filter the partition data based on different year range different format using Hive functions Defining a schema creating new relations performing PigJoin sorting and filtering using PigGroup on large data sets Performed MapSide joins and ReduceSide joins for large tables Involved in filtering the partition data based on different year range different format using with Hive functions Creating HBase tables for random readwrites by the map reduce programs Designed and developed entire pipeline from data ingestion to reporting tables Performed data cleaning integration transformation reduction by developing MapReduce jobs in java for data mining Creating Hive tables loading data into it and customizing hive queries internally operating in MapReduce way Performed MapSide joins and ReduceSide joins for large tables Used Cloudera Manager to monitor and manage Hadoop Cluster Environments HDFS CDH BigInsights Apache Spark Flume Hive Pig Scala Java Sqoop SQL Perl Shell scripting C C Java Oracle WebSphere Application Server Spring Hibernate Struts JMS SQLJava Developer Staples Location Framingham MA September 2012 to November 2014 Responsibilities Responsible for the analysis documenting the requirements and architecting the application based on J2EE standards Attended Scrum meetings daily as a part of Agile Methodology Involved in complete Software Development Life Cycle SDLC with Object Oriented Approach of clients business process and continuous client feedback Implementing MVC Architecture using Spring Framework customized user interfaces Used Core Java and Spring Aspect Oriented programming concepts for logging security error handling mechanism Developed application modules using Spring MVC Spring Annotations Spring Beans Dependency Injection with database interfaceusing Hibernate Used the Java Collections API extensively in the application as security protection for XML SOAP REST and JSON to make a secure Web Deployment Developed serverside services using Java Spring Web Services SOAP Restful WSDL JAXB JAXRPC Built Web pages that are more userinteractive using JQuery plugins for Drag and Drop AutoComplete AJAX JSON Angular JS JavaScript and Bootstrap Used XSL to transform XML data structure into HTML pages Used Struts as the framework in this project and developed struts action classes form beans Created dispatch Action classes and Validation plugin using Struts framework DB2 was used as the database and wrote queries to extract data from the database Developed SQL queries and stored procedures Designed Developed whitebox test cases using JUnit Git JMeter Mockito Framework Environment Core Java AgileScrum XML HTML Jmeter SOAP REST JDK JSP Servlets JDBC HTML CSS JUnit SQL MySQL Windows Oracle Eclipse Python Developer Autodesk San Rafael CA February 2011 to August 2012 Responsibilities Involved in the analysis and development of Software Development Life Cycle SDLC Contributed in developing a web services middletier in Pythonto integrate with an existing MySQL backend Wrote several internal API utilities and micro services to carry out specific tasks Wrote and executed exhaustive SQL queries using Pythonwith help from various query builders inPythonto frame the queries Migrated MySQL to NoSQL data store using ETL processes using Python Tasks included CRUD elaborate cleansing of data and packing it into the expected format JSON Developed Pythonscripts to perform auditing tasks and generating Excel reports to support engineering a logistics sales and inventory management system Worked as assistant to Web Programmer in creating DjangoFlask Web apps Generated sample JSON format queries for testing REST endpoints Frequently performed ad hoc File IO tasks in python Dealt with CSV JSON Text XML and XLSX files Used shell scripting to automate repetitive tasks Followed a testdriven approach closely Used Pythons Unit Testing extensively Environment Python MySQL PostgreSQL Jira Flask REST JSON CSV Excel Eclipse Education Bachelors Skills HDFS IMPALA MAPREDUCE OOZIE SQOOP",
    "unique_id": "2d37e52f-8a96-472d-8adf-e11e1129fbc5"
}