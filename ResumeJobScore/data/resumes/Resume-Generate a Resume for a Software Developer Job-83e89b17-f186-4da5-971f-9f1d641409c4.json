{
    "clean_data": "Spark Developer Spark span lDeveloperspan Spark Developer Highmark Health Solutions 39 years of overall IT experience in Application development in Python and Hadoop 25 years of experience in deployment of Hadoop Ecosystems like HadoopHDFS Hive Pig HBase Sqoop Flume Spark and Scala Good working experience on Hive and Pig Having experience on developing Apache Spark programs using scala for largedataset processing and using the inmemory computing capabilities for faster data processing with spark core and  Expertise in developing spark RDD transformation actions Dataframes case classes for the required input data and performed the data transformation using sparkcore Very good understanding of partition bucketing concepts in hive and designed both external and internal tables in hive optimize performance Involved in creating Hive tables loading with data and writing hive queries which will run internally in map reduce way Experience in writing Pig Scripts to build the source to target mapping Experience in migrating the data using Sqoop from HDFS to relational data base system and viceversa according to the other clients Good experience working with NoSQL technologies using HBase Having good knowledge on UNIX commands I have knowledge on PySpark Work Experience Spark Developer Highmark Health Solutions October 2017 to Present Project Summary Highmark is national diversified health care partner serving members through its business in health insurance dental insurance vision care and reinsurance Highmark provides information guidance and operational services necessary for hospitals and health plans to transform their organizations into highperforming accountable delivery systems Highmark operate health care plans in Pennsylvania Delaware and West Virginia that serve 53 million members Roles Responsibilities Responsible for processing scalable distributed data solutions using Hadoop Developing Data pipe line using spark and hive to ingest data into hadoop cluster for analysis Used various transformation and actions for cleaning the input Loaded data into spark RDD and Data Frame to do in memory data computation to generate the output response Used SparkSQL to process the huge amount of structured data to perform SQL operations Exploring with the spark improving the performance and Optimizing of existing algorithms in Hadoop using Spark Context SparkSQL Data frames and pair RDDs Involved in creating hive tables loading and analyzing data using hive queries Worked extensively with sqoop for importing data from RDBMS Environment HDFS Spark Scala Hive Sqoop Kafka HBase Oozie Flume Parquet Linux Eclipse Maven Project2 Software Engineer TCS Hyderabad ANDHRA PRADESH IN July 2016 to Present Hadoop Developer Prepaid and Merchant solutions July 2016 to September 2017 Project Summary TSYS make it possible for millions of buyers and sellers to move money around the world with trust and confidence supports Issuing Services Acquiring services Prepaid and Merchant solutions This project is responsible to get the merchant sales and the credit card transactions Then it splits the transactions based on the card type and generates the settlement files for the respective providers The project is critical as it deals with millions of dollars on a daily basis The platform is built on Hadoop ecosystem with HDFSHBase being the primary data storage Roles Responsibilities Involved in end to end data processing like ingestion processing quality checks and splitting Bringing the data into Big Data Lake using Pig Sqoop and Hive Refined terabytes of data from different sources and created hive tables Importing and exporting data into HDFS and HIVE from MySql database using Sqoop Responsible to manage data coming from different sources Responsible for loading data from UNIX file systems into HDFS Wrote Pig scripts to process unstructured data and create structure data for use with Hive Environment Sqoop HDFS Pig Hive Map Reduce Java Oozie Eclipse Linux Oracle Teradata Project3 Python Developer IMI Mobile March 2015 to June 2016 Project Summary The main idea for implementing this project is to replace existing manual tender system with computerized system Online tendering in its simplest form is described as the electronic publishing communicating accessing receiving and submitting of all tenderrelated information and documentation via the internet there by replacing the traditional processes and achieving a more efficient and effective business business process for all government PWDs and Contractors Roles Responsibilities Preparation of functional requirement specification as per client requirement Implemented code specified by client DevelopingEnhancing the code based on the client requirements Involved in development enhancement and support activities coordinating Involved in creating different models using python required for the project Environment python Django MySQL db IMI Mobile Hyderabad ANDHRA PRADESH IN February 2015 to June 2016 Additional Information Technical Skills Big Data Technologies Hadoop HDFS MapReduce Hive Pig Hbase Sqoop Flume Oozie Spark Data Bases Oracle MySQL Sql Programming Languages C C Python Scala Operating Systems Windows Linux centos and Ubuntu",
    "entities": [
        "Hadoop Ecosystems",
        "Python",
        "Present Project Summary Highmark",
        "DevelopingEnhancing",
        "Spark Context",
        "Delaware",
        "SparkSQL",
        "Sqoop",
        "PySpark Work Experience Spark Developer Highmark Health Solutions",
        "HIVE",
        "Optimizing",
        "Sqoop Responsible",
        "HDFS",
        "UNIX",
        "Oozie Spark Data Bases Oracle",
        "Hive Refined",
        "Merchant",
        "Highmark",
        "HBase Having",
        "Present Hadoop Developer Prepaid",
        "Spark Developer Highmark Health Solutions",
        "Spark Developer Spark",
        "SQL",
        "RDD",
        "Hadoop",
        "RDBMS Environment HDFS Spark Scala Hive",
        "Big Data Lake",
        "Software Engineer TCS Hyderabad",
        "Hive Environment Sqoop HDFS Pig Hive Map Reduce Java Oozie",
        "NoSQL",
        "West Virginia",
        "Application",
        "Additional Information Technical Skills Big Data Technologies Hadoop HDFS MapReduce Hive Pig Hbase",
        "Issuing Services Acquiring",
        "Data Frame",
        "Contractors Roles Responsibilities Preparation",
        "Pennsylvania",
        "MySql"
    ],
    "experience": "Experience in writing Pig Scripts to build the source to target mapping Experience in migrating the data using Sqoop from HDFS to relational data base system and viceversa according to the other clients Good experience working with NoSQL technologies using HBase Having good knowledge on UNIX commands I have knowledge on PySpark Work Experience Spark Developer Highmark Health Solutions October 2017 to Present Project Summary Highmark is national diversified health care partner serving members through its business in health insurance dental insurance vision care and reinsurance Highmark provides information guidance and operational services necessary for hospitals and health plans to transform their organizations into highperforming accountable delivery systems Highmark operate health care plans in Pennsylvania Delaware and West Virginia that serve 53 million members Roles Responsibilities Responsible for processing scalable distributed data solutions using Hadoop Developing Data pipe line using spark and hive to ingest data into hadoop cluster for analysis Used various transformation and actions for cleaning the input Loaded data into spark RDD and Data Frame to do in memory data computation to generate the output response Used SparkSQL to process the huge amount of structured data to perform SQL operations Exploring with the spark improving the performance and Optimizing of existing algorithms in Hadoop using Spark Context SparkSQL Data frames and pair RDDs Involved in creating hive tables loading and analyzing data using hive queries Worked extensively with sqoop for importing data from RDBMS Environment HDFS Spark Scala Hive Sqoop Kafka HBase Oozie Flume Parquet Linux Eclipse Maven Project2 Software Engineer TCS Hyderabad ANDHRA PRADESH IN July 2016 to Present Hadoop Developer Prepaid and Merchant solutions July 2016 to September 2017 Project Summary TSYS make it possible for millions of buyers and sellers to move money around the world with trust and confidence supports Issuing Services Acquiring services Prepaid and Merchant solutions This project is responsible to get the merchant sales and the credit card transactions Then it splits the transactions based on the card type and generates the settlement files for the respective providers The project is critical as it deals with millions of dollars on a daily basis The platform is built on Hadoop ecosystem with HDFSHBase being the primary data storage Roles Responsibilities Involved in end to end data processing like ingestion processing quality checks and splitting Bringing the data into Big Data Lake using Pig Sqoop and Hive Refined terabytes of data from different sources and created hive tables Importing and exporting data into HDFS and HIVE from MySql database using Sqoop Responsible to manage data coming from different sources Responsible for loading data from UNIX file systems into HDFS Wrote Pig scripts to process unstructured data and create structure data for use with Hive Environment Sqoop HDFS Pig Hive Map Reduce Java Oozie Eclipse Linux Oracle Teradata Project3 Python Developer IMI Mobile March 2015 to June 2016 Project Summary The main idea for implementing this project is to replace existing manual tender system with computerized system Online tendering in its simplest form is described as the electronic publishing communicating accessing receiving and submitting of all tenderrelated information and documentation via the internet there by replacing the traditional processes and achieving a more efficient and effective business business process for all government PWDs and Contractors Roles Responsibilities Preparation of functional requirement specification as per client requirement Implemented code specified by client DevelopingEnhancing the code based on the client requirements Involved in development enhancement and support activities coordinating Involved in creating different models using python required for the project Environment python Django MySQL db IMI Mobile Hyderabad ANDHRA PRADESH IN February 2015 to June 2016 Additional Information Technical Skills Big Data Technologies Hadoop HDFS MapReduce Hive Pig Hbase Sqoop Flume Oozie Spark Data Bases Oracle MySQL Sql Programming Languages C C Python Scala Operating Systems Windows Linux centos and Ubuntu",
    "extracted_keywords": [
        "Spark",
        "Developer",
        "Spark",
        "span",
        "lDeveloperspan",
        "Spark",
        "Developer",
        "Highmark",
        "Health",
        "Solutions",
        "years",
        "IT",
        "experience",
        "Application",
        "development",
        "Python",
        "Hadoop",
        "years",
        "experience",
        "deployment",
        "Hadoop",
        "Ecosystems",
        "Hive",
        "Pig",
        "HBase",
        "Sqoop",
        "Flume",
        "Spark",
        "Scala",
        "Good",
        "working",
        "experience",
        "Hive",
        "Pig",
        "experience",
        "Apache",
        "Spark",
        "programs",
        "scala",
        "largedataset",
        "processing",
        "inmemory",
        "computing",
        "capabilities",
        "data",
        "processing",
        "spark",
        "core",
        "Expertise",
        "spark",
        "RDD",
        "transformation",
        "actions",
        "case",
        "classes",
        "input",
        "data",
        "data",
        "transformation",
        "sparkcore",
        "understanding",
        "partition",
        "bucketing",
        "concepts",
        "hive",
        "tables",
        "hive",
        "optimize",
        "performance",
        "Hive",
        "tables",
        "data",
        "hive",
        "queries",
        "map",
        "way",
        "Experience",
        "Pig",
        "Scripts",
        "source",
        "mapping",
        "Experience",
        "data",
        "Sqoop",
        "HDFS",
        "data",
        "base",
        "system",
        "viceversa",
        "clients",
        "experience",
        "NoSQL",
        "technologies",
        "HBase",
        "knowledge",
        "UNIX",
        "commands",
        "knowledge",
        "PySpark",
        "Work",
        "Experience",
        "Spark",
        "Developer",
        "Highmark",
        "Health",
        "Solutions",
        "October",
        "Present",
        "Project",
        "Summary",
        "Highmark",
        "health",
        "care",
        "partner",
        "members",
        "business",
        "health",
        "insurance",
        "insurance",
        "vision",
        "care",
        "reinsurance",
        "Highmark",
        "information",
        "guidance",
        "services",
        "hospitals",
        "health",
        "plans",
        "organizations",
        "delivery",
        "systems",
        "Highmark",
        "health",
        "care",
        "plans",
        "Pennsylvania",
        "Delaware",
        "West",
        "Virginia",
        "members",
        "Roles",
        "Responsibilities",
        "data",
        "solutions",
        "Hadoop",
        "Developing",
        "Data",
        "pipe",
        "line",
        "spark",
        "hive",
        "data",
        "hadoop",
        "cluster",
        "analysis",
        "transformation",
        "actions",
        "input",
        "data",
        "spark",
        "RDD",
        "Data",
        "Frame",
        "memory",
        "data",
        "computation",
        "output",
        "response",
        "SparkSQL",
        "amount",
        "data",
        "SQL",
        "operations",
        "spark",
        "performance",
        "Optimizing",
        "algorithms",
        "Hadoop",
        "Spark",
        "Context",
        "SparkSQL",
        "Data",
        "frames",
        "RDDs",
        "tables",
        "data",
        "hive",
        "queries",
        "sqoop",
        "data",
        "RDBMS",
        "Environment",
        "HDFS",
        "Spark",
        "Scala",
        "Hive",
        "Sqoop",
        "Kafka",
        "HBase",
        "Oozie",
        "Flume",
        "Parquet",
        "Linux",
        "Eclipse",
        "Maven",
        "Project2",
        "Software",
        "Engineer",
        "TCS",
        "Hyderabad",
        "ANDHRA",
        "PRADESH",
        "July",
        "Present",
        "Hadoop",
        "Developer",
        "Prepaid",
        "Merchant",
        "solutions",
        "July",
        "September",
        "Project",
        "Summary",
        "TSYS",
        "millions",
        "buyers",
        "sellers",
        "money",
        "world",
        "trust",
        "confidence",
        "Issuing",
        "Services",
        "services",
        "Prepaid",
        "Merchant",
        "solutions",
        "project",
        "merchant",
        "sales",
        "credit",
        "card",
        "transactions",
        "transactions",
        "card",
        "type",
        "settlement",
        "files",
        "providers",
        "project",
        "millions",
        "dollars",
        "basis",
        "platform",
        "Hadoop",
        "ecosystem",
        "HDFSHBase",
        "data",
        "storage",
        "Roles",
        "Responsibilities",
        "end",
        "data",
        "processing",
        "ingestion",
        "quality",
        "checks",
        "data",
        "Big",
        "Data",
        "Lake",
        "Pig",
        "Sqoop",
        "Hive",
        "Refined",
        "terabytes",
        "data",
        "sources",
        "hive",
        "tables",
        "data",
        "HDFS",
        "HIVE",
        "MySql",
        "database",
        "Sqoop",
        "Responsible",
        "data",
        "sources",
        "loading",
        "data",
        "UNIX",
        "file",
        "systems",
        "HDFS",
        "Wrote",
        "Pig",
        "scripts",
        "data",
        "structure",
        "data",
        "use",
        "Hive",
        "Environment",
        "Sqoop",
        "HDFS",
        "Pig",
        "Hive",
        "Map",
        "Java",
        "Oozie",
        "Eclipse",
        "Linux",
        "Oracle",
        "Teradata",
        "Project3",
        "Python",
        "Developer",
        "IMI",
        "Mobile",
        "March",
        "June",
        "Project",
        "Summary",
        "idea",
        "project",
        "tender",
        "system",
        "system",
        "Online",
        "form",
        "publishing",
        "accessing",
        "submitting",
        "information",
        "documentation",
        "internet",
        "processes",
        "business",
        "business",
        "process",
        "government",
        "PWDs",
        "Contractors",
        "Roles",
        "Responsibilities",
        "Preparation",
        "requirement",
        "specification",
        "client",
        "requirement",
        "Implemented",
        "code",
        "client",
        "DevelopingEnhancing",
        "code",
        "client",
        "requirements",
        "development",
        "enhancement",
        "support",
        "activities",
        "models",
        "python",
        "project",
        "Environment",
        "Django",
        "MySQL",
        "IMI",
        "Mobile",
        "Hyderabad",
        "ANDHRA",
        "PRADESH",
        "February",
        "June",
        "Additional",
        "Information",
        "Technical",
        "Skills",
        "Big",
        "Data",
        "Technologies",
        "Hadoop",
        "HDFS",
        "MapReduce",
        "Hive",
        "Pig",
        "Hbase",
        "Sqoop",
        "Flume",
        "Oozie",
        "Spark",
        "Data",
        "Bases",
        "Oracle",
        "MySQL",
        "Sql",
        "Programming",
        "Languages",
        "C",
        "C",
        "Python",
        "Scala",
        "Operating",
        "Systems",
        "Windows",
        "Linux",
        "centos",
        "Ubuntu"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T21:59:51.201300",
    "resume_data": "Spark Developer Spark span lDeveloperspan Spark Developer Highmark Health Solutions 39 years of overall IT experience in Application development in Python and Hadoop 25 years of experience in deployment of Hadoop Ecosystems like HadoopHDFS Hive Pig HBase Sqoop Flume Spark and Scala Good working experience on Hive and Pig Having experience on developing Apache Spark programs using scala for largedataset processing and using the inmemory computing capabilities for faster data processing with spark core and sparkSQL Expertise in developing spark RDD transformation actions Dataframes case classes for the required input data and performed the data transformation using sparkcore Very good understanding of partition bucketing concepts in hive and designed both external and internal tables in hive optimize performance Involved in creating Hive tables loading with data and writing hive queries which will run internally in map reduce way Experience in writing Pig Scripts to build the source to target mapping Experience in migrating the data using Sqoop from HDFS to relational data base system and viceversa according to the other clients Good experience working with NoSQL technologies using HBase Having good knowledge on UNIX commands I have knowledge on PySpark Work Experience Spark Developer Highmark Health Solutions October 2017 to Present Project Summary Highmark is national diversified health care partner serving members through its business in health insurance dental insurance vision care and reinsurance Highmark provides information guidance and operational services necessary for hospitals and health plans to transform their organizations into highperforming accountable delivery systems Highmark operate health care plans in Pennsylvania Delaware and West Virginia that serve 53 million members Roles Responsibilities Responsible for processing scalable distributed data solutions using Hadoop Developing Data pipe line using spark and hive to ingest data into hadoop cluster for analysis Used various transformation and actions for cleaning the input Loaded data into spark RDD and Data Frame to do in memory data computation to generate the output response Used SparkSQL to process the huge amount of structured data to perform SQL operations Exploring with the spark improving the performance and Optimizing of existing algorithms in Hadoop using Spark Context SparkSQL Data frames and pair RDDs Involved in creating hive tables loading and analyzing data using hive queries Worked extensively with sqoop for importing data from RDBMS Environment HDFS Spark Scala Hive Sqoop Kafka HBase Oozie Flume Parquet Linux Eclipse Maven Project2 Software Engineer TCS Hyderabad ANDHRA PRADESH IN July 2016 to Present Hadoop Developer Prepaid and Merchant solutions July 2016 to September 2017 Project Summary TSYS make it possible for millions of buyers and sellers to move money around the world with trust and confidence supports Issuing Services Acquiring services Prepaid and Merchant solutions This project is responsible to get the merchant sales and the credit card transactions Then it splits the transactions based on the card type and generates the settlement files for the respective providers The project is critical as it deals with millions of dollars on a daily basis The platform is built on Hadoop ecosystem with HDFSHBase being the primary data storage Roles Responsibilities Involved in end to end data processing like ingestion processing quality checks and splitting Bringing the data into Big Data Lake using Pig Sqoop and Hive Refined terabytes of data from different sources and created hive tables Importing and exporting data into HDFS and HIVE from MySql database using Sqoop Responsible to manage data coming from different sources Responsible for loading data from UNIX file systems into HDFS Wrote Pig scripts to process unstructured data and create structure data for use with Hive Environment Sqoop HDFS Pig Hive Map Reduce Java Oozie Eclipse Linux Oracle Teradata Project3 Python Developer IMI Mobile March 2015 to June 2016 Project Summary The main idea for implementing this project is to replace existing manual tender system with computerized system Online tendering in its simplest form is described as the electronic publishing communicating accessing receiving and submitting of all tenderrelated information and documentation via the internet there by replacing the traditional processes and achieving a more efficient and effective business business process for all government PWDs and Contractors Roles Responsibilities Preparation of functional requirement specification as per client requirement Implemented code specified by client DevelopingEnhancing the code based on the client requirements Involved in development enhancement and support activities coordinating Involved in creating different models using python required for the project Environment python Django MySQL db IMI Mobile Hyderabad ANDHRA PRADESH IN February 2015 to June 2016 Additional Information Technical Skills Big Data Technologies Hadoop HDFS MapReduce Hive Pig Hbase Sqoop Flume Oozie Spark Data Bases Oracle MySQL Sql Programming Languages C C Python Scala Operating Systems Windows Linux centos and Ubuntu",
    "unique_id": "83e89b17-f186-4da5-971f-9f1d641409c4"
}