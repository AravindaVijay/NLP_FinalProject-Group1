{
    "clean_data": "Hadoop Developer Hadoop span lDeveloperspan Hadoop Developer Liberty Mutual Dover NH About 9years of experience with heavy exposure to Big DataTechnologies and implementation of various trending technologies in Big Data Eco Systems under various verticals such as HealthcareRetail and Private Sectors 4 Years of exposure to Data Science Machine Learning Hadoop and Spark Ecosystems implementing innovative solutions for various organizations Hands on experience in Apache Hadoop ecosystem components like Hive Pig Hadoop MapReduce Sqoop Flume Kafka Storm Spark Oozie Zookeeper YARN Impala and HBase along other Hadoop ecosystem components Authorized to work in the US for any employer Work Experience Hadoop Developer Liberty Mutual Dover NH August 2016 to Present Responsibilities Evaluated suitability of Hadoop and its ecosystem to the above project and implemented various proof of concept POC applications to eventually adopt them to benefit from the Big Data Hadoop initiative Estimated Software Hardware requirements for the Name Node and Data Node planning theYARN cluster Extracted the needed data from the server into HDFS and Bulk Loaded the cleaned data into HBase and Cassandra Written the Map Reduce programs HiveUDFs in Java where the functionality is too complex Involved in importing and exporting data intoHDFS and Hive using Sqoop Integrating bulk data into Cassandra file system using MapReduce programs Develop Hive queries for the analysis to categorize different items Designing and creating Hive external tables using shared metastore instead of derby with partitioning dynamic partitioning and buckets Reviewing peer table creation in Hive data loading and queries Used Flume to handle the real time log processing for attribution reports Developed Pig Latin scripts to extract the data from web server output files to load in HDFS Used Oozie workflow for Pig and Hive jobs Involved in working with Spark on top of YARN for interactive and batch analysis Good understanding on DAG cycle for entire spark application flow on Spark application WebUI Experienced in writing live Realtime Processing using Spark Streaming with Kafka Maintained System integrity of all subcomponents like Pig Hive Spark Kafka HBase and Cassandra Weekly meetings with technical collaborators and active participation in code review sessions with senior and junior developers Involved in launching managing and monitoring the Hadoop cluster using ClouderaManager Environment Apache Hadoop HDFS HBase Hive Pig MapReduce Java Ambari Cassandra Sqoop Flume Cloudera Kafka Scala Spark Oozie MySQL Linux Hadoop Developer September 2015 to July 2016 Target MN Responsibilities Evaluated business requirements and prepared detailed specifications that follow project guidelines required to develop suitable programs Worked on analyzing HadoopCluster and different big data analytic tools including Pig Hive and MongoDB Involved in importing data from Oracle into HDFS and Hive using Sqoop Created tables using Impala and involved in creating Queries which are stored in HBase Implemented complex scripts to support test driven development and continuous integration Installed and configured Hadoop MapReduce and HDFS and developed multiple MapReduce jobs in Java for data Involved in data modeling the large data from various sources using NoSQL databases HBase MongoDB Created Pig Latin scripts to sort group join and filter the enterprise wide to get transformed data sets Involved in creating Hive tables loading data and creating Hive queries that will run internally in MapReduce way Used Hive to analyze the partitioned and bucketed data and compute various metrics for reporting Experience in HiveHBase integration by defining external tables in hive and pointing the HBase as data store for better performance and lower IO Exported the analyzed data to relational databases using Sqoop for visualization and to generate reports for the BI team Hands on experience in Tableau which is used for visualization and reports generation Involved in maintaining and debugging MR programs Mentoring analyst and test team for writing Hive queries Installed Oozie workflow engine to run multiple MapReduce jobs Worked with application teams to install operating systems Hadoop updates patches version upgrades as required Good experience in monitoring and managing the Hadoop cluster using Cloudera Manager Environment Hadoop HDFS MapReduce Hive Eclipse Pig Sqoop Linux Flume Java Shell Scripting Tableau Python Zookeeper HBase Informatica MongoDB Cloudera Oozie Hadoop Developer Home Depot Atlanta GA June 2013 to August 2015 Responsibilities Responsible for building scalable distributed data solutions using Hadoop Implemented 20 nodesCDH4 Hadoop cluster on LINUX Involved in loading data from LINUX file system to HDFS Estimated Software Hardware requirements for the NameNode and DataNode planning the cluster Implemented a script to transmit data from Teradata to HBase using Sqoop and created tables out of that data on HBase Implemented best income logic using Pigscripts and UDFs Implemented test scripts to support test driven development and continuous integration Worked on tuning the performance Pig queries Cluster Coordination service through Zookeeper Load and transform large sets of structured semi structured and unstructured data Responsible for cluster maintenance by Cloudera adding and removing cluster nodes cluster monitoring and troubleshooting manage and review data backups manage and review Hadoop log files Installing Oozie workflow engine to run multiple Hive and Pig jobs Job management using Fair scheduler Exported the analyzed data to the relational databases using Sqoop for visualization and to generate reports for the BI team Analyzed large amounts of data sets to determine optimal way to aggregate and report on it Supported in setting up QA environment and updating configurations for implementing scripts with Pig and Sqoop Environment Hadoop HDFS Hive Pig Sqoop HBase Shell Scripting Ubuntu Linux Zookeeper Teradata Oozie Cloudera Java Developer ChevronCorporation Irvine CA November 2010 to May 2012 Responsibilities Worked with business analyst in understanding business requirements design and development of the project Implemented the JSP frame work with MVC architecture Created new JSPs for the front end using HTML Java Script Jquery and Ajax Developed the presentation layer using JSP HTML CSS and client side validations using JavaScript Involved in creating Restful web services using JAX_RS and JERSEY tool Involved in designing creating reviewing Technical Design Documents Developed DAOs Data Access Object using Hibernate as ORM to interact with DBMS Oracle Applied J2EE design patterns like Business Delegate DAO and Singleton Involved in developing DAOs using JDBC Worked with QA team in preparation and review of test cases JUnit was used for unit testing for the integration testing tool Writing SQL queries to fetch the business data using Oracle as database Developed UI for Customer Service Modules and Reports using JSF JSPs and My Faces Components Log4j used for logging the application log of the running system to trace the errors and certain automated routine functions Environment Java JSP JavaScript Servlets Hibernate REST EJB JSF JSP Ant Tomcat Eclipse SQL Oracle Jr Java Developer July 2008 to October 2010 Project Employee Billing Management System EBMS Responsibilities Worked and maintained the front end functionality of the Website from scratch Using HTML CSS and Java Script Developed server side functionality of the website using JAVA and SPRING MVC Involved in discussions with clients to gather requirements to fulfill the objectives Interacted with several teams to implement the requirements I was responsible for preparing the technical documentation Environment JAVA and SPRING MVC HTML CSS and Java Script Education Hadoop Architecture Secondary NameNode Bachelors Skills JAVA 6 years APACHE HADOOP HDFS 4 years APACHE HADOOP OOZIE 4 years APACHE HADOOP SQOOP 4 years APACHE HBASE 4 years Apache Hadoop HDFS HBase Hive Pig MapReduce Java Ambari Cassandra Sqoop Flume Cloudera Kafka Scala Spark Oozie MySQL Linux Additional Information TECHNICAL SKILLS Hadoop HDFS YARN Map Reduce Pig Hive Sqoop Flume Hbase MongoDB Oozie Zookeeper Kafka Spark No SQL Database HBase MongoDB Cassandra Databases MS SQL Server Oracle 11g10g9iDB2 MySQL MS Access JEE Technologies J2EE JSP Servlets JUnit and JDBC Frameworks Struts Hibernate Spring IOC Spring AOP and Spring JDBC ETL Tools Pentaho Tableau Operating systems Ubuntu Windows iOS Unix languages C Java Python and Shell scripting",
    "entities": [
        "DAG",
        "Installed Oozie",
        "Pigscripts",
        "Business Delegate DAO",
        "Writing SQL",
        "Linux Additional Information",
        "NameNode",
        "Data Node",
        "US",
        "Sqoop",
        "sort group join",
        "QA",
        "HDFS Estimated Software Hardware",
        "Apache Hadoop",
        "Spring JDBC",
        "LINUX",
        "BI",
        "Impala",
        "HDFS",
        "Created",
        "ClouderaManager Environment Apache Hadoop HDFS HBase Hive",
        "SPRING MVC HTML CSS",
        "Technical Design Documents Developed DAOs Data Access Object using",
        "Hadoop MapReduce",
        "Hadoop Developer Hadoop",
        "Present Responsibilities Evaluated",
        "Project Employee Billing Management System EBMS Responsibilities Worked",
        "Sqoop Created",
        "Oracle",
        "Zookeeper Load",
        "Restful",
        "Java Script Education Hadoop",
        "Big Data Eco Systems",
        "JSF",
        "Pig and Sqoop Environment Hadoop HDFS Hive Pig Sqoop HBase",
        "Spark Ecosystems",
        "Cassandra Weekly",
        "Hive Pig Hadoop MapReduce Sqoop",
        "MVC",
        "Data Science Machine Learning Hadoop",
        "Kafka Maintained System",
        "JSP",
        "Cloudera Oozie Hadoop Developer Home Depot",
        "My Faces Components",
        "Realtime Processing using Spark Streaming",
        "Hadoop",
        "MapReduce",
        "Reviewing",
        "HBase Implemented",
        "Developed UI",
        "Apache Hadoop HDFS HBase Hive",
        "NoSQL",
        "Tableau",
        "SPRING MVC Involved",
        "JUnit",
        "JSP Servlets JUnit",
        "Shell",
        "Fair",
        "HBase",
        "Work Experience Hadoop Developer Liberty Mutual Dover NH",
        "Teradata",
        "JDBC Frameworks Struts Hibernate Spring IOC Spring AOP",
        "Hive",
        "Spark",
        "HealthcareRetail",
        "Zookeeper HBase Informatica"
    ],
    "experience": "Experience Hadoop Developer Liberty Mutual Dover NH August 2016 to Present Responsibilities Evaluated suitability of Hadoop and its ecosystem to the above project and implemented various proof of concept POC applications to eventually adopt them to benefit from the Big Data Hadoop initiative Estimated Software Hardware requirements for the Name Node and Data Node planning theYARN cluster Extracted the needed data from the server into HDFS and Bulk Loaded the cleaned data into HBase and Cassandra Written the Map Reduce programs HiveUDFs in Java where the functionality is too complex Involved in importing and exporting data intoHDFS and Hive using Sqoop Integrating bulk data into Cassandra file system using MapReduce programs Develop Hive queries for the analysis to categorize different items Designing and creating Hive external tables using shared metastore instead of derby with partitioning dynamic partitioning and buckets Reviewing peer table creation in Hive data loading and queries Used Flume to handle the real time log processing for attribution reports Developed Pig Latin scripts to extract the data from web server output files to load in HDFS Used Oozie workflow for Pig and Hive jobs Involved in working with Spark on top of YARN for interactive and batch analysis Good understanding on DAG cycle for entire spark application flow on Spark application WebUI Experienced in writing live Realtime Processing using Spark Streaming with Kafka Maintained System integrity of all subcomponents like Pig Hive Spark Kafka HBase and Cassandra Weekly meetings with technical collaborators and active participation in code review sessions with senior and junior developers Involved in launching managing and monitoring the Hadoop cluster using ClouderaManager Environment Apache Hadoop HDFS HBase Hive Pig MapReduce Java Ambari Cassandra Sqoop Flume Cloudera Kafka Scala Spark Oozie MySQL Linux Hadoop Developer September 2015 to July 2016 Target MN Responsibilities Evaluated business requirements and prepared detailed specifications that follow project guidelines required to develop suitable programs Worked on analyzing HadoopCluster and different big data analytic tools including Pig Hive and MongoDB Involved in importing data from Oracle into HDFS and Hive using Sqoop Created tables using Impala and involved in creating Queries which are stored in HBase Implemented complex scripts to support test driven development and continuous integration Installed and configured Hadoop MapReduce and HDFS and developed multiple MapReduce jobs in Java for data Involved in data modeling the large data from various sources using NoSQL databases HBase MongoDB Created Pig Latin scripts to sort group join and filter the enterprise wide to get transformed data sets Involved in creating Hive tables loading data and creating Hive queries that will run internally in MapReduce way Used Hive to analyze the partitioned and bucketed data and compute various metrics for reporting Experience in HiveHBase integration by defining external tables in hive and pointing the HBase as data store for better performance and lower IO Exported the analyzed data to relational databases using Sqoop for visualization and to generate reports for the BI team Hands on experience in Tableau which is used for visualization and reports generation Involved in maintaining and debugging MR programs Mentoring analyst and test team for writing Hive queries Installed Oozie workflow engine to run multiple MapReduce jobs Worked with application teams to install operating systems Hadoop updates patches version upgrades as required Good experience in monitoring and managing the Hadoop cluster using Cloudera Manager Environment Hadoop HDFS MapReduce Hive Eclipse Pig Sqoop Linux Flume Java Shell Scripting Tableau Python Zookeeper HBase Informatica MongoDB Cloudera Oozie Hadoop Developer Home Depot Atlanta GA June 2013 to August 2015 Responsibilities Responsible for building scalable distributed data solutions using Hadoop Implemented 20 nodesCDH4 Hadoop cluster on LINUX Involved in loading data from LINUX file system to HDFS Estimated Software Hardware requirements for the NameNode and DataNode planning the cluster Implemented a script to transmit data from Teradata to HBase using Sqoop and created tables out of that data on HBase Implemented best income logic using Pigscripts and UDFs Implemented test scripts to support test driven development and continuous integration Worked on tuning the performance Pig queries Cluster Coordination service through Zookeeper Load and transform large sets of structured semi structured and unstructured data Responsible for cluster maintenance by Cloudera adding and removing cluster nodes cluster monitoring and troubleshooting manage and review data backups manage and review Hadoop log files Installing Oozie workflow engine to run multiple Hive and Pig jobs Job management using Fair scheduler Exported the analyzed data to the relational databases using Sqoop for visualization and to generate reports for the BI team Analyzed large amounts of data sets to determine optimal way to aggregate and report on it Supported in setting up QA environment and updating configurations for implementing scripts with Pig and Sqoop Environment Hadoop HDFS Hive Pig Sqoop HBase Shell Scripting Ubuntu Linux Zookeeper Teradata Oozie Cloudera Java Developer ChevronCorporation Irvine CA November 2010 to May 2012 Responsibilities Worked with business analyst in understanding business requirements design and development of the project Implemented the JSP frame work with MVC architecture Created new JSPs for the front end using HTML Java Script Jquery and Ajax Developed the presentation layer using JSP HTML CSS and client side validations using JavaScript Involved in creating Restful web services using JAX_RS and JERSEY tool Involved in designing creating reviewing Technical Design Documents Developed DAOs Data Access Object using Hibernate as ORM to interact with DBMS Oracle Applied J2EE design patterns like Business Delegate DAO and Singleton Involved in developing DAOs using JDBC Worked with QA team in preparation and review of test cases JUnit was used for unit testing for the integration testing tool Writing SQL queries to fetch the business data using Oracle as database Developed UI for Customer Service Modules and Reports using JSF JSPs and My Faces Components Log4j used for logging the application log of the running system to trace the errors and certain automated routine functions Environment Java JSP JavaScript Servlets Hibernate REST EJB JSF JSP Ant Tomcat Eclipse SQL Oracle Jr Java Developer July 2008 to October 2010 Project Employee Billing Management System EBMS Responsibilities Worked and maintained the front end functionality of the Website from scratch Using HTML CSS and Java Script Developed server side functionality of the website using JAVA and SPRING MVC Involved in discussions with clients to gather requirements to fulfill the objectives Interacted with several teams to implement the requirements I was responsible for preparing the technical documentation Environment JAVA and SPRING MVC HTML CSS and Java Script Education Hadoop Architecture Secondary NameNode Bachelors Skills JAVA 6 years APACHE HADOOP HDFS 4 years APACHE HADOOP OOZIE 4 years APACHE HADOOP SQOOP 4 years APACHE HBASE 4 years Apache Hadoop HDFS HBase Hive Pig MapReduce Java Ambari Cassandra Sqoop Flume Cloudera Kafka Scala Spark Oozie MySQL Linux Additional Information TECHNICAL SKILLS Hadoop HDFS YARN Map Reduce Pig Hive Sqoop Flume Hbase MongoDB Oozie Zookeeper Kafka Spark No SQL Database HBase MongoDB Cassandra Databases MS SQL Server Oracle 11g10g9iDB2 MySQL MS Access JEE Technologies J2EE JSP Servlets JUnit and JDBC Frameworks Struts Hibernate Spring IOC Spring AOP and Spring JDBC ETL Tools Pentaho Tableau Operating systems Ubuntu Windows iOS Unix languages C Java Python and Shell scripting",
    "extracted_keywords": [
        "Hadoop",
        "Developer",
        "Hadoop",
        "span",
        "lDeveloperspan",
        "Hadoop",
        "Developer",
        "Liberty",
        "Mutual",
        "Dover",
        "NH",
        "9years",
        "experience",
        "exposure",
        "Big",
        "DataTechnologies",
        "implementation",
        "trending",
        "technologies",
        "Big",
        "Data",
        "Eco",
        "Systems",
        "verticals",
        "HealthcareRetail",
        "Private",
        "Sectors",
        "Years",
        "exposure",
        "Data",
        "Science",
        "Machine",
        "Learning",
        "Hadoop",
        "Spark",
        "Ecosystems",
        "solutions",
        "organizations",
        "Hands",
        "experience",
        "Apache",
        "Hadoop",
        "ecosystem",
        "components",
        "Hive",
        "Pig",
        "Hadoop",
        "MapReduce",
        "Sqoop",
        "Flume",
        "Kafka",
        "Storm",
        "Spark",
        "Oozie",
        "Zookeeper",
        "YARN",
        "Impala",
        "HBase",
        "Hadoop",
        "ecosystem",
        "components",
        "US",
        "employer",
        "Work",
        "Experience",
        "Hadoop",
        "Developer",
        "Liberty",
        "Mutual",
        "Dover",
        "NH",
        "August",
        "Present",
        "Responsibilities",
        "suitability",
        "Hadoop",
        "ecosystem",
        "project",
        "proof",
        "concept",
        "POC",
        "applications",
        "Big",
        "Data",
        "Hadoop",
        "initiative",
        "Estimated",
        "Software",
        "Hardware",
        "requirements",
        "Name",
        "Node",
        "Data",
        "Node",
        "planning",
        "theYARN",
        "cluster",
        "data",
        "server",
        "HDFS",
        "Bulk",
        "Loaded",
        "data",
        "HBase",
        "Cassandra",
        "Written",
        "Map",
        "Reduce",
        "programs",
        "HiveUDFs",
        "Java",
        "functionality",
        "data",
        "intoHDFS",
        "Hive",
        "Sqoop",
        "Integrating",
        "data",
        "Cassandra",
        "file",
        "system",
        "MapReduce",
        "programs",
        "Develop",
        "Hive",
        "analysis",
        "items",
        "Designing",
        "Hive",
        "tables",
        "metastore",
        "derby",
        "partitioning",
        "buckets",
        "peer",
        "table",
        "creation",
        "Hive",
        "data",
        "loading",
        "Flume",
        "time",
        "log",
        "processing",
        "attribution",
        "reports",
        "Developed",
        "Pig",
        "Latin",
        "scripts",
        "data",
        "web",
        "server",
        "output",
        "files",
        "HDFS",
        "Oozie",
        "workflow",
        "Pig",
        "Hive",
        "jobs",
        "Spark",
        "top",
        "YARN",
        "analysis",
        "understanding",
        "DAG",
        "cycle",
        "spark",
        "application",
        "flow",
        "Spark",
        "application",
        "WebUI",
        "Realtime",
        "Processing",
        "Spark",
        "Streaming",
        "Kafka",
        "Maintained",
        "System",
        "integrity",
        "subcomponents",
        "Pig",
        "Hive",
        "Spark",
        "Kafka",
        "HBase",
        "Cassandra",
        "Weekly",
        "meetings",
        "collaborators",
        "participation",
        "code",
        "review",
        "sessions",
        "developers",
        "Hadoop",
        "cluster",
        "ClouderaManager",
        "Environment",
        "Apache",
        "Hadoop",
        "HDFS",
        "HBase",
        "Hive",
        "Pig",
        "MapReduce",
        "Java",
        "Ambari",
        "Cassandra",
        "Sqoop",
        "Flume",
        "Cloudera",
        "Kafka",
        "Scala",
        "Spark",
        "Oozie",
        "MySQL",
        "Linux",
        "Hadoop",
        "Developer",
        "September",
        "July",
        "Target",
        "MN",
        "Responsibilities",
        "business",
        "requirements",
        "specifications",
        "project",
        "guidelines",
        "programs",
        "HadoopCluster",
        "data",
        "tools",
        "Pig",
        "Hive",
        "MongoDB",
        "data",
        "Oracle",
        "HDFS",
        "Hive",
        "Sqoop",
        "Created",
        "tables",
        "Impala",
        "Queries",
        "HBase",
        "scripts",
        "test",
        "development",
        "integration",
        "Hadoop",
        "MapReduce",
        "HDFS",
        "MapReduce",
        "jobs",
        "Java",
        "data",
        "data",
        "data",
        "sources",
        "NoSQL",
        "HBase",
        "MongoDB",
        "Created",
        "Pig",
        "Latin",
        "scripts",
        "group",
        "join",
        "enterprise",
        "data",
        "sets",
        "Hive",
        "tables",
        "loading",
        "data",
        "Hive",
        "queries",
        "MapReduce",
        "way",
        "Hive",
        "data",
        "metrics",
        "Experience",
        "HiveHBase",
        "integration",
        "tables",
        "hive",
        "HBase",
        "data",
        "store",
        "performance",
        "IO",
        "data",
        "databases",
        "Sqoop",
        "visualization",
        "reports",
        "BI",
        "team",
        "experience",
        "Tableau",
        "visualization",
        "reports",
        "generation",
        "MR",
        "programs",
        "Mentoring",
        "analyst",
        "test",
        "team",
        "Hive",
        "queries",
        "Installed",
        "Oozie",
        "workflow",
        "engine",
        "MapReduce",
        "jobs",
        "application",
        "teams",
        "operating",
        "systems",
        "Hadoop",
        "updates",
        "version",
        "upgrades",
        "experience",
        "monitoring",
        "Hadoop",
        "cluster",
        "Cloudera",
        "Manager",
        "Environment",
        "Hadoop",
        "HDFS",
        "MapReduce",
        "Hive",
        "Eclipse",
        "Pig",
        "Sqoop",
        "Linux",
        "Flume",
        "Java",
        "Shell",
        "Scripting",
        "Tableau",
        "Python",
        "Zookeeper",
        "HBase",
        "Informatica",
        "MongoDB",
        "Cloudera",
        "Oozie",
        "Hadoop",
        "Developer",
        "Home",
        "Depot",
        "Atlanta",
        "GA",
        "June",
        "August",
        "Responsibilities",
        "data",
        "solutions",
        "Hadoop",
        "nodesCDH4",
        "Hadoop",
        "cluster",
        "LINUX",
        "loading",
        "data",
        "LINUX",
        "file",
        "system",
        "HDFS",
        "Estimated",
        "Software",
        "Hardware",
        "requirements",
        "NameNode",
        "DataNode",
        "cluster",
        "script",
        "data",
        "Teradata",
        "HBase",
        "Sqoop",
        "tables",
        "data",
        "HBase",
        "income",
        "logic",
        "Pigscripts",
        "UDFs",
        "test",
        "scripts",
        "test",
        "development",
        "integration",
        "performance",
        "Pig",
        "Cluster",
        "Coordination",
        "service",
        "Zookeeper",
        "Load",
        "sets",
        "data",
        "cluster",
        "maintenance",
        "Cloudera",
        "cluster",
        "nodes",
        "cluster",
        "monitoring",
        "troubleshooting",
        "manage",
        "data",
        "backups",
        "Hadoop",
        "log",
        "files",
        "Oozie",
        "workflow",
        "engine",
        "Hive",
        "Pig",
        "jobs",
        "Job",
        "management",
        "Fair",
        "scheduler",
        "data",
        "databases",
        "Sqoop",
        "visualization",
        "reports",
        "BI",
        "team",
        "amounts",
        "data",
        "sets",
        "way",
        "QA",
        "environment",
        "configurations",
        "scripts",
        "Pig",
        "Sqoop",
        "Environment",
        "Hadoop",
        "HDFS",
        "Hive",
        "Pig",
        "Sqoop",
        "HBase",
        "Shell",
        "Scripting",
        "Ubuntu",
        "Linux",
        "Zookeeper",
        "Teradata",
        "Oozie",
        "Cloudera",
        "Java",
        "Developer",
        "ChevronCorporation",
        "Irvine",
        "CA",
        "November",
        "May",
        "Responsibilities",
        "business",
        "analyst",
        "business",
        "requirements",
        "design",
        "development",
        "project",
        "JSP",
        "frame",
        "work",
        "MVC",
        "architecture",
        "JSPs",
        "end",
        "HTML",
        "Java",
        "Script",
        "Jquery",
        "Ajax",
        "presentation",
        "layer",
        "JSP",
        "HTML",
        "CSS",
        "client",
        "side",
        "validations",
        "JavaScript",
        "web",
        "services",
        "JAX_RS",
        "JERSEY",
        "tool",
        "Technical",
        "Design",
        "Documents",
        "DAOs",
        "Data",
        "Access",
        "Object",
        "Hibernate",
        "ORM",
        "DBMS",
        "Oracle",
        "Applied",
        "J2EE",
        "design",
        "patterns",
        "Business",
        "Delegate",
        "DAO",
        "Singleton",
        "DAOs",
        "JDBC",
        "QA",
        "team",
        "preparation",
        "review",
        "test",
        "cases",
        "JUnit",
        "unit",
        "testing",
        "integration",
        "testing",
        "tool",
        "SQL",
        "business",
        "data",
        "Oracle",
        "database",
        "UI",
        "Customer",
        "Service",
        "Modules",
        "Reports",
        "JSF",
        "JSPs",
        "Faces",
        "Components",
        "Log4j",
        "application",
        "log",
        "running",
        "system",
        "errors",
        "functions",
        "Environment",
        "Java",
        "JSP",
        "JavaScript",
        "Servlets",
        "Hibernate",
        "REST",
        "EJB",
        "JSF",
        "JSP",
        "Ant",
        "Tomcat",
        "Eclipse",
        "SQL",
        "Oracle",
        "Jr",
        "Java",
        "Developer",
        "July",
        "October",
        "Project",
        "Employee",
        "Billing",
        "Management",
        "System",
        "EBMS",
        "Responsibilities",
        "end",
        "functionality",
        "Website",
        "scratch",
        "HTML",
        "CSS",
        "Java",
        "Script",
        "server",
        "side",
        "functionality",
        "website",
        "JAVA",
        "SPRING",
        "MVC",
        "discussions",
        "clients",
        "requirements",
        "objectives",
        "teams",
        "requirements",
        "documentation",
        "Environment",
        "JAVA",
        "SPRING",
        "MVC",
        "HTML",
        "CSS",
        "Java",
        "Script",
        "Education",
        "Hadoop",
        "Architecture",
        "Secondary",
        "NameNode",
        "Bachelors",
        "Skills",
        "JAVA",
        "years",
        "APACHE",
        "HADOOP",
        "HDFS",
        "years",
        "APACHE",
        "HADOOP",
        "OOZIE",
        "years",
        "APACHE",
        "HADOOP",
        "years",
        "APACHE",
        "HBASE",
        "years",
        "Apache",
        "Hadoop",
        "HDFS",
        "HBase",
        "Hive",
        "Pig",
        "MapReduce",
        "Java",
        "Ambari",
        "Cassandra",
        "Sqoop",
        "Flume",
        "Cloudera",
        "Kafka",
        "Scala",
        "Spark",
        "Oozie",
        "MySQL",
        "Linux",
        "Additional",
        "Information",
        "TECHNICAL",
        "SKILLS",
        "Hadoop",
        "HDFS",
        "YARN",
        "Map",
        "Reduce",
        "Pig",
        "Hive",
        "Sqoop",
        "Flume",
        "Hbase",
        "MongoDB",
        "Oozie",
        "Zookeeper",
        "Kafka",
        "Spark",
        "SQL",
        "Database",
        "HBase",
        "MongoDB",
        "Cassandra",
        "Databases",
        "MS",
        "SQL",
        "Server",
        "Oracle",
        "11g10g9iDB2",
        "MySQL",
        "MS",
        "Access",
        "JEE",
        "Technologies",
        "J2EE",
        "JSP",
        "Servlets",
        "JUnit",
        "JDBC",
        "Frameworks",
        "Struts",
        "Hibernate",
        "Spring",
        "IOC",
        "Spring",
        "AOP",
        "Spring",
        "JDBC",
        "ETL",
        "Tools",
        "Pentaho",
        "Tableau",
        "Operating",
        "systems",
        "Ubuntu",
        "Windows",
        "iOS",
        "Unix",
        "C",
        "Java",
        "Python",
        "Shell",
        "scripting"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T23:11:08.761668",
    "resume_data": "Hadoop Developer Hadoop span lDeveloperspan Hadoop Developer Liberty Mutual Dover NH About 9years of experience with heavy exposure to Big DataTechnologies and implementation of various trending technologies in Big Data Eco Systems under various verticals such as HealthcareRetail and Private Sectors 4 Years of exposure to Data Science Machine Learning Hadoop and Spark Ecosystems implementing innovative solutions for various organizations Hands on experience in Apache Hadoop ecosystem components like Hive Pig Hadoop MapReduce Sqoop Flume Kafka Storm Spark Oozie Zookeeper YARN Impala and HBase along other Hadoop ecosystem components Authorized to work in the US for any employer Work Experience Hadoop Developer Liberty Mutual Dover NH August 2016 to Present Responsibilities Evaluated suitability of Hadoop and its ecosystem to the above project and implemented various proof of concept POC applications to eventually adopt them to benefit from the Big Data Hadoop initiative Estimated Software Hardware requirements for the Name Node and Data Node planning theYARN cluster Extracted the needed data from the server into HDFS and Bulk Loaded the cleaned data into HBase and Cassandra Written the Map Reduce programs HiveUDFs in Java where the functionality is too complex Involved in importing and exporting data intoHDFS and Hive using Sqoop Integrating bulk data into Cassandra file system using MapReduce programs Develop Hive queries for the analysis to categorize different items Designing and creating Hive external tables using shared metastore instead of derby with partitioning dynamic partitioning and buckets Reviewing peer table creation in Hive data loading and queries Used Flume to handle the real time log processing for attribution reports Developed Pig Latin scripts to extract the data from web server output files to load in HDFS Used Oozie workflow for Pig and Hive jobs Involved in working with Spark on top of YARN for interactive and batch analysis Good understanding on DAG cycle for entire spark application flow on Spark application WebUI Experienced in writing live Realtime Processing using Spark Streaming with Kafka Maintained System integrity of all subcomponents like Pig Hive Spark Kafka HBase and Cassandra Weekly meetings with technical collaborators and active participation in code review sessions with senior and junior developers Involved in launching managing and monitoring the Hadoop cluster using ClouderaManager Environment Apache Hadoop HDFS HBase Hive Pig MapReduce Java Ambari Cassandra Sqoop Flume Cloudera Kafka Scala Spark Oozie MySQL Linux Hadoop Developer September 2015 to July 2016 Target MN Responsibilities Evaluated business requirements and prepared detailed specifications that follow project guidelines required to develop suitable programs Worked on analyzing HadoopCluster and different big data analytic tools including Pig Hive and MongoDB Involved in importing data from Oracle into HDFS and Hive using Sqoop Created tables using Impala and involved in creating Queries which are stored in HBase Implemented complex scripts to support test driven development and continuous integration Installed and configured Hadoop MapReduce and HDFS and developed multiple MapReduce jobs in Java for data Involved in data modeling the large data from various sources using NoSQL databases HBase MongoDB Created Pig Latin scripts to sort group join and filter the enterprise wide to get transformed data sets Involved in creating Hive tables loading data and creating Hive queries that will run internally in MapReduce way Used Hive to analyze the partitioned and bucketed data and compute various metrics for reporting Experience in HiveHBase integration by defining external tables in hive and pointing the HBase as data store for better performance and lower IO Exported the analyzed data to relational databases using Sqoop for visualization and to generate reports for the BI team Hands on experience in Tableau which is used for visualization and reports generation Involved in maintaining and debugging MR programs Mentoring analyst and test team for writing Hive queries Installed Oozie workflow engine to run multiple MapReduce jobs Worked with application teams to install operating systems Hadoop updates patches version upgrades as required Good experience in monitoring and managing the Hadoop cluster using Cloudera Manager Environment Hadoop HDFS MapReduce Hive Eclipse Pig Sqoop Linux Flume Java Shell Scripting Tableau Python Zookeeper HBase Informatica MongoDB Cloudera Oozie Hadoop Developer Home Depot Atlanta GA June 2013 to August 2015 Responsibilities Responsible for building scalable distributed data solutions using Hadoop Implemented 20 nodesCDH4 Hadoop cluster on LINUX Involved in loading data from LINUX file system to HDFS Estimated Software Hardware requirements for the NameNode and DataNode planning the cluster Implemented a script to transmit data from Teradata to HBase using Sqoop and created tables out of that data on HBase Implemented best income logic using Pigscripts and UDFs Implemented test scripts to support test driven development and continuous integration Worked on tuning the performance Pig queries Cluster Coordination service through Zookeeper Load and transform large sets of structured semi structured and unstructured data Responsible for cluster maintenance by Cloudera adding and removing cluster nodes cluster monitoring and troubleshooting manage and review data backups manage and review Hadoop log files Installing Oozie workflow engine to run multiple Hive and Pig jobs Job management using Fair scheduler Exported the analyzed data to the relational databases using Sqoop for visualization and to generate reports for the BI team Analyzed large amounts of data sets to determine optimal way to aggregate and report on it Supported in setting up QA environment and updating configurations for implementing scripts with Pig and Sqoop Environment Hadoop HDFS Hive Pig Sqoop HBase Shell Scripting Ubuntu Linux Zookeeper Teradata Oozie Cloudera Java Developer ChevronCorporation Irvine CA November 2010 to May 2012 Responsibilities Worked with business analyst in understanding business requirements design and development of the project Implemented the JSP frame work with MVC architecture Created new JSPs for the front end using HTML Java Script Jquery and Ajax Developed the presentation layer using JSP HTML CSS and client side validations using JavaScript Involved in creating Restful web services using JAX_RS and JERSEY tool Involved in designing creating reviewing Technical Design Documents Developed DAOs Data Access Object using Hibernate as ORM to interact with DBMS Oracle Applied J2EE design patterns like Business Delegate DAO and Singleton Involved in developing DAOs using JDBC Worked with QA team in preparation and review of test cases JUnit was used for unit testing for the integration testing tool Writing SQL queries to fetch the business data using Oracle as database Developed UI for Customer Service Modules and Reports using JSF JSPs and My Faces Components Log4j used for logging the application log of the running system to trace the errors and certain automated routine functions Environment Java JSP JavaScript Servlets Hibernate REST EJB JSF JSP Ant Tomcat Eclipse SQL Oracle Jr Java Developer July 2008 to October 2010 Project Employee Billing Management System EBMS Responsibilities Worked and maintained the front end functionality of the Website from scratch Using HTML CSS and Java Script Developed server side functionality of the website using JAVA and SPRING MVC Involved in discussions with clients to gather requirements to fulfill the objectives Interacted with several teams to implement the requirements I was responsible for preparing the technical documentation Environment JAVA and SPRING MVC HTML CSS and Java Script Education Hadoop Architecture Secondary NameNode Bachelors Skills JAVA 6 years APACHE HADOOP HDFS 4 years APACHE HADOOP OOZIE 4 years APACHE HADOOP SQOOP 4 years APACHE HBASE 4 years Apache Hadoop HDFS HBase Hive Pig MapReduce Java Ambari Cassandra Sqoop Flume Cloudera Kafka Scala Spark Oozie MySQL Linux Additional Information TECHNICAL SKILLS Hadoop HDFS YARN Map Reduce Pig Hive Sqoop Flume Hbase MongoDB Oozie Zookeeper Kafka Spark No SQL Database HBase MongoDB Cassandra Databases MS SQL Server Oracle 11g10g9iDB2 MySQL MS Access JEE Technologies J2EE JSP Servlets JUnit and JDBC Frameworks Struts Hibernate Spring IOC Spring AOP and Spring JDBC ETL Tools Pentaho Tableau Operating systems Ubuntu Windows iOS Unix languages C Java Python and Shell scripting",
    "unique_id": "06353a79-0ad2-4b8c-b906-7cad358a0108"
}