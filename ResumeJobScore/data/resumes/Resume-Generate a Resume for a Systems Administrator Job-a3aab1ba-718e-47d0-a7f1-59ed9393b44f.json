{
    "clean_data": "Hadoop Developer Hadoop span lDeveloperspan Hadoop Developer CompuVision Consultin New York NY Around 8 years of Information Technology experience with 5 years of experience in Hadoop Ecosystem Worked with Big Data distributions like Cloudera CDH 3 and 4 with Cloudera Manager Strong experience in using Hive for processing and analyzing large volume of data Expert in using Sqoop for fetching data from different systems to analyze in HDFS and putting it back to the previous system for further processing Expert in creating PIG and HiveUDFs using Java in order to analyze the data efficiently Strong experience in using Spark for end to end analytics Strong knowledge of Software Development Life Cycle SDLC Experienced in creating and analyzing Software Requirement Specifications SRS and Functional Specification Document FSD Worked in Windows UNIXLINUX platform with different Technologies such as Big Data SQL PLSQL XML HTML Core Java Shell Scripting Experience of creating Map Reduce codes in Java as per the business requirements Extensive knowledge in creating PLSQL stored Procedures packages functions cursors etc against Oracle 9i 10g 11g and MySQL server Having strong technical skills in Core Java with working knowledge Worked in ETL tools like Talend to simplify Map Reduce jobs from the front end Also have knowledge of Informatica IBM InfoSphere as another working ETL tool with Big Data Worked with BI tools like Tableau for report creation and further analysis from the front end Extensive knowledge in using SQL queries for backend database analysis Involved in developing distributed Enterprise and Web applications using UML JavaJ2EE Web technologies that include EJB JSP Servlets JMS JDBC JPA HTML XML Tomcat spring and Hibernate Expertise in Defect Management and Defect Tracking to do performance tuning for delivering utmost Quality product Authorized to work in the US for any employer Work Experience Hadoop Developer CompuVision Consultin New York NY December 2015 to Present Around 8 years of Information Technology experience with 5 years of experience in Hadoop Ecosystem Worked with Big Data distributions like Cloudera CDH 3 and 4 with Cloudera Manager Strong experience in using Hive for processing and analyzing large volume of data Expert in using Sqoop for fetching data from different systems to analyze in HDFS and putting it back to the previous system for further processing Expert in creating PIG and HiveUDFs using Java in order to analyze the data efficiently Strong experience in using Spark for end to end analytics Strong knowledge of Software Development Life Cycle SDLC Experienced in creating and analyzing Software Requirement Specifications SRS and Functional Specification Document FSD Worked in Windows UNIXLINUX platform with different Technologies such as Big Data SQL PLSQL XML HTML Core Java Shell Scripting Experience of creating Map Reduce codes in Java as per the business requirements Extensive knowledge in creating PLSQL stored Procedures packages functions cursors etc against Oracle 9i 10g 11g and MySQL server Having strong technical skills in Core Java with working knowledge Worked in ETL tools like Talend to simplify Map Reduce jobs from the front end Also have knowledge of Informatica IBM InfoSphere as another working ETL tool with Big Data Worked with BI tools like Tableau for report creation and further analysis from the front end Extensive knowledge in using SQL queries for backend database analysis Involved in developing distributed Enterprise and Web applications using UML JavaJ2EE Web technologies that include EJB JSP Servlets JMS JDBC JPA HTML XML Tomcat spring and Hibernate Expertise in Defect Management and Defect Tracking to do performance tuning for delivering utmost Quality product Hadoop Developer TMG Health Jessup PA November 2012 to December 2015 TMG Health is an American medical technology company that manufactures and sells medical devices instrument systems and reagents Founded in 1897 and headquartered in Franklin Lakes TMG Health employs nearly 30000 people in more than 50 countries throughout the world Responsibilities Worked with systems engineering team to plan and deploy new Hadoop environments and expand existing Hadoop clusters with agile methodology Monitored multiple Hadoop clusters environments using Ganglia monitored workload job performance and capacity planning using Cloudera Manager Experienced with through handson experience in all Hadoop Java SQL and Python Used Flume to collect aggregate and store the web log data from different sources like web servers mobile and network devices and pushed to HDFS Participated in functional reviews test specifications and documentation review Performed MapReduce programs on log data to transform into structured way to find user location age group spending time Analyzed the web log data using the HiveQL to extract number of unique visitors per day page views visit duration most purchased product on website Exported the analyzed data to the relational databases using Sqoop for visualization and to generate reports by Business Intelligence tools Proactively monitored systems and services architecture design and implementation of Hadoop deployment configuration management backup and disaster recovery systems and procedures Documented the systems processes and procedures for future references responsible to manage data coming from different sources EnvironmentHadoop HDFS Map Reduce Flume Pig Sqoop Hive Pig Sqoop Oozie Ganglia HBase Shell Scripting Java Developer FedEx Memphis TN October 2009 to November 2012 FedEx is a leading national provider of expert solutions for Medicare advantage Medicare part D and managed Medicaid plans With more than 17 years of experience of providing technologyenabled services to the government market exclusively our knowledge of health plan processes CMS requirements and the daily challenges plan face within the government market is second to none Responsibilities Created the database user environment activity and class diagram for the project UML Implemented the database using oracle database engine Created an entity object business rules and policy validation logic default value logic security Web application development using J2EE JSP Servlets JDBC JavaBeans Struts Ajax Custom Tags EJB Hibernate Ant Junitand ApacheLog4j Web Services Message queueMQ Designing GUI prototype using ADF 11G GUI component before finalizing it for development Experience in using version controls such as CVS PVCS Involved in consuming producing Restful web services using JAXRS Collaborated with ETLInformatica team to determine the necessary data modules and UI designs to support Cognos reports Junit was used for unit testing for the integration testing tool Created modules using task flow with bounded and unbounded Generating WSDL web services and create work flow using BPEL Created the skin for the layout Made integrated testing for the application Created dynamic report and using JFreechart Environment Java Servlets JSF Adf rich client UI framework ADFBC BC4J 11g Web Services using Oracle SOA Oracle WebLogic Software Engineer Arteria Technologies Pvt Ltd Hyderabad Telangana September 2008 to October 2009 Responsibilities Developed the user interface screens using swing for accepting various system inputs such as contractual terms monthly data pertaining to production inventory and transportation Involved in designing database connections using JDBC Involved in design and development of UI using HTML JavaScript and CSS Involved in creating tables stored procedures in Sqlfor data manipulation and retrieval using sqlsever2000 database modification using Sql PlSql triggers views in oracle Used dispatch action to group related actions into a single class Build the applications using Ant tool also used eclipse as the IDE Developed the business components used for the calculation module Involved in the logical and physical database design and implemented it by creating suitable tables views and triggers Applied J2EE design patterns like business delegate DAO and singleton Created the related procedures and functions used by JDBC calls in the above requirements Actively involved in testing debugging and deployment of the application on WebLogic application server Developed test cases and performed unit testing using JUnit Involved in fixing bugs and minor enhancements for the frontend modules Environment Java HTML Java script CSS Oracle JDBC ANT tool SQL Swing and Eclipse Hadoop Developer Becton Dickinson Memphis TN 1973 to 2000 is an American multinational courier delivery Services Company headquartered in Memphis Tennessee The name Becton Dickinson is a syllabic abbreviation of the name of the companys original air division Becton Dickinson which was used from 1973 until 2000 The company is known for its overnight shipping service but also for pioneering a system that could track packages and provide realtime updates on package location to help in finding lost packages a feature that has now been implemented by most other carrier services Responsibilities Extracted and updated the data into HDFS using Sqoop import and export command line utility interface Responsible for developing data pipeline using Flume Sqoop and Pig to extract the data from weblogs and store in HDFS Implemented advanced procedures like text analytics and processing using the inmemory computing capabilities like spark Enhanced and optimized product Spark code to aggregate group and run data mining tasks using the Spark framework Extending Hive and Pig core functionality by writing custom UDFs Developed data pipeline using Flume Sqoop pig and java MapReduce to ingest customer behavioral data and financial histories into HDFS for analysis Loaded cache data into HBase using Sqoop Experience in custom talend jobs to ingest enrich and distribute data in MapR Cloudera Hadoop ecosystem Created lots of external tables on Hive pointed to HBase tables Analyzed HBase data in Hive by creating external partitioned and bucketed tables Worked with cache data stored in Cassandra Supported MapReduce Programs those are running on the cluster Environment Hadoop MapReduce Hdfs Pig Hive HBase Impala Sqoop Flume Oozie Apache Spark Java Linux SQL Server Zookeeper Autosys Tableau Cassandra Education Bachelors Skills JAVA 10 years Hadoop 10 years HADOOP 10 years SQL 10 years APACHE HADOOP HDFS 10 years Additional Information SKILLS database 6 years java 10 years JDBC 6 years oracle 6 years SQL 7 years Technical Skills HadoopBig Data Technologies HDFS Map Reduce Sqoop Pig Hive Oozie impala Spark Zookeeper and Cloudera Manager NO SQL Database HBase Monitoring and Reporting Tableau Custom shell scripts Hadoop Distribution Horton Works Cloudera MapR Build Tools Maven SQL Developer Programming Scripting JAVA C SQL Shell Scripting Java Technologies Servlets JavaBeans JDBC Spring Hibernate Databases Oracle MY SQL MS SQL server Teradata Web Dev Technologies HTML XML JSON CSS Version Control SVN CVS GIT Operating Systems Linux Unix Mac OSX Windows 8 Windows 7 Windows Server",
    "entities": [
        "UML Implemented",
        "Memphis",
        "Analyzed HBase",
        "MapR",
        "FedEx",
        "ETL",
        "Developed",
        "Medicare",
        "Functional Specification Document FSD Worked",
        "US",
        "Sqoop",
        "New York",
        "Work Experience Hadoop Developer CompuVision Consultin New York",
        "Technical Skills",
        "Build",
        "JUnit Involved",
        "Maven",
        "BI",
        "Information Technology",
        "Created",
        "IDE Developed",
        "Impala",
        "CSS Version",
        "Hadoop Developer Hadoop",
        "Restful",
        "UI",
        "Software Development Life Cycle SDLC Experienced",
        "BPEL Created",
        "Tennessee",
        "Oracle SOA Oracle WebLogic Software",
        "PIG",
        "Defect Management and Defect Tracking",
        "Medicaid",
        "CVS PVCS Involved",
        "Oracle 9i",
        "Hive",
        "Hibernate Expertise",
        "Hadoop Developer TMG Health Jessup PA",
        "UML JavaJ2EE",
        "Business Intelligence",
        "CMS",
        "the cluster Environment Hadoop MapReduce Hdfs Pig Hive HBase",
        "Informatica IBM InfoSphere",
        "Ganglia",
        "SQL",
        "Linux Unix Mac",
        "Hadoop",
        "CSS Involved",
        "Cassandra Supported MapReduce Programs",
        "Franklin Lakes TMG Health",
        "JAXRS Collaborated",
        "Sql PlSql",
        "Software Requirement Specifications SRS",
        "Talend",
        "Tableau",
        "WebLogic",
        "Flume Sqoop",
        "Java Technologies",
        "Responsibilities Created",
        "Control SVN CVS GIT",
        "HBase",
        "Monitored multiple Hadoop",
        "Big Data",
        "Big Data Worked",
        "EJB JSP Servlets",
        "SQL MS",
        "Spark"
    ],
    "experience": "Experience of creating Map Reduce codes in Java as per the business requirements Extensive knowledge in creating PLSQL stored Procedures packages functions cursors etc against Oracle 9i 10 g 11 g and MySQL server Having strong technical skills in Core Java with working knowledge Worked in ETL tools like Talend to simplify Map Reduce jobs from the front end Also have knowledge of Informatica IBM InfoSphere as another working ETL tool with Big Data Worked with BI tools like Tableau for report creation and further analysis from the front end Extensive knowledge in using SQL queries for backend database analysis Involved in developing distributed Enterprise and Web applications using UML JavaJ2EE Web technologies that include EJB JSP Servlets JMS JDBC JPA HTML XML Tomcat spring and Hibernate Expertise in Defect Management and Defect Tracking to do performance tuning for delivering utmost Quality product Authorized to work in the US for any employer Work Experience Hadoop Developer CompuVision Consultin New York NY December 2015 to Present Around 8 years of Information Technology experience with 5 years of experience in Hadoop Ecosystem Worked with Big Data distributions like Cloudera CDH 3 and 4 with Cloudera Manager Strong experience in using Hive for processing and analyzing large volume of data Expert in using Sqoop for fetching data from different systems to analyze in HDFS and putting it back to the previous system for further processing Expert in creating PIG and HiveUDFs using Java in order to analyze the data efficiently Strong experience in using Spark for end to end analytics Strong knowledge of Software Development Life Cycle SDLC Experienced in creating and analyzing Software Requirement Specifications SRS and Functional Specification Document FSD Worked in Windows UNIXLINUX platform with different Technologies such as Big Data SQL PLSQL XML HTML Core Java Shell Scripting Experience of creating Map Reduce codes in Java as per the business requirements Extensive knowledge in creating PLSQL stored Procedures packages functions cursors etc against Oracle 9i 10 g 11 g and MySQL server Having strong technical skills in Core Java with working knowledge Worked in ETL tools like Talend to simplify Map Reduce jobs from the front end Also have knowledge of Informatica IBM InfoSphere as another working ETL tool with Big Data Worked with BI tools like Tableau for report creation and further analysis from the front end Extensive knowledge in using SQL queries for backend database analysis Involved in developing distributed Enterprise and Web applications using UML JavaJ2EE Web technologies that include EJB JSP Servlets JMS JDBC JPA HTML XML Tomcat spring and Hibernate Expertise in Defect Management and Defect Tracking to do performance tuning for delivering utmost Quality product Hadoop Developer TMG Health Jessup PA November 2012 to December 2015 TMG Health is an American medical technology company that manufactures and sells medical devices instrument systems and reagents Founded in 1897 and headquartered in Franklin Lakes TMG Health employs nearly 30000 people in more than 50 countries throughout the world Responsibilities Worked with systems engineering team to plan and deploy new Hadoop environments and expand existing Hadoop clusters with agile methodology Monitored multiple Hadoop clusters environments using Ganglia monitored workload job performance and capacity planning using Cloudera Manager Experienced with through handson experience in all Hadoop Java SQL and Python Used Flume to collect aggregate and store the web log data from different sources like web servers mobile and network devices and pushed to HDFS Participated in functional reviews test specifications and documentation review Performed MapReduce programs on log data to transform into structured way to find user location age group spending time Analyzed the web log data using the HiveQL to extract number of unique visitors per day page views visit duration most purchased product on website Exported the analyzed data to the relational databases using Sqoop for visualization and to generate reports by Business Intelligence tools Proactively monitored systems and services architecture design and implementation of Hadoop deployment configuration management backup and disaster recovery systems and procedures Documented the systems processes and procedures for future references responsible to manage data coming from different sources EnvironmentHadoop HDFS Map Reduce Flume Pig Sqoop Hive Pig Sqoop Oozie Ganglia HBase Shell Scripting Java Developer FedEx Memphis TN October 2009 to November 2012 FedEx is a leading national provider of expert solutions for Medicare advantage Medicare part D and managed Medicaid plans With more than 17 years of experience of providing technologyenabled services to the government market exclusively our knowledge of health plan processes CMS requirements and the daily challenges plan face within the government market is second to none Responsibilities Created the database user environment activity and class diagram for the project UML Implemented the database using oracle database engine Created an entity object business rules and policy validation logic default value logic security Web application development using J2EE JSP Servlets JDBC JavaBeans Struts Ajax Custom Tags EJB Hibernate Ant Junitand ApacheLog4j Web Services Message queueMQ Designing GUI prototype using ADF 11 G GUI component before finalizing it for development Experience in using version controls such as CVS PVCS Involved in consuming producing Restful web services using JAXRS Collaborated with ETLInformatica team to determine the necessary data modules and UI designs to support Cognos reports Junit was used for unit testing for the integration testing tool Created modules using task flow with bounded and unbounded Generating WSDL web services and create work flow using BPEL Created the skin for the layout Made integrated testing for the application Created dynamic report and using JFreechart Environment Java Servlets JSF Adf rich client UI framework ADFBC BC4J 11 g Web Services using Oracle SOA Oracle WebLogic Software Engineer Arteria Technologies Pvt Ltd Hyderabad Telangana September 2008 to October 2009 Responsibilities Developed the user interface screens using swing for accepting various system inputs such as contractual terms monthly data pertaining to production inventory and transportation Involved in designing database connections using JDBC Involved in design and development of UI using HTML JavaScript and CSS Involved in creating tables stored procedures in Sqlfor data manipulation and retrieval using sqlsever2000 database modification using Sql PlSql triggers views in oracle Used dispatch action to group related actions into a single class Build the applications using Ant tool also used eclipse as the IDE Developed the business components used for the calculation module Involved in the logical and physical database design and implemented it by creating suitable tables views and triggers Applied J2EE design patterns like business delegate DAO and singleton Created the related procedures and functions used by JDBC calls in the above requirements Actively involved in testing debugging and deployment of the application on WebLogic application server Developed test cases and performed unit testing using JUnit Involved in fixing bugs and minor enhancements for the frontend modules Environment Java HTML Java script CSS Oracle JDBC ANT tool SQL Swing and Eclipse Hadoop Developer Becton Dickinson Memphis TN 1973 to 2000 is an American multinational courier delivery Services Company headquartered in Memphis Tennessee The name Becton Dickinson is a syllabic abbreviation of the name of the companys original air division Becton Dickinson which was used from 1973 until 2000 The company is known for its overnight shipping service but also for pioneering a system that could track packages and provide realtime updates on package location to help in finding lost packages a feature that has now been implemented by most other carrier services Responsibilities Extracted and updated the data into HDFS using Sqoop import and export command line utility interface Responsible for developing data pipeline using Flume Sqoop and Pig to extract the data from weblogs and store in HDFS Implemented advanced procedures like text analytics and processing using the inmemory computing capabilities like spark Enhanced and optimized product Spark code to aggregate group and run data mining tasks using the Spark framework Extending Hive and Pig core functionality by writing custom UDFs Developed data pipeline using Flume Sqoop pig and java MapReduce to ingest customer behavioral data and financial histories into HDFS for analysis Loaded cache data into HBase using Sqoop Experience in custom talend jobs to ingest enrich and distribute data in MapR Cloudera Hadoop ecosystem Created lots of external tables on Hive pointed to HBase tables Analyzed HBase data in Hive by creating external partitioned and bucketed tables Worked with cache data stored in Cassandra Supported MapReduce Programs those are running on the cluster Environment Hadoop MapReduce Hdfs Pig Hive HBase Impala Sqoop Flume Oozie Apache Spark Java Linux SQL Server Zookeeper Autosys Tableau Cassandra Education Bachelors Skills JAVA 10 years Hadoop 10 years HADOOP 10 years SQL 10 years APACHE HADOOP HDFS 10 years Additional Information SKILLS database 6 years java 10 years JDBC 6 years oracle 6 years SQL 7 years Technical Skills HadoopBig Data Technologies HDFS Map Reduce Sqoop Pig Hive Oozie impala Spark Zookeeper and Cloudera Manager NO SQL Database HBase Monitoring and Reporting Tableau Custom shell scripts Hadoop Distribution Horton Works Cloudera MapR Build Tools Maven SQL Developer Programming Scripting JAVA C SQL Shell Scripting Java Technologies Servlets JavaBeans JDBC Spring Hibernate Databases Oracle MY SQL MS SQL server Teradata Web Dev Technologies HTML XML JSON CSS Version Control SVN CVS GIT Operating Systems Linux Unix Mac OSX Windows 8 Windows 7 Windows Server",
    "extracted_keywords": [
        "Hadoop",
        "Developer",
        "Hadoop",
        "span",
        "lDeveloperspan",
        "Hadoop",
        "Developer",
        "CompuVision",
        "Consultin",
        "New",
        "York",
        "NY",
        "years",
        "Information",
        "Technology",
        "experience",
        "years",
        "experience",
        "Hadoop",
        "Ecosystem",
        "Big",
        "Data",
        "distributions",
        "Cloudera",
        "CDH",
        "Cloudera",
        "Manager",
        "Strong",
        "experience",
        "Hive",
        "processing",
        "volume",
        "data",
        "Expert",
        "Sqoop",
        "data",
        "systems",
        "HDFS",
        "system",
        "Expert",
        "PIG",
        "HiveUDFs",
        "Java",
        "order",
        "data",
        "experience",
        "Spark",
        "end",
        "end",
        "analytics",
        "knowledge",
        "Software",
        "Development",
        "Life",
        "Cycle",
        "SDLC",
        "Software",
        "Requirement",
        "Specifications",
        "SRS",
        "Functional",
        "Specification",
        "Document",
        "FSD",
        "Windows",
        "UNIXLINUX",
        "platform",
        "Technologies",
        "Big",
        "Data",
        "SQL",
        "PLSQL",
        "XML",
        "HTML",
        "Core",
        "Java",
        "Shell",
        "Scripting",
        "Experience",
        "Map",
        "Reduce",
        "codes",
        "Java",
        "business",
        "requirements",
        "knowledge",
        "Procedures",
        "packages",
        "functions",
        "cursors",
        "Oracle",
        "9i",
        "g",
        "g",
        "MySQL",
        "server",
        "skills",
        "Core",
        "Java",
        "knowledge",
        "ETL",
        "tools",
        "Talend",
        "Map",
        "Reduce",
        "jobs",
        "end",
        "knowledge",
        "Informatica",
        "IBM",
        "InfoSphere",
        "ETL",
        "tool",
        "Big",
        "Data",
        "BI",
        "tools",
        "Tableau",
        "report",
        "creation",
        "analysis",
        "knowledge",
        "SQL",
        "queries",
        "database",
        "analysis",
        "Enterprise",
        "Web",
        "applications",
        "UML",
        "JavaJ2EE",
        "Web",
        "technologies",
        "EJB",
        "JSP",
        "Servlets",
        "JMS",
        "JDBC",
        "JPA",
        "HTML",
        "XML",
        "Tomcat",
        "spring",
        "Hibernate",
        "Expertise",
        "Defect",
        "Management",
        "Defect",
        "Tracking",
        "performance",
        "Quality",
        "product",
        "US",
        "employer",
        "Work",
        "Experience",
        "Hadoop",
        "Developer",
        "CompuVision",
        "Consultin",
        "New",
        "York",
        "NY",
        "December",
        "Present",
        "years",
        "Information",
        "Technology",
        "experience",
        "years",
        "experience",
        "Hadoop",
        "Ecosystem",
        "Big",
        "Data",
        "distributions",
        "Cloudera",
        "CDH",
        "Cloudera",
        "Manager",
        "Strong",
        "experience",
        "Hive",
        "processing",
        "volume",
        "data",
        "Expert",
        "Sqoop",
        "data",
        "systems",
        "HDFS",
        "system",
        "Expert",
        "PIG",
        "HiveUDFs",
        "Java",
        "order",
        "data",
        "experience",
        "Spark",
        "end",
        "end",
        "analytics",
        "knowledge",
        "Software",
        "Development",
        "Life",
        "Cycle",
        "SDLC",
        "Software",
        "Requirement",
        "Specifications",
        "SRS",
        "Functional",
        "Specification",
        "Document",
        "FSD",
        "Windows",
        "UNIXLINUX",
        "platform",
        "Technologies",
        "Big",
        "Data",
        "SQL",
        "PLSQL",
        "XML",
        "HTML",
        "Core",
        "Java",
        "Shell",
        "Scripting",
        "Experience",
        "Map",
        "Reduce",
        "codes",
        "Java",
        "business",
        "requirements",
        "knowledge",
        "Procedures",
        "packages",
        "functions",
        "cursors",
        "Oracle",
        "9i",
        "g",
        "g",
        "MySQL",
        "server",
        "skills",
        "Core",
        "Java",
        "knowledge",
        "ETL",
        "tools",
        "Talend",
        "Map",
        "Reduce",
        "jobs",
        "end",
        "knowledge",
        "Informatica",
        "IBM",
        "InfoSphere",
        "ETL",
        "tool",
        "Big",
        "Data",
        "BI",
        "tools",
        "Tableau",
        "report",
        "creation",
        "analysis",
        "knowledge",
        "SQL",
        "queries",
        "database",
        "analysis",
        "Enterprise",
        "Web",
        "applications",
        "UML",
        "JavaJ2EE",
        "Web",
        "technologies",
        "EJB",
        "JSP",
        "Servlets",
        "JMS",
        "JDBC",
        "JPA",
        "HTML",
        "XML",
        "Tomcat",
        "spring",
        "Hibernate",
        "Expertise",
        "Defect",
        "Management",
        "Defect",
        "Tracking",
        "performance",
        "Quality",
        "product",
        "Hadoop",
        "Developer",
        "TMG",
        "Health",
        "Jessup",
        "PA",
        "November",
        "December",
        "TMG",
        "Health",
        "technology",
        "company",
        "devices",
        "instrument",
        "systems",
        "reagents",
        "Franklin",
        "Lakes",
        "TMG",
        "Health",
        "people",
        "countries",
        "world",
        "Responsibilities",
        "systems",
        "engineering",
        "team",
        "Hadoop",
        "environments",
        "Hadoop",
        "clusters",
        "methodology",
        "Monitored",
        "multiple",
        "Hadoop",
        "clusters",
        "environments",
        "Ganglia",
        "workload",
        "job",
        "performance",
        "capacity",
        "planning",
        "Cloudera",
        "Manager",
        "handson",
        "experience",
        "Hadoop",
        "Java",
        "SQL",
        "Python",
        "Flume",
        "aggregate",
        "web",
        "log",
        "data",
        "sources",
        "web",
        "servers",
        "mobile",
        "network",
        "devices",
        "HDFS",
        "reviews",
        "test",
        "specifications",
        "documentation",
        "review",
        "Performed",
        "MapReduce",
        "programs",
        "log",
        "data",
        "way",
        "user",
        "location",
        "age",
        "group",
        "spending",
        "time",
        "web",
        "log",
        "data",
        "HiveQL",
        "number",
        "visitors",
        "day",
        "page",
        "views",
        "duration",
        "product",
        "website",
        "data",
        "databases",
        "Sqoop",
        "visualization",
        "reports",
        "Business",
        "Intelligence",
        "tools",
        "systems",
        "services",
        "architecture",
        "design",
        "implementation",
        "Hadoop",
        "deployment",
        "configuration",
        "management",
        "backup",
        "disaster",
        "recovery",
        "systems",
        "procedures",
        "systems",
        "processes",
        "procedures",
        "references",
        "data",
        "sources",
        "EnvironmentHadoop",
        "HDFS",
        "Map",
        "Reduce",
        "Flume",
        "Pig",
        "Sqoop",
        "Hive",
        "Pig",
        "Sqoop",
        "Oozie",
        "Ganglia",
        "HBase",
        "Shell",
        "Scripting",
        "Java",
        "Developer",
        "FedEx",
        "Memphis",
        "TN",
        "October",
        "November",
        "FedEx",
        "provider",
        "solutions",
        "Medicare",
        "advantage",
        "Medicare",
        "part",
        "D",
        "Medicaid",
        "years",
        "experience",
        "services",
        "government",
        "market",
        "knowledge",
        "health",
        "plan",
        "CMS",
        "requirements",
        "challenges",
        "plan",
        "face",
        "government",
        "market",
        "none",
        "Responsibilities",
        "database",
        "user",
        "environment",
        "activity",
        "class",
        "diagram",
        "project",
        "UML",
        "database",
        "oracle",
        "database",
        "engine",
        "entity",
        "object",
        "business",
        "rules",
        "policy",
        "validation",
        "logic",
        "default",
        "value",
        "logic",
        "security",
        "Web",
        "application",
        "development",
        "J2EE",
        "JSP",
        "Servlets",
        "JDBC",
        "JavaBeans",
        "Struts",
        "Custom",
        "Tags",
        "EJB",
        "Hibernate",
        "Ant",
        "Junitand",
        "ApacheLog4j",
        "Web",
        "Services",
        "Message",
        "queueMQ",
        "Designing",
        "GUI",
        "prototype",
        "ADF",
        "G",
        "GUI",
        "component",
        "development",
        "Experience",
        "version",
        "controls",
        "CVS",
        "PVCS",
        "Restful",
        "web",
        "services",
        "JAXRS",
        "Collaborated",
        "ETLInformatica",
        "team",
        "data",
        "modules",
        "UI",
        "Cognos",
        "reports",
        "Junit",
        "unit",
        "testing",
        "integration",
        "testing",
        "tool",
        "modules",
        "task",
        "flow",
        "Generating",
        "WSDL",
        "web",
        "services",
        "work",
        "flow",
        "BPEL",
        "skin",
        "layout",
        "testing",
        "application",
        "report",
        "JFreechart",
        "Environment",
        "Java",
        "Servlets",
        "JSF",
        "Adf",
        "client",
        "UI",
        "framework",
        "ADFBC",
        "BC4J",
        "g",
        "Web",
        "Services",
        "Oracle",
        "SOA",
        "Oracle",
        "WebLogic",
        "Software",
        "Engineer",
        "Arteria",
        "Technologies",
        "Pvt",
        "Ltd",
        "Hyderabad",
        "Telangana",
        "September",
        "October",
        "Responsibilities",
        "user",
        "interface",
        "screens",
        "swing",
        "system",
        "inputs",
        "terms",
        "data",
        "production",
        "inventory",
        "transportation",
        "database",
        "connections",
        "JDBC",
        "design",
        "development",
        "UI",
        "HTML",
        "JavaScript",
        "CSS",
        "tables",
        "procedures",
        "Sqlfor",
        "data",
        "manipulation",
        "retrieval",
        "sqlsever2000",
        "database",
        "modification",
        "Sql",
        "PlSql",
        "views",
        "oracle",
        "dispatch",
        "action",
        "group",
        "actions",
        "class",
        "applications",
        "Ant",
        "tool",
        "eclipse",
        "IDE",
        "business",
        "components",
        "calculation",
        "module",
        "database",
        "design",
        "tables",
        "views",
        "Applied",
        "J2EE",
        "design",
        "patterns",
        "business",
        "delegate",
        "DAO",
        "singleton",
        "procedures",
        "functions",
        "JDBC",
        "calls",
        "requirements",
        "testing",
        "debugging",
        "deployment",
        "application",
        "WebLogic",
        "application",
        "server",
        "test",
        "cases",
        "unit",
        "testing",
        "JUnit",
        "bugs",
        "enhancements",
        "frontend",
        "modules",
        "Environment",
        "Java",
        "HTML",
        "Java",
        "script",
        "CSS",
        "Oracle",
        "JDBC",
        "ANT",
        "tool",
        "SQL",
        "Swing",
        "Eclipse",
        "Hadoop",
        "Developer",
        "Becton",
        "Dickinson",
        "Memphis",
        "TN",
        "courier",
        "delivery",
        "Services",
        "Company",
        "Memphis",
        "Tennessee",
        "name",
        "Becton",
        "Dickinson",
        "abbreviation",
        "name",
        "air",
        "division",
        "Becton",
        "Dickinson",
        "company",
        "shipping",
        "service",
        "system",
        "packages",
        "updates",
        "package",
        "location",
        "packages",
        "feature",
        "carrier",
        "services",
        "Responsibilities",
        "data",
        "HDFS",
        "Sqoop",
        "import",
        "export",
        "command",
        "line",
        "utility",
        "interface",
        "data",
        "pipeline",
        "Flume",
        "Sqoop",
        "Pig",
        "data",
        "weblogs",
        "HDFS",
        "procedures",
        "text",
        "analytics",
        "processing",
        "inmemory",
        "computing",
        "capabilities",
        "spark",
        "product",
        "Spark",
        "code",
        "group",
        "data",
        "mining",
        "tasks",
        "Spark",
        "framework",
        "Hive",
        "Pig",
        "core",
        "functionality",
        "custom",
        "UDFs",
        "data",
        "pipeline",
        "Flume",
        "Sqoop",
        "pig",
        "MapReduce",
        "customer",
        "data",
        "histories",
        "HDFS",
        "analysis",
        "cache",
        "data",
        "HBase",
        "Sqoop",
        "Experience",
        "custom",
        "talend",
        "jobs",
        "enrich",
        "data",
        "MapR",
        "Cloudera",
        "Hadoop",
        "ecosystem",
        "lots",
        "tables",
        "Hive",
        "HBase",
        "tables",
        "HBase",
        "data",
        "Hive",
        "tables",
        "cache",
        "data",
        "Cassandra",
        "Supported",
        "MapReduce",
        "Programs",
        "cluster",
        "Environment",
        "Hadoop",
        "MapReduce",
        "Hdfs",
        "Pig",
        "Hive",
        "HBase",
        "Impala",
        "Sqoop",
        "Flume",
        "Oozie",
        "Apache",
        "Spark",
        "Java",
        "Linux",
        "SQL",
        "Server",
        "Zookeeper",
        "Autosys",
        "Tableau",
        "Cassandra",
        "Education",
        "Bachelors",
        "Skills",
        "JAVA",
        "years",
        "Hadoop",
        "years",
        "HADOOP",
        "years",
        "SQL",
        "years",
        "APACHE",
        "HADOOP",
        "HDFS",
        "years",
        "Information",
        "SKILLS",
        "database",
        "years",
        "years",
        "JDBC",
        "years",
        "oracle",
        "years",
        "SQL",
        "years",
        "Technical",
        "Skills",
        "HadoopBig",
        "Data",
        "Technologies",
        "HDFS",
        "Map",
        "Reduce",
        "Sqoop",
        "Pig",
        "Hive",
        "Oozie",
        "impala",
        "Spark",
        "Zookeeper",
        "Cloudera",
        "Manager",
        "NO",
        "SQL",
        "Database",
        "HBase",
        "Monitoring",
        "Tableau",
        "Custom",
        "shell",
        "Hadoop",
        "Distribution",
        "Horton",
        "Cloudera",
        "MapR",
        "Build",
        "Tools",
        "Maven",
        "SQL",
        "Developer",
        "Programming",
        "Scripting",
        "C",
        "SQL",
        "Shell",
        "Scripting",
        "Java",
        "Technologies",
        "Servlets",
        "JavaBeans",
        "JDBC",
        "Spring",
        "Hibernate",
        "Oracle",
        "MY",
        "SQL",
        "MS",
        "SQL",
        "server",
        "Teradata",
        "Web",
        "Dev",
        "Technologies",
        "HTML",
        "JSON",
        "CSS",
        "Version",
        "Control",
        "SVN",
        "CVS",
        "GIT",
        "Operating",
        "Systems",
        "Linux",
        "Unix",
        "Mac",
        "OSX",
        "Windows",
        "Windows",
        "Server"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T22:40:25.203122",
    "resume_data": "Hadoop Developer Hadoop span lDeveloperspan Hadoop Developer CompuVision Consultin New York NY Around 8 years of Information Technology experience with 5 years of experience in Hadoop Ecosystem Worked with Big Data distributions like Cloudera CDH 3 and 4 with Cloudera Manager Strong experience in using Hive for processing and analyzing large volume of data Expert in using Sqoop for fetching data from different systems to analyze in HDFS and putting it back to the previous system for further processing Expert in creating PIG and HiveUDFs using Java in order to analyze the data efficiently Strong experience in using Spark for end to end analytics Strong knowledge of Software Development Life Cycle SDLC Experienced in creating and analyzing Software Requirement Specifications SRS and Functional Specification Document FSD Worked in Windows UNIXLINUX platform with different Technologies such as Big Data SQL PLSQL XML HTML Core Java Shell Scripting Experience of creating Map Reduce codes in Java as per the business requirements Extensive knowledge in creating PLSQL stored Procedures packages functions cursors etc against Oracle 9i 10g 11g and MySQL server Having strong technical skills in Core Java with working knowledge Worked in ETL tools like Talend to simplify Map Reduce jobs from the front end Also have knowledge of Informatica IBM InfoSphere as another working ETL tool with Big Data Worked with BI tools like Tableau for report creation and further analysis from the front end Extensive knowledge in using SQL queries for backend database analysis Involved in developing distributed Enterprise and Web applications using UML JavaJ2EE Web technologies that include EJB JSP Servlets JMS JDBC JPA HTML XML Tomcat spring and Hibernate Expertise in Defect Management and Defect Tracking to do performance tuning for delivering utmost Quality product Authorized to work in the US for any employer Work Experience Hadoop Developer CompuVision Consultin New York NY December 2015 to Present Around 8 years of Information Technology experience with 5 years of experience in Hadoop Ecosystem Worked with Big Data distributions like Cloudera CDH 3 and 4 with Cloudera Manager Strong experience in using Hive for processing and analyzing large volume of data Expert in using Sqoop for fetching data from different systems to analyze in HDFS and putting it back to the previous system for further processing Expert in creating PIG and HiveUDFs using Java in order to analyze the data efficiently Strong experience in using Spark for end to end analytics Strong knowledge of Software Development Life Cycle SDLC Experienced in creating and analyzing Software Requirement Specifications SRS and Functional Specification Document FSD Worked in Windows UNIXLINUX platform with different Technologies such as Big Data SQL PLSQL XML HTML Core Java Shell Scripting Experience of creating Map Reduce codes in Java as per the business requirements Extensive knowledge in creating PLSQL stored Procedures packages functions cursors etc against Oracle 9i 10g 11g and MySQL server Having strong technical skills in Core Java with working knowledge Worked in ETL tools like Talend to simplify Map Reduce jobs from the front end Also have knowledge of Informatica IBM InfoSphere as another working ETL tool with Big Data Worked with BI tools like Tableau for report creation and further analysis from the front end Extensive knowledge in using SQL queries for backend database analysis Involved in developing distributed Enterprise and Web applications using UML JavaJ2EE Web technologies that include EJB JSP Servlets JMS JDBC JPA HTML XML Tomcat spring and Hibernate Expertise in Defect Management and Defect Tracking to do performance tuning for delivering utmost Quality product Hadoop Developer TMG Health Jessup PA November 2012 to December 2015 TMG Health is an American medical technology company that manufactures and sells medical devices instrument systems and reagents Founded in 1897 and headquartered in Franklin Lakes TMG Health employs nearly 30000 people in more than 50 countries throughout the world Responsibilities Worked with systems engineering team to plan and deploy new Hadoop environments and expand existing Hadoop clusters with agile methodology Monitored multiple Hadoop clusters environments using Ganglia monitored workload job performance and capacity planning using Cloudera Manager Experienced with through handson experience in all Hadoop Java SQL and Python Used Flume to collect aggregate and store the web log data from different sources like web servers mobile and network devices and pushed to HDFS Participated in functional reviews test specifications and documentation review Performed MapReduce programs on log data to transform into structured way to find user location age group spending time Analyzed the web log data using the HiveQL to extract number of unique visitors per day page views visit duration most purchased product on website Exported the analyzed data to the relational databases using Sqoop for visualization and to generate reports by Business Intelligence tools Proactively monitored systems and services architecture design and implementation of Hadoop deployment configuration management backup and disaster recovery systems and procedures Documented the systems processes and procedures for future references responsible to manage data coming from different sources EnvironmentHadoop HDFS Map Reduce Flume Pig Sqoop Hive Pig Sqoop Oozie Ganglia HBase Shell Scripting Java Developer FedEx Memphis TN October 2009 to November 2012 FedEx is a leading national provider of expert solutions for Medicare advantage Medicare part D and managed Medicaid plans With more than 17 years of experience of providing technologyenabled services to the government market exclusively our knowledge of health plan processes CMS requirements and the daily challenges plan face within the government market is second to none Responsibilities Created the database user environment activity and class diagram for the project UML Implemented the database using oracle database engine Created an entity object business rules and policy validation logic default value logic security Web application development using J2EE JSP Servlets JDBC JavaBeans Struts Ajax Custom Tags EJB Hibernate Ant Junitand ApacheLog4j Web Services Message queueMQ Designing GUI prototype using ADF 11G GUI component before finalizing it for development Experience in using version controls such as CVS PVCS Involved in consuming producing Restful web services using JAXRS Collaborated with ETLInformatica team to determine the necessary data modules and UI designs to support Cognos reports Junit was used for unit testing for the integration testing tool Created modules using task flow with bounded and unbounded Generating WSDL web services and create work flow using BPEL Created the skin for the layout Made integrated testing for the application Created dynamic report and using JFreechart Environment Java Servlets JSF Adf rich client UI framework ADFBC BC4J 11g Web Services using Oracle SOA Oracle WebLogic Software Engineer Arteria Technologies Pvt Ltd Hyderabad Telangana September 2008 to October 2009 Responsibilities Developed the user interface screens using swing for accepting various system inputs such as contractual terms monthly data pertaining to production inventory and transportation Involved in designing database connections using JDBC Involved in design and development of UI using HTML JavaScript and CSS Involved in creating tables stored procedures in Sqlfor data manipulation and retrieval using sqlsever2000 database modification using Sql PlSql triggers views in oracle Used dispatch action to group related actions into a single class Build the applications using Ant tool also used eclipse as the IDE Developed the business components used for the calculation module Involved in the logical and physical database design and implemented it by creating suitable tables views and triggers Applied J2EE design patterns like business delegate DAO and singleton Created the related procedures and functions used by JDBC calls in the above requirements Actively involved in testing debugging and deployment of the application on WebLogic application server Developed test cases and performed unit testing using JUnit Involved in fixing bugs and minor enhancements for the frontend modules Environment Java HTML Java script CSS Oracle JDBC ANT tool SQL Swing and Eclipse Hadoop Developer Becton Dickinson Memphis TN 1973 to 2000 is an American multinational courier delivery Services Company headquartered in Memphis Tennessee The name Becton Dickinson is a syllabic abbreviation of the name of the companys original air division Becton Dickinson which was used from 1973 until 2000 The company is known for its overnight shipping service but also for pioneering a system that could track packages and provide realtime updates on package location to help in finding lost packages a feature that has now been implemented by most other carrier services Responsibilities Extracted and updated the data into HDFS using Sqoop import and export command line utility interface Responsible for developing data pipeline using Flume Sqoop and Pig to extract the data from weblogs and store in HDFS Implemented advanced procedures like text analytics and processing using the inmemory computing capabilities like spark Enhanced and optimized product Spark code to aggregate group and run data mining tasks using the Spark framework Extending Hive and Pig core functionality by writing custom UDFs Developed data pipeline using Flume Sqoop pig and java MapReduce to ingest customer behavioral data and financial histories into HDFS for analysis Loaded cache data into HBase using Sqoop Experience in custom talend jobs to ingest enrich and distribute data in MapR Cloudera Hadoop ecosystem Created lots of external tables on Hive pointed to HBase tables Analyzed HBase data in Hive by creating external partitioned and bucketed tables Worked with cache data stored in Cassandra Supported MapReduce Programs those are running on the cluster Environment Hadoop MapReduce Hdfs Pig Hive HBase Impala Sqoop Flume Oozie Apache Spark Java Linux SQL Server Zookeeper Autosys Tableau Cassandra Education Bachelors Skills JAVA 10 years Hadoop 10 years HADOOP 10 years SQL 10 years APACHE HADOOP HDFS 10 years Additional Information SKILLS database 6 years java 10 years JDBC 6 years oracle 6 years SQL 7 years Technical Skills HadoopBig Data Technologies HDFS Map Reduce Sqoop Pig Hive Oozie impala Spark Zookeeper and Cloudera Manager NO SQL Database HBase Monitoring and Reporting Tableau Custom shell scripts Hadoop Distribution Horton Works Cloudera MapR Build Tools Maven SQL Developer Programming Scripting JAVA C SQL Shell Scripting Java Technologies Servlets JavaBeans JDBC Spring Hibernate Databases Oracle MY SQL MS SQL server Teradata Web Dev Technologies HTML XML JSON CSS Version Control SVN CVS GIT Operating Systems Linux Unix Mac OSX Windows 8 Windows 7 Windows Server",
    "unique_id": "a3aab1ba-718e-47d0-a7f1-59ed9393b44f"
}