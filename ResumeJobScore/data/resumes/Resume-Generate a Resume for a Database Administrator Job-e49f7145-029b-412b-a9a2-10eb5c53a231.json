{
    "clean_data": "Senior Bigdata Hadoop Developer Senior Bigdata Hadoop span lDeveloperspan Senior Bigdata Hadoop Developer Kohls Work Experience Senior Bigdata Hadoop Developer Kohls Milpitas CA February 2018 to Present Kohls is an American department store retailing chain Kohls started Marketing Transformation Program MTP to create Data Lake for all the data in Google Cloud platform Ingesting the data from various sources files real time data RDBMS and processing the data using Big data technologies and infrastructure Roles and Responsibilities Creating and executing Azkaban flows using Java Hive Pig shell scripts to ingest the data into GCP Google Cloud Platform from various sources such as server logs Mount point server files and Real time data from Mobile applications to utilize Google Cloud Platform as Data Lake Designing Row keys Schema with NoSQL databases such as HBase to load huge volumes of data for real time transactions Writing multiple MapReduce Jobs using Java API Pig and Hive for data extraction transformation and aggregation from multiple file formats including Parquet Avro XML JSON CSV ORCFILE and other compressed file format Codecs like gzip Snappy LZO Building advanced analytical applications by making use of Spark SQL Big Query to create detail level summary reports and Dashboard using KPIs Working on SparkStreaming APIs using Data frames to perform necessary transformations and actions on the fly for building the common learner data model which gets the data from Kafka in near real time and persists into Google Cloud Platform GCP and Big Query Building and deploying the applications using Jenkins and Tonomi with appropriate configurations Optimizing Spark jobs by tuning executors memory configurations DStreams Accumulator Broadcast variables RDD caching to deliver the best results for the large datasets Responsible for deploying the scripts into Github version control repository hosting service and deployed the code using Jenkins Creating and executing apache Airflow jobs in Python language for orchestration purposes Involving in design code reviews and supporting test teams in testing to fix the identified issues in the developed applications Involved in business and functional requirements gathering and prepared user stories in JIRA and documentation in Confluence Environment Hortonworks Data Platform HDP HDFS MapReduce YARN HBase Java Hive Pig Sqoop Spark Parquet Flume GCP Google Cloud Platform Azkaban Big query Python Scala Airflow Kafka Senior Hadoop Developer ATT El Segundo CA February 2015 to January 2018 DirecTV Now part of ATT provides television and audio services to subscribers through satellite transmissions Services include the equivalent of many local television stations broadcast television networks subscription television services satellite radio services and private video services AMS is a system which enables in DIRECTV set to collect information about events that occur on the Set Top Box STB for later reporting Event information includes live viewing DVR usage playback and record interactive application usage CA OSD events advertising data and diagnostic information Roles and Responsibilities Designing the applications from the ingestion to reports delivery to third party vendors using big data technologies flume kafka sqoop mapreduce hive pig Process the raw log files from the set top boxes using java map reduce code and shell scripts and stored them as text files in HDFS Ingesting the data from legacy and upstream systems to HDFS using apache Sqoop Flume java map reduce programs hive queries and pig scripts Generating the required reports using Oozie workflow and Hive queries for operations team from the ingested data Alert and monitoring mechanism for the oozie jobs for the failure conditions and successful conditions using email notifications Involved in bluecoat proxy approach while sharing the data from the inhouse Hadoop cluster to external vendors Working with application teams to install operating system Hadoop updates patches version upgrades as required Writing Map Reduce code to make unstructured and semi structured data into structured data and loading into Hive tables Worked on debugging performance tuning of Hive Pig Jobs Analyzing system failures identifying root causes and recommended course of actions as part of operations support Involved in Spark streaming solution for the time sensitive revenue generating reports to match the pace with upstream STB systems data Experience in working on Hbase with Apache phoenix as a data layer to serve the web requests to meet the SLA requirements Created HBASE tables and load the data using sparkstreaming application Worked on SFDC ODATA connector to get the data from NodeJS services which in turns the fetch the data from HBASE Utilized AWS S3 services to pushstore and pull the data from AWS from external applications Responsible for functional requirements gathering code reviews deployment scripts and procedures offshore coordination and ontime deliverables Environment Hadoop HDFS Pig Hive Flume Kafka MapReduce Sqoop Spark Oozie LINUX NodeJS SFDC ODATA and AWS Hadoop Developer Dun Bradstreet Parsippany NJ August 2014 to January 2015 Dun Bradstreet D B provides commercial data to businesses on credit history businesstobusiness sales and marketing counterparty risk exposure supply chain management lead scoring and social identity matching It maintains a database contains information on more than 235 million companies across 200 countries worldwide This database contains over 53 million professional contact names using a variety of sources including public records trade references telco providers telephone interviews print digital and trade publications Roles and Responsibilities Involved in designing and developing the Cascading work flows Actively participated in validating and cleansing phases of the data flow Worked with the Data Science team to gather requirements for various data transformations Involved in writing a new subassembly for a complex query used for lookup the data Worked on custom functions and subassemblies which helps for code reuse Involved in running cascading job in local and Hadoop modes Worked with application teams to install VM HBASE Hadoop updates patches version upgrades as required Created and tested JUNIT test cases for different Cascading flows Worked on Tableau workbooks to perform year over year quarter over quarter YTD QTD and MTD type of analysis Utilized Tableau server to publish and share the reports with the business users Idea on continuous integration tools like Jenkins to automate the process for code builds and deployments Environment Hadoop HDFS Cascading Lingual HBASE REDIS Zookeeper JUnit Jenkins Gradle Tableau JavaHadoop Developer Tata Consultancy Services Hyderabad Telangana December 2011 to July 2013 India Client Verizon NY Verizon is one of the worlds leading providers of highgrowth communications services Verizon is the nations largest local exchange carrier and a rapidly emerging long distance provider Verizon companies are the largest providers of wired line and wireless communications in the United States serving the equivalent of nearly 135 million access lines and 30 million wireless customers Responsibilities Created Temporary Tables to store the data from Legacy system Used SQLLoader scripts to load the data into temporary tables and procedures to validate the data Creation of Materialized views and function based indexes Setting up various notification scripts which will notify if the database is not up and running or if any directory space is filled up Used Visual SourceSafe for version control and performing various builds Involved in Unit testing User Acceptance Testing to check whether the data is loading into target Extracted files from DB2 through Sqoop and placed in HDFS and processed Extracted from different source systems according to the user requirements Involved in POC for analyzing large data sets by running Hive queries and Pig scripts Worked with the Data Science team to gather requirements for various data mining projects Worked on Hive tables creation and loading and analyzing data using hive queries Analyzed large data sets by running Hive queries and Pig scripts Worked with the Data Science team to gather requirements for various data mining projects Created and maintained Technical documentation for launching HADOOP Clusters and for executing Hive queries and Pig Scripts Providing daily development status weekly status reports and weekly development summary and defects report Environment Core Java JDBC JavaScript MySQL JUnit Eclipse QA Hadoop Sqoop Hive Pig JavaJ2EE Developer Zensar Technologies Hyderabad Telangana April 2010 to November 2011 India Client Assurant Solutions Miami FL Assurant Solutions is one of the leading Auto Insurance companies in the US Assurant has the various applications within that XYCOR NonBill and Special Products application comes under Insurance and financial application sector These application mainly deals with Credit insurance Credit Insurance is an insurance policy and risk management product that covers the payment risk in the event a borrower is unable to repay because of certain events Roles and Responsibilities Utilized Agile Methodologies to manage full lifecycle development of the project Developed front end validations using JavaScript and developed design and layouts of JSPs and custom taglibs for all JSPs Used JDBC for database connectivity Developed web application using JSP custom tag libraries and Action Designed Java Servlets and Objects using J2EE standards Used JSP for presentation layer developed high performance objectrelational persistence and query service for entire application Developed the XML Schema and Web services for the data maintenance and structures Used WebLogic Application Server and RAD to develop and deploy the application Worked with various Style Sheets like Cascading Style Sheets CSS Designed database and created tables written the complex SQL Queries and stored procedures as per the requirements Involved in coding for JUnit Test cases ANT for building the application Environment JavaJ2EE Oracle 10g SQL PLSQL JSP EJB WebLogic 80 HTML AJAX Java Script JDBC XML JMS JUnit log4j MyEclipse 60 JavaJ2EE Developer Satyam Computer Services Ltd Hyderabad Telangana August 2009 to April 2010 India Client Central Bancompany Jefferson City MO Worked on application named Access portal It is a part of online banking that allows a customer to view quick summary of transactions and account details It also shows mutual funds associated with account Roles and Responsibilities Responsible for understanding the scope of the project and requirement gathering Review and analyze the design and implementation of software componentsapplications and outline the development process strategies Coordinate with Project managers Development and QA teams during the course of the project Used Spring JDBC to write some DAO classes to interact with the database to access account information Used Tomcat web server for development purpose Involved in creation of Test Cases for JUnit Testing Used Oracle as Database and used Toad for queries execution and also involved in writing SQL scripts PLSQL code for procedures and functions Used CVS Perforce as configuration management tool for code versioning and release Developed application using Eclipse and used build and deploy tool as Maven Used Log4J to print the logging debugging warning info on the server console Environment Java15 J2EE Servlet JSP XML Spring 30 Design Patterns Log4j CVS Maven Eclipse Apache Tomcat 6 and Oracle 11g Java Developer Satyam Computer Services Ltd Hyderabad Telangana September 2008 to July 2009 India Client OneBeacon Insurance Group Minnetonka MN Disability Income Application is used for the Underwriting and Administration of Disabilities products This is a Maintenance and Enhancement Project for Individual and Institutional policies of Metlife Responsibilities Coded the business methods according to the IBM Rational Rose UML model Extensively used Core Java Servlets JSP and XML Used DB2 Database to store the system data Used Rational Application Developer RAD as Integrated Development Environment IDE Used unit testing for all the components using JUnit Used Apache log 4j Logging framework for logging of trace and Auditing Used Asynchronous JavaScript and XML AJAX for better and faster interactive FrontEnd Provide support to resolve performance testing issues profiling and cache mechanism Performs code reviews to ensure consistency to style standards and code quality Environment Java 16 Servlets JSP IBM Rational Application Developer RAD 6 Websphere 60 iText AJAX DB2 log4j JavaJ2EE Developer Satyam Computer Services Ltd Hyderabad Telangana October 2007 to August 2008 India Client Lowes Mooresville North Carolina Lowes Companies Inc is a USbased chain of retail home improvement and appliance stores Founded in 1946 in North Wilkesboro North Carolina the chain now serves over 14 million customers a week in its 1616 stores in the United States and Canada The import project will provide and support overseas product and relationship growth as well as supported IT systems and processes for both sourcing item and vendor functions and purchase order management creating and managing POs Mainframes act as a host between PeopleSoft tables and Tradestone The objective of the project is to design develop and integrate Lowes Purchase Order System with TradeStone a Lowes web based application for PO maintenance Responsibilities Designed User Interface using Java Server Pages JSP and XML Developed the Enterprise Java Beans Stateless Stateful Session beans Entity beans to handle different transactions such as online funds transfer bill payments to the service providers Implemented Service Oriented Architecture SOA using JMS in MDB for sending and receiving messages while creating web services Worked on Web Services for data transfer from client to server and vice versa using SOAP WSDL and UDDI Involved in testing the web services using SOAP UI Extensively worked on JMS using pointpoint publishersubscriber messaging Domains for implementing Exchange of information through Messages Environment Windows Java 14 HTML JavaScript 16 XML JUnit JMS Web Services SOAP 11 UDDI 2 Maven 20 Eclipse IDE CVS Oracle 10g Skills Eclipse J2ee Java Intellij idea Jsp Additional Information Around 11 years of experience in IT industry which includes around 6 years of experience in Big Data in implementing complete Hadoop solutions Architecture and Design Hands on experience in installing configuring and using Apache Hadoop ecosystem components like HDFS Hadoop MapReduce Zoo Keeper Oozie Hive Sqoop Kafka Spark Pig and Cascading Azkaban Airflow Big query Expertise in writing Hadoop Jobs for analyzing data using Hive and Pig Experience in working with MapReduce programs using Apache Hadoop for working with Big Data Experience in importing and exporting data using Sqoop from HDFS to Relational Database SystemsRDBMS and viceversa Experience in working with ETL tool Kettle by Pentaho Involved in developing Tableau Dashboards with interactive views trends and drill downs for the users data In depth understandingknowledge of Hadoop Architecture and various components such as HDFS Job Tracker Task Tracker Name Node Data Node and MapReduce concepts Worked on Cascading API for Hadoop application development and work flows Good understanding of Data Mining and Machine Learning techniques Experience in analyzing data using Hive QL Pig Latin and custom MapReduce programs in Java Knowledge of job workflow scheduling and monitoring tools like Oozie and Zookeeper Worked on Spark streaming application which consumes the data from Kafka and persist it in to HBASE Experience in optimization of Map reduce algorithm using combiners and partitioners to deliver the best results Involved in Spark solution for the time sensitive use cases Good understanding of NoSQL databases like MongoDB REDIS Expertise in core Java J2EE Multithreading JDBC Shell Scripting and proficient in using Java APIs for application development Proficient in Working with Various IDE tools including Eclipse Galileo IBM Rational Application Developer RAD and IntelliJ IDEA Worked on different operating systems like UNIXLinuxWindows XP and Windows 2K Good knowledge on functional programming language Scala Integrate Salesforce using ODATA connector for Rest API Worked on NODEJS API to pull the data from HBASE Written Airflow jobs using inbuild and custom operators in Python to schedule and run the flows Very good experience in customer specification study requirements gathering system architectural design and turning the requirements into final product Strong background in mathematics and have very good analytical and problem solving skills Technical Expertise HadoopBig Data HDFS MapReduce Pig Hive Sqoop Oozie Zookeeper Cascading SPSS Kafka Flume Hbase Phoenix Spark Azkaban YARN Airflow Java J2EE technologies Core Java JSP JDBC Hadoop distributions Cloudera Hortonworks Google Cloud Platform IDE Tools Eclipse IntelliJ IDEA Programming languages C C Java Linux shell scripts VBNET COBOL Python Scala Databases Oracle 11g10g9i MySQL DB2 MSSQL Server MongoDB Big Query Web Technologies HTML XML JavaScript ETL Tools Kettle Reporting Tools Tableau Operating Systems Windows 95982000XPVista7 LINUX Monitoring Reporting Nagios Custom shell scripts Version control Git SVN Testing Tools JUnit MRUnit",
    "entities": [
        "Canada",
        "AWS Hadoop Developer Dun Bradstreet Parsippany NJ",
        "Objects",
        "XP",
        "Used Rational Application Developer RAD",
        "Jefferson City",
        "JMS",
        "Working",
        "MTD",
        "Data Lake",
        "Metlife Responsibilities Coded",
        "Java Server Pages",
        "PO",
        "HBASE Written Airflow",
        "Mobile",
        "Lowes",
        "RDD",
        "Hadoop",
        "Responsibilities Created Temporary Tables",
        "XML",
        "Special Products",
        "SOAP",
        "Java Developer Satyam Computer Services Ltd Hyderabad",
        "Telangana",
        "Idea",
        "Coordinate",
        "WebLogic",
        "JIRA",
        "JUnit",
        "HDFS Hadoop MapReduce Zoo Keeper Oozie Hive",
        "HBase",
        "Test Cases for JUnit Testing Used Oracle",
        "Python",
        "Hive and Pig Experience",
        "Tata Consultancy Services Hyderabad",
        "FL Assurant Solutions",
        "Developed",
        "Skills",
        "Lowes Purchase Order System",
        "Node Data",
        "Process",
        "RAD",
        "Messages Environment Windows Java",
        "Responsibilities Involved",
        "QA Hadoop",
        "UDDI Involved",
        "Bigdata Hadoop Developer Kohls Work Experience Senior Bigdata Hadoop Developer Kohls Milpitas",
        "DVR",
        "YTD QTD",
        "Review",
        "the IBM Rational Rose UML",
        "JSP",
        "Tableau Dashboards",
        "Codecs",
        "Airflow",
        "JUnit Test",
        "STB",
        "Version",
        "SQL Queries",
        "JMS Web Services",
        "Big Query Building",
        "User Acceptance Testing",
        "FrontEnd",
        "Present Kohls",
        "Optimizing Spark",
        "the Underwriting and Administration of Disabilities",
        "Creation of Materialized",
        "MyEclipse 60 JavaJ2EE Developer Satyam Computer Services Ltd Hyderabad",
        "Spark",
        "Project managers Development",
        "ODATA",
        "Zookeeper JUnit Jenkins Gradle Tableau JavaHadoop Developer",
        "HBASE Utilized AWS S3",
        "Hadoop Jobs",
        "PeopleSoft",
        "Sqoop",
        "QA",
        "Created",
        "AWS",
        "CA",
        "MDB",
        "Domains",
        "Hadoop Architecture",
        "JavaJ2EE Developer Satyam Computer Services Ltd Hyderabad",
        "HDFS Job Tracker Task Tracker",
        "the Data Science",
        "Data Mining and Machine Learning",
        "JSP IBM Rational Application Developer RAD",
        "Cascading",
        "IBM Rational Application Developer RAD",
        "SQL",
        "Worked on Hive",
        "Mainframes",
        "the US Assurant",
        "Verizon",
        "Azkaban",
        "Credit insurance Credit Insurance",
        "Github",
        "Spark SQL Big Query",
        "AMS",
        "XML Developed",
        "Integrated Development Environment IDE Used",
        "HADOOP Clusters",
        "the United States",
        "Big Data",
        "Hive",
        "JUNIT",
        "Environment Hadoop HDFS Cascading Lingual HBASE",
        "UDDI",
        "Core Java Servlets JSP",
        "ETL",
        "Created HBASE",
        "Utilized Tableau",
        "India",
        "Maven",
        "Apache Hadoop",
        "OneBeacon Insurance Group Minnetonka MN Disability Income Application",
        "Implemented Service Oriented Architecture SOA",
        "GCP",
        "JavaScript",
        "ANT",
        "XYCOR NonBill",
        "North Carolina",
        "Jsp Additional Information",
        "VM HBASE Hadoop",
        "CVS Maven",
        "ATT",
        "Tomcat",
        "Data",
        "Bigdata Hadoop Developer",
        "Miami",
        "MapReduce",
        "Architecture and Design Hands",
        "Google Cloud Platform GCP",
        "NoSQL",
        "Tableau",
        "WebLogic Application Server",
        "Technical Expertise HadoopBig Data HDFS MapReduce Pig Hive Sqoop",
        "IDEA Programming",
        "Auto Insurance"
    ],
    "experience": "Experience Senior Bigdata Hadoop Developer Kohls Milpitas CA February 2018 to Present Kohls is an American department store retailing chain Kohls started Marketing Transformation Program MTP to create Data Lake for all the data in Google Cloud platform Ingesting the data from various sources files real time data RDBMS and processing the data using Big data technologies and infrastructure Roles and Responsibilities Creating and executing Azkaban flows using Java Hive Pig shell scripts to ingest the data into GCP Google Cloud Platform from various sources such as server logs Mount point server files and Real time data from Mobile applications to utilize Google Cloud Platform as Data Lake Designing Row keys Schema with NoSQL databases such as HBase to load huge volumes of data for real time transactions Writing multiple MapReduce Jobs using Java API Pig and Hive for data extraction transformation and aggregation from multiple file formats including Parquet Avro XML JSON CSV ORCFILE and other compressed file format Codecs like gzip Snappy LZO Building advanced analytical applications by making use of Spark SQL Big Query to create detail level summary reports and Dashboard using KPIs Working on SparkStreaming APIs using Data frames to perform necessary transformations and actions on the fly for building the common learner data model which gets the data from Kafka in near real time and persists into Google Cloud Platform GCP and Big Query Building and deploying the applications using Jenkins and Tonomi with appropriate configurations Optimizing Spark jobs by tuning executors memory configurations DStreams Accumulator Broadcast variables RDD caching to deliver the best results for the large datasets Responsible for deploying the scripts into Github version control repository hosting service and deployed the code using Jenkins Creating and executing apache Airflow jobs in Python language for orchestration purposes Involving in design code reviews and supporting test teams in testing to fix the identified issues in the developed applications Involved in business and functional requirements gathering and prepared user stories in JIRA and documentation in Confluence Environment Hortonworks Data Platform HDP HDFS MapReduce YARN HBase Java Hive Pig Sqoop Spark Parquet Flume GCP Google Cloud Platform Azkaban Big query Python Scala Airflow Kafka Senior Hadoop Developer ATT El Segundo CA February 2015 to January 2018 DirecTV Now part of ATT provides television and audio services to subscribers through satellite transmissions Services include the equivalent of many local television stations broadcast television networks subscription television services satellite radio services and private video services AMS is a system which enables in DIRECTV set to collect information about events that occur on the Set Top Box STB for later reporting Event information includes live viewing DVR usage playback and record interactive application usage CA OSD events advertising data and diagnostic information Roles and Responsibilities Designing the applications from the ingestion to reports delivery to third party vendors using big data technologies flume kafka sqoop mapreduce hive pig Process the raw log files from the set top boxes using java map reduce code and shell scripts and stored them as text files in HDFS Ingesting the data from legacy and upstream systems to HDFS using apache Sqoop Flume java map reduce programs hive queries and pig scripts Generating the required reports using Oozie workflow and Hive queries for operations team from the ingested data Alert and monitoring mechanism for the oozie jobs for the failure conditions and successful conditions using email notifications Involved in bluecoat proxy approach while sharing the data from the inhouse Hadoop cluster to external vendors Working with application teams to install operating system Hadoop updates patches version upgrades as required Writing Map Reduce code to make unstructured and semi structured data into structured data and loading into Hive tables Worked on debugging performance tuning of Hive Pig Jobs Analyzing system failures identifying root causes and recommended course of actions as part of operations support Involved in Spark streaming solution for the time sensitive revenue generating reports to match the pace with upstream STB systems data Experience in working on Hbase with Apache phoenix as a data layer to serve the web requests to meet the SLA requirements Created HBASE tables and load the data using sparkstreaming application Worked on SFDC ODATA connector to get the data from NodeJS services which in turns the fetch the data from HBASE Utilized AWS S3 services to pushstore and pull the data from AWS from external applications Responsible for functional requirements gathering code reviews deployment scripts and procedures offshore coordination and ontime deliverables Environment Hadoop HDFS Pig Hive Flume Kafka MapReduce Sqoop Spark Oozie LINUX NodeJS SFDC ODATA and AWS Hadoop Developer Dun Bradstreet Parsippany NJ August 2014 to January 2015 Dun Bradstreet D B provides commercial data to businesses on credit history businesstobusiness sales and marketing counterparty risk exposure supply chain management lead scoring and social identity matching It maintains a database contains information on more than 235 million companies across 200 countries worldwide This database contains over 53 million professional contact names using a variety of sources including public records trade references telco providers telephone interviews print digital and trade publications Roles and Responsibilities Involved in designing and developing the Cascading work flows Actively participated in validating and cleansing phases of the data flow Worked with the Data Science team to gather requirements for various data transformations Involved in writing a new subassembly for a complex query used for lookup the data Worked on custom functions and subassemblies which helps for code reuse Involved in running cascading job in local and Hadoop modes Worked with application teams to install VM HBASE Hadoop updates patches version upgrades as required Created and tested JUNIT test cases for different Cascading flows Worked on Tableau workbooks to perform year over year quarter over quarter YTD QTD and MTD type of analysis Utilized Tableau server to publish and share the reports with the business users Idea on continuous integration tools like Jenkins to automate the process for code builds and deployments Environment Hadoop HDFS Cascading Lingual HBASE REDIS Zookeeper JUnit Jenkins Gradle Tableau JavaHadoop Developer Tata Consultancy Services Hyderabad Telangana December 2011 to July 2013 India Client Verizon NY Verizon is one of the worlds leading providers of highgrowth communications services Verizon is the nations largest local exchange carrier and a rapidly emerging long distance provider Verizon companies are the largest providers of wired line and wireless communications in the United States serving the equivalent of nearly 135 million access lines and 30 million wireless customers Responsibilities Created Temporary Tables to store the data from Legacy system Used SQLLoader scripts to load the data into temporary tables and procedures to validate the data Creation of Materialized views and function based indexes Setting up various notification scripts which will notify if the database is not up and running or if any directory space is filled up Used Visual SourceSafe for version control and performing various builds Involved in Unit testing User Acceptance Testing to check whether the data is loading into target Extracted files from DB2 through Sqoop and placed in HDFS and processed Extracted from different source systems according to the user requirements Involved in POC for analyzing large data sets by running Hive queries and Pig scripts Worked with the Data Science team to gather requirements for various data mining projects Worked on Hive tables creation and loading and analyzing data using hive queries Analyzed large data sets by running Hive queries and Pig scripts Worked with the Data Science team to gather requirements for various data mining projects Created and maintained Technical documentation for launching HADOOP Clusters and for executing Hive queries and Pig Scripts Providing daily development status weekly status reports and weekly development summary and defects report Environment Core Java JDBC JavaScript MySQL JUnit Eclipse QA Hadoop Sqoop Hive Pig JavaJ2EE Developer Zensar Technologies Hyderabad Telangana April 2010 to November 2011 India Client Assurant Solutions Miami FL Assurant Solutions is one of the leading Auto Insurance companies in the US Assurant has the various applications within that XYCOR NonBill and Special Products application comes under Insurance and financial application sector These application mainly deals with Credit insurance Credit Insurance is an insurance policy and risk management product that covers the payment risk in the event a borrower is unable to repay because of certain events Roles and Responsibilities Utilized Agile Methodologies to manage full lifecycle development of the project Developed front end validations using JavaScript and developed design and layouts of JSPs and custom taglibs for all JSPs Used JDBC for database connectivity Developed web application using JSP custom tag libraries and Action Designed Java Servlets and Objects using J2EE standards Used JSP for presentation layer developed high performance objectrelational persistence and query service for entire application Developed the XML Schema and Web services for the data maintenance and structures Used WebLogic Application Server and RAD to develop and deploy the application Worked with various Style Sheets like Cascading Style Sheets CSS Designed database and created tables written the complex SQL Queries and stored procedures as per the requirements Involved in coding for JUnit Test cases ANT for building the application Environment JavaJ2EE Oracle 10 g SQL PLSQL JSP EJB WebLogic 80 HTML AJAX Java Script JDBC XML JMS JUnit log4j MyEclipse 60 JavaJ2EE Developer Satyam Computer Services Ltd Hyderabad Telangana August 2009 to April 2010 India Client Central Bancompany Jefferson City MO Worked on application named Access portal It is a part of online banking that allows a customer to view quick summary of transactions and account details It also shows mutual funds associated with account Roles and Responsibilities Responsible for understanding the scope of the project and requirement gathering Review and analyze the design and implementation of software componentsapplications and outline the development process strategies Coordinate with Project managers Development and QA teams during the course of the project Used Spring JDBC to write some DAO classes to interact with the database to access account information Used Tomcat web server for development purpose Involved in creation of Test Cases for JUnit Testing Used Oracle as Database and used Toad for queries execution and also involved in writing SQL scripts PLSQL code for procedures and functions Used CVS Perforce as configuration management tool for code versioning and release Developed application using Eclipse and used build and deploy tool as Maven Used Log4J to print the logging debugging warning info on the server console Environment Java15 J2EE Servlet JSP XML Spring 30 Design Patterns Log4j CVS Maven Eclipse Apache Tomcat 6 and Oracle 11 g Java Developer Satyam Computer Services Ltd Hyderabad Telangana September 2008 to July 2009 India Client OneBeacon Insurance Group Minnetonka MN Disability Income Application is used for the Underwriting and Administration of Disabilities products This is a Maintenance and Enhancement Project for Individual and Institutional policies of Metlife Responsibilities Coded the business methods according to the IBM Rational Rose UML model Extensively used Core Java Servlets JSP and XML Used DB2 Database to store the system data Used Rational Application Developer RAD as Integrated Development Environment IDE Used unit testing for all the components using JUnit Used Apache log 4j Logging framework for logging of trace and Auditing Used Asynchronous JavaScript and XML AJAX for better and faster interactive FrontEnd Provide support to resolve performance testing issues profiling and cache mechanism Performs code reviews to ensure consistency to style standards and code quality Environment Java 16 Servlets JSP IBM Rational Application Developer RAD 6 Websphere 60 iText AJAX DB2 log4j JavaJ2EE Developer Satyam Computer Services Ltd Hyderabad Telangana October 2007 to August 2008 India Client Lowes Mooresville North Carolina Lowes Companies Inc is a USbased chain of retail home improvement and appliance stores Founded in 1946 in North Wilkesboro North Carolina the chain now serves over 14 million customers a week in its 1616 stores in the United States and Canada The import project will provide and support overseas product and relationship growth as well as supported IT systems and processes for both sourcing item and vendor functions and purchase order management creating and managing POs Mainframes act as a host between PeopleSoft tables and Tradestone The objective of the project is to design develop and integrate Lowes Purchase Order System with TradeStone a Lowes web based application for PO maintenance Responsibilities Designed User Interface using Java Server Pages JSP and XML Developed the Enterprise Java Beans Stateless Stateful Session beans Entity beans to handle different transactions such as online funds transfer bill payments to the service providers Implemented Service Oriented Architecture SOA using JMS in MDB for sending and receiving messages while creating web services Worked on Web Services for data transfer from client to server and vice versa using SOAP WSDL and UDDI Involved in testing the web services using SOAP UI Extensively worked on JMS using pointpoint publishersubscriber messaging Domains for implementing Exchange of information through Messages Environment Windows Java 14 HTML JavaScript 16 XML JUnit JMS Web Services SOAP 11 UDDI 2 Maven 20 Eclipse IDE CVS Oracle 10 g Skills Eclipse J2ee Java Intellij idea Jsp Additional Information Around 11 years of experience in IT industry which includes around 6 years of experience in Big Data in implementing complete Hadoop solutions Architecture and Design Hands on experience in installing configuring and using Apache Hadoop ecosystem components like HDFS Hadoop MapReduce Zoo Keeper Oozie Hive Sqoop Kafka Spark Pig and Cascading Azkaban Airflow Big query Expertise in writing Hadoop Jobs for analyzing data using Hive and Pig Experience in working with MapReduce programs using Apache Hadoop for working with Big Data Experience in importing and exporting data using Sqoop from HDFS to Relational Database SystemsRDBMS and viceversa Experience in working with ETL tool Kettle by Pentaho Involved in developing Tableau Dashboards with interactive views trends and drill downs for the users data In depth understandingknowledge of Hadoop Architecture and various components such as HDFS Job Tracker Task Tracker Name Node Data Node and MapReduce concepts Worked on Cascading API for Hadoop application development and work flows Good understanding of Data Mining and Machine Learning techniques Experience in analyzing data using Hive QL Pig Latin and custom MapReduce programs in Java Knowledge of job workflow scheduling and monitoring tools like Oozie and Zookeeper Worked on Spark streaming application which consumes the data from Kafka and persist it in to HBASE Experience in optimization of Map reduce algorithm using combiners and partitioners to deliver the best results Involved in Spark solution for the time sensitive use cases Good understanding of NoSQL databases like MongoDB REDIS Expertise in core Java J2EE Multithreading JDBC Shell Scripting and proficient in using Java APIs for application development Proficient in Working with Various IDE tools including Eclipse Galileo IBM Rational Application Developer RAD and IntelliJ IDEA Worked on different operating systems like UNIXLinuxWindows XP and Windows 2 K Good knowledge on functional programming language Scala Integrate Salesforce using ODATA connector for Rest API Worked on NODEJS API to pull the data from HBASE Written Airflow jobs using inbuild and custom operators in Python to schedule and run the flows Very good experience in customer specification study requirements gathering system architectural design and turning the requirements into final product Strong background in mathematics and have very good analytical and problem solving skills Technical Expertise HadoopBig Data HDFS MapReduce Pig Hive Sqoop Oozie Zookeeper Cascading SPSS Kafka Flume Hbase Phoenix Spark Azkaban YARN Airflow Java J2EE technologies Core Java JSP JDBC Hadoop distributions Cloudera Hortonworks Google Cloud Platform IDE Tools Eclipse IntelliJ IDEA Programming languages C C Java Linux shell scripts VBNET COBOL Python Scala Databases Oracle 11g10g9i MySQL DB2 MSSQL Server MongoDB Big Query Web Technologies HTML XML JavaScript ETL Tools Kettle Reporting Tools Tableau Operating Systems Windows 95982000XPVista7 LINUX Monitoring Reporting Nagios Custom shell scripts Version control Git SVN Testing Tools JUnit MRUnit",
    "extracted_keywords": [
        "Senior",
        "Bigdata",
        "Hadoop",
        "Developer",
        "Senior",
        "Bigdata",
        "Hadoop",
        "span",
        "lDeveloperspan",
        "Senior",
        "Bigdata",
        "Hadoop",
        "Developer",
        "Kohls",
        "Work",
        "Experience",
        "Senior",
        "Bigdata",
        "Hadoop",
        "Developer",
        "Kohls",
        "Milpitas",
        "CA",
        "February",
        "Present",
        "Kohls",
        "department",
        "store",
        "retailing",
        "chain",
        "Kohls",
        "Marketing",
        "Transformation",
        "Program",
        "MTP",
        "Data",
        "Lake",
        "data",
        "Google",
        "Cloud",
        "platform",
        "data",
        "sources",
        "time",
        "data",
        "RDBMS",
        "data",
        "data",
        "technologies",
        "infrastructure",
        "Roles",
        "Responsibilities",
        "flows",
        "Java",
        "Hive",
        "Pig",
        "shell",
        "scripts",
        "data",
        "GCP",
        "Google",
        "Cloud",
        "Platform",
        "sources",
        "server",
        "logs",
        "Mount",
        "point",
        "server",
        "files",
        "time",
        "data",
        "Mobile",
        "applications",
        "Google",
        "Cloud",
        "Platform",
        "Data",
        "Lake",
        "Designing",
        "Row",
        "Schema",
        "NoSQL",
        "databases",
        "HBase",
        "volumes",
        "data",
        "time",
        "transactions",
        "MapReduce",
        "Jobs",
        "Java",
        "API",
        "Pig",
        "Hive",
        "data",
        "extraction",
        "transformation",
        "aggregation",
        "file",
        "formats",
        "Parquet",
        "Avro",
        "XML",
        "JSON",
        "CSV",
        "file",
        "format",
        "Codecs",
        "gzip",
        "Snappy",
        "LZO",
        "Building",
        "applications",
        "use",
        "Spark",
        "SQL",
        "Big",
        "Query",
        "detail",
        "level",
        "summary",
        "reports",
        "Dashboard",
        "KPIs",
        "Working",
        "SparkStreaming",
        "APIs",
        "Data",
        "frames",
        "transformations",
        "actions",
        "fly",
        "learner",
        "data",
        "model",
        "data",
        "Kafka",
        "time",
        "persists",
        "Google",
        "Cloud",
        "Platform",
        "GCP",
        "Big",
        "Query",
        "Building",
        "applications",
        "Jenkins",
        "Tonomi",
        "configurations",
        "Spark",
        "jobs",
        "executors",
        "memory",
        "configurations",
        "DStreams",
        "Accumulator",
        "Broadcast",
        "variables",
        "results",
        "datasets",
        "scripts",
        "Github",
        "version",
        "control",
        "repository",
        "service",
        "code",
        "Jenkins",
        "Creating",
        "apache",
        "Airflow",
        "jobs",
        "Python",
        "language",
        "orchestration",
        "purposes",
        "design",
        "code",
        "reviews",
        "test",
        "teams",
        "testing",
        "issues",
        "applications",
        "business",
        "requirements",
        "user",
        "stories",
        "JIRA",
        "documentation",
        "Confluence",
        "Environment",
        "Hortonworks",
        "Data",
        "Platform",
        "HDP",
        "HDFS",
        "MapReduce",
        "YARN",
        "HBase",
        "Java",
        "Hive",
        "Pig",
        "Sqoop",
        "Spark",
        "Parquet",
        "Flume",
        "GCP",
        "Google",
        "Cloud",
        "Platform",
        "query",
        "Python",
        "Scala",
        "Airflow",
        "Kafka",
        "Senior",
        "Hadoop",
        "Developer",
        "ATT",
        "El",
        "Segundo",
        "CA",
        "February",
        "January",
        "part",
        "ATT",
        "television",
        "services",
        "subscribers",
        "satellite",
        "transmissions",
        "Services",
        "equivalent",
        "television",
        "stations",
        "television",
        "networks",
        "subscription",
        "television",
        "services",
        "satellite",
        "radio",
        "services",
        "video",
        "services",
        "AMS",
        "system",
        "DIRECTV",
        "information",
        "events",
        "Set",
        "Top",
        "Box",
        "STB",
        "Event",
        "information",
        "DVR",
        "usage",
        "playback",
        "application",
        "usage",
        "CA",
        "OSD",
        "events",
        "advertising",
        "data",
        "information",
        "Roles",
        "Responsibilities",
        "applications",
        "ingestion",
        "reports",
        "delivery",
        "party",
        "vendors",
        "data",
        "technologies",
        "flume",
        "kafka",
        "sqoop",
        "mapreduce",
        "hive",
        "pig",
        "Process",
        "log",
        "files",
        "boxes",
        "java",
        "map",
        "code",
        "shell",
        "scripts",
        "text",
        "files",
        "HDFS",
        "data",
        "legacy",
        "systems",
        "HDFS",
        "apache",
        "Sqoop",
        "Flume",
        "map",
        "programs",
        "hive",
        "queries",
        "pig",
        "scripts",
        "reports",
        "Oozie",
        "workflow",
        "Hive",
        "queries",
        "operations",
        "team",
        "data",
        "Alert",
        "mechanism",
        "oozie",
        "jobs",
        "failure",
        "conditions",
        "conditions",
        "email",
        "notifications",
        "bluecoat",
        "proxy",
        "approach",
        "data",
        "inhouse",
        "Hadoop",
        "cluster",
        "vendors",
        "application",
        "teams",
        "operating",
        "system",
        "Hadoop",
        "updates",
        "version",
        "upgrades",
        "Writing",
        "Map",
        "code",
        "data",
        "data",
        "loading",
        "Hive",
        "tables",
        "performance",
        "tuning",
        "Hive",
        "Pig",
        "Jobs",
        "system",
        "failures",
        "root",
        "causes",
        "course",
        "actions",
        "part",
        "operations",
        "support",
        "Spark",
        "streaming",
        "solution",
        "time",
        "revenue",
        "generating",
        "reports",
        "pace",
        "STB",
        "systems",
        "data",
        "Experience",
        "Hbase",
        "Apache",
        "phoenix",
        "data",
        "layer",
        "web",
        "requests",
        "SLA",
        "requirements",
        "HBASE",
        "tables",
        "data",
        "application",
        "SFDC",
        "connector",
        "data",
        "NodeJS",
        "services",
        "fetch",
        "data",
        "HBASE",
        "AWS",
        "S3",
        "services",
        "data",
        "AWS",
        "applications",
        "requirements",
        "code",
        "deployment",
        "scripts",
        "procedures",
        "coordination",
        "ontime",
        "deliverables",
        "Environment",
        "Hadoop",
        "HDFS",
        "Pig",
        "Hive",
        "Flume",
        "Kafka",
        "MapReduce",
        "Sqoop",
        "Spark",
        "Oozie",
        "LINUX",
        "NodeJS",
        "SFDC",
        "ODATA",
        "AWS",
        "Hadoop",
        "Developer",
        "Dun",
        "Bradstreet",
        "Parsippany",
        "NJ",
        "August",
        "January",
        "Dun",
        "Bradstreet",
        "D",
        "B",
        "data",
        "businesses",
        "credit",
        "history",
        "sales",
        "marketing",
        "counterparty",
        "risk",
        "exposure",
        "supply",
        "chain",
        "management",
        "lead",
        "scoring",
        "identity",
        "matching",
        "database",
        "information",
        "companies",
        "countries",
        "database",
        "contact",
        "names",
        "variety",
        "sources",
        "records",
        "trade",
        "references",
        "providers",
        "telephone",
        "interviews",
        "trade",
        "publications",
        "Roles",
        "Responsibilities",
        "work",
        "flows",
        "phases",
        "data",
        "flow",
        "Data",
        "Science",
        "team",
        "requirements",
        "data",
        "transformations",
        "query",
        "lookup",
        "data",
        "custom",
        "functions",
        "subassemblies",
        "code",
        "reuse",
        "job",
        "Hadoop",
        "modes",
        "application",
        "teams",
        "VM",
        "HBASE",
        "Hadoop",
        "updates",
        "version",
        "upgrades",
        "Created",
        "JUNIT",
        "test",
        "cases",
        "Cascading",
        "flows",
        "Tableau",
        "workbooks",
        "year",
        "year",
        "quarter",
        "quarter",
        "YTD",
        "QTD",
        "MTD",
        "type",
        "analysis",
        "Tableau",
        "server",
        "reports",
        "business",
        "users",
        "Idea",
        "integration",
        "tools",
        "Jenkins",
        "process",
        "code",
        "builds",
        "deployments",
        "Environment",
        "Hadoop",
        "HDFS",
        "Cascading",
        "Lingual",
        "HBASE",
        "REDIS",
        "Zookeeper",
        "JUnit",
        "Jenkins",
        "Gradle",
        "Tableau",
        "JavaHadoop",
        "Developer",
        "Tata",
        "Consultancy",
        "Services",
        "Hyderabad",
        "Telangana",
        "December",
        "July",
        "India",
        "Client",
        "Verizon",
        "NY",
        "Verizon",
        "worlds",
        "providers",
        "highgrowth",
        "communications",
        "services",
        "Verizon",
        "nations",
        "exchange",
        "carrier",
        "distance",
        "provider",
        "Verizon",
        "companies",
        "providers",
        "line",
        "communications",
        "United",
        "States",
        "equivalent",
        "access",
        "lines",
        "customers",
        "Responsibilities",
        "Temporary",
        "Tables",
        "data",
        "Legacy",
        "system",
        "SQLLoader",
        "scripts",
        "data",
        "tables",
        "procedures",
        "data",
        "Creation",
        "Materialized",
        "views",
        "function",
        "indexes",
        "notification",
        "scripts",
        "database",
        "directory",
        "space",
        "Visual",
        "SourceSafe",
        "version",
        "control",
        "builds",
        "Unit",
        "User",
        "Acceptance",
        "Testing",
        "data",
        "target",
        "files",
        "DB2",
        "Sqoop",
        "HDFS",
        "source",
        "systems",
        "user",
        "requirements",
        "POC",
        "data",
        "sets",
        "Hive",
        "queries",
        "Pig",
        "scripts",
        "Data",
        "Science",
        "team",
        "requirements",
        "data",
        "mining",
        "projects",
        "Hive",
        "tables",
        "creation",
        "loading",
        "data",
        "hive",
        "queries",
        "data",
        "sets",
        "Hive",
        "queries",
        "Pig",
        "scripts",
        "Data",
        "Science",
        "team",
        "requirements",
        "data",
        "mining",
        "projects",
        "documentation",
        "HADOOP",
        "Clusters",
        "Hive",
        "queries",
        "Pig",
        "Scripts",
        "development",
        "status",
        "status",
        "reports",
        "development",
        "summary",
        "defects",
        "Environment",
        "Core",
        "Java",
        "JDBC",
        "JavaScript",
        "MySQL",
        "JUnit",
        "Eclipse",
        "QA",
        "Hadoop",
        "Sqoop",
        "Hive",
        "Pig",
        "JavaJ2EE",
        "Developer",
        "Zensar",
        "Technologies",
        "Hyderabad",
        "Telangana",
        "April",
        "November",
        "India",
        "Client",
        "Assurant",
        "Solutions",
        "Miami",
        "FL",
        "Assurant",
        "Solutions",
        "Auto",
        "Insurance",
        "companies",
        "US",
        "Assurant",
        "applications",
        "XYCOR",
        "NonBill",
        "Special",
        "Products",
        "application",
        "Insurance",
        "application",
        "sector",
        "application",
        "Credit",
        "insurance",
        "Credit",
        "Insurance",
        "insurance",
        "policy",
        "risk",
        "management",
        "product",
        "payment",
        "risk",
        "event",
        "borrower",
        "events",
        "Roles",
        "Responsibilities",
        "Agile",
        "Methodologies",
        "lifecycle",
        "development",
        "project",
        "end",
        "validations",
        "JavaScript",
        "design",
        "layouts",
        "JSPs",
        "custom",
        "taglibs",
        "JSPs",
        "JDBC",
        "database",
        "connectivity",
        "web",
        "application",
        "JSP",
        "custom",
        "tag",
        "libraries",
        "Action",
        "Java",
        "Servlets",
        "Objects",
        "J2EE",
        "standards",
        "JSP",
        "presentation",
        "layer",
        "performance",
        "persistence",
        "query",
        "service",
        "application",
        "XML",
        "Schema",
        "Web",
        "services",
        "data",
        "maintenance",
        "structures",
        "WebLogic",
        "Application",
        "Server",
        "RAD",
        "application",
        "Style",
        "Sheets",
        "Style",
        "Sheets",
        "CSS",
        "database",
        "tables",
        "SQL",
        "Queries",
        "procedures",
        "requirements",
        "JUnit",
        "Test",
        "cases",
        "ANT",
        "application",
        "Environment",
        "JavaJ2EE",
        "Oracle",
        "g",
        "SQL",
        "PLSQL",
        "JSP",
        "EJB",
        "WebLogic",
        "HTML",
        "AJAX",
        "Java",
        "Script",
        "JDBC",
        "XML",
        "JMS",
        "JUnit",
        "log4j",
        "MyEclipse",
        "JavaJ2EE",
        "Developer",
        "Satyam",
        "Computer",
        "Services",
        "Ltd",
        "Hyderabad",
        "Telangana",
        "August",
        "April",
        "India",
        "Client",
        "Central",
        "Bancompany",
        "Jefferson",
        "City",
        "MO",
        "application",
        "Access",
        "portal",
        "part",
        "banking",
        "customer",
        "summary",
        "transactions",
        "account",
        "details",
        "funds",
        "account",
        "Roles",
        "Responsibilities",
        "scope",
        "project",
        "requirement",
        "Review",
        "design",
        "implementation",
        "software",
        "componentsapplications",
        "development",
        "process",
        "strategies",
        "Coordinate",
        "Project",
        "managers",
        "Development",
        "QA",
        "teams",
        "course",
        "project",
        "Spring",
        "JDBC",
        "DAO",
        "classes",
        "database",
        "access",
        "account",
        "information",
        "Tomcat",
        "web",
        "server",
        "development",
        "purpose",
        "creation",
        "Test",
        "Cases",
        "JUnit",
        "Testing",
        "Oracle",
        "Database",
        "Toad",
        "queries",
        "execution",
        "SQL",
        "scripts",
        "PLSQL",
        "code",
        "procedures",
        "functions",
        "CVS",
        "Perforce",
        "configuration",
        "management",
        "tool",
        "code",
        "versioning",
        "application",
        "Eclipse",
        "build",
        "tool",
        "Maven",
        "Log4J",
        "warning",
        "info",
        "server",
        "console",
        "Environment",
        "Java15",
        "J2EE",
        "Servlet",
        "JSP",
        "XML",
        "Spring",
        "Design",
        "Patterns",
        "Log4j",
        "CVS",
        "Maven",
        "Eclipse",
        "Apache",
        "Tomcat",
        "Oracle",
        "g",
        "Java",
        "Developer",
        "Satyam",
        "Computer",
        "Services",
        "Ltd",
        "Hyderabad",
        "Telangana",
        "September",
        "July",
        "India",
        "Client",
        "OneBeacon",
        "Insurance",
        "Group",
        "Minnetonka",
        "MN",
        "Disability",
        "Income",
        "Application",
        "Underwriting",
        "Administration",
        "Disabilities",
        "products",
        "Maintenance",
        "Enhancement",
        "Project",
        "policies",
        "Metlife",
        "Responsibilities",
        "business",
        "methods",
        "IBM",
        "Rational",
        "Rose",
        "UML",
        "model",
        "Core",
        "Java",
        "Servlets",
        "JSP",
        "XML",
        "DB2",
        "Database",
        "system",
        "data",
        "Rational",
        "Application",
        "Developer",
        "RAD",
        "Integrated",
        "Development",
        "Environment",
        "IDE",
        "unit",
        "testing",
        "components",
        "JUnit",
        "Used",
        "Apache",
        "4j",
        "framework",
        "trace",
        "Auditing",
        "Asynchronous",
        "JavaScript",
        "XML",
        "AJAX",
        "FrontEnd",
        "support",
        "performance",
        "testing",
        "issues",
        "profiling",
        "cache",
        "mechanism",
        "code",
        "reviews",
        "consistency",
        "style",
        "standards",
        "code",
        "quality",
        "Environment",
        "Java",
        "Servlets",
        "JSP",
        "IBM",
        "Rational",
        "Application",
        "Developer",
        "RAD",
        "Websphere",
        "iText",
        "AJAX",
        "DB2",
        "log4j",
        "JavaJ2EE",
        "Developer",
        "Satyam",
        "Computer",
        "Services",
        "Ltd",
        "Hyderabad",
        "Telangana",
        "October",
        "August",
        "India",
        "Client",
        "Lowes",
        "Mooresville",
        "North",
        "Carolina",
        "Lowes",
        "Companies",
        "Inc",
        "USbased",
        "chain",
        "home",
        "improvement",
        "appliance",
        "stores",
        "North",
        "Wilkesboro",
        "North",
        "Carolina",
        "chain",
        "customers",
        "week",
        "stores",
        "United",
        "States",
        "Canada",
        "import",
        "project",
        "product",
        "relationship",
        "growth",
        "IT",
        "systems",
        "processes",
        "item",
        "vendor",
        "functions",
        "purchase",
        "order",
        "management",
        "POs",
        "Mainframes",
        "host",
        "PeopleSoft",
        "tables",
        "objective",
        "project",
        "Lowes",
        "Purchase",
        "Order",
        "System",
        "TradeStone",
        "Lowes",
        "web",
        "application",
        "PO",
        "maintenance",
        "Responsibilities",
        "User",
        "Interface",
        "Java",
        "Server",
        "Pages",
        "JSP",
        "XML",
        "Enterprise",
        "Java",
        "Beans",
        "Stateless",
        "Stateful",
        "Session",
        "Entity",
        "beans",
        "transactions",
        "funds",
        "transfer",
        "bill",
        "payments",
        "service",
        "providers",
        "Service",
        "Oriented",
        "Architecture",
        "SOA",
        "JMS",
        "MDB",
        "messages",
        "web",
        "services",
        "Web",
        "Services",
        "data",
        "transfer",
        "client",
        "SOAP",
        "WSDL",
        "UDDI",
        "web",
        "services",
        "SOAP",
        "UI",
        "JMS",
        "pointpoint",
        "publishersubscriber",
        "Domains",
        "Exchange",
        "information",
        "Messages",
        "Environment",
        "Windows",
        "Java",
        "HTML",
        "JavaScript",
        "XML",
        "JUnit",
        "JMS",
        "Web",
        "Services",
        "SOAP",
        "UDDI",
        "Maven",
        "Eclipse",
        "IDE",
        "CVS",
        "Oracle",
        "g",
        "Skills",
        "Eclipse",
        "J2ee",
        "Java",
        "Intellij",
        "idea",
        "Jsp",
        "Additional",
        "Information",
        "years",
        "experience",
        "IT",
        "industry",
        "years",
        "experience",
        "Big",
        "Data",
        "Hadoop",
        "solutions",
        "Architecture",
        "Design",
        "Hands",
        "experience",
        "configuring",
        "Apache",
        "Hadoop",
        "ecosystem",
        "components",
        "HDFS",
        "Hadoop",
        "MapReduce",
        "Zoo",
        "Keeper",
        "Oozie",
        "Hive",
        "Sqoop",
        "Kafka",
        "Spark",
        "Pig",
        "Azkaban",
        "Airflow",
        "query",
        "Expertise",
        "Hadoop",
        "Jobs",
        "data",
        "Hive",
        "Pig",
        "Experience",
        "MapReduce",
        "programs",
        "Apache",
        "Hadoop",
        "Big",
        "Data",
        "Experience",
        "data",
        "Sqoop",
        "HDFS",
        "Relational",
        "Database",
        "viceversa",
        "Experience",
        "ETL",
        "tool",
        "Kettle",
        "Pentaho",
        "Tableau",
        "Dashboards",
        "views",
        "trends",
        "downs",
        "users",
        "data",
        "depth",
        "understandingknowledge",
        "Hadoop",
        "Architecture",
        "components",
        "HDFS",
        "Job",
        "Tracker",
        "Task",
        "Tracker",
        "Name",
        "Node",
        "Data",
        "Node",
        "MapReduce",
        "concepts",
        "API",
        "Hadoop",
        "application",
        "development",
        "work",
        "understanding",
        "Data",
        "Mining",
        "Machine",
        "Learning",
        "techniques",
        "Experience",
        "data",
        "Hive",
        "QL",
        "Pig",
        "Latin",
        "MapReduce",
        "programs",
        "Java",
        "Knowledge",
        "job",
        "workflow",
        "scheduling",
        "monitoring",
        "tools",
        "Oozie",
        "Zookeeper",
        "Worked",
        "Spark",
        "streaming",
        "application",
        "data",
        "Kafka",
        "HBASE",
        "Experience",
        "optimization",
        "Map",
        "algorithm",
        "combiners",
        "partitioners",
        "results",
        "Spark",
        "solution",
        "time",
        "use",
        "cases",
        "understanding",
        "MongoDB",
        "REDIS",
        "Expertise",
        "core",
        "Java",
        "J2EE",
        "Multithreading",
        "JDBC",
        "Shell",
        "Scripting",
        "Java",
        "APIs",
        "application",
        "development",
        "Proficient",
        "Working",
        "IDE",
        "tools",
        "Eclipse",
        "Galileo",
        "IBM",
        "Rational",
        "Application",
        "Developer",
        "RAD",
        "IntelliJ",
        "IDEA",
        "operating",
        "systems",
        "XP",
        "Windows",
        "knowledge",
        "programming",
        "language",
        "Scala",
        "Integrate",
        "Salesforce",
        "connector",
        "Rest",
        "API",
        "NODEJS",
        "API",
        "data",
        "HBASE",
        "Airflow",
        "jobs",
        "custom",
        "operators",
        "Python",
        "flows",
        "experience",
        "customer",
        "specification",
        "study",
        "requirements",
        "system",
        "design",
        "requirements",
        "product",
        "background",
        "mathematics",
        "problem",
        "skills",
        "Technical",
        "Expertise",
        "HadoopBig",
        "Data",
        "HDFS",
        "MapReduce",
        "Pig",
        "Hive",
        "Sqoop",
        "Oozie",
        "Zookeeper",
        "Cascading",
        "SPSS",
        "Kafka",
        "Flume",
        "Hbase",
        "Phoenix",
        "Spark",
        "Azkaban",
        "YARN",
        "Airflow",
        "Java",
        "J2EE",
        "technologies",
        "Core",
        "Java",
        "JSP",
        "JDBC",
        "Hadoop",
        "distributions",
        "Cloudera",
        "Hortonworks",
        "Google",
        "Cloud",
        "Platform",
        "IDE",
        "Tools",
        "Eclipse",
        "IntelliJ",
        "IDEA",
        "Programming",
        "C",
        "C",
        "Java",
        "Linux",
        "shell",
        "VBNET",
        "COBOL",
        "Python",
        "Scala",
        "Oracle",
        "MySQL",
        "DB2",
        "MSSQL",
        "Server",
        "MongoDB",
        "Big",
        "Query",
        "Web",
        "Technologies",
        "HTML",
        "XML",
        "JavaScript",
        "ETL",
        "Tools",
        "Kettle",
        "Reporting",
        "Tools",
        "Tableau",
        "Operating",
        "Systems",
        "Windows",
        "95982000XPVista7",
        "LINUX",
        "Monitoring",
        "Nagios",
        "Custom",
        "shell",
        "Version",
        "control",
        "Git",
        "SVN",
        "Testing",
        "Tools",
        "JUnit",
        "MRUnit"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T19:44:26.678437",
    "resume_data": "Senior Bigdata Hadoop Developer Senior Bigdata Hadoop span lDeveloperspan Senior Bigdata Hadoop Developer Kohls Work Experience Senior Bigdata Hadoop Developer Kohls Milpitas CA February 2018 to Present Kohls is an American department store retailing chain Kohls started Marketing Transformation Program MTP to create Data Lake for all the data in Google Cloud platform Ingesting the data from various sources files real time data RDBMS and processing the data using Big data technologies and infrastructure Roles and Responsibilities Creating and executing Azkaban flows using Java Hive Pig shell scripts to ingest the data into GCP Google Cloud Platform from various sources such as server logs Mount point server files and Real time data from Mobile applications to utilize Google Cloud Platform as Data Lake Designing Row keys Schema with NoSQL databases such as HBase to load huge volumes of data for real time transactions Writing multiple MapReduce Jobs using Java API Pig and Hive for data extraction transformation and aggregation from multiple file formats including Parquet Avro XML JSON CSV ORCFILE and other compressed file format Codecs like gzip Snappy LZO Building advanced analytical applications by making use of Spark SQL Big Query to create detail level summary reports and Dashboard using KPIs Working on SparkStreaming APIs using Data frames to perform necessary transformations and actions on the fly for building the common learner data model which gets the data from Kafka in near real time and persists into Google Cloud Platform GCP and Big Query Building and deploying the applications using Jenkins and Tonomi with appropriate configurations Optimizing Spark jobs by tuning executors memory configurations DStreams Accumulator Broadcast variables RDD caching to deliver the best results for the large datasets Responsible for deploying the scripts into Github version control repository hosting service and deployed the code using Jenkins Creating and executing apache Airflow jobs in Python language for orchestration purposes Involving in design code reviews and supporting test teams in testing to fix the identified issues in the developed applications Involved in business and functional requirements gathering and prepared user stories in JIRA and documentation in Confluence Environment Hortonworks Data Platform HDP HDFS MapReduce YARN HBase Java Hive Pig Sqoop Spark Parquet Flume GCP Google Cloud Platform Azkaban Big query Python Scala Airflow Kafka Senior Hadoop Developer ATT El Segundo CA February 2015 to January 2018 DirecTV Now part of ATT provides television and audio services to subscribers through satellite transmissions Services include the equivalent of many local television stations broadcast television networks subscription television services satellite radio services and private video services AMS is a system which enables in DIRECTV set to collect information about events that occur on the Set Top Box STB for later reporting Event information includes live viewing DVR usage playback and record interactive application usage CA OSD events advertising data and diagnostic information Roles and Responsibilities Designing the applications from the ingestion to reports delivery to third party vendors using big data technologies flume kafka sqoop mapreduce hive pig Process the raw log files from the set top boxes using java map reduce code and shell scripts and stored them as text files in HDFS Ingesting the data from legacy and upstream systems to HDFS using apache Sqoop Flume java map reduce programs hive queries and pig scripts Generating the required reports using Oozie workflow and Hive queries for operations team from the ingested data Alert and monitoring mechanism for the oozie jobs for the failure conditions and successful conditions using email notifications Involved in bluecoat proxy approach while sharing the data from the inhouse Hadoop cluster to external vendors Working with application teams to install operating system Hadoop updates patches version upgrades as required Writing Map Reduce code to make unstructured and semi structured data into structured data and loading into Hive tables Worked on debugging performance tuning of Hive Pig Jobs Analyzing system failures identifying root causes and recommended course of actions as part of operations support Involved in Spark streaming solution for the time sensitive revenue generating reports to match the pace with upstream STB systems data Experience in working on Hbase with Apache phoenix as a data layer to serve the web requests to meet the SLA requirements Created HBASE tables and load the data using sparkstreaming application Worked on SFDC ODATA connector to get the data from NodeJS services which in turns the fetch the data from HBASE Utilized AWS S3 services to pushstore and pull the data from AWS from external applications Responsible for functional requirements gathering code reviews deployment scripts and procedures offshore coordination and ontime deliverables Environment Hadoop HDFS Pig Hive Flume Kafka MapReduce Sqoop Spark Oozie LINUX NodeJS SFDC ODATA and AWS Hadoop Developer Dun Bradstreet Parsippany NJ August 2014 to January 2015 Dun Bradstreet D B provides commercial data to businesses on credit history businesstobusiness sales and marketing counterparty risk exposure supply chain management lead scoring and social identity matching It maintains a database contains information on more than 235 million companies across 200 countries worldwide This database contains over 53 million professional contact names using a variety of sources including public records trade references telco providers telephone interviews print digital and trade publications Roles and Responsibilities Involved in designing and developing the Cascading work flows Actively participated in validating and cleansing phases of the data flow Worked with the Data Science team to gather requirements for various data transformations Involved in writing a new subassembly for a complex query used for lookup the data Worked on custom functions and subassemblies which helps for code reuse Involved in running cascading job in local and Hadoop modes Worked with application teams to install VM HBASE Hadoop updates patches version upgrades as required Created and tested JUNIT test cases for different Cascading flows Worked on Tableau workbooks to perform year over year quarter over quarter YTD QTD and MTD type of analysis Utilized Tableau server to publish and share the reports with the business users Idea on continuous integration tools like Jenkins to automate the process for code builds and deployments Environment Hadoop HDFS Cascading Lingual HBASE REDIS Zookeeper JUnit Jenkins Gradle Tableau JavaHadoop Developer Tata Consultancy Services Hyderabad Telangana December 2011 to July 2013 India Client Verizon NY Verizon is one of the worlds leading providers of highgrowth communications services Verizon is the nations largest local exchange carrier and a rapidly emerging long distance provider Verizon companies are the largest providers of wired line and wireless communications in the United States serving the equivalent of nearly 135 million access lines and 30 million wireless customers Responsibilities Created Temporary Tables to store the data from Legacy system Used SQLLoader scripts to load the data into temporary tables and procedures to validate the data Creation of Materialized views and function based indexes Setting up various notification scripts which will notify if the database is not up and running or if any directory space is filled up Used Visual SourceSafe for version control and performing various builds Involved in Unit testing User Acceptance Testing to check whether the data is loading into target Extracted files from DB2 through Sqoop and placed in HDFS and processed Extracted from different source systems according to the user requirements Involved in POC for analyzing large data sets by running Hive queries and Pig scripts Worked with the Data Science team to gather requirements for various data mining projects Worked on Hive tables creation and loading and analyzing data using hive queries Analyzed large data sets by running Hive queries and Pig scripts Worked with the Data Science team to gather requirements for various data mining projects Created and maintained Technical documentation for launching HADOOP Clusters and for executing Hive queries and Pig Scripts Providing daily development status weekly status reports and weekly development summary and defects report Environment Core Java JDBC JavaScript MySQL JUnit Eclipse QA Hadoop Sqoop Hive Pig JavaJ2EE Developer Zensar Technologies Hyderabad Telangana April 2010 to November 2011 India Client Assurant Solutions Miami FL Assurant Solutions is one of the leading Auto Insurance companies in the US Assurant has the various applications within that XYCOR NonBill and Special Products application comes under Insurance and financial application sector These application mainly deals with Credit insurance Credit Insurance is an insurance policy and risk management product that covers the payment risk in the event a borrower is unable to repay because of certain events Roles and Responsibilities Utilized Agile Methodologies to manage full lifecycle development of the project Developed front end validations using JavaScript and developed design and layouts of JSPs and custom taglibs for all JSPs Used JDBC for database connectivity Developed web application using JSP custom tag libraries and Action Designed Java Servlets and Objects using J2EE standards Used JSP for presentation layer developed high performance objectrelational persistence and query service for entire application Developed the XML Schema and Web services for the data maintenance and structures Used WebLogic Application Server and RAD to develop and deploy the application Worked with various Style Sheets like Cascading Style Sheets CSS Designed database and created tables written the complex SQL Queries and stored procedures as per the requirements Involved in coding for JUnit Test cases ANT for building the application Environment JavaJ2EE Oracle 10g SQL PLSQL JSP EJB WebLogic 80 HTML AJAX Java Script JDBC XML JMS JUnit log4j MyEclipse 60 JavaJ2EE Developer Satyam Computer Services Ltd Hyderabad Telangana August 2009 to April 2010 India Client Central Bancompany Jefferson City MO Worked on application named Access portal It is a part of online banking that allows a customer to view quick summary of transactions and account details It also shows mutual funds associated with account Roles and Responsibilities Responsible for understanding the scope of the project and requirement gathering Review and analyze the design and implementation of software componentsapplications and outline the development process strategies Coordinate with Project managers Development and QA teams during the course of the project Used Spring JDBC to write some DAO classes to interact with the database to access account information Used Tomcat web server for development purpose Involved in creation of Test Cases for JUnit Testing Used Oracle as Database and used Toad for queries execution and also involved in writing SQL scripts PLSQL code for procedures and functions Used CVS Perforce as configuration management tool for code versioning and release Developed application using Eclipse and used build and deploy tool as Maven Used Log4J to print the logging debugging warning info on the server console Environment Java15 J2EE Servlet JSP XML Spring 30 Design Patterns Log4j CVS Maven Eclipse Apache Tomcat 6 and Oracle 11g Java Developer Satyam Computer Services Ltd Hyderabad Telangana September 2008 to July 2009 India Client OneBeacon Insurance Group Minnetonka MN Disability Income Application is used for the Underwriting and Administration of Disabilities products This is a Maintenance and Enhancement Project for Individual and Institutional policies of Metlife Responsibilities Coded the business methods according to the IBM Rational Rose UML model Extensively used Core Java Servlets JSP and XML Used DB2 Database to store the system data Used Rational Application Developer RAD as Integrated Development Environment IDE Used unit testing for all the components using JUnit Used Apache log 4j Logging framework for logging of trace and Auditing Used Asynchronous JavaScript and XML AJAX for better and faster interactive FrontEnd Provide support to resolve performance testing issues profiling and cache mechanism Performs code reviews to ensure consistency to style standards and code quality Environment Java 16 Servlets JSP IBM Rational Application Developer RAD 6 Websphere 60 iText AJAX DB2 log4j JavaJ2EE Developer Satyam Computer Services Ltd Hyderabad Telangana October 2007 to August 2008 India Client Lowes Mooresville North Carolina Lowes Companies Inc is a USbased chain of retail home improvement and appliance stores Founded in 1946 in North Wilkesboro North Carolina the chain now serves over 14 million customers a week in its 1616 stores in the United States and Canada The import project will provide and support overseas product and relationship growth as well as supported IT systems and processes for both sourcing item and vendor functions and purchase order management creating and managing POs Mainframes act as a host between PeopleSoft tables and Tradestone The objective of the project is to design develop and integrate Lowes Purchase Order System with TradeStone a Lowes web based application for PO maintenance Responsibilities Designed User Interface using Java Server Pages JSP and XML Developed the Enterprise Java Beans Stateless Stateful Session beans Entity beans to handle different transactions such as online funds transfer bill payments to the service providers Implemented Service Oriented Architecture SOA using JMS in MDB for sending and receiving messages while creating web services Worked on Web Services for data transfer from client to server and vice versa using SOAP WSDL and UDDI Involved in testing the web services using SOAP UI Extensively worked on JMS using pointpoint publishersubscriber messaging Domains for implementing Exchange of information through Messages Environment Windows Java 14 HTML JavaScript 16 XML JUnit JMS Web Services SOAP 11 UDDI 2 Maven 20 Eclipse IDE CVS Oracle 10g Skills Eclipse J2ee Java Intellij idea Jsp Additional Information Around 11 years of experience in IT industry which includes around 6 years of experience in Big Data in implementing complete Hadoop solutions Architecture and Design Hands on experience in installing configuring and using Apache Hadoop ecosystem components like HDFS Hadoop MapReduce Zoo Keeper Oozie Hive Sqoop Kafka Spark Pig and Cascading Azkaban Airflow Big query Expertise in writing Hadoop Jobs for analyzing data using Hive and Pig Experience in working with MapReduce programs using Apache Hadoop for working with Big Data Experience in importing and exporting data using Sqoop from HDFS to Relational Database SystemsRDBMS and viceversa Experience in working with ETL tool Kettle by Pentaho Involved in developing Tableau Dashboards with interactive views trends and drill downs for the users data In depth understandingknowledge of Hadoop Architecture and various components such as HDFS Job Tracker Task Tracker Name Node Data Node and MapReduce concepts Worked on Cascading API for Hadoop application development and work flows Good understanding of Data Mining and Machine Learning techniques Experience in analyzing data using Hive QL Pig Latin and custom MapReduce programs in Java Knowledge of job workflow scheduling and monitoring tools like Oozie and Zookeeper Worked on Spark streaming application which consumes the data from Kafka and persist it in to HBASE Experience in optimization of Map reduce algorithm using combiners and partitioners to deliver the best results Involved in Spark solution for the time sensitive use cases Good understanding of NoSQL databases like MongoDB REDIS Expertise in core Java J2EE Multithreading JDBC Shell Scripting and proficient in using Java APIs for application development Proficient in Working with Various IDE tools including Eclipse Galileo IBM Rational Application Developer RAD and IntelliJ IDEA Worked on different operating systems like UNIXLinuxWindows XP and Windows 2K Good knowledge on functional programming language Scala Integrate Salesforce using ODATA connector for Rest API Worked on NODEJS API to pull the data from HBASE Written Airflow jobs using inbuild and custom operators in Python to schedule and run the flows Very good experience in customer specification study requirements gathering system architectural design and turning the requirements into final product Strong background in mathematics and have very good analytical and problem solving skills Technical Expertise HadoopBig Data HDFS MapReduce Pig Hive Sqoop Oozie Zookeeper Cascading SPSS Kafka Flume Hbase Phoenix Spark Azkaban YARN Airflow Java J2EE technologies Core Java JSP JDBC Hadoop distributions Cloudera Hortonworks Google Cloud Platform IDE Tools Eclipse IntelliJ IDEA Programming languages C C Java Linux shell scripts VBNET COBOL Python Scala Databases Oracle 11g10g9i MySQL DB2 MSSQL Server MongoDB Big Query Web Technologies HTML XML JavaScript ETL Tools Kettle Reporting Tools Tableau Operating Systems Windows 95982000XPVista7 LINUX Monitoring Reporting Nagios Custom shell scripts Version control Git SVN Testing Tools JUnit MRUnit",
    "unique_id": "e49f7145-029b-412b-a9a2-10eb5c53a231"
}