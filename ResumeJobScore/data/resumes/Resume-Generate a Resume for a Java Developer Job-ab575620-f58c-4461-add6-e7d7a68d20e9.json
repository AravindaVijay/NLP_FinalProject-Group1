{
    "clean_data": "Senior Spark DeveloperHadoop Developer Senior Spark span lDeveloperspanHadoop span lDeveloperspan Senior Spark DeveloperHadoop Developer Synchrony Financial Bank Atlanta GA Around 9years of experience in HadoopBig Data technologies such as in Hadoop Pig Hive HBase Oozie Zookeeper Sqoop Storm Flink Flume Zookeeper Impala Tez Kafka and Spark with hands on experience in writing Map ReduceYARN and SparkScala jobs Have good IT experience with special emphasis on Analysis Design and Development and Testing of ETL methodologies in all the phases of the Data Warehousing Expertise in OLTPOLAP System Study Analysis and ER modeling developing Database Schemas like star schema and Snowflake schema used in relational dimensional modeling Experience in optimizing and performance tuning of Mappings and implementing the complex business rules by creating reusable Transformations Mapplets and Tasks Solid Experience in Cloud with Amazon Web services AWS EC2 S3 CloudWatch RDSEMRSNS and Google cloud GCP Responsible for developing data pipeline using flume Sqoop and pig to extract the data from weblogs and store in HDFS Queried Vertica SQL Server for data validation along with developing validation worksheets in Excel in order to validate the dashboards on Tableau Have knowledge on GCP cloud PubSub and Microservices Event sourcing cloud functions and algorithm Experience in implementing and migrating and deploying workloads on Azure VM Developed Spark code using Scala and SparkSQL for faster testing and data processing Extensively used SQL and PLSQL for development of Procedures Functions Packages and Triggers Experienced on Tableau Desktop Tableau Server and good understanding of tableau architecture Experienced in integrating Kafka with Spark Streaming for high speed data processing Experience in Implementing AWS solutions using EC2 S3 and Azure storage Experienced in developing business reports by writing complex SQL queries using views macros volatile and global temporary tables Working with AWS team in testing our Apache Spark ETL application on EMREC2 using S3 Experience in designing both time driven and data driven automated workflows using Oozie Experienced with work flow schedulers data architecture including data ingestion pipeline design and data modelling Configuration of ElasticSearch on Amazon Web Service with static IP authentication security features Experience in AWS Cloud platform and its features which includes EC2 AMI EBS Cloudwatch AWS Config Autoscaling IAM user management and AWS S3 Managed AWS EC2 instances utilizing Auto Scaling Elastic Load Balancing and Glacier for our QA and UAT environments as well as infrastructure servers for GI Solved performance issues in Hive and Pig scripts with understanding of Joins Group and Aggregation and how does it translate to MapReduce jobs Work Experience Senior Spark DeveloperHadoop Developer Synchrony Financial Bank Atlanta GA May 2018 to Present Responsibilities Worked on different tools for Presto to process these large datasets Worked on Core tables of Revenue DataFeedRDF that calculates the revenue of the advertisers of the Facebook Involved into testing and migration to Presto Worked extensively with Data migration Data cleansing Data profiling and ETL Processes features for data warehouses Involved in converting HiveSQL queries into Spark transformations using Spark RDDs and Scala Experienced with the tools in Hadoop Ecosystem including Pig Hive HDFS Sqoop Spark Yarn and Oozie Have used Python with the Spark Python API PySpark to create and analyze Spark DataFrames Involved in importing the real time data to Hadoop using kafka and implemented the Oozie job for daily Experienced in writing complex SQL Queries Stored Procedures Triggers Views Cursors Joins Constraints DDL DML and User Defined Functions to implement the business logic Developed Custom ETL Solution Batch processing and RealTime data ingestion pipeline to move data in and out of Hadoop using Python and shell Script Experience in Large Data processing and transformation using HadoopHive and Sqoop Real time predictive analytics capabilities using Spark Streaming Spark SQL and Oracle Data Mining tools Experience with Tableau for Data Acquisition and visualizations Working with AWS team in testing our Apache Spark ETL application on EMREC2 using S3 Assisted in data analysis star schema data modeling and design specific to data warehousing and business intelligence environment Have been using pyspark with Jupyter in Docker Containers Expertise in platform related Hadoop Production support tasks by analyzing the job logs Have been using Spark with Python working with RDD provided by the library Py4j in pyspark module Monitored System health and logs and responded accordingly to any warning or failure conditions EnvironmentAmazon Web ServiceVertica InformaticaPowerCenter Pyspark Spark AWS Kafka AWSS3 ApacheHadoop Hive Pig Shell Script ETL tableau Agile Methodology Spark DeveloperHadoop developer View Lift New York NY September 2016 to April 2018 Responsibilities Collected data using Spark Streaming from AWS S3 bucket in nearrealtime and performs necessary Transformations and Aggregation on the fly to build the common learner data model and persists the data in HDFS Developed Spark Code using Scala and SparkSQLStreaming for faster testing and processing of data Realtime experience in Hadoop Distributed files system Hadoop framework and Parallel processing implementation AWS EMRCloudera with hands on experience in HDFS Using Glue Data Catalog for storing the schemametadata of Hive External tables Used Aws EMR long running clusters for processing the spark jobs use AWS S3 for storing data in buckets Created Hive InternalExternal tables metastore for storing the metadata Used sqoop jobs for ingestion from various sources such as Oracle Salesforce SAS etc Worked on various complex SQL queries on the source side which is Oracle database Worked on using cloudformation template CICD tools like concourse to automate the data pipeline Worked on AWS CLI Auto Scaling and Cloud Watch Monitoring creation and update Designed Data Quality Framework to perform schema validation and data profiling on Spark Pyspark Experience in Implementing AWS solutions using EC2 S3 and Azure storage Responsible for monitor the tableau dashboard for reporting purpose and providing the refined data to end users Environment AWS EMR S3 Spark Spark sql Scala Azure Hive Sqoop AWSCli Shell Script ETL Tableau Agile Methodology Hadoop Developer Cognizant Technology Solutions Teaneck NJ October 2014 to August 2016 Responsibilities Worked with variables and parameter files and designed ETL framework to create parameter files to make it dynamic Currently working on the Teradata to HP Vertica Data Migration Project Working extensively on the Copy Command for extracting the data from the files to Vertica Monitor the ETL process job and validate the data loaded in Vertica DW Built a FullService Catalog System which has a full workflow using ElasticSearch Logstash Kibana Kinesis CloudWatch Responsible for data extraction and data ingestion from different data sources into Hadoop Data Lake by creating ETL pipelines using Pig and Hive Experienced in transferring data from different data sources into HDFS systems using kafka producers consumers and kafka brokers The logs and semi structured content that are stored on HDFS were preprocessed using PIG and the processed data is imported into Hivewarehouse which enabled business analysts to write Hive queries Worked with data migration form Hadoop clusters to cloud Good knowledge of cloud components like AWS S3 EMR Elastic Cache and EC2 Responsible to write Hiveand Pig scripts as ETL tool to do transformations event joins filter both traffic and some preaggregations before storing into the HDFS Developed the Vertica UDFs to preprocess the data for analysis Designed the reporting application that uses the Spark SQL to fetch and generate reports on HBase Build custom batch aggression framework for creating reporting aggregates in Hadoop Experience in working with Hive data warehouse toolcreating tables data distribution by implementing partitioning and bucketing writing and optimizing the Hive queries Built real time pipeline for streaming data using Kafka and SparkStreaming Experienced with NoSQL databases like HBase MongoDB and Cassandra and wrote Storm topology to accept the events from Kafka producer and emit into Cassandra DB Experienced in working with spark ecosystem using spark SQL and scala queries on different formats like Text file CSV file Great hands on experience with Pyspark for using Spark libraries by using python scripting for data analysis Wrote Python Script to access databases and execute scripts and commands Involved in converting CassandraHiveSQL queries into Spark transformations using Spark RDDs in Scala and Python Created ODBC connection through Sqoop between Hortonworks and SQL Server Building publishing customized interactive reports and dashboards report scheduling using Tableau server Creating New Schedules and checking the tasks daily on the server Environment Hadoop Hive Apache Spark Apache Kafka Hortonworks AWS ElasticSearch Lambda Apache Cassandra Hbase MongoDB SQL Sqoop Flume Oozie Java jdk 16 Eclipse InformaticaPower Center 91 Tableau Teradata 13x Teradata SQL Assistant Java Developer PTC Boston MA August 2012 to September 2014 Responsibilities Involved in developing testing and implementation of the system using Struts JSF and Hibernate Developing modifying fixing reviewing testing and migrating the Java JSP XML Servlet SQLs JSF Updated userinteractive web pages from JSP and CSSto Html5 CSS and JavaScript for the best user experience Developed Servlets Session and Entity Beans handling business logic and data Created enterprise deployment strategy and designed the enterprise deployment process to deploy Web Services J2EE programs on more than 7 different SOAWebLogic instances across development test and production environments Designed user interface HTML Swing CSS XML Java Script and JSP Implemented the presentation using a combination of Java Server Pages JSP to render the HTML and welldefined API interface to allow access to the application services layer Used Enterprise Java Beans EJBs extensively in the application Developed and deployed Session Beans to perform user authentication Involve in Requirement Analysis Design Code Testing and debugging Implementation activities Involved in the Performance Tuning of Database and Informatica Improved performance by identifying and rectifying the performance bottle necks Understanding how to apply technologies to solve big data problems and to develop innovative big data solutions Designed and developed Job flows using Oozie Developed Sqoop commands to pull the data from Teradata The data is collected from distributed sources into Avro models Applied transformations and standardizations and loaded into HBase for further data processing Wrote PLSQL Packages and Stored procedures to implement business rules and validations Environment Java J2EE Java Server Pages JSP JavaScriptHadoop Oozie Hive Teradata Servlets JDBC PLSQL ODBC Struts Framework XML CSS HTML DHTML XSL XSLT and MySQL Java Developer New York NY February 2011 to July 2012 Responsibilities Involved in Requirements study Functional analysis detailed design including entity relations and various table design Ability to support application deployments building new systems and upgrading and patching existing ones through ATG methodologies Designed and implemented a GUI framework for Swing Involved in Creation of Adobe Flex Families in Content Server and associated the JDBC and XSD XML pages to the assets Involved in executing all Selenium test scripts on the different browsers and checked for compatibility regression test cases were automated using Selenium Web Driver and Web Driver Backed Selenium Used validation frameworks for specifying the validations rules Extensive work on Web services SOAP and Restful application Developed Java Script for Client Side validations Uses coding methods in JNI to initiate or enhance inhouse custom developed RF optimization drive test software in support of 1Xused SDLC concepts Involved in applying SDLC Agile Scrum RUP Waterfall concepts Designed and developed the Java bean components and OR Mapping using Hibernate designed roles and groups for users and resources using AWS Identity Access Management IAM Involved in writing the screen classes and Action classes for implementing the business logic of Pilot and object oriented programming and monitored and responsible for troubleshooting the WebSphere Application Server with JVM logs Process Logs Service logs Involved in Design Development and Support phases of Software Development Life Cycle SDLC developed Custom Tags to simplify the JSP code Designed UI screens using JSP CSS XML and HTML Used JavaScript for client side validation Expertise in creating DevOps strategy in a mix environment of Linux servers responsible for the implementation of application system with core java and Spring framework uses Rational Rose for model driven development and UML modeling Used Spring Core Annotations for Dependency Injection and used Apache Camel to integrate Spring framework use Apache Camel to route and transform messages and designed and implemented new customer flow using Apache Velocity template Created PHPHTML5CSS3 Web pages to support Comcast Business Voice Xpress VOIP phone support portal using Agile practices and Rally management software Extensively used JSTL tags and Struts tag libraries Used Struts tiles as well in the presentation tier participated in coding Spring AOP components for the Transactional Model to handle many requests Involved in writing JSP and Servlet components Actively involved in designing and implementing the application using various design patterns such as Singleton DAO Front Controller Service Locator Business Delegate Faade and Data Access Object used Java Message Service JMS for reliable and asynchronous exchange of important information such as loan status report Involved in the JMS Connection Pool and the implementation of publish and subscribe using Spring JMS Used JMS Template to publish and Message Driven Bean MDB to subscribe from the JMS provider Used Hibernate objectrelationalmapping ORM solution technique of mapping data representation from MVC model to Oracle Relational data model with SQLbased schema Develop Isolated tests in JRuby with Gherkincucumber using Spring Beans config and mocks and execute the tests in an isolated environment and Implemented in Ruby Java and JRuby and used a number of AWS Services Also help set up some of the AWS account peered with Shared Services so some users can use their CORP login creds for logging into AWS accounts help set AWS federation with on prem Shared Services Used Junit framework for unit testing of application and Log4j to capture the log that includes runtime exceptions Worked on Agile SCRUM development methodology and built the application with Test Driven Development TDD deployed the application on Web Sphere Application Server Used ANT as a build tool and IVY as dependency tool Used CVS version control for implementing the application Work involved extensive usage of HTML DHTML CSS JQuery JavaScript and Ajax for client side development and validations Rewrote several pieces to make them compliant with the emerging JSF standard experience in working with relational database MySQL developed complex SQL queries for extracting data from the database Environment Core Java J2EE JSP 20 Struts 12 EJB 20 JMS JNDI Oracle DHTML XML DOM SAX Rationale Rose Groovy gails UNIX IBM Web Sphere Application Server 51 Hibernate 20 spring LOG4J CVS Java Developer Boeing Everett San Francisco CA March 2009 to January 2011 Responsibilities Worked with business teams on requirements analysis building use cases and estimations Generating high level and low level design documentation Developed JavaJ2EE code business logic using Spring Hibernate framework and OOP concepts involved in Peer code reviews Created WSDL Generated data objects using WSDL Java Spring JAXWS Axis apache CXF and developed mapping code for several Web Services interfaces for various profile management endpoints Developed Java multi threaded batch offline bulk upload tool web applications using Spring Servlets and UI layer using JSPs JavaScript HTML CSS Angular JS Worked on implementation of new complex implementations criticalquick deliveries Developed and build Ant scripts Maven for packaging the application code Developed database scripts and procedures using PLSQL Deployed code on Tomcat web application server Validated requirement deliverables unit testing using SOAP UI set up executed system endurance performance tests using JMeter Set up build automation deployment to DEVQAPRD servers using TeamCity continuous integration platform Built Regression suits using SOAP UI for automated regression test on CI platform Coordinated with Architects and Security teams on defining SOAP RESTful web services architecture and generatingmanaging artifacts documentation Confluence Performed use case analysis and design SOAP REST APIs on social integration of web and native mobile applications implementing oAuth Ping Federate CreatingManaging release plan sprint deliveries responsible for version control SVN and configuration of the project keeping track of project activities Sprint execution planning in JIRA Coordinated with cross functional teams on resolving integrations issues bug fixes RCAs RFCs Worked in Agile model Conducting Daily ScrumStand ups Backlog Grooming Sprint Planning Sprint Review Sprint Retrospective Meeting supporting Product owner in refining and grooming product backlog Lead development team Motivating team and helping them work in self organized manner Acted as Liaison Between Product Owner and Development Team resolving impediments in order to achieve Sprint Goals Worked as SME and SPOC for the project within Adidas CRM area Managing knowledge sharing resolved problems on critical issues in live system Master Data management Data schemas web forms configuration workflow campaign maintenance and set up using Adobe Campaign formerly Neolane Environment Java J2EE Spring Hibernatecore java JSP HTML XML CSS JavaScript SubversionSVN SVN Oracle PLSQLJms WSDL SOAP XML JAXWS RESTful JSON TomCat Eclipse SQL Developer Toad for Oracle MS Visio JIRA Confluence Maven Ant Beyond Compare Team City CI JUnit SOAP UI Apache Jmeter Unix Education Bachelors Skills DATA MODELING DB2 JDBC MS SQL SERVER SQL SERVER MYSQL OLTP ORACLE SQL CASSANDRA MAHOUT OOZIE SQOOP HBASE KAFKA DATA ARCHITECTURE ETL HADOOP MAP REDUCE OLAP Additional Information TECHNICAL SKILLS Specialties Data warehousingETLBI Concepts Data Architecture Software Development methodologies Data Modeling Business Tools Tableau 10X Business Objects XI R2 InformaticaPowercenter 8x OLAPOLTP Talend Teradata 13x Teradata SQL Assistant Big Data hadoop map reduce 1020 pig hive hbase sqoop oozie zookeeper kafka spark flume storm impala mahout hue tez hcatalog storm Cassandra pyspark Cloud Technologies AWSEMR AWS S3 Glue Data Catalog Kinesis Lambda ELKElastic Logstash Kibana Stack cloudwatch metric Azure Databases DB2 MySQL MS SQL server Vertica Mongo DB Oracle SQL 2008 Hortonworks Cloudera Languages Python Java J2EE Scala HTML SQL JDBC JavaScript PHP Operating System Mac OS Unix Linux Various Versions Windows 20037881XP Web Development HTML Java Script XML PHP JSP Servlets JavaScript Application Server Apache Tomcat WebLogic WebSphere Tools Eclipse NetBeans",
    "entities": [
        "Joins Group",
        "the Transactional Model",
        "JavaScript Application",
        "the Facebook Involved",
        "Test Driven Development TDD",
        "GUI",
        "New York",
        "Python Created ODBC",
        "Hadoop Experience",
        "GCP Responsible",
        "Web Sphere Application Server Used",
        "Content Server",
        "AWS S3 Managed AWS EC2",
        "Auto Scaling Elastic Load Balancing",
        "Created WSDL Generated",
        "ETL Processes",
        "JSP CSS XML",
        "Hive External",
        "Cloud",
        "Hadoop Ecosystem",
        "CVS",
        "IP",
        "Google",
        "ER",
        "Requirements",
        "Ajax",
        "PubSub",
        "RDD",
        "Hadoop",
        "SOAP",
        "DHTML",
        "Atlanta",
        "Created Hive InternalExternal",
        "Software Development Life Cycle SDLC",
        "DevOps",
        "User Defined Functions",
        "Hiveand",
        "AWS Services",
        "Built Regression",
        "the Java JSP XML Servlet",
        "Front Controller Service Locator Business Delegate Faade",
        "HBase",
        "Avro",
        "Spark Streaming Spark",
        "SparkScala",
        "Oozie Developed Sqoop",
        "UAT",
        "Amazon",
        "SparkStreaming Experienced",
        "Creating New Schedules",
        "Developed JavaJ2EE",
        "OLAPOLTP",
        "Spark DeveloperHadoop Developer",
        "AWS S3 EMR Elastic Cache",
        "HBase Build",
        "Designed Data Quality Framework",
        "SparkSQL",
        "Developed Servlets Session",
        "Oracle Relational",
        "View Lift New York",
        "AWS S3",
        "Oozie Experienced",
        "Informatica Improved",
        "SME",
        "Vertica Data Migration Project Working",
        "UML",
        "Restful",
        "ElasticSearch",
        "JRuby",
        "Hortonworks",
        "Hadoop Data Lake",
        "Cassandra DB Experienced",
        "RUP Waterfall",
        "Analysis Design and Development and Testing of ETL",
        "HadoopHive",
        "Apache Camel",
        "HadoopBig Data",
        "Apache Spark ETL",
        "Environment AWS",
        "Tableau Desktop Tableau Server",
        "JSP",
        "Linux",
        "Shell Script ETL Tableau Agile Methodology Hadoop Developer Cognizant Technology Solutions",
        "Large Data",
        "CassandraHiveSQL",
        "Acted",
        "JIRA Coordinated",
        "Spark Streaming",
        "Process Logs Service",
        "SPOC",
        "SQLbased",
        "Adobe Campaign",
        "MVC",
        "Monitored System",
        "Spark",
        "Agile",
        "Shell Script ETL",
        "EJB",
        "the Spark Python API PySpark",
        "Data migration Data cleansing Data",
        "EnvironmentAmazon",
        "the Data Warehousing Expertise",
        "CSV",
        "the WebSphere Application Server",
        "Kibana Kinesis CloudWatch Responsible",
        "API",
        "Backlog Grooming Sprint Planning Sprint Review Sprint",
        "Sqoop",
        "Vertica",
        "QA",
        "GI",
        "Custom Tags",
        "Storm",
        "Created",
        "AWS",
        "Singleton",
        "AWS Identity Access Management IAM Involved",
        "Pyspark",
        "JSF",
        "Developed Custom",
        "CSSto Html5 CSS",
        "PIG",
        "DHTML XML DOM",
        "Environment Hadoop Hive Apache Spark",
        "HDFS Using Glue Data Catalog",
        "HTML",
        "Vertica Monitor",
        "CXF",
        "Spark DataFrames Involved",
        "KAFKA DATA ARCHITECTURE ETL HADOOP",
        "SQL",
        "Data Modeling Business Tools",
        "Oracle Data Mining",
        "OLTP",
        "JNI",
        "Spark Pyspark Experience",
        "Comcast Business Voice Xpress",
        "Java Message Service JMS",
        "Shared Services Used Junit",
        "Conducting Daily ScrumStand",
        "Py4j",
        "CI",
        "Hive",
        "CICD",
        "Big Data",
        "Rally",
        "JDBC",
        "XSD XML",
        "the Copy Command",
        "Designed UI",
        "Mappings",
        "Coordinated with Architects and Security",
        "ETL",
        "AMI EBS Cloudwatch AWS Config Autoscaling IAM",
        "Implementing AWS",
        "Servlet",
        "Spring Hibernate",
        "Vertica Mongo DB",
        "ATG",
        "GCP",
        "JavaScript",
        "ANT",
        "UI",
        "Selenium",
        "Hadoop Pig Hive HBase Oozie Zookeeper Sqoop Storm Flink",
        "Synchrony Financial Bank",
        "Oracle Salesforce SAS",
        "Spring Servlets",
        "SOAP UI",
        "SVN",
        "Present Responsibilities Worked",
        "Shared Services",
        "HDFS Developed Spark Code",
        "AWS CLI Auto Scaling",
        "Amazon Web Service",
        "Tomcat",
        "Data",
        "MapReduce",
        "NetBeans",
        "Presto",
        "Tableau",
        "NoSQL",
        "Agile Methodology Spark DeveloperHadoop",
        "Used Spring Core Annotations for Dependency Injection",
        "Sprint",
        "JavaScript PHP Operating System Mac",
        "Java Server Pages JSP",
        "HTML Used JavaScript"
    ],
    "experience": "Experience in optimizing and performance tuning of Mappings and implementing the complex business rules by creating reusable Transformations Mapplets and Tasks Solid Experience in Cloud with Amazon Web services AWS EC2 S3 CloudWatch RDSEMRSNS and Google cloud GCP Responsible for developing data pipeline using flume Sqoop and pig to extract the data from weblogs and store in HDFS Queried Vertica SQL Server for data validation along with developing validation worksheets in Excel in order to validate the dashboards on Tableau Have knowledge on GCP cloud PubSub and Microservices Event sourcing cloud functions and algorithm Experience in implementing and migrating and deploying workloads on Azure VM Developed Spark code using Scala and SparkSQL for faster testing and data processing Extensively used SQL and PLSQL for development of Procedures Functions Packages and Triggers Experienced on Tableau Desktop Tableau Server and good understanding of tableau architecture Experienced in integrating Kafka with Spark Streaming for high speed data processing Experience in Implementing AWS solutions using EC2 S3 and Azure storage Experienced in developing business reports by writing complex SQL queries using views macros volatile and global temporary tables Working with AWS team in testing our Apache Spark ETL application on EMREC2 using S3 Experience in designing both time driven and data driven automated workflows using Oozie Experienced with work flow schedulers data architecture including data ingestion pipeline design and data modelling Configuration of ElasticSearch on Amazon Web Service with static IP authentication security features Experience in AWS Cloud platform and its features which includes EC2 AMI EBS Cloudwatch AWS Config Autoscaling IAM user management and AWS S3 Managed AWS EC2 instances utilizing Auto Scaling Elastic Load Balancing and Glacier for our QA and UAT environments as well as infrastructure servers for GI Solved performance issues in Hive and Pig scripts with understanding of Joins Group and Aggregation and how does it translate to MapReduce jobs Work Experience Senior Spark DeveloperHadoop Developer Synchrony Financial Bank Atlanta GA May 2018 to Present Responsibilities Worked on different tools for Presto to process these large datasets Worked on Core tables of Revenue DataFeedRDF that calculates the revenue of the advertisers of the Facebook Involved into testing and migration to Presto Worked extensively with Data migration Data cleansing Data profiling and ETL Processes features for data warehouses Involved in converting HiveSQL queries into Spark transformations using Spark RDDs and Scala Experienced with the tools in Hadoop Ecosystem including Pig Hive HDFS Sqoop Spark Yarn and Oozie Have used Python with the Spark Python API PySpark to create and analyze Spark DataFrames Involved in importing the real time data to Hadoop using kafka and implemented the Oozie job for daily Experienced in writing complex SQL Queries Stored Procedures Triggers Views Cursors Joins Constraints DDL DML and User Defined Functions to implement the business logic Developed Custom ETL Solution Batch processing and RealTime data ingestion pipeline to move data in and out of Hadoop using Python and shell Script Experience in Large Data processing and transformation using HadoopHive and Sqoop Real time predictive analytics capabilities using Spark Streaming Spark SQL and Oracle Data Mining tools Experience with Tableau for Data Acquisition and visualizations Working with AWS team in testing our Apache Spark ETL application on EMREC2 using S3 Assisted in data analysis star schema data modeling and design specific to data warehousing and business intelligence environment Have been using pyspark with Jupyter in Docker Containers Expertise in platform related Hadoop Production support tasks by analyzing the job logs Have been using Spark with Python working with RDD provided by the library Py4j in pyspark module Monitored System health and logs and responded accordingly to any warning or failure conditions EnvironmentAmazon Web ServiceVertica InformaticaPowerCenter Pyspark Spark AWS Kafka AWSS3 ApacheHadoop Hive Pig Shell Script ETL tableau Agile Methodology Spark DeveloperHadoop developer View Lift New York NY September 2016 to April 2018 Responsibilities Collected data using Spark Streaming from AWS S3 bucket in nearrealtime and performs necessary Transformations and Aggregation on the fly to build the common learner data model and persists the data in HDFS Developed Spark Code using Scala and SparkSQLStreaming for faster testing and processing of data Realtime experience in Hadoop Distributed files system Hadoop framework and Parallel processing implementation AWS EMRCloudera with hands on experience in HDFS Using Glue Data Catalog for storing the schemametadata of Hive External tables Used Aws EMR long running clusters for processing the spark jobs use AWS S3 for storing data in buckets Created Hive InternalExternal tables metastore for storing the metadata Used sqoop jobs for ingestion from various sources such as Oracle Salesforce SAS etc Worked on various complex SQL queries on the source side which is Oracle database Worked on using cloudformation template CICD tools like concourse to automate the data pipeline Worked on AWS CLI Auto Scaling and Cloud Watch Monitoring creation and update Designed Data Quality Framework to perform schema validation and data profiling on Spark Pyspark Experience in Implementing AWS solutions using EC2 S3 and Azure storage Responsible for monitor the tableau dashboard for reporting purpose and providing the refined data to end users Environment AWS EMR S3 Spark Spark sql Scala Azure Hive Sqoop AWSCli Shell Script ETL Tableau Agile Methodology Hadoop Developer Cognizant Technology Solutions Teaneck NJ October 2014 to August 2016 Responsibilities Worked with variables and parameter files and designed ETL framework to create parameter files to make it dynamic Currently working on the Teradata to HP Vertica Data Migration Project Working extensively on the Copy Command for extracting the data from the files to Vertica Monitor the ETL process job and validate the data loaded in Vertica DW Built a FullService Catalog System which has a full workflow using ElasticSearch Logstash Kibana Kinesis CloudWatch Responsible for data extraction and data ingestion from different data sources into Hadoop Data Lake by creating ETL pipelines using Pig and Hive Experienced in transferring data from different data sources into HDFS systems using kafka producers consumers and kafka brokers The logs and semi structured content that are stored on HDFS were preprocessed using PIG and the processed data is imported into Hivewarehouse which enabled business analysts to write Hive queries Worked with data migration form Hadoop clusters to cloud Good knowledge of cloud components like AWS S3 EMR Elastic Cache and EC2 Responsible to write Hiveand Pig scripts as ETL tool to do transformations event joins filter both traffic and some preaggregations before storing into the HDFS Developed the Vertica UDFs to preprocess the data for analysis Designed the reporting application that uses the Spark SQL to fetch and generate reports on HBase Build custom batch aggression framework for creating reporting aggregates in Hadoop Experience in working with Hive data warehouse toolcreating tables data distribution by implementing partitioning and bucketing writing and optimizing the Hive queries Built real time pipeline for streaming data using Kafka and SparkStreaming Experienced with NoSQL databases like HBase MongoDB and Cassandra and wrote Storm topology to accept the events from Kafka producer and emit into Cassandra DB Experienced in working with spark ecosystem using spark SQL and scala queries on different formats like Text file CSV file Great hands on experience with Pyspark for using Spark libraries by using python scripting for data analysis Wrote Python Script to access databases and execute scripts and commands Involved in converting CassandraHiveSQL queries into Spark transformations using Spark RDDs in Scala and Python Created ODBC connection through Sqoop between Hortonworks and SQL Server Building publishing customized interactive reports and dashboards report scheduling using Tableau server Creating New Schedules and checking the tasks daily on the server Environment Hadoop Hive Apache Spark Apache Kafka Hortonworks AWS ElasticSearch Lambda Apache Cassandra Hbase MongoDB SQL Sqoop Flume Oozie Java jdk 16 Eclipse InformaticaPower Center 91 Tableau Teradata 13x Teradata SQL Assistant Java Developer PTC Boston MA August 2012 to September 2014 Responsibilities Involved in developing testing and implementation of the system using Struts JSF and Hibernate Developing modifying fixing reviewing testing and migrating the Java JSP XML Servlet SQLs JSF Updated userinteractive web pages from JSP and CSSto Html5 CSS and JavaScript for the best user experience Developed Servlets Session and Entity Beans handling business logic and data Created enterprise deployment strategy and designed the enterprise deployment process to deploy Web Services J2EE programs on more than 7 different SOAWebLogic instances across development test and production environments Designed user interface HTML Swing CSS XML Java Script and JSP Implemented the presentation using a combination of Java Server Pages JSP to render the HTML and welldefined API interface to allow access to the application services layer Used Enterprise Java Beans EJBs extensively in the application Developed and deployed Session Beans to perform user authentication Involve in Requirement Analysis Design Code Testing and debugging Implementation activities Involved in the Performance Tuning of Database and Informatica Improved performance by identifying and rectifying the performance bottle necks Understanding how to apply technologies to solve big data problems and to develop innovative big data solutions Designed and developed Job flows using Oozie Developed Sqoop commands to pull the data from Teradata The data is collected from distributed sources into Avro models Applied transformations and standardizations and loaded into HBase for further data processing Wrote PLSQL Packages and Stored procedures to implement business rules and validations Environment Java J2EE Java Server Pages JSP JavaScriptHadoop Oozie Hive Teradata Servlets JDBC PLSQL ODBC Struts Framework XML CSS HTML DHTML XSL XSLT and MySQL Java Developer New York NY February 2011 to July 2012 Responsibilities Involved in Requirements study Functional analysis detailed design including entity relations and various table design Ability to support application deployments building new systems and upgrading and patching existing ones through ATG methodologies Designed and implemented a GUI framework for Swing Involved in Creation of Adobe Flex Families in Content Server and associated the JDBC and XSD XML pages to the assets Involved in executing all Selenium test scripts on the different browsers and checked for compatibility regression test cases were automated using Selenium Web Driver and Web Driver Backed Selenium Used validation frameworks for specifying the validations rules Extensive work on Web services SOAP and Restful application Developed Java Script for Client Side validations Uses coding methods in JNI to initiate or enhance inhouse custom developed RF optimization drive test software in support of 1Xused SDLC concepts Involved in applying SDLC Agile Scrum RUP Waterfall concepts Designed and developed the Java bean components and OR Mapping using Hibernate designed roles and groups for users and resources using AWS Identity Access Management IAM Involved in writing the screen classes and Action classes for implementing the business logic of Pilot and object oriented programming and monitored and responsible for troubleshooting the WebSphere Application Server with JVM logs Process Logs Service logs Involved in Design Development and Support phases of Software Development Life Cycle SDLC developed Custom Tags to simplify the JSP code Designed UI screens using JSP CSS XML and HTML Used JavaScript for client side validation Expertise in creating DevOps strategy in a mix environment of Linux servers responsible for the implementation of application system with core java and Spring framework uses Rational Rose for model driven development and UML modeling Used Spring Core Annotations for Dependency Injection and used Apache Camel to integrate Spring framework use Apache Camel to route and transform messages and designed and implemented new customer flow using Apache Velocity template Created PHPHTML5CSS3 Web pages to support Comcast Business Voice Xpress VOIP phone support portal using Agile practices and Rally management software Extensively used JSTL tags and Struts tag libraries Used Struts tiles as well in the presentation tier participated in coding Spring AOP components for the Transactional Model to handle many requests Involved in writing JSP and Servlet components Actively involved in designing and implementing the application using various design patterns such as Singleton DAO Front Controller Service Locator Business Delegate Faade and Data Access Object used Java Message Service JMS for reliable and asynchronous exchange of important information such as loan status report Involved in the JMS Connection Pool and the implementation of publish and subscribe using Spring JMS Used JMS Template to publish and Message Driven Bean MDB to subscribe from the JMS provider Used Hibernate objectrelationalmapping ORM solution technique of mapping data representation from MVC model to Oracle Relational data model with SQLbased schema Develop Isolated tests in JRuby with Gherkincucumber using Spring Beans config and mocks and execute the tests in an isolated environment and Implemented in Ruby Java and JRuby and used a number of AWS Services Also help set up some of the AWS account peered with Shared Services so some users can use their CORP login creds for logging into AWS accounts help set AWS federation with on prem Shared Services Used Junit framework for unit testing of application and Log4j to capture the log that includes runtime exceptions Worked on Agile SCRUM development methodology and built the application with Test Driven Development TDD deployed the application on Web Sphere Application Server Used ANT as a build tool and IVY as dependency tool Used CVS version control for implementing the application Work involved extensive usage of HTML DHTML CSS JQuery JavaScript and Ajax for client side development and validations Rewrote several pieces to make them compliant with the emerging JSF standard experience in working with relational database MySQL developed complex SQL queries for extracting data from the database Environment Core Java J2EE JSP 20 Struts 12 EJB 20 JMS JNDI Oracle DHTML XML DOM SAX Rationale Rose Groovy gails UNIX IBM Web Sphere Application Server 51 Hibernate 20 spring LOG4J CVS Java Developer Boeing Everett San Francisco CA March 2009 to January 2011 Responsibilities Worked with business teams on requirements analysis building use cases and estimations Generating high level and low level design documentation Developed JavaJ2EE code business logic using Spring Hibernate framework and OOP concepts involved in Peer code reviews Created WSDL Generated data objects using WSDL Java Spring JAXWS Axis apache CXF and developed mapping code for several Web Services interfaces for various profile management endpoints Developed Java multi threaded batch offline bulk upload tool web applications using Spring Servlets and UI layer using JSPs JavaScript HTML CSS Angular JS Worked on implementation of new complex implementations criticalquick deliveries Developed and build Ant scripts Maven for packaging the application code Developed database scripts and procedures using PLSQL Deployed code on Tomcat web application server Validated requirement deliverables unit testing using SOAP UI set up executed system endurance performance tests using JMeter Set up build automation deployment to DEVQAPRD servers using TeamCity continuous integration platform Built Regression suits using SOAP UI for automated regression test on CI platform Coordinated with Architects and Security teams on defining SOAP RESTful web services architecture and generatingmanaging artifacts documentation Confluence Performed use case analysis and design SOAP REST APIs on social integration of web and native mobile applications implementing oAuth Ping Federate CreatingManaging release plan sprint deliveries responsible for version control SVN and configuration of the project keeping track of project activities Sprint execution planning in JIRA Coordinated with cross functional teams on resolving integrations issues bug fixes RCAs RFCs Worked in Agile model Conducting Daily ScrumStand ups Backlog Grooming Sprint Planning Sprint Review Sprint Retrospective Meeting supporting Product owner in refining and grooming product backlog Lead development team Motivating team and helping them work in self organized manner Acted as Liaison Between Product Owner and Development Team resolving impediments in order to achieve Sprint Goals Worked as SME and SPOC for the project within Adidas CRM area Managing knowledge sharing resolved problems on critical issues in live system Master Data management Data schemas web forms configuration workflow campaign maintenance and set up using Adobe Campaign formerly Neolane Environment Java J2EE Spring Hibernatecore java JSP HTML XML CSS JavaScript SubversionSVN SVN Oracle PLSQLJms WSDL SOAP XML JAXWS RESTful JSON TomCat Eclipse SQL Developer Toad for Oracle MS Visio JIRA Confluence Maven Ant Beyond Compare Team City CI JUnit SOAP UI Apache Jmeter Unix Education Bachelors Skills DATA MODELING DB2 JDBC MS SQL SERVER SQL SERVER MYSQL OLTP ORACLE SQL CASSANDRA MAHOUT OOZIE SQOOP HBASE KAFKA DATA ARCHITECTURE ETL HADOOP MAP REDUCE OLAP Additional Information TECHNICAL SKILLS Specialties Data warehousingETLBI Concepts Data Architecture Software Development methodologies Data Modeling Business Tools Tableau 10X Business Objects XI R2 InformaticaPowercenter 8x OLAPOLTP Talend Teradata 13x Teradata SQL Assistant Big Data hadoop map reduce 1020 pig hive hbase sqoop oozie zookeeper kafka spark flume storm impala mahout hue tez hcatalog storm Cassandra pyspark Cloud Technologies AWSEMR AWS S3 Glue Data Catalog Kinesis Lambda ELKElastic Logstash Kibana Stack cloudwatch metric Azure Databases DB2 MySQL MS SQL server Vertica Mongo DB Oracle SQL 2008 Hortonworks Cloudera Languages Python Java J2EE Scala HTML SQL JDBC JavaScript PHP Operating System Mac OS Unix Linux Various Versions Windows 20037881XP Web Development HTML Java Script XML PHP JSP Servlets JavaScript Application Server Apache Tomcat WebLogic WebSphere Tools Eclipse NetBeans",
    "extracted_keywords": [
        "Senior",
        "Spark",
        "DeveloperHadoop",
        "Developer",
        "Senior",
        "Spark",
        "span",
        "lDeveloperspanHadoop",
        "span",
        "lDeveloperspan",
        "Senior",
        "Spark",
        "DeveloperHadoop",
        "Developer",
        "Synchrony",
        "Financial",
        "Bank",
        "Atlanta",
        "GA",
        "9years",
        "experience",
        "HadoopBig",
        "Data",
        "technologies",
        "Hadoop",
        "Pig",
        "Hive",
        "HBase",
        "Oozie",
        "Zookeeper",
        "Sqoop",
        "Storm",
        "Flink",
        "Flume",
        "Zookeeper",
        "Impala",
        "Tez",
        "Kafka",
        "Spark",
        "hands",
        "experience",
        "Map",
        "ReduceYARN",
        "SparkScala",
        "jobs",
        "IT",
        "experience",
        "emphasis",
        "Analysis",
        "Design",
        "Development",
        "Testing",
        "ETL",
        "methodologies",
        "phases",
        "Data",
        "Warehousing",
        "Expertise",
        "System",
        "Study",
        "Analysis",
        "ER",
        "modeling",
        "Database",
        "Schemas",
        "star",
        "schema",
        "Snowflake",
        "schema",
        "modeling",
        "Experience",
        "optimizing",
        "performance",
        "tuning",
        "Mappings",
        "business",
        "rules",
        "Transformations",
        "Mapplets",
        "Tasks",
        "Solid",
        "Experience",
        "Cloud",
        "Amazon",
        "Web",
        "services",
        "EC2",
        "S3",
        "CloudWatch",
        "RDSEMRSNS",
        "Google",
        "cloud",
        "GCP",
        "Responsible",
        "data",
        "pipeline",
        "flume",
        "Sqoop",
        "pig",
        "data",
        "weblogs",
        "HDFS",
        "Queried",
        "Vertica",
        "SQL",
        "Server",
        "data",
        "validation",
        "validation",
        "worksheets",
        "Excel",
        "order",
        "dashboards",
        "Tableau",
        "knowledge",
        "cloud",
        "PubSub",
        "Microservices",
        "Event",
        "cloud",
        "functions",
        "algorithm",
        "Experience",
        "migrating",
        "workloads",
        "Azure",
        "VM",
        "Developed",
        "Spark",
        "code",
        "Scala",
        "SparkSQL",
        "testing",
        "data",
        "SQL",
        "PLSQL",
        "development",
        "Procedures",
        "Functions",
        "Packages",
        "Triggers",
        "Tableau",
        "Desktop",
        "Tableau",
        "Server",
        "understanding",
        "tableau",
        "architecture",
        "Kafka",
        "Spark",
        "Streaming",
        "speed",
        "data",
        "Experience",
        "AWS",
        "solutions",
        "EC2",
        "S3",
        "Azure",
        "storage",
        "business",
        "reports",
        "SQL",
        "queries",
        "views",
        "macros",
        "tables",
        "AWS",
        "team",
        "Apache",
        "Spark",
        "ETL",
        "application",
        "EMREC2",
        "S3",
        "Experience",
        "time",
        "data",
        "workflows",
        "Oozie",
        "work",
        "flow",
        "schedulers",
        "data",
        "architecture",
        "data",
        "ingestion",
        "pipeline",
        "design",
        "data",
        "Configuration",
        "ElasticSearch",
        "Amazon",
        "Web",
        "Service",
        "IP",
        "authentication",
        "security",
        "Experience",
        "AWS",
        "Cloud",
        "platform",
        "features",
        "EC2",
        "AMI",
        "EBS",
        "Cloudwatch",
        "AWS",
        "Config",
        "Autoscaling",
        "IAM",
        "user",
        "management",
        "AWS",
        "S3",
        "Managed",
        "AWS",
        "EC2",
        "instances",
        "Auto",
        "Scaling",
        "Elastic",
        "Load",
        "Balancing",
        "Glacier",
        "QA",
        "environments",
        "infrastructure",
        "servers",
        "GI",
        "performance",
        "issues",
        "Hive",
        "Pig",
        "scripts",
        "understanding",
        "Joins",
        "Group",
        "Aggregation",
        "MapReduce",
        "jobs",
        "Work",
        "Experience",
        "Senior",
        "Spark",
        "DeveloperHadoop",
        "Developer",
        "Synchrony",
        "Financial",
        "Bank",
        "Atlanta",
        "GA",
        "May",
        "Present",
        "Responsibilities",
        "tools",
        "Presto",
        "datasets",
        "tables",
        "Revenue",
        "DataFeedRDF",
        "revenue",
        "advertisers",
        "Facebook",
        "testing",
        "migration",
        "Presto",
        "Data",
        "migration",
        "Data",
        "Data",
        "profiling",
        "ETL",
        "Processes",
        "features",
        "data",
        "warehouses",
        "HiveSQL",
        "queries",
        "Spark",
        "transformations",
        "Spark",
        "RDDs",
        "Scala",
        "tools",
        "Hadoop",
        "Ecosystem",
        "Pig",
        "Hive",
        "HDFS",
        "Sqoop",
        "Spark",
        "Yarn",
        "Oozie",
        "Python",
        "Spark",
        "Python",
        "API",
        "PySpark",
        "Spark",
        "DataFrames",
        "time",
        "data",
        "Hadoop",
        "kafka",
        "Oozie",
        "job",
        "SQL",
        "Queries",
        "Stored",
        "Procedures",
        "Triggers",
        "Views",
        "Cursors",
        "Joins",
        "Constraints",
        "DDL",
        "DML",
        "User",
        "Defined",
        "Functions",
        "business",
        "logic",
        "Custom",
        "ETL",
        "Solution",
        "Batch",
        "processing",
        "RealTime",
        "data",
        "ingestion",
        "pipeline",
        "data",
        "Hadoop",
        "Python",
        "Script",
        "Experience",
        "Data",
        "processing",
        "transformation",
        "HadoopHive",
        "Sqoop",
        "time",
        "analytics",
        "capabilities",
        "Spark",
        "Streaming",
        "Spark",
        "SQL",
        "Oracle",
        "Data",
        "Mining",
        "tools",
        "Experience",
        "Tableau",
        "Data",
        "Acquisition",
        "visualizations",
        "AWS",
        "team",
        "Apache",
        "Spark",
        "ETL",
        "application",
        "EMREC2",
        "S3",
        "Assisted",
        "data",
        "analysis",
        "star",
        "schema",
        "modeling",
        "data",
        "warehousing",
        "business",
        "intelligence",
        "environment",
        "pyspark",
        "Jupyter",
        "Docker",
        "Containers",
        "Expertise",
        "platform",
        "Hadoop",
        "Production",
        "support",
        "tasks",
        "job",
        "logs",
        "Spark",
        "Python",
        "RDD",
        "library",
        "Py4j",
        "pyspark",
        "module",
        "Monitored",
        "System",
        "health",
        "logs",
        "warning",
        "failure",
        "conditions",
        "EnvironmentAmazon",
        "Web",
        "ServiceVertica",
        "InformaticaPowerCenter",
        "Pyspark",
        "Spark",
        "AWS",
        "Kafka",
        "AWSS3",
        "ApacheHadoop",
        "Hive",
        "Pig",
        "Shell",
        "Script",
        "ETL",
        "tableau",
        "Agile",
        "Methodology",
        "Spark",
        "DeveloperHadoop",
        "developer",
        "View",
        "Lift",
        "New",
        "York",
        "NY",
        "September",
        "April",
        "Responsibilities",
        "data",
        "Spark",
        "Streaming",
        "AWS",
        "S3",
        "bucket",
        "nearrealtime",
        "Transformations",
        "Aggregation",
        "fly",
        "learner",
        "data",
        "model",
        "data",
        "HDFS",
        "Developed",
        "Spark",
        "Code",
        "Scala",
        "SparkSQLStreaming",
        "testing",
        "processing",
        "data",
        "Realtime",
        "experience",
        "Hadoop",
        "files",
        "system",
        "Hadoop",
        "framework",
        "processing",
        "implementation",
        "AWS",
        "hands",
        "experience",
        "HDFS",
        "Glue",
        "Data",
        "Catalog",
        "schemametadata",
        "Hive",
        "External",
        "tables",
        "Aws",
        "EMR",
        "clusters",
        "spark",
        "jobs",
        "AWS",
        "S3",
        "data",
        "buckets",
        "Hive",
        "InternalExternal",
        "tables",
        "metastore",
        "metadata",
        "sqoop",
        "jobs",
        "ingestion",
        "sources",
        "Oracle",
        "Salesforce",
        "SAS",
        "SQL",
        "source",
        "side",
        "Oracle",
        "database",
        "cloudformation",
        "template",
        "CICD",
        "tools",
        "concourse",
        "data",
        "pipeline",
        "AWS",
        "CLI",
        "Auto",
        "Scaling",
        "Cloud",
        "Watch",
        "Monitoring",
        "creation",
        "Designed",
        "Data",
        "Quality",
        "Framework",
        "schema",
        "validation",
        "data",
        "profiling",
        "Spark",
        "Pyspark",
        "Experience",
        "AWS",
        "solutions",
        "EC2",
        "S3",
        "Azure",
        "storage",
        "tableau",
        "dashboard",
        "purpose",
        "data",
        "users",
        "Environment",
        "AWS",
        "EMR",
        "S3",
        "Spark",
        "Spark",
        "sql",
        "Scala",
        "Azure",
        "Hive",
        "Sqoop",
        "AWSCli",
        "Shell",
        "Script",
        "ETL",
        "Tableau",
        "Agile",
        "Methodology",
        "Hadoop",
        "Developer",
        "Cognizant",
        "Technology",
        "Solutions",
        "Teaneck",
        "NJ",
        "October",
        "August",
        "Responsibilities",
        "variables",
        "parameter",
        "files",
        "ETL",
        "framework",
        "parameter",
        "files",
        "Teradata",
        "HP",
        "Vertica",
        "Data",
        "Migration",
        "Project",
        "Working",
        "Copy",
        "Command",
        "data",
        "files",
        "Vertica",
        "ETL",
        "process",
        "job",
        "data",
        "Vertica",
        "DW",
        "FullService",
        "Catalog",
        "System",
        "workflow",
        "ElasticSearch",
        "Logstash",
        "Kibana",
        "Kinesis",
        "CloudWatch",
        "Responsible",
        "data",
        "extraction",
        "data",
        "ingestion",
        "data",
        "sources",
        "Hadoop",
        "Data",
        "Lake",
        "ETL",
        "pipelines",
        "Pig",
        "Hive",
        "data",
        "data",
        "sources",
        "HDFS",
        "systems",
        "producers",
        "consumers",
        "brokers",
        "logs",
        "content",
        "HDFS",
        "PIG",
        "data",
        "Hivewarehouse",
        "business",
        "analysts",
        "Hive",
        "queries",
        "data",
        "migration",
        "form",
        "Hadoop",
        "clusters",
        "knowledge",
        "cloud",
        "components",
        "AWS",
        "S3",
        "EMR",
        "Elastic",
        "Cache",
        "EC2",
        "Responsible",
        "Hiveand",
        "Pig",
        "scripts",
        "ETL",
        "tool",
        "transformations",
        "event",
        "filter",
        "traffic",
        "preaggregations",
        "HDFS",
        "Developed",
        "Vertica",
        "UDFs",
        "data",
        "analysis",
        "reporting",
        "application",
        "Spark",
        "SQL",
        "reports",
        "HBase",
        "Build",
        "custom",
        "batch",
        "aggression",
        "framework",
        "reporting",
        "aggregates",
        "Hadoop",
        "Experience",
        "Hive",
        "data",
        "warehouse",
        "tables",
        "data",
        "distribution",
        "bucketing",
        "writing",
        "Hive",
        "queries",
        "time",
        "pipeline",
        "streaming",
        "data",
        "Kafka",
        "SparkStreaming",
        "databases",
        "HBase",
        "MongoDB",
        "Cassandra",
        "Storm",
        "topology",
        "events",
        "Kafka",
        "producer",
        "Cassandra",
        "DB",
        "spark",
        "ecosystem",
        "spark",
        "SQL",
        "scala",
        "queries",
        "formats",
        "Text",
        "file",
        "CSV",
        "file",
        "hands",
        "experience",
        "Pyspark",
        "Spark",
        "libraries",
        "python",
        "scripting",
        "data",
        "analysis",
        "Wrote",
        "Python",
        "Script",
        "databases",
        "scripts",
        "commands",
        "CassandraHiveSQL",
        "queries",
        "Spark",
        "transformations",
        "Spark",
        "RDDs",
        "Scala",
        "Python",
        "Created",
        "ODBC",
        "connection",
        "Sqoop",
        "Hortonworks",
        "SQL",
        "Server",
        "Building",
        "reports",
        "dashboards",
        "scheduling",
        "Tableau",
        "server",
        "New",
        "Schedules",
        "tasks",
        "server",
        "Environment",
        "Hadoop",
        "Hive",
        "Apache",
        "Spark",
        "Apache",
        "Kafka",
        "Hortonworks",
        "AWS",
        "ElasticSearch",
        "Lambda",
        "Apache",
        "Cassandra",
        "Hbase",
        "MongoDB",
        "SQL",
        "Sqoop",
        "Flume",
        "Oozie",
        "Java",
        "jdk",
        "Eclipse",
        "InformaticaPower",
        "Center",
        "Tableau",
        "Teradata",
        "13x",
        "Teradata",
        "SQL",
        "Assistant",
        "Java",
        "Developer",
        "PTC",
        "Boston",
        "MA",
        "August",
        "September",
        "Responsibilities",
        "testing",
        "implementation",
        "system",
        "Struts",
        "JSF",
        "Hibernate",
        "Developing",
        "testing",
        "Java",
        "JSP",
        "XML",
        "Servlet",
        "SQLs",
        "JSF",
        "web",
        "pages",
        "JSP",
        "CSSto",
        "Html5",
        "CSS",
        "JavaScript",
        "user",
        "experience",
        "Developed",
        "Servlets",
        "Session",
        "Entity",
        "Beans",
        "business",
        "logic",
        "data",
        "enterprise",
        "deployment",
        "strategy",
        "enterprise",
        "deployment",
        "process",
        "Web",
        "Services",
        "J2EE",
        "programs",
        "instances",
        "development",
        "test",
        "production",
        "environments",
        "user",
        "interface",
        "HTML",
        "Swing",
        "CSS",
        "XML",
        "Java",
        "Script",
        "JSP",
        "presentation",
        "combination",
        "Java",
        "Server",
        "Pages",
        "JSP",
        "HTML",
        "API",
        "interface",
        "access",
        "application",
        "services",
        "layer",
        "Enterprise",
        "Java",
        "Beans",
        "EJBs",
        "application",
        "Session",
        "Beans",
        "user",
        "authentication",
        "Requirement",
        "Analysis",
        "Design",
        "Code",
        "Testing",
        "Implementation",
        "activities",
        "Performance",
        "Tuning",
        "Database",
        "Informatica",
        "performance",
        "performance",
        "bottle",
        "necks",
        "technologies",
        "data",
        "problems",
        "data",
        "solutions",
        "Job",
        "flows",
        "Oozie",
        "Developed",
        "Sqoop",
        "data",
        "Teradata",
        "data",
        "sources",
        "Avro",
        "models",
        "transformations",
        "standardizations",
        "HBase",
        "data",
        "processing",
        "Wrote",
        "PLSQL",
        "Packages",
        "procedures",
        "business",
        "rules",
        "Environment",
        "Java",
        "J2EE",
        "Java",
        "Server",
        "Pages",
        "JSP",
        "JavaScriptHadoop",
        "Oozie",
        "Hive",
        "Teradata",
        "Servlets",
        "JDBC",
        "PLSQL",
        "ODBC",
        "Struts",
        "Framework",
        "XML",
        "CSS",
        "HTML",
        "DHTML",
        "XSL",
        "XSLT",
        "MySQL",
        "Java",
        "Developer",
        "New",
        "York",
        "NY",
        "February",
        "July",
        "Responsibilities",
        "Requirements",
        "analysis",
        "design",
        "entity",
        "relations",
        "table",
        "design",
        "Ability",
        "application",
        "deployments",
        "systems",
        "upgrading",
        "ones",
        "ATG",
        "methodologies",
        "GUI",
        "framework",
        "Swing",
        "Creation",
        "Adobe",
        "Flex",
        "Families",
        "Content",
        "Server",
        "JDBC",
        "XSD",
        "XML",
        "pages",
        "assets",
        "Selenium",
        "test",
        "scripts",
        "browsers",
        "compatibility",
        "regression",
        "test",
        "cases",
        "Selenium",
        "Web",
        "Driver",
        "Web",
        "Driver",
        "Backed",
        "Selenium",
        "validation",
        "frameworks",
        "validations",
        "work",
        "Web",
        "services",
        "SOAP",
        "application",
        "Java",
        "Script",
        "Client",
        "Side",
        "methods",
        "JNI",
        "inhouse",
        "custom",
        "RF",
        "optimization",
        "drive",
        "test",
        "software",
        "support",
        "SDLC",
        "concepts",
        "SDLC",
        "Agile",
        "Scrum",
        "RUP",
        "Waterfall",
        "concepts",
        "Java",
        "bean",
        "components",
        "Mapping",
        "Hibernate",
        "roles",
        "groups",
        "users",
        "resources",
        "AWS",
        "Identity",
        "Access",
        "Management",
        "IAM",
        "screen",
        "classes",
        "Action",
        "classes",
        "business",
        "logic",
        "Pilot",
        "programming",
        "WebSphere",
        "Application",
        "Server",
        "JVM",
        "logs",
        "Process",
        "Logs",
        "Service",
        "logs",
        "Design",
        "Development",
        "Support",
        "phases",
        "Software",
        "Development",
        "Life",
        "Cycle",
        "SDLC",
        "Custom",
        "Tags",
        "JSP",
        "code",
        "UI",
        "screens",
        "JSP",
        "CSS",
        "XML",
        "HTML",
        "JavaScript",
        "client",
        "side",
        "validation",
        "Expertise",
        "DevOps",
        "strategy",
        "mix",
        "environment",
        "Linux",
        "servers",
        "implementation",
        "application",
        "system",
        "core",
        "java",
        "Spring",
        "framework",
        "Rational",
        "Rose",
        "model",
        "development",
        "UML",
        "modeling",
        "Spring",
        "Core",
        "Annotations",
        "Dependency",
        "Injection",
        "Apache",
        "Camel",
        "Spring",
        "framework",
        "use",
        "Apache",
        "Camel",
        "messages",
        "customer",
        "flow",
        "Apache",
        "Velocity",
        "template",
        "PHPHTML5CSS3",
        "Web",
        "pages",
        "Comcast",
        "Business",
        "Voice",
        "Xpress",
        "VOIP",
        "phone",
        "support",
        "portal",
        "practices",
        "Rally",
        "management",
        "software",
        "JSTL",
        "tags",
        "Struts",
        "tag",
        "Struts",
        "tiles",
        "presentation",
        "tier",
        "Spring",
        "AOP",
        "components",
        "Transactional",
        "Model",
        "requests",
        "JSP",
        "Servlet",
        "components",
        "application",
        "design",
        "patterns",
        "Singleton",
        "DAO",
        "Front",
        "Controller",
        "Service",
        "Locator",
        "Business",
        "Delegate",
        "Faade",
        "Data",
        "Access",
        "Object",
        "Java",
        "Message",
        "Service",
        "JMS",
        "exchange",
        "information",
        "loan",
        "status",
        "report",
        "JMS",
        "Connection",
        "Pool",
        "implementation",
        "publish",
        "subscribe",
        "Spring",
        "JMS",
        "JMS",
        "Template",
        "Message",
        "Driven",
        "Bean",
        "MDB",
        "JMS",
        "provider",
        "Hibernate",
        "ORM",
        "solution",
        "technique",
        "mapping",
        "data",
        "representation",
        "MVC",
        "model",
        "Oracle",
        "Relational",
        "data",
        "model",
        "schema",
        "tests",
        "JRuby",
        "Gherkincucumber",
        "Spring",
        "Beans",
        "config",
        "mocks",
        "tests",
        "environment",
        "Ruby",
        "Java",
        "JRuby",
        "number",
        "AWS",
        "Services",
        "AWS",
        "account",
        "Shared",
        "Services",
        "users",
        "CORP",
        "login",
        "AWS",
        "accounts",
        "AWS",
        "federation",
        "prem",
        "Shared",
        "Services",
        "Junit",
        "framework",
        "unit",
        "testing",
        "application",
        "Log4j",
        "log",
        "runtime",
        "exceptions",
        "Agile",
        "SCRUM",
        "development",
        "methodology",
        "application",
        "Test",
        "Driven",
        "Development",
        "TDD",
        "application",
        "Web",
        "Sphere",
        "Application",
        "Server",
        "ANT",
        "build",
        "tool",
        "IVY",
        "dependency",
        "tool",
        "CVS",
        "version",
        "control",
        "application",
        "Work",
        "usage",
        "HTML",
        "DHTML",
        "CSS",
        "JQuery",
        "JavaScript",
        "Ajax",
        "client",
        "side",
        "development",
        "Rewrote",
        "pieces",
        "JSF",
        "experience",
        "database",
        "MySQL",
        "SQL",
        "data",
        "database",
        "Environment",
        "Core",
        "Java",
        "J2EE",
        "JSP",
        "Struts",
        "EJB",
        "JMS",
        "JNDI",
        "Oracle",
        "DHTML",
        "XML",
        "DOM",
        "SAX",
        "Rationale",
        "Rose",
        "Groovy",
        "UNIX",
        "IBM",
        "Web",
        "Sphere",
        "Application",
        "Server",
        "Hibernate",
        "spring",
        "LOG4J",
        "CVS",
        "Java",
        "Developer",
        "Boeing",
        "Everett",
        "San",
        "Francisco",
        "CA",
        "March",
        "January",
        "Responsibilities",
        "business",
        "teams",
        "requirements",
        "analysis",
        "building",
        "use",
        "cases",
        "estimations",
        "level",
        "level",
        "design",
        "documentation",
        "JavaJ2EE",
        "code",
        "business",
        "logic",
        "Spring",
        "Hibernate",
        "framework",
        "OOP",
        "concepts",
        "Peer",
        "code",
        "reviews",
        "Created",
        "WSDL",
        "data",
        "objects",
        "WSDL",
        "Java",
        "Spring",
        "JAXWS",
        "Axis",
        "apache",
        "CXF",
        "mapping",
        "code",
        "Web",
        "Services",
        "interfaces",
        "profile",
        "management",
        "endpoints",
        "Java",
        "multi",
        "batch",
        "bulk",
        "upload",
        "tool",
        "web",
        "applications",
        "Spring",
        "Servlets",
        "UI",
        "layer",
        "JSPs",
        "JavaScript",
        "HTML",
        "CSS",
        "Angular",
        "JS",
        "implementation",
        "implementations",
        "criticalquick",
        "deliveries",
        "Ant",
        "scripts",
        "Maven",
        "application",
        "code",
        "database",
        "scripts",
        "procedures",
        "code",
        "Tomcat",
        "web",
        "application",
        "server",
        "requirement",
        "deliverables",
        "unit",
        "testing",
        "SOAP",
        "UI",
        "system",
        "endurance",
        "performance",
        "tests",
        "JMeter",
        "Set",
        "build",
        "automation",
        "deployment",
        "DEVQAPRD",
        "servers",
        "TeamCity",
        "integration",
        "platform",
        "Regression",
        "suits",
        "SOAP",
        "UI",
        "regression",
        "test",
        "CI",
        "platform",
        "Architects",
        "Security",
        "teams",
        "SOAP",
        "web",
        "services",
        "architecture",
        "generatingmanaging",
        "artifacts",
        "documentation",
        "Confluence",
        "Performed",
        "use",
        "case",
        "analysis",
        "design",
        "SOAP",
        "REST",
        "APIs",
        "integration",
        "web",
        "applications",
        "oAuth",
        "Ping",
        "Federate",
        "CreatingManaging",
        "release",
        "plan",
        "sprint",
        "deliveries",
        "version",
        "control",
        "SVN",
        "configuration",
        "project",
        "track",
        "project",
        "activities",
        "Sprint",
        "execution",
        "planning",
        "JIRA",
        "Coordinated",
        "cross",
        "teams",
        "integrations",
        "issues",
        "bug",
        "fixes",
        "RCAs",
        "RFCs",
        "model",
        "Conducting",
        "Daily",
        "ScrumStand",
        "Backlog",
        "Grooming",
        "Sprint",
        "Planning",
        "Sprint",
        "Review",
        "Sprint",
        "Retrospective",
        "Meeting",
        "Product",
        "owner",
        "refining",
        "product",
        "backlog",
        "Lead",
        "development",
        "team",
        "Motivating",
        "team",
        "self",
        "manner",
        "Liaison",
        "Product",
        "Owner",
        "Development",
        "Team",
        "impediments",
        "order",
        "Sprint",
        "Goals",
        "SME",
        "SPOC",
        "project",
        "Adidas",
        "CRM",
        "area",
        "knowledge",
        "sharing",
        "problems",
        "issues",
        "system",
        "Master",
        "Data",
        "management",
        "Data",
        "schemas",
        "web",
        "forms",
        "configuration",
        "workflow",
        "campaign",
        "maintenance",
        "Adobe",
        "Campaign",
        "Neolane",
        "Environment",
        "Java",
        "J2EE",
        "Spring",
        "Hibernatecore",
        "JSP",
        "HTML",
        "XML",
        "CSS",
        "JavaScript",
        "SVN",
        "Oracle",
        "PLSQLJms",
        "WSDL",
        "SOAP",
        "XML",
        "JAXWS",
        "JSON",
        "TomCat",
        "Eclipse",
        "SQL",
        "Developer",
        "Toad",
        "Oracle",
        "MS",
        "Visio",
        "JIRA",
        "Confluence",
        "Maven",
        "Ant",
        "Beyond",
        "Compare",
        "Team",
        "City",
        "CI",
        "JUnit",
        "SOAP",
        "UI",
        "Apache",
        "Jmeter",
        "Unix",
        "Education",
        "Bachelors",
        "Skills",
        "DATA",
        "DB2",
        "JDBC",
        "MS",
        "SQL",
        "SERVER",
        "SQL",
        "SERVER",
        "MYSQL",
        "OLTP",
        "ORACLE",
        "SQL",
        "CASSANDRA",
        "MAHOUT",
        "OOZIE",
        "SQOOP",
        "HBASE",
        "KAFKA",
        "DATA",
        "ARCHITECTURE",
        "ETL",
        "HADOOP",
        "MAP",
        "REDUCE",
        "Information",
        "TECHNICAL",
        "SKILLS",
        "Specialties",
        "Data",
        "warehousingETLBI",
        "Concepts",
        "Data",
        "Architecture",
        "Software",
        "Development",
        "Data",
        "Modeling",
        "Business",
        "Tools",
        "Tableau",
        "10X",
        "Business",
        "XI",
        "R2",
        "InformaticaPowercenter",
        "8x",
        "OLAPOLTP",
        "Teradata",
        "13x",
        "Teradata",
        "SQL",
        "Assistant",
        "Big",
        "Data",
        "hadoop",
        "map",
        "pig",
        "hive",
        "hbase",
        "sqoop",
        "oozie",
        "zookeeper",
        "kafka",
        "spark",
        "flume",
        "storm",
        "impala",
        "mahout",
        "hue",
        "tez",
        "hcatalog",
        "storm",
        "Cassandra",
        "pyspark",
        "Cloud",
        "Technologies",
        "AWSEMR",
        "AWS",
        "S3",
        "Glue",
        "Data",
        "Catalog",
        "Kinesis",
        "Lambda",
        "ELKElastic",
        "Logstash",
        "Kibana",
        "Stack",
        "Azure",
        "Databases",
        "DB2",
        "MySQL",
        "MS",
        "SQL",
        "server",
        "Vertica",
        "Mongo",
        "DB",
        "Oracle",
        "SQL",
        "Hortonworks",
        "Cloudera",
        "Languages",
        "Python",
        "Java",
        "J2EE",
        "Scala",
        "HTML",
        "SQL",
        "JDBC",
        "JavaScript",
        "PHP",
        "Operating",
        "System",
        "Mac",
        "OS",
        "Unix",
        "Linux",
        "Various",
        "Versions",
        "Windows",
        "20037881XP",
        "Web",
        "Development",
        "HTML",
        "Java",
        "Script",
        "XML",
        "PHP",
        "JSP",
        "Servlets",
        "JavaScript",
        "Application",
        "Server",
        "Apache",
        "Tomcat",
        "WebLogic",
        "WebSphere",
        "Tools",
        "Eclipse",
        "NetBeans"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T20:21:07.187399",
    "resume_data": "Senior Spark DeveloperHadoop Developer Senior Spark span lDeveloperspanHadoop span lDeveloperspan Senior Spark DeveloperHadoop Developer Synchrony Financial Bank Atlanta GA Around 9years of experience in HadoopBig Data technologies such as in Hadoop Pig Hive HBase Oozie Zookeeper Sqoop Storm Flink Flume Zookeeper Impala Tez Kafka and Spark with hands on experience in writing Map ReduceYARN and SparkScala jobs Have good IT experience with special emphasis on Analysis Design and Development and Testing of ETL methodologies in all the phases of the Data Warehousing Expertise in OLTPOLAP System Study Analysis and ER modeling developing Database Schemas like star schema and Snowflake schema used in relational dimensional modeling Experience in optimizing and performance tuning of Mappings and implementing the complex business rules by creating reusable Transformations Mapplets and Tasks Solid Experience in Cloud with Amazon Web services AWS EC2 S3 CloudWatch RDSEMRSNS and Google cloud GCP Responsible for developing data pipeline using flume Sqoop and pig to extract the data from weblogs and store in HDFS Queried Vertica SQL Server for data validation along with developing validation worksheets in Excel in order to validate the dashboards on Tableau Have knowledge on GCP cloud PubSub and Microservices Event sourcing cloud functions and algorithm Experience in implementing and migrating and deploying workloads on Azure VM Developed Spark code using Scala and SparkSQL for faster testing and data processing Extensively used SQL and PLSQL for development of Procedures Functions Packages and Triggers Experienced on Tableau Desktop Tableau Server and good understanding of tableau architecture Experienced in integrating Kafka with Spark Streaming for high speed data processing Experience in Implementing AWS solutions using EC2 S3 and Azure storage Experienced in developing business reports by writing complex SQL queries using views macros volatile and global temporary tables Working with AWS team in testing our Apache Spark ETL application on EMREC2 using S3 Experience in designing both time driven and data driven automated workflows using Oozie Experienced with work flow schedulers data architecture including data ingestion pipeline design and data modelling Configuration of ElasticSearch on Amazon Web Service with static IP authentication security features Experience in AWS Cloud platform and its features which includes EC2 AMI EBS Cloudwatch AWS Config Autoscaling IAM user management and AWS S3 Managed AWS EC2 instances utilizing Auto Scaling Elastic Load Balancing and Glacier for our QA and UAT environments as well as infrastructure servers for GI Solved performance issues in Hive and Pig scripts with understanding of Joins Group and Aggregation and how does it translate to MapReduce jobs Work Experience Senior Spark DeveloperHadoop Developer Synchrony Financial Bank Atlanta GA May 2018 to Present Responsibilities Worked on different tools for Presto to process these large datasets Worked on Core tables of Revenue DataFeedRDF that calculates the revenue of the advertisers of the Facebook Involved into testing and migration to Presto Worked extensively with Data migration Data cleansing Data profiling and ETL Processes features for data warehouses Involved in converting HiveSQL queries into Spark transformations using Spark RDDs and Scala Experienced with the tools in Hadoop Ecosystem including Pig Hive HDFS Sqoop Spark Yarn and Oozie Have used Python with the Spark Python API PySpark to create and analyze Spark DataFrames Involved in importing the real time data to Hadoop using kafka and implemented the Oozie job for daily Experienced in writing complex SQL Queries Stored Procedures Triggers Views Cursors Joins Constraints DDL DML and User Defined Functions to implement the business logic Developed Custom ETL Solution Batch processing and RealTime data ingestion pipeline to move data in and out of Hadoop using Python and shell Script Experience in Large Data processing and transformation using HadoopHive and Sqoop Real time predictive analytics capabilities using Spark Streaming Spark SQL and Oracle Data Mining tools Experience with Tableau for Data Acquisition and visualizations Working with AWS team in testing our Apache Spark ETL application on EMREC2 using S3 Assisted in data analysis star schema data modeling and design specific to data warehousing and business intelligence environment Have been using pyspark with Jupyter in Docker Containers Expertise in platform related Hadoop Production support tasks by analyzing the job logs Have been using Spark with Python working with RDD provided by the library Py4j in pyspark module Monitored System health and logs and responded accordingly to any warning or failure conditions EnvironmentAmazon Web ServiceVertica InformaticaPowerCenter Pyspark Spark AWS Kafka AWSS3 ApacheHadoop Hive Pig Shell Script ETL tableau Agile Methodology Spark DeveloperHadoop developer View Lift New York NY September 2016 to April 2018 Responsibilities Collected data using Spark Streaming from AWS S3 bucket in nearrealtime and performs necessary Transformations and Aggregation on the fly to build the common learner data model and persists the data in HDFS Developed Spark Code using Scala and SparkSQLStreaming for faster testing and processing of data Realtime experience in Hadoop Distributed files system Hadoop framework and Parallel processing implementation AWS EMRCloudera with hands on experience in HDFS Using Glue Data Catalog for storing the schemametadata of Hive External tables Used Aws EMR long running clusters for processing the spark jobs use AWS S3 for storing data in buckets Created Hive InternalExternal tables metastore for storing the metadata Used sqoop jobs for ingestion from various sources such as Oracle Salesforce SAS etc Worked on various complex SQL queries on the source side which is Oracle database Worked on using cloudformation template CICD tools like concourse to automate the data pipeline Worked on AWS CLI Auto Scaling and Cloud Watch Monitoring creation and update Designed Data Quality Framework to perform schema validation and data profiling on Spark Pyspark Experience in Implementing AWS solutions using EC2 S3 and Azure storage Responsible for monitor the tableau dashboard for reporting purpose and providing the refined data to end users Environment AWS EMR S3 Spark Spark sql Scala Azure Hive Sqoop AWSCli Shell Script ETL Tableau Agile Methodology Hadoop Developer Cognizant Technology Solutions Teaneck NJ October 2014 to August 2016 Responsibilities Worked with variables and parameter files and designed ETL framework to create parameter files to make it dynamic Currently working on the Teradata to HP Vertica Data Migration Project Working extensively on the Copy Command for extracting the data from the files to Vertica Monitor the ETL process job and validate the data loaded in Vertica DW Built a FullService Catalog System which has a full workflow using ElasticSearch Logstash Kibana Kinesis CloudWatch Responsible for data extraction and data ingestion from different data sources into Hadoop Data Lake by creating ETL pipelines using Pig and Hive Experienced in transferring data from different data sources into HDFS systems using kafka producers consumers and kafka brokers The logs and semi structured content that are stored on HDFS were preprocessed using PIG and the processed data is imported into Hivewarehouse which enabled business analysts to write Hive queries Worked with data migration form Hadoop clusters to cloud Good knowledge of cloud components like AWS S3 EMR Elastic Cache and EC2 Responsible to write Hiveand Pig scripts as ETL tool to do transformations event joins filter both traffic and some preaggregations before storing into the HDFS Developed the Vertica UDFs to preprocess the data for analysis Designed the reporting application that uses the Spark SQL to fetch and generate reports on HBase Build custom batch aggression framework for creating reporting aggregates in Hadoop Experience in working with Hive data warehouse toolcreating tables data distribution by implementing partitioning and bucketing writing and optimizing the Hive queries Built real time pipeline for streaming data using Kafka and SparkStreaming Experienced with NoSQL databases like HBase MongoDB and Cassandra and wrote Storm topology to accept the events from Kafka producer and emit into Cassandra DB Experienced in working with spark ecosystem using spark SQL and scala queries on different formats like Text file CSV file Great hands on experience with Pyspark for using Spark libraries by using python scripting for data analysis Wrote Python Script to access databases and execute scripts and commands Involved in converting CassandraHiveSQL queries into Spark transformations using Spark RDDs in Scala and Python Created ODBC connection through Sqoop between Hortonworks and SQL Server Building publishing customized interactive reports and dashboards report scheduling using Tableau server Creating New Schedules and checking the tasks daily on the server Environment Hadoop Hive Apache Spark Apache Kafka Hortonworks AWS ElasticSearch Lambda Apache Cassandra Hbase MongoDB SQL Sqoop Flume Oozie Java jdk 16 Eclipse InformaticaPower Center 91 Tableau Teradata 13x Teradata SQL Assistant Java Developer PTC Boston MA August 2012 to September 2014 Responsibilities Involved in developing testing and implementation of the system using Struts JSF and Hibernate Developing modifying fixing reviewing testing and migrating the Java JSP XML Servlet SQLs JSF Updated userinteractive web pages from JSP and CSSto Html5 CSS and JavaScript for the best user experience Developed Servlets Session and Entity Beans handling business logic and data Created enterprise deployment strategy and designed the enterprise deployment process to deploy Web Services J2EE programs on more than 7 different SOAWebLogic instances across development test and production environments Designed user interface HTML Swing CSS XML Java Script and JSP Implemented the presentation using a combination of Java Server Pages JSP to render the HTML and welldefined API interface to allow access to the application services layer Used Enterprise Java Beans EJBs extensively in the application Developed and deployed Session Beans to perform user authentication Involve in Requirement Analysis Design Code Testing and debugging Implementation activities Involved in the Performance Tuning of Database and Informatica Improved performance by identifying and rectifying the performance bottle necks Understanding how to apply technologies to solve big data problems and to develop innovative big data solutions Designed and developed Job flows using Oozie Developed Sqoop commands to pull the data from Teradata The data is collected from distributed sources into Avro models Applied transformations and standardizations and loaded into HBase for further data processing Wrote PLSQL Packages and Stored procedures to implement business rules and validations Environment Java J2EE Java Server Pages JSP JavaScriptHadoop Oozie Hive Teradata Servlets JDBC PLSQL ODBC Struts Framework XML CSS HTML DHTML XSL XSLT and MySQL Java Developer New York NY February 2011 to July 2012 Responsibilities Involved in Requirements study Functional analysis detailed design including entity relations and various table design Ability to support application deployments building new systems and upgrading and patching existing ones through ATG methodologies Designed and implemented a GUI framework for Swing Involved in Creation of Adobe Flex Families in Content Server and associated the JDBC and XSD XML pages to the assets Involved in executing all Selenium test scripts on the different browsers and checked for compatibility regression test cases were automated using Selenium Web Driver and Web Driver Backed Selenium Used validation frameworks for specifying the validations rules Extensive work on Web services SOAP and Restful application Developed Java Script for Client Side validations Uses coding methods in JNI to initiate or enhance inhouse custom developed RF optimization drive test software in support of 1Xused SDLC concepts Involved in applying SDLC Agile Scrum RUP Waterfall concepts Designed and developed the Java bean components and OR Mapping using Hibernate designed roles and groups for users and resources using AWS Identity Access Management IAM Involved in writing the screen classes and Action classes for implementing the business logic of Pilot and object oriented programming and monitored and responsible for troubleshooting the WebSphere Application Server with JVM logs Process Logs Service logs Involved in Design Development and Support phases of Software Development Life Cycle SDLC developed Custom Tags to simplify the JSP code Designed UI screens using JSP CSS XML and HTML Used JavaScript for client side validation Expertise in creating DevOps strategy in a mix environment of Linux servers responsible for the implementation of application system with core java and Spring framework uses Rational Rose for model driven development and UML modeling Used Spring Core Annotations for Dependency Injection and used Apache Camel to integrate Spring framework use Apache Camel to route and transform messages and designed and implemented new customer flow using Apache Velocity template Created PHPHTML5CSS3 Web pages to support Comcast Business Voice Xpress VOIP phone support portal using Agile practices and Rally management software Extensively used JSTL tags and Struts tag libraries Used Struts tiles as well in the presentation tier participated in coding Spring AOP components for the Transactional Model to handle many requests Involved in writing JSP and Servlet components Actively involved in designing and implementing the application using various design patterns such as Singleton DAO Front Controller Service Locator Business Delegate Faade and Data Access Object used Java Message Service JMS for reliable and asynchronous exchange of important information such as loan status report Involved in the JMS Connection Pool and the implementation of publish and subscribe using Spring JMS Used JMS Template to publish and Message Driven Bean MDB to subscribe from the JMS provider Used Hibernate objectrelationalmapping ORM solution technique of mapping data representation from MVC model to Oracle Relational data model with SQLbased schema Develop Isolated tests in JRuby with Gherkincucumber using Spring Beans config and mocks and execute the tests in an isolated environment and Implemented in Ruby Java and JRuby and used a number of AWS Services Also help set up some of the AWS account peered with Shared Services so some users can use their CORP login creds for logging into AWS accounts help set AWS federation with on prem Shared Services Used Junit framework for unit testing of application and Log4j to capture the log that includes runtime exceptions Worked on Agile SCRUM development methodology and built the application with Test Driven Development TDD deployed the application on Web Sphere Application Server Used ANT as a build tool and IVY as dependency tool Used CVS version control for implementing the application Work involved extensive usage of HTML DHTML CSS JQuery JavaScript and Ajax for client side development and validations Rewrote several pieces to make them compliant with the emerging JSF standard experience in working with relational database MySQL developed complex SQL queries for extracting data from the database Environment Core Java J2EE JSP 20 Struts 12 EJB 20 JMS JNDI Oracle DHTML XML DOM SAX Rationale Rose Groovy gails UNIX IBM Web Sphere Application Server 51 Hibernate 20 spring LOG4J CVS Java Developer Boeing Everett San Francisco CA March 2009 to January 2011 Responsibilities Worked with business teams on requirements analysis building use cases and estimations Generating high level and low level design documentation Developed JavaJ2EE code business logic using Spring Hibernate framework and OOP concepts involved in Peer code reviews Created WSDL Generated data objects using WSDL Java Spring JAXWS Axis apache CXF and developed mapping code for several Web Services interfaces for various profile management endpoints Developed Java multi threaded batch offline bulk upload tool web applications using Spring Servlets and UI layer using JSPs JavaScript HTML CSS Angular JS Worked on implementation of new complex implementations criticalquick deliveries Developed and build Ant scripts Maven for packaging the application code Developed database scripts and procedures using PLSQL Deployed code on Tomcat web application server Validated requirement deliverables unit testing using SOAP UI set up executed system endurance performance tests using JMeter Set up build automation deployment to DEVQAPRD servers using TeamCity continuous integration platform Built Regression suits using SOAP UI for automated regression test on CI platform Coordinated with Architects and Security teams on defining SOAP RESTful web services architecture and generatingmanaging artifacts documentation Confluence Performed use case analysis and design SOAP REST APIs on social integration of web and native mobile applications implementing oAuth Ping Federate CreatingManaging release plan sprint deliveries responsible for version control SVN and configuration of the project keeping track of project activities Sprint execution planning in JIRA Coordinated with cross functional teams on resolving integrations issues bug fixes RCAs RFCs Worked in Agile model Conducting Daily ScrumStand ups Backlog Grooming Sprint Planning Sprint Review Sprint Retrospective Meeting supporting Product owner in refining and grooming product backlog Lead development team Motivating team and helping them work in self organized manner Acted as Liaison Between Product Owner and Development Team resolving impediments in order to achieve Sprint Goals Worked as SME and SPOC for the project within Adidas CRM area Managing knowledge sharing resolved problems on critical issues in live system Master Data management Data schemas web forms configuration workflow campaign maintenance and set up using Adobe Campaign formerly Neolane Environment Java J2EE Spring Hibernatecore java JSP HTML XML CSS JavaScript SubversionSVN SVN Oracle PLSQLJms WSDL SOAP XML JAXWS RESTful JSON TomCat Eclipse SQL Developer Toad for Oracle MS Visio JIRA Confluence Maven Ant Beyond Compare Team City CI JUnit SOAP UI Apache Jmeter Unix Education Bachelors Skills DATA MODELING DB2 JDBC MS SQL SERVER SQL SERVER MYSQL OLTP ORACLE SQL CASSANDRA MAHOUT OOZIE SQOOP HBASE KAFKA DATA ARCHITECTURE ETL HADOOP MAP REDUCE OLAP Additional Information TECHNICAL SKILLS Specialties Data warehousingETLBI Concepts Data Architecture Software Development methodologies Data Modeling Business Tools Tableau 10X Business Objects XI R2 InformaticaPowercenter 8x OLAPOLTP Talend Teradata 13x Teradata SQL Assistant Big Data hadoop map reduce 1020 pig hive hbase sqoop oozie zookeeper kafka spark flume storm impala mahout hue tez hcatalog storm Cassandra pyspark Cloud Technologies AWSEMR AWS S3 Glue Data Catalog Kinesis Lambda ELKElastic Logstash Kibana Stack cloudwatch metric Azure Databases DB2 MySQL MS SQL server Vertica Mongo DB Oracle SQL 2008 Hortonworks Cloudera Languages Python Java J2EE Scala HTML SQL JDBC JavaScript PHP Operating System Mac OS Unix Linux Various Versions Windows 20037881XP Web Development HTML Java Script XML PHP JSP Servlets JavaScript Application Server Apache Tomcat WebLogic WebSphere Tools Eclipse NetBeans",
    "unique_id": "ab575620-f58c-4461-add6-e7d7a68d20e9"
}