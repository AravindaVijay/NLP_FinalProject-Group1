{
    "clean_data": "Hadoopspark Developer Hadoopspark span lDeveloperspan Hadoopspark Developer Verizon Wireless Fort Worth TX Around 7 years of IT experience with over Four years of work experience in Big Data Hadoop Experience in working on various components in Hadoop ecosystem like HDFS YARN HIVE SPARK MAPREDUCE PIG HBASE SCOOP FLUME KAFKA OOZIE and ZOOKEEPER Excellent programming skills in SCALA JAVA and PYTHON Experience in Managing scalable Hadoop clusters including Cluster designing provisioning custom configurations monitoring and maintaining using Hadoop distributions Cloudera CDH HortonWorks HDP Experience in migrating an existing onpremises application to AWS Using AWS services like EC2 and S3 for small data sets processing and storage and Maintaining the Hadoop cluster on AWS EMR Experience in using Sequence files like ORC AVRO and Parquet file formats Exposure to Data Lake Implementation using Apache Spark and developed Data pipe lines and applied business logics using Spark Experience in working on Spark APIs like Spark Core Spark SQL and Spark Streaming Experience in performing Transformations and Actions on data using Spark RDDs Datasets DataFrames and DStreams Worked on minimizing data transfers using Broadcast variables and Accumulators in Spark Experience in monitoring scheduler stages and tasks RDD sizes and memory usage Environmental information Information about the running executors using Spark UI Experience in using dependency management tools like SBT to create a selfcontained jar file Well versed with developing and implementing MapReduce jobs using Hadoop to work with Big Data Implemented the processing framework for converting SQL to a graph of MapReduce jobs and the execution time framework to run those jobs in the order of dependencies using Hive Expertise in using Aggregate functions in Hive using HQL Worked on partitioning Hive tables and running the scripts in parallel to reduce runtime of the scripts Ability to develop Pig UDFs to preprocess the data for analysis Experience in performing transformations like event joins filter bot traffic and some preaggregations using PIG Worked on HBase to put the data in indexed StoreFiles that exist on HDFS for highspeed lookups Expert knowledge on MongoDB NoSQL data modeling tuning disaster recovery backup used it for distributed storage and processing using CRUD Worked on Ad hoc queries Indexing Replication and Load balancing Developed Sqoop Jobs to load data from Relational Database Systems like Oracle and MySQL into HDFS and Hive Used Kafka to read and write streams of data like a messaging system in Hadoop Experience in managing Hadoop jobs using Oozie Generated various kinds of reports using Power BI and Tableau based on Client specification Experience in all stages of SDLC Agile Waterfall writing Technical Design document Development Testing and Implementation of Enterprise level Data mart and Data warehouses Work Experience Hadoopspark Developer Verizon Wireless Fort Worth TX March 2017 to Present The project deals with analyzing call logs and complex data from multiple sources using Apache Hadoop and spark across the data to predict the likelihood that any particular customer would leave Hadoop helped company build more valuable customer relationships and reduce churn Responsibilities Responsible for building scalable distributed data solutions using Apache Hadoop and Spark Involved in combining traditional trasactional and event data with social network data Involved in loading data from relational database into HDFS using Sqoop Deployed Scalable Hadoop cluster on AWS using S3 as underlying file system for Hadoop Worked with spark core Spark Streaming and spark SQL modules of Spark Involved in developing generic SparkScala functions for transformations aggregations and designing schema for rows Experienced in working with Spark APIs like RDDs Datasets DataFrames and DStreams to perform transformations on the data Used Spark SQL to perform interactive analysis on the data using SQL and HiveQL Experienced in processing live data streams using Spark Streaming with high level functions like map reduce join and window Experienced in minimizing data transfers over Hadoop clusters using Spark optimizations like broadcasts variables and Accumulators Worked on troubleshooting spark application to make them more error tolerant Involved in loading the processed data into Hive warehouse Stored the data in tabular formats using Hive tables and Hive Serdes Implemented Static partitions Dynamic partitions and Buckets in Hive Used Oozie Operational Services for scheduling workflows dynamically Used Reporting tools like Tableau to connect with Impala for generating daily reports of data Designed documented operational problems by following standards and procedures using JIRA Collaborated with the infrastructure network database application and BA teams to ensure data quality and availability Environment Hadoop 2x Spark Core Spark SQL Spark Streaming Scala Hive Sqoop Oozie Amazon EMR Tableau Impala JIRA Hadoop Developer Molina Healthcare Long Beach CA September 2015 to March 2017 The project deals with developing a centralized data repository for maintaining medical information and performing relevant transformations on the data using Hadoop and spark ecosystems to obtain optimal solutions and provide further data query capabilities to improve medical and economic value Responsibilities Developed the code for Importing and exporting data into HDFS and Hive using Sqoop Deployed multiple clusters Clouderas CDH distributions on AWS Automated Sqoop incremental imports by using Sqoop jobs and automated the jobs using Oozie Experience in creating Hive tables to apply schema on read and view the data in structured format Worked on various compression and file formats like Avro Parquet and Text formats Responsible for writing Hive Queries for analyzing data in Hive warehouse using HQL Responsible for creating complex dynamic partition tables using Hive for best performance and faster querying Experienced in developing Hive User Defined Functions in java compiling them into jars and adding them to the HDFS and executing them with Hive Queries Developed several advanced Map Reduce programs in Java as part of functional requirements for Big Data Submitted MapReduce jobs to Queues as collection of jobs to allow the system to provide specified functionality Developed multiple POCs using Spark and deployed on the Yarn cluster compared the performance of Spark with Hive and SQLTeradata Developed spark scripts by using Scala shell as per requirements Developed Kafka producers and consumers Spark clients along with components on HDFS Hive Worked on NoSQL databases like HBase for readwrite access on random realtime data Involved in defining job flows using Oozie for scheduling jobs to manage apache Hadoop jobs Tested and reported defects in an Agile Methodology perspective Environment Hadoop HDFS Hive MapReduce Spark Scala Kafka HBase Oozie Java Linux Cloudera Hadoop Developer Bank of America Charlotte NC February 2014 to July 2015 The project is to collect separate data warehouses from multiple departments and combined them into a single global repository in Hadoop for analysis to construct a new and more accurate score of the risk in its customer portfolios This accurate score allowed bank to manage its exposure better and hence increased the revenue and improved customer satisfaction Responsibilities Collected and aggregated large amounts of structured and complex data from multiple silos and combined the data and look for patterns Involved in using Apache Flume and stored the data into HDFS for analysis Implemented multiple MapReduce Jobs in java for data cleansing and preprocessing of data Developed MapReduce programs to parse the raw data populate staging tables and store the refined data in partitioned tables in the EDW Monitored workload job performance and Capacity planning using Cloudera Manager Involved in defining job flows managing and reviewing log files Worked on analyzing the web log data using the HiveQL to extract number of unique visitors per day page views and returning visitors Experienced in creating Hive schema external tables and managing views Responsible for data loading involved in creating Hive tables and partitions based on the requirement Created Hive queries that helped market analysts to spot emerging trends by comparing fresh data with EDW reference tables and historical metrics Experienced in optimizing ETL workflows using PIG Developed spark scripts by using python shell commands as per the requirement Involved in data migration from various databases to Hadoop HDFS and Hive using Sqoop Worked on Oozie workflow engine for job scheduling Used zookeeper for various types of centralized configurations Good experience in working with Tableau visualization tool using Tableau Desktop Environment Map Reduce Hive PIG Spark Flume Zookeeper Oozie Tableau Java Python Unix Java Developer Hierarchy Technologies September 2012 to November 2013 Hierarchy Technologies has been designed to facilitate the business process of tracking a Bill of Materials Order through its life cycle This encompasses the initial forecasting of the project through to the required reports being available to the relevant Finance and Billing teams for verification The solution is designed to automate as much of the business process and eliminate as many manual steps as possible Responsibilities Followed Agile Rational Unified Process throughout the lifecycle of the project Involved in requirements analysis and gathering and converting them into technical specifications using UML diagrams Implemented core framework components for executing workflows using Core Java JDBC and Servlets and JSPs Created Responsive Designs MobileTabletDesktop using HTML5 CSS3 Applied Object Oriented concepts inheritance composition interface design patterns singleton strategy Applied Spring IOC Container to facilitate Dependency Injection Used Spring AOP to implement security where cross cutting concerns were identified Responsible for developing SOAP based Web Services consuming and packaging using Axis Involved in design and decision making for Hibernate ORM mapping Responsible for designing front end system using JSP HTML jQuery and JavaScript Involved in Managing Web Services and operations Used Maven as the build tool to manage the dependencies of the project Implemented Stored Procedures for the tables in the database Develop and perform Mock Testing and Unit Testing using JUNIT and Easy Mock Involved in implementing the continuous integration using Jenkins Built project using Apache Maven build scripts Environment Java16J2EE Microsoft Visio Web Sphere Application Server Spring MVC IOC Spring AOP Apache Axis SOAP Hibernate Web services Maven jQuery JUnit Easy Mockito Git Jenkins Jr JAVA Developer Serveen Software Systems P Ltd June 2011 to September 2012 This is a web based project which is used to create surveys for the participants with a user friendly GUI This tool will not record any kind of information aboutusers giving priority to the confidentiality of the user information which includes modules like Admin Registered User and Manager Responsibilities Involved in various phases of Software Development Life Cycle SDLC Used Rational Rose for the Use Case Diagrams Object Diagrams class Diagrams and Sequence diagrams to represent the detailed design phase Frontend is designed by using HTML CSS JSP Servlets Ajax and Struts Involved in writing the exception and validation classes using Struts validation rules Used JavaScript for the web page validation Used SOAP for Web Services by exchanging XML data between applications over HTTP Created Servlets which route submittals to appropriate Enterprise Java Bean EJB components and render retrieved information Written ANT scripts for building application artifacts Environment Java J2EE Servlets Struts JSP XML DOM HTML JavaScript JDBC Web Services Eclipse Plugins Education Bachelors Skills DYNAMODB HDFS IMPALA OOZIE SQOOP Additional Information TECHNICAL SKILLS Data Access Tools HDFS YARN Hive Pig HBase Solr Impala Spark Core Spark SQL Spark Streaming Data Management HDFS YARN Data Workflow Sqoop Flume Kafka Data Operation Zookeper Oozie Data Security Ranger Knox BigData Distributions Hortonworks Cloudera Cloud Technologies AWS Amazon Web Services EC2 S3 IAM CLOUD WATCH DynamoDB SNS SQS EMR KINESIS Programming and Scripting Languages Java Scala Pig Latin HQL SQL Shell Scripting HTML CSS JavaScript IDEBuild Tools Eclipse Intellij Business Intelligence Tools Tableau Power Bi MS Excel Microsoft Visio SSIS SSAS SSRS JavaJ2EE Technologies XML Junit JDBC AJAX JSON JSP Operating Systems Linux Windows Kali Linux SDLC AgileSCRUM Waterfall",
    "entities": [
        "Impala JIRA Hadoop Developer",
        "Aggregate",
        "Data Lake Implementation",
        "Hadoop Experience",
        "HDFS",
        "Software Development Life Cycle SDLC Used Rational Rose for the Use Case Diagrams Object Diagrams class Diagrams",
        "Hortonworks Cloudera Cloud Technologies",
        "Big Data Hadoop",
        "Data mart",
        "KINESIS Programming and",
        "Hadoopspark Developer Hadoopspark",
        "AWS Using AWS",
        "RDD",
        "Hadoop",
        "Agile Methodology",
        "Transformations and Actions",
        "XML",
        "Spark Streaming Experience",
        "Cloudera Hadoop Developer Bank of America Charlotte",
        "Fort Worth",
        "HBase",
        "SparkScala",
        "TX",
        "Serveen Software Systems P Ltd",
        "Created Responsive Designs MobileTabletDesktop",
        "Spark RDDs Datasets DataFrames",
        "Responsibilities Collected",
        "NC",
        "Java Developer Hierarchy Technologies",
        "UML",
        "Client",
        "SDLC Agile Waterfall",
        "Sequence",
        "Hadoop Worked",
        "Oozie Operational Services",
        "SQLTeradata Developed",
        "AWS Automated Sqoop",
        "Big Data Implemented",
        "HTML CSS JSP Servlets Ajax",
        "JSP",
        "Data warehouses Work Experience Hadoopspark Developer",
        "Hive Queries",
        "Tableau Desktop",
        "ZOOKEEPER Excellent",
        "Jenkins Built",
        "Spark Experience",
        "Spark Streaming",
        "Parquet",
        "Environment Hadoop HDFS Hive MapReduce Spark",
        "Hive Queries Developed",
        "Applied Spring IOC Container",
        "MVC",
        "Spark",
        "EJB",
        "Created Hive",
        "Sqoop",
        "Oozie Experience",
        "Power BI",
        "Spark Core Spark",
        "AWS",
        "Mock Testing",
        "JIRA Collaborated",
        "HTTP Created Servlets",
        "PIG",
        "Spark Involved",
        "SQL",
        "Relational Database Systems",
        "OOZIE SQOOP",
        "Tested",
        "Hive",
        "Axis Involved",
        "Queues",
        "ETL",
        "HiveQL Experienced",
        "Hierarchy Technologies",
        "Apache Hadoop",
        "Maven",
        "Impala",
        "Spark UI",
        "ANT",
        "BA",
        "Maintaining the Hadoop",
        "Oozie Generated",
        "SBT",
        "Microsoft",
        "CSS",
        "RDDs Datasets DataFrames",
        "EDW",
        "PIG Worked",
        "MapReduce",
        "Cloudera",
        "StoreFiles",
        "NoSQL",
        "Tableau",
        "Admin Registered User",
        "Technical Design",
        "Responsibilities Followed Agile Rational Unified Process"
    ],
    "experience": "Experience in working on various components in Hadoop ecosystem like HDFS YARN HIVE SPARK MAPREDUCE PIG HBASE SCOOP FLUME KAFKA OOZIE and ZOOKEEPER Excellent programming skills in SCALA JAVA and PYTHON Experience in Managing scalable Hadoop clusters including Cluster designing provisioning custom configurations monitoring and maintaining using Hadoop distributions Cloudera CDH HortonWorks HDP Experience in migrating an existing onpremises application to AWS Using AWS services like EC2 and S3 for small data sets processing and storage and Maintaining the Hadoop cluster on AWS EMR Experience in using Sequence files like ORC AVRO and Parquet file formats Exposure to Data Lake Implementation using Apache Spark and developed Data pipe lines and applied business logics using Spark Experience in working on Spark APIs like Spark Core Spark SQL and Spark Streaming Experience in performing Transformations and Actions on data using Spark RDDs Datasets DataFrames and DStreams Worked on minimizing data transfers using Broadcast variables and Accumulators in Spark Experience in monitoring scheduler stages and tasks RDD sizes and memory usage Environmental information Information about the running executors using Spark UI Experience in using dependency management tools like SBT to create a selfcontained jar file Well versed with developing and implementing MapReduce jobs using Hadoop to work with Big Data Implemented the processing framework for converting SQL to a graph of MapReduce jobs and the execution time framework to run those jobs in the order of dependencies using Hive Expertise in using Aggregate functions in Hive using HQL Worked on partitioning Hive tables and running the scripts in parallel to reduce runtime of the scripts Ability to develop Pig UDFs to preprocess the data for analysis Experience in performing transformations like event joins filter bot traffic and some preaggregations using PIG Worked on HBase to put the data in indexed StoreFiles that exist on HDFS for highspeed lookups Expert knowledge on MongoDB NoSQL data modeling tuning disaster recovery backup used it for distributed storage and processing using CRUD Worked on Ad hoc queries Indexing Replication and Load balancing Developed Sqoop Jobs to load data from Relational Database Systems like Oracle and MySQL into HDFS and Hive Used Kafka to read and write streams of data like a messaging system in Hadoop Experience in managing Hadoop jobs using Oozie Generated various kinds of reports using Power BI and Tableau based on Client specification Experience in all stages of SDLC Agile Waterfall writing Technical Design document Development Testing and Implementation of Enterprise level Data mart and Data warehouses Work Experience Hadoopspark Developer Verizon Wireless Fort Worth TX March 2017 to Present The project deals with analyzing call logs and complex data from multiple sources using Apache Hadoop and spark across the data to predict the likelihood that any particular customer would leave Hadoop helped company build more valuable customer relationships and reduce churn Responsibilities Responsible for building scalable distributed data solutions using Apache Hadoop and Spark Involved in combining traditional trasactional and event data with social network data Involved in loading data from relational database into HDFS using Sqoop Deployed Scalable Hadoop cluster on AWS using S3 as underlying file system for Hadoop Worked with spark core Spark Streaming and spark SQL modules of Spark Involved in developing generic SparkScala functions for transformations aggregations and designing schema for rows Experienced in working with Spark APIs like RDDs Datasets DataFrames and DStreams to perform transformations on the data Used Spark SQL to perform interactive analysis on the data using SQL and HiveQL Experienced in processing live data streams using Spark Streaming with high level functions like map reduce join and window Experienced in minimizing data transfers over Hadoop clusters using Spark optimizations like broadcasts variables and Accumulators Worked on troubleshooting spark application to make them more error tolerant Involved in loading the processed data into Hive warehouse Stored the data in tabular formats using Hive tables and Hive Serdes Implemented Static partitions Dynamic partitions and Buckets in Hive Used Oozie Operational Services for scheduling workflows dynamically Used Reporting tools like Tableau to connect with Impala for generating daily reports of data Designed documented operational problems by following standards and procedures using JIRA Collaborated with the infrastructure network database application and BA teams to ensure data quality and availability Environment Hadoop 2x Spark Core Spark SQL Spark Streaming Scala Hive Sqoop Oozie Amazon EMR Tableau Impala JIRA Hadoop Developer Molina Healthcare Long Beach CA September 2015 to March 2017 The project deals with developing a centralized data repository for maintaining medical information and performing relevant transformations on the data using Hadoop and spark ecosystems to obtain optimal solutions and provide further data query capabilities to improve medical and economic value Responsibilities Developed the code for Importing and exporting data into HDFS and Hive using Sqoop Deployed multiple clusters Clouderas CDH distributions on AWS Automated Sqoop incremental imports by using Sqoop jobs and automated the jobs using Oozie Experience in creating Hive tables to apply schema on read and view the data in structured format Worked on various compression and file formats like Avro Parquet and Text formats Responsible for writing Hive Queries for analyzing data in Hive warehouse using HQL Responsible for creating complex dynamic partition tables using Hive for best performance and faster querying Experienced in developing Hive User Defined Functions in java compiling them into jars and adding them to the HDFS and executing them with Hive Queries Developed several advanced Map Reduce programs in Java as part of functional requirements for Big Data Submitted MapReduce jobs to Queues as collection of jobs to allow the system to provide specified functionality Developed multiple POCs using Spark and deployed on the Yarn cluster compared the performance of Spark with Hive and SQLTeradata Developed spark scripts by using Scala shell as per requirements Developed Kafka producers and consumers Spark clients along with components on HDFS Hive Worked on NoSQL databases like HBase for readwrite access on random realtime data Involved in defining job flows using Oozie for scheduling jobs to manage apache Hadoop jobs Tested and reported defects in an Agile Methodology perspective Environment Hadoop HDFS Hive MapReduce Spark Scala Kafka HBase Oozie Java Linux Cloudera Hadoop Developer Bank of America Charlotte NC February 2014 to July 2015 The project is to collect separate data warehouses from multiple departments and combined them into a single global repository in Hadoop for analysis to construct a new and more accurate score of the risk in its customer portfolios This accurate score allowed bank to manage its exposure better and hence increased the revenue and improved customer satisfaction Responsibilities Collected and aggregated large amounts of structured and complex data from multiple silos and combined the data and look for patterns Involved in using Apache Flume and stored the data into HDFS for analysis Implemented multiple MapReduce Jobs in java for data cleansing and preprocessing of data Developed MapReduce programs to parse the raw data populate staging tables and store the refined data in partitioned tables in the EDW Monitored workload job performance and Capacity planning using Cloudera Manager Involved in defining job flows managing and reviewing log files Worked on analyzing the web log data using the HiveQL to extract number of unique visitors per day page views and returning visitors Experienced in creating Hive schema external tables and managing views Responsible for data loading involved in creating Hive tables and partitions based on the requirement Created Hive queries that helped market analysts to spot emerging trends by comparing fresh data with EDW reference tables and historical metrics Experienced in optimizing ETL workflows using PIG Developed spark scripts by using python shell commands as per the requirement Involved in data migration from various databases to Hadoop HDFS and Hive using Sqoop Worked on Oozie workflow engine for job scheduling Used zookeeper for various types of centralized configurations Good experience in working with Tableau visualization tool using Tableau Desktop Environment Map Reduce Hive PIG Spark Flume Zookeeper Oozie Tableau Java Python Unix Java Developer Hierarchy Technologies September 2012 to November 2013 Hierarchy Technologies has been designed to facilitate the business process of tracking a Bill of Materials Order through its life cycle This encompasses the initial forecasting of the project through to the required reports being available to the relevant Finance and Billing teams for verification The solution is designed to automate as much of the business process and eliminate as many manual steps as possible Responsibilities Followed Agile Rational Unified Process throughout the lifecycle of the project Involved in requirements analysis and gathering and converting them into technical specifications using UML diagrams Implemented core framework components for executing workflows using Core Java JDBC and Servlets and JSPs Created Responsive Designs MobileTabletDesktop using HTML5 CSS3 Applied Object Oriented concepts inheritance composition interface design patterns singleton strategy Applied Spring IOC Container to facilitate Dependency Injection Used Spring AOP to implement security where cross cutting concerns were identified Responsible for developing SOAP based Web Services consuming and packaging using Axis Involved in design and decision making for Hibernate ORM mapping Responsible for designing front end system using JSP HTML jQuery and JavaScript Involved in Managing Web Services and operations Used Maven as the build tool to manage the dependencies of the project Implemented Stored Procedures for the tables in the database Develop and perform Mock Testing and Unit Testing using JUNIT and Easy Mock Involved in implementing the continuous integration using Jenkins Built project using Apache Maven build scripts Environment Java16J2EE Microsoft Visio Web Sphere Application Server Spring MVC IOC Spring AOP Apache Axis SOAP Hibernate Web services Maven jQuery JUnit Easy Mockito Git Jenkins Jr JAVA Developer Serveen Software Systems P Ltd June 2011 to September 2012 This is a web based project which is used to create surveys for the participants with a user friendly GUI This tool will not record any kind of information aboutusers giving priority to the confidentiality of the user information which includes modules like Admin Registered User and Manager Responsibilities Involved in various phases of Software Development Life Cycle SDLC Used Rational Rose for the Use Case Diagrams Object Diagrams class Diagrams and Sequence diagrams to represent the detailed design phase Frontend is designed by using HTML CSS JSP Servlets Ajax and Struts Involved in writing the exception and validation classes using Struts validation rules Used JavaScript for the web page validation Used SOAP for Web Services by exchanging XML data between applications over HTTP Created Servlets which route submittals to appropriate Enterprise Java Bean EJB components and render retrieved information Written ANT scripts for building application artifacts Environment Java J2EE Servlets Struts JSP XML DOM HTML JavaScript JDBC Web Services Eclipse Plugins Education Bachelors Skills DYNAMODB HDFS IMPALA OOZIE SQOOP Additional Information TECHNICAL SKILLS Data Access Tools HDFS YARN Hive Pig HBase Solr Impala Spark Core Spark SQL Spark Streaming Data Management HDFS YARN Data Workflow Sqoop Flume Kafka Data Operation Zookeper Oozie Data Security Ranger Knox BigData Distributions Hortonworks Cloudera Cloud Technologies AWS Amazon Web Services EC2 S3 IAM CLOUD WATCH DynamoDB SNS SQS EMR KINESIS Programming and Scripting Languages Java Scala Pig Latin HQL SQL Shell Scripting HTML CSS JavaScript IDEBuild Tools Eclipse Intellij Business Intelligence Tools Tableau Power Bi MS Excel Microsoft Visio SSIS SSAS SSRS JavaJ2EE Technologies XML Junit JDBC AJAX JSON JSP Operating Systems Linux Windows Kali Linux SDLC AgileSCRUM Waterfall",
    "extracted_keywords": [
        "Hadoopspark",
        "Developer",
        "Hadoopspark",
        "span",
        "lDeveloperspan",
        "Hadoopspark",
        "Developer",
        "Verizon",
        "Wireless",
        "Fort",
        "Worth",
        "TX",
        "years",
        "IT",
        "years",
        "work",
        "experience",
        "Big",
        "Data",
        "Hadoop",
        "Experience",
        "components",
        "Hadoop",
        "ecosystem",
        "HDFS",
        "YARN",
        "HIVE",
        "SPARK",
        "MAPREDUCE",
        "PIG",
        "HBASE",
        "FLUME",
        "KAFKA",
        "OOZIE",
        "ZOOKEEPER",
        "programming",
        "skills",
        "SCALA",
        "PYTHON",
        "Experience",
        "Hadoop",
        "clusters",
        "Cluster",
        "custom",
        "configurations",
        "Hadoop",
        "distributions",
        "Cloudera",
        "CDH",
        "HortonWorks",
        "HDP",
        "Experience",
        "onpremises",
        "application",
        "AWS",
        "AWS",
        "services",
        "EC2",
        "S3",
        "data",
        "sets",
        "processing",
        "storage",
        "Hadoop",
        "cluster",
        "AWS",
        "EMR",
        "Experience",
        "Sequence",
        "files",
        "AVRO",
        "Parquet",
        "file",
        "Exposure",
        "Data",
        "Lake",
        "Implementation",
        "Apache",
        "Spark",
        "Data",
        "pipe",
        "lines",
        "business",
        "logics",
        "Spark",
        "Experience",
        "Spark",
        "APIs",
        "Spark",
        "Core",
        "Spark",
        "SQL",
        "Spark",
        "Streaming",
        "Experience",
        "Transformations",
        "Actions",
        "data",
        "Spark",
        "RDDs",
        "Datasets",
        "DataFrames",
        "DStreams",
        "Worked",
        "data",
        "transfers",
        "Broadcast",
        "variables",
        "Accumulators",
        "Spark",
        "Experience",
        "scheduler",
        "stages",
        "tasks",
        "RDD",
        "sizes",
        "memory",
        "usage",
        "information",
        "Information",
        "running",
        "executors",
        "Spark",
        "UI",
        "Experience",
        "dependency",
        "management",
        "tools",
        "SBT",
        "jar",
        "file",
        "MapReduce",
        "jobs",
        "Hadoop",
        "Big",
        "Data",
        "processing",
        "framework",
        "SQL",
        "graph",
        "MapReduce",
        "jobs",
        "execution",
        "time",
        "framework",
        "jobs",
        "order",
        "dependencies",
        "Hive",
        "Expertise",
        "functions",
        "Hive",
        "HQL",
        "Hive",
        "tables",
        "scripts",
        "parallel",
        "runtime",
        "scripts",
        "Ability",
        "Pig",
        "UDFs",
        "data",
        "analysis",
        "Experience",
        "transformations",
        "event",
        "filter",
        "bot",
        "traffic",
        "preaggregations",
        "PIG",
        "Worked",
        "HBase",
        "data",
        "StoreFiles",
        "HDFS",
        "highspeed",
        "lookups",
        "Expert",
        "knowledge",
        "NoSQL",
        "data",
        "modeling",
        "disaster",
        "recovery",
        "backup",
        "storage",
        "processing",
        "CRUD",
        "Worked",
        "Ad",
        "Indexing",
        "Replication",
        "Load",
        "Developed",
        "Sqoop",
        "Jobs",
        "data",
        "Relational",
        "Database",
        "Systems",
        "Oracle",
        "MySQL",
        "HDFS",
        "Hive",
        "Used",
        "Kafka",
        "streams",
        "data",
        "system",
        "Hadoop",
        "Experience",
        "Hadoop",
        "jobs",
        "Oozie",
        "kinds",
        "reports",
        "Power",
        "BI",
        "Tableau",
        "Client",
        "specification",
        "Experience",
        "stages",
        "SDLC",
        "Agile",
        "Waterfall",
        "Technical",
        "Design",
        "document",
        "Development",
        "Testing",
        "Implementation",
        "Enterprise",
        "level",
        "Data",
        "mart",
        "Data",
        "Work",
        "Experience",
        "Hadoopspark",
        "Developer",
        "Verizon",
        "Wireless",
        "Fort",
        "Worth",
        "TX",
        "March",
        "Present",
        "project",
        "call",
        "logs",
        "data",
        "sources",
        "Apache",
        "Hadoop",
        "spark",
        "data",
        "likelihood",
        "customer",
        "Hadoop",
        "company",
        "customer",
        "relationships",
        "churn",
        "Responsibilities",
        "data",
        "solutions",
        "Apache",
        "Hadoop",
        "Spark",
        "event",
        "data",
        "network",
        "data",
        "loading",
        "data",
        "database",
        "HDFS",
        "Sqoop",
        "Deployed",
        "Scalable",
        "Hadoop",
        "cluster",
        "AWS",
        "S3",
        "file",
        "system",
        "Hadoop",
        "Worked",
        "spark",
        "core",
        "Spark",
        "Streaming",
        "SQL",
        "modules",
        "Spark",
        "SparkScala",
        "functions",
        "transformations",
        "aggregations",
        "schema",
        "rows",
        "Spark",
        "APIs",
        "RDDs",
        "Datasets",
        "DataFrames",
        "DStreams",
        "transformations",
        "data",
        "Spark",
        "SQL",
        "analysis",
        "data",
        "SQL",
        "HiveQL",
        "data",
        "streams",
        "Spark",
        "Streaming",
        "level",
        "functions",
        "map",
        "join",
        "window",
        "data",
        "transfers",
        "Hadoop",
        "clusters",
        "Spark",
        "optimizations",
        "broadcasts",
        "variables",
        "Accumulators",
        "spark",
        "application",
        "error",
        "tolerant",
        "data",
        "Hive",
        "warehouse",
        "data",
        "formats",
        "Hive",
        "tables",
        "Hive",
        "Serdes",
        "Static",
        "partitions",
        "partitions",
        "Buckets",
        "Hive",
        "Oozie",
        "Operational",
        "Services",
        "scheduling",
        "workflows",
        "tools",
        "Tableau",
        "Impala",
        "reports",
        "data",
        "problems",
        "standards",
        "procedures",
        "JIRA",
        "Collaborated",
        "infrastructure",
        "network",
        "database",
        "application",
        "BA",
        "teams",
        "data",
        "quality",
        "availability",
        "Environment",
        "Hadoop",
        "2x",
        "Spark",
        "Core",
        "Spark",
        "SQL",
        "Spark",
        "Streaming",
        "Scala",
        "Hive",
        "Sqoop",
        "Oozie",
        "Amazon",
        "EMR",
        "Tableau",
        "Impala",
        "JIRA",
        "Hadoop",
        "Developer",
        "Molina",
        "Healthcare",
        "Long",
        "Beach",
        "CA",
        "September",
        "March",
        "project",
        "data",
        "repository",
        "information",
        "transformations",
        "data",
        "Hadoop",
        "spark",
        "ecosystems",
        "solutions",
        "data",
        "query",
        "capabilities",
        "value",
        "Responsibilities",
        "code",
        "data",
        "HDFS",
        "Hive",
        "Sqoop",
        "Deployed",
        "clusters",
        "Clouderas",
        "CDH",
        "distributions",
        "AWS",
        "Automated",
        "Sqoop",
        "imports",
        "Sqoop",
        "jobs",
        "jobs",
        "Oozie",
        "Experience",
        "Hive",
        "tables",
        "schema",
        "read",
        "data",
        "format",
        "compression",
        "file",
        "formats",
        "Avro",
        "Parquet",
        "Text",
        "formats",
        "Hive",
        "Queries",
        "data",
        "Hive",
        "warehouse",
        "HQL",
        "partition",
        "tables",
        "Hive",
        "performance",
        "Hive",
        "User",
        "Defined",
        "Functions",
        "jars",
        "HDFS",
        "Hive",
        "Queries",
        "Map",
        "Reduce",
        "programs",
        "Java",
        "part",
        "requirements",
        "Big",
        "Data",
        "Submitted",
        "MapReduce",
        "jobs",
        "Queues",
        "collection",
        "jobs",
        "system",
        "functionality",
        "POCs",
        "Spark",
        "Yarn",
        "cluster",
        "performance",
        "Spark",
        "Hive",
        "SQLTeradata",
        "spark",
        "scripts",
        "Scala",
        "shell",
        "requirements",
        "Kafka",
        "producers",
        "consumers",
        "Spark",
        "clients",
        "components",
        "HDFS",
        "Hive",
        "Worked",
        "NoSQL",
        "databases",
        "HBase",
        "access",
        "data",
        "job",
        "flows",
        "Oozie",
        "scheduling",
        "jobs",
        "apache",
        "Hadoop",
        "jobs",
        "defects",
        "Agile",
        "Methodology",
        "perspective",
        "Environment",
        "Hadoop",
        "HDFS",
        "Hive",
        "MapReduce",
        "Spark",
        "Scala",
        "Kafka",
        "HBase",
        "Oozie",
        "Java",
        "Linux",
        "Cloudera",
        "Hadoop",
        "Developer",
        "Bank",
        "America",
        "Charlotte",
        "NC",
        "February",
        "July",
        "project",
        "data",
        "warehouses",
        "departments",
        "repository",
        "Hadoop",
        "analysis",
        "score",
        "risk",
        "customer",
        "portfolios",
        "score",
        "bank",
        "exposure",
        "revenue",
        "customer",
        "satisfaction",
        "Responsibilities",
        "amounts",
        "data",
        "silos",
        "data",
        "patterns",
        "Apache",
        "Flume",
        "data",
        "HDFS",
        "analysis",
        "MapReduce",
        "Jobs",
        "java",
        "data",
        "cleansing",
        "preprocessing",
        "data",
        "MapReduce",
        "programs",
        "data",
        "staging",
        "tables",
        "data",
        "tables",
        "EDW",
        "Monitored",
        "workload",
        "job",
        "performance",
        "Capacity",
        "planning",
        "Cloudera",
        "Manager",
        "job",
        "log",
        "files",
        "web",
        "log",
        "data",
        "HiveQL",
        "number",
        "visitors",
        "day",
        "page",
        "views",
        "visitors",
        "Hive",
        "schema",
        "tables",
        "managing",
        "views",
        "data",
        "loading",
        "Hive",
        "tables",
        "partitions",
        "requirement",
        "Created",
        "Hive",
        "queries",
        "analysts",
        "trends",
        "data",
        "EDW",
        "reference",
        "tables",
        "metrics",
        "ETL",
        "workflows",
        "PIG",
        "spark",
        "scripts",
        "python",
        "shell",
        "commands",
        "requirement",
        "data",
        "migration",
        "databases",
        "Hadoop",
        "HDFS",
        "Hive",
        "Sqoop",
        "Worked",
        "Oozie",
        "workflow",
        "engine",
        "job",
        "scheduling",
        "zookeeper",
        "types",
        "configurations",
        "experience",
        "Tableau",
        "visualization",
        "tool",
        "Tableau",
        "Desktop",
        "Environment",
        "Map",
        "Reduce",
        "Hive",
        "PIG",
        "Spark",
        "Flume",
        "Zookeeper",
        "Oozie",
        "Tableau",
        "Java",
        "Python",
        "Unix",
        "Java",
        "Developer",
        "Hierarchy",
        "Technologies",
        "September",
        "November",
        "Hierarchy",
        "Technologies",
        "business",
        "process",
        "Bill",
        "Materials",
        "Order",
        "life",
        "cycle",
        "forecasting",
        "project",
        "reports",
        "Finance",
        "Billing",
        "teams",
        "verification",
        "solution",
        "business",
        "process",
        "steps",
        "Responsibilities",
        "Agile",
        "Rational",
        "Unified",
        "Process",
        "lifecycle",
        "project",
        "requirements",
        "analysis",
        "gathering",
        "specifications",
        "UML",
        "diagrams",
        "core",
        "framework",
        "components",
        "workflows",
        "Core",
        "Java",
        "JDBC",
        "Servlets",
        "JSPs",
        "Responsive",
        "Designs",
        "HTML5",
        "CSS3",
        "Applied",
        "Object",
        "concepts",
        "inheritance",
        "composition",
        "interface",
        "design",
        "patterns",
        "singleton",
        "strategy",
        "Applied",
        "Spring",
        "IOC",
        "Container",
        "Dependency",
        "Injection",
        "Spring",
        "AOP",
        "security",
        "cross",
        "concerns",
        "SOAP",
        "Web",
        "Services",
        "packaging",
        "Axis",
        "design",
        "decision",
        "making",
        "Hibernate",
        "ORM",
        "mapping",
        "end",
        "system",
        "JSP",
        "HTML",
        "jQuery",
        "JavaScript",
        "Managing",
        "Web",
        "Services",
        "operations",
        "Maven",
        "build",
        "tool",
        "dependencies",
        "project",
        "Stored",
        "Procedures",
        "tables",
        "database",
        "Mock",
        "Testing",
        "Unit",
        "Testing",
        "JUNIT",
        "Easy",
        "Mock",
        "integration",
        "Jenkins",
        "project",
        "Apache",
        "Maven",
        "scripts",
        "Environment",
        "Java16J2EE",
        "Microsoft",
        "Visio",
        "Web",
        "Sphere",
        "Application",
        "Server",
        "Spring",
        "MVC",
        "IOC",
        "Spring",
        "AOP",
        "Apache",
        "Axis",
        "SOAP",
        "Hibernate",
        "Web",
        "services",
        "Maven",
        "jQuery",
        "JUnit",
        "Easy",
        "Mockito",
        "Git",
        "Jenkins",
        "Jr",
        "Developer",
        "Serveen",
        "Software",
        "Systems",
        "P",
        "Ltd",
        "June",
        "September",
        "web",
        "project",
        "surveys",
        "participants",
        "user",
        "GUI",
        "tool",
        "kind",
        "information",
        "aboutusers",
        "priority",
        "confidentiality",
        "user",
        "information",
        "modules",
        "Admin",
        "Registered",
        "User",
        "Manager",
        "Responsibilities",
        "phases",
        "Software",
        "Development",
        "Life",
        "Cycle",
        "SDLC",
        "Rational",
        "Rose",
        "Use",
        "Case",
        "Diagrams",
        "Object",
        "Diagrams",
        "class",
        "Diagrams",
        "Sequence",
        "diagrams",
        "design",
        "phase",
        "Frontend",
        "HTML",
        "CSS",
        "JSP",
        "Servlets",
        "Ajax",
        "Struts",
        "exception",
        "validation",
        "classes",
        "Struts",
        "validation",
        "rules",
        "JavaScript",
        "web",
        "page",
        "validation",
        "SOAP",
        "Web",
        "Services",
        "XML",
        "data",
        "applications",
        "HTTP",
        "Created",
        "Servlets",
        "submittals",
        "Enterprise",
        "Java",
        "Bean",
        "EJB",
        "components",
        "information",
        "ANT",
        "scripts",
        "application",
        "artifacts",
        "Environment",
        "Java",
        "J2EE",
        "Servlets",
        "Struts",
        "JSP",
        "XML",
        "DOM",
        "HTML",
        "JavaScript",
        "JDBC",
        "Web",
        "Services",
        "Eclipse",
        "Plugins",
        "Education",
        "Bachelors",
        "Skills",
        "IMPALA",
        "OOZIE",
        "Information",
        "TECHNICAL",
        "SKILLS",
        "Data",
        "Access",
        "Tools",
        "HDFS",
        "YARN",
        "Hive",
        "Pig",
        "HBase",
        "Solr",
        "Impala",
        "Spark",
        "Core",
        "Spark",
        "SQL",
        "Spark",
        "Streaming",
        "Data",
        "Management",
        "HDFS",
        "YARN",
        "Data",
        "Workflow",
        "Sqoop",
        "Flume",
        "Kafka",
        "Data",
        "Operation",
        "Zookeper",
        "Oozie",
        "Data",
        "Security",
        "Ranger",
        "Knox",
        "BigData",
        "Distributions",
        "Hortonworks",
        "Cloudera",
        "Cloud",
        "Technologies",
        "AWS",
        "Amazon",
        "Web",
        "Services",
        "EC2",
        "S3",
        "IAM",
        "CLOUD",
        "WATCH",
        "SNS",
        "SQS",
        "EMR",
        "KINESIS",
        "Programming",
        "Scripting",
        "Languages",
        "Java",
        "Scala",
        "Pig",
        "Latin",
        "HQL",
        "SQL",
        "Shell",
        "Scripting",
        "HTML",
        "CSS",
        "JavaScript",
        "IDEBuild",
        "Tools",
        "Eclipse",
        "Intellij",
        "Business",
        "Intelligence",
        "Tools",
        "Tableau",
        "Power",
        "Bi",
        "MS",
        "Excel",
        "Microsoft",
        "Visio",
        "SSIS",
        "SSAS",
        "SSRS",
        "JavaJ2EE",
        "Technologies",
        "XML",
        "Junit",
        "JDBC",
        "AJAX",
        "JSP",
        "Operating",
        "Systems",
        "Linux",
        "Windows",
        "Kali",
        "Linux",
        "SDLC",
        "Waterfall"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T21:49:48.542877",
    "resume_data": "Hadoopspark Developer Hadoopspark span lDeveloperspan Hadoopspark Developer Verizon Wireless Fort Worth TX Around 7 years of IT experience with over Four years of work experience in Big Data Hadoop Experience in working on various components in Hadoop ecosystem like HDFS YARN HIVE SPARK MAPREDUCE PIG HBASE SCOOP FLUME KAFKA OOZIE and ZOOKEEPER Excellent programming skills in SCALA JAVA and PYTHON Experience in Managing scalable Hadoop clusters including Cluster designing provisioning custom configurations monitoring and maintaining using Hadoop distributions Cloudera CDH HortonWorks HDP Experience in migrating an existing onpremises application to AWS Using AWS services like EC2 and S3 for small data sets processing and storage and Maintaining the Hadoop cluster on AWS EMR Experience in using Sequence files like ORC AVRO and Parquet file formats Exposure to Data Lake Implementation using Apache Spark and developed Data pipe lines and applied business logics using Spark Experience in working on Spark APIs like Spark Core Spark SQL and Spark Streaming Experience in performing Transformations and Actions on data using Spark RDDs Datasets DataFrames and DStreams Worked on minimizing data transfers using Broadcast variables and Accumulators in Spark Experience in monitoring scheduler stages and tasks RDD sizes and memory usage Environmental information Information about the running executors using Spark UI Experience in using dependency management tools like SBT to create a selfcontained jar file Well versed with developing and implementing MapReduce jobs using Hadoop to work with Big Data Implemented the processing framework for converting SQL to a graph of MapReduce jobs and the execution time framework to run those jobs in the order of dependencies using Hive Expertise in using Aggregate functions in Hive using HQL Worked on partitioning Hive tables and running the scripts in parallel to reduce runtime of the scripts Ability to develop Pig UDFs to preprocess the data for analysis Experience in performing transformations like event joins filter bot traffic and some preaggregations using PIG Worked on HBase to put the data in indexed StoreFiles that exist on HDFS for highspeed lookups Expert knowledge on MongoDB NoSQL data modeling tuning disaster recovery backup used it for distributed storage and processing using CRUD Worked on Ad hoc queries Indexing Replication and Load balancing Developed Sqoop Jobs to load data from Relational Database Systems like Oracle and MySQL into HDFS and Hive Used Kafka to read and write streams of data like a messaging system in Hadoop Experience in managing Hadoop jobs using Oozie Generated various kinds of reports using Power BI and Tableau based on Client specification Experience in all stages of SDLC Agile Waterfall writing Technical Design document Development Testing and Implementation of Enterprise level Data mart and Data warehouses Work Experience Hadoopspark Developer Verizon Wireless Fort Worth TX March 2017 to Present The project deals with analyzing call logs and complex data from multiple sources using Apache Hadoop and spark across the data to predict the likelihood that any particular customer would leave Hadoop helped company build more valuable customer relationships and reduce churn Responsibilities Responsible for building scalable distributed data solutions using Apache Hadoop and Spark Involved in combining traditional trasactional and event data with social network data Involved in loading data from relational database into HDFS using Sqoop Deployed Scalable Hadoop cluster on AWS using S3 as underlying file system for Hadoop Worked with spark core Spark Streaming and spark SQL modules of Spark Involved in developing generic SparkScala functions for transformations aggregations and designing schema for rows Experienced in working with Spark APIs like RDDs Datasets DataFrames and DStreams to perform transformations on the data Used Spark SQL to perform interactive analysis on the data using SQL and HiveQL Experienced in processing live data streams using Spark Streaming with high level functions like map reduce join and window Experienced in minimizing data transfers over Hadoop clusters using Spark optimizations like broadcasts variables and Accumulators Worked on troubleshooting spark application to make them more error tolerant Involved in loading the processed data into Hive warehouse Stored the data in tabular formats using Hive tables and Hive Serdes Implemented Static partitions Dynamic partitions and Buckets in Hive Used Oozie Operational Services for scheduling workflows dynamically Used Reporting tools like Tableau to connect with Impala for generating daily reports of data Designed documented operational problems by following standards and procedures using JIRA Collaborated with the infrastructure network database application and BA teams to ensure data quality and availability Environment Hadoop 2x Spark Core Spark SQL Spark Streaming Scala Hive Sqoop Oozie Amazon EMR Tableau Impala JIRA Hadoop Developer Molina Healthcare Long Beach CA September 2015 to March 2017 The project deals with developing a centralized data repository for maintaining medical information and performing relevant transformations on the data using Hadoop and spark ecosystems to obtain optimal solutions and provide further data query capabilities to improve medical and economic value Responsibilities Developed the code for Importing and exporting data into HDFS and Hive using Sqoop Deployed multiple clusters Clouderas CDH distributions on AWS Automated Sqoop incremental imports by using Sqoop jobs and automated the jobs using Oozie Experience in creating Hive tables to apply schema on read and view the data in structured format Worked on various compression and file formats like Avro Parquet and Text formats Responsible for writing Hive Queries for analyzing data in Hive warehouse using HQL Responsible for creating complex dynamic partition tables using Hive for best performance and faster querying Experienced in developing Hive User Defined Functions in java compiling them into jars and adding them to the HDFS and executing them with Hive Queries Developed several advanced Map Reduce programs in Java as part of functional requirements for Big Data Submitted MapReduce jobs to Queues as collection of jobs to allow the system to provide specified functionality Developed multiple POCs using Spark and deployed on the Yarn cluster compared the performance of Spark with Hive and SQLTeradata Developed spark scripts by using Scala shell as per requirements Developed Kafka producers and consumers Spark clients along with components on HDFS Hive Worked on NoSQL databases like HBase for readwrite access on random realtime data Involved in defining job flows using Oozie for scheduling jobs to manage apache Hadoop jobs Tested and reported defects in an Agile Methodology perspective Environment Hadoop HDFS Hive MapReduce Spark Scala Kafka HBase Oozie Java Linux Cloudera Hadoop Developer Bank of America Charlotte NC February 2014 to July 2015 The project is to collect separate data warehouses from multiple departments and combined them into a single global repository in Hadoop for analysis to construct a new and more accurate score of the risk in its customer portfolios This accurate score allowed bank to manage its exposure better and hence increased the revenue and improved customer satisfaction Responsibilities Collected and aggregated large amounts of structured and complex data from multiple silos and combined the data and look for patterns Involved in using Apache Flume and stored the data into HDFS for analysis Implemented multiple MapReduce Jobs in java for data cleansing and preprocessing of data Developed MapReduce programs to parse the raw data populate staging tables and store the refined data in partitioned tables in the EDW Monitored workload job performance and Capacity planning using Cloudera Manager Involved in defining job flows managing and reviewing log files Worked on analyzing the web log data using the HiveQL to extract number of unique visitors per day page views and returning visitors Experienced in creating Hive schema external tables and managing views Responsible for data loading involved in creating Hive tables and partitions based on the requirement Created Hive queries that helped market analysts to spot emerging trends by comparing fresh data with EDW reference tables and historical metrics Experienced in optimizing ETL workflows using PIG Developed spark scripts by using python shell commands as per the requirement Involved in data migration from various databases to Hadoop HDFS and Hive using Sqoop Worked on Oozie workflow engine for job scheduling Used zookeeper for various types of centralized configurations Good experience in working with Tableau visualization tool using Tableau Desktop Environment Map Reduce Hive PIG Spark Flume Zookeeper Oozie Tableau Java Python Unix Java Developer Hierarchy Technologies September 2012 to November 2013 Hierarchy Technologies has been designed to facilitate the business process of tracking a Bill of Materials Order through its life cycle This encompasses the initial forecasting of the project through to the required reports being available to the relevant Finance and Billing teams for verification The solution is designed to automate as much of the business process and eliminate as many manual steps as possible Responsibilities Followed Agile Rational Unified Process throughout the lifecycle of the project Involved in requirements analysis and gathering and converting them into technical specifications using UML diagrams Implemented core framework components for executing workflows using Core Java JDBC and Servlets and JSPs Created Responsive Designs MobileTabletDesktop using HTML5 CSS3 Applied Object Oriented concepts inheritance composition interface design patterns singleton strategy Applied Spring IOC Container to facilitate Dependency Injection Used Spring AOP to implement security where cross cutting concerns were identified Responsible for developing SOAP based Web Services consuming and packaging using Axis Involved in design and decision making for Hibernate ORM mapping Responsible for designing front end system using JSP HTML jQuery and JavaScript Involved in Managing Web Services and operations Used Maven as the build tool to manage the dependencies of the project Implemented Stored Procedures for the tables in the database Develop and perform Mock Testing and Unit Testing using JUNIT and Easy Mock Involved in implementing the continuous integration using Jenkins Built project using Apache Maven build scripts Environment Java16J2EE Microsoft Visio Web Sphere Application Server Spring MVC IOC Spring AOP Apache Axis SOAP Hibernate Web services Maven jQuery JUnit Easy Mockito Git Jenkins Jr JAVA Developer Serveen Software Systems P Ltd June 2011 to September 2012 This is a web based project which is used to create surveys for the participants with a user friendly GUI This tool will not record any kind of information aboutusers giving priority to the confidentiality of the user information which includes modules like Admin Registered User and Manager Responsibilities Involved in various phases of Software Development Life Cycle SDLC Used Rational Rose for the Use Case Diagrams Object Diagrams class Diagrams and Sequence diagrams to represent the detailed design phase Frontend is designed by using HTML CSS JSP Servlets Ajax and Struts Involved in writing the exception and validation classes using Struts validation rules Used JavaScript for the web page validation Used SOAP for Web Services by exchanging XML data between applications over HTTP Created Servlets which route submittals to appropriate Enterprise Java Bean EJB components and render retrieved information Written ANT scripts for building application artifacts Environment Java J2EE Servlets Struts JSP XML DOM HTML JavaScript JDBC Web Services Eclipse Plugins Education Bachelors Skills DYNAMODB HDFS IMPALA OOZIE SQOOP Additional Information TECHNICAL SKILLS Data Access Tools HDFS YARN Hive Pig HBase Solr Impala Spark Core Spark SQL Spark Streaming Data Management HDFS YARN Data Workflow Sqoop Flume Kafka Data Operation Zookeper Oozie Data Security Ranger Knox BigData Distributions Hortonworks Cloudera Cloud Technologies AWS Amazon Web Services EC2 S3 IAM CLOUD WATCH DynamoDB SNS SQS EMR KINESIS Programming and Scripting Languages Java Scala Pig Latin HQL SQL Shell Scripting HTML CSS JavaScript IDEBuild Tools Eclipse Intellij Business Intelligence Tools Tableau Power Bi MS Excel Microsoft Visio SSIS SSAS SSRS JavaJ2EE Technologies XML Junit JDBC AJAX JSON JSP Operating Systems Linux Windows Kali Linux SDLC AgileSCRUM Waterfall",
    "unique_id": "734ef8db-09a5-450f-8542-9d664a6ee1a7"
}