{
    "clean_data": "Sr HadoopSpark Developer Sr HadoopSpark span lDeveloperspan Sr HadoopSpark Developer Astra Zeneca Wilmington DE Authorized to work in the US for any employer Work Experience Sr HadoopSpark Developer Astra Zeneca Wilmington DE March 2018 to Present Roles Responsibilities Ingested gigabytes of click stream data from external servers such as FTP server and S3 buckets on daily basis using custom Input Adapters Created Sqoop scripts to importexport user profile data from RDBMS to S3 data lake Developed various spark applications using Scala to perform various enrichments of user behavioral data click stream data merged with user profile data Involved in data cleansing event enrichment data aggregation denormalization and data preparation needed for downstream model learning and reporting Utilized Spark Scala API to implement batch processing of jobs Trouble Shooting Spark applications for improved error tolerance Finetuning spark applicationsjobs to improve the efficiency and overall processing time for the pipelines Created Kafka producer API to send livestream data into various Kafka topics Developed SparkStreaming applications to consume the data from Kafka topics and to insert the processed streams to HBase Utilized Spark in Memory capabilities to handle large datasets Used Broadcast variables in Spark effective efficient Joins transformations and other capabilities for data processing Worked on EMR cluster and S3 in AWS cloud Creating Hive tables loading and analyzing data using hive scripts Implemented Partitioning Dynamic Partitions Buckets in HIVE Involved in continuous Integration of application using Jenkins Interacted with the infrastructure network database application and BA teams to ensure data quality and availability Environment AWS EMR Spark Hive HDFS Sqoop Kafka Oozie HBase Scala MapReduce Spark Developer ATT Ledgewood NJ March 2016 to January 2018 Roles Responsibilities Extensively worked on migrating data from traditional RDBMS to HDFS Ingested data into HDFS from Teradata MySQL using Sqoop Involved in developing spark application to perform ELT kind of operations on the data Modified existing MapReduce jobs to Spark transformations and actions by utilizing Spark RDDs Dataframe and Spark SQL APIs Utilized Hive partitioning Bucketing and performed various kinds of joins on Hive tables Involved in creating Hive external tables to perform ETL on data that is produced on daily basis Validated the data being ingested into HIVE for further filtering and cleansing Developed Sqoop jobs for performing incremental loads from RDBMS into HDFS and further applied Spark transformations Loaded data into hive tables from spark and used Parquet columnar format Created Oozie workflows to automate and productionize the data pipelines Migrating Map Reduce code into Spark transformations using Spark and Scala Collecting and aggregating large amounts of log data using Apache Flume and staging data in HDFS for further analysis Designed documented operational problems by following standards and procedures using JIRA Environment HDP Hortonworks Data Platform Spark Scala Sqoop Oozie Hive Cent OS MySQL Oracle DB Flume Hadoop Developer Life Lock Tempe AZ September 2015 to February 2016 Roles Responsibilities Involved in writing Spark applications using Scala to perform various data cleansing validation transformation and summarization activities according to the requirement Load the data into Spark RDD and Perform inmemory data computation to generate the output as per the requirements Developed data pipelines using Spark Hive and Sqoop to ingest transform and analyze operational data Developed Spark jobs Hive jobs to summarize and transform data Worked on performance tuning of Spark application to improve performance Performance tuning the Spark jobs by changing the configuration properties and using broadcast variables Real time streaming the data using Spark with Kafka Responsible for handling Streaming data from web server console logs Worked on different file formats like Text Sequence files Avro Parquet JSON XML files and Flat files using Map Reduce Programs Developed daily process to do incremental import of data from DB2 and Teradata into Hive tables using Sqoop Wrote Pig Scripts to generate Map Reduce jobs and performed ETL procedures on the data in HDFS Analyzed the SQL scripts and designed the solution to implement using Scala Solved performance issues in Hive and Pig scripts with understanding of Joins Group and Aggregation and how does it translate to MR jobs Work with cross functional consulting teams within the data science and analytics team to design develop and execute solutions to derive business insights and solve clients operational and strategic problems Exported the analyzed data to the relational databases using Sqoop for visualization and to generate reports for the BI team Extensively used HiveHQL or Hive queries to query data in Hive Tables and loaded data into HBase tables Extensively worked with Partitions Dynamic Partitioning bucketing tables in Hive designed both Managed and External tables also worked on optimization of Hive queries Involved in collecting and aggregating large amounts of log data using Flume and staging data in HDFS for further analysis Assisted analytics team by writing Pig and Hive scripts to perform further detailed analysis of the data Designed Oozie workflows for job scheduling and batch processing Environment Java Scala Apache Spark MySQL CDH IntelliJ IDEA Hive HDFS YARN Map Reduce Sqoop PIG Flume Unix Shell Scripting Python Apache Kafka Big Data Hadoop Developer Zyno Medical Natick MA November 2013 to August 2015 Roles Responsibilities Coordinated with business customers to gather business requirements and interacted with other technical peers to derive Technical requirements and delivered the BRD and TDD documents Involved in validating the aggregate table based on the rollup process documented in the data mapping Designed and Modified Database tables and used HBASE Queries to insert and fetch data from tables Involved in loading and transforming large sets of structured semi structured and unstructured data from relational databases into HDFS using Sqoop imports Responsible for analyzing and cleansing raw data by performing Hive queries and running Pig scripts on data Developed Pig Latin scripts to extract the data from the web server output files to load into HDFS Created Hive tables loaded data and wrote Hive queries that run within the map Used OOZIE Operational Services for batch processing and scheduling workflows dynamically and created UDFs to store specialized data structures in HBase and Cassandra Involved in application development using RDBMS and Linux shell scripting Developed and updated social media analytics dashboards on regular basis Created a complete processing engine based on Hortonworks distribution Manage and review Hadoop log files Developed and generated insights based on brand conversations which in turn helpful for effectively driving brand awareness engagement and traffic to social media pages Involved in the identifying analyzing defects questionable function error and inconsistencies in output Environment Hadoop MapReduce Yarn Hive Pig HBase Oozie Sqoop Oracle 11g Scala HDFS Eclipse Java Developer Mariner Finance Laurel MD October 2012 to October 2013 Roles Responsibilities Gathered requirements from end users and create functional requirements Worked on process flow analyzing the functional requirements Development of Graphical user interface for user selfservice screen Implemented four eyes principle and created quality check process reusable across all workflow on overall platform level Development of UI models using HTML JSP JavaScript Web Link and CSS Developed Struts Action classes and Validation classes using Struts controller component and Struts validation framework Support in end user training testing and documentation Implemented Backing beans for handling UI components and stores its state in a scope Worked on implementing EJB Stateless sessions for communicating with Controller Implemented database integration using Hibernate and utilized spring with Hibernate for mapping with Oracle database Worked on Oracle PLSQL queries to Select Update and Delete data Worked on MAVEN for build automation Used GIT for version control Environment Java J2EE JSP Maven Linux CSS GIT Oracle XML SAX Rational Rose UML Java Developer Acute Soft Solutions India Private Limited Hyderabad Telangana July 2011 to August 2012 Roles Responsibilities Involved in developing the application using JavaJ2EE platform Implemented the Model View Control MVC structure using Struts Responsible to enhance the Portal UI using HTML JavaScript XML JSP Java CSS as per the requirements and providing the clientside Java script validations and Serverside bean Validation Framework JSR 303 Used Spring Core Annotations for Dependency Injection Used Hibernate as persistence framework mapping the ORM objects to table using Hibernate annotations Responsible to write the different service classes and utility API which will be used across the framework Used Axis to implementing Web Services for integration of different systems Developed Web services component using XML WSDL and SOAP with DOM parser to transfer and transform data between applications Exposed various capabilities as Web Services using SOAPWSDL Used SOAP UI for testing the Restful Web Services by sending and SOAP request Used AJAX framework for server communication and seamless user experience Created test framework on Selenium and executed Web testing in Chrome IE and Mozilla through Web driver Used clientside java scripting JQUERY for designing TABS and DIALOGBOX Created UNIX shell scripts to automate the build process to perform regular jobs like file transfers between different hosts Used Log4j for the logging the output to the files Used JUnit Eclipse for the unit testing of various modules Involved in production support monitoring server and error logs and foreseeing the Potential issues and escalating to the higher levels Environment Java J2EE JSP Servlets Spring Servlets Custom Tags Java Beans JMS Hibernate IBM MQ Series AJAX Junit Log4j JNDI Oracle XML SAX Rational Rose UML Education Bachelors Degree in Computer Science in Computer Science Acharya Nagarjuna University 2011 Skills Cassandra Hdfs Mapreduce Oozie Sqoop Hbase Db2 Mongodb Nosql Teradata Hbase Hive Mapreduce Pig Python Database Sql server Mysql Oracle Sql Additional Information Resultdriven IT Professional with referable 8 experience in Software development with 5 years of recent expertise in BigData technologies including Hadoop and Spark Professional Java developer with strong expertise in data engineering and big data technologies Highly skilled on Spark Hive Pig MapReduce Sqoop Kafka Oozie HBase Impala and Yarn Hands on experience in programming using Java Python Scala and SQL Sound knowledge of architecture of Distributed Systems and parallel processing frameworks Experience with Hadoop distributions both on premise CDH HDP and in cloud AWS Good experience working with various data analytics and big data services in AWS Cloud like EMR Redshift S3 Athena Glue etc Expert in developing production ready spark application using Spark RDD APIs Data frames SparkSQL and SparkStreaming APIs Strong experience in using Spark Streaming Spark SQL and other components of spark like accumulators Broadcast variables different levels of caching and optimization techniques for spark jobs Proficient in importingexporting data from RDBMS to HDFS using Sqoop Solid experience in working various data formats like Parquet Orc Avro JSon etc Experience automating endtoend data pipelines with strong resilience and recoverability Strong knowledge of NoSQL databases and worked with HBase Cassandra and Mongo DB Expert in SQL extensively worked RDBMSs like Oracle SQL Server DB2 MySQL and Teradata Proficient and Worked with GIT Jenkins and Maven Good understanding and Experience with Agile and Waterfall methodologies of Software Development Life Cycle SDLC Highly motivated selflearner with a positive attitude willingness to learn new concepts and accepts challenges Technical competencies Big Data Ecosystems HDFS MapReduce YARN Hive Sqoop Pig Spark HBase Oozie Programming Languages Java Scala Python SQL AWS technologies S3 EMR Redshift Athena Glue Database SQL Server MySQL Oracle DB2 Teradata NoSQL Databases HBase MongoDB Cassandra IDEs Utilities Eclipse IntelliJ Development Methodologies Agile Waterfall Model",
    "entities": [
        "Joins Group",
        "Oracle SQL Server",
        "AJAX",
        "Created Oozie",
        "ORM",
        "MapReduce Spark",
        "BI",
        "Input Adapters Created",
        "Environment Hadoop MapReduce Yarn Hive Pig HBase Oozie",
        "Nagarjuna University",
        "Controller Implemented",
        "DIALOGBOX Created",
        "IBM",
        "Teradata MySQL using Sqoop Involved",
        "Distributed Systems",
        "Model View Control MVC",
        "Hadoop",
        "SOAP",
        "Created Kafka",
        "JUnit",
        "HBase Cassandra",
        "HBase",
        "ELT",
        "Spark Streaming Spark",
        "JavaJ2EE",
        "Oracle XML SAX Rational",
        "Developed Sqoop",
        "Maven Good",
        "SQL Sound",
        "Spring Servlets Custom Tags",
        "Spark RDDs Dataframe",
        "Assisted",
        "Designed Oozie",
        "Developed",
        "Spark Professional Java",
        "HBase Utilized Spark",
        "Validation",
        "JQUERY",
        "Software Development Life Cycle SDLC Highly",
        "JSP",
        "Worked",
        "Work Experience Sr HadoopSpark Developer",
        "Parquet",
        "DOM",
        "Utilized Spark",
        "BigData",
        "Computer Science in Computer Science",
        "Big Data Hadoop Developer Zyno Medical",
        "Spark",
        "Agile",
        "EJB",
        "Created Hive",
        "GIT",
        "Select Update and Delete",
        "Spark Hive Pig MapReduce Sqoop",
        "Hive Tables",
        "API",
        "US",
        "Perform",
        "Sqoop",
        "Developer Acute Soft Solutions India Private Limited Hyderabad",
        "HIVE",
        "Sr HadoopSpark Developer",
        "Created",
        "AWS",
        "Scala Solved",
        "log data",
        "Java Developer Mariner Finance",
        "HDFS Analyzed the",
        "the Portal UI",
        "SQL",
        "BRD",
        "Spark RDD",
        "MD",
        "Utilized Hive",
        "Mysql Oracle Sql Additional Information Resultdriven IT Professional",
        "Hive queries Involved",
        "Hive",
        "Serverside",
        "FTP",
        "Spark Hive",
        "ETL",
        "SparkStreaming APIs Strong",
        "Potential",
        "BA",
        "UI",
        "Selenium",
        "CSS",
        "JNDI Oracle XML SAX Rational",
        "HTML JavaScript XML",
        "Data",
        "MapReduce",
        "TDD",
        "NoSQL",
        "SQL AWS",
        "Development of UI",
        "Development of Graphical",
        "the Restful Web Services"
    ],
    "experience": "Experience Sr HadoopSpark Developer Astra Zeneca Wilmington DE March 2018 to Present Roles Responsibilities Ingested gigabytes of click stream data from external servers such as FTP server and S3 buckets on daily basis using custom Input Adapters Created Sqoop scripts to importexport user profile data from RDBMS to S3 data lake Developed various spark applications using Scala to perform various enrichments of user behavioral data click stream data merged with user profile data Involved in data cleansing event enrichment data aggregation denormalization and data preparation needed for downstream model learning and reporting Utilized Spark Scala API to implement batch processing of jobs Trouble Shooting Spark applications for improved error tolerance Finetuning spark applicationsjobs to improve the efficiency and overall processing time for the pipelines Created Kafka producer API to send livestream data into various Kafka topics Developed SparkStreaming applications to consume the data from Kafka topics and to insert the processed streams to HBase Utilized Spark in Memory capabilities to handle large datasets Used Broadcast variables in Spark effective efficient Joins transformations and other capabilities for data processing Worked on EMR cluster and S3 in AWS cloud Creating Hive tables loading and analyzing data using hive scripts Implemented Partitioning Dynamic Partitions Buckets in HIVE Involved in continuous Integration of application using Jenkins Interacted with the infrastructure network database application and BA teams to ensure data quality and availability Environment AWS EMR Spark Hive HDFS Sqoop Kafka Oozie HBase Scala MapReduce Spark Developer ATT Ledgewood NJ March 2016 to January 2018 Roles Responsibilities Extensively worked on migrating data from traditional RDBMS to HDFS Ingested data into HDFS from Teradata MySQL using Sqoop Involved in developing spark application to perform ELT kind of operations on the data Modified existing MapReduce jobs to Spark transformations and actions by utilizing Spark RDDs Dataframe and Spark SQL APIs Utilized Hive partitioning Bucketing and performed various kinds of joins on Hive tables Involved in creating Hive external tables to perform ETL on data that is produced on daily basis Validated the data being ingested into HIVE for further filtering and cleansing Developed Sqoop jobs for performing incremental loads from RDBMS into HDFS and further applied Spark transformations Loaded data into hive tables from spark and used Parquet columnar format Created Oozie workflows to automate and productionize the data pipelines Migrating Map Reduce code into Spark transformations using Spark and Scala Collecting and aggregating large amounts of log data using Apache Flume and staging data in HDFS for further analysis Designed documented operational problems by following standards and procedures using JIRA Environment HDP Hortonworks Data Platform Spark Scala Sqoop Oozie Hive Cent OS MySQL Oracle DB Flume Hadoop Developer Life Lock Tempe AZ September 2015 to February 2016 Roles Responsibilities Involved in writing Spark applications using Scala to perform various data cleansing validation transformation and summarization activities according to the requirement Load the data into Spark RDD and Perform inmemory data computation to generate the output as per the requirements Developed data pipelines using Spark Hive and Sqoop to ingest transform and analyze operational data Developed Spark jobs Hive jobs to summarize and transform data Worked on performance tuning of Spark application to improve performance Performance tuning the Spark jobs by changing the configuration properties and using broadcast variables Real time streaming the data using Spark with Kafka Responsible for handling Streaming data from web server console logs Worked on different file formats like Text Sequence files Avro Parquet JSON XML files and Flat files using Map Reduce Programs Developed daily process to do incremental import of data from DB2 and Teradata into Hive tables using Sqoop Wrote Pig Scripts to generate Map Reduce jobs and performed ETL procedures on the data in HDFS Analyzed the SQL scripts and designed the solution to implement using Scala Solved performance issues in Hive and Pig scripts with understanding of Joins Group and Aggregation and how does it translate to MR jobs Work with cross functional consulting teams within the data science and analytics team to design develop and execute solutions to derive business insights and solve clients operational and strategic problems Exported the analyzed data to the relational databases using Sqoop for visualization and to generate reports for the BI team Extensively used HiveHQL or Hive queries to query data in Hive Tables and loaded data into HBase tables Extensively worked with Partitions Dynamic Partitioning bucketing tables in Hive designed both Managed and External tables also worked on optimization of Hive queries Involved in collecting and aggregating large amounts of log data using Flume and staging data in HDFS for further analysis Assisted analytics team by writing Pig and Hive scripts to perform further detailed analysis of the data Designed Oozie workflows for job scheduling and batch processing Environment Java Scala Apache Spark MySQL CDH IntelliJ IDEA Hive HDFS YARN Map Reduce Sqoop PIG Flume Unix Shell Scripting Python Apache Kafka Big Data Hadoop Developer Zyno Medical Natick MA November 2013 to August 2015 Roles Responsibilities Coordinated with business customers to gather business requirements and interacted with other technical peers to derive Technical requirements and delivered the BRD and TDD documents Involved in validating the aggregate table based on the rollup process documented in the data mapping Designed and Modified Database tables and used HBASE Queries to insert and fetch data from tables Involved in loading and transforming large sets of structured semi structured and unstructured data from relational databases into HDFS using Sqoop imports Responsible for analyzing and cleansing raw data by performing Hive queries and running Pig scripts on data Developed Pig Latin scripts to extract the data from the web server output files to load into HDFS Created Hive tables loaded data and wrote Hive queries that run within the map Used OOZIE Operational Services for batch processing and scheduling workflows dynamically and created UDFs to store specialized data structures in HBase and Cassandra Involved in application development using RDBMS and Linux shell scripting Developed and updated social media analytics dashboards on regular basis Created a complete processing engine based on Hortonworks distribution Manage and review Hadoop log files Developed and generated insights based on brand conversations which in turn helpful for effectively driving brand awareness engagement and traffic to social media pages Involved in the identifying analyzing defects questionable function error and inconsistencies in output Environment Hadoop MapReduce Yarn Hive Pig HBase Oozie Sqoop Oracle 11 g Scala HDFS Eclipse Java Developer Mariner Finance Laurel MD October 2012 to October 2013 Roles Responsibilities Gathered requirements from end users and create functional requirements Worked on process flow analyzing the functional requirements Development of Graphical user interface for user selfservice screen Implemented four eyes principle and created quality check process reusable across all workflow on overall platform level Development of UI models using HTML JSP JavaScript Web Link and CSS Developed Struts Action classes and Validation classes using Struts controller component and Struts validation framework Support in end user training testing and documentation Implemented Backing beans for handling UI components and stores its state in a scope Worked on implementing EJB Stateless sessions for communicating with Controller Implemented database integration using Hibernate and utilized spring with Hibernate for mapping with Oracle database Worked on Oracle PLSQL queries to Select Update and Delete data Worked on MAVEN for build automation Used GIT for version control Environment Java J2EE JSP Maven Linux CSS GIT Oracle XML SAX Rational Rose UML Java Developer Acute Soft Solutions India Private Limited Hyderabad Telangana July 2011 to August 2012 Roles Responsibilities Involved in developing the application using JavaJ2EE platform Implemented the Model View Control MVC structure using Struts Responsible to enhance the Portal UI using HTML JavaScript XML JSP Java CSS as per the requirements and providing the clientside Java script validations and Serverside bean Validation Framework JSR 303 Used Spring Core Annotations for Dependency Injection Used Hibernate as persistence framework mapping the ORM objects to table using Hibernate annotations Responsible to write the different service classes and utility API which will be used across the framework Used Axis to implementing Web Services for integration of different systems Developed Web services component using XML WSDL and SOAP with DOM parser to transfer and transform data between applications Exposed various capabilities as Web Services using SOAPWSDL Used SOAP UI for testing the Restful Web Services by sending and SOAP request Used AJAX framework for server communication and seamless user experience Created test framework on Selenium and executed Web testing in Chrome IE and Mozilla through Web driver Used clientside java scripting JQUERY for designing TABS and DIALOGBOX Created UNIX shell scripts to automate the build process to perform regular jobs like file transfers between different hosts Used Log4j for the logging the output to the files Used JUnit Eclipse for the unit testing of various modules Involved in production support monitoring server and error logs and foreseeing the Potential issues and escalating to the higher levels Environment Java J2EE JSP Servlets Spring Servlets Custom Tags Java Beans JMS Hibernate IBM MQ Series AJAX Junit Log4j JNDI Oracle XML SAX Rational Rose UML Education Bachelors Degree in Computer Science in Computer Science Acharya Nagarjuna University 2011 Skills Cassandra Hdfs Mapreduce Oozie Sqoop Hbase Db2 Mongodb Nosql Teradata Hbase Hive Mapreduce Pig Python Database Sql server Mysql Oracle Sql Additional Information Resultdriven IT Professional with referable 8 experience in Software development with 5 years of recent expertise in BigData technologies including Hadoop and Spark Professional Java developer with strong expertise in data engineering and big data technologies Highly skilled on Spark Hive Pig MapReduce Sqoop Kafka Oozie HBase Impala and Yarn Hands on experience in programming using Java Python Scala and SQL Sound knowledge of architecture of Distributed Systems and parallel processing frameworks Experience with Hadoop distributions both on premise CDH HDP and in cloud AWS Good experience working with various data analytics and big data services in AWS Cloud like EMR Redshift S3 Athena Glue etc Expert in developing production ready spark application using Spark RDD APIs Data frames SparkSQL and SparkStreaming APIs Strong experience in using Spark Streaming Spark SQL and other components of spark like accumulators Broadcast variables different levels of caching and optimization techniques for spark jobs Proficient in importingexporting data from RDBMS to HDFS using Sqoop Solid experience in working various data formats like Parquet Orc Avro JSon etc Experience automating endtoend data pipelines with strong resilience and recoverability Strong knowledge of NoSQL databases and worked with HBase Cassandra and Mongo DB Expert in SQL extensively worked RDBMSs like Oracle SQL Server DB2 MySQL and Teradata Proficient and Worked with GIT Jenkins and Maven Good understanding and Experience with Agile and Waterfall methodologies of Software Development Life Cycle SDLC Highly motivated selflearner with a positive attitude willingness to learn new concepts and accepts challenges Technical competencies Big Data Ecosystems HDFS MapReduce YARN Hive Sqoop Pig Spark HBase Oozie Programming Languages Java Scala Python SQL AWS technologies S3 EMR Redshift Athena Glue Database SQL Server MySQL Oracle DB2 Teradata NoSQL Databases HBase MongoDB Cassandra IDEs Utilities Eclipse IntelliJ Development Methodologies Agile Waterfall Model",
    "extracted_keywords": [
        "Sr",
        "HadoopSpark",
        "Developer",
        "Sr",
        "HadoopSpark",
        "span",
        "lDeveloperspan",
        "Sr",
        "HadoopSpark",
        "Developer",
        "Astra",
        "Zeneca",
        "Wilmington",
        "DE",
        "Authorized",
        "US",
        "employer",
        "Work",
        "Experience",
        "Sr",
        "HadoopSpark",
        "Developer",
        "Astra",
        "Zeneca",
        "Wilmington",
        "DE",
        "March",
        "Present",
        "Roles",
        "Responsibilities",
        "gigabytes",
        "stream",
        "data",
        "servers",
        "FTP",
        "server",
        "S3",
        "buckets",
        "basis",
        "custom",
        "Input",
        "Adapters",
        "Sqoop",
        "scripts",
        "user",
        "profile",
        "data",
        "RDBMS",
        "S3",
        "data",
        "lake",
        "spark",
        "applications",
        "Scala",
        "enrichments",
        "user",
        "data",
        "stream",
        "data",
        "user",
        "profile",
        "data",
        "data",
        "cleansing",
        "event",
        "enrichment",
        "data",
        "aggregation",
        "denormalization",
        "data",
        "preparation",
        "model",
        "learning",
        "Spark",
        "Scala",
        "API",
        "batch",
        "processing",
        "jobs",
        "Trouble",
        "Shooting",
        "Spark",
        "applications",
        "error",
        "tolerance",
        "spark",
        "applicationsjobs",
        "efficiency",
        "processing",
        "time",
        "pipelines",
        "Kafka",
        "producer",
        "API",
        "livestream",
        "data",
        "Kafka",
        "topics",
        "SparkStreaming",
        "applications",
        "data",
        "Kafka",
        "topics",
        "streams",
        "HBase",
        "Spark",
        "Memory",
        "capabilities",
        "datasets",
        "Broadcast",
        "variables",
        "Spark",
        "Joins",
        "transformations",
        "capabilities",
        "data",
        "processing",
        "EMR",
        "cluster",
        "S3",
        "AWS",
        "cloud",
        "Hive",
        "tables",
        "data",
        "hive",
        "scripts",
        "Partitioning",
        "Dynamic",
        "Partitions",
        "Buckets",
        "HIVE",
        "Integration",
        "application",
        "Jenkins",
        "Interacted",
        "infrastructure",
        "network",
        "database",
        "application",
        "BA",
        "teams",
        "data",
        "quality",
        "availability",
        "Environment",
        "AWS",
        "EMR",
        "Spark",
        "Hive",
        "HDFS",
        "Sqoop",
        "Kafka",
        "Oozie",
        "HBase",
        "Scala",
        "MapReduce",
        "Spark",
        "Developer",
        "ATT",
        "Ledgewood",
        "NJ",
        "March",
        "January",
        "Roles",
        "Responsibilities",
        "data",
        "RDBMS",
        "data",
        "HDFS",
        "Teradata",
        "MySQL",
        "Sqoop",
        "spark",
        "application",
        "ELT",
        "kind",
        "operations",
        "data",
        "MapReduce",
        "jobs",
        "Spark",
        "transformations",
        "actions",
        "Spark",
        "RDDs",
        "Dataframe",
        "Spark",
        "SQL",
        "APIs",
        "Hive",
        "Bucketing",
        "kinds",
        "joins",
        "Hive",
        "tables",
        "tables",
        "ETL",
        "data",
        "basis",
        "data",
        "HIVE",
        "filtering",
        "Developed",
        "Sqoop",
        "jobs",
        "loads",
        "RDBMS",
        "HDFS",
        "Spark",
        "transformations",
        "data",
        "tables",
        "spark",
        "Parquet",
        "format",
        "Oozie",
        "workflows",
        "data",
        "pipelines",
        "Migrating",
        "Map",
        "code",
        "Spark",
        "transformations",
        "Spark",
        "Scala",
        "Collecting",
        "amounts",
        "log",
        "data",
        "Apache",
        "Flume",
        "data",
        "HDFS",
        "analysis",
        "problems",
        "standards",
        "procedures",
        "JIRA",
        "Environment",
        "HDP",
        "Hortonworks",
        "Data",
        "Platform",
        "Spark",
        "Scala",
        "Sqoop",
        "Oozie",
        "Hive",
        "Cent",
        "OS",
        "MySQL",
        "Oracle",
        "DB",
        "Flume",
        "Hadoop",
        "Developer",
        "Life",
        "Lock",
        "Tempe",
        "AZ",
        "September",
        "February",
        "Roles",
        "Responsibilities",
        "Spark",
        "applications",
        "Scala",
        "data",
        "cleansing",
        "validation",
        "transformation",
        "summarization",
        "activities",
        "requirement",
        "data",
        "Spark",
        "RDD",
        "data",
        "computation",
        "output",
        "requirements",
        "data",
        "pipelines",
        "Spark",
        "Hive",
        "Sqoop",
        "transform",
        "data",
        "Spark",
        "jobs",
        "Hive",
        "jobs",
        "data",
        "performance",
        "tuning",
        "Spark",
        "application",
        "performance",
        "Performance",
        "Spark",
        "jobs",
        "configuration",
        "properties",
        "broadcast",
        "variables",
        "time",
        "data",
        "Spark",
        "Kafka",
        "Responsible",
        "Streaming",
        "data",
        "web",
        "server",
        "console",
        "logs",
        "file",
        "formats",
        "Text",
        "Sequence",
        "Avro",
        "Parquet",
        "JSON",
        "XML",
        "files",
        "files",
        "Map",
        "Reduce",
        "Programs",
        "process",
        "import",
        "data",
        "DB2",
        "Teradata",
        "Hive",
        "tables",
        "Sqoop",
        "Wrote",
        "Pig",
        "Scripts",
        "Map",
        "Reduce",
        "jobs",
        "ETL",
        "procedures",
        "data",
        "HDFS",
        "SQL",
        "scripts",
        "solution",
        "Scala",
        "Solved",
        "performance",
        "issues",
        "Hive",
        "Pig",
        "scripts",
        "understanding",
        "Joins",
        "Group",
        "Aggregation",
        "MR",
        "jobs",
        "cross",
        "consulting",
        "teams",
        "data",
        "science",
        "analytics",
        "team",
        "solutions",
        "business",
        "insights",
        "clients",
        "problems",
        "data",
        "databases",
        "Sqoop",
        "visualization",
        "reports",
        "BI",
        "team",
        "HiveHQL",
        "Hive",
        "queries",
        "data",
        "Hive",
        "Tables",
        "data",
        "HBase",
        "tables",
        "Partitions",
        "Partitioning",
        "bucketing",
        "tables",
        "Hive",
        "Managed",
        "tables",
        "optimization",
        "Hive",
        "queries",
        "amounts",
        "log",
        "data",
        "Flume",
        "data",
        "HDFS",
        "analysis",
        "Assisted",
        "analytics",
        "team",
        "Pig",
        "Hive",
        "scripts",
        "analysis",
        "data",
        "Oozie",
        "workflows",
        "job",
        "scheduling",
        "batch",
        "Environment",
        "Java",
        "Scala",
        "Apache",
        "Spark",
        "MySQL",
        "CDH",
        "IntelliJ",
        "IDEA",
        "Hive",
        "HDFS",
        "YARN",
        "Map",
        "Reduce",
        "Sqoop",
        "PIG",
        "Flume",
        "Unix",
        "Shell",
        "Scripting",
        "Python",
        "Apache",
        "Kafka",
        "Big",
        "Data",
        "Hadoop",
        "Developer",
        "Zyno",
        "Medical",
        "Natick",
        "MA",
        "November",
        "August",
        "Roles",
        "Responsibilities",
        "business",
        "customers",
        "business",
        "requirements",
        "peers",
        "requirements",
        "BRD",
        "TDD",
        "documents",
        "table",
        "rollup",
        "process",
        "data",
        "mapping",
        "Modified",
        "Database",
        "tables",
        "HBASE",
        "Queries",
        "data",
        "tables",
        "loading",
        "sets",
        "data",
        "databases",
        "HDFS",
        "Sqoop",
        "imports",
        "data",
        "Hive",
        "queries",
        "Pig",
        "scripts",
        "data",
        "Developed",
        "Pig",
        "Latin",
        "scripts",
        "data",
        "web",
        "server",
        "output",
        "files",
        "HDFS",
        "Hive",
        "tables",
        "data",
        "Hive",
        "queries",
        "map",
        "OOZIE",
        "Operational",
        "Services",
        "batch",
        "processing",
        "scheduling",
        "workflows",
        "UDFs",
        "data",
        "structures",
        "HBase",
        "Cassandra",
        "application",
        "development",
        "RDBMS",
        "Linux",
        "shell",
        "Developed",
        "media",
        "analytics",
        "dashboards",
        "basis",
        "processing",
        "engine",
        "Hortonworks",
        "distribution",
        "Manage",
        "Hadoop",
        "log",
        "insights",
        "brand",
        "conversations",
        "turn",
        "brand",
        "awareness",
        "engagement",
        "traffic",
        "media",
        "pages",
        "identifying",
        "defects",
        "function",
        "error",
        "inconsistencies",
        "output",
        "Environment",
        "Hadoop",
        "MapReduce",
        "Yarn",
        "Hive",
        "Pig",
        "HBase",
        "Oozie",
        "Sqoop",
        "Oracle",
        "g",
        "Scala",
        "HDFS",
        "Eclipse",
        "Java",
        "Developer",
        "Mariner",
        "Finance",
        "Laurel",
        "MD",
        "October",
        "October",
        "Roles",
        "Responsibilities",
        "requirements",
        "end",
        "users",
        "requirements",
        "process",
        "flow",
        "requirements",
        "Development",
        "Graphical",
        "user",
        "interface",
        "user",
        "selfservice",
        "screen",
        "eyes",
        "quality",
        "check",
        "process",
        "workflow",
        "platform",
        "level",
        "Development",
        "UI",
        "models",
        "HTML",
        "JSP",
        "JavaScript",
        "Web",
        "Link",
        "CSS",
        "Developed",
        "Struts",
        "Action",
        "classes",
        "Validation",
        "classes",
        "Struts",
        "controller",
        "component",
        "Struts",
        "validation",
        "framework",
        "Support",
        "end",
        "user",
        "training",
        "testing",
        "documentation",
        "Backing",
        "beans",
        "UI",
        "components",
        "stores",
        "state",
        "scope",
        "EJB",
        "Stateless",
        "sessions",
        "Controller",
        "database",
        "integration",
        "Hibernate",
        "spring",
        "Hibernate",
        "mapping",
        "Oracle",
        "database",
        "Oracle",
        "PLSQL",
        "Select",
        "Update",
        "Delete",
        "data",
        "MAVEN",
        "build",
        "automation",
        "GIT",
        "version",
        "control",
        "Environment",
        "Java",
        "J2EE",
        "JSP",
        "Maven",
        "Linux",
        "CSS",
        "GIT",
        "Oracle",
        "XML",
        "SAX",
        "Rational",
        "Rose",
        "UML",
        "Java",
        "Developer",
        "Acute",
        "Soft",
        "Solutions",
        "India",
        "Private",
        "Limited",
        "Hyderabad",
        "Telangana",
        "July",
        "August",
        "Roles",
        "Responsibilities",
        "application",
        "JavaJ2EE",
        "platform",
        "Model",
        "View",
        "Control",
        "MVC",
        "structure",
        "Struts",
        "Portal",
        "UI",
        "HTML",
        "JavaScript",
        "XML",
        "JSP",
        "Java",
        "CSS",
        "requirements",
        "clientside",
        "Java",
        "script",
        "validations",
        "Serverside",
        "bean",
        "Validation",
        "Framework",
        "JSR",
        "Spring",
        "Core",
        "Annotations",
        "Dependency",
        "Injection",
        "Hibernate",
        "persistence",
        "framework",
        "mapping",
        "ORM",
        "table",
        "Hibernate",
        "annotations",
        "service",
        "classes",
        "utility",
        "API",
        "framework",
        "Axis",
        "Web",
        "Services",
        "integration",
        "systems",
        "Developed",
        "Web",
        "services",
        "component",
        "XML",
        "WSDL",
        "SOAP",
        "DOM",
        "parser",
        "data",
        "applications",
        "capabilities",
        "Web",
        "Services",
        "SOAPWSDL",
        "SOAP",
        "UI",
        "Restful",
        "Web",
        "Services",
        "request",
        "AJAX",
        "framework",
        "server",
        "communication",
        "user",
        "experience",
        "test",
        "framework",
        "Selenium",
        "Web",
        "testing",
        "Chrome",
        "IE",
        "Mozilla",
        "Web",
        "driver",
        "JQUERY",
        "TABS",
        "UNIX",
        "shell",
        "scripts",
        "build",
        "process",
        "jobs",
        "file",
        "transfers",
        "hosts",
        "Log4j",
        "output",
        "files",
        "JUnit",
        "Eclipse",
        "unit",
        "testing",
        "modules",
        "production",
        "support",
        "monitoring",
        "server",
        "error",
        "logs",
        "issues",
        "levels",
        "Environment",
        "Java",
        "J2EE",
        "JSP",
        "Servlets",
        "Spring",
        "Servlets",
        "Custom",
        "Tags",
        "Java",
        "Beans",
        "JMS",
        "Hibernate",
        "IBM",
        "MQ",
        "Series",
        "AJAX",
        "Junit",
        "Log4j",
        "JNDI",
        "Oracle",
        "XML",
        "SAX",
        "Rational",
        "Rose",
        "UML",
        "Education",
        "Bachelors",
        "Degree",
        "Computer",
        "Science",
        "Computer",
        "Science",
        "Acharya",
        "Nagarjuna",
        "University",
        "Skills",
        "Cassandra",
        "Hdfs",
        "Mapreduce",
        "Oozie",
        "Sqoop",
        "Hbase",
        "Db2",
        "Mongodb",
        "Nosql",
        "Teradata",
        "Hbase",
        "Hive",
        "Mapreduce",
        "Pig",
        "Python",
        "Database",
        "Sql",
        "server",
        "Mysql",
        "Oracle",
        "Sql",
        "Additional",
        "Information",
        "Resultdriven",
        "IT",
        "Professional",
        "experience",
        "Software",
        "development",
        "years",
        "expertise",
        "BigData",
        "technologies",
        "Hadoop",
        "Spark",
        "Professional",
        "Java",
        "developer",
        "expertise",
        "data",
        "engineering",
        "data",
        "technologies",
        "Spark",
        "Hive",
        "Pig",
        "MapReduce",
        "Sqoop",
        "Kafka",
        "Oozie",
        "HBase",
        "Impala",
        "Yarn",
        "Hands",
        "experience",
        "programming",
        "Java",
        "Python",
        "Scala",
        "SQL",
        "Sound",
        "knowledge",
        "architecture",
        "Distributed",
        "Systems",
        "processing",
        "frameworks",
        "Experience",
        "Hadoop",
        "distributions",
        "premise",
        "CDH",
        "HDP",
        "cloud",
        "AWS",
        "experience",
        "data",
        "analytics",
        "data",
        "services",
        "AWS",
        "Cloud",
        "EMR",
        "Redshift",
        "S3",
        "Athena",
        "Glue",
        "Expert",
        "production",
        "spark",
        "application",
        "Spark",
        "RDD",
        "APIs",
        "Data",
        "SparkSQL",
        "SparkStreaming",
        "APIs",
        "experience",
        "Spark",
        "Streaming",
        "Spark",
        "SQL",
        "components",
        "spark",
        "accumulators",
        "Broadcast",
        "levels",
        "optimization",
        "techniques",
        "spark",
        "jobs",
        "Proficient",
        "data",
        "RDBMS",
        "HDFS",
        "Sqoop",
        "Solid",
        "experience",
        "data",
        "formats",
        "Parquet",
        "Orc",
        "Avro",
        "JSon",
        "Experience",
        "endtoend",
        "data",
        "pipelines",
        "resilience",
        "recoverability",
        "knowledge",
        "NoSQL",
        "HBase",
        "Cassandra",
        "Mongo",
        "DB",
        "Expert",
        "SQL",
        "RDBMSs",
        "Oracle",
        "SQL",
        "Server",
        "DB2",
        "MySQL",
        "Teradata",
        "Proficient",
        "Worked",
        "GIT",
        "Jenkins",
        "Maven",
        "Good",
        "understanding",
        "Experience",
        "Agile",
        "Waterfall",
        "methodologies",
        "Software",
        "Development",
        "Life",
        "Cycle",
        "SDLC",
        "selflearner",
        "attitude",
        "willingness",
        "concepts",
        "challenges",
        "competencies",
        "Big",
        "Data",
        "Ecosystems",
        "HDFS",
        "MapReduce",
        "YARN",
        "Hive",
        "Sqoop",
        "Pig",
        "Spark",
        "HBase",
        "Oozie",
        "Programming",
        "Languages",
        "Java",
        "Scala",
        "Python",
        "SQL",
        "AWS",
        "S3",
        "EMR",
        "Redshift",
        "Athena",
        "Glue",
        "Database",
        "SQL",
        "Server",
        "MySQL",
        "Oracle",
        "DB2",
        "Teradata",
        "NoSQL",
        "HBase",
        "MongoDB",
        "Cassandra",
        "IDEs",
        "Utilities",
        "Eclipse",
        "IntelliJ",
        "Development",
        "Methodologies",
        "Agile",
        "Waterfall",
        "Model"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T19:37:17.390814",
    "resume_data": "Sr HadoopSpark Developer Sr HadoopSpark span lDeveloperspan Sr HadoopSpark Developer Astra Zeneca Wilmington DE Authorized to work in the US for any employer Work Experience Sr HadoopSpark Developer Astra Zeneca Wilmington DE March 2018 to Present Roles Responsibilities Ingested gigabytes of click stream data from external servers such as FTP server and S3 buckets on daily basis using custom Input Adapters Created Sqoop scripts to importexport user profile data from RDBMS to S3 data lake Developed various spark applications using Scala to perform various enrichments of user behavioral data click stream data merged with user profile data Involved in data cleansing event enrichment data aggregation denormalization and data preparation needed for downstream model learning and reporting Utilized Spark Scala API to implement batch processing of jobs Trouble Shooting Spark applications for improved error tolerance Finetuning spark applicationsjobs to improve the efficiency and overall processing time for the pipelines Created Kafka producer API to send livestream data into various Kafka topics Developed SparkStreaming applications to consume the data from Kafka topics and to insert the processed streams to HBase Utilized Spark in Memory capabilities to handle large datasets Used Broadcast variables in Spark effective efficient Joins transformations and other capabilities for data processing Worked on EMR cluster and S3 in AWS cloud Creating Hive tables loading and analyzing data using hive scripts Implemented Partitioning Dynamic Partitions Buckets in HIVE Involved in continuous Integration of application using Jenkins Interacted with the infrastructure network database application and BA teams to ensure data quality and availability Environment AWS EMR Spark Hive HDFS Sqoop Kafka Oozie HBase Scala MapReduce Spark Developer ATT Ledgewood NJ March 2016 to January 2018 Roles Responsibilities Extensively worked on migrating data from traditional RDBMS to HDFS Ingested data into HDFS from Teradata MySQL using Sqoop Involved in developing spark application to perform ELT kind of operations on the data Modified existing MapReduce jobs to Spark transformations and actions by utilizing Spark RDDs Dataframe and Spark SQL APIs Utilized Hive partitioning Bucketing and performed various kinds of joins on Hive tables Involved in creating Hive external tables to perform ETL on data that is produced on daily basis Validated the data being ingested into HIVE for further filtering and cleansing Developed Sqoop jobs for performing incremental loads from RDBMS into HDFS and further applied Spark transformations Loaded data into hive tables from spark and used Parquet columnar format Created Oozie workflows to automate and productionize the data pipelines Migrating Map Reduce code into Spark transformations using Spark and Scala Collecting and aggregating large amounts of log data using Apache Flume and staging data in HDFS for further analysis Designed documented operational problems by following standards and procedures using JIRA Environment HDP Hortonworks Data Platform Spark Scala Sqoop Oozie Hive Cent OS MySQL Oracle DB Flume Hadoop Developer Life Lock Tempe AZ September 2015 to February 2016 Roles Responsibilities Involved in writing Spark applications using Scala to perform various data cleansing validation transformation and summarization activities according to the requirement Load the data into Spark RDD and Perform inmemory data computation to generate the output as per the requirements Developed data pipelines using Spark Hive and Sqoop to ingest transform and analyze operational data Developed Spark jobs Hive jobs to summarize and transform data Worked on performance tuning of Spark application to improve performance Performance tuning the Spark jobs by changing the configuration properties and using broadcast variables Real time streaming the data using Spark with Kafka Responsible for handling Streaming data from web server console logs Worked on different file formats like Text Sequence files Avro Parquet JSON XML files and Flat files using Map Reduce Programs Developed daily process to do incremental import of data from DB2 and Teradata into Hive tables using Sqoop Wrote Pig Scripts to generate Map Reduce jobs and performed ETL procedures on the data in HDFS Analyzed the SQL scripts and designed the solution to implement using Scala Solved performance issues in Hive and Pig scripts with understanding of Joins Group and Aggregation and how does it translate to MR jobs Work with cross functional consulting teams within the data science and analytics team to design develop and execute solutions to derive business insights and solve clients operational and strategic problems Exported the analyzed data to the relational databases using Sqoop for visualization and to generate reports for the BI team Extensively used HiveHQL or Hive queries to query data in Hive Tables and loaded data into HBase tables Extensively worked with Partitions Dynamic Partitioning bucketing tables in Hive designed both Managed and External tables also worked on optimization of Hive queries Involved in collecting and aggregating large amounts of log data using Flume and staging data in HDFS for further analysis Assisted analytics team by writing Pig and Hive scripts to perform further detailed analysis of the data Designed Oozie workflows for job scheduling and batch processing Environment Java Scala Apache Spark MySQL CDH IntelliJ IDEA Hive HDFS YARN Map Reduce Sqoop PIG Flume Unix Shell Scripting Python Apache Kafka Big Data Hadoop Developer Zyno Medical Natick MA November 2013 to August 2015 Roles Responsibilities Coordinated with business customers to gather business requirements and interacted with other technical peers to derive Technical requirements and delivered the BRD and TDD documents Involved in validating the aggregate table based on the rollup process documented in the data mapping Designed and Modified Database tables and used HBASE Queries to insert and fetch data from tables Involved in loading and transforming large sets of structured semi structured and unstructured data from relational databases into HDFS using Sqoop imports Responsible for analyzing and cleansing raw data by performing Hive queries and running Pig scripts on data Developed Pig Latin scripts to extract the data from the web server output files to load into HDFS Created Hive tables loaded data and wrote Hive queries that run within the map Used OOZIE Operational Services for batch processing and scheduling workflows dynamically and created UDFs to store specialized data structures in HBase and Cassandra Involved in application development using RDBMS and Linux shell scripting Developed and updated social media analytics dashboards on regular basis Created a complete processing engine based on Hortonworks distribution Manage and review Hadoop log files Developed and generated insights based on brand conversations which in turn helpful for effectively driving brand awareness engagement and traffic to social media pages Involved in the identifying analyzing defects questionable function error and inconsistencies in output Environment Hadoop MapReduce Yarn Hive Pig HBase Oozie Sqoop Oracle 11g Scala HDFS Eclipse Java Developer Mariner Finance Laurel MD October 2012 to October 2013 Roles Responsibilities Gathered requirements from end users and create functional requirements Worked on process flow analyzing the functional requirements Development of Graphical user interface for user selfservice screen Implemented four eyes principle and created quality check process reusable across all workflow on overall platform level Development of UI models using HTML JSP JavaScript Web Link and CSS Developed Struts Action classes and Validation classes using Struts controller component and Struts validation framework Support in end user training testing and documentation Implemented Backing beans for handling UI components and stores its state in a scope Worked on implementing EJB Stateless sessions for communicating with Controller Implemented database integration using Hibernate and utilized spring with Hibernate for mapping with Oracle database Worked on Oracle PLSQL queries to Select Update and Delete data Worked on MAVEN for build automation Used GIT for version control Environment Java J2EE JSP Maven Linux CSS GIT Oracle XML SAX Rational Rose UML Java Developer Acute Soft Solutions India Private Limited Hyderabad Telangana July 2011 to August 2012 Roles Responsibilities Involved in developing the application using JavaJ2EE platform Implemented the Model View Control MVC structure using Struts Responsible to enhance the Portal UI using HTML JavaScript XML JSP Java CSS as per the requirements and providing the clientside Java script validations and Serverside bean Validation Framework JSR 303 Used Spring Core Annotations for Dependency Injection Used Hibernate as persistence framework mapping the ORM objects to table using Hibernate annotations Responsible to write the different service classes and utility API which will be used across the framework Used Axis to implementing Web Services for integration of different systems Developed Web services component using XML WSDL and SOAP with DOM parser to transfer and transform data between applications Exposed various capabilities as Web Services using SOAPWSDL Used SOAP UI for testing the Restful Web Services by sending and SOAP request Used AJAX framework for server communication and seamless user experience Created test framework on Selenium and executed Web testing in Chrome IE and Mozilla through Web driver Used clientside java scripting JQUERY for designing TABS and DIALOGBOX Created UNIX shell scripts to automate the build process to perform regular jobs like file transfers between different hosts Used Log4j for the logging the output to the files Used JUnit Eclipse for the unit testing of various modules Involved in production support monitoring server and error logs and foreseeing the Potential issues and escalating to the higher levels Environment Java J2EE JSP Servlets Spring Servlets Custom Tags Java Beans JMS Hibernate IBM MQ Series AJAX Junit Log4j JNDI Oracle XML SAX Rational Rose UML Education Bachelors Degree in Computer Science in Computer Science Acharya Nagarjuna University 2011 Skills Cassandra Hdfs Mapreduce Oozie Sqoop Hbase Db2 Mongodb Nosql Teradata Hbase Hive Mapreduce Pig Python Database Sql server Mysql Oracle Sql Additional Information Resultdriven IT Professional with referable 8 experience in Software development with 5 years of recent expertise in BigData technologies including Hadoop and Spark Professional Java developer with strong expertise in data engineering and big data technologies Highly skilled on Spark Hive Pig MapReduce Sqoop Kafka Oozie HBase Impala and Yarn Hands on experience in programming using Java Python Scala and SQL Sound knowledge of architecture of Distributed Systems and parallel processing frameworks Experience with Hadoop distributions both on premise CDH HDP and in cloud AWS Good experience working with various data analytics and big data services in AWS Cloud like EMR Redshift S3 Athena Glue etc Expert in developing production ready spark application using Spark RDD APIs Data frames SparkSQL and SparkStreaming APIs Strong experience in using Spark Streaming Spark SQL and other components of spark like accumulators Broadcast variables different levels of caching and optimization techniques for spark jobs Proficient in importingexporting data from RDBMS to HDFS using Sqoop Solid experience in working various data formats like Parquet Orc Avro JSon etc Experience automating endtoend data pipelines with strong resilience and recoverability Strong knowledge of NoSQL databases and worked with HBase Cassandra and Mongo DB Expert in SQL extensively worked RDBMSs like Oracle SQL Server DB2 MySQL and Teradata Proficient and Worked with GIT Jenkins and Maven Good understanding and Experience with Agile and Waterfall methodologies of Software Development Life Cycle SDLC Highly motivated selflearner with a positive attitude willingness to learn new concepts and accepts challenges Technical competencies Big Data Ecosystems HDFS MapReduce YARN Hive Sqoop Pig Spark HBase Oozie Programming Languages Java Scala Python SQL AWS technologies S3 EMR Redshift Athena Glue Database SQL Server MySQL Oracle DB2 Teradata NoSQL Databases HBase MongoDB Cassandra IDEs Utilities Eclipse IntelliJ Development Methodologies Agile Waterfall Model",
    "unique_id": "0719d545-ac21-4835-b188-feb0cd01213b"
}