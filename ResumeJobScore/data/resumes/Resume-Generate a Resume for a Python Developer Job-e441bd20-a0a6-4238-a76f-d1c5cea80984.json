{
    "clean_data": "Sr HadoopSpark Developer Sr HadoopSpark span lDeveloperspan Sr HadoopSpark Developer JPMC Jersey City NJ Extensive IT experience of over 9 years with multinational clients which includes of Big Data related architecture experience developing SparkHadoop applications Excellent understanding knowledge of Hadoop architecture and various components such as HDFS JobTracker TaskTracker NameNode DataNode and MapReduce programming paradigm Experience in all major components of Hadoop Ecocomponents such as HDFS HIVE PIG Oozie Sqoop Map Reduce and YARN on Cloudera MapR and Hortonworks distributions Experience in tuning and troubleshooting performance issues in Hadoop cluster Designing and creating HIVE external tables using shared metastore instead of the derby with partitioning dynamic partitioning and buckets Experience in Oozie and workflow scheduler to manage Hadoop jobs by Direct Acyclic Graph DAG of actions with control flows Experience in integrating Hive and HBase for effective operations Developed the Pig UDFS to preprocess the data for analysis Experience working on different file formats like Avro Parquet ORC Sequence and Compression techniques like Gzip Lzo and Snappy in Hadoop Strong understanding of NoSQL databases and hands on work experience in writing applications on NoSQL databases like HBase Cassandra and MongoDB Redis Neo4j Experience in working on CQL Cassandra Query Language for retrieving the data present in Cassandra cluster by running queries in CQL Proficient with Cluster management and configuring Cassandra Database Experienced in working with Spark ecosystem using SparkSQL and Scala queries on different data file formats like txt csv etc Implemented POC to migrate Map Reduce jobs into Spark RDD transformations using SCALA Have good experience in creating real time data streaming solutions using Apache SparkSpark StreamingApache Storm Kafka and Flume Working knowledge on major Hadoop ecosystems PIG HIVE Sqoop and Flume Good experience in Cloudera Hortonworks Apache Hadoop distributions Knowledge on AWS Amazon EC2 Hadoop distribution Developed highthroughput streaming apps reading from Kafka queues and writing enriched data back to outbound Kafka queues Wrote and worked on complex performance improvements on PLSQL queries stored procedures triggers indexes with databases like MySQL and Oracle Also working towards improvement of knowledge on NoSQL databases like MongoDB Experience on NoSQL databases including HBase Cassandra Handson experience in scripting skills in Python Linux and UNIX Shell Good working experience using Sqoop to import data into HDFS from RDBMS and viceversa Knowledge on creating Solr collection configuration to scale up the infrastructure Experience in developing webbased applications using Python Experience in application development using Java J2EE EJB Hibernate JDBC Jakarta Struts JSP and Servlets Experience in using various IDEs Eclipse My Eclipse and repositories SVN and CVS Experience of using build tools Ant and Maven Working with relative ease with different working strategies like Agile Waterfall and Scrum methodologies Excellent communication and analytical skills and flexible to adapt to evolving technology Authorized to work in the US for any employer Work Experience Sr HadoopSpark Developer JPMC Chicago IL April 2018 to Present Roles Responsibilities Responsible for building scalable distributed data solutions using Hadoop Responsible for managing and scheduling Jobs on a Hadoop cluster Loading data from UNIX file system to HDFS and vice versa Improving the performance and optimization of existing algorithms in Hadoop using Spark context SparkSQL and Spark YARN Involved in converting HiveSQL queries into Spark transformations using Spark RDDs Python and Scala Worked with Apache Spark for large data processing integrated with functional programming language Scala Developed POC using Scala Spark SQL and MLlib libraries along with Kafka and other tools as per requirement then deployed on the Yarn cluster Extract Real time feed using Kafka and Spark Streaming and convert it to RDD and process data in the form of Data Frame and save the data as Parquet format in HDFS Implemented Data Ingestion in real time processing using Kafka Developed Spark code using Scala and SparkSQLStreaming for faster processing of data Configured Spark Streaming to receive real time data and store the stream data to HDFS Developed Spark scripts by using Scala shell commands as per the requirement Used Spark API over Cloudera Hadoop YARN to perform analytics on data in Hive Documented the requirements including the available code which should be implemented using Spark Hive HDFS and SOLR Tested Apache TEZ an extensible framework for building high performance batch and interactive data processing applications on Pig and Hive jobs Used Kafka Streams to Configure Spark streaming to get information and then store it in HDFS Developed multiple Kafka Producers and Consumers as per the software requirement specifications Extract Real time feed using Kafka and Spark Streaming and convert it to RDD and process data in the form of Data Frame and save the data as Parquet format in HDFS Real time streaming the data using Spark with Kafka Responsible for creating Hive tables and working on them using Hive QL Implementing various Hive UDFs as per business requirements Exported the analyzed data to the databases using Sqoop for visualization and to generate reports for the BI team Involved in Data Visualization using Tableau for Reporting from Hive Tables Developed Python Mapper and Reducer scripts and implemented them using Hadoop Streaming Developed multiple Map Reduce jobs in java for data cleaning and preprocessing Optimized Map Reduce Jobs to use HDFS efficiently by using various compression mechanisms Responsible for writing Hive queries for data analysis to meet the business requirements Customized Apache Solr to handle fallback searching and provide custom functions Responsible for setup and benchmarking of HadoopHBase clusters Environment Hadoop HDFS HBase Sqoop Hive Map Reduce Spark  Scala Kafka Solr Sbt Java Python UbuntuCent OS MySQL Linux GitHub Maven Jenkins Scala Developer TMobile Atlanta GA May 2016 to March 2018 Responsibilities Creating end to end SparkSolr applications using Scala to perform various data cleansing Involved in converting HiveSQL queries into Spark transformations using Spark RDDS and Scala Developed Spark scripts by using Scala shell commands as per the requirement Used Akka as a framework to create reactive distributed parallel and resilient concurrent applications in Scala Used Slick to query and storing in database in a Scala fashion using the powerful Scala collection framework Developed POC using Scala deployed on Yarn cluster compared the performance of Spark with Hive and SQL Deployed and maintained multinode Dev and Test Kafka Clusters Using Spark API over Cloudera Hadoop YARN to perform analytics on data in Hive Developed Scala scripts UDFFs using both Data framesSQL and RDDMapReduce in Spark 16 for Data Aggregation queries and writing data back into OLTP system through Sqoop and Developed enterprise application using Scala as well Performed advanced procedures like text analytics and processing using the inmemory computing capabilities of Spark using Scala Developed an equivalent Spark Scala code for existing SAS code to extract summary insights on the hive tables Implemented Spark using Scala and utilizing Data frames and Spark SQL API for faster processing of data Implemented the Data Bricks API in Scala program to push the processed data to Redshift DB Redshift is columnar and compressed storage scale linearly and seamlessly Worked on the performance tuning of spark data frames for aggregation using dynamic partition creating the temp views needed Developed Spark applications for the entire batch processing by using Scala Developed Spark scripts by using Scala shell commands as per the requirement Environment Hive Flume Java Maven Impala Spark Oozie Oracle Yarn GitHub Junit Tableau Unix Cloudera Flume Sqoop HDFS Tomcat Java Scala Hbase BigData Developer GE Waukesha WI October 2014 to April 2016 Roles Responsibilities Involved in Automation of clickstream data collection and store into HDFS using Flume Involved in creating Data Lake by extracting customers data from various data sources into HDFS Used Sqoop to load data from Oracle Database into Hive Developed MapReduce programs to cleanse the data in HDFS obtained from multiple data sources Implemented various Pig UDFs for converting unstructured data into structured data Developed Pig Latin scripts for data processing Optimizing of existing algorithms in Hadoop using Spark Context SparkSQL Data Frames and Pair RDDs Load the data into Spark RDD and performed inmemory data computation to generate the output response Developed the Apache Spark Flume and HDFS integration project to do a realtime data analysis Developed data pipeline using Flume Spark and Hive to ingest transform and analyzing data Wrote Flume configuration files for importing streaming log data into MongoDB with Flume Performed masking on customer sensitive data using Flume interceptors Used IMPALA to analyze data ingested into Hive tables and compute various metrics for reporting on the dashboard Implemented Spark using Scala and utilizing Data frames and Spark SQL API for faster processing of data Making code changes for a module in turbine simulation for processing across the cluster using sparksubmit Involved in performing the analytics and visualization for the data from the logs and estimate the error rate and study the probability of future errors using regressing models Used WEB HDFS REST API to make the HTTP GET PUT POST and DELETE requests from the webserver to perform analytics on the data lake Involved in creating Hive tables as per requirement defined with appropriate static and dynamic partitions Used Hive to analyze the data in HDFS to identify issues and behavioral patterns Involved in production Hadoop cluster set up administration maintenance monitoring and support Logical implementation and interaction with HBase Assisted in creation of large HBase tables using large set of data from various portfolios Cluster coordination services through Zookeeper Efficiently put and fetched data tofrom HBase by writing MapReduce job Developed MapReduce jobs to automate transfer of data fromto HBase Assisted with the addition of Hadoop processing to the IT infrastructure Used flume to collect the entire web log from the online adservers and push into HDFS Implemented custom business logic by writing UDFs in Java and used various UDFs from Piggybank and other sources Implemented MapReduce job and execute the MapReduce job to process the log data from the adservers Load and transform large sets of structured semi structured and unstructured data Performing analysis using high level languages like Python Launching Amazon EC2 cloud instances using Amazon images and configuring launched instances with respect to specific applications Backend Java developer for Data Management Platform DMP and building RESTful APIs to build and letother groups build dashboards Environment Hadoop Pig Sqoop Oozie MapReduce HDFS Hive Java Python Eclipse HBase Flume AWS Oracle 10g UNIX Shell Scripting GitHub Maven JavaJ2EE Developer Capital Coact Inc Columbia MD January 2012 to September 2014 Roles Responsibilities Involved in writing programs for XA transaction management on multiple databases of the application Developed java programs JSP pages and servlets using Cantata Struts framework Involved in creating database tables writing complex TSQL queries and stored procedures in the SQL server Worked with AJAX framework to get the asynchronous response for the user request and used JavaScript for the validation Used EJBs in the application and developed Session beans to implement business logic at the middle tier level Actively involved in writing SQL using SQL Query Builder Involved in coordinating the onshoreOffshore development and mentoring the new team members Extensively Used Ant tool to build and configure J2EE applications and used Log4J for logging in the application Used JAXB to read and manipulate the xml properties Used JNI for calling the libraries and other implemented functions in C language Used prototype MooTools and scriptaculous for fluid User Interface Involved in fixing defects and unit testing with test cases using JUnit Environment Java EJB Servlets XSLT CVS J2EE AJAX Struts Hibernate ANT Tomcat JMS UML Log4J Oracle 10g Eclipse Solaris JUnit and Windows 7XP Maven Java Developer Choice Perficient St Louis MO April 2010 to December 2012 Roles Responsibilities Played an active role in the team by interacting with business and program specialists and converted business requirements into system requirements Conducted Design reviews and Technical reviews with other project stakeholders Implemented Services using Core Java Involved in development of classes using java Good proficiency in developing algorithms for serial interfaces Involved in testing of CAN protocols Developed the flow of algorithm in UML Used Servlets to implement Business components Designed and Developed required Manager Classes for database operations Developed various Servlets for monitoring the application Designed and developed the front end using HTML and JSP Developed XML files DTDs Schemas and parsing XML by using both SAX and DOM parser Wrote deployment descriptors using XML and Test java classes for a direct testing of the Session and Entity beans Did Packaging and Deployment of builds through ANT script Wrote stored procedure and used JAVA APIs to call these procedures Database designing that includes defining tables views constraints triggers sequences index and stored procedures Developed verification and validation scripts in java Followed verification and validation cycle for development of algorithms Developed Test cases for Unit Test cases and as well as System and User test scenarios Involved in Unit Testing User Acceptance Testing and Bug Fixing Environment Java JSP Servlets JDBC JavaScript MySQL JUnit Eclipse IDE Windows 7XPVista UNIX LINUX Education Bachelors Skills Cassandra Hdfs Mapreduce Oozie Sqoop CertificationsLicenses Drivers License",
    "entities": [
        "CQL Cassandra Query Language",
        "MLlib",
        "Implemented Spark",
        "SparkHadoop",
        "JobTracker",
        "Spark Context",
        "AJAX",
        "HadoopHBase clusters Environment Hadoop HDFS HBase",
        "Python Launching Amazon",
        "BI",
        "HDFS",
        "UNIX",
        "IMPALA",
        "Technical",
        "Developed Spark",
        "SOLR",
        "Maven JavaJ2EE Developer Capital Coact Inc Columbia",
        "JPMC Jersey City NJ",
        "Logical",
        "Data Lake",
        "Gzip Lzo",
        "CVS",
        "JPMC Chicago",
        "Avro Parquet ORC Sequence and Compression",
        "Spark YARN Involved",
        "RDD",
        "Hadoop",
        "XML",
        "Atlanta",
        "Redshift DB Redshift",
        "Spark Hive HDFS",
        "HDFS Used Sqoop",
        "Hive Developed",
        "HBase",
        "Apache Spark",
        "Amazon",
        "Cloudera Hadoop",
        "Spark with",
        "Servlets Experience",
        "SparkSQL",
        "Developed",
        "Scala Developed",
        "Flume Involved",
        "Yarn",
        "HDFS Implemented Data Ingestion",
        "Windows",
        "Implemented MapReduce job",
        "HBase Cassandra Handson",
        "Conducted Design",
        "Spark RDDS",
        "JSP",
        "Zookeeper Efficiently",
        "Work Experience Sr HadoopSpark Developer",
        "DOM",
        "Spark SQL API",
        "Hadoop Strong",
        "Oracle Database",
        "Spark",
        "Packaging and Deployment",
        "EJB",
        "Implemented Services",
        "Data Visualization",
        "JSP Developed XML",
        "Direct Acyclic Graph DAG",
        "Cantata Struts framework Involved",
        "US",
        "Database",
        "Sqoop",
        "HIVE",
        "Optimizing",
        "Apache SparkSpark StreamingApache",
        "Sr HadoopSpark Developer",
        "Data Aggregation",
        "PIG",
        "SAX",
        "Consumers",
        "java",
        "SQL Deployed",
        "SAS",
        "HTML",
        "Oozie",
        "Test",
        "SQL",
        "java Followed",
        "OLTP",
        "Spark RDD",
        "MD",
        "SQL Query Builder Involved",
        "JNI",
        "Data Management Platform DMP",
        "Ant",
        "DELETE",
        "the Data Bricks API",
        "Data Frame",
        "Big Data",
        "Hive",
        "Schemas",
        "Maven Working",
        "Hadoop Ecocomponents",
        "HBase Assisted",
        "UML Used Servlets",
        "Performed",
        "Cluster",
        "Impala",
        "JavaScript",
        "XA",
        "ANT",
        "Piggybank",
        "Flume Spark",
        "SVN",
        "Hadoop Responsible",
        "Cloudera Hortonworks Apache Hadoop",
        "Interface Involved",
        "Data",
        "Developed Test",
        "MapReduce",
        "NoSQL",
        "CQL Proficient",
        "GE",
        "IDE Windows 7XPVista"
    ],
    "experience": "Experience in all major components of Hadoop Ecocomponents such as HDFS HIVE PIG Oozie Sqoop Map Reduce and YARN on Cloudera MapR and Hortonworks distributions Experience in tuning and troubleshooting performance issues in Hadoop cluster Designing and creating HIVE external tables using shared metastore instead of the derby with partitioning dynamic partitioning and buckets Experience in Oozie and workflow scheduler to manage Hadoop jobs by Direct Acyclic Graph DAG of actions with control flows Experience in integrating Hive and HBase for effective operations Developed the Pig UDFS to preprocess the data for analysis Experience working on different file formats like Avro Parquet ORC Sequence and Compression techniques like Gzip Lzo and Snappy in Hadoop Strong understanding of NoSQL databases and hands on work experience in writing applications on NoSQL databases like HBase Cassandra and MongoDB Redis Neo4j Experience in working on CQL Cassandra Query Language for retrieving the data present in Cassandra cluster by running queries in CQL Proficient with Cluster management and configuring Cassandra Database Experienced in working with Spark ecosystem using SparkSQL and Scala queries on different data file formats like txt csv etc Implemented POC to migrate Map Reduce jobs into Spark RDD transformations using SCALA Have good experience in creating real time data streaming solutions using Apache SparkSpark StreamingApache Storm Kafka and Flume Working knowledge on major Hadoop ecosystems PIG HIVE Sqoop and Flume Good experience in Cloudera Hortonworks Apache Hadoop distributions Knowledge on AWS Amazon EC2 Hadoop distribution Developed highthroughput streaming apps reading from Kafka queues and writing enriched data back to outbound Kafka queues Wrote and worked on complex performance improvements on PLSQL queries stored procedures triggers indexes with databases like MySQL and Oracle Also working towards improvement of knowledge on NoSQL databases like MongoDB Experience on NoSQL databases including HBase Cassandra Handson experience in scripting skills in Python Linux and UNIX Shell Good working experience using Sqoop to import data into HDFS from RDBMS and viceversa Knowledge on creating Solr collection configuration to scale up the infrastructure Experience in developing webbased applications using Python Experience in application development using Java J2EE EJB Hibernate JDBC Jakarta Struts JSP and Servlets Experience in using various IDEs Eclipse My Eclipse and repositories SVN and CVS Experience of using build tools Ant and Maven Working with relative ease with different working strategies like Agile Waterfall and Scrum methodologies Excellent communication and analytical skills and flexible to adapt to evolving technology Authorized to work in the US for any employer Work Experience Sr HadoopSpark Developer JPMC Chicago IL April 2018 to Present Roles Responsibilities Responsible for building scalable distributed data solutions using Hadoop Responsible for managing and scheduling Jobs on a Hadoop cluster Loading data from UNIX file system to HDFS and vice versa Improving the performance and optimization of existing algorithms in Hadoop using Spark context SparkSQL and Spark YARN Involved in converting HiveSQL queries into Spark transformations using Spark RDDs Python and Scala Worked with Apache Spark for large data processing integrated with functional programming language Scala Developed POC using Scala Spark SQL and MLlib libraries along with Kafka and other tools as per requirement then deployed on the Yarn cluster Extract Real time feed using Kafka and Spark Streaming and convert it to RDD and process data in the form of Data Frame and save the data as Parquet format in HDFS Implemented Data Ingestion in real time processing using Kafka Developed Spark code using Scala and SparkSQLStreaming for faster processing of data Configured Spark Streaming to receive real time data and store the stream data to HDFS Developed Spark scripts by using Scala shell commands as per the requirement Used Spark API over Cloudera Hadoop YARN to perform analytics on data in Hive Documented the requirements including the available code which should be implemented using Spark Hive HDFS and SOLR Tested Apache TEZ an extensible framework for building high performance batch and interactive data processing applications on Pig and Hive jobs Used Kafka Streams to Configure Spark streaming to get information and then store it in HDFS Developed multiple Kafka Producers and Consumers as per the software requirement specifications Extract Real time feed using Kafka and Spark Streaming and convert it to RDD and process data in the form of Data Frame and save the data as Parquet format in HDFS Real time streaming the data using Spark with Kafka Responsible for creating Hive tables and working on them using Hive QL Implementing various Hive UDFs as per business requirements Exported the analyzed data to the databases using Sqoop for visualization and to generate reports for the BI team Involved in Data Visualization using Tableau for Reporting from Hive Tables Developed Python Mapper and Reducer scripts and implemented them using Hadoop Streaming Developed multiple Map Reduce jobs in java for data cleaning and preprocessing Optimized Map Reduce Jobs to use HDFS efficiently by using various compression mechanisms Responsible for writing Hive queries for data analysis to meet the business requirements Customized Apache Solr to handle fallback searching and provide custom functions Responsible for setup and benchmarking of HadoopHBase clusters Environment Hadoop HDFS HBase Sqoop Hive Map Reduce Spark   Scala Kafka Solr Sbt Java Python UbuntuCent OS MySQL Linux GitHub Maven Jenkins Scala Developer TMobile Atlanta GA May 2016 to March 2018 Responsibilities Creating end to end SparkSolr applications using Scala to perform various data cleansing Involved in converting HiveSQL queries into Spark transformations using Spark RDDS and Scala Developed Spark scripts by using Scala shell commands as per the requirement Used Akka as a framework to create reactive distributed parallel and resilient concurrent applications in Scala Used Slick to query and storing in database in a Scala fashion using the powerful Scala collection framework Developed POC using Scala deployed on Yarn cluster compared the performance of Spark with Hive and SQL Deployed and maintained multinode Dev and Test Kafka Clusters Using Spark API over Cloudera Hadoop YARN to perform analytics on data in Hive Developed Scala scripts UDFFs using both Data framesSQL and RDDMapReduce in Spark 16 for Data Aggregation queries and writing data back into OLTP system through Sqoop and Developed enterprise application using Scala as well Performed advanced procedures like text analytics and processing using the inmemory computing capabilities of Spark using Scala Developed an equivalent Spark Scala code for existing SAS code to extract summary insights on the hive tables Implemented Spark using Scala and utilizing Data frames and Spark SQL API for faster processing of data Implemented the Data Bricks API in Scala program to push the processed data to Redshift DB Redshift is columnar and compressed storage scale linearly and seamlessly Worked on the performance tuning of spark data frames for aggregation using dynamic partition creating the temp views needed Developed Spark applications for the entire batch processing by using Scala Developed Spark scripts by using Scala shell commands as per the requirement Environment Hive Flume Java Maven Impala Spark Oozie Oracle Yarn GitHub Junit Tableau Unix Cloudera Flume Sqoop HDFS Tomcat Java Scala Hbase BigData Developer GE Waukesha WI October 2014 to April 2016 Roles Responsibilities Involved in Automation of clickstream data collection and store into HDFS using Flume Involved in creating Data Lake by extracting customers data from various data sources into HDFS Used Sqoop to load data from Oracle Database into Hive Developed MapReduce programs to cleanse the data in HDFS obtained from multiple data sources Implemented various Pig UDFs for converting unstructured data into structured data Developed Pig Latin scripts for data processing Optimizing of existing algorithms in Hadoop using Spark Context SparkSQL Data Frames and Pair RDDs Load the data into Spark RDD and performed inmemory data computation to generate the output response Developed the Apache Spark Flume and HDFS integration project to do a realtime data analysis Developed data pipeline using Flume Spark and Hive to ingest transform and analyzing data Wrote Flume configuration files for importing streaming log data into MongoDB with Flume Performed masking on customer sensitive data using Flume interceptors Used IMPALA to analyze data ingested into Hive tables and compute various metrics for reporting on the dashboard Implemented Spark using Scala and utilizing Data frames and Spark SQL API for faster processing of data Making code changes for a module in turbine simulation for processing across the cluster using sparksubmit Involved in performing the analytics and visualization for the data from the logs and estimate the error rate and study the probability of future errors using regressing models Used WEB HDFS REST API to make the HTTP GET PUT POST and DELETE requests from the webserver to perform analytics on the data lake Involved in creating Hive tables as per requirement defined with appropriate static and dynamic partitions Used Hive to analyze the data in HDFS to identify issues and behavioral patterns Involved in production Hadoop cluster set up administration maintenance monitoring and support Logical implementation and interaction with HBase Assisted in creation of large HBase tables using large set of data from various portfolios Cluster coordination services through Zookeeper Efficiently put and fetched data tofrom HBase by writing MapReduce job Developed MapReduce jobs to automate transfer of data fromto HBase Assisted with the addition of Hadoop processing to the IT infrastructure Used flume to collect the entire web log from the online adservers and push into HDFS Implemented custom business logic by writing UDFs in Java and used various UDFs from Piggybank and other sources Implemented MapReduce job and execute the MapReduce job to process the log data from the adservers Load and transform large sets of structured semi structured and unstructured data Performing analysis using high level languages like Python Launching Amazon EC2 cloud instances using Amazon images and configuring launched instances with respect to specific applications Backend Java developer for Data Management Platform DMP and building RESTful APIs to build and letother groups build dashboards Environment Hadoop Pig Sqoop Oozie MapReduce HDFS Hive Java Python Eclipse HBase Flume AWS Oracle 10 g UNIX Shell Scripting GitHub Maven JavaJ2EE Developer Capital Coact Inc Columbia MD January 2012 to September 2014 Roles Responsibilities Involved in writing programs for XA transaction management on multiple databases of the application Developed java programs JSP pages and servlets using Cantata Struts framework Involved in creating database tables writing complex TSQL queries and stored procedures in the SQL server Worked with AJAX framework to get the asynchronous response for the user request and used JavaScript for the validation Used EJBs in the application and developed Session beans to implement business logic at the middle tier level Actively involved in writing SQL using SQL Query Builder Involved in coordinating the onshoreOffshore development and mentoring the new team members Extensively Used Ant tool to build and configure J2EE applications and used Log4J for logging in the application Used JAXB to read and manipulate the xml properties Used JNI for calling the libraries and other implemented functions in C language Used prototype MooTools and scriptaculous for fluid User Interface Involved in fixing defects and unit testing with test cases using JUnit Environment Java EJB Servlets XSLT CVS J2EE AJAX Struts Hibernate ANT Tomcat JMS UML Log4J Oracle 10 g Eclipse Solaris JUnit and Windows 7XP Maven Java Developer Choice Perficient St Louis MO April 2010 to December 2012 Roles Responsibilities Played an active role in the team by interacting with business and program specialists and converted business requirements into system requirements Conducted Design reviews and Technical reviews with other project stakeholders Implemented Services using Core Java Involved in development of classes using java Good proficiency in developing algorithms for serial interfaces Involved in testing of CAN protocols Developed the flow of algorithm in UML Used Servlets to implement Business components Designed and Developed required Manager Classes for database operations Developed various Servlets for monitoring the application Designed and developed the front end using HTML and JSP Developed XML files DTDs Schemas and parsing XML by using both SAX and DOM parser Wrote deployment descriptors using XML and Test java classes for a direct testing of the Session and Entity beans Did Packaging and Deployment of builds through ANT script Wrote stored procedure and used JAVA APIs to call these procedures Database designing that includes defining tables views constraints triggers sequences index and stored procedures Developed verification and validation scripts in java Followed verification and validation cycle for development of algorithms Developed Test cases for Unit Test cases and as well as System and User test scenarios Involved in Unit Testing User Acceptance Testing and Bug Fixing Environment Java JSP Servlets JDBC JavaScript MySQL JUnit Eclipse IDE Windows 7XPVista UNIX LINUX Education Bachelors Skills Cassandra Hdfs Mapreduce Oozie Sqoop CertificationsLicenses Drivers License",
    "extracted_keywords": [
        "Sr",
        "HadoopSpark",
        "Developer",
        "Sr",
        "HadoopSpark",
        "span",
        "lDeveloperspan",
        "Sr",
        "HadoopSpark",
        "Developer",
        "JPMC",
        "Jersey",
        "City",
        "NJ",
        "IT",
        "experience",
        "years",
        "clients",
        "Big",
        "Data",
        "architecture",
        "experience",
        "SparkHadoop",
        "applications",
        "Excellent",
        "knowledge",
        "Hadoop",
        "architecture",
        "components",
        "HDFS",
        "JobTracker",
        "TaskTracker",
        "NameNode",
        "DataNode",
        "MapReduce",
        "programming",
        "paradigm",
        "Experience",
        "components",
        "Hadoop",
        "Ecocomponents",
        "HDFS",
        "HIVE",
        "PIG",
        "Oozie",
        "Sqoop",
        "Map",
        "Reduce",
        "YARN",
        "Cloudera",
        "MapR",
        "Hortonworks",
        "distributions",
        "Experience",
        "troubleshooting",
        "performance",
        "issues",
        "Hadoop",
        "cluster",
        "Designing",
        "tables",
        "metastore",
        "derby",
        "partitioning",
        "Experience",
        "Oozie",
        "workflow",
        "scheduler",
        "Hadoop",
        "jobs",
        "Direct",
        "Acyclic",
        "Graph",
        "DAG",
        "actions",
        "control",
        "Experience",
        "Hive",
        "HBase",
        "operations",
        "Pig",
        "UDFS",
        "data",
        "analysis",
        "Experience",
        "file",
        "formats",
        "Avro",
        "Parquet",
        "ORC",
        "Sequence",
        "Compression",
        "techniques",
        "Gzip",
        "Lzo",
        "Snappy",
        "Hadoop",
        "Strong",
        "understanding",
        "NoSQL",
        "databases",
        "hands",
        "work",
        "experience",
        "applications",
        "NoSQL",
        "databases",
        "HBase",
        "Cassandra",
        "MongoDB",
        "Redis",
        "Neo4j",
        "Experience",
        "CQL",
        "Cassandra",
        "Query",
        "Language",
        "data",
        "Cassandra",
        "cluster",
        "queries",
        "CQL",
        "Proficient",
        "Cluster",
        "management",
        "Cassandra",
        "Database",
        "Spark",
        "ecosystem",
        "SparkSQL",
        "Scala",
        "data",
        "file",
        "formats",
        "txt",
        "csv",
        "Implemented",
        "POC",
        "Map",
        "Reduce",
        "jobs",
        "Spark",
        "RDD",
        "transformations",
        "SCALA",
        "experience",
        "time",
        "data",
        "streaming",
        "solutions",
        "Apache",
        "SparkSpark",
        "StreamingApache",
        "Storm",
        "Kafka",
        "Flume",
        "Working",
        "knowledge",
        "Hadoop",
        "PIG",
        "HIVE",
        "Sqoop",
        "Flume",
        "experience",
        "Cloudera",
        "Hortonworks",
        "Apache",
        "Hadoop",
        "Knowledge",
        "AWS",
        "Amazon",
        "EC2",
        "Hadoop",
        "distribution",
        "Developed",
        "apps",
        "Kafka",
        "queues",
        "data",
        "Kafka",
        "queues",
        "Wrote",
        "performance",
        "improvements",
        "PLSQL",
        "queries",
        "procedures",
        "indexes",
        "databases",
        "MySQL",
        "Oracle",
        "improvement",
        "knowledge",
        "NoSQL",
        "databases",
        "MongoDB",
        "Experience",
        "NoSQL",
        "databases",
        "HBase",
        "Cassandra",
        "Handson",
        "experience",
        "scripting",
        "skills",
        "Python",
        "Linux",
        "UNIX",
        "Shell",
        "Good",
        "working",
        "experience",
        "Sqoop",
        "data",
        "HDFS",
        "RDBMS",
        "viceversa",
        "Knowledge",
        "Solr",
        "collection",
        "configuration",
        "infrastructure",
        "Experience",
        "applications",
        "Python",
        "Experience",
        "application",
        "development",
        "Java",
        "J2EE",
        "EJB",
        "Hibernate",
        "JDBC",
        "Jakarta",
        "Struts",
        "JSP",
        "Servlets",
        "Experience",
        "IDEs",
        "Eclipse",
        "Eclipse",
        "repositories",
        "SVN",
        "CVS",
        "Experience",
        "build",
        "tools",
        "Ant",
        "Maven",
        "Working",
        "ease",
        "working",
        "strategies",
        "Agile",
        "Waterfall",
        "Scrum",
        "methodologies",
        "communication",
        "skills",
        "technology",
        "US",
        "employer",
        "Work",
        "Experience",
        "Sr",
        "HadoopSpark",
        "Developer",
        "JPMC",
        "Chicago",
        "IL",
        "April",
        "Present",
        "Roles",
        "Responsibilities",
        "data",
        "solutions",
        "Hadoop",
        "Responsible",
        "scheduling",
        "Jobs",
        "Hadoop",
        "cluster",
        "Loading",
        "data",
        "UNIX",
        "file",
        "system",
        "HDFS",
        "vice",
        "performance",
        "optimization",
        "algorithms",
        "Hadoop",
        "Spark",
        "context",
        "SparkSQL",
        "Spark",
        "YARN",
        "HiveSQL",
        "queries",
        "Spark",
        "transformations",
        "Spark",
        "RDDs",
        "Python",
        "Scala",
        "Apache",
        "Spark",
        "data",
        "processing",
        "programming",
        "language",
        "Scala",
        "Developed",
        "POC",
        "Scala",
        "Spark",
        "SQL",
        "MLlib",
        "libraries",
        "Kafka",
        "tools",
        "requirement",
        "Yarn",
        "cluster",
        "Extract",
        "time",
        "feed",
        "Kafka",
        "Spark",
        "Streaming",
        "process",
        "data",
        "form",
        "Data",
        "Frame",
        "data",
        "Parquet",
        "format",
        "HDFS",
        "Data",
        "Ingestion",
        "time",
        "processing",
        "Kafka",
        "Developed",
        "Spark",
        "code",
        "Scala",
        "SparkSQLStreaming",
        "processing",
        "data",
        "Configured",
        "Spark",
        "Streaming",
        "time",
        "data",
        "stream",
        "data",
        "HDFS",
        "Spark",
        "scripts",
        "Scala",
        "shell",
        "commands",
        "requirement",
        "Spark",
        "API",
        "Cloudera",
        "Hadoop",
        "YARN",
        "analytics",
        "data",
        "Hive",
        "requirements",
        "code",
        "Spark",
        "Hive",
        "HDFS",
        "SOLR",
        "Tested",
        "Apache",
        "TEZ",
        "framework",
        "performance",
        "batch",
        "data",
        "processing",
        "applications",
        "Pig",
        "Hive",
        "jobs",
        "Kafka",
        "Streams",
        "Spark",
        "streaming",
        "information",
        "HDFS",
        "Kafka",
        "Producers",
        "Consumers",
        "software",
        "requirement",
        "specifications",
        "Extract",
        "time",
        "feed",
        "Kafka",
        "Spark",
        "Streaming",
        "process",
        "data",
        "form",
        "Data",
        "Frame",
        "data",
        "Parquet",
        "format",
        "HDFS",
        "time",
        "data",
        "Spark",
        "Kafka",
        "Responsible",
        "Hive",
        "tables",
        "Hive",
        "QL",
        "Hive",
        "UDFs",
        "business",
        "requirements",
        "data",
        "databases",
        "Sqoop",
        "visualization",
        "reports",
        "BI",
        "team",
        "Data",
        "Visualization",
        "Tableau",
        "Hive",
        "Tables",
        "Developed",
        "Python",
        "Mapper",
        "Reducer",
        "scripts",
        "Hadoop",
        "Streaming",
        "Map",
        "Reduce",
        "jobs",
        "java",
        "data",
        "cleaning",
        "Map",
        "Reduce",
        "Jobs",
        "HDFS",
        "compression",
        "mechanisms",
        "Hive",
        "queries",
        "data",
        "analysis",
        "business",
        "requirements",
        "Customized",
        "Apache",
        "Solr",
        "fallback",
        "searching",
        "custom",
        "functions",
        "setup",
        "benchmarking",
        "HadoopHBase",
        "clusters",
        "Environment",
        "Hadoop",
        "HDFS",
        "HBase",
        "Sqoop",
        "Hive",
        "Map",
        "Reduce",
        "Spark",
        "Scala",
        "Kafka",
        "Solr",
        "Sbt",
        "Java",
        "Python",
        "UbuntuCent",
        "OS",
        "MySQL",
        "Linux",
        "GitHub",
        "Maven",
        "Jenkins",
        "Scala",
        "Developer",
        "TMobile",
        "Atlanta",
        "GA",
        "May",
        "March",
        "Responsibilities",
        "end",
        "SparkSolr",
        "applications",
        "Scala",
        "data",
        "cleansing",
        "HiveSQL",
        "queries",
        "Spark",
        "transformations",
        "Spark",
        "RDDS",
        "Scala",
        "Spark",
        "scripts",
        "Scala",
        "shell",
        "commands",
        "requirement",
        "Akka",
        "framework",
        "reactive",
        "applications",
        "Scala",
        "Slick",
        "database",
        "Scala",
        "fashion",
        "Scala",
        "collection",
        "framework",
        "Developed",
        "POC",
        "Scala",
        "Yarn",
        "cluster",
        "performance",
        "Spark",
        "Hive",
        "SQL",
        "Deployed",
        "multinode",
        "Dev",
        "Test",
        "Kafka",
        "Clusters",
        "Spark",
        "API",
        "Cloudera",
        "Hadoop",
        "YARN",
        "analytics",
        "data",
        "Hive",
        "Developed",
        "Scala",
        "UDFFs",
        "Data",
        "framesSQL",
        "RDDMapReduce",
        "Spark",
        "Data",
        "Aggregation",
        "data",
        "OLTP",
        "system",
        "Sqoop",
        "enterprise",
        "application",
        "Scala",
        "procedures",
        "text",
        "analytics",
        "processing",
        "inmemory",
        "computing",
        "capabilities",
        "Spark",
        "Scala",
        "Spark",
        "Scala",
        "code",
        "SAS",
        "code",
        "summary",
        "insights",
        "tables",
        "Spark",
        "Scala",
        "Data",
        "frames",
        "Spark",
        "SQL",
        "API",
        "processing",
        "data",
        "Data",
        "Bricks",
        "API",
        "Scala",
        "program",
        "data",
        "Redshift",
        "DB",
        "Redshift",
        "storage",
        "scale",
        "performance",
        "tuning",
        "spark",
        "data",
        "frames",
        "aggregation",
        "partition",
        "temp",
        "views",
        "Developed",
        "Spark",
        "applications",
        "batch",
        "processing",
        "Scala",
        "Developed",
        "Spark",
        "scripts",
        "Scala",
        "shell",
        "commands",
        "requirement",
        "Environment",
        "Hive",
        "Flume",
        "Java",
        "Maven",
        "Impala",
        "Spark",
        "Oozie",
        "Oracle",
        "Yarn",
        "GitHub",
        "Junit",
        "Tableau",
        "Unix",
        "Cloudera",
        "Flume",
        "Sqoop",
        "HDFS",
        "Tomcat",
        "Java",
        "Scala",
        "Hbase",
        "BigData",
        "Developer",
        "GE",
        "Waukesha",
        "WI",
        "October",
        "April",
        "Roles",
        "Responsibilities",
        "Automation",
        "clickstream",
        "data",
        "collection",
        "store",
        "HDFS",
        "Flume",
        "Data",
        "Lake",
        "customers",
        "data",
        "data",
        "sources",
        "HDFS",
        "Used",
        "Sqoop",
        "data",
        "Oracle",
        "Database",
        "Hive",
        "Developed",
        "MapReduce",
        "programs",
        "data",
        "HDFS",
        "data",
        "sources",
        "Pig",
        "UDFs",
        "data",
        "data",
        "Developed",
        "Pig",
        "Latin",
        "scripts",
        "data",
        "processing",
        "Optimizing",
        "algorithms",
        "Hadoop",
        "Spark",
        "Context",
        "SparkSQL",
        "Data",
        "Frames",
        "Pair",
        "RDDs",
        "data",
        "Spark",
        "RDD",
        "data",
        "computation",
        "output",
        "response",
        "Apache",
        "Spark",
        "Flume",
        "HDFS",
        "integration",
        "project",
        "data",
        "analysis",
        "data",
        "pipeline",
        "Flume",
        "Spark",
        "Hive",
        "transform",
        "data",
        "Wrote",
        "Flume",
        "configuration",
        "files",
        "streaming",
        "log",
        "data",
        "MongoDB",
        "Flume",
        "Performed",
        "masking",
        "customer",
        "data",
        "Flume",
        "interceptors",
        "IMPALA",
        "data",
        "Hive",
        "tables",
        "metrics",
        "dashboard",
        "Spark",
        "Scala",
        "Data",
        "frames",
        "Spark",
        "SQL",
        "API",
        "processing",
        "data",
        "code",
        "changes",
        "module",
        "turbine",
        "simulation",
        "cluster",
        "sparksubmit",
        "analytics",
        "visualization",
        "data",
        "logs",
        "error",
        "rate",
        "probability",
        "errors",
        "models",
        "WEB",
        "HDFS",
        "REST",
        "API",
        "HTTP",
        "GET",
        "PUT",
        "POST",
        "DELETE",
        "requests",
        "webserver",
        "analytics",
        "data",
        "lake",
        "Hive",
        "tables",
        "requirement",
        "partitions",
        "Hive",
        "data",
        "HDFS",
        "issues",
        "patterns",
        "production",
        "Hadoop",
        "cluster",
        "administration",
        "maintenance",
        "monitoring",
        "implementation",
        "interaction",
        "HBase",
        "Assisted",
        "creation",
        "HBase",
        "tables",
        "set",
        "data",
        "portfolios",
        "Cluster",
        "coordination",
        "services",
        "Zookeeper",
        "data",
        "HBase",
        "MapReduce",
        "job",
        "Developed",
        "MapReduce",
        "jobs",
        "transfer",
        "data",
        "fromto",
        "HBase",
        "addition",
        "Hadoop",
        "processing",
        "IT",
        "infrastructure",
        "flume",
        "web",
        "log",
        "adservers",
        "custom",
        "business",
        "logic",
        "UDFs",
        "Java",
        "UDFs",
        "Piggybank",
        "sources",
        "MapReduce",
        "job",
        "MapReduce",
        "job",
        "log",
        "data",
        "adservers",
        "Load",
        "sets",
        "data",
        "analysis",
        "level",
        "languages",
        "Python",
        "Amazon",
        "EC2",
        "cloud",
        "instances",
        "Amazon",
        "images",
        "instances",
        "respect",
        "applications",
        "Backend",
        "Java",
        "developer",
        "Data",
        "Management",
        "Platform",
        "DMP",
        "APIs",
        "letother",
        "groups",
        "dashboards",
        "Environment",
        "Hadoop",
        "Pig",
        "Sqoop",
        "Oozie",
        "MapReduce",
        "HDFS",
        "Hive",
        "Java",
        "Python",
        "Eclipse",
        "HBase",
        "Flume",
        "AWS",
        "Oracle",
        "g",
        "UNIX",
        "Shell",
        "Scripting",
        "GitHub",
        "Maven",
        "JavaJ2EE",
        "Developer",
        "Capital",
        "Coact",
        "Inc",
        "Columbia",
        "MD",
        "January",
        "September",
        "Roles",
        "Responsibilities",
        "programs",
        "XA",
        "transaction",
        "management",
        "databases",
        "application",
        "programs",
        "JSP",
        "pages",
        "servlets",
        "Cantata",
        "Struts",
        "framework",
        "database",
        "tables",
        "TSQL",
        "queries",
        "procedures",
        "SQL",
        "server",
        "framework",
        "response",
        "user",
        "request",
        "JavaScript",
        "validation",
        "EJBs",
        "application",
        "Session",
        "beans",
        "business",
        "logic",
        "tier",
        "level",
        "SQL",
        "SQL",
        "Query",
        "Builder",
        "onshoreOffshore",
        "development",
        "team",
        "members",
        "Ant",
        "tool",
        "J2EE",
        "applications",
        "Log4J",
        "application",
        "JAXB",
        "xml",
        "properties",
        "JNI",
        "libraries",
        "functions",
        "C",
        "language",
        "prototype",
        "MooTools",
        "User",
        "Interface",
        "defects",
        "unit",
        "testing",
        "test",
        "cases",
        "JUnit",
        "Environment",
        "Java",
        "EJB",
        "Servlets",
        "XSLT",
        "CVS",
        "J2EE",
        "AJAX",
        "Struts",
        "Hibernate",
        "ANT",
        "Tomcat",
        "JMS",
        "UML",
        "Log4J",
        "Oracle",
        "g",
        "Eclipse",
        "Solaris",
        "JUnit",
        "Windows",
        "Maven",
        "Java",
        "Developer",
        "Choice",
        "Perficient",
        "St",
        "Louis",
        "MO",
        "April",
        "December",
        "Roles",
        "Responsibilities",
        "role",
        "team",
        "business",
        "program",
        "specialists",
        "business",
        "requirements",
        "system",
        "requirements",
        "Design",
        "reviews",
        "reviews",
        "project",
        "stakeholders",
        "Services",
        "Core",
        "Java",
        "development",
        "classes",
        "proficiency",
        "algorithms",
        "interfaces",
        "testing",
        "CAN",
        "protocols",
        "flow",
        "algorithm",
        "UML",
        "Servlets",
        "Business",
        "components",
        "Developed",
        "Manager",
        "Classes",
        "database",
        "operations",
        "Servlets",
        "application",
        "end",
        "HTML",
        "JSP",
        "Developed",
        "XML",
        "files",
        "DTDs",
        "Schemas",
        "XML",
        "SAX",
        "DOM",
        "parser",
        "Wrote",
        "deployment",
        "descriptors",
        "XML",
        "Test",
        "classes",
        "testing",
        "Session",
        "Entity",
        "beans",
        "Packaging",
        "Deployment",
        "builds",
        "ANT",
        "script",
        "Wrote",
        "procedure",
        "APIs",
        "procedures",
        "Database",
        "tables",
        "views",
        "constraints",
        "sequences",
        "index",
        "procedures",
        "verification",
        "validation",
        "scripts",
        "verification",
        "validation",
        "cycle",
        "development",
        "algorithms",
        "Developed",
        "Test",
        "cases",
        "Unit",
        "Test",
        "cases",
        "System",
        "User",
        "test",
        "scenarios",
        "Unit",
        "Testing",
        "User",
        "Acceptance",
        "Testing",
        "Bug",
        "Fixing",
        "Environment",
        "Java",
        "JSP",
        "Servlets",
        "JDBC",
        "JavaScript",
        "MySQL",
        "JUnit",
        "Eclipse",
        "IDE",
        "Windows",
        "7XPVista",
        "UNIX",
        "LINUX",
        "Education",
        "Bachelors",
        "Skills",
        "Cassandra",
        "Hdfs",
        "Mapreduce",
        "Oozie",
        "Sqoop",
        "CertificationsLicenses",
        "Drivers",
        "License"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T21:14:54.570717",
    "resume_data": "Sr HadoopSpark Developer Sr HadoopSpark span lDeveloperspan Sr HadoopSpark Developer JPMC Jersey City NJ Extensive IT experience of over 9 years with multinational clients which includes of Big Data related architecture experience developing SparkHadoop applications Excellent understanding knowledge of Hadoop architecture and various components such as HDFS JobTracker TaskTracker NameNode DataNode and MapReduce programming paradigm Experience in all major components of Hadoop Ecocomponents such as HDFS HIVE PIG Oozie Sqoop Map Reduce and YARN on Cloudera MapR and Hortonworks distributions Experience in tuning and troubleshooting performance issues in Hadoop cluster Designing and creating HIVE external tables using shared metastore instead of the derby with partitioning dynamic partitioning and buckets Experience in Oozie and workflow scheduler to manage Hadoop jobs by Direct Acyclic Graph DAG of actions with control flows Experience in integrating Hive and HBase for effective operations Developed the Pig UDFS to preprocess the data for analysis Experience working on different file formats like Avro Parquet ORC Sequence and Compression techniques like Gzip Lzo and Snappy in Hadoop Strong understanding of NoSQL databases and hands on work experience in writing applications on NoSQL databases like HBase Cassandra and MongoDB Redis Neo4j Experience in working on CQL Cassandra Query Language for retrieving the data present in Cassandra cluster by running queries in CQL Proficient with Cluster management and configuring Cassandra Database Experienced in working with Spark ecosystem using SparkSQL and Scala queries on different data file formats like txt csv etc Implemented POC to migrate Map Reduce jobs into Spark RDD transformations using SCALA Have good experience in creating real time data streaming solutions using Apache SparkSpark StreamingApache Storm Kafka and Flume Working knowledge on major Hadoop ecosystems PIG HIVE Sqoop and Flume Good experience in Cloudera Hortonworks Apache Hadoop distributions Knowledge on AWS Amazon EC2 Hadoop distribution Developed highthroughput streaming apps reading from Kafka queues and writing enriched data back to outbound Kafka queues Wrote and worked on complex performance improvements on PLSQL queries stored procedures triggers indexes with databases like MySQL and Oracle Also working towards improvement of knowledge on NoSQL databases like MongoDB Experience on NoSQL databases including HBase Cassandra Handson experience in scripting skills in Python Linux and UNIX Shell Good working experience using Sqoop to import data into HDFS from RDBMS and viceversa Knowledge on creating Solr collection configuration to scale up the infrastructure Experience in developing webbased applications using Python Experience in application development using Java J2EE EJB Hibernate JDBC Jakarta Struts JSP and Servlets Experience in using various IDEs Eclipse My Eclipse and repositories SVN and CVS Experience of using build tools Ant and Maven Working with relative ease with different working strategies like Agile Waterfall and Scrum methodologies Excellent communication and analytical skills and flexible to adapt to evolving technology Authorized to work in the US for any employer Work Experience Sr HadoopSpark Developer JPMC Chicago IL April 2018 to Present Roles Responsibilities Responsible for building scalable distributed data solutions using Hadoop Responsible for managing and scheduling Jobs on a Hadoop cluster Loading data from UNIX file system to HDFS and vice versa Improving the performance and optimization of existing algorithms in Hadoop using Spark context SparkSQL and Spark YARN Involved in converting HiveSQL queries into Spark transformations using Spark RDDs Python and Scala Worked with Apache Spark for large data processing integrated with functional programming language Scala Developed POC using Scala Spark SQL and MLlib libraries along with Kafka and other tools as per requirement then deployed on the Yarn cluster Extract Real time feed using Kafka and Spark Streaming and convert it to RDD and process data in the form of Data Frame and save the data as Parquet format in HDFS Implemented Data Ingestion in real time processing using Kafka Developed Spark code using Scala and SparkSQLStreaming for faster processing of data Configured Spark Streaming to receive real time data and store the stream data to HDFS Developed Spark scripts by using Scala shell commands as per the requirement Used Spark API over Cloudera Hadoop YARN to perform analytics on data in Hive Documented the requirements including the available code which should be implemented using Spark Hive HDFS and SOLR Tested Apache TEZ an extensible framework for building high performance batch and interactive data processing applications on Pig and Hive jobs Used Kafka Streams to Configure Spark streaming to get information and then store it in HDFS Developed multiple Kafka Producers and Consumers as per the software requirement specifications Extract Real time feed using Kafka and Spark Streaming and convert it to RDD and process data in the form of Data Frame and save the data as Parquet format in HDFS Real time streaming the data using Spark with Kafka Responsible for creating Hive tables and working on them using Hive QL Implementing various Hive UDFs as per business requirements Exported the analyzed data to the databases using Sqoop for visualization and to generate reports for the BI team Involved in Data Visualization using Tableau for Reporting from Hive Tables Developed Python Mapper and Reducer scripts and implemented them using Hadoop Streaming Developed multiple Map Reduce jobs in java for data cleaning and preprocessing Optimized Map Reduce Jobs to use HDFS efficiently by using various compression mechanisms Responsible for writing Hive queries for data analysis to meet the business requirements Customized Apache Solr to handle fallback searching and provide custom functions Responsible for setup and benchmarking of HadoopHBase clusters Environment Hadoop HDFS HBase Sqoop Hive Map Reduce Spark StreamingSQL Scala Kafka Solr Sbt Java Python UbuntuCent OS MySQL Linux GitHub Maven Jenkins Scala Developer TMobile Atlanta GA May 2016 to March 2018 Responsibilities Creating end to end SparkSolr applications using Scala to perform various data cleansing Involved in converting HiveSQL queries into Spark transformations using Spark RDDS and Scala Developed Spark scripts by using Scala shell commands as per the requirement Used Akka as a framework to create reactive distributed parallel and resilient concurrent applications in Scala Used Slick to query and storing in database in a Scala fashion using the powerful Scala collection framework Developed POC using Scala deployed on Yarn cluster compared the performance of Spark with Hive and SQL Deployed and maintained multinode Dev and Test Kafka Clusters Using Spark API over Cloudera Hadoop YARN to perform analytics on data in Hive Developed Scala scripts UDFFs using both Data framesSQL and RDDMapReduce in Spark 16 for Data Aggregation queries and writing data back into OLTP system through Sqoop and Developed enterprise application using Scala as well Performed advanced procedures like text analytics and processing using the inmemory computing capabilities of Spark using Scala Developed an equivalent Spark Scala code for existing SAS code to extract summary insights on the hive tables Implemented Spark using Scala and utilizing Data frames and Spark SQL API for faster processing of data Implemented the Data Bricks API in Scala program to push the processed data to Redshift DB Redshift is columnar and compressed storage scale linearly and seamlessly Worked on the performance tuning of spark data frames for aggregation using dynamic partition creating the temp views needed Developed Spark applications for the entire batch processing by using Scala Developed Spark scripts by using Scala shell commands as per the requirement Environment Hive Flume Java Maven Impala Spark Oozie Oracle Yarn GitHub Junit Tableau Unix Cloudera Flume Sqoop HDFS Tomcat Java Scala Hbase BigData Developer GE Waukesha WI October 2014 to April 2016 Roles Responsibilities Involved in Automation of clickstream data collection and store into HDFS using Flume Involved in creating Data Lake by extracting customers data from various data sources into HDFS Used Sqoop to load data from Oracle Database into Hive Developed MapReduce programs to cleanse the data in HDFS obtained from multiple data sources Implemented various Pig UDFs for converting unstructured data into structured data Developed Pig Latin scripts for data processing Optimizing of existing algorithms in Hadoop using Spark Context SparkSQL Data Frames and Pair RDDs Load the data into Spark RDD and performed inmemory data computation to generate the output response Developed the Apache Spark Flume and HDFS integration project to do a realtime data analysis Developed data pipeline using Flume Spark and Hive to ingest transform and analyzing data Wrote Flume configuration files for importing streaming log data into MongoDB with Flume Performed masking on customer sensitive data using Flume interceptors Used IMPALA to analyze data ingested into Hive tables and compute various metrics for reporting on the dashboard Implemented Spark using Scala and utilizing Data frames and Spark SQL API for faster processing of data Making code changes for a module in turbine simulation for processing across the cluster using sparksubmit Involved in performing the analytics and visualization for the data from the logs and estimate the error rate and study the probability of future errors using regressing models Used WEB HDFS REST API to make the HTTP GET PUT POST and DELETE requests from the webserver to perform analytics on the data lake Involved in creating Hive tables as per requirement defined with appropriate static and dynamic partitions Used Hive to analyze the data in HDFS to identify issues and behavioral patterns Involved in production Hadoop cluster set up administration maintenance monitoring and support Logical implementation and interaction with HBase Assisted in creation of large HBase tables using large set of data from various portfolios Cluster coordination services through Zookeeper Efficiently put and fetched data tofrom HBase by writing MapReduce job Developed MapReduce jobs to automate transfer of data fromto HBase Assisted with the addition of Hadoop processing to the IT infrastructure Used flume to collect the entire web log from the online adservers and push into HDFS Implemented custom business logic by writing UDFs in Java and used various UDFs from Piggybank and other sources Implemented MapReduce job and execute the MapReduce job to process the log data from the adservers Load and transform large sets of structured semi structured and unstructured data Performing analysis using high level languages like Python Launching Amazon EC2 cloud instances using Amazon images and configuring launched instances with respect to specific applications Backend Java developer for Data Management Platform DMP and building RESTful APIs to build and letother groups build dashboards Environment Hadoop Pig Sqoop Oozie MapReduce HDFS Hive Java Python Eclipse HBase Flume AWS Oracle 10g UNIX Shell Scripting GitHub Maven JavaJ2EE Developer Capital Coact Inc Columbia MD January 2012 to September 2014 Roles Responsibilities Involved in writing programs for XA transaction management on multiple databases of the application Developed java programs JSP pages and servlets using Cantata Struts framework Involved in creating database tables writing complex TSQL queries and stored procedures in the SQL server Worked with AJAX framework to get the asynchronous response for the user request and used JavaScript for the validation Used EJBs in the application and developed Session beans to implement business logic at the middle tier level Actively involved in writing SQL using SQL Query Builder Involved in coordinating the onshoreOffshore development and mentoring the new team members Extensively Used Ant tool to build and configure J2EE applications and used Log4J for logging in the application Used JAXB to read and manipulate the xml properties Used JNI for calling the libraries and other implemented functions in C language Used prototype MooTools and scriptaculous for fluid User Interface Involved in fixing defects and unit testing with test cases using JUnit Environment Java EJB Servlets XSLT CVS J2EE AJAX Struts Hibernate ANT Tomcat JMS UML Log4J Oracle 10g Eclipse Solaris JUnit and Windows 7XP Maven Java Developer Choice Perficient St Louis MO April 2010 to December 2012 Roles Responsibilities Played an active role in the team by interacting with business and program specialists and converted business requirements into system requirements Conducted Design reviews and Technical reviews with other project stakeholders Implemented Services using Core Java Involved in development of classes using java Good proficiency in developing algorithms for serial interfaces Involved in testing of CAN protocols Developed the flow of algorithm in UML Used Servlets to implement Business components Designed and Developed required Manager Classes for database operations Developed various Servlets for monitoring the application Designed and developed the front end using HTML and JSP Developed XML files DTDs Schemas and parsing XML by using both SAX and DOM parser Wrote deployment descriptors using XML and Test java classes for a direct testing of the Session and Entity beans Did Packaging and Deployment of builds through ANT script Wrote stored procedure and used JAVA APIs to call these procedures Database designing that includes defining tables views constraints triggers sequences index and stored procedures Developed verification and validation scripts in java Followed verification and validation cycle for development of algorithms Developed Test cases for Unit Test cases and as well as System and User test scenarios Involved in Unit Testing User Acceptance Testing and Bug Fixing Environment Java JSP Servlets JDBC JavaScript MySQL JUnit Eclipse IDE Windows 7XPVista UNIX LINUX Education Bachelors Skills Cassandra Hdfs Mapreduce Oozie Sqoop CertificationsLicenses Drivers License",
    "unique_id": "e441bd20-a0a6-4238-a76f-d1c5cea80984"
}