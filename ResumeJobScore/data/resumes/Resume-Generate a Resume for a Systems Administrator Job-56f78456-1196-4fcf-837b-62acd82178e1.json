{
    "clean_data": "Sr Python Developer Sr span lPythonspan span lDeveloperspan Sr Python Developer EASTBAYFOOTLOCKER Atlanta GA 10 years of experience as a WebApplication Developer and coding with analytical programming using Python Java Experienced with full software development lifecycle architecting scalable platforms object oriented programming database design and agile methodologies Experienced in MVW frameworks like Django Angularjs Java Script JQuery and Nodejs Expert knowledge of and experience in Object oriented Design and Programming concepts Experience object oriented programming OOP concepts using Python C and PHP Experienced in WAMP Windows Apache MYSQL and PythonPHP and LAMP Linux Apache MySQL and PythonPHP Architecture Experience in leading multiple efforts to build Hadoop platforms maximizing business value by combining data science with big data Advised organizations about big data a big data strategy the implementation of big data which technologies best fit the needs of the organization and even implements the selected big data solution Write MATLAB code to create discretized computer models of sloped levy geometries Experienced in developing webbased applications using Python Django PHP C XML CSS HTML DHTML JavaScript and Jquery Experienced in installing configuring modifying testing and deploying applications with Apache Well versed with design and development of presentation layer for web applications using technologies like HTML CSS and JavaScript Familiar with JSON based REST Web services and Amazon Web services Experienced in developing Web Services with Python programming language Experience in writing Sub Queries Stored Procedures Triggers Cursors and Functions on MySQL and PostgreSQL database Experienced in agile and waterfall methodologies with high quality deliverables delivered ontime Experience in utilizing SAS Procedures Macros and other SAS application for data extraction data cleansing data loading and reporting Maintained detailed documentation and architectural solutions in IT infrastructure and sales systems Very strong full life cycle application development experience Strong knowledge on Dev Express Controls Strong database design and programming skills in SQL Server 05 SQL Stored Procedures functions triggers Cursors Indexing importingexporting data from varied data sources Experience with continuous integration and automation using Jenkins Experience with Unit testing Test driven Development TDD Load Testing Have the ability to understand complex systems and be in command of the details to provide solutions Ability to learn and adapt quickly to the emerging new technologies and paradigms Excellent communication interpersonal and analytical skills and a highly motivated team player with the ability to work independently Practical experience with working on multiple environments like development testing production Handson experience in writing and reviewing requirements architecture documents test plans design documents quality analysis and audits Excellent analytical and problem solving skills and ability to work on own besides being a valuable and contributing team player Authorized to work in the US for any employer Work Experience Sr Python Developer EASTBAYFOOTLOCKER Wausau WI July 2017 to Present with Hadoop Eastbay is a supplier of athletic footwear apparel and sports equipment which sells by direct mail Since 1997 it has been the directtomail division of Foot Locker Inc Resposibilities Participated to develop a data platform from scratch and took part in requirement gathering and analysis phase of the project in documenting the business requirements Worked with team of Hadoop developers on maintaining the data platform applications for RISK management Worked in designing tables in Hive MYSQL using SQOOP and processing data like importing and exporting of databases to the HDFS Experienced in processing large datasets of different forms including structured semistructured and unstructured data Developed rest APIs using python with flask and django framework Experienced the integration of various data sources including Java JDBC RDBMS Shell Scripting Spreadsheets and Text files Exposure to various markup languages including XML JSON CSV Good Understanding of Hadoop architecture and the daemons of Hadoop including NameNode Data Node Job Tracker Task Tracker Resource Manager Hands on experience in ingesting data into Data Warehouse using various data loading techniques Developed python scripts to load data to hive from HDFS Participated in developing ETL components for executing various workflows Developed pig scripts and hive scripts for processing the data Handled the JSON XML Log data using Hive SERDE Pig and filter the data based on query factor Worked in agile methodology with 2 weeks sprints Scheduled Jobs using crontab rundeck and controlm Performed Branching Tagging Release Activities on Version Control Tools GIT and GITLAB Importing and exporting data jobs to perform operations like copying data from HDFS and to HDFS using Sqoop Developed Spark code and SparkSQLStreaming for faster testing and processing of data Data was Ingested which is received from various database providers using Sqoop onto HDFS for analysis and data processing Wrote and Implemented Apache PIG scripts to load data from and to store data into Hive Managed the imported data form different data sources performed transformation using Hive Pig and Map Reduce and loaded data in HDFS Executed Oozie workflow engine to run multiple Hive and Pig jobs which run independently with time and data availability To achieve Continuous Delivery goal on high scalable environment used Docker coupled with loadbalancing tool Nginx Developed Oozie workflow to run job onto data availability of transactions Created User Defined Functions UDFs for maintaining Incremental IDs Used Shell scripting to analyse the data from SQL Server source and processed it to store into HDFS Had good exposure to spark Mlib Streaming and sql by closely working with data scientists Generated reports from Hive data using Microstrategy Worked with complex sql queries to make joins Increased the time efficiency of the HIVEQL and reduced the time difference of executing the sets of data by applying the compression techniques for MapReduce Jobs Created Hive Partitions for storing Data for Different Companies under Different Partitions Environment Hadoop Hive sqoop pig Python 27 java Django 14 Flask XML MySQL MS SQL Server Linux Shell Scripting mongodb SQL Sr Python Developer FM Global January 2015 to June 2017 This is a mutual insurance company with offices worldwide that specializes in loss prevention services primarily to large corporations throughout the world in the Highly Protected RiskHPR property insurance market sector Responsibilities Participate in requirement gathering and analysis phase of the project in documenting the business requirements by conducting workshopsmeetings with various business users Worked with team of developers on Python applications for RISK management Developed PythonDjango application for Google Analytics aggregation and reporting Used Django configuration to manage URLs and application parameters Worked on Python Open stack APIs Used Python scripts to update content in the database and manipulate files Generated Python Django Forms to record data of online users Detailed Understanding on existing build system Tools related for information of various products and releases and test results information Designed and implemented map reduce jobs to support distributed processing using java Hive and Apache Pig Configured ec2 instances and configured IAM users and roles Created s3 data pipe using Boto API to load data from internal data sources Configured Jboss cluster and mysql database for application access Developed UDFs to provide custom hive and pig capabilities Built a mechanism for automatically moving the existing proprietary binary format data files to HDFS using a service called Ingestion service Comprehensive knowledge and experience in process improvement normalizationdenormalization data extraction data cleansing data manipulation Performed Data transformations in HIVE and used partitions buckets for performance improvements Ingestion of data into Hadoop using Sqoop and apply data transformations and using Pig and HIVE Used Python and Django creating graphics XML processing data exchange and business logic implementation Developed PlSql store procedures to convert the data from Oracle to MongoDB I have used Pandas API to put the data as time series and tabular format for east timestamp data manipulation and retrieval Automate report generation in MongoDB using Javascript shell scripting sed java Added support for Amazon AWS S3 and RDS to host staticmedia files and the database into Amazon Cloud Systems automation utilizing ControlM for scheduling and PowershellC for script development Used Pandas library for statistical Analysis Developed tools using Python Shell scripting XML to automate some of the menial tasks Interfacing with supervisors artists systems administrators and production to ensure production deadlines are met Worked very closely with designer tightly integrating Flash into the CMS with the use of Flashvars stored in the Django models Also created XML with Django to be used by the Flash Used HTML CSS JQuery JSON and Javascript for front end applications Designed and developed the UI of the website using HTML XHTML AJAX CSS and JavaScript Also used Bootstrap as a mechanism to manage and organize the html page layout Used Django configuration to manage URLs and application parameters Wrote and executed various MYSQL database queries from Python using PythonMySQL connector and MySQLdb package Involved in development of Web Services using SOAP for sending and getting data from the external interface in the XML format Worked on development of SQL and stored procedures on MYSQL Responsible for debugging the project monitored on JIRA Agile Performed troubleshooting fixed and deployed many Python bug fixes of the two main applications that were a main source of data for both customers and internal customer service team Environment Python hive oozie Amazon AWS S3 MySQL HTML Python 27 Django 14 HTML5 CSS XML MySQL MS SQL Server JavaScript AWS Linux Shell Scripting AJAX Mongodb Sr Python Developer Deustche bank NJ December 2013 to November 2014 Deustche bank is a German global banking and financial services company offering financial products and services for corporate and institutional clients along with private and business clients Services include sales trading research and origination of debt and equity mergers and acquisitions MA risk management products such as derivatives corporate finance wealth management retail banking fund management and transaction banking Responsibilities Worked with team of developers on Python applications for RISK management Designed the database schema for the content management system Designed and developed the UI of the website using HTML XHTML AJAX CSS and JavaScript Involved in development of Web Services using SOAP for sending and getting data from the external interface in the XML format Wrote Python routines to log into the websites and fetch data for selected options Performed testing using Djangos Test Module Skilled experience in installing configuring and using Apache Hadoop ecosystems such as Pig and Spark Built the entire Hadoop platform from scratch Experience in ingesting real timenear real time data using Flume Kafka Storm Evaluated suitability of Hadoop and its ecosystem to the above project and implementing validating with various proof of concept POC applications to eventually adopt them to benefit from the Big Data Hadoop initiative Estimated the Software Hardware requirements for the Name Node and Data Node in the cluster Extracted the needed data from the server into HDFS and Bulk Loaded the cleaned data into HBase using MapReduce Written the Map Reduce programs Hive UDFs in Java Develop HIVE queries for the analysts Created an email notification service upon completion of job for the particular team which requested for the data Defined job work flows as per their dependencies in Oozie Closely observed building the Reporting Application which uses the Spark SQL to fetch and generate reports on table data Knowledge in performance troubleshooting and tuning Hadoop clusters in Cloudera Worked on middle tier and persistence layer Created service and model layer classes and Value objectsPOJO to hold values between java classes and database fields ExportedImported data between different data sources using SQL Server Management Studio Maintained program libraries users manuals and technical documentation Responsible for debugging and troubleshooting the web application Successfully migrated all the data to the database while the site was in production Implemented the validation error handling and caching framework with Oracle Coherence cache Worked on scripts for setting up the discovery client with attribute data Worked on scriptsgranite reference data scripts for setting up adapter attributes in granite system Environment Python 27 Hadoop Django 14 HTML5 CSS XML MySQL JavaScript JQuery Mongo DB MS SQL Server JavaScript GitHub AWS Linux Shell Scripting AJAX Sr Python Developer Mangstor Austin TX January 2012 to November 2013 Mangstor is a leading developer of NVMe Flash Storage Solutions to accelerate application workloads in Webscale and Enterprise Data Centers Mangstors NVMe over Fabric Flash Storage Arrays and NVMe SolidState devices provide the building blocks necessary for highperformance highlyavailable and costeffective storage infrastructure Responsibilities Data warehouse migration Sybase 125 to Sybase 157 More than 70 of work is developing code in Python remaining time spent on database development and data modelling Worked on requirement gathering high level design implementation testing and deployment of code Creating and following the production deployment runbook Did Proof of Concept on DB2 BLUcolumn organized database Task prioritization with clients Designed and developed reusable Autosys jobs parsing and documentation software in Python using object oriented features being implemented in other projects in firm Designed and developed database object parsing dependency builder and documenting software in Python using object oriented features Developed ETL Extract Transform Load software for DB2 columnar database fact and dimension tables Massive data processing Sybase 15TB and Db2 2TB Data modelling in Sybase and DB2 Setting up schema users permissions creating database objects Database performance tuning procedures table functionsdb2 Provide L2L3 support on rotational basis Database objects refactoring removing legacy duplication Built pluggable software for housekeeping and cleanup being used by other projects in firm Data load analysis package to reveal errors during load based on historic trend Data reconciliation program for Sybase vs Db2 and Db2 vs Db2 prod vs qa in Python Other tools developed in Python to automate daily activities in python like monitoring DB Worked with shell scripts to build wrapper around ETL to do one time historic load Environment Python Java MySQL Linux HTML XHTML CSS AJAX JavaScript Apache Web Server Python Developer Bosch Designs Novi MI March 2010 to December 2011 Bosch designs and produces precision automotive components and systems sold to vehicle and powertrain manufacturers worldwide These include systems and components for gasoline and diesel injection airbag components ABS and conventional braking systems telematics as well as small motors electrical and electronic equipment Worked on development of Warranty and defects tracking system Responsibilities Wrote Python routines to log into the websites and fetch data for selected options Used Python modules such as requests urllib urllib2 for web crawling Used other packages such as Beautifulsoup for data parsing Worked on writing and as well as read data from csv and excel file formats Developed a MATLAB algorithm which determines an objects dimensions from digital images Webservices backend development using Python CherryPy Django SQLAlchemy Participated in developing the companys internal framework on Python This framework became a basement for the quick services development Framework based on CherryPy with GnuPg encryption reGnuPg module on the top Worked on resulting reports of the application and Tableau reports Worked on HTML5 CSS3 JavaScript AngularJS NodeJS Git REST API Mongo DB intelliJ IDEA Design and Setting up of environment of Mongodb with shards and replicasets DevTest and Production Private VPN using Ubuntu Python Django CherryPy Postgres Redis Bootstrap Jquery Mongo Fabric Git Tenjin Selenium Sphinx Nose Modifying data using SASBASE SAS MACROS Extracting data from the database using SASAccess SAS SQL procedures and create SAS data sets Performed QA testing on the application Developed approaches for improving NLP pipeline Create custom VB scripts in repackaging applications as needed NLP File Prep SettlementPrepare files for review for Settlement Held meetings with client and worked all alone for the entire project with limited help from the client Participated in the complete SDLC process Developed rich user interface using CSS HTML JavaScript and JQuery Created a Python based GUI application For Freight Tracking and processing Used Django framework for application development Developed and maintained various automated web tools for reducing manual effort and increasing efficiency of the Global Shipping Team Created database using MySQL wrote several queries to extract data from database Setup automated cron jobs to upload data into database generate graphs bar charts upload these charts to wiki and backup the database Wrote scripts in Python for extracting data from HTML file Effectively communicated with the external vendors to resolve queries Used Perforce for the version control Environment Python Django 14 MySQL Windows Linux HTML CSS JQuery JavaScript Apache Linux Software Engineer Python United Airlines Chicago IL January 2008 to February 2010 Responsibilities Django Framework was used in developing web applications to implement the MVT architecture Exposure on MultiThreading factory to distribute learning process backtesting and into various worker processes Worked on user portal creating forms and adding users Developed dynamic web pages using Python Django Frameworks Used Django Restful API for database access Wrote Python scripts to parse JSON documents and load the data in database Features for dashboard were developed and tested using CSS AngularJS and Bootstrap Developed forms using HTML and performing clientside validations using JavaScript jQuery and Bootstrap Cleansing data generated from weblogs with automated scripts in Python Used PostgreSQL database for storing the information and experience in performing CRUD operations to operate Schema objects to browse functions in python Experience in Performance Tuning Query Optimization ClientServer Connectivity and Database consistency checks Backup and Recovery Involved in Continuous Integration CI and Continuous Delivery CD process implementation using Jenkins along with Shell script Used GIT repository for software configuration management and version control Used JIRA for tracking and Updating Project issue Performed troubleshooting fixed and deployed many Python bug fixes of the two main applications that were Maintained main source of data for both customers and internal customer service team Maintained technical documentation for resolved issues for future reference Coded test programs and evaluated existing engineering process Effectively communicated with the external vendors to resolve queries Performed research to explore and identify new technological platforms Environment Python 352 Django Rest HTML JavaScript JIRA CSS Bootstrap PostgreSQL Jenkins Education Bachelors Skills PYTHON 10 years HTML 9 years JAVASCRIPT 9 years DJANGO 8 years APACHE 8 years Additional Information TECHNICAL PROFICIENCY OS Platforms LinuxUnix Windows98NT MAC OSX Languages Python 2724 Java Shell c Databases MySQL SQL Server 2008 PostgreSQL Web Technologies AJAX AWS EC Cloud Amazon S3 JavaScript HTML XML Versioning Tools Git SVNSTASH Web servers Apache Nginix Tomcat Framework DjangoFlask Other Tools PuttySQl developerPhotoshopJIRA",
    "entities": [
        "CSS HTML JavaScript",
        "NLP File Prep SettlementPrepare",
        "Flashvars",
        "MapReduce Written",
        "ExportedImported",
        "GUI",
        "HDFS",
        "SQL Server Management Studio Maintained",
        "GITLAB Importing",
        "Work Experience Sr Python Developer",
        "Interfacing",
        "Dev Express",
        "LAMP Linux",
        "Updating Project",
        "Hadoop",
        "XML",
        "SOAP",
        "DHTML",
        "Nginx Developed Oozie",
        "CherryPy",
        "MAC",
        "MA",
        "Maintained",
        "IL",
        "Shell",
        "csv",
        "Hive Managed",
        "HBase",
        "SASAccess SAS SQL",
        "JavaScript AWS Linux Shell Scripting AJAX Mongodb Sr Python Developer Deustche bank",
        "Hive SERDE Pig",
        "Amazon",
        "Cursors Indexing",
        "Responsibilities Data",
        "Python C",
        "Python",
        "SQL Server",
        "Generated",
        "Oracle Coherence",
        "Amazon AWS S3",
        "Developed",
        "Data Warehouse",
        "Djangos Test Module Skilled",
        "United Airlines",
        "MATLAB",
        "Foot Locker Inc Resposibilities Participated",
        "Django",
        "MultiThreading",
        "Amazon Cloud Systems",
        "Executed Oozie",
        "Additional Information TECHNICAL PROFICIENCY OS Platforms",
        "PythonPHP",
        "Python Used PostgreSQL",
        "JavaScript Involved",
        "Bootstrap Developed",
        "Created User Defined Functions",
        "MapReduce Jobs Created Hive Partitions",
        "the Reporting Application",
        "PHP Experienced",
        "RDS",
        "MS",
        "the JSON XML Log",
        "Autosys",
        "Webscale and Enterprise Data Centers Mangstors",
        "the Highly Protected",
        "TB Data",
        "PythonPHP Architecture",
        "GIT",
        "HTML CSS",
        "Responsibilities Participate",
        "US",
        "Sybase",
        "Sqoop",
        "Database",
        "HIVE",
        "Performance Tuning Query Optimization ClientServer Connectivity",
        "Pandas API",
        "Created",
        "Warranty",
        "WAMP Windows Apache",
        "Oracle",
        "Coded",
        "Sub Queries Stored Procedures Triggers Cursors and Functions",
        "Performed Data",
        "PIG",
        "Flume Kafka Storm Evaluated",
        "HTML",
        "SAS",
        "SQL",
        "Incremental",
        "NLP",
        "lPythonspan",
        "Responsibilities Worked",
        "Apache Pig Configured",
        "WebApplication Developer",
        "Chicago",
        "Hive",
        "SQOOP",
        "SolidState",
        "MVW",
        "Amazon AWS",
        "Wrote",
        "Handson",
        "Practical",
        "Python Django PHP C XML",
        "Pandas",
        "ETL",
        "CRUD",
        "JQuery Created",
        "Apache Hadoop",
        "Performed",
        "java Added",
        "Developed ETL Extract Transform Load",
        "Google Analytics",
        "CSS",
        "CMS",
        "Backup and Recovery Involved",
        "HIVEQL",
        "the Global Shipping Team Created",
        "Shell Scripting Spreadsheets",
        "TB",
        "Data for Different Companies",
        "REST",
        "Flash Storage Solutions",
        "Python Django Frameworks Used Django Restful API",
        "Tableau",
        "NameNode Data",
        "Different Partitions Environment Hadoop Hive",
        "Value",
        "DevTest",
        "Implemented Apache"
    ],
    "experience": "Experience object oriented programming OOP concepts using Python C and PHP Experienced in WAMP Windows Apache MYSQL and PythonPHP and LAMP Linux Apache MySQL and PythonPHP Architecture Experience in leading multiple efforts to build Hadoop platforms maximizing business value by combining data science with big data Advised organizations about big data a big data strategy the implementation of big data which technologies best fit the needs of the organization and even implements the selected big data solution Write MATLAB code to create discretized computer models of sloped levy geometries Experienced in developing webbased applications using Python Django PHP C XML CSS HTML DHTML JavaScript and Jquery Experienced in installing configuring modifying testing and deploying applications with Apache Well versed with design and development of presentation layer for web applications using technologies like HTML CSS and JavaScript Familiar with JSON based REST Web services and Amazon Web services Experienced in developing Web Services with Python programming language Experience in writing Sub Queries Stored Procedures Triggers Cursors and Functions on MySQL and PostgreSQL database Experienced in agile and waterfall methodologies with high quality deliverables delivered ontime Experience in utilizing SAS Procedures Macros and other SAS application for data extraction data cleansing data loading and reporting Maintained detailed documentation and architectural solutions in IT infrastructure and sales systems Very strong full life cycle application development experience Strong knowledge on Dev Express Controls Strong database design and programming skills in SQL Server 05 SQL Stored Procedures functions triggers Cursors Indexing importingexporting data from varied data sources Experience with continuous integration and automation using Jenkins Experience with Unit testing Test driven Development TDD Load Testing Have the ability to understand complex systems and be in command of the details to provide solutions Ability to learn and adapt quickly to the emerging new technologies and paradigms Excellent communication interpersonal and analytical skills and a highly motivated team player with the ability to work independently Practical experience with working on multiple environments like development testing production Handson experience in writing and reviewing requirements architecture documents test plans design documents quality analysis and audits Excellent analytical and problem solving skills and ability to work on own besides being a valuable and contributing team player Authorized to work in the US for any employer Work Experience Sr Python Developer EASTBAYFOOTLOCKER Wausau WI July 2017 to Present with Hadoop Eastbay is a supplier of athletic footwear apparel and sports equipment which sells by direct mail Since 1997 it has been the directtomail division of Foot Locker Inc Resposibilities Participated to develop a data platform from scratch and took part in requirement gathering and analysis phase of the project in documenting the business requirements Worked with team of Hadoop developers on maintaining the data platform applications for RISK management Worked in designing tables in Hive MYSQL using SQOOP and processing data like importing and exporting of databases to the HDFS Experienced in processing large datasets of different forms including structured semistructured and unstructured data Developed rest APIs using python with flask and django framework Experienced the integration of various data sources including Java JDBC RDBMS Shell Scripting Spreadsheets and Text files Exposure to various markup languages including XML JSON CSV Good Understanding of Hadoop architecture and the daemons of Hadoop including NameNode Data Node Job Tracker Task Tracker Resource Manager Hands on experience in ingesting data into Data Warehouse using various data loading techniques Developed python scripts to load data to hive from HDFS Participated in developing ETL components for executing various workflows Developed pig scripts and hive scripts for processing the data Handled the JSON XML Log data using Hive SERDE Pig and filter the data based on query factor Worked in agile methodology with 2 weeks sprints Scheduled Jobs using crontab rundeck and controlm Performed Branching Tagging Release Activities on Version Control Tools GIT and GITLAB Importing and exporting data jobs to perform operations like copying data from HDFS and to HDFS using Sqoop Developed Spark code and SparkSQLStreaming for faster testing and processing of data Data was Ingested which is received from various database providers using Sqoop onto HDFS for analysis and data processing Wrote and Implemented Apache PIG scripts to load data from and to store data into Hive Managed the imported data form different data sources performed transformation using Hive Pig and Map Reduce and loaded data in HDFS Executed Oozie workflow engine to run multiple Hive and Pig jobs which run independently with time and data availability To achieve Continuous Delivery goal on high scalable environment used Docker coupled with loadbalancing tool Nginx Developed Oozie workflow to run job onto data availability of transactions Created User Defined Functions UDFs for maintaining Incremental IDs Used Shell scripting to analyse the data from SQL Server source and processed it to store into HDFS Had good exposure to spark Mlib Streaming and sql by closely working with data scientists Generated reports from Hive data using Microstrategy Worked with complex sql queries to make joins Increased the time efficiency of the HIVEQL and reduced the time difference of executing the sets of data by applying the compression techniques for MapReduce Jobs Created Hive Partitions for storing Data for Different Companies under Different Partitions Environment Hadoop Hive sqoop pig Python 27 java Django 14 Flask XML MySQL MS SQL Server Linux Shell Scripting mongodb SQL Sr Python Developer FM Global January 2015 to June 2017 This is a mutual insurance company with offices worldwide that specializes in loss prevention services primarily to large corporations throughout the world in the Highly Protected RiskHPR property insurance market sector Responsibilities Participate in requirement gathering and analysis phase of the project in documenting the business requirements by conducting workshopsmeetings with various business users Worked with team of developers on Python applications for RISK management Developed PythonDjango application for Google Analytics aggregation and reporting Used Django configuration to manage URLs and application parameters Worked on Python Open stack APIs Used Python scripts to update content in the database and manipulate files Generated Python Django Forms to record data of online users Detailed Understanding on existing build system Tools related for information of various products and releases and test results information Designed and implemented map reduce jobs to support distributed processing using java Hive and Apache Pig Configured ec2 instances and configured IAM users and roles Created s3 data pipe using Boto API to load data from internal data sources Configured Jboss cluster and mysql database for application access Developed UDFs to provide custom hive and pig capabilities Built a mechanism for automatically moving the existing proprietary binary format data files to HDFS using a service called Ingestion service Comprehensive knowledge and experience in process improvement normalizationdenormalization data extraction data cleansing data manipulation Performed Data transformations in HIVE and used partitions buckets for performance improvements Ingestion of data into Hadoop using Sqoop and apply data transformations and using Pig and HIVE Used Python and Django creating graphics XML processing data exchange and business logic implementation Developed PlSql store procedures to convert the data from Oracle to MongoDB I have used Pandas API to put the data as time series and tabular format for east timestamp data manipulation and retrieval Automate report generation in MongoDB using Javascript shell scripting sed java Added support for Amazon AWS S3 and RDS to host staticmedia files and the database into Amazon Cloud Systems automation utilizing ControlM for scheduling and PowershellC for script development Used Pandas library for statistical Analysis Developed tools using Python Shell scripting XML to automate some of the menial tasks Interfacing with supervisors artists systems administrators and production to ensure production deadlines are met Worked very closely with designer tightly integrating Flash into the CMS with the use of Flashvars stored in the Django models Also created XML with Django to be used by the Flash Used HTML CSS JQuery JSON and Javascript for front end applications Designed and developed the UI of the website using HTML XHTML AJAX CSS and JavaScript Also used Bootstrap as a mechanism to manage and organize the html page layout Used Django configuration to manage URLs and application parameters Wrote and executed various MYSQL database queries from Python using PythonMySQL connector and MySQLdb package Involved in development of Web Services using SOAP for sending and getting data from the external interface in the XML format Worked on development of SQL and stored procedures on MYSQL Responsible for debugging the project monitored on JIRA Agile Performed troubleshooting fixed and deployed many Python bug fixes of the two main applications that were a main source of data for both customers and internal customer service team Environment Python hive oozie Amazon AWS S3 MySQL HTML Python 27 Django 14 HTML5 CSS XML MySQL MS SQL Server JavaScript AWS Linux Shell Scripting AJAX Mongodb Sr Python Developer Deustche bank NJ December 2013 to November 2014 Deustche bank is a German global banking and financial services company offering financial products and services for corporate and institutional clients along with private and business clients Services include sales trading research and origination of debt and equity mergers and acquisitions MA risk management products such as derivatives corporate finance wealth management retail banking fund management and transaction banking Responsibilities Worked with team of developers on Python applications for RISK management Designed the database schema for the content management system Designed and developed the UI of the website using HTML XHTML AJAX CSS and JavaScript Involved in development of Web Services using SOAP for sending and getting data from the external interface in the XML format Wrote Python routines to log into the websites and fetch data for selected options Performed testing using Djangos Test Module Skilled experience in installing configuring and using Apache Hadoop ecosystems such as Pig and Spark Built the entire Hadoop platform from scratch Experience in ingesting real timenear real time data using Flume Kafka Storm Evaluated suitability of Hadoop and its ecosystem to the above project and implementing validating with various proof of concept POC applications to eventually adopt them to benefit from the Big Data Hadoop initiative Estimated the Software Hardware requirements for the Name Node and Data Node in the cluster Extracted the needed data from the server into HDFS and Bulk Loaded the cleaned data into HBase using MapReduce Written the Map Reduce programs Hive UDFs in Java Develop HIVE queries for the analysts Created an email notification service upon completion of job for the particular team which requested for the data Defined job work flows as per their dependencies in Oozie Closely observed building the Reporting Application which uses the Spark SQL to fetch and generate reports on table data Knowledge in performance troubleshooting and tuning Hadoop clusters in Cloudera Worked on middle tier and persistence layer Created service and model layer classes and Value objectsPOJO to hold values between java classes and database fields ExportedImported data between different data sources using SQL Server Management Studio Maintained program libraries users manuals and technical documentation Responsible for debugging and troubleshooting the web application Successfully migrated all the data to the database while the site was in production Implemented the validation error handling and caching framework with Oracle Coherence cache Worked on scripts for setting up the discovery client with attribute data Worked on scriptsgranite reference data scripts for setting up adapter attributes in granite system Environment Python 27 Hadoop Django 14 HTML5 CSS XML MySQL JavaScript JQuery Mongo DB MS SQL Server JavaScript GitHub AWS Linux Shell Scripting AJAX Sr Python Developer Mangstor Austin TX January 2012 to November 2013 Mangstor is a leading developer of NVMe Flash Storage Solutions to accelerate application workloads in Webscale and Enterprise Data Centers Mangstors NVMe over Fabric Flash Storage Arrays and NVMe SolidState devices provide the building blocks necessary for highperformance highlyavailable and costeffective storage infrastructure Responsibilities Data warehouse migration Sybase 125 to Sybase 157 More than 70 of work is developing code in Python remaining time spent on database development and data modelling Worked on requirement gathering high level design implementation testing and deployment of code Creating and following the production deployment runbook Did Proof of Concept on DB2 BLUcolumn organized database Task prioritization with clients Designed and developed reusable Autosys jobs parsing and documentation software in Python using object oriented features being implemented in other projects in firm Designed and developed database object parsing dependency builder and documenting software in Python using object oriented features Developed ETL Extract Transform Load software for DB2 columnar database fact and dimension tables Massive data processing Sybase 15 TB and Db2 2 TB Data modelling in Sybase and DB2 Setting up schema users permissions creating database objects Database performance tuning procedures table functionsdb2 Provide L2L3 support on rotational basis Database objects refactoring removing legacy duplication Built pluggable software for housekeeping and cleanup being used by other projects in firm Data load analysis package to reveal errors during load based on historic trend Data reconciliation program for Sybase vs Db2 and Db2 vs Db2 prod vs qa in Python Other tools developed in Python to automate daily activities in python like monitoring DB Worked with shell scripts to build wrapper around ETL to do one time historic load Environment Python Java MySQL Linux HTML XHTML CSS AJAX JavaScript Apache Web Server Python Developer Bosch Designs Novi MI March 2010 to December 2011 Bosch designs and produces precision automotive components and systems sold to vehicle and powertrain manufacturers worldwide These include systems and components for gasoline and diesel injection airbag components ABS and conventional braking systems telematics as well as small motors electrical and electronic equipment Worked on development of Warranty and defects tracking system Responsibilities Wrote Python routines to log into the websites and fetch data for selected options Used Python modules such as requests urllib urllib2 for web crawling Used other packages such as Beautifulsoup for data parsing Worked on writing and as well as read data from csv and excel file formats Developed a MATLAB algorithm which determines an objects dimensions from digital images Webservices backend development using Python CherryPy Django SQLAlchemy Participated in developing the companys internal framework on Python This framework became a basement for the quick services development Framework based on CherryPy with GnuPg encryption reGnuPg module on the top Worked on resulting reports of the application and Tableau reports Worked on HTML5 CSS3 JavaScript AngularJS NodeJS Git REST API Mongo DB intelliJ IDEA Design and Setting up of environment of Mongodb with shards and replicasets DevTest and Production Private VPN using Ubuntu Python Django CherryPy Postgres Redis Bootstrap Jquery Mongo Fabric Git Tenjin Selenium Sphinx Nose Modifying data using SASBASE SAS MACROS Extracting data from the database using SASAccess SAS SQL procedures and create SAS data sets Performed QA testing on the application Developed approaches for improving NLP pipeline Create custom VB scripts in repackaging applications as needed NLP File Prep SettlementPrepare files for review for Settlement Held meetings with client and worked all alone for the entire project with limited help from the client Participated in the complete SDLC process Developed rich user interface using CSS HTML JavaScript and JQuery Created a Python based GUI application For Freight Tracking and processing Used Django framework for application development Developed and maintained various automated web tools for reducing manual effort and increasing efficiency of the Global Shipping Team Created database using MySQL wrote several queries to extract data from database Setup automated cron jobs to upload data into database generate graphs bar charts upload these charts to wiki and backup the database Wrote scripts in Python for extracting data from HTML file Effectively communicated with the external vendors to resolve queries Used Perforce for the version control Environment Python Django 14 MySQL Windows Linux HTML CSS JQuery JavaScript Apache Linux Software Engineer Python United Airlines Chicago IL January 2008 to February 2010 Responsibilities Django Framework was used in developing web applications to implement the MVT architecture Exposure on MultiThreading factory to distribute learning process backtesting and into various worker processes Worked on user portal creating forms and adding users Developed dynamic web pages using Python Django Frameworks Used Django Restful API for database access Wrote Python scripts to parse JSON documents and load the data in database Features for dashboard were developed and tested using CSS AngularJS and Bootstrap Developed forms using HTML and performing clientside validations using JavaScript jQuery and Bootstrap Cleansing data generated from weblogs with automated scripts in Python Used PostgreSQL database for storing the information and experience in performing CRUD operations to operate Schema objects to browse functions in python Experience in Performance Tuning Query Optimization ClientServer Connectivity and Database consistency checks Backup and Recovery Involved in Continuous Integration CI and Continuous Delivery CD process implementation using Jenkins along with Shell script Used GIT repository for software configuration management and version control Used JIRA for tracking and Updating Project issue Performed troubleshooting fixed and deployed many Python bug fixes of the two main applications that were Maintained main source of data for both customers and internal customer service team Maintained technical documentation for resolved issues for future reference Coded test programs and evaluated existing engineering process Effectively communicated with the external vendors to resolve queries Performed research to explore and identify new technological platforms Environment Python 352 Django Rest HTML JavaScript JIRA CSS Bootstrap PostgreSQL Jenkins Education Bachelors Skills PYTHON 10 years HTML 9 years JAVASCRIPT 9 years DJANGO 8 years APACHE 8 years Additional Information TECHNICAL PROFICIENCY OS Platforms LinuxUnix Windows98NT MAC OSX Languages Python 2724 Java Shell c Databases MySQL SQL Server 2008 PostgreSQL Web Technologies AJAX AWS EC Cloud Amazon S3 JavaScript HTML XML Versioning Tools Git SVNSTASH Web servers Apache Nginix Tomcat Framework DjangoFlask Other Tools PuttySQl developerPhotoshopJIRA",
    "extracted_keywords": [
        "Sr",
        "Python",
        "Developer",
        "Sr",
        "lPythonspan",
        "span",
        "lDeveloperspan",
        "Sr",
        "Python",
        "Developer",
        "EASTBAYFOOTLOCKER",
        "Atlanta",
        "GA",
        "years",
        "experience",
        "WebApplication",
        "Developer",
        "programming",
        "Python",
        "Java",
        "software",
        "development",
        "lifecycle",
        "platforms",
        "programming",
        "database",
        "design",
        "methodologies",
        "MVW",
        "frameworks",
        "Django",
        "Angularjs",
        "Java",
        "Script",
        "JQuery",
        "Nodejs",
        "Expert",
        "knowledge",
        "Object",
        "Design",
        "Programming",
        "concepts",
        "Experience",
        "object",
        "programming",
        "OOP",
        "concepts",
        "Python",
        "C",
        "PHP",
        "WAMP",
        "Windows",
        "Apache",
        "MYSQL",
        "PythonPHP",
        "LAMP",
        "Linux",
        "Apache",
        "MySQL",
        "PythonPHP",
        "Architecture",
        "Experience",
        "efforts",
        "Hadoop",
        "platforms",
        "business",
        "value",
        "data",
        "science",
        "data",
        "organizations",
        "data",
        "data",
        "strategy",
        "implementation",
        "data",
        "technologies",
        "needs",
        "organization",
        "data",
        "solution",
        "Write",
        "MATLAB",
        "code",
        "computer",
        "models",
        "levy",
        "geometries",
        "applications",
        "Python",
        "Django",
        "PHP",
        "C",
        "XML",
        "CSS",
        "HTML",
        "DHTML",
        "JavaScript",
        "Jquery",
        "testing",
        "applications",
        "Apache",
        "design",
        "development",
        "presentation",
        "layer",
        "web",
        "applications",
        "technologies",
        "HTML",
        "CSS",
        "JavaScript",
        "Familiar",
        "JSON",
        "REST",
        "Web",
        "services",
        "Amazon",
        "Web",
        "services",
        "Web",
        "Services",
        "Python",
        "programming",
        "language",
        "Experience",
        "Sub",
        "Queries",
        "Stored",
        "Procedures",
        "Triggers",
        "Cursors",
        "Functions",
        "MySQL",
        "PostgreSQL",
        "database",
        "methodologies",
        "quality",
        "deliverables",
        "ontime",
        "Experience",
        "SAS",
        "Procedures",
        "Macros",
        "SAS",
        "application",
        "data",
        "extraction",
        "data",
        "data",
        "loading",
        "documentation",
        "solutions",
        "IT",
        "infrastructure",
        "sales",
        "systems",
        "life",
        "cycle",
        "application",
        "development",
        "experience",
        "knowledge",
        "Dev",
        "Express",
        "Strong",
        "database",
        "design",
        "programming",
        "skills",
        "SQL",
        "Server",
        "SQL",
        "Stored",
        "Procedures",
        "functions",
        "Cursors",
        "Indexing",
        "data",
        "data",
        "sources",
        "integration",
        "automation",
        "Jenkins",
        "Experience",
        "Unit",
        "testing",
        "Test",
        "Development",
        "TDD",
        "Load",
        "Testing",
        "ability",
        "systems",
        "command",
        "details",
        "solutions",
        "Ability",
        "technologies",
        "communication",
        "skills",
        "team",
        "player",
        "ability",
        "experience",
        "environments",
        "development",
        "testing",
        "production",
        "Handson",
        "experience",
        "reviewing",
        "requirements",
        "architecture",
        "documents",
        "test",
        "design",
        "documents",
        "quality",
        "analysis",
        "audits",
        "Excellent",
        "analytical",
        "problem",
        "skills",
        "ability",
        "team",
        "player",
        "US",
        "employer",
        "Work",
        "Experience",
        "Sr",
        "Python",
        "Developer",
        "EASTBAYFOOTLOCKER",
        "Wausau",
        "WI",
        "July",
        "Present",
        "Hadoop",
        "Eastbay",
        "supplier",
        "footwear",
        "apparel",
        "sports",
        "equipment",
        "mail",
        "directtomail",
        "division",
        "Foot",
        "Locker",
        "Inc",
        "Resposibilities",
        "data",
        "platform",
        "scratch",
        "part",
        "requirement",
        "gathering",
        "analysis",
        "phase",
        "project",
        "business",
        "requirements",
        "team",
        "Hadoop",
        "developers",
        "data",
        "platform",
        "applications",
        "RISK",
        "management",
        "tables",
        "Hive",
        "MYSQL",
        "SQOOP",
        "processing",
        "data",
        "exporting",
        "databases",
        "HDFS",
        "datasets",
        "forms",
        "data",
        "rest",
        "APIs",
        "python",
        "flask",
        "django",
        "framework",
        "integration",
        "data",
        "sources",
        "Java",
        "JDBC",
        "RDBMS",
        "Shell",
        "Scripting",
        "Spreadsheets",
        "Text",
        "Exposure",
        "languages",
        "XML",
        "CSV",
        "Understanding",
        "Hadoop",
        "architecture",
        "daemons",
        "Hadoop",
        "NameNode",
        "Data",
        "Node",
        "Job",
        "Tracker",
        "Task",
        "Tracker",
        "Resource",
        "Manager",
        "Hands",
        "experience",
        "data",
        "Data",
        "Warehouse",
        "data",
        "loading",
        "techniques",
        "python",
        "scripts",
        "data",
        "HDFS",
        "Participated",
        "ETL",
        "components",
        "workflows",
        "pig",
        "scripts",
        "hive",
        "scripts",
        "data",
        "JSON",
        "XML",
        "Log",
        "data",
        "Hive",
        "SERDE",
        "Pig",
        "data",
        "query",
        "factor",
        "methodology",
        "weeks",
        "sprints",
        "Jobs",
        "crontab",
        "rundeck",
        "Performed",
        "Tagging",
        "Release",
        "Activities",
        "Version",
        "Control",
        "Tools",
        "GIT",
        "GITLAB",
        "Importing",
        "data",
        "jobs",
        "operations",
        "data",
        "HDFS",
        "HDFS",
        "Sqoop",
        "Developed",
        "Spark",
        "code",
        "SparkSQLStreaming",
        "testing",
        "processing",
        "data",
        "Data",
        "database",
        "providers",
        "Sqoop",
        "HDFS",
        "analysis",
        "data",
        "processing",
        "Wrote",
        "Apache",
        "PIG",
        "scripts",
        "data",
        "data",
        "Hive",
        "Managed",
        "data",
        "data",
        "sources",
        "transformation",
        "Hive",
        "Pig",
        "Map",
        "Reduce",
        "data",
        "Oozie",
        "workflow",
        "engine",
        "Hive",
        "Pig",
        "jobs",
        "time",
        "data",
        "availability",
        "Continuous",
        "Delivery",
        "goal",
        "environment",
        "Docker",
        "loadbalancing",
        "tool",
        "Nginx",
        "Developed",
        "Oozie",
        "workflow",
        "job",
        "data",
        "availability",
        "transactions",
        "User",
        "Defined",
        "Functions",
        "UDFs",
        "IDs",
        "Shell",
        "data",
        "SQL",
        "Server",
        "source",
        "HDFS",
        "exposure",
        "Mlib",
        "Streaming",
        "sql",
        "data",
        "scientists",
        "reports",
        "Hive",
        "data",
        "Microstrategy",
        "Worked",
        "queries",
        "joins",
        "time",
        "efficiency",
        "HIVEQL",
        "time",
        "difference",
        "sets",
        "data",
        "compression",
        "techniques",
        "MapReduce",
        "Jobs",
        "Hive",
        "Partitions",
        "Data",
        "Companies",
        "Different",
        "Partitions",
        "Environment",
        "Hadoop",
        "Hive",
        "sqoop",
        "pig",
        "Python",
        "Django",
        "Flask",
        "XML",
        "MySQL",
        "MS",
        "SQL",
        "Server",
        "Linux",
        "Shell",
        "Scripting",
        "mongodb",
        "SQL",
        "Sr",
        "Python",
        "Developer",
        "FM",
        "Global",
        "January",
        "June",
        "insurance",
        "company",
        "offices",
        "loss",
        "prevention",
        "services",
        "corporations",
        "world",
        "property",
        "insurance",
        "market",
        "sector",
        "Responsibilities",
        "requirement",
        "gathering",
        "analysis",
        "phase",
        "project",
        "business",
        "requirements",
        "workshopsmeetings",
        "business",
        "users",
        "team",
        "developers",
        "Python",
        "applications",
        "RISK",
        "management",
        "PythonDjango",
        "application",
        "Google",
        "Analytics",
        "aggregation",
        "Used",
        "Django",
        "configuration",
        "URLs",
        "application",
        "parameters",
        "Python",
        "stack",
        "APIs",
        "Python",
        "scripts",
        "content",
        "database",
        "manipulate",
        "files",
        "Python",
        "Django",
        "Forms",
        "data",
        "users",
        "Understanding",
        "build",
        "system",
        "Tools",
        "information",
        "products",
        "releases",
        "test",
        "results",
        "information",
        "map",
        "jobs",
        "processing",
        "Hive",
        "Apache",
        "Pig",
        "ec2",
        "instances",
        "IAM",
        "users",
        "roles",
        "Created",
        "s3",
        "data",
        "pipe",
        "Boto",
        "API",
        "data",
        "data",
        "sources",
        "Configured",
        "Jboss",
        "cluster",
        "mysql",
        "database",
        "application",
        "access",
        "UDFs",
        "custom",
        "hive",
        "pig",
        "capabilities",
        "mechanism",
        "format",
        "data",
        "files",
        "HDFS",
        "service",
        "Ingestion",
        "service",
        "Comprehensive",
        "knowledge",
        "experience",
        "process",
        "improvement",
        "normalizationdenormalization",
        "data",
        "extraction",
        "data",
        "cleansing",
        "data",
        "manipulation",
        "Performed",
        "Data",
        "transformations",
        "HIVE",
        "partitions",
        "buckets",
        "performance",
        "improvements",
        "Ingestion",
        "data",
        "Hadoop",
        "Sqoop",
        "data",
        "transformations",
        "Pig",
        "HIVE",
        "Python",
        "Django",
        "graphics",
        "XML",
        "processing",
        "data",
        "exchange",
        "business",
        "logic",
        "implementation",
        "PlSql",
        "store",
        "procedures",
        "data",
        "Oracle",
        "Pandas",
        "API",
        "data",
        "time",
        "series",
        "format",
        "east",
        "timestamp",
        "data",
        "manipulation",
        "retrieval",
        "Automate",
        "report",
        "generation",
        "MongoDB",
        "Javascript",
        "shell",
        "scripting",
        "support",
        "Amazon",
        "AWS",
        "S3",
        "RDS",
        "files",
        "database",
        "Amazon",
        "Cloud",
        "Systems",
        "automation",
        "ControlM",
        "scheduling",
        "PowershellC",
        "script",
        "development",
        "Pandas",
        "library",
        "Analysis",
        "tools",
        "Python",
        "Shell",
        "XML",
        "tasks",
        "supervisors",
        "artists",
        "systems",
        "administrators",
        "production",
        "production",
        "deadlines",
        "Worked",
        "designer",
        "Flash",
        "CMS",
        "use",
        "Flashvars",
        "Django",
        "models",
        "XML",
        "Django",
        "Flash",
        "HTML",
        "CSS",
        "JQuery",
        "JSON",
        "Javascript",
        "end",
        "applications",
        "UI",
        "website",
        "HTML",
        "XHTML",
        "AJAX",
        "CSS",
        "JavaScript",
        "Bootstrap",
        "mechanism",
        "html",
        "page",
        "layout",
        "Django",
        "configuration",
        "URLs",
        "application",
        "parameters",
        "Wrote",
        "MYSQL",
        "database",
        "Python",
        "PythonMySQL",
        "connector",
        "package",
        "development",
        "Web",
        "Services",
        "SOAP",
        "data",
        "interface",
        "XML",
        "format",
        "development",
        "SQL",
        "procedures",
        "MYSQL",
        "project",
        "JIRA",
        "Agile",
        "Performed",
        "troubleshooting",
        "Python",
        "bug",
        "fixes",
        "applications",
        "source",
        "data",
        "customers",
        "customer",
        "service",
        "team",
        "Environment",
        "Python",
        "hive",
        "oozie",
        "Amazon",
        "AWS",
        "S3",
        "MySQL",
        "HTML",
        "Python",
        "Django",
        "HTML5",
        "CSS",
        "XML",
        "MySQL",
        "MS",
        "SQL",
        "Server",
        "JavaScript",
        "Linux",
        "Shell",
        "Scripting",
        "AJAX",
        "Mongodb",
        "Sr",
        "Python",
        "Developer",
        "Deustche",
        "bank",
        "NJ",
        "December",
        "November",
        "Deustche",
        "bank",
        "banking",
        "services",
        "company",
        "products",
        "services",
        "clients",
        "business",
        "clients",
        "Services",
        "sales",
        "trading",
        "research",
        "origination",
        "debt",
        "equity",
        "mergers",
        "acquisitions",
        "MA",
        "risk",
        "management",
        "products",
        "derivatives",
        "finance",
        "wealth",
        "management",
        "banking",
        "fund",
        "management",
        "transaction",
        "banking",
        "Responsibilities",
        "team",
        "developers",
        "Python",
        "applications",
        "RISK",
        "management",
        "database",
        "schema",
        "content",
        "management",
        "system",
        "UI",
        "website",
        "HTML",
        "XHTML",
        "AJAX",
        "CSS",
        "JavaScript",
        "development",
        "Web",
        "Services",
        "SOAP",
        "data",
        "interface",
        "XML",
        "format",
        "Wrote",
        "Python",
        "websites",
        "data",
        "options",
        "Performed",
        "testing",
        "Djangos",
        "Test",
        "Module",
        "experience",
        "configuring",
        "Apache",
        "Hadoop",
        "ecosystems",
        "Pig",
        "Spark",
        "Hadoop",
        "platform",
        "scratch",
        "Experience",
        "time",
        "data",
        "Flume",
        "Kafka",
        "Storm",
        "suitability",
        "Hadoop",
        "ecosystem",
        "project",
        "proof",
        "concept",
        "POC",
        "applications",
        "Big",
        "Data",
        "Hadoop",
        "initiative",
        "Software",
        "Hardware",
        "requirements",
        "Name",
        "Node",
        "Data",
        "Node",
        "cluster",
        "data",
        "server",
        "HDFS",
        "Bulk",
        "Loaded",
        "data",
        "HBase",
        "MapReduce",
        "Written",
        "Map",
        "Reduce",
        "programs",
        "Hive",
        "UDFs",
        "Java",
        "Develop",
        "HIVE",
        "analysts",
        "email",
        "notification",
        "service",
        "completion",
        "job",
        "team",
        "data",
        "job",
        "work",
        "dependencies",
        "Oozie",
        "Reporting",
        "Application",
        "Spark",
        "SQL",
        "reports",
        "table",
        "data",
        "Knowledge",
        "performance",
        "Hadoop",
        "clusters",
        "Cloudera",
        "tier",
        "persistence",
        "layer",
        "service",
        "model",
        "layer",
        "classes",
        "Value",
        "values",
        "classes",
        "database",
        "ExportedImported",
        "data",
        "data",
        "sources",
        "SQL",
        "Server",
        "Management",
        "Studio",
        "program",
        "users",
        "manuals",
        "documentation",
        "web",
        "application",
        "data",
        "database",
        "site",
        "production",
        "validation",
        "error",
        "handling",
        "framework",
        "Oracle",
        "Coherence",
        "cache",
        "scripts",
        "discovery",
        "client",
        "attribute",
        "data",
        "scriptsgranite",
        "reference",
        "data",
        "scripts",
        "adapter",
        "attributes",
        "granite",
        "system",
        "Environment",
        "Python",
        "Hadoop",
        "Django",
        "HTML5",
        "CSS",
        "XML",
        "MySQL",
        "JavaScript",
        "JQuery",
        "Mongo",
        "DB",
        "MS",
        "SQL",
        "Server",
        "JavaScript",
        "GitHub",
        "Linux",
        "Shell",
        "Scripting",
        "AJAX",
        "Sr",
        "Python",
        "Developer",
        "Mangstor",
        "Austin",
        "TX",
        "January",
        "November",
        "Mangstor",
        "developer",
        "NVMe",
        "Flash",
        "Storage",
        "Solutions",
        "application",
        "workloads",
        "Webscale",
        "Enterprise",
        "Data",
        "Centers",
        "Mangstors",
        "NVMe",
        "Fabric",
        "Flash",
        "Storage",
        "Arrays",
        "NVMe",
        "SolidState",
        "devices",
        "building",
        "blocks",
        "highperformance",
        "storage",
        "infrastructure",
        "Responsibilities",
        "Data",
        "warehouse",
        "migration",
        "Sybase",
        "Sybase",
        "work",
        "code",
        "Python",
        "time",
        "database",
        "development",
        "data",
        "requirement",
        "level",
        "design",
        "implementation",
        "testing",
        "deployment",
        "code",
        "Creating",
        "production",
        "deployment",
        "runbook",
        "Proof",
        "Concept",
        "DB2",
        "BLUcolumn",
        "database",
        "Task",
        "prioritization",
        "clients",
        "Autosys",
        "jobs",
        "documentation",
        "software",
        "Python",
        "object",
        "features",
        "projects",
        "firm",
        "database",
        "object",
        "dependency",
        "builder",
        "documenting",
        "software",
        "Python",
        "object",
        "features",
        "ETL",
        "Extract",
        "Transform",
        "Load",
        "software",
        "DB2",
        "columnar",
        "database",
        "fact",
        "dimension",
        "data",
        "Sybase",
        "TB",
        "TB",
        "Data",
        "modelling",
        "Sybase",
        "DB2",
        "schema",
        "users",
        "permissions",
        "database",
        "Database",
        "performance",
        "procedures",
        "table",
        "functionsdb2",
        "L2L3",
        "support",
        "basis",
        "Database",
        "refactoring",
        "duplication",
        "software",
        "housekeeping",
        "cleanup",
        "projects",
        "Data",
        "load",
        "analysis",
        "package",
        "errors",
        "load",
        "trend",
        "Data",
        "reconciliation",
        "program",
        "Sybase",
        "prod",
        "qa",
        "Python",
        "tools",
        "Python",
        "activities",
        "python",
        "DB",
        "Worked",
        "scripts",
        "wrapper",
        "ETL",
        "time",
        "load",
        "Environment",
        "Python",
        "Java",
        "MySQL",
        "Linux",
        "HTML",
        "XHTML",
        "CSS",
        "AJAX",
        "JavaScript",
        "Apache",
        "Web",
        "Server",
        "Python",
        "Developer",
        "Bosch",
        "Novi",
        "MI",
        "March",
        "December",
        "Bosch",
        "precision",
        "components",
        "systems",
        "vehicle",
        "manufacturers",
        "systems",
        "components",
        "gasoline",
        "diesel",
        "injection",
        "airbag",
        "components",
        "ABS",
        "braking",
        "systems",
        "telematics",
        "motors",
        "equipment",
        "development",
        "Warranty",
        "tracking",
        "system",
        "Responsibilities",
        "Wrote",
        "Python",
        "websites",
        "data",
        "options",
        "Python",
        "modules",
        "requests",
        "urllib2",
        "web",
        "packages",
        "Beautifulsoup",
        "data",
        "writing",
        "data",
        "csv",
        "file",
        "formats",
        "MATLAB",
        "algorithm",
        "objects",
        "dimensions",
        "images",
        "Webservices",
        "development",
        "Python",
        "CherryPy",
        "Django",
        "SQLAlchemy",
        "companys",
        "framework",
        "Python",
        "framework",
        "basement",
        "services",
        "development",
        "Framework",
        "CherryPy",
        "GnuPg",
        "encryption",
        "reGnuPg",
        "module",
        "top",
        "reports",
        "application",
        "Tableau",
        "HTML5",
        "CSS3",
        "JavaScript",
        "NodeJS",
        "Git",
        "REST",
        "API",
        "Mongo",
        "DB",
        "intelliJ",
        "IDEA",
        "Design",
        "environment",
        "Mongodb",
        "shards",
        "DevTest",
        "Production",
        "VPN",
        "Ubuntu",
        "Python",
        "Django",
        "CherryPy",
        "Postgres",
        "Redis",
        "Bootstrap",
        "Jquery",
        "Mongo",
        "Fabric",
        "Git",
        "Tenjin",
        "Selenium",
        "Sphinx",
        "Nose",
        "Modifying",
        "data",
        "SASBASE",
        "SAS",
        "MACROS",
        "data",
        "database",
        "SASAccess",
        "SAS",
        "SQL",
        "procedures",
        "SAS",
        "data",
        "Performed",
        "QA",
        "testing",
        "application",
        "approaches",
        "NLP",
        "pipeline",
        "custom",
        "VB",
        "scripts",
        "applications",
        "NLP",
        "File",
        "Prep",
        "SettlementPrepare",
        "review",
        "Settlement",
        "meetings",
        "client",
        "project",
        "help",
        "client",
        "SDLC",
        "process",
        "user",
        "interface",
        "CSS",
        "HTML",
        "JavaScript",
        "JQuery",
        "Python",
        "GUI",
        "application",
        "Freight",
        "Tracking",
        "processing",
        "Django",
        "framework",
        "application",
        "development",
        "web",
        "tools",
        "effort",
        "efficiency",
        "Global",
        "Shipping",
        "Team",
        "database",
        "MySQL",
        "queries",
        "data",
        "database",
        "Setup",
        "cron",
        "jobs",
        "data",
        "database",
        "graphs",
        "bar",
        "charts",
        "charts",
        "wiki",
        "database",
        "scripts",
        "Python",
        "data",
        "HTML",
        "file",
        "vendors",
        "queries",
        "Perforce",
        "version",
        "control",
        "Environment",
        "Python",
        "Django",
        "MySQL",
        "Windows",
        "Linux",
        "HTML",
        "CSS",
        "JQuery",
        "JavaScript",
        "Apache",
        "Linux",
        "Software",
        "Engineer",
        "Python",
        "United",
        "Airlines",
        "Chicago",
        "IL",
        "January",
        "February",
        "Responsibilities",
        "Django",
        "Framework",
        "web",
        "applications",
        "MVT",
        "architecture",
        "Exposure",
        "MultiThreading",
        "factory",
        "learning",
        "process",
        "worker",
        "processes",
        "user",
        "forms",
        "users",
        "web",
        "pages",
        "Python",
        "Django",
        "Frameworks",
        "Django",
        "API",
        "database",
        "access",
        "Wrote",
        "Python",
        "scripts",
        "documents",
        "data",
        "database",
        "Features",
        "dashboard",
        "CSS",
        "AngularJS",
        "Bootstrap",
        "forms",
        "HTML",
        "validations",
        "JavaScript",
        "jQuery",
        "Bootstrap",
        "Cleansing",
        "data",
        "weblogs",
        "scripts",
        "Python",
        "PostgreSQL",
        "database",
        "information",
        "experience",
        "CRUD",
        "operations",
        "Schema",
        "functions",
        "python",
        "Experience",
        "Performance",
        "Tuning",
        "Query",
        "Optimization",
        "ClientServer",
        "Connectivity",
        "Database",
        "consistency",
        "checks",
        "Backup",
        "Recovery",
        "Continuous",
        "Integration",
        "CI",
        "Continuous",
        "Delivery",
        "CD",
        "process",
        "implementation",
        "Jenkins",
        "Shell",
        "script",
        "GIT",
        "repository",
        "software",
        "configuration",
        "management",
        "version",
        "control",
        "JIRA",
        "tracking",
        "Project",
        "issue",
        "Performed",
        "troubleshooting",
        "Python",
        "bug",
        "fixes",
        "applications",
        "source",
        "data",
        "customers",
        "customer",
        "service",
        "team",
        "documentation",
        "issues",
        "reference",
        "test",
        "programs",
        "engineering",
        "process",
        "vendors",
        "queries",
        "research",
        "platforms",
        "Environment",
        "Python",
        "Django",
        "Rest",
        "HTML",
        "JavaScript",
        "JIRA",
        "CSS",
        "Bootstrap",
        "PostgreSQL",
        "Jenkins",
        "Education",
        "Bachelors",
        "Skills",
        "PYTHON",
        "years",
        "HTML",
        "years",
        "JAVASCRIPT",
        "years",
        "DJANGO",
        "years",
        "APACHE",
        "years",
        "Additional",
        "Information",
        "TECHNICAL",
        "PROFICIENCY",
        "OS",
        "Platforms",
        "LinuxUnix",
        "MAC",
        "OSX",
        "Languages",
        "Python",
        "Java",
        "Shell",
        "MySQL",
        "SQL",
        "Server",
        "PostgreSQL",
        "Web",
        "Technologies",
        "AJAX",
        "EC",
        "Cloud",
        "Amazon",
        "S3",
        "JavaScript",
        "HTML",
        "XML",
        "Versioning",
        "Tools",
        "Git",
        "SVNSTASH",
        "Web",
        "servers",
        "Apache",
        "Nginix",
        "Tomcat",
        "Framework",
        "DjangoFlask",
        "Tools",
        "PuttySQl",
        "developerPhotoshopJIRA"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T22:33:20.847031",
    "resume_data": "Sr Python Developer Sr span lPythonspan span lDeveloperspan Sr Python Developer EASTBAYFOOTLOCKER Atlanta GA 10 years of experience as a WebApplication Developer and coding with analytical programming using Python Java Experienced with full software development lifecycle architecting scalable platforms object oriented programming database design and agile methodologies Experienced in MVW frameworks like Django Angularjs Java Script JQuery and Nodejs Expert knowledge of and experience in Object oriented Design and Programming concepts Experience object oriented programming OOP concepts using Python C and PHP Experienced in WAMP Windows Apache MYSQL and PythonPHP and LAMP Linux Apache MySQL and PythonPHP Architecture Experience in leading multiple efforts to build Hadoop platforms maximizing business value by combining data science with big data Advised organizations about big data a big data strategy the implementation of big data which technologies best fit the needs of the organization and even implements the selected big data solution Write MATLAB code to create discretized computer models of sloped levy geometries Experienced in developing webbased applications using Python Django PHP C XML CSS HTML DHTML JavaScript and Jquery Experienced in installing configuring modifying testing and deploying applications with Apache Well versed with design and development of presentation layer for web applications using technologies like HTML CSS and JavaScript Familiar with JSON based REST Web services and Amazon Web services Experienced in developing Web Services with Python programming language Experience in writing Sub Queries Stored Procedures Triggers Cursors and Functions on MySQL and PostgreSQL database Experienced in agile and waterfall methodologies with high quality deliverables delivered ontime Experience in utilizing SAS Procedures Macros and other SAS application for data extraction data cleansing data loading and reporting Maintained detailed documentation and architectural solutions in IT infrastructure and sales systems Very strong full life cycle application development experience Strong knowledge on Dev Express Controls Strong database design and programming skills in SQL Server 201220082005 SQL Stored Procedures functions triggers Cursors Indexing importingexporting data from varied data sources Experience with continuous integration and automation using Jenkins Experience with Unit testing Test driven Development TDD Load Testing Have the ability to understand complex systems and be in command of the details to provide solutions Ability to learn and adapt quickly to the emerging new technologies and paradigms Excellent communication interpersonal and analytical skills and a highly motivated team player with the ability to work independently Practical experience with working on multiple environments like development testing production Handson experience in writing and reviewing requirements architecture documents test plans design documents quality analysis and audits Excellent analytical and problem solving skills and ability to work on own besides being a valuable and contributing team player Authorized to work in the US for any employer Work Experience Sr Python Developer EASTBAYFOOTLOCKER Wausau WI July 2017 to Present with Hadoop Eastbay is a supplier of athletic footwear apparel and sports equipment which sells by direct mail Since 1997 it has been the directtomail division of Foot Locker Inc Resposibilities Participated to develop a data platform from scratch and took part in requirement gathering and analysis phase of the project in documenting the business requirements Worked with team of Hadoop developers on maintaining the data platform applications for RISK management Worked in designing tables in Hive MYSQL using SQOOP and processing data like importing and exporting of databases to the HDFS Experienced in processing large datasets of different forms including structured semistructured and unstructured data Developed rest APIs using python with flask and django framework Experienced the integration of various data sources including Java JDBC RDBMS Shell Scripting Spreadsheets and Text files Exposure to various markup languages including XML JSON CSV Good Understanding of Hadoop architecture and the daemons of Hadoop including NameNode Data Node Job Tracker Task Tracker Resource Manager Hands on experience in ingesting data into Data Warehouse using various data loading techniques Developed python scripts to load data to hive from HDFS Participated in developing ETL components for executing various workflows Developed pig scripts and hive scripts for processing the data Handled the JSON XML Log data using Hive SERDE Pig and filter the data based on query factor Worked in agile methodology with 2 weeks sprints Scheduled Jobs using crontab rundeck and controlm Performed Branching Tagging Release Activities on Version Control Tools GIT and GITLAB Importing and exporting data jobs to perform operations like copying data from HDFS and to HDFS using Sqoop Developed Spark code and SparkSQLStreaming for faster testing and processing of data Data was Ingested which is received from various database providers using Sqoop onto HDFS for analysis and data processing Wrote and Implemented Apache PIG scripts to load data from and to store data into Hive Managed the imported data form different data sources performed transformation using Hive Pig and Map Reduce and loaded data in HDFS Executed Oozie workflow engine to run multiple Hive and Pig jobs which run independently with time and data availability To achieve Continuous Delivery goal on high scalable environment used Docker coupled with loadbalancing tool Nginx Developed Oozie workflow to run job onto data availability of transactions Created User Defined Functions UDFs for maintaining Incremental IDs Used Shell scripting to analyse the data from SQL Server source and processed it to store into HDFS Had good exposure to spark Mlib Streaming and sql by closely working with data scientists Generated reports from Hive data using Microstrategy Worked with complex sql queries to make joins Increased the time efficiency of the HIVEQL and reduced the time difference of executing the sets of data by applying the compression techniques for MapReduce Jobs Created Hive Partitions for storing Data for Different Companies under Different Partitions Environment Hadoop Hive sqoop pig Python 27 java Django 14 Flask XML MySQL MS SQL Server Linux Shell Scripting mongodb SQL Sr Python Developer FM Global January 2015 to June 2017 This is a mutual insurance company with offices worldwide that specializes in loss prevention services primarily to large corporations throughout the world in the Highly Protected RiskHPR property insurance market sector Responsibilities Participate in requirement gathering and analysis phase of the project in documenting the business requirements by conducting workshopsmeetings with various business users Worked with team of developers on Python applications for RISK management Developed PythonDjango application for Google Analytics aggregation and reporting Used Django configuration to manage URLs and application parameters Worked on Python Open stack APIs Used Python scripts to update content in the database and manipulate files Generated Python Django Forms to record data of online users Detailed Understanding on existing build system Tools related for information of various products and releases and test results information Designed and implemented map reduce jobs to support distributed processing using java Hive and Apache Pig Configured ec2 instances and configured IAM users and roles Created s3 data pipe using Boto API to load data from internal data sources Configured Jboss cluster and mysql database for application access Developed UDFs to provide custom hive and pig capabilities Built a mechanism for automatically moving the existing proprietary binary format data files to HDFS using a service called Ingestion service Comprehensive knowledge and experience in process improvement normalizationdenormalization data extraction data cleansing data manipulation Performed Data transformations in HIVE and used partitions buckets for performance improvements Ingestion of data into Hadoop using Sqoop and apply data transformations and using Pig and HIVE Used Python and Django creating graphics XML processing data exchange and business logic implementation Developed PlSql store procedures to convert the data from Oracle to MongoDB I have used Pandas API to put the data as time series and tabular format for east timestamp data manipulation and retrieval Automate report generation in MongoDB using Javascript shell scripting sed java Added support for Amazon AWS S3 and RDS to host staticmedia files and the database into Amazon Cloud Systems automation utilizing ControlM for scheduling and PowershellC for script development Used Pandas library for statistical Analysis Developed tools using Python Shell scripting XML to automate some of the menial tasks Interfacing with supervisors artists systems administrators and production to ensure production deadlines are met Worked very closely with designer tightly integrating Flash into the CMS with the use of Flashvars stored in the Django models Also created XML with Django to be used by the Flash Used HTML CSS JQuery JSON and Javascript for front end applications Designed and developed the UI of the website using HTML XHTML AJAX CSS and JavaScript Also used Bootstrap as a mechanism to manage and organize the html page layout Used Django configuration to manage URLs and application parameters Wrote and executed various MYSQL database queries from Python using PythonMySQL connector and MySQLdb package Involved in development of Web Services using SOAP for sending and getting data from the external interface in the XML format Worked on development of SQL and stored procedures on MYSQL Responsible for debugging the project monitored on JIRA Agile Performed troubleshooting fixed and deployed many Python bug fixes of the two main applications that were a main source of data for both customers and internal customer service team Environment Python hive oozie Amazon AWS S3 MySQL HTML Python 27 Django 14 HTML5 CSS XML MySQL MS SQL Server JavaScript AWS Linux Shell Scripting AJAX Mongodb Sr Python Developer Deustche bank NJ December 2013 to November 2014 Deustche bank is a German global banking and financial services company offering financial products and services for corporate and institutional clients along with private and business clients Services include sales trading research and origination of debt and equity mergers and acquisitions MA risk management products such as derivatives corporate finance wealth management retail banking fund management and transaction banking Responsibilities Worked with team of developers on Python applications for RISK management Designed the database schema for the content management system Designed and developed the UI of the website using HTML XHTML AJAX CSS and JavaScript Involved in development of Web Services using SOAP for sending and getting data from the external interface in the XML format Wrote Python routines to log into the websites and fetch data for selected options Performed testing using Djangos Test Module Skilled experience in installing configuring and using Apache Hadoop ecosystems such as Pig and Spark Built the entire Hadoop platform from scratch Experience in ingesting real timenear real time data using Flume Kafka Storm Evaluated suitability of Hadoop and its ecosystem to the above project and implementing validating with various proof of concept POC applications to eventually adopt them to benefit from the Big Data Hadoop initiative Estimated the Software Hardware requirements for the Name Node and Data Node in the cluster Extracted the needed data from the server into HDFS and Bulk Loaded the cleaned data into HBase using MapReduce Written the Map Reduce programs Hive UDFs in Java Develop HIVE queries for the analysts Created an email notification service upon completion of job for the particular team which requested for the data Defined job work flows as per their dependencies in Oozie Closely observed building the Reporting Application which uses the Spark SQL to fetch and generate reports on table data Knowledge in performance troubleshooting and tuning Hadoop clusters in Cloudera Worked on middle tier and persistence layer Created service and model layer classes and Value objectsPOJO to hold values between java classes and database fields ExportedImported data between different data sources using SQL Server Management Studio Maintained program libraries users manuals and technical documentation Responsible for debugging and troubleshooting the web application Successfully migrated all the data to the database while the site was in production Implemented the validation error handling and caching framework with Oracle Coherence cache Worked on scripts for setting up the discovery client with attribute data Worked on scriptsgranite reference data scripts for setting up adapter attributes in granite system Environment Python 27 Hadoop Django 14 HTML5 CSS XML MySQL JavaScript JQuery Mongo DB MS SQL Server JavaScript GitHub AWS Linux Shell Scripting AJAX Sr Python Developer Mangstor Austin TX January 2012 to November 2013 Mangstor is a leading developer of NVMe Flash Storage Solutions to accelerate application workloads in Webscale and Enterprise Data Centers Mangstors NVMe over Fabric Flash Storage Arrays and NVMe SolidState devices provide the building blocks necessary for highperformance highlyavailable and costeffective storage infrastructure Responsibilities Data warehouse migration Sybase 125 to Sybase 157 More than 70 of work is developing code in Python remaining time spent on database development and data modelling Worked on requirement gathering high level design implementation testing and deployment of code Creating and following the production deployment runbook Did Proof of Concept on DB2 BLUcolumn organized database Task prioritization with clients Designed and developed reusable Autosys jobs parsing and documentation software in Python using object oriented features being implemented in other projects in firm Designed and developed database object parsing dependency builder and documenting software in Python using object oriented features Developed ETL Extract Transform Load software for DB2 columnar database fact and dimension tables Massive data processing Sybase 15TB and Db2 2TB Data modelling in Sybase and DB2 Setting up schema users permissions creating database objects Database performance tuning procedures table functionsdb2 Provide L2L3 support on rotational basis Database objects refactoring removing legacy duplication Built pluggable software for housekeeping and cleanup being used by other projects in firm Data load analysis package to reveal errors during load based on historic trend Data reconciliation program for Sybase vs Db2 and Db2 vs Db2 prod vs qa in Python Other tools developed in Python to automate daily activities in python like monitoring DB Worked with shell scripts to build wrapper around ETL to do one time historic load Environment Python Java MySQL Linux HTML XHTML CSS AJAX JavaScript Apache Web Server Python Developer Bosch Designs Novi MI March 2010 to December 2011 Bosch designs and produces precision automotive components and systems sold to vehicle and powertrain manufacturers worldwide These include systems and components for gasoline and diesel injection airbag components ABS and conventional braking systems telematics as well as small motors electrical and electronic equipment Worked on development of Warranty and defects tracking system Responsibilities Wrote Python routines to log into the websites and fetch data for selected options Used Python modules such as requests urllib urllib2 for web crawling Used other packages such as Beautifulsoup for data parsing Worked on writing and as well as read data from csv and excel file formats Developed a MATLAB algorithm which determines an objects dimensions from digital images Webservices backend development using Python CherryPy Django SQLAlchemy Participated in developing the companys internal framework on Python This framework became a basement for the quick services development Framework based on CherryPy with GnuPg encryption reGnuPg module on the top Worked on resulting reports of the application and Tableau reports Worked on HTML5 CSS3 JavaScript AngularJS NodeJS Git REST API Mongo DB intelliJ IDEA Design and Setting up of environment of Mongodb with shards and replicasets DevTest and Production Private VPN using Ubuntu Python Django CherryPy Postgres Redis Bootstrap Jquery Mongo Fabric Git Tenjin Selenium Sphinx Nose Modifying data using SASBASE SAS MACROS Extracting data from the database using SASAccess SAS SQL procedures and create SAS data sets Performed QA testing on the application Developed approaches for improving NLP pipeline Create custom VB scripts in repackaging applications as needed NLP File Prep SettlementPrepare files for review for Settlement Held meetings with client and worked all alone for the entire project with limited help from the client Participated in the complete SDLC process Developed rich user interface using CSS HTML JavaScript and JQuery Created a Python based GUI application For Freight Tracking and processing Used Django framework for application development Developed and maintained various automated web tools for reducing manual effort and increasing efficiency of the Global Shipping Team Created database using MySQL wrote several queries to extract data from database Setup automated cron jobs to upload data into database generate graphs bar charts upload these charts to wiki and backup the database Wrote scripts in Python for extracting data from HTML file Effectively communicated with the external vendors to resolve queries Used Perforce for the version control Environment Python Django 14 MySQL Windows Linux HTML CSS JQuery JavaScript Apache Linux Software Engineer Python United Airlines Chicago IL January 2008 to February 2010 Responsibilities Django Framework was used in developing web applications to implement the MVT architecture Exposure on MultiThreading factory to distribute learning process backtesting and into various worker processes Worked on user portal creating forms and adding users Developed dynamic web pages using Python Django Frameworks Used Django Restful API for database access Wrote Python scripts to parse JSON documents and load the data in database Features for dashboard were developed and tested using CSS AngularJS and Bootstrap Developed forms using HTML and performing clientside validations using JavaScript jQuery and Bootstrap Cleansing data generated from weblogs with automated scripts in Python Used PostgreSQL database for storing the information and experience in performing CRUD operations to operate Schema objects to browse functions in python Experience in Performance Tuning Query Optimization ClientServer Connectivity and Database consistency checks Backup and Recovery Involved in Continuous Integration CI and Continuous Delivery CD process implementation using Jenkins along with Shell script Used GIT repository for software configuration management and version control Used JIRA for tracking and Updating Project issue Performed troubleshooting fixed and deployed many Python bug fixes of the two main applications that were Maintained main source of data for both customers and internal customer service team Maintained technical documentation for resolved issues for future reference Coded test programs and evaluated existing engineering process Effectively communicated with the external vendors to resolve queries Performed research to explore and identify new technological platforms Environment Python 352 Django Rest HTML JavaScript JIRA CSS Bootstrap PostgreSQL Jenkins Education Bachelors Skills PYTHON 10 years HTML 9 years JAVASCRIPT 9 years DJANGO 8 years APACHE 8 years Additional Information TECHNICAL PROFICIENCY OS Platforms LinuxUnix Windows98NT MAC OSX Languages Python 2724 Java Shell c Databases MySQL SQL Server 2008 PostgreSQL Web Technologies AJAX AWS EC Cloud Amazon S3 JavaScript HTML XML Versioning Tools Git SVNSTASH Web servers Apache Nginix Tomcat Framework DjangoFlask Other Tools PuttySQl developerPhotoshopJIRA",
    "unique_id": "56f78456-1196-4fcf-837b-62acd82178e1"
}