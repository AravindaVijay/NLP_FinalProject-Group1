{
    "clean_data": "Hadoop Developer Hadoop span lDeveloperspan Hadoop Developer Dell Austin TX Hadoop Developer with 8 Years of IT experience including 4 years in Big Data and Analytics field in Storage Querying Processing and Analysis for developing E2E Data pipelines Expertise in designing scalable Big Data solutions data warehouse models on largescale distributed data performing wide range of analytics Expertise in all components of HadoopSpark Ecosystems Spark Hive Pig Flume Sqoop HBase Kafka Oozie Impala Stream sets Apache NIFI Hue AWS 3 years of experience working in programming languages ScalaPython Extensive knowledge on data serialization techniques like Avro Sequence Files Parquet JSON and ORC Acute knowledge on Spark architecture and realtime streaming using Spark Hands on experience with Spark Core Spark SQL and Data FramesData SetsRDD API Good knowledge on Amazon Web Services AWS cloud services like EC2 S3 EMR and VPC Experienced in Data Ingestion Data Processing Data Aggregations Visualization in Spark Environment Hands on experience in working with large volume of Structured and UnStructured data Expert in migrating the code components from SVN repository to Bit Bucket repository Experienced in building Jenkins pipelines for continuous code integration from Github into Linux machine Experience in Object Oriented Analysis Design OOAD and development Good understanding in endto end web applications and design patterns Hands on experience in application development using Java RDBMS and Linux shell scripting Experience in implementing by using agile methodology Well versed in using Software development methodologies like Agile Methodology and Waterfall processes Experienced in handling databases Netezza Oracle and Teradata Strong team player with good communication analytical presentation and interpersonal skills Work Experience Hadoop Developer Dell Austin TX US December 2017 to Present Dell EMC is the undisputed industry leader for unstructured data storage Tens of thousands of customers have bet their business on UDS products including Isilon ScaleOu t File Storage and ECS Object Storage Moreover Gartner has consistently recognized Dell EMC as the clear Magic Quadrant leader for Distributed File and Object Storage As a part of Data science team we are responsible for analyzing processing and visualizing one of our clients information to improve the business Responsibilities Worked on HortonworksHDP 25 distribution Involved in review of functional and nonfunctional requirements Responsible for designing and implementing the data pipeline using Big Data tools including Hive Spark Scala and Stream Sets Experience in using Apache Storm Spark Streaming Apache Spark Apache NiFi Kafka and Flume in creating data streaming solutions Developed and implemented Apache NIFI across various environments written QA scripts in Python for tracking files Involved in importing data from Microsoft SQL Server MySQL and Teradata into HDFS using Sqoop Good knowledge in using Apache NIFI to automate the data movement Developed Sqoop scripts to import data from relational sources and handled incremental loading Extensively used Stream Sets Data Collector to create ETL pipeline for pulling the data from RDBMS system to HDFS Implemented the data processing framework using Scala and Spark SQL Worked on implementing the performance optimization methods to improve the data processing timing Experienced in creating the shell scripts and made jobs automated Extensively worked on Data frames and Datasets using Spark and Spark SQL Responsible for defining the data flow within Hadoop eco system and direct the team in implement them and exported the result set from Hive to MySQL using Shell scripts Worked on Kafka Streaming using stream sets to process continuous integration of data from Oracle systems to hive tables Developed a generic utility in Spark for pulling the data from RDBMS system using multiple parallel connections Integrated existing code logic in HiveQL and implemented in the Spark application for data processing Extensively used HiveSpark optimization techniques like Partitioning Bucketing Map Join parallel execution Broadcast join and Repartitioning Environment Spark Python Scala Hive Hue UNIX Scripting Spark SQL Stream sets Kafka Impala Beeline Git Tidal Hadoop Developer InterAmerican Development Bank Washington DC US January 2016 to November 2017 A currency transaction report CTR is a report that US financial institutions are required to file with FinCEN for each deposit withdrawal exchange of currency or other payment and transfer by through or to the financial institution which involves a transaction in currency of more than 10000 Report is generated daily basis with T1 data with predefined rules and alerts Once report is generated with potential CTRs analysts will parse alerted CTRs to identify actual vs fractious CTRs vial Web interface Final consolidated report will be regenerated every night and sent to FINCEN in predefined FINCEN format Responsibilities Worked on HortonworksHDP 25distribution Experience in implementing Scala framework code using IntelliJ and UNIX scripting to implement the workflow for the jobs Involved in gathering business requirement analyze the use case and implement the use case end to end Worked closely with the Architect enhanced and optimized product Spark and Scala code to aggregate group and run data mining tasks using Spark framework Experienced in loading the raw data into RDDs and validate the data Experienced in converting the validated RDDs into Data frames for further processing Implemented the Spark SQL code logic to join multiple data frames to generate application specific aggregated results Experienced in fine tuning the jobs for better performance in the production cluster space Worked totally in agile methodologies used Rally scrum tool to track the User stories and Team performance Worked extensively in Impala Hue to analyze the processed data and to generate the end reports Experienced working with hive database through beeline Worked on analyzing and resolving the production job failures in several scenarios Implemented UNIXscripts to define the use case workflow and also to process the data files and automate the jobs Environment Spark Scala Hive Sqoop UNIX Scripting Spark SQL IntelliJ Hbase Kafka Impala Hue Beeline Git Hadoop Developer Atlanta Gas Light Atlanta GA US October 2014 to January 2016 Responsibilities Worked on Cloudera CDH distribution Hand on experience on cloud services like Amazon Web Services AWS Created data pipelines for different events to load the data from DynamoDB to AWS S3 bucket and then into HDFS location Involved in complete SDLC Requirement Analysis Development Testing and Deployment into Cluster Worked handinhand with the Architect enhanced and optimized product Spark code to aggregate group and run data mining tasks using Spark framework Extracted data from various SQL database sources into HDFS using Sqoopand also ran Hive scripts on the huge chunks of data Implemented a prototype for the complete requirements using Splunk python and Machine learning concepts Design and Implementation of Map reduce code logic for Natural Language Processing of Free Form Text Deployed the project on Amazon EMR with S3 Connectivity Implemented usage of Amazon EMR for processing Big Data across a Hadoop Cluster of virtual servers on Amazon Elastic Compute Cloud EC2 and Amazon Simple Storage ServiceS3 Loaded the data into Simple Storage Service S3 in the AWS Cloud Good Knowledge in using of Amazon Load Balancer for Auto scaling in EC2 servers Implemented Spark scripts to migrate map reduce jobs into Spark RDD transformations streaming data using Apache Kafka Implemented Spark SQL queries which intermix the Hive queries with the programmatic data manipulations supported by RDDs and data frames in scala and python Involved in Deployment of Code Logic and UDFsacross the cluster Communicate deliverables status to userstakeholders client and drive periodic review meetings Worked on Data Processing using Hive queries in HDFS and the shell Scripts to wrap the HQL scripts Developed and Deployed Oozie Workflows for recurring operations on Clusters Experienced in performance tuning of hadoop jobs for setting right Batch Interval time correct level of Parallelism and memory tuning Worked extensively with Sqoop for importing metadata from Oracle Used Tableau reporting tool to generate reports from the outputs stored in HDFS Environment Hadoop Spark HDFS Hive Map Reduce Sqoop Oozie Tableau Hadoop Developer Toyota Motors July 2012 to September 2014 Responsibilities Worked on Cloudera CDH distribution Design and Implement historical and incremental data ingestion techniques from multiple external systems using Hive pig and sqoop ingestion tools Design physical data models for structured and semistructured to validate the raw data into HDFS Design mapreduce logic and HIVES queries for generating aggregated metrics Involved in Design implementation development and testing phases in the project Responsible to monitor the jobs in production cluster while and trace the error logs when the jobs fails Design and Develop data migration logic for exporting data from MySQL to Hive Design and Develop complex workflow in Oozie for recurrent job execution Used SSRS reporting tool for the generation of data analysis reports Environment Hadoop MapReduce HDFS Pig Hive Oozie Eclipse Cloudera Sqoop SSRS Software Developer Garima Software Solutions November 2010 to June 2012 Responsibilities Involved in complete SDLC Requirement Analysis Development Testing and Deployments Involved in resolving critical Errors Responsible to deploy the deliverables of sprints successfully Involved in capturing the clients requirements and enhancements on the application document the requirements and populate to the associated teams Design and Implementation of REST Full services and WSDL in VORDEL Implemented complex SQL quires to get the analysis reports Created Desktop applications using J2EE Swings Involved in developing applications using Java JSP Servlets Swings Developed UI using HTML CSS Ajax JQuery and developed Business logic and Interfacing Components using Business Objects XML and JDBC Created applications connection pools deployment of JSP Servlets Used Oracle MySQL database for storing user information Developed backed for application using PHP for web applications Experienced with the Agile Methodologies Environment SOAP REST HTML WSDL 22Vordel SQL Developer Education Bachelors Skills DYNAMODB CASSANDRA AMBARI HDFS OOZIE Additional Information Technical Skills Bigdata Technologies HDFS Map Reduce Pig Hive Sqoop Oozie Scala Spark Kafka Flume Ambari Hue Hadoop Frameworks Cloudera CDHs Hortonworks HDPs MAPR Database Oracle 10g11g PLSQL MySQL MS SQL Server 2012 DB2 Language C C Java Scala Python AWS Components IAH S3 EMR EC2Lambda Route 53 Cloud Watch SNS Methodologies Agile Waterfall Build Tools Maven Gradle Jenkins NOSQL Databases HBase Cassandra MongoDB DynamoDB IDE Tools Eclipse Net Beans Intellij Modelling Tools Rational Rose Star UML Visual paradigm for UML Architecture Relational DBMS ClientServer Architecture Cloud Platforms AWS Cloud BI Tools Tableau Operating System Windows 7810 Vista UNIX Linux Ubuntu Mac OS X",
    "entities": [
        "Implemented Spark",
        "UML Architecture Relational DBMS ClientServer Architecture Cloud Platforms AWS",
        "CTR",
        "SQL Developer",
        "Distributed File and Object Storage",
        "E2E Data",
        "UNIX",
        "python Involved in Deployment of Code Logic",
        "Hadoop Developer Hadoop",
        "OOZIE Additional Information Technical Skills Bigdata Technologies",
        "Cloudera Sqoop",
        "Amazon Elastic Compute Cloud EC2",
        "Impala Hue",
        "Toyota",
        "Amazon Web Services AWS",
        "Agile Methodology",
        "Hadoop",
        "Atlanta",
        "Hive Spark",
        "Storage Querying Processing and Analysis",
        "ORC Acute",
        "Dell EMC",
        "Shell",
        "UDFsacross",
        "HDFS Environment Hadoop Spark",
        "Developed Sqoop",
        "Architect",
        "HadoopSpark Ecosystems Spark",
        "Machine",
        "Developed",
        "Spark SQL Responsible",
        "Simple Storage Service S3",
        "AWS S3",
        "VPC Experienced",
        "J2EE Swings Involved",
        "Waterfall",
        "S3 Connectivity Implemented",
        "ECS Object Storage Moreover Gartner",
        "Spark Hands",
        "ScalaPython Extensive",
        "Communicate",
        "Linux",
        "JSP",
        "Created Desktop",
        "Design and Implementation of Map",
        "Washington DC",
        "Amazon Load Balancer for Auto",
        "Implemented the Spark SQL",
        "Spark",
        "Sqoopand",
        "Amazon EMR",
        "SDLC Requirement Analysis Development Testing and Deployments Involved",
        "MAPR Database Oracle",
        "US",
        "Sqoop",
        "QA",
        "Netezza Oracle",
        "Spark Core Spark",
        "Oracle",
        "Clusters Experienced",
        "Oracle Used Tableau",
        "SDLC Requirement Analysis Development Testing and Deployment",
        "Amazon Web Services AWS Created",
        "Oozie",
        "SSRS",
        "UnStructured",
        "SQL",
        "MySQL to Hive Design and Develop",
        "Spark RDD",
        "HTML CSS Ajax JQuery",
        "Data Ingestion Data Processing Data Aggregations Visualization",
        "Github",
        "HiveSpark",
        "Object Oriented Analysis Design OOAD",
        "Big Data",
        "Hive",
        "Rally",
        "HiveQL",
        "ETL",
        "Stream Sets Experience",
        "Implemented UNIXscripts",
        "BI Tools Tableau Operating System Windows 7810",
        "Interfacing Components",
        "Microsoft",
        "Work Experience Hadoop Developer",
        "Environment Hadoop MapReduce HDFS Pig Hive Oozie",
        "the Agile Methodologies Environment SOAP",
        "SVN",
        "IDE Tools Eclipse Net Beans",
        "Java JSP Servlets",
        "Data",
        "Structured",
        "Apache NIFI",
        "Worked on Data Processing using Hive",
        "Team",
        "PHP",
        "TX Hadoop Developer",
        "Stream Sets Data Collector",
        "Repartitioning Environment Spark",
        "Amazon Simple Storage ServiceS3 Loaded"
    ],
    "experience": "Experience in Object Oriented Analysis Design OOAD and development Good understanding in endto end web applications and design patterns Hands on experience in application development using Java RDBMS and Linux shell scripting Experience in implementing by using agile methodology Well versed in using Software development methodologies like Agile Methodology and Waterfall processes Experienced in handling databases Netezza Oracle and Teradata Strong team player with good communication analytical presentation and interpersonal skills Work Experience Hadoop Developer Dell Austin TX US December 2017 to Present Dell EMC is the undisputed industry leader for unstructured data storage Tens of thousands of customers have bet their business on UDS products including Isilon ScaleOu t File Storage and ECS Object Storage Moreover Gartner has consistently recognized Dell EMC as the clear Magic Quadrant leader for Distributed File and Object Storage As a part of Data science team we are responsible for analyzing processing and visualizing one of our clients information to improve the business Responsibilities Worked on HortonworksHDP 25 distribution Involved in review of functional and nonfunctional requirements Responsible for designing and implementing the data pipeline using Big Data tools including Hive Spark Scala and Stream Sets Experience in using Apache Storm Spark Streaming Apache Spark Apache NiFi Kafka and Flume in creating data streaming solutions Developed and implemented Apache NIFI across various environments written QA scripts in Python for tracking files Involved in importing data from Microsoft SQL Server MySQL and Teradata into HDFS using Sqoop Good knowledge in using Apache NIFI to automate the data movement Developed Sqoop scripts to import data from relational sources and handled incremental loading Extensively used Stream Sets Data Collector to create ETL pipeline for pulling the data from RDBMS system to HDFS Implemented the data processing framework using Scala and Spark SQL Worked on implementing the performance optimization methods to improve the data processing timing Experienced in creating the shell scripts and made jobs automated Extensively worked on Data frames and Datasets using Spark and Spark SQL Responsible for defining the data flow within Hadoop eco system and direct the team in implement them and exported the result set from Hive to MySQL using Shell scripts Worked on Kafka Streaming using stream sets to process continuous integration of data from Oracle systems to hive tables Developed a generic utility in Spark for pulling the data from RDBMS system using multiple parallel connections Integrated existing code logic in HiveQL and implemented in the Spark application for data processing Extensively used HiveSpark optimization techniques like Partitioning Bucketing Map Join parallel execution Broadcast join and Repartitioning Environment Spark Python Scala Hive Hue UNIX Scripting Spark SQL Stream sets Kafka Impala Beeline Git Tidal Hadoop Developer InterAmerican Development Bank Washington DC US January 2016 to November 2017 A currency transaction report CTR is a report that US financial institutions are required to file with FinCEN for each deposit withdrawal exchange of currency or other payment and transfer by through or to the financial institution which involves a transaction in currency of more than 10000 Report is generated daily basis with T1 data with predefined rules and alerts Once report is generated with potential CTRs analysts will parse alerted CTRs to identify actual vs fractious CTRs vial Web interface Final consolidated report will be regenerated every night and sent to FINCEN in predefined FINCEN format Responsibilities Worked on HortonworksHDP 25distribution Experience in implementing Scala framework code using IntelliJ and UNIX scripting to implement the workflow for the jobs Involved in gathering business requirement analyze the use case and implement the use case end to end Worked closely with the Architect enhanced and optimized product Spark and Scala code to aggregate group and run data mining tasks using Spark framework Experienced in loading the raw data into RDDs and validate the data Experienced in converting the validated RDDs into Data frames for further processing Implemented the Spark SQL code logic to join multiple data frames to generate application specific aggregated results Experienced in fine tuning the jobs for better performance in the production cluster space Worked totally in agile methodologies used Rally scrum tool to track the User stories and Team performance Worked extensively in Impala Hue to analyze the processed data and to generate the end reports Experienced working with hive database through beeline Worked on analyzing and resolving the production job failures in several scenarios Implemented UNIXscripts to define the use case workflow and also to process the data files and automate the jobs Environment Spark Scala Hive Sqoop UNIX Scripting Spark SQL IntelliJ Hbase Kafka Impala Hue Beeline Git Hadoop Developer Atlanta Gas Light Atlanta GA US October 2014 to January 2016 Responsibilities Worked on Cloudera CDH distribution Hand on experience on cloud services like Amazon Web Services AWS Created data pipelines for different events to load the data from DynamoDB to AWS S3 bucket and then into HDFS location Involved in complete SDLC Requirement Analysis Development Testing and Deployment into Cluster Worked handinhand with the Architect enhanced and optimized product Spark code to aggregate group and run data mining tasks using Spark framework Extracted data from various SQL database sources into HDFS using Sqoopand also ran Hive scripts on the huge chunks of data Implemented a prototype for the complete requirements using Splunk python and Machine learning concepts Design and Implementation of Map reduce code logic for Natural Language Processing of Free Form Text Deployed the project on Amazon EMR with S3 Connectivity Implemented usage of Amazon EMR for processing Big Data across a Hadoop Cluster of virtual servers on Amazon Elastic Compute Cloud EC2 and Amazon Simple Storage ServiceS3 Loaded the data into Simple Storage Service S3 in the AWS Cloud Good Knowledge in using of Amazon Load Balancer for Auto scaling in EC2 servers Implemented Spark scripts to migrate map reduce jobs into Spark RDD transformations streaming data using Apache Kafka Implemented Spark SQL queries which intermix the Hive queries with the programmatic data manipulations supported by RDDs and data frames in scala and python Involved in Deployment of Code Logic and UDFsacross the cluster Communicate deliverables status to userstakeholders client and drive periodic review meetings Worked on Data Processing using Hive queries in HDFS and the shell Scripts to wrap the HQL scripts Developed and Deployed Oozie Workflows for recurring operations on Clusters Experienced in performance tuning of hadoop jobs for setting right Batch Interval time correct level of Parallelism and memory tuning Worked extensively with Sqoop for importing metadata from Oracle Used Tableau reporting tool to generate reports from the outputs stored in HDFS Environment Hadoop Spark HDFS Hive Map Reduce Sqoop Oozie Tableau Hadoop Developer Toyota Motors July 2012 to September 2014 Responsibilities Worked on Cloudera CDH distribution Design and Implement historical and incremental data ingestion techniques from multiple external systems using Hive pig and sqoop ingestion tools Design physical data models for structured and semistructured to validate the raw data into HDFS Design mapreduce logic and HIVES queries for generating aggregated metrics Involved in Design implementation development and testing phases in the project Responsible to monitor the jobs in production cluster while and trace the error logs when the jobs fails Design and Develop data migration logic for exporting data from MySQL to Hive Design and Develop complex workflow in Oozie for recurrent job execution Used SSRS reporting tool for the generation of data analysis reports Environment Hadoop MapReduce HDFS Pig Hive Oozie Eclipse Cloudera Sqoop SSRS Software Developer Garima Software Solutions November 2010 to June 2012 Responsibilities Involved in complete SDLC Requirement Analysis Development Testing and Deployments Involved in resolving critical Errors Responsible to deploy the deliverables of sprints successfully Involved in capturing the clients requirements and enhancements on the application document the requirements and populate to the associated teams Design and Implementation of REST Full services and WSDL in VORDEL Implemented complex SQL quires to get the analysis reports Created Desktop applications using J2EE Swings Involved in developing applications using Java JSP Servlets Swings Developed UI using HTML CSS Ajax JQuery and developed Business logic and Interfacing Components using Business Objects XML and JDBC Created applications connection pools deployment of JSP Servlets Used Oracle MySQL database for storing user information Developed backed for application using PHP for web applications Experienced with the Agile Methodologies Environment SOAP REST HTML WSDL 22Vordel SQL Developer Education Bachelors Skills DYNAMODB CASSANDRA AMBARI HDFS OOZIE Additional Information Technical Skills Bigdata Technologies HDFS Map Reduce Pig Hive Sqoop Oozie Scala Spark Kafka Flume Ambari Hue Hadoop Frameworks Cloudera CDHs Hortonworks HDPs MAPR Database Oracle 10g11 g PLSQL MySQL MS SQL Server 2012 DB2 Language C C Java Scala Python AWS Components IAH S3 EMR EC2Lambda Route 53 Cloud Watch SNS Methodologies Agile Waterfall Build Tools Maven Gradle Jenkins NOSQL Databases HBase Cassandra MongoDB DynamoDB IDE Tools Eclipse Net Beans Intellij Modelling Tools Rational Rose Star UML Visual paradigm for UML Architecture Relational DBMS ClientServer Architecture Cloud Platforms AWS Cloud BI Tools Tableau Operating System Windows 7810 Vista UNIX Linux Ubuntu Mac OS X",
    "extracted_keywords": [
        "Hadoop",
        "Developer",
        "Hadoop",
        "span",
        "lDeveloperspan",
        "Hadoop",
        "Developer",
        "Dell",
        "Austin",
        "TX",
        "Hadoop",
        "Developer",
        "Years",
        "IT",
        "experience",
        "years",
        "Big",
        "Data",
        "Analytics",
        "field",
        "Storage",
        "Querying",
        "Processing",
        "Analysis",
        "E2E",
        "Data",
        "pipelines",
        "Expertise",
        "Big",
        "Data",
        "solutions",
        "data",
        "warehouse",
        "models",
        "largescale",
        "data",
        "range",
        "analytics",
        "Expertise",
        "components",
        "HadoopSpark",
        "Ecosystems",
        "Spark",
        "Hive",
        "Pig",
        "Flume",
        "Sqoop",
        "HBase",
        "Kafka",
        "Oozie",
        "Impala",
        "Stream",
        "Apache",
        "NIFI",
        "Hue",
        "AWS",
        "years",
        "experience",
        "programming",
        "languages",
        "ScalaPython",
        "knowledge",
        "data",
        "serialization",
        "techniques",
        "Avro",
        "Sequence",
        "Files",
        "Parquet",
        "JSON",
        "ORC",
        "Acute",
        "knowledge",
        "Spark",
        "architecture",
        "streaming",
        "Spark",
        "Hands",
        "experience",
        "Spark",
        "Core",
        "Spark",
        "SQL",
        "Data",
        "FramesData",
        "API",
        "knowledge",
        "Amazon",
        "Web",
        "Services",
        "AWS",
        "cloud",
        "services",
        "EC2",
        "S3",
        "EMR",
        "VPC",
        "Data",
        "Ingestion",
        "Data",
        "Processing",
        "Data",
        "Aggregations",
        "Visualization",
        "Spark",
        "Environment",
        "Hands",
        "experience",
        "volume",
        "Structured",
        "UnStructured",
        "data",
        "Expert",
        "code",
        "components",
        "SVN",
        "repository",
        "Bit",
        "Bucket",
        "repository",
        "Jenkins",
        "pipelines",
        "code",
        "integration",
        "Github",
        "Linux",
        "machine",
        "Experience",
        "Object",
        "Oriented",
        "Analysis",
        "Design",
        "OOAD",
        "development",
        "understanding",
        "endto",
        "end",
        "web",
        "applications",
        "design",
        "patterns",
        "Hands",
        "experience",
        "application",
        "development",
        "Java",
        "RDBMS",
        "Linux",
        "shell",
        "Experience",
        "methodology",
        "Software",
        "development",
        "methodologies",
        "Agile",
        "Methodology",
        "Waterfall",
        "processes",
        "Netezza",
        "Oracle",
        "Teradata",
        "Strong",
        "team",
        "player",
        "communication",
        "presentation",
        "skills",
        "Work",
        "Experience",
        "Hadoop",
        "Developer",
        "Dell",
        "Austin",
        "TX",
        "US",
        "December",
        "Present",
        "Dell",
        "EMC",
        "industry",
        "leader",
        "data",
        "storage",
        "Tens",
        "thousands",
        "customers",
        "business",
        "UDS",
        "products",
        "Isilon",
        "ScaleOu",
        "t",
        "File",
        "Storage",
        "ECS",
        "Object",
        "Storage",
        "Moreover",
        "Gartner",
        "Dell",
        "EMC",
        "Magic",
        "Quadrant",
        "leader",
        "Distributed",
        "File",
        "Object",
        "Storage",
        "part",
        "Data",
        "science",
        "team",
        "processing",
        "clients",
        "information",
        "business",
        "Responsibilities",
        "HortonworksHDP",
        "distribution",
        "review",
        "requirements",
        "data",
        "pipeline",
        "Big",
        "Data",
        "tools",
        "Hive",
        "Spark",
        "Scala",
        "Stream",
        "Sets",
        "Experience",
        "Apache",
        "Storm",
        "Spark",
        "Streaming",
        "Apache",
        "Spark",
        "Apache",
        "NiFi",
        "Kafka",
        "Flume",
        "data",
        "streaming",
        "solutions",
        "Apache",
        "NIFI",
        "environments",
        "QA",
        "scripts",
        "Python",
        "files",
        "data",
        "Microsoft",
        "SQL",
        "Server",
        "MySQL",
        "Teradata",
        "HDFS",
        "Sqoop",
        "knowledge",
        "Apache",
        "NIFI",
        "data",
        "movement",
        "Developed",
        "Sqoop",
        "scripts",
        "data",
        "sources",
        "loading",
        "Stream",
        "Sets",
        "Data",
        "Collector",
        "ETL",
        "pipeline",
        "data",
        "RDBMS",
        "system",
        "HDFS",
        "data",
        "processing",
        "framework",
        "Scala",
        "Spark",
        "SQL",
        "performance",
        "optimization",
        "methods",
        "data",
        "processing",
        "timing",
        "shell",
        "scripts",
        "jobs",
        "Data",
        "frames",
        "Datasets",
        "Spark",
        "Spark",
        "SQL",
        "Responsible",
        "data",
        "flow",
        "Hadoop",
        "eco",
        "system",
        "team",
        "result",
        "Hive",
        "MySQL",
        "Shell",
        "scripts",
        "Kafka",
        "Streaming",
        "stream",
        "sets",
        "integration",
        "data",
        "Oracle",
        "systems",
        "tables",
        "utility",
        "Spark",
        "data",
        "RDBMS",
        "system",
        "connections",
        "code",
        "logic",
        "HiveQL",
        "Spark",
        "application",
        "data",
        "HiveSpark",
        "optimization",
        "techniques",
        "Partitioning",
        "Bucketing",
        "Map",
        "execution",
        "Broadcast",
        "join",
        "Repartitioning",
        "Environment",
        "Spark",
        "Python",
        "Scala",
        "Hive",
        "Hue",
        "UNIX",
        "Scripting",
        "Spark",
        "SQL",
        "Stream",
        "Kafka",
        "Impala",
        "Beeline",
        "Git",
        "Tidal",
        "Hadoop",
        "Developer",
        "InterAmerican",
        "Development",
        "Bank",
        "Washington",
        "DC",
        "US",
        "January",
        "November",
        "currency",
        "transaction",
        "report",
        "CTR",
        "report",
        "US",
        "institutions",
        "FinCEN",
        "deposit",
        "withdrawal",
        "exchange",
        "currency",
        "payment",
        "transfer",
        "institution",
        "transaction",
        "currency",
        "Report",
        "basis",
        "T1",
        "data",
        "rules",
        "alerts",
        "report",
        "CTRs",
        "analysts",
        "CTRs",
        "CTRs",
        "vial",
        "Web",
        "interface",
        "report",
        "night",
        "FINCEN",
        "FINCEN",
        "format",
        "Responsibilities",
        "HortonworksHDP",
        "25distribution",
        "Experience",
        "Scala",
        "framework",
        "code",
        "IntelliJ",
        "UNIX",
        "scripting",
        "workflow",
        "jobs",
        "business",
        "requirement",
        "use",
        "case",
        "use",
        "case",
        "end",
        "Architect",
        "product",
        "Spark",
        "Scala",
        "code",
        "group",
        "data",
        "mining",
        "tasks",
        "Spark",
        "framework",
        "data",
        "RDDs",
        "data",
        "RDDs",
        "Data",
        "frames",
        "processing",
        "Spark",
        "SQL",
        "code",
        "logic",
        "data",
        "frames",
        "application",
        "results",
        "jobs",
        "performance",
        "production",
        "cluster",
        "space",
        "methodologies",
        "Rally",
        "tool",
        "User",
        "stories",
        "Team",
        "performance",
        "Impala",
        "Hue",
        "data",
        "end",
        "reports",
        "hive",
        "database",
        "beeline",
        "production",
        "job",
        "failures",
        "scenarios",
        "UNIXscripts",
        "use",
        "case",
        "data",
        "files",
        "jobs",
        "Environment",
        "Spark",
        "Scala",
        "Hive",
        "Sqoop",
        "UNIX",
        "Scripting",
        "Spark",
        "SQL",
        "IntelliJ",
        "Hbase",
        "Kafka",
        "Impala",
        "Hue",
        "Beeline",
        "Git",
        "Hadoop",
        "Developer",
        "Atlanta",
        "Gas",
        "Light",
        "Atlanta",
        "GA",
        "US",
        "October",
        "January",
        "Responsibilities",
        "Cloudera",
        "CDH",
        "distribution",
        "Hand",
        "experience",
        "cloud",
        "services",
        "Amazon",
        "Web",
        "Services",
        "AWS",
        "data",
        "pipelines",
        "events",
        "data",
        "DynamoDB",
        "AWS",
        "S3",
        "bucket",
        "HDFS",
        "location",
        "SDLC",
        "Requirement",
        "Analysis",
        "Development",
        "Testing",
        "Deployment",
        "Cluster",
        "Worked",
        "handinhand",
        "Architect",
        "product",
        "Spark",
        "code",
        "group",
        "data",
        "mining",
        "tasks",
        "Spark",
        "framework",
        "data",
        "SQL",
        "database",
        "sources",
        "HDFS",
        "Sqoopand",
        "Hive",
        "scripts",
        "chunks",
        "data",
        "prototype",
        "requirements",
        "Splunk",
        "python",
        "Machine",
        "learning",
        "concepts",
        "Design",
        "Implementation",
        "Map",
        "code",
        "logic",
        "Natural",
        "Language",
        "Processing",
        "Free",
        "Form",
        "Text",
        "project",
        "Amazon",
        "EMR",
        "S3",
        "Connectivity",
        "usage",
        "Amazon",
        "EMR",
        "Big",
        "Data",
        "Hadoop",
        "Cluster",
        "servers",
        "Amazon",
        "Elastic",
        "Compute",
        "Cloud",
        "EC2",
        "Amazon",
        "Simple",
        "Storage",
        "ServiceS3",
        "data",
        "Simple",
        "Storage",
        "Service",
        "S3",
        "AWS",
        "Cloud",
        "Good",
        "Knowledge",
        "Amazon",
        "Load",
        "Balancer",
        "Auto",
        "scaling",
        "EC2",
        "servers",
        "Spark",
        "scripts",
        "migrate",
        "map",
        "jobs",
        "Spark",
        "RDD",
        "transformations",
        "data",
        "Apache",
        "Kafka",
        "Spark",
        "SQL",
        "queries",
        "Hive",
        "data",
        "manipulations",
        "RDDs",
        "data",
        "frames",
        "scala",
        "python",
        "Deployment",
        "Code",
        "Logic",
        "UDFsacross",
        "cluster",
        "Communicate",
        "status",
        "userstakeholders",
        "client",
        "review",
        "meetings",
        "Data",
        "Processing",
        "Hive",
        "queries",
        "HDFS",
        "Scripts",
        "HQL",
        "scripts",
        "Deployed",
        "Oozie",
        "Workflows",
        "operations",
        "Clusters",
        "performance",
        "tuning",
        "hadoop",
        "jobs",
        "Batch",
        "Interval",
        "time",
        "level",
        "Parallelism",
        "memory",
        "tuning",
        "Sqoop",
        "metadata",
        "Oracle",
        "Used",
        "Tableau",
        "tool",
        "reports",
        "outputs",
        "HDFS",
        "Environment",
        "Hadoop",
        "Spark",
        "HDFS",
        "Hive",
        "Map",
        "Reduce",
        "Sqoop",
        "Oozie",
        "Tableau",
        "Hadoop",
        "Developer",
        "Toyota",
        "Motors",
        "July",
        "September",
        "Responsibilities",
        "Cloudera",
        "CDH",
        "distribution",
        "Design",
        "Implement",
        "data",
        "ingestion",
        "techniques",
        "systems",
        "Hive",
        "pig",
        "sqoop",
        "ingestion",
        "tools",
        "Design",
        "data",
        "models",
        "data",
        "HDFS",
        "Design",
        "mapreduce",
        "logic",
        "HIVES",
        "queries",
        "metrics",
        "Design",
        "implementation",
        "development",
        "phases",
        "project",
        "jobs",
        "production",
        "cluster",
        "error",
        "logs",
        "jobs",
        "Design",
        "Develop",
        "data",
        "migration",
        "logic",
        "data",
        "MySQL",
        "Hive",
        "Design",
        "Develop",
        "workflow",
        "Oozie",
        "job",
        "execution",
        "SSRS",
        "reporting",
        "tool",
        "generation",
        "data",
        "analysis",
        "Environment",
        "Hadoop",
        "MapReduce",
        "HDFS",
        "Pig",
        "Hive",
        "Oozie",
        "Eclipse",
        "Cloudera",
        "Sqoop",
        "SSRS",
        "Software",
        "Developer",
        "Garima",
        "Software",
        "Solutions",
        "November",
        "June",
        "Responsibilities",
        "SDLC",
        "Requirement",
        "Analysis",
        "Development",
        "Testing",
        "Deployments",
        "Errors",
        "deliverables",
        "sprints",
        "clients",
        "requirements",
        "enhancements",
        "application",
        "document",
        "requirements",
        "teams",
        "Design",
        "Implementation",
        "REST",
        "services",
        "WSDL",
        "VORDEL",
        "SQL",
        "analysis",
        "reports",
        "Desktop",
        "applications",
        "J2EE",
        "Swings",
        "applications",
        "Java",
        "JSP",
        "Servlets",
        "Swings",
        "UI",
        "HTML",
        "CSS",
        "Ajax",
        "JQuery",
        "Business",
        "logic",
        "Interfacing",
        "Components",
        "Business",
        "Objects",
        "XML",
        "JDBC",
        "Created",
        "applications",
        "connection",
        "pools",
        "deployment",
        "JSP",
        "Servlets",
        "Oracle",
        "MySQL",
        "database",
        "user",
        "information",
        "Developed",
        "application",
        "PHP",
        "web",
        "applications",
        "Agile",
        "Methodologies",
        "Environment",
        "SOAP",
        "REST",
        "HTML",
        "WSDL",
        "SQL",
        "Developer",
        "Education",
        "Bachelors",
        "Skills",
        "CASSANDRA",
        "AMBARI",
        "HDFS",
        "OOZIE",
        "Additional",
        "Information",
        "Technical",
        "Skills",
        "Bigdata",
        "Technologies",
        "HDFS",
        "Map",
        "Reduce",
        "Pig",
        "Hive",
        "Sqoop",
        "Oozie",
        "Scala",
        "Spark",
        "Kafka",
        "Flume",
        "Ambari",
        "Hue",
        "Hadoop",
        "Frameworks",
        "Cloudera",
        "CDHs",
        "Hortonworks",
        "HDPs",
        "MAPR",
        "Database",
        "Oracle",
        "g",
        "PLSQL",
        "MySQL",
        "MS",
        "SQL",
        "Server",
        "DB2",
        "Language",
        "C",
        "C",
        "Java",
        "Scala",
        "Python",
        "AWS",
        "Components",
        "IAH",
        "S3",
        "EMR",
        "EC2Lambda",
        "Route",
        "Cloud",
        "Watch",
        "SNS",
        "Methodologies",
        "Agile",
        "Waterfall",
        "Build",
        "Tools",
        "Maven",
        "Gradle",
        "Jenkins",
        "NOSQL",
        "HBase",
        "Cassandra",
        "MongoDB",
        "DynamoDB",
        "IDE",
        "Tools",
        "Eclipse",
        "Net",
        "Beans",
        "Intellij",
        "Modelling",
        "Tools",
        "Rational",
        "Rose",
        "Star",
        "UML",
        "paradigm",
        "UML",
        "Architecture",
        "Relational",
        "DBMS",
        "ClientServer",
        "Architecture",
        "Cloud",
        "Platforms",
        "Cloud",
        "BI",
        "Tools",
        "Tableau",
        "Operating",
        "System",
        "Windows",
        "Vista",
        "UNIX",
        "Linux",
        "Ubuntu",
        "Mac",
        "OS",
        "X"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T20:55:39.090549",
    "resume_data": "Hadoop Developer Hadoop span lDeveloperspan Hadoop Developer Dell Austin TX Hadoop Developer with 8 Years of IT experience including 4 years in Big Data and Analytics field in Storage Querying Processing and Analysis for developing E2E Data pipelines Expertise in designing scalable Big Data solutions data warehouse models on largescale distributed data performing wide range of analytics Expertise in all components of HadoopSpark Ecosystems Spark Hive Pig Flume Sqoop HBase Kafka Oozie Impala Stream sets Apache NIFI Hue AWS 3 years of experience working in programming languages ScalaPython Extensive knowledge on data serialization techniques like Avro Sequence Files Parquet JSON and ORC Acute knowledge on Spark architecture and realtime streaming using Spark Hands on experience with Spark Core Spark SQL and Data FramesData SetsRDD API Good knowledge on Amazon Web Services AWS cloud services like EC2 S3 EMR and VPC Experienced in Data Ingestion Data Processing Data Aggregations Visualization in Spark Environment Hands on experience in working with large volume of Structured and UnStructured data Expert in migrating the code components from SVN repository to Bit Bucket repository Experienced in building Jenkins pipelines for continuous code integration from Github into Linux machine Experience in Object Oriented Analysis Design OOAD and development Good understanding in endto end web applications and design patterns Hands on experience in application development using Java RDBMS and Linux shell scripting Experience in implementing by using agile methodology Well versed in using Software development methodologies like Agile Methodology and Waterfall processes Experienced in handling databases Netezza Oracle and Teradata Strong team player with good communication analytical presentation and interpersonal skills Work Experience Hadoop Developer Dell Austin TX US December 2017 to Present Dell EMC is the undisputed industry leader for unstructured data storage Tens of thousands of customers have bet their business on UDS products including Isilon ScaleOu t File Storage and ECS Object Storage Moreover Gartner has consistently recognized Dell EMC as the clear Magic Quadrant leader for Distributed File and Object Storage As a part of Data science team we are responsible for analyzing processing and visualizing one of our clients information to improve the business Responsibilities Worked on HortonworksHDP 25 distribution Involved in review of functional and nonfunctional requirements Responsible for designing and implementing the data pipeline using Big Data tools including Hive Spark Scala and Stream Sets Experience in using Apache Storm Spark Streaming Apache Spark Apache NiFi Kafka and Flume in creating data streaming solutions Developed and implemented Apache NIFI across various environments written QA scripts in Python for tracking files Involved in importing data from Microsoft SQL Server MySQL and Teradata into HDFS using Sqoop Good knowledge in using Apache NIFI to automate the data movement Developed Sqoop scripts to import data from relational sources and handled incremental loading Extensively used Stream Sets Data Collector to create ETL pipeline for pulling the data from RDBMS system to HDFS Implemented the data processing framework using Scala and Spark SQL Worked on implementing the performance optimization methods to improve the data processing timing Experienced in creating the shell scripts and made jobs automated Extensively worked on Data frames and Datasets using Spark and Spark SQL Responsible for defining the data flow within Hadoop eco system and direct the team in implement them and exported the result set from Hive to MySQL using Shell scripts Worked on Kafka Streaming using stream sets to process continuous integration of data from Oracle systems to hive tables Developed a generic utility in Spark for pulling the data from RDBMS system using multiple parallel connections Integrated existing code logic in HiveQL and implemented in the Spark application for data processing Extensively used HiveSpark optimization techniques like Partitioning Bucketing Map Join parallel execution Broadcast join and Repartitioning Environment Spark Python Scala Hive Hue UNIX Scripting Spark SQL Stream sets Kafka Impala Beeline Git Tidal Hadoop Developer InterAmerican Development Bank Washington DC US January 2016 to November 2017 A currency transaction report CTR is a report that US financial institutions are required to file with FinCEN for each deposit withdrawal exchange of currency or other payment and transfer by through or to the financial institution which involves a transaction in currency of more than 10000 Report is generated daily basis with T1 data with predefined rules and alerts Once report is generated with potential CTRs analysts will parse alerted CTRs to identify actual vs fractious CTRs vial Web interface Final consolidated report will be regenerated every night and sent to FINCEN in predefined FINCEN format Responsibilities Worked on HortonworksHDP 25distribution Experience in implementing Scala framework code using IntelliJ and UNIX scripting to implement the workflow for the jobs Involved in gathering business requirement analyze the use case and implement the use case end to end Worked closely with the Architect enhanced and optimized product Spark and Scala code to aggregate group and run data mining tasks using Spark framework Experienced in loading the raw data into RDDs and validate the data Experienced in converting the validated RDDs into Data frames for further processing Implemented the Spark SQL code logic to join multiple data frames to generate application specific aggregated results Experienced in fine tuning the jobs for better performance in the production cluster space Worked totally in agile methodologies used Rally scrum tool to track the User stories and Team performance Worked extensively in Impala Hue to analyze the processed data and to generate the end reports Experienced working with hive database through beeline Worked on analyzing and resolving the production job failures in several scenarios Implemented UNIXscripts to define the use case workflow and also to process the data files and automate the jobs Environment Spark Scala Hive Sqoop UNIX Scripting Spark SQL IntelliJ Hbase Kafka Impala Hue Beeline Git Hadoop Developer Atlanta Gas Light Atlanta GA US October 2014 to January 2016 Responsibilities Worked on Cloudera CDH distribution Hand on experience on cloud services like Amazon Web Services AWS Created data pipelines for different events to load the data from DynamoDB to AWS S3 bucket and then into HDFS location Involved in complete SDLC Requirement Analysis Development Testing and Deployment into Cluster Worked handinhand with the Architect enhanced and optimized product Spark code to aggregate group and run data mining tasks using Spark framework Extracted data from various SQL database sources into HDFS using Sqoopand also ran Hive scripts on the huge chunks of data Implemented a prototype for the complete requirements using Splunk python and Machine learning concepts Design and Implementation of Map reduce code logic for Natural Language Processing of Free Form Text Deployed the project on Amazon EMR with S3 Connectivity Implemented usage of Amazon EMR for processing Big Data across a Hadoop Cluster of virtual servers on Amazon Elastic Compute Cloud EC2 and Amazon Simple Storage ServiceS3 Loaded the data into Simple Storage Service S3 in the AWS Cloud Good Knowledge in using of Amazon Load Balancer for Auto scaling in EC2 servers Implemented Spark scripts to migrate map reduce jobs into Spark RDD transformations streaming data using Apache Kafka Implemented Spark SQL queries which intermix the Hive queries with the programmatic data manipulations supported by RDDs and data frames in scala and python Involved in Deployment of Code Logic and UDFsacross the cluster Communicate deliverables status to userstakeholders client and drive periodic review meetings Worked on Data Processing using Hive queries in HDFS and the shell Scripts to wrap the HQL scripts Developed and Deployed Oozie Workflows for recurring operations on Clusters Experienced in performance tuning of hadoop jobs for setting right Batch Interval time correct level of Parallelism and memory tuning Worked extensively with Sqoop for importing metadata from Oracle Used Tableau reporting tool to generate reports from the outputs stored in HDFS Environment Hadoop Spark HDFS Hive Map Reduce Sqoop Oozie Tableau Hadoop Developer Toyota Motors July 2012 to September 2014 Responsibilities Worked on Cloudera CDH distribution Design and Implement historical and incremental data ingestion techniques from multiple external systems using Hive pig and sqoop ingestion tools Design physical data models for structured and semistructured to validate the raw data into HDFS Design mapreduce logic and HIVES queries for generating aggregated metrics Involved in Design implementation development and testing phases in the project Responsible to monitor the jobs in production cluster while and trace the error logs when the jobs fails Design and Develop data migration logic for exporting data from MySQL to Hive Design and Develop complex workflow in Oozie for recurrent job execution Used SSRS reporting tool for the generation of data analysis reports Environment Hadoop MapReduce HDFS Pig Hive Oozie Eclipse Cloudera Sqoop SSRS Software Developer Garima Software Solutions November 2010 to June 2012 Responsibilities Involved in complete SDLC Requirement Analysis Development Testing and Deployments Involved in resolving critical Errors Responsible to deploy the deliverables of sprints successfully Involved in capturing the clients requirements and enhancements on the application document the requirements and populate to the associated teams Design and Implementation of REST Full services and WSDL in VORDEL Implemented complex SQL quires to get the analysis reports Created Desktop applications using J2EE Swings Involved in developing applications using Java JSP Servlets Swings Developed UI using HTML CSS Ajax JQuery and developed Business logic and Interfacing Components using Business Objects XML and JDBC Created applications connection pools deployment of JSP Servlets Used Oracle MySQL database for storing user information Developed backed for application using PHP for web applications Experienced with the Agile Methodologies Environment SOAP REST HTML WSDL 22Vordel SQL Developer Education Bachelors Skills DYNAMODB CASSANDRA AMBARI HDFS OOZIE Additional Information Technical Skills Bigdata Technologies HDFS Map Reduce Pig Hive Sqoop Oozie Scala Spark Kafka Flume Ambari Hue Hadoop Frameworks Cloudera CDHs Hortonworks HDPs MAPR Database Oracle 10g11g PLSQL MySQL MS SQL Server 2012 DB2 Language C C Java Scala Python AWS Components IAH S3 EMR EC2Lambda Route 53 Cloud Watch SNS Methodologies Agile Waterfall Build Tools Maven Gradle Jenkins NOSQL Databases HBase Cassandra MongoDB DynamoDB IDE Tools Eclipse Net Beans Intellij Modelling Tools Rational Rose Star UML Visual paradigm for UML Architecture Relational DBMS ClientServer Architecture Cloud Platforms AWS Cloud BI Tools Tableau Operating System Windows 7810 Vista UNIX Linux Ubuntu Mac OS X",
    "unique_id": "619b292c-955a-4301-ad13-31253f86659a"
}