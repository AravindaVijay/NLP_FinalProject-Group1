{
    "clean_data": "PYTHON DEVELOPER span lPYTHONspan span lDEVELOPERspan PYTHON DEVELOPER CAPITAL ONE 5 years of experience in Machine Learning Datamining with large datasets of Structured and Unstructured data Data Acquisition Data Validation Predictive modeling Data Visualization Experience in coding SQLPL SQL using Procedures Triggers and Packages Extensive experience in Text Analytics developing different Statistical Machine Learning Data Mining solutions to various business problems and generating data visualizations using R Python Excellent Knowledge of Relational Database Design Data WarehouseOLAP concepts and methodologies Data Driven and highly analytical with working knowledge and statistical model approaches and methodologies Clustering Regression analysis Hypothesis testing Decision trees Machine learning rules and everevolving regulatory environment Professional working experience in Machine Learning algorithms such as Linear Regression Logistic Regression Naive Bayes Decision Trees KMeans Clustering and Association Rules Experience with data visualization using tools like Ggplot Matplotlib Seaborn Tableau and using Tableau software to publish and presenting dashboards storyline on web and desktop platforms Experienced in python data manipulation for loading and extraction as well as with python libraries such as NumPy SciPy and Pandas for data analysis and numerical computations Well experienced in Normalization DeNormalization and Standardization techniques for optimal performance in relational and dimensional database environments Experience in multiple software tools and languages to provide datadriven analytical solutions to decision makers or research teams Familiar with predictive models using numeric and classification prediction algorithms like support vector machines and neural networks and ensemble methods like bagging boosting and random forest to improve the efficiency of the predictive model Good Knowledge of NoSQL databases like Mongo DB and HBase Develop maintain and teach new tools and methodologies related to data science and highperformance computing Extensive handson experience and high proficiency with structures semistructured and unstructured data using a broad range of data science programming languages and big data tools including R Python Spark SQL Scikit Learn Hadoop Map Reduce Expertise in Technical proficiency in Designing Data Modeling Online Application Solution Lead for Architecting Data WarehouseBusiness Intelligence Applications Cluster Analysis Principal Component Analysis PCA Association Rules Recommender Systems Strong experience in Software Development Life Cycle SDLC including Requirements Analysis Design Specification and Testing as per Cycle in both Waterfall and Agile methodologies Adept in statistical programming languages like R and Python including Big Data technologies like Hadoop Hive Hands on experience with RStudio for doing data preprocessing and building machine learning algorithms on different datasets Worked and extracted data from various database sources like Oracle SQL Server and DB2 Implemented machine learning algorithms on large datasets to understand hidden patterns and capture insights Predictive Modelling Algorithms Logistic Regression Linear Regression Decision Trees KNearest Neighbors Bootstrap Aggregation Bagging Naive Bayes Classifier Random Forests Boosting Support Vector Machines Flexible with UnixLinux and Windows Environments working with Operating Systems like Centos56 Ubuntu1314 Cosmos Work Experience PYTHON DEVELOPER CAPITAL ONE McLean VA July 2018 to Present Responsibilities Implemented Machine Learning Computer Vision Deep Learning and Neural Networks algorithms using TensorFlow and designed Prediction Model using Data Mining Techniques with help of Python and Libraries like NumPy SciPy Matplotlib Pandas scikitlearn Used pandas NumPy Seaborn SciPy matplotlib scikitlearn in Python for developing various machine learning algorithms Used scikitlearn in modeling various classification regression and clustering algorithms including support vector machines random forests gradient boosting kmeans Worked on different data formats such as JSON XML and performed machine learning algorithms in Python Worked as Data Architects and IT Architects to understand the movement of data and its storage and ER Studio 97 Participated in all phases of data mining data collection data cleaning developing models validation visualization and performed Gap analysis Data Manipulation and Aggregation from a different source using Nexus Toad Business Objects Powerball and Smart View Good knowledge of Hadoop Architecture and various components such as HDFS Job Tracker Task Tracker Name Node Data Node Secondary Name Node and Map Reduce concepts Programmed by a utility in Python that used multiple packages SciPy NumPy pandas Implemented Classification using supervised algorithms like Logistic Regression Decision trees KNN Naive Bayes Responsible for design and development of advanced R Python programs to prepare to transform and harmonize data sets in preparation for modeling Worked on different data formats such as JSON XML and performed machine learning algorithms in Python Worked as Data Architects and IT Architects to understand the movement of data and its storage and ER Studio 97 Participated in all phases of data mining data collection data cleaning developing models validation visualization and performed Gap analysis Data Manipulation and Aggregation from a different source using Nexus Toad Business Objects Power BI and Smart View Updated Python scripts to match training data with our database stored in AWS Cloud Search so that we would be able to assign each document a response label for further classification Data transformation from various resources data organization features extraction from raw and stored Handled importing data from various data sources performed transformations using Hive Map Reduce and loaded data into HDFS Interaction with Business Analyst SMEs and other Data Architects to understand Business needs and functionality for various project solutions Researched evaluated architected and deployed new tools frameworks and patterns to build sustainable Big Data platforms for the clients Updated Python scripts to match training data with our database stored in AWS Cloud Search so that we would be able to assign each document a response label for further classification Data transformation from various resources data organization features extraction from raw and stored Handled importing data from various data sources performed transformations using Hive Map Reduce and loaded data into HDFS Identifying and executing process improvements handson in various technologies such as Oracle Informatica and Business Objects Designed both 3NF data models for ODS OLTP systems and dimensional data models using Star and Snowflake Schemas Environment Erwin r96 Python SQL Oracle 12c SQL Server SSRS PLSQL TSQL Tableau MLlib regression Cluster analysis Scala NLP Spark Kafka Mongo DB logistic regression Hadoop PySpark Teradata random forest OLAP Azure MariaDB SAP CRM HDFS ODS NLTK SVM JSON Tableau XML Cassandra MapReduce AWS DATA SCIENTISTMACHINE LEARNING RESEARCH ASSISTANT COX AUTO Atlanta GA July 2016 to June 2018 Responsibilities Utilized Spark Scala Hadoop HQL VQL oozie pySpark Data Lake TensorFlow HBase Cassandra Redshift Mongo DB Kafka Kinesis Spark Streaming Edward CUDA MLLib AWS Python a broad variety of machine learning methods including classifications regressions dimensionally reduction etc Application of various machine learning algorithms and statistical modeling like decision trees text analytics natural language processing NLP supervised and unsupervised regression models social network analysis neural networks deep learning SVM clustering to identify Volume using scikitlearn package in python Mat lab Used the version control tools like Git 2X and build tools like Apache MavenAnt Worked on analyzing data from Google Analytics Ad Words and Facebook etc Evaluated models using Cross Validation Log loss function ROC curves and used AUC for feature selection and elastic technologies like Elastic Search Kibana Used Python scripts to update content in the database and manipulate files Skilled in using dplyr and pandas in R and Python for performing exploratory data analysis Performed Multinomial Logistic Regression Decision Tree Random forest SVM to classify package is going to deliver on time for the new route Used Jenkins for Continuous Integration Builds and deployments CICD Performed data analysis by using Hive to retrieve the data from Hadoop cluster Sql to retrieve data from Oracle database and used ETL for data transformation Performed Data Cleaning features scaling features engineering using pandas and numpy packages in python Developed SparkScala R Python for regular expression regex project in the HadoopHive environment with LinuxWindows for big data resources Used clustering technique KMeans to identify outliers and to classify unlabeled data Tracking operations using sensors until certain criteria is met using Air Flow technology Responsible for different Data mapping activities from Source systems to Teradata using utilities like TPump FEXP BTEQ MLOAD FLOAD etc CICD pipeline implementation for Java applications CICD implementation on Azure cloud platform Ensured that the model has low False Positive Rate and Text classification and sentiment analysis for unstructured and semistructured data Addressed over fitting by implementing of the algorithm regularization methods like L1 and L2 Used Principal Component Analysis in feature engineering to analyze high dimensional data Used MLlib Sparks Machine learning library to build and evaluate different models Implemented rule based expertise system from the results of exploratory analysis and information gathered from the people from different departments Created Data Quality Scripts using SQL and Hive to validate successful data load and quality of the data Created various types of data visualizations using Python and Tableau Communicated the results with operations team for taking best decisions Collected data needs and requirements by Interacting with the other departments Environment R Python HDFS ODS OLTP Oracle 10g Hive OLAP DB2 Metadata MS Excel Mainframes MS Vision MapReduce Rational Rose SQL and MongoDB DATA ANALYSTDATA SCIENTIST Bengaluru Karnataka May 2015 to June 2016 Responsibilities Worked closely with data mapping SME and QA team to understand the business rules for acceptable data quality standards Created and provided support on various monitoring and control reports which includes Customer verification report that accepted offer in sales engine AMF waiver report Credit fulfillment report qualification and offer load volume reconciliation report and upgrade performance monitoring report Wrote complex SQL queries to identify granularity issues and relationships between data sets and created recommended solutions based on analysis of the query results Wrote the SQL queries on data staging tables and data warehouse tables to validate the data results Performed data profiling on datasets with millions of rows on Teradata environment validating key gen elements ensuring correctness of codes and identifiers and recommending mapping changes Performed unit testing on transformation rules to ensure data moved correctly Created Python scripts to take client content documents and images as input and create web pages including home page table of contents and links Involved in full life cycle of Business Objects reporting Application Worked directly with Cloud System Administrators and project managers supporting Amazon Web Services AWS migration Delivered Enterprise Data Governance Data Quality Metadata and ETL Informatica solution Maintained Excel workbooks such as development of pivot tables exporting data from external SQL databases producing reports and updating spreadsheet information Used Pythons Panda library in the process of analyzing the data Involved in extracting and transforming of Data from Enterprise data warehousing Actively involved in communication with Business Analyst user acceptance testers and BPM for requirements gathering Engaged in performing various adhoc queries Implanted various Data transformation such as SQL extract Split and data validation Analyzing and supporting day to day marketing strategy solutions Researched and fixed data issues pointed out by QA team during regression tests Interfaced with business users to verify business rules and communicated changes to ETL development team Created Tableau views with complex calculations and hierarchies making it possible to analyze and obtain insights into large data sets Created and executed SQL queries to perform Data Integrity testing on a Teradata Database to validate and test data using TOAD Worked with data architects team to make appropriate changes to the data models Worked on the ETL Informatica mappings and other ETL Processes Data Warehouse Worked with the data governance team to ensure the data quality of compliance reports for EDI transactions Utilized Tableau server to publish and share the reports with the business users Experienced in designing complex DrillDown amp DrillThrough Reports using Business Objects Experienced in creating UNIX scripts for file transfer and file manipulation Generated adhoc or management specific reports using Tableau and Excel Analyzed the subscriber provider members and claim data to continuously scan and create authoritative master data Prepare the data rules spreadsheet using MS Excel that will be used to update allowed values findings and profiling results Performed auditing in the development phase in order to assure data quality and integrity Provided insight and ideas in support of customer management processes and reporting Involved in data cleansing mechanism in order to eliminate duplicate and inaccurate data Environment Windows 7 Linux Tableau desktop Tableau Server Business Objects R SQL Developer MySQL MSAccess MS Excel and SQL DATA ANALYST INTERN AMARA RAJA Tirupati Andhra Pradesh April 2014 to April 2015 Responsibilities Worked with Data governance Data quality data lineage Data architect to design various models and processes Implemented endtoend systems for Data Analytics Data Automation and integrated with custom visualization tools using Informatica Tableau and business objects Designed developed tested and maintained Tableau functional reports based on user requirements Mastered the ability to design and deploy rich Graphic visualizations using Tableau and Converted existing Business objects reports into tableau dashboards Interaction with Business Analyst SMEs and other Data Architects to understand Business needs and functionality for various project solutions Used Informatica Power Center for ETL extraction transformation and loading data from heterogeneous source systems into target database Created mappings using Designer and extracted data from various sources transformed data according to the requirement Involved in extracting the data from the Flat Files and Relational databases into staging area Developed Informatica Mappings and Reusable Transformations to facilitate timely Loading of Data of a star schema Developed the Informatica Mappings by usage of Aggregator SQL overrides usage in Lookups source filter usage in Source qualifiers and data flow management into multiple targets using Router Created Sessions and extracted data from various sources transformed data according to the requirement and loading into data warehouse Used various transformations like Filter Expression Sequence Generator Update Strategy Joiner Router and Aggregator to create robust mappings in the Informatica Power Center Designer Imported various heterogeneous files using Informatica Power Center  Source Analyzer Developed several reusable transformations that were used in other mappings Prepared Technical Design documents and Test cases Involved in Unit Testing and Resolution of various Bottlenecks came across Environment SASBase SASConnect SASUNIX SASODS SASMacros SQL Tableau MS Excel Power Point Mainframe DB2 Teradata SAS Enterprise guide Education BE Honors in Electronics and Communication Engineering Jawaharlal Nehru Technological University",
    "entities": [
        "Apache MavenAnt Worked",
        "Statistical Machine Learning Data Mining",
        "Python Worked as Data Architects",
        "Oracle SQL Server",
        "Windows Environments",
        "HBase Develop",
        "AUC",
        "Relational",
        "Customer",
        "UNIX",
        "SQL Oracle",
        "Researched",
        "ETL Processes Data Warehouse Worked",
        "Flat Files",
        "Requirements Analysis Design Specification",
        "Amazon Web Services AWS",
        "Operating Systems",
        "Hadoop",
        "Nehru Technological University",
        "Atlanta",
        "Informatica Power Center for ETL",
        "Informatica Tableau",
        "Tableau Server Business",
        "Software Development Life Cycle SDLC",
        "Oracle Informatica",
        "Principal Component Analysis",
        "Designing Data Modeling Online Application Solution Lead for Architecting Data WarehouseBusiness Intelligence Applications Cluster Analysis Principal Component Analysis PCA Association",
        "Nexus Toad Business Objects Powerball",
        "Air Flow technology Responsible",
        "Python",
        "Performed Data Cleaning",
        "KNN Naive Bayes Responsible",
        "Data Visualization Experience",
        "Created Tableau",
        "Business Analyst",
        "Neural Networks",
        "Present Responsibilities Implemented Machine Learning Computer Vision Deep Learning",
        "Implemented Classification",
        "ODS NLTK",
        "SME",
        "Created Python",
        "Clustering Regression analysis Hypothesis",
        "Data from Enterprise",
        "MS Excel",
        "Tableau Communicated",
        "Waterfall",
        "Linear Regression Logistic Regression Naive Bayes Decision Trees KMeans Clustering and Association Rules",
        "Mastered",
        "SQL DATA",
        "Filter Expression Sequence Generator Update Strategy Joiner Router",
        "Unstructured",
        "Provided",
        "Source",
        "Hadoop Hive Hands",
        "Lookups",
        "ER Studio",
        "HDFS Interaction with Business Analyst",
        "ROC",
        "McLean VA",
        "Mainframes MS Vision MapReduce Rational Rose SQL",
        "MySQL MSAccess MS Excel",
        "ETL Informatica",
        "Performed Multinomial Logistic Regression Decision Tree Random",
        "UnixLinux",
        "Cloud System Administrators",
        "LinuxWindows",
        "QA",
        "Data Architects",
        "Created",
        "Machine Learning Datamining",
        "Hadoop Architecture",
        "Informatica Power Center  Source",
        "Oracle",
        "Text Analytics",
        "Updated Python",
        "Star and Snowflake Schemas Environment Erwin",
        "Sql",
        "HDFS Job Tracker Task Tracker",
        "Test cases Involved",
        "Google Analytics Ad Words",
        "Data Acquisition Data Validation Predictive",
        "ODS OLTP",
        "SQL",
        "Metadata MS",
        "NLP",
        "RStudio",
        "Big Data",
        "Hive",
        "CICD",
        "Nexus Toad Business Objects Power BI",
        "Normalization DeNormalization and Standardization",
        "Pandas",
        "Router Created Sessions",
        "ETL",
        "Utilized Tableau",
        "CAPITAL",
        "Prepared Technical Design",
        "Performed",
        "EDI",
        "TPump",
        "Logistic Regression Decision",
        "BPM",
        "Data Analytics Data Automation",
        "Created Data Quality Scripts",
        "Developed Informatica Mappings",
        "Business Objects",
        "Data Integrity",
        "Interaction with Business Analyst",
        "Tirupati Andhra Pradesh",
        "Structured",
        "Data Driven",
        "Tableau",
        "the Informatica Mappings",
        "Machine Learning",
        "TOAD",
        "SAP CRM",
        "Maintained Excel",
        "SVM",
        "Node",
        "Designer",
        "Cross Validation Log",
        "JSON XML"
    ],
    "experience": "Experience in coding SQLPL SQL using Procedures Triggers and Packages Extensive experience in Text Analytics developing different Statistical Machine Learning Data Mining solutions to various business problems and generating data visualizations using R Python Excellent Knowledge of Relational Database Design Data WarehouseOLAP concepts and methodologies Data Driven and highly analytical with working knowledge and statistical model approaches and methodologies Clustering Regression analysis Hypothesis testing Decision trees Machine learning rules and everevolving regulatory environment Professional working experience in Machine Learning algorithms such as Linear Regression Logistic Regression Naive Bayes Decision Trees KMeans Clustering and Association Rules Experience with data visualization using tools like Ggplot Matplotlib Seaborn Tableau and using Tableau software to publish and presenting dashboards storyline on web and desktop platforms Experienced in python data manipulation for loading and extraction as well as with python libraries such as NumPy SciPy and Pandas for data analysis and numerical computations Well experienced in Normalization DeNormalization and Standardization techniques for optimal performance in relational and dimensional database environments Experience in multiple software tools and languages to provide datadriven analytical solutions to decision makers or research teams Familiar with predictive models using numeric and classification prediction algorithms like support vector machines and neural networks and ensemble methods like bagging boosting and random forest to improve the efficiency of the predictive model Good Knowledge of NoSQL databases like Mongo DB and HBase Develop maintain and teach new tools and methodologies related to data science and highperformance computing Extensive handson experience and high proficiency with structures semistructured and unstructured data using a broad range of data science programming languages and big data tools including R Python Spark SQL Scikit Learn Hadoop Map Reduce Expertise in Technical proficiency in Designing Data Modeling Online Application Solution Lead for Architecting Data WarehouseBusiness Intelligence Applications Cluster Analysis Principal Component Analysis PCA Association Rules Recommender Systems Strong experience in Software Development Life Cycle SDLC including Requirements Analysis Design Specification and Testing as per Cycle in both Waterfall and Agile methodologies Adept in statistical programming languages like R and Python including Big Data technologies like Hadoop Hive Hands on experience with RStudio for doing data preprocessing and building machine learning algorithms on different datasets Worked and extracted data from various database sources like Oracle SQL Server and DB2 Implemented machine learning algorithms on large datasets to understand hidden patterns and capture insights Predictive Modelling Algorithms Logistic Regression Linear Regression Decision Trees KNearest Neighbors Bootstrap Aggregation Bagging Naive Bayes Classifier Random Forests Boosting Support Vector Machines Flexible with UnixLinux and Windows Environments working with Operating Systems like Centos56 Ubuntu1314 Cosmos Work Experience PYTHON DEVELOPER CAPITAL ONE McLean VA July 2018 to Present Responsibilities Implemented Machine Learning Computer Vision Deep Learning and Neural Networks algorithms using TensorFlow and designed Prediction Model using Data Mining Techniques with help of Python and Libraries like NumPy SciPy Matplotlib Pandas scikitlearn Used pandas NumPy Seaborn SciPy matplotlib scikitlearn in Python for developing various machine learning algorithms Used scikitlearn in modeling various classification regression and clustering algorithms including support vector machines random forests gradient boosting kmeans Worked on different data formats such as JSON XML and performed machine learning algorithms in Python Worked as Data Architects and IT Architects to understand the movement of data and its storage and ER Studio 97 Participated in all phases of data mining data collection data cleaning developing models validation visualization and performed Gap analysis Data Manipulation and Aggregation from a different source using Nexus Toad Business Objects Powerball and Smart View Good knowledge of Hadoop Architecture and various components such as HDFS Job Tracker Task Tracker Name Node Data Node Secondary Name Node and Map Reduce concepts Programmed by a utility in Python that used multiple packages SciPy NumPy pandas Implemented Classification using supervised algorithms like Logistic Regression Decision trees KNN Naive Bayes Responsible for design and development of advanced R Python programs to prepare to transform and harmonize data sets in preparation for modeling Worked on different data formats such as JSON XML and performed machine learning algorithms in Python Worked as Data Architects and IT Architects to understand the movement of data and its storage and ER Studio 97 Participated in all phases of data mining data collection data cleaning developing models validation visualization and performed Gap analysis Data Manipulation and Aggregation from a different source using Nexus Toad Business Objects Power BI and Smart View Updated Python scripts to match training data with our database stored in AWS Cloud Search so that we would be able to assign each document a response label for further classification Data transformation from various resources data organization features extraction from raw and stored Handled importing data from various data sources performed transformations using Hive Map Reduce and loaded data into HDFS Interaction with Business Analyst SMEs and other Data Architects to understand Business needs and functionality for various project solutions Researched evaluated architected and deployed new tools frameworks and patterns to build sustainable Big Data platforms for the clients Updated Python scripts to match training data with our database stored in AWS Cloud Search so that we would be able to assign each document a response label for further classification Data transformation from various resources data organization features extraction from raw and stored Handled importing data from various data sources performed transformations using Hive Map Reduce and loaded data into HDFS Identifying and executing process improvements handson in various technologies such as Oracle Informatica and Business Objects Designed both 3NF data models for ODS OLTP systems and dimensional data models using Star and Snowflake Schemas Environment Erwin r96 Python SQL Oracle 12c SQL Server SSRS PLSQL TSQL Tableau MLlib regression Cluster analysis Scala NLP Spark Kafka Mongo DB logistic regression Hadoop PySpark Teradata random forest OLAP Azure MariaDB SAP CRM HDFS ODS NLTK SVM JSON Tableau XML Cassandra MapReduce AWS DATA SCIENTISTMACHINE LEARNING RESEARCH ASSISTANT COX AUTO Atlanta GA July 2016 to June 2018 Responsibilities Utilized Spark Scala Hadoop HQL VQL oozie pySpark Data Lake TensorFlow HBase Cassandra Redshift Mongo DB Kafka Kinesis Spark Streaming Edward CUDA MLLib AWS Python a broad variety of machine learning methods including classifications regressions dimensionally reduction etc Application of various machine learning algorithms and statistical modeling like decision trees text analytics natural language processing NLP supervised and unsupervised regression models social network analysis neural networks deep learning SVM clustering to identify Volume using scikitlearn package in python Mat lab Used the version control tools like Git 2X and build tools like Apache MavenAnt Worked on analyzing data from Google Analytics Ad Words and Facebook etc Evaluated models using Cross Validation Log loss function ROC curves and used AUC for feature selection and elastic technologies like Elastic Search Kibana Used Python scripts to update content in the database and manipulate files Skilled in using dplyr and pandas in R and Python for performing exploratory data analysis Performed Multinomial Logistic Regression Decision Tree Random forest SVM to classify package is going to deliver on time for the new route Used Jenkins for Continuous Integration Builds and deployments CICD Performed data analysis by using Hive to retrieve the data from Hadoop cluster Sql to retrieve data from Oracle database and used ETL for data transformation Performed Data Cleaning features scaling features engineering using pandas and numpy packages in python Developed SparkScala R Python for regular expression regex project in the HadoopHive environment with LinuxWindows for big data resources Used clustering technique KMeans to identify outliers and to classify unlabeled data Tracking operations using sensors until certain criteria is met using Air Flow technology Responsible for different Data mapping activities from Source systems to Teradata using utilities like TPump FEXP BTEQ MLOAD FLOAD etc CICD pipeline implementation for Java applications CICD implementation on Azure cloud platform Ensured that the model has low False Positive Rate and Text classification and sentiment analysis for unstructured and semistructured data Addressed over fitting by implementing of the algorithm regularization methods like L1 and L2 Used Principal Component Analysis in feature engineering to analyze high dimensional data Used MLlib Sparks Machine learning library to build and evaluate different models Implemented rule based expertise system from the results of exploratory analysis and information gathered from the people from different departments Created Data Quality Scripts using SQL and Hive to validate successful data load and quality of the data Created various types of data visualizations using Python and Tableau Communicated the results with operations team for taking best decisions Collected data needs and requirements by Interacting with the other departments Environment R Python HDFS ODS OLTP Oracle 10 g Hive OLAP DB2 Metadata MS Excel Mainframes MS Vision MapReduce Rational Rose SQL and MongoDB DATA ANALYSTDATA SCIENTIST Bengaluru Karnataka May 2015 to June 2016 Responsibilities Worked closely with data mapping SME and QA team to understand the business rules for acceptable data quality standards Created and provided support on various monitoring and control reports which includes Customer verification report that accepted offer in sales engine AMF waiver report Credit fulfillment report qualification and offer load volume reconciliation report and upgrade performance monitoring report Wrote complex SQL queries to identify granularity issues and relationships between data sets and created recommended solutions based on analysis of the query results Wrote the SQL queries on data staging tables and data warehouse tables to validate the data results Performed data profiling on datasets with millions of rows on Teradata environment validating key gen elements ensuring correctness of codes and identifiers and recommending mapping changes Performed unit testing on transformation rules to ensure data moved correctly Created Python scripts to take client content documents and images as input and create web pages including home page table of contents and links Involved in full life cycle of Business Objects reporting Application Worked directly with Cloud System Administrators and project managers supporting Amazon Web Services AWS migration Delivered Enterprise Data Governance Data Quality Metadata and ETL Informatica solution Maintained Excel workbooks such as development of pivot tables exporting data from external SQL databases producing reports and updating spreadsheet information Used Pythons Panda library in the process of analyzing the data Involved in extracting and transforming of Data from Enterprise data warehousing Actively involved in communication with Business Analyst user acceptance testers and BPM for requirements gathering Engaged in performing various adhoc queries Implanted various Data transformation such as SQL extract Split and data validation Analyzing and supporting day to day marketing strategy solutions Researched and fixed data issues pointed out by QA team during regression tests Interfaced with business users to verify business rules and communicated changes to ETL development team Created Tableau views with complex calculations and hierarchies making it possible to analyze and obtain insights into large data sets Created and executed SQL queries to perform Data Integrity testing on a Teradata Database to validate and test data using TOAD Worked with data architects team to make appropriate changes to the data models Worked on the ETL Informatica mappings and other ETL Processes Data Warehouse Worked with the data governance team to ensure the data quality of compliance reports for EDI transactions Utilized Tableau server to publish and share the reports with the business users Experienced in designing complex DrillDown amp DrillThrough Reports using Business Objects Experienced in creating UNIX scripts for file transfer and file manipulation Generated adhoc or management specific reports using Tableau and Excel Analyzed the subscriber provider members and claim data to continuously scan and create authoritative master data Prepare the data rules spreadsheet using MS Excel that will be used to update allowed values findings and profiling results Performed auditing in the development phase in order to assure data quality and integrity Provided insight and ideas in support of customer management processes and reporting Involved in data cleansing mechanism in order to eliminate duplicate and inaccurate data Environment Windows 7 Linux Tableau desktop Tableau Server Business Objects R SQL Developer MySQL MSAccess MS Excel and SQL DATA ANALYST INTERN AMARA RAJA Tirupati Andhra Pradesh April 2014 to April 2015 Responsibilities Worked with Data governance Data quality data lineage Data architect to design various models and processes Implemented endtoend systems for Data Analytics Data Automation and integrated with custom visualization tools using Informatica Tableau and business objects Designed developed tested and maintained Tableau functional reports based on user requirements Mastered the ability to design and deploy rich Graphic visualizations using Tableau and Converted existing Business objects reports into tableau dashboards Interaction with Business Analyst SMEs and other Data Architects to understand Business needs and functionality for various project solutions Used Informatica Power Center for ETL extraction transformation and loading data from heterogeneous source systems into target database Created mappings using Designer and extracted data from various sources transformed data according to the requirement Involved in extracting the data from the Flat Files and Relational databases into staging area Developed Informatica Mappings and Reusable Transformations to facilitate timely Loading of Data of a star schema Developed the Informatica Mappings by usage of Aggregator SQL overrides usage in Lookups source filter usage in Source qualifiers and data flow management into multiple targets using Router Created Sessions and extracted data from various sources transformed data according to the requirement and loading into data warehouse Used various transformations like Filter Expression Sequence Generator Update Strategy Joiner Router and Aggregator to create robust mappings in the Informatica Power Center Designer Imported various heterogeneous files using Informatica Power Center   Source Analyzer Developed several reusable transformations that were used in other mappings Prepared Technical Design documents and Test cases Involved in Unit Testing and Resolution of various Bottlenecks came across Environment SASBase SASConnect SASUNIX SASODS SASMacros SQL Tableau MS Excel Power Point Mainframe DB2 Teradata SAS Enterprise guide Education BE Honors in Electronics and Communication Engineering Jawaharlal Nehru Technological University",
    "extracted_keywords": [
        "PYTHON",
        "DEVELOPER",
        "lPYTHONspan",
        "span",
        "lDEVELOPERspan",
        "PYTHON",
        "DEVELOPER",
        "CAPITAL",
        "years",
        "experience",
        "Machine",
        "Learning",
        "Datamining",
        "datasets",
        "Structured",
        "Unstructured",
        "data",
        "Data",
        "Acquisition",
        "Data",
        "Validation",
        "Predictive",
        "Data",
        "Visualization",
        "Experience",
        "SQLPL",
        "SQL",
        "Procedures",
        "Triggers",
        "Packages",
        "experience",
        "Text",
        "Analytics",
        "Machine",
        "Learning",
        "Data",
        "Mining",
        "solutions",
        "business",
        "problems",
        "data",
        "visualizations",
        "R",
        "Python",
        "Excellent",
        "Knowledge",
        "Relational",
        "Database",
        "Design",
        "Data",
        "concepts",
        "methodologies",
        "Data",
        "Driven",
        "knowledge",
        "model",
        "approaches",
        "methodologies",
        "Clustering",
        "Regression",
        "analysis",
        "Hypothesis",
        "testing",
        "Decision",
        "trees",
        "Machine",
        "rules",
        "environment",
        "Professional",
        "working",
        "experience",
        "Machine",
        "Learning",
        "Linear",
        "Regression",
        "Logistic",
        "Regression",
        "Naive",
        "Bayes",
        "Decision",
        "Trees",
        "KMeans",
        "Clustering",
        "Association",
        "Rules",
        "Experience",
        "data",
        "visualization",
        "tools",
        "Ggplot",
        "Matplotlib",
        "Seaborn",
        "Tableau",
        "Tableau",
        "software",
        "dashboards",
        "storyline",
        "web",
        "desktop",
        "platforms",
        "python",
        "data",
        "manipulation",
        "loading",
        "extraction",
        "python",
        "libraries",
        "NumPy",
        "SciPy",
        "Pandas",
        "data",
        "analysis",
        "computations",
        "Normalization",
        "DeNormalization",
        "Standardization",
        "techniques",
        "performance",
        "database",
        "Experience",
        "software",
        "tools",
        "languages",
        "solutions",
        "decision",
        "makers",
        "research",
        "teams",
        "models",
        "classification",
        "prediction",
        "support",
        "vector",
        "machines",
        "networks",
        "methods",
        "forest",
        "efficiency",
        "model",
        "Good",
        "Knowledge",
        "NoSQL",
        "Mongo",
        "DB",
        "HBase",
        "Develop",
        "tools",
        "methodologies",
        "data",
        "science",
        "highperformance",
        "handson",
        "experience",
        "proficiency",
        "structures",
        "data",
        "range",
        "data",
        "science",
        "programming",
        "languages",
        "data",
        "tools",
        "R",
        "Python",
        "Spark",
        "SQL",
        "Scikit",
        "Learn",
        "Hadoop",
        "Map",
        "Reduce",
        "Expertise",
        "proficiency",
        "Designing",
        "Data",
        "Modeling",
        "Online",
        "Application",
        "Solution",
        "Lead",
        "Data",
        "WarehouseBusiness",
        "Intelligence",
        "Applications",
        "Cluster",
        "Analysis",
        "Principal",
        "Component",
        "Analysis",
        "PCA",
        "Association",
        "Rules",
        "Recommender",
        "Systems",
        "Strong",
        "experience",
        "Software",
        "Development",
        "Life",
        "Cycle",
        "SDLC",
        "Requirements",
        "Analysis",
        "Design",
        "Specification",
        "Testing",
        "Cycle",
        "Waterfall",
        "methodologies",
        "Adept",
        "programming",
        "languages",
        "R",
        "Python",
        "Big",
        "Data",
        "technologies",
        "Hadoop",
        "Hive",
        "Hands",
        "experience",
        "RStudio",
        "data",
        "machine",
        "algorithms",
        "datasets",
        "data",
        "database",
        "sources",
        "Oracle",
        "SQL",
        "Server",
        "DB2",
        "machine",
        "algorithms",
        "datasets",
        "patterns",
        "capture",
        "insights",
        "Predictive",
        "Modelling",
        "Algorithms",
        "Logistic",
        "Regression",
        "Linear",
        "Regression",
        "Decision",
        "Trees",
        "KNearest",
        "Neighbors",
        "Bootstrap",
        "Aggregation",
        "Bagging",
        "Naive",
        "Bayes",
        "Classifier",
        "Random",
        "Forests",
        "Support",
        "Vector",
        "Machines",
        "Flexible",
        "UnixLinux",
        "Windows",
        "Environments",
        "Operating",
        "Systems",
        "Centos56",
        "Ubuntu1314",
        "Cosmos",
        "Work",
        "Experience",
        "PYTHON",
        "DEVELOPER",
        "CAPITAL",
        "McLean",
        "VA",
        "July",
        "Present",
        "Responsibilities",
        "Machine",
        "Learning",
        "Computer",
        "Vision",
        "Deep",
        "Learning",
        "Neural",
        "Networks",
        "TensorFlow",
        "Prediction",
        "Model",
        "Data",
        "Mining",
        "Techniques",
        "help",
        "Python",
        "Libraries",
        "NumPy",
        "SciPy",
        "Matplotlib",
        "Pandas",
        "pandas",
        "NumPy",
        "Seaborn",
        "SciPy",
        "matplotlib",
        "Python",
        "machine",
        "algorithms",
        "classification",
        "regression",
        "algorithms",
        "support",
        "vector",
        "machines",
        "forests",
        "kmeans",
        "data",
        "formats",
        "JSON",
        "XML",
        "machine",
        "algorithms",
        "Python",
        "Worked",
        "Data",
        "Architects",
        "IT",
        "Architects",
        "movement",
        "data",
        "storage",
        "ER",
        "Studio",
        "phases",
        "data",
        "mining",
        "data",
        "collection",
        "data",
        "models",
        "validation",
        "visualization",
        "analysis",
        "Data",
        "Manipulation",
        "Aggregation",
        "source",
        "Nexus",
        "Toad",
        "Business",
        "Powerball",
        "Smart",
        "View",
        "knowledge",
        "Hadoop",
        "Architecture",
        "components",
        "HDFS",
        "Job",
        "Tracker",
        "Task",
        "Tracker",
        "Name",
        "Node",
        "Data",
        "Node",
        "Secondary",
        "Name",
        "Node",
        "Map",
        "Reduce",
        "concepts",
        "utility",
        "Python",
        "packages",
        "SciPy",
        "NumPy",
        "Classification",
        "algorithms",
        "Logistic",
        "Regression",
        "Decision",
        "KNN",
        "Naive",
        "Bayes",
        "Responsible",
        "design",
        "development",
        "R",
        "Python",
        "programs",
        "data",
        "sets",
        "preparation",
        "data",
        "formats",
        "JSON",
        "XML",
        "machine",
        "algorithms",
        "Python",
        "Worked",
        "Data",
        "Architects",
        "IT",
        "Architects",
        "movement",
        "data",
        "storage",
        "ER",
        "Studio",
        "phases",
        "data",
        "mining",
        "data",
        "collection",
        "data",
        "models",
        "validation",
        "visualization",
        "analysis",
        "Data",
        "Manipulation",
        "Aggregation",
        "source",
        "Nexus",
        "Toad",
        "Business",
        "Objects",
        "Power",
        "BI",
        "Smart",
        "View",
        "Python",
        "scripts",
        "training",
        "data",
        "database",
        "AWS",
        "Cloud",
        "Search",
        "document",
        "response",
        "label",
        "classification",
        "Data",
        "transformation",
        "resources",
        "data",
        "organization",
        "extraction",
        "data",
        "data",
        "sources",
        "transformations",
        "Hive",
        "Map",
        "Reduce",
        "data",
        "HDFS",
        "Interaction",
        "Business",
        "Analyst",
        "SMEs",
        "Data",
        "Architects",
        "Business",
        "needs",
        "functionality",
        "project",
        "solutions",
        "tools",
        "frameworks",
        "patterns",
        "Big",
        "Data",
        "platforms",
        "clients",
        "Python",
        "scripts",
        "training",
        "data",
        "database",
        "AWS",
        "Cloud",
        "Search",
        "document",
        "response",
        "label",
        "classification",
        "Data",
        "transformation",
        "resources",
        "data",
        "organization",
        "extraction",
        "data",
        "data",
        "sources",
        "transformations",
        "Hive",
        "Map",
        "Reduce",
        "data",
        "HDFS",
        "process",
        "improvements",
        "handson",
        "technologies",
        "Oracle",
        "Informatica",
        "Business",
        "Objects",
        "data",
        "models",
        "ODS",
        "OLTP",
        "systems",
        "data",
        "models",
        "Star",
        "Snowflake",
        "Schemas",
        "Environment",
        "Erwin",
        "Python",
        "SQL",
        "Oracle",
        "12c",
        "SQL",
        "Server",
        "SSRS",
        "PLSQL",
        "TSQL",
        "Tableau",
        "MLlib",
        "regression",
        "Cluster",
        "analysis",
        "Scala",
        "NLP",
        "Spark",
        "Kafka",
        "Mongo",
        "DB",
        "regression",
        "Hadoop",
        "PySpark",
        "Teradata",
        "forest",
        "OLAP",
        "Azure",
        "SAP",
        "CRM",
        "HDFS",
        "ODS",
        "NLTK",
        "SVM",
        "JSON",
        "Tableau",
        "XML",
        "Cassandra",
        "MapReduce",
        "AWS",
        "DATA",
        "SCIENTISTMACHINE",
        "LEARNING",
        "RESEARCH",
        "ASSISTANT",
        "COX",
        "AUTO",
        "Atlanta",
        "GA",
        "July",
        "June",
        "Responsibilities",
        "Spark",
        "Scala",
        "Hadoop",
        "HQL",
        "VQL",
        "oozie",
        "pySpark",
        "Data",
        "Lake",
        "TensorFlow",
        "HBase",
        "Cassandra",
        "Redshift",
        "Mongo",
        "DB",
        "Kafka",
        "Kinesis",
        "Spark",
        "Streaming",
        "Edward",
        "CUDA",
        "AWS",
        "Python",
        "variety",
        "machine",
        "learning",
        "methods",
        "classifications",
        "regressions",
        "reduction",
        "Application",
        "machine",
        "algorithms",
        "modeling",
        "decision",
        "trees",
        "text",
        "analytics",
        "language",
        "processing",
        "NLP",
        "regression",
        "models",
        "network",
        "analysis",
        "networks",
        "SVM",
        "Volume",
        "package",
        "python",
        "Mat",
        "lab",
        "version",
        "control",
        "tools",
        "Git",
        "2X",
        "tools",
        "Apache",
        "MavenAnt",
        "data",
        "Google",
        "Analytics",
        "Ad",
        "Words",
        "Facebook",
        "models",
        "Cross",
        "Validation",
        "Log",
        "loss",
        "function",
        "ROC",
        "curves",
        "AUC",
        "feature",
        "selection",
        "technologies",
        "Elastic",
        "Search",
        "Kibana",
        "Python",
        "scripts",
        "content",
        "database",
        "manipulate",
        "files",
        "dplyr",
        "pandas",
        "R",
        "Python",
        "data",
        "analysis",
        "Performed",
        "Multinomial",
        "Logistic",
        "Regression",
        "Decision",
        "Tree",
        "Random",
        "forest",
        "SVM",
        "package",
        "time",
        "route",
        "Jenkins",
        "Continuous",
        "Integration",
        "Builds",
        "deployments",
        "CICD",
        "data",
        "analysis",
        "Hive",
        "data",
        "Hadoop",
        "cluster",
        "Sql",
        "data",
        "Oracle",
        "database",
        "ETL",
        "data",
        "transformation",
        "Performed",
        "Data",
        "Cleaning",
        "features",
        "engineering",
        "pandas",
        "packages",
        "python",
        "Developed",
        "SparkScala",
        "R",
        "Python",
        "expression",
        "regex",
        "project",
        "environment",
        "LinuxWindows",
        "data",
        "resources",
        "clustering",
        "technique",
        "KMeans",
        "outliers",
        "data",
        "Tracking",
        "operations",
        "sensors",
        "criteria",
        "Air",
        "Flow",
        "technology",
        "Data",
        "mapping",
        "activities",
        "Source",
        "systems",
        "Teradata",
        "utilities",
        "TPump",
        "FEXP",
        "BTEQ",
        "MLOAD",
        "CICD",
        "pipeline",
        "implementation",
        "Java",
        "applications",
        "CICD",
        "implementation",
        "cloud",
        "platform",
        "model",
        "False",
        "Positive",
        "Rate",
        "Text",
        "classification",
        "sentiment",
        "analysis",
        "data",
        "algorithm",
        "regularization",
        "methods",
        "L1",
        "L2",
        "Principal",
        "Component",
        "Analysis",
        "feature",
        "engineering",
        "data",
        "MLlib",
        "Sparks",
        "Machine",
        "library",
        "models",
        "rule",
        "expertise",
        "system",
        "results",
        "analysis",
        "information",
        "people",
        "departments",
        "Created",
        "Data",
        "Quality",
        "Scripts",
        "SQL",
        "Hive",
        "data",
        "load",
        "quality",
        "data",
        "types",
        "data",
        "visualizations",
        "Python",
        "Tableau",
        "results",
        "operations",
        "team",
        "decisions",
        "data",
        "needs",
        "requirements",
        "departments",
        "Environment",
        "R",
        "Python",
        "HDFS",
        "ODS",
        "OLTP",
        "Oracle",
        "g",
        "Hive",
        "OLAP",
        "DB2",
        "Metadata",
        "MS",
        "Excel",
        "Mainframes",
        "MS",
        "Vision",
        "MapReduce",
        "Rational",
        "Rose",
        "SQL",
        "MongoDB",
        "DATA",
        "ANALYSTDATA",
        "SCIENTIST",
        "Bengaluru",
        "Karnataka",
        "June",
        "Responsibilities",
        "data",
        "mapping",
        "SME",
        "QA",
        "team",
        "business",
        "rules",
        "data",
        "quality",
        "standards",
        "support",
        "monitoring",
        "control",
        "reports",
        "Customer",
        "verification",
        "report",
        "offer",
        "sales",
        "engine",
        "AMF",
        "waiver",
        "Credit",
        "fulfillment",
        "report",
        "qualification",
        "load",
        "volume",
        "reconciliation",
        "report",
        "performance",
        "monitoring",
        "report",
        "Wrote",
        "SQL",
        "granularity",
        "issues",
        "relationships",
        "data",
        "sets",
        "solutions",
        "analysis",
        "query",
        "results",
        "SQL",
        "data",
        "tables",
        "data",
        "warehouse",
        "tables",
        "data",
        "data",
        "profiling",
        "datasets",
        "millions",
        "rows",
        "Teradata",
        "environment",
        "gen",
        "elements",
        "correctness",
        "codes",
        "identifiers",
        "mapping",
        "changes",
        "Performed",
        "unit",
        "testing",
        "transformation",
        "rules",
        "data",
        "Python",
        "scripts",
        "client",
        "content",
        "documents",
        "images",
        "input",
        "web",
        "pages",
        "home",
        "page",
        "table",
        "contents",
        "links",
        "life",
        "cycle",
        "Business",
        "Objects",
        "Application",
        "Cloud",
        "System",
        "Administrators",
        "project",
        "managers",
        "Amazon",
        "Web",
        "Services",
        "AWS",
        "migration",
        "Enterprise",
        "Data",
        "Governance",
        "Data",
        "Quality",
        "Metadata",
        "ETL",
        "Informatica",
        "solution",
        "Excel",
        "workbooks",
        "development",
        "tables",
        "data",
        "SQL",
        "reports",
        "spreadsheet",
        "information",
        "Pythons",
        "Panda",
        "library",
        "process",
        "data",
        "transforming",
        "Data",
        "Enterprise",
        "data",
        "warehousing",
        "communication",
        "Business",
        "Analyst",
        "user",
        "acceptance",
        "testers",
        "BPM",
        "requirements",
        "queries",
        "Data",
        "transformation",
        "SQL",
        "Split",
        "data",
        "validation",
        "Analyzing",
        "day",
        "day",
        "marketing",
        "strategy",
        "solutions",
        "data",
        "issues",
        "QA",
        "team",
        "regression",
        "tests",
        "business",
        "users",
        "business",
        "rules",
        "changes",
        "ETL",
        "development",
        "team",
        "Tableau",
        "calculations",
        "hierarchies",
        "insights",
        "data",
        "sets",
        "SQL",
        "queries",
        "Data",
        "Integrity",
        "testing",
        "Teradata",
        "Database",
        "test",
        "data",
        "TOAD",
        "Worked",
        "data",
        "architects",
        "team",
        "changes",
        "data",
        "models",
        "ETL",
        "Informatica",
        "mappings",
        "ETL",
        "Processes",
        "Data",
        "Warehouse",
        "data",
        "governance",
        "team",
        "data",
        "quality",
        "compliance",
        "reports",
        "EDI",
        "transactions",
        "Tableau",
        "server",
        "reports",
        "business",
        "users",
        "DrillDown",
        "amp",
        "DrillThrough",
        "Reports",
        "Business",
        "Objects",
        "UNIX",
        "scripts",
        "file",
        "transfer",
        "file",
        "manipulation",
        "adhoc",
        "management",
        "reports",
        "Tableau",
        "Excel",
        "subscriber",
        "provider",
        "members",
        "data",
        "master",
        "data",
        "data",
        "rules",
        "MS",
        "Excel",
        "values",
        "findings",
        "profiling",
        "results",
        "Performed",
        "auditing",
        "development",
        "phase",
        "order",
        "data",
        "quality",
        "integrity",
        "insight",
        "ideas",
        "support",
        "customer",
        "management",
        "processes",
        "data",
        "cleansing",
        "mechanism",
        "order",
        "data",
        "Environment",
        "Windows",
        "Linux",
        "Tableau",
        "desktop",
        "Tableau",
        "Server",
        "Business",
        "R",
        "SQL",
        "Developer",
        "MySQL",
        "MSAccess",
        "MS",
        "Excel",
        "SQL",
        "DATA",
        "ANALYST",
        "INTERN",
        "AMARA",
        "RAJA",
        "Tirupati",
        "Andhra",
        "Pradesh",
        "April",
        "April",
        "Responsibilities",
        "Data",
        "governance",
        "Data",
        "quality",
        "data",
        "lineage",
        "Data",
        "architect",
        "models",
        "processes",
        "systems",
        "Data",
        "Analytics",
        "Data",
        "Automation",
        "custom",
        "visualization",
        "tools",
        "Informatica",
        "Tableau",
        "business",
        "objects",
        "Tableau",
        "reports",
        "user",
        "requirements",
        "ability",
        "visualizations",
        "Tableau",
        "Converted",
        "Business",
        "reports",
        "tableau",
        "dashboards",
        "Interaction",
        "Business",
        "Analyst",
        "SMEs",
        "Data",
        "Architects",
        "Business",
        "needs",
        "functionality",
        "project",
        "solutions",
        "Informatica",
        "Power",
        "Center",
        "ETL",
        "extraction",
        "transformation",
        "loading",
        "data",
        "source",
        "systems",
        "target",
        "database",
        "mappings",
        "Designer",
        "data",
        "sources",
        "data",
        "requirement",
        "data",
        "Flat",
        "Files",
        "Relational",
        "databases",
        "staging",
        "area",
        "Developed",
        "Informatica",
        "Mappings",
        "Reusable",
        "Transformations",
        "Loading",
        "Data",
        "star",
        "schema",
        "Informatica",
        "Mappings",
        "usage",
        "Aggregator",
        "SQL",
        "usage",
        "Lookups",
        "source",
        "filter",
        "usage",
        "Source",
        "qualifiers",
        "data",
        "flow",
        "management",
        "targets",
        "Router",
        "Created",
        "Sessions",
        "data",
        "sources",
        "data",
        "requirement",
        "loading",
        "data",
        "warehouse",
        "transformations",
        "Filter",
        "Expression",
        "Sequence",
        "Generator",
        "Update",
        "Strategy",
        "Joiner",
        "Router",
        "Aggregator",
        "mappings",
        "Informatica",
        "Power",
        "Center",
        "Designer",
        "files",
        "Informatica",
        "Power",
        "Center",
        "Source",
        "Analyzer",
        "transformations",
        "mappings",
        "Prepared",
        "Technical",
        "Design",
        "documents",
        "Test",
        "cases",
        "Unit",
        "Testing",
        "Resolution",
        "Bottlenecks",
        "Environment",
        "SASBase",
        "SASUNIX",
        "SASODS",
        "SASMacros",
        "SQL",
        "Tableau",
        "MS",
        "Excel",
        "Power",
        "Point",
        "Mainframe",
        "DB2",
        "Teradata",
        "SAS",
        "Enterprise",
        "guide",
        "Education",
        "Honors",
        "Electronics",
        "Communication",
        "Engineering",
        "Jawaharlal",
        "Nehru",
        "Technological",
        "University"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T20:32:52.931880",
    "resume_data": "PYTHON DEVELOPER span lPYTHONspan span lDEVELOPERspan PYTHON DEVELOPER CAPITAL ONE 5 years of experience in Machine Learning Datamining with large datasets of Structured and Unstructured data Data Acquisition Data Validation Predictive modeling Data Visualization Experience in coding SQLPL SQL using Procedures Triggers and Packages Extensive experience in Text Analytics developing different Statistical Machine Learning Data Mining solutions to various business problems and generating data visualizations using R Python Excellent Knowledge of Relational Database Design Data WarehouseOLAP concepts and methodologies Data Driven and highly analytical with working knowledge and statistical model approaches and methodologies Clustering Regression analysis Hypothesis testing Decision trees Machine learning rules and everevolving regulatory environment Professional working experience in Machine Learning algorithms such as Linear Regression Logistic Regression Naive Bayes Decision Trees KMeans Clustering and Association Rules Experience with data visualization using tools like Ggplot Matplotlib Seaborn Tableau and using Tableau software to publish and presenting dashboards storyline on web and desktop platforms Experienced in python data manipulation for loading and extraction as well as with python libraries such as NumPy SciPy and Pandas for data analysis and numerical computations Well experienced in Normalization DeNormalization and Standardization techniques for optimal performance in relational and dimensional database environments Experience in multiple software tools and languages to provide datadriven analytical solutions to decision makers or research teams Familiar with predictive models using numeric and classification prediction algorithms like support vector machines and neural networks and ensemble methods like bagging boosting and random forest to improve the efficiency of the predictive model Good Knowledge of NoSQL databases like Mongo DB and HBase Develop maintain and teach new tools and methodologies related to data science and highperformance computing Extensive handson experience and high proficiency with structures semistructured and unstructured data using a broad range of data science programming languages and big data tools including R Python Spark SQL Scikit Learn Hadoop Map Reduce Expertise in Technical proficiency in Designing Data Modeling Online Application Solution Lead for Architecting Data WarehouseBusiness Intelligence Applications Cluster Analysis Principal Component Analysis PCA Association Rules Recommender Systems Strong experience in Software Development Life Cycle SDLC including Requirements Analysis Design Specification and Testing as per Cycle in both Waterfall and Agile methodologies Adept in statistical programming languages like R and Python including Big Data technologies like Hadoop Hive Hands on experience with RStudio for doing data preprocessing and building machine learning algorithms on different datasets Worked and extracted data from various database sources like Oracle SQL Server and DB2 Implemented machine learning algorithms on large datasets to understand hidden patterns and capture insights Predictive Modelling Algorithms Logistic Regression Linear Regression Decision Trees KNearest Neighbors Bootstrap Aggregation Bagging Naive Bayes Classifier Random Forests Boosting Support Vector Machines Flexible with UnixLinux and Windows Environments working with Operating Systems like Centos56 Ubuntu1314 Cosmos Work Experience PYTHON DEVELOPER CAPITAL ONE McLean VA July 2018 to Present Responsibilities Implemented Machine Learning Computer Vision Deep Learning and Neural Networks algorithms using TensorFlow and designed Prediction Model using Data Mining Techniques with help of Python and Libraries like NumPy SciPy Matplotlib Pandas scikitlearn Used pandas NumPy Seaborn SciPy matplotlib scikitlearn in Python for developing various machine learning algorithms Used scikitlearn in modeling various classification regression and clustering algorithms including support vector machines random forests gradient boosting kmeans Worked on different data formats such as JSON XML and performed machine learning algorithms in Python Worked as Data Architects and IT Architects to understand the movement of data and its storage and ER Studio 97 Participated in all phases of data mining data collection data cleaning developing models validation visualization and performed Gap analysis Data Manipulation and Aggregation from a different source using Nexus Toad Business Objects Powerball and Smart View Good knowledge of Hadoop Architecture and various components such as HDFS Job Tracker Task Tracker Name Node Data Node Secondary Name Node and Map Reduce concepts Programmed by a utility in Python that used multiple packages SciPy NumPy pandas Implemented Classification using supervised algorithms like Logistic Regression Decision trees KNN Naive Bayes Responsible for design and development of advanced R Python programs to prepare to transform and harmonize data sets in preparation for modeling Worked on different data formats such as JSON XML and performed machine learning algorithms in Python Worked as Data Architects and IT Architects to understand the movement of data and its storage and ER Studio 97 Participated in all phases of data mining data collection data cleaning developing models validation visualization and performed Gap analysis Data Manipulation and Aggregation from a different source using Nexus Toad Business Objects Power BI and Smart View Updated Python scripts to match training data with our database stored in AWS Cloud Search so that we would be able to assign each document a response label for further classification Data transformation from various resources data organization features extraction from raw and stored Handled importing data from various data sources performed transformations using Hive Map Reduce and loaded data into HDFS Interaction with Business Analyst SMEs and other Data Architects to understand Business needs and functionality for various project solutions Researched evaluated architected and deployed new tools frameworks and patterns to build sustainable Big Data platforms for the clients Updated Python scripts to match training data with our database stored in AWS Cloud Search so that we would be able to assign each document a response label for further classification Data transformation from various resources data organization features extraction from raw and stored Handled importing data from various data sources performed transformations using Hive Map Reduce and loaded data into HDFS Identifying and executing process improvements handson in various technologies such as Oracle Informatica and Business Objects Designed both 3NF data models for ODS OLTP systems and dimensional data models using Star and Snowflake Schemas Environment Erwin r96 Python SQL Oracle 12c SQL Server SSRS PLSQL TSQL Tableau MLlib regression Cluster analysis Scala NLP Spark Kafka Mongo DB logistic regression Hadoop PySpark Teradata random forest OLAP Azure MariaDB SAP CRM HDFS ODS NLTK SVM JSON Tableau XML Cassandra MapReduce AWS DATA SCIENTISTMACHINE LEARNING RESEARCH ASSISTANT COX AUTO Atlanta GA July 2016 to June 2018 Responsibilities Utilized Spark Scala Hadoop HQL VQL oozie pySpark Data Lake TensorFlow HBase Cassandra Redshift Mongo DB Kafka Kinesis Spark Streaming Edward CUDA MLLib AWS Python a broad variety of machine learning methods including classifications regressions dimensionally reduction etc Application of various machine learning algorithms and statistical modeling like decision trees text analytics natural language processing NLP supervised and unsupervised regression models social network analysis neural networks deep learning SVM clustering to identify Volume using scikitlearn package in python Mat lab Used the version control tools like Git 2X and build tools like Apache MavenAnt Worked on analyzing data from Google Analytics Ad Words and Facebook etc Evaluated models using Cross Validation Log loss function ROC curves and used AUC for feature selection and elastic technologies like Elastic Search Kibana Used Python scripts to update content in the database and manipulate files Skilled in using dplyr and pandas in R and Python for performing exploratory data analysis Performed Multinomial Logistic Regression Decision Tree Random forest SVM to classify package is going to deliver on time for the new route Used Jenkins for Continuous Integration Builds and deployments CICD Performed data analysis by using Hive to retrieve the data from Hadoop cluster Sql to retrieve data from Oracle database and used ETL for data transformation Performed Data Cleaning features scaling features engineering using pandas and numpy packages in python Developed SparkScala R Python for regular expression regex project in the HadoopHive environment with LinuxWindows for big data resources Used clustering technique KMeans to identify outliers and to classify unlabeled data Tracking operations using sensors until certain criteria is met using Air Flow technology Responsible for different Data mapping activities from Source systems to Teradata using utilities like TPump FEXP BTEQ MLOAD FLOAD etc CICD pipeline implementation for Java applications CICD implementation on Azure cloud platform Ensured that the model has low False Positive Rate and Text classification and sentiment analysis for unstructured and semistructured data Addressed over fitting by implementing of the algorithm regularization methods like L1 and L2 Used Principal Component Analysis in feature engineering to analyze high dimensional data Used MLlib Sparks Machine learning library to build and evaluate different models Implemented rule based expertise system from the results of exploratory analysis and information gathered from the people from different departments Created Data Quality Scripts using SQL and Hive to validate successful data load and quality of the data Created various types of data visualizations using Python and Tableau Communicated the results with operations team for taking best decisions Collected data needs and requirements by Interacting with the other departments Environment R Python HDFS ODS OLTP Oracle 10g Hive OLAP DB2 Metadata MS Excel Mainframes MS Vision MapReduce Rational Rose SQL and MongoDB DATA ANALYSTDATA SCIENTIST Bengaluru Karnataka May 2015 to June 2016 Responsibilities Worked closely with data mapping SME and QA team to understand the business rules for acceptable data quality standards Created and provided support on various monitoring and control reports which includes Customer verification report that accepted offer in sales engine AMF waiver report Credit fulfillment report qualification and offer load volume reconciliation report and upgrade performance monitoring report Wrote complex SQL queries to identify granularity issues and relationships between data sets and created recommended solutions based on analysis of the query results Wrote the SQL queries on data staging tables and data warehouse tables to validate the data results Performed data profiling on datasets with millions of rows on Teradata environment validating key gen elements ensuring correctness of codes and identifiers and recommending mapping changes Performed unit testing on transformation rules to ensure data moved correctly Created Python scripts to take client content documents and images as input and create web pages including home page table of contents and links Involved in full life cycle of Business Objects reporting Application Worked directly with Cloud System Administrators and project managers supporting Amazon Web Services AWS migration Delivered Enterprise Data Governance Data Quality Metadata and ETL Informatica solution Maintained Excel workbooks such as development of pivot tables exporting data from external SQL databases producing reports and updating spreadsheet information Used Pythons Panda library in the process of analyzing the data Involved in extracting and transforming of Data from Enterprise data warehousing Actively involved in communication with Business Analyst user acceptance testers and BPM for requirements gathering Engaged in performing various adhoc queries Implanted various Data transformation such as SQL extract Split and data validation Analyzing and supporting day to day marketing strategy solutions Researched and fixed data issues pointed out by QA team during regression tests Interfaced with business users to verify business rules and communicated changes to ETL development team Created Tableau views with complex calculations and hierarchies making it possible to analyze and obtain insights into large data sets Created and executed SQL queries to perform Data Integrity testing on a Teradata Database to validate and test data using TOAD Worked with data architects team to make appropriate changes to the data models Worked on the ETL Informatica mappings and other ETL Processes Data Warehouse Worked with the data governance team to ensure the data quality of compliance reports for EDI transactions Utilized Tableau server to publish and share the reports with the business users Experienced in designing complex DrillDown amp DrillThrough Reports using Business Objects Experienced in creating UNIX scripts for file transfer and file manipulation Generated adhoc or management specific reports using Tableau and Excel Analyzed the subscriber provider members and claim data to continuously scan and create authoritative master data Prepare the data rules spreadsheet using MS Excel that will be used to update allowed values findings and profiling results Performed auditing in the development phase in order to assure data quality and integrity Provided insight and ideas in support of customer management processes and reporting Involved in data cleansing mechanism in order to eliminate duplicate and inaccurate data Environment Windows 7 Linux Tableau desktop Tableau Server Business Objects R SQL Developer MySQL MSAccess MS Excel and SQL DATA ANALYST INTERN AMARA RAJA Tirupati Andhra Pradesh April 2014 to April 2015 Responsibilities Worked with Data governance Data quality data lineage Data architect to design various models and processes Implemented endtoend systems for Data Analytics Data Automation and integrated with custom visualization tools using Informatica Tableau and business objects Designed developed tested and maintained Tableau functional reports based on user requirements Mastered the ability to design and deploy rich Graphic visualizations using Tableau and Converted existing Business objects reports into tableau dashboards Interaction with Business Analyst SMEs and other Data Architects to understand Business needs and functionality for various project solutions Used Informatica Power Center for ETL extraction transformation and loading data from heterogeneous source systems into target database Created mappings using Designer and extracted data from various sources transformed data according to the requirement Involved in extracting the data from the Flat Files and Relational databases into staging area Developed Informatica Mappings and Reusable Transformations to facilitate timely Loading of Data of a star schema Developed the Informatica Mappings by usage of Aggregator SQL overrides usage in Lookups source filter usage in Source qualifiers and data flow management into multiple targets using Router Created Sessions and extracted data from various sources transformed data according to the requirement and loading into data warehouse Used various transformations like Filter Expression Sequence Generator Update Strategy Joiner Router and Aggregator to create robust mappings in the Informatica Power Center Designer Imported various heterogeneous files using Informatica Power Center 8x Source Analyzer Developed several reusable transformations that were used in other mappings Prepared Technical Design documents and Test cases Involved in Unit Testing and Resolution of various Bottlenecks came across Environment SASBase SASConnect SASUNIX SASODS SASMacros SQL Tableau MS Excel Power Point Mainframe DB2 Teradata SAS Enterprise guide Education BE Honors in Electronics and Communication Engineering Jawaharlal Nehru Technological University",
    "unique_id": "0a2085f0-0378-4a69-8d47-95cd844b8ac8"
}