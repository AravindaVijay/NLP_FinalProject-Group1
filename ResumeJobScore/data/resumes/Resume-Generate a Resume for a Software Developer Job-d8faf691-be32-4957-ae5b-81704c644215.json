{
    "clean_data": "Data Scientist Consultant Data Scientist Consultant Data scientist Birmingham AL 8 years of IT experience which includes Machine Learning Data mining with large datasets of Structured and Unstructured data Data Acquisition Data Validation Predictive modeling Data Visualization A deep understanding of Statistical Modeling Multivariate Analysis Big data analytics and Standard Procedures Highly efficient in Dimensionality Reduction methods such as PCA Principal component Analysis Factor Analysis etc Implemented bootstrapping methods such as Random Forests Classification KMeans Clustering KNN NaiveBayes SVM Decision Tree BFS Linear and Logistic Regression Methods The experience of working in text understanding classification pattern recognition recommendation systems targeting systems and ranking systems using Python Experience with Natural Language Processing NLP Proficiency in application of statistical prediction modeling machine learning classification techniques and econometric forecasting techniques Indepth knowledge of statistical procedures that are applied in Supervised Unsupervised problems Experience in the application of Neural Network Support Vector Machines SVM and Random Forest Experienced in working with advanced analytical teams to design build validate and refresh data models that enable the next generation of sophisticated solutions for global clients Extensively worked on Python 3527 NumPy Pandas Matplotlib NLTK and Scikit learn Extensive experience in Text Analytics developing different Statistical Machine Learning Data Mining solutions to various business problems and generating data visualizations using R Python and Tableau Experience in implementing data analysis with various analytic tools such as Anaconda 40 Jupyter Notebook 4X R 30ggplot2 and Excel Experience in designing star schema Snowflake schema for Data Warehouse ODS architecture Hands on experience in business understanding data understanding and preparation of large databases Experience in working with relational databases MySQL Oracle with advanced SQL programming skills Expertise in transforming business requirements into analytical models designing algorithms building models developing data mining and reporting solutions that scales across massive volume of structured and unstructured data Experience in using various packages in Rand pythonlike ggplot2 caret dplyr Rweka gmodels RCurl tm C50 twitter NLP Reshape2 rjson dplyr pandas NumPy Seaborn SciPy Matplotlib Scikitlearn Beautiful Soup Rpy2 Extensive experience in Text Analytics generating data visualizations using R Python and creating dashboards using tools like Tableau Hands on experience with big data tools like Hadoop Spark Hive Pig Impala PySpark Spark SQL Experience in using one or more cloud computing frameworks such as AWS Azure Google Cloud etc Mapping and tracing data from system to system in order to establish data hierarchy and lineage Experience with distributed datacomputing tools MapReduce Hadoop Hive Spark MySQL Worked on Tableau to create dashboards and visualizations Proficiency in various type of optimization Market Mix modeling Segmentation Time Series Price Promo models etc Identifiescreates the appropriate algorithm to discover patterns validate their findings using an experimental and iterative approach Strong skills in statistical methodologies such as AB testExperiment design Hypothesis test ANOVA Crosstabs Ttests and Correlation Techniques Applies advanced statistical and predictive modeling techniques to build maintain and improve on multiple realtime decision systems Experience in designing stunning visualizations using Tableau software and publishing and presenting dashboards Storyline on web and desktop platforms Proficiency in SAS Base SAS Enterprise Guide Enterprise Miner Excellent communication skills verbal and written to communicate with clients and team prepare deliver effective presentations Strong experience in Software Development Life Cycle SDLC including Requirements Analysis Design Specification and Testing as per Cycle in both Waterfall and Agile methodologies Strong experience in interacting with stakeholderscustomers gathering requirements through interviews workshops and existing system documentation or procedures defining business processes identifying and analyzing risks using appropriate templates and analysis tools Closely works with product managers Service development managers and product development team in productizing the algorithms developed Work Experience Data Scientist Consultant BBVA Compass Birmingham AL August 2018 to Present Description BBVA Compass understands that every individual and company has unique dreams and ambitions needs and wants We realize that few take the same path in the faster busier and more complex world we live in We get it Whichever path you choose and whenever you need us we want to create opportunities for your bright future From the smallest moment to the largest personal or professional life event BBVA Compass is there for you Responsibilities Performed Data Profiling to learn about behavior with various features such as traffic pattern location and time Date and Time etc Extensively used Pythons multiple data science packages like Pandas NumPy Matplotlib SciPy Scikit learn and NLTK Worked on data cleaning data preparation and feature engineering with Python 3X In depth understanding of NLP for tokenization lemmatization and stemming using NLTK Spacy Pattern libraries Experience in evaluating and proposing solutions to NLP problems through various approaches Deep background in information retrieval OCR SpeechtoText etc Natural Language Processing NLP knowledge representation or computational linguistics Understanding of NLP techniques for text representation semantic extraction techniques data structures and modeling Experience in Java Python and NLPMLframeworks and libraries Worked to apply a broad array of capabilities spanning machine learning statistics  and modeling to extract insights to structured and unstructured healthcare data sources preclinical clinical trial and complementary real world information streams to apply a broad array of capabilities spanning machine learning statistics  and modeling to extract insights to structured and unstructured healthcare data sources preclinical clinical trial and complementary real world information streams Developed User Defined Functions in Python for rapid analysis and performed data imputation using Scikit learnpackage in Python Developed the applications models used appropriate algorithms for arriving at the required insights by analyzing business requirements Worked with unsupervised Kmeans DBSCAN and supervised learning techniques Regression Classification for feature engineering and did Principal Component Analysis for dimensionality reduction of features Used the Classification machine learning algorithms Nave Bayes Linear regression Logistic regression SVM Neural Networks and used Clustering Algorithm K Means Performed text classification task using NLTK package and implemented various natural language processing techniques Worked collaboratively with senior management to develop strategy and approach to defining business challenges to be answered by data science Established partnerships with product and engineering teams and work closely with other teams Used SparkStreamingAPIs to perform necessary transformations and actions on the fly for building the common learner data model which gets the data from Kafka in near real time Worked on end to end pipe line in Spark and on Apache spark for analyzing the live streaming data Worked on Spark Python modules for machine learning and predictive analytics in Spark on AWS Explored and analyzed the customer specific features by using SparkSQL Created Hive scripts to create external internal data tables on Hive Worked on creating datasets to load data into HIVE Worked on Tableau for data visualization to create reports dashboards for insights and business process improvement Created the dashboards and reports in tableau for visualizing the data in required format Environment Spark Apache Spark Hive Machine learning Python NumPy NLTK Pandas SciPy MySQL Tableau Sqoop HBase HDFS Tableau  Mongo DB SQL Server and ETL Data Scientist Visa Austin TX May 2017 to July 2018 Description Visa is a dynamic global enterprise and innovation is at the heart of everything we do Were looking for smart ambitious people to join our Austin team We are a global payments technology company working to enable consumers businesses banks and governments to use digital currency Responsibilities Analyzed Pandas NumPy seaborn SciPy Matplotlib Scikitlearn NLTK in Python for developing various machine learning algorithms and utilized machine learning algorithms such as linear regression multivariate regression naive Bayes Random Forests Kmeans KNN for data analysis Demonstrated experience in design and implementation of Statistical models Predictive models enterprise data model metadata solution and data life cycle management in both RDBMS Big Data environments Utilized domain knowledge and application portfolio knowledge to play a key role in defining the future state of large business technology programs Developed MapReduceSparkPython modules for machine learning predictive analytics in Hadoop on AWS Implemented a Pythonbased distributed random forest via Python streaming Performed Source System Analysis database design data modeling for the warehouse layer using MLDM concepts and package layer using Dimensional modeling Created ecosystem models eg conceptual logical physical canonical that are required for supporting services within the enterprise data architecture conceptual data model for defining the major subject areas used ecosystem logical model for defining standard business meaning for entities and fields and an ecosystem canonical model for defining the standard messages and formats to be used in data integration services throughout the ecosystem Developed LINUXShell scripts by using NZSQLNZLOAD utilities to load data from flat files to Netezza database Designed and implemented system architecture for AmazonEC2 based cloudhosted solution for client Tested Complex ETL Mappings and Sessions based on business user requirements and business rules to load data from source flat files and RDBMS tables to target tables Hands on database design relational integrity constraints OLAP OLTP Cubes and Normalization 3NF and Denormalization of database Developed MapReduceSpark Python modules for machine learning predictive analytics in Hadoop on AWS Worked on customer segmentation using an unsupervised learning technique clustering Worked with various Teradata15 tools and utilities like TeradataViewpoint Multi Load ARC TeradataAdministrator BTEQ and other Teradata Utilities Utilized Spark Scala Hadoop HBase Kafka SparkStreaming MLlib Python a broad variety of machine learning methods including classifications regressions dimensionally reduction Analyzed large data sets apply machine learning techniques and develop predictive models statistical models and developing and enhancing statistical models by leveraging bestinclass modeling techniques Environment Erwin r96 Python SQL Oracle 12c Netezza SQL Server Informatica Java SSRS PLSQL TSQL Tableau MLlib regression Cluster analysis Scala NLP Cassandra Map Reduce Spark Kafka Mongo DB logistic regression Hadoop Hive Teradata0 random forest OLAP Azure Maria DB SAP CRM HDFS ODS NLTK SVM JSON Tableau XML AWS Data Scientist Black Tree Healthcare Consulting King of Prussia PA January 2016 to April 2017 Description Black Tree Healthcare Consulting provides revenue cycle outsourcing and clinical consulting services to the home health hospice and skilled nursing industries The primary goal of the project was to determine the most efficient inputoutput ratio through data modeling and analysis thus providing the best resource combination Responsibilities Incident and problem management coordinating resolution of data movement disruptions Capacity analysis and planning for a sizable application server environment Performance metering and tuning for application servers database monitors and alerts Risk identification and evaluation managing and improving upon internal controls which mitigate risks Working with development teams to identify enhancement opportunities and resolve code defects Code deployment and configuration management Coordination and execution of changes within a complex testing environment Data collection procedures enhancement to include information that is relevant for building analytic systems processing cleansing and verifying the integrity of data used for analysis Building analytic models using a variety of techniques such as logistic regression risk scorecards and pattern recognition technologies Analyze and understand large amounts of data to determine suitability for use in models and then work to segment the data create variables build models and test those models Excellent understanding of machine learning techniques and algorithms such as Logistic Regression SVM Random Forests Deep Learning etc In depth understanding and experience in NLP and Deep learning Expertise in Sentiment Analysis Entity Extraction Document Classification Topic Modeling Natural Language Understanding NLU and Natural Language Generation NLG Leads all NLPNLG driven solutions for the project Design NLP models for searching structuredunstructured data in realnearreal time Interacts regularly with product team members ensuring successful integration of NLP solutions in the product architecture Keeps the solutions updated with the recent developments in NLPNLG Perform a proper EDA Univariateand bivariate analysis to understand the intrinsic effectcombined Performing adhoc analysis and presenting results in a clear manner and constant tracking of model performance Worked with Data governance Data quality Data lineage Data architect to design various models Designed data models and data flow diagrams using Erwin and MS Visio Independently coded new programs and designed Tables to load and test the program effectively for the given POCs Developed Implemented maintained the Conceptual Logical Physical Data Models using Erwin for ForwardReverse Engineered Databases Experience with common data science toolkits such as R Python Spark etc Applied statistics skills such as statistical sampling testing regression etc Work with technical and development teams to deploy models Build Model Performance Reports and Designing Technical Documentation to support each of the models for the product line Exploratory Data Analysis and Data Visualizations using R and Tableau Established Data architecture strategy best practices standards and roadmaps Lead the development and presentation of a data analytics datahub prototype with the help of the other members of the emerging solutions team Involved in analysis of Business requirement Design and Development of High level and Lowlevel designs Unit and Integration testing Interacted with the other departments to understand and identify data needs and requirements Worked with several R packages including knitr dplyr SparkR Causal Infer space time Environment R 3x UNIX Python 352 MLLib SAS regression logistic regression Hadoop NoSQL MySQL OLTP Random forest OLAP HDFS ODS Data Analyst Grey Hound Dallas TX March 2014 to December 2015 Description Greyhound is an intercity bus common carrier serving over 3800 destinations across North America Behavior of users on our website generate large amount of data which challenges our ability to gather parse analyze and infer from it So a major task is to implement new capabilities for reporting from existing data warehouse and empower users with better solutions to increase operational efficiency for data warehousing needs The aim of the project is to gain more visibility into key metrics of operations in the company and to gain insights into how to drive improvements and take the business to a whole new level with advanced analysis through the application of data science Responsibilities Involved in requirement gathering data analysis and Interacted with Business users to understand the reporting requirements analyzing BI needs for the user community Created EntityRelationship Diagrams grouped and created the tables validated the data identified PKs for lookup tables Involved in modeling Star Schema methodologies in building and designing the logical data model into Dimensional Models Created and maintained logical dimensional data models for different Claim types and HIPAA Standards Implemented onemany manymany Entity relationships in the data modeling of Data warehouse Experience working with MDM team with various business operations involved within the organization Identify the Primary Key Foreign Key relationships across the entities and across subject areas Developed ETL routines using SSIS packages to plan an effective package development process and design the control flow within the packages Worked with Big Data Architects for setting up Big Data Platform in the organization and on Hive platform to create Hive Data Models Developed customized training documentation based on each clients technical needs and built a curriculum to help each client learn both basic and advanced techniques for using Postgre SQL Took an active role in the design architecture and development of user interface objects in Qlik View applications Connected to various data sources like SQL Server Oracle and flat files Presented the Dashboard to Business users and crossfunctional teams define KPIs Key Performance Indicators and identify data sources Designed data flows that ETL extract transform and load data by optimizing SSIS performance Deliver end to end mapping from source Guide wire application to target CDW and legacy systems coverages to Landing Zone and to Guide wire Reporting Pack Involved in loading the data from Source Tables to Operational Data Source tables using Transformation and Cleansing Logic Performed the Data Accuracy Data Analysis Data Quality checks before and after loading the data Resolved the data type inconsistencies between the source systems and the target system using the Mapping Documents Generated tableau dashboards for Claims with forecast and reference lines Designed developed implemented and maintained InformaticaPower center and Informatica Data Quality IDQ application for matching and merging process Created adhoc reports to users in Tableau by connecting various data sources Worked on the reporting requirements for the data warehouse Created support documentation and worked closely with production support and testing team Environment Erwin82 Oracle 11g OBIEE Crystal Reports Toad Sybase Power Designer Datahub MS Visio DB2 QlikView 116 Informatica Data Analyst Unisys Global Services Bengaluru Karnataka December 2012 to February 2014 Description Unisys is a global information technology company that builds highperformance securitycentric solutions for the most demanding businesses and governments on Earth Unisys offerings include security software and services digital transformation and workplace services industry applications and services and innovative software operating environments for highintensity enterprise computing Responsibilities Implemented and updated analytical methods such as regression modelling classification tree statistical tests and data visualization techniques with Python Performed exploratory Data Analysis Data Wrangling and development of algorithms in R and Python for data mining and analysis Analysis of customer data and other operational data in MySQL and MS Access to provide insights that enable improvements to customer experience Utilized pandas and NumPy packages in Python to improve data collection and distribution processes as well as to enhance reporting capabilities to provide clear line of sight into key performance trends and metrics Performed data analysis data manipulation data transformation and data mapping of source data from the MySQLserver Understanding and adherence to the principles of data quality management including metadata lineage and business definitions Analyzed historical demand filter out outliersexceptions identify the most appropriate statistical forecasting algorithm develop base plan understand variance propose improvement opportunities and incorporate demand signal into forecast and executed data visualization by using plotly package in Python Participated in all phases of research including data collection data cleaning data mining development models and visualizations Examine customer feedback and activity for use in detecting or confirming fraud using a combination of text analytics statistical modeling and classification Deep understanding of Software Development Life Cycle SDLC as well as AgileScrum methodology to accelerate Software Development iteration Improvisations and maintenance to existing automated solutions Used MS Visio MS Project to assist the team in project planning quality plan risk management requirements management change management defect management change management and release management Environment MySQL Statistical modelling Python libraries Pandas NumPy packages R MS Visio MS Project MS Access Data Analyst Python Developer Hidden Brains Hyderabad Telangana January 2011 to November 2012 Description Hidden Brains InfoTech Pvt Ltd is an Enterprise Web Mobile Apps Development Company With an industry experience of over a decade we offer a plethora of clientcentric services by enabling customers to achieve competitive advantage through flexible and next generation global delivery models Responsibilities Designed and developed transformation logic for BI tools Informatica for data transformation into various layers of Data warehouse Performed code development with the help of internal Python library that speeds up database querying and allow users to write more resilient ETL jobs Implemented PostgreSQL SQL servers to develop stored procedures views to create result sets to meet varying reporting requirements Ensured data integrity using advanced excel formulas lookup functions pivot table If Statements etc for analyzing data Hands on experience in writing queries in SQL and R to extract transform and load ETL data from large datasets using Data Staging Participated in the AB testing conducted by BIAnalytics team for data extraction and exploratory analysis Generated dashboards and presented the analysis to researchers explaining insights on the data Provided analytical insights and decision support tools for executives for accurate decision making by identifying measuring and recommending improvement strategies for KPIs across all business areas Submitted summaries charts and graphs to team and stakeholders that help to interpret findings based on complex excel reports Responsible for Credit data related warehouse creation that could help with Risk Assessment for Commercial loans Performed competitor and customer analysis risk and pricing analysis and forecasted results for credit card holders on demographical basis Worked in manipulating various management reports in MS Excel for sales metrics using VLOOKUP and Pivot tables Involved in estimating defining implementing and utilizing business metrics calculations and methodologies Environment Excel 2010 R Informatica Power Center 90 Python PostgreSQL MS SQL Server 200 Education Bachelors Skills CASSANDRA HDFS IMPALA MAPREDUCE SQOOP HBASE KAFKA ELASTICSEARCH ETL FLUME HADOOP INFORMATICA MONGODB NLP REDIS TABLEAU SERVER TERADATA DATA MODELING DATABASE DATABASE DESIGN Additional Information Technical Skills Programming Scripting Languages R C C JAVA JCL COBOL HTML CSS JSP Java Script Databases SQL Hive Impala Pig Spark SQL Databases SQLServer My SQL MS Access HDFS HBase Teradata Netezza MongoDB Cassandra Statistical Software SPSS R SAS Web Packages ggplot2 caret dplyr Rweka gmodels RCurl tm C50 twitteR NLP Reshape2 rjson plyr pandas numPy seaborn sciPy matplot lib scikitlearn Beautiful Soup Rpy2 sqlalchemy Big data Ecosystem HDFS PIG MapReduce HIVE SQOOP FLUME HBase Storm Kafka Elastic Search Redis Flume Storm Kafka Elastic Search Redis Flume Scoop Statistical Methods Time Series regression models splines confidence intervals principal component analysis and Dimensionality Reduction bootstrapping BI Tools Tableau Tableau server Tableau Reader SAP Business Objects OBIEE QlikView SAP Business Intelligence Amazon Redshift or Azure Data Warehouse Database Design Tools and Data Modeling Erwin r 96 95 91 8x Rational Rose ERStudio MS Visio SAP Power designer Cloud ETL Tools AWS S3 EC2 Informatica Power Centre SSIS Operating System Windows Linux Unix Macintosh HD Red Hat Big Data Grid Technologies Cassandra Coherence Mongo DB Zookeeper Titan Elasticsearch Storm Kafka Hadoop Tools and Utilities SQL Server Management Studio SQL Server Enterprise Manager SQL Server Profiler Import Export Wizard Visual StudioNet Microsoft Management Console Visual Source Safe 60 DTS Crystal Reports Power Pivot ProClarity Microsoft Office Excel Power Pivot Excel Data Explorer Tableau JIRASparkMLlib Project1 Role Data Scientist",
    "entities": [
        "Python Performed",
        "Statistical Machine Learning Data Mining",
        "NLTK",
        "C50",
        "Informatica",
        "Statistical Modeling Multivariate Analysis Big",
        "Informatica Power Center",
        "Informatica Data Quality IDQ",
        "OLTP Random",
        "Landing Zone",
        "Implemented PostgreSQL",
        "BI",
        "Machine Learning Data",
        "Guide",
        "Applied",
        "Data Visualization A",
        "SQL Oracle",
        "NLTK Spacy Pattern",
        "MySQL and MS Access",
        "Requirements Analysis Design Specification",
        "Responsibilities Performed Data Profiling",
        "Design NLP",
        "Indepth",
        "Transformation and Cleansing Logic Performed the Data Accuracy Data Analysis Data Quality",
        "NLPNLG",
        "Responsibilities Implemented",
        "Hadoop",
        "SVM Neural Networks",
        "Software Development Life Cycle SDLC",
        "Source Tables to Operational Data Source",
        "Responsible for Credit",
        "PySpark Spark",
        "Utilities SQL Server Management Studio",
        "Principal Component Analysis",
        "Rand",
        "Hive Data Models Developed",
        "TX",
        "RCurl",
        "Hadoop Hive Teradata0",
        "SSIS",
        "the Mapping Documents Generated",
        "MapReduce Hadoop Hive Spark",
        "Generated",
        "Dimensionality Reduction",
        "SparkSQL",
        "Data Staging Participated",
        "Dallas",
        "ODS NLTK",
        "the Conceptual Logical Physical Data Models",
        "Utilized",
        "MS Excel",
        "Responsibilities Involved",
        "Waterfall",
        "Data Warehouse ODS",
        "Apps Development Company",
        "Lowlevel",
        "Keeps",
        "BIAnalytics",
        "Natural Language Processing NLP Proficiency",
        "EDA Univariateand",
        "SQL Server Profiler Import Export",
        "Unstructured",
        "VLOOKUP",
        "Classification",
        "Submitted",
        "Tableau Established Data",
        "NZSQLNZLOAD",
        "Tables",
        "Spark",
        "Created Hive",
        "linear",
        "Scikit",
        "Developed User Defined Functions",
        "Tableau Sqoop HBase",
        "KNN",
        "Created",
        "Hypothesis",
        "NLU",
        "Developed ETL",
        "NLPNLG Perform",
        "Text Analytics",
        "PCA Principal",
        "Tableau Hands",
        "Deep",
        "NLPMLframeworks",
        "Clustering Algorithm K Means Performed",
        "Random Forest Experienced",
        "Performed Source System Analysis",
        "ANOVA Crosstabs Ttests and Correlation Techniques Applies",
        "MLDM",
        "SAS",
        "Market Mix",
        "StudioNet Microsoft Management Console Visual",
        "Data Acquisition Data Validation Predictive",
        "SQL",
        "Exploratory Data Analysis",
        "Standard Procedures Highly",
        "NLP",
        "Software Development",
        "Informatica Data",
        "Bayes Random Forests Kmeans",
        "OCR",
        "Anaconda",
        "BI Tools Tableau Tableau",
        "Hive",
        "Regression Classification",
        "Informatica Power Centre SSIS Operating System",
        "Macintosh",
        "Risk Assessment for Commercial",
        "SAP Power",
        "Developed Implemented",
        "MS Visio MS Project",
        "MDM",
        "CDW",
        "ETL",
        "Tableau Reader SAP Business Objects OBIEE QlikView SAP Business Intelligence Amazon Redshift or Azure Data Warehouse Database Design Tools",
        "SQL Server Oracle",
        "Responsibilities Incident",
        "Performed",
        "Impala",
        "My SQL MS Access HDFS HBase Teradata Netezza MongoDB",
        "Teradata Utilities",
        "Postgre SQL Took",
        "Qlik View",
        "InformaticaPower",
        "Expertise",
        "Big Data Architects",
        "Hadoop Spark",
        "Data",
        "Structured",
        "Neural Network Support",
        "Tableau",
        "Software Development Life Cycle",
        "ForwardReverse Engineered Databases",
        "Dimensional Models Created",
        "Random Forests Classification",
        "TeradataViewpoint Multi Load ARC TeradataAdministrator",
        "Created EntityRelationship Diagrams",
        "ETL Data",
        "Red Hat Big Data Grid Technologies Cassandra Coherence Mongo DB Zookeeper"
    ],
    "experience": "Experience with Natural Language Processing NLP Proficiency in application of statistical prediction modeling machine learning classification techniques and econometric forecasting techniques Indepth knowledge of statistical procedures that are applied in Supervised Unsupervised problems Experience in the application of Neural Network Support Vector Machines SVM and Random Forest Experienced in working with advanced analytical teams to design build validate and refresh data models that enable the next generation of sophisticated solutions for global clients Extensively worked on Python 3527 NumPy Pandas Matplotlib NLTK and Scikit learn Extensive experience in Text Analytics developing different Statistical Machine Learning Data Mining solutions to various business problems and generating data visualizations using R Python and Tableau Experience in implementing data analysis with various analytic tools such as Anaconda 40 Jupyter Notebook 4X R 30ggplot2 and Excel Experience in designing star schema Snowflake schema for Data Warehouse ODS architecture Hands on experience in business understanding data understanding and preparation of large databases Experience in working with relational databases MySQL Oracle with advanced SQL programming skills Expertise in transforming business requirements into analytical models designing algorithms building models developing data mining and reporting solutions that scales across massive volume of structured and unstructured data Experience in using various packages in Rand pythonlike ggplot2 caret dplyr Rweka gmodels RCurl tm C50 twitter NLP Reshape2 rjson dplyr pandas NumPy Seaborn SciPy Matplotlib Scikitlearn Beautiful Soup Rpy2 Extensive experience in Text Analytics generating data visualizations using R Python and creating dashboards using tools like Tableau Hands on experience with big data tools like Hadoop Spark Hive Pig Impala PySpark Spark SQL Experience in using one or more cloud computing frameworks such as AWS Azure Google Cloud etc Mapping and tracing data from system to system in order to establish data hierarchy and lineage Experience with distributed datacomputing tools MapReduce Hadoop Hive Spark MySQL Worked on Tableau to create dashboards and visualizations Proficiency in various type of optimization Market Mix modeling Segmentation Time Series Price Promo models etc Identifiescreates the appropriate algorithm to discover patterns validate their findings using an experimental and iterative approach Strong skills in statistical methodologies such as AB testExperiment design Hypothesis test ANOVA Crosstabs Ttests and Correlation Techniques Applies advanced statistical and predictive modeling techniques to build maintain and improve on multiple realtime decision systems Experience in designing stunning visualizations using Tableau software and publishing and presenting dashboards Storyline on web and desktop platforms Proficiency in SAS Base SAS Enterprise Guide Enterprise Miner Excellent communication skills verbal and written to communicate with clients and team prepare deliver effective presentations Strong experience in Software Development Life Cycle SDLC including Requirements Analysis Design Specification and Testing as per Cycle in both Waterfall and Agile methodologies Strong experience in interacting with stakeholderscustomers gathering requirements through interviews workshops and existing system documentation or procedures defining business processes identifying and analyzing risks using appropriate templates and analysis tools Closely works with product managers Service development managers and product development team in productizing the algorithms developed Work Experience Data Scientist Consultant BBVA Compass Birmingham AL August 2018 to Present Description BBVA Compass understands that every individual and company has unique dreams and ambitions needs and wants We realize that few take the same path in the faster busier and more complex world we live in We get it Whichever path you choose and whenever you need us we want to create opportunities for your bright future From the smallest moment to the largest personal or professional life event BBVA Compass is there for you Responsibilities Performed Data Profiling to learn about behavior with various features such as traffic pattern location and time Date and Time etc Extensively used Pythons multiple data science packages like Pandas NumPy Matplotlib SciPy Scikit learn and NLTK Worked on data cleaning data preparation and feature engineering with Python 3X In depth understanding of NLP for tokenization lemmatization and stemming using NLTK Spacy Pattern libraries Experience in evaluating and proposing solutions to NLP problems through various approaches Deep background in information retrieval OCR SpeechtoText etc Natural Language Processing NLP knowledge representation or computational linguistics Understanding of NLP techniques for text representation semantic extraction techniques data structures and modeling Experience in Java Python and NLPMLframeworks and libraries Worked to apply a broad array of capabilities spanning machine learning statistics   and modeling to extract insights to structured and unstructured healthcare data sources preclinical clinical trial and complementary real world information streams to apply a broad array of capabilities spanning machine learning statistics   and modeling to extract insights to structured and unstructured healthcare data sources preclinical clinical trial and complementary real world information streams Developed User Defined Functions in Python for rapid analysis and performed data imputation using Scikit learnpackage in Python Developed the applications models used appropriate algorithms for arriving at the required insights by analyzing business requirements Worked with unsupervised Kmeans DBSCAN and supervised learning techniques Regression Classification for feature engineering and did Principal Component Analysis for dimensionality reduction of features Used the Classification machine learning algorithms Nave Bayes Linear regression Logistic regression SVM Neural Networks and used Clustering Algorithm K Means Performed text classification task using NLTK package and implemented various natural language processing techniques Worked collaboratively with senior management to develop strategy and approach to defining business challenges to be answered by data science Established partnerships with product and engineering teams and work closely with other teams Used SparkStreamingAPIs to perform necessary transformations and actions on the fly for building the common learner data model which gets the data from Kafka in near real time Worked on end to end pipe line in Spark and on Apache spark for analyzing the live streaming data Worked on Spark Python modules for machine learning and predictive analytics in Spark on AWS Explored and analyzed the customer specific features by using SparkSQL Created Hive scripts to create external internal data tables on Hive Worked on creating datasets to load data into HIVE Worked on Tableau for data visualization to create reports dashboards for insights and business process improvement Created the dashboards and reports in tableau for visualizing the data in required format Environment Spark Apache Spark Hive Machine learning Python NumPy NLTK Pandas SciPy MySQL Tableau Sqoop HBase HDFS Tableau   Mongo DB SQL Server and ETL Data Scientist Visa Austin TX May 2017 to July 2018 Description Visa is a dynamic global enterprise and innovation is at the heart of everything we do Were looking for smart ambitious people to join our Austin team We are a global payments technology company working to enable consumers businesses banks and governments to use digital currency Responsibilities Analyzed Pandas NumPy seaborn SciPy Matplotlib Scikitlearn NLTK in Python for developing various machine learning algorithms and utilized machine learning algorithms such as linear regression multivariate regression naive Bayes Random Forests Kmeans KNN for data analysis Demonstrated experience in design and implementation of Statistical models Predictive models enterprise data model metadata solution and data life cycle management in both RDBMS Big Data environments Utilized domain knowledge and application portfolio knowledge to play a key role in defining the future state of large business technology programs Developed MapReduceSparkPython modules for machine learning predictive analytics in Hadoop on AWS Implemented a Pythonbased distributed random forest via Python streaming Performed Source System Analysis database design data modeling for the warehouse layer using MLDM concepts and package layer using Dimensional modeling Created ecosystem models eg conceptual logical physical canonical that are required for supporting services within the enterprise data architecture conceptual data model for defining the major subject areas used ecosystem logical model for defining standard business meaning for entities and fields and an ecosystem canonical model for defining the standard messages and formats to be used in data integration services throughout the ecosystem Developed LINUXShell scripts by using NZSQLNZLOAD utilities to load data from flat files to Netezza database Designed and implemented system architecture for AmazonEC2 based cloudhosted solution for client Tested Complex ETL Mappings and Sessions based on business user requirements and business rules to load data from source flat files and RDBMS tables to target tables Hands on database design relational integrity constraints OLAP OLTP Cubes and Normalization 3NF and Denormalization of database Developed MapReduceSpark Python modules for machine learning predictive analytics in Hadoop on AWS Worked on customer segmentation using an unsupervised learning technique clustering Worked with various Teradata15 tools and utilities like TeradataViewpoint Multi Load ARC TeradataAdministrator BTEQ and other Teradata Utilities Utilized Spark Scala Hadoop HBase Kafka SparkStreaming MLlib Python a broad variety of machine learning methods including classifications regressions dimensionally reduction Analyzed large data sets apply machine learning techniques and develop predictive models statistical models and developing and enhancing statistical models by leveraging bestinclass modeling techniques Environment Erwin r96 Python SQL Oracle 12c Netezza SQL Server Informatica Java SSRS PLSQL TSQL Tableau MLlib regression Cluster analysis Scala NLP Cassandra Map Reduce Spark Kafka Mongo DB logistic regression Hadoop Hive Teradata0 random forest OLAP Azure Maria DB SAP CRM HDFS ODS NLTK SVM JSON Tableau XML AWS Data Scientist Black Tree Healthcare Consulting King of Prussia PA January 2016 to April 2017 Description Black Tree Healthcare Consulting provides revenue cycle outsourcing and clinical consulting services to the home health hospice and skilled nursing industries The primary goal of the project was to determine the most efficient inputoutput ratio through data modeling and analysis thus providing the best resource combination Responsibilities Incident and problem management coordinating resolution of data movement disruptions Capacity analysis and planning for a sizable application server environment Performance metering and tuning for application servers database monitors and alerts Risk identification and evaluation managing and improving upon internal controls which mitigate risks Working with development teams to identify enhancement opportunities and resolve code defects Code deployment and configuration management Coordination and execution of changes within a complex testing environment Data collection procedures enhancement to include information that is relevant for building analytic systems processing cleansing and verifying the integrity of data used for analysis Building analytic models using a variety of techniques such as logistic regression risk scorecards and pattern recognition technologies Analyze and understand large amounts of data to determine suitability for use in models and then work to segment the data create variables build models and test those models Excellent understanding of machine learning techniques and algorithms such as Logistic Regression SVM Random Forests Deep Learning etc In depth understanding and experience in NLP and Deep learning Expertise in Sentiment Analysis Entity Extraction Document Classification Topic Modeling Natural Language Understanding NLU and Natural Language Generation NLG Leads all NLPNLG driven solutions for the project Design NLP models for searching structuredunstructured data in realnearreal time Interacts regularly with product team members ensuring successful integration of NLP solutions in the product architecture Keeps the solutions updated with the recent developments in NLPNLG Perform a proper EDA Univariateand bivariate analysis to understand the intrinsic effectcombined Performing adhoc analysis and presenting results in a clear manner and constant tracking of model performance Worked with Data governance Data quality Data lineage Data architect to design various models Designed data models and data flow diagrams using Erwin and MS Visio Independently coded new programs and designed Tables to load and test the program effectively for the given POCs Developed Implemented maintained the Conceptual Logical Physical Data Models using Erwin for ForwardReverse Engineered Databases Experience with common data science toolkits such as R Python Spark etc Applied statistics skills such as statistical sampling testing regression etc Work with technical and development teams to deploy models Build Model Performance Reports and Designing Technical Documentation to support each of the models for the product line Exploratory Data Analysis and Data Visualizations using R and Tableau Established Data architecture strategy best practices standards and roadmaps Lead the development and presentation of a data analytics datahub prototype with the help of the other members of the emerging solutions team Involved in analysis of Business requirement Design and Development of High level and Lowlevel designs Unit and Integration testing Interacted with the other departments to understand and identify data needs and requirements Worked with several R packages including knitr dplyr SparkR Causal Infer space time Environment R 3x UNIX Python 352 MLLib SAS regression logistic regression Hadoop NoSQL MySQL OLTP Random forest OLAP HDFS ODS Data Analyst Grey Hound Dallas TX March 2014 to December 2015 Description Greyhound is an intercity bus common carrier serving over 3800 destinations across North America Behavior of users on our website generate large amount of data which challenges our ability to gather parse analyze and infer from it So a major task is to implement new capabilities for reporting from existing data warehouse and empower users with better solutions to increase operational efficiency for data warehousing needs The aim of the project is to gain more visibility into key metrics of operations in the company and to gain insights into how to drive improvements and take the business to a whole new level with advanced analysis through the application of data science Responsibilities Involved in requirement gathering data analysis and Interacted with Business users to understand the reporting requirements analyzing BI needs for the user community Created EntityRelationship Diagrams grouped and created the tables validated the data identified PKs for lookup tables Involved in modeling Star Schema methodologies in building and designing the logical data model into Dimensional Models Created and maintained logical dimensional data models for different Claim types and HIPAA Standards Implemented onemany manymany Entity relationships in the data modeling of Data warehouse Experience working with MDM team with various business operations involved within the organization Identify the Primary Key Foreign Key relationships across the entities and across subject areas Developed ETL routines using SSIS packages to plan an effective package development process and design the control flow within the packages Worked with Big Data Architects for setting up Big Data Platform in the organization and on Hive platform to create Hive Data Models Developed customized training documentation based on each clients technical needs and built a curriculum to help each client learn both basic and advanced techniques for using Postgre SQL Took an active role in the design architecture and development of user interface objects in Qlik View applications Connected to various data sources like SQL Server Oracle and flat files Presented the Dashboard to Business users and crossfunctional teams define KPIs Key Performance Indicators and identify data sources Designed data flows that ETL extract transform and load data by optimizing SSIS performance Deliver end to end mapping from source Guide wire application to target CDW and legacy systems coverages to Landing Zone and to Guide wire Reporting Pack Involved in loading the data from Source Tables to Operational Data Source tables using Transformation and Cleansing Logic Performed the Data Accuracy Data Analysis Data Quality checks before and after loading the data Resolved the data type inconsistencies between the source systems and the target system using the Mapping Documents Generated tableau dashboards for Claims with forecast and reference lines Designed developed implemented and maintained InformaticaPower center and Informatica Data Quality IDQ application for matching and merging process Created adhoc reports to users in Tableau by connecting various data sources Worked on the reporting requirements for the data warehouse Created support documentation and worked closely with production support and testing team Environment Erwin82 Oracle 11 g OBIEE Crystal Reports Toad Sybase Power Designer Datahub MS Visio DB2 QlikView 116 Informatica Data Analyst Unisys Global Services Bengaluru Karnataka December 2012 to February 2014 Description Unisys is a global information technology company that builds highperformance securitycentric solutions for the most demanding businesses and governments on Earth Unisys offerings include security software and services digital transformation and workplace services industry applications and services and innovative software operating environments for highintensity enterprise computing Responsibilities Implemented and updated analytical methods such as regression modelling classification tree statistical tests and data visualization techniques with Python Performed exploratory Data Analysis Data Wrangling and development of algorithms in R and Python for data mining and analysis Analysis of customer data and other operational data in MySQL and MS Access to provide insights that enable improvements to customer experience Utilized pandas and NumPy packages in Python to improve data collection and distribution processes as well as to enhance reporting capabilities to provide clear line of sight into key performance trends and metrics Performed data analysis data manipulation data transformation and data mapping of source data from the MySQLserver Understanding and adherence to the principles of data quality management including metadata lineage and business definitions Analyzed historical demand filter out outliersexceptions identify the most appropriate statistical forecasting algorithm develop base plan understand variance propose improvement opportunities and incorporate demand signal into forecast and executed data visualization by using plotly package in Python Participated in all phases of research including data collection data cleaning data mining development models and visualizations Examine customer feedback and activity for use in detecting or confirming fraud using a combination of text analytics statistical modeling and classification Deep understanding of Software Development Life Cycle SDLC as well as AgileScrum methodology to accelerate Software Development iteration Improvisations and maintenance to existing automated solutions Used MS Visio MS Project to assist the team in project planning quality plan risk management requirements management change management defect management change management and release management Environment MySQL Statistical modelling Python libraries Pandas NumPy packages R MS Visio MS Project MS Access Data Analyst Python Developer Hidden Brains Hyderabad Telangana January 2011 to November 2012 Description Hidden Brains InfoTech Pvt Ltd is an Enterprise Web Mobile Apps Development Company With an industry experience of over a decade we offer a plethora of clientcentric services by enabling customers to achieve competitive advantage through flexible and next generation global delivery models Responsibilities Designed and developed transformation logic for BI tools Informatica for data transformation into various layers of Data warehouse Performed code development with the help of internal Python library that speeds up database querying and allow users to write more resilient ETL jobs Implemented PostgreSQL SQL servers to develop stored procedures views to create result sets to meet varying reporting requirements Ensured data integrity using advanced excel formulas lookup functions pivot table If Statements etc for analyzing data Hands on experience in writing queries in SQL and R to extract transform and load ETL data from large datasets using Data Staging Participated in the AB testing conducted by BIAnalytics team for data extraction and exploratory analysis Generated dashboards and presented the analysis to researchers explaining insights on the data Provided analytical insights and decision support tools for executives for accurate decision making by identifying measuring and recommending improvement strategies for KPIs across all business areas Submitted summaries charts and graphs to team and stakeholders that help to interpret findings based on complex excel reports Responsible for Credit data related warehouse creation that could help with Risk Assessment for Commercial loans Performed competitor and customer analysis risk and pricing analysis and forecasted results for credit card holders on demographical basis Worked in manipulating various management reports in MS Excel for sales metrics using VLOOKUP and Pivot tables Involved in estimating defining implementing and utilizing business metrics calculations and methodologies Environment Excel 2010 R Informatica Power Center 90 Python PostgreSQL MS SQL Server 200 Education Bachelors Skills CASSANDRA HDFS IMPALA MAPREDUCE SQOOP HBASE KAFKA ELASTICSEARCH ETL FLUME HADOOP INFORMATICA MONGODB NLP REDIS TABLEAU SERVER TERADATA DATA MODELING DATABASE DATABASE DESIGN Additional Information Technical Skills Programming Scripting Languages R C C JAVA JCL COBOL HTML CSS JSP Java Script Databases SQL Hive Impala Pig Spark SQL Databases SQLServer My SQL MS Access HDFS HBase Teradata Netezza MongoDB Cassandra Statistical Software SPSS R SAS Web Packages ggplot2 caret dplyr Rweka gmodels RCurl tm C50 twitteR NLP Reshape2 rjson plyr pandas numPy seaborn sciPy matplot lib scikitlearn Beautiful Soup Rpy2 sqlalchemy Big data Ecosystem HDFS PIG MapReduce HIVE SQOOP FLUME HBase Storm Kafka Elastic Search Redis Flume Storm Kafka Elastic Search Redis Flume Scoop Statistical Methods Time Series regression models splines confidence intervals principal component analysis and Dimensionality Reduction bootstrapping BI Tools Tableau Tableau server Tableau Reader SAP Business Objects OBIEE QlikView SAP Business Intelligence Amazon Redshift or Azure Data Warehouse Database Design Tools and Data Modeling Erwin r 96 95 91 8x Rational Rose ERStudio MS Visio SAP Power designer Cloud ETL Tools AWS S3 EC2 Informatica Power Centre SSIS Operating System Windows Linux Unix Macintosh HD Red Hat Big Data Grid Technologies Cassandra Coherence Mongo DB Zookeeper Titan Elasticsearch Storm Kafka Hadoop Tools and Utilities SQL Server Management Studio SQL Server Enterprise Manager SQL Server Profiler Import Export Wizard Visual StudioNet Microsoft Management Console Visual Source Safe 60 DTS Crystal Reports Power Pivot ProClarity Microsoft Office Excel Power Pivot Excel Data Explorer Tableau JIRASparkMLlib Project1 Role Data Scientist",
    "extracted_keywords": [
        "Data",
        "Scientist",
        "Consultant",
        "Data",
        "Scientist",
        "Consultant",
        "Data",
        "scientist",
        "Birmingham",
        "AL",
        "years",
        "IT",
        "experience",
        "Machine",
        "Learning",
        "Data",
        "mining",
        "datasets",
        "Structured",
        "Unstructured",
        "data",
        "Data",
        "Acquisition",
        "Data",
        "Validation",
        "Predictive",
        "Data",
        "Visualization",
        "understanding",
        "Statistical",
        "Modeling",
        "Multivariate",
        "Analysis",
        "data",
        "analytics",
        "Standard",
        "Procedures",
        "Dimensionality",
        "Reduction",
        "methods",
        "PCA",
        "Principal",
        "component",
        "Analysis",
        "Factor",
        "Analysis",
        "methods",
        "Random",
        "Forests",
        "Classification",
        "KMeans",
        "KNN",
        "NaiveBayes",
        "SVM",
        "Decision",
        "Tree",
        "Linear",
        "Logistic",
        "Regression",
        "Methods",
        "experience",
        "text",
        "classification",
        "pattern",
        "recognition",
        "recommendation",
        "systems",
        "systems",
        "systems",
        "Python",
        "Experience",
        "Natural",
        "Language",
        "Processing",
        "NLP",
        "Proficiency",
        "application",
        "prediction",
        "machine",
        "classification",
        "techniques",
        "forecasting",
        "techniques",
        "knowledge",
        "procedures",
        "Supervised",
        "Unsupervised",
        "problems",
        "Experience",
        "application",
        "Neural",
        "Network",
        "Support",
        "Vector",
        "Machines",
        "SVM",
        "Random",
        "Forest",
        "teams",
        "build",
        "validate",
        "data",
        "models",
        "generation",
        "solutions",
        "clients",
        "Python",
        "NumPy",
        "Pandas",
        "Matplotlib",
        "NLTK",
        "Scikit",
        "experience",
        "Text",
        "Analytics",
        "Machine",
        "Learning",
        "Data",
        "Mining",
        "solutions",
        "business",
        "problems",
        "data",
        "visualizations",
        "R",
        "Python",
        "Tableau",
        "Experience",
        "data",
        "analysis",
        "tools",
        "Anaconda",
        "Jupyter",
        "Notebook",
        "4X",
        "R",
        "30ggplot2",
        "Excel",
        "Experience",
        "star",
        "schema",
        "Snowflake",
        "schema",
        "Data",
        "Warehouse",
        "ODS",
        "architecture",
        "Hands",
        "experience",
        "business",
        "data",
        "understanding",
        "preparation",
        "databases",
        "Experience",
        "databases",
        "MySQL",
        "Oracle",
        "SQL",
        "programming",
        "skills",
        "Expertise",
        "business",
        "requirements",
        "models",
        "algorithms",
        "building",
        "models",
        "data",
        "mining",
        "reporting",
        "solutions",
        "volume",
        "data",
        "Experience",
        "packages",
        "Rand",
        "pythonlike",
        "ggplot2",
        "dplyr",
        "Rweka",
        "RCurl",
        "tm",
        "C50",
        "twitter",
        "NLP",
        "Reshape2",
        "rjson",
        "NumPy",
        "Seaborn",
        "SciPy",
        "Matplotlib",
        "Scikitlearn",
        "Beautiful",
        "Soup",
        "experience",
        "Text",
        "Analytics",
        "data",
        "visualizations",
        "R",
        "Python",
        "dashboards",
        "tools",
        "Tableau",
        "Hands",
        "experience",
        "data",
        "tools",
        "Hadoop",
        "Spark",
        "Hive",
        "Pig",
        "Impala",
        "PySpark",
        "Spark",
        "SQL",
        "Experience",
        "frameworks",
        "AWS",
        "Google",
        "Cloud",
        "Mapping",
        "data",
        "system",
        "system",
        "order",
        "data",
        "hierarchy",
        "Experience",
        "tools",
        "MapReduce",
        "Hadoop",
        "Hive",
        "Spark",
        "MySQL",
        "Tableau",
        "dashboards",
        "visualizations",
        "Proficiency",
        "type",
        "optimization",
        "Market",
        "Mix",
        "Segmentation",
        "Time",
        "Series",
        "Price",
        "Promo",
        "models",
        "Identifiescreates",
        "algorithm",
        "patterns",
        "findings",
        "approach",
        "Strong",
        "skills",
        "methodologies",
        "AB",
        "testExperiment",
        "design",
        "Hypothesis",
        "test",
        "ANOVA",
        "Crosstabs",
        "Ttests",
        "Correlation",
        "Techniques",
        "Applies",
        "modeling",
        "techniques",
        "maintain",
        "decision",
        "systems",
        "Experience",
        "visualizations",
        "Tableau",
        "software",
        "publishing",
        "dashboards",
        "Storyline",
        "web",
        "desktop",
        "platforms",
        "Proficiency",
        "SAS",
        "Base",
        "SAS",
        "Enterprise",
        "Guide",
        "Enterprise",
        "Miner",
        "Excellent",
        "communication",
        "clients",
        "team",
        "presentations",
        "experience",
        "Software",
        "Development",
        "Life",
        "Cycle",
        "SDLC",
        "Requirements",
        "Analysis",
        "Design",
        "Specification",
        "Testing",
        "Cycle",
        "Waterfall",
        "methodologies",
        "experience",
        "stakeholderscustomers",
        "requirements",
        "interviews",
        "workshops",
        "system",
        "documentation",
        "procedures",
        "business",
        "processes",
        "risks",
        "templates",
        "analysis",
        "tools",
        "product",
        "managers",
        "Service",
        "development",
        "managers",
        "product",
        "development",
        "team",
        "algorithms",
        "Work",
        "Experience",
        "Data",
        "Scientist",
        "Consultant",
        "BBVA",
        "Compass",
        "Birmingham",
        "AL",
        "August",
        "Present",
        "Description",
        "BBVA",
        "Compass",
        "individual",
        "company",
        "dreams",
        "ambitions",
        "path",
        "world",
        "path",
        "opportunities",
        "future",
        "moment",
        "life",
        "event",
        "BBVA",
        "Compass",
        "Responsibilities",
        "Performed",
        "Data",
        "Profiling",
        "behavior",
        "features",
        "traffic",
        "pattern",
        "location",
        "time",
        "Date",
        "Time",
        "Pythons",
        "data",
        "science",
        "packages",
        "Pandas",
        "NumPy",
        "Matplotlib",
        "SciPy",
        "Scikit",
        "NLTK",
        "data",
        "data",
        "preparation",
        "feature",
        "engineering",
        "Python",
        "3X",
        "depth",
        "understanding",
        "NLP",
        "tokenization",
        "lemmatization",
        "NLTK",
        "Spacy",
        "Pattern",
        "Experience",
        "solutions",
        "problems",
        "approaches",
        "background",
        "information",
        "retrieval",
        "OCR",
        "SpeechtoText",
        "Natural",
        "Language",
        "Processing",
        "NLP",
        "knowledge",
        "representation",
        "linguistics",
        "Understanding",
        "NLP",
        "techniques",
        "text",
        "representation",
        "extraction",
        "techniques",
        "data",
        "structures",
        "Experience",
        "Java",
        "Python",
        "NLPMLframeworks",
        "libraries",
        "array",
        "capabilities",
        "machine",
        "statistics",
        "insights",
        "healthcare",
        "data",
        "sources",
        "trial",
        "world",
        "information",
        "array",
        "capabilities",
        "machine",
        "statistics",
        "insights",
        "healthcare",
        "data",
        "sources",
        "trial",
        "world",
        "information",
        "Developed",
        "User",
        "Defined",
        "Functions",
        "Python",
        "analysis",
        "data",
        "imputation",
        "Scikit",
        "learnpackage",
        "Python",
        "applications",
        "models",
        "algorithms",
        "insights",
        "business",
        "requirements",
        "Kmeans",
        "DBSCAN",
        "techniques",
        "Regression",
        "Classification",
        "feature",
        "engineering",
        "Principal",
        "Component",
        "Analysis",
        "dimensionality",
        "reduction",
        "features",
        "Classification",
        "machine",
        "learning",
        "algorithms",
        "Nave",
        "Bayes",
        "Linear",
        "regression",
        "regression",
        "SVM",
        "Neural",
        "Networks",
        "Clustering",
        "Algorithm",
        "K",
        "text",
        "classification",
        "task",
        "NLTK",
        "package",
        "language",
        "processing",
        "techniques",
        "management",
        "strategy",
        "approach",
        "business",
        "challenges",
        "data",
        "science",
        "partnerships",
        "product",
        "engineering",
        "teams",
        "teams",
        "SparkStreamingAPIs",
        "transformations",
        "actions",
        "fly",
        "learner",
        "data",
        "model",
        "data",
        "Kafka",
        "time",
        "end",
        "pipe",
        "line",
        "Spark",
        "Apache",
        "spark",
        "streaming",
        "data",
        "Spark",
        "Python",
        "modules",
        "machine",
        "learning",
        "analytics",
        "Spark",
        "AWS",
        "customer",
        "features",
        "SparkSQL",
        "Created",
        "Hive",
        "scripts",
        "data",
        "tables",
        "Hive",
        "Worked",
        "datasets",
        "data",
        "HIVE",
        "Worked",
        "Tableau",
        "data",
        "visualization",
        "reports",
        "dashboards",
        "insights",
        "business",
        "process",
        "improvement",
        "dashboards",
        "reports",
        "tableau",
        "data",
        "format",
        "Environment",
        "Spark",
        "Apache",
        "Spark",
        "Hive",
        "Machine",
        "Python",
        "NumPy",
        "NLTK",
        "Pandas",
        "SciPy",
        "MySQL",
        "Tableau",
        "Sqoop",
        "HBase",
        "HDFS",
        "Tableau",
        "Mongo",
        "DB",
        "SQL",
        "Server",
        "ETL",
        "Data",
        "Scientist",
        "Visa",
        "Austin",
        "TX",
        "May",
        "July",
        "Description",
        "Visa",
        "enterprise",
        "innovation",
        "heart",
        "people",
        "Austin",
        "team",
        "payments",
        "technology",
        "company",
        "consumers",
        "businesses",
        "banks",
        "governments",
        "currency",
        "Responsibilities",
        "Pandas",
        "NumPy",
        "SciPy",
        "Matplotlib",
        "Scikitlearn",
        "NLTK",
        "Python",
        "machine",
        "algorithms",
        "machine",
        "learning",
        "algorithms",
        "linear",
        "regression",
        "multivariate",
        "regression",
        "Bayes",
        "Random",
        "Forests",
        "Kmeans",
        "KNN",
        "data",
        "analysis",
        "experience",
        "design",
        "implementation",
        "models",
        "models",
        "enterprise",
        "data",
        "model",
        "metadata",
        "solution",
        "data",
        "life",
        "cycle",
        "management",
        "Big",
        "Data",
        "domain",
        "knowledge",
        "application",
        "portfolio",
        "knowledge",
        "role",
        "state",
        "business",
        "technology",
        "programs",
        "MapReduceSparkPython",
        "modules",
        "machine",
        "analytics",
        "Hadoop",
        "AWS",
        "Pythonbased",
        "forest",
        "Python",
        "Performed",
        "Source",
        "System",
        "Analysis",
        "database",
        "design",
        "data",
        "warehouse",
        "layer",
        "MLDM",
        "concepts",
        "package",
        "layer",
        "modeling",
        "ecosystem",
        "models",
        "canonical",
        "services",
        "enterprise",
        "data",
        "data",
        "model",
        "subject",
        "areas",
        "ecosystem",
        "model",
        "business",
        "meaning",
        "entities",
        "fields",
        "model",
        "messages",
        "formats",
        "data",
        "integration",
        "services",
        "ecosystem",
        "LINUXShell",
        "scripts",
        "NZSQLNZLOAD",
        "utilities",
        "data",
        "files",
        "Netezza",
        "database",
        "system",
        "architecture",
        "AmazonEC2",
        "solution",
        "client",
        "Complex",
        "ETL",
        "Mappings",
        "Sessions",
        "business",
        "user",
        "requirements",
        "business",
        "rules",
        "data",
        "source",
        "files",
        "RDBMS",
        "tables",
        "tables",
        "Hands",
        "database",
        "design",
        "integrity",
        "constraints",
        "OLTP",
        "Cubes",
        "Normalization",
        "Denormalization",
        "database",
        "Developed",
        "MapReduceSpark",
        "Python",
        "modules",
        "machine",
        "analytics",
        "Hadoop",
        "AWS",
        "customer",
        "segmentation",
        "learning",
        "technique",
        "clustering",
        "Teradata15",
        "tools",
        "utilities",
        "TeradataViewpoint",
        "Multi",
        "Load",
        "ARC",
        "TeradataAdministrator",
        "BTEQ",
        "Teradata",
        "Utilities",
        "Spark",
        "Scala",
        "Hadoop",
        "HBase",
        "Kafka",
        "SparkStreaming",
        "MLlib",
        "Python",
        "variety",
        "machine",
        "learning",
        "methods",
        "classifications",
        "regressions",
        "reduction",
        "data",
        "sets",
        "machine",
        "techniques",
        "models",
        "models",
        "models",
        "bestinclass",
        "modeling",
        "techniques",
        "Environment",
        "Erwin",
        "Python",
        "SQL",
        "Oracle",
        "12c",
        "Netezza",
        "SQL",
        "Server",
        "Informatica",
        "Java",
        "SSRS",
        "PLSQL",
        "TSQL",
        "Tableau",
        "MLlib",
        "regression",
        "Cluster",
        "analysis",
        "Scala",
        "NLP",
        "Cassandra",
        "Map",
        "Spark",
        "Kafka",
        "Mongo",
        "DB",
        "regression",
        "Hadoop",
        "Hive",
        "Teradata0",
        "forest",
        "OLAP",
        "Azure",
        "Maria",
        "DB",
        "SAP",
        "CRM",
        "HDFS",
        "ODS",
        "NLTK",
        "SVM",
        "JSON",
        "Tableau",
        "XML",
        "AWS",
        "Data",
        "Scientist",
        "Black",
        "Tree",
        "Healthcare",
        "Consulting",
        "King",
        "Prussia",
        "PA",
        "January",
        "April",
        "Description",
        "Black",
        "Tree",
        "Healthcare",
        "Consulting",
        "revenue",
        "cycle",
        "outsourcing",
        "consulting",
        "services",
        "home",
        "health",
        "hospice",
        "nursing",
        "industries",
        "goal",
        "project",
        "ratio",
        "data",
        "modeling",
        "analysis",
        "resource",
        "combination",
        "Responsibilities",
        "Incident",
        "problem",
        "management",
        "resolution",
        "data",
        "movement",
        "disruptions",
        "Capacity",
        "analysis",
        "planning",
        "application",
        "server",
        "environment",
        "Performance",
        "metering",
        "application",
        "servers",
        "database",
        "monitors",
        "Risk",
        "identification",
        "evaluation",
        "controls",
        "risks",
        "development",
        "teams",
        "enhancement",
        "opportunities",
        "code",
        "defects",
        "Code",
        "deployment",
        "configuration",
        "management",
        "Coordination",
        "execution",
        "changes",
        "testing",
        "environment",
        "Data",
        "collection",
        "procedures",
        "enhancement",
        "information",
        "systems",
        "processing",
        "cleansing",
        "integrity",
        "data",
        "analysis",
        "models",
        "variety",
        "techniques",
        "regression",
        "risk",
        "scorecards",
        "pattern",
        "recognition",
        "technologies",
        "Analyze",
        "amounts",
        "data",
        "suitability",
        "use",
        "models",
        "data",
        "variables",
        "models",
        "models",
        "understanding",
        "machine",
        "techniques",
        "algorithms",
        "Logistic",
        "Regression",
        "SVM",
        "Random",
        "Forests",
        "Deep",
        "Learning",
        "depth",
        "understanding",
        "experience",
        "NLP",
        "Deep",
        "Expertise",
        "Sentiment",
        "Analysis",
        "Entity",
        "Extraction",
        "Document",
        "Classification",
        "Topic",
        "Modeling",
        "Natural",
        "Language",
        "Understanding",
        "NLU",
        "Natural",
        "Language",
        "Generation",
        "NLG",
        "NLPNLG",
        "solutions",
        "project",
        "Design",
        "NLP",
        "models",
        "data",
        "time",
        "product",
        "team",
        "members",
        "integration",
        "NLP",
        "solutions",
        "product",
        "architecture",
        "solutions",
        "developments",
        "NLPNLG",
        "Perform",
        "EDA",
        "Univariateand",
        "bivariate",
        "analysis",
        "analysis",
        "results",
        "manner",
        "tracking",
        "model",
        "performance",
        "Data",
        "governance",
        "Data",
        "quality",
        "Data",
        "lineage",
        "Data",
        "architect",
        "models",
        "data",
        "models",
        "data",
        "flow",
        "diagrams",
        "Erwin",
        "MS",
        "Visio",
        "programs",
        "Tables",
        "program",
        "POCs",
        "Developed",
        "Implemented",
        "Conceptual",
        "Logical",
        "Physical",
        "Data",
        "Models",
        "Erwin",
        "ForwardReverse",
        "Engineered",
        "Databases",
        "Experience",
        "data",
        "science",
        "toolkits",
        "R",
        "Python",
        "Spark",
        "statistics",
        "skills",
        "sampling",
        "testing",
        "regression",
        "development",
        "teams",
        "models",
        "Build",
        "Model",
        "Performance",
        "Reports",
        "Designing",
        "Technical",
        "Documentation",
        "models",
        "product",
        "line",
        "Exploratory",
        "Data",
        "Analysis",
        "Data",
        "Visualizations",
        "R",
        "Tableau",
        "Established",
        "Data",
        "architecture",
        "strategy",
        "practices",
        "standards",
        "roadmaps",
        "development",
        "presentation",
        "data",
        "analytics",
        "datahub",
        "prototype",
        "help",
        "members",
        "solutions",
        "team",
        "analysis",
        "Business",
        "requirement",
        "Design",
        "Development",
        "level",
        "Lowlevel",
        "designs",
        "Unit",
        "Integration",
        "testing",
        "departments",
        "data",
        "needs",
        "requirements",
        "R",
        "packages",
        "knitr",
        "dplyr",
        "Causal",
        "Infer",
        "space",
        "time",
        "Environment",
        "R",
        "UNIX",
        "Python",
        "MLLib",
        "SAS",
        "regression",
        "regression",
        "Hadoop",
        "NoSQL",
        "MySQL",
        "OLTP",
        "Random",
        "forest",
        "OLAP",
        "HDFS",
        "ODS",
        "Data",
        "Analyst",
        "Grey",
        "Hound",
        "Dallas",
        "TX",
        "March",
        "December",
        "Description",
        "Greyhound",
        "bus",
        "carrier",
        "destinations",
        "North",
        "America",
        "Behavior",
        "users",
        "website",
        "amount",
        "data",
        "ability",
        "parse",
        "task",
        "capabilities",
        "data",
        "warehouse",
        "users",
        "solutions",
        "efficiency",
        "data",
        "warehousing",
        "aim",
        "project",
        "visibility",
        "metrics",
        "operations",
        "company",
        "insights",
        "improvements",
        "business",
        "level",
        "analysis",
        "application",
        "data",
        "science",
        "Responsibilities",
        "requirement",
        "gathering",
        "data",
        "analysis",
        "Business",
        "users",
        "reporting",
        "requirements",
        "BI",
        "user",
        "community",
        "Created",
        "EntityRelationship",
        "Diagrams",
        "tables",
        "data",
        "PKs",
        "tables",
        "Star",
        "Schema",
        "building",
        "data",
        "model",
        "Dimensional",
        "Models",
        "data",
        "models",
        "Claim",
        "types",
        "HIPAA",
        "Standards",
        "onemany",
        "manymany",
        "Entity",
        "relationships",
        "data",
        "modeling",
        "Data",
        "warehouse",
        "Experience",
        "MDM",
        "team",
        "business",
        "operations",
        "organization",
        "Primary",
        "Key",
        "Foreign",
        "Key",
        "relationships",
        "entities",
        "areas",
        "ETL",
        "routines",
        "SSIS",
        "packages",
        "package",
        "development",
        "process",
        "control",
        "flow",
        "packages",
        "Big",
        "Data",
        "Architects",
        "Big",
        "Data",
        "Platform",
        "organization",
        "Hive",
        "platform",
        "Hive",
        "Data",
        "Models",
        "training",
        "documentation",
        "clients",
        "needs",
        "curriculum",
        "client",
        "techniques",
        "Postgre",
        "SQL",
        "role",
        "design",
        "architecture",
        "development",
        "user",
        "interface",
        "objects",
        "Qlik",
        "View",
        "applications",
        "data",
        "sources",
        "SQL",
        "Server",
        "Oracle",
        "files",
        "Dashboard",
        "Business",
        "users",
        "teams",
        "KPIs",
        "Key",
        "Performance",
        "Indicators",
        "data",
        "sources",
        "data",
        "ETL",
        "extract",
        "transform",
        "load",
        "data",
        "performance",
        "Deliver",
        "end",
        "mapping",
        "source",
        "Guide",
        "wire",
        "application",
        "CDW",
        "legacy",
        "systems",
        "coverages",
        "Landing",
        "Zone",
        "wire",
        "Pack",
        "data",
        "Source",
        "Tables",
        "Operational",
        "Data",
        "Source",
        "tables",
        "Transformation",
        "Cleansing",
        "Logic",
        "Data",
        "Accuracy",
        "Data",
        "Analysis",
        "Data",
        "Quality",
        "checks",
        "data",
        "data",
        "type",
        "inconsistencies",
        "source",
        "systems",
        "target",
        "system",
        "Mapping",
        "Documents",
        "tableau",
        "dashboards",
        "Claims",
        "forecast",
        "reference",
        "lines",
        "InformaticaPower",
        "center",
        "Informatica",
        "Data",
        "Quality",
        "IDQ",
        "application",
        "merging",
        "process",
        "reports",
        "users",
        "Tableau",
        "data",
        "sources",
        "reporting",
        "requirements",
        "data",
        "warehouse",
        "support",
        "documentation",
        "production",
        "support",
        "testing",
        "team",
        "Environment",
        "Erwin82",
        "Oracle",
        "g",
        "OBIEE",
        "Crystal",
        "Reports",
        "Toad",
        "Sybase",
        "Power",
        "Designer",
        "Datahub",
        "MS",
        "Visio",
        "DB2",
        "QlikView",
        "Informatica",
        "Data",
        "Analyst",
        "Unisys",
        "Global",
        "Services",
        "Bengaluru",
        "Karnataka",
        "December",
        "February",
        "Description",
        "Unisys",
        "information",
        "technology",
        "company",
        "highperformance",
        "solutions",
        "businesses",
        "governments",
        "Earth",
        "Unisys",
        "offerings",
        "security",
        "software",
        "services",
        "transformation",
        "workplace",
        "services",
        "industry",
        "applications",
        "services",
        "software",
        "environments",
        "highintensity",
        "enterprise",
        "computing",
        "Responsibilities",
        "methods",
        "regression",
        "classification",
        "tree",
        "tests",
        "data",
        "visualization",
        "techniques",
        "Python",
        "Performed",
        "Data",
        "Analysis",
        "Data",
        "Wrangling",
        "development",
        "algorithms",
        "R",
        "Python",
        "data",
        "mining",
        "analysis",
        "Analysis",
        "customer",
        "data",
        "data",
        "MySQL",
        "MS",
        "Access",
        "insights",
        "improvements",
        "customer",
        "experience",
        "pandas",
        "NumPy",
        "packages",
        "Python",
        "data",
        "collection",
        "distribution",
        "processes",
        "reporting",
        "capabilities",
        "line",
        "sight",
        "performance",
        "trends",
        "metrics",
        "data",
        "analysis",
        "data",
        "manipulation",
        "data",
        "transformation",
        "data",
        "mapping",
        "source",
        "data",
        "Understanding",
        "adherence",
        "principles",
        "data",
        "quality",
        "management",
        "metadata",
        "lineage",
        "business",
        "definitions",
        "demand",
        "filter",
        "outliersexceptions",
        "forecasting",
        "algorithm",
        "base",
        "plan",
        "variance",
        "improvement",
        "opportunities",
        "demand",
        "signal",
        "forecast",
        "data",
        "visualization",
        "package",
        "Python",
        "phases",
        "research",
        "data",
        "collection",
        "data",
        "data",
        "mining",
        "development",
        "models",
        "visualizations",
        "Examine",
        "customer",
        "feedback",
        "activity",
        "use",
        "fraud",
        "combination",
        "text",
        "analytics",
        "modeling",
        "classification",
        "understanding",
        "Software",
        "Development",
        "Life",
        "Cycle",
        "SDLC",
        "AgileScrum",
        "methodology",
        "Software",
        "Development",
        "iteration",
        "Improvisations",
        "maintenance",
        "solutions",
        "MS",
        "Visio",
        "MS",
        "Project",
        "team",
        "project",
        "quality",
        "plan",
        "risk",
        "management",
        "requirements",
        "management",
        "change",
        "management",
        "defect",
        "management",
        "change",
        "management",
        "management",
        "Environment",
        "MySQL",
        "Python",
        "Pandas",
        "NumPy",
        "packages",
        "R",
        "MS",
        "Visio",
        "MS",
        "Project",
        "MS",
        "Access",
        "Data",
        "Analyst",
        "Python",
        "Developer",
        "Hidden",
        "Brains",
        "Hyderabad",
        "Telangana",
        "January",
        "November",
        "Description",
        "Hidden",
        "Brains",
        "InfoTech",
        "Pvt",
        "Ltd",
        "Enterprise",
        "Web",
        "Mobile",
        "Apps",
        "Development",
        "Company",
        "industry",
        "experience",
        "decade",
        "plethora",
        "services",
        "customers",
        "advantage",
        "generation",
        "delivery",
        "models",
        "Responsibilities",
        "transformation",
        "logic",
        "BI",
        "tools",
        "Informatica",
        "data",
        "transformation",
        "layers",
        "Data",
        "warehouse",
        "Performed",
        "code",
        "development",
        "help",
        "Python",
        "library",
        "database",
        "users",
        "ETL",
        "jobs",
        "PostgreSQL",
        "SQL",
        "servers",
        "procedures",
        "views",
        "result",
        "sets",
        "reporting",
        "requirements",
        "data",
        "integrity",
        "excel",
        "formulas",
        "lookup",
        "functions",
        "pivot",
        "table",
        "Statements",
        "data",
        "Hands",
        "experience",
        "queries",
        "SQL",
        "R",
        "transform",
        "load",
        "ETL",
        "data",
        "datasets",
        "Data",
        "Staging",
        "AB",
        "testing",
        "BIAnalytics",
        "team",
        "data",
        "extraction",
        "analysis",
        "dashboards",
        "analysis",
        "researchers",
        "insights",
        "data",
        "insights",
        "decision",
        "support",
        "tools",
        "executives",
        "decision",
        "improvement",
        "strategies",
        "KPIs",
        "business",
        "areas",
        "summaries",
        "charts",
        "graphs",
        "team",
        "stakeholders",
        "findings",
        "excel",
        "reports",
        "Credit",
        "data",
        "warehouse",
        "creation",
        "Risk",
        "Assessment",
        "loans",
        "Performed",
        "competitor",
        "customer",
        "analysis",
        "risk",
        "pricing",
        "analysis",
        "results",
        "credit",
        "card",
        "holders",
        "basis",
        "management",
        "reports",
        "MS",
        "Excel",
        "sales",
        "metrics",
        "VLOOKUP",
        "Pivot",
        "tables",
        "business",
        "metrics",
        "calculations",
        "methodologies",
        "Environment",
        "Excel",
        "R",
        "Informatica",
        "Power",
        "Center",
        "Python",
        "PostgreSQL",
        "MS",
        "SQL",
        "Server",
        "Education",
        "Bachelors",
        "Skills",
        "CASSANDRA",
        "HDFS",
        "IMPALA",
        "MAPREDUCE",
        "HBASE",
        "KAFKA",
        "ELASTICSEARCH",
        "ETL",
        "FLUME",
        "HADOOP",
        "INFORMATICA",
        "NLP",
        "REDIS",
        "TABLEAU",
        "SERVER",
        "TERADATA",
        "DATA",
        "DATABASE",
        "DATABASE",
        "Additional",
        "Information",
        "Technical",
        "Skills",
        "Programming",
        "Scripting",
        "Languages",
        "R",
        "C",
        "C",
        "JCL",
        "COBOL",
        "HTML",
        "CSS",
        "JSP",
        "Java",
        "Script",
        "Databases",
        "SQL",
        "Hive",
        "Impala",
        "Pig",
        "Spark",
        "SQL",
        "Databases",
        "SQLServer",
        "SQL",
        "MS",
        "Access",
        "HDFS",
        "HBase",
        "Teradata",
        "Netezza",
        "MongoDB",
        "Cassandra",
        "Statistical",
        "Software",
        "SPSS",
        "R",
        "SAS",
        "Web",
        "Packages",
        "ggplot2",
        "dplyr",
        "Rweka",
        "RCurl",
        "tm",
        "C50",
        "twitteR",
        "NLP",
        "Reshape2",
        "rjson",
        "plyr",
        "numPy",
        "sciPy",
        "matplot",
        "lib",
        "Beautiful",
        "Soup",
        "Rpy2",
        "sqlalchemy",
        "data",
        "Ecosystem",
        "HDFS",
        "PIG",
        "MapReduce",
        "HIVE",
        "SQOOP",
        "FLUME",
        "HBase",
        "Storm",
        "Kafka",
        "Elastic",
        "Search",
        "Redis",
        "Flume",
        "Storm",
        "Kafka",
        "Elastic",
        "Search",
        "Redis",
        "Flume",
        "Scoop",
        "Statistical",
        "Methods",
        "Time",
        "Series",
        "regression",
        "models",
        "confidence",
        "intervals",
        "component",
        "analysis",
        "Dimensionality",
        "Reduction",
        "BI",
        "Tools",
        "Tableau",
        "Tableau",
        "server",
        "Tableau",
        "Reader",
        "SAP",
        "Business",
        "OBIEE",
        "QlikView",
        "SAP",
        "Business",
        "Intelligence",
        "Amazon",
        "Redshift",
        "Azure",
        "Data",
        "Warehouse",
        "Database",
        "Design",
        "Tools",
        "Data",
        "Modeling",
        "Erwin",
        "r",
        "8x",
        "Rational",
        "Rose",
        "ERStudio",
        "MS",
        "Visio",
        "SAP",
        "Power",
        "designer",
        "Cloud",
        "ETL",
        "Tools",
        "AWS",
        "S3",
        "EC2",
        "Informatica",
        "Power",
        "Centre",
        "SSIS",
        "Operating",
        "System",
        "Linux",
        "Unix",
        "Macintosh",
        "HD",
        "Red",
        "Hat",
        "Big",
        "Data",
        "Grid",
        "Technologies",
        "Cassandra",
        "Coherence",
        "Mongo",
        "DB",
        "Zookeeper",
        "Titan",
        "Elasticsearch",
        "Storm",
        "Kafka",
        "Hadoop",
        "Tools",
        "Utilities",
        "SQL",
        "Server",
        "Management",
        "Studio",
        "SQL",
        "Server",
        "Enterprise",
        "Manager",
        "SQL",
        "Server",
        "Profiler",
        "Import",
        "Export",
        "Wizard",
        "Visual",
        "StudioNet",
        "Microsoft",
        "Management",
        "Console",
        "Visual",
        "Source",
        "Safe",
        "DTS",
        "Crystal",
        "Reports",
        "Power",
        "Pivot",
        "ProClarity",
        "Microsoft",
        "Office",
        "Excel",
        "Power",
        "Pivot",
        "Excel",
        "Data",
        "Explorer",
        "Tableau",
        "Project1",
        "Role",
        "Data",
        "Scientist"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T21:42:47.356072",
    "resume_data": "Data Scientist Consultant Data Scientist Consultant Data scientist Birmingham AL 8 years of IT experience which includes Machine Learning Data mining with large datasets of Structured and Unstructured data Data Acquisition Data Validation Predictive modeling Data Visualization A deep understanding of Statistical Modeling Multivariate Analysis Big data analytics and Standard Procedures Highly efficient in Dimensionality Reduction methods such as PCA Principal component Analysis Factor Analysis etc Implemented bootstrapping methods such as Random Forests Classification KMeans Clustering KNN NaiveBayes SVM Decision Tree BFS Linear and Logistic Regression Methods The experience of working in text understanding classification pattern recognition recommendation systems targeting systems and ranking systems using Python Experience with Natural Language Processing NLP Proficiency in application of statistical prediction modeling machine learning classification techniques and econometric forecasting techniques Indepth knowledge of statistical procedures that are applied in Supervised Unsupervised problems Experience in the application of Neural Network Support Vector Machines SVM and Random Forest Experienced in working with advanced analytical teams to design build validate and refresh data models that enable the next generation of sophisticated solutions for global clients Extensively worked on Python 3527 NumPy Pandas Matplotlib NLTK and Scikit learn Extensive experience in Text Analytics developing different Statistical Machine Learning Data Mining solutions to various business problems and generating data visualizations using R Python and Tableau Experience in implementing data analysis with various analytic tools such as Anaconda 40 Jupyter Notebook 4X R 30ggplot2 and Excel Experience in designing star schema Snowflake schema for Data Warehouse ODS architecture Hands on experience in business understanding data understanding and preparation of large databases Experience in working with relational databases MySQL Oracle with advanced SQL programming skills Expertise in transforming business requirements into analytical models designing algorithms building models developing data mining and reporting solutions that scales across massive volume of structured and unstructured data Experience in using various packages in Rand pythonlike ggplot2 caret dplyr Rweka gmodels RCurl tm C50 twitter NLP Reshape2 rjson dplyr pandas NumPy Seaborn SciPy Matplotlib Scikitlearn Beautiful Soup Rpy2 Extensive experience in Text Analytics generating data visualizations using R Python and creating dashboards using tools like Tableau Hands on experience with big data tools like Hadoop Spark Hive Pig Impala PySpark Spark SQL Experience in using one or more cloud computing frameworks such as AWS Azure Google Cloud etc Mapping and tracing data from system to system in order to establish data hierarchy and lineage Experience with distributed datacomputing tools MapReduce Hadoop Hive Spark MySQL Worked on Tableau to create dashboards and visualizations Proficiency in various type of optimization Market Mix modeling Segmentation Time Series Price Promo models etc Identifiescreates the appropriate algorithm to discover patterns validate their findings using an experimental and iterative approach Strong skills in statistical methodologies such as AB testExperiment design Hypothesis test ANOVA Crosstabs Ttests and Correlation Techniques Applies advanced statistical and predictive modeling techniques to build maintain and improve on multiple realtime decision systems Experience in designing stunning visualizations using Tableau software and publishing and presenting dashboards Storyline on web and desktop platforms Proficiency in SAS Base SAS Enterprise Guide Enterprise Miner Excellent communication skills verbal and written to communicate with clients and team prepare deliver effective presentations Strong experience in Software Development Life Cycle SDLC including Requirements Analysis Design Specification and Testing as per Cycle in both Waterfall and Agile methodologies Strong experience in interacting with stakeholderscustomers gathering requirements through interviews workshops and existing system documentation or procedures defining business processes identifying and analyzing risks using appropriate templates and analysis tools Closely works with product managers Service development managers and product development team in productizing the algorithms developed Work Experience Data Scientist Consultant BBVA Compass Birmingham AL August 2018 to Present Description BBVA Compass understands that every individual and company has unique dreams and ambitions needs and wants We realize that few take the same path in the faster busier and more complex world we live in We get it Whichever path you choose and whenever you need us we want to create opportunities for your bright future From the smallest moment to the largest personal or professional life event BBVA Compass is there for you Responsibilities Performed Data Profiling to learn about behavior with various features such as traffic pattern location and time Date and Time etc Extensively used Pythons multiple data science packages like Pandas NumPy Matplotlib SciPy Scikit learn and NLTK Worked on data cleaning data preparation and feature engineering with Python 3X In depth understanding of NLP for tokenization lemmatization and stemming using NLTK Spacy Pattern libraries Experience in evaluating and proposing solutions to NLP problems through various approaches Deep background in information retrieval OCR SpeechtoText etc Natural Language Processing NLP knowledge representation or computational linguistics Understanding of NLP techniques for text representation semantic extraction techniques data structures and modeling Experience in Java Python and NLPMLframeworks and libraries Worked to apply a broad array of capabilities spanning machine learning statistics textminingNLP and modeling to extract insights to structured and unstructured healthcare data sources preclinical clinical trial and complementary real world information streams to apply a broad array of capabilities spanning machine learning statistics textminingNLP and modeling to extract insights to structured and unstructured healthcare data sources preclinical clinical trial and complementary real world information streams Developed User Defined Functions in Python for rapid analysis and performed data imputation using Scikit learnpackage in Python Developed the applications models used appropriate algorithms for arriving at the required insights by analyzing business requirements Worked with unsupervised Kmeans DBSCAN and supervised learning techniques Regression Classification for feature engineering and did Principal Component Analysis for dimensionality reduction of features Used the Classification machine learning algorithms Nave Bayes Linear regression Logistic regression SVM Neural Networks and used Clustering Algorithm K Means Performed text classification task using NLTK package and implemented various natural language processing techniques Worked collaboratively with senior management to develop strategy and approach to defining business challenges to be answered by data science Established partnerships with product and engineering teams and work closely with other teams Used SparkStreamingAPIs to perform necessary transformations and actions on the fly for building the common learner data model which gets the data from Kafka in near real time Worked on end to end pipe line in Spark and on Apache spark for analyzing the live streaming data Worked on Spark Python modules for machine learning and predictive analytics in Spark on AWS Explored and analyzed the customer specific features by using SparkSQL Created Hive scripts to create external internal data tables on Hive Worked on creating datasets to load data into HIVE Worked on Tableau for data visualization to create reports dashboards for insights and business process improvement Created the dashboards and reports in tableau for visualizing the data in required format Environment Spark Apache Spark Hive Machine learning Python NumPy NLTK Pandas SciPy MySQL Tableau Sqoop HBase HDFS Tableau DynamoDB Mongo DB SQL Server and ETL Data Scientist Visa Austin TX May 2017 to July 2018 Description Visa is a dynamic global enterprise and innovation is at the heart of everything we do Were looking for smart ambitious people to join our Austin team We are a global payments technology company working to enable consumers businesses banks and governments to use digital currency Responsibilities Analyzed Pandas NumPy seaborn SciPy Matplotlib Scikitlearn NLTK in Python for developing various machine learning algorithms and utilized machine learning algorithms such as linear regression multivariate regression naive Bayes Random Forests Kmeans KNN for data analysis Demonstrated experience in design and implementation of Statistical models Predictive models enterprise data model metadata solution and data life cycle management in both RDBMS Big Data environments Utilized domain knowledge and application portfolio knowledge to play a key role in defining the future state of large business technology programs Developed MapReduceSparkPython modules for machine learning predictive analytics in Hadoop on AWS Implemented a Pythonbased distributed random forest via Python streaming Performed Source System Analysis database design data modeling for the warehouse layer using MLDM concepts and package layer using Dimensional modeling Created ecosystem models eg conceptual logical physical canonical that are required for supporting services within the enterprise data architecture conceptual data model for defining the major subject areas used ecosystem logical model for defining standard business meaning for entities and fields and an ecosystem canonical model for defining the standard messages and formats to be used in data integration services throughout the ecosystem Developed LINUXShell scripts by using NZSQLNZLOAD utilities to load data from flat files to Netezza database Designed and implemented system architecture for AmazonEC2 based cloudhosted solution for client Tested Complex ETL Mappings and Sessions based on business user requirements and business rules to load data from source flat files and RDBMS tables to target tables Hands on database design relational integrity constraints OLAP OLTP Cubes and Normalization 3NF and Denormalization of database Developed MapReduceSpark Python modules for machine learning predictive analytics in Hadoop on AWS Worked on customer segmentation using an unsupervised learning technique clustering Worked with various Teradata15 tools and utilities like TeradataViewpoint Multi Load ARC TeradataAdministrator BTEQ and other Teradata Utilities Utilized Spark Scala Hadoop HBase Kafka SparkStreaming MLlib Python a broad variety of machine learning methods including classifications regressions dimensionally reduction Analyzed large data sets apply machine learning techniques and develop predictive models statistical models and developing and enhancing statistical models by leveraging bestinclass modeling techniques Environment Erwin r96 Python SQL Oracle 12c Netezza SQL Server Informatica Java SSRS PLSQL TSQL Tableau MLlib regression Cluster analysis Scala NLP Cassandra Map Reduce Spark Kafka Mongo DB logistic regression Hadoop Hive Teradata0 random forest OLAP Azure Maria DB SAP CRM HDFS ODS NLTK SVM JSON Tableau XML AWS Data Scientist Black Tree Healthcare Consulting King of Prussia PA January 2016 to April 2017 Description Black Tree Healthcare Consulting provides revenue cycle outsourcing and clinical consulting services to the home health hospice and skilled nursing industries The primary goal of the project was to determine the most efficient inputoutput ratio through data modeling and analysis thus providing the best resource combination Responsibilities Incident and problem management coordinating resolution of data movement disruptions Capacity analysis and planning for a sizable application server environment Performance metering and tuning for application servers database monitors and alerts Risk identification and evaluation managing and improving upon internal controls which mitigate risks Working with development teams to identify enhancement opportunities and resolve code defects Code deployment and configuration management Coordination and execution of changes within a complex testing environment Data collection procedures enhancement to include information that is relevant for building analytic systems processing cleansing and verifying the integrity of data used for analysis Building analytic models using a variety of techniques such as logistic regression risk scorecards and pattern recognition technologies Analyze and understand large amounts of data to determine suitability for use in models and then work to segment the data create variables build models and test those models Excellent understanding of machine learning techniques and algorithms such as Logistic Regression SVM Random Forests Deep Learning etc In depth understanding and experience in NLP and Deep learning Expertise in Sentiment Analysis Entity Extraction Document Classification Topic Modeling Natural Language Understanding NLU and Natural Language Generation NLG Leads all NLPNLG driven solutions for the project Design NLP models for searching structuredunstructured data in realnearreal time Interacts regularly with product team members ensuring successful integration of NLP solutions in the product architecture Keeps the solutions updated with the recent developments in NLPNLG Perform a proper EDA Univariateand bivariate analysis to understand the intrinsic effectcombined Performing adhoc analysis and presenting results in a clear manner and constant tracking of model performance Worked with Data governance Data quality Data lineage Data architect to design various models Designed data models and data flow diagrams using Erwin and MS Visio Independently coded new programs and designed Tables to load and test the program effectively for the given POCs Developed Implemented maintained the Conceptual Logical Physical Data Models using Erwin for ForwardReverse Engineered Databases Experience with common data science toolkits such as R Python Spark etc Applied statistics skills such as statistical sampling testing regression etc Work with technical and development teams to deploy models Build Model Performance Reports and Designing Technical Documentation to support each of the models for the product line Exploratory Data Analysis and Data Visualizations using R and Tableau Established Data architecture strategy best practices standards and roadmaps Lead the development and presentation of a data analytics datahub prototype with the help of the other members of the emerging solutions team Involved in analysis of Business requirement Design and Development of High level and Lowlevel designs Unit and Integration testing Interacted with the other departments to understand and identify data needs and requirements Worked with several R packages including knitr dplyr SparkR Causal Infer space time Environment R 3x UNIX Python 352 MLLib SAS regression logistic regression Hadoop NoSQL MySQL OLTP Random forest OLAP HDFS ODS Data Analyst Grey Hound Dallas TX March 2014 to December 2015 Description Greyhound is an intercity bus common carrier serving over 3800 destinations across North America Behavior of users on our website generate large amount of data which challenges our ability to gather parse analyze and infer from it So a major task is to implement new capabilities for reporting from existing data warehouse and empower users with better solutions to increase operational efficiency for data warehousing needs The aim of the project is to gain more visibility into key metrics of operations in the company and to gain insights into how to drive improvements and take the business to a whole new level with advanced analysis through the application of data science Responsibilities Involved in requirement gathering data analysis and Interacted with Business users to understand the reporting requirements analyzing BI needs for the user community Created EntityRelationship Diagrams grouped and created the tables validated the data identified PKs for lookup tables Involved in modeling Star Schema methodologies in building and designing the logical data model into Dimensional Models Created and maintained logical dimensional data models for different Claim types and HIPAA Standards Implemented onemany manymany Entity relationships in the data modeling of Data warehouse Experience working with MDM team with various business operations involved within the organization Identify the Primary Key Foreign Key relationships across the entities and across subject areas Developed ETL routines using SSIS packages to plan an effective package development process and design the control flow within the packages Worked with Big Data Architects for setting up Big Data Platform in the organization and on Hive platform to create Hive Data Models Developed customized training documentation based on each clients technical needs and built a curriculum to help each client learn both basic and advanced techniques for using Postgre SQL Took an active role in the design architecture and development of user interface objects in Qlik View applications Connected to various data sources like SQL Server Oracle and flat files Presented the Dashboard to Business users and crossfunctional teams define KPIs Key Performance Indicators and identify data sources Designed data flows that ETL extract transform and load data by optimizing SSIS performance Deliver end to end mapping from source Guide wire application to target CDW and legacy systems coverages to Landing Zone and to Guide wire Reporting Pack Involved in loading the data from Source Tables to Operational Data Source tables using Transformation and Cleansing Logic Performed the Data Accuracy Data Analysis Data Quality checks before and after loading the data Resolved the data type inconsistencies between the source systems and the target system using the Mapping Documents Generated tableau dashboards for Claims with forecast and reference lines Designed developed implemented and maintained InformaticaPower center and Informatica Data Quality IDQ application for matching and merging process Created adhoc reports to users in Tableau by connecting various data sources Worked on the reporting requirements for the data warehouse Created support documentation and worked closely with production support and testing team Environment Erwin82 Oracle 11g OBIEE Crystal Reports Toad Sybase Power Designer Datahub MS Visio DB2 QlikView 116 Informatica Data Analyst Unisys Global Services Bengaluru Karnataka December 2012 to February 2014 Description Unisys is a global information technology company that builds highperformance securitycentric solutions for the most demanding businesses and governments on Earth Unisys offerings include security software and services digital transformation and workplace services industry applications and services and innovative software operating environments for highintensity enterprise computing Responsibilities Implemented and updated analytical methods such as regression modelling classification tree statistical tests and data visualization techniques with Python Performed exploratory Data Analysis Data Wrangling and development of algorithms in R and Python for data mining and analysis Analysis of customer data and other operational data in MySQL and MS Access to provide insights that enable improvements to customer experience Utilized pandas and NumPy packages in Python to improve data collection and distribution processes as well as to enhance reporting capabilities to provide clear line of sight into key performance trends and metrics Performed data analysis data manipulation data transformation and data mapping of source data from the MySQLserver Understanding and adherence to the principles of data quality management including metadata lineage and business definitions Analyzed historical demand filter out outliersexceptions identify the most appropriate statistical forecasting algorithm develop base plan understand variance propose improvement opportunities and incorporate demand signal into forecast and executed data visualization by using plotly package in Python Participated in all phases of research including data collection data cleaning data mining development models and visualizations Examine customer feedback and activity for use in detecting or confirming fraud using a combination of text analytics statistical modeling and classification Deep understanding of Software Development Life Cycle SDLC as well as AgileScrum methodology to accelerate Software Development iteration Improvisations and maintenance to existing automated solutions Used MS Visio MS Project to assist the team in project planning quality plan risk management requirements management change management defect management change management and release management Environment MySQL Statistical modelling Python libraries Pandas NumPy packages R MS Visio MS Project MS Access Data Analyst Python Developer Hidden Brains Hyderabad Telangana January 2011 to November 2012 Description Hidden Brains InfoTech Pvt Ltd is an Enterprise Web Mobile Apps Development Company With an industry experience of over a decade we offer a plethora of clientcentric services by enabling customers to achieve competitive advantage through flexible and next generation global delivery models Responsibilities Designed and developed transformation logic for BI tools Informatica for data transformation into various layers of Data warehouse Performed code development with the help of internal Python library that speeds up database querying and allow users to write more resilient ETL jobs Implemented PostgreSQL SQL servers to develop stored procedures views to create result sets to meet varying reporting requirements Ensured data integrity using advanced excel formulas lookup functions pivot table If Statements etc for analyzing data Hands on experience in writing queries in SQL and R to extract transform and load ETL data from large datasets using Data Staging Participated in the AB testing conducted by BIAnalytics team for data extraction and exploratory analysis Generated dashboards and presented the analysis to researchers explaining insights on the data Provided analytical insights and decision support tools for executives for accurate decision making by identifying measuring and recommending improvement strategies for KPIs across all business areas Submitted summaries charts and graphs to team and stakeholders that help to interpret findings based on complex excel reports Responsible for Credit data related warehouse creation that could help with Risk Assessment for Commercial loans Performed competitor and customer analysis risk and pricing analysis and forecasted results for credit card holders on demographical basis Worked in manipulating various management reports in MS Excel for sales metrics using VLOOKUP and Pivot tables Involved in estimating defining implementing and utilizing business metrics calculations and methodologies Environment Excel 2010 R Informatica Power Center 90 Python PostgreSQL MS SQL Server 200 Education Bachelors Skills CASSANDRA HDFS IMPALA MAPREDUCE SQOOP HBASE KAFKA ELASTICSEARCH ETL FLUME HADOOP INFORMATICA MONGODB NLP REDIS TABLEAU SERVER TERADATA DATA MODELING DATABASE DATABASE DESIGN Additional Information Technical Skills Programming Scripting Languages R C C JAVA JCL COBOL HTML CSS JSP Java Script Databases SQL Hive Impala Pig Spark SQL Databases SQLServer My SQL MS Access HDFS HBase Teradata Netezza MongoDB Cassandra Statistical Software SPSS R SAS Web Packages ggplot2 caret dplyr Rweka gmodels RCurl tm C50 twitteR NLP Reshape2 rjson plyr pandas numPy seaborn sciPy matplot lib scikitlearn Beautiful Soup Rpy2 sqlalchemy Big data Ecosystem HDFS PIG MapReduce HIVE SQOOP FLUME HBase Storm Kafka Elastic Search Redis Flume Storm Kafka Elastic Search Redis Flume Scoop Statistical Methods Time Series regression models splines confidence intervals principal component analysis and Dimensionality Reduction bootstrapping BI Tools Tableau Tableau server Tableau Reader SAP Business Objects OBIEE QlikView SAP Business Intelligence Amazon Redshift or Azure Data Warehouse Database Design Tools and Data Modeling Erwin r 96 95 91 8x Rational Rose ERStudio MS Visio SAP Power designer Cloud ETL Tools AWS S3 EC2 Informatica Power Centre SSIS Operating System Windows Linux Unix Macintosh HD Red Hat Big Data Grid Technologies Cassandra Coherence Mongo DB Zookeeper Titan Elasticsearch Storm Kafka Hadoop Tools and Utilities SQL Server Management Studio SQL Server Enterprise Manager SQL Server Profiler Import Export Wizard Visual StudioNet Microsoft Management Console Visual Source Safe 60 DTS Crystal Reports Power Pivot ProClarity Microsoft Office Excel Power Pivot Excel Data Explorer Tableau JIRASparkMLlib Project1 Role Data Scientist",
    "unique_id": "d8faf691-be32-4957-ae5b-81704c644215"
}