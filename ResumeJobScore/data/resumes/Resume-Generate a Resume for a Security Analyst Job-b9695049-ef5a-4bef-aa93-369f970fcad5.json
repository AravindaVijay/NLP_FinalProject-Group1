{
    "clean_data": "Datawarehouse Architect Lead Datawarehouse Consultant Datawarehouse Architect Lead Datawarehouse Consultant Datawarehouse Architect Lead Datawarehouse Consultant Waste Management Houston TX Over 8 years of experience in all phases of Analysis Design Development Implementation and support Relational Data Warehousing applications for business requirements Architecture functions Experience in full life cycle implementation of Data Warehouses and Data Marting Over 7 years of experience in Netezza database in all phases of Analysis Design Development Implementation Architecting and Administration Worked for the projects in the areas of Informatica Data stage SSIS UNIX shell scripting Netezza appliance SQLServer COGNOS reporting SQLGen API Expertise in outstanding performance tuning of NZSQL TSQL statements using Explain Plan SQL trace for optimizing the cost and evaluating the performance Experience in translating business requirements into creation of database objects tables indexes constraints packages stored procedures functions and triggers using Oracle PLSQL tool and Netezza Tool Kit Extensive experience in analysis and design of database database modeling ER Diagrams normalization and denormalization Performing Analysis for complex problems and solving the problems Having good domain knowledge based applications such as Health Care Retail Marketing Transactional and Financial industries Experience in using versioning tools SVN and GIT Created scripts which will send notificationswarning emails which helped in smooth running of daily batch jobs Worked extensively with Visio and ER Studio Power Designer for Relational Dimensional Modeling and Data Migration projects Capturing responses API GET and PUT calls using UNIX shell scripts wget command line utility and save the successful response in FTP sites and automate them Parsing out Json files using UNIX Shell Script jq command line utility Proficient with the SQL development languages Knowledge on scheduling Autosys CORN DAC and Control M Knowledge on API calls and usage of the calls from clients and products Work Experience Datawarehouse Architect Lead Datawarehouse Consultant Carlson Wagonlit Travels Minnetonka MN November 2017 to Present Responsibilities Working on gathering different travel related data feeds Air Rail car for both Commercial and Government Implementing the multi tenancy architecture in Data lake and data planes using queues to gather data from multiple national and international travel agencies and clients source systems Working in Agile methodology with thin slicing environment 1 and 2 week sprints Creating Travel segment and Travel accountable Data Model components using Visio Create Users and assign permissions using Informatica Admin console and maintaining the repositories and domains Capturing responses API GET and PUT calls using UNIX shell scripts wget command line utility and save the successful response in FTP sites and automate them Parsing out Json files using Unix Shell Script jq command line utility and loaded the data in to Mart queues for ISOS Alerts Closely working relation with external and international clients to gather travel data for requirement and data distribution level agreements Working and implementing Multitenancy Commercial Local International and Government architecture Very good Knowledge of Informatica Power center for ETL Data transformation and Developer client Design develop and provide unit test data loading and data transformation programs related to Oracle Data Warehouses SQLserver and Pivotal GreenPlum Data Marts Performed unit performance and scalability testing at data tool and DB level Resolved clients data incidents and request incidents at data and transformation level Involved in Weekly BiWeekly and Monthly release activities to lower and upper environment Knowledge on using API interface and retrieve data from different API calls Created wrapper scripts for Informatica in UNIX shell script for scheduling purposes Architecture and Working knowledge on scheduling ControlM Autosys Crontab and TMUX Environment Oracle Exadata TOAD Json XML SQL Developer SQLServer 2016 Aginity Work Bench for GreenPlum Informatica 961101 Control_M Postman APIs UNIX SunOS Jr Datawarehouse Architect Lead Datawarehouse Engineer Morningstar Chicago IL April 2015 to November 2017 Responsibilities Design and develop data loading programs to support real time data loading to Netezza databases and SqlServer Marts Worked on ETLInformatica jobs from data collection to data integration mainly from source documents to Netezza and Netezza to SqlServer Marts Working in Agile methodology Fast phased environment Worked on mapping document gathering requirements and process flow diagrams Create database objects in lower and upper level Environments Create Users and assign permissions based on the level of database access the user would need Hands on experience in Amazon Web Services console and products Designed and implemented Cloud solutions with AWS Virtual private cloud VPC Elastic Compute Cloud EC2 S3 Auto scaling RDS and other AWS services Worked closely with subject matter expertise to gather and understand the calculations and requirements Created database objects in lower and upper level Environments Netezza and SQLServer Worked on the investment data and calculated the different breakdowns statistics Total Daily Price Primary Return Indexs at different universes for portfolio and securities Worked on Multitenancy architecture Very good Knowledge of Informatica ETL tool Administrator Designer and Monitor Design develop and provide unit test data loading and data transformation programs related to Netezza Data Warehouses and SQLserver Data Marts Performed unit testing integration testing regression testing system testing and scalability Resolved issuesbug fixes during testing phase Performance tuned the transformations using NZ Explain Plan Involved in BiWeekly and Monthly release activities to lower and upper environment Knowledge on using API interface and retrieve data from different API calls Working knowledge on scheduling ControlM Environment Netezza 702 and 711 SQLServer 2012 NZsql Nzload NZmigrate TSQL Aginity WorkBench Informatica 961 Control_M SqlServer Management tool Postman MorningstarDirect v2 and MorningstarDirectv3 Lead Datawarehouse Engineer Sr Consultant Fossil INC Richardson TX August 2014 to April 2015 Responsibilities Design develop and provide unit test data loading and data transformation programs related to Netezza Data Warehouses and Data Marts Worked on ETLDatastage jobs from Source to Target Worked on thin slicing in Agile methodology Create databases users and groups for Lower and upper level Environments Knowledge of Datastage Administrator client Designer client and Director client Performed unit testing integration testing regression testing and system testing Resolved issues during testing phase Performance tuned the transformations using ASYNC and Finalization Maintained system documentation of best database procedures practices and standards Environment Netezza 71 UNIX Shell Scripting NZsql Nzload NZmigrate Netezza Aginity Work Bench nzDIF 39 Datastage 91 Netezza Database Administrator DBA Saint Petersburg FL February 2014 to August 2014 Responsibilities Prepared strategic plans for data warehousing projects and related quality documentation Assisted in designing and implementation of detailed data warehouse models and mappings Provided technical expertise to technologies in fields of data warehouse and relevant analytics Performed establishment and maintenance of databases for assigned projects as per business requirements Implemented procedures for daily database monitoring backup and configuration changes Developed queries backups and session management and log management records in detailed manner Executed processes for management of migration of tables schemas database as per defined and assigned locations Working in Agile methodology Create databases users and groups for Lower and upper level Environments Proficient with UNIXLINUX commands Suggested enhancement methods rollout and upgrades for maintenance of existing databases Maintained system documentation of best database procedures practices and standards Resolved database problems and issues by troubleshooting and responding to service requests Environment Netezza 704X Netezza 60XX UNIX Shell Scripting NZsql Nzload NZmigrate Netezza TwinFin 24 Netezza Mustang Netezza N2001 Aginity Work Bench WINSQL Lead Datawarehouse EngineerDatawarehouse Engineer PremierInc Healthcare Solutions Charlotte NC February 2011 to February 2014 Premier Connect Enterprise Claims Carolina Health Partners Responsibilities Worked on mapping document and gathering requirements Design and develop data loading programs to support real time data loading to Netezza Data Marts Worked on ETLDatastage jobs from Source to Target Worked on thin slicing in Agile methodology Create databases users and groups for Lower and upper level Environments Worked loading claims data from the sourceInformed to the Dimensional Data Marts Worked on Multitenancy architecture Extensive knowledge of Bash and Korn scripting language Knowledge of Datastage Administrator client Designer client and Director client Design develop and provide unit test data loading and data transformation programs related to Netezza Data Warehouses and Data Marts Worked on creating reports in Cognos Extensive knowledge on scheduling Autosys Environment Netezza 7 UNIX Shell Scripting NZSQL SQL Netezza TwinFin 24 Aginity Work Bench Datastage 92 Cognos 101 Premier Connect Enterprise Clinical Carolina Health Partners Responsibilities Design develop and provide unit test data loading and data transformation programs related to Netezza Data Warehouses and Data Marts Design Trickle feed data loading programs to support near real time data loading of Netezza Data Marts Designs data transformations and exceptions processing in addition to refresh and replay data loading capabilities Using SSIS building high performance data integration solutions including extraction transformation and load ETL packages for data bases and Scheduling the SSIS packages and Jobs Development coding and unit tests the data loading and transformation programs Codes orchestration programs that schedule the data loading and data transformations routines Unit tests all data loading and transformation programs Extensive knowledge of the nzDIF Data Integration Framework Extensive knowledge of Bash shell scripting language Extensive knowledge on scheduling Autosys Environment Netezza 6x 7 UNIX Shell Scripting NZSQL sql Netezza TwinFin 24 Brightlight Framework 38 39 Aginity Work Bench AQT Silktest tool SSIS 2012 SQLServer 2012 TSQL IPPSYMPONYSADVISOR Responsibilities Worked extensively on Brightlight Framework 35 38 Used different Framework assets like nzf app_init Worked on Application asserts like setup_cfgsh run log ddl meta data auto Involved in admin workflows like db_create db_init Involved in dm workflows like ddl_convert ora_ddl_convert and db_install Involved in dev workflows like  db_xfr_templates db_referential_config Worked extensively on Aginity Work Bench and AQT Written transformations using shell scripting mainly bash Performed unit testing integration testing regression testing and system testing Resolved issues during testing phase Performance tuned the transformations using ASYNC and Finalization Created list files flow files and shell scripts sh Created data files for test cases using Silktest tool and creating manifest files Performed load test on transformations by using generated test data Worked on Cognos views by using the table sql Worked on the mapping documents Assisted QA in testing the QA test cases and resolving the issues Worked on updating the data model changes Worked on the triggers and RDM joins Wrote DIFF script for data model changes to find the difference between two scripts Worked in Agile Environment Environment Netezza 605 UNIX Shell Scripting MS Excel NZSQL Netezza TwinFin 24 Brightlight Framework 35 38 NZF Aginity Work Bench AQT Silktest tool triggers and RDM joins Physician Focus Project Responsibilities Worked extensively on SQL PLSQL Perl and UNIX shell scripting Analyzed the source system of ETL Maps Developed the ETL routines Designed and developed the factdimension entities Involved in the Unit testing Event Thread testing and System testing of the individual Written Unix Shell Scripting based on requirements Worked in Agile environment Migration of data from Oracle to Netezza Worked on Brightlight Framework 30 Worked extensively in NZSQL to migrate the data from Oracle to Netezza database Written Unix Shell Scripting based on requirements Performed unit testing of the shell scripts developed with various scenarios Performed integration testing and system testing Environment Netezza UNIX Shell Scripting MS Excel NZSQL Netezza TwinFin 12 Brightlight Framework 30 Education Master in Mechanical Engineering Sciences Lamar University Beaumont TX 2010 Bachelors in Mechanical Engineering BTech 2008 Jawaharlal Nehru Technological University Hyderabad Telangana Skills DATABASE 6 years DATABASES 6 years ETL 6 years EXTRACT TRANSFORM AND LOAD 6 years SCHEDULING 5 years Additional Information Technical Skills ETL Informatica 86961101 DATA STAGE 91 Scheduling Tool DAC Data Warehousing Admin Console Control_M Autosys Crontab and TMUX Reporting Tool COGNOS 9X 10X Databases Netezza 451 603 605 70X 71X NPS Oracle 11g10g9i8i Oracle Exadata Operating Systems Windows 03710 UNIX and LINUX Programming C SQL PLSQL XML Json Other Tools SQLLoader Oracle SQL Developer TOAD Eclipse MS Visio MS Excel Eclipse Platform APIPostman Aginity Workbench SQL server studio Scripting Languages UNIX Shell Scripting bash ksh SQL PLSQL Postgres and TSQL Methodologies ER Modeling Star Schema Snowflake Schema Hybrid Models Data warehousing Data Modeling Tool Microsoft Visio 20102016 ER Studio",
    "entities": [
        "Physician Focus Project Responsibilities Worked",
        "Control M Knowledge",
        "Informatica",
        "Codes",
        "Mechanical Engineering",
        "Travel",
        "API GET",
        "Analysis Design Development Implementation",
        "UNIX",
        "Informatica Power",
        "Target Worked",
        "Working",
        "Oracle Data Warehouses SQLserver",
        "db_install Involved",
        "log management",
        "Netezza",
        "Nehru Technological University",
        "Brightlight Framework",
        "Worked on Application",
        "API Expertise",
        "DAC",
        "Maintained",
        "Aginity Work Bench Datastage 92 Cognos",
        "Additional Information Technical Skills ETL",
        "BiWeekly",
        "Present Responsibilities Working",
        "Beaumont",
        "SSIS",
        "Health Care Retail Marketing Transactional and Financial industries Experience",
        "Informatica Admin",
        "AQT Silktest",
        "Visio Create Users",
        "Developed",
        "Bash",
        "NC",
        "UNIXLINUX",
        "log ddl meta data auto Involved",
        "ASYNC",
        "RDM",
        "Air Rail",
        "Data Modeling Tool",
        "Developer client Design",
        "Capturing",
        "NZSQL",
        "Oracle PLSQL",
        "Mechanical Engineering Sciences",
        "Worked",
        "ER Studio",
        "Netezza Worked",
        "Shell Scripting",
        "Weekly BiWeekly",
        "RDS",
        "Architecture",
        "Autosys",
        "Environments Create Users",
        "Oracle Exadata Operating Systems",
        "Agile",
        "Jawaharlal",
        "ISOS Alerts Closely",
        "Multitenancy",
        "API",
        "QA",
        "Relational Data Warehousing",
        "Finalization Created",
        "Created",
        "NZ Explain Plan Involved",
        "AWS",
        "Jobs Development",
        "Oracle",
        "SqlServer Management",
        "NPS Oracle",
        "Netezza Data Marts Worked",
        "db_create db_init Involved",
        "Netezza Data Marts Designs",
        "Charlotte",
        "SQL",
        "Netezza Tool Kit Extensive",
        "Amazon Web Services",
        "Environments Proficient",
        "LINUX Programming C SQL PLSQL",
        "Informatica Data",
        "ER Diagrams",
        "Postman",
        "db_referential_config Worked",
        "MS Visio MS Excel",
        "Netezza Data Warehouses",
        "Aginity Work Bench",
        "FTP",
        "SQL PLSQL Perl and UNIX",
        "TwinFin",
        "ETL",
        "Scripting Languages UNIX Shell Scripting",
        "Performed",
        "GIT Created",
        "Data Migration",
        "AQT Written",
        "NZF Aginity",
        "Creating Travel",
        "AWS Virtual",
        "PUT",
        "Data Model",
        "Autosys Environment Netezza",
        "NZsql Nzload NZmigrate Netezza Aginity Work Bench",
        "SVN",
        "Shell Scripting MS Excel",
        "Commercial and Government Implementing",
        "Postman MorningstarDirect v2 and MorningstarDirectv3",
        "Data",
        "Analysis Design Development Implementation Architecting and Administration Worked",
        "TOAD",
        "Suggested",
        "ETL Data",
        "Commercial Local International and Government",
        "Oracle Exadata",
        "Designer",
        "Data Warehouses",
        "NZsql Nzload NZmigrate TSQL Aginity WorkBench Informatica"
    ],
    "experience": "Experience in full life cycle implementation of Data Warehouses and Data Marting Over 7 years of experience in Netezza database in all phases of Analysis Design Development Implementation Architecting and Administration Worked for the projects in the areas of Informatica Data stage SSIS UNIX shell scripting Netezza appliance SQLServer COGNOS reporting SQLGen API Expertise in outstanding performance tuning of NZSQL TSQL statements using Explain Plan SQL trace for optimizing the cost and evaluating the performance Experience in translating business requirements into creation of database objects tables indexes constraints packages stored procedures functions and triggers using Oracle PLSQL tool and Netezza Tool Kit Extensive experience in analysis and design of database database modeling ER Diagrams normalization and denormalization Performing Analysis for complex problems and solving the problems Having good domain knowledge based applications such as Health Care Retail Marketing Transactional and Financial industries Experience in using versioning tools SVN and GIT Created scripts which will send notificationswarning emails which helped in smooth running of daily batch jobs Worked extensively with Visio and ER Studio Power Designer for Relational Dimensional Modeling and Data Migration projects Capturing responses API GET and PUT calls using UNIX shell scripts wget command line utility and save the successful response in FTP sites and automate them Parsing out Json files using UNIX Shell Script jq command line utility Proficient with the SQL development languages Knowledge on scheduling Autosys CORN DAC and Control M Knowledge on API calls and usage of the calls from clients and products Work Experience Datawarehouse Architect Lead Datawarehouse Consultant Carlson Wagonlit Travels Minnetonka MN November 2017 to Present Responsibilities Working on gathering different travel related data feeds Air Rail car for both Commercial and Government Implementing the multi tenancy architecture in Data lake and data planes using queues to gather data from multiple national and international travel agencies and clients source systems Working in Agile methodology with thin slicing environment 1 and 2 week sprints Creating Travel segment and Travel accountable Data Model components using Visio Create Users and assign permissions using Informatica Admin console and maintaining the repositories and domains Capturing responses API GET and PUT calls using UNIX shell scripts wget command line utility and save the successful response in FTP sites and automate them Parsing out Json files using Unix Shell Script jq command line utility and loaded the data in to Mart queues for ISOS Alerts Closely working relation with external and international clients to gather travel data for requirement and data distribution level agreements Working and implementing Multitenancy Commercial Local International and Government architecture Very good Knowledge of Informatica Power center for ETL Data transformation and Developer client Design develop and provide unit test data loading and data transformation programs related to Oracle Data Warehouses SQLserver and Pivotal GreenPlum Data Marts Performed unit performance and scalability testing at data tool and DB level Resolved clients data incidents and request incidents at data and transformation level Involved in Weekly BiWeekly and Monthly release activities to lower and upper environment Knowledge on using API interface and retrieve data from different API calls Created wrapper scripts for Informatica in UNIX shell script for scheduling purposes Architecture and Working knowledge on scheduling ControlM Autosys Crontab and TMUX Environment Oracle Exadata TOAD Json XML SQL Developer SQLServer 2016 Aginity Work Bench for GreenPlum Informatica 961101 Control_M Postman APIs UNIX SunOS Jr Datawarehouse Architect Lead Datawarehouse Engineer Morningstar Chicago IL April 2015 to November 2017 Responsibilities Design and develop data loading programs to support real time data loading to Netezza databases and SqlServer Marts Worked on ETLInformatica jobs from data collection to data integration mainly from source documents to Netezza and Netezza to SqlServer Marts Working in Agile methodology Fast phased environment Worked on mapping document gathering requirements and process flow diagrams Create database objects in lower and upper level Environments Create Users and assign permissions based on the level of database access the user would need Hands on experience in Amazon Web Services console and products Designed and implemented Cloud solutions with AWS Virtual private cloud VPC Elastic Compute Cloud EC2 S3 Auto scaling RDS and other AWS services Worked closely with subject matter expertise to gather and understand the calculations and requirements Created database objects in lower and upper level Environments Netezza and SQLServer Worked on the investment data and calculated the different breakdowns statistics Total Daily Price Primary Return Indexs at different universes for portfolio and securities Worked on Multitenancy architecture Very good Knowledge of Informatica ETL tool Administrator Designer and Monitor Design develop and provide unit test data loading and data transformation programs related to Netezza Data Warehouses and SQLserver Data Marts Performed unit testing integration testing regression testing system testing and scalability Resolved issuesbug fixes during testing phase Performance tuned the transformations using NZ Explain Plan Involved in BiWeekly and Monthly release activities to lower and upper environment Knowledge on using API interface and retrieve data from different API calls Working knowledge on scheduling ControlM Environment Netezza 702 and 711 SQLServer 2012 NZsql Nzload NZmigrate TSQL Aginity WorkBench Informatica 961 Control_M SqlServer Management tool Postman MorningstarDirect v2 and MorningstarDirectv3 Lead Datawarehouse Engineer Sr Consultant Fossil INC Richardson TX August 2014 to April 2015 Responsibilities Design develop and provide unit test data loading and data transformation programs related to Netezza Data Warehouses and Data Marts Worked on ETLDatastage jobs from Source to Target Worked on thin slicing in Agile methodology Create databases users and groups for Lower and upper level Environments Knowledge of Datastage Administrator client Designer client and Director client Performed unit testing integration testing regression testing and system testing Resolved issues during testing phase Performance tuned the transformations using ASYNC and Finalization Maintained system documentation of best database procedures practices and standards Environment Netezza 71 UNIX Shell Scripting NZsql Nzload NZmigrate Netezza Aginity Work Bench nzDIF 39 Datastage 91 Netezza Database Administrator DBA Saint Petersburg FL February 2014 to August 2014 Responsibilities Prepared strategic plans for data warehousing projects and related quality documentation Assisted in designing and implementation of detailed data warehouse models and mappings Provided technical expertise to technologies in fields of data warehouse and relevant analytics Performed establishment and maintenance of databases for assigned projects as per business requirements Implemented procedures for daily database monitoring backup and configuration changes Developed queries backups and session management and log management records in detailed manner Executed processes for management of migration of tables schemas database as per defined and assigned locations Working in Agile methodology Create databases users and groups for Lower and upper level Environments Proficient with UNIXLINUX commands Suggested enhancement methods rollout and upgrades for maintenance of existing databases Maintained system documentation of best database procedures practices and standards Resolved database problems and issues by troubleshooting and responding to service requests Environment Netezza 704X Netezza 60XX UNIX Shell Scripting NZsql Nzload NZmigrate Netezza TwinFin 24 Netezza Mustang Netezza N2001 Aginity Work Bench WINSQL Lead Datawarehouse EngineerDatawarehouse Engineer PremierInc Healthcare Solutions Charlotte NC February 2011 to February 2014 Premier Connect Enterprise Claims Carolina Health Partners Responsibilities Worked on mapping document and gathering requirements Design and develop data loading programs to support real time data loading to Netezza Data Marts Worked on ETLDatastage jobs from Source to Target Worked on thin slicing in Agile methodology Create databases users and groups for Lower and upper level Environments Worked loading claims data from the sourceInformed to the Dimensional Data Marts Worked on Multitenancy architecture Extensive knowledge of Bash and Korn scripting language Knowledge of Datastage Administrator client Designer client and Director client Design develop and provide unit test data loading and data transformation programs related to Netezza Data Warehouses and Data Marts Worked on creating reports in Cognos Extensive knowledge on scheduling Autosys Environment Netezza 7 UNIX Shell Scripting NZSQL SQL Netezza TwinFin 24 Aginity Work Bench Datastage 92 Cognos 101 Premier Connect Enterprise Clinical Carolina Health Partners Responsibilities Design develop and provide unit test data loading and data transformation programs related to Netezza Data Warehouses and Data Marts Design Trickle feed data loading programs to support near real time data loading of Netezza Data Marts Designs data transformations and exceptions processing in addition to refresh and replay data loading capabilities Using SSIS building high performance data integration solutions including extraction transformation and load ETL packages for data bases and Scheduling the SSIS packages and Jobs Development coding and unit tests the data loading and transformation programs Codes orchestration programs that schedule the data loading and data transformations routines Unit tests all data loading and transformation programs Extensive knowledge of the nzDIF Data Integration Framework Extensive knowledge of Bash shell scripting language Extensive knowledge on scheduling Autosys Environment Netezza 6x 7 UNIX Shell Scripting NZSQL sql Netezza TwinFin 24 Brightlight Framework 38 39 Aginity Work Bench AQT Silktest tool SSIS 2012 SQLServer 2012 TSQL IPPSYMPONYSADVISOR Responsibilities Worked extensively on Brightlight Framework 35 38 Used different Framework assets like nzf app_init Worked on Application asserts like setup_cfgsh run log ddl meta data auto Involved in admin workflows like db_create db_init Involved in dm workflows like ddl_convert ora_ddl_convert and db_install Involved in dev workflows like   db_xfr_templates db_referential_config Worked extensively on Aginity Work Bench and AQT Written transformations using shell scripting mainly bash Performed unit testing integration testing regression testing and system testing Resolved issues during testing phase Performance tuned the transformations using ASYNC and Finalization Created list files flow files and shell scripts sh Created data files for test cases using Silktest tool and creating manifest files Performed load test on transformations by using generated test data Worked on Cognos views by using the table sql Worked on the mapping documents Assisted QA in testing the QA test cases and resolving the issues Worked on updating the data model changes Worked on the triggers and RDM joins Wrote DIFF script for data model changes to find the difference between two scripts Worked in Agile Environment Environment Netezza 605 UNIX Shell Scripting MS Excel NZSQL Netezza TwinFin 24 Brightlight Framework 35 38 NZF Aginity Work Bench AQT Silktest tool triggers and RDM joins Physician Focus Project Responsibilities Worked extensively on SQL PLSQL Perl and UNIX shell scripting Analyzed the source system of ETL Maps Developed the ETL routines Designed and developed the factdimension entities Involved in the Unit testing Event Thread testing and System testing of the individual Written Unix Shell Scripting based on requirements Worked in Agile environment Migration of data from Oracle to Netezza Worked on Brightlight Framework 30 Worked extensively in NZSQL to migrate the data from Oracle to Netezza database Written Unix Shell Scripting based on requirements Performed unit testing of the shell scripts developed with various scenarios Performed integration testing and system testing Environment Netezza UNIX Shell Scripting MS Excel NZSQL Netezza TwinFin 12 Brightlight Framework 30 Education Master in Mechanical Engineering Sciences Lamar University Beaumont TX 2010 Bachelors in Mechanical Engineering BTech 2008 Jawaharlal Nehru Technological University Hyderabad Telangana Skills DATABASE 6 years DATABASES 6 years ETL 6 years EXTRACT TRANSFORM AND LOAD 6 years SCHEDULING 5 years Additional Information Technical Skills ETL Informatica 86961101 DATA STAGE 91 Scheduling Tool DAC Data Warehousing Admin Console Control_M Autosys Crontab and TMUX Reporting Tool COGNOS 9X 10X Databases Netezza 451 603 605 70X 71X NPS Oracle 11g10g9i8i Oracle Exadata Operating Systems Windows 03710 UNIX and LINUX Programming C SQL PLSQL XML Json Other Tools SQLLoader Oracle SQL Developer TOAD Eclipse MS Visio MS Excel Eclipse Platform APIPostman Aginity Workbench SQL server studio Scripting Languages UNIX Shell Scripting bash ksh SQL PLSQL Postgres and TSQL Methodologies ER Modeling Star Schema Snowflake Schema Hybrid Models Data warehousing Data Modeling Tool Microsoft Visio 20102016 ER Studio",
    "extracted_keywords": [
        "Datawarehouse",
        "Architect",
        "Lead",
        "Datawarehouse",
        "Consultant",
        "Datawarehouse",
        "Architect",
        "Lead",
        "Datawarehouse",
        "Consultant",
        "Datawarehouse",
        "Architect",
        "Lead",
        "Datawarehouse",
        "Consultant",
        "Waste",
        "Management",
        "Houston",
        "TX",
        "years",
        "experience",
        "phases",
        "Analysis",
        "Design",
        "Development",
        "Implementation",
        "Relational",
        "Data",
        "Warehousing",
        "applications",
        "business",
        "requirements",
        "Architecture",
        "functions",
        "Experience",
        "life",
        "cycle",
        "implementation",
        "Data",
        "Warehouses",
        "Data",
        "Marting",
        "years",
        "experience",
        "Netezza",
        "database",
        "phases",
        "Analysis",
        "Design",
        "Development",
        "Implementation",
        "Architecting",
        "Administration",
        "projects",
        "areas",
        "Informatica",
        "Data",
        "stage",
        "SSIS",
        "UNIX",
        "shell",
        "Netezza",
        "appliance",
        "SQLServer",
        "COGNOS",
        "SQLGen",
        "API",
        "Expertise",
        "performance",
        "tuning",
        "NZSQL",
        "TSQL",
        "statements",
        "Explain",
        "Plan",
        "SQL",
        "trace",
        "cost",
        "performance",
        "Experience",
        "business",
        "requirements",
        "creation",
        "database",
        "tables",
        "indexes",
        "constraints",
        "packages",
        "procedures",
        "functions",
        "triggers",
        "Oracle",
        "PLSQL",
        "tool",
        "Netezza",
        "Tool",
        "experience",
        "analysis",
        "design",
        "database",
        "database",
        "ER",
        "Diagrams",
        "normalization",
        "denormalization",
        "Analysis",
        "problems",
        "problems",
        "domain",
        "knowledge",
        "applications",
        "Health",
        "Care",
        "Retail",
        "Marketing",
        "Transactional",
        "Financial",
        "industries",
        "Experience",
        "tools",
        "SVN",
        "GIT",
        "scripts",
        "emails",
        "running",
        "batch",
        "jobs",
        "Visio",
        "ER",
        "Studio",
        "Power",
        "Designer",
        "Relational",
        "Dimensional",
        "Modeling",
        "Data",
        "Migration",
        "projects",
        "responses",
        "API",
        "GET",
        "PUT",
        "calls",
        "UNIX",
        "shell",
        "scripts",
        "command",
        "line",
        "utility",
        "response",
        "FTP",
        "sites",
        "Json",
        "files",
        "UNIX",
        "Shell",
        "Script",
        "jq",
        "command",
        "line",
        "utility",
        "Proficient",
        "SQL",
        "development",
        "Knowledge",
        "scheduling",
        "Autosys",
        "CORN",
        "DAC",
        "Control",
        "M",
        "Knowledge",
        "API",
        "calls",
        "usage",
        "calls",
        "clients",
        "products",
        "Work",
        "Experience",
        "Datawarehouse",
        "Architect",
        "Lead",
        "Datawarehouse",
        "Consultant",
        "Carlson",
        "Wagonlit",
        "Minnetonka",
        "MN",
        "November",
        "Present",
        "Responsibilities",
        "travel",
        "data",
        "Air",
        "Rail",
        "car",
        "Commercial",
        "Government",
        "tenancy",
        "architecture",
        "Data",
        "lake",
        "data",
        "planes",
        "queues",
        "data",
        "travel",
        "agencies",
        "clients",
        "source",
        "systems",
        "methodology",
        "slicing",
        "environment",
        "week",
        "sprints",
        "Travel",
        "segment",
        "Data",
        "Model",
        "components",
        "Visio",
        "Create",
        "Users",
        "permissions",
        "Informatica",
        "Admin",
        "console",
        "repositories",
        "domains",
        "responses",
        "API",
        "GET",
        "PUT",
        "calls",
        "UNIX",
        "shell",
        "scripts",
        "command",
        "line",
        "utility",
        "response",
        "FTP",
        "sites",
        "Json",
        "files",
        "Unix",
        "Shell",
        "Script",
        "jq",
        "command",
        "line",
        "utility",
        "data",
        "Mart",
        "queues",
        "ISOS",
        "Alerts",
        "relation",
        "clients",
        "travel",
        "data",
        "requirement",
        "data",
        "distribution",
        "level",
        "agreements",
        "Multitenancy",
        "Commercial",
        "Local",
        "International",
        "Government",
        "architecture",
        "Knowledge",
        "Informatica",
        "Power",
        "center",
        "ETL",
        "Data",
        "transformation",
        "Developer",
        "client",
        "Design",
        "unit",
        "test",
        "data",
        "loading",
        "data",
        "transformation",
        "programs",
        "Oracle",
        "Data",
        "Warehouses",
        "SQLserver",
        "Pivotal",
        "GreenPlum",
        "Data",
        "Marts",
        "Performed",
        "unit",
        "performance",
        "scalability",
        "testing",
        "data",
        "tool",
        "DB",
        "level",
        "clients",
        "data",
        "incidents",
        "request",
        "incidents",
        "data",
        "transformation",
        "level",
        "Weekly",
        "BiWeekly",
        "release",
        "activities",
        "environment",
        "Knowledge",
        "API",
        "interface",
        "data",
        "API",
        "calls",
        "wrapper",
        "scripts",
        "Informatica",
        "UNIX",
        "shell",
        "script",
        "scheduling",
        "purposes",
        "Architecture",
        "Working",
        "knowledge",
        "scheduling",
        "ControlM",
        "Autosys",
        "Crontab",
        "TMUX",
        "Environment",
        "Oracle",
        "Exadata",
        "TOAD",
        "Json",
        "XML",
        "SQL",
        "Developer",
        "SQLServer",
        "Aginity",
        "Work",
        "Bench",
        "GreenPlum",
        "Informatica",
        "Control_M",
        "Postman",
        "APIs",
        "UNIX",
        "SunOS",
        "Jr",
        "Datawarehouse",
        "Architect",
        "Lead",
        "Datawarehouse",
        "Engineer",
        "Morningstar",
        "Chicago",
        "IL",
        "April",
        "November",
        "Responsibilities",
        "Design",
        "data",
        "loading",
        "programs",
        "time",
        "data",
        "Netezza",
        "databases",
        "SqlServer",
        "Marts",
        "ETLInformatica",
        "jobs",
        "data",
        "collection",
        "data",
        "integration",
        "source",
        "documents",
        "Netezza",
        "Netezza",
        "SqlServer",
        "Marts",
        "Working",
        "methodology",
        "environment",
        "mapping",
        "document",
        "gathering",
        "requirements",
        "process",
        "flow",
        "diagrams",
        "database",
        "objects",
        "level",
        "Environments",
        "Users",
        "permissions",
        "level",
        "database",
        "access",
        "user",
        "Hands",
        "experience",
        "Amazon",
        "Web",
        "Services",
        "console",
        "products",
        "Cloud",
        "solutions",
        "AWS",
        "cloud",
        "VPC",
        "Elastic",
        "Compute",
        "Cloud",
        "EC2",
        "S3",
        "Auto",
        "RDS",
        "AWS",
        "services",
        "subject",
        "matter",
        "expertise",
        "calculations",
        "requirements",
        "database",
        "level",
        "Netezza",
        "SQLServer",
        "investment",
        "data",
        "breakdowns",
        "statistics",
        "Total",
        "Daily",
        "Price",
        "Primary",
        "Return",
        "Indexs",
        "universes",
        "portfolio",
        "securities",
        "Multitenancy",
        "architecture",
        "Knowledge",
        "Informatica",
        "ETL",
        "tool",
        "Administrator",
        "Designer",
        "Monitor",
        "Design",
        "unit",
        "test",
        "data",
        "loading",
        "data",
        "transformation",
        "programs",
        "Netezza",
        "Data",
        "Warehouses",
        "SQLserver",
        "Data",
        "Marts",
        "Performed",
        "unit",
        "testing",
        "integration",
        "testing",
        "regression",
        "testing",
        "system",
        "testing",
        "scalability",
        "Resolved",
        "issuesbug",
        "fixes",
        "testing",
        "phase",
        "Performance",
        "transformations",
        "NZ",
        "Explain",
        "Plan",
        "BiWeekly",
        "release",
        "activities",
        "environment",
        "Knowledge",
        "API",
        "interface",
        "data",
        "API",
        "Working",
        "knowledge",
        "scheduling",
        "Environment",
        "Netezza",
        "SQLServer",
        "NZsql",
        "Nzload",
        "NZmigrate",
        "TSQL",
        "Aginity",
        "WorkBench",
        "Informatica",
        "Control_M",
        "SqlServer",
        "Management",
        "tool",
        "Postman",
        "MorningstarDirect",
        "v2",
        "MorningstarDirectv3",
        "Lead",
        "Datawarehouse",
        "Engineer",
        "Sr",
        "Consultant",
        "Fossil",
        "INC",
        "Richardson",
        "TX",
        "August",
        "April",
        "Responsibilities",
        "Design",
        "unit",
        "test",
        "data",
        "loading",
        "data",
        "transformation",
        "programs",
        "Netezza",
        "Data",
        "Warehouses",
        "Data",
        "Marts",
        "ETLDatastage",
        "jobs",
        "Source",
        "Target",
        "Worked",
        "slicing",
        "methodology",
        "users",
        "groups",
        "level",
        "Knowledge",
        "Datastage",
        "Administrator",
        "client",
        "Designer",
        "client",
        "Director",
        "client",
        "Performed",
        "unit",
        "testing",
        "integration",
        "testing",
        "regression",
        "testing",
        "system",
        "testing",
        "issues",
        "testing",
        "phase",
        "Performance",
        "transformations",
        "ASYNC",
        "Finalization",
        "system",
        "documentation",
        "database",
        "procedures",
        "practices",
        "standards",
        "Environment",
        "Netezza",
        "UNIX",
        "Shell",
        "Scripting",
        "NZsql",
        "Nzload",
        "NZmigrate",
        "Netezza",
        "Aginity",
        "Work",
        "Bench",
        "Datastage",
        "Netezza",
        "Database",
        "Administrator",
        "DBA",
        "Saint",
        "Petersburg",
        "FL",
        "February",
        "August",
        "Responsibilities",
        "plans",
        "data",
        "warehousing",
        "projects",
        "quality",
        "documentation",
        "designing",
        "implementation",
        "data",
        "warehouse",
        "models",
        "mappings",
        "expertise",
        "technologies",
        "fields",
        "data",
        "warehouse",
        "analytics",
        "establishment",
        "maintenance",
        "databases",
        "projects",
        "business",
        "requirements",
        "procedures",
        "database",
        "backup",
        "configuration",
        "changes",
        "queries",
        "backups",
        "session",
        "management",
        "management",
        "records",
        "manner",
        "processes",
        "management",
        "migration",
        "tables",
        "schemas",
        "database",
        "locations",
        "methodology",
        "users",
        "groups",
        "level",
        "UNIXLINUX",
        "commands",
        "enhancement",
        "methods",
        "rollout",
        "upgrades",
        "maintenance",
        "databases",
        "system",
        "documentation",
        "database",
        "procedures",
        "practices",
        "standards",
        "database",
        "problems",
        "issues",
        "service",
        "requests",
        "Environment",
        "Netezza",
        "704X",
        "Netezza",
        "60XX",
        "UNIX",
        "Shell",
        "Scripting",
        "NZsql",
        "Nzload",
        "NZmigrate",
        "Netezza",
        "TwinFin",
        "Netezza",
        "Mustang",
        "Netezza",
        "N2001",
        "Aginity",
        "Work",
        "Bench",
        "WINSQL",
        "Lead",
        "Datawarehouse",
        "EngineerDatawarehouse",
        "Engineer",
        "PremierInc",
        "Healthcare",
        "Solutions",
        "Charlotte",
        "NC",
        "February",
        "February",
        "Premier",
        "Connect",
        "Enterprise",
        "Claims",
        "Carolina",
        "Health",
        "Partners",
        "Responsibilities",
        "mapping",
        "document",
        "gathering",
        "requirements",
        "data",
        "loading",
        "programs",
        "time",
        "data",
        "Netezza",
        "Data",
        "Marts",
        "ETLDatastage",
        "jobs",
        "Source",
        "Target",
        "Worked",
        "slicing",
        "methodology",
        "users",
        "groups",
        "level",
        "loading",
        "claims",
        "data",
        "sourceInformed",
        "Dimensional",
        "Data",
        "Marts",
        "Multitenancy",
        "architecture",
        "knowledge",
        "Bash",
        "Korn",
        "scripting",
        "language",
        "Knowledge",
        "Datastage",
        "Administrator",
        "client",
        "Designer",
        "client",
        "Director",
        "client",
        "Design",
        "unit",
        "test",
        "data",
        "loading",
        "data",
        "transformation",
        "programs",
        "Netezza",
        "Data",
        "Warehouses",
        "Data",
        "Marts",
        "reports",
        "Cognos",
        "knowledge",
        "scheduling",
        "Autosys",
        "Environment",
        "Netezza",
        "UNIX",
        "Shell",
        "Scripting",
        "NZSQL",
        "SQL",
        "Netezza",
        "TwinFin",
        "Aginity",
        "Work",
        "Bench",
        "Datastage",
        "Cognos",
        "Premier",
        "Connect",
        "Enterprise",
        "Clinical",
        "Carolina",
        "Health",
        "Partners",
        "Responsibilities",
        "Design",
        "unit",
        "test",
        "data",
        "loading",
        "data",
        "transformation",
        "programs",
        "Netezza",
        "Data",
        "Warehouses",
        "Data",
        "Marts",
        "Design",
        "Trickle",
        "feed",
        "data",
        "loading",
        "programs",
        "time",
        "data",
        "loading",
        "Netezza",
        "Data",
        "Marts",
        "Designs",
        "data",
        "transformations",
        "exceptions",
        "addition",
        "data",
        "loading",
        "capabilities",
        "SSIS",
        "building",
        "performance",
        "data",
        "integration",
        "solutions",
        "extraction",
        "transformation",
        "load",
        "ETL",
        "packages",
        "data",
        "bases",
        "Scheduling",
        "SSIS",
        "packages",
        "Jobs",
        "Development",
        "unit",
        "data",
        "loading",
        "transformation",
        "programs",
        "orchestration",
        "programs",
        "data",
        "loading",
        "data",
        "transformations",
        "routines",
        "Unit",
        "data",
        "loading",
        "transformation",
        "programs",
        "knowledge",
        "nzDIF",
        "Data",
        "Integration",
        "Framework",
        "knowledge",
        "Bash",
        "shell",
        "scripting",
        "language",
        "knowledge",
        "scheduling",
        "Autosys",
        "Environment",
        "Netezza",
        "6x",
        "UNIX",
        "Shell",
        "Scripting",
        "NZSQL",
        "sql",
        "Netezza",
        "TwinFin",
        "Brightlight",
        "Framework",
        "Aginity",
        "Work",
        "Bench",
        "AQT",
        "Silktest",
        "tool",
        "SSIS",
        "SQLServer",
        "TSQL",
        "Responsibilities",
        "Brightlight",
        "Framework",
        "Framework",
        "assets",
        "nzf",
        "app_init",
        "Application",
        "run",
        "log",
        "ddl",
        "data",
        "auto",
        "admin",
        "workflows",
        "db_create",
        "db_init",
        "dm",
        "workflows",
        "ddl_convert",
        "ora_ddl_convert",
        "db_install",
        "dev",
        "workflows",
        "db_xfr_templates",
        "db_referential_config",
        "Aginity",
        "Work",
        "Bench",
        "AQT",
        "transformations",
        "shell",
        "scripting",
        "Performed",
        "unit",
        "testing",
        "integration",
        "testing",
        "regression",
        "testing",
        "system",
        "testing",
        "issues",
        "testing",
        "phase",
        "Performance",
        "transformations",
        "ASYNC",
        "Finalization",
        "list",
        "files",
        "files",
        "scripts",
        "sh",
        "data",
        "files",
        "test",
        "cases",
        "Silktest",
        "tool",
        "files",
        "Performed",
        "load",
        "test",
        "transformations",
        "test",
        "data",
        "Cognos",
        "views",
        "table",
        "sql",
        "mapping",
        "documents",
        "Assisted",
        "QA",
        "QA",
        "test",
        "cases",
        "issues",
        "data",
        "model",
        "changes",
        "triggers",
        "RDM",
        "Wrote",
        "DIFF",
        "script",
        "data",
        "model",
        "changes",
        "difference",
        "scripts",
        "Agile",
        "Environment",
        "Environment",
        "Netezza",
        "UNIX",
        "Shell",
        "Scripting",
        "MS",
        "Excel",
        "NZSQL",
        "Netezza",
        "TwinFin",
        "Brightlight",
        "Framework",
        "NZF",
        "Aginity",
        "Work",
        "Bench",
        "AQT",
        "Silktest",
        "tool",
        "triggers",
        "RDM",
        "Focus",
        "Project",
        "Responsibilities",
        "SQL",
        "PLSQL",
        "Perl",
        "UNIX",
        "shell",
        "scripting",
        "source",
        "system",
        "ETL",
        "Maps",
        "ETL",
        "routines",
        "factdimension",
        "entities",
        "Unit",
        "testing",
        "Event",
        "Thread",
        "testing",
        "System",
        "testing",
        "individual",
        "Written",
        "Unix",
        "Shell",
        "Scripting",
        "requirements",
        "environment",
        "Migration",
        "data",
        "Oracle",
        "Netezza",
        "Brightlight",
        "Framework",
        "NZSQL",
        "data",
        "Oracle",
        "Netezza",
        "database",
        "Written",
        "Unix",
        "Shell",
        "Scripting",
        "requirements",
        "Performed",
        "unit",
        "testing",
        "shell",
        "scripts",
        "scenarios",
        "integration",
        "testing",
        "system",
        "testing",
        "Environment",
        "Netezza",
        "UNIX",
        "Shell",
        "Scripting",
        "MS",
        "Excel",
        "NZSQL",
        "Netezza",
        "TwinFin",
        "Brightlight",
        "Framework",
        "Education",
        "Master",
        "Mechanical",
        "Engineering",
        "Sciences",
        "Lamar",
        "University",
        "Beaumont",
        "TX",
        "Bachelors",
        "Mechanical",
        "Engineering",
        "BTech",
        "Jawaharlal",
        "Nehru",
        "Technological",
        "University",
        "Hyderabad",
        "Telangana",
        "Skills",
        "DATABASE",
        "years",
        "DATABASES",
        "years",
        "ETL",
        "years",
        "EXTRACT",
        "TRANSFORM",
        "years",
        "SCHEDULING",
        "years",
        "Additional",
        "Information",
        "Technical",
        "Skills",
        "Informatica",
        "DATA",
        "STAGE",
        "Scheduling",
        "Tool",
        "DAC",
        "Data",
        "Warehousing",
        "Admin",
        "Console",
        "Control_M",
        "Autosys",
        "Crontab",
        "TMUX",
        "Reporting",
        "Tool",
        "COGNOS",
        "9X",
        "Netezza",
        "71X",
        "NPS",
        "Oracle",
        "11g10g9i8i",
        "Oracle",
        "Exadata",
        "Operating",
        "Systems",
        "Windows",
        "UNIX",
        "LINUX",
        "Programming",
        "C",
        "SQL",
        "PLSQL",
        "XML",
        "Json",
        "Tools",
        "SQLLoader",
        "Oracle",
        "SQL",
        "Developer",
        "TOAD",
        "Eclipse",
        "MS",
        "Visio",
        "MS",
        "Excel",
        "Eclipse",
        "Platform",
        "APIPostman",
        "Aginity",
        "Workbench",
        "SQL",
        "server",
        "studio",
        "Scripting",
        "Languages",
        "UNIX",
        "Shell",
        "Scripting",
        "bash",
        "ksh",
        "SQL",
        "PLSQL",
        "Postgres",
        "TSQL",
        "Methodologies",
        "ER",
        "Modeling",
        "Star",
        "Schema",
        "Snowflake",
        "Schema",
        "Hybrid",
        "Models",
        "Data",
        "Data",
        "Modeling",
        "Tool",
        "Microsoft",
        "Visio",
        "ER",
        "Studio"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T21:33:23.468796",
    "resume_data": "Datawarehouse Architect Lead Datawarehouse Consultant Datawarehouse Architect Lead Datawarehouse Consultant Datawarehouse Architect Lead Datawarehouse Consultant Waste Management Houston TX Over 8 years of experience in all phases of Analysis Design Development Implementation and support Relational Data Warehousing applications for business requirements Architecture functions Experience in full life cycle implementation of Data Warehouses and Data Marting Over 7 years of experience in Netezza database in all phases of Analysis Design Development Implementation Architecting and Administration Worked for the projects in the areas of Informatica Data stage SSIS UNIX shell scripting Netezza appliance SQLServer COGNOS reporting SQLGen API Expertise in outstanding performance tuning of NZSQL TSQL statements using Explain Plan SQL trace for optimizing the cost and evaluating the performance Experience in translating business requirements into creation of database objects tables indexes constraints packages stored procedures functions and triggers using Oracle PLSQL tool and Netezza Tool Kit Extensive experience in analysis and design of database database modeling ER Diagrams normalization and denormalization Performing Analysis for complex problems and solving the problems Having good domain knowledge based applications such as Health Care Retail Marketing Transactional and Financial industries Experience in using versioning tools SVN and GIT Created scripts which will send notificationswarning emails which helped in smooth running of daily batch jobs Worked extensively with Visio and ER Studio Power Designer for Relational Dimensional Modeling and Data Migration projects Capturing responses API GET and PUT calls using UNIX shell scripts wget command line utility and save the successful response in FTP sites and automate them Parsing out Json files using UNIX Shell Script jq command line utility Proficient with the SQL development languages Knowledge on scheduling Autosys CORN DAC and Control M Knowledge on API calls and usage of the calls from clients and products Work Experience Datawarehouse Architect Lead Datawarehouse Consultant Carlson Wagonlit Travels Minnetonka MN November 2017 to Present Responsibilities Working on gathering different travel related data feeds Air Rail car for both Commercial and Government Implementing the multi tenancy architecture in Data lake and data planes using queues to gather data from multiple national and international travel agencies and clients source systems Working in Agile methodology with thin slicing environment 1 and 2 week sprints Creating Travel segment and Travel accountable Data Model components using Visio Create Users and assign permissions using Informatica Admin console and maintaining the repositories and domains Capturing responses API GET and PUT calls using UNIX shell scripts wget command line utility and save the successful response in FTP sites and automate them Parsing out Json files using Unix Shell Script jq command line utility and loaded the data in to Mart queues for ISOS Alerts Closely working relation with external and international clients to gather travel data for requirement and data distribution level agreements Working and implementing Multitenancy Commercial Local International and Government architecture Very good Knowledge of Informatica Power center for ETL Data transformation and Developer client Design develop and provide unit test data loading and data transformation programs related to Oracle Data Warehouses SQLserver and Pivotal GreenPlum Data Marts Performed unit performance and scalability testing at data tool and DB level Resolved clients data incidents and request incidents at data and transformation level Involved in Weekly BiWeekly and Monthly release activities to lower and upper environment Knowledge on using API interface and retrieve data from different API calls Created wrapper scripts for Informatica in UNIX shell script for scheduling purposes Architecture and Working knowledge on scheduling ControlM Autosys Crontab and TMUX Environment Oracle Exadata TOAD Json XML SQL Developer SQLServer 2016 Aginity Work Bench for GreenPlum Informatica 961101 Control_M Postman APIs UNIX SunOS Jr Datawarehouse Architect Lead Datawarehouse Engineer Morningstar Chicago IL April 2015 to November 2017 Responsibilities Design and develop data loading programs to support real time data loading to Netezza databases and SqlServer Marts Worked on ETLInformatica jobs from data collection to data integration mainly from source documents to Netezza and Netezza to SqlServer Marts Working in Agile methodology Fast phased environment Worked on mapping document gathering requirements and process flow diagrams Create database objects in lower and upper level Environments Create Users and assign permissions based on the level of database access the user would need Hands on experience in Amazon Web Services console and products Designed and implemented Cloud solutions with AWS Virtual private cloud VPC Elastic Compute Cloud EC2 S3 Auto scaling RDS and other AWS services Worked closely with subject matter expertise to gather and understand the calculations and requirements Created database objects in lower and upper level Environments Netezza and SQLServer Worked on the investment data and calculated the different breakdowns statistics Total Daily Price Primary Return Indexs at different universes for portfolio and securities Worked on Multitenancy architecture Very good Knowledge of Informatica ETL tool Administrator Designer and Monitor Design develop and provide unit test data loading and data transformation programs related to Netezza Data Warehouses and SQLserver Data Marts Performed unit testing integration testing regression testing system testing and scalability Resolved issuesbug fixes during testing phase Performance tuned the transformations using NZ Explain Plan Involved in BiWeekly and Monthly release activities to lower and upper environment Knowledge on using API interface and retrieve data from different API calls Working knowledge on scheduling ControlM Environment Netezza 702 and 711 SQLServer 2012 NZsql Nzload NZmigrate TSQL Aginity WorkBench Informatica 961 Control_M SqlServer Management tool Postman MorningstarDirect v2 and MorningstarDirectv3 Lead Datawarehouse Engineer Sr Consultant Fossil INC Richardson TX August 2014 to April 2015 Responsibilities Design develop and provide unit test data loading and data transformation programs related to Netezza Data Warehouses and Data Marts Worked on ETLDatastage jobs from Source to Target Worked on thin slicing in Agile methodology Create databases users and groups for Lower and upper level Environments Knowledge of Datastage Administrator client Designer client and Director client Performed unit testing integration testing regression testing and system testing Resolved issues during testing phase Performance tuned the transformations using ASYNC and Finalization Maintained system documentation of best database procedures practices and standards Environment Netezza 71 UNIX Shell Scripting NZsql Nzload NZmigrate Netezza Aginity Work Bench nzDIF 39 Datastage 91 Netezza Database Administrator DBA Saint Petersburg FL February 2014 to August 2014 Responsibilities Prepared strategic plans for data warehousing projects and related quality documentation Assisted in designing and implementation of detailed data warehouse models and mappings Provided technical expertise to technologies in fields of data warehouse and relevant analytics Performed establishment and maintenance of databases for assigned projects as per business requirements Implemented procedures for daily database monitoring backup and configuration changes Developed queries backups and session management and log management records in detailed manner Executed processes for management of migration of tables schemas database as per defined and assigned locations Working in Agile methodology Create databases users and groups for Lower and upper level Environments Proficient with UNIXLINUX commands Suggested enhancement methods rollout and upgrades for maintenance of existing databases Maintained system documentation of best database procedures practices and standards Resolved database problems and issues by troubleshooting and responding to service requests Environment Netezza 704X Netezza 60XX UNIX Shell Scripting NZsql Nzload NZmigrate Netezza TwinFin 24 Netezza Mustang Netezza N2001 Aginity Work Bench WINSQL Lead Datawarehouse EngineerDatawarehouse Engineer PremierInc Healthcare Solutions Charlotte NC February 2011 to February 2014 Premier Connect Enterprise Claims Carolina Health Partners Responsibilities Worked on mapping document and gathering requirements Design and develop data loading programs to support real time data loading to Netezza Data Marts Worked on ETLDatastage jobs from Source to Target Worked on thin slicing in Agile methodology Create databases users and groups for Lower and upper level Environments Worked loading claims data from the sourceInformed to the Dimensional Data Marts Worked on Multitenancy architecture Extensive knowledge of Bash and Korn scripting language Knowledge of Datastage Administrator client Designer client and Director client Design develop and provide unit test data loading and data transformation programs related to Netezza Data Warehouses and Data Marts Worked on creating reports in Cognos Extensive knowledge on scheduling Autosys Environment Netezza 7 UNIX Shell Scripting NZSQL SQL Netezza TwinFin 24 Aginity Work Bench Datastage 92 Cognos 101 Premier Connect Enterprise Clinical Carolina Health Partners Responsibilities Design develop and provide unit test data loading and data transformation programs related to Netezza Data Warehouses and Data Marts Design Trickle feed data loading programs to support near real time data loading of Netezza Data Marts Designs data transformations and exceptions processing in addition to refresh and replay data loading capabilities Using SSIS building high performance data integration solutions including extraction transformation and load ETL packages for data bases and Scheduling the SSIS packages and Jobs Development coding and unit tests the data loading and transformation programs Codes orchestration programs that schedule the data loading and data transformations routines Unit tests all data loading and transformation programs Extensive knowledge of the nzDIF Data Integration Framework Extensive knowledge of Bash shell scripting language Extensive knowledge on scheduling Autosys Environment Netezza 6x 7 UNIX Shell Scripting NZSQL sql Netezza TwinFin 24 Brightlight Framework 38 39 Aginity Work Bench AQT Silktest tool SSIS 2012 SQLServer 2012 TSQL IPPSYMPONYSADVISOR Responsibilities Worked extensively on Brightlight Framework 35 38 Used different Framework assets like nzf app_init Worked on Application asserts like setup_cfgsh run log ddl meta data auto Involved in admin workflows like db_create db_init Involved in dm workflows like ddl_convert ora_ddl_convert and db_install Involved in dev workflows like db_intake_config db_xfr_templates db_referential_config Worked extensively on Aginity Work Bench and AQT Written transformations using shell scripting mainly bash Performed unit testing integration testing regression testing and system testing Resolved issues during testing phase Performance tuned the transformations using ASYNC and Finalization Created list files flow files and shell scripts sh Created data files for test cases using Silktest tool and creating manifest files Performed load test on transformations by using generated test data Worked on Cognos views by using the table sql Worked on the mapping documents Assisted QA in testing the QA test cases and resolving the issues Worked on updating the data model changes Worked on the triggers and RDM joins Wrote DIFF script for data model changes to find the difference between two scripts Worked in Agile Environment Environment Netezza 605 UNIX Shell Scripting MS Excel NZSQL Netezza TwinFin 24 Brightlight Framework 35 38 NZF Aginity Work Bench AQT Silktest tool triggers and RDM joins Physician Focus Project Responsibilities Worked extensively on SQL PLSQL Perl and UNIX shell scripting Analyzed the source system of ETL Maps Developed the ETL routines Designed and developed the factdimension entities Involved in the Unit testing Event Thread testing and System testing of the individual Written Unix Shell Scripting based on requirements Worked in Agile environment Migration of data from Oracle to Netezza Worked on Brightlight Framework 30 Worked extensively in NZSQL to migrate the data from Oracle to Netezza database Written Unix Shell Scripting based on requirements Performed unit testing of the shell scripts developed with various scenarios Performed integration testing and system testing Environment Netezza UNIX Shell Scripting MS Excel NZSQL Netezza TwinFin 12 Brightlight Framework 30 Education Master in Mechanical Engineering Sciences Lamar University Beaumont TX 2010 Bachelors in Mechanical Engineering BTech 2008 Jawaharlal Nehru Technological University Hyderabad Telangana Skills DATABASE 6 years DATABASES 6 years ETL 6 years EXTRACT TRANSFORM AND LOAD 6 years SCHEDULING 5 years Additional Information Technical Skills ETL Informatica 86961101 DATA STAGE 91 Scheduling Tool DAC Data Warehousing Admin Console Control_M Autosys Crontab and TMUX Reporting Tool COGNOS 9X 10X Databases Netezza 451 603 605 70X 71X NPS Oracle 11g10g9i8i Oracle Exadata Operating Systems Windows 959820002003710 UNIX and LINUX Programming C SQL PLSQL XML Json Other Tools SQLLoader Oracle SQL Developer TOAD Eclipse MS Visio MS Excel Eclipse Platform APIPostman Aginity Workbench SQL server studio Scripting Languages UNIX Shell Scripting bash ksh SQL PLSQL Postgres and TSQL Methodologies ER Modeling Star Schema Snowflake Schema Hybrid Models Data warehousing Data Modeling Tool Microsoft Visio 20102016 ER Studio",
    "unique_id": "b9695049-ef5a-4bef-aa93-369f970fcad5"
}