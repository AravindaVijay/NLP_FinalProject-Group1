{
    "clean_data": "Big Data Engineer Big Data Engineer Big Data Engineer American Express Having around 6 years of total IT experience with over 4years experience in Big Data Hadoop 2 years of experience in Development and Design of Java based enterprise applications Extensive working experience on Hadoop ecosystem components like HDFS MapReduce Hive Pig Sqoop Flume Spark Kafka Oozie and Zookeeper Implemented performance tuning techniques for Hive queries Strong knowledge on Hadoop HDFS architecture MapReduce MRv1 and YARN MRv2 framework Expert in working with Hive tool creating tables data distribution by implementing partitioning and bucketing writing and optimizing the HiveQL queries Strong hands on Experience in publishing the messages to various Kafka topics using Apache NIFI and consuming the message to HBase and MySql tables using Spark and Scala Worked on creating Spark jobs that process the true source files and successful in performing various transformations on the source data using SparkDataframeDataSet Spark SQL APIs Developed Sqoop scripts to migrate data from Teradata Oracle to Bigdata Environment Worked with Hue GUI in scheduling jobs with ease and File browsing Job browsing Metastore management Experience in importing and exporting the data using Sqoop from HDFS to Relational Database systems and viceversa Hands on experience in installation configuration supporting and managing Hadoop Clusters using Apache Cloudera CDH3 CDH4 Yarndistributions CDH 5X Performed Data scrubbing and processing with Oozie and for workflow automation and coordination Good knowledge in EMR Elastic Map Reducing to perform big data operations in AWSKnowledge in working with Amazon Web Services AWS using EC2 for computing and S3 as storage mechanism Hands on experience in analyzing log files for Hadoop and ecosystem services and finding root cause Hands on experience on handling different file formats like AVRO PARQUET Sequential files MAP Files CSV xml log ORC and RC Experience with NoSQL Database HBase Cassandra MongoDB Experience with AIXLinux RHEL Unix Shell Scripting and SQL Server 2008 Worked on data search toolElastic Search and data collection tool Logstash Strong knowledge in Hadoop cluster installation capacity planning and performance tuning benchmarking disaster recovery plan and application deployment in production cluster Experience in developing stored procedures triggers using SQL PLSQL in relational databases such as MS SQL Server 20052008 Exposed into methodologies Scrum Agile and Waterfall Work Experience Big Data Engineer American Express Phoenix AZ November 2018 to Present RESPONSIBILITIES Worked with extensive data sets in Big Data to uncover pattern problem unleash value for the Enterprise Worked with internal and external data sources on improving data accuracy coverage and generate recommendation on the process flow to accomplish the goal Ingestion of various types of data feeds from SOR and usecase perspective into Cornerstone 30 platform Reengineered legacy IDN FastTrack process to get the Bloomberg data directly from source to the CS30 Converted legacy Shell scripts to MapReduce jobs in a distributed manner without performing any kind of processing on the Edgenode to eliminate the burden Created Spark applications for data preprocessing for greater performance Developed Spark code and SparkSQLstreaming for faster testing and processing of data Experience in creating spark applications using RDD Dataframes Worked extensively on hive to analyse the data and create reports for data quality Implemented Partitioning Dynamic Partitions and Buckets in HIVE for increasing performance benefit and helping in organizing data in a logical fashion Written Hive queries for data analysis to meet the business requirements and Designed and developed User Defined Function UDF for Hive Involved in creating Hive tables Managed tables and External tables loading and analyzing data using hive queries Good knowledge about the configuration management tools like SVNCVSGithub Experience in configuring Event Engine nodes to import and export the data from Teradata to HDFS and viceversa Worked with source to get the history data as well as BAU data from IDN Teradata to the CornerStoneplatform and migrated also feeds from CS20 Expert in creating the nodes in Event Engine as per the usecase requirement to automate the process for the BAU data flow Exported the Event Engine nodes created in the silver environment to the IDN repository in BitBucket and created DaVinci package to migrate it to Platinum Worked with FDP team to create a secured flow to get the data from KAFKA Queue to CS30 Expert in creating the SFTP Connection to the internal and external source to get data in secured manner without any breakage Handle the production Incidents assigned to our workgroup promptly and fix the bugs or route it to the respective teams and optimized the SLAs Hadoop Developer Avast Emeryville CA September 2017 to 2018 RESPONSIBILITIES Developed real time data processing applications by using Scala and Python and implemented Apache Spark Streaming from various streaming sources like Kafka and JMS Experienced in writing live Realtime Processing and core jobs using Spark Streaming with Kafka as a data pipeline system Developed Shell Perl and Python scripts to automate and provide Control flow to Pig scripts Worked on Amazon AWS concepts like EMR and EC2 web services for fast and efficient processing of Big Data Involved in loading data from Linux file systems servers java web services using Kafka producers and partitions Applied Kafka custom encoders for custom input format to load data into Kafka Partitions Implement POC with Hadoop Extract data with Spark into HDFS Used Spark SQL with Scala for creating data frames and performed transformations on data frames Used Spark API over Cloudera Hadoop YARN to perform analytics on data in Hive Developed code to read data stream from Kafka and send it to respective bolts through respective stream Worked on Spark streaming using Apache Kafka for real time data processing Developed Map Reduce jobs using Map Reduce Java API and HIVEQL Developed UDF UDAF UDTF functions and implemented it in HIVE Queries Developing Scripts and Batch Job to schedule a bundle group of coordinators which consists of various Hadoop Programs using Oozie Experienced in optimizing Hive queries joins to handle different data sets Involved in ETL Data Integration and Migration by writing pig scripts Integrated Hadoop with Solr and implement search algorithms Experience in Storm for handling realtime processing Hands on Experience working in Hortonworks distribution Worked hands on NoSQL databases like MongoDB for POC purpose in storing images and URIs Designed and implemented MongoDB and associated RESTful web service Involved in writing test cases and implement test classes using MRUnit and mocking frameworks Developed Sqoop scripts to extract the data from MYSQL and load into HDFS Experience in processing large volume of data and skills in parallel execution of process using Talend functionality Used Talend tool to create workflows for processing data from multiple source systems Environment MapReduce HDFS Sqoop LINUX Oozie Hadoop Pig Hive Solr Spark Streaming Kafka Storm Spark Scala Python MongoDB Hadoop Cluster Amazon Web Services Talend Hadoop Developer Kelloggs Dec 2016 Aug2017Oakbrook IL RESPONSIBILITIES Experience with professional software engineering practices and best practices for the full software development life cycle including coding standards code reviews source control management and build processes Worked on analyzing Hadoop cluster and different big data analytic tools including Map Reduce Hive Written multiple MapReduce programs for data extraction transformation and aggregation from multiple file formats including XML JSON CSV other compressed file formats Worked on Teradata parallel transport TPT to load data from databases and files to Teradata Wrote views based on user andor reporting requirements Wrote Teradata Macros and used various Teradata analytic functions Involved in migration projects to migrate data from data warehouses on OracleDB2 and migrated those to Teradata Configured Flume source sink and memory channel to handle streaming data from server logs and JMS sources Experience in working with Flume to load the log data from multiple sources directly into HDFS Worked in the BI team in Big Data Hadoop cluster implementation and data integration in developing largescale system software Involved in source system analysis data analysis data modeling to ETL Extract Transform and Load Handling structured and unstructured data and applying ETL processes Worked extensively with Sqoop for importing and exporting the data from HDFS to Relational Database systemsmainframe and viceversa Loading data into HDFS Involved in collecting aggregating and moving data from servers to HDFS using Flume Implemented logging framework ELK stack Elastic Search LogStash Kibana on AWS Developed the Pig UDFS to preprocess the data for analysis Coding complex Oracle stored procedures functions packages and cursors for the client specific applications Experienced in using Java Rest API to perform CURD operations on HBase data Applied Hive queries to perform data analysis on HBase using Storage Handler to meet the business requirements Writing Hive Queries to Aggregate Data that needs to be pushed to the HBase Tables CreateModify shell scripts for scheduling various data cleansing scripts and ETL loading process Supports and assist QA Engineers in understanding testing and troubleshooting EnvironmentHadoop Hive Linux Map Reduce Sqoop Storm HBase Flume Eclipse Maven Junit agile methodologies Java Developer Mize Software Solutions 2014 to December 2014 July 2016Hyderabad India RESPONSIBILITIES Review the system requirements and attending requirements meetings with analysts and users Involved in the life cycle of the project from documentation to unit testing making development as priority Developed web pages using Struts framework JSP XML JavaScript HTML DHTML and CSS configure struts application use tag library Used Apache Struts framework includes the integrated AJAX Played major role in designing developing JSP pages and XML reports Developed Servlets and custom tags for JSP pages Developed few module Web pages using Springs IOC and Hibernate Designed and developed dynamic pages using HTML CSS layout techniques Java script Took the various challenges in the enhancement and completed them on time Extensive Used Exception handling and Multithreading for the optimum performance of the application Involved in design and implemented SOA SOAP next generation system on distributed platform Extensively used XSL as a XML parsing mechanism for showing Dynamic Web Pages in HTML format Implemented SOAP protocol to get the requests from the outside System Used CVS as a source control for code changes Used ANT scripts to build the project and JUnit to develop unit test cases Developed coding using SQL PLSQL Queries Joins Views ProceduresFunctions Triggers and Packages Provided development support for System Testing Product Testing User Acceptance Testing Data Conversion Testing Load Testing and Production Environment Java 15 J2EE AJAX Servlets JSP RUP Eclipse 31 Struts Spring 20 Hibernate XML CVS Java Script JQuery ANT SOAP Log4J DB2 Web Sphere server UNIX IBM Web Sphere Portal Server Javaweb Developer Highgate Systems Hyderabad Telangana May 2013 to September 2014 Hyderabad India RESPONSIBILITIES Collecting and understanding the User requirements and Functional specifications Development of GUI Using HTML CSS JSP and JavaScript Creating components for isolated business logic Deployment of application in J2EE Architecture Implemented Session Facade Pattern using Session and Entity Beans Developed message driven beans to listen to JMS Developed the Web Interface using Servlets Java Server Pages HTML and CSS Used WebLogic to deploy applications on local and development environments of the application Extensively used the JDBC Prepared Statement to embed the SQL queries into the java code Developed DAO Data Access Objects using Spring Framework 3 Developed Web applications with Rich Internet applications using Java applets Silverlight Java Used JavaScript to perform client side validations and StrutsValidator Framework for serverside validation Environment Java J2EE JDBC JSP Struts JMS Spring SQL MSAccess JavaScript HTML Education Master of Science in Computer Science Western Illinois University 2017 Bachelor of Technology in Information Technology JNTU Hyderabad Telangana 2013 Skills SQL 2 years JAVA 1 year CSS 1 year HTML 1 year JAVASCRIPT 1 year Additional Information TECHNICAL SKILLS Programming Languages Java Python SQL Scala and CC Big Data Ecosystem Hadoop MapReduce Kafka Spark Pig Hive YARN Flume Sqoop Oozie Zookeeper Talend Hadoop Distributions Cloudera Enterprise Horton Works EMC Pivotal Databases Oracle SQL Server PostgreSQL Web Technologies HTML XML JQuery Ajax CSS JavaScript JSON Streaming Tools Kafka RabbitMQ Testing Hadoop Testing Hive Testing MRUnit Operating Systems Linux Red HatUbuntuCentOS Windows 10817XP Cloud AWS S3 Redshift Cluster Technologies and Tools Servlets JSP Spring Boot MVC Batch Security Web Services Hibernate Maven GitHub Application Servers Tomcat JBoss IDEs Eclipse Net Beans IntelliJ",
    "entities": [
        "Hadoop Clusters",
        "File",
        "MS SQL Server 20052008 Exposed",
        "CC Big Data Ecosystem Hadoop MapReduce Kafka Spark",
        "BI",
        "FastTrack",
        "HDFS",
        "Developed Spark",
        "Teradata Oracle",
        "Big Data Hadoop",
        "IBM",
        "CornerStoneplatform",
        "Amazon Web Services AWS",
        "Metastore",
        "Hadoop",
        "HDFS Involved",
        "XML",
        "WebLogic",
        "JUnit",
        "Deployment of application",
        "Hive Developed",
        "Shell",
        "Platinum Worked",
        "HBase",
        "Developer Highgate Systems Hyderabad",
        "Zookeeper Implemented",
        "Developed Sqoop",
        "CDH3",
        "ELK",
        "HDFS MapReduce Hive Pig",
        "SOR",
        "Cloudera Hadoop",
        "Oracle SQL Server PostgreSQL",
        "SQL Server",
        "HTML CSS JSP",
        "Developed",
        "Flume Implemented",
        "IDN Teradata",
        "Reengineered",
        "Oozie Experienced",
        "Spark SQL APIs",
        "Teradata Wrote",
        "RUP",
        "AIXLinux RHEL Unix Shell Scripting",
        "Big Data Engineer Big Data Engineer Big Data",
        "Aggregate Data",
        "User Defined Function UDF",
        "Linux",
        "JSP",
        "HIVEQL Developed UDF UDAF",
        "Development and Design of Java",
        "KAFKA Queue",
        "MRUnit",
        "Spark Streaming",
        "Talend",
        "ORC",
        "Storage Handler",
        "Functional specifications Development of GUI Using",
        "Developed Servlets",
        "Spark",
        "Developed DAO Data Access Objects",
        "the HBase Tables CreateModify",
        "HTML CSS",
        "BitBucket",
        "TPT",
        "Java Developer Mize Software Solutions",
        "Sqoop",
        "HIVE",
        "Storm",
        "Hadoop Extract",
        "Oracle",
        "Coding",
        "Big Data Involved",
        "Performed Data",
        "Realtime Processing",
        "Kafka Partitions Implement POC",
        "ETL Data Integration",
        "DaVinci",
        "SQL",
        "Additional Information TECHNICAL SKILLS Programming Languages",
        "Exported the Event Engine",
        "Big Data",
        "Hive",
        "Integrated Hadoop",
        "Amazon AWS",
        "AJAX Played",
        "AWS Developed the Pig UDFS",
        "Cloudera Enterprise",
        "Hibernate Designed",
        "BAU",
        "ETL",
        "India",
        "RDD Dataframes Worked",
        "Writing Hive Queries",
        "Bloomberg",
        "the SLAs Hadoop Developer",
        "J2EE Architecture Implemented Session Facade Pattern",
        "XSL",
        "AJAX Servlets JSP",
        "JavaScript",
        "ANT",
        "Oozie Hadoop",
        "ETL Extract Transform",
        "SVNCVSGithub Experience",
        "Hadoop Programs",
        "CSS",
        "Solr",
        "Science in Computer Science Western Illinois University 2017 Bachelor of Technology in Information Technology JNTU Hyderabad",
        "SQL PLSQL Queries Joins Views ProceduresFunctions Triggers",
        "Apache NIFI",
        "Relational Database",
        "MapReduce",
        "NoSQL",
        "Springs IOC",
        "Created Spark",
        "Skills SQL",
        "MySql"
    ],
    "experience": "Experience in publishing the messages to various Kafka topics using Apache NIFI and consuming the message to HBase and MySql tables using Spark and Scala Worked on creating Spark jobs that process the true source files and successful in performing various transformations on the source data using SparkDataframeDataSet Spark SQL APIs Developed Sqoop scripts to migrate data from Teradata Oracle to Bigdata Environment Worked with Hue GUI in scheduling jobs with ease and File browsing Job browsing Metastore management Experience in importing and exporting the data using Sqoop from HDFS to Relational Database systems and viceversa Hands on experience in installation configuration supporting and managing Hadoop Clusters using Apache Cloudera CDH3 CDH4 Yarndistributions CDH 5X Performed Data scrubbing and processing with Oozie and for workflow automation and coordination Good knowledge in EMR Elastic Map Reducing to perform big data operations in AWSKnowledge in working with Amazon Web Services AWS using EC2 for computing and S3 as storage mechanism Hands on experience in analyzing log files for Hadoop and ecosystem services and finding root cause Hands on experience on handling different file formats like AVRO PARQUET Sequential files MAP Files CSV xml log ORC and RC Experience with NoSQL Database HBase Cassandra MongoDB Experience with AIXLinux RHEL Unix Shell Scripting and SQL Server 2008 Worked on data search toolElastic Search and data collection tool Logstash Strong knowledge in Hadoop cluster installation capacity planning and performance tuning benchmarking disaster recovery plan and application deployment in production cluster Experience in developing stored procedures triggers using SQL PLSQL in relational databases such as MS SQL Server 20052008 Exposed into methodologies Scrum Agile and Waterfall Work Experience Big Data Engineer American Express Phoenix AZ November 2018 to Present RESPONSIBILITIES Worked with extensive data sets in Big Data to uncover pattern problem unleash value for the Enterprise Worked with internal and external data sources on improving data accuracy coverage and generate recommendation on the process flow to accomplish the goal Ingestion of various types of data feeds from SOR and usecase perspective into Cornerstone 30 platform Reengineered legacy IDN FastTrack process to get the Bloomberg data directly from source to the CS30 Converted legacy Shell scripts to MapReduce jobs in a distributed manner without performing any kind of processing on the Edgenode to eliminate the burden Created Spark applications for data preprocessing for greater performance Developed Spark code and SparkSQLstreaming for faster testing and processing of data Experience in creating spark applications using RDD Dataframes Worked extensively on hive to analyse the data and create reports for data quality Implemented Partitioning Dynamic Partitions and Buckets in HIVE for increasing performance benefit and helping in organizing data in a logical fashion Written Hive queries for data analysis to meet the business requirements and Designed and developed User Defined Function UDF for Hive Involved in creating Hive tables Managed tables and External tables loading and analyzing data using hive queries Good knowledge about the configuration management tools like SVNCVSGithub Experience in configuring Event Engine nodes to import and export the data from Teradata to HDFS and viceversa Worked with source to get the history data as well as BAU data from IDN Teradata to the CornerStoneplatform and migrated also feeds from CS20 Expert in creating the nodes in Event Engine as per the usecase requirement to automate the process for the BAU data flow Exported the Event Engine nodes created in the silver environment to the IDN repository in BitBucket and created DaVinci package to migrate it to Platinum Worked with FDP team to create a secured flow to get the data from KAFKA Queue to CS30 Expert in creating the SFTP Connection to the internal and external source to get data in secured manner without any breakage Handle the production Incidents assigned to our workgroup promptly and fix the bugs or route it to the respective teams and optimized the SLAs Hadoop Developer Avast Emeryville CA September 2017 to 2018 RESPONSIBILITIES Developed real time data processing applications by using Scala and Python and implemented Apache Spark Streaming from various streaming sources like Kafka and JMS Experienced in writing live Realtime Processing and core jobs using Spark Streaming with Kafka as a data pipeline system Developed Shell Perl and Python scripts to automate and provide Control flow to Pig scripts Worked on Amazon AWS concepts like EMR and EC2 web services for fast and efficient processing of Big Data Involved in loading data from Linux file systems servers java web services using Kafka producers and partitions Applied Kafka custom encoders for custom input format to load data into Kafka Partitions Implement POC with Hadoop Extract data with Spark into HDFS Used Spark SQL with Scala for creating data frames and performed transformations on data frames Used Spark API over Cloudera Hadoop YARN to perform analytics on data in Hive Developed code to read data stream from Kafka and send it to respective bolts through respective stream Worked on Spark streaming using Apache Kafka for real time data processing Developed Map Reduce jobs using Map Reduce Java API and HIVEQL Developed UDF UDAF UDTF functions and implemented it in HIVE Queries Developing Scripts and Batch Job to schedule a bundle group of coordinators which consists of various Hadoop Programs using Oozie Experienced in optimizing Hive queries joins to handle different data sets Involved in ETL Data Integration and Migration by writing pig scripts Integrated Hadoop with Solr and implement search algorithms Experience in Storm for handling realtime processing Hands on Experience working in Hortonworks distribution Worked hands on NoSQL databases like MongoDB for POC purpose in storing images and URIs Designed and implemented MongoDB and associated RESTful web service Involved in writing test cases and implement test classes using MRUnit and mocking frameworks Developed Sqoop scripts to extract the data from MYSQL and load into HDFS Experience in processing large volume of data and skills in parallel execution of process using Talend functionality Used Talend tool to create workflows for processing data from multiple source systems Environment MapReduce HDFS Sqoop LINUX Oozie Hadoop Pig Hive Solr Spark Streaming Kafka Storm Spark Scala Python MongoDB Hadoop Cluster Amazon Web Services Talend Hadoop Developer Kelloggs Dec 2016 Aug2017Oakbrook IL RESPONSIBILITIES Experience with professional software engineering practices and best practices for the full software development life cycle including coding standards code reviews source control management and build processes Worked on analyzing Hadoop cluster and different big data analytic tools including Map Reduce Hive Written multiple MapReduce programs for data extraction transformation and aggregation from multiple file formats including XML JSON CSV other compressed file formats Worked on Teradata parallel transport TPT to load data from databases and files to Teradata Wrote views based on user andor reporting requirements Wrote Teradata Macros and used various Teradata analytic functions Involved in migration projects to migrate data from data warehouses on OracleDB2 and migrated those to Teradata Configured Flume source sink and memory channel to handle streaming data from server logs and JMS sources Experience in working with Flume to load the log data from multiple sources directly into HDFS Worked in the BI team in Big Data Hadoop cluster implementation and data integration in developing largescale system software Involved in source system analysis data analysis data modeling to ETL Extract Transform and Load Handling structured and unstructured data and applying ETL processes Worked extensively with Sqoop for importing and exporting the data from HDFS to Relational Database systemsmainframe and viceversa Loading data into HDFS Involved in collecting aggregating and moving data from servers to HDFS using Flume Implemented logging framework ELK stack Elastic Search LogStash Kibana on AWS Developed the Pig UDFS to preprocess the data for analysis Coding complex Oracle stored procedures functions packages and cursors for the client specific applications Experienced in using Java Rest API to perform CURD operations on HBase data Applied Hive queries to perform data analysis on HBase using Storage Handler to meet the business requirements Writing Hive Queries to Aggregate Data that needs to be pushed to the HBase Tables CreateModify shell scripts for scheduling various data cleansing scripts and ETL loading process Supports and assist QA Engineers in understanding testing and troubleshooting EnvironmentHadoop Hive Linux Map Reduce Sqoop Storm HBase Flume Eclipse Maven Junit agile methodologies Java Developer Mize Software Solutions 2014 to December 2014 July 2016Hyderabad India RESPONSIBILITIES Review the system requirements and attending requirements meetings with analysts and users Involved in the life cycle of the project from documentation to unit testing making development as priority Developed web pages using Struts framework JSP XML JavaScript HTML DHTML and CSS configure struts application use tag library Used Apache Struts framework includes the integrated AJAX Played major role in designing developing JSP pages and XML reports Developed Servlets and custom tags for JSP pages Developed few module Web pages using Springs IOC and Hibernate Designed and developed dynamic pages using HTML CSS layout techniques Java script Took the various challenges in the enhancement and completed them on time Extensive Used Exception handling and Multithreading for the optimum performance of the application Involved in design and implemented SOA SOAP next generation system on distributed platform Extensively used XSL as a XML parsing mechanism for showing Dynamic Web Pages in HTML format Implemented SOAP protocol to get the requests from the outside System Used CVS as a source control for code changes Used ANT scripts to build the project and JUnit to develop unit test cases Developed coding using SQL PLSQL Queries Joins Views ProceduresFunctions Triggers and Packages Provided development support for System Testing Product Testing User Acceptance Testing Data Conversion Testing Load Testing and Production Environment Java 15 J2EE AJAX Servlets JSP RUP Eclipse 31 Struts Spring 20 Hibernate XML CVS Java Script JQuery ANT SOAP Log4J DB2 Web Sphere server UNIX IBM Web Sphere Portal Server Javaweb Developer Highgate Systems Hyderabad Telangana May 2013 to September 2014 Hyderabad India RESPONSIBILITIES Collecting and understanding the User requirements and Functional specifications Development of GUI Using HTML CSS JSP and JavaScript Creating components for isolated business logic Deployment of application in J2EE Architecture Implemented Session Facade Pattern using Session and Entity Beans Developed message driven beans to listen to JMS Developed the Web Interface using Servlets Java Server Pages HTML and CSS Used WebLogic to deploy applications on local and development environments of the application Extensively used the JDBC Prepared Statement to embed the SQL queries into the java code Developed DAO Data Access Objects using Spring Framework 3 Developed Web applications with Rich Internet applications using Java applets Silverlight Java Used JavaScript to perform client side validations and StrutsValidator Framework for serverside validation Environment Java J2EE JDBC JSP Struts JMS Spring SQL MSAccess JavaScript HTML Education Master of Science in Computer Science Western Illinois University 2017 Bachelor of Technology in Information Technology JNTU Hyderabad Telangana 2013 Skills SQL 2 years JAVA 1 year CSS 1 year HTML 1 year JAVASCRIPT 1 year Additional Information TECHNICAL SKILLS Programming Languages Java Python SQL Scala and CC Big Data Ecosystem Hadoop MapReduce Kafka Spark Pig Hive YARN Flume Sqoop Oozie Zookeeper Talend Hadoop Distributions Cloudera Enterprise Horton Works EMC Pivotal Databases Oracle SQL Server PostgreSQL Web Technologies HTML XML JQuery Ajax CSS JavaScript JSON Streaming Tools Kafka RabbitMQ Testing Hadoop Testing Hive Testing MRUnit Operating Systems Linux Red HatUbuntuCentOS Windows 10817XP Cloud AWS S3 Redshift Cluster Technologies and Tools Servlets JSP Spring Boot MVC Batch Security Web Services Hibernate Maven GitHub Application Servers Tomcat JBoss IDEs Eclipse Net Beans IntelliJ",
    "extracted_keywords": [
        "Data",
        "Engineer",
        "Big",
        "Data",
        "Engineer",
        "Big",
        "Data",
        "Engineer",
        "American",
        "Express",
        "years",
        "IT",
        "experience",
        "experience",
        "Big",
        "Data",
        "Hadoop",
        "years",
        "experience",
        "Development",
        "Design",
        "Java",
        "enterprise",
        "applications",
        "working",
        "experience",
        "Hadoop",
        "ecosystem",
        "components",
        "MapReduce",
        "Hive",
        "Pig",
        "Sqoop",
        "Flume",
        "Spark",
        "Kafka",
        "Oozie",
        "Zookeeper",
        "performance",
        "techniques",
        "Hive",
        "knowledge",
        "Hadoop",
        "HDFS",
        "architecture",
        "MapReduce",
        "YARN",
        "framework",
        "Expert",
        "Hive",
        "tool",
        "tables",
        "data",
        "distribution",
        "bucketing",
        "writing",
        "HiveQL",
        "hands",
        "Experience",
        "messages",
        "Kafka",
        "topics",
        "Apache",
        "NIFI",
        "message",
        "HBase",
        "MySql",
        "tables",
        "Spark",
        "Scala",
        "Spark",
        "jobs",
        "source",
        "files",
        "transformations",
        "source",
        "data",
        "SparkDataframeDataSet",
        "Spark",
        "SQL",
        "APIs",
        "Developed",
        "Sqoop",
        "scripts",
        "data",
        "Teradata",
        "Oracle",
        "Bigdata",
        "Environment",
        "Hue",
        "GUI",
        "scheduling",
        "jobs",
        "ease",
        "File",
        "Job",
        "Metastore",
        "management",
        "Experience",
        "data",
        "Sqoop",
        "HDFS",
        "Relational",
        "Database",
        "systems",
        "viceversa",
        "Hands",
        "experience",
        "installation",
        "configuration",
        "Hadoop",
        "Clusters",
        "Apache",
        "Cloudera",
        "CDH3",
        "CDH4",
        "Yarndistributions",
        "CDH",
        "5X",
        "Performed",
        "Data",
        "processing",
        "Oozie",
        "workflow",
        "automation",
        "knowledge",
        "EMR",
        "Elastic",
        "Map",
        "data",
        "operations",
        "AWSKnowledge",
        "Amazon",
        "Web",
        "Services",
        "AWS",
        "EC2",
        "computing",
        "S3",
        "storage",
        "mechanism",
        "Hands",
        "experience",
        "log",
        "files",
        "Hadoop",
        "ecosystem",
        "services",
        "root",
        "Hands",
        "experience",
        "file",
        "formats",
        "AVRO",
        "PARQUET",
        "Sequential",
        "MAP",
        "Files",
        "CSV",
        "xml",
        "ORC",
        "RC",
        "Experience",
        "NoSQL",
        "Database",
        "HBase",
        "Cassandra",
        "MongoDB",
        "Experience",
        "AIXLinux",
        "RHEL",
        "Unix",
        "Shell",
        "Scripting",
        "SQL",
        "Server",
        "data",
        "search",
        "Search",
        "data",
        "collection",
        "tool",
        "Logstash",
        "Strong",
        "knowledge",
        "Hadoop",
        "cluster",
        "installation",
        "capacity",
        "planning",
        "performance",
        "disaster",
        "recovery",
        "plan",
        "application",
        "deployment",
        "production",
        "cluster",
        "Experience",
        "procedures",
        "triggers",
        "SQL",
        "PLSQL",
        "databases",
        "MS",
        "SQL",
        "Server",
        "methodologies",
        "Scrum",
        "Agile",
        "Waterfall",
        "Work",
        "Experience",
        "Big",
        "Data",
        "Engineer",
        "American",
        "Express",
        "Phoenix",
        "AZ",
        "November",
        "RESPONSIBILITIES",
        "data",
        "sets",
        "Big",
        "Data",
        "pattern",
        "problem",
        "unleash",
        "value",
        "Enterprise",
        "data",
        "sources",
        "data",
        "accuracy",
        "coverage",
        "recommendation",
        "process",
        "goal",
        "Ingestion",
        "types",
        "data",
        "SOR",
        "perspective",
        "Cornerstone",
        "platform",
        "legacy",
        "IDN",
        "FastTrack",
        "process",
        "Bloomberg",
        "data",
        "source",
        "CS30",
        "legacy",
        "Shell",
        "scripts",
        "MapReduce",
        "jobs",
        "manner",
        "kind",
        "processing",
        "Edgenode",
        "burden",
        "Spark",
        "applications",
        "data",
        "performance",
        "Developed",
        "Spark",
        "code",
        "SparkSQLstreaming",
        "testing",
        "processing",
        "data",
        "Experience",
        "spark",
        "applications",
        "RDD",
        "Dataframes",
        "hive",
        "data",
        "reports",
        "data",
        "quality",
        "Partitioning",
        "Dynamic",
        "Partitions",
        "Buckets",
        "HIVE",
        "performance",
        "benefit",
        "data",
        "fashion",
        "Written",
        "Hive",
        "data",
        "analysis",
        "business",
        "requirements",
        "User",
        "Defined",
        "Function",
        "UDF",
        "Hive",
        "Hive",
        "tables",
        "tables",
        "tables",
        "data",
        "hive",
        "queries",
        "knowledge",
        "configuration",
        "management",
        "tools",
        "SVNCVSGithub",
        "Experience",
        "Event",
        "Engine",
        "nodes",
        "data",
        "Teradata",
        "HDFS",
        "viceversa",
        "source",
        "history",
        "data",
        "BAU",
        "data",
        "IDN",
        "Teradata",
        "CornerStoneplatform",
        "CS20",
        "Expert",
        "nodes",
        "Event",
        "Engine",
        "requirement",
        "process",
        "BAU",
        "data",
        "flow",
        "Event",
        "Engine",
        "nodes",
        "environment",
        "IDN",
        "repository",
        "BitBucket",
        "DaVinci",
        "package",
        "Platinum",
        "Worked",
        "FDP",
        "team",
        "flow",
        "data",
        "KAFKA",
        "Queue",
        "CS30",
        "Expert",
        "SFTP",
        "Connection",
        "source",
        "data",
        "manner",
        "breakage",
        "production",
        "Incidents",
        "workgroup",
        "bugs",
        "teams",
        "SLAs",
        "Hadoop",
        "Developer",
        "Avast",
        "Emeryville",
        "CA",
        "September",
        "RESPONSIBILITIES",
        "time",
        "data",
        "processing",
        "applications",
        "Scala",
        "Python",
        "Apache",
        "Spark",
        "Streaming",
        "sources",
        "Kafka",
        "JMS",
        "Realtime",
        "Processing",
        "core",
        "jobs",
        "Spark",
        "Streaming",
        "Kafka",
        "data",
        "pipeline",
        "system",
        "Developed",
        "Shell",
        "Perl",
        "Python",
        "scripts",
        "Control",
        "flow",
        "Pig",
        "scripts",
        "Amazon",
        "AWS",
        "concepts",
        "EMR",
        "EC2",
        "web",
        "services",
        "processing",
        "Big",
        "Data",
        "loading",
        "data",
        "Linux",
        "file",
        "systems",
        "servers",
        "java",
        "web",
        "services",
        "Kafka",
        "producers",
        "partitions",
        "Kafka",
        "custom",
        "encoders",
        "custom",
        "input",
        "format",
        "data",
        "Kafka",
        "Partitions",
        "Implement",
        "POC",
        "Hadoop",
        "Extract",
        "data",
        "Spark",
        "HDFS",
        "Spark",
        "SQL",
        "Scala",
        "data",
        "frames",
        "transformations",
        "data",
        "frames",
        "Spark",
        "API",
        "Cloudera",
        "Hadoop",
        "YARN",
        "analytics",
        "data",
        "Hive",
        "Developed",
        "code",
        "data",
        "stream",
        "Kafka",
        "bolts",
        "stream",
        "Spark",
        "streaming",
        "Apache",
        "Kafka",
        "time",
        "data",
        "Developed",
        "Map",
        "Reduce",
        "jobs",
        "Map",
        "Reduce",
        "Java",
        "API",
        "HIVEQL",
        "UDF",
        "UDTF",
        "functions",
        "HIVE",
        "Queries",
        "Scripts",
        "Batch",
        "Job",
        "bundle",
        "group",
        "coordinators",
        "Hadoop",
        "Programs",
        "Oozie",
        "Hive",
        "queries",
        "data",
        "sets",
        "ETL",
        "Data",
        "Integration",
        "Migration",
        "pig",
        "Integrated",
        "Hadoop",
        "Solr",
        "search",
        "algorithms",
        "Experience",
        "Storm",
        "realtime",
        "Hands",
        "Experience",
        "Hortonworks",
        "distribution",
        "hands",
        "databases",
        "MongoDB",
        "POC",
        "purpose",
        "images",
        "URIs",
        "MongoDB",
        "RESTful",
        "web",
        "service",
        "test",
        "cases",
        "test",
        "classes",
        "MRUnit",
        "frameworks",
        "Sqoop",
        "scripts",
        "data",
        "MYSQL",
        "HDFS",
        "Experience",
        "volume",
        "data",
        "skills",
        "execution",
        "process",
        "Talend",
        "functionality",
        "Talend",
        "tool",
        "workflows",
        "processing",
        "data",
        "source",
        "systems",
        "Environment",
        "MapReduce",
        "HDFS",
        "Sqoop",
        "LINUX",
        "Oozie",
        "Hadoop",
        "Pig",
        "Hive",
        "Solr",
        "Spark",
        "Streaming",
        "Kafka",
        "Storm",
        "Spark",
        "Scala",
        "Python",
        "MongoDB",
        "Hadoop",
        "Cluster",
        "Amazon",
        "Web",
        "Services",
        "Talend",
        "Hadoop",
        "Developer",
        "Kelloggs",
        "Dec",
        "Aug2017Oakbrook",
        "IL",
        "RESPONSIBILITIES",
        "Experience",
        "software",
        "engineering",
        "practices",
        "practices",
        "software",
        "development",
        "life",
        "cycle",
        "standards",
        "code",
        "source",
        "control",
        "management",
        "processes",
        "Hadoop",
        "cluster",
        "data",
        "tools",
        "Map",
        "Reduce",
        "Hive",
        "Written",
        "MapReduce",
        "programs",
        "data",
        "extraction",
        "transformation",
        "aggregation",
        "file",
        "formats",
        "XML",
        "CSV",
        "file",
        "formats",
        "transport",
        "TPT",
        "data",
        "databases",
        "files",
        "Teradata",
        "Wrote",
        "views",
        "user",
        "reporting",
        "requirements",
        "Wrote",
        "Teradata",
        "Macros",
        "functions",
        "migration",
        "projects",
        "data",
        "data",
        "warehouses",
        "OracleDB2",
        "Teradata",
        "Configured",
        "Flume",
        "source",
        "sink",
        "memory",
        "channel",
        "streaming",
        "data",
        "server",
        "logs",
        "JMS",
        "Experience",
        "Flume",
        "log",
        "data",
        "sources",
        "HDFS",
        "Worked",
        "BI",
        "team",
        "Big",
        "Data",
        "Hadoop",
        "cluster",
        "implementation",
        "data",
        "integration",
        "largescale",
        "system",
        "software",
        "source",
        "system",
        "analysis",
        "data",
        "analysis",
        "data",
        "ETL",
        "Extract",
        "Transform",
        "Load",
        "Handling",
        "data",
        "ETL",
        "processes",
        "Sqoop",
        "data",
        "HDFS",
        "Relational",
        "Database",
        "systemsmainframe",
        "viceversa",
        "Loading",
        "data",
        "HDFS",
        "data",
        "servers",
        "HDFS",
        "Flume",
        "framework",
        "ELK",
        "stack",
        "Elastic",
        "Search",
        "LogStash",
        "Kibana",
        "AWS",
        "Pig",
        "UDFS",
        "data",
        "analysis",
        "Oracle",
        "procedures",
        "functions",
        "packages",
        "cursors",
        "client",
        "applications",
        "Java",
        "Rest",
        "API",
        "CURD",
        "operations",
        "HBase",
        "data",
        "Applied",
        "Hive",
        "data",
        "analysis",
        "HBase",
        "Storage",
        "Handler",
        "business",
        "requirements",
        "Hive",
        "Queries",
        "Data",
        "HBase",
        "Tables",
        "CreateModify",
        "shell",
        "scripts",
        "data",
        "cleansing",
        "scripts",
        "ETL",
        "loading",
        "process",
        "QA",
        "Engineers",
        "testing",
        "EnvironmentHadoop",
        "Hive",
        "Linux",
        "Map",
        "Reduce",
        "Sqoop",
        "Storm",
        "HBase",
        "Flume",
        "Eclipse",
        "Maven",
        "Junit",
        "methodologies",
        "Java",
        "Developer",
        "Mize",
        "Software",
        "Solutions",
        "December",
        "July",
        "India",
        "RESPONSIBILITIES",
        "system",
        "requirements",
        "requirements",
        "meetings",
        "analysts",
        "users",
        "life",
        "cycle",
        "project",
        "documentation",
        "unit",
        "testing",
        "development",
        "priority",
        "web",
        "pages",
        "Struts",
        "framework",
        "JSP",
        "XML",
        "JavaScript",
        "HTML",
        "DHTML",
        "CSS",
        "configure",
        "struts",
        "application",
        "use",
        "tag",
        "library",
        "Apache",
        "Struts",
        "framework",
        "AJAX",
        "role",
        "JSP",
        "pages",
        "XML",
        "reports",
        "Developed",
        "Servlets",
        "custom",
        "tags",
        "JSP",
        "pages",
        "module",
        "Web",
        "pages",
        "Springs",
        "IOC",
        "Hibernate",
        "pages",
        "HTML",
        "CSS",
        "layout",
        "Java",
        "script",
        "challenges",
        "enhancement",
        "time",
        "Exception",
        "handling",
        "performance",
        "application",
        "design",
        "SOA",
        "SOAP",
        "generation",
        "system",
        "platform",
        "XSL",
        "XML",
        "parsing",
        "mechanism",
        "Dynamic",
        "Web",
        "Pages",
        "HTML",
        "format",
        "SOAP",
        "protocol",
        "requests",
        "System",
        "CVS",
        "source",
        "control",
        "code",
        "changes",
        "ANT",
        "scripts",
        "project",
        "JUnit",
        "unit",
        "test",
        "cases",
        "SQL",
        "PLSQL",
        "Queries",
        "Joins",
        "Views",
        "ProceduresFunctions",
        "Triggers",
        "Packages",
        "development",
        "support",
        "System",
        "Testing",
        "Product",
        "Testing",
        "User",
        "Acceptance",
        "Testing",
        "Data",
        "Conversion",
        "Testing",
        "Load",
        "Testing",
        "Production",
        "Environment",
        "Java",
        "J2EE",
        "AJAX",
        "Servlets",
        "JSP",
        "RUP",
        "Eclipse",
        "Struts",
        "Spring",
        "Hibernate",
        "XML",
        "CVS",
        "Java",
        "Script",
        "JQuery",
        "ANT",
        "SOAP",
        "DB2",
        "Web",
        "Sphere",
        "server",
        "UNIX",
        "IBM",
        "Web",
        "Sphere",
        "Portal",
        "Server",
        "Javaweb",
        "Developer",
        "Highgate",
        "Systems",
        "Hyderabad",
        "Telangana",
        "May",
        "September",
        "Hyderabad",
        "India",
        "User",
        "requirements",
        "specifications",
        "Development",
        "GUI",
        "HTML",
        "CSS",
        "JSP",
        "JavaScript",
        "components",
        "business",
        "logic",
        "Deployment",
        "application",
        "J2EE",
        "Architecture",
        "Implemented",
        "Session",
        "Facade",
        "Pattern",
        "Session",
        "Entity",
        "Beans",
        "message",
        "beans",
        "JMS",
        "Web",
        "Interface",
        "Servlets",
        "Java",
        "Server",
        "Pages",
        "HTML",
        "CSS",
        "WebLogic",
        "applications",
        "development",
        "environments",
        "application",
        "JDBC",
        "Prepared",
        "Statement",
        "SQL",
        "java",
        "code",
        "Developed",
        "DAO",
        "Data",
        "Access",
        "Spring",
        "Framework",
        "Web",
        "applications",
        "Rich",
        "Internet",
        "applications",
        "Java",
        "applets",
        "Silverlight",
        "Java",
        "JavaScript",
        "client",
        "side",
        "validations",
        "StrutsValidator",
        "Framework",
        "serverside",
        "validation",
        "Environment",
        "Java",
        "J2EE",
        "JDBC",
        "JSP",
        "Struts",
        "JMS",
        "Spring",
        "SQL",
        "MSAccess",
        "JavaScript",
        "HTML",
        "Education",
        "Master",
        "Science",
        "Computer",
        "Science",
        "Western",
        "Illinois",
        "University",
        "Bachelor",
        "Technology",
        "Information",
        "Technology",
        "JNTU",
        "Hyderabad",
        "Telangana",
        "Skills",
        "SQL",
        "years",
        "year",
        "CSS",
        "year",
        "HTML",
        "year",
        "JAVASCRIPT",
        "year",
        "Additional",
        "Information",
        "TECHNICAL",
        "SKILLS",
        "Programming",
        "Languages",
        "Java",
        "Python",
        "SQL",
        "Scala",
        "CC",
        "Big",
        "Data",
        "Ecosystem",
        "Hadoop",
        "MapReduce",
        "Kafka",
        "Spark",
        "Pig",
        "Hive",
        "YARN",
        "Flume",
        "Sqoop",
        "Oozie",
        "Zookeeper",
        "Talend",
        "Hadoop",
        "Distributions",
        "Cloudera",
        "Enterprise",
        "Horton",
        "EMC",
        "Pivotal",
        "Databases",
        "Oracle",
        "SQL",
        "Server",
        "PostgreSQL",
        "Web",
        "Technologies",
        "HTML",
        "XML",
        "JQuery",
        "Ajax",
        "CSS",
        "JavaScript",
        "JSON",
        "Streaming",
        "Tools",
        "Kafka",
        "RabbitMQ",
        "Testing",
        "Hadoop",
        "Testing",
        "Hive",
        "Testing",
        "MRUnit",
        "Operating",
        "Systems",
        "Linux",
        "Red",
        "HatUbuntuCentOS",
        "Windows",
        "10817XP",
        "Cloud",
        "S3",
        "Redshift",
        "Cluster",
        "Technologies",
        "Tools",
        "Servlets",
        "JSP",
        "Spring",
        "Boot",
        "MVC",
        "Batch",
        "Security",
        "Web",
        "Services",
        "Hibernate",
        "Maven",
        "GitHub",
        "Application",
        "Servers",
        "Tomcat",
        "JBoss",
        "IDEs",
        "Eclipse",
        "Net",
        "Beans"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T19:49:25.154533",
    "resume_data": "Big Data Engineer Big Data Engineer Big Data Engineer American Express Having around 6 years of total IT experience with over 4years experience in Big Data Hadoop 2 years of experience in Development and Design of Java based enterprise applications Extensive working experience on Hadoop ecosystem components like HDFS MapReduce Hive Pig Sqoop Flume Spark Kafka Oozie and Zookeeper Implemented performance tuning techniques for Hive queries Strong knowledge on Hadoop HDFS architecture MapReduce MRv1 and YARN MRv2 framework Expert in working with Hive tool creating tables data distribution by implementing partitioning and bucketing writing and optimizing the HiveQL queries Strong hands on Experience in publishing the messages to various Kafka topics using Apache NIFI and consuming the message to HBase and MySql tables using Spark and Scala Worked on creating Spark jobs that process the true source files and successful in performing various transformations on the source data using SparkDataframeDataSet Spark SQL APIs Developed Sqoop scripts to migrate data from Teradata Oracle to Bigdata Environment Worked with Hue GUI in scheduling jobs with ease and File browsing Job browsing Metastore management Experience in importing and exporting the data using Sqoop from HDFS to Relational Database systems and viceversa Hands on experience in installation configuration supporting and managing Hadoop Clusters using Apache Cloudera CDH3 CDH4 Yarndistributions CDH 5X Performed Data scrubbing and processing with Oozie and for workflow automation and coordination Good knowledge in EMR Elastic Map Reducing to perform big data operations in AWSKnowledge in working with Amazon Web Services AWS using EC2 for computing and S3 as storage mechanism Hands on experience in analyzing log files for Hadoop and ecosystem services and finding root cause Hands on experience on handling different file formats like AVRO PARQUET Sequential files MAP Files CSV xml log ORC and RC Experience with NoSQL Database HBase Cassandra MongoDB Experience with AIXLinux RHEL Unix Shell Scripting and SQL Server 2008 Worked on data search toolElastic Search and data collection tool Logstash Strong knowledge in Hadoop cluster installation capacity planning and performance tuning benchmarking disaster recovery plan and application deployment in production cluster Experience in developing stored procedures triggers using SQL PLSQL in relational databases such as MS SQL Server 20052008 Exposed into methodologies Scrum Agile and Waterfall Work Experience Big Data Engineer American Express Phoenix AZ November 2018 to Present RESPONSIBILITIES Worked with extensive data sets in Big Data to uncover pattern problem unleash value for the Enterprise Worked with internal and external data sources on improving data accuracy coverage and generate recommendation on the process flow to accomplish the goal Ingestion of various types of data feeds from SOR and usecase perspective into Cornerstone 30 platform Reengineered legacy IDN FastTrack process to get the Bloomberg data directly from source to the CS30 Converted legacy Shell scripts to MapReduce jobs in a distributed manner without performing any kind of processing on the Edgenode to eliminate the burden Created Spark applications for data preprocessing for greater performance Developed Spark code and SparkSQLstreaming for faster testing and processing of data Experience in creating spark applications using RDD Dataframes Worked extensively on hive to analyse the data and create reports for data quality Implemented Partitioning Dynamic Partitions and Buckets in HIVE for increasing performance benefit and helping in organizing data in a logical fashion Written Hive queries for data analysis to meet the business requirements and Designed and developed User Defined Function UDF for Hive Involved in creating Hive tables Managed tables and External tables loading and analyzing data using hive queries Good knowledge about the configuration management tools like SVNCVSGithub Experience in configuring Event Engine nodes to import and export the data from Teradata to HDFS and viceversa Worked with source to get the history data as well as BAU data from IDN Teradata to the CornerStoneplatform and migrated also feeds from CS20 Expert in creating the nodes in Event Engine as per the usecase requirement to automate the process for the BAU data flow Exported the Event Engine nodes created in the silver environment to the IDN repository in BitBucket and created DaVinci package to migrate it to Platinum Worked with FDP team to create a secured flow to get the data from KAFKA Queue to CS30 Expert in creating the SFTP Connection to the internal and external source to get data in secured manner without any breakage Handle the production Incidents assigned to our workgroup promptly and fix the bugs or route it to the respective teams and optimized the SLAs Hadoop Developer Avast Emeryville CA September 2017 to 2018 RESPONSIBILITIES Developed real time data processing applications by using Scala and Python and implemented Apache Spark Streaming from various streaming sources like Kafka and JMS Experienced in writing live Realtime Processing and core jobs using Spark Streaming with Kafka as a data pipeline system Developed Shell Perl and Python scripts to automate and provide Control flow to Pig scripts Worked on Amazon AWS concepts like EMR and EC2 web services for fast and efficient processing of Big Data Involved in loading data from Linux file systems servers java web services using Kafka producers and partitions Applied Kafka custom encoders for custom input format to load data into Kafka Partitions Implement POC with Hadoop Extract data with Spark into HDFS Used Spark SQL with Scala for creating data frames and performed transformations on data frames Used Spark API over Cloudera Hadoop YARN to perform analytics on data in Hive Developed code to read data stream from Kafka and send it to respective bolts through respective stream Worked on Spark streaming using Apache Kafka for real time data processing Developed Map Reduce jobs using Map Reduce Java API and HIVEQL Developed UDF UDAF UDTF functions and implemented it in HIVE Queries Developing Scripts and Batch Job to schedule a bundle group of coordinators which consists of various Hadoop Programs using Oozie Experienced in optimizing Hive queries joins to handle different data sets Involved in ETL Data Integration and Migration by writing pig scripts Integrated Hadoop with Solr and implement search algorithms Experience in Storm for handling realtime processing Hands on Experience working in Hortonworks distribution Worked hands on NoSQL databases like MongoDB for POC purpose in storing images and URIs Designed and implemented MongoDB and associated RESTful web service Involved in writing test cases and implement test classes using MRUnit and mocking frameworks Developed Sqoop scripts to extract the data from MYSQL and load into HDFS Experience in processing large volume of data and skills in parallel execution of process using Talend functionality Used Talend tool to create workflows for processing data from multiple source systems Environment MapReduce HDFS Sqoop LINUX Oozie Hadoop Pig Hive Solr Spark Streaming Kafka Storm Spark Scala Python MongoDB Hadoop Cluster Amazon Web Services Talend Hadoop Developer Kelloggs Dec 2016 Aug2017Oakbrook IL RESPONSIBILITIES Experience with professional software engineering practices and best practices for the full software development life cycle including coding standards code reviews source control management and build processes Worked on analyzing Hadoop cluster and different big data analytic tools including Map Reduce Hive Written multiple MapReduce programs for data extraction transformation and aggregation from multiple file formats including XML JSON CSV other compressed file formats Worked on Teradata parallel transport TPT to load data from databases and files to Teradata Wrote views based on user andor reporting requirements Wrote Teradata Macros and used various Teradata analytic functions Involved in migration projects to migrate data from data warehouses on OracleDB2 and migrated those to Teradata Configured Flume source sink and memory channel to handle streaming data from server logs and JMS sources Experience in working with Flume to load the log data from multiple sources directly into HDFS Worked in the BI team in Big Data Hadoop cluster implementation and data integration in developing largescale system software Involved in source system analysis data analysis data modeling to ETL Extract Transform and Load Handling structured and unstructured data and applying ETL processes Worked extensively with Sqoop for importing and exporting the data from HDFS to Relational Database systemsmainframe and viceversa Loading data into HDFS Involved in collecting aggregating and moving data from servers to HDFS using Flume Implemented logging framework ELK stack Elastic Search LogStash Kibana on AWS Developed the Pig UDFS to preprocess the data for analysis Coding complex Oracle stored procedures functions packages and cursors for the client specific applications Experienced in using Java Rest API to perform CURD operations on HBase data Applied Hive queries to perform data analysis on HBase using Storage Handler to meet the business requirements Writing Hive Queries to Aggregate Data that needs to be pushed to the HBase Tables CreateModify shell scripts for scheduling various data cleansing scripts and ETL loading process Supports and assist QA Engineers in understanding testing and troubleshooting EnvironmentHadoop Hive Linux Map Reduce Sqoop Storm HBase Flume Eclipse Maven Junit agile methodologies Java Developer Mize Software Solutions 2014 to December 2014 July 2016Hyderabad India RESPONSIBILITIES Review the system requirements and attending requirements meetings with analysts and users Involved in the life cycle of the project from documentation to unit testing making development as priority Developed web pages using Struts framework JSP XML JavaScript HTML DHTML and CSS configure struts application use tag library Used Apache Struts framework includes the integrated AJAX Played major role in designing developing JSP pages and XML reports Developed Servlets and custom tags for JSP pages Developed few module Web pages using Springs IOC and Hibernate Designed and developed dynamic pages using HTML CSS layout techniques Java script Took the various challenges in the enhancement and completed them on time Extensive Used Exception handling and Multithreading for the optimum performance of the application Involved in design and implemented SOA SOAP next generation system on distributed platform Extensively used XSL as a XML parsing mechanism for showing Dynamic Web Pages in HTML format Implemented SOAP protocol to get the requests from the outside System Used CVS as a source control for code changes Used ANT scripts to build the project and JUnit to develop unit test cases Developed coding using SQL PLSQL Queries Joins Views ProceduresFunctions Triggers and Packages Provided development support for System Testing Product Testing User Acceptance Testing Data Conversion Testing Load Testing and Production Environment Java 15 J2EE AJAX Servlets JSP RUP Eclipse 31 Struts Spring 20 Hibernate XML CVS Java Script JQuery ANT SOAP Log4J DB2 Web Sphere server UNIX IBM Web Sphere Portal Server Javaweb Developer Highgate Systems Hyderabad Telangana May 2013 to September 2014 Hyderabad India RESPONSIBILITIES Collecting and understanding the User requirements and Functional specifications Development of GUI Using HTML CSS JSP and JavaScript Creating components for isolated business logic Deployment of application in J2EE Architecture Implemented Session Facade Pattern using Session and Entity Beans Developed message driven beans to listen to JMS Developed the Web Interface using Servlets Java Server Pages HTML and CSS Used WebLogic to deploy applications on local and development environments of the application Extensively used the JDBC Prepared Statement to embed the SQL queries into the java code Developed DAO Data Access Objects using Spring Framework 3 Developed Web applications with Rich Internet applications using Java applets Silverlight Java Used JavaScript to perform client side validations and StrutsValidator Framework for serverside validation Environment Java J2EE JDBC JSP Struts JMS Spring SQL MSAccess JavaScript HTML Education Master of Science in Computer Science Western Illinois University 2017 Bachelor of Technology in Information Technology JNTU Hyderabad Telangana 2013 Skills SQL 2 years JAVA 1 year CSS 1 year HTML 1 year JAVASCRIPT 1 year Additional Information TECHNICAL SKILLS Programming Languages Java Python SQL Scala and CC Big Data Ecosystem Hadoop MapReduce Kafka Spark Pig Hive YARN Flume Sqoop Oozie Zookeeper Talend Hadoop Distributions Cloudera Enterprise Horton Works EMC Pivotal Databases Oracle SQL Server PostgreSQL Web Technologies HTML XML JQuery Ajax CSS JavaScript JSON Streaming Tools Kafka RabbitMQ Testing Hadoop Testing Hive Testing MRUnit Operating Systems Linux Red HatUbuntuCentOS Windows 10817XP Cloud AWS S3 Redshift Cluster Technologies and Tools Servlets JSP Spring Boot MVC Batch Security Web Services Hibernate Maven GitHub Application Servers Tomcat JBoss IDEs Eclipse Net Beans IntelliJ",
    "unique_id": "44905177-f47e-4dc4-8ff5-6524c0d6647f"
}