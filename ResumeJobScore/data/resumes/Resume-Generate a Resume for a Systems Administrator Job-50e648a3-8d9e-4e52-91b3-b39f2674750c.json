{
    "clean_data": "Spark and Hadoop Developer Spark and Hadoop span lDeveloperspan Spark and Hadoop Developer Vanguard 6 Years of professional experience in IT which includes around 4 years of comprehensive experience as Hadoop and Spark Developer and related technologies In depth understandingknowledge of Hadoop Architecture and various components such as HDFS Name Node Data Node Map Reduce Spark and Spark SQL Experience in converting HiveSQL queries into Spark transformations using Spark RDDs Exposure on usage of Apache Kafka to develop data pipeline of logs as a stream of messages using producers and consumers Knowledge in handling Kafka cluster and created several topologies to support realtime processing requirements Experience of converting various File Formats queries into Spark transformations using Data Frames and Datasets Experience of developing SQL scripts using Spark for handling different data sets and verifying the performance over Map Reduce jobs Experience in creating Kafka producer and Kafka consumer for Spark streaming Exposure to Spark Spark Streaming Scala and Creating the Data Frames handled in Spark with Scala Indepth understanding of Spark Architecture including Spark SQL Data Frames Spark Streaming Extensive experience on importing and exporting data using stream processing platforms like Flume and Kafka Implemented Sqoop for large dataset transfer between Hadoop and RDBMS Hands on experience in working on Spark SQL queries Data frames and import data from Data sources perform transformations perform readwrite operations save the results to output directory into HDFS Worked with AWS to migrate the entire Data Centers to the cloud using EC2 S3 and EMR Experience in importing and exporting Multi Terabytes of data using Sqoop from HDFS to Relational Database Systems RDBMS and viceversa Experience in writing shell scripts to dump the Shared data from MySQL Oracle servers to HDFS Highly experienced in importing and exporting data between HDFS and Relational Database Management systems using Sqoop Good Experience working with Amazon AWS for setting up Hadoop cluster Hand on experience in installing configuring and using Hadoop ecosystem components like Hadoop MapReduce HDFS HBase Oozie Hive Sqoop Zookeeper and Flume Worked on custom Spark Transformations and variety of data formats such as JSON Compressed CSV ORC AVRO etc and reading data from various sources like HBase and Hive Performed mapside joins on RDD Good understanding of Amazon web services like Elastic MapReduce EMR EC2 Experience in ETL operations on Hive to Spark Gained hands on experience in writing shell scripts in UNIX Experienced with processing different file formats like Avro XML JSON and Sequence file formats using Spark Experience in implementing Spark using Scala and Spark SQL for faster analyzing and processing of data Good understanding in configuring simple to complex work flows using Oozie Good understanding of NoSQL databases like MongoDB and HBase Worked on different operating systems like UNIXLinux Windows Worked in managing VMs in Cloudera and Horton Works Experience as a java Developer in clientserver technologies using JSP JDBC and SQL Work Experience Spark and Hadoop Developer Vanguard Malvern PA October 2018 to Present Responsibilities Worked on Spark streaming using Apache Kafka for real time data processing Experienced with Kafka to ingest data into Spark Engine Configured Spark streaming to get ongoing information from the Kafka and store the stream information to HDFS Involved in converting HiveSQL queries into Spark transformations using Spark RDDs Data frames Spark SQL and Scala Performed advanced procedures like text analytics and processing using the inmemory computing capabilities of Spark using Scala Developed Scala and Spark SQL code to extract data from various databases Used Spark SQL to process the huge amount of structured data and Implemented Spark RDD transformations actions to migrate Map reduce algorithms Extensively worked with all kinds of UnStructured SemiStructured and Structured data Developed multiple Kafka Producers and Consumers from as per the software requirement specifications Creatively communicated and presented models to business customers and executives utilizing a variety of formats and visualization methodologies Used Kafka extensively in gathering and moving log data files from Application Servers to a central location in Hadoop Distributed File System HDFS Used Spark and SparkSQL to read the parquet data and create the tables in hive using the Scala API Developed robust set of codes that are tested automated structured and efficient Performed mapside joins on RDD Spark SQL and Data Frames Migrating the needed data from Oracle MySQL in to HDFS using Sqoop and importing various formats of flat files in to HDFS Uploaded and processed more than 30 terabytes of data from various structured and unstructured sources into HDFS using Sqoop Implemented the data backup strategies for the data in the HDFS cluster Developed a data pipeline using Spark and Hive to ingest transform and analyzing data Developed Spark code using Scala and SparkSQL for faster testing and data processing Imported the data from relational databases into HDFS using Sqoop Understanding of data storage and retrieval techniques ETL and databases relational databases tuple stores Hadoop HUE MySQL and Oracle databases Environment Apache Spark Cloudera CDH 512 Scala Spark SQL Data Frames Kafka SBT build HUE Sqoop Zookeeper HDFS Hadoop Developer Century link Littleton CO January 2018 to October 2018 Responsibilities Involved in converting Map Reduce programs into Spark transformations using Spark RDDs on Scala Developed analytical component using Scala and Spark Involved in converting HiveSQL queries into Spark transformations using Spark RDDs and Scala Real streaming the data using Spark with Flume and store the stream data to HDFS using Scala Installing configuring troubleshooting of Hadoop with Cloudera Ecosystem Configuring Troubleshooting job tracker Nodes task tracker data nodes MapReduce Spark Hive HBase Sqoop and Oozie Involved in complete Implementation lifecycle specialized in writing custom Map Reduce Pig and Hive programs Extensively used Sqoop to get data from RDBMS sources like Teradata Worked on reading multiple data formats on HDFS using Scala Used Cloudera Manager to monitor and manage Hadoop Cluster Deployed a multinode Hadoop cluster Worked extensively with SQOOP for importing metadata from Oracle Extensively used HiveHQL or Hive queries to query or search for a string in Hive tables in HDFS Written Hive jobs to parse the logs and structure them in tabular format to facilitate effective querying on the log data Created HBase tables to store various data formats of data coming from various sources Used Impala to query the Hadoop data stored in HDFS Worked on streaming log data into HDFS from web servers using Flume Was responsible for importing the data Log files from various sources into HDFS using Flume Creating Hive tables on JSON data Creating Hive tables on JSON data Run the Hive queries in Hue and CLI Implemented POC for using APACHE IMPALA for data processing on top of HIVE Migrated ETL jobs to Spark to do Transformations even joins and some preaggregations before storing the data onto HDFS Implement Flume Spark Stream framework for real time data processing Environment UNIX HDFS Hive Spark Scala Flume Sqoop HBase Zookeeper CDH 54 Hadoop Developer ArcelorMittal Burns Harbor IN December 2015 to December 2017 Responsibilities Build and maintain scalable data pipelines using the Hadoop ecosystem and other open source components like Hive and HBase Handle the data exchange between HDFS and RDBMS using Sqoop Responsible for building scalable distributed data solutions using Hadoop Cloudera Distribution Developed several advanced Map Reduce programs to process data files received Developed Hive Scripts Hive UDFs to load data files into Hadoop Close monitoring and analysis of the Map Reduce job executions on cluster at task level Import the data from various sources like HDFSHBase into Hive Extensively worked on Hive for ETL Transformations and optimized Hive Queries Used Flume to collect aggregate and store the web log data from various sources like web servers mobile and network Devices and pushed to HDFS Created tables in Impala Created Partition tables run the hive queries in hue created tables HBase Created folders in HDFS Importing and Exporting Data from MySQLOracle to HiveQL Importing and Exporting Data from MySQLOracle to HDFS Write the Impala queries Installed Oozie workflow engine to run multiple Hive jobs Environment CDH59 HDFS Hive hue Sqoop Impala HBase Oozie Hive Flume Linux Java developer Integra Micro Systems February 2015 to November 2015 Responsibilities Involved in Analysis Design Development Integration and Testing of application modules Involved in developing a custom framework like Spring Framework with more features to meet the business needs Performed requirement analysis design coding and implementation team coordination code review testing and Installation Developed server side utilities using JAVA technologies Servlets JSP Developed presentation layers using JSP custom tags and JavaScript Implemented design patterns Business Delegate Singleton Flow Controller DAO and Value Object patterns Developed Role Based Access Control to restrict the users to access specific modules based on their roles Used Oracle as the backend application and used Hibernate Framework for ORM Deployed the application on WebSphere server using Eclipse as the IDE Used Tomcat server 55 and configured it with Eclipse IDE Performed extensive Unit Testing for the application Responsible for Design and development of Web pages using HTML CSS including Ajax controls and XML Environment WebSphere 51 Tomcat 50 Oracle 9i Hibernate30 Eclipse 32 JSP Java Script Servlets XML Eclipse Junit Spring plugins Tomcat Jr Java Developer PRIME TECHNOLOGY GROUP May 2013 to February 2015 Responsibilities Involved in the analysis design implementation and testing of the project Implemented the presentation layer with HTML and JavaScript Developed web components using JSP Servlets and JDBC Implemented database using SQL Server Designed tables and indexes Wrote complex SQL and stored procedures Involved in fixing bugs and unit testing with test cases using JUnit Developed user and technical documentation Environment Java JSP Servlets JDBC HTML JavaScript MySQL JUnit Eclipse IDE Education Bachelors Skills MYSQL 4 years ORACLE 4 years Hadoop 3 years HADOOP 3 years APACHE HADOOP HDFS 3 years Additional Information TECHNICAL SKILLS Big dataHadoop Ecosystem HDFS Map Reduce Hive HBase Sqoop Flume Oozie Spark Hue Impala Kafka Spark Data Frames Spark SQL Hadoop Distribution Platforms Cloudera CDH4CDH5 Horton works Programming Languages Core Java Scala SQL Linux Application Servers Tomcat WebLogic Databases MySQL Oracle NoSQL Database HBase IDE Tools Eclipse IntelliJ Putty MobaXterm Tableau Operating System Centos Windows 7 8 10 Ubuntu UNIX",
    "entities": [
        "Implemented Spark",
        "Spark Transformations",
        "Integra Micro Systems",
        "HDFS Importing and Exporting Data",
        "Oracle MySQL",
        "HDFS",
        "UNIX",
        "Data Centers",
        "Impala Created Partition",
        "RDD Good",
        "Additional Information TECHNICAL SKILLS Big dataHadoop Ecosystem",
        "RDD",
        "Hadoop",
        "HDFS Involved",
        "JavaScript Implemented",
        "Spark SQL Data",
        "Shared",
        "HBase",
        "Amazon",
        "Hibernate Framework",
        "Hadoop Cloudera Distribution Developed",
        "WebSphere",
        "SQL Server",
        "SparkSQL",
        "Sqoop Responsible",
        "Hue",
        "MapReduce Spark Hive HBase Sqoop",
        "IDE Performed",
        "Avro XML JSON",
        "CDH 54",
        "Hadoop Developer Spark",
        "Sequence",
        "Used Spark",
        "Horton Works",
        "Spark Developer",
        "Oozie Involved",
        "JSP",
        "Application Servers",
        "HIVE Migrated ETL",
        "RDD Spark",
        "Spark Engine Configured Spark",
        "Spark Experience",
        "HDFS Uploaded",
        "UnStructured SemiStructured",
        "CLI",
        "Spark",
        "Hadoop Close",
        "File Formats",
        "HTML CSS",
        "Sqoop",
        "Hadoop Distributed File System HDFS Used Spark",
        "AWS",
        "Hadoop Architecture",
        "Oracle",
        "Developed Role Based Access Control",
        "Data Frames and Datasets Experience",
        "Hive for ETL Transformations",
        "Scala Indepth",
        "Spark Involved",
        "UNIX Experienced",
        "UNIXLinux Windows Worked",
        "Hadoop Cluster Deployed",
        "Developed Hive Scripts Hive",
        "Consumers",
        "HTML",
        "Oozie Good",
        "SQL",
        "the Data Frames",
        "Relational Database Management",
        "HDFS Highly",
        "Sqoop Good",
        "Relational Database Systems",
        "JDBC Implemented",
        "SQL Work",
        "Hadoop MapReduce HDFS HBase Oozie Hive Sqoop Zookeeper",
        "Hive",
        "SQOOP",
        "HDFS Created",
        "Amazon AWS",
        "Elastic MapReduce EMR EC2",
        "Value Object",
        "HBase Worked",
        "Business Delegate Singleton",
        "ETL",
        "JAVA",
        "Performed",
        "HUE Sqoop Zookeeper HDFS Hadoop Developer Century",
        "Impala",
        "HBase Created",
        "Putty MobaXterm Tableau Operating System Centos Windows",
        "HDFS Implement Flume Spark Stream",
        "JSP Servlets",
        "HiveQL Importing and Exporting Data",
        "Nodes",
        "Cloudera Ecosystem Configuring Troubleshooting",
        "Created HBase",
        "Data Frames Migrating",
        "Tomcat",
        "Data",
        "Structured",
        "Cloudera",
        "NoSQL",
        "Spark Architecture",
        "Spark Gained",
        "Present Responsibilities Worked on Spark"
    ],
    "experience": "Experience in converting HiveSQL queries into Spark transformations using Spark RDDs Exposure on usage of Apache Kafka to develop data pipeline of logs as a stream of messages using producers and consumers Knowledge in handling Kafka cluster and created several topologies to support realtime processing requirements Experience of converting various File Formats queries into Spark transformations using Data Frames and Datasets Experience of developing SQL scripts using Spark for handling different data sets and verifying the performance over Map Reduce jobs Experience in creating Kafka producer and Kafka consumer for Spark streaming Exposure to Spark Spark Streaming Scala and Creating the Data Frames handled in Spark with Scala Indepth understanding of Spark Architecture including Spark SQL Data Frames Spark Streaming Extensive experience on importing and exporting data using stream processing platforms like Flume and Kafka Implemented Sqoop for large dataset transfer between Hadoop and RDBMS Hands on experience in working on Spark SQL queries Data frames and import data from Data sources perform transformations perform readwrite operations save the results to output directory into HDFS Worked with AWS to migrate the entire Data Centers to the cloud using EC2 S3 and EMR Experience in importing and exporting Multi Terabytes of data using Sqoop from HDFS to Relational Database Systems RDBMS and viceversa Experience in writing shell scripts to dump the Shared data from MySQL Oracle servers to HDFS Highly experienced in importing and exporting data between HDFS and Relational Database Management systems using Sqoop Good Experience working with Amazon AWS for setting up Hadoop cluster Hand on experience in installing configuring and using Hadoop ecosystem components like Hadoop MapReduce HDFS HBase Oozie Hive Sqoop Zookeeper and Flume Worked on custom Spark Transformations and variety of data formats such as JSON Compressed CSV ORC AVRO etc and reading data from various sources like HBase and Hive Performed mapside joins on RDD Good understanding of Amazon web services like Elastic MapReduce EMR EC2 Experience in ETL operations on Hive to Spark Gained hands on experience in writing shell scripts in UNIX Experienced with processing different file formats like Avro XML JSON and Sequence file formats using Spark Experience in implementing Spark using Scala and Spark SQL for faster analyzing and processing of data Good understanding in configuring simple to complex work flows using Oozie Good understanding of NoSQL databases like MongoDB and HBase Worked on different operating systems like UNIXLinux Windows Worked in managing VMs in Cloudera and Horton Works Experience as a java Developer in clientserver technologies using JSP JDBC and SQL Work Experience Spark and Hadoop Developer Vanguard Malvern PA October 2018 to Present Responsibilities Worked on Spark streaming using Apache Kafka for real time data processing Experienced with Kafka to ingest data into Spark Engine Configured Spark streaming to get ongoing information from the Kafka and store the stream information to HDFS Involved in converting HiveSQL queries into Spark transformations using Spark RDDs Data frames Spark SQL and Scala Performed advanced procedures like text analytics and processing using the inmemory computing capabilities of Spark using Scala Developed Scala and Spark SQL code to extract data from various databases Used Spark SQL to process the huge amount of structured data and Implemented Spark RDD transformations actions to migrate Map reduce algorithms Extensively worked with all kinds of UnStructured SemiStructured and Structured data Developed multiple Kafka Producers and Consumers from as per the software requirement specifications Creatively communicated and presented models to business customers and executives utilizing a variety of formats and visualization methodologies Used Kafka extensively in gathering and moving log data files from Application Servers to a central location in Hadoop Distributed File System HDFS Used Spark and SparkSQL to read the parquet data and create the tables in hive using the Scala API Developed robust set of codes that are tested automated structured and efficient Performed mapside joins on RDD Spark SQL and Data Frames Migrating the needed data from Oracle MySQL in to HDFS using Sqoop and importing various formats of flat files in to HDFS Uploaded and processed more than 30 terabytes of data from various structured and unstructured sources into HDFS using Sqoop Implemented the data backup strategies for the data in the HDFS cluster Developed a data pipeline using Spark and Hive to ingest transform and analyzing data Developed Spark code using Scala and SparkSQL for faster testing and data processing Imported the data from relational databases into HDFS using Sqoop Understanding of data storage and retrieval techniques ETL and databases relational databases tuple stores Hadoop HUE MySQL and Oracle databases Environment Apache Spark Cloudera CDH 512 Scala Spark SQL Data Frames Kafka SBT build HUE Sqoop Zookeeper HDFS Hadoop Developer Century link Littleton CO January 2018 to October 2018 Responsibilities Involved in converting Map Reduce programs into Spark transformations using Spark RDDs on Scala Developed analytical component using Scala and Spark Involved in converting HiveSQL queries into Spark transformations using Spark RDDs and Scala Real streaming the data using Spark with Flume and store the stream data to HDFS using Scala Installing configuring troubleshooting of Hadoop with Cloudera Ecosystem Configuring Troubleshooting job tracker Nodes task tracker data nodes MapReduce Spark Hive HBase Sqoop and Oozie Involved in complete Implementation lifecycle specialized in writing custom Map Reduce Pig and Hive programs Extensively used Sqoop to get data from RDBMS sources like Teradata Worked on reading multiple data formats on HDFS using Scala Used Cloudera Manager to monitor and manage Hadoop Cluster Deployed a multinode Hadoop cluster Worked extensively with SQOOP for importing metadata from Oracle Extensively used HiveHQL or Hive queries to query or search for a string in Hive tables in HDFS Written Hive jobs to parse the logs and structure them in tabular format to facilitate effective querying on the log data Created HBase tables to store various data formats of data coming from various sources Used Impala to query the Hadoop data stored in HDFS Worked on streaming log data into HDFS from web servers using Flume Was responsible for importing the data Log files from various sources into HDFS using Flume Creating Hive tables on JSON data Creating Hive tables on JSON data Run the Hive queries in Hue and CLI Implemented POC for using APACHE IMPALA for data processing on top of HIVE Migrated ETL jobs to Spark to do Transformations even joins and some preaggregations before storing the data onto HDFS Implement Flume Spark Stream framework for real time data processing Environment UNIX HDFS Hive Spark Scala Flume Sqoop HBase Zookeeper CDH 54 Hadoop Developer ArcelorMittal Burns Harbor IN December 2015 to December 2017 Responsibilities Build and maintain scalable data pipelines using the Hadoop ecosystem and other open source components like Hive and HBase Handle the data exchange between HDFS and RDBMS using Sqoop Responsible for building scalable distributed data solutions using Hadoop Cloudera Distribution Developed several advanced Map Reduce programs to process data files received Developed Hive Scripts Hive UDFs to load data files into Hadoop Close monitoring and analysis of the Map Reduce job executions on cluster at task level Import the data from various sources like HDFSHBase into Hive Extensively worked on Hive for ETL Transformations and optimized Hive Queries Used Flume to collect aggregate and store the web log data from various sources like web servers mobile and network Devices and pushed to HDFS Created tables in Impala Created Partition tables run the hive queries in hue created tables HBase Created folders in HDFS Importing and Exporting Data from MySQLOracle to HiveQL Importing and Exporting Data from MySQLOracle to HDFS Write the Impala queries Installed Oozie workflow engine to run multiple Hive jobs Environment CDH59 HDFS Hive hue Sqoop Impala HBase Oozie Hive Flume Linux Java developer Integra Micro Systems February 2015 to November 2015 Responsibilities Involved in Analysis Design Development Integration and Testing of application modules Involved in developing a custom framework like Spring Framework with more features to meet the business needs Performed requirement analysis design coding and implementation team coordination code review testing and Installation Developed server side utilities using JAVA technologies Servlets JSP Developed presentation layers using JSP custom tags and JavaScript Implemented design patterns Business Delegate Singleton Flow Controller DAO and Value Object patterns Developed Role Based Access Control to restrict the users to access specific modules based on their roles Used Oracle as the backend application and used Hibernate Framework for ORM Deployed the application on WebSphere server using Eclipse as the IDE Used Tomcat server 55 and configured it with Eclipse IDE Performed extensive Unit Testing for the application Responsible for Design and development of Web pages using HTML CSS including Ajax controls and XML Environment WebSphere 51 Tomcat 50 Oracle 9i Hibernate30 Eclipse 32 JSP Java Script Servlets XML Eclipse Junit Spring plugins Tomcat Jr Java Developer PRIME TECHNOLOGY GROUP May 2013 to February 2015 Responsibilities Involved in the analysis design implementation and testing of the project Implemented the presentation layer with HTML and JavaScript Developed web components using JSP Servlets and JDBC Implemented database using SQL Server Designed tables and indexes Wrote complex SQL and stored procedures Involved in fixing bugs and unit testing with test cases using JUnit Developed user and technical documentation Environment Java JSP Servlets JDBC HTML JavaScript MySQL JUnit Eclipse IDE Education Bachelors Skills MYSQL 4 years ORACLE 4 years Hadoop 3 years HADOOP 3 years APACHE HADOOP HDFS 3 years Additional Information TECHNICAL SKILLS Big dataHadoop Ecosystem HDFS Map Reduce Hive HBase Sqoop Flume Oozie Spark Hue Impala Kafka Spark Data Frames Spark SQL Hadoop Distribution Platforms Cloudera CDH4CDH5 Horton works Programming Languages Core Java Scala SQL Linux Application Servers Tomcat WebLogic Databases MySQL Oracle NoSQL Database HBase IDE Tools Eclipse IntelliJ Putty MobaXterm Tableau Operating System Centos Windows 7 8 10 Ubuntu UNIX",
    "extracted_keywords": [
        "Spark",
        "Hadoop",
        "Developer",
        "Spark",
        "Hadoop",
        "span",
        "lDeveloperspan",
        "Spark",
        "Hadoop",
        "Developer",
        "Vanguard",
        "Years",
        "experience",
        "IT",
        "years",
        "experience",
        "Hadoop",
        "Spark",
        "Developer",
        "technologies",
        "depth",
        "understandingknowledge",
        "Hadoop",
        "Architecture",
        "components",
        "HDFS",
        "Name",
        "Node",
        "Data",
        "Node",
        "Map",
        "Spark",
        "Spark",
        "SQL",
        "Experience",
        "HiveSQL",
        "queries",
        "Spark",
        "transformations",
        "Spark",
        "RDDs",
        "Exposure",
        "usage",
        "Apache",
        "Kafka",
        "data",
        "pipeline",
        "logs",
        "stream",
        "messages",
        "producers",
        "consumers",
        "Knowledge",
        "Kafka",
        "cluster",
        "topologies",
        "processing",
        "requirements",
        "Experience",
        "File",
        "Formats",
        "Spark",
        "transformations",
        "Data",
        "Frames",
        "Datasets",
        "Experience",
        "SQL",
        "scripts",
        "Spark",
        "data",
        "sets",
        "performance",
        "Map",
        "Reduce",
        "jobs",
        "Experience",
        "Kafka",
        "producer",
        "Kafka",
        "consumer",
        "Spark",
        "Exposure",
        "Spark",
        "Spark",
        "Streaming",
        "Scala",
        "Data",
        "Frames",
        "Spark",
        "Scala",
        "Indepth",
        "understanding",
        "Spark",
        "Architecture",
        "Spark",
        "SQL",
        "Data",
        "Frames",
        "Spark",
        "Streaming",
        "experience",
        "data",
        "stream",
        "processing",
        "platforms",
        "Flume",
        "Kafka",
        "Sqoop",
        "transfer",
        "Hadoop",
        "Hands",
        "experience",
        "Spark",
        "SQL",
        "Data",
        "frames",
        "import",
        "data",
        "Data",
        "sources",
        "transformations",
        "operations",
        "results",
        "directory",
        "HDFS",
        "AWS",
        "Data",
        "Centers",
        "cloud",
        "EC2",
        "S3",
        "EMR",
        "Experience",
        "Multi",
        "Terabytes",
        "data",
        "Sqoop",
        "HDFS",
        "Relational",
        "Database",
        "Systems",
        "RDBMS",
        "viceversa",
        "Experience",
        "shell",
        "scripts",
        "data",
        "MySQL",
        "Oracle",
        "servers",
        "data",
        "HDFS",
        "Relational",
        "Database",
        "Management",
        "systems",
        "Sqoop",
        "Good",
        "Experience",
        "Amazon",
        "AWS",
        "Hadoop",
        "cluster",
        "Hand",
        "experience",
        "configuring",
        "Hadoop",
        "ecosystem",
        "components",
        "Hadoop",
        "MapReduce",
        "HDFS",
        "HBase",
        "Oozie",
        "Hive",
        "Sqoop",
        "Zookeeper",
        "Flume",
        "custom",
        "Spark",
        "Transformations",
        "variety",
        "data",
        "formats",
        "JSON",
        "Compressed",
        "CSV",
        "ORC",
        "AVRO",
        "data",
        "sources",
        "HBase",
        "Hive",
        "Performed",
        "mapside",
        "RDD",
        "understanding",
        "Amazon",
        "web",
        "services",
        "MapReduce",
        "EMR",
        "EC2",
        "Experience",
        "ETL",
        "operations",
        "Hive",
        "Spark",
        "hands",
        "experience",
        "scripts",
        "UNIX",
        "file",
        "formats",
        "Avro",
        "XML",
        "JSON",
        "Sequence",
        "file",
        "formats",
        "Spark",
        "Experience",
        "Spark",
        "Scala",
        "Spark",
        "SQL",
        "processing",
        "data",
        "understanding",
        "work",
        "flows",
        "Oozie",
        "understanding",
        "MongoDB",
        "HBase",
        "operating",
        "systems",
        "UNIXLinux",
        "Windows",
        "VMs",
        "Cloudera",
        "Horton",
        "Experience",
        "Developer",
        "clientserver",
        "technologies",
        "JSP",
        "JDBC",
        "SQL",
        "Work",
        "Experience",
        "Spark",
        "Hadoop",
        "Developer",
        "Vanguard",
        "Malvern",
        "PA",
        "October",
        "Present",
        "Responsibilities",
        "Spark",
        "streaming",
        "Apache",
        "Kafka",
        "time",
        "data",
        "processing",
        "Kafka",
        "data",
        "Spark",
        "Engine",
        "Configured",
        "Spark",
        "streaming",
        "information",
        "Kafka",
        "stream",
        "information",
        "HDFS",
        "HiveSQL",
        "queries",
        "Spark",
        "transformations",
        "Spark",
        "RDDs",
        "Data",
        "Spark",
        "SQL",
        "Scala",
        "Performed",
        "procedures",
        "text",
        "analytics",
        "processing",
        "inmemory",
        "computing",
        "capabilities",
        "Spark",
        "Scala",
        "Developed",
        "Scala",
        "Spark",
        "SQL",
        "code",
        "data",
        "databases",
        "Spark",
        "SQL",
        "amount",
        "data",
        "Spark",
        "RDD",
        "transformations",
        "actions",
        "Map",
        "algorithms",
        "kinds",
        "UnStructured",
        "SemiStructured",
        "Structured",
        "data",
        "Kafka",
        "Producers",
        "Consumers",
        "software",
        "requirement",
        "specifications",
        "models",
        "business",
        "customers",
        "executives",
        "variety",
        "formats",
        "visualization",
        "methodologies",
        "Kafka",
        "log",
        "data",
        "files",
        "Application",
        "Servers",
        "location",
        "Hadoop",
        "Distributed",
        "File",
        "System",
        "HDFS",
        "Spark",
        "SparkSQL",
        "parquet",
        "data",
        "tables",
        "hive",
        "Scala",
        "API",
        "set",
        "codes",
        "Performed",
        "mapside",
        "RDD",
        "Spark",
        "SQL",
        "Data",
        "Frames",
        "Migrating",
        "data",
        "Oracle",
        "MySQL",
        "HDFS",
        "Sqoop",
        "formats",
        "files",
        "HDFS",
        "terabytes",
        "data",
        "sources",
        "HDFS",
        "Sqoop",
        "data",
        "backup",
        "strategies",
        "data",
        "HDFS",
        "cluster",
        "data",
        "pipeline",
        "Spark",
        "Hive",
        "transform",
        "data",
        "Developed",
        "Spark",
        "code",
        "Scala",
        "SparkSQL",
        "testing",
        "data",
        "processing",
        "data",
        "databases",
        "HDFS",
        "Sqoop",
        "Understanding",
        "data",
        "storage",
        "retrieval",
        "techniques",
        "ETL",
        "databases",
        "tuple",
        "stores",
        "Hadoop",
        "HUE",
        "MySQL",
        "Oracle",
        "Environment",
        "Apache",
        "Spark",
        "Cloudera",
        "CDH",
        "Scala",
        "Spark",
        "SQL",
        "Data",
        "Frames",
        "Kafka",
        "SBT",
        "HUE",
        "Sqoop",
        "Zookeeper",
        "HDFS",
        "Hadoop",
        "Developer",
        "Century",
        "link",
        "Littleton",
        "CO",
        "January",
        "October",
        "Responsibilities",
        "Map",
        "Reduce",
        "programs",
        "Spark",
        "transformations",
        "Spark",
        "RDDs",
        "Scala",
        "component",
        "Scala",
        "Spark",
        "HiveSQL",
        "queries",
        "Spark",
        "transformations",
        "Spark",
        "RDDs",
        "Scala",
        "Real",
        "data",
        "Spark",
        "Flume",
        "stream",
        "data",
        "HDFS",
        "Scala",
        "configuring",
        "troubleshooting",
        "Hadoop",
        "Cloudera",
        "Ecosystem",
        "Configuring",
        "Troubleshooting",
        "job",
        "tracker",
        "Nodes",
        "task",
        "tracker",
        "data",
        "MapReduce",
        "Spark",
        "Hive",
        "HBase",
        "Sqoop",
        "Oozie",
        "Implementation",
        "lifecycle",
        "custom",
        "Map",
        "Reduce",
        "Pig",
        "Hive",
        "programs",
        "Sqoop",
        "data",
        "sources",
        "Teradata",
        "Worked",
        "data",
        "formats",
        "HDFS",
        "Scala",
        "Cloudera",
        "Manager",
        "Hadoop",
        "Cluster",
        "multinode",
        "Hadoop",
        "cluster",
        "SQOOP",
        "metadata",
        "Oracle",
        "HiveHQL",
        "Hive",
        "queries",
        "query",
        "string",
        "Hive",
        "tables",
        "HDFS",
        "Written",
        "Hive",
        "jobs",
        "logs",
        "format",
        "querying",
        "log",
        "data",
        "Created",
        "HBase",
        "data",
        "formats",
        "data",
        "sources",
        "Impala",
        "Hadoop",
        "data",
        "HDFS",
        "log",
        "data",
        "HDFS",
        "web",
        "servers",
        "Flume",
        "data",
        "Log",
        "files",
        "sources",
        "HDFS",
        "Flume",
        "Creating",
        "Hive",
        "tables",
        "data",
        "Hive",
        "tables",
        "data",
        "Hive",
        "queries",
        "Hue",
        "CLI",
        "Implemented",
        "POC",
        "IMPALA",
        "data",
        "processing",
        "top",
        "HIVE",
        "ETL",
        "jobs",
        "Spark",
        "Transformations",
        "preaggregations",
        "data",
        "HDFS",
        "Implement",
        "Flume",
        "Spark",
        "Stream",
        "framework",
        "time",
        "data",
        "Environment",
        "UNIX",
        "HDFS",
        "Hive",
        "Spark",
        "Scala",
        "Flume",
        "Sqoop",
        "HBase",
        "Zookeeper",
        "CDH",
        "Hadoop",
        "Developer",
        "ArcelorMittal",
        "Burns",
        "Harbor",
        "December",
        "December",
        "Responsibilities",
        "data",
        "pipelines",
        "Hadoop",
        "ecosystem",
        "source",
        "components",
        "Hive",
        "HBase",
        "data",
        "exchange",
        "HDFS",
        "RDBMS",
        "Sqoop",
        "Responsible",
        "data",
        "solutions",
        "Hadoop",
        "Cloudera",
        "Distribution",
        "Map",
        "Reduce",
        "programs",
        "data",
        "files",
        "Developed",
        "Hive",
        "Scripts",
        "Hive",
        "UDFs",
        "data",
        "files",
        "Hadoop",
        "monitoring",
        "analysis",
        "Map",
        "Reduce",
        "job",
        "executions",
        "cluster",
        "task",
        "level",
        "Import",
        "data",
        "sources",
        "HDFSHBase",
        "Hive",
        "Extensively",
        "Hive",
        "ETL",
        "Transformations",
        "Hive",
        "Queries",
        "Flume",
        "aggregate",
        "web",
        "log",
        "data",
        "sources",
        "web",
        "servers",
        "mobile",
        "network",
        "Devices",
        "tables",
        "Impala",
        "Created",
        "Partition",
        "tables",
        "hive",
        "queries",
        "hue",
        "tables",
        "HBase",
        "folders",
        "HDFS",
        "Importing",
        "Exporting",
        "Data",
        "MySQLOracle",
        "HiveQL",
        "Importing",
        "Exporting",
        "Data",
        "MySQLOracle",
        "HDFS",
        "Impala",
        "Installed",
        "Oozie",
        "workflow",
        "engine",
        "Hive",
        "jobs",
        "Environment",
        "CDH59",
        "HDFS",
        "Hive",
        "hue",
        "Sqoop",
        "Impala",
        "HBase",
        "Oozie",
        "Hive",
        "Flume",
        "Linux",
        "Java",
        "developer",
        "Integra",
        "Micro",
        "Systems",
        "February",
        "November",
        "Responsibilities",
        "Analysis",
        "Design",
        "Development",
        "Integration",
        "Testing",
        "application",
        "modules",
        "custom",
        "framework",
        "Spring",
        "Framework",
        "features",
        "business",
        "requirement",
        "analysis",
        "design",
        "coding",
        "implementation",
        "team",
        "coordination",
        "code",
        "review",
        "testing",
        "Installation",
        "server",
        "side",
        "utilities",
        "JAVA",
        "technologies",
        "Servlets",
        "JSP",
        "presentation",
        "layers",
        "JSP",
        "custom",
        "tags",
        "JavaScript",
        "design",
        "patterns",
        "Business",
        "Delegate",
        "Singleton",
        "Flow",
        "Controller",
        "DAO",
        "Value",
        "Object",
        "patterns",
        "Role",
        "Based",
        "Access",
        "Control",
        "users",
        "modules",
        "roles",
        "Oracle",
        "application",
        "Hibernate",
        "Framework",
        "ORM",
        "application",
        "WebSphere",
        "server",
        "Eclipse",
        "IDE",
        "Used",
        "Tomcat",
        "server",
        "Eclipse",
        "IDE",
        "Unit",
        "Testing",
        "application",
        "Design",
        "development",
        "Web",
        "pages",
        "HTML",
        "CSS",
        "controls",
        "XML",
        "Environment",
        "WebSphere",
        "Tomcat",
        "Oracle",
        "9i",
        "Hibernate30",
        "Eclipse",
        "JSP",
        "Java",
        "Script",
        "Servlets",
        "XML",
        "Eclipse",
        "Junit",
        "Spring",
        "Tomcat",
        "Jr",
        "Java",
        "Developer",
        "PRIME",
        "TECHNOLOGY",
        "GROUP",
        "May",
        "February",
        "Responsibilities",
        "analysis",
        "design",
        "implementation",
        "testing",
        "project",
        "presentation",
        "layer",
        "HTML",
        "JavaScript",
        "Developed",
        "web",
        "components",
        "JSP",
        "Servlets",
        "JDBC",
        "database",
        "SQL",
        "Server",
        "tables",
        "indexes",
        "Wrote",
        "SQL",
        "procedures",
        "bugs",
        "unit",
        "testing",
        "test",
        "cases",
        "JUnit",
        "Developed",
        "user",
        "documentation",
        "Environment",
        "Java",
        "JSP",
        "Servlets",
        "JDBC",
        "HTML",
        "JavaScript",
        "MySQL",
        "JUnit",
        "Eclipse",
        "IDE",
        "Education",
        "Bachelors",
        "Skills",
        "MYSQL",
        "years",
        "ORACLE",
        "years",
        "Hadoop",
        "years",
        "HADOOP",
        "years",
        "APACHE",
        "HADOOP",
        "HDFS",
        "years",
        "Information",
        "TECHNICAL",
        "SKILLS",
        "Big",
        "dataHadoop",
        "Ecosystem",
        "HDFS",
        "Map",
        "Reduce",
        "Hive",
        "HBase",
        "Sqoop",
        "Flume",
        "Oozie",
        "Spark",
        "Hue",
        "Impala",
        "Kafka",
        "Spark",
        "Data",
        "Frames",
        "Spark",
        "SQL",
        "Hadoop",
        "Distribution",
        "Platforms",
        "Cloudera",
        "CDH4CDH5",
        "Horton",
        "Programming",
        "Languages",
        "Core",
        "Java",
        "Scala",
        "SQL",
        "Linux",
        "Application",
        "Servers",
        "Tomcat",
        "WebLogic",
        "MySQL",
        "Oracle",
        "NoSQL",
        "Database",
        "HBase",
        "IDE",
        "Tools",
        "Eclipse",
        "IntelliJ",
        "Putty",
        "MobaXterm",
        "Tableau",
        "Operating",
        "System",
        "Centos",
        "Windows",
        "Ubuntu",
        "UNIX"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T22:38:05.860355",
    "resume_data": "Spark and Hadoop Developer Spark and Hadoop span lDeveloperspan Spark and Hadoop Developer Vanguard 6 Years of professional experience in IT which includes around 4 years of comprehensive experience as Hadoop and Spark Developer and related technologies In depth understandingknowledge of Hadoop Architecture and various components such as HDFS Name Node Data Node Map Reduce Spark and Spark SQL Experience in converting HiveSQL queries into Spark transformations using Spark RDDs Exposure on usage of Apache Kafka to develop data pipeline of logs as a stream of messages using producers and consumers Knowledge in handling Kafka cluster and created several topologies to support realtime processing requirements Experience of converting various File Formats queries into Spark transformations using Data Frames and Datasets Experience of developing SQL scripts using Spark for handling different data sets and verifying the performance over Map Reduce jobs Experience in creating Kafka producer and Kafka consumer for Spark streaming Exposure to Spark Spark Streaming Scala and Creating the Data Frames handled in Spark with Scala Indepth understanding of Spark Architecture including Spark SQL Data Frames Spark Streaming Extensive experience on importing and exporting data using stream processing platforms like Flume and Kafka Implemented Sqoop for large dataset transfer between Hadoop and RDBMS Hands on experience in working on Spark SQL queries Data frames and import data from Data sources perform transformations perform readwrite operations save the results to output directory into HDFS Worked with AWS to migrate the entire Data Centers to the cloud using EC2 S3 and EMR Experience in importing and exporting Multi Terabytes of data using Sqoop from HDFS to Relational Database Systems RDBMS and viceversa Experience in writing shell scripts to dump the Shared data from MySQL Oracle servers to HDFS Highly experienced in importing and exporting data between HDFS and Relational Database Management systems using Sqoop Good Experience working with Amazon AWS for setting up Hadoop cluster Hand on experience in installing configuring and using Hadoop ecosystem components like Hadoop MapReduce HDFS HBase Oozie Hive Sqoop Zookeeper and Flume Worked on custom Spark Transformations and variety of data formats such as JSON Compressed CSV ORC AVRO etc and reading data from various sources like HBase and Hive Performed mapside joins on RDD Good understanding of Amazon web services like Elastic MapReduce EMR EC2 Experience in ETL operations on Hive to Spark Gained hands on experience in writing shell scripts in UNIX Experienced with processing different file formats like Avro XML JSON and Sequence file formats using Spark Experience in implementing Spark using Scala and Spark SQL for faster analyzing and processing of data Good understanding in configuring simple to complex work flows using Oozie Good understanding of NoSQL databases like MongoDB and HBase Worked on different operating systems like UNIXLinux Windows Worked in managing VMs in Cloudera and Horton Works Experience as a java Developer in clientserver technologies using JSP JDBC and SQL Work Experience Spark and Hadoop Developer Vanguard Malvern PA October 2018 to Present Responsibilities Worked on Spark streaming using Apache Kafka for real time data processing Experienced with Kafka to ingest data into Spark Engine Configured Spark streaming to get ongoing information from the Kafka and store the stream information to HDFS Involved in converting HiveSQL queries into Spark transformations using Spark RDDs Data frames Spark SQL and Scala Performed advanced procedures like text analytics and processing using the inmemory computing capabilities of Spark using Scala Developed Scala and Spark SQL code to extract data from various databases Used Spark SQL to process the huge amount of structured data and Implemented Spark RDD transformations actions to migrate Map reduce algorithms Extensively worked with all kinds of UnStructured SemiStructured and Structured data Developed multiple Kafka Producers and Consumers from as per the software requirement specifications Creatively communicated and presented models to business customers and executives utilizing a variety of formats and visualization methodologies Used Kafka extensively in gathering and moving log data files from Application Servers to a central location in Hadoop Distributed File System HDFS Used Spark and SparkSQL to read the parquet data and create the tables in hive using the Scala API Developed robust set of codes that are tested automated structured and efficient Performed mapside joins on RDD Spark SQL and Data Frames Migrating the needed data from Oracle MySQL in to HDFS using Sqoop and importing various formats of flat files in to HDFS Uploaded and processed more than 30 terabytes of data from various structured and unstructured sources into HDFS using Sqoop Implemented the data backup strategies for the data in the HDFS cluster Developed a data pipeline using Spark and Hive to ingest transform and analyzing data Developed Spark code using Scala and SparkSQL for faster testing and data processing Imported the data from relational databases into HDFS using Sqoop Understanding of data storage and retrieval techniques ETL and databases relational databases tuple stores Hadoop HUE MySQL and Oracle databases Environment Apache Spark Cloudera CDH 512 Scala Spark SQL Data Frames Kafka SBT build HUE Sqoop Zookeeper HDFS Hadoop Developer Century link Littleton CO January 2018 to October 2018 Responsibilities Involved in converting Map Reduce programs into Spark transformations using Spark RDDs on Scala Developed analytical component using Scala and Spark Involved in converting HiveSQL queries into Spark transformations using Spark RDDs and Scala Real streaming the data using Spark with Flume and store the stream data to HDFS using Scala Installing configuring troubleshooting of Hadoop with Cloudera Ecosystem Configuring Troubleshooting job tracker Nodes task tracker data nodes MapReduce Spark Hive HBase Sqoop and Oozie Involved in complete Implementation lifecycle specialized in writing custom Map Reduce Pig and Hive programs Extensively used Sqoop to get data from RDBMS sources like Teradata Worked on reading multiple data formats on HDFS using Scala Used Cloudera Manager to monitor and manage Hadoop Cluster Deployed a multinode Hadoop cluster Worked extensively with SQOOP for importing metadata from Oracle Extensively used HiveHQL or Hive queries to query or search for a string in Hive tables in HDFS Written Hive jobs to parse the logs and structure them in tabular format to facilitate effective querying on the log data Created HBase tables to store various data formats of data coming from various sources Used Impala to query the Hadoop data stored in HDFS Worked on streaming log data into HDFS from web servers using Flume Was responsible for importing the data Log files from various sources into HDFS using Flume Creating Hive tables on JSON data Creating Hive tables on JSON data Run the Hive queries in Hue and CLI Implemented POC for using APACHE IMPALA for data processing on top of HIVE Migrated ETL jobs to Spark to do Transformations even joins and some preaggregations before storing the data onto HDFS Implement Flume Spark Stream framework for real time data processing Environment UNIX HDFS Hive Spark Scala Flume Sqoop HBase Zookeeper CDH 54 Hadoop Developer ArcelorMittal Burns Harbor IN December 2015 to December 2017 Responsibilities Build and maintain scalable data pipelines using the Hadoop ecosystem and other open source components like Hive and HBase Handle the data exchange between HDFS and RDBMS using Sqoop Responsible for building scalable distributed data solutions using Hadoop Cloudera Distribution Developed several advanced Map Reduce programs to process data files received Developed Hive Scripts Hive UDFs to load data files into Hadoop Close monitoring and analysis of the Map Reduce job executions on cluster at task level Import the data from various sources like HDFSHBase into Hive Extensively worked on Hive for ETL Transformations and optimized Hive Queries Used Flume to collect aggregate and store the web log data from various sources like web servers mobile and network Devices and pushed to HDFS Created tables in Impala Created Partition tables run the hive queries in hue created tables HBase Created folders in HDFS Importing and Exporting Data from MySQLOracle to HiveQL Importing and Exporting Data from MySQLOracle to HDFS Write the Impala queries Installed Oozie workflow engine to run multiple Hive jobs Environment CDH59 HDFS Hive hue Sqoop Impala HBase Oozie Hive Flume Linux Java developer Integra Micro Systems February 2015 to November 2015 Responsibilities Involved in Analysis Design Development Integration and Testing of application modules Involved in developing a custom framework like Spring Framework with more features to meet the business needs Performed requirement analysis design coding and implementation team coordination code review testing and Installation Developed server side utilities using JAVA technologies Servlets JSP Developed presentation layers using JSP custom tags and JavaScript Implemented design patterns Business Delegate Singleton Flow Controller DAO and Value Object patterns Developed Role Based Access Control to restrict the users to access specific modules based on their roles Used Oracle as the backend application and used Hibernate Framework for ORM Deployed the application on WebSphere server using Eclipse as the IDE Used Tomcat server 55 and configured it with Eclipse IDE Performed extensive Unit Testing for the application Responsible for Design and development of Web pages using HTML CSS including Ajax controls and XML Environment WebSphere 51 Tomcat 50 Oracle 9i Hibernate30 Eclipse 32 JSP Java Script Servlets XML Eclipse Junit Spring plugins Tomcat Jr Java Developer PRIME TECHNOLOGY GROUP May 2013 to February 2015 Responsibilities Involved in the analysis design implementation and testing of the project Implemented the presentation layer with HTML and JavaScript Developed web components using JSP Servlets and JDBC Implemented database using SQL Server Designed tables and indexes Wrote complex SQL and stored procedures Involved in fixing bugs and unit testing with test cases using JUnit Developed user and technical documentation Environment Java JSP Servlets JDBC HTML JavaScript MySQL JUnit Eclipse IDE Education Bachelors Skills MYSQL 4 years ORACLE 4 years Hadoop 3 years HADOOP 3 years APACHE HADOOP HDFS 3 years Additional Information TECHNICAL SKILLS Big dataHadoop Ecosystem HDFS Map Reduce Hive HBase Sqoop Flume Oozie Spark Hue Impala Kafka Spark Data Frames Spark SQL Hadoop Distribution Platforms Cloudera CDH4CDH5 Horton works Programming Languages Core Java Scala SQL Linux Application Servers Tomcat WebLogic Databases MySQL Oracle NoSQL Database HBase IDE Tools Eclipse IntelliJ Putty MobaXterm Tableau Operating System Centos Windows 7 8 10 Ubuntu UNIX",
    "unique_id": "50e648a3-8d9e-4e52-91b3-b39f2674750c"
}