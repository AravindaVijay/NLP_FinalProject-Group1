{
    "clean_data": "Data Engineer Data Engineer Data Engineer Nike Beaverton OR Over 5 years of experience in IT industry and have been active in Big Data technologies including Hadoop and Spark Driven by curiosity innovative thinking and pleasure of learning developed apt solutions on performance by leveraging emerging technologies and methods Strong believer in collaboration teamwork integrity and interest of the client Work Experience Data Engineer Nike Beaverton OR August 2018 to Present This project is an assortment product which brings the analytical solutions to users based on their request data comes from different sources like DB2 Oracle and Teradata etc This data will be imported and will undergo several cleansings and valueadded processing and then finally views will be created on this data And this data is consumed by data science team to forecast the demand and supply Responsibilities Ingested the data from various data sources like Teradata into AWS S3 and Snowflake using spark API Developed ETL frameworks for data using PySpark Performed finetuning of spark applicationsjobs to improve the efficiency and overall processing time for the pipelines Used broadcast variables in Spark effective efficient Joins transformations and other capabilities for data processing Used SparkSQL to perform event enrichment and to prepare various levels of user behavioral summaries Worked with EMR S3 Athena services in AWS cloud Implemented the workflows using Airflow scheduler to automate tasks Created Athena query service for adhoc queries analysis and data discovery Experienced working with Spark Core and Spark SQL using Python Interacted with the infrastructure network database application and BA teams to ensure data quality and availability Environment AWS EMR Athena Spark Python S3 Airflow Snowflake ETL Teradata Hive PySpark TDCH Hadoop Developer Apttus San Mateo CA January 2018 to July 2018 Retail Enterprise Credit Risk application calculates Banks retail data such as credit cards auto student and home loans for risk domains including Enterprise Capital Management Data comes from different system of records such mainly from Teradata This data will undergo several cleansing and processing and then finally views will be created on this data in Hadoop warehouse This data will be consumed by BI tools for analyzing and generating reports Responsibilities Developed SQOOP scripts for importing and exporting data into HDFS and Hive Developing design documents considering all possible approaches and identifying best of them Responsible to manage data coming from different sources Responsible for loading data from UNIX file systems to HDFS Installed and configured Hive and written Hive UDFs Involved in creating Hive Tables loading with data and writing Hive queries Used Bucketing and Dynamic Partitioning on Hive tables Import the data from different sources like HDFSHive into Spark RDD Developed Spark SQL scripts using Scala to perform transformations and actions on RDDs in spark for faster data Processing Experienced with in working with Spark Core and Spark SQL using Scala Performed data transformations and analytics on large dataset using Spark Implemented the workflows using Apache Oozie framework to automate tasks Imported results into visualization BI tool Tableau to create dashboards Environment Hadoop Hive Spark Oozie Teradata Yarn Tableau Unix Hortonworks Sqoop Scala Software Engineer Accenture Solutions Pvt Ltd Hyderabad Telangana July 2014 to July 2017 ExpressScripts being the leading company in American Pharmacy Benefit Management company with many claims daily and there is every possibility that some of them are fraudulent claims The goal of this project is to identify possible fraudulent claims out of the total claims processed daily Responsibilities Data analysis and generated reports which helped to improve product quality and decision making Involved in development of Hadoop System and improving multinode Hadoop Cluster performance Experience in importing the data from relational databases such as MySQL to HDFS and exporting the data from HDFS to relational databases using SQOOP Involved in Big Data Frameworks and tools such as Hadoop Spark Hive Experience in troubleshooting the issues and failed jobs in the Hadoop cluster Implemented the workflows using Apache Oozie framework to automate the tasks Communicating with clients to gather the requirements SQL querying and performance tuning creating backup tables Developed Web services component using XML WSDL and SOAP with DOM parser to transfer and transform data between applications Exposed various capabilities as Web Services using SOAPWSDL Used SOAP UI for testing the Restful Web services by sending and SOAP request Used AJAX framework for server communication and seamless user experience Mentoring and Training the new recruits Environment Hadoop Hive SQOOP Spark Oozie Cloudera Manager Tableau Education Master of Science in Information Technology Arkansas Tech University Russellville AR Skills Cassandra Hdfs Mapreduce Oozie Sqoop Hbase Cdh Db2 Etl Hadoop Nosql Power bi Teradata Amazon web services Apache spark Api Git Hadoop Hbase Hive Additional Information CORE COMPETENCIES IT experience in Software Development Life Cycle Analysis Design Development Testing Deployment and Support using WATERFALL and AGILE methodologies Experience in Big Data Technologies using Hadoop Eco System components Spark HDFS MapReduce Sqoop Hive in Retail Healthcare sector and Financial sectors Experienced in working with Hadoop distributions predominantly Amazon EMR Cloudera CDH and knowledge on Hortonworks HDP Experienced in working with cloud services such as EMR S3 EC2 Athena Experience in ETL jobs and developing and managing data pipelines Working knowledge in creating ETL jobs to load huge volumes of data into Hadoop Ecosystem and relational databases Experience in running Hive scripts Unix and Linux shell scripting Created User Defined Functions UDFs in Hive Flexible with full implementation of spark jobs with PySpark API and Spark Scala API Designed Hive queries to perform data analysis data transfer and table design to load into Hadoop environment Proficient in importing exporting data from RDBMS to HDFS using Sqoop and TDCH Experience in using Airflow Oozie schedulers and Unix scripting to implement cron jobs that execute different Hadoop actions Experience in using SQL server MYSQL and Teradata Hands on experience with ORC and Parquet File formats Proficient in working with Jira GIT and Jenkins Knowledge with NoSQL Databases HBase and Cassandra Good analytical communication problem solving skills and adore learning new technical and functional skills Strong believer in collaboration teamwork integrity and interest of the client AREAS OF EXPERTISE Big Data Ecosystem HDFS MapReduce Hive YARN Apache Spark Airflow Oozie Zookeeper HUE Sqoop TDCH NoSQL Databases HBase Cassandra Hadoop Distributions AWS Amazon Web Services Cloudera Hortonworks Programming Languages Python Scala HiveQL Scripting Languages Shell Scripting Java Scripting Databases Teradata Snowflake MySQL DB2 IDE Eclipse PyCharm IntelliJ BI Tools Tableau Power BI Version control tools Git GitHub Bitbucket",
    "entities": [
        "Spark HDFS MapReduce Sqoop Hive",
        "Scala Performed",
        "Responsibilities Data",
        "Communicating",
        "ExpressScripts",
        "PySpark API",
        "Responsibilities Developed",
        "WATERFALL",
        "AJAX",
        "SparkSQL",
        "ETL",
        "Created Athena",
        "Sqoop",
        "Python S3 Airflow Snowflake",
        "Spark Core",
        "AWS S3 and Snowflake",
        "HDFS Installed",
        "BI",
        "Hadoop Spark Hive Experience",
        "Parquet File",
        "UNIX",
        "AWS",
        "CA",
        "BA",
        "Api Git Hadoop Hbase Hive Additional Information CORE",
        "Spark RDD Developed Spark",
        "Working",
        "Data Engineer Data Engineer Data",
        "OF EXPERTISE Big Data Ecosystem",
        "PyCharm",
        "Mentoring and Training",
        "Hadoop Ecosystem",
        "AGILE",
        "Amazon",
        "API Developed",
        "Enterprise Capital Management Data",
        "Big Data Technologies",
        "Processing Experienced",
        "Spark Driven",
        "Airflow",
        "SQL",
        "Hadoop",
        "Airflow Oozie",
        "SQOOP Involved",
        "SOAP",
        "American Pharmacy Benefit Management",
        "Restful Web",
        "Arkansas Tech University",
        "Created User Defined Functions",
        "Software Development Life Cycle Analysis Design Development Testing Deployment and Support",
        "Retail Healthcare",
        "Hadoop Cluster",
        "Tableau",
        "Hortonworks Sqoop",
        "DOM",
        "ORC",
        "Hadoop System",
        "PySpark Performed",
        "Spark Implemented",
        "BI Tools Tableau Power BI Version",
        "Teradata",
        "Hortonworks HDP Experienced",
        "Athena",
        "Big Data",
        "Hive",
        "Spark"
    ],
    "experience": "Experience Data Engineer Nike Beaverton OR August 2018 to Present This project is an assortment product which brings the analytical solutions to users based on their request data comes from different sources like DB2 Oracle and Teradata etc This data will be imported and will undergo several cleansings and valueadded processing and then finally views will be created on this data And this data is consumed by data science team to forecast the demand and supply Responsibilities Ingested the data from various data sources like Teradata into AWS S3 and Snowflake using spark API Developed ETL frameworks for data using PySpark Performed finetuning of spark applicationsjobs to improve the efficiency and overall processing time for the pipelines Used broadcast variables in Spark effective efficient Joins transformations and other capabilities for data processing Used SparkSQL to perform event enrichment and to prepare various levels of user behavioral summaries Worked with EMR S3 Athena services in AWS cloud Implemented the workflows using Airflow scheduler to automate tasks Created Athena query service for adhoc queries analysis and data discovery Experienced working with Spark Core and Spark SQL using Python Interacted with the infrastructure network database application and BA teams to ensure data quality and availability Environment AWS EMR Athena Spark Python S3 Airflow Snowflake ETL Teradata Hive PySpark TDCH Hadoop Developer Apttus San Mateo CA January 2018 to July 2018 Retail Enterprise Credit Risk application calculates Banks retail data such as credit cards auto student and home loans for risk domains including Enterprise Capital Management Data comes from different system of records such mainly from Teradata This data will undergo several cleansing and processing and then finally views will be created on this data in Hadoop warehouse This data will be consumed by BI tools for analyzing and generating reports Responsibilities Developed SQOOP scripts for importing and exporting data into HDFS and Hive Developing design documents considering all possible approaches and identifying best of them Responsible to manage data coming from different sources Responsible for loading data from UNIX file systems to HDFS Installed and configured Hive and written Hive UDFs Involved in creating Hive Tables loading with data and writing Hive queries Used Bucketing and Dynamic Partitioning on Hive tables Import the data from different sources like HDFSHive into Spark RDD Developed Spark SQL scripts using Scala to perform transformations and actions on RDDs in spark for faster data Processing Experienced with in working with Spark Core and Spark SQL using Scala Performed data transformations and analytics on large dataset using Spark Implemented the workflows using Apache Oozie framework to automate tasks Imported results into visualization BI tool Tableau to create dashboards Environment Hadoop Hive Spark Oozie Teradata Yarn Tableau Unix Hortonworks Sqoop Scala Software Engineer Accenture Solutions Pvt Ltd Hyderabad Telangana July 2014 to July 2017 ExpressScripts being the leading company in American Pharmacy Benefit Management company with many claims daily and there is every possibility that some of them are fraudulent claims The goal of this project is to identify possible fraudulent claims out of the total claims processed daily Responsibilities Data analysis and generated reports which helped to improve product quality and decision making Involved in development of Hadoop System and improving multinode Hadoop Cluster performance Experience in importing the data from relational databases such as MySQL to HDFS and exporting the data from HDFS to relational databases using SQOOP Involved in Big Data Frameworks and tools such as Hadoop Spark Hive Experience in troubleshooting the issues and failed jobs in the Hadoop cluster Implemented the workflows using Apache Oozie framework to automate the tasks Communicating with clients to gather the requirements SQL querying and performance tuning creating backup tables Developed Web services component using XML WSDL and SOAP with DOM parser to transfer and transform data between applications Exposed various capabilities as Web Services using SOAPWSDL Used SOAP UI for testing the Restful Web services by sending and SOAP request Used AJAX framework for server communication and seamless user experience Mentoring and Training the new recruits Environment Hadoop Hive SQOOP Spark Oozie Cloudera Manager Tableau Education Master of Science in Information Technology Arkansas Tech University Russellville AR Skills Cassandra Hdfs Mapreduce Oozie Sqoop Hbase Cdh Db2 Etl Hadoop Nosql Power bi Teradata Amazon web services Apache spark Api Git Hadoop Hbase Hive Additional Information CORE COMPETENCIES IT experience in Software Development Life Cycle Analysis Design Development Testing Deployment and Support using WATERFALL and AGILE methodologies Experience in Big Data Technologies using Hadoop Eco System components Spark HDFS MapReduce Sqoop Hive in Retail Healthcare sector and Financial sectors Experienced in working with Hadoop distributions predominantly Amazon EMR Cloudera CDH and knowledge on Hortonworks HDP Experienced in working with cloud services such as EMR S3 EC2 Athena Experience in ETL jobs and developing and managing data pipelines Working knowledge in creating ETL jobs to load huge volumes of data into Hadoop Ecosystem and relational databases Experience in running Hive scripts Unix and Linux shell scripting Created User Defined Functions UDFs in Hive Flexible with full implementation of spark jobs with PySpark API and Spark Scala API Designed Hive queries to perform data analysis data transfer and table design to load into Hadoop environment Proficient in importing exporting data from RDBMS to HDFS using Sqoop and TDCH Experience in using Airflow Oozie schedulers and Unix scripting to implement cron jobs that execute different Hadoop actions Experience in using SQL server MYSQL and Teradata Hands on experience with ORC and Parquet File formats Proficient in working with Jira GIT and Jenkins Knowledge with NoSQL Databases HBase and Cassandra Good analytical communication problem solving skills and adore learning new technical and functional skills Strong believer in collaboration teamwork integrity and interest of the client AREAS OF EXPERTISE Big Data Ecosystem HDFS MapReduce Hive YARN Apache Spark Airflow Oozie Zookeeper HUE Sqoop TDCH NoSQL Databases HBase Cassandra Hadoop Distributions AWS Amazon Web Services Cloudera Hortonworks Programming Languages Python Scala HiveQL Scripting Languages Shell Scripting Java Scripting Databases Teradata Snowflake MySQL DB2 IDE Eclipse PyCharm IntelliJ BI Tools Tableau Power BI Version control tools Git GitHub Bitbucket",
    "extracted_keywords": [
        "Data",
        "Engineer",
        "Data",
        "Engineer",
        "Data",
        "Engineer",
        "Nike",
        "Beaverton",
        "years",
        "experience",
        "IT",
        "industry",
        "Big",
        "Data",
        "technologies",
        "Hadoop",
        "Spark",
        "thinking",
        "pleasure",
        "solutions",
        "performance",
        "technologies",
        "methods",
        "believer",
        "collaboration",
        "teamwork",
        "integrity",
        "interest",
        "client",
        "Work",
        "Experience",
        "Data",
        "Engineer",
        "Nike",
        "Beaverton",
        "August",
        "Present",
        "project",
        "assortment",
        "product",
        "solutions",
        "users",
        "request",
        "data",
        "sources",
        "DB2",
        "Oracle",
        "Teradata",
        "data",
        "cleansings",
        "processing",
        "views",
        "data",
        "data",
        "data",
        "science",
        "team",
        "demand",
        "supply",
        "Responsibilities",
        "data",
        "data",
        "sources",
        "Teradata",
        "AWS",
        "S3",
        "Snowflake",
        "spark",
        "API",
        "ETL",
        "frameworks",
        "data",
        "PySpark",
        "Performed",
        "finetuning",
        "spark",
        "applicationsjobs",
        "efficiency",
        "processing",
        "time",
        "pipelines",
        "broadcast",
        "variables",
        "Spark",
        "Joins",
        "transformations",
        "capabilities",
        "data",
        "processing",
        "SparkSQL",
        "event",
        "enrichment",
        "levels",
        "user",
        "summaries",
        "EMR",
        "S3",
        "Athena",
        "services",
        "AWS",
        "cloud",
        "workflows",
        "Airflow",
        "scheduler",
        "tasks",
        "Athena",
        "query",
        "service",
        "queries",
        "analysis",
        "data",
        "discovery",
        "Spark",
        "Core",
        "Spark",
        "SQL",
        "Python",
        "Interacted",
        "infrastructure",
        "network",
        "database",
        "application",
        "BA",
        "teams",
        "data",
        "quality",
        "availability",
        "Environment",
        "AWS",
        "EMR",
        "Athena",
        "Spark",
        "Python",
        "S3",
        "Airflow",
        "Snowflake",
        "ETL",
        "Teradata",
        "Hive",
        "PySpark",
        "TDCH",
        "Hadoop",
        "Developer",
        "Apttus",
        "San",
        "Mateo",
        "CA",
        "January",
        "July",
        "Retail",
        "Enterprise",
        "Credit",
        "Risk",
        "application",
        "Banks",
        "data",
        "credit",
        "cards",
        "auto",
        "student",
        "home",
        "loans",
        "risk",
        "domains",
        "Enterprise",
        "Capital",
        "Management",
        "Data",
        "system",
        "records",
        "Teradata",
        "data",
        "cleansing",
        "processing",
        "views",
        "data",
        "Hadoop",
        "warehouse",
        "data",
        "BI",
        "tools",
        "generating",
        "reports",
        "Responsibilities",
        "scripts",
        "data",
        "HDFS",
        "Hive",
        "Developing",
        "design",
        "documents",
        "approaches",
        "data",
        "sources",
        "loading",
        "data",
        "UNIX",
        "file",
        "systems",
        "HDFS",
        "Installed",
        "Hive",
        "Hive",
        "UDFs",
        "Hive",
        "Tables",
        "data",
        "Hive",
        "queries",
        "Bucketing",
        "Dynamic",
        "Partitioning",
        "Hive",
        "tables",
        "Import",
        "data",
        "sources",
        "HDFSHive",
        "Spark",
        "RDD",
        "Developed",
        "Spark",
        "SQL",
        "scripts",
        "Scala",
        "transformations",
        "actions",
        "RDDs",
        "spark",
        "data",
        "Processing",
        "Spark",
        "Core",
        "Spark",
        "SQL",
        "Scala",
        "Performed",
        "data",
        "transformations",
        "analytics",
        "dataset",
        "Spark",
        "workflows",
        "Apache",
        "Oozie",
        "framework",
        "tasks",
        "results",
        "visualization",
        "BI",
        "tool",
        "Tableau",
        "dashboards",
        "Environment",
        "Hadoop",
        "Hive",
        "Spark",
        "Oozie",
        "Teradata",
        "Yarn",
        "Tableau",
        "Unix",
        "Hortonworks",
        "Sqoop",
        "Scala",
        "Software",
        "Engineer",
        "Accenture",
        "Solutions",
        "Pvt",
        "Ltd",
        "Hyderabad",
        "Telangana",
        "July",
        "July",
        "ExpressScripts",
        "company",
        "American",
        "Pharmacy",
        "Benefit",
        "Management",
        "company",
        "claims",
        "possibility",
        "claims",
        "goal",
        "project",
        "claims",
        "claims",
        "Responsibilities",
        "Data",
        "analysis",
        "reports",
        "product",
        "quality",
        "decision",
        "making",
        "development",
        "Hadoop",
        "System",
        "multinode",
        "Hadoop",
        "Cluster",
        "performance",
        "Experience",
        "data",
        "databases",
        "MySQL",
        "HDFS",
        "data",
        "HDFS",
        "databases",
        "SQOOP",
        "Big",
        "Data",
        "Frameworks",
        "tools",
        "Hadoop",
        "Spark",
        "Hive",
        "Experience",
        "issues",
        "jobs",
        "Hadoop",
        "cluster",
        "workflows",
        "Apache",
        "Oozie",
        "framework",
        "tasks",
        "clients",
        "requirements",
        "SQL",
        "performance",
        "tables",
        "Web",
        "services",
        "component",
        "XML",
        "WSDL",
        "SOAP",
        "DOM",
        "parser",
        "data",
        "applications",
        "capabilities",
        "Web",
        "Services",
        "SOAPWSDL",
        "SOAP",
        "UI",
        "Restful",
        "Web",
        "services",
        "request",
        "AJAX",
        "framework",
        "server",
        "communication",
        "user",
        "experience",
        "Mentoring",
        "recruits",
        "Environment",
        "Hadoop",
        "Hive",
        "SQOOP",
        "Spark",
        "Oozie",
        "Cloudera",
        "Manager",
        "Tableau",
        "Education",
        "Master",
        "Science",
        "Information",
        "Technology",
        "Arkansas",
        "Tech",
        "University",
        "Russellville",
        "AR",
        "Skills",
        "Cassandra",
        "Hdfs",
        "Mapreduce",
        "Oozie",
        "Sqoop",
        "Hbase",
        "Cdh",
        "Db2",
        "Etl",
        "Hadoop",
        "Nosql",
        "Power",
        "bi",
        "Teradata",
        "Amazon",
        "web",
        "services",
        "Apache",
        "spark",
        "Api",
        "Git",
        "Hadoop",
        "Hbase",
        "Hive",
        "Additional",
        "Information",
        "COMPETENCIES",
        "IT",
        "experience",
        "Software",
        "Development",
        "Life",
        "Cycle",
        "Analysis",
        "Design",
        "Development",
        "Testing",
        "Deployment",
        "Support",
        "WATERFALL",
        "AGILE",
        "methodologies",
        "Experience",
        "Big",
        "Data",
        "Technologies",
        "Hadoop",
        "Eco",
        "System",
        "Spark",
        "HDFS",
        "MapReduce",
        "Sqoop",
        "Hive",
        "Retail",
        "Healthcare",
        "sector",
        "sectors",
        "Hadoop",
        "distributions",
        "Amazon",
        "EMR",
        "Cloudera",
        "CDH",
        "knowledge",
        "Hortonworks",
        "HDP",
        "services",
        "EMR",
        "S3",
        "EC2",
        "Athena",
        "Experience",
        "ETL",
        "jobs",
        "data",
        "pipelines",
        "knowledge",
        "ETL",
        "jobs",
        "volumes",
        "data",
        "Hadoop",
        "Ecosystem",
        "databases",
        "Experience",
        "Hive",
        "scripts",
        "Unix",
        "Linux",
        "shell",
        "scripting",
        "Created",
        "User",
        "Defined",
        "Functions",
        "UDFs",
        "Hive",
        "Flexible",
        "implementation",
        "spark",
        "jobs",
        "PySpark",
        "API",
        "Spark",
        "Scala",
        "API",
        "Hive",
        "data",
        "analysis",
        "data",
        "transfer",
        "table",
        "design",
        "Hadoop",
        "environment",
        "Proficient",
        "data",
        "RDBMS",
        "HDFS",
        "Sqoop",
        "TDCH",
        "Experience",
        "Airflow",
        "Oozie",
        "schedulers",
        "Unix",
        "cron",
        "jobs",
        "Hadoop",
        "actions",
        "Experience",
        "SQL",
        "server",
        "MYSQL",
        "Teradata",
        "Hands",
        "experience",
        "ORC",
        "Parquet",
        "File",
        "formats",
        "Jira",
        "GIT",
        "Jenkins",
        "Knowledge",
        "NoSQL",
        "Databases",
        "HBase",
        "Cassandra",
        "communication",
        "problem",
        "skills",
        "adore",
        "skills",
        "believer",
        "collaboration",
        "teamwork",
        "integrity",
        "interest",
        "client",
        "AREAS",
        "EXPERTISE",
        "Big",
        "Data",
        "Ecosystem",
        "HDFS",
        "MapReduce",
        "Hive",
        "YARN",
        "Apache",
        "Spark",
        "Airflow",
        "Oozie",
        "Zookeeper",
        "HUE",
        "Sqoop",
        "TDCH",
        "NoSQL",
        "HBase",
        "Cassandra",
        "Hadoop",
        "Distributions",
        "AWS",
        "Amazon",
        "Web",
        "Services",
        "Cloudera",
        "Hortonworks",
        "Programming",
        "Languages",
        "Python",
        "Scala",
        "HiveQL",
        "Scripting",
        "Languages",
        "Shell",
        "Scripting",
        "Java",
        "Scripting",
        "Databases",
        "Teradata",
        "Snowflake",
        "MySQL",
        "DB2",
        "IDE",
        "Eclipse",
        "PyCharm",
        "IntelliJ",
        "BI",
        "Tools",
        "Tableau",
        "Power",
        "BI",
        "Version",
        "control",
        "Git",
        "GitHub",
        "Bitbucket"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T21:41:54.441468",
    "resume_data": "Data Engineer Data Engineer Data Engineer Nike Beaverton OR Over 5 years of experience in IT industry and have been active in Big Data technologies including Hadoop and Spark Driven by curiosity innovative thinking and pleasure of learning developed apt solutions on performance by leveraging emerging technologies and methods Strong believer in collaboration teamwork integrity and interest of the client Work Experience Data Engineer Nike Beaverton OR August 2018 to Present This project is an assortment product which brings the analytical solutions to users based on their request data comes from different sources like DB2 Oracle and Teradata etc This data will be imported and will undergo several cleansings and valueadded processing and then finally views will be created on this data And this data is consumed by data science team to forecast the demand and supply Responsibilities Ingested the data from various data sources like Teradata into AWS S3 and Snowflake using spark API Developed ETL frameworks for data using PySpark Performed finetuning of spark applicationsjobs to improve the efficiency and overall processing time for the pipelines Used broadcast variables in Spark effective efficient Joins transformations and other capabilities for data processing Used SparkSQL to perform event enrichment and to prepare various levels of user behavioral summaries Worked with EMR S3 Athena services in AWS cloud Implemented the workflows using Airflow scheduler to automate tasks Created Athena query service for adhoc queries analysis and data discovery Experienced working with Spark Core and Spark SQL using Python Interacted with the infrastructure network database application and BA teams to ensure data quality and availability Environment AWS EMR Athena Spark Python S3 Airflow Snowflake ETL Teradata Hive PySpark TDCH Hadoop Developer Apttus San Mateo CA January 2018 to July 2018 Retail Enterprise Credit Risk application calculates Banks retail data such as credit cards auto student and home loans for risk domains including Enterprise Capital Management Data comes from different system of records such mainly from Teradata This data will undergo several cleansing and processing and then finally views will be created on this data in Hadoop warehouse This data will be consumed by BI tools for analyzing and generating reports Responsibilities Developed SQOOP scripts for importing and exporting data into HDFS and Hive Developing design documents considering all possible approaches and identifying best of them Responsible to manage data coming from different sources Responsible for loading data from UNIX file systems to HDFS Installed and configured Hive and written Hive UDFs Involved in creating Hive Tables loading with data and writing Hive queries Used Bucketing and Dynamic Partitioning on Hive tables Import the data from different sources like HDFSHive into Spark RDD Developed Spark SQL scripts using Scala to perform transformations and actions on RDDs in spark for faster data Processing Experienced with in working with Spark Core and Spark SQL using Scala Performed data transformations and analytics on large dataset using Spark Implemented the workflows using Apache Oozie framework to automate tasks Imported results into visualization BI tool Tableau to create dashboards Environment Hadoop Hive Spark Oozie Teradata Yarn Tableau Unix Hortonworks Sqoop Scala Software Engineer Accenture Solutions Pvt Ltd Hyderabad Telangana July 2014 to July 2017 ExpressScripts being the leading company in American Pharmacy Benefit Management company with many claims daily and there is every possibility that some of them are fraudulent claims The goal of this project is to identify possible fraudulent claims out of the total claims processed daily Responsibilities Data analysis and generated reports which helped to improve product quality and decision making Involved in development of Hadoop System and improving multinode Hadoop Cluster performance Experience in importing the data from relational databases such as MySQL to HDFS and exporting the data from HDFS to relational databases using SQOOP Involved in Big Data Frameworks and tools such as Hadoop Spark Hive Experience in troubleshooting the issues and failed jobs in the Hadoop cluster Implemented the workflows using Apache Oozie framework to automate the tasks Communicating with clients to gather the requirements SQL querying and performance tuning creating backup tables Developed Web services component using XML WSDL and SOAP with DOM parser to transfer and transform data between applications Exposed various capabilities as Web Services using SOAPWSDL Used SOAP UI for testing the Restful Web services by sending and SOAP request Used AJAX framework for server communication and seamless user experience Mentoring and Training the new recruits Environment Hadoop Hive SQOOP Spark Oozie Cloudera Manager Tableau Education Master of Science in Information Technology Arkansas Tech University Russellville AR Skills Cassandra Hdfs Mapreduce Oozie Sqoop Hbase Cdh Db2 Etl Hadoop Nosql Power bi Teradata Amazon web services Apache spark Api Git Hadoop Hbase Hive Additional Information CORE COMPETENCIES IT experience in Software Development Life Cycle Analysis Design Development Testing Deployment and Support using WATERFALL and AGILE methodologies Experience in Big Data Technologies using Hadoop Eco System components Spark HDFS MapReduce Sqoop Hive in Retail Healthcare sector and Financial sectors Experienced in working with Hadoop distributions predominantly Amazon EMR Cloudera CDH and knowledge on Hortonworks HDP Experienced in working with cloud services such as EMR S3 EC2 Athena Experience in ETL jobs and developing and managing data pipelines Working knowledge in creating ETL jobs to load huge volumes of data into Hadoop Ecosystem and relational databases Experience in running Hive scripts Unix and Linux shell scripting Created User Defined Functions UDFs in Hive Flexible with full implementation of spark jobs with PySpark API and Spark Scala API Designed Hive queries to perform data analysis data transfer and table design to load into Hadoop environment Proficient in importing exporting data from RDBMS to HDFS using Sqoop and TDCH Experience in using Airflow Oozie schedulers and Unix scripting to implement cron jobs that execute different Hadoop actions Experience in using SQL server MYSQL and Teradata Hands on experience with ORC and Parquet File formats Proficient in working with Jira GIT and Jenkins Knowledge with NoSQL Databases HBase and Cassandra Good analytical communication problem solving skills and adore learning new technical and functional skills Strong believer in collaboration teamwork integrity and interest of the client AREAS OF EXPERTISE Big Data Ecosystem HDFS MapReduce Hive YARN Apache Spark Airflow Oozie Zookeeper HUE Sqoop TDCH NoSQL Databases HBase Cassandra Hadoop Distributions AWS Amazon Web Services Cloudera Hortonworks Programming Languages Python Scala HiveQL Scripting Languages Shell Scripting Java Scripting Databases Teradata Snowflake MySQL DB2 IDE Eclipse PyCharm IntelliJ BI Tools Tableau Power BI Version control tools Git GitHub Bitbucket",
    "unique_id": "305f91a7-d600-469b-97a9-731e6b66e037"
}