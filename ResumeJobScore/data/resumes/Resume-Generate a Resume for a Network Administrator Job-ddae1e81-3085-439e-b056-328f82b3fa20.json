{
    "clean_data": "Sr Hadoop Developer Sr Hadoop span lDeveloperspan Sr Hadoop Developer Inovalon Herndon VA VA Around 8 years of professional IT industry experience encompassing wide range of skill set in Big Data technologies and JavaJ2EE technologies 4 years of experience in working with Big Data Technologies on systems which comprises of massive amount of data running in highly distributive mode in Cloudera Hortonworks Hadoop distributions Hands on experience in using Hadoop ecosystem components like Hadoop Hive Pig Sqoop HBase Cassandra Spark Spark Streaming Spark SQL Oozie ZooKeeper Kafka Flume MapReduce and Yarn Strong Knowledge on architecture and components of Spark and efficient in working with Spark Core  Spark streaming Implemented Spark Streaming jobs by developing RDDs Resilient Distributed Datasets and used pyspark and sparkshell accordingly Experience in configuringSpark Streaming to receive real time data from the Apache Kafka and store the stream data to HDFS using Scala Experience in importing and exporting data using stream processing platforms like Flume and Kafka Accomplished complex HiveQL queries for required data extraction from Hive tables and written Hive UDFs as required Pleasant experience of Partitions bucketing concepts in Hive and designed both Managed and External tables in Hive to optimize performance Involved in converting HiveSQL queries into Spark transformations using Spark Data frames and Scala Used Spark Data Frames API over Cloudera platform to perform analytics on Hive data Used Spark Data Frame Operations to perform required Validations in the data Experience in integrating Hive queries into Spark environment using Spark SQL Good understanding and knowledge of NoSQL databases like MongoDB Hbase and Cassandra Worked on HBase to load and retrieve data for real time processing using Rest API Excellent understanding and knowledge of job workflow scheduling and locking toolsservices like Oozie and Zookeeper Experienced in designing different time driven and data driven automated workflows using Oozie Knowledge of ETL methods for data extraction transformation and loading in corporatewide ETL Solutions and Data Warehouse tools for reporting and data analysis Worked on developing ETL Workflows on the data obtained using Python for processing it in HDFS and HBase using Oozie Experience in configuring the Zookeeper to coordinate the servers in clusters and to maintain the data consistency Experienced in working with Amazon Web Services AWS using EC2 for computing and S3 as storage mechanism Capable of using AWS utilities such as EMR S3 and cloud watch to run and monitor Hadoop and spark jobs on AWS Hands on experience with Spark Data frames  and RDD API of Spark for performing various data transformations and dataset building Involved in developing Impala scripts for extraction transformation loading of data into data warehouse Good knowledge in using apache NiFi to automate the data movement between different Hadoop systems Experienced in using Pig scripts to do transformations event joins filters and some preaggregations before storing the data onto HDFS Experience in importing and exporting the data using Sqoop from HDFS to Relational Database systems and viceversa Good Knowledge in UNIX Shell Scripting for automating deployments and other routine tasks Experience in relational databases like Oracle MySQL and SQL Server Experienced in using Integrated Development environments like Eclipse NetBeans IntelliJ Spring Tool Suite Used various Project Management services like JIRA for tracking issues bugs related to code and GitHub for various code reviews and Worked on various version control tools like GIT SVN Experienced in developing and implementing web applications using Java J2EE JSP Servlets JSF HTML DHTML EJB JavaScript AJAX JSON JQuery CSS XML JDBC and JNDI Experience in Java development GUI using JFC Swing JavaBeans and AWT Excellent Java development skills using J2EE J2SE Servlets JSP EJB JDBC SOAP and RESTful web services Experienced in working in SDLC Agile and Waterfall Methodologies Excellent Communication skills Interpersonal skills problem solving skills and a team player Ability to quickly adapt new environment and technologies Work Experience Sr Hadoop Developer Inovalon Herndon VA December 2017 to Present Inovalon is a leading technology company providing cloudbased platforms empowering datadriven healthcare We bring to the marketplace a nationalscale capability to interconnect with the healthcare ecosystem aggregate and analyze data in realtime and empower the application of resulting insights to drive meaningful impact at the point of care Responsibilities Working knowledge of Spark RDD Data Frame API Data set API Data Source API Spark SQL and Spark Streaming Exploring with the Spark improving the performance and optimization of the existing algorithms in Hadoop using Spark Context  Data Frame Pair RDDs Spark YARN Used Spark SQL on data frames to access hive tables into spark for faster processing of data Performed Spark join optimizations troubleshooted monitored and wrote efficient codes using Scala Experienced with batch processing of data sources using Apache Spark and Elastic search Used Different Spark Modules like Spark core Spark SQL Spark Streaming Spark Data sets and Data frames Worked on analyzing Hadoop stack and different big data analytic tools including Pig Hive HBase database and Sqoop Developed Spark Streaming script which consumes topics from distributed messaging source Kafka and periodically pushes batch of data to spark for real time processing Experienced to implement Hortonworks distribution system Creating Hive tables and working on them for data analysis to cope up with the requirements Developed a frame work to handle loading and transform large sets of unstructured data from UNIX system to HIVE tables Used Spark Data Frames Operations to perform required Validations in the data and to perform analytics on the Hive data Experienced in working with Elastic MapReduce EMR Developed Map Reduce programs for some refined queries on big data Indepth understanding of classic MapReduce and YARN architecture Worked with business team in creating Hive queried for ad hoc access Use Hive to analyze the partitioned and bucketed data and compute various metrics for reporting Implemented Hive Generic UDFs to implement business logic Analyzed the data by performing Hive queries ran Pig scripts Spark SQL and Spark Streaming Installed and configured Pig for ETL jobs Responsible for developing multiple Kafka Producers and Consumers from scratch as per the software requirement specifications Involved in creating generic Sqoop import script for loading data into Hive tables from RDBMS Designed Columnar families in Cassandra and Ingested data from RDBMS performed data transformations and then exported the transformed data to Cassandra as per the business requirement Extracted files from Cassandra through Sqoop and placed in HDFS for further processing Enabled speedy reviews and first mover advantages by using Oozie to automate data loading into the Hadoop Distributed File System and Pig to preprocess the data Created detailed AWS Security groups which behaved as virtual firewalls that controlled the traffic allowed reaching one or more AWS EC2 instances Involved in creating datalake by extracting customers data from various data sources to HDFS which include data from Excel databases and log data from servers Performed data integration with a goal of moving more data effectively efficiently and with high performance to assist in businesscritical projects using Talend Data Integration Design developed unit test and support ETL mapping and scripts for data marts using Talend Used HUE for running Hive queries Created partitions according to day using Hive to improve performance Built a data flow pipeline using flume Java MapReduce and Pig Involved in Agile methodologies daily Scrum meetings Sprint planning Experience in using version control tools like GITHUB to share the code snippet among the team members Environment Hadoop Map Reduce HDFS Hive Cassandra Sqoop Oozie SQL Kafka Spark Scala Java AWS GitHub Talend Big Data Integration Solr Impala Hadoop Developer Surgere Inc Green OH September 2016 to November 2017 Since 2004 Surgere has been reinventing how packaging assets influence profitability While raising the bar on packaging supply chain standards Surgere developed a wide variety of visibility and control technologies to increase data accuracy and manage assets packaging parts tools finished vehicles moving through manufacturing supply chain and logistics verticals Responsibilities Good in implementing advanced procedures like text analytics and processing using the inmemory computing capabilities like Apache Spark written in Scala Used Spark for interactive queries processing of streaming data and integration with popular NoSQL database for huge volume of data Developed Spark scripts by using Scala shell commands as per the requirement Enhanced and optimized product Spark code to aggregate group and run data mining tasks using the Spark framework Used Spark RDD for faster Data sharing Experienced in querying data using  on top of Spark engine for faster data sets processing Worked on Ad hoc queries Indexing Replication Load balancing Aggregation in MongoDB Extracted and restructured the data into MongoDB using import and export command line utility tool Worked on the largescale Hadoop YARN cluster for distributed data processing and analysis using Hive and MongoDB Wrote XML scripts to build Oozie functionality Experience in workflow Scheduler Oozie to manage and schedule job on Hadoop cluster for generating reports on Day and weekly basis Used Flume to collect aggregate and store the web log data from various sources like web servers mobile and network devices and pushed to HDFS Implemented custom serializer interceptor source and sink in Flume to ingest data from multiple sources Involved in writing query using Impala for better and faster processing of data Responsible for analyzing and cleansing raw data by performing HiveImpala queries and running Pig scripts on data Involved in moving log files generated from various sources to HDFS for further processing through Flume Worked extensively with importing metadata into Hive and migrated existing tables and applications to work on Hive and AWS cloud Worked on partitioning the HIVE table and running the scripts in parallel to reduce the run time of the scripts Analyzed the data by performing Hive queries and running Pig scripts to know user behavior Programmed pig scripts with complex joins like replicated and skewed to achieve better performance Developed data pipeline expending Pig and Java MapReduce to consume customer behavioral data and financial antiquities into HDFS for analysis Designing creating ETL jobs through Talend to load huge volumes of data into MongoDB Hadoop Ecosystem and relational databases Created Talend jobs to connect to Quality Stage using FTP connection and process data received from Quality Stage Migrated data from MySQL server to Hadoop using Sqoop for processing data Exported the analyzed data to the relational databases using Sqoop for visualization and to generate reports for the BI team Experienced in developing Shell scripts and Python scripts for system management Worked with application teams to install operating system Hadoop updates patches version upgrades as required Worked with SCRUM team in delivering agreed user stories on time for every Sprint Environment CDH 3x and 4x Java Hadoop Python Map Reduce Hive Pig Impala Flume MongoDB Sqoop Talend Spark MySQL AWS Hadoop Developer Archimedes Inc San Francisco CA November 2015 to July 2016 Archimedes is a healthcare modelling and analytics organization The company enables people to combine realworld healthcare data and simulation data to create compelling and actionable evidence used in individual healthcare decisionmaking as well as population with applications in health and economic outcomes research policy creation and clinical trial design and operations Responsibilities Worked on migrating MapReduce programs into Spark transformations using Scala Responsible for building scalable distributed data solutions using Hadoop Responsible for building scalable distributed data solutions using Hadoop Responsible for cluster maintenance by adding and removing cluster nodes Cluster monitoring troubleshooting managing and reviewing data backups and log files Wrote complex MapReduce jobs in Java to perform operations by extracting transforming and aggregating to process terabytes of data Collected and aggregated large amounts of stream data into HDFS using Flume and defined channel selectors to multiplex data into different sinks Analyzed data using Hadoop components Hive and Pig Scripted complex HiveQL queries on Hive tables to analyze large datasets and wrote complex Hive UDFs to work with sequence files Scheduled workflows using Oozie to automate multiple Hive and Pig jobs which run independently with time and data availability Responsible for creating Hive tables loading data and writing Hive queries to analyze data Generated reports using QlikView Wrote several Hive queries to get valuable information from the hidden large datasets Loaded and transformed large sets of structured semistructured and unstructured data using HadoopBig Data concepts Handled importing data from various data sources performed transformations using Hive Map Reduce and loaded data into HDFS Imported data from Teradatadatabase into HDFS and exported the analyzed patterns data back to Teradata using Sqoop Worked with Talend Open Studio to perform ETL jobs Environment Hadoop Hortonworks HDFS Hive Pig Sqoop Map Reduce HBase Shell Scripting QlikView Teradata 14 Oozie Java 7 Maven 3x JavaHadoop Developer Evidera Bethesda MD August 2014 to October 2015 Evidera provides health economics outcomes research market access data analytics and epidemiology researchand consulting services We partner with life science organization worldwide to optimize the market access and commercial success of their products Responsibilities Involved in Installation and configuration of JDK Hadoop Pig Sqoop Hive HBase on Linux environment Assisted with performance tuning and monitoring Imported data using Sqoop to load data from MySQL to HDFS on regular basis Created reports for the BI team using Sqoop to export data into HDFS and Hive Worked on creating MapReduceprograms to parse the data for claim report generation and running the Jars in Hadoop Coordinated with Java team in creating MapReduce programs Worked on creating Pig scripts for most modules to give a comparison effort estimation on code development Collaborated with BI teams to ensure data quality and availability with live visualization Created HIVE Queries to process large sets of structured semistructured and unstructured data and store in Managed and External tables Created HBase tables to store variable data formats coming from different portfolios Performed realtime analytics on HBase using Java API and Rest API Performed test run of the module components to understand the productivity Written Java program to retrieve data from HDFS and providing REST services Shared responsibility and assistance for administration of Hadoop Hive Sqoop HBase and Pig in team Shared the knowledge of Hadoop concepts with team members Used JUnit for unit testing and Continuum for integration testing Environment Cloudera Hadoop Pig Sqoop Hive HBase Java Eclipse MySQL MapReduce Java Developer Metanoia software Solutions January 2012 to May 2014 Metanoia offers an Ecommerce platform designed and developed for rapid deployment of Ecommerce services by small and medium size organizations Ecloud is a strong and vibrant deployment option and business model for ecommerce solution Our cloud system is a cost effective alternative designed to address the diverse needs of companies of varying sizes across industries Responsibilities Responsible for the analyzing documenting the requirements designing and developing the application based on J2EE standards Strictly Followed Test Driven Development Used Microsoft Visio for designing use cases like Class Diagrams Sequence Diagrams and Data Models Extensively developed user interface using HTML JavaScript jQuery AJAX and CSSon the front end Designed Rich Internet Application by implementing jQuery based accordion styles Used JavaScript for the clientside web page validation Used Spring MVC and Dependency Injection for handling presentation and business logic Integrated Spring DAO for data access using Hibernate Developed Struts web forms and actions for validation of user request data and application functionality Developed programs for accessing the database using JDBC thin driver to execute queries prepared statements Stored Procedures and to manipulate the data in the database Created tile definitions Struts configuration files validation files and resource bundles for all modules using Struts framework Involved in the coding and integration of several businesscritical modules using Java JSF and Hibernate Developed SOAPbased web services for communication between its upstream applications Implemented different Design patterns like DAO Singleton Pattern and MVC architectural design pattern of spring Implemented Service Oriented Architecture SOA on Enterprise Service Bus ESB Developed MessageDriven Beans for asynchronous processing of alerts using JMS Implemented Rational Rose tool for application development Used Clear case for source code control and JUnit for unit testing Performed integration testing of the modules Used putty for UNIX login to run the batch jobs and check server logs Deployed application on to Glassfish Server Involved in peer code reviews Environment Java 6 7 J2EE Struts 2 Glassfish JSP JDBC EJB ANT XML IBM Web Sphere JUnit IBM DB2 Rational Rose 7 CVS UNIX SOAP SQL PLSQL Java Developer Thirdware Solutions Limited July 2009 to December 2011 Thirdware delivers industryspecific technological expertise through a range of services spanning business applications consulting design implementation and support It is involved in developing Smart Data solutions yielding clean organized actionable data to extract information and insight Responsibilities  functional and technical requirements wrote Technical Design Documents Developed analysis level documentation such as Use Case Business Domain Model Activity Sequence and Class Diagrams Developed presentation layer components comprising of JSP AJAX Servlets and JavaBeans using the Struts framework Implemented MVC Model View Controller architecture Developed XML configuration and data description using Hibernate Developed Web services usingCXF to interact with Mainframe applications Responsible for the deployment of the application in the development environment using BEA WebLogic 90 application server Participated in the configuration of BEA WebLogic application server Designed and developed front end user interface using HTML and Java Server Pages JSP for customer profile setup Developed ANT Script to compile the Java files and to build the jars and wars Responsible for Analysis Coding and Unit Testing and Production Support Used JUnit for testing Modules Environment Java 16 J2EE JDBC Struts Framework Hibernate Servlets MVC JSP Web Services CXF SOAP BEA WebLogic 9 Oracle 9i JavaScript XML HTML Ant JUnit SVN My Eclipse Skills JAVA 8 years JUNIT 6 years AJAX 4 years HTML 4 years JAVASCRIPT 4 years Additional Information Technical Skills Hadoop Distributions Cloudera CDH3 CDH4 and CDH5 Hortonworks MapR and Apache Big Data Ecosystem Hadoop MapReduce Pig Hive YARN Kafka Flume Sqoop Impala Oozie Zookeeper Spark Ambari Mahout MongoDB Cassandra Avro Storm Parquet and Snappy Languages Java Python Scala Java Technologies Servlets JavaBeans JSP JDBC and Spring MVC Web Design Tools HTML DHTML AJAX JavaScript JQuery and CSS AngularJS ExtJS and JSON No SQL Databases Cassandra MongoDB and HBase Development Build Tools Eclipse Ant Maven IntelliJ JUNIT and log4J ETL Tools Talend Informatica DB Languages MySQL PLSQL PostgreSQL and Oracle RDBMS Teradata Oracle 9i 10g 11i MS SQL Server MySQL and DB2 Operating systems UNIX LINUX Mac os and Windows Variants Data analytical tools R and MATLAB ETL Tools Talend Informatica Pentaho",
    "entities": [
        "ETL Solutions",
        "RDD API of Spark",
        "Oracle MySQL",
        "Partitions",
        "Cassandra",
        "Validations",
        "BI",
        "HDFS",
        "UNIX",
        "Sr Hadoop Developer Inovalon",
        "Spark Data Frames Operations",
        "Responsibilities",
        "Environment Hadoop Hortonworks HDFS Hive Pig",
        "NiFi",
        "JMS Implemented Rational Rose",
        "Hadoop Ecosystem",
        "Amazon Web Services AWS",
        "Indepth",
        "RDD",
        "Hadoop",
        "ETL Workflows",
        "CSSon",
        "Work Experience Sr Hadoop Developer Inovalon Herndon VA",
        "Implemented Service Oriented Architecture SOA on Enterprise Service",
        "Scala Responsible",
        "JUnit",
        "Spark Core  Spark",
        "Shell",
        "Created HIVE Queries",
        "HBase",
        "Apache Spark",
        "Sr Hadoop Developer Sr Hadoop",
        "JavaJ2EE",
        "Windows Variants Data",
        "Data Models",
        "CDH3",
        "Hadoop Hive Pig Sqoop",
        "Spark RDD Data Frame API Data",
        "Developed",
        "Data Warehouse",
        "Spark Data",
        "Talend Data Integration Design",
        "Flume Worked",
        "Waterfall Methodologies Excellent Communication",
        "Scala Used",
        "Use Case Business Domain Model Activity Sequence",
        "HadoopBig Data",
        "the Hadoop Distributed File System",
        "Linux",
        "JSP",
        "Hibernate Developed",
        "Collaborated",
        "Built",
        "Java Technologies Servlets",
        "Talend",
        "GITHUB",
        "Java J2EE JSP Servlets",
        "BEA WebLogic 90",
        "MVC",
        "Spark",
        "EJB",
        "JavaHadoop",
        "J2EE J2SE Servlets JSP EJB",
        "Quality Stage Migrated",
        "Smart Data",
        "Sqoop",
        "Oozie Experience",
        "HIVE",
        "Created",
        "Analyzed",
        "AWS",
        "Oracle RDBMS Teradata Oracle",
        "Teradatadatabase",
        "Apache Big Data Ecosystem Hadoop MapReduce Pig Hive",
        "GIT SVN Experienced",
        "Spark Streaming Exploring",
        "Project Management",
        "Spark Data Frame Operations",
        "JSP AJAX Servlets",
        "Developed XML",
        "Hibernate Developed Struts",
        "Consumers",
        "HTML",
        "Responsibilities Responsible",
        "Oozie",
        "Created Talend",
        "Technical Design Documents Developed",
        "GitHub",
        "Spark Context  Data Frame Pair RDDs Spark",
        "Flume",
        "API Data",
        "JDK Hadoop Pig Sqoop Hive HBase",
        "DAO Singleton Pattern",
        "Designed Rich Internet Application",
        "Spark Streaming Installed",
        "Responsibilities Working",
        "AWT",
        "Big Data",
        "Hive",
        "JUNIT",
        "Integrated Development",
        "FTP",
        "Kafka Producers",
        "Quality Stage",
        "ETL",
        "Integrated Spring",
        "Spark SQL Good",
        "Performed",
        "JavaBeans",
        "Impala",
        "ANT XML",
        "Managed and External",
        "SQL Server Experienced",
        "Additional Information Technical Skills Hadoop",
        "Oozie Knowledge of ETL",
        "BEA WebLogic",
        "Sphere JUnit IBM",
        "Present Inovalon",
        "Hadoop Responsible",
        "CSS",
        "Java Developer Thirdware Solutions Limited",
        "Big Data Technologies",
        "Created HBase",
        "HiveImpala",
        "Glassfish Server Involved",
        "REST",
        "Relational Database",
        "AWS Hands",
        "MapReduce",
        "AWS Security",
        "RDBMS",
        "NoSQL",
        "Hadoop Hive Sqoop HBase",
        "Oozie Zookeeper Spark Ambari Mahout",
        "MapReduceprograms",
        "Dependency Injection",
        "Pleasant",
        "Cloudera"
    ],
    "experience": "Experience in configuringSpark Streaming to receive real time data from the Apache Kafka and store the stream data to HDFS using Scala Experience in importing and exporting data using stream processing platforms like Flume and Kafka Accomplished complex HiveQL queries for required data extraction from Hive tables and written Hive UDFs as required Pleasant experience of Partitions bucketing concepts in Hive and designed both Managed and External tables in Hive to optimize performance Involved in converting HiveSQL queries into Spark transformations using Spark Data frames and Scala Used Spark Data Frames API over Cloudera platform to perform analytics on Hive data Used Spark Data Frame Operations to perform required Validations in the data Experience in integrating Hive queries into Spark environment using Spark SQL Good understanding and knowledge of NoSQL databases like MongoDB Hbase and Cassandra Worked on HBase to load and retrieve data for real time processing using Rest API Excellent understanding and knowledge of job workflow scheduling and locking toolsservices like Oozie and Zookeeper Experienced in designing different time driven and data driven automated workflows using Oozie Knowledge of ETL methods for data extraction transformation and loading in corporatewide ETL Solutions and Data Warehouse tools for reporting and data analysis Worked on developing ETL Workflows on the data obtained using Python for processing it in HDFS and HBase using Oozie Experience in configuring the Zookeeper to coordinate the servers in clusters and to maintain the data consistency Experienced in working with Amazon Web Services AWS using EC2 for computing and S3 as storage mechanism Capable of using AWS utilities such as EMR S3 and cloud watch to run and monitor Hadoop and spark jobs on AWS Hands on experience with Spark Data frames   and RDD API of Spark for performing various data transformations and dataset building Involved in developing Impala scripts for extraction transformation loading of data into data warehouse Good knowledge in using apache NiFi to automate the data movement between different Hadoop systems Experienced in using Pig scripts to do transformations event joins filters and some preaggregations before storing the data onto HDFS Experience in importing and exporting the data using Sqoop from HDFS to Relational Database systems and viceversa Good Knowledge in UNIX Shell Scripting for automating deployments and other routine tasks Experience in relational databases like Oracle MySQL and SQL Server Experienced in using Integrated Development environments like Eclipse NetBeans IntelliJ Spring Tool Suite Used various Project Management services like JIRA for tracking issues bugs related to code and GitHub for various code reviews and Worked on various version control tools like GIT SVN Experienced in developing and implementing web applications using Java J2EE JSP Servlets JSF HTML DHTML EJB JavaScript AJAX JSON JQuery CSS XML JDBC and JNDI Experience in Java development GUI using JFC Swing JavaBeans and AWT Excellent Java development skills using J2EE J2SE Servlets JSP EJB JDBC SOAP and RESTful web services Experienced in working in SDLC Agile and Waterfall Methodologies Excellent Communication skills Interpersonal skills problem solving skills and a team player Ability to quickly adapt new environment and technologies Work Experience Sr Hadoop Developer Inovalon Herndon VA December 2017 to Present Inovalon is a leading technology company providing cloudbased platforms empowering datadriven healthcare We bring to the marketplace a nationalscale capability to interconnect with the healthcare ecosystem aggregate and analyze data in realtime and empower the application of resulting insights to drive meaningful impact at the point of care Responsibilities Working knowledge of Spark RDD Data Frame API Data set API Data Source API Spark SQL and Spark Streaming Exploring with the Spark improving the performance and optimization of the existing algorithms in Hadoop using Spark Context   Data Frame Pair RDDs Spark YARN Used Spark SQL on data frames to access hive tables into spark for faster processing of data Performed Spark join optimizations troubleshooted monitored and wrote efficient codes using Scala Experienced with batch processing of data sources using Apache Spark and Elastic search Used Different Spark Modules like Spark core Spark SQL Spark Streaming Spark Data sets and Data frames Worked on analyzing Hadoop stack and different big data analytic tools including Pig Hive HBase database and Sqoop Developed Spark Streaming script which consumes topics from distributed messaging source Kafka and periodically pushes batch of data to spark for real time processing Experienced to implement Hortonworks distribution system Creating Hive tables and working on them for data analysis to cope up with the requirements Developed a frame work to handle loading and transform large sets of unstructured data from UNIX system to HIVE tables Used Spark Data Frames Operations to perform required Validations in the data and to perform analytics on the Hive data Experienced in working with Elastic MapReduce EMR Developed Map Reduce programs for some refined queries on big data Indepth understanding of classic MapReduce and YARN architecture Worked with business team in creating Hive queried for ad hoc access Use Hive to analyze the partitioned and bucketed data and compute various metrics for reporting Implemented Hive Generic UDFs to implement business logic Analyzed the data by performing Hive queries ran Pig scripts Spark SQL and Spark Streaming Installed and configured Pig for ETL jobs Responsible for developing multiple Kafka Producers and Consumers from scratch as per the software requirement specifications Involved in creating generic Sqoop import script for loading data into Hive tables from RDBMS Designed Columnar families in Cassandra and Ingested data from RDBMS performed data transformations and then exported the transformed data to Cassandra as per the business requirement Extracted files from Cassandra through Sqoop and placed in HDFS for further processing Enabled speedy reviews and first mover advantages by using Oozie to automate data loading into the Hadoop Distributed File System and Pig to preprocess the data Created detailed AWS Security groups which behaved as virtual firewalls that controlled the traffic allowed reaching one or more AWS EC2 instances Involved in creating datalake by extracting customers data from various data sources to HDFS which include data from Excel databases and log data from servers Performed data integration with a goal of moving more data effectively efficiently and with high performance to assist in businesscritical projects using Talend Data Integration Design developed unit test and support ETL mapping and scripts for data marts using Talend Used HUE for running Hive queries Created partitions according to day using Hive to improve performance Built a data flow pipeline using flume Java MapReduce and Pig Involved in Agile methodologies daily Scrum meetings Sprint planning Experience in using version control tools like GITHUB to share the code snippet among the team members Environment Hadoop Map Reduce HDFS Hive Cassandra Sqoop Oozie SQL Kafka Spark Scala Java AWS GitHub Talend Big Data Integration Solr Impala Hadoop Developer Surgere Inc Green OH September 2016 to November 2017 Since 2004 Surgere has been reinventing how packaging assets influence profitability While raising the bar on packaging supply chain standards Surgere developed a wide variety of visibility and control technologies to increase data accuracy and manage assets packaging parts tools finished vehicles moving through manufacturing supply chain and logistics verticals Responsibilities Good in implementing advanced procedures like text analytics and processing using the inmemory computing capabilities like Apache Spark written in Scala Used Spark for interactive queries processing of streaming data and integration with popular NoSQL database for huge volume of data Developed Spark scripts by using Scala shell commands as per the requirement Enhanced and optimized product Spark code to aggregate group and run data mining tasks using the Spark framework Used Spark RDD for faster Data sharing Experienced in querying data using   on top of Spark engine for faster data sets processing Worked on Ad hoc queries Indexing Replication Load balancing Aggregation in MongoDB Extracted and restructured the data into MongoDB using import and export command line utility tool Worked on the largescale Hadoop YARN cluster for distributed data processing and analysis using Hive and MongoDB Wrote XML scripts to build Oozie functionality Experience in workflow Scheduler Oozie to manage and schedule job on Hadoop cluster for generating reports on Day and weekly basis Used Flume to collect aggregate and store the web log data from various sources like web servers mobile and network devices and pushed to HDFS Implemented custom serializer interceptor source and sink in Flume to ingest data from multiple sources Involved in writing query using Impala for better and faster processing of data Responsible for analyzing and cleansing raw data by performing HiveImpala queries and running Pig scripts on data Involved in moving log files generated from various sources to HDFS for further processing through Flume Worked extensively with importing metadata into Hive and migrated existing tables and applications to work on Hive and AWS cloud Worked on partitioning the HIVE table and running the scripts in parallel to reduce the run time of the scripts Analyzed the data by performing Hive queries and running Pig scripts to know user behavior Programmed pig scripts with complex joins like replicated and skewed to achieve better performance Developed data pipeline expending Pig and Java MapReduce to consume customer behavioral data and financial antiquities into HDFS for analysis Designing creating ETL jobs through Talend to load huge volumes of data into MongoDB Hadoop Ecosystem and relational databases Created Talend jobs to connect to Quality Stage using FTP connection and process data received from Quality Stage Migrated data from MySQL server to Hadoop using Sqoop for processing data Exported the analyzed data to the relational databases using Sqoop for visualization and to generate reports for the BI team Experienced in developing Shell scripts and Python scripts for system management Worked with application teams to install operating system Hadoop updates patches version upgrades as required Worked with SCRUM team in delivering agreed user stories on time for every Sprint Environment CDH 3x and 4x Java Hadoop Python Map Reduce Hive Pig Impala Flume MongoDB Sqoop Talend Spark MySQL AWS Hadoop Developer Archimedes Inc San Francisco CA November 2015 to July 2016 Archimedes is a healthcare modelling and analytics organization The company enables people to combine realworld healthcare data and simulation data to create compelling and actionable evidence used in individual healthcare decisionmaking as well as population with applications in health and economic outcomes research policy creation and clinical trial design and operations Responsibilities Worked on migrating MapReduce programs into Spark transformations using Scala Responsible for building scalable distributed data solutions using Hadoop Responsible for building scalable distributed data solutions using Hadoop Responsible for cluster maintenance by adding and removing cluster nodes Cluster monitoring troubleshooting managing and reviewing data backups and log files Wrote complex MapReduce jobs in Java to perform operations by extracting transforming and aggregating to process terabytes of data Collected and aggregated large amounts of stream data into HDFS using Flume and defined channel selectors to multiplex data into different sinks Analyzed data using Hadoop components Hive and Pig Scripted complex HiveQL queries on Hive tables to analyze large datasets and wrote complex Hive UDFs to work with sequence files Scheduled workflows using Oozie to automate multiple Hive and Pig jobs which run independently with time and data availability Responsible for creating Hive tables loading data and writing Hive queries to analyze data Generated reports using QlikView Wrote several Hive queries to get valuable information from the hidden large datasets Loaded and transformed large sets of structured semistructured and unstructured data using HadoopBig Data concepts Handled importing data from various data sources performed transformations using Hive Map Reduce and loaded data into HDFS Imported data from Teradatadatabase into HDFS and exported the analyzed patterns data back to Teradata using Sqoop Worked with Talend Open Studio to perform ETL jobs Environment Hadoop Hortonworks HDFS Hive Pig Sqoop Map Reduce HBase Shell Scripting QlikView Teradata 14 Oozie Java 7 Maven 3x JavaHadoop Developer Evidera Bethesda MD August 2014 to October 2015 Evidera provides health economics outcomes research market access data analytics and epidemiology researchand consulting services We partner with life science organization worldwide to optimize the market access and commercial success of their products Responsibilities Involved in Installation and configuration of JDK Hadoop Pig Sqoop Hive HBase on Linux environment Assisted with performance tuning and monitoring Imported data using Sqoop to load data from MySQL to HDFS on regular basis Created reports for the BI team using Sqoop to export data into HDFS and Hive Worked on creating MapReduceprograms to parse the data for claim report generation and running the Jars in Hadoop Coordinated with Java team in creating MapReduce programs Worked on creating Pig scripts for most modules to give a comparison effort estimation on code development Collaborated with BI teams to ensure data quality and availability with live visualization Created HIVE Queries to process large sets of structured semistructured and unstructured data and store in Managed and External tables Created HBase tables to store variable data formats coming from different portfolios Performed realtime analytics on HBase using Java API and Rest API Performed test run of the module components to understand the productivity Written Java program to retrieve data from HDFS and providing REST services Shared responsibility and assistance for administration of Hadoop Hive Sqoop HBase and Pig in team Shared the knowledge of Hadoop concepts with team members Used JUnit for unit testing and Continuum for integration testing Environment Cloudera Hadoop Pig Sqoop Hive HBase Java Eclipse MySQL MapReduce Java Developer Metanoia software Solutions January 2012 to May 2014 Metanoia offers an Ecommerce platform designed and developed for rapid deployment of Ecommerce services by small and medium size organizations Ecloud is a strong and vibrant deployment option and business model for ecommerce solution Our cloud system is a cost effective alternative designed to address the diverse needs of companies of varying sizes across industries Responsibilities Responsible for the analyzing documenting the requirements designing and developing the application based on J2EE standards Strictly Followed Test Driven Development Used Microsoft Visio for designing use cases like Class Diagrams Sequence Diagrams and Data Models Extensively developed user interface using HTML JavaScript jQuery AJAX and CSSon the front end Designed Rich Internet Application by implementing jQuery based accordion styles Used JavaScript for the clientside web page validation Used Spring MVC and Dependency Injection for handling presentation and business logic Integrated Spring DAO for data access using Hibernate Developed Struts web forms and actions for validation of user request data and application functionality Developed programs for accessing the database using JDBC thin driver to execute queries prepared statements Stored Procedures and to manipulate the data in the database Created tile definitions Struts configuration files validation files and resource bundles for all modules using Struts framework Involved in the coding and integration of several businesscritical modules using Java JSF and Hibernate Developed SOAPbased web services for communication between its upstream applications Implemented different Design patterns like DAO Singleton Pattern and MVC architectural design pattern of spring Implemented Service Oriented Architecture SOA on Enterprise Service Bus ESB Developed MessageDriven Beans for asynchronous processing of alerts using JMS Implemented Rational Rose tool for application development Used Clear case for source code control and JUnit for unit testing Performed integration testing of the modules Used putty for UNIX login to run the batch jobs and check server logs Deployed application on to Glassfish Server Involved in peer code reviews Environment Java 6 7 J2EE Struts 2 Glassfish JSP JDBC EJB ANT XML IBM Web Sphere JUnit IBM DB2 Rational Rose 7 CVS UNIX SOAP SQL PLSQL Java Developer Thirdware Solutions Limited July 2009 to December 2011 Thirdware delivers industryspecific technological expertise through a range of services spanning business applications consulting design implementation and support It is involved in developing Smart Data solutions yielding clean organized actionable data to extract information and insight Responsibilities   functional and technical requirements wrote Technical Design Documents Developed analysis level documentation such as Use Case Business Domain Model Activity Sequence and Class Diagrams Developed presentation layer components comprising of JSP AJAX Servlets and JavaBeans using the Struts framework Implemented MVC Model View Controller architecture Developed XML configuration and data description using Hibernate Developed Web services usingCXF to interact with Mainframe applications Responsible for the deployment of the application in the development environment using BEA WebLogic 90 application server Participated in the configuration of BEA WebLogic application server Designed and developed front end user interface using HTML and Java Server Pages JSP for customer profile setup Developed ANT Script to compile the Java files and to build the jars and wars Responsible for Analysis Coding and Unit Testing and Production Support Used JUnit for testing Modules Environment Java 16 J2EE JDBC Struts Framework Hibernate Servlets MVC JSP Web Services CXF SOAP BEA WebLogic 9 Oracle 9i JavaScript XML HTML Ant JUnit SVN My Eclipse Skills JAVA 8 years JUNIT 6 years AJAX 4 years HTML 4 years JAVASCRIPT 4 years Additional Information Technical Skills Hadoop Distributions Cloudera CDH3 CDH4 and CDH5 Hortonworks MapR and Apache Big Data Ecosystem Hadoop MapReduce Pig Hive YARN Kafka Flume Sqoop Impala Oozie Zookeeper Spark Ambari Mahout MongoDB Cassandra Avro Storm Parquet and Snappy Languages Java Python Scala Java Technologies Servlets JavaBeans JSP JDBC and Spring MVC Web Design Tools HTML DHTML AJAX JavaScript JQuery and CSS AngularJS ExtJS and JSON No SQL Databases Cassandra MongoDB and HBase Development Build Tools Eclipse Ant Maven IntelliJ JUNIT and log4J ETL Tools Talend Informatica DB Languages MySQL PLSQL PostgreSQL and Oracle RDBMS Teradata Oracle 9i 10 g 11i MS SQL Server MySQL and DB2 Operating systems UNIX LINUX Mac os and Windows Variants Data analytical tools R and MATLAB ETL Tools Talend Informatica Pentaho",
    "extracted_keywords": [
        "Sr",
        "Hadoop",
        "Developer",
        "Sr",
        "Hadoop",
        "span",
        "lDeveloperspan",
        "Sr",
        "Hadoop",
        "Developer",
        "Inovalon",
        "Herndon",
        "VA",
        "VA",
        "years",
        "IT",
        "industry",
        "experience",
        "range",
        "skill",
        "Big",
        "Data",
        "technologies",
        "JavaJ2EE",
        "technologies",
        "years",
        "experience",
        "Big",
        "Data",
        "Technologies",
        "systems",
        "amount",
        "data",
        "mode",
        "Cloudera",
        "Hortonworks",
        "Hadoop",
        "Hands",
        "experience",
        "Hadoop",
        "ecosystem",
        "components",
        "Hadoop",
        "Hive",
        "Pig",
        "Sqoop",
        "HBase",
        "Cassandra",
        "Spark",
        "Spark",
        "Streaming",
        "Spark",
        "SQL",
        "Oozie",
        "ZooKeeper",
        "Kafka",
        "Flume",
        "MapReduce",
        "Yarn",
        "Strong",
        "Knowledge",
        "architecture",
        "components",
        "Spark",
        "Spark",
        "Core",
        "Spark",
        "streaming",
        "Spark",
        "Streaming",
        "jobs",
        "RDDs",
        "Resilient",
        "Distributed",
        "Datasets",
        "pyspark",
        "Experience",
        "configuringSpark",
        "Streaming",
        "time",
        "data",
        "Apache",
        "Kafka",
        "stream",
        "data",
        "HDFS",
        "Scala",
        "Experience",
        "data",
        "stream",
        "processing",
        "platforms",
        "Flume",
        "Kafka",
        "Accomplished",
        "HiveQL",
        "queries",
        "data",
        "extraction",
        "Hive",
        "tables",
        "Hive",
        "UDFs",
        "experience",
        "Partitions",
        "bucketing",
        "concepts",
        "Hive",
        "Managed",
        "tables",
        "Hive",
        "performance",
        "HiveSQL",
        "queries",
        "Spark",
        "transformations",
        "Spark",
        "Data",
        "frames",
        "Scala",
        "Spark",
        "Data",
        "Frames",
        "API",
        "Cloudera",
        "platform",
        "analytics",
        "Hive",
        "data",
        "Spark",
        "Data",
        "Frame",
        "Operations",
        "Validations",
        "data",
        "Experience",
        "Hive",
        "queries",
        "Spark",
        "environment",
        "Spark",
        "SQL",
        "understanding",
        "knowledge",
        "MongoDB",
        "Hbase",
        "Cassandra",
        "Worked",
        "HBase",
        "data",
        "time",
        "processing",
        "Rest",
        "API",
        "Excellent",
        "understanding",
        "knowledge",
        "job",
        "workflow",
        "scheduling",
        "toolsservices",
        "Oozie",
        "Zookeeper",
        "time",
        "data",
        "workflows",
        "Oozie",
        "Knowledge",
        "ETL",
        "methods",
        "data",
        "extraction",
        "transformation",
        "loading",
        "ETL",
        "Solutions",
        "Data",
        "Warehouse",
        "tools",
        "reporting",
        "data",
        "analysis",
        "ETL",
        "Workflows",
        "data",
        "Python",
        "HDFS",
        "HBase",
        "Oozie",
        "Experience",
        "Zookeeper",
        "servers",
        "clusters",
        "data",
        "consistency",
        "Amazon",
        "Web",
        "Services",
        "AWS",
        "EC2",
        "computing",
        "S3",
        "storage",
        "mechanism",
        "AWS",
        "utilities",
        "EMR",
        "S3",
        "cloud",
        "Hadoop",
        "spark",
        "jobs",
        "AWS",
        "Hands",
        "experience",
        "Spark",
        "Data",
        "frames",
        "RDD",
        "API",
        "Spark",
        "data",
        "transformations",
        "building",
        "Impala",
        "scripts",
        "extraction",
        "transformation",
        "loading",
        "data",
        "data",
        "warehouse",
        "knowledge",
        "apache",
        "NiFi",
        "data",
        "movement",
        "Hadoop",
        "systems",
        "Pig",
        "scripts",
        "transformations",
        "event",
        "filters",
        "preaggregations",
        "data",
        "HDFS",
        "Experience",
        "data",
        "Sqoop",
        "HDFS",
        "Relational",
        "Database",
        "systems",
        "viceversa",
        "Good",
        "Knowledge",
        "UNIX",
        "Shell",
        "Scripting",
        "deployments",
        "tasks",
        "Experience",
        "databases",
        "Oracle",
        "MySQL",
        "SQL",
        "Server",
        "Integrated",
        "Development",
        "environments",
        "Eclipse",
        "NetBeans",
        "Spring",
        "Tool",
        "Suite",
        "Project",
        "Management",
        "services",
        "JIRA",
        "tracking",
        "issues",
        "bugs",
        "code",
        "GitHub",
        "code",
        "reviews",
        "version",
        "control",
        "tools",
        "GIT",
        "SVN",
        "web",
        "applications",
        "Java",
        "J2EE",
        "JSP",
        "Servlets",
        "JSF",
        "HTML",
        "DHTML",
        "EJB",
        "JavaScript",
        "AJAX",
        "JSON",
        "JQuery",
        "CSS",
        "XML",
        "JDBC",
        "JNDI",
        "Experience",
        "Java",
        "development",
        "GUI",
        "JFC",
        "Swing",
        "JavaBeans",
        "AWT",
        "Excellent",
        "Java",
        "development",
        "skills",
        "J2EE",
        "J2SE",
        "Servlets",
        "JSP",
        "EJB",
        "JDBC",
        "SOAP",
        "web",
        "services",
        "SDLC",
        "Agile",
        "Waterfall",
        "Methodologies",
        "Excellent",
        "Communication",
        "skills",
        "problem",
        "skills",
        "team",
        "player",
        "Ability",
        "environment",
        "technologies",
        "Work",
        "Experience",
        "Sr",
        "Hadoop",
        "Developer",
        "Inovalon",
        "Herndon",
        "VA",
        "December",
        "Present",
        "Inovalon",
        "technology",
        "company",
        "platforms",
        "healthcare",
        "marketplace",
        "nationalscale",
        "capability",
        "healthcare",
        "ecosystem",
        "data",
        "realtime",
        "application",
        "insights",
        "impact",
        "point",
        "care",
        "Responsibilities",
        "knowledge",
        "Spark",
        "RDD",
        "Data",
        "Frame",
        "API",
        "Data",
        "API",
        "Data",
        "Source",
        "API",
        "Spark",
        "SQL",
        "Spark",
        "Streaming",
        "Exploring",
        "Spark",
        "performance",
        "optimization",
        "algorithms",
        "Hadoop",
        "Spark",
        "Context",
        "Data",
        "Frame",
        "Pair",
        "Spark",
        "YARN",
        "Spark",
        "SQL",
        "data",
        "frames",
        "tables",
        "spark",
        "processing",
        "data",
        "Performed",
        "Spark",
        "join",
        "optimizations",
        "codes",
        "Scala",
        "batch",
        "processing",
        "data",
        "sources",
        "Apache",
        "Spark",
        "search",
        "Different",
        "Spark",
        "Modules",
        "Spark",
        "core",
        "Spark",
        "SQL",
        "Spark",
        "Streaming",
        "Spark",
        "Data",
        "sets",
        "Data",
        "frames",
        "Hadoop",
        "stack",
        "data",
        "tools",
        "Pig",
        "Hive",
        "HBase",
        "database",
        "Sqoop",
        "Developed",
        "Spark",
        "Streaming",
        "script",
        "topics",
        "source",
        "Kafka",
        "batch",
        "data",
        "time",
        "processing",
        "Hortonworks",
        "distribution",
        "system",
        "Hive",
        "tables",
        "data",
        "analysis",
        "requirements",
        "frame",
        "work",
        "loading",
        "sets",
        "data",
        "UNIX",
        "system",
        "HIVE",
        "tables",
        "Spark",
        "Data",
        "Frames",
        "Operations",
        "Validations",
        "data",
        "analytics",
        "Hive",
        "data",
        "MapReduce",
        "EMR",
        "Developed",
        "Map",
        "programs",
        "queries",
        "data",
        "understanding",
        "MapReduce",
        "YARN",
        "architecture",
        "business",
        "team",
        "Hive",
        "access",
        "Use",
        "Hive",
        "data",
        "metrics",
        "Hive",
        "Generic",
        "UDFs",
        "business",
        "logic",
        "data",
        "Hive",
        "queries",
        "Pig",
        "Spark",
        "SQL",
        "Spark",
        "Streaming",
        "Installed",
        "Pig",
        "ETL",
        "jobs",
        "Kafka",
        "Producers",
        "Consumers",
        "scratch",
        "software",
        "requirement",
        "specifications",
        "Sqoop",
        "import",
        "script",
        "loading",
        "data",
        "Hive",
        "tables",
        "Columnar",
        "families",
        "Cassandra",
        "data",
        "data",
        "transformations",
        "data",
        "Cassandra",
        "business",
        "requirement",
        "files",
        "Cassandra",
        "Sqoop",
        "HDFS",
        "processing",
        "reviews",
        "advantages",
        "Oozie",
        "data",
        "loading",
        "Hadoop",
        "Distributed",
        "File",
        "System",
        "Pig",
        "data",
        "AWS",
        "Security",
        "groups",
        "firewalls",
        "traffic",
        "AWS",
        "EC2",
        "instances",
        "datalake",
        "customers",
        "data",
        "data",
        "sources",
        "HDFS",
        "data",
        "Excel",
        "databases",
        "data",
        "servers",
        "Performed",
        "data",
        "integration",
        "goal",
        "data",
        "performance",
        "projects",
        "Talend",
        "Data",
        "Integration",
        "Design",
        "unit",
        "test",
        "ETL",
        "mapping",
        "scripts",
        "data",
        "marts",
        "Talend",
        "HUE",
        "Hive",
        "queries",
        "partitions",
        "day",
        "Hive",
        "performance",
        "data",
        "flow",
        "pipeline",
        "flume",
        "Java",
        "MapReduce",
        "Pig",
        "methodologies",
        "Scrum",
        "meetings",
        "Sprint",
        "planning",
        "Experience",
        "version",
        "control",
        "tools",
        "GITHUB",
        "code",
        "snippet",
        "team",
        "members",
        "Environment",
        "Hadoop",
        "Map",
        "HDFS",
        "Hive",
        "Cassandra",
        "Sqoop",
        "Oozie",
        "SQL",
        "Kafka",
        "Spark",
        "Scala",
        "Java",
        "GitHub",
        "Talend",
        "Big",
        "Data",
        "Integration",
        "Solr",
        "Impala",
        "Hadoop",
        "Developer",
        "Surgere",
        "Inc",
        "Green",
        "OH",
        "September",
        "November",
        "Surgere",
        "packaging",
        "assets",
        "influence",
        "profitability",
        "bar",
        "packaging",
        "supply",
        "chain",
        "standards",
        "Surgere",
        "variety",
        "visibility",
        "control",
        "technologies",
        "data",
        "accuracy",
        "assets",
        "packaging",
        "parts",
        "tools",
        "vehicles",
        "supply",
        "chain",
        "logistics",
        "verticals",
        "Responsibilities",
        "procedures",
        "text",
        "analytics",
        "processing",
        "inmemory",
        "computing",
        "capabilities",
        "Apache",
        "Spark",
        "Scala",
        "Spark",
        "queries",
        "processing",
        "data",
        "integration",
        "NoSQL",
        "database",
        "volume",
        "data",
        "Spark",
        "scripts",
        "Scala",
        "shell",
        "commands",
        "requirement",
        "product",
        "Spark",
        "code",
        "group",
        "data",
        "mining",
        "tasks",
        "Spark",
        "framework",
        "Spark",
        "RDD",
        "Data",
        "sharing",
        "data",
        "top",
        "Spark",
        "engine",
        "data",
        "sets",
        "Ad",
        "Indexing",
        "Replication",
        "Load",
        "Aggregation",
        "MongoDB",
        "data",
        "MongoDB",
        "import",
        "export",
        "command",
        "line",
        "utility",
        "tool",
        "largescale",
        "Hadoop",
        "YARN",
        "cluster",
        "data",
        "processing",
        "analysis",
        "Hive",
        "MongoDB",
        "Wrote",
        "XML",
        "scripts",
        "Oozie",
        "functionality",
        "Experience",
        "workflow",
        "Scheduler",
        "Oozie",
        "schedule",
        "job",
        "Hadoop",
        "cluster",
        "reports",
        "Day",
        "basis",
        "Flume",
        "aggregate",
        "web",
        "log",
        "data",
        "sources",
        "web",
        "servers",
        "mobile",
        "network",
        "devices",
        "custom",
        "serializer",
        "interceptor",
        "source",
        "sink",
        "Flume",
        "data",
        "sources",
        "query",
        "Impala",
        "processing",
        "data",
        "data",
        "HiveImpala",
        "queries",
        "Pig",
        "scripts",
        "data",
        "log",
        "files",
        "sources",
        "HDFS",
        "processing",
        "Flume",
        "metadata",
        "Hive",
        "tables",
        "applications",
        "Hive",
        "AWS",
        "cloud",
        "HIVE",
        "table",
        "scripts",
        "parallel",
        "time",
        "scripts",
        "data",
        "Hive",
        "queries",
        "Pig",
        "scripts",
        "user",
        "behavior",
        "pig",
        "scripts",
        "joins",
        "performance",
        "data",
        "pipeline",
        "Pig",
        "Java",
        "MapReduce",
        "customer",
        "data",
        "antiquities",
        "HDFS",
        "analysis",
        "Designing",
        "ETL",
        "jobs",
        "Talend",
        "volumes",
        "data",
        "MongoDB",
        "Hadoop",
        "Ecosystem",
        "databases",
        "Talend",
        "jobs",
        "Quality",
        "Stage",
        "FTP",
        "connection",
        "process",
        "data",
        "Quality",
        "Stage",
        "data",
        "MySQL",
        "server",
        "Hadoop",
        "Sqoop",
        "processing",
        "data",
        "data",
        "databases",
        "Sqoop",
        "visualization",
        "reports",
        "BI",
        "team",
        "Shell",
        "scripts",
        "Python",
        "scripts",
        "system",
        "management",
        "application",
        "teams",
        "operating",
        "system",
        "Hadoop",
        "updates",
        "version",
        "upgrades",
        "Worked",
        "SCRUM",
        "team",
        "user",
        "stories",
        "time",
        "Sprint",
        "Environment",
        "CDH",
        "4x",
        "Java",
        "Hadoop",
        "Python",
        "Map",
        "Reduce",
        "Hive",
        "Pig",
        "Impala",
        "Flume",
        "MongoDB",
        "Sqoop",
        "Talend",
        "Spark",
        "MySQL",
        "AWS",
        "Hadoop",
        "Developer",
        "Archimedes",
        "Inc",
        "San",
        "Francisco",
        "CA",
        "November",
        "July",
        "Archimedes",
        "healthcare",
        "modelling",
        "analytics",
        "organization",
        "company",
        "people",
        "realworld",
        "healthcare",
        "data",
        "simulation",
        "data",
        "evidence",
        "healthcare",
        "population",
        "applications",
        "health",
        "outcomes",
        "research",
        "policy",
        "creation",
        "trial",
        "design",
        "operations",
        "Responsibilities",
        "MapReduce",
        "programs",
        "Spark",
        "transformations",
        "Scala",
        "Responsible",
        "data",
        "solutions",
        "Hadoop",
        "Responsible",
        "data",
        "solutions",
        "Hadoop",
        "Responsible",
        "cluster",
        "maintenance",
        "cluster",
        "nodes",
        "Cluster",
        "monitoring",
        "data",
        "backups",
        "log",
        "files",
        "Wrote",
        "MapReduce",
        "jobs",
        "Java",
        "operations",
        "transforming",
        "terabytes",
        "data",
        "amounts",
        "stream",
        "data",
        "HDFS",
        "Flume",
        "channel",
        "selectors",
        "data",
        "sinks",
        "data",
        "Hadoop",
        "components",
        "Hive",
        "Pig",
        "Scripted",
        "HiveQL",
        "queries",
        "Hive",
        "tables",
        "datasets",
        "Hive",
        "UDFs",
        "sequence",
        "files",
        "workflows",
        "Oozie",
        "Hive",
        "Pig",
        "jobs",
        "time",
        "data",
        "availability",
        "Hive",
        "tables",
        "loading",
        "data",
        "Hive",
        "queries",
        "data",
        "reports",
        "QlikView",
        "Wrote",
        "Hive",
        "queries",
        "information",
        "datasets",
        "sets",
        "data",
        "HadoopBig",
        "Data",
        "concepts",
        "data",
        "data",
        "sources",
        "transformations",
        "Hive",
        "Map",
        "Reduce",
        "data",
        "HDFS",
        "data",
        "Teradatadatabase",
        "HDFS",
        "patterns",
        "data",
        "Teradata",
        "Sqoop",
        "Worked",
        "Talend",
        "Open",
        "Studio",
        "ETL",
        "jobs",
        "Environment",
        "Hadoop",
        "Hortonworks",
        "HDFS",
        "Hive",
        "Pig",
        "Sqoop",
        "Map",
        "Reduce",
        "HBase",
        "Shell",
        "Scripting",
        "QlikView",
        "Teradata",
        "Oozie",
        "Java",
        "Maven",
        "JavaHadoop",
        "Developer",
        "Evidera",
        "Bethesda",
        "MD",
        "August",
        "October",
        "Evidera",
        "health",
        "economics",
        "outcomes",
        "research",
        "market",
        "access",
        "data",
        "analytics",
        "epidemiology",
        "researchand",
        "consulting",
        "services",
        "partner",
        "life",
        "science",
        "organization",
        "market",
        "access",
        "success",
        "products",
        "Responsibilities",
        "Installation",
        "configuration",
        "JDK",
        "Hadoop",
        "Pig",
        "Sqoop",
        "Hive",
        "HBase",
        "Linux",
        "environment",
        "performance",
        "tuning",
        "data",
        "Sqoop",
        "data",
        "MySQL",
        "HDFS",
        "basis",
        "reports",
        "BI",
        "team",
        "Sqoop",
        "data",
        "HDFS",
        "Hive",
        "Worked",
        "MapReduceprograms",
        "data",
        "claim",
        "report",
        "generation",
        "Jars",
        "Hadoop",
        "Coordinated",
        "Java",
        "team",
        "MapReduce",
        "programs",
        "Pig",
        "scripts",
        "modules",
        "comparison",
        "effort",
        "estimation",
        "code",
        "development",
        "BI",
        "teams",
        "data",
        "quality",
        "availability",
        "visualization",
        "HIVE",
        "Queries",
        "sets",
        "data",
        "store",
        "Managed",
        "tables",
        "Created",
        "HBase",
        "data",
        "formats",
        "portfolios",
        "analytics",
        "HBase",
        "Java",
        "API",
        "Rest",
        "API",
        "Performed",
        "test",
        "run",
        "module",
        "components",
        "productivity",
        "Java",
        "program",
        "data",
        "HDFS",
        "REST",
        "services",
        "responsibility",
        "assistance",
        "administration",
        "Hadoop",
        "Hive",
        "Sqoop",
        "HBase",
        "Pig",
        "team",
        "knowledge",
        "Hadoop",
        "concepts",
        "team",
        "members",
        "JUnit",
        "unit",
        "testing",
        "Continuum",
        "integration",
        "testing",
        "Environment",
        "Cloudera",
        "Hadoop",
        "Pig",
        "Sqoop",
        "Hive",
        "HBase",
        "Java",
        "Eclipse",
        "MySQL",
        "MapReduce",
        "Java",
        "Developer",
        "Metanoia",
        "software",
        "Solutions",
        "January",
        "May",
        "Metanoia",
        "Ecommerce",
        "platform",
        "deployment",
        "Ecommerce",
        "services",
        "size",
        "organizations",
        "Ecloud",
        "deployment",
        "option",
        "business",
        "model",
        "ecommerce",
        "solution",
        "cloud",
        "system",
        "cost",
        "alternative",
        "needs",
        "companies",
        "sizes",
        "industries",
        "Responsibilities",
        "analyzing",
        "requirements",
        "application",
        "J2EE",
        "standards",
        "Test",
        "Driven",
        "Development",
        "Microsoft",
        "Visio",
        "use",
        "cases",
        "Class",
        "Diagrams",
        "Sequence",
        "Diagrams",
        "Data",
        "Models",
        "user",
        "interface",
        "HTML",
        "JavaScript",
        "jQuery",
        "AJAX",
        "end",
        "Rich",
        "Internet",
        "Application",
        "jQuery",
        "accordion",
        "styles",
        "JavaScript",
        "web",
        "page",
        "validation",
        "Spring",
        "MVC",
        "Dependency",
        "Injection",
        "presentation",
        "business",
        "logic",
        "Integrated",
        "Spring",
        "DAO",
        "data",
        "access",
        "Hibernate",
        "Developed",
        "Struts",
        "web",
        "forms",
        "actions",
        "validation",
        "user",
        "request",
        "data",
        "application",
        "functionality",
        "programs",
        "database",
        "JDBC",
        "driver",
        "queries",
        "statements",
        "Stored",
        "Procedures",
        "data",
        "database",
        "tile",
        "definitions",
        "Struts",
        "configuration",
        "validation",
        "files",
        "resource",
        "bundles",
        "modules",
        "Struts",
        "framework",
        "coding",
        "integration",
        "modules",
        "Java",
        "JSF",
        "Hibernate",
        "Developed",
        "web",
        "services",
        "communication",
        "applications",
        "Design",
        "patterns",
        "DAO",
        "Singleton",
        "Pattern",
        "MVC",
        "design",
        "pattern",
        "spring",
        "Implemented",
        "Service",
        "Oriented",
        "Architecture",
        "SOA",
        "Enterprise",
        "Service",
        "Bus",
        "ESB",
        "MessageDriven",
        "Beans",
        "processing",
        "alerts",
        "JMS",
        "Rational",
        "Rose",
        "tool",
        "application",
        "development",
        "Clear",
        "case",
        "source",
        "code",
        "control",
        "JUnit",
        "unit",
        "Performed",
        "integration",
        "testing",
        "modules",
        "putty",
        "UNIX",
        "login",
        "batch",
        "jobs",
        "server",
        "logs",
        "application",
        "Glassfish",
        "Server",
        "peer",
        "code",
        "reviews",
        "Environment",
        "Java",
        "J2EE",
        "Struts",
        "JSP",
        "JDBC",
        "EJB",
        "ANT",
        "IBM",
        "Web",
        "Sphere",
        "JUnit",
        "IBM",
        "DB2",
        "Rational",
        "Rose",
        "CVS",
        "UNIX",
        "SOAP",
        "SQL",
        "PLSQL",
        "Java",
        "Developer",
        "Thirdware",
        "Solutions",
        "Limited",
        "July",
        "December",
        "Thirdware",
        "expertise",
        "range",
        "services",
        "business",
        "applications",
        "design",
        "implementation",
        "support",
        "Smart",
        "Data",
        "solutions",
        "data",
        "information",
        "insight",
        "Responsibilities",
        "requirements",
        "Technical",
        "Design",
        "Documents",
        "analysis",
        "level",
        "documentation",
        "Use",
        "Case",
        "Business",
        "Domain",
        "Model",
        "Activity",
        "Sequence",
        "Class",
        "Diagrams",
        "presentation",
        "layer",
        "components",
        "JSP",
        "AJAX",
        "Servlets",
        "JavaBeans",
        "Struts",
        "framework",
        "MVC",
        "Model",
        "View",
        "Controller",
        "architecture",
        "Developed",
        "XML",
        "configuration",
        "data",
        "description",
        "Hibernate",
        "Developed",
        "Web",
        "services",
        "Mainframe",
        "applications",
        "deployment",
        "application",
        "development",
        "environment",
        "BEA",
        "WebLogic",
        "application",
        "server",
        "configuration",
        "BEA",
        "WebLogic",
        "application",
        "server",
        "end",
        "user",
        "interface",
        "HTML",
        "Java",
        "Server",
        "Pages",
        "JSP",
        "customer",
        "profile",
        "setup",
        "ANT",
        "Script",
        "Java",
        "files",
        "jars",
        "wars",
        "Analysis",
        "Coding",
        "Unit",
        "Testing",
        "Production",
        "Support",
        "JUnit",
        "Modules",
        "Environment",
        "Java",
        "J2EE",
        "JDBC",
        "Struts",
        "Framework",
        "Hibernate",
        "Servlets",
        "MVC",
        "JSP",
        "Web",
        "Services",
        "CXF",
        "SOAP",
        "BEA",
        "WebLogic",
        "Oracle",
        "9i",
        "JavaScript",
        "XML",
        "HTML",
        "Ant",
        "JUnit",
        "SVN",
        "Eclipse",
        "Skills",
        "JAVA",
        "years",
        "JUNIT",
        "years",
        "AJAX",
        "years",
        "HTML",
        "years",
        "JAVASCRIPT",
        "years",
        "Additional",
        "Information",
        "Technical",
        "Skills",
        "Hadoop",
        "Distributions",
        "Cloudera",
        "CDH3",
        "CDH4",
        "CDH5",
        "Hortonworks",
        "MapR",
        "Apache",
        "Big",
        "Data",
        "Ecosystem",
        "Hadoop",
        "MapReduce",
        "Pig",
        "Hive",
        "YARN",
        "Kafka",
        "Flume",
        "Sqoop",
        "Impala",
        "Oozie",
        "Zookeeper",
        "Spark",
        "Ambari",
        "Mahout",
        "MongoDB",
        "Cassandra",
        "Avro",
        "Storm",
        "Parquet",
        "Snappy",
        "Languages",
        "Java",
        "Python",
        "Scala",
        "Java",
        "Technologies",
        "Servlets",
        "JavaBeans",
        "JSP",
        "JDBC",
        "Spring",
        "MVC",
        "Web",
        "Design",
        "Tools",
        "HTML",
        "DHTML",
        "AJAX",
        "JavaScript",
        "JQuery",
        "CSS",
        "AngularJS",
        "ExtJS",
        "JSON",
        "SQL",
        "Cassandra",
        "MongoDB",
        "HBase",
        "Development",
        "Build",
        "Tools",
        "Eclipse",
        "Ant",
        "Maven",
        "IntelliJ",
        "JUNIT",
        "ETL",
        "Tools",
        "Talend",
        "Informatica",
        "DB",
        "Languages",
        "MySQL",
        "PLSQL",
        "PostgreSQL",
        "Oracle",
        "RDBMS",
        "Teradata",
        "Oracle",
        "9i",
        "g",
        "11i",
        "MS",
        "SQL",
        "Server",
        "MySQL",
        "DB2",
        "Operating",
        "systems",
        "UNIX",
        "LINUX",
        "Mac",
        "Windows",
        "Variants",
        "Data",
        "tools",
        "R",
        "MATLAB",
        "ETL",
        "Tools",
        "Talend",
        "Informatica",
        "Pentaho"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T20:30:15.680753",
    "resume_data": "Sr Hadoop Developer Sr Hadoop span lDeveloperspan Sr Hadoop Developer Inovalon Herndon VA VA Around 8 years of professional IT industry experience encompassing wide range of skill set in Big Data technologies and JavaJ2EE technologies 4 years of experience in working with Big Data Technologies on systems which comprises of massive amount of data running in highly distributive mode in Cloudera Hortonworks Hadoop distributions Hands on experience in using Hadoop ecosystem components like Hadoop Hive Pig Sqoop HBase Cassandra Spark Spark Streaming Spark SQL Oozie ZooKeeper Kafka Flume MapReduce and Yarn Strong Knowledge on architecture and components of Spark and efficient in working with Spark Core SparkSQL Spark streaming Implemented Spark Streaming jobs by developing RDDs Resilient Distributed Datasets and used pyspark and sparkshell accordingly Experience in configuringSpark Streaming to receive real time data from the Apache Kafka and store the stream data to HDFS using Scala Experience in importing and exporting data using stream processing platforms like Flume and Kafka Accomplished complex HiveQL queries for required data extraction from Hive tables and written Hive UDFs as required Pleasant experience of Partitions bucketing concepts in Hive and designed both Managed and External tables in Hive to optimize performance Involved in converting HiveSQL queries into Spark transformations using Spark Data frames and Scala Used Spark Data Frames API over Cloudera platform to perform analytics on Hive data Used Spark Data Frame Operations to perform required Validations in the data Experience in integrating Hive queries into Spark environment using Spark SQL Good understanding and knowledge of NoSQL databases like MongoDB Hbase and Cassandra Worked on HBase to load and retrieve data for real time processing using Rest API Excellent understanding and knowledge of job workflow scheduling and locking toolsservices like Oozie and Zookeeper Experienced in designing different time driven and data driven automated workflows using Oozie Knowledge of ETL methods for data extraction transformation and loading in corporatewide ETL Solutions and Data Warehouse tools for reporting and data analysis Worked on developing ETL Workflows on the data obtained using Python for processing it in HDFS and HBase using Oozie Experience in configuring the Zookeeper to coordinate the servers in clusters and to maintain the data consistency Experienced in working with Amazon Web Services AWS using EC2 for computing and S3 as storage mechanism Capable of using AWS utilities such as EMR S3 and cloud watch to run and monitor Hadoop and spark jobs on AWS Hands on experience with Spark Data frames SparkSQL and RDD API of Spark for performing various data transformations and dataset building Involved in developing Impala scripts for extraction transformation loading of data into data warehouse Good knowledge in using apache NiFi to automate the data movement between different Hadoop systems Experienced in using Pig scripts to do transformations event joins filters and some preaggregations before storing the data onto HDFS Experience in importing and exporting the data using Sqoop from HDFS to Relational Database systems and viceversa Good Knowledge in UNIX Shell Scripting for automating deployments and other routine tasks Experience in relational databases like Oracle MySQL and SQL Server Experienced in using Integrated Development environments like Eclipse NetBeans IntelliJ Spring Tool Suite Used various Project Management services like JIRA for tracking issues bugs related to code and GitHub for various code reviews and Worked on various version control tools like GIT SVN Experienced in developing and implementing web applications using Java J2EE JSP Servlets JSF HTML DHTML EJB JavaScript AJAX JSON JQuery CSS XML JDBC and JNDI Experience in Java development GUI using JFC Swing JavaBeans and AWT Excellent Java development skills using J2EE J2SE Servlets JSP EJB JDBC SOAP and RESTful web services Experienced in working in SDLC Agile and Waterfall Methodologies Excellent Communication skills Interpersonal skills problem solving skills and a team player Ability to quickly adapt new environment and technologies Work Experience Sr Hadoop Developer Inovalon Herndon VA December 2017 to Present Inovalon is a leading technology company providing cloudbased platforms empowering datadriven healthcare We bring to the marketplace a nationalscale capability to interconnect with the healthcare ecosystem aggregate and analyze data in realtime and empower the application of resulting insights to drive meaningful impact at the point of care Responsibilities Working knowledge of Spark RDD Data Frame API Data set API Data Source API Spark SQL and Spark Streaming Exploring with the Spark improving the performance and optimization of the existing algorithms in Hadoop using Spark Context SparkSQL Data Frame Pair RDDs Spark YARN Used Spark SQL on data frames to access hive tables into spark for faster processing of data Performed Spark join optimizations troubleshooted monitored and wrote efficient codes using Scala Experienced with batch processing of data sources using Apache Spark and Elastic search Used Different Spark Modules like Spark core Spark SQL Spark Streaming Spark Data sets and Data frames Worked on analyzing Hadoop stack and different big data analytic tools including Pig Hive HBase database and Sqoop Developed Spark Streaming script which consumes topics from distributed messaging source Kafka and periodically pushes batch of data to spark for real time processing Experienced to implement Hortonworks distribution system Creating Hive tables and working on them for data analysis to cope up with the requirements Developed a frame work to handle loading and transform large sets of unstructured data from UNIX system to HIVE tables Used Spark Data Frames Operations to perform required Validations in the data and to perform analytics on the Hive data Experienced in working with Elastic MapReduce EMR Developed Map Reduce programs for some refined queries on big data Indepth understanding of classic MapReduce and YARN architecture Worked with business team in creating Hive queried for ad hoc access Use Hive to analyze the partitioned and bucketed data and compute various metrics for reporting Implemented Hive Generic UDFs to implement business logic Analyzed the data by performing Hive queries ran Pig scripts Spark SQL and Spark Streaming Installed and configured Pig for ETL jobs Responsible for developing multiple Kafka Producers and Consumers from scratch as per the software requirement specifications Involved in creating generic Sqoop import script for loading data into Hive tables from RDBMS Designed Columnar families in Cassandra and Ingested data from RDBMS performed data transformations and then exported the transformed data to Cassandra as per the business requirement Extracted files from Cassandra through Sqoop and placed in HDFS for further processing Enabled speedy reviews and first mover advantages by using Oozie to automate data loading into the Hadoop Distributed File System and Pig to preprocess the data Created detailed AWS Security groups which behaved as virtual firewalls that controlled the traffic allowed reaching one or more AWS EC2 instances Involved in creating datalake by extracting customers data from various data sources to HDFS which include data from Excel databases and log data from servers Performed data integration with a goal of moving more data effectively efficiently and with high performance to assist in businesscritical projects using Talend Data Integration Design developed unit test and support ETL mapping and scripts for data marts using Talend Used HUE for running Hive queries Created partitions according to day using Hive to improve performance Built a data flow pipeline using flume Java MapReduce and Pig Involved in Agile methodologies daily Scrum meetings Sprint planning Experience in using version control tools like GITHUB to share the code snippet among the team members Environment Hadoop Map Reduce HDFS Hive Cassandra Sqoop Oozie SQL Kafka Spark Scala Java AWS GitHub Talend Big Data Integration Solr Impala Hadoop Developer Surgere Inc Green OH September 2016 to November 2017 Since 2004 Surgere has been reinventing how packaging assets influence profitability While raising the bar on packaging supply chain standards Surgere developed a wide variety of visibility and control technologies to increase data accuracy and manage assets packaging parts tools finished vehicles moving through manufacturing supply chain and logistics verticals Responsibilities Good in implementing advanced procedures like text analytics and processing using the inmemory computing capabilities like Apache Spark written in Scala Used Spark for interactive queries processing of streaming data and integration with popular NoSQL database for huge volume of data Developed Spark scripts by using Scala shell commands as per the requirement Enhanced and optimized product Spark code to aggregate group and run data mining tasks using the Spark framework Used Spark RDD for faster Data sharing Experienced in querying data using SparkSQL on top of Spark engine for faster data sets processing Worked on Ad hoc queries Indexing Replication Load balancing Aggregation in MongoDB Extracted and restructured the data into MongoDB using import and export command line utility tool Worked on the largescale Hadoop YARN cluster for distributed data processing and analysis using Hive and MongoDB Wrote XML scripts to build Oozie functionality Experience in workflow Scheduler Oozie to manage and schedule job on Hadoop cluster for generating reports on Day and weekly basis Used Flume to collect aggregate and store the web log data from various sources like web servers mobile and network devices and pushed to HDFS Implemented custom serializer interceptor source and sink in Flume to ingest data from multiple sources Involved in writing query using Impala for better and faster processing of data Responsible for analyzing and cleansing raw data by performing HiveImpala queries and running Pig scripts on data Involved in moving log files generated from various sources to HDFS for further processing through Flume Worked extensively with importing metadata into Hive and migrated existing tables and applications to work on Hive and AWS cloud Worked on partitioning the HIVE table and running the scripts in parallel to reduce the run time of the scripts Analyzed the data by performing Hive queries and running Pig scripts to know user behavior Programmed pig scripts with complex joins like replicated and skewed to achieve better performance Developed data pipeline expending Pig and Java MapReduce to consume customer behavioral data and financial antiquities into HDFS for analysis Designing creating ETL jobs through Talend to load huge volumes of data into MongoDB Hadoop Ecosystem and relational databases Created Talend jobs to connect to Quality Stage using FTP connection and process data received from Quality Stage Migrated data from MySQL server to Hadoop using Sqoop for processing data Exported the analyzed data to the relational databases using Sqoop for visualization and to generate reports for the BI team Experienced in developing Shell scripts and Python scripts for system management Worked with application teams to install operating system Hadoop updates patches version upgrades as required Worked with SCRUM team in delivering agreed user stories on time for every Sprint Environment CDH 3x and 4x Java Hadoop Python Map Reduce Hive Pig Impala Flume MongoDB Sqoop Talend Spark MySQL AWS Hadoop Developer Archimedes Inc San Francisco CA November 2015 to July 2016 Archimedes is a healthcare modelling and analytics organization The company enables people to combine realworld healthcare data and simulation data to create compelling and actionable evidence used in individual healthcare decisionmaking as well as population with applications in health and economic outcomes research policy creation and clinical trial design and operations Responsibilities Worked on migrating MapReduce programs into Spark transformations using Scala Responsible for building scalable distributed data solutions using Hadoop Responsible for building scalable distributed data solutions using Hadoop Responsible for cluster maintenance by adding and removing cluster nodes Cluster monitoring troubleshooting managing and reviewing data backups and log files Wrote complex MapReduce jobs in Java to perform operations by extracting transforming and aggregating to process terabytes of data Collected and aggregated large amounts of stream data into HDFS using Flume and defined channel selectors to multiplex data into different sinks Analyzed data using Hadoop components Hive and Pig Scripted complex HiveQL queries on Hive tables to analyze large datasets and wrote complex Hive UDFs to work with sequence files Scheduled workflows using Oozie to automate multiple Hive and Pig jobs which run independently with time and data availability Responsible for creating Hive tables loading data and writing Hive queries to analyze data Generated reports using QlikView Wrote several Hive queries to get valuable information from the hidden large datasets Loaded and transformed large sets of structured semistructured and unstructured data using HadoopBig Data concepts Handled importing data from various data sources performed transformations using Hive Map Reduce and loaded data into HDFS Imported data from Teradatadatabase into HDFS and exported the analyzed patterns data back to Teradata using Sqoop Worked with Talend Open Studio to perform ETL jobs Environment Hadoop Hortonworks HDFS Hive Pig Sqoop Map Reduce HBase Shell Scripting QlikView Teradata 14 Oozie Java 7 Maven 3x JavaHadoop Developer Evidera Bethesda MD August 2014 to October 2015 Evidera provides health economics outcomes research market access data analytics and epidemiology researchand consulting services We partner with life science organization worldwide to optimize the market access and commercial success of their products Responsibilities Involved in Installation and configuration of JDK Hadoop Pig Sqoop Hive HBase on Linux environment Assisted with performance tuning and monitoring Imported data using Sqoop to load data from MySQL to HDFS on regular basis Created reports for the BI team using Sqoop to export data into HDFS and Hive Worked on creating MapReduceprograms to parse the data for claim report generation and running the Jars in Hadoop Coordinated with Java team in creating MapReduce programs Worked on creating Pig scripts for most modules to give a comparison effort estimation on code development Collaborated with BI teams to ensure data quality and availability with live visualization Created HIVE Queries to process large sets of structured semistructured and unstructured data and store in Managed and External tables Created HBase tables to store variable data formats coming from different portfolios Performed realtime analytics on HBase using Java API and Rest API Performed test run of the module components to understand the productivity Written Java program to retrieve data from HDFS and providing REST services Shared responsibility and assistance for administration of Hadoop Hive Sqoop HBase and Pig in team Shared the knowledge of Hadoop concepts with team members Used JUnit for unit testing and Continuum for integration testing Environment Cloudera Hadoop Pig Sqoop Hive HBase Java Eclipse MySQL MapReduce Java Developer Metanoia software Solutions January 2012 to May 2014 Metanoia offers an Ecommerce platform designed and developed for rapid deployment of Ecommerce services by small and medium size organizations Ecloud is a strong and vibrant deployment option and business model for ecommerce solution Our cloud system is a cost effective alternative designed to address the diverse needs of companies of varying sizes across industries Responsibilities Responsible for the analyzing documenting the requirements designing and developing the application based on J2EE standards Strictly Followed Test Driven Development Used Microsoft Visio for designing use cases like Class Diagrams Sequence Diagrams and Data Models Extensively developed user interface using HTML JavaScript jQuery AJAX and CSSon the front end Designed Rich Internet Application by implementing jQuery based accordion styles Used JavaScript for the clientside web page validation Used Spring MVC and Dependency Injection for handling presentation and business logic Integrated Spring DAO for data access using Hibernate Developed Struts web forms and actions for validation of user request data and application functionality Developed programs for accessing the database using JDBC thin driver to execute queries prepared statements Stored Procedures and to manipulate the data in the database Created tile definitions Struts configuration files validation files and resource bundles for all modules using Struts framework Involved in the coding and integration of several businesscritical modules using Java JSF and Hibernate Developed SOAPbased web services for communication between its upstream applications Implemented different Design patterns like DAO Singleton Pattern and MVC architectural design pattern of spring Implemented Service Oriented Architecture SOA on Enterprise Service Bus ESB Developed MessageDriven Beans for asynchronous processing of alerts using JMS Implemented Rational Rose tool for application development Used Clear case for source code control and JUnit for unit testing Performed integration testing of the modules Used putty for UNIX login to run the batch jobs and check server logs Deployed application on to Glassfish Server Involved in peer code reviews Environment Java 6 7 J2EE Struts 2 Glassfish JSP JDBC EJB ANT XML IBM Web Sphere JUnit IBM DB2 Rational Rose 7 CVS UNIX SOAP SQL PLSQL Java Developer Thirdware Solutions Limited July 2009 to December 2011 Thirdware delivers industryspecific technological expertise through a range of services spanning business applications consulting design implementation and support It is involved in developing Smart Data solutions yielding clean organized actionable data to extract information and insight Responsibilities Documented functional and technical requirements wrote Technical Design Documents Developed analysis level documentation such as Use Case Business Domain Model Activity Sequence and Class Diagrams Developed presentation layer components comprising of JSP AJAX Servlets and JavaBeans using the Struts framework Implemented MVC Model View Controller architecture Developed XML configuration and data description using Hibernate Developed Web services usingCXF to interact with Mainframe applications Responsible for the deployment of the application in the development environment using BEA WebLogic 90 application server Participated in the configuration of BEA WebLogic application server Designed and developed front end user interface using HTML and Java Server Pages JSP for customer profile setup Developed ANT Script to compile the Java files and to build the jars and wars Responsible for Analysis Coding and Unit Testing and Production Support Used JUnit for testing Modules Environment Java 16 J2EE JDBC Struts Framework Hibernate Servlets MVC JSP Web Services CXF SOAP BEA WebLogic 9 Oracle 9i JavaScript XML HTML Ant JUnit SVN My Eclipse Skills JAVA 8 years JUNIT 6 years AJAX 4 years HTML 4 years JAVASCRIPT 4 years Additional Information Technical Skills Hadoop Distributions Cloudera CDH3 CDH4 and CDH5 Hortonworks MapR and Apache Big Data Ecosystem Hadoop MapReduce Pig Hive YARN Kafka Flume Sqoop Impala Oozie Zookeeper Spark Ambari Mahout MongoDB Cassandra Avro Storm Parquet and Snappy Languages Java Python Scala Java Technologies Servlets JavaBeans JSP JDBC and Spring MVC Web Design Tools HTML DHTML AJAX JavaScript JQuery and CSS AngularJS ExtJS and JSON No SQL Databases Cassandra MongoDB and HBase Development Build Tools Eclipse Ant Maven IntelliJ JUNIT and log4J ETL Tools Talend Informatica DB Languages MySQL PLSQL PostgreSQL and Oracle RDBMS Teradata Oracle 9i 10g 11i MS SQL Server MySQL and DB2 Operating systems UNIX LINUX Mac os and Windows Variants Data analytical tools R and MATLAB ETL Tools Talend Informatica Pentaho",
    "unique_id": "ddae1e81-3085-439e-b056-328f82b3fa20"
}