{
    "clean_data": "Sr Hadoop Spark Developer Sr Hadoop Spark span lDeveloperspan Sr Hadoop Spark Developer Atlanta GA Over all7years of progressive experience in the IT industry with proven expertise in architecting and implementing Software Solutions using JavaBig Data technologies 4 years of experience on Batch Analytics using Hadoop working environment includes Map Reduce HDFS Hive Pig HBase Oozie Kafka SparkandSqoop Worked extensively in Real time analytics using Storm and SparkStreamingUsed ingestion tools like Kafka and Sqoop Worked extensively with NoSql databases like Cassandra and HBase In depth understanding of Hadoop Architecture and its various components such as Resource ManagerNode Manager Applications Master Name Node Data Node concepts Experience in importing and exporting data using Sqoop from Relational DatabaseSystems to HDFS andviceversa Extending HIVE and PIG core functionality by using custom User Defined Functions UDF User Defined TableGenerating Functions UDTF and User Defined Aggregating Functions UDAF for Hive and Pig Designed and built user interface using spring and JavaScript employed collection libraries Designed a website for understanding the user requirements and validated the web page using JQuery in Conjunction with Java Spring Hibernate PHP Experience in analyzing data using HiveQL Pig Latin and custom MapReduce programs in Java Developed Pig Latin scripts for data cleansing and Transformation Job workflow scheduling and monitoring using tools like Oozie IBM Tivoli Created HBase tables to load large sets of structured semistructured and unstructured data coming from different sources like Storm and Spark Good experience in Cloudera Horton worksMapRApache Hadoop distributions Worked with relational database systems RDBMS such as MySQL MSSQL OracleRelational database systems like HBase and Cassandra Assisted with performance tuning and monitoring of Kafka HBase Storm Pig and Hive Used Shell Scripting to move log files into HDFS Good hands on experience in creating the RDDs DFs for the required input data and performed the data transformations using Spark Developed Scala scripts UDFs using both Data framesSQL and RDDMapReduce in Spark for Data Aggregation queries and writing data back into RDBMS through Sqoop Good understanding in processing of realtime data using SparkImport the data from different sources like HDFSHBase into Spark RDD Experience in writing MapReduce jobs in python for some complicated queries Experienced with different file formats like Parquet ORC CSV Text Sequence XML JSON and Avro files Good knowledge on Data Modelling and Data Mining to model the data as per business requirements Involved in unit testing of Map Reduce programs using Apache MRunit Good knowledge on python scripting and bash scripting languages Expert in Data Visualization development using Tableau to create complex and innovative dashboards Extensively used Java and J2EE technologies like Core Java Java Beans Servlet JSP spring Hibernate JDBC JSON Object and Design Patterns Experienced in Application Development using Java J2EE JSP Servlets RDBMS Tag Libraries JDBC Hibernate and XML Worked with different software version control Jira bug tracking and code review systems like CVS Clear Case Authorized to work in the US for any employer Work Experience Sr Hadoop Spark Developer American Express Phoenix AZ August 2016 to July 2017 Description American express is a global services company that provides customers with access to products insights and experiences that enrich lives and build business successThe company is best known for its credit card charge card and travelers Cheque businesses Storm Kafka and Hbase are used here to processing the users credit card charge card and cheques business data for processing all the data in the real time UI layer is developed for retrieving the data from Hbase Responsibilities Designed the solution using Storm Spouts to stream data from Kafka and Bolts connecting to Java APIs developed independently based on the application logic Imported bulk data into HBase Using Map Reduce programs Written Storm topology to accept the events from Kafka producer and emit into HBase Developed a data pipeline using Kafka and Strom to store data into HDFS Developed HDFS with huge amounts of data using Apache Kafka Implemented a proof of concept Pocs using Kafka Strom HBase for processing streaming data Created HBase tables to load large sets of structured semistructured and unstructured data coming from UNIX NoSQL and a variety of portfolios Development of software using core java with integration of Apache Storm Apache Kafka Integrated Oozie with the rest of the Hadoop stack supporting several types ofHadoop jobs out of the box such as MapReduce Pig Hive and Sqoop as well as system specific jobs such as Java programs and shell scripts Exploring with Spark improving the performance and optimization of the existing algorithms in Hadoopusing Spark context SparkSQLData Frame pair RDDs Spark YARN Developed Spark code and SparkSQLStreaming for faster testing and processing of data Experience in deploying data from various sources into HDFS and building reports using Tableau Performed real time analysis on the incoming data Load the data into Spark RDD and performed inmemory data computation to generate the output response Developed Spark scripts by using Python shell commands as per the requirement Developed Shell scripts and Python programs to automate tasks Environment HadoopMap Reduce HDFS Spark Java Kafka Hive HBase maven Jenkins Pig UNIX Python Git Storm MapR Oozie Sr Hadoop Developer Celgene Corporation July 2016 to July 2016 Description Celgene is a global biopharmaceutical company committed to improving the lives of patients worldwide At Celgene we seek to deliver truly innovative and lifechanging treatments for our patients Responsibilities Imported data from different relational data sources like RDBMS Teradata to HDFS using Sqoop Imported bulk data into HBase Using Map Reduce programs Perform analytics on Time Series Data exists in HBase using HBase API Designed and implemented Incremental Imports into Hive tables Used Rest ApI to Access HBase data to perform analytics Developed Spark code using Scala and SparkSQLStreaming for faster testing and processing of data Involved in converting Map Reduce programs intoSpark transformations using Spark RDDs on Scala Implemented Spark using Scala and Spark SQL for faster testing and processing of data Experienced in working with various kinds of data sources such as Teradata and Oracle Successfully loaded files to HDFS from Teradata and load loaded from HDFSto hive and impala Experienced in running query using Impala and used BI tools to run adhoc queries directly on Hadoop Experienced with batch processing of data sources using Apache SparkElastic search Develop wrapper using shell scripting for Hive Pig Sqoop Scala jobs Worked on developing Unix Shell scripts to automate SparkSql Performed advanced procedures like text analytics and processing using the inmemory computing capabilities of Spark using Scala Worked in Loading and transforming large sets of structured semi structured and unstructured data Involved in collecting aggregating and moving data from servers to HDFS using Apache Flume Written Hive jobs to parse the logs and structure them in tabular format to facilitate effective querying on the log data Involved in creating Hive tables loading with data and writing hive queries that will run internally in MapReduce way Developed java Restfulweb services to upload data from local to Amazon S3 listing S3 objects and file manipulation operations Configured a 2030 node Amazon EC2 spot instance Hadoop cluster to transfer the data from Amazon S3 to HDFS and HDFS to Amazon S3 and also to direct input and output to the Hadoop MapReduce framework Experienced in managing and reviewing the Hadoop log files Successfully ran all Hadoop MapReduce programs on Amazon Elastic MapReduce framework by using Amazon S3 for input and output Involve in Data Asset Inventory to gather analyze and document business requirements functional requirements and data specifications for Member Retention from sources SQL Hadoop Worked on solving performance and limit queries to the workbooks that when it connects to live database by using a data extract option in Tableau Designed and developed Dashboards for Analytical purposes using Tableau Designed and implemented facts dimensions measure groups measures and OLAP cubes using dimensional data modeling standards in SQL Server 2008 that maintained data Creating and Designing OLAP using SAS OLAP Cube Studio Designing Source Job Target using SAS OLAP Cube Studio and SASDIS Analyzing OLAP Using SAS OLAP Viewer and SAS Dataset using SASEG Migrated ETL jobs to Pig scripts do Transformations even joins and some preaggregations before storing the data onto HDFS Worked with Avro Data Serialization system to work with JSON data formatselastic search Worked on different file formats like Sequence files XML files and Map files usingMap ReducePrograms Involved in Unit testing and delivered Unit test plans and results documents using Junit and MRUnit Exported data from HDFS environment into RDBMS using Sqoop for report generation and visualization purpose Worked on Oozie workflow engine for job scheduling Created and maintained Technical documentation for launching HADOOP Clusters and for executing Pig Scripts Environment CDH 53 Map Reduce Hive014 Spark 141 Oozie Sqoop Pig011 Java Rest API Maven MRunit Junit Tableau Cloudera Python Java DeveloperHadoop Developer CNAINSURANCE Chicago IL January 2014 to March 2015 Description CNA insurance provided B2B insurance to their customers As part of enhancements we developed ELS Enterprise Logging Service to provide statistics to support team and implemented Processor to send alerts to Support Teams Responsibilities Participated in requirement gathering and converting the requirements into technical specifications Created UML diagrams like use cases class diagrams interaction diagrams and activity diagrams Developed the application using Spring Framework that leverages classical Model View Controller MVC architecture Created Business Logic using Servlets POJOs and deployed them on Web logic server Wrote complex SQL queries and stored procedures Developed the XML Schema and Web services for the data maintenance and structures Implemented the Web Service client for the login authentication credit reports and applicant information using Apache Axis 2 Web Service Developed and implemented custom data validation stored procedures for metadata summarization for the data warehouse tables for aggregating telephone subscribers switching data for identifying winning and losing carriers and for identifying value subscribers Identified issue and developed a procedure for correcting the problem which resulted in the improved quality of critical tables by eliminating the possibility of entering duplicate data in a Data Warehouse Designed and implemented SQL based tools stored procedures and functions for daily data volume and aggregation status Responsible to manage data coming from different sources Developed map reduce algorithms Got good experience with NOSQL database Involved in loading data from UNIX file system to HDFS Installed and configured Hive and also written Hive UDFs Worked with cloud services like Amazon web services AWS Designed the logical and physical data model generated DDL scripts and wrote DML scripts for Oracle 10g database Used Hibernate ORM framework with Spring framework for data persistence and transaction management Wrote test cases in JUnit for unit testing of classes Involved in creating templates and screens in HTML and JavaScript Involved in integrating Web Services using SOAP Environment Hive 071 Apache Solr 3x HBase090x020x JDK Spring MVC WebSphere 61 HTML XML JavaScript JUnit 38 Oracle 10g Amazon Web Services Java Developer INTEQ SOFTWARE PVT LTD Hyderabad Telangana January 2012 to December 2013 Description This project deals with the development of automotive protocol for vehicles based on serial interfaces of ECU The protocol development has to finally test by the automotive tools to confirm the message integrity Responsibilities Using JAVA developed a website for eRecruitment consists of many modules Followed MVC Architecture for implementing the functionality Designed and reviewed the test scenarios and scripts for given functional requirements Implemented Services using Core Java Involved in development of classes using java Designed and built user interface using spring and JavaScript employed collection libraries Designed and involved in preparing activity diagrams usecasediagrams sequence diagrams as per the business requirement Used JavaScript for Client validation Designed a website for understanding the user requirements and validated the web page using JQuery in Conjunction with Java Spring Hibernate PHP Developed user interfaces using Servlets CSS XSLT XML HTML and JavaScript Good proficiency in developing algorithms for serial interfaces Involved in testing of CAN protocols Developed the flow of algorithm in UML Developed verification and validation scripts in java Environment Java JSP Servlets JDBC JavaScript MySQL JUnit Eclipse IDE Windows 7XPVista UNIX LINUX Education Bachelors Skills Html 3 years java 4 years Javascript 3 years Model view controller 3 years Xml 3 years",
    "entities": [
        "Resource",
        "Sqoop Imported",
        "Spring Framework",
        "BI",
        "UNIX",
        "Developed Spark",
        "MapReduce Pig Hive",
        "Tableau Designed",
        "Responsibilities Imported",
        "Hadoop",
        "Oozie Sqoop",
        "XML",
        "JavaBig Data",
        "Atlanta",
        "Telangana",
        "NOSQL",
        "Sr Hadoop Spark Developer Sr Hadoop Spark",
        "JUnit",
        "Shell",
        "Amazon Elastic MapReduce",
        "HBase",
        "Java Spring Hibernate PHP Developed",
        "Amazon S3",
        "Amazon",
        "Storm and Spark Good",
        "SQL Server",
        "Responsibilities Using JAVA",
        "Avro Data Serialization",
        "Relational DatabaseSystems",
        "SASDIS Analyzing OLAP",
        "Time Series Data",
        "Created Business Logic using Servlets",
        "SparkImport",
        "Hadoop MapReduce",
        "Created UML",
        "DDL",
        "Sequence",
        "Spark for Data Aggregation",
        "Data Mining",
        "Develop",
        "the Hadoop MapReduce",
        "JavaScript Involved",
        "Oozie IBM Tivoli Created HBase",
        "HTML XML JavaScript JUnit",
        "SASEG Migrated ETL",
        "HDFS Developed HDFS",
        "Cloudera Horton",
        "Oracle 10",
        "Incremental Imports",
        "Spark",
        "Apache Storm Apache Kafka Integrated Oozie",
        "Implemented Services",
        "Data Visualization",
        "Data Asset Inventory",
        "ReducePrograms Involved",
        "Java DeveloperHadoop Developer",
        "US",
        "Sqoop",
        "HIVE",
        "Hadoop Experienced",
        "Work Experience Sr Hadoop Spark Developer American Express",
        "Storm",
        "NoSql",
        "Created",
        "Cheque",
        "Scala",
        "Hadoop Architecture",
        "Oracle",
        "PIG",
        "Access HBase",
        "HTML",
        "SAS",
        "Transformation Job",
        "Application Development",
        "Dashboards for Analytical",
        "SQL",
        "Software Solutions",
        "Spark RDD",
        "Amazon Web Services",
        "DML",
        "Java Spring Hibernate PHP",
        "MVC Architecture",
        "HADOOP Clusters",
        "SQL Hadoop Worked",
        "Hbase Responsibilities Designed",
        "Loading",
        "Hadoopusing Spark",
        "eRecruitment",
        "Data Modelling",
        "OLAP",
        "Impala",
        "JavaScript",
        "UI",
        "Apache SparkElastic",
        "CVS Clear Case Authorized",
        "PVT LTD Hyderabad",
        "UML Developed",
        "Spark RDD Experience",
        "Bolts",
        "Created HBase",
        "SparkSql Performed",
        "Data",
        "MapReduce",
        "NoSQL",
        "Tableau",
        "Oracle Successfully",
        "ELS Enterprise Logging Service",
        "JQuery",
        "Teradata",
        "Description American",
        "HBase Developed",
        "IDE Windows 7XPVista",
        "Servlets CSS XSLT XML HTML"
    ],
    "experience": "Experience in importing and exporting data using Sqoop from Relational DatabaseSystems to HDFS andviceversa Extending HIVE and PIG core functionality by using custom User Defined Functions UDF User Defined TableGenerating Functions UDTF and User Defined Aggregating Functions UDAF for Hive and Pig Designed and built user interface using spring and JavaScript employed collection libraries Designed a website for understanding the user requirements and validated the web page using JQuery in Conjunction with Java Spring Hibernate PHP Experience in analyzing data using HiveQL Pig Latin and custom MapReduce programs in Java Developed Pig Latin scripts for data cleansing and Transformation Job workflow scheduling and monitoring using tools like Oozie IBM Tivoli Created HBase tables to load large sets of structured semistructured and unstructured data coming from different sources like Storm and Spark Good experience in Cloudera Horton worksMapRApache Hadoop distributions Worked with relational database systems RDBMS such as MySQL MSSQL OracleRelational database systems like HBase and Cassandra Assisted with performance tuning and monitoring of Kafka HBase Storm Pig and Hive Used Shell Scripting to move log files into HDFS Good hands on experience in creating the RDDs DFs for the required input data and performed the data transformations using Spark Developed Scala scripts UDFs using both Data framesSQL and RDDMapReduce in Spark for Data Aggregation queries and writing data back into RDBMS through Sqoop Good understanding in processing of realtime data using SparkImport the data from different sources like HDFSHBase into Spark RDD Experience in writing MapReduce jobs in python for some complicated queries Experienced with different file formats like Parquet ORC CSV Text Sequence XML JSON and Avro files Good knowledge on Data Modelling and Data Mining to model the data as per business requirements Involved in unit testing of Map Reduce programs using Apache MRunit Good knowledge on python scripting and bash scripting languages Expert in Data Visualization development using Tableau to create complex and innovative dashboards Extensively used Java and J2EE technologies like Core Java Java Beans Servlet JSP spring Hibernate JDBC JSON Object and Design Patterns Experienced in Application Development using Java J2EE JSP Servlets RDBMS Tag Libraries JDBC Hibernate and XML Worked with different software version control Jira bug tracking and code review systems like CVS Clear Case Authorized to work in the US for any employer Work Experience Sr Hadoop Spark Developer American Express Phoenix AZ August 2016 to July 2017 Description American express is a global services company that provides customers with access to products insights and experiences that enrich lives and build business successThe company is best known for its credit card charge card and travelers Cheque businesses Storm Kafka and Hbase are used here to processing the users credit card charge card and cheques business data for processing all the data in the real time UI layer is developed for retrieving the data from Hbase Responsibilities Designed the solution using Storm Spouts to stream data from Kafka and Bolts connecting to Java APIs developed independently based on the application logic Imported bulk data into HBase Using Map Reduce programs Written Storm topology to accept the events from Kafka producer and emit into HBase Developed a data pipeline using Kafka and Strom to store data into HDFS Developed HDFS with huge amounts of data using Apache Kafka Implemented a proof of concept Pocs using Kafka Strom HBase for processing streaming data Created HBase tables to load large sets of structured semistructured and unstructured data coming from UNIX NoSQL and a variety of portfolios Development of software using core java with integration of Apache Storm Apache Kafka Integrated Oozie with the rest of the Hadoop stack supporting several types ofHadoop jobs out of the box such as MapReduce Pig Hive and Sqoop as well as system specific jobs such as Java programs and shell scripts Exploring with Spark improving the performance and optimization of the existing algorithms in Hadoopusing Spark context SparkSQLData Frame pair RDDs Spark YARN Developed Spark code and SparkSQLStreaming for faster testing and processing of data Experience in deploying data from various sources into HDFS and building reports using Tableau Performed real time analysis on the incoming data Load the data into Spark RDD and performed inmemory data computation to generate the output response Developed Spark scripts by using Python shell commands as per the requirement Developed Shell scripts and Python programs to automate tasks Environment HadoopMap Reduce HDFS Spark Java Kafka Hive HBase maven Jenkins Pig UNIX Python Git Storm MapR Oozie Sr Hadoop Developer Celgene Corporation July 2016 to July 2016 Description Celgene is a global biopharmaceutical company committed to improving the lives of patients worldwide At Celgene we seek to deliver truly innovative and lifechanging treatments for our patients Responsibilities Imported data from different relational data sources like RDBMS Teradata to HDFS using Sqoop Imported bulk data into HBase Using Map Reduce programs Perform analytics on Time Series Data exists in HBase using HBase API Designed and implemented Incremental Imports into Hive tables Used Rest ApI to Access HBase data to perform analytics Developed Spark code using Scala and SparkSQLStreaming for faster testing and processing of data Involved in converting Map Reduce programs intoSpark transformations using Spark RDDs on Scala Implemented Spark using Scala and Spark SQL for faster testing and processing of data Experienced in working with various kinds of data sources such as Teradata and Oracle Successfully loaded files to HDFS from Teradata and load loaded from HDFSto hive and impala Experienced in running query using Impala and used BI tools to run adhoc queries directly on Hadoop Experienced with batch processing of data sources using Apache SparkElastic search Develop wrapper using shell scripting for Hive Pig Sqoop Scala jobs Worked on developing Unix Shell scripts to automate SparkSql Performed advanced procedures like text analytics and processing using the inmemory computing capabilities of Spark using Scala Worked in Loading and transforming large sets of structured semi structured and unstructured data Involved in collecting aggregating and moving data from servers to HDFS using Apache Flume Written Hive jobs to parse the logs and structure them in tabular format to facilitate effective querying on the log data Involved in creating Hive tables loading with data and writing hive queries that will run internally in MapReduce way Developed java Restfulweb services to upload data from local to Amazon S3 listing S3 objects and file manipulation operations Configured a 2030 node Amazon EC2 spot instance Hadoop cluster to transfer the data from Amazon S3 to HDFS and HDFS to Amazon S3 and also to direct input and output to the Hadoop MapReduce framework Experienced in managing and reviewing the Hadoop log files Successfully ran all Hadoop MapReduce programs on Amazon Elastic MapReduce framework by using Amazon S3 for input and output Involve in Data Asset Inventory to gather analyze and document business requirements functional requirements and data specifications for Member Retention from sources SQL Hadoop Worked on solving performance and limit queries to the workbooks that when it connects to live database by using a data extract option in Tableau Designed and developed Dashboards for Analytical purposes using Tableau Designed and implemented facts dimensions measure groups measures and OLAP cubes using dimensional data modeling standards in SQL Server 2008 that maintained data Creating and Designing OLAP using SAS OLAP Cube Studio Designing Source Job Target using SAS OLAP Cube Studio and SASDIS Analyzing OLAP Using SAS OLAP Viewer and SAS Dataset using SASEG Migrated ETL jobs to Pig scripts do Transformations even joins and some preaggregations before storing the data onto HDFS Worked with Avro Data Serialization system to work with JSON data formatselastic search Worked on different file formats like Sequence files XML files and Map files usingMap ReducePrograms Involved in Unit testing and delivered Unit test plans and results documents using Junit and MRUnit Exported data from HDFS environment into RDBMS using Sqoop for report generation and visualization purpose Worked on Oozie workflow engine for job scheduling Created and maintained Technical documentation for launching HADOOP Clusters and for executing Pig Scripts Environment CDH 53 Map Reduce Hive014 Spark 141 Oozie Sqoop Pig011 Java Rest API Maven MRunit Junit Tableau Cloudera Python Java DeveloperHadoop Developer CNAINSURANCE Chicago IL January 2014 to March 2015 Description CNA insurance provided B2B insurance to their customers As part of enhancements we developed ELS Enterprise Logging Service to provide statistics to support team and implemented Processor to send alerts to Support Teams Responsibilities Participated in requirement gathering and converting the requirements into technical specifications Created UML diagrams like use cases class diagrams interaction diagrams and activity diagrams Developed the application using Spring Framework that leverages classical Model View Controller MVC architecture Created Business Logic using Servlets POJOs and deployed them on Web logic server Wrote complex SQL queries and stored procedures Developed the XML Schema and Web services for the data maintenance and structures Implemented the Web Service client for the login authentication credit reports and applicant information using Apache Axis 2 Web Service Developed and implemented custom data validation stored procedures for metadata summarization for the data warehouse tables for aggregating telephone subscribers switching data for identifying winning and losing carriers and for identifying value subscribers Identified issue and developed a procedure for correcting the problem which resulted in the improved quality of critical tables by eliminating the possibility of entering duplicate data in a Data Warehouse Designed and implemented SQL based tools stored procedures and functions for daily data volume and aggregation status Responsible to manage data coming from different sources Developed map reduce algorithms Got good experience with NOSQL database Involved in loading data from UNIX file system to HDFS Installed and configured Hive and also written Hive UDFs Worked with cloud services like Amazon web services AWS Designed the logical and physical data model generated DDL scripts and wrote DML scripts for Oracle 10 g database Used Hibernate ORM framework with Spring framework for data persistence and transaction management Wrote test cases in JUnit for unit testing of classes Involved in creating templates and screens in HTML and JavaScript Involved in integrating Web Services using SOAP Environment Hive 071 Apache Solr 3x HBase090x020x JDK Spring MVC WebSphere 61 HTML XML JavaScript JUnit 38 Oracle 10 g Amazon Web Services Java Developer INTEQ SOFTWARE PVT LTD Hyderabad Telangana January 2012 to December 2013 Description This project deals with the development of automotive protocol for vehicles based on serial interfaces of ECU The protocol development has to finally test by the automotive tools to confirm the message integrity Responsibilities Using JAVA developed a website for eRecruitment consists of many modules Followed MVC Architecture for implementing the functionality Designed and reviewed the test scenarios and scripts for given functional requirements Implemented Services using Core Java Involved in development of classes using java Designed and built user interface using spring and JavaScript employed collection libraries Designed and involved in preparing activity diagrams usecasediagrams sequence diagrams as per the business requirement Used JavaScript for Client validation Designed a website for understanding the user requirements and validated the web page using JQuery in Conjunction with Java Spring Hibernate PHP Developed user interfaces using Servlets CSS XSLT XML HTML and JavaScript Good proficiency in developing algorithms for serial interfaces Involved in testing of CAN protocols Developed the flow of algorithm in UML Developed verification and validation scripts in java Environment Java JSP Servlets JDBC JavaScript MySQL JUnit Eclipse IDE Windows 7XPVista UNIX LINUX Education Bachelors Skills Html 3 years java 4 years Javascript 3 years Model view controller 3 years Xml 3 years",
    "extracted_keywords": [
        "Sr",
        "Hadoop",
        "Spark",
        "Developer",
        "Sr",
        "Hadoop",
        "Spark",
        "span",
        "lDeveloperspan",
        "Sr",
        "Hadoop",
        "Spark",
        "Developer",
        "Atlanta",
        "GA",
        "all7years",
        "experience",
        "IT",
        "industry",
        "expertise",
        "Software",
        "Solutions",
        "JavaBig",
        "Data",
        "technologies",
        "years",
        "experience",
        "Batch",
        "Analytics",
        "Hadoop",
        "working",
        "environment",
        "Map",
        "Reduce",
        "HDFS",
        "Hive",
        "Pig",
        "HBase",
        "Oozie",
        "Kafka",
        "SparkandSqoop",
        "time",
        "analytics",
        "Storm",
        "ingestion",
        "tools",
        "Kafka",
        "Sqoop",
        "NoSql",
        "Cassandra",
        "HBase",
        "depth",
        "understanding",
        "Hadoop",
        "Architecture",
        "components",
        "Resource",
        "ManagerNode",
        "Manager",
        "Applications",
        "Master",
        "Name",
        "Node",
        "Data",
        "Node",
        "Experience",
        "data",
        "Sqoop",
        "Relational",
        "DatabaseSystems",
        "HDFS",
        "andviceversa",
        "HIVE",
        "PIG",
        "core",
        "functionality",
        "custom",
        "User",
        "Defined",
        "Functions",
        "UDF",
        "User",
        "Defined",
        "TableGenerating",
        "Functions",
        "UDTF",
        "User",
        "Defined",
        "Aggregating",
        "Functions",
        "UDAF",
        "Hive",
        "Pig",
        "user",
        "interface",
        "spring",
        "JavaScript",
        "collection",
        "libraries",
        "website",
        "user",
        "requirements",
        "web",
        "page",
        "JQuery",
        "Conjunction",
        "Java",
        "Spring",
        "Hibernate",
        "PHP",
        "Experience",
        "data",
        "HiveQL",
        "Pig",
        "Latin",
        "MapReduce",
        "programs",
        "Java",
        "Developed",
        "Pig",
        "Latin",
        "scripts",
        "data",
        "cleansing",
        "Transformation",
        "Job",
        "workflow",
        "scheduling",
        "monitoring",
        "tools",
        "Oozie",
        "IBM",
        "Tivoli",
        "Created",
        "HBase",
        "sets",
        "data",
        "sources",
        "Storm",
        "Spark",
        "Good",
        "experience",
        "Cloudera",
        "Horton",
        "worksMapRApache",
        "Hadoop",
        "distributions",
        "database",
        "systems",
        "MySQL",
        "MSSQL",
        "OracleRelational",
        "database",
        "systems",
        "HBase",
        "Cassandra",
        "Assisted",
        "performance",
        "tuning",
        "monitoring",
        "Kafka",
        "HBase",
        "Storm",
        "Pig",
        "Hive",
        "Shell",
        "Scripting",
        "files",
        "hands",
        "experience",
        "RDDs",
        "DFs",
        "input",
        "data",
        "data",
        "transformations",
        "Spark",
        "Developed",
        "Scala",
        "scripts",
        "UDFs",
        "Data",
        "framesSQL",
        "RDDMapReduce",
        "Spark",
        "Data",
        "Aggregation",
        "data",
        "RDBMS",
        "Sqoop",
        "Good",
        "understanding",
        "processing",
        "data",
        "SparkImport",
        "data",
        "sources",
        "HDFSHBase",
        "Spark",
        "RDD",
        "Experience",
        "MapReduce",
        "jobs",
        "python",
        "queries",
        "file",
        "formats",
        "Parquet",
        "ORC",
        "CSV",
        "Text",
        "Sequence",
        "Avro",
        "knowledge",
        "Data",
        "Modelling",
        "Data",
        "Mining",
        "data",
        "business",
        "requirements",
        "unit",
        "testing",
        "Map",
        "Reduce",
        "programs",
        "Apache",
        "MRunit",
        "knowledge",
        "scripting",
        "bash",
        "scripting",
        "languages",
        "Expert",
        "Data",
        "Visualization",
        "development",
        "Tableau",
        "dashboards",
        "Java",
        "J2EE",
        "technologies",
        "Core",
        "Java",
        "Java",
        "Beans",
        "Servlet",
        "JSP",
        "spring",
        "Hibernate",
        "JDBC",
        "JSON",
        "Object",
        "Design",
        "Patterns",
        "Application",
        "Development",
        "Java",
        "J2EE",
        "JSP",
        "Servlets",
        "RDBMS",
        "Tag",
        "JDBC",
        "Hibernate",
        "XML",
        "software",
        "version",
        "control",
        "Jira",
        "bug",
        "tracking",
        "code",
        "review",
        "systems",
        "CVS",
        "Clear",
        "Case",
        "US",
        "employer",
        "Work",
        "Experience",
        "Sr",
        "Hadoop",
        "Spark",
        "Developer",
        "American",
        "Express",
        "Phoenix",
        "AZ",
        "August",
        "July",
        "Description",
        "express",
        "services",
        "company",
        "customers",
        "access",
        "products",
        "insights",
        "experiences",
        "business",
        "company",
        "credit",
        "card",
        "charge",
        "card",
        "travelers",
        "businesses",
        "Storm",
        "Kafka",
        "Hbase",
        "users",
        "credit",
        "card",
        "charge",
        "card",
        "business",
        "data",
        "data",
        "time",
        "UI",
        "layer",
        "data",
        "Hbase",
        "Responsibilities",
        "solution",
        "Storm",
        "Spouts",
        "data",
        "Kafka",
        "Bolts",
        "Java",
        "APIs",
        "application",
        "logic",
        "data",
        "HBase",
        "Map",
        "Reduce",
        "programs",
        "Storm",
        "topology",
        "events",
        "Kafka",
        "producer",
        "HBase",
        "data",
        "pipeline",
        "Kafka",
        "Strom",
        "data",
        "HDFS",
        "HDFS",
        "amounts",
        "data",
        "Apache",
        "Kafka",
        "proof",
        "concept",
        "Pocs",
        "Kafka",
        "Strom",
        "HBase",
        "streaming",
        "data",
        "Created",
        "HBase",
        "sets",
        "data",
        "UNIX",
        "NoSQL",
        "variety",
        "portfolios",
        "Development",
        "software",
        "core",
        "integration",
        "Apache",
        "Storm",
        "Apache",
        "Kafka",
        "Integrated",
        "Oozie",
        "rest",
        "Hadoop",
        "stack",
        "types",
        "ofHadoop",
        "jobs",
        "box",
        "MapReduce",
        "Pig",
        "Hive",
        "Sqoop",
        "system",
        "jobs",
        "Java",
        "programs",
        "scripts",
        "Spark",
        "performance",
        "optimization",
        "algorithms",
        "Hadoopusing",
        "Spark",
        "context",
        "SparkSQLData",
        "Frame",
        "pair",
        "RDDs",
        "Spark",
        "YARN",
        "Spark",
        "code",
        "SparkSQLStreaming",
        "testing",
        "processing",
        "data",
        "Experience",
        "data",
        "sources",
        "HDFS",
        "building",
        "reports",
        "Tableau",
        "Performed",
        "time",
        "analysis",
        "data",
        "data",
        "Spark",
        "RDD",
        "data",
        "computation",
        "output",
        "response",
        "Developed",
        "Spark",
        "scripts",
        "Python",
        "shell",
        "commands",
        "requirement",
        "Developed",
        "Shell",
        "scripts",
        "Python",
        "programs",
        "tasks",
        "Environment",
        "HadoopMap",
        "HDFS",
        "Spark",
        "Java",
        "Kafka",
        "Hive",
        "HBase",
        "Jenkins",
        "Pig",
        "UNIX",
        "Python",
        "Git",
        "Storm",
        "MapR",
        "Oozie",
        "Sr",
        "Hadoop",
        "Developer",
        "Celgene",
        "Corporation",
        "July",
        "July",
        "Description",
        "Celgene",
        "company",
        "lives",
        "patients",
        "Celgene",
        "treatments",
        "patients",
        "Responsibilities",
        "data",
        "data",
        "sources",
        "RDBMS",
        "Teradata",
        "HDFS",
        "Sqoop",
        "data",
        "HBase",
        "Map",
        "Reduce",
        "programs",
        "analytics",
        "Time",
        "Series",
        "Data",
        "HBase",
        "HBase",
        "API",
        "Incremental",
        "Imports",
        "Hive",
        "tables",
        "Rest",
        "ApI",
        "Access",
        "HBase",
        "data",
        "analytics",
        "Spark",
        "code",
        "Scala",
        "SparkSQLStreaming",
        "testing",
        "processing",
        "data",
        "Map",
        "Reduce",
        "programs",
        "intoSpark",
        "transformations",
        "Spark",
        "RDDs",
        "Scala",
        "Spark",
        "Scala",
        "Spark",
        "SQL",
        "testing",
        "processing",
        "data",
        "kinds",
        "data",
        "sources",
        "Teradata",
        "Oracle",
        "files",
        "HDFS",
        "Teradata",
        "load",
        "hive",
        "impala",
        "query",
        "Impala",
        "BI",
        "tools",
        "queries",
        "Hadoop",
        "batch",
        "processing",
        "data",
        "sources",
        "Apache",
        "SparkElastic",
        "search",
        "Develop",
        "wrapper",
        "shell",
        "scripting",
        "Hive",
        "Pig",
        "Sqoop",
        "Scala",
        "jobs",
        "Unix",
        "Shell",
        "scripts",
        "SparkSql",
        "procedures",
        "text",
        "analytics",
        "processing",
        "inmemory",
        "computing",
        "capabilities",
        "Spark",
        "Scala",
        "Worked",
        "Loading",
        "sets",
        "data",
        "data",
        "servers",
        "HDFS",
        "Apache",
        "Flume",
        "Written",
        "Hive",
        "jobs",
        "logs",
        "format",
        "querying",
        "log",
        "data",
        "Hive",
        "tables",
        "data",
        "hive",
        "queries",
        "MapReduce",
        "way",
        "Restfulweb",
        "services",
        "data",
        "Amazon",
        "S3",
        "S3",
        "objects",
        "file",
        "manipulation",
        "operations",
        "node",
        "Amazon",
        "EC2",
        "spot",
        "instance",
        "Hadoop",
        "cluster",
        "data",
        "Amazon",
        "S3",
        "HDFS",
        "HDFS",
        "Amazon",
        "S3",
        "input",
        "output",
        "Hadoop",
        "MapReduce",
        "framework",
        "Hadoop",
        "log",
        "files",
        "Hadoop",
        "MapReduce",
        "programs",
        "Amazon",
        "Elastic",
        "MapReduce",
        "framework",
        "Amazon",
        "S3",
        "input",
        "output",
        "Involve",
        "Data",
        "Asset",
        "Inventory",
        "document",
        "business",
        "requirements",
        "requirements",
        "data",
        "specifications",
        "Member",
        "Retention",
        "sources",
        "SQL",
        "Hadoop",
        "performance",
        "queries",
        "workbooks",
        "database",
        "data",
        "extract",
        "option",
        "Tableau",
        "Dashboards",
        "Analytical",
        "purposes",
        "Tableau",
        "facts",
        "dimensions",
        "groups",
        "measures",
        "OLAP",
        "cubes",
        "data",
        "modeling",
        "standards",
        "SQL",
        "Server",
        "data",
        "Creating",
        "Designing",
        "OLAP",
        "SAS",
        "OLAP",
        "Cube",
        "Studio",
        "Designing",
        "Source",
        "Job",
        "Target",
        "SAS",
        "OLAP",
        "Cube",
        "Studio",
        "SASDIS",
        "OLAP",
        "SAS",
        "OLAP",
        "Viewer",
        "SAS",
        "Dataset",
        "ETL",
        "jobs",
        "scripts",
        "Transformations",
        "preaggregations",
        "data",
        "HDFS",
        "Avro",
        "Data",
        "Serialization",
        "system",
        "JSON",
        "data",
        "search",
        "file",
        "formats",
        "Sequence",
        "files",
        "XML",
        "files",
        "Map",
        "files",
        "usingMap",
        "ReducePrograms",
        "Unit",
        "testing",
        "Unit",
        "test",
        "plans",
        "documents",
        "Junit",
        "MRUnit",
        "data",
        "HDFS",
        "environment",
        "RDBMS",
        "Sqoop",
        "report",
        "generation",
        "visualization",
        "purpose",
        "Oozie",
        "workflow",
        "engine",
        "job",
        "scheduling",
        "documentation",
        "HADOOP",
        "Clusters",
        "Pig",
        "Scripts",
        "Environment",
        "CDH",
        "Map",
        "Reduce",
        "Hive014",
        "Spark",
        "Oozie",
        "Sqoop",
        "Pig011",
        "Java",
        "Rest",
        "API",
        "Maven",
        "MRunit",
        "Junit",
        "Tableau",
        "Cloudera",
        "Python",
        "Java",
        "DeveloperHadoop",
        "Developer",
        "CNAINSURANCE",
        "Chicago",
        "IL",
        "January",
        "March",
        "Description",
        "CNA",
        "insurance",
        "B2B",
        "insurance",
        "customers",
        "part",
        "enhancements",
        "ELS",
        "Enterprise",
        "Logging",
        "Service",
        "statistics",
        "team",
        "Processor",
        "alerts",
        "Teams",
        "Responsibilities",
        "requirement",
        "gathering",
        "requirements",
        "specifications",
        "UML",
        "diagrams",
        "use",
        "cases",
        "class",
        "diagrams",
        "interaction",
        "diagrams",
        "activity",
        "diagrams",
        "application",
        "Spring",
        "Framework",
        "Model",
        "View",
        "Controller",
        "MVC",
        "architecture",
        "Created",
        "Business",
        "Logic",
        "Servlets",
        "POJOs",
        "Web",
        "logic",
        "server",
        "Wrote",
        "SQL",
        "queries",
        "procedures",
        "XML",
        "Schema",
        "Web",
        "services",
        "data",
        "maintenance",
        "structures",
        "Web",
        "Service",
        "client",
        "login",
        "authentication",
        "credit",
        "reports",
        "information",
        "Apache",
        "Axis",
        "Web",
        "Service",
        "Developed",
        "custom",
        "data",
        "validation",
        "procedures",
        "metadata",
        "summarization",
        "data",
        "warehouse",
        "tables",
        "telephone",
        "subscribers",
        "data",
        "carriers",
        "value",
        "subscribers",
        "issue",
        "procedure",
        "problem",
        "quality",
        "tables",
        "possibility",
        "data",
        "Data",
        "Warehouse",
        "SQL",
        "tools",
        "procedures",
        "functions",
        "data",
        "volume",
        "aggregation",
        "status",
        "data",
        "sources",
        "map",
        "algorithms",
        "experience",
        "NOSQL",
        "database",
        "loading",
        "data",
        "UNIX",
        "file",
        "system",
        "HDFS",
        "Installed",
        "Hive",
        "Hive",
        "UDFs",
        "cloud",
        "services",
        "Amazon",
        "web",
        "services",
        "AWS",
        "data",
        "model",
        "DDL",
        "scripts",
        "DML",
        "scripts",
        "Oracle",
        "g",
        "database",
        "Hibernate",
        "ORM",
        "framework",
        "Spring",
        "framework",
        "data",
        "persistence",
        "transaction",
        "management",
        "test",
        "cases",
        "JUnit",
        "unit",
        "testing",
        "classes",
        "templates",
        "screens",
        "HTML",
        "JavaScript",
        "Web",
        "Services",
        "SOAP",
        "Environment",
        "Hive",
        "Apache",
        "Solr",
        "3x",
        "JDK",
        "Spring",
        "MVC",
        "WebSphere",
        "HTML",
        "XML",
        "JavaScript",
        "JUnit",
        "Oracle",
        "g",
        "Amazon",
        "Web",
        "Services",
        "Java",
        "Developer",
        "INTEQ",
        "SOFTWARE",
        "PVT",
        "LTD",
        "Hyderabad",
        "Telangana",
        "January",
        "December",
        "Description",
        "project",
        "development",
        "protocol",
        "vehicles",
        "interfaces",
        "ECU",
        "protocol",
        "development",
        "tools",
        "message",
        "integrity",
        "Responsibilities",
        "JAVA",
        "website",
        "eRecruitment",
        "modules",
        "MVC",
        "Architecture",
        "functionality",
        "test",
        "scenarios",
        "scripts",
        "requirements",
        "Services",
        "Core",
        "Java",
        "development",
        "classes",
        "user",
        "interface",
        "spring",
        "JavaScript",
        "collection",
        "libraries",
        "activity",
        "diagrams",
        "sequence",
        "diagrams",
        "business",
        "requirement",
        "JavaScript",
        "Client",
        "validation",
        "website",
        "user",
        "requirements",
        "web",
        "page",
        "JQuery",
        "Conjunction",
        "Java",
        "Spring",
        "Hibernate",
        "PHP",
        "user",
        "interfaces",
        "Servlets",
        "CSS",
        "XSLT",
        "XML",
        "HTML",
        "JavaScript",
        "proficiency",
        "algorithms",
        "interfaces",
        "testing",
        "CAN",
        "protocols",
        "flow",
        "algorithm",
        "UML",
        "verification",
        "validation",
        "scripts",
        "Environment",
        "Java",
        "JSP",
        "Servlets",
        "JDBC",
        "JavaScript",
        "MySQL",
        "JUnit",
        "Eclipse",
        "IDE",
        "Windows",
        "7XPVista",
        "UNIX",
        "LINUX",
        "Education",
        "Bachelors",
        "Skills",
        "Html",
        "years",
        "years",
        "Javascript",
        "years",
        "Model",
        "view",
        "controller",
        "years",
        "Xml",
        "years"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T21:02:16.621550",
    "resume_data": "Sr Hadoop Spark Developer Sr Hadoop Spark span lDeveloperspan Sr Hadoop Spark Developer Atlanta GA Over all7years of progressive experience in the IT industry with proven expertise in architecting and implementing Software Solutions using JavaBig Data technologies 4 years of experience on Batch Analytics using Hadoop working environment includes Map Reduce HDFS Hive Pig HBase Oozie Kafka SparkandSqoop Worked extensively in Real time analytics using Storm and SparkStreamingUsed ingestion tools like Kafka and Sqoop Worked extensively with NoSql databases like Cassandra and HBase In depth understanding of Hadoop Architecture and its various components such as Resource ManagerNode Manager Applications Master Name Node Data Node concepts Experience in importing and exporting data using Sqoop from Relational DatabaseSystems to HDFS andviceversa Extending HIVE and PIG core functionality by using custom User Defined Functions UDF User Defined TableGenerating Functions UDTF and User Defined Aggregating Functions UDAF for Hive and Pig Designed and built user interface using spring and JavaScript employed collection libraries Designed a website for understanding the user requirements and validated the web page using JQuery in Conjunction with Java Spring Hibernate PHP Experience in analyzing data using HiveQL Pig Latin and custom MapReduce programs in Java Developed Pig Latin scripts for data cleansing and Transformation Job workflow scheduling and monitoring using tools like Oozie IBM Tivoli Created HBase tables to load large sets of structured semistructured and unstructured data coming from different sources like Storm and Spark Good experience in Cloudera Horton worksMapRApache Hadoop distributions Worked with relational database systems RDBMS such as MySQL MSSQL OracleRelational database systems like HBase and Cassandra Assisted with performance tuning and monitoring of Kafka HBase Storm Pig and Hive Used Shell Scripting to move log files into HDFS Good hands on experience in creating the RDDs DFs for the required input data and performed the data transformations using Spark Developed Scala scripts UDFs using both Data framesSQL and RDDMapReduce in Spark for Data Aggregation queries and writing data back into RDBMS through Sqoop Good understanding in processing of realtime data using SparkImport the data from different sources like HDFSHBase into Spark RDD Experience in writing MapReduce jobs in python for some complicated queries Experienced with different file formats like Parquet ORC CSV Text Sequence XML JSON and Avro files Good knowledge on Data Modelling and Data Mining to model the data as per business requirements Involved in unit testing of Map Reduce programs using Apache MRunit Good knowledge on python scripting and bash scripting languages Expert in Data Visualization development using Tableau to create complex and innovative dashboards Extensively used Java and J2EE technologies like Core Java Java Beans Servlet JSP spring Hibernate JDBC JSON Object and Design Patterns Experienced in Application Development using Java J2EE JSP Servlets RDBMS Tag Libraries JDBC Hibernate and XML Worked with different software version control Jira bug tracking and code review systems like CVS Clear Case Authorized to work in the US for any employer Work Experience Sr Hadoop Spark Developer American Express Phoenix AZ August 2016 to July 2017 Description American express is a global services company that provides customers with access to products insights and experiences that enrich lives and build business successThe company is best known for its credit card charge card and travelers Cheque businesses Storm Kafka and Hbase are used here to processing the users credit card charge card and cheques business data for processing all the data in the real time UI layer is developed for retrieving the data from Hbase Responsibilities Designed the solution using Storm Spouts to stream data from Kafka and Bolts connecting to Java APIs developed independently based on the application logic Imported bulk data into HBase Using Map Reduce programs Written Storm topology to accept the events from Kafka producer and emit into HBase Developed a data pipeline using Kafka and Strom to store data into HDFS Developed HDFS with huge amounts of data using Apache Kafka Implemented a proof of concept Pocs using Kafka Strom HBase for processing streaming data Created HBase tables to load large sets of structured semistructured and unstructured data coming from UNIX NoSQL and a variety of portfolios Development of software using core java with integration of Apache Storm Apache Kafka Integrated Oozie with the rest of the Hadoop stack supporting several types ofHadoop jobs out of the box such as MapReduce Pig Hive and Sqoop as well as system specific jobs such as Java programs and shell scripts Exploring with Spark improving the performance and optimization of the existing algorithms in Hadoopusing Spark context SparkSQLData Frame pair RDDs Spark YARN Developed Spark code and SparkSQLStreaming for faster testing and processing of data Experience in deploying data from various sources into HDFS and building reports using Tableau Performed real time analysis on the incoming data Load the data into Spark RDD and performed inmemory data computation to generate the output response Developed Spark scripts by using Python shell commands as per the requirement Developed Shell scripts and Python programs to automate tasks Environment HadoopMap Reduce HDFS Spark Java Kafka Hive HBase maven Jenkins Pig UNIX Python Git Storm MapR Oozie Sr Hadoop Developer Celgene Corporation July 2016 to July 2016 Description Celgene is a global biopharmaceutical company committed to improving the lives of patients worldwide At Celgene we seek to deliver truly innovative and lifechanging treatments for our patients Responsibilities Imported data from different relational data sources like RDBMS Teradata to HDFS using Sqoop Imported bulk data into HBase Using Map Reduce programs Perform analytics on Time Series Data exists in HBase using HBase API Designed and implemented Incremental Imports into Hive tables Used Rest ApI to Access HBase data to perform analytics Developed Spark code using Scala and SparkSQLStreaming for faster testing and processing of data Involved in converting Map Reduce programs intoSpark transformations using Spark RDDs on Scala Implemented Spark using Scala and Spark SQL for faster testing and processing of data Experienced in working with various kinds of data sources such as Teradata and Oracle Successfully loaded files to HDFS from Teradata and load loaded from HDFSto hive and impala Experienced in running query using Impala and used BI tools to run adhoc queries directly on Hadoop Experienced with batch processing of data sources using Apache SparkElastic search Develop wrapper using shell scripting for Hive Pig Sqoop Scala jobs Worked on developing Unix Shell scripts to automate SparkSql Performed advanced procedures like text analytics and processing using the inmemory computing capabilities of Spark using Scala Worked in Loading and transforming large sets of structured semi structured and unstructured data Involved in collecting aggregating and moving data from servers to HDFS using Apache Flume Written Hive jobs to parse the logs and structure them in tabular format to facilitate effective querying on the log data Involved in creating Hive tables loading with data and writing hive queries that will run internally in MapReduce way Developed java Restfulweb services to upload data from local to Amazon S3 listing S3 objects and file manipulation operations Configured a 2030 node Amazon EC2 spot instance Hadoop cluster to transfer the data from Amazon S3 to HDFS and HDFS to Amazon S3 and also to direct input and output to the Hadoop MapReduce framework Experienced in managing and reviewing the Hadoop log files Successfully ran all Hadoop MapReduce programs on Amazon Elastic MapReduce framework by using Amazon S3 for input and output Involve in Data Asset Inventory to gather analyze and document business requirements functional requirements and data specifications for Member Retention from sources SQL Hadoop Worked on solving performance and limit queries to the workbooks that when it connects to live database by using a data extract option in Tableau Designed and developed Dashboards for Analytical purposes using Tableau Designed and implemented facts dimensions measure groups measures and OLAP cubes using dimensional data modeling standards in SQL Server 2008 that maintained data Creating and Designing OLAP using SAS OLAP Cube Studio Designing Source Job Target using SAS OLAP Cube Studio and SASDIS Analyzing OLAP Using SAS OLAP Viewer and SAS Dataset using SASEG Migrated ETL jobs to Pig scripts do Transformations even joins and some preaggregations before storing the data onto HDFS Worked with Avro Data Serialization system to work with JSON data formatselastic search Worked on different file formats like Sequence files XML files and Map files usingMap ReducePrograms Involved in Unit testing and delivered Unit test plans and results documents using Junit and MRUnit Exported data from HDFS environment into RDBMS using Sqoop for report generation and visualization purpose Worked on Oozie workflow engine for job scheduling Created and maintained Technical documentation for launching HADOOP Clusters and for executing Pig Scripts Environment CDH 53 Map Reduce Hive014 Spark 141 Oozie Sqoop Pig011 Java Rest API Maven MRunit Junit Tableau Cloudera Python Java DeveloperHadoop Developer CNAINSURANCE Chicago IL January 2014 to March 2015 Description CNA insurance provided B2B insurance to their customers As part of enhancements we developed ELS Enterprise Logging Service to provide statistics to support team and implemented Processor to send alerts to Support Teams Responsibilities Participated in requirement gathering and converting the requirements into technical specifications Created UML diagrams like use cases class diagrams interaction diagrams and activity diagrams Developed the application using Spring Framework that leverages classical Model View Controller MVC architecture Created Business Logic using Servlets POJOs and deployed them on Web logic server Wrote complex SQL queries and stored procedures Developed the XML Schema and Web services for the data maintenance and structures Implemented the Web Service client for the login authentication credit reports and applicant information using Apache Axis 2 Web Service Developed and implemented custom data validation stored procedures for metadata summarization for the data warehouse tables for aggregating telephone subscribers switching data for identifying winning and losing carriers and for identifying value subscribers Identified issue and developed a procedure for correcting the problem which resulted in the improved quality of critical tables by eliminating the possibility of entering duplicate data in a Data Warehouse Designed and implemented SQL based tools stored procedures and functions for daily data volume and aggregation status Responsible to manage data coming from different sources Developed map reduce algorithms Got good experience with NOSQL database Involved in loading data from UNIX file system to HDFS Installed and configured Hive and also written Hive UDFs Worked with cloud services like Amazon web services AWS Designed the logical and physical data model generated DDL scripts and wrote DML scripts for Oracle 10g database Used Hibernate ORM framework with Spring framework for data persistence and transaction management Wrote test cases in JUnit for unit testing of classes Involved in creating templates and screens in HTML and JavaScript Involved in integrating Web Services using SOAP Environment Hive 071 Apache Solr 3x HBase090x020x JDK Spring MVC WebSphere 61 HTML XML JavaScript JUnit 38 Oracle 10g Amazon Web Services Java Developer INTEQ SOFTWARE PVT LTD Hyderabad Telangana January 2012 to December 2013 Description This project deals with the development of automotive protocol for vehicles based on serial interfaces of ECU The protocol development has to finally test by the automotive tools to confirm the message integrity Responsibilities Using JAVA developed a website for eRecruitment consists of many modules Followed MVC Architecture for implementing the functionality Designed and reviewed the test scenarios and scripts for given functional requirements Implemented Services using Core Java Involved in development of classes using java Designed and built user interface using spring and JavaScript employed collection libraries Designed and involved in preparing activity diagrams usecasediagrams sequence diagrams as per the business requirement Used JavaScript for Client validation Designed a website for understanding the user requirements and validated the web page using JQuery in Conjunction with Java Spring Hibernate PHP Developed user interfaces using Servlets CSS XSLT XML HTML and JavaScript Good proficiency in developing algorithms for serial interfaces Involved in testing of CAN protocols Developed the flow of algorithm in UML Developed verification and validation scripts in java Environment Java JSP Servlets JDBC JavaScript MySQL JUnit Eclipse IDE Windows 7XPVista UNIX LINUX Education Bachelors Skills Html 3 years java 4 years Javascript 3 years Model view controller 3 years Xml 3 years",
    "unique_id": "303ae6ec-4c62-46af-befc-5f2a220aca18"
}