{
    "clean_data": "Data Scientist Data Scientist Data Scientist VF Corporation Virginia MN Data Scientist with 8 years of professional experience in theEcommerce Retail and Music Streaming domain performing Statistical Modelling Data Extraction Data screening Data cleaning Data Exploration and Data Visualization of structured and unstructured datasets as well as implementing large scale Machine Learningand Deep Learning algorithms to deliver resourceful insights inferences and significantly impacted business revenues and user experience Experiencedin Facilitating the entire lifecycle of a data science project Data Extraction Data PreProcessing Feature Engineering Dimensionality Reduction Algorithm implementation Back Testing and Validation Proficient in Data transformations using log squareroot reciprocal differencing and complete boxcox transformation depending upon the dataset Knowledge of normality tests like ShapiroWilk AndersonDarling Adept at Analysis of Missing data by exploring correlations and similarities introducing dummy variables for missingness and choosing from imputation methods such iterative imputer on Python Experienced inMachine Learning techniques such as regression and classification models like Linear Polynomial Support Vector Decision Trees Logistic Regression Support Vector Machines Experienced in Ensemble learning usingBagging Boosting Random Forests clustering like Kmeans Indepth Knowledge of Dimensionality Reduction PCA LDA Hyperparameter tuning Model Regularization Ridge Lasso Elastic Net and Grid Search techniques to optimize model performance Adept with Python and OOP concepts such as Inheritance Polymorphism Abstraction Association etc Experienced in developing algorithms to create Artificial Neural Networks Deep Learning Convolution Neural Networks to implement AI solutions Expertise in creating executive Tableau Dashboards for Data visualization and deploying it to the servers Skilled in using tidyversein R and Pandasin Python for performing exploratory data analysis Proficient in Data Visualization tools such as Tableau and PowerBI Big Data tools such as Hadoop HDFS Spark and MapReduce MySQLOracle SQL and Redshift SQLand Microsoft Excel VLOOKUP Pivot tables Skilled in Big Data Technologies like Spark Spark SQL PySpark HDFS Hadoop MapReduce Kafka Experience in Web Data Mining with Pythons ScraPy and BeautifulSoup packages along with working knowledge of Natural Language Processing NLP to analyze text patterns Excellent exposure to Data Visualization with Tableau PowerBI Seaborn Matplotlib and ggplot2 Experience with Python libraries including NumPy Pandas SciPy ScikitLearn statsmodels MatplotLib SeabornNLTKand R libraries like ggplot2 dplyr Working knowledge of Database Creation and maintenance of Physical data models with Oracle DB2 and SQL server databases as well as normalizing databases up to third form using SQL functions Authorized to work in the US for any employer Work Experience Data Scientist VF Corporation Greensboro NC August 2017 to Present Description VF Corporation is an American worldwide apparel and footwear company Worked on the recommender system by implementing Sentiment Analysis on other people reviews and extract the best product for the user to buy Responsibilities Reviewed business requirements to analyze the data sources and worked closely with the business analysts to understand business objectives Extracted data by webscraping through the reviews using Beautiful Soup Involved in various preprocessing phases of textdata like Tokenization Stemming Lemmatization and converting the raw text data to structured data Performed data collection data cleaning feature scaling feature engineering validation visualization report findings develop strategic uses of data by Python libraries like NumPy Pandas Scipy MatplotLib ScikitLearn Used Tableau for visualizing and analyzing the data to facilitate the understanding of the team about the data Implemented various statistical techniques to manipulate the data like missing data imputation Principal Component Analysis for dimensionreduction Worked with customer churn models including Lasso regression along with preprocessing of the data Constructed new vocabulary to convert the data into numbers to be processed by the machine by using the approaches like Bag of Words model tfidf Word2Vec Employed statistical methodologies such as AB test experiment design and hypothesis testing and deployed models on Docker Performed Nave Bayes KNN Logistic Regression Random Forest SVM and KMeans to categorize customers into certain groups Employed various metrics such as CrossValidation LogLoss function Confusion Matrix ROC and AUC to evaluate the performance of each model Using NLP developed deep learning algorithms for analyzing text over the existing dictionarybased approaches Created distributed environment of TensorFlow across multiple devices and ran them in parallel Environment PythonNumPy Pandas Matplotlib TensorFlow NLP Data Scientist SiriusXM Washington DC April 2015 to July 2017 Description SiriusXM is a broadcasting company that provides satellite radio and online radio The project required to build a recommender system by extracting and analyzing different features from raw audio files which took new songs into account Responsibilities Performed Data Collection Data Cleaning Data Visualization using Python Deep Feature Synthesis and extracted key statistical findings to develop business strategies Since sound is represented in the form of audio signals parameters like frequency decibel timbre pitch were usedfor analysis Used Librosa Python library to analyze the audio signals and plot wave plot and create a spectrogram to analyze the behavior for the sound Cleaned the audio files to remove the audio with no noise by setting a threshold and retrieve the audio above the set threshold Created 2D Convolution Neural Networks using Keras on GPUs by extracting different number of MFCC features using Librosa Used globaltemporal pooling layer to effectively compute statistics of learned features across time Implemented regularization methods like Dropout Lasso Regression and Ridge Regression to prevent the model from overfitting Final model was selected by evaluating them using various metrics like Accuracy Confusion Matrix Precision Recall Environment Python Pandas Scikit Numpy TensorFlow Keras Librosa Data Scientist Swiggy Gurgaon Haryana January 2013 to March 2015 Description Swiggy is online food delivery company in India The project was predicting deals and coupons for frequent customers of the company Responsibilities Participated in all phases of project life cycle including data collection data mining data cleaning developing models validation and creating reports Performed data cleaning on a huge dataset which had missing data and extreme outliers from Hadoop workbooks and explored data to draw relationships and correlations between variables Performed datapreprocessing on messy data including imputation normalization scaling and feature engineering using ScikitLearn Conducted exploratory data analysis using Python Matplotlib and Seaborn to identify underlying patterns and correlations between features Build classification models based on Logistic Regression Decision Trees Support Vector Machine to predict the probability of a customer using the application Employed Ensemble Learning techniques such as Random Forests and Ada Gradient Boosting to improve the model performance by 10 Used various metrics such as FScore ROC and AUC to evaluate the performance of each model and 5Fold Cross Validation to test the models with different batches of data to optimize the models Implemented and tested the model on AWS EC2 and collaborated with development team to get the best algorithms and parameters Prepared datavisualization designed dashboards with Tableau and generated complex reports including summaries and graphs to interpret the findings to the team Environment PythonNumPy Pandas Matplotlib Amazon Web Services Jupyter Notebook Tableau Data Analyst Python Developer BigBasket Bengaluru Karnataka September 2010 to December 2012 Description Bigbasket is the Indian online grocery delivery service My responsibilities included working on RESTful Web Services on Python Flask and working on a team building a predictive model to enhance the online shopping for the users Responsibilities Worked on both legacy data and new data mostly built around the user experience and grocery inventory available PerformedData Analysis on target data after transfer to Data Warehouse Created ETL solution using MS SQL Server and worked with Agile and TestDriven development within SDLC Worked on RESTful Web Services on Python Flask and built primary functions for classification Conducted data preparation and outlier detection using Pythonand implemented Logistic Regression Random Forest Nave Bayes Classifier for classification for recommendation Employed KFold Crossvalidation to test and verify the model accuracy Worked with the team to host data and certain web interfaces on Amazon Web Services EC2 and store data on S3 bucket Worked with Team manager to develop a lucrative system of classifying auditions and vendors best fitting for the company in the long run Presented executive dashboards and scorecards to visualize and present trends in the data using Excel and Python Matplotlib Environment PythonNumPy Pandas Matplotlib Amazon Web Services Python Flask REST APIs Linux Education Bachelors Skills Amazon web services Hadoop Hdfs Mapreduce Python Ggplot2 Matplotlib Anova Mapreduce Kafka Data visualization Hadoop Mongodb Snowflake schema Data modeling Database Microsoft sql server Sql server Mysql Oracle",
    "entities": [
        "TestDriven",
        "Artificial Neural Networks Deep Learning Convolution Neural Networks",
        "Data Visualization",
        "MFCC",
        "MS SQL Server",
        "Redshift SQLand",
        "Natural Language Processing NLP",
        "Created 2D Convolution Neural Networks",
        "Amazon Web Services EC2",
        "ScikitLearn Conducted",
        "US",
        "Accuracy Confusion Matrix Precision Recall Environment Python",
        "India",
        "Bag of Words",
        "AUC",
        "Present Description VF Corporation",
        "Performed",
        "Database Creation",
        "NC",
        "Data Warehouse Created",
        "Presented",
        "MatplotLib ScikitLearn",
        "AI",
        "AWS",
        "Ensemble",
        "Pythonand",
        "Keras",
        "Microsoft",
        "Employed Ensemble Learning",
        "Cross Validation",
        "Hadoop Mongodb Snowflake",
        "Responsibilities Performed Data Collection Data Cleaning Data Visualization",
        "Kmeans Indepth Knowledge of Dimensionality Reduction PCA LDA Hyperparameter",
        "Linear Polynomial Support Vector Decision Trees Logistic Regression Support Vector Machines Experienced",
        "CrossValidation LogLoss",
        "ScikitLearn",
        "Sql",
        "Data Exploration and Data Visualization",
        "TensorFlow",
        "Data Scientist Data Scientist Data Scientist VF Corporation",
        "Web Data Mining with Pythons ScraPy",
        "Big Data Technologies",
        "Hadoop HDFS Spark",
        "SQL",
        "PerformedData Analysis",
        "Hadoop",
        "Data",
        "Oracle DB2",
        "Responsibilities Reviewed",
        "NLP",
        "Washington DC",
        "Team",
        "Database Microsoft",
        "Statistical Modelling Data Extraction Data",
        "Tableau",
        "Logistic Regression Random Forest Nave Bayes Classifier",
        "Data Visualization with",
        "Logistic Regression Decision Trees Support Vector Machine",
        "NLP Data Scientist",
        "ShapiroWilk AndersonDarling Adept at Analysis",
        "ROC",
        "Tokenization Stemming Lemmatization",
        "Python Experienced inMachine Learning",
        "MN Data Scientist",
        "Principal Component Analysis",
        "SDLC Worked on RESTful Web Services on Python Flask",
        "Beautiful Soup Involved",
        "Data Extraction Data PreProcessing Feature Engineering Dimensionality Reduction Algorithm",
        "Inheritance Polymorphism Abstraction Association etc Experienced",
        "Agile"
    ],
    "experience": "Experience in Web Data Mining with Pythons ScraPy and BeautifulSoup packages along with working knowledge of Natural Language Processing NLP to analyze text patterns Excellent exposure to Data Visualization with Tableau PowerBI Seaborn Matplotlib and ggplot2 Experience with Python libraries including NumPy Pandas SciPy ScikitLearn statsmodels MatplotLib SeabornNLTKand R libraries like ggplot2 dplyr Working knowledge of Database Creation and maintenance of Physical data models with Oracle DB2 and SQL server databases as well as normalizing databases up to third form using SQL functions Authorized to work in the US for any employer Work Experience Data Scientist VF Corporation Greensboro NC August 2017 to Present Description VF Corporation is an American worldwide apparel and footwear company Worked on the recommender system by implementing Sentiment Analysis on other people reviews and extract the best product for the user to buy Responsibilities Reviewed business requirements to analyze the data sources and worked closely with the business analysts to understand business objectives Extracted data by webscraping through the reviews using Beautiful Soup Involved in various preprocessing phases of textdata like Tokenization Stemming Lemmatization and converting the raw text data to structured data Performed data collection data cleaning feature scaling feature engineering validation visualization report findings develop strategic uses of data by Python libraries like NumPy Pandas Scipy MatplotLib ScikitLearn Used Tableau for visualizing and analyzing the data to facilitate the understanding of the team about the data Implemented various statistical techniques to manipulate the data like missing data imputation Principal Component Analysis for dimensionreduction Worked with customer churn models including Lasso regression along with preprocessing of the data Constructed new vocabulary to convert the data into numbers to be processed by the machine by using the approaches like Bag of Words model tfidf Word2Vec Employed statistical methodologies such as AB test experiment design and hypothesis testing and deployed models on Docker Performed Nave Bayes KNN Logistic Regression Random Forest SVM and KMeans to categorize customers into certain groups Employed various metrics such as CrossValidation LogLoss function Confusion Matrix ROC and AUC to evaluate the performance of each model Using NLP developed deep learning algorithms for analyzing text over the existing dictionarybased approaches Created distributed environment of TensorFlow across multiple devices and ran them in parallel Environment PythonNumPy Pandas Matplotlib TensorFlow NLP Data Scientist SiriusXM Washington DC April 2015 to July 2017 Description SiriusXM is a broadcasting company that provides satellite radio and online radio The project required to build a recommender system by extracting and analyzing different features from raw audio files which took new songs into account Responsibilities Performed Data Collection Data Cleaning Data Visualization using Python Deep Feature Synthesis and extracted key statistical findings to develop business strategies Since sound is represented in the form of audio signals parameters like frequency decibel timbre pitch were usedfor analysis Used Librosa Python library to analyze the audio signals and plot wave plot and create a spectrogram to analyze the behavior for the sound Cleaned the audio files to remove the audio with no noise by setting a threshold and retrieve the audio above the set threshold Created 2D Convolution Neural Networks using Keras on GPUs by extracting different number of MFCC features using Librosa Used globaltemporal pooling layer to effectively compute statistics of learned features across time Implemented regularization methods like Dropout Lasso Regression and Ridge Regression to prevent the model from overfitting Final model was selected by evaluating them using various metrics like Accuracy Confusion Matrix Precision Recall Environment Python Pandas Scikit Numpy TensorFlow Keras Librosa Data Scientist Swiggy Gurgaon Haryana January 2013 to March 2015 Description Swiggy is online food delivery company in India The project was predicting deals and coupons for frequent customers of the company Responsibilities Participated in all phases of project life cycle including data collection data mining data cleaning developing models validation and creating reports Performed data cleaning on a huge dataset which had missing data and extreme outliers from Hadoop workbooks and explored data to draw relationships and correlations between variables Performed datapreprocessing on messy data including imputation normalization scaling and feature engineering using ScikitLearn Conducted exploratory data analysis using Python Matplotlib and Seaborn to identify underlying patterns and correlations between features Build classification models based on Logistic Regression Decision Trees Support Vector Machine to predict the probability of a customer using the application Employed Ensemble Learning techniques such as Random Forests and Ada Gradient Boosting to improve the model performance by 10 Used various metrics such as FScore ROC and AUC to evaluate the performance of each model and 5Fold Cross Validation to test the models with different batches of data to optimize the models Implemented and tested the model on AWS EC2 and collaborated with development team to get the best algorithms and parameters Prepared datavisualization designed dashboards with Tableau and generated complex reports including summaries and graphs to interpret the findings to the team Environment PythonNumPy Pandas Matplotlib Amazon Web Services Jupyter Notebook Tableau Data Analyst Python Developer BigBasket Bengaluru Karnataka September 2010 to December 2012 Description Bigbasket is the Indian online grocery delivery service My responsibilities included working on RESTful Web Services on Python Flask and working on a team building a predictive model to enhance the online shopping for the users Responsibilities Worked on both legacy data and new data mostly built around the user experience and grocery inventory available PerformedData Analysis on target data after transfer to Data Warehouse Created ETL solution using MS SQL Server and worked with Agile and TestDriven development within SDLC Worked on RESTful Web Services on Python Flask and built primary functions for classification Conducted data preparation and outlier detection using Pythonand implemented Logistic Regression Random Forest Nave Bayes Classifier for classification for recommendation Employed KFold Crossvalidation to test and verify the model accuracy Worked with the team to host data and certain web interfaces on Amazon Web Services EC2 and store data on S3 bucket Worked with Team manager to develop a lucrative system of classifying auditions and vendors best fitting for the company in the long run Presented executive dashboards and scorecards to visualize and present trends in the data using Excel and Python Matplotlib Environment PythonNumPy Pandas Matplotlib Amazon Web Services Python Flask REST APIs Linux Education Bachelors Skills Amazon web services Hadoop Hdfs Mapreduce Python Ggplot2 Matplotlib Anova Mapreduce Kafka Data visualization Hadoop Mongodb Snowflake schema Data modeling Database Microsoft sql server Sql server Mysql Oracle",
    "extracted_keywords": [
        "Data",
        "Scientist",
        "Data",
        "Scientist",
        "Data",
        "Scientist",
        "VF",
        "Corporation",
        "Virginia",
        "MN",
        "Data",
        "Scientist",
        "years",
        "experience",
        "theEcommerce",
        "Retail",
        "Music",
        "Streaming",
        "domain",
        "Statistical",
        "Modelling",
        "Data",
        "Extraction",
        "Data",
        "Data",
        "Data",
        "Exploration",
        "Data",
        "Visualization",
        "datasets",
        "scale",
        "Machine",
        "Learningand",
        "Deep",
        "Learning",
        "insights",
        "inferences",
        "business",
        "revenues",
        "user",
        "experience",
        "Experiencedin",
        "lifecycle",
        "data",
        "science",
        "project",
        "Data",
        "Extraction",
        "Data",
        "PreProcessing",
        "Feature",
        "Engineering",
        "Dimensionality",
        "Reduction",
        "Algorithm",
        "implementation",
        "Testing",
        "Validation",
        "Proficient",
        "Data",
        "transformations",
        "log",
        "squareroot",
        "differencing",
        "boxcox",
        "transformation",
        "Knowledge",
        "normality",
        "tests",
        "ShapiroWilk",
        "AndersonDarling",
        "Adept",
        "Analysis",
        "Missing",
        "data",
        "correlations",
        "similarities",
        "variables",
        "missingness",
        "imputation",
        "methods",
        "imputer",
        "Python",
        "Experienced",
        "inMachine",
        "Learning",
        "techniques",
        "regression",
        "classification",
        "models",
        "Linear",
        "Polynomial",
        "Support",
        "Vector",
        "Decision",
        "Trees",
        "Logistic",
        "Regression",
        "Support",
        "Vector",
        "Machines",
        "Ensemble",
        "Random",
        "Forests",
        "Kmeans",
        "Knowledge",
        "Dimensionality",
        "Reduction",
        "PCA",
        "LDA",
        "Hyperparameter",
        "Model",
        "Regularization",
        "Ridge",
        "Lasso",
        "Elastic",
        "Net",
        "Grid",
        "Search",
        "techniques",
        "model",
        "performance",
        "Adept",
        "Python",
        "OOP",
        "concepts",
        "Inheritance",
        "Polymorphism",
        "Abstraction",
        "Association",
        "algorithms",
        "Artificial",
        "Neural",
        "Networks",
        "Deep",
        "Learning",
        "Convolution",
        "Neural",
        "Networks",
        "AI",
        "solutions",
        "Expertise",
        "executive",
        "Tableau",
        "Dashboards",
        "Data",
        "visualization",
        "servers",
        "R",
        "Pandasin",
        "Python",
        "data",
        "analysis",
        "Proficient",
        "Data",
        "Visualization",
        "tools",
        "Tableau",
        "PowerBI",
        "Big",
        "Data",
        "tools",
        "Hadoop",
        "HDFS",
        "Spark",
        "MapReduce",
        "MySQLOracle",
        "SQL",
        "Redshift",
        "SQLand",
        "Microsoft",
        "Excel",
        "VLOOKUP",
        "Pivot",
        "Big",
        "Data",
        "Technologies",
        "Spark",
        "Spark",
        "SQL",
        "PySpark",
        "HDFS",
        "Hadoop",
        "MapReduce",
        "Kafka",
        "Experience",
        "Web",
        "Data",
        "Mining",
        "Pythons",
        "BeautifulSoup",
        "packages",
        "knowledge",
        "Natural",
        "Language",
        "Processing",
        "NLP",
        "text",
        "patterns",
        "exposure",
        "Data",
        "Visualization",
        "Tableau",
        "PowerBI",
        "Seaborn",
        "Matplotlib",
        "ggplot2",
        "Experience",
        "Python",
        "libraries",
        "NumPy",
        "Pandas",
        "SciPy",
        "ScikitLearn",
        "MatplotLib",
        "SeabornNLTKand",
        "R",
        "libraries",
        "ggplot2",
        "dplyr",
        "Working",
        "knowledge",
        "Database",
        "Creation",
        "maintenance",
        "data",
        "models",
        "Oracle",
        "DB2",
        "SQL",
        "server",
        "databases",
        "databases",
        "form",
        "SQL",
        "functions",
        "US",
        "employer",
        "Work",
        "Experience",
        "Data",
        "Scientist",
        "VF",
        "Corporation",
        "Greensboro",
        "NC",
        "August",
        "Present",
        "Description",
        "VF",
        "Corporation",
        "apparel",
        "footwear",
        "company",
        "recommender",
        "system",
        "Sentiment",
        "Analysis",
        "people",
        "reviews",
        "product",
        "user",
        "Responsibilities",
        "business",
        "requirements",
        "data",
        "sources",
        "business",
        "analysts",
        "business",
        "objectives",
        "data",
        "reviews",
        "Beautiful",
        "Soup",
        "phases",
        "textdata",
        "Tokenization",
        "Stemming",
        "Lemmatization",
        "text",
        "data",
        "data",
        "data",
        "collection",
        "data",
        "feature",
        "feature",
        "engineering",
        "validation",
        "visualization",
        "report",
        "findings",
        "uses",
        "data",
        "Python",
        "libraries",
        "NumPy",
        "Pandas",
        "Scipy",
        "MatplotLib",
        "ScikitLearn",
        "Tableau",
        "data",
        "understanding",
        "team",
        "data",
        "techniques",
        "data",
        "data",
        "imputation",
        "Principal",
        "Component",
        "Analysis",
        "dimensionreduction",
        "customer",
        "churn",
        "models",
        "Lasso",
        "regression",
        "preprocessing",
        "data",
        "vocabulary",
        "data",
        "numbers",
        "machine",
        "approaches",
        "Bag",
        "Words",
        "model",
        "methodologies",
        "AB",
        "test",
        "experiment",
        "design",
        "hypothesis",
        "testing",
        "models",
        "Docker",
        "Performed",
        "Nave",
        "Bayes",
        "KNN",
        "Logistic",
        "Regression",
        "Random",
        "Forest",
        "SVM",
        "KMeans",
        "customers",
        "groups",
        "metrics",
        "CrossValidation",
        "LogLoss",
        "function",
        "Confusion",
        "Matrix",
        "ROC",
        "AUC",
        "performance",
        "model",
        "NLP",
        "learning",
        "algorithms",
        "text",
        "approaches",
        "environment",
        "TensorFlow",
        "devices",
        "Environment",
        "Pandas",
        "Matplotlib",
        "TensorFlow",
        "NLP",
        "Data",
        "Scientist",
        "SiriusXM",
        "Washington",
        "DC",
        "April",
        "July",
        "Description",
        "SiriusXM",
        "broadcasting",
        "company",
        "satellite",
        "radio",
        "radio",
        "project",
        "recommender",
        "system",
        "features",
        "files",
        "songs",
        "account",
        "Responsibilities",
        "Performed",
        "Data",
        "Collection",
        "Data",
        "Cleaning",
        "Data",
        "Visualization",
        "Python",
        "Deep",
        "Feature",
        "Synthesis",
        "findings",
        "business",
        "strategies",
        "sound",
        "form",
        "signals",
        "parameters",
        "frequency",
        "decibel",
        "timbre",
        "pitch",
        "analysis",
        "Librosa",
        "Python",
        "library",
        "signals",
        "plot",
        "wave",
        "plot",
        "spectrogram",
        "behavior",
        "sound",
        "files",
        "audio",
        "noise",
        "threshold",
        "audio",
        "threshold",
        "2D",
        "Convolution",
        "Neural",
        "Networks",
        "Keras",
        "GPUs",
        "number",
        "MFCC",
        "features",
        "Librosa",
        "pooling",
        "layer",
        "statistics",
        "features",
        "time",
        "regularization",
        "methods",
        "Dropout",
        "Lasso",
        "Regression",
        "Ridge",
        "Regression",
        "model",
        "model",
        "metrics",
        "Accuracy",
        "Confusion",
        "Matrix",
        "Precision",
        "Recall",
        "Environment",
        "Python",
        "Scikit",
        "Numpy",
        "TensorFlow",
        "Keras",
        "Librosa",
        "Data",
        "Scientist",
        "Swiggy",
        "Gurgaon",
        "Haryana",
        "January",
        "March",
        "Description",
        "Swiggy",
        "food",
        "delivery",
        "company",
        "India",
        "project",
        "deals",
        "coupons",
        "customers",
        "company",
        "Responsibilities",
        "phases",
        "project",
        "life",
        "cycle",
        "data",
        "collection",
        "data",
        "mining",
        "data",
        "models",
        "validation",
        "reports",
        "Performed",
        "data",
        "dataset",
        "data",
        "outliers",
        "Hadoop",
        "workbooks",
        "data",
        "relationships",
        "correlations",
        "variables",
        "data",
        "imputation",
        "normalization",
        "scaling",
        "feature",
        "engineering",
        "ScikitLearn",
        "data",
        "analysis",
        "Python",
        "Matplotlib",
        "Seaborn",
        "patterns",
        "correlations",
        "features",
        "classification",
        "models",
        "Logistic",
        "Regression",
        "Decision",
        "Trees",
        "Support",
        "Vector",
        "Machine",
        "probability",
        "customer",
        "application",
        "Employed",
        "Ensemble",
        "Learning",
        "techniques",
        "Random",
        "Forests",
        "Ada",
        "Gradient",
        "Boosting",
        "model",
        "performance",
        "metrics",
        "FScore",
        "ROC",
        "AUC",
        "performance",
        "model",
        "Cross",
        "Validation",
        "models",
        "batches",
        "data",
        "models",
        "model",
        "AWS",
        "EC2",
        "development",
        "team",
        "algorithms",
        "parameters",
        "Prepared",
        "datavisualization",
        "dashboards",
        "Tableau",
        "reports",
        "summaries",
        "graphs",
        "findings",
        "team",
        "Environment",
        "Matplotlib",
        "Amazon",
        "Web",
        "Services",
        "Jupyter",
        "Notebook",
        "Tableau",
        "Data",
        "Analyst",
        "Python",
        "Developer",
        "BigBasket",
        "Bengaluru",
        "Karnataka",
        "September",
        "December",
        "Description",
        "Bigbasket",
        "grocery",
        "delivery",
        "service",
        "responsibilities",
        "Web",
        "Services",
        "Python",
        "Flask",
        "team",
        "model",
        "online",
        "shopping",
        "users",
        "Responsibilities",
        "legacy",
        "data",
        "data",
        "user",
        "experience",
        "grocery",
        "inventory",
        "PerformedData",
        "Analysis",
        "target",
        "data",
        "transfer",
        "Data",
        "Warehouse",
        "ETL",
        "solution",
        "MS",
        "SQL",
        "Server",
        "Agile",
        "TestDriven",
        "development",
        "SDLC",
        "Web",
        "Services",
        "Python",
        "Flask",
        "functions",
        "classification",
        "data",
        "preparation",
        "outlier",
        "detection",
        "Pythonand",
        "Logistic",
        "Regression",
        "Random",
        "Forest",
        "Nave",
        "Bayes",
        "Classifier",
        "classification",
        "recommendation",
        "Employed",
        "KFold",
        "Crossvalidation",
        "model",
        "accuracy",
        "team",
        "data",
        "web",
        "interfaces",
        "Amazon",
        "Web",
        "Services",
        "EC2",
        "store",
        "data",
        "S3",
        "bucket",
        "Team",
        "manager",
        "system",
        "auditions",
        "vendors",
        "company",
        "run",
        "executive",
        "dashboards",
        "scorecards",
        "trends",
        "data",
        "Excel",
        "Python",
        "Matplotlib",
        "Environment",
        "Pandas",
        "Matplotlib",
        "Amazon",
        "Web",
        "Services",
        "Python",
        "Flask",
        "REST",
        "APIs",
        "Linux",
        "Education",
        "Bachelors",
        "Skills",
        "Amazon",
        "web",
        "services",
        "Hadoop",
        "Hdfs",
        "Mapreduce",
        "Python",
        "Ggplot2",
        "Matplotlib",
        "Anova",
        "Mapreduce",
        "Kafka",
        "Data",
        "visualization",
        "Hadoop",
        "Mongodb",
        "Snowflake",
        "schema",
        "Data",
        "Database",
        "Microsoft",
        "server",
        "Sql",
        "server",
        "Mysql",
        "Oracle"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T22:55:44.590751",
    "resume_data": "Data Scientist Data Scientist Data Scientist VF Corporation Virginia MN Data Scientist with 8 years of professional experience in theEcommerce Retail and Music Streaming domain performing Statistical Modelling Data Extraction Data screening Data cleaning Data Exploration and Data Visualization of structured and unstructured datasets as well as implementing large scale Machine Learningand Deep Learning algorithms to deliver resourceful insights inferences and significantly impacted business revenues and user experience Experiencedin Facilitating the entire lifecycle of a data science project Data Extraction Data PreProcessing Feature Engineering Dimensionality Reduction Algorithm implementation Back Testing and Validation Proficient in Data transformations using log squareroot reciprocal differencing and complete boxcox transformation depending upon the dataset Knowledge of normality tests like ShapiroWilk AndersonDarling Adept at Analysis of Missing data by exploring correlations and similarities introducing dummy variables for missingness and choosing from imputation methods such iterative imputer on Python Experienced inMachine Learning techniques such as regression and classification models like Linear Polynomial Support Vector Decision Trees Logistic Regression Support Vector Machines Experienced in Ensemble learning usingBagging Boosting Random Forests clustering like Kmeans Indepth Knowledge of Dimensionality Reduction PCA LDA Hyperparameter tuning Model Regularization Ridge Lasso Elastic Net and Grid Search techniques to optimize model performance Adept with Python and OOP concepts such as Inheritance Polymorphism Abstraction Association etc Experienced in developing algorithms to create Artificial Neural Networks Deep Learning Convolution Neural Networks to implement AI solutions Expertise in creating executive Tableau Dashboards for Data visualization and deploying it to the servers Skilled in using tidyversein R and Pandasin Python for performing exploratory data analysis Proficient in Data Visualization tools such as Tableau and PowerBI Big Data tools such as Hadoop HDFS Spark and MapReduce MySQLOracle SQL and Redshift SQLand Microsoft Excel VLOOKUP Pivot tables Skilled in Big Data Technologies like Spark Spark SQL PySpark HDFS Hadoop MapReduce Kafka Experience in Web Data Mining with Pythons ScraPy and BeautifulSoup packages along with working knowledge of Natural Language Processing NLP to analyze text patterns Excellent exposure to Data Visualization with Tableau PowerBI Seaborn Matplotlib and ggplot2 Experience with Python libraries including NumPy Pandas SciPy ScikitLearn statsmodels MatplotLib SeabornNLTKand R libraries like ggplot2 dplyr Working knowledge of Database Creation and maintenance of Physical data models with Oracle DB2 and SQL server databases as well as normalizing databases up to third form using SQL functions Authorized to work in the US for any employer Work Experience Data Scientist VF Corporation Greensboro NC August 2017 to Present Description VF Corporation is an American worldwide apparel and footwear company Worked on the recommender system by implementing Sentiment Analysis on other people reviews and extract the best product for the user to buy Responsibilities Reviewed business requirements to analyze the data sources and worked closely with the business analysts to understand business objectives Extracted data by webscraping through the reviews using Beautiful Soup Involved in various preprocessing phases of textdata like Tokenization Stemming Lemmatization and converting the raw text data to structured data Performed data collection data cleaning feature scaling feature engineering validation visualization report findings develop strategic uses of data by Python libraries like NumPy Pandas Scipy MatplotLib ScikitLearn Used Tableau for visualizing and analyzing the data to facilitate the understanding of the team about the data Implemented various statistical techniques to manipulate the data like missing data imputation Principal Component Analysis for dimensionreduction Worked with customer churn models including Lasso regression along with preprocessing of the data Constructed new vocabulary to convert the data into numbers to be processed by the machine by using the approaches like Bag of Words model tfidf Word2Vec Employed statistical methodologies such as AB test experiment design and hypothesis testing and deployed models on Docker Performed Nave Bayes KNN Logistic Regression Random Forest SVM and KMeans to categorize customers into certain groups Employed various metrics such as CrossValidation LogLoss function Confusion Matrix ROC and AUC to evaluate the performance of each model Using NLP developed deep learning algorithms for analyzing text over the existing dictionarybased approaches Created distributed environment of TensorFlow across multiple devices and ran them in parallel Environment PythonNumPy Pandas Matplotlib TensorFlow NLP Data Scientist SiriusXM Washington DC April 2015 to July 2017 Description SiriusXM is a broadcasting company that provides satellite radio and online radio The project required to build a recommender system by extracting and analyzing different features from raw audio files which took new songs into account Responsibilities Performed Data Collection Data Cleaning Data Visualization using Python Deep Feature Synthesis and extracted key statistical findings to develop business strategies Since sound is represented in the form of audio signals parameters like frequency decibel timbre pitch were usedfor analysis Used Librosa Python library to analyze the audio signals and plot wave plot and create a spectrogram to analyze the behavior for the sound Cleaned the audio files to remove the audio with no noise by setting a threshold and retrieve the audio above the set threshold Created 2D Convolution Neural Networks using Keras on GPUs by extracting different number of MFCC features using Librosa Used globaltemporal pooling layer to effectively compute statistics of learned features across time Implemented regularization methods like Dropout Lasso Regression and Ridge Regression to prevent the model from overfitting Final model was selected by evaluating them using various metrics like Accuracy Confusion Matrix Precision Recall Environment Python Pandas Scikit Numpy TensorFlow Keras Librosa Data Scientist Swiggy Gurgaon Haryana January 2013 to March 2015 Description Swiggy is online food delivery company in India The project was predicting deals and coupons for frequent customers of the company Responsibilities Participated in all phases of project life cycle including data collection data mining data cleaning developing models validation and creating reports Performed data cleaning on a huge dataset which had missing data and extreme outliers from Hadoop workbooks and explored data to draw relationships and correlations between variables Performed datapreprocessing on messy data including imputation normalization scaling and feature engineering using ScikitLearn Conducted exploratory data analysis using Python Matplotlib and Seaborn to identify underlying patterns and correlations between features Build classification models based on Logistic Regression Decision Trees Support Vector Machine to predict the probability of a customer using the application Employed Ensemble Learning techniques such as Random Forests and Ada Gradient Boosting to improve the model performance by 10 Used various metrics such as FScore ROC and AUC to evaluate the performance of each model and 5Fold Cross Validation to test the models with different batches of data to optimize the models Implemented and tested the model on AWS EC2 and collaborated with development team to get the best algorithms and parameters Prepared datavisualization designed dashboards with Tableau and generated complex reports including summaries and graphs to interpret the findings to the team Environment PythonNumPy Pandas Matplotlib Amazon Web Services Jupyter Notebook Tableau Data Analyst Python Developer BigBasket Bengaluru Karnataka September 2010 to December 2012 Description Bigbasket is the Indian online grocery delivery service My responsibilities included working on RESTful Web Services on Python Flask and working on a team building a predictive model to enhance the online shopping for the users Responsibilities Worked on both legacy data and new data mostly built around the user experience and grocery inventory available PerformedData Analysis on target data after transfer to Data Warehouse Created ETL solution using MS SQL Server and worked with Agile and TestDriven development within SDLC Worked on RESTful Web Services on Python Flask and built primary functions for classification Conducted data preparation and outlier detection using Pythonand implemented Logistic Regression Random Forest Nave Bayes Classifier for classification for recommendation Employed KFold Crossvalidation to test and verify the model accuracy Worked with the team to host data and certain web interfaces on Amazon Web Services EC2 and store data on S3 bucket Worked with Team manager to develop a lucrative system of classifying auditions and vendors best fitting for the company in the long run Presented executive dashboards and scorecards to visualize and present trends in the data using Excel and Python Matplotlib Environment PythonNumPy Pandas Matplotlib Amazon Web Services Python Flask REST APIs Linux Education Bachelors Skills Amazon web services Hadoop Hdfs Mapreduce Python Ggplot2 Matplotlib Anova Mapreduce Kafka Data visualization Hadoop Mongodb Snowflake schema Data modeling Database Microsoft sql server Sql server Mysql Oracle",
    "unique_id": "1cccf7fd-4d03-49fd-a95f-be5ed1c942fa"
}