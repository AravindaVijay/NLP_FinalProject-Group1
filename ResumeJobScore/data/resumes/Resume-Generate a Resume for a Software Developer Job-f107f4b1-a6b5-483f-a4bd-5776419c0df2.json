{
    "clean_data": "Senior Hadoop DeveloperBig Data Solution Architect Senior Hadoop span lDeveloperspanBig Data Solution Architect Senior Hadoop DeveloperBig Data Solution Architect AVEVA Inc Cumming GA 11 years of experience in IT industry as a Software developer Technical lead Solution Architecture and Project Manager in various domains More than 2 years of client facing experience in Europe Finland Belgium 3 years of experience on Big Data ecosystems Hadoop MapReduce HDFS Yarn Hbase Hive Pig Sqoop Flume Spark Kafka Oozie Hue Zookeeper Ambari Hortonworks MapR AWS Hands on experience in creating realtime data streaming solutions using Apache Spark Kafka HBase Spark Streaming APIpySpark Kafka API Experienced working with different file formats Avro Sequence and JSON Good understanding on building Big DataHadoop applications using AWS Services like Amazon S3 EMRFS EMR RDS Airflow etc Certified Scrum master Hands on experience in Object oriented analysis design and programming with C Java Python Strong domain knowledge in Telecom GIS Oil and Gas Marine and Electrical Hands on experience in developing the GIS applications CAD application customization using GeoServer HTML CSS JQuery AngularJS JavaScript AutoCAD ObjectARX RealDWG Hands on experience in SQL and PLSQL Experience in using Custom distributions like Hortonworks MapR and Cloudera Excellent in technical organization skills communication skills Collaboration skills Work Experience Senior Hadoop DeveloperBig Data Solution Architect AVEVA Inc Huntsville AL March 2014 to March 2017 Description AVEVA provides engineeringplants Software Solutions to industries such as Process Plants EngineeringContracting companies such as EPC Power Shipbuilding It is a nichespecialized segment As now a days many Oil companies are facing problems like Oil is hard to find Oil is expensive to produce human safety etc Our objective is to provide a solution using Big DataHadoop technologies to overcome the problem reported by customers like Chevron Shell and BP etc Responsibilities Closely work with the customers to understand the Challenges and requirements Analyze the customer data to provide a solutions Installed and configured Apache Hadoop MapReduce HDFS developed multiple MapReduce jobs in Java for data cleaning and preprocessing Loading the customer data into HDFS using Hadoop commands Processing and analyze the data using MapReduce jobs Configure Apache Kafka producer and consumer coding part in java to establish connection from data sources and loaded into HDFS Created HIVE tables to store the processed results in tabular format Written PIG scripts to process the HDFS data Worked on pulling the data from MySQL database into HDFS using Sqoop Developed Web application for Data Analysis in reporting format using HTML CSS JQuery Responsible for Hadoop Admin related tasks like debugging performance finetuning and monitoring for daytoday activities Designed and implemented Apache Spark streaming application using PythonpySpark and Scala Load and transform large setspetabytes of structured semistructured and unstructured data Strong expertise in writing and implementing PigHive scripts UDFs Experience in defining and coordination of job flows Good experience in NoSQL database like Hbase Responsibility to manage data coming from different sources Responsibility to write and implement python Scala coding for Spark implementations Responsible to implement the Spark applications on AWS Write a integration tool in python to connect to AVEVA NET Workhub and Dashboard from Hadoop ecosystem Involved in the loading data from Linux file system to HDFS Responsible for managing systems on AWS platforms Automated all jobs in Linux shell scripting As a technical leader ensure that ontime delivery of the agreed delivery Ensuring that the development process is properly focused and controlled Managing risks and issues at the development with in estimated time escalating to project manager as required Running the daily meetings ensuring they are timely focused and brief Environment Java Hadoop HDFS Hive Pig Flume Sqoop Hbase Oozie Spark Kafka Zookeeper Ambari Hue Python Linux Eclipse Hortonworks AWS Hadoop Developer EarthLink Inc Atlanta GA January 2013 to March 2014 Description Traditional GIS applications like GeoServer is limited in dealing with big data challenges including versatile data forms processing parallel computing and dynamic mapping and visualization As a part of predictive analysis we have studied the GIS data and provided solutions Responsibilities Gathered requirements and design the application Installation and Configuration of GeoServer and Hadoop ecosystem tools Installation and Configuration of Tomcat for Development and Testing environment Loading of Shape files into GeoServer Loading the GIS data into HDFS Used MapReduce framework to identify Delta updates and to refresh persistent files PIG and HIVE extensively used for data analyses Responsible to manage GIS data coming from different sources Developed PIG scripts in the areas where extensive coding needs to be reduced Experienced in MapReduce programs to load the data from application generated log file to Hbase database Write a reusable and tested code in python for Spark Implementation Extensively used Python in Big Data implementations Created Hbase tables to store variable data formats of data coming from several data sources Analyzing the data with Hive Ping and Hadoop streaming Responsible for monitoring Hadoop cluster manage and review logs performance tuning etc Developed Coverage Maps Web application on top of Google Maps using HTML CSS JQuery JavaScript Implemented RESTful scripting for Querying on Shape files Deployed the application in Development and Testing environments Coordinated the Onsite and Offshore teams for smooth implementation Implemented Agile process for this project Give training and mentorship to team members to make them better on the job Review the completed tasks to ascertain compliance with standards Coach all team members and provide necessary advice and guidance Write and forward regular reports to the management Perform regular appraisal of team members performance to help with improvement Environment HTML CSS JavaScript JQuery Apache Tomcat Linux GeoServer Eclipse Java Hadoop HDFS Hive Pig Flume Oozie Zookeeper Linux Hue Ambari Hortonworks Team Lead Techno Functional Architect Proximus Group Brussels BE January 2010 to December 2012 Project Graphical Documentation Automation Description GDA provides to the NEO division several tools to create or modify the digital graphical documentation of the local loop network These softwares give also several facilities to manage control and export data from this documentation During this period below technical functional enhancements done VBB BOSO ARK Project ARKIII PDF Generator GDA 64Bit Migration GDA Migration from 2006 to 2012 Responsibilities Requirements capturing and Analysis of the requirements Functional design and Technical design of the requirements Resource allocation and assigning the tasks to team members Creating the required classes for the development Responsibility of Quality of output to customer Developed use cases in VC ObjectARX Implemented AutoCAD migration project from AutoCAD 2006 to AutoCAD 2012 Implemented AutoCAD 64bit project from AutoCAD 32bit to AutoCAD 64bit Embedded Google Maps into GDA application for interactive selection of area Setting up of DevelopmentDev Integration ITT acceptanceUAT environments for application deployment and testing Weekly meetings with customer regarding the health of application and current project activities Environment SQL Enterprise Architect VSS AutoCAD Map 3D ObjectARX 2006 to 2012 Object ARX Developer ObjectARX Brussels BE July 2009 to January 2010 Project JMSFTB for GDA Description The main objective of this project is to attach Job Management Application attributes like JMS number JMS driver and JMS building block number to IVP planned objects like Cables Distributor etc JMS application will call a web service which in turn will call a stored procedure in CAS database to collect information related to all planned cables and street cabinets associated with a particular JMS id Responsibilities Implemented use cases in VC and Object ARX Prepared Userfriendly interfaces using MFC Implementation of various PLSQL procedures Used Visual Source safe for doing proper Versioning Control Unit testing and integration testing Prepared technical design documentation for use cases Environment VC Developer Neilsoft Limited Pune Maharashtra March 2009 to July 2009 Project Arc Software protection and Camera implementation Description Arc Software protection main aim of the project is to create the evaluation edition and standard edition for the Arc software using Aladdin software locking system Camera Implementation Main aim of the project is to implement the camera entity like AutoCAD camera feature in the arc software Responsibilities Capturing requirements from client Understanding the requirements and doing functional design Implementation of Technical design for software protection Developed the application using VC Implemented Software Protection using Aladdin HASP softwares Module test design for the project Unit testing and integration testing Prepared userfriendly interfaces using MFC Environment VC Aladdin HASP Codejack xtreme toolkit Visual Studio VSS ObjectARX Developer Proximus Group Brussels BE December 2007 to February 2009 Project GenesysGDA Description GDA provides to the ANS division several tools to create or modify the digital graphical documentation of the local loop network These softwares give also several facilities to manage control and export data from this documentation The graphical documentation can be defined as A description of all the underground cables splices optical fibers and tubes on road map and cable schemes An input for exploitation followup of the logical connections and planning obvious way to overview the network Complementary to the technical documentation ABR ITR An input for SNS The graphical documentation is stored in the Scan Search SNS document management system Responsibilities Understand the requirements and preparing technical design for use cases Implemented use cases in VC and Object ARX Prepared Userfriendly interfaces using MFC Implementation of various PLSQL procedures Used Visual Source safe for doing proper Versioning Control Prepared Module test design for use cases Setting up of developmentDEV IntegrationITT acceptanceUAT environments for deployment and testing Unit testing and integration testing Prepared documentation for use cases Environment VC Developer Neilsoft Limited Pune Maharashtra June 2007 to November 2007 Project GISCAD System Integration Description Objective of this software application is to exchange the data between Intergraphs GIS system and AutoCAD 2006 in order to have better control on the data flow and flexibility to the designers for editing the drawings XML will be used as an intermediate medium for data exchange between GIS System and this application This application has 4 modules XML2DWG module converts XML document generated by Intergraph GIS system into AutoCAD Drawing DWG2XML module converts AutoCAD drawing into XML document which is input to Intergraph GIS System Asset numbering tool assign asset numbers to new features Design Validation tool validates the AutoCAD drawing before converting into XML Responsibilities Understanding the Requirements given by Customer Preparing Technical design for the assigned Use cases Programs are developed Using MSADO15dll for retrieving data from MSAcess Database Programs are developed using MSSOAP1dll for SOAP programming Used Visual Source safe for doing proper Versioning Control Unit testing and integration testing Environment VC ObjectARX RealDWG MFC SOAP MS Access Visual Studio AutoCAD 2006 VSS Java Developer NAPA Oy Helsinki FI November 2005 to May 2007 Project Joint Bulker Project Description The Joint Bulker project JBP Rules developed by the International Association of Classification Societies IACS for both single and double side bulk carriers are expected to come into force for ships contracted for construction on or after 1st April 2006 The purpose of the project is to develop and implement computer software components to support the Joint Bulker Project IACS Common Structural Rules for Bulk Carriers checking process This is to be carried out by modeling the most common structural elements necessary for checking the JBP Rules Responsibilities Understanding of Specifications and Design Rule Suites development in Java and XML Implementation of test cases using JUnit Framework Used BIRT for implementing Reporting tool Environment Java Junit BIRT Eclipse Star Team SQL Enterprise Architect VSS AutoCAD Map 3D ObjectARX 2006 to 2006 Python VC Developer Neilsoft Limited Pune Maharashtra March 2005 to November 2005 Project CAD Customization Description The project was for developing the interface software for exporting the 2D sheets created in PDMS to AUTOCAD After exporting the sheet to ACAD user was able to add different types of customized annotations to the elements in the sheet The synchronization was maintained between the 2 CAD systems with the help of COMSINK function implementation in ACAD and PDMS Responsibilities Understanding of Specification and Design Understanding of code Implementation of use cases in VC MFC Python wxPython Testing the application Defect Fixing Environment VC MFC Object ARX Visual Studio VSS AutoCAD 2005 AVEVA PDMS Python wxPython Tribon M3 Education BTech in Computer Science and Information Technology Jawaharlal Nehru Technological University Hyderabad Andhra Pradesh 2000 to 2004 Additional Information Technical Skill Set Big Data Ecosystem Hadoop MapReduce HDFS Yarn Hbase Hive Pig Flume Kafka Oozie Hue Spark Ambari Zookeeper Hortonworks MapR Programming LanguagesIDEs C VC Java Python Scala Eclipse Visual Studio Web Technologies HTML XML CSS JavaScript JQuery AngularJS Methodologies Agile UML Design Patterns APIs ObjectARX AutoCAD Real DWG Google Maps API MFC Operating System Windows Linux Database Oracle SQL PLSQL Software Applications AutoCAD AVEVA NET Workhub Dashboard GeoServer TribonM3 Versioning Control VSS StarTeam TFS Cloud Infrastructure Amazon Web Services AWS EMR S3 Dynamo DB etc Others Enterprise Architect Visio MS Project",
    "entities": [
        "HTML CSS JQuery Responsible for Hadoop Admin",
        "Use cases Programs",
        "Resource",
        "Versioning Control Unit",
        "SNS",
        "Development and Testing",
        "HTML CSS JQuery JavaScript Implemented",
        "HDFS",
        "GIS System",
        "JMS",
        "Project GenesysGDA Description GDA",
        "Delta",
        "Responsibilities Capturing",
        "Requirements",
        "Running",
        "Spark Implementation",
        "Hadoop",
        "XML",
        "Atlanta",
        "CAD",
        "AWS Services",
        "Module",
        "Automated",
        "Design Validation",
        "Amazon",
        "Computer Science and Information Technology Jawaharlal",
        "PDF Generator GDA",
        "Job Management Application",
        "Offshore",
        "Project Graphical Documentation Automation Description GDA",
        "Solution Architecture",
        "GDA",
        "Google Maps",
        "Defect Fixing Environment",
        "CAS",
        "MSSOAP1dll for SOAP",
        "Telecom",
        "MSAcess Database Programs",
        "Review",
        "Linux",
        "Customer Preparing Technical",
        "AWS Write",
        "the International Association of Classification Societies",
        "Apache Hadoop MapReduce HDFS",
        "Chevron Shell",
        "Hbase Responsibility",
        "Spark",
        "the JBP Rules Responsibilities Understanding of Specifications and Design Rule Suites",
        "Perform",
        "HIVE",
        "EarthLink Inc",
        "Created Hbase",
        "AWS",
        "2D",
        "Project CAD Customization Description",
        "PIG",
        "HDFS Responsible",
        "VC Implemented Software Protection",
        "java",
        "VBB BOSO ARK Project",
        "Versioning Control",
        "SQL",
        "Software Solutions",
        "GDA Description",
        "Data Analysis",
        "ANS",
        "Implementation of Technical",
        "Big Data",
        "Versioning Control Prepared Module",
        "HDFS Created",
        "Hive Ping",
        "BP etc Responsibilities Closely",
        "Aladdin",
        "GIS",
        "EPC Power Shipbuilding",
        "IntegrationITT",
        "PigHive",
        "Coach",
        "Cloudera Excellent",
        "Visual Studio",
        "Responsibility",
        "Arc",
        "Big DataHadoop",
        "VC",
        "Belgium",
        "DevelopmentDev Integration",
        "Arc Software",
        "Intergraph",
        "Project Joint Bulker Project Description The Joint Bulker",
        "Project GISCAD System Integration Description Objective",
        "PythonpySpark",
        "GeoServer",
        "Nehru Technological University Hyderabad Andhra Pradesh",
        "Responsibility of Quality",
        "MapReduce",
        "JBP Rules",
        "NoSQL",
        "ARX Visual Studio",
        "lDeveloperspanBig Data Solution Architect Senior Hadoop DeveloperBig Data Solution Architect AVEVA Inc Cumming",
        "NEO",
        "GeoServer HTML CSS JQuery"
    ],
    "experience": "Experience in using Custom distributions like Hortonworks MapR and Cloudera Excellent in technical organization skills communication skills Collaboration skills Work Experience Senior Hadoop DeveloperBig Data Solution Architect AVEVA Inc Huntsville AL March 2014 to March 2017 Description AVEVA provides engineeringplants Software Solutions to industries such as Process Plants EngineeringContracting companies such as EPC Power Shipbuilding It is a nichespecialized segment As now a days many Oil companies are facing problems like Oil is hard to find Oil is expensive to produce human safety etc Our objective is to provide a solution using Big DataHadoop technologies to overcome the problem reported by customers like Chevron Shell and BP etc Responsibilities Closely work with the customers to understand the Challenges and requirements Analyze the customer data to provide a solutions Installed and configured Apache Hadoop MapReduce HDFS developed multiple MapReduce jobs in Java for data cleaning and preprocessing Loading the customer data into HDFS using Hadoop commands Processing and analyze the data using MapReduce jobs Configure Apache Kafka producer and consumer coding part in java to establish connection from data sources and loaded into HDFS Created HIVE tables to store the processed results in tabular format Written PIG scripts to process the HDFS data Worked on pulling the data from MySQL database into HDFS using Sqoop Developed Web application for Data Analysis in reporting format using HTML CSS JQuery Responsible for Hadoop Admin related tasks like debugging performance finetuning and monitoring for daytoday activities Designed and implemented Apache Spark streaming application using PythonpySpark and Scala Load and transform large setspetabytes of structured semistructured and unstructured data Strong expertise in writing and implementing PigHive scripts UDFs Experience in defining and coordination of job flows Good experience in NoSQL database like Hbase Responsibility to manage data coming from different sources Responsibility to write and implement python Scala coding for Spark implementations Responsible to implement the Spark applications on AWS Write a integration tool in python to connect to AVEVA NET Workhub and Dashboard from Hadoop ecosystem Involved in the loading data from Linux file system to HDFS Responsible for managing systems on AWS platforms Automated all jobs in Linux shell scripting As a technical leader ensure that ontime delivery of the agreed delivery Ensuring that the development process is properly focused and controlled Managing risks and issues at the development with in estimated time escalating to project manager as required Running the daily meetings ensuring they are timely focused and brief Environment Java Hadoop HDFS Hive Pig Flume Sqoop Hbase Oozie Spark Kafka Zookeeper Ambari Hue Python Linux Eclipse Hortonworks AWS Hadoop Developer EarthLink Inc Atlanta GA January 2013 to March 2014 Description Traditional GIS applications like GeoServer is limited in dealing with big data challenges including versatile data forms processing parallel computing and dynamic mapping and visualization As a part of predictive analysis we have studied the GIS data and provided solutions Responsibilities Gathered requirements and design the application Installation and Configuration of GeoServer and Hadoop ecosystem tools Installation and Configuration of Tomcat for Development and Testing environment Loading of Shape files into GeoServer Loading the GIS data into HDFS Used MapReduce framework to identify Delta updates and to refresh persistent files PIG and HIVE extensively used for data analyses Responsible to manage GIS data coming from different sources Developed PIG scripts in the areas where extensive coding needs to be reduced Experienced in MapReduce programs to load the data from application generated log file to Hbase database Write a reusable and tested code in python for Spark Implementation Extensively used Python in Big Data implementations Created Hbase tables to store variable data formats of data coming from several data sources Analyzing the data with Hive Ping and Hadoop streaming Responsible for monitoring Hadoop cluster manage and review logs performance tuning etc Developed Coverage Maps Web application on top of Google Maps using HTML CSS JQuery JavaScript Implemented RESTful scripting for Querying on Shape files Deployed the application in Development and Testing environments Coordinated the Onsite and Offshore teams for smooth implementation Implemented Agile process for this project Give training and mentorship to team members to make them better on the job Review the completed tasks to ascertain compliance with standards Coach all team members and provide necessary advice and guidance Write and forward regular reports to the management Perform regular appraisal of team members performance to help with improvement Environment HTML CSS JavaScript JQuery Apache Tomcat Linux GeoServer Eclipse Java Hadoop HDFS Hive Pig Flume Oozie Zookeeper Linux Hue Ambari Hortonworks Team Lead Techno Functional Architect Proximus Group Brussels BE January 2010 to December 2012 Project Graphical Documentation Automation Description GDA provides to the NEO division several tools to create or modify the digital graphical documentation of the local loop network These softwares give also several facilities to manage control and export data from this documentation During this period below technical functional enhancements done VBB BOSO ARK Project ARKIII PDF Generator GDA 64Bit Migration GDA Migration from 2006 to 2012 Responsibilities Requirements capturing and Analysis of the requirements Functional design and Technical design of the requirements Resource allocation and assigning the tasks to team members Creating the required classes for the development Responsibility of Quality of output to customer Developed use cases in VC ObjectARX Implemented AutoCAD migration project from AutoCAD 2006 to AutoCAD 2012 Implemented AutoCAD 64bit project from AutoCAD 32bit to AutoCAD 64bit Embedded Google Maps into GDA application for interactive selection of area Setting up of DevelopmentDev Integration ITT acceptanceUAT environments for application deployment and testing Weekly meetings with customer regarding the health of application and current project activities Environment SQL Enterprise Architect VSS AutoCAD Map 3D ObjectARX 2006 to 2012 Object ARX Developer ObjectARX Brussels BE July 2009 to January 2010 Project JMSFTB for GDA Description The main objective of this project is to attach Job Management Application attributes like JMS number JMS driver and JMS building block number to IVP planned objects like Cables Distributor etc JMS application will call a web service which in turn will call a stored procedure in CAS database to collect information related to all planned cables and street cabinets associated with a particular JMS i d Responsibilities Implemented use cases in VC and Object ARX Prepared Userfriendly interfaces using MFC Implementation of various PLSQL procedures Used Visual Source safe for doing proper Versioning Control Unit testing and integration testing Prepared technical design documentation for use cases Environment VC Developer Neilsoft Limited Pune Maharashtra March 2009 to July 2009 Project Arc Software protection and Camera implementation Description Arc Software protection main aim of the project is to create the evaluation edition and standard edition for the Arc software using Aladdin software locking system Camera Implementation Main aim of the project is to implement the camera entity like AutoCAD camera feature in the arc software Responsibilities Capturing requirements from client Understanding the requirements and doing functional design Implementation of Technical design for software protection Developed the application using VC Implemented Software Protection using Aladdin HASP softwares Module test design for the project Unit testing and integration testing Prepared userfriendly interfaces using MFC Environment VC Aladdin HASP Codejack xtreme toolkit Visual Studio VSS ObjectARX Developer Proximus Group Brussels BE December 2007 to February 2009 Project GenesysGDA Description GDA provides to the ANS division several tools to create or modify the digital graphical documentation of the local loop network These softwares give also several facilities to manage control and export data from this documentation The graphical documentation can be defined as A description of all the underground cables splices optical fibers and tubes on road map and cable schemes An input for exploitation followup of the logical connections and planning obvious way to overview the network Complementary to the technical documentation ABR ITR An input for SNS The graphical documentation is stored in the Scan Search SNS document management system Responsibilities Understand the requirements and preparing technical design for use cases Implemented use cases in VC and Object ARX Prepared Userfriendly interfaces using MFC Implementation of various PLSQL procedures Used Visual Source safe for doing proper Versioning Control Prepared Module test design for use cases Setting up of developmentDEV IntegrationITT acceptanceUAT environments for deployment and testing Unit testing and integration testing Prepared documentation for use cases Environment VC Developer Neilsoft Limited Pune Maharashtra June 2007 to November 2007 Project GISCAD System Integration Description Objective of this software application is to exchange the data between Intergraphs GIS system and AutoCAD 2006 in order to have better control on the data flow and flexibility to the designers for editing the drawings XML will be used as an intermediate medium for data exchange between GIS System and this application This application has 4 modules XML2DWG module converts XML document generated by Intergraph GIS system into AutoCAD Drawing DWG2XML module converts AutoCAD drawing into XML document which is input to Intergraph GIS System Asset numbering tool assign asset numbers to new features Design Validation tool validates the AutoCAD drawing before converting into XML Responsibilities Understanding the Requirements given by Customer Preparing Technical design for the assigned Use cases Programs are developed Using MSADO15dll for retrieving data from MSAcess Database Programs are developed using MSSOAP1dll for SOAP programming Used Visual Source safe for doing proper Versioning Control Unit testing and integration testing Environment VC ObjectARX RealDWG MFC SOAP MS Access Visual Studio AutoCAD 2006 VSS Java Developer NAPA Oy Helsinki FI November 2005 to May 2007 Project Joint Bulker Project Description The Joint Bulker project JBP Rules developed by the International Association of Classification Societies IACS for both single and double side bulk carriers are expected to come into force for ships contracted for construction on or after 1st April 2006 The purpose of the project is to develop and implement computer software components to support the Joint Bulker Project IACS Common Structural Rules for Bulk Carriers checking process This is to be carried out by modeling the most common structural elements necessary for checking the JBP Rules Responsibilities Understanding of Specifications and Design Rule Suites development in Java and XML Implementation of test cases using JUnit Framework Used BIRT for implementing Reporting tool Environment Java Junit BIRT Eclipse Star Team SQL Enterprise Architect VSS AutoCAD Map 3D ObjectARX 2006 to 2006 Python VC Developer Neilsoft Limited Pune Maharashtra March 2005 to November 2005 Project CAD Customization Description The project was for developing the interface software for exporting the 2D sheets created in PDMS to AUTOCAD After exporting the sheet to ACAD user was able to add different types of customized annotations to the elements in the sheet The synchronization was maintained between the 2 CAD systems with the help of COMSINK function implementation in ACAD and PDMS Responsibilities Understanding of Specification and Design Understanding of code Implementation of use cases in VC MFC Python wxPython Testing the application Defect Fixing Environment VC MFC Object ARX Visual Studio VSS AutoCAD 2005 AVEVA PDMS Python wxPython Tribon M3 Education BTech in Computer Science and Information Technology Jawaharlal Nehru Technological University Hyderabad Andhra Pradesh 2000 to 2004 Additional Information Technical Skill Set Big Data Ecosystem Hadoop MapReduce HDFS Yarn Hbase Hive Pig Flume Kafka Oozie Hue Spark Ambari Zookeeper Hortonworks MapR Programming LanguagesIDEs C VC Java Python Scala Eclipse Visual Studio Web Technologies HTML XML CSS JavaScript JQuery AngularJS Methodologies Agile UML Design Patterns APIs ObjectARX AutoCAD Real DWG Google Maps API MFC Operating System Windows Linux Database Oracle SQL PLSQL Software Applications AutoCAD AVEVA NET Workhub Dashboard GeoServer TribonM3 Versioning Control VSS StarTeam TFS Cloud Infrastructure Amazon Web Services AWS EMR S3 Dynamo DB etc Others Enterprise Architect Visio MS Project",
    "extracted_keywords": [
        "Senior",
        "Hadoop",
        "DeveloperBig",
        "Data",
        "Solution",
        "Architect",
        "Senior",
        "Hadoop",
        "span",
        "lDeveloperspanBig",
        "Data",
        "Solution",
        "Architect",
        "Senior",
        "Hadoop",
        "DeveloperBig",
        "Data",
        "Solution",
        "Architect",
        "AVEVA",
        "Inc",
        "Cumming",
        "GA",
        "years",
        "experience",
        "IT",
        "industry",
        "Software",
        "developer",
        "lead",
        "Solution",
        "Architecture",
        "Project",
        "Manager",
        "domains",
        "years",
        "client",
        "facing",
        "experience",
        "Europe",
        "Finland",
        "Belgium",
        "years",
        "experience",
        "Big",
        "Data",
        "Hadoop",
        "MapReduce",
        "HDFS",
        "Yarn",
        "Hbase",
        "Hive",
        "Pig",
        "Sqoop",
        "Flume",
        "Spark",
        "Kafka",
        "Oozie",
        "Hue",
        "Zookeeper",
        "Ambari",
        "Hortonworks",
        "MapR",
        "Hands",
        "experience",
        "data",
        "streaming",
        "solutions",
        "Apache",
        "Spark",
        "Kafka",
        "HBase",
        "Spark",
        "Streaming",
        "APIpySpark",
        "Kafka",
        "API",
        "file",
        "formats",
        "Avro",
        "Sequence",
        "JSON",
        "understanding",
        "Big",
        "DataHadoop",
        "applications",
        "AWS",
        "Services",
        "Amazon",
        "S3",
        "EMRFS",
        "EMR",
        "RDS",
        "Airflow",
        "Certified",
        "Scrum",
        "master",
        "Hands",
        "experience",
        "Object",
        "analysis",
        "design",
        "programming",
        "C",
        "Java",
        "Python",
        "domain",
        "knowledge",
        "Telecom",
        "GIS",
        "Oil",
        "Gas",
        "Marine",
        "Electrical",
        "Hands",
        "experience",
        "GIS",
        "applications",
        "CAD",
        "application",
        "customization",
        "GeoServer",
        "HTML",
        "CSS",
        "JQuery",
        "JavaScript",
        "ObjectARX",
        "Hands",
        "experience",
        "SQL",
        "PLSQL",
        "Experience",
        "Custom",
        "distributions",
        "Hortonworks",
        "MapR",
        "Cloudera",
        "Excellent",
        "organization",
        "communication",
        "Collaboration",
        "skills",
        "Work",
        "Experience",
        "Senior",
        "Hadoop",
        "DeveloperBig",
        "Data",
        "Solution",
        "Architect",
        "AVEVA",
        "Inc",
        "Huntsville",
        "AL",
        "March",
        "March",
        "Description",
        "AVEVA",
        "engineeringplants",
        "Software",
        "Solutions",
        "industries",
        "Process",
        "Plants",
        "EngineeringContracting",
        "companies",
        "EPC",
        "Power",
        "Shipbuilding",
        "segment",
        "days",
        "Oil",
        "companies",
        "problems",
        "Oil",
        "Oil",
        "safety",
        "objective",
        "solution",
        "Big",
        "DataHadoop",
        "technologies",
        "problem",
        "customers",
        "Chevron",
        "Shell",
        "BP",
        "Responsibilities",
        "customers",
        "Challenges",
        "requirements",
        "customer",
        "data",
        "solutions",
        "Apache",
        "Hadoop",
        "MapReduce",
        "MapReduce",
        "jobs",
        "Java",
        "data",
        "Loading",
        "customer",
        "data",
        "HDFS",
        "Hadoop",
        "commands",
        "Processing",
        "data",
        "MapReduce",
        "jobs",
        "Configure",
        "Apache",
        "Kafka",
        "producer",
        "consumer",
        "part",
        "connection",
        "data",
        "sources",
        "HDFS",
        "HIVE",
        "tables",
        "results",
        "format",
        "PIG",
        "scripts",
        "HDFS",
        "data",
        "data",
        "MySQL",
        "database",
        "HDFS",
        "Sqoop",
        "Developed",
        "Web",
        "application",
        "Data",
        "Analysis",
        "reporting",
        "format",
        "HTML",
        "CSS",
        "JQuery",
        "Responsible",
        "Hadoop",
        "Admin",
        "tasks",
        "performance",
        "monitoring",
        "daytoday",
        "activities",
        "Apache",
        "Spark",
        "streaming",
        "application",
        "PythonpySpark",
        "Scala",
        "Load",
        "setspetabytes",
        "data",
        "expertise",
        "PigHive",
        "scripts",
        "UDFs",
        "Experience",
        "coordination",
        "job",
        "experience",
        "NoSQL",
        "database",
        "Hbase",
        "Responsibility",
        "data",
        "sources",
        "Responsibility",
        "python",
        "Scala",
        "Spark",
        "implementations",
        "Spark",
        "applications",
        "AWS",
        "integration",
        "tool",
        "python",
        "NET",
        "Workhub",
        "Dashboard",
        "Hadoop",
        "ecosystem",
        "loading",
        "data",
        "Linux",
        "file",
        "system",
        "systems",
        "AWS",
        "platforms",
        "jobs",
        "Linux",
        "shell",
        "scripting",
        "leader",
        "delivery",
        "delivery",
        "development",
        "process",
        "Managing",
        "risks",
        "issues",
        "development",
        "time",
        "project",
        "manager",
        "meetings",
        "Environment",
        "Java",
        "Hadoop",
        "HDFS",
        "Hive",
        "Pig",
        "Flume",
        "Sqoop",
        "Hbase",
        "Oozie",
        "Spark",
        "Kafka",
        "Zookeeper",
        "Ambari",
        "Hue",
        "Python",
        "Linux",
        "Eclipse",
        "Hortonworks",
        "AWS",
        "Hadoop",
        "Developer",
        "EarthLink",
        "Inc",
        "Atlanta",
        "GA",
        "January",
        "March",
        "Description",
        "GIS",
        "applications",
        "GeoServer",
        "data",
        "challenges",
        "data",
        "forms",
        "computing",
        "mapping",
        "visualization",
        "part",
        "analysis",
        "GIS",
        "data",
        "solutions",
        "Responsibilities",
        "Gathered",
        "requirements",
        "application",
        "Installation",
        "Configuration",
        "GeoServer",
        "Hadoop",
        "ecosystem",
        "tools",
        "Installation",
        "Configuration",
        "Tomcat",
        "Development",
        "Testing",
        "environment",
        "Loading",
        "Shape",
        "files",
        "GeoServer",
        "Loading",
        "GIS",
        "data",
        "HDFS",
        "MapReduce",
        "framework",
        "Delta",
        "updates",
        "files",
        "PIG",
        "HIVE",
        "data",
        "analyses",
        "GIS",
        "data",
        "sources",
        "PIG",
        "scripts",
        "areas",
        "MapReduce",
        "programs",
        "data",
        "application",
        "log",
        "file",
        "Hbase",
        "database",
        "code",
        "python",
        "Spark",
        "Implementation",
        "Python",
        "Big",
        "Data",
        "implementations",
        "Created",
        "Hbase",
        "data",
        "formats",
        "data",
        "data",
        "sources",
        "data",
        "Hive",
        "Ping",
        "Hadoop",
        "streaming",
        "Hadoop",
        "cluster",
        "manage",
        "review",
        "logs",
        "performance",
        "Developed",
        "Coverage",
        "Maps",
        "Web",
        "application",
        "top",
        "Google",
        "Maps",
        "HTML",
        "CSS",
        "JQuery",
        "JavaScript",
        "scripting",
        "Querying",
        "Shape",
        "files",
        "application",
        "Development",
        "Testing",
        "environments",
        "Onsite",
        "Offshore",
        "teams",
        "implementation",
        "process",
        "project",
        "training",
        "mentorship",
        "members",
        "job",
        "Review",
        "tasks",
        "compliance",
        "standards",
        "Coach",
        "team",
        "members",
        "advice",
        "guidance",
        "Write",
        "reports",
        "management",
        "appraisal",
        "team",
        "members",
        "improvement",
        "Environment",
        "HTML",
        "CSS",
        "JavaScript",
        "JQuery",
        "Apache",
        "Tomcat",
        "Linux",
        "GeoServer",
        "Eclipse",
        "Java",
        "Hadoop",
        "HDFS",
        "Hive",
        "Pig",
        "Flume",
        "Oozie",
        "Zookeeper",
        "Linux",
        "Hue",
        "Ambari",
        "Hortonworks",
        "Team",
        "Lead",
        "Techno",
        "Functional",
        "Architect",
        "Proximus",
        "Group",
        "Brussels",
        "January",
        "December",
        "Project",
        "Graphical",
        "Documentation",
        "Automation",
        "Description",
        "GDA",
        "NEO",
        "division",
        "tools",
        "documentation",
        "loop",
        "network",
        "softwares",
        "facilities",
        "control",
        "export",
        "data",
        "documentation",
        "period",
        "enhancements",
        "VBB",
        "BOSO",
        "ARK",
        "Project",
        "ARKIII",
        "PDF",
        "Generator",
        "GDA",
        "64Bit",
        "Migration",
        "GDA",
        "Migration",
        "Responsibilities",
        "Requirements",
        "Analysis",
        "requirements",
        "design",
        "design",
        "requirements",
        "Resource",
        "allocation",
        "tasks",
        "team",
        "members",
        "classes",
        "development",
        "Responsibility",
        "Quality",
        "output",
        "customer",
        "use",
        "cases",
        "VC",
        "ObjectARX",
        "migration",
        "project",
        "project",
        "32bit",
        "Google",
        "Maps",
        "GDA",
        "application",
        "selection",
        "area",
        "DevelopmentDev",
        "Integration",
        "ITT",
        "acceptanceUAT",
        "environments",
        "application",
        "deployment",
        "meetings",
        "customer",
        "health",
        "application",
        "project",
        "activities",
        "Environment",
        "SQL",
        "Enterprise",
        "Architect",
        "VSS",
        "AutoCAD",
        "Map",
        "ObjectARX",
        "Object",
        "ARX",
        "Developer",
        "ObjectARX",
        "Brussels",
        "July",
        "January",
        "Project",
        "JMSFTB",
        "GDA",
        "Description",
        "objective",
        "project",
        "Job",
        "Management",
        "Application",
        "JMS",
        "number",
        "JMS",
        "driver",
        "JMS",
        "building",
        "block",
        "number",
        "IVP",
        "objects",
        "Cables",
        "Distributor",
        "JMS",
        "application",
        "web",
        "service",
        "turn",
        "procedure",
        "CAS",
        "database",
        "information",
        "cables",
        "street",
        "cabinets",
        "JMS",
        "i",
        "d",
        "Responsibilities",
        "use",
        "cases",
        "VC",
        "Object",
        "ARX",
        "Prepared",
        "interfaces",
        "MFC",
        "Implementation",
        "PLSQL",
        "procedures",
        "Visual",
        "Source",
        "Versioning",
        "Control",
        "Unit",
        "testing",
        "integration",
        "testing",
        "Prepared",
        "design",
        "documentation",
        "use",
        "cases",
        "Environment",
        "VC",
        "Developer",
        "Neilsoft",
        "Limited",
        "Pune",
        "Maharashtra",
        "March",
        "July",
        "Project",
        "Arc",
        "Software",
        "protection",
        "Camera",
        "implementation",
        "Description",
        "Arc",
        "Software",
        "protection",
        "aim",
        "project",
        "evaluation",
        "edition",
        "edition",
        "Arc",
        "software",
        "Aladdin",
        "software",
        "system",
        "Camera",
        "Implementation",
        "Main",
        "aim",
        "project",
        "camera",
        "entity",
        "camera",
        "feature",
        "arc",
        "software",
        "Responsibilities",
        "Capturing",
        "requirements",
        "client",
        "requirements",
        "design",
        "Implementation",
        "design",
        "software",
        "protection",
        "application",
        "VC",
        "Implemented",
        "Software",
        "Protection",
        "Aladdin",
        "HASP",
        "Module",
        "test",
        "design",
        "project",
        "Unit",
        "testing",
        "integration",
        "testing",
        "interfaces",
        "MFC",
        "Environment",
        "VC",
        "Aladdin",
        "HASP",
        "Codejack",
        "xtreme",
        "toolkit",
        "Visual",
        "Studio",
        "VSS",
        "ObjectARX",
        "Developer",
        "Proximus",
        "Group",
        "Brussels",
        "December",
        "February",
        "Project",
        "GenesysGDA",
        "Description",
        "GDA",
        "ANS",
        "division",
        "tools",
        "documentation",
        "loop",
        "network",
        "softwares",
        "facilities",
        "control",
        "export",
        "data",
        "documentation",
        "documentation",
        "description",
        "cables",
        "splices",
        "fibers",
        "tubes",
        "road",
        "map",
        "cable",
        "schemes",
        "input",
        "exploitation",
        "followup",
        "connections",
        "way",
        "network",
        "Complementary",
        "documentation",
        "ABR",
        "ITR",
        "input",
        "SNS",
        "documentation",
        "Scan",
        "Search",
        "SNS",
        "document",
        "management",
        "system",
        "Responsibilities",
        "requirements",
        "design",
        "use",
        "cases",
        "use",
        "cases",
        "VC",
        "Object",
        "ARX",
        "Prepared",
        "interfaces",
        "MFC",
        "Implementation",
        "PLSQL",
        "procedures",
        "Visual",
        "Source",
        "Versioning",
        "Control",
        "Prepared",
        "Module",
        "test",
        "design",
        "use",
        "cases",
        "developmentDEV",
        "IntegrationITT",
        "acceptanceUAT",
        "environments",
        "deployment",
        "testing",
        "Unit",
        "testing",
        "integration",
        "testing",
        "Prepared",
        "documentation",
        "use",
        "cases",
        "Environment",
        "VC",
        "Developer",
        "Neilsoft",
        "Limited",
        "Pune",
        "Maharashtra",
        "June",
        "November",
        "Project",
        "GISCAD",
        "System",
        "Integration",
        "Description",
        "Objective",
        "software",
        "application",
        "data",
        "Intergraphs",
        "GIS",
        "system",
        "order",
        "control",
        "data",
        "flow",
        "flexibility",
        "designers",
        "drawings",
        "XML",
        "medium",
        "data",
        "exchange",
        "GIS",
        "System",
        "application",
        "application",
        "modules",
        "module",
        "converts",
        "XML",
        "document",
        "Intergraph",
        "GIS",
        "system",
        "Drawing",
        "DWG2XML",
        "module",
        "converts",
        "XML",
        "document",
        "input",
        "Intergraph",
        "GIS",
        "System",
        "Asset",
        "numbering",
        "tool",
        "asset",
        "numbers",
        "features",
        "Design",
        "Validation",
        "tool",
        "drawing",
        "XML",
        "Responsibilities",
        "Requirements",
        "Customer",
        "Preparing",
        "Technical",
        "design",
        "Use",
        "cases",
        "Programs",
        "data",
        "MSAcess",
        "Database",
        "Programs",
        "MSSOAP1dll",
        "SOAP",
        "programming",
        "Visual",
        "Source",
        "Versioning",
        "Control",
        "Unit",
        "testing",
        "integration",
        "testing",
        "Environment",
        "VC",
        "ObjectARX",
        "RealDWG",
        "MFC",
        "SOAP",
        "MS",
        "Access",
        "Visual",
        "Studio",
        "VSS",
        "Java",
        "Developer",
        "NAPA",
        "Oy",
        "Helsinki",
        "FI",
        "November",
        "May",
        "Project",
        "Joint",
        "Bulker",
        "Project",
        "Description",
        "Joint",
        "Bulker",
        "project",
        "JBP",
        "Rules",
        "International",
        "Association",
        "Classification",
        "Societies",
        "IACS",
        "side",
        "carriers",
        "force",
        "ships",
        "construction",
        "April",
        "purpose",
        "project",
        "computer",
        "software",
        "components",
        "Joint",
        "Bulker",
        "Project",
        "IACS",
        "Common",
        "Structural",
        "Rules",
        "Bulk",
        "Carriers",
        "process",
        "elements",
        "JBP",
        "Rules",
        "Responsibilities",
        "Understanding",
        "Specifications",
        "Design",
        "Rule",
        "Suites",
        "development",
        "Java",
        "XML",
        "Implementation",
        "test",
        "cases",
        "JUnit",
        "Framework",
        "BIRT",
        "Reporting",
        "tool",
        "Environment",
        "Java",
        "Junit",
        "BIRT",
        "Eclipse",
        "Star",
        "Team",
        "SQL",
        "Enterprise",
        "Architect",
        "VSS",
        "AutoCAD",
        "Map",
        "ObjectARX",
        "Python",
        "VC",
        "Developer",
        "Neilsoft",
        "Limited",
        "Pune",
        "Maharashtra",
        "March",
        "November",
        "Project",
        "CAD",
        "Customization",
        "Description",
        "project",
        "interface",
        "software",
        "2D",
        "sheets",
        "PDMS",
        "AUTOCAD",
        "sheet",
        "ACAD",
        "user",
        "types",
        "annotations",
        "elements",
        "sheet",
        "synchronization",
        "CAD",
        "systems",
        "help",
        "COMSINK",
        "function",
        "implementation",
        "ACAD",
        "PDMS",
        "Responsibilities",
        "Understanding",
        "Specification",
        "Design",
        "Understanding",
        "code",
        "Implementation",
        "use",
        "cases",
        "VC",
        "MFC",
        "Python",
        "wxPython",
        "application",
        "Defect",
        "Fixing",
        "Environment",
        "VC",
        "MFC",
        "Object",
        "ARX",
        "Visual",
        "Studio",
        "VSS",
        "AVEVA",
        "PDMS",
        "Python",
        "wxPython",
        "Tribon",
        "M3",
        "Education",
        "BTech",
        "Computer",
        "Science",
        "Information",
        "Technology",
        "Jawaharlal",
        "Nehru",
        "Technological",
        "University",
        "Hyderabad",
        "Andhra",
        "Pradesh",
        "Additional",
        "Information",
        "Technical",
        "Skill",
        "Set",
        "Big",
        "Data",
        "Ecosystem",
        "Hadoop",
        "MapReduce",
        "HDFS",
        "Yarn",
        "Hbase",
        "Hive",
        "Pig",
        "Flume",
        "Kafka",
        "Oozie",
        "Hue",
        "Spark",
        "Ambari",
        "Zookeeper",
        "Hortonworks",
        "MapR",
        "Programming",
        "LanguagesIDEs",
        "C",
        "VC",
        "Java",
        "Python",
        "Scala",
        "Eclipse",
        "Visual",
        "Studio",
        "Web",
        "Technologies",
        "HTML",
        "XML",
        "CSS",
        "JavaScript",
        "JQuery",
        "Methodologies",
        "Agile",
        "UML",
        "Design",
        "Patterns",
        "APIs",
        "ObjectARX",
        "DWG",
        "Google",
        "Maps",
        "API",
        "MFC",
        "Operating",
        "System",
        "Windows",
        "Linux",
        "Database",
        "Oracle",
        "SQL",
        "PLSQL",
        "Software",
        "Applications",
        "AVEVA",
        "NET",
        "Workhub",
        "Dashboard",
        "GeoServer",
        "Versioning",
        "Control",
        "VSS",
        "StarTeam",
        "TFS",
        "Cloud",
        "Infrastructure",
        "Amazon",
        "Web",
        "Services",
        "AWS",
        "EMR",
        "S3",
        "Dynamo",
        "DB",
        "Others",
        "Enterprise",
        "Architect",
        "Visio",
        "MS",
        "Project"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T21:48:13.366266",
    "resume_data": "Senior Hadoop DeveloperBig Data Solution Architect Senior Hadoop span lDeveloperspanBig Data Solution Architect Senior Hadoop DeveloperBig Data Solution Architect AVEVA Inc Cumming GA 11 years of experience in IT industry as a Software developer Technical lead Solution Architecture and Project Manager in various domains More than 2 years of client facing experience in Europe Finland Belgium 3 years of experience on Big Data ecosystems Hadoop MapReduce HDFS Yarn Hbase Hive Pig Sqoop Flume Spark Kafka Oozie Hue Zookeeper Ambari Hortonworks MapR AWS Hands on experience in creating realtime data streaming solutions using Apache Spark Kafka HBase Spark Streaming APIpySpark Kafka API Experienced working with different file formats Avro Sequence and JSON Good understanding on building Big DataHadoop applications using AWS Services like Amazon S3 EMRFS EMR RDS Airflow etc Certified Scrum master Hands on experience in Object oriented analysis design and programming with C Java Python Strong domain knowledge in Telecom GIS Oil and Gas Marine and Electrical Hands on experience in developing the GIS applications CAD application customization using GeoServer HTML CSS JQuery AngularJS JavaScript AutoCAD ObjectARX RealDWG Hands on experience in SQL and PLSQL Experience in using Custom distributions like Hortonworks MapR and Cloudera Excellent in technical organization skills communication skills Collaboration skills Work Experience Senior Hadoop DeveloperBig Data Solution Architect AVEVA Inc Huntsville AL March 2014 to March 2017 Description AVEVA provides engineeringplants Software Solutions to industries such as Process Plants EngineeringContracting companies such as EPC Power Shipbuilding It is a nichespecialized segment As now a days many Oil companies are facing problems like Oil is hard to find Oil is expensive to produce human safety etc Our objective is to provide a solution using Big DataHadoop technologies to overcome the problem reported by customers like Chevron Shell and BP etc Responsibilities Closely work with the customers to understand the Challenges and requirements Analyze the customer data to provide a solutions Installed and configured Apache Hadoop MapReduce HDFS developed multiple MapReduce jobs in Java for data cleaning and preprocessing Loading the customer data into HDFS using Hadoop commands Processing and analyze the data using MapReduce jobs Configure Apache Kafka producer and consumer coding part in java to establish connection from data sources and loaded into HDFS Created HIVE tables to store the processed results in tabular format Written PIG scripts to process the HDFS data Worked on pulling the data from MySQL database into HDFS using Sqoop Developed Web application for Data Analysis in reporting format using HTML CSS JQuery Responsible for Hadoop Admin related tasks like debugging performance finetuning and monitoring for daytoday activities Designed and implemented Apache Spark streaming application using PythonpySpark and Scala Load and transform large setspetabytes of structured semistructured and unstructured data Strong expertise in writing and implementing PigHive scripts UDFs Experience in defining and coordination of job flows Good experience in NoSQL database like Hbase Responsibility to manage data coming from different sources Responsibility to write and implement python Scala coding for Spark implementations Responsible to implement the Spark applications on AWS Write a integration tool in python to connect to AVEVA NET Workhub and Dashboard from Hadoop ecosystem Involved in the loading data from Linux file system to HDFS Responsible for managing systems on AWS platforms Automated all jobs in Linux shell scripting As a technical leader ensure that ontime delivery of the agreed delivery Ensuring that the development process is properly focused and controlled Managing risks and issues at the development with in estimated time escalating to project manager as required Running the daily meetings ensuring they are timely focused and brief Environment Java Hadoop HDFS Hive Pig Flume Sqoop Hbase Oozie Spark Kafka Zookeeper Ambari Hue Python Linux Eclipse Hortonworks AWS Hadoop Developer EarthLink Inc Atlanta GA January 2013 to March 2014 Description Traditional GIS applications like GeoServer is limited in dealing with big data challenges including versatile data forms processing parallel computing and dynamic mapping and visualization As a part of predictive analysis we have studied the GIS data and provided solutions Responsibilities Gathered requirements and design the application Installation and Configuration of GeoServer and Hadoop ecosystem tools Installation and Configuration of Tomcat for Development and Testing environment Loading of Shape files into GeoServer Loading the GIS data into HDFS Used MapReduce framework to identify Delta updates and to refresh persistent files PIG and HIVE extensively used for data analyses Responsible to manage GIS data coming from different sources Developed PIG scripts in the areas where extensive coding needs to be reduced Experienced in MapReduce programs to load the data from application generated log file to Hbase database Write a reusable and tested code in python for Spark Implementation Extensively used Python in Big Data implementations Created Hbase tables to store variable data formats of data coming from several data sources Analyzing the data with Hive Ping and Hadoop streaming Responsible for monitoring Hadoop cluster manage and review logs performance tuning etc Developed Coverage Maps Web application on top of Google Maps using HTML CSS JQuery JavaScript Implemented RESTful scripting for Querying on Shape files Deployed the application in Development and Testing environments Coordinated the Onsite and Offshore teams for smooth implementation Implemented Agile process for this project Give training and mentorship to team members to make them better on the job Review the completed tasks to ascertain compliance with standards Coach all team members and provide necessary advice and guidance Write and forward regular reports to the management Perform regular appraisal of team members performance to help with improvement Environment HTML CSS JavaScript JQuery Apache Tomcat Linux GeoServer Eclipse Java Hadoop HDFS Hive Pig Flume Oozie Zookeeper Linux Hue Ambari Hortonworks Team Lead Techno Functional Architect Proximus Group Brussels BE January 2010 to December 2012 Project Graphical Documentation Automation Description GDA provides to the NEO division several tools to create or modify the digital graphical documentation of the local loop network These softwares give also several facilities to manage control and export data from this documentation During this period below technical functional enhancements done VBB BOSO ARK Project ARKIII PDF Generator GDA 64Bit Migration GDA Migration from 2006 to 2012 Responsibilities Requirements capturing and Analysis of the requirements Functional design and Technical design of the requirements Resource allocation and assigning the tasks to team members Creating the required classes for the development Responsibility of Quality of output to customer Developed use cases in VC ObjectARX Implemented AutoCAD migration project from AutoCAD 2006 to AutoCAD 2012 Implemented AutoCAD 64bit project from AutoCAD 32bit to AutoCAD 64bit Embedded Google Maps into GDA application for interactive selection of area Setting up of DevelopmentDev Integration ITT acceptanceUAT environments for application deployment and testing Weekly meetings with customer regarding the health of application and current project activities Environment SQL Enterprise Architect VSS AutoCAD Map 3D ObjectARX 2006 to 2012 Object ARX Developer ObjectARX Brussels BE July 2009 to January 2010 Project JMSFTB for GDA Description The main objective of this project is to attach Job Management Application attributes like JMS number JMS driver and JMS building block number to IVP planned objects like Cables Distributor etc JMS application will call a web service which in turn will call a stored procedure in CAS database to collect information related to all planned cables and street cabinets associated with a particular JMS id Responsibilities Implemented use cases in VC and Object ARX Prepared Userfriendly interfaces using MFC Implementation of various PLSQL procedures Used Visual Source safe for doing proper Versioning Control Unit testing and integration testing Prepared technical design documentation for use cases Environment VC Developer Neilsoft Limited Pune Maharashtra March 2009 to July 2009 Project Arc Software protection and Camera implementation Description Arc Software protection main aim of the project is to create the evaluation edition and standard edition for the Arc software using Aladdin software locking system Camera Implementation Main aim of the project is to implement the camera entity like AutoCAD camera feature in the arc software Responsibilities Capturing requirements from client Understanding the requirements and doing functional design Implementation of Technical design for software protection Developed the application using VC Implemented Software Protection using Aladdin HASP softwares Module test design for the project Unit testing and integration testing Prepared userfriendly interfaces using MFC Environment VC Aladdin HASP Codejack xtreme toolkit Visual Studio VSS ObjectARX Developer Proximus Group Brussels BE December 2007 to February 2009 Project GenesysGDA Description GDA provides to the ANS division several tools to create or modify the digital graphical documentation of the local loop network These softwares give also several facilities to manage control and export data from this documentation The graphical documentation can be defined as A description of all the underground cables splices optical fibers and tubes on road map and cable schemes An input for exploitation followup of the logical connections and planning obvious way to overview the network Complementary to the technical documentation ABR ITR An input for SNS The graphical documentation is stored in the Scan Search SNS document management system Responsibilities Understand the requirements and preparing technical design for use cases Implemented use cases in VC and Object ARX Prepared Userfriendly interfaces using MFC Implementation of various PLSQL procedures Used Visual Source safe for doing proper Versioning Control Prepared Module test design for use cases Setting up of developmentDEV IntegrationITT acceptanceUAT environments for deployment and testing Unit testing and integration testing Prepared documentation for use cases Environment VC Developer Neilsoft Limited Pune Maharashtra June 2007 to November 2007 Project GISCAD System Integration Description Objective of this software application is to exchange the data between Intergraphs GIS system and AutoCAD 2006 in order to have better control on the data flow and flexibility to the designers for editing the drawings XML will be used as an intermediate medium for data exchange between GIS System and this application This application has 4 modules XML2DWG module converts XML document generated by Intergraph GIS system into AutoCAD Drawing DWG2XML module converts AutoCAD drawing into XML document which is input to Intergraph GIS System Asset numbering tool assign asset numbers to new features Design Validation tool validates the AutoCAD drawing before converting into XML Responsibilities Understanding the Requirements given by Customer Preparing Technical design for the assigned Use cases Programs are developed Using MSADO15dll for retrieving data from MSAcess Database Programs are developed using MSSOAP1dll for SOAP programming Used Visual Source safe for doing proper Versioning Control Unit testing and integration testing Environment VC ObjectARX RealDWG MFC SOAP MS Access Visual Studio AutoCAD 2006 VSS Java Developer NAPA Oy Helsinki FI November 2005 to May 2007 Project Joint Bulker Project Description The Joint Bulker project JBP Rules developed by the International Association of Classification Societies IACS for both single and double side bulk carriers are expected to come into force for ships contracted for construction on or after 1st April 2006 The purpose of the project is to develop and implement computer software components to support the Joint Bulker Project IACS Common Structural Rules for Bulk Carriers checking process This is to be carried out by modeling the most common structural elements necessary for checking the JBP Rules Responsibilities Understanding of Specifications and Design Rule Suites development in Java and XML Implementation of test cases using JUnit Framework Used BIRT for implementing Reporting tool Environment Java Junit BIRT Eclipse Star Team SQL Enterprise Architect VSS AutoCAD Map 3D ObjectARX 2006 to 2006 Python VC Developer Neilsoft Limited Pune Maharashtra March 2005 to November 2005 Project CAD Customization Description The project was for developing the interface software for exporting the 2D sheets created in PDMS to AUTOCAD After exporting the sheet to ACAD user was able to add different types of customized annotations to the elements in the sheet The synchronization was maintained between the 2 CAD systems with the help of COMSINK function implementation in ACAD and PDMS Responsibilities Understanding of Specification and Design Understanding of code Implementation of use cases in VC MFC Python wxPython Testing the application Defect Fixing Environment VC MFC Object ARX Visual Studio VSS AutoCAD 2005 AVEVA PDMS Python wxPython Tribon M3 Education BTech in Computer Science and Information Technology Jawaharlal Nehru Technological University Hyderabad Andhra Pradesh 2000 to 2004 Additional Information Technical Skill Set Big Data Ecosystem Hadoop MapReduce HDFS Yarn Hbase Hive Pig Flume Kafka Oozie Hue Spark Ambari Zookeeper Hortonworks MapR Programming LanguagesIDEs C VC Java Python Scala Eclipse Visual Studio Web Technologies HTML XML CSS JavaScript JQuery AngularJS Methodologies Agile UML Design Patterns APIs ObjectARX AutoCAD Real DWG Google Maps API MFC Operating System Windows Linux Database Oracle SQL PLSQL Software Applications AutoCAD AVEVA NET Workhub Dashboard GeoServer TribonM3 Versioning Control VSS StarTeam TFS Cloud Infrastructure Amazon Web Services AWS EMR S3 Dynamo DB etc Others Enterprise Architect Visio MS Project",
    "unique_id": "f107f4b1-a6b5-483f-a4bd-5776419c0df2"
}