{
    "clean_data": "Data Scientist Data Scientist Data Scientist TracFone Wireless Inc Mary SC Professional Qualified Data Scientist with over 7 years of experience in Data Science and Analytics including Machine Learning Data Mining and Statistical Analysis Involved in the entire data science project life cycle and actively involved in all the phases including data extraction data cleaning statistical modeling and data visualization with large data sets of structured and unstructured data Having knowledge on Apache Spark and developing data processing and analysis algorithms using Python Understanding of machine learning ML concepts and application of algorithms in nonacademic environments Strong software development background in functional and objectoriented programming Expertized in developing Machine learning algorithm using Python Ability to manipulate transform and analyze abstract data structures such as Dataframes Fundamental understanding of machine learning concepts including training models as well as understanding precision and recall in the real world Strong programming experience in the following R Python Matlab Performed Collection cleansing and verification of structured and unstructured data Experience in visualization tools like Tableau 9 10 for creating dashboards Excellent understanding Agile and Scrum development methodology Used the version control tools like Git Passionate about cleaning insightful information from massive data assets and developing a culture of sound datadriven decision making Ability to maintain a fun casual professional and productive team atmosphere Authorized to work in the US for any employer Work Experience Data Scientist TracFone Wireless Inc Miami FL October 2016 to Present Responsibilities Enhancing data collection procedures to include information that is relevant for building analytic systems Processing cleansing and verifying the integrity of data used for analysis Doing adhoc analysis and presenting results in a clear manner Constant tracking of model performance Excellent understanding of machine learning techniques and algorithms such as Logistic Regression SVM Random Forests Deep Learning etc Worked with Data governance Data quality Data lineage Data architect to design various models Independently coded new programs and designed Tables to load and test the program effectively for the given POCs Extending companys data with third party sources of information when needed Designed data models and data flow diagrams using Erwin and MS Visio As an Architect implemented MDM hub to provide clean consistent data for a SOA implementation Developed Implemented maintained the Conceptual Logical Physical Data Models using Erwin for ForwardReverse Engineered Databases Experience with common data science toolkits such as R Python Spark etc Good applied statistics skills such as statistical sampling testing regression etc Build analytic models using a variety of techniques such as logistic regression risk scorecards and pattern recognition technologies Analyze and understand large amounts of data to determine suitability for use in models and then work to segment the data create variables build models and test those models Work with technical and development teams to deploy models Build Model Performance Reports and Modeling Technical Documentation to support each of the models for the product line Performed Exploratory Data Analysis and Data Visualizations using R and Tableau Perform a proper EDA Univariate and bivariate analysis to understand the intrinsic effectcombined Established Data architecture strategy best practices standards and roadmaps Lead the development and presentation of a data analytics datahub prototype with the help of the other members of the emerging solutions team Involved in analysis of Business requirement Design and Development of High level and Lowlevel designs Unit and Integration testing Worked with several R packages including knitr dplyr SparkR CausalInfer spacetime Interacted with the other departments to understand and identify data needs and requirements Environment UNIX Python 352 MLLib SAS regression logistic regression Hadoop NoSQL Teradata OLTP Random forest OLAP HDFS ODS Data Scientist CNY Asset Management Syracuse NY January 2014 to September 2016 Responsibilities Communicated and coordinated with other departments to collection business requirement Worked on miss value imputation outliers identification with statistical methodologies using Pandas Numpy Participated in features engineering such as feature creating feature scaling and OneHot encoding with Scikitlearn Tackled highly imbalanced Fraud dataset using undersampling with ensemble methods oversampling and cost sensitive algorithms Improved fraud prediction performance by using random forest and gradient boosting for feature selection with Python Scikitlearn Implemented machine learning model logistic regression XGboost with Python Scikit learn Optimized algorithm with stochastic gradient descent algorithm Finetuned the algorithm parameter with manual tuning and automated tuning such as Bayesian Optimization Validated and select models using kfold cross validation confusion matrices and worked on optimizing models for high recall rate Implemented Ensemble Models with majority votes to enhance the efficiency and performance Environments Python scikitlearn pandas Numpy Machine Learning logistic regression XGboost Gradient Descent algorithm Bayesian optimization Tableau Data Scientist PDF Solutions San Jose CA October 2012 to December 2013 Responsibilities Build Analytics systems data structures gather and manipulate data using statistical techniques and predictive modelling to tell people story Designing suite of interactive dashboards which provided an opportunity to scale and measure the statistics of the HR dept which was not possible earlier and schedule and publish reports Provided and created data presentation to reduce biases and telling true story of people by Pulling millions of rows of data using SQL analysis of Data Worked on Machine Learning to compare the Metrics of HR data closely Technology Stack R Machine Learning SQL server and Tableau Building Data capabilities for HR with respect to measure and resolve attrition issues through metrics Analyzing recruiting history and employee performance to identify best candidates for hiring Identify employees for training and succession planning Environments R Python pandas numpy scikitlearn Machine Learning predictive modeling SQL Tableau Big Data Developer Applied Materials Santa Clara CA July 2011 to September 2012 Responsibilities Involved in the process of load transform and analyze data from various sources into HDFS Hadoop Distributed File System using Hive Pig and Sqoop Experienced in handling data from different datasets join and preprocess those using Pig join operations Worked on Pig script to count the number of times a particular URL was opened in a particular duration Developed PIG UDFs for the needed functionality such as custom Pigsloader known as timestamp loader Created Hive tables based on the business requirements Pig scripts and Hive queries were used to analyze the large data sets Scheduling and managing jobs on a Hadoop cluster using Oozie work flow Written MapReduce code to process and parsing the data from various sources and storing parsed data into HBase and Hive using HBase Hive Integration Worked on Developing custom MapReduce programs and User Defined Functions UDFs in Hive to transform the large volumes of data with respect to business requirement Involved in creating Hive tables and working on them using Hive QL Involved in moving all log files generated from various sources to HDFS for further processing through Flume Implemented Frameworks using Java and python to automate the ingestion flow Loading and transforming of large sets of structured and semi structured data Exported filtered data into HBase for fast query Involved in the installation configuration and used the Hadoop ecosystem components such as Map Reduce HDFS Pig Hive Flume HBase Environments Hadoop HDFS Hive Pig Sqoop HBase MapReduce Flume UDFs Python Developer American River Bank Sacramento CA March 2010 to June 2011 Responsibilities Exposure on MultiThreading factory to distribute learning process backtesting and the into various worker processes Different testing methodologies like unit testing Integration testing and web application testing Design and implemented custom scripts Extensive use of version controller Team Foundation Server TFS Test and validated the custom scripts Delivered automated solutions for science models Managed developed and designed a dashboard control panel for customers and Administrators using Oracle DB and VMWare API calls Implemented configuration changes for data models Maintained and updated existing automated solutions Handled potential points of failure through error handling and communication of failure Anticipated potential parts of failure database communication points file system errors Troubleshoot the process execution and worked with other team members to correct them Developed GUI using webapp2 for dynamically displaying the test block documentation and other features of python code using a web browser Interacted with QA to develop test plans from highlevel design documentation Environments Python Mysql HTML Javascript HQL Git Web services Education Bachelors Skills PYTHON 7 years MACHINE LEARNING 5 years Hadoop 3 years HADOOP 3 years DEEP LEARNING 1 year Additional Information TECHNICAL SKILLS Machine Learning classification regression clustering feature engineering deep learning neural networks Programming Languages Python pandas scikitlearn numpy  GraphLab Create R SQL Hadoop MapReduce Sqoop Flume Scala Java Spark PySpark MLlib Operating Systems Linux CentOS Ubuntu Kali Linux Windows MacOS Software Tools PyCharm Jupyter Notebook R studio Tableau Microsoft Office Eclipse IDE",
    "entities": [
        "Architect",
        "Developed Implemented",
        "ODS Data Scientist CNY Asset Management Syracuse",
        "PDF Solutions",
        "Created Hive",
        "Performed Exploratory Data Analysis",
        "Machine",
        "Technology Stack R Machine Learning",
        "MDM",
        "Established Data",
        "Python Scikitlearn Implemented",
        "US",
        "QA",
        "Build",
        "Data Scientist Data Scientist Data Scientist TracFone Wireless Inc Mary SC Professional Qualified Data Scientist",
        "Responsibilities Communicated",
        "CausalInfer",
        "Data Worked on Machine Learning",
        "Identify",
        "the Conceptual Logical Physical Data Models",
        "Programming Languages Python",
        "XGboost Gradient Descent",
        "CA",
        "HBase Hive Integration Worked",
        "Expertized",
        "MultiThreading",
        "webapp2",
        "Numpy Machine Learning",
        "Python Developer American River Bank",
        "Oracle DB",
        "Lowlevel",
        "EDA",
        "Present Responsibilities Enhancing",
        "ML",
        "San Jose",
        "SOA",
        "Sacramento",
        "SQL Tableau",
        "Pigsloader",
        "Bayesian Optimization Validated",
        "Oozie",
        "Tableau Data",
        "highlevel",
        "SQL",
        "Provided",
        "Hadoop",
        "Miami",
        "Tableau Building Data",
        "Dataframes Fundamental",
        "VMWare API",
        "HDFS Hadoop Distributed File System",
        "Tableau Microsoft Office",
        "Data Science and Analytics",
        "Administrators",
        "Maintained",
        "User Defined Functions",
        "ForwardReverse Engineered Databases",
        "OneHot",
        "HBase",
        "Flume Implemented Frameworks",
        "Integration",
        "Apache Spark",
        "Team Foundation",
        "GraphLab",
        "Hive",
        "Tables",
        "Troubleshoot",
        "Additional Information TECHNICAL SKILLS Machine Learning",
        "Ability"
    ],
    "experience": "Experience in visualization tools like Tableau 9 10 for creating dashboards Excellent understanding Agile and Scrum development methodology Used the version control tools like Git Passionate about cleaning insightful information from massive data assets and developing a culture of sound datadriven decision making Ability to maintain a fun casual professional and productive team atmosphere Authorized to work in the US for any employer Work Experience Data Scientist TracFone Wireless Inc Miami FL October 2016 to Present Responsibilities Enhancing data collection procedures to include information that is relevant for building analytic systems Processing cleansing and verifying the integrity of data used for analysis Doing adhoc analysis and presenting results in a clear manner Constant tracking of model performance Excellent understanding of machine learning techniques and algorithms such as Logistic Regression SVM Random Forests Deep Learning etc Worked with Data governance Data quality Data lineage Data architect to design various models Independently coded new programs and designed Tables to load and test the program effectively for the given POCs Extending companys data with third party sources of information when needed Designed data models and data flow diagrams using Erwin and MS Visio As an Architect implemented MDM hub to provide clean consistent data for a SOA implementation Developed Implemented maintained the Conceptual Logical Physical Data Models using Erwin for ForwardReverse Engineered Databases Experience with common data science toolkits such as R Python Spark etc Good applied statistics skills such as statistical sampling testing regression etc Build analytic models using a variety of techniques such as logistic regression risk scorecards and pattern recognition technologies Analyze and understand large amounts of data to determine suitability for use in models and then work to segment the data create variables build models and test those models Work with technical and development teams to deploy models Build Model Performance Reports and Modeling Technical Documentation to support each of the models for the product line Performed Exploratory Data Analysis and Data Visualizations using R and Tableau Perform a proper EDA Univariate and bivariate analysis to understand the intrinsic effectcombined Established Data architecture strategy best practices standards and roadmaps Lead the development and presentation of a data analytics datahub prototype with the help of the other members of the emerging solutions team Involved in analysis of Business requirement Design and Development of High level and Lowlevel designs Unit and Integration testing Worked with several R packages including knitr dplyr SparkR CausalInfer spacetime Interacted with the other departments to understand and identify data needs and requirements Environment UNIX Python 352 MLLib SAS regression logistic regression Hadoop NoSQL Teradata OLTP Random forest OLAP HDFS ODS Data Scientist CNY Asset Management Syracuse NY January 2014 to September 2016 Responsibilities Communicated and coordinated with other departments to collection business requirement Worked on miss value imputation outliers identification with statistical methodologies using Pandas Numpy Participated in features engineering such as feature creating feature scaling and OneHot encoding with Scikitlearn Tackled highly imbalanced Fraud dataset using undersampling with ensemble methods oversampling and cost sensitive algorithms Improved fraud prediction performance by using random forest and gradient boosting for feature selection with Python Scikitlearn Implemented machine learning model logistic regression XGboost with Python Scikit learn Optimized algorithm with stochastic gradient descent algorithm Finetuned the algorithm parameter with manual tuning and automated tuning such as Bayesian Optimization Validated and select models using kfold cross validation confusion matrices and worked on optimizing models for high recall rate Implemented Ensemble Models with majority votes to enhance the efficiency and performance Environments Python scikitlearn pandas Numpy Machine Learning logistic regression XGboost Gradient Descent algorithm Bayesian optimization Tableau Data Scientist PDF Solutions San Jose CA October 2012 to December 2013 Responsibilities Build Analytics systems data structures gather and manipulate data using statistical techniques and predictive modelling to tell people story Designing suite of interactive dashboards which provided an opportunity to scale and measure the statistics of the HR dept which was not possible earlier and schedule and publish reports Provided and created data presentation to reduce biases and telling true story of people by Pulling millions of rows of data using SQL analysis of Data Worked on Machine Learning to compare the Metrics of HR data closely Technology Stack R Machine Learning SQL server and Tableau Building Data capabilities for HR with respect to measure and resolve attrition issues through metrics Analyzing recruiting history and employee performance to identify best candidates for hiring Identify employees for training and succession planning Environments R Python pandas numpy scikitlearn Machine Learning predictive modeling SQL Tableau Big Data Developer Applied Materials Santa Clara CA July 2011 to September 2012 Responsibilities Involved in the process of load transform and analyze data from various sources into HDFS Hadoop Distributed File System using Hive Pig and Sqoop Experienced in handling data from different datasets join and preprocess those using Pig join operations Worked on Pig script to count the number of times a particular URL was opened in a particular duration Developed PIG UDFs for the needed functionality such as custom Pigsloader known as timestamp loader Created Hive tables based on the business requirements Pig scripts and Hive queries were used to analyze the large data sets Scheduling and managing jobs on a Hadoop cluster using Oozie work flow Written MapReduce code to process and parsing the data from various sources and storing parsed data into HBase and Hive using HBase Hive Integration Worked on Developing custom MapReduce programs and User Defined Functions UDFs in Hive to transform the large volumes of data with respect to business requirement Involved in creating Hive tables and working on them using Hive QL Involved in moving all log files generated from various sources to HDFS for further processing through Flume Implemented Frameworks using Java and python to automate the ingestion flow Loading and transforming of large sets of structured and semi structured data Exported filtered data into HBase for fast query Involved in the installation configuration and used the Hadoop ecosystem components such as Map Reduce HDFS Pig Hive Flume HBase Environments Hadoop HDFS Hive Pig Sqoop HBase MapReduce Flume UDFs Python Developer American River Bank Sacramento CA March 2010 to June 2011 Responsibilities Exposure on MultiThreading factory to distribute learning process backtesting and the into various worker processes Different testing methodologies like unit testing Integration testing and web application testing Design and implemented custom scripts Extensive use of version controller Team Foundation Server TFS Test and validated the custom scripts Delivered automated solutions for science models Managed developed and designed a dashboard control panel for customers and Administrators using Oracle DB and VMWare API calls Implemented configuration changes for data models Maintained and updated existing automated solutions Handled potential points of failure through error handling and communication of failure Anticipated potential parts of failure database communication points file system errors Troubleshoot the process execution and worked with other team members to correct them Developed GUI using webapp2 for dynamically displaying the test block documentation and other features of python code using a web browser Interacted with QA to develop test plans from highlevel design documentation Environments Python Mysql HTML Javascript HQL Git Web services Education Bachelors Skills PYTHON 7 years MACHINE LEARNING 5 years Hadoop 3 years HADOOP 3 years DEEP LEARNING 1 year Additional Information TECHNICAL SKILLS Machine Learning classification regression clustering feature engineering deep learning neural networks Programming Languages Python pandas scikitlearn numpy   GraphLab Create R SQL Hadoop MapReduce Sqoop Flume Scala Java Spark PySpark MLlib Operating Systems Linux CentOS Ubuntu Kali Linux Windows MacOS Software Tools PyCharm Jupyter Notebook R studio Tableau Microsoft Office Eclipse IDE",
    "extracted_keywords": [
        "Data",
        "Scientist",
        "Data",
        "Scientist",
        "Data",
        "Scientist",
        "TracFone",
        "Wireless",
        "Inc",
        "Mary",
        "SC",
        "Professional",
        "Qualified",
        "Data",
        "Scientist",
        "years",
        "experience",
        "Data",
        "Science",
        "Analytics",
        "Machine",
        "Learning",
        "Data",
        "Mining",
        "Statistical",
        "Analysis",
        "data",
        "science",
        "project",
        "life",
        "cycle",
        "phases",
        "data",
        "extraction",
        "data",
        "modeling",
        "data",
        "visualization",
        "data",
        "sets",
        "data",
        "knowledge",
        "Apache",
        "Spark",
        "data",
        "processing",
        "analysis",
        "algorithms",
        "Python",
        "Understanding",
        "machine",
        "ML",
        "concepts",
        "application",
        "algorithms",
        "environments",
        "software",
        "development",
        "background",
        "programming",
        "Machine",
        "algorithm",
        "Python",
        "Ability",
        "transform",
        "data",
        "structures",
        "Dataframes",
        "understanding",
        "machine",
        "learning",
        "concepts",
        "training",
        "models",
        "precision",
        "recall",
        "world",
        "programming",
        "experience",
        "R",
        "Python",
        "Matlab",
        "Performed",
        "Collection",
        "cleansing",
        "verification",
        "data",
        "Experience",
        "visualization",
        "tools",
        "Tableau",
        "dashboards",
        "understanding",
        "Agile",
        "Scrum",
        "development",
        "methodology",
        "version",
        "control",
        "tools",
        "Git",
        "Passionate",
        "information",
        "data",
        "assets",
        "culture",
        "decision",
        "Ability",
        "team",
        "atmosphere",
        "US",
        "employer",
        "Work",
        "Experience",
        "Data",
        "Scientist",
        "TracFone",
        "Wireless",
        "Inc",
        "Miami",
        "FL",
        "October",
        "Present",
        "Responsibilities",
        "data",
        "collection",
        "procedures",
        "information",
        "systems",
        "Processing",
        "cleansing",
        "integrity",
        "data",
        "analysis",
        "adhoc",
        "analysis",
        "results",
        "manner",
        "tracking",
        "model",
        "performance",
        "Excellent",
        "understanding",
        "machine",
        "techniques",
        "algorithms",
        "Logistic",
        "Regression",
        "SVM",
        "Random",
        "Forests",
        "Deep",
        "Learning",
        "Data",
        "governance",
        "Data",
        "quality",
        "Data",
        "lineage",
        "Data",
        "architect",
        "models",
        "programs",
        "Tables",
        "program",
        "POCs",
        "companys",
        "data",
        "party",
        "sources",
        "information",
        "data",
        "models",
        "data",
        "flow",
        "diagrams",
        "Erwin",
        "MS",
        "Visio",
        "Architect",
        "MDM",
        "hub",
        "data",
        "SOA",
        "implementation",
        "Implemented",
        "Conceptual",
        "Logical",
        "Physical",
        "Data",
        "Models",
        "Erwin",
        "ForwardReverse",
        "Engineered",
        "Databases",
        "Experience",
        "data",
        "science",
        "toolkits",
        "R",
        "Python",
        "Spark",
        "statistics",
        "skills",
        "sampling",
        "testing",
        "regression",
        "models",
        "variety",
        "techniques",
        "regression",
        "risk",
        "scorecards",
        "pattern",
        "recognition",
        "technologies",
        "Analyze",
        "amounts",
        "data",
        "suitability",
        "use",
        "models",
        "data",
        "variables",
        "models",
        "models",
        "development",
        "teams",
        "models",
        "Build",
        "Model",
        "Performance",
        "Reports",
        "Modeling",
        "Technical",
        "Documentation",
        "models",
        "product",
        "line",
        "Performed",
        "Exploratory",
        "Data",
        "Analysis",
        "Data",
        "Visualizations",
        "R",
        "Tableau",
        "EDA",
        "Univariate",
        "analysis",
        "Established",
        "Data",
        "architecture",
        "strategy",
        "practices",
        "standards",
        "roadmaps",
        "development",
        "presentation",
        "data",
        "analytics",
        "datahub",
        "prototype",
        "help",
        "members",
        "solutions",
        "team",
        "analysis",
        "Business",
        "requirement",
        "Design",
        "Development",
        "level",
        "Lowlevel",
        "designs",
        "Unit",
        "Integration",
        "testing",
        "R",
        "packages",
        "knitr",
        "dplyr",
        "CausalInfer",
        "spacetime",
        "departments",
        "data",
        "needs",
        "requirements",
        "Environment",
        "UNIX",
        "Python",
        "MLLib",
        "SAS",
        "regression",
        "regression",
        "Hadoop",
        "NoSQL",
        "Teradata",
        "OLTP",
        "Random",
        "forest",
        "OLAP",
        "HDFS",
        "ODS",
        "Data",
        "Scientist",
        "CNY",
        "Asset",
        "Management",
        "Syracuse",
        "NY",
        "January",
        "September",
        "Responsibilities",
        "departments",
        "collection",
        "business",
        "requirement",
        "value",
        "imputation",
        "outliers",
        "identification",
        "methodologies",
        "Pandas",
        "Numpy",
        "features",
        "engineering",
        "feature",
        "feature",
        "scaling",
        "OneHot",
        "encoding",
        "Scikitlearn",
        "Tackled",
        "Fraud",
        "dataset",
        "methods",
        "algorithms",
        "fraud",
        "prediction",
        "performance",
        "forest",
        "gradient",
        "feature",
        "selection",
        "Python",
        "Scikitlearn",
        "machine",
        "model",
        "regression",
        "XGboost",
        "Python",
        "Scikit",
        "algorithm",
        "descent",
        "algorithm",
        "algorithm",
        "parameter",
        "tuning",
        "tuning",
        "Bayesian",
        "Optimization",
        "Validated",
        "models",
        "kfold",
        "cross",
        "validation",
        "confusion",
        "matrices",
        "models",
        "recall",
        "rate",
        "Ensemble",
        "Models",
        "majority",
        "votes",
        "efficiency",
        "performance",
        "Environments",
        "Python",
        "Numpy",
        "Machine",
        "Learning",
        "regression",
        "XGboost",
        "Gradient",
        "Descent",
        "algorithm",
        "Bayesian",
        "optimization",
        "Tableau",
        "Data",
        "Scientist",
        "PDF",
        "Solutions",
        "San",
        "Jose",
        "CA",
        "October",
        "December",
        "Responsibilities",
        "Build",
        "Analytics",
        "systems",
        "data",
        "structures",
        "manipulate",
        "data",
        "techniques",
        "modelling",
        "people",
        "story",
        "Designing",
        "suite",
        "dashboards",
        "opportunity",
        "statistics",
        "HR",
        "dept",
        "reports",
        "data",
        "presentation",
        "biases",
        "story",
        "people",
        "millions",
        "rows",
        "data",
        "SQL",
        "analysis",
        "Data",
        "Machine",
        "Learning",
        "Metrics",
        "HR",
        "data",
        "Technology",
        "Stack",
        "R",
        "Machine",
        "Learning",
        "SQL",
        "server",
        "Tableau",
        "Building",
        "Data",
        "capabilities",
        "HR",
        "respect",
        "attrition",
        "issues",
        "metrics",
        "recruiting",
        "history",
        "employee",
        "performance",
        "candidates",
        "employees",
        "training",
        "succession",
        "planning",
        "Environments",
        "R",
        "Python",
        "numpy",
        "scikitlearn",
        "Machine",
        "Learning",
        "modeling",
        "SQL",
        "Tableau",
        "Big",
        "Data",
        "Developer",
        "Materials",
        "Santa",
        "Clara",
        "CA",
        "July",
        "September",
        "Responsibilities",
        "process",
        "load",
        "transform",
        "data",
        "sources",
        "HDFS",
        "Hadoop",
        "Distributed",
        "File",
        "System",
        "Hive",
        "Pig",
        "Sqoop",
        "data",
        "datasets",
        "Pig",
        "join",
        "operations",
        "Pig",
        "script",
        "number",
        "times",
        "URL",
        "duration",
        "PIG",
        "UDFs",
        "functionality",
        "custom",
        "Pigsloader",
        "timestamp",
        "loader",
        "Created",
        "Hive",
        "tables",
        "business",
        "requirements",
        "scripts",
        "Hive",
        "queries",
        "data",
        "Scheduling",
        "managing",
        "jobs",
        "Hadoop",
        "cluster",
        "Oozie",
        "work",
        "flow",
        "MapReduce",
        "code",
        "data",
        "sources",
        "data",
        "HBase",
        "Hive",
        "HBase",
        "Hive",
        "Integration",
        "custom",
        "MapReduce",
        "programs",
        "User",
        "Defined",
        "Functions",
        "UDFs",
        "Hive",
        "volumes",
        "data",
        "respect",
        "business",
        "requirement",
        "Hive",
        "tables",
        "Hive",
        "QL",
        "log",
        "files",
        "sources",
        "HDFS",
        "processing",
        "Flume",
        "Frameworks",
        "Java",
        "ingestion",
        "flow",
        "Loading",
        "transforming",
        "sets",
        "data",
        "data",
        "HBase",
        "query",
        "installation",
        "configuration",
        "Hadoop",
        "ecosystem",
        "components",
        "Map",
        "Reduce",
        "HDFS",
        "Pig",
        "Hive",
        "Flume",
        "HBase",
        "Environments",
        "Hadoop",
        "HDFS",
        "Hive",
        "Pig",
        "Sqoop",
        "HBase",
        "MapReduce",
        "Flume",
        "UDFs",
        "Python",
        "Developer",
        "American",
        "River",
        "Bank",
        "Sacramento",
        "CA",
        "March",
        "June",
        "Responsibilities",
        "Exposure",
        "MultiThreading",
        "factory",
        "learning",
        "process",
        "worker",
        "testing",
        "methodologies",
        "unit",
        "testing",
        "Integration",
        "testing",
        "web",
        "application",
        "Design",
        "custom",
        "scripts",
        "use",
        "version",
        "controller",
        "Team",
        "Foundation",
        "Server",
        "TFS",
        "Test",
        "custom",
        "scripts",
        "solutions",
        "science",
        "models",
        "dashboard",
        "control",
        "panel",
        "customers",
        "Administrators",
        "Oracle",
        "DB",
        "VMWare",
        "API",
        "configuration",
        "changes",
        "data",
        "models",
        "solutions",
        "points",
        "failure",
        "error",
        "handling",
        "communication",
        "failure",
        "parts",
        "failure",
        "database",
        "communication",
        "points",
        "file",
        "system",
        "errors",
        "process",
        "execution",
        "team",
        "members",
        "Developed",
        "GUI",
        "webapp2",
        "test",
        "block",
        "documentation",
        "features",
        "python",
        "code",
        "web",
        "browser",
        "QA",
        "test",
        "plans",
        "highlevel",
        "design",
        "documentation",
        "Environments",
        "Python",
        "Mysql",
        "HTML",
        "Javascript",
        "HQL",
        "Git",
        "Web",
        "services",
        "Education",
        "Bachelors",
        "Skills",
        "PYTHON",
        "years",
        "MACHINE",
        "LEARNING",
        "years",
        "Hadoop",
        "years",
        "HADOOP",
        "years",
        "LEARNING",
        "year",
        "Additional",
        "Information",
        "TECHNICAL",
        "SKILLS",
        "Machine",
        "Learning",
        "classification",
        "regression",
        "feature",
        "engineering",
        "networks",
        "Programming",
        "Languages",
        "Python",
        "scikitlearn",
        "numpy",
        "GraphLab",
        "Create",
        "R",
        "SQL",
        "Hadoop",
        "MapReduce",
        "Sqoop",
        "Flume",
        "Scala",
        "Java",
        "Spark",
        "PySpark",
        "MLlib",
        "Operating",
        "Systems",
        "Linux",
        "CentOS",
        "Ubuntu",
        "Kali",
        "Linux",
        "Windows",
        "MacOS",
        "Software",
        "Tools",
        "PyCharm",
        "Jupyter",
        "Notebook",
        "R",
        "studio",
        "Tableau",
        "Microsoft",
        "Office",
        "Eclipse",
        "IDE"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T21:35:49.817411",
    "resume_data": "Data Scientist Data Scientist Data Scientist TracFone Wireless Inc Mary SC Professional Qualified Data Scientist with over 7 years of experience in Data Science and Analytics including Machine Learning Data Mining and Statistical Analysis Involved in the entire data science project life cycle and actively involved in all the phases including data extraction data cleaning statistical modeling and data visualization with large data sets of structured and unstructured data Having knowledge on Apache Spark and developing data processing and analysis algorithms using Python Understanding of machine learning ML concepts and application of algorithms in nonacademic environments Strong software development background in functional and objectoriented programming Expertized in developing Machine learning algorithm using Python Ability to manipulate transform and analyze abstract data structures such as Dataframes Fundamental understanding of machine learning concepts including training models as well as understanding precision and recall in the real world Strong programming experience in the following R Python Matlab Performed Collection cleansing and verification of structured and unstructured data Experience in visualization tools like Tableau 9 10 for creating dashboards Excellent understanding Agile and Scrum development methodology Used the version control tools like Git Passionate about cleaning insightful information from massive data assets and developing a culture of sound datadriven decision making Ability to maintain a fun casual professional and productive team atmosphere Authorized to work in the US for any employer Work Experience Data Scientist TracFone Wireless Inc Miami FL October 2016 to Present Responsibilities Enhancing data collection procedures to include information that is relevant for building analytic systems Processing cleansing and verifying the integrity of data used for analysis Doing adhoc analysis and presenting results in a clear manner Constant tracking of model performance Excellent understanding of machine learning techniques and algorithms such as Logistic Regression SVM Random Forests Deep Learning etc Worked with Data governance Data quality Data lineage Data architect to design various models Independently coded new programs and designed Tables to load and test the program effectively for the given POCs Extending companys data with third party sources of information when needed Designed data models and data flow diagrams using Erwin and MS Visio As an Architect implemented MDM hub to provide clean consistent data for a SOA implementation Developed Implemented maintained the Conceptual Logical Physical Data Models using Erwin for ForwardReverse Engineered Databases Experience with common data science toolkits such as R Python Spark etc Good applied statistics skills such as statistical sampling testing regression etc Build analytic models using a variety of techniques such as logistic regression risk scorecards and pattern recognition technologies Analyze and understand large amounts of data to determine suitability for use in models and then work to segment the data create variables build models and test those models Work with technical and development teams to deploy models Build Model Performance Reports and Modeling Technical Documentation to support each of the models for the product line Performed Exploratory Data Analysis and Data Visualizations using R and Tableau Perform a proper EDA Univariate and bivariate analysis to understand the intrinsic effectcombined Established Data architecture strategy best practices standards and roadmaps Lead the development and presentation of a data analytics datahub prototype with the help of the other members of the emerging solutions team Involved in analysis of Business requirement Design and Development of High level and Lowlevel designs Unit and Integration testing Worked with several R packages including knitr dplyr SparkR CausalInfer spacetime Interacted with the other departments to understand and identify data needs and requirements Environment UNIX Python 352 MLLib SAS regression logistic regression Hadoop NoSQL Teradata OLTP Random forest OLAP HDFS ODS Data Scientist CNY Asset Management Syracuse NY January 2014 to September 2016 Responsibilities Communicated and coordinated with other departments to collection business requirement Worked on miss value imputation outliers identification with statistical methodologies using Pandas Numpy Participated in features engineering such as feature creating feature scaling and OneHot encoding with Scikitlearn Tackled highly imbalanced Fraud dataset using undersampling with ensemble methods oversampling and cost sensitive algorithms Improved fraud prediction performance by using random forest and gradient boosting for feature selection with Python Scikitlearn Implemented machine learning model logistic regression XGboost with Python Scikit learn Optimized algorithm with stochastic gradient descent algorithm Finetuned the algorithm parameter with manual tuning and automated tuning such as Bayesian Optimization Validated and select models using kfold cross validation confusion matrices and worked on optimizing models for high recall rate Implemented Ensemble Models with majority votes to enhance the efficiency and performance Environments Python scikitlearn pandas Numpy Machine Learning logistic regression XGboost Gradient Descent algorithm Bayesian optimization Tableau Data Scientist PDF Solutions San Jose CA October 2012 to December 2013 Responsibilities Build Analytics systems data structures gather and manipulate data using statistical techniques and predictive modelling to tell people story Designing suite of interactive dashboards which provided an opportunity to scale and measure the statistics of the HR dept which was not possible earlier and schedule and publish reports Provided and created data presentation to reduce biases and telling true story of people by Pulling millions of rows of data using SQL analysis of Data Worked on Machine Learning to compare the Metrics of HR data closely Technology Stack R Machine Learning SQL server and Tableau Building Data capabilities for HR with respect to measure and resolve attrition issues through metrics Analyzing recruiting history and employee performance to identify best candidates for hiring Identify employees for training and succession planning Environments R Python pandas numpy scikitlearn Machine Learning predictive modeling SQL Tableau Big Data Developer Applied Materials Santa Clara CA July 2011 to September 2012 Responsibilities Involved in the process of load transform and analyze data from various sources into HDFS Hadoop Distributed File System using Hive Pig and Sqoop Experienced in handling data from different datasets join and preprocess those using Pig join operations Worked on Pig script to count the number of times a particular URL was opened in a particular duration Developed PIG UDFs for the needed functionality such as custom Pigsloader known as timestamp loader Created Hive tables based on the business requirements Pig scripts and Hive queries were used to analyze the large data sets Scheduling and managing jobs on a Hadoop cluster using Oozie work flow Written MapReduce code to process and parsing the data from various sources and storing parsed data into HBase and Hive using HBase Hive Integration Worked on Developing custom MapReduce programs and User Defined Functions UDFs in Hive to transform the large volumes of data with respect to business requirement Involved in creating Hive tables and working on them using Hive QL Involved in moving all log files generated from various sources to HDFS for further processing through Flume Implemented Frameworks using Java and python to automate the ingestion flow Loading and transforming of large sets of structured and semi structured data Exported filtered data into HBase for fast query Involved in the installation configuration and used the Hadoop ecosystem components such as Map Reduce HDFS Pig Hive Flume HBase Environments Hadoop HDFS Hive Pig Sqoop HBase MapReduce Flume UDFs Python Developer American River Bank Sacramento CA March 2010 to June 2011 Responsibilities Exposure on MultiThreading factory to distribute learning process backtesting and the into various worker processes Different testing methodologies like unit testing Integration testing and web application testing Design and implemented custom scripts Extensive use of version controller Team Foundation Server TFS Test and validated the custom scripts Delivered automated solutions for science models Managed developed and designed a dashboard control panel for customers and Administrators using Oracle DB and VMWare API calls Implemented configuration changes for data models Maintained and updated existing automated solutions Handled potential points of failure through error handling and communication of failure Anticipated potential parts of failure database communication points file system errors Troubleshoot the process execution and worked with other team members to correct them Developed GUI using webapp2 for dynamically displaying the test block documentation and other features of python code using a web browser Interacted with QA to develop test plans from highlevel design documentation Environments Python Mysql HTML Javascript HQL Git Web services Education Bachelors Skills PYTHON 7 years MACHINE LEARNING 5 years Hadoop 3 years HADOOP 3 years DEEP LEARNING 1 year Additional Information TECHNICAL SKILLS Machine Learning classification regression clustering feature engineering deep learning neural networks Programming Languages Python pandas scikitlearn numpy scipy GraphLab Create R SQL Hadoop MapReduce Sqoop Flume Scala Java Spark PySpark MLlib Operating Systems Linux CentOS Ubuntu Kali Linux Windows MacOS Software Tools PyCharm Jupyter Notebook R studio Tableau Microsoft Office Eclipse IDE",
    "unique_id": "89c1c039-9cc0-4c07-94fa-67a6c94d26c6"
}