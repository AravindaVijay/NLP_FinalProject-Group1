{
    "clean_data": "Hadoop developer Hadoop span ldeveloperspan Hadoop developer Humac Inc Albany NY Around 8 years of experience in Information Technology with Hands on experience in all the stages of system development efforts including requirement definition design implementation testing and documentation Experience in Hadoop ecosystem including the state of Art technologies Worked in a highly dynamic team using agile methodologies like Scrum and waterfall A quick learner punctual and trustworthy Good working experience on Hadoop architecture and various components such as HDFS Job Tracker Task Tracker Name Node Data Node and Map Reduce programming Techno functional responsibilities include interfacing with users identifying technical and functional gaps estimates designing developing producing documentation and extensive Production support Integrated various Big Data Technologies into the overall Experiences in analyzing developing testing and optimizing data transformation processes using Hadoop components Proficiency in importing and exporting data using Sqoop from Relational Database Systems to HDFS and vice versa Extensive Experience in Developing and maintaining Big Data streaming applications using Kafka Storm Spark and other Hadoop Components Experience in columnfamily based Databases HBase and Accumulo Skilled in Creating scheduling and maintaining Workflows in UC4 Procedural knowledge on cleansing and analysing data using Hive Presto on Hadoop Platform and also on Relational databases such as Oracle SQL Teradata and MongoDB Extensively worked on creating Teradata Bteq Scripts and used Informatica to load data into Teradata Preparation of Standard Code guidelines analysis and testing documentations In depth understanding of data structures and algorithms Worked on various IDEs Eclipse IntelliJ and repositories Git and SVN Hands on Experience in designing and developing applications in Spark using Scala to compare performance of Spark with Hive Involved in ooptimizing existing algorithms in Hadoop using Spark Context SparkSQL Data Frames and Pair RDDs Good knowledge on installing configuring and using Hadoop components like Map Reduce HDFS Hive Sqoop Pig Zookeeper and Flume In depth knowledge of database like SQL MySQL and extensive experience in writing SQL queries Stored Procedures Triggers Cursors Functions and Packages Proficient in writing Shell JavaScript and Python Scripts Good understanding on Installing and maintaining the Linux servers Experience in Monitoring System Metrics and logs for any problems adding removing or updating user account information resetting passwords etc Authorized to work in the US for any employer Work Experience Hadoop developer Humac Inc Phoenix AZ December 2016 to Present Responsibilities Analyzed various Relational Databases such as Oracle SQL Teradata and MongoDB to understand the source systems and develop application to migrate the data to Hadoop Environment Implemented several Batch Ingestion jobs for Historical data migration from various relational databases and files using Sqoop Hands on Developing a Near RealTime Framework using Kafka storm to ingest data from several source systems like Oracle SQL Teradata and MongoDB into Hadoop Environment Involved in implementing a Realtime framework to capture Streaming data and store in HDFS using Kafka Spark Developed Kafka consumer component for near realtime and RealTime data processing in Java and Scala Defined multiple Kafka Topics with several Partitions and replication factors across data centers Part of designing and developing a custom Java deamon to pull data from source systems and publish the resultant to a specific Kafka Topic Expertise in integrating Kafka with Storm and Spark streaming for near realtime and realtime Frameworks Migrated streaming data to HDFS Accumulo by creating Hdfs Bolt and Accumulo Bolt in Storm Successfully migrated complex SQL transformations in Pentaho that belong to Xfinity Home Security to Hadoop using Spark and Scala Optimizing of existing algorithms in Hadoop using Spark Context SparkSQL Data Frames and Pair RDDs Responsible for creating Hive tables and views using Partitions HQL Scripts in landing layer for analytics Also created tables in Accumulo Database and built external tables on top of them in hive using Accumulo Connectors for data analysis Analyzed massive Data sets in Hadoop ranging in Gigabytes using Hive Accumulo and Presto Developed Data archival and data purge in HDFS using Shell and Python scripts UC4 workflow to automate the process Defined UC4 workflows for running sequential job flows in Production Responsible for creating Deployment SMOP and LLDs Documents Created a Wiki page and migrated code to Git repository Wrote tested and implemented Teradata Bteq Scripts DML and DDL which is transactional data of Xfinity Mobile Performance tuned and optimized various SQL queries Hands on experience on Informatica to load data to Teradata by making various connections to load and extract data to and from Teradata efficiently Environment Hortonworks Hadoop 26 HDFS Hive 243 Presto Spark 22 Scala 2118 Kafka 09 Apache Storm 010 Accumulo Sqoop UC4 Rest API Java 17 18 Shell Scripting Python Scripting MySQL Oracle 11g MongoDB Teradata 15 SQL Informatica 1011 Hadoop Developer State Street Quincy MA October 2015 to December 2016 Responsibilities Involved in requirement gathering to setup a cluster Part of Configuring Hadoop cluster and load balancing across the nodes Developed MapReduce programs in Java for parsing the raw data and populating staging Tables Created Hive queries to compare the raw data with EDW reference tables and performing aggregates Experienced in developing custom input formats and data types to parse and process unstructured and semi structured input data and mapped them into key value pairs to implement business logic in MapReduce Experience in implementing custom sterilizer interceptor source and sink as per the requirement in Flume to ingest data from multiple sources Experience in setting up Fanout workflow in flume to design v shaped architecture to take data from many sources and ingest into single sink Importing and exporting data into HDFS and Hive using Sqoop Experienced in analyzing data with Hive and Pig Experienced knowledge over designing Restful services using java based APIs like JERSEY Developed Pig Latin scripts to extract the data from the web server output files to load into HDFS Integrating bulk data into Cassandra file system using MapReduce programs Expertise in designing data modeling for Cassandra NoSQL database Experienced in managing and reviewing Hadoop log files Defined multiple job flows using Oozie workflow Involved in working with Spark on top of  for interactive and Batch Analysis Worked closely with AWS EC2 infrastructure teams to troubleshoot complex issues Expertise in writing the Scala code using higher order functions for the iterative algorithms in spark for performance consideration Experience in managing and monitoring Hadoop cluster using Cloudera Manager Experienced in analyzing and Optimizing RDDs by controlling partitions for the given data Good understanding on DAG cycle for entire spark application flow on Spark application WebUI Experienced in writing live Realtime Processing using Spark Streaming with Kafka Developed custom mappers in python script and Hive UDFs and UDAFs based on the given requirement Used HiveQL to analyze the partitioned and bucketed data and compute various metrics for reporting Supported in setting up QA environment and updating configurations for implementing scripts with Pig Hive and Sqoop Unit tested a sample of raw data and improved performance and turned over to production Environment CDH JavaJDK17 Hadoop MapReduce HDFS Hive Sqoop Flume Cassandra Pig Oozie Kerberos Scala Spark SQL Spark Streaming Kafka Linux AWS Shell Scripting MySQL Oracle 11g SQLPLUS Hadoop Developer Geometric Ltd August 2013 to October 2015 Responsibilities Installed Name node Secondary name node Resource Manager Node manager Application master Data node using Cloudera Installed and configured Hortonworks Ambari for easy management of existing Hadoop cluster Installed and Configured HDP Installed and configured multinodes fully distributed Hadoop cluster of large number of nodes Provided Hadoop OS Hardware optimizations Setting up the machines with Network Control Static IP Disabled Firewalls Swap memory Understanding the performance bottlenecks by analyzing the existing hadoop cluster and provided performance tuning accordingly Regular Commissioning and Decommissioning of nodes depending upon the amount of data Installed and configured Hadoop components Hdfs Hive HBase Communicating with the development teams and attending daily meetings Addressing and Troubleshooting issues on a daily basis Working with data delivery teams to setup new Hadoop users This job includes setting up Linux users setting up Kerberos principals and testing HDFS Hive Cluster maintenance as well as creation and removal of nodes Monitor Hadoop cluster connectivity and security Manage and review Hadoop log files Configured the cluster to achieve the optimal results by finetuning the cluster Dumped the data from one cluster to other cluster by using DISTCP and automated the dumping procedure using shell scripts Designed the shell script for backing up of important metadata and rotating the logs on a monthly basis Implemented open source monitoring tool GANGLIA for monitoring the various services across the cluster Testing evaluation and troubleshooting of different NoSQL database systems and cluster configurations to ensure highavailability in various crash scenarios Performance tuning and stress testing of NoSQL database environments to ensure acceptable database performance in production mode Designed the cluster so that only one secondary name node daemon could be run at any given time Implemented commissioning and decommissioning of data nodes killing the unresponsive task tracker and dealing with blacklisted task trackers Dumped the data from HDFS to MYSQL database and viceversa using SQOOP Provided the necessary support to the ETL team when required Integrated Nagios in the Hadoop cluster for alerts Performed both major and minor upgrades to the existing cluster and rolling back to the previous version Environment LINUX HDFS MapReduce KDC NAGIOS GANGLIA OOZIE SQOOP Cloudera Manager Teradata Developer Enquero Milpitas CA June 2011 to August 2013 Responsibilities Extensively used ETL to load data from Oracle and Flat files to Data Warehouse Extensively worked in data Extraction Transformation and Loading from source to target system using power center of Informatica Developed complex mappings in Informatica to load the data from various sources Implemented performance tuning logic on targets sources mappings sessions to provide maximum efficiency and performance Parameterized the mappings and increased the reusability Used Informatica Power Center Workflow manager to create sessions workflows and batches to run with the logic embedded in the mappings Created procedures to truncate data in the target before the session run Extensively used Toad utility for executing SQL scripts and worked on SQL for enhancing the performance of the conversion mapping Used the PLSQL procedures for Informatica mappings for truncating the data in target tables at run time Worked on Teradata RDBMS using Fast load Multi load Tpump Fast export Multi load Export Teradata Sql and Bteq Teradata utilities Involved in Performance Tuning at various levels including Target Source Mapping and Session for large data files Extracted data from various source systems like Oracle SQL Server and flat files as per the requirements Performed bulk data load from multiple data source ORACLE 8i legacy systems to Teradata RDBMS using BTEQ Multi Load and Fast Load Created optimized reviewed and executed Teradata SQL test queries to validate transformation rules used in source to target mappingssource views and to verify data in target tables Performed tuning and optimization of complex SQL queries using Teradata Explain Responsible for Collect Statics on FACT tables Design and development of the complete Decision Support System using Business Objects Worked on Migration Strategies between Development Test and Production Repositories Supported the Quality Assurance team in testing and validating the Informatica workflows Extensively involved in development of mappings using various transformations of Informatica according to business logic Created and Scheduled Sessions and Batches using Server Manager Created and Monitor the sessions using workflow manager and workflow monitor Conducting unit testing Environment Informatica Power Centre 86 UNIX Teradata Oracle 8i TOAD Java Developer Aroghia Aurora CO April 2009 to June 2011 Responsibilities Developed the spring AOP programming to configure logging for the application Expertise in developing enterprise applications using Struts Frameworks Developed the front end using JSF and Portlet Developed Scalable applications using Stateless session EJBs Developed the UI panels using JSF XHTML CSS DOJO and JQuery MySQL to access data in the database at different Levels Making a connection to backend MySQL database Design and Developed using Web Service using Apache Axis wrote numerous session and message driven beans for operation on JBoss and WebLogic Used VSS Visual Source Safe as configuration management tool Created automated test cases using Selenium Worked with SDLC process like water fall model AGILE methodology JSP interfaces were developed Custom tags were used Developed Servlets and Worked extensively on Sql Used ANT for building the application and deployed on BEA WebLogic Application Server Was responsible for Developing XML Parsing logic using SAXDOM Parsers Good network at EMC Documentum Support Teams who help solve product issues and bugs Worked on tickets from servicenow and Jira on daily basis Designed the front end using Swing Used IBM MQ Series in the project Apache Tomcat Server was used to deploy the application Involving in Building the modules in Linux environment with ant script Used Resource Manager to schedule the job in UNIX server Used web services REST to bridge the gap between our MS and DrupalWord press technology Design online stores using ASP JavaScript develop custom storefront applications and custom userinterfaces for client sites J2EE to communicate legacy COBOL based mainframe implementations Worked on PLSQL and SQL queries Developed Java Script and Action Script VB Script macros for Client Side validations Environment Spring Struts JSF EJBs JQuery MySQL DB2 Net Beans JBoss CVS VSS water fall model UML JSP Servlets ANT XML EMC Jira IBM MQ Tomcat Server Linux Unix server Education Bachelor of Technology in Technology Texas AM University Skills MYSQL 6 years SQL 6 years LINUX 5 years APACHE HADOOP HDFS 4 years APACHE HADOOP SQOOP 4 years Additional Information Technical Skills Big Data Technologies Hadoop MapReduce HDFS Hive Pig Sqoop Flume solr Kafka Spark Storm Reporting Tools Jaspersoft Qlik Sense Tableau Scripting Languages Python Shell R Programming Languages C C Java Web Technologies HTML J2EE CSS JavaScript AJAX Servlets JSP DOM XML XSLT Application Server WebLogic Server Apache Tomcat DB Languages SQL PLSQL Postgres NoSQL Databases HBase Cassandra Accumulo Databases ETL Oracle 10g11g MySQL 52 DB2 Informatica v 8x Talend Operating Systems Linux UNIX Windows 2003 Server IDEs Eclipse NetBeans JDeveloper IntelliJ IDEA Version Control CVS SVN Git",
    "entities": [
        "IDEA Version Control",
        "Sql Used ANT",
        "Resource",
        "Oracle SQL Server",
        "Spark Context",
        "Provided Hadoop OS Hardware",
        "Informatica",
        "Partitions",
        "Relational",
        "Information Technology",
        "HDFS",
        "Building",
        "UNIX",
        "JQuery MySQL DB2 Net Beans JBoss CVS VSS",
        "IBM",
        "UDAFs",
        "Production Responsible",
        "node",
        "Realtime Processing using Spark Streaming",
        "Hadoop",
        "SQLPLUS Hadoop Developer Geometric Ltd",
        "WebLogic",
        "UML JSP Servlets",
        "Shell",
        "Hive Presto on Hadoop Platform",
        "Pentaho",
        "Network Control Static IP",
        "Monitor Hadoop",
        "JSP DOM XML XSLT Application",
        "Data Warehouse",
        "Firewalls Swap",
        "Kerberos",
        "Node Data",
        "BTEQ Multi Load",
        "Xfinity Mobile Performance",
        "Git",
        "Restful",
        "Developing",
        "Storm and Spark",
        "DDL",
        "SQL Informatica",
        "Hadoop Environment Implemented",
        "Eclipse NetBeans JDeveloper IntelliJ",
        "ORACLE",
        "Accumulo Database",
        "BEA WebLogic Application",
        "AGILE",
        "Linux",
        "JSP",
        "Worked",
        "Gigabytes",
        "Present Responsibilities Analyzed",
        "JBoss",
        "Talend Operating Systems Linux",
        "Developed Servlets",
        "MS",
        "Informatica Power Center Workflow",
        "Spark",
        "Shell Scripting Python",
        "Hadoop Environment Involved",
        "US",
        "Sqoop",
        "QA",
        "SVN Hands",
        "Created",
        "Analyzed",
        "AWS",
        "Informatica Developed",
        "Teradata Preparation of Standard Code",
        "Decision Support System using Business Objects Worked on Migration Strategies between Development Test and Production Repositories Supported the Quality Assurance",
        "Integrated Nagios",
        "Target Source Mapping and Session",
        "Storm Successfully",
        "JSF",
        "Environment Informatica Power Centre",
        "Additional Information Technical Skills Big Data Technologies Hadoop MapReduce HDFS Hive Pig",
        "Workflows",
        "Sqoop Unit",
        "Humac Inc Albany NY",
        "HDFS Job Tracker Task Tracker",
        "java",
        "SQL",
        "Oracle SQL Teradata",
        "Flume",
        "Pig Hive and",
        "Hdfs Bolt",
        "Relational Database Systems",
        "EMC Documentum Support Teams",
        "MapReduce Experience",
        "Created and Scheduled Sessions and Batches",
        "Big Data",
        "Hive",
        "SQOOP",
        "Teradata Explain Responsible for Collect Statics",
        "HiveQL",
        "DAG",
        "Monitoring System Metrics",
        "ETL",
        "Work Experience Hadoop",
        "Performed",
        "Partitions HQL Scripts",
        "Documents Created",
        "SAXDOM Parsers Good",
        "UI",
        "Tables Created Hive",
        "ASP",
        "Multi",
        "Hadoop Components",
        "Expertise",
        "Hdfs Hive HBase Communicating",
        "Big Data Technologies",
        "Developed MapReduce",
        "SQL MySQL",
        "EDW",
        "Toad",
        "MapReduce",
        "NoSQL",
        "DrupalWord",
        "Accumulo Connectors",
        "Xfinity Home Security"
    ],
    "experience": "Experience in Hadoop ecosystem including the state of Art technologies Worked in a highly dynamic team using agile methodologies like Scrum and waterfall A quick learner punctual and trustworthy Good working experience on Hadoop architecture and various components such as HDFS Job Tracker Task Tracker Name Node Data Node and Map Reduce programming Techno functional responsibilities include interfacing with users identifying technical and functional gaps estimates designing developing producing documentation and extensive Production support Integrated various Big Data Technologies into the overall Experiences in analyzing developing testing and optimizing data transformation processes using Hadoop components Proficiency in importing and exporting data using Sqoop from Relational Database Systems to HDFS and vice versa Extensive Experience in Developing and maintaining Big Data streaming applications using Kafka Storm Spark and other Hadoop Components Experience in columnfamily based Databases HBase and Accumulo Skilled in Creating scheduling and maintaining Workflows in UC4 Procedural knowledge on cleansing and analysing data using Hive Presto on Hadoop Platform and also on Relational databases such as Oracle SQL Teradata and MongoDB Extensively worked on creating Teradata Bteq Scripts and used Informatica to load data into Teradata Preparation of Standard Code guidelines analysis and testing documentations In depth understanding of data structures and algorithms Worked on various IDEs Eclipse IntelliJ and repositories Git and SVN Hands on Experience in designing and developing applications in Spark using Scala to compare performance of Spark with Hive Involved in ooptimizing existing algorithms in Hadoop using Spark Context SparkSQL Data Frames and Pair RDDs Good knowledge on installing configuring and using Hadoop components like Map Reduce HDFS Hive Sqoop Pig Zookeeper and Flume In depth knowledge of database like SQL MySQL and extensive experience in writing SQL queries Stored Procedures Triggers Cursors Functions and Packages Proficient in writing Shell JavaScript and Python Scripts Good understanding on Installing and maintaining the Linux servers Experience in Monitoring System Metrics and logs for any problems adding removing or updating user account information resetting passwords etc Authorized to work in the US for any employer Work Experience Hadoop developer Humac Inc Phoenix AZ December 2016 to Present Responsibilities Analyzed various Relational Databases such as Oracle SQL Teradata and MongoDB to understand the source systems and develop application to migrate the data to Hadoop Environment Implemented several Batch Ingestion jobs for Historical data migration from various relational databases and files using Sqoop Hands on Developing a Near RealTime Framework using Kafka storm to ingest data from several source systems like Oracle SQL Teradata and MongoDB into Hadoop Environment Involved in implementing a Realtime framework to capture Streaming data and store in HDFS using Kafka Spark Developed Kafka consumer component for near realtime and RealTime data processing in Java and Scala Defined multiple Kafka Topics with several Partitions and replication factors across data centers Part of designing and developing a custom Java deamon to pull data from source systems and publish the resultant to a specific Kafka Topic Expertise in integrating Kafka with Storm and Spark streaming for near realtime and realtime Frameworks Migrated streaming data to HDFS Accumulo by creating Hdfs Bolt and Accumulo Bolt in Storm Successfully migrated complex SQL transformations in Pentaho that belong to Xfinity Home Security to Hadoop using Spark and Scala Optimizing of existing algorithms in Hadoop using Spark Context SparkSQL Data Frames and Pair RDDs Responsible for creating Hive tables and views using Partitions HQL Scripts in landing layer for analytics Also created tables in Accumulo Database and built external tables on top of them in hive using Accumulo Connectors for data analysis Analyzed massive Data sets in Hadoop ranging in Gigabytes using Hive Accumulo and Presto Developed Data archival and data purge in HDFS using Shell and Python scripts UC4 workflow to automate the process Defined UC4 workflows for running sequential job flows in Production Responsible for creating Deployment SMOP and LLDs Documents Created a Wiki page and migrated code to Git repository Wrote tested and implemented Teradata Bteq Scripts DML and DDL which is transactional data of Xfinity Mobile Performance tuned and optimized various SQL queries Hands on experience on Informatica to load data to Teradata by making various connections to load and extract data to and from Teradata efficiently Environment Hortonworks Hadoop 26 HDFS Hive 243 Presto Spark 22 Scala 2118 Kafka 09 Apache Storm 010 Accumulo Sqoop UC4 Rest API Java 17 18 Shell Scripting Python Scripting MySQL Oracle 11 g MongoDB Teradata 15 SQL Informatica 1011 Hadoop Developer State Street Quincy MA October 2015 to December 2016 Responsibilities Involved in requirement gathering to setup a cluster Part of Configuring Hadoop cluster and load balancing across the nodes Developed MapReduce programs in Java for parsing the raw data and populating staging Tables Created Hive queries to compare the raw data with EDW reference tables and performing aggregates Experienced in developing custom input formats and data types to parse and process unstructured and semi structured input data and mapped them into key value pairs to implement business logic in MapReduce Experience in implementing custom sterilizer interceptor source and sink as per the requirement in Flume to ingest data from multiple sources Experience in setting up Fanout workflow in flume to design v shaped architecture to take data from many sources and ingest into single sink Importing and exporting data into HDFS and Hive using Sqoop Experienced in analyzing data with Hive and Pig Experienced knowledge over designing Restful services using java based APIs like JERSEY Developed Pig Latin scripts to extract the data from the web server output files to load into HDFS Integrating bulk data into Cassandra file system using MapReduce programs Expertise in designing data modeling for Cassandra NoSQL database Experienced in managing and reviewing Hadoop log files Defined multiple job flows using Oozie workflow Involved in working with Spark on top of   for interactive and Batch Analysis Worked closely with AWS EC2 infrastructure teams to troubleshoot complex issues Expertise in writing the Scala code using higher order functions for the iterative algorithms in spark for performance consideration Experience in managing and monitoring Hadoop cluster using Cloudera Manager Experienced in analyzing and Optimizing RDDs by controlling partitions for the given data Good understanding on DAG cycle for entire spark application flow on Spark application WebUI Experienced in writing live Realtime Processing using Spark Streaming with Kafka Developed custom mappers in python script and Hive UDFs and UDAFs based on the given requirement Used HiveQL to analyze the partitioned and bucketed data and compute various metrics for reporting Supported in setting up QA environment and updating configurations for implementing scripts with Pig Hive and Sqoop Unit tested a sample of raw data and improved performance and turned over to production Environment CDH JavaJDK17 Hadoop MapReduce HDFS Hive Sqoop Flume Cassandra Pig Oozie Kerberos Scala Spark SQL Spark Streaming Kafka Linux AWS Shell Scripting MySQL Oracle 11 g SQLPLUS Hadoop Developer Geometric Ltd August 2013 to October 2015 Responsibilities Installed Name node Secondary name node Resource Manager Node manager Application master Data node using Cloudera Installed and configured Hortonworks Ambari for easy management of existing Hadoop cluster Installed and Configured HDP Installed and configured multinodes fully distributed Hadoop cluster of large number of nodes Provided Hadoop OS Hardware optimizations Setting up the machines with Network Control Static IP Disabled Firewalls Swap memory Understanding the performance bottlenecks by analyzing the existing hadoop cluster and provided performance tuning accordingly Regular Commissioning and Decommissioning of nodes depending upon the amount of data Installed and configured Hadoop components Hdfs Hive HBase Communicating with the development teams and attending daily meetings Addressing and Troubleshooting issues on a daily basis Working with data delivery teams to setup new Hadoop users This job includes setting up Linux users setting up Kerberos principals and testing HDFS Hive Cluster maintenance as well as creation and removal of nodes Monitor Hadoop cluster connectivity and security Manage and review Hadoop log files Configured the cluster to achieve the optimal results by finetuning the cluster Dumped the data from one cluster to other cluster by using DISTCP and automated the dumping procedure using shell scripts Designed the shell script for backing up of important metadata and rotating the logs on a monthly basis Implemented open source monitoring tool GANGLIA for monitoring the various services across the cluster Testing evaluation and troubleshooting of different NoSQL database systems and cluster configurations to ensure highavailability in various crash scenarios Performance tuning and stress testing of NoSQL database environments to ensure acceptable database performance in production mode Designed the cluster so that only one secondary name node daemon could be run at any given time Implemented commissioning and decommissioning of data nodes killing the unresponsive task tracker and dealing with blacklisted task trackers Dumped the data from HDFS to MYSQL database and viceversa using SQOOP Provided the necessary support to the ETL team when required Integrated Nagios in the Hadoop cluster for alerts Performed both major and minor upgrades to the existing cluster and rolling back to the previous version Environment LINUX HDFS MapReduce KDC NAGIOS GANGLIA OOZIE SQOOP Cloudera Manager Teradata Developer Enquero Milpitas CA June 2011 to August 2013 Responsibilities Extensively used ETL to load data from Oracle and Flat files to Data Warehouse Extensively worked in data Extraction Transformation and Loading from source to target system using power center of Informatica Developed complex mappings in Informatica to load the data from various sources Implemented performance tuning logic on targets sources mappings sessions to provide maximum efficiency and performance Parameterized the mappings and increased the reusability Used Informatica Power Center Workflow manager to create sessions workflows and batches to run with the logic embedded in the mappings Created procedures to truncate data in the target before the session run Extensively used Toad utility for executing SQL scripts and worked on SQL for enhancing the performance of the conversion mapping Used the PLSQL procedures for Informatica mappings for truncating the data in target tables at run time Worked on Teradata RDBMS using Fast load Multi load Tpump Fast export Multi load Export Teradata Sql and Bteq Teradata utilities Involved in Performance Tuning at various levels including Target Source Mapping and Session for large data files Extracted data from various source systems like Oracle SQL Server and flat files as per the requirements Performed bulk data load from multiple data source ORACLE 8i legacy systems to Teradata RDBMS using BTEQ Multi Load and Fast Load Created optimized reviewed and executed Teradata SQL test queries to validate transformation rules used in source to target mappingssource views and to verify data in target tables Performed tuning and optimization of complex SQL queries using Teradata Explain Responsible for Collect Statics on FACT tables Design and development of the complete Decision Support System using Business Objects Worked on Migration Strategies between Development Test and Production Repositories Supported the Quality Assurance team in testing and validating the Informatica workflows Extensively involved in development of mappings using various transformations of Informatica according to business logic Created and Scheduled Sessions and Batches using Server Manager Created and Monitor the sessions using workflow manager and workflow monitor Conducting unit testing Environment Informatica Power Centre 86 UNIX Teradata Oracle 8i TOAD Java Developer Aroghia Aurora CO April 2009 to June 2011 Responsibilities Developed the spring AOP programming to configure logging for the application Expertise in developing enterprise applications using Struts Frameworks Developed the front end using JSF and Portlet Developed Scalable applications using Stateless session EJBs Developed the UI panels using JSF XHTML CSS DOJO and JQuery MySQL to access data in the database at different Levels Making a connection to backend MySQL database Design and Developed using Web Service using Apache Axis wrote numerous session and message driven beans for operation on JBoss and WebLogic Used VSS Visual Source Safe as configuration management tool Created automated test cases using Selenium Worked with SDLC process like water fall model AGILE methodology JSP interfaces were developed Custom tags were used Developed Servlets and Worked extensively on Sql Used ANT for building the application and deployed on BEA WebLogic Application Server Was responsible for Developing XML Parsing logic using SAXDOM Parsers Good network at EMC Documentum Support Teams who help solve product issues and bugs Worked on tickets from servicenow and Jira on daily basis Designed the front end using Swing Used IBM MQ Series in the project Apache Tomcat Server was used to deploy the application Involving in Building the modules in Linux environment with ant script Used Resource Manager to schedule the job in UNIX server Used web services REST to bridge the gap between our MS and DrupalWord press technology Design online stores using ASP JavaScript develop custom storefront applications and custom userinterfaces for client sites J2EE to communicate legacy COBOL based mainframe implementations Worked on PLSQL and SQL queries Developed Java Script and Action Script VB Script macros for Client Side validations Environment Spring Struts JSF EJBs JQuery MySQL DB2 Net Beans JBoss CVS VSS water fall model UML JSP Servlets ANT XML EMC Jira IBM MQ Tomcat Server Linux Unix server Education Bachelor of Technology in Technology Texas AM University Skills MYSQL 6 years SQL 6 years LINUX 5 years APACHE HADOOP HDFS 4 years APACHE HADOOP SQOOP 4 years Additional Information Technical Skills Big Data Technologies Hadoop MapReduce HDFS Hive Pig Sqoop Flume solr Kafka Spark Storm Reporting Tools Jaspersoft Qlik Sense Tableau Scripting Languages Python Shell R Programming Languages C C Java Web Technologies HTML J2EE CSS JavaScript AJAX Servlets JSP DOM XML XSLT Application Server WebLogic Server Apache Tomcat DB Languages SQL PLSQL Postgres NoSQL Databases HBase Cassandra Accumulo Databases ETL Oracle 10g11 g MySQL 52 DB2 Informatica v 8x Talend Operating Systems Linux UNIX Windows 2003 Server IDEs Eclipse NetBeans JDeveloper IntelliJ IDEA Version Control CVS SVN Git",
    "extracted_keywords": [
        "Hadoop",
        "developer",
        "Hadoop",
        "span",
        "ldeveloperspan",
        "Hadoop",
        "developer",
        "Humac",
        "Inc",
        "Albany",
        "NY",
        "years",
        "experience",
        "Information",
        "Technology",
        "Hands",
        "experience",
        "stages",
        "system",
        "development",
        "efforts",
        "requirement",
        "definition",
        "design",
        "implementation",
        "testing",
        "documentation",
        "Experience",
        "Hadoop",
        "ecosystem",
        "state",
        "Art",
        "technologies",
        "team",
        "methodologies",
        "Scrum",
        "learner",
        "working",
        "experience",
        "Hadoop",
        "architecture",
        "components",
        "HDFS",
        "Job",
        "Tracker",
        "Task",
        "Tracker",
        "Name",
        "Node",
        "Data",
        "Node",
        "Map",
        "Reduce",
        "programming",
        "Techno",
        "responsibilities",
        "users",
        "gaps",
        "documentation",
        "Production",
        "support",
        "Big",
        "Data",
        "Technologies",
        "Experiences",
        "testing",
        "data",
        "transformation",
        "processes",
        "Hadoop",
        "components",
        "Proficiency",
        "data",
        "Sqoop",
        "Relational",
        "Database",
        "Systems",
        "HDFS",
        "vice",
        "versa",
        "Experience",
        "Big",
        "Data",
        "streaming",
        "applications",
        "Kafka",
        "Storm",
        "Spark",
        "Hadoop",
        "Components",
        "Experience",
        "Databases",
        "HBase",
        "Accumulo",
        "Skilled",
        "scheduling",
        "Workflows",
        "UC4",
        "Procedural",
        "knowledge",
        "cleansing",
        "data",
        "Hive",
        "Presto",
        "Hadoop",
        "Platform",
        "Relational",
        "databases",
        "Oracle",
        "SQL",
        "Teradata",
        "MongoDB",
        "Teradata",
        "Bteq",
        "Scripts",
        "Informatica",
        "data",
        "Teradata",
        "Preparation",
        "Standard",
        "Code",
        "guidelines",
        "analysis",
        "testing",
        "documentations",
        "depth",
        "understanding",
        "data",
        "structures",
        "algorithms",
        "IDEs",
        "Eclipse",
        "IntelliJ",
        "repositories",
        "Git",
        "SVN",
        "Hands",
        "Experience",
        "applications",
        "Spark",
        "Scala",
        "performance",
        "Spark",
        "Hive",
        "ooptimizing",
        "algorithms",
        "Hadoop",
        "Spark",
        "Context",
        "SparkSQL",
        "Data",
        "Frames",
        "Pair",
        "knowledge",
        "configuring",
        "Hadoop",
        "components",
        "Map",
        "Reduce",
        "HDFS",
        "Hive",
        "Sqoop",
        "Pig",
        "Zookeeper",
        "Flume",
        "depth",
        "knowledge",
        "database",
        "SQL",
        "MySQL",
        "experience",
        "SQL",
        "Stored",
        "Procedures",
        "Triggers",
        "Cursors",
        "Functions",
        "Packages",
        "Proficient",
        "Shell",
        "JavaScript",
        "Python",
        "Scripts",
        "understanding",
        "Linux",
        "servers",
        "Experience",
        "Monitoring",
        "System",
        "Metrics",
        "logs",
        "problems",
        "user",
        "account",
        "information",
        "passwords",
        "US",
        "employer",
        "Work",
        "Experience",
        "Hadoop",
        "developer",
        "Humac",
        "Inc",
        "Phoenix",
        "AZ",
        "December",
        "Present",
        "Responsibilities",
        "Relational",
        "Databases",
        "Oracle",
        "SQL",
        "Teradata",
        "source",
        "systems",
        "application",
        "data",
        "Hadoop",
        "Environment",
        "Batch",
        "Ingestion",
        "jobs",
        "data",
        "migration",
        "databases",
        "files",
        "Sqoop",
        "Hands",
        "Near",
        "RealTime",
        "Framework",
        "Kafka",
        "storm",
        "data",
        "source",
        "systems",
        "Oracle",
        "SQL",
        "Teradata",
        "MongoDB",
        "Hadoop",
        "Environment",
        "Realtime",
        "framework",
        "Streaming",
        "data",
        "store",
        "HDFS",
        "Kafka",
        "Spark",
        "Developed",
        "Kafka",
        "consumer",
        "component",
        "realtime",
        "RealTime",
        "data",
        "processing",
        "Java",
        "Scala",
        "Kafka",
        "Topics",
        "Partitions",
        "replication",
        "factors",
        "data",
        "centers",
        "Part",
        "custom",
        "Java",
        "deamon",
        "data",
        "source",
        "systems",
        "resultant",
        "Kafka",
        "Topic",
        "Expertise",
        "Kafka",
        "Storm",
        "Spark",
        "streaming",
        "realtime",
        "Frameworks",
        "streaming",
        "data",
        "HDFS",
        "Accumulo",
        "Hdfs",
        "Bolt",
        "Accumulo",
        "Bolt",
        "Storm",
        "SQL",
        "transformations",
        "Pentaho",
        "Xfinity",
        "Home",
        "Security",
        "Hadoop",
        "Spark",
        "Scala",
        "Optimizing",
        "algorithms",
        "Hadoop",
        "Spark",
        "Context",
        "SparkSQL",
        "Data",
        "Frames",
        "Pair",
        "RDDs",
        "Hive",
        "tables",
        "views",
        "Partitions",
        "HQL",
        "Scripts",
        "landing",
        "layer",
        "analytics",
        "tables",
        "Accumulo",
        "Database",
        "tables",
        "top",
        "hive",
        "Accumulo",
        "Connectors",
        "data",
        "analysis",
        "Data",
        "sets",
        "Hadoop",
        "Gigabytes",
        "Hive",
        "Accumulo",
        "Presto",
        "Developed",
        "Data",
        "archival",
        "data",
        "purge",
        "HDFS",
        "Shell",
        "Python",
        "UC4",
        "workflow",
        "process",
        "UC4",
        "workflows",
        "job",
        "flows",
        "Production",
        "Deployment",
        "SMOP",
        "LLDs",
        "Documents",
        "Wiki",
        "page",
        "code",
        "Git",
        "repository",
        "Wrote",
        "Teradata",
        "Bteq",
        "Scripts",
        "DML",
        "DDL",
        "data",
        "Xfinity",
        "Mobile",
        "Performance",
        "SQL",
        "Hands",
        "experience",
        "Informatica",
        "data",
        "Teradata",
        "connections",
        "data",
        "Teradata",
        "Environment",
        "Hortonworks",
        "Hadoop",
        "HDFS",
        "Hive",
        "Presto",
        "Spark",
        "Scala",
        "Kafka",
        "Apache",
        "Storm",
        "Accumulo",
        "Sqoop",
        "UC4",
        "Rest",
        "API",
        "Java",
        "Shell",
        "Scripting",
        "Python",
        "Scripting",
        "MySQL",
        "Oracle",
        "g",
        "Teradata",
        "SQL",
        "Informatica",
        "Hadoop",
        "Developer",
        "State",
        "Street",
        "Quincy",
        "MA",
        "October",
        "December",
        "Responsibilities",
        "requirement",
        "gathering",
        "cluster",
        "Part",
        "Configuring",
        "Hadoop",
        "cluster",
        "load",
        "nodes",
        "MapReduce",
        "programs",
        "Java",
        "data",
        "staging",
        "Tables",
        "Created",
        "Hive",
        "data",
        "EDW",
        "reference",
        "tables",
        "aggregates",
        "custom",
        "input",
        "formats",
        "data",
        "types",
        "process",
        "input",
        "data",
        "value",
        "pairs",
        "business",
        "logic",
        "MapReduce",
        "Experience",
        "custom",
        "sterilizer",
        "interceptor",
        "source",
        "sink",
        "requirement",
        "Flume",
        "data",
        "sources",
        "Experience",
        "Fanout",
        "flume",
        "architecture",
        "data",
        "sources",
        "sink",
        "data",
        "HDFS",
        "Hive",
        "Sqoop",
        "Experienced",
        "data",
        "Hive",
        "Pig",
        "knowledge",
        "services",
        "APIs",
        "JERSEY",
        "Developed",
        "Pig",
        "Latin",
        "scripts",
        "data",
        "web",
        "server",
        "output",
        "files",
        "HDFS",
        "Integrating",
        "data",
        "Cassandra",
        "file",
        "system",
        "MapReduce",
        "programs",
        "Expertise",
        "data",
        "Cassandra",
        "NoSQL",
        "database",
        "Hadoop",
        "log",
        "files",
        "job",
        "flows",
        "Oozie",
        "workflow",
        "Spark",
        "top",
        "Batch",
        "Analysis",
        "AWS",
        "EC2",
        "infrastructure",
        "teams",
        "issues",
        "Expertise",
        "Scala",
        "code",
        "order",
        "functions",
        "algorithms",
        "spark",
        "performance",
        "consideration",
        "Experience",
        "Hadoop",
        "cluster",
        "Cloudera",
        "Manager",
        "RDDs",
        "partitions",
        "data",
        "understanding",
        "DAG",
        "cycle",
        "spark",
        "application",
        "flow",
        "Spark",
        "application",
        "WebUI",
        "Realtime",
        "Processing",
        "Spark",
        "Streaming",
        "Kafka",
        "custom",
        "mappers",
        "python",
        "script",
        "Hive",
        "UDFs",
        "UDAFs",
        "requirement",
        "HiveQL",
        "data",
        "metrics",
        "QA",
        "environment",
        "configurations",
        "scripts",
        "Pig",
        "Hive",
        "Sqoop",
        "Unit",
        "sample",
        "data",
        "performance",
        "production",
        "Environment",
        "CDH",
        "JavaJDK17",
        "Hadoop",
        "MapReduce",
        "HDFS",
        "Hive",
        "Sqoop",
        "Flume",
        "Cassandra",
        "Pig",
        "Oozie",
        "Kerberos",
        "Scala",
        "Spark",
        "SQL",
        "Spark",
        "Streaming",
        "Kafka",
        "Linux",
        "Shell",
        "Scripting",
        "MySQL",
        "Oracle",
        "g",
        "SQLPLUS",
        "Hadoop",
        "Developer",
        "Geometric",
        "Ltd",
        "August",
        "October",
        "Responsibilities",
        "Name",
        "node",
        "name",
        "node",
        "Resource",
        "Manager",
        "Node",
        "manager",
        "Application",
        "master",
        "Data",
        "node",
        "Cloudera",
        "Installed",
        "Hortonworks",
        "Ambari",
        "management",
        "Hadoop",
        "cluster",
        "Installed",
        "Configured",
        "HDP",
        "multinodes",
        "Hadoop",
        "cluster",
        "number",
        "nodes",
        "Hadoop",
        "OS",
        "Hardware",
        "optimizations",
        "machines",
        "Network",
        "Control",
        "Static",
        "IP",
        "Disabled",
        "Firewalls",
        "Swap",
        "memory",
        "performance",
        "bottlenecks",
        "hadoop",
        "cluster",
        "performance",
        "Commissioning",
        "Decommissioning",
        "nodes",
        "amount",
        "data",
        "Hadoop",
        "components",
        "Hdfs",
        "Hive",
        "HBase",
        "Communicating",
        "development",
        "teams",
        "meetings",
        "Addressing",
        "Troubleshooting",
        "issues",
        "basis",
        "data",
        "delivery",
        "teams",
        "Hadoop",
        "users",
        "job",
        "Linux",
        "users",
        "Kerberos",
        "principals",
        "HDFS",
        "Hive",
        "Cluster",
        "maintenance",
        "creation",
        "removal",
        "nodes",
        "Monitor",
        "Hadoop",
        "cluster",
        "connectivity",
        "security",
        "Manage",
        "Hadoop",
        "log",
        "files",
        "cluster",
        "results",
        "cluster",
        "data",
        "cluster",
        "cluster",
        "DISTCP",
        "procedure",
        "shell",
        "scripts",
        "shell",
        "script",
        "metadata",
        "logs",
        "basis",
        "source",
        "tool",
        "GANGLIA",
        "services",
        "cluster",
        "Testing",
        "evaluation",
        "troubleshooting",
        "NoSQL",
        "database",
        "systems",
        "cluster",
        "configurations",
        "highavailability",
        "crash",
        "scenarios",
        "Performance",
        "tuning",
        "stress",
        "testing",
        "NoSQL",
        "database",
        "environments",
        "database",
        "performance",
        "production",
        "mode",
        "cluster",
        "name",
        "node",
        "daemon",
        "time",
        "commissioning",
        "decommissioning",
        "data",
        "nodes",
        "task",
        "tracker",
        "task",
        "trackers",
        "data",
        "HDFS",
        "MYSQL",
        "database",
        "viceversa",
        "SQOOP",
        "support",
        "ETL",
        "team",
        "Integrated",
        "Nagios",
        "Hadoop",
        "cluster",
        "alerts",
        "Performed",
        "upgrades",
        "cluster",
        "version",
        "Environment",
        "LINUX",
        "HDFS",
        "MapReduce",
        "KDC",
        "NAGIOS",
        "OOZIE",
        "SQOOP",
        "Cloudera",
        "Manager",
        "Teradata",
        "Developer",
        "Enquero",
        "Milpitas",
        "CA",
        "June",
        "August",
        "Responsibilities",
        "ETL",
        "data",
        "Oracle",
        "files",
        "Data",
        "Warehouse",
        "data",
        "Extraction",
        "Transformation",
        "Loading",
        "source",
        "system",
        "power",
        "center",
        "Informatica",
        "mappings",
        "Informatica",
        "data",
        "sources",
        "performance",
        "logic",
        "targets",
        "sources",
        "mappings",
        "sessions",
        "efficiency",
        "performance",
        "Parameterized",
        "mappings",
        "reusability",
        "Informatica",
        "Power",
        "Center",
        "Workflow",
        "manager",
        "sessions",
        "workflows",
        "batches",
        "logic",
        "mappings",
        "procedures",
        "data",
        "target",
        "session",
        "Toad",
        "utility",
        "SQL",
        "scripts",
        "SQL",
        "performance",
        "conversion",
        "mapping",
        "PLSQL",
        "procedures",
        "Informatica",
        "mappings",
        "data",
        "target",
        "tables",
        "time",
        "Teradata",
        "RDBMS",
        "load",
        "Multi",
        "load",
        "Tpump",
        "export",
        "Multi",
        "load",
        "Export",
        "Teradata",
        "Sql",
        "Bteq",
        "Teradata",
        "utilities",
        "Performance",
        "levels",
        "Target",
        "Source",
        "Mapping",
        "Session",
        "data",
        "files",
        "data",
        "source",
        "systems",
        "Oracle",
        "SQL",
        "Server",
        "files",
        "requirements",
        "data",
        "load",
        "data",
        "source",
        "ORACLE",
        "legacy",
        "systems",
        "Teradata",
        "RDBMS",
        "BTEQ",
        "Multi",
        "Load",
        "Fast",
        "Load",
        "Teradata",
        "SQL",
        "test",
        "transformation",
        "rules",
        "source",
        "mappingssource",
        "views",
        "data",
        "target",
        "tables",
        "tuning",
        "optimization",
        "SQL",
        "queries",
        "Teradata",
        "Explain",
        "Responsible",
        "Collect",
        "Statics",
        "FACT",
        "tables",
        "Design",
        "development",
        "Decision",
        "Support",
        "System",
        "Business",
        "Objects",
        "Migration",
        "Strategies",
        "Development",
        "Test",
        "Production",
        "Repositories",
        "Quality",
        "Assurance",
        "team",
        "testing",
        "Informatica",
        "workflows",
        "development",
        "mappings",
        "transformations",
        "Informatica",
        "business",
        "logic",
        "Sessions",
        "Batches",
        "Server",
        "Manager",
        "Created",
        "sessions",
        "manager",
        "workflow",
        "monitor",
        "Conducting",
        "unit",
        "testing",
        "Environment",
        "Informatica",
        "Power",
        "Centre",
        "UNIX",
        "Teradata",
        "Oracle",
        "TOAD",
        "Java",
        "Developer",
        "Aroghia",
        "Aurora",
        "CO",
        "April",
        "June",
        "Responsibilities",
        "spring",
        "AOP",
        "programming",
        "configure",
        "application",
        "Expertise",
        "enterprise",
        "applications",
        "Struts",
        "Frameworks",
        "end",
        "JSF",
        "Portlet",
        "Developed",
        "Scalable",
        "applications",
        "Stateless",
        "session",
        "EJBs",
        "UI",
        "panels",
        "JSF",
        "XHTML",
        "CSS",
        "DOJO",
        "JQuery",
        "MySQL",
        "data",
        "database",
        "Levels",
        "connection",
        "MySQL",
        "database",
        "Design",
        "Developed",
        "Web",
        "Service",
        "Apache",
        "Axis",
        "session",
        "message",
        "beans",
        "operation",
        "JBoss",
        "WebLogic",
        "VSS",
        "Visual",
        "Source",
        "Safe",
        "configuration",
        "management",
        "tool",
        "test",
        "cases",
        "Selenium",
        "Worked",
        "SDLC",
        "process",
        "water",
        "fall",
        "model",
        "AGILE",
        "methodology",
        "JSP",
        "interfaces",
        "Custom",
        "tags",
        "Developed",
        "Servlets",
        "Sql",
        "ANT",
        "application",
        "BEA",
        "WebLogic",
        "Application",
        "Server",
        "XML",
        "logic",
        "SAXDOM",
        "Parsers",
        "Good",
        "network",
        "EMC",
        "Documentum",
        "Support",
        "Teams",
        "product",
        "issues",
        "bugs",
        "tickets",
        "servicenow",
        "Jira",
        "basis",
        "end",
        "Swing",
        "IBM",
        "MQ",
        "Series",
        "project",
        "Apache",
        "Tomcat",
        "Server",
        "application",
        "modules",
        "Linux",
        "environment",
        "script",
        "Resource",
        "Manager",
        "job",
        "UNIX",
        "server",
        "web",
        "services",
        "REST",
        "gap",
        "MS",
        "DrupalWord",
        "press",
        "technology",
        "Design",
        "stores",
        "ASP",
        "JavaScript",
        "custom",
        "storefront",
        "applications",
        "custom",
        "userinterfaces",
        "client",
        "sites",
        "J2EE",
        "legacy",
        "COBOL",
        "mainframe",
        "implementations",
        "PLSQL",
        "SQL",
        "Developed",
        "Java",
        "Script",
        "Action",
        "Script",
        "VB",
        "Script",
        "macros",
        "Client",
        "Side",
        "Environment",
        "Spring",
        "Struts",
        "JSF",
        "JQuery",
        "MySQL",
        "DB2",
        "Net",
        "Beans",
        "JBoss",
        "CVS",
        "VSS",
        "water",
        "fall",
        "model",
        "UML",
        "JSP",
        "Servlets",
        "ANT",
        "XML",
        "EMC",
        "Jira",
        "IBM",
        "MQ",
        "Tomcat",
        "Server",
        "Linux",
        "Unix",
        "server",
        "Education",
        "Bachelor",
        "Technology",
        "Technology",
        "Texas",
        "AM",
        "University",
        "Skills",
        "MYSQL",
        "years",
        "SQL",
        "years",
        "LINUX",
        "years",
        "APACHE",
        "HADOOP",
        "HDFS",
        "years",
        "APACHE",
        "HADOOP",
        "years",
        "Additional",
        "Information",
        "Technical",
        "Skills",
        "Big",
        "Data",
        "Technologies",
        "Hadoop",
        "MapReduce",
        "HDFS",
        "Hive",
        "Pig",
        "Sqoop",
        "Flume",
        "solr",
        "Kafka",
        "Spark",
        "Storm",
        "Tools",
        "Jaspersoft",
        "Qlik",
        "Sense",
        "Tableau",
        "Scripting",
        "Languages",
        "Python",
        "Shell",
        "R",
        "Programming",
        "Languages",
        "C",
        "C",
        "Java",
        "Web",
        "Technologies",
        "HTML",
        "J2EE",
        "CSS",
        "JavaScript",
        "AJAX",
        "Servlets",
        "JSP",
        "DOM",
        "XML",
        "XSLT",
        "Application",
        "Server",
        "WebLogic",
        "Server",
        "Apache",
        "Tomcat",
        "DB",
        "Languages",
        "SQL",
        "PLSQL",
        "Postgres",
        "NoSQL",
        "HBase",
        "Cassandra",
        "Accumulo",
        "Databases",
        "ETL",
        "Oracle",
        "g",
        "MySQL",
        "DB2",
        "Informatica",
        "8x",
        "Talend",
        "Operating",
        "Systems",
        "Linux",
        "UNIX",
        "Windows",
        "Server",
        "IDEs",
        "Eclipse",
        "NetBeans",
        "JDeveloper",
        "IntelliJ",
        "IDEA",
        "Version",
        "Control",
        "CVS",
        "SVN",
        "Git"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T21:27:00.812466",
    "resume_data": "Hadoop developer Hadoop span ldeveloperspan Hadoop developer Humac Inc Albany NY Around 8 years of experience in Information Technology with Hands on experience in all the stages of system development efforts including requirement definition design implementation testing and documentation Experience in Hadoop ecosystem including the state of Art technologies Worked in a highly dynamic team using agile methodologies like Scrum and waterfall A quick learner punctual and trustworthy Good working experience on Hadoop architecture and various components such as HDFS Job Tracker Task Tracker Name Node Data Node and Map Reduce programming Techno functional responsibilities include interfacing with users identifying technical and functional gaps estimates designing developing producing documentation and extensive Production support Integrated various Big Data Technologies into the overall Experiences in analyzing developing testing and optimizing data transformation processes using Hadoop components Proficiency in importing and exporting data using Sqoop from Relational Database Systems to HDFS and vice versa Extensive Experience in Developing and maintaining Big Data streaming applications using Kafka Storm Spark and other Hadoop Components Experience in columnfamily based Databases HBase and Accumulo Skilled in Creating scheduling and maintaining Workflows in UC4 Procedural knowledge on cleansing and analysing data using Hive Presto on Hadoop Platform and also on Relational databases such as Oracle SQL Teradata and MongoDB Extensively worked on creating Teradata Bteq Scripts and used Informatica to load data into Teradata Preparation of Standard Code guidelines analysis and testing documentations In depth understanding of data structures and algorithms Worked on various IDEs Eclipse IntelliJ and repositories Git and SVN Hands on Experience in designing and developing applications in Spark using Scala to compare performance of Spark with Hive Involved in ooptimizing existing algorithms in Hadoop using Spark Context SparkSQL Data Frames and Pair RDDs Good knowledge on installing configuring and using Hadoop components like Map Reduce HDFS Hive Sqoop Pig Zookeeper and Flume In depth knowledge of database like SQL MySQL and extensive experience in writing SQL queries Stored Procedures Triggers Cursors Functions and Packages Proficient in writing Shell JavaScript and Python Scripts Good understanding on Installing and maintaining the Linux servers Experience in Monitoring System Metrics and logs for any problems adding removing or updating user account information resetting passwords etc Authorized to work in the US for any employer Work Experience Hadoop developer Humac Inc Phoenix AZ December 2016 to Present Responsibilities Analyzed various Relational Databases such as Oracle SQL Teradata and MongoDB to understand the source systems and develop application to migrate the data to Hadoop Environment Implemented several Batch Ingestion jobs for Historical data migration from various relational databases and files using Sqoop Hands on Developing a Near RealTime Framework using Kafka storm to ingest data from several source systems like Oracle SQL Teradata and MongoDB into Hadoop Environment Involved in implementing a Realtime framework to capture Streaming data and store in HDFS using Kafka Spark Developed Kafka consumer component for near realtime and RealTime data processing in Java and Scala Defined multiple Kafka Topics with several Partitions and replication factors across data centers Part of designing and developing a custom Java deamon to pull data from source systems and publish the resultant to a specific Kafka Topic Expertise in integrating Kafka with Storm and Spark streaming for near realtime and realtime Frameworks Migrated streaming data to HDFS Accumulo by creating Hdfs Bolt and Accumulo Bolt in Storm Successfully migrated complex SQL transformations in Pentaho that belong to Xfinity Home Security to Hadoop using Spark and Scala Optimizing of existing algorithms in Hadoop using Spark Context SparkSQL Data Frames and Pair RDDs Responsible for creating Hive tables and views using Partitions HQL Scripts in landing layer for analytics Also created tables in Accumulo Database and built external tables on top of them in hive using Accumulo Connectors for data analysis Analyzed massive Data sets in Hadoop ranging in Gigabytes using Hive Accumulo and Presto Developed Data archival and data purge in HDFS using Shell and Python scripts UC4 workflow to automate the process Defined UC4 workflows for running sequential job flows in Production Responsible for creating Deployment SMOP and LLDs Documents Created a Wiki page and migrated code to Git repository Wrote tested and implemented Teradata Bteq Scripts DML and DDL which is transactional data of Xfinity Mobile Performance tuned and optimized various SQL queries Hands on experience on Informatica to load data to Teradata by making various connections to load and extract data to and from Teradata efficiently Environment Hortonworks Hadoop 26 HDFS Hive 243 Presto Spark 22 Scala 2118 Kafka 09 Apache Storm 010 Accumulo Sqoop UC4 Rest API Java 17 18 Shell Scripting Python Scripting MySQL Oracle 11g MongoDB Teradata 15 SQL Informatica 1011 Hadoop Developer State Street Quincy MA October 2015 to December 2016 Responsibilities Involved in requirement gathering to setup a cluster Part of Configuring Hadoop cluster and load balancing across the nodes Developed MapReduce programs in Java for parsing the raw data and populating staging Tables Created Hive queries to compare the raw data with EDW reference tables and performing aggregates Experienced in developing custom input formats and data types to parse and process unstructured and semi structured input data and mapped them into key value pairs to implement business logic in MapReduce Experience in implementing custom sterilizer interceptor source and sink as per the requirement in Flume to ingest data from multiple sources Experience in setting up Fanout workflow in flume to design v shaped architecture to take data from many sources and ingest into single sink Importing and exporting data into HDFS and Hive using Sqoop Experienced in analyzing data with Hive and Pig Experienced knowledge over designing Restful services using java based APIs like JERSEY Developed Pig Latin scripts to extract the data from the web server output files to load into HDFS Integrating bulk data into Cassandra file system using MapReduce programs Expertise in designing data modeling for Cassandra NoSQL database Experienced in managing and reviewing Hadoop log files Defined multiple job flows using Oozie workflow Involved in working with Spark on top of YarnMRv2 for interactive and Batch Analysis Worked closely with AWS EC2 infrastructure teams to troubleshoot complex issues Expertise in writing the Scala code using higher order functions for the iterative algorithms in spark for performance consideration Experience in managing and monitoring Hadoop cluster using Cloudera Manager Experienced in analyzing and Optimizing RDDs by controlling partitions for the given data Good understanding on DAG cycle for entire spark application flow on Spark application WebUI Experienced in writing live Realtime Processing using Spark Streaming with Kafka Developed custom mappers in python script and Hive UDFs and UDAFs based on the given requirement Used HiveQL to analyze the partitioned and bucketed data and compute various metrics for reporting Supported in setting up QA environment and updating configurations for implementing scripts with Pig Hive and Sqoop Unit tested a sample of raw data and improved performance and turned over to production Environment CDH JavaJDK17 Hadoop MapReduce HDFS Hive Sqoop Flume Cassandra Pig Oozie Kerberos Scala Spark SQL Spark Streaming Kafka Linux AWS Shell Scripting MySQL Oracle 11g SQLPLUS Hadoop Developer Geometric Ltd August 2013 to October 2015 Responsibilities Installed Name node Secondary name node Resource Manager Node manager Application master Data node using Cloudera Installed and configured Hortonworks Ambari for easy management of existing Hadoop cluster Installed and Configured HDP Installed and configured multinodes fully distributed Hadoop cluster of large number of nodes Provided Hadoop OS Hardware optimizations Setting up the machines with Network Control Static IP Disabled Firewalls Swap memory Understanding the performance bottlenecks by analyzing the existing hadoop cluster and provided performance tuning accordingly Regular Commissioning and Decommissioning of nodes depending upon the amount of data Installed and configured Hadoop components Hdfs Hive HBase Communicating with the development teams and attending daily meetings Addressing and Troubleshooting issues on a daily basis Working with data delivery teams to setup new Hadoop users This job includes setting up Linux users setting up Kerberos principals and testing HDFS Hive Cluster maintenance as well as creation and removal of nodes Monitor Hadoop cluster connectivity and security Manage and review Hadoop log files Configured the cluster to achieve the optimal results by finetuning the cluster Dumped the data from one cluster to other cluster by using DISTCP and automated the dumping procedure using shell scripts Designed the shell script for backing up of important metadata and rotating the logs on a monthly basis Implemented open source monitoring tool GANGLIA for monitoring the various services across the cluster Testing evaluation and troubleshooting of different NoSQL database systems and cluster configurations to ensure highavailability in various crash scenarios Performance tuning and stress testing of NoSQL database environments to ensure acceptable database performance in production mode Designed the cluster so that only one secondary name node daemon could be run at any given time Implemented commissioning and decommissioning of data nodes killing the unresponsive task tracker and dealing with blacklisted task trackers Dumped the data from HDFS to MYSQL database and viceversa using SQOOP Provided the necessary support to the ETL team when required Integrated Nagios in the Hadoop cluster for alerts Performed both major and minor upgrades to the existing cluster and rolling back to the previous version Environment LINUX HDFS MapReduce KDC NAGIOS GANGLIA OOZIE SQOOP Cloudera Manager Teradata Developer Enquero Milpitas CA June 2011 to August 2013 Responsibilities Extensively used ETL to load data from Oracle and Flat files to Data Warehouse Extensively worked in data Extraction Transformation and Loading from source to target system using power center of Informatica Developed complex mappings in Informatica to load the data from various sources Implemented performance tuning logic on targets sources mappings sessions to provide maximum efficiency and performance Parameterized the mappings and increased the reusability Used Informatica Power Center Workflow manager to create sessions workflows and batches to run with the logic embedded in the mappings Created procedures to truncate data in the target before the session run Extensively used Toad utility for executing SQL scripts and worked on SQL for enhancing the performance of the conversion mapping Used the PLSQL procedures for Informatica mappings for truncating the data in target tables at run time Worked on Teradata RDBMS using Fast load Multi load Tpump Fast export Multi load Export Teradata Sql and Bteq Teradata utilities Involved in Performance Tuning at various levels including Target Source Mapping and Session for large data files Extracted data from various source systems like Oracle SQL Server and flat files as per the requirements Performed bulk data load from multiple data source ORACLE 8i legacy systems to Teradata RDBMS using BTEQ Multi Load and Fast Load Created optimized reviewed and executed Teradata SQL test queries to validate transformation rules used in source to target mappingssource views and to verify data in target tables Performed tuning and optimization of complex SQL queries using Teradata Explain Responsible for Collect Statics on FACT tables Design and development of the complete Decision Support System using Business Objects Worked on Migration Strategies between Development Test and Production Repositories Supported the Quality Assurance team in testing and validating the Informatica workflows Extensively involved in development of mappings using various transformations of Informatica according to business logic Created and Scheduled Sessions and Batches using Server Manager Created and Monitor the sessions using workflow manager and workflow monitor Conducting unit testing Environment Informatica Power Centre 86 UNIX Teradata Oracle 8i TOAD Java Developer Aroghia Aurora CO April 2009 to June 2011 Responsibilities Developed the spring AOP programming to configure logging for the application Expertise in developing enterprise applications using Struts Frameworks Developed the front end using JSF and Portlet Developed Scalable applications using Stateless session EJBs Developed the UI panels using JSF XHTML CSS DOJO and JQuery MySQL to access data in the database at different Levels Making a connection to backend MySQL database Design and Developed using Web Service using Apache Axis wrote numerous session and message driven beans for operation on JBoss and WebLogic Used VSS Visual Source Safe as configuration management tool Created automated test cases using Selenium Worked with SDLC process like water fall model AGILE methodology JSP interfaces were developed Custom tags were used Developed Servlets and Worked extensively on Sql Used ANT for building the application and deployed on BEA WebLogic Application Server Was responsible for Developing XML Parsing logic using SAXDOM Parsers Good network at EMC Documentum Support Teams who help solve product issues and bugs Worked on tickets from servicenow and Jira on daily basis Designed the front end using Swing Used IBM MQ Series in the project Apache Tomcat Server was used to deploy the application Involving in Building the modules in Linux environment with ant script Used Resource Manager to schedule the job in UNIX server Used web services REST to bridge the gap between our MS and DrupalWord press technology Design online stores using ASP JavaScript develop custom storefront applications and custom userinterfaces for client sites J2EE to communicate legacy COBOL based mainframe implementations Worked on PLSQL and SQL queries Developed Java Script and Action Script VB Script macros for Client Side validations Environment Spring Struts JSF EJBs JQuery MySQL DB2 Net Beans JBoss CVS VSS water fall model UML JSP Servlets ANT XML EMC Jira IBM MQ Tomcat Server Linux Unix server Education Bachelor of Technology in Technology Texas AM University Skills MYSQL 6 years SQL 6 years LINUX 5 years APACHE HADOOP HDFS 4 years APACHE HADOOP SQOOP 4 years Additional Information Technical Skills Big Data Technologies Hadoop MapReduce HDFS Hive Pig Sqoop Flume solr Kafka Spark Storm Reporting Tools Jaspersoft Qlik Sense Tableau Scripting Languages Python Shell R Programming Languages C C Java Web Technologies HTML J2EE CSS JavaScript AJAX Servlets JSP DOM XML XSLT Application Server WebLogic Server Apache Tomcat DB Languages SQL PLSQL Postgres NoSQL Databases HBase Cassandra Accumulo Databases ETL Oracle 10g11g MySQL 52 DB2 Informatica v 8x Talend Operating Systems Linux UNIX Windows 2003 Server IDEs Eclipse NetBeans JDeveloper IntelliJ IDEA Version Control CVS SVN Git",
    "unique_id": "cea32ca4-e91f-43df-809d-24501fdc5fa7"
}