{
    "clean_data": "Job Seeker Plano TX Cloudera Certified Hadoop and Spark developer with 4 years of experience in IT industry which includes design development testing and application support of various Web Based applications using Java J2EE Technologies and Big Data Ecosystem Around 2 years of comprehensive experience in Big Data processing and its ecosystem like HDFS Hive Sqoop Oozie Falcon HBase Yarn Zookeeper Attunity Replicate Hue and Spark components Excellent knowledge on Spark Scala Spark SQL Spark Streaming Flume Pig Hadoop MapReduce concepts and NiFi Around 15 years of experience in Software development using Java J2EEJSF and Chordiant Framework and Oracle Database in various areas like Requirements gathering Analysis Design Development Testing Implementation and Maintenance Good handson knowledge in Hortonworks and Cloudera Hadoop Distributions Experience in importing and exporting data using Sqoop from HDFS to Relational Database systems RDBMS and viceversa Expertise in loading different file based sources into Hadoop Landing zone Hands on experience in developing SPARK applications using Spark APIs like Spark transformations and actions RDD Spark MLlib and Spark SQLDataframes using Scala and Python Scheduled job workflow for FTP Sqoop and hive scripts using Oozie coordinators Designed and Developed Data Ingestion Framework using Java to ingest data from various RDBMS and file based sources and housed them in Hive tables from which reports are generated Worked with various file formats such as delimited text files click stream log files Sequence Files Avro files ORC files JSON files Parquet and XML File formats Excellent knowledge of Hadoop Architecture and various components such as HDFS Job Tracker Task Tracker Name Node Data Node and MapReduce Programming paradigm Involved in converting HiveSQL queries into Spark transformations using Spark RDDs Scala and have a good experience in using SparkShell and Spark Streaming Extensive experience in working with structured data using Hive QL join operations Partitioning Bucketing and experienced in optimizing Hive Queries Experience in implementing Kerberos authentication protocol in Hadoop for data security Used Scala SBT to develop Scala coded spark projects and executed using sparksubmit Having experience in analyzing large amounts of data sets to determine optimal way to aggregate and report on it using Qlikview Experienced with code versioning and dependency management systems such as Accurev and Maven Monitoring Job failures and performing the root cause analysis and corrective action Developed workflows for the batch jobs and scheduled using Oozie Experience in data workflow management tools such as Falcon Oozie and Autosys which is job scheduling software Experience with cloud computing platforms like Amazon Web ServicesAWS Experience in developing applications using Java technologies include Core Java J2EE Java Server Pages JSP Servlets and Java Script Well versed in using Software development methodologies like Agile and DevOps Methodology Experience in preparing and executing unit test plan and unit test cases after software development using JUNIT Highly motivated and passionate to learn and explore emerging technologies Authorized to work in the US for any employer Work Experience UdemySelfLearning November 2018 to Present Project Similar Movies Prediction based on rating in Apache Spark GroupLens Research has collected and made available rating data sets from the MovieLens website The data sets were collected over various periods of time depending on the size of the set For my project I took a 20 million ratings and 465000 tag applications applied to 27000 movies by 138000 users and analyzed them using Spark Spark SQL Python Scala on Amazon EMR to find the similar movies to each other based on the user ratings Project Details Worked on Cloudera distribution and deployed on AWS EC2 Instances Analyzed movie rating dataset by building data ETL pipeline using PySpark Scala and Spark SQL Designed movies recommendation using Spark MLlibs collaborative filtering algorithm by parallel processing million of records on Amazon s3 Implemented Alternative Least Square model for customized movie recommendation Migrated the historical data from movieLens site to Amazon s3 and cached the data in memory for repetitive use Used Rating class for parsing the dat file and applied transformation to create RDD objects Used Matrix Factorization Model to make product predictions for users Evaluated the performance of models and got recommendations successfully proven boosted running speed by parallelism Implemented usage of Amazon EMR for processing Big Data across a Hadoop cluster of virtual servers on Amazon Elastic Compute Cloud EC2 and Amazon Simple Storage Service S3 Good Understanding on Extracted Real time feed using Spark streaming and convert it to RDD and process data into Data Frame and load the data into HDFS Worked on improving the performance and optimization of the existing algorithms in Hadoop using Spark Context SparkSQL Data Frame Pair RDDs Spark YARN Environment AWS S3 Spark Spark SQL Hive LINUX Scala UNIX Shell Scripting Python YARN Cloudera Hadoop Developer Ford Motor Company Dearborn MI July 2016 to November 2017 Project Details Data Supply ChainDSC is a Data lake on Hadoop platform to receive consolidate data DSC is the Ford information platform for analytics that balances the demands of data management and information access It helps integrate structured semistructured and unstructured information into single logical information The DSC Application supports the Global Data Insight and Analytic Skill teamGDIA and their mission to drive evidencebased decision making provide timely actionable forwardlooking insights to their business partners Responsibilities Developed a Custom Sqoop project based on Data Ingestion Framework using Java to ingest IncrementalFull load data from Teradata source into Hadoop Landing Zone based on timestamp partitions Analyze and understanding the requirement given by downstream users Hands on experience on extracting data from different databases file based sources and scheduled Oozie falcon workflows to execute this job on daily and monthly basis Responsible for loading various sources to HDFS using SQOOP and command line bash scripts Prepared and executed HBase entries to run Oozie jobs in Adhoc Created Hive tables to load the Data and stored as ORC files for processing Implemented Hive Partitioning and bucketing for further classification of data Involved in creating Hive Tables loading with data and writing Hive queries which will invoke and run Map Reduce jobs in the backend Setting up and worked on Kerberos authentication principals to establish secure network communication on cluster and testing of HDFS Hive Pig and MapReduce to access cluster for new users Autosys scheduler to automate the jobs and time scheduling Worked with No SQL databases like HBase and created HBase tables to load large sets of semi structured data coming from various sources Used Attunity Replicate web based Interface tool to load data efficiently and quickly into HDFS from different sources Performed data masking and special character removal tasks in the data transformation using SPARK Been part of Design Reviews Daily Project Scrums and sprint planning based on Agile methodology Used Maven to build jar files and Accurev as software configuration management tool Used Rally as a work tracking tool and BMC for incident management Worked with different file formats such as Text Sequence files Avro ORC and Parquet Involved in Qlikview development of business analytic and visualization reports for DSC Management Dashboard SupportTroubleshoot hive programs running on the cluster and Involved in fixing issues arising out of duration testing Environment Hortonworks HDP 253 HadoopYARN HDFS Hive Sqoop Oozie Falcon Accurev LINUX Hue HBase Zookeeper Java Maven Autosys Mainframe Teradata Shell scripting Qlikview Rally Edureka November 2015 to April 2016 Project Details In Internet Telephony a call detail record is a data record that contains information related to a telephone call such as the origination and destination addresses of the call the time the call started and ended the duration of the call the time of day the call was made and any toll charges that were added through the network or charges for operator services among other details of the call This project is to find out the customers who are facing frequent call drops in Roaming This is a very important report which telecom companies uses to prevent customer churn out by calling them back and at the same time contacting their roaming partners to improve the connectivity issues in specific areas Responsibilities Developed Spark code using Scala and SparkSQL for faster testing and data processing as per the requirement Installed and configured Spark Hive Pig Sqoop and Oozie on the Hadoop cluster Imported millions of structured data from relational databases using Sqoop import to process using Spark and stored the data into HDFS in CSV format Used Spark SQL to process the huge amount of structured data Developed traits and case classes etc in Scala implemented business logic using Scala Exploring with the Spark for improving the performance and optimization of the existing algorithms in Hadoop using Spark Context SparkSQL Data Frame Pair RDDs Spark YARN Used DataFrame API in Scala for converting the distributed collection of data organized into named columns Registered the datasets as Hive Table Involved in converting HiveSQL queries into Spark transformations using Spark RDDs and Scala Used various spark Transformations and Actions for cleansing the input data Experienced in handling large datasets using Partitions Spark in Memory capabilities Broadcasts in Spark Effective efficient Joins Transformations and other during ingestion process itself Responsible in performing sort join aggregations filter and other transformations on the datasets using Spark Developed solutions to preprocess large sets of structured with different file formats Text file Avro data files Sequence files Xml and JSON files ORC and Parquet Experienced with batch processing of data sources using Apache Spark Environment HDFS YARN Sqoop Apache Spark SparkSQL Cloudera CDH 5X Sparkshell Hive Hue MYSQL JavaJ2EE Tata Consultancy Services February 2013 to May 2014 Developer Project Details Lloyds Banking Group is a major British financial institution formed through the acquisition of HBOS by Lloyds with millions of UK customers with a presence in nearly every community Its business is focused on retail and commercial financial services The project Savings ReEngineering project is one of the big milestone deliveries which went on live in three different group releases working in agile methodology The main requirements of this project were around Savings swimlanes It is the fundamental rethinking and radical redesign of business process to achieve dramatic improvements in critical measures of performance such as cost service and speed Responsibilities Involved in complete Software Development Life cycle SDLC of the project from Analysis Design Programming Testing and Deploying the application Used Java Server Faces JSF and Java Server Pages JSP for developing UI pages Developed HTML Java script prototypes for the Savings reengineering screens Developed web application using Chordiant MVC Framework Generated Web Service Client from a Web Services Description LanguageWSDL Used JUNIT for unit testing the code changes Involved in testing of application on various levels like Unit and Integration testing Provided support for SIT and UAT Used Rational Clear Case Clear Quest for source control and defect management Submitted the unit tested code for Review and rectified the concerns raised Interact with business people SMEs onshore partners and other supporting teams through mails voice calls video conference calls online communicator chats to ensure the delivery of an optimized solution to the customer Fixed the defects in SIT phase immediately and provided build with minimal turnaround time Used Quality Center QC for requirements and bug tracking Environment Core Java Chordiant MVC JSFJSP Servlets Oracle SQL Windows XPVista RAD TOAD WebSphereApplication Server Web Developer GEE Communication IN June 2012 to January 2013 Project Details Gee communication is part of a telecom company where they created a portal that helps to create and send EMail campaigns track leads automate salesforce activities and help in effective Customer Relationship Management CRM Responsibilities Involved in developing easytouse user interface and stepbystep process that helps to create eyecatching EMail campaign Design and developed webbased software using Java Server FacesJSF framework Data has been handled using the JDBC connection Java Beans were used to handle business logic as a Model and Servlets to control the flow of application as Controller Proficient in developing responsive Front End components using HTML CSS JSP tags JavaScript Used IBMs WebSphere Application Server to deploy code Involved in Code Reviews Defect Fixing and UAT support Familiarity with all aspects of Software Development Life Cycle Make code modifications for the assigned projects according to business specifications and application standards Perform unit and system testing for all coding changes Environment HTML CSS Servlets JSF JSP JUNIT Oracle 11g Eclipse JavaScript Core Java Education Bachelors Skills Apache hadoop hdfs 2 years Hadoop 2 years Hadoop distributed file system 2 years Java 3 years Sql 3 years Spark Less than 1 year Additional Information TECHNICAL SKILLS Hadoop Ecosystem HDFS MapReduce Hive Sqoop Oozie Zookeeper Spark Falcon Kafka Attunity Replicate Pig Spark Ecosystem Spark Core Spark SQL Spark MLlib Databases HBaseNoSQLMYSQL Teradata Programming Languages Java Scala Hive QL Shell Scripting Framework Hadoop API JSF Web Technologies HTML Java script CSS JSP Servlets XML IDEInterfaces Eclipse IntelliJ SparkShell PySpark JUNIT Maven Methodologies Agile DevOps Operating Systems Windows UNIX LINUX Cloud Platform AWS EMR",
    "entities": [
        "XML File",
        "Spark Context",
        "AWS EC2",
        "CSS JSP Servlets XML IDEInterfaces",
        "Used Rally",
        "SPARK",
        "BMC",
        "Spark Effective",
        "HBOS",
        "Customer Relationship Management CRM Responsibilities Involved",
        "Data Ingestion Framework",
        "Spark SQL Designed",
        "Project Details In Internet Telephony",
        "Responsibilities Developed Spark",
        "HDFS Hive Sqoop Oozie Falcon HBase Yarn Zookeeper Attunity Replicate Hue",
        "UK",
        "Design Reviews Daily Project Scrums",
        "Falcon Oozie",
        "Present Project Similar Movies Prediction",
        "Software Development Life Cycle Make",
        "Maven Monitoring Job",
        "Amazon Elastic Compute Cloud EC2",
        "DevOps Methodology",
        "Spark MLlibs",
        "Apache Spark GroupLens Research",
        "IncrementalFull",
        "RDD",
        "Hadoop",
        "Oozie coordinators Designed and Developed Data Ingestion Framework",
        "Amazon Simple Storage Service",
        "HBase",
        "Avro",
        "DSC Management Dashboard SupportTroubleshoot",
        "Amazon",
        "Hive QL",
        "Spark Hive Pig Sqoop",
        "Hadoop Landing Zone",
        "HTML CSS JSP",
        "Developed",
        "SparkSQL",
        "Java J2EE Technologies and Big Data Ecosystem",
        "Lloyds",
        "UAT Used Rational Clear Case Clear Quest",
        "Quality Center QC",
        "Kerberos",
        "Project Details Lloyds Banking Group",
        "Node Data",
        "Responsibilities Involved",
        "Adhoc Created Hive",
        "Sequence",
        "Hortonworks",
        "Apache Spark Environment HDFS YARN Sqoop",
        "Hive Queries",
        "FTP Sqoop",
        "the Global Data Insight",
        "RDD Spark",
        "Submitted",
        "ORC",
        "Project Details Data Supply ChainDSC",
        "Requirements gathering Analysis Design Development Testing Implementation and Maintenance Good",
        "Amazon s3 Implemented Alternative Least Square",
        "Autosys",
        "Parquet Involved",
        "Oracle Database",
        "Interface",
        "Savings",
        "Spark",
        "Agile",
        "SparkShell",
        "Responsibilities Developed a Custom",
        "Amazon EMR",
        "EMail",
        "Spark Developed",
        "SIT",
        "US",
        "Perform",
        "Sqoop",
        "Oozie Experience",
        "The DSC Application",
        "Scala",
        "Hadoop Architecture",
        "DSC",
        "SPARK Been",
        "Scala SBT",
        "HDFS Job Tracker Task Tracker",
        "Analysis Design Programming Testing and Deploying",
        "Oozie",
        "Software Development Life",
        "Additional Information TECHNICAL SKILLS Hadoop Ecosystem HDFS MapReduce Hive Sqoop",
        "MapReduce Programming",
        "Spark Spark",
        "Hadoop Landing",
        "PySpark Scala",
        "Cloudera Hadoop Distributions",
        "Data Frame",
        "Big Data",
        "Hive",
        "SQOOP",
        "Partitioning Bucketing",
        "JUNIT",
        "Partitions Spark",
        "Ford",
        "ETL",
        "Spark Scala Spark",
        "Controller Proficient",
        "Maven",
        "Performed",
        "Front End",
        "UI",
        "TX Cloudera Certified Hadoop",
        "JUNIT Highly",
        "Spark Streaming Extensive",
        "Spark SQLDataframes",
        "Attunity Replicate",
        "Chordiant MVC Framework Generated Web Service Client",
        "Data",
        "Relational Database",
        "Ford Motor Company Dearborn",
        "MapReduce",
        "Project Details Worked",
        "WebSphere Application Server",
        "Teradata",
        "Model and Servlets"
    ],
    "experience": "Experience in importing and exporting data using Sqoop from HDFS to Relational Database systems RDBMS and viceversa Expertise in loading different file based sources into Hadoop Landing zone Hands on experience in developing SPARK applications using Spark APIs like Spark transformations and actions RDD Spark MLlib and Spark SQLDataframes using Scala and Python Scheduled job workflow for FTP Sqoop and hive scripts using Oozie coordinators Designed and Developed Data Ingestion Framework using Java to ingest data from various RDBMS and file based sources and housed them in Hive tables from which reports are generated Worked with various file formats such as delimited text files click stream log files Sequence Files Avro files ORC files JSON files Parquet and XML File formats Excellent knowledge of Hadoop Architecture and various components such as HDFS Job Tracker Task Tracker Name Node Data Node and MapReduce Programming paradigm Involved in converting HiveSQL queries into Spark transformations using Spark RDDs Scala and have a good experience in using SparkShell and Spark Streaming Extensive experience in working with structured data using Hive QL join operations Partitioning Bucketing and experienced in optimizing Hive Queries Experience in implementing Kerberos authentication protocol in Hadoop for data security Used Scala SBT to develop Scala coded spark projects and executed using sparksubmit Having experience in analyzing large amounts of data sets to determine optimal way to aggregate and report on it using Qlikview Experienced with code versioning and dependency management systems such as Accurev and Maven Monitoring Job failures and performing the root cause analysis and corrective action Developed workflows for the batch jobs and scheduled using Oozie Experience in data workflow management tools such as Falcon Oozie and Autosys which is job scheduling software Experience with cloud computing platforms like Amazon Web ServicesAWS Experience in developing applications using Java technologies include Core Java J2EE Java Server Pages JSP Servlets and Java Script Well versed in using Software development methodologies like Agile and DevOps Methodology Experience in preparing and executing unit test plan and unit test cases after software development using JUNIT Highly motivated and passionate to learn and explore emerging technologies Authorized to work in the US for any employer Work Experience UdemySelfLearning November 2018 to Present Project Similar Movies Prediction based on rating in Apache Spark GroupLens Research has collected and made available rating data sets from the MovieLens website The data sets were collected over various periods of time depending on the size of the set For my project I took a 20 million ratings and 465000 tag applications applied to 27000 movies by 138000 users and analyzed them using Spark Spark SQL Python Scala on Amazon EMR to find the similar movies to each other based on the user ratings Project Details Worked on Cloudera distribution and deployed on AWS EC2 Instances Analyzed movie rating dataset by building data ETL pipeline using PySpark Scala and Spark SQL Designed movies recommendation using Spark MLlibs collaborative filtering algorithm by parallel processing million of records on Amazon s3 Implemented Alternative Least Square model for customized movie recommendation Migrated the historical data from movieLens site to Amazon s3 and cached the data in memory for repetitive use Used Rating class for parsing the dat file and applied transformation to create RDD objects Used Matrix Factorization Model to make product predictions for users Evaluated the performance of models and got recommendations successfully proven boosted running speed by parallelism Implemented usage of Amazon EMR for processing Big Data across a Hadoop cluster of virtual servers on Amazon Elastic Compute Cloud EC2 and Amazon Simple Storage Service S3 Good Understanding on Extracted Real time feed using Spark streaming and convert it to RDD and process data into Data Frame and load the data into HDFS Worked on improving the performance and optimization of the existing algorithms in Hadoop using Spark Context SparkSQL Data Frame Pair RDDs Spark YARN Environment AWS S3 Spark Spark SQL Hive LINUX Scala UNIX Shell Scripting Python YARN Cloudera Hadoop Developer Ford Motor Company Dearborn MI July 2016 to November 2017 Project Details Data Supply ChainDSC is a Data lake on Hadoop platform to receive consolidate data DSC is the Ford information platform for analytics that balances the demands of data management and information access It helps integrate structured semistructured and unstructured information into single logical information The DSC Application supports the Global Data Insight and Analytic Skill teamGDIA and their mission to drive evidencebased decision making provide timely actionable forwardlooking insights to their business partners Responsibilities Developed a Custom Sqoop project based on Data Ingestion Framework using Java to ingest IncrementalFull load data from Teradata source into Hadoop Landing Zone based on timestamp partitions Analyze and understanding the requirement given by downstream users Hands on experience on extracting data from different databases file based sources and scheduled Oozie falcon workflows to execute this job on daily and monthly basis Responsible for loading various sources to HDFS using SQOOP and command line bash scripts Prepared and executed HBase entries to run Oozie jobs in Adhoc Created Hive tables to load the Data and stored as ORC files for processing Implemented Hive Partitioning and bucketing for further classification of data Involved in creating Hive Tables loading with data and writing Hive queries which will invoke and run Map Reduce jobs in the backend Setting up and worked on Kerberos authentication principals to establish secure network communication on cluster and testing of HDFS Hive Pig and MapReduce to access cluster for new users Autosys scheduler to automate the jobs and time scheduling Worked with No SQL databases like HBase and created HBase tables to load large sets of semi structured data coming from various sources Used Attunity Replicate web based Interface tool to load data efficiently and quickly into HDFS from different sources Performed data masking and special character removal tasks in the data transformation using SPARK Been part of Design Reviews Daily Project Scrums and sprint planning based on Agile methodology Used Maven to build jar files and Accurev as software configuration management tool Used Rally as a work tracking tool and BMC for incident management Worked with different file formats such as Text Sequence files Avro ORC and Parquet Involved in Qlikview development of business analytic and visualization reports for DSC Management Dashboard SupportTroubleshoot hive programs running on the cluster and Involved in fixing issues arising out of duration testing Environment Hortonworks HDP 253 HadoopYARN HDFS Hive Sqoop Oozie Falcon Accurev LINUX Hue HBase Zookeeper Java Maven Autosys Mainframe Teradata Shell scripting Qlikview Rally Edureka November 2015 to April 2016 Project Details In Internet Telephony a call detail record is a data record that contains information related to a telephone call such as the origination and destination addresses of the call the time the call started and ended the duration of the call the time of day the call was made and any toll charges that were added through the network or charges for operator services among other details of the call This project is to find out the customers who are facing frequent call drops in Roaming This is a very important report which telecom companies uses to prevent customer churn out by calling them back and at the same time contacting their roaming partners to improve the connectivity issues in specific areas Responsibilities Developed Spark code using Scala and SparkSQL for faster testing and data processing as per the requirement Installed and configured Spark Hive Pig Sqoop and Oozie on the Hadoop cluster Imported millions of structured data from relational databases using Sqoop import to process using Spark and stored the data into HDFS in CSV format Used Spark SQL to process the huge amount of structured data Developed traits and case classes etc in Scala implemented business logic using Scala Exploring with the Spark for improving the performance and optimization of the existing algorithms in Hadoop using Spark Context SparkSQL Data Frame Pair RDDs Spark YARN Used DataFrame API in Scala for converting the distributed collection of data organized into named columns Registered the datasets as Hive Table Involved in converting HiveSQL queries into Spark transformations using Spark RDDs and Scala Used various spark Transformations and Actions for cleansing the input data Experienced in handling large datasets using Partitions Spark in Memory capabilities Broadcasts in Spark Effective efficient Joins Transformations and other during ingestion process itself Responsible in performing sort join aggregations filter and other transformations on the datasets using Spark Developed solutions to preprocess large sets of structured with different file formats Text file Avro data files Sequence files Xml and JSON files ORC and Parquet Experienced with batch processing of data sources using Apache Spark Environment HDFS YARN Sqoop Apache Spark SparkSQL Cloudera CDH 5X Sparkshell Hive Hue MYSQL JavaJ2EE Tata Consultancy Services February 2013 to May 2014 Developer Project Details Lloyds Banking Group is a major British financial institution formed through the acquisition of HBOS by Lloyds with millions of UK customers with a presence in nearly every community Its business is focused on retail and commercial financial services The project Savings ReEngineering project is one of the big milestone deliveries which went on live in three different group releases working in agile methodology The main requirements of this project were around Savings swimlanes It is the fundamental rethinking and radical redesign of business process to achieve dramatic improvements in critical measures of performance such as cost service and speed Responsibilities Involved in complete Software Development Life cycle SDLC of the project from Analysis Design Programming Testing and Deploying the application Used Java Server Faces JSF and Java Server Pages JSP for developing UI pages Developed HTML Java script prototypes for the Savings reengineering screens Developed web application using Chordiant MVC Framework Generated Web Service Client from a Web Services Description LanguageWSDL Used JUNIT for unit testing the code changes Involved in testing of application on various levels like Unit and Integration testing Provided support for SIT and UAT Used Rational Clear Case Clear Quest for source control and defect management Submitted the unit tested code for Review and rectified the concerns raised Interact with business people SMEs onshore partners and other supporting teams through mails voice calls video conference calls online communicator chats to ensure the delivery of an optimized solution to the customer Fixed the defects in SIT phase immediately and provided build with minimal turnaround time Used Quality Center QC for requirements and bug tracking Environment Core Java Chordiant MVC JSFJSP Servlets Oracle SQL Windows XPVista RAD TOAD WebSphereApplication Server Web Developer GEE Communication IN June 2012 to January 2013 Project Details Gee communication is part of a telecom company where they created a portal that helps to create and send EMail campaigns track leads automate salesforce activities and help in effective Customer Relationship Management CRM Responsibilities Involved in developing easytouse user interface and stepbystep process that helps to create eyecatching EMail campaign Design and developed webbased software using Java Server FacesJSF framework Data has been handled using the JDBC connection Java Beans were used to handle business logic as a Model and Servlets to control the flow of application as Controller Proficient in developing responsive Front End components using HTML CSS JSP tags JavaScript Used IBMs WebSphere Application Server to deploy code Involved in Code Reviews Defect Fixing and UAT support Familiarity with all aspects of Software Development Life Cycle Make code modifications for the assigned projects according to business specifications and application standards Perform unit and system testing for all coding changes Environment HTML CSS Servlets JSF JSP JUNIT Oracle 11 g Eclipse JavaScript Core Java Education Bachelors Skills Apache hadoop hdfs 2 years Hadoop 2 years Hadoop distributed file system 2 years Java 3 years Sql 3 years Spark Less than 1 year Additional Information TECHNICAL SKILLS Hadoop Ecosystem HDFS MapReduce Hive Sqoop Oozie Zookeeper Spark Falcon Kafka Attunity Replicate Pig Spark Ecosystem Spark Core Spark SQL Spark MLlib Databases HBaseNoSQLMYSQL Teradata Programming Languages Java Scala Hive QL Shell Scripting Framework Hadoop API JSF Web Technologies HTML Java script CSS JSP Servlets XML IDEInterfaces Eclipse IntelliJ SparkShell PySpark JUNIT Maven Methodologies Agile DevOps Operating Systems Windows UNIX LINUX Cloud Platform AWS EMR",
    "extracted_keywords": [
        "Job",
        "Seeker",
        "Plano",
        "TX",
        "Cloudera",
        "Certified",
        "Hadoop",
        "Spark",
        "developer",
        "years",
        "experience",
        "IT",
        "industry",
        "design",
        "development",
        "testing",
        "application",
        "support",
        "Web",
        "applications",
        "Java",
        "J2EE",
        "Technologies",
        "Big",
        "Data",
        "Ecosystem",
        "years",
        "experience",
        "Big",
        "Data",
        "processing",
        "ecosystem",
        "HDFS",
        "Hive",
        "Sqoop",
        "Oozie",
        "Falcon",
        "HBase",
        "Yarn",
        "Zookeeper",
        "Attunity",
        "Replicate",
        "Hue",
        "Spark",
        "components",
        "knowledge",
        "Spark",
        "Scala",
        "Spark",
        "SQL",
        "Spark",
        "Streaming",
        "Flume",
        "Pig",
        "Hadoop",
        "MapReduce",
        "concepts",
        "NiFi",
        "years",
        "experience",
        "Software",
        "development",
        "Java",
        "J2EEJSF",
        "Chordiant",
        "Framework",
        "Oracle",
        "Database",
        "areas",
        "Requirements",
        "Analysis",
        "Design",
        "Development",
        "Testing",
        "Implementation",
        "Maintenance",
        "Good",
        "handson",
        "knowledge",
        "Hortonworks",
        "Cloudera",
        "Hadoop",
        "Distributions",
        "Experience",
        "data",
        "Sqoop",
        "HDFS",
        "Relational",
        "Database",
        "systems",
        "RDBMS",
        "viceversa",
        "Expertise",
        "file",
        "sources",
        "Hadoop",
        "Landing",
        "zone",
        "Hands",
        "experience",
        "SPARK",
        "applications",
        "Spark",
        "APIs",
        "Spark",
        "transformations",
        "actions",
        "Spark",
        "MLlib",
        "Spark",
        "SQLDataframes",
        "Scala",
        "Python",
        "job",
        "workflow",
        "FTP",
        "Sqoop",
        "hive",
        "scripts",
        "Oozie",
        "coordinators",
        "Developed",
        "Data",
        "Ingestion",
        "Framework",
        "Java",
        "data",
        "RDBMS",
        "sources",
        "Hive",
        "tables",
        "reports",
        "file",
        "formats",
        "text",
        "files",
        "stream",
        "log",
        "files",
        "Sequence",
        "Files",
        "Avro",
        "files",
        "JSON",
        "Parquet",
        "XML",
        "File",
        "formats",
        "knowledge",
        "Hadoop",
        "Architecture",
        "components",
        "HDFS",
        "Job",
        "Tracker",
        "Task",
        "Tracker",
        "Name",
        "Node",
        "Data",
        "Node",
        "MapReduce",
        "Programming",
        "paradigm",
        "HiveSQL",
        "queries",
        "Spark",
        "transformations",
        "Spark",
        "RDDs",
        "Scala",
        "experience",
        "SparkShell",
        "Spark",
        "Streaming",
        "experience",
        "data",
        "Hive",
        "QL",
        "join",
        "operations",
        "Partitioning",
        "Bucketing",
        "Hive",
        "Queries",
        "Experience",
        "Kerberos",
        "authentication",
        "protocol",
        "Hadoop",
        "data",
        "security",
        "Scala",
        "SBT",
        "Scala",
        "spark",
        "projects",
        "sparksubmit",
        "experience",
        "amounts",
        "data",
        "sets",
        "way",
        "Qlikview",
        "Experienced",
        "code",
        "versioning",
        "dependency",
        "management",
        "systems",
        "Accurev",
        "Maven",
        "Monitoring",
        "Job",
        "failures",
        "root",
        "analysis",
        "action",
        "workflows",
        "batch",
        "jobs",
        "Oozie",
        "Experience",
        "data",
        "workflow",
        "management",
        "tools",
        "Falcon",
        "Oozie",
        "Autosys",
        "job",
        "scheduling",
        "software",
        "Experience",
        "cloud",
        "computing",
        "platforms",
        "Amazon",
        "Web",
        "ServicesAWS",
        "Experience",
        "applications",
        "Java",
        "technologies",
        "Core",
        "Java",
        "J2EE",
        "Java",
        "Server",
        "Pages",
        "JSP",
        "Servlets",
        "Java",
        "Script",
        "Software",
        "development",
        "methodologies",
        "DevOps",
        "Methodology",
        "Experience",
        "unit",
        "test",
        "plan",
        "unit",
        "test",
        "cases",
        "software",
        "development",
        "JUNIT",
        "technologies",
        "US",
        "employer",
        "Work",
        "Experience",
        "UdemySelfLearning",
        "November",
        "Present",
        "Project",
        "Similar",
        "Movies",
        "Prediction",
        "rating",
        "Apache",
        "Spark",
        "GroupLens",
        "Research",
        "rating",
        "data",
        "sets",
        "MovieLens",
        "website",
        "data",
        "sets",
        "periods",
        "time",
        "size",
        "set",
        "project",
        "ratings",
        "tag",
        "applications",
        "movies",
        "users",
        "Spark",
        "Spark",
        "SQL",
        "Python",
        "Scala",
        "Amazon",
        "EMR",
        "movies",
        "user",
        "ratings",
        "Project",
        "Details",
        "Cloudera",
        "distribution",
        "AWS",
        "EC2",
        "Instances",
        "movie",
        "rating",
        "dataset",
        "data",
        "ETL",
        "pipeline",
        "PySpark",
        "Scala",
        "Spark",
        "SQL",
        "movies",
        "recommendation",
        "Spark",
        "MLlibs",
        "filtering",
        "algorithm",
        "processing",
        "records",
        "Amazon",
        "s3",
        "Square",
        "model",
        "movie",
        "recommendation",
        "data",
        "movieLens",
        "site",
        "Amazon",
        "s3",
        "data",
        "memory",
        "use",
        "Rating",
        "class",
        "file",
        "transformation",
        "RDD",
        "objects",
        "Used",
        "Matrix",
        "Factorization",
        "Model",
        "product",
        "predictions",
        "users",
        "performance",
        "models",
        "recommendations",
        "speed",
        "parallelism",
        "usage",
        "Amazon",
        "EMR",
        "Big",
        "Data",
        "Hadoop",
        "cluster",
        "servers",
        "Amazon",
        "Elastic",
        "Compute",
        "Cloud",
        "EC2",
        "Amazon",
        "Simple",
        "Storage",
        "Service",
        "S3",
        "Good",
        "Understanding",
        "Extracted",
        "time",
        "feed",
        "Spark",
        "streaming",
        "process",
        "data",
        "Data",
        "Frame",
        "data",
        "HDFS",
        "Worked",
        "performance",
        "optimization",
        "algorithms",
        "Hadoop",
        "Spark",
        "Context",
        "SparkSQL",
        "Data",
        "Frame",
        "Pair",
        "Spark",
        "YARN",
        "Environment",
        "AWS",
        "S3",
        "Spark",
        "Spark",
        "SQL",
        "Hive",
        "LINUX",
        "Scala",
        "UNIX",
        "Shell",
        "Scripting",
        "Python",
        "YARN",
        "Cloudera",
        "Hadoop",
        "Developer",
        "Ford",
        "Motor",
        "Company",
        "Dearborn",
        "MI",
        "July",
        "November",
        "Project",
        "Details",
        "Data",
        "Supply",
        "ChainDSC",
        "Data",
        "lake",
        "Hadoop",
        "platform",
        "data",
        "DSC",
        "Ford",
        "information",
        "platform",
        "analytics",
        "demands",
        "data",
        "management",
        "information",
        "access",
        "information",
        "information",
        "DSC",
        "Application",
        "Global",
        "Data",
        "Insight",
        "Analytic",
        "Skill",
        "teamGDIA",
        "mission",
        "decision",
        "insights",
        "business",
        "partners",
        "Responsibilities",
        "Custom",
        "Sqoop",
        "project",
        "Data",
        "Ingestion",
        "Framework",
        "Java",
        "IncrementalFull",
        "load",
        "data",
        "Teradata",
        "source",
        "Hadoop",
        "Landing",
        "Zone",
        "timestamp",
        "partitions",
        "Analyze",
        "requirement",
        "users",
        "Hands",
        "experience",
        "data",
        "databases",
        "file",
        "sources",
        "Oozie",
        "falcon",
        "workflows",
        "job",
        "basis",
        "sources",
        "HDFS",
        "SQOOP",
        "command",
        "line",
        "bash",
        "scripts",
        "Prepared",
        "HBase",
        "entries",
        "Oozie",
        "jobs",
        "Adhoc",
        "Hive",
        "tables",
        "Data",
        "ORC",
        "files",
        "Hive",
        "Partitioning",
        "classification",
        "data",
        "Hive",
        "Tables",
        "data",
        "Hive",
        "queries",
        "Map",
        "Reduce",
        "jobs",
        "authentication",
        "principals",
        "network",
        "communication",
        "cluster",
        "testing",
        "HDFS",
        "Hive",
        "Pig",
        "MapReduce",
        "cluster",
        "users",
        "Autosys",
        "jobs",
        "time",
        "scheduling",
        "SQL",
        "HBase",
        "HBase",
        "tables",
        "sets",
        "data",
        "sources",
        "Attunity",
        "Replicate",
        "web",
        "Interface",
        "tool",
        "data",
        "HDFS",
        "sources",
        "data",
        "masking",
        "character",
        "removal",
        "tasks",
        "data",
        "transformation",
        "SPARK",
        "part",
        "Design",
        "Reviews",
        "Daily",
        "Project",
        "Scrums",
        "sprint",
        "planning",
        "methodology",
        "Maven",
        "jar",
        "files",
        "Accurev",
        "software",
        "configuration",
        "management",
        "tool",
        "Rally",
        "work",
        "tracking",
        "tool",
        "BMC",
        "incident",
        "management",
        "file",
        "formats",
        "Text",
        "Sequence",
        "Avro",
        "ORC",
        "Parquet",
        "Qlikview",
        "development",
        "business",
        "visualization",
        "reports",
        "DSC",
        "Management",
        "Dashboard",
        "SupportTroubleshoot",
        "hive",
        "programs",
        "cluster",
        "issues",
        "duration",
        "testing",
        "Environment",
        "Hortonworks",
        "HDP",
        "HDFS",
        "Hive",
        "Sqoop",
        "Oozie",
        "Falcon",
        "Accurev",
        "LINUX",
        "Hue",
        "HBase",
        "Zookeeper",
        "Java",
        "Maven",
        "Autosys",
        "Mainframe",
        "Teradata",
        "Shell",
        "Qlikview",
        "Rally",
        "Edureka",
        "November",
        "April",
        "Project",
        "Details",
        "Internet",
        "Telephony",
        "call",
        "detail",
        "record",
        "data",
        "record",
        "information",
        "telephone",
        "call",
        "origination",
        "destination",
        "addresses",
        "call",
        "time",
        "call",
        "duration",
        "call",
        "time",
        "day",
        "call",
        "toll",
        "charges",
        "network",
        "charges",
        "operator",
        "services",
        "details",
        "call",
        "project",
        "customers",
        "call",
        "drops",
        "report",
        "telecom",
        "companies",
        "customer",
        "churn",
        "time",
        "partners",
        "connectivity",
        "issues",
        "areas",
        "Responsibilities",
        "Spark",
        "code",
        "Scala",
        "SparkSQL",
        "testing",
        "data",
        "processing",
        "requirement",
        "Installed",
        "Spark",
        "Hive",
        "Pig",
        "Sqoop",
        "Oozie",
        "Hadoop",
        "cluster",
        "millions",
        "data",
        "databases",
        "Sqoop",
        "import",
        "process",
        "Spark",
        "data",
        "HDFS",
        "CSV",
        "format",
        "Spark",
        "SQL",
        "amount",
        "data",
        "traits",
        "case",
        "classes",
        "Scala",
        "business",
        "logic",
        "Scala",
        "Spark",
        "performance",
        "optimization",
        "algorithms",
        "Hadoop",
        "Spark",
        "Context",
        "SparkSQL",
        "Data",
        "Frame",
        "Pair",
        "Spark",
        "YARN",
        "DataFrame",
        "API",
        "Scala",
        "collection",
        "data",
        "columns",
        "datasets",
        "Hive",
        "Table",
        "HiveSQL",
        "queries",
        "Spark",
        "transformations",
        "Spark",
        "RDDs",
        "Scala",
        "spark",
        "Transformations",
        "Actions",
        "input",
        "data",
        "datasets",
        "Partitions",
        "Spark",
        "Memory",
        "capabilities",
        "Broadcasts",
        "Spark",
        "Effective",
        "Joins",
        "Transformations",
        "ingestion",
        "process",
        "sort",
        "join",
        "aggregations",
        "filter",
        "transformations",
        "datasets",
        "Spark",
        "solutions",
        "sets",
        "file",
        "formats",
        "Text",
        "file",
        "Avro",
        "data",
        "Sequence",
        "files",
        "Xml",
        "JSON",
        "files",
        "ORC",
        "Parquet",
        "batch",
        "processing",
        "data",
        "sources",
        "Apache",
        "Spark",
        "Environment",
        "HDFS",
        "YARN",
        "Sqoop",
        "Apache",
        "Spark",
        "SparkSQL",
        "Cloudera",
        "CDH",
        "5X",
        "Sparkshell",
        "Hive",
        "Hue",
        "MYSQL",
        "JavaJ2EE",
        "Tata",
        "Consultancy",
        "Services",
        "February",
        "May",
        "Developer",
        "Project",
        "Details",
        "Lloyds",
        "Banking",
        "Group",
        "institution",
        "acquisition",
        "HBOS",
        "Lloyds",
        "millions",
        "UK",
        "customers",
        "presence",
        "community",
        "business",
        "services",
        "project",
        "Savings",
        "ReEngineering",
        "project",
        "milestone",
        "deliveries",
        "group",
        "releases",
        "methodology",
        "requirements",
        "project",
        "Savings",
        "swimlanes",
        "rethinking",
        "redesign",
        "business",
        "process",
        "improvements",
        "measures",
        "performance",
        "cost",
        "service",
        "speed",
        "Responsibilities",
        "Software",
        "Development",
        "Life",
        "cycle",
        "SDLC",
        "project",
        "Analysis",
        "Design",
        "Programming",
        "Testing",
        "application",
        "Java",
        "Server",
        "JSF",
        "Java",
        "Server",
        "Pages",
        "JSP",
        "UI",
        "pages",
        "Developed",
        "HTML",
        "Java",
        "script",
        "prototypes",
        "Savings",
        "reengineering",
        "screens",
        "web",
        "application",
        "Chordiant",
        "MVC",
        "Framework",
        "Generated",
        "Web",
        "Service",
        "Client",
        "Web",
        "Services",
        "Description",
        "LanguageWSDL",
        "JUNIT",
        "unit",
        "code",
        "changes",
        "testing",
        "application",
        "levels",
        "Unit",
        "Integration",
        "testing",
        "support",
        "SIT",
        "UAT",
        "Rational",
        "Clear",
        "Case",
        "Clear",
        "Quest",
        "source",
        "control",
        "management",
        "unit",
        "code",
        "Review",
        "concerns",
        "Interact",
        "business",
        "people",
        "partners",
        "teams",
        "mails",
        "voice",
        "video",
        "conference",
        "online",
        "communicator",
        "chats",
        "delivery",
        "solution",
        "customer",
        "defects",
        "SIT",
        "phase",
        "build",
        "turnaround",
        "time",
        "Quality",
        "Center",
        "QC",
        "requirements",
        "bug",
        "Environment",
        "Core",
        "Java",
        "Chordiant",
        "MVC",
        "JSFJSP",
        "Servlets",
        "Oracle",
        "SQL",
        "Windows",
        "XPVista",
        "RAD",
        "TOAD",
        "WebSphereApplication",
        "Server",
        "Web",
        "Developer",
        "GEE",
        "Communication",
        "June",
        "January",
        "Project",
        "Details",
        "Gee",
        "communication",
        "part",
        "telecom",
        "company",
        "portal",
        "EMail",
        "campaigns",
        "track",
        "salesforce",
        "activities",
        "help",
        "Customer",
        "Relationship",
        "Management",
        "CRM",
        "Responsibilities",
        "easytouse",
        "user",
        "interface",
        "stepbystep",
        "process",
        "EMail",
        "campaign",
        "Design",
        "software",
        "Java",
        "Server",
        "FacesJSF",
        "framework",
        "Data",
        "JDBC",
        "connection",
        "Java",
        "Beans",
        "business",
        "logic",
        "Model",
        "Servlets",
        "flow",
        "application",
        "Controller",
        "Proficient",
        "Front",
        "End",
        "components",
        "HTML",
        "CSS",
        "JSP",
        "JavaScript",
        "IBMs",
        "WebSphere",
        "Application",
        "Server",
        "code",
        "Code",
        "Reviews",
        "Defect",
        "Fixing",
        "support",
        "Familiarity",
        "aspects",
        "Software",
        "Development",
        "Life",
        "Cycle",
        "code",
        "modifications",
        "projects",
        "business",
        "specifications",
        "application",
        "standards",
        "Perform",
        "unit",
        "system",
        "testing",
        "changes",
        "Environment",
        "HTML",
        "CSS",
        "Servlets",
        "JSF",
        "JSP",
        "JUNIT",
        "Oracle",
        "g",
        "Eclipse",
        "JavaScript",
        "Core",
        "Java",
        "Education",
        "Bachelors",
        "Skills",
        "Apache",
        "hadoop",
        "hdfs",
        "years",
        "Hadoop",
        "years",
        "Hadoop",
        "file",
        "system",
        "years",
        "Java",
        "years",
        "Sql",
        "years",
        "Spark",
        "year",
        "Additional",
        "Information",
        "TECHNICAL",
        "SKILLS",
        "Hadoop",
        "Ecosystem",
        "HDFS",
        "MapReduce",
        "Hive",
        "Sqoop",
        "Oozie",
        "Zookeeper",
        "Spark",
        "Falcon",
        "Kafka",
        "Attunity",
        "Replicate",
        "Pig",
        "Spark",
        "Ecosystem",
        "Spark",
        "Core",
        "Spark",
        "SQL",
        "Spark",
        "MLlib",
        "Databases",
        "HBaseNoSQLMYSQL",
        "Teradata",
        "Programming",
        "Languages",
        "Java",
        "Scala",
        "Hive",
        "QL",
        "Shell",
        "Scripting",
        "Framework",
        "Hadoop",
        "API",
        "JSF",
        "Web",
        "Technologies",
        "HTML",
        "Java",
        "script",
        "CSS",
        "JSP",
        "Servlets",
        "XML",
        "IDEInterfaces",
        "Eclipse",
        "IntelliJ",
        "SparkShell",
        "PySpark",
        "JUNIT",
        "Maven",
        "Methodologies",
        "Agile",
        "DevOps",
        "Operating",
        "Systems",
        "Windows",
        "UNIX",
        "LINUX",
        "Cloud",
        "Platform",
        "AWS",
        "EMR"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T20:14:12.016693",
    "resume_data": "Job Seeker Plano TX Cloudera Certified Hadoop and Spark developer with 4 years of experience in IT industry which includes design development testing and application support of various Web Based applications using Java J2EE Technologies and Big Data Ecosystem Around 2 years of comprehensive experience in Big Data processing and its ecosystem like HDFS Hive Sqoop Oozie Falcon HBase Yarn Zookeeper Attunity Replicate Hue and Spark components Excellent knowledge on Spark Scala Spark SQL Spark Streaming Flume Pig Hadoop MapReduce concepts and NiFi Around 15 years of experience in Software development using Java J2EEJSF and Chordiant Framework and Oracle Database in various areas like Requirements gathering Analysis Design Development Testing Implementation and Maintenance Good handson knowledge in Hortonworks and Cloudera Hadoop Distributions Experience in importing and exporting data using Sqoop from HDFS to Relational Database systems RDBMS and viceversa Expertise in loading different file based sources into Hadoop Landing zone Hands on experience in developing SPARK applications using Spark APIs like Spark transformations and actions RDD Spark MLlib and Spark SQLDataframes using Scala and Python Scheduled job workflow for FTP Sqoop and hive scripts using Oozie coordinators Designed and Developed Data Ingestion Framework using Java to ingest data from various RDBMS and file based sources and housed them in Hive tables from which reports are generated Worked with various file formats such as delimited text files click stream log files Sequence Files Avro files ORC files JSON files Parquet and XML File formats Excellent knowledge of Hadoop Architecture and various components such as HDFS Job Tracker Task Tracker Name Node Data Node and MapReduce Programming paradigm Involved in converting HiveSQL queries into Spark transformations using Spark RDDs Scala and have a good experience in using SparkShell and Spark Streaming Extensive experience in working with structured data using Hive QL join operations Partitioning Bucketing and experienced in optimizing Hive Queries Experience in implementing Kerberos authentication protocol in Hadoop for data security Used Scala SBT to develop Scala coded spark projects and executed using sparksubmit Having experience in analyzing large amounts of data sets to determine optimal way to aggregate and report on it using Qlikview Experienced with code versioning and dependency management systems such as Accurev and Maven Monitoring Job failures and performing the root cause analysis and corrective action Developed workflows for the batch jobs and scheduled using Oozie Experience in data workflow management tools such as Falcon Oozie and Autosys which is job scheduling software Experience with cloud computing platforms like Amazon Web ServicesAWS Experience in developing applications using Java technologies include Core Java J2EE Java Server Pages JSP Servlets and Java Script Well versed in using Software development methodologies like Agile and DevOps Methodology Experience in preparing and executing unit test plan and unit test cases after software development using JUNIT Highly motivated and passionate to learn and explore emerging technologies Authorized to work in the US for any employer Work Experience UdemySelfLearning November 2018 to Present Project Similar Movies Prediction based on rating in Apache Spark GroupLens Research has collected and made available rating data sets from the MovieLens website The data sets were collected over various periods of time depending on the size of the set For my project I took a 20 million ratings and 465000 tag applications applied to 27000 movies by 138000 users and analyzed them using Spark Spark SQL Python Scala on Amazon EMR to find the similar movies to each other based on the user ratings Project Details Worked on Cloudera distribution and deployed on AWS EC2 Instances Analyzed movie rating dataset by building data ETL pipeline using PySpark Scala and Spark SQL Designed movies recommendation using Spark MLlibs collaborative filtering algorithm by parallel processing million of records on Amazon s3 Implemented Alternative Least Square model for customized movie recommendation Migrated the historical data from movieLens site to Amazon s3 and cached the data in memory for repetitive use Used Rating class for parsing the dat file and applied transformation to create RDD objects Used Matrix Factorization Model to make product predictions for users Evaluated the performance of models and got recommendations successfully proven boosted running speed by parallelism Implemented usage of Amazon EMR for processing Big Data across a Hadoop cluster of virtual servers on Amazon Elastic Compute Cloud EC2 and Amazon Simple Storage Service S3 Good Understanding on Extracted Real time feed using Spark streaming and convert it to RDD and process data into Data Frame and load the data into HDFS Worked on improving the performance and optimization of the existing algorithms in Hadoop using Spark Context SparkSQL Data Frame Pair RDDs Spark YARN Environment AWS S3 Spark Spark SQL Hive LINUX Scala UNIX Shell Scripting Python YARN Cloudera Hadoop Developer Ford Motor Company Dearborn MI July 2016 to November 2017 Project Details Data Supply ChainDSC is a Data lake on Hadoop platform to receive consolidate data DSC is the Ford information platform for analytics that balances the demands of data management and information access It helps integrate structured semistructured and unstructured information into single logical information The DSC Application supports the Global Data Insight and Analytic Skill teamGDIA and their mission to drive evidencebased decision making provide timely actionable forwardlooking insights to their business partners Responsibilities Developed a Custom Sqoop project based on Data Ingestion Framework using Java to ingest IncrementalFull load data from Teradata source into Hadoop Landing Zone based on timestamp partitions Analyze and understanding the requirement given by downstream users Hands on experience on extracting data from different databases file based sources and scheduled Oozie falcon workflows to execute this job on daily and monthly basis Responsible for loading various sources to HDFS using SQOOP and command line bash scripts Prepared and executed HBase entries to run Oozie jobs in Adhoc Created Hive tables to load the Data and stored as ORC files for processing Implemented Hive Partitioning and bucketing for further classification of data Involved in creating Hive Tables loading with data and writing Hive queries which will invoke and run Map Reduce jobs in the backend Setting up and worked on Kerberos authentication principals to establish secure network communication on cluster and testing of HDFS Hive Pig and MapReduce to access cluster for new users Autosys scheduler to automate the jobs and time scheduling Worked with No SQL databases like HBase and created HBase tables to load large sets of semi structured data coming from various sources Used Attunity Replicate web based Interface tool to load data efficiently and quickly into HDFS from different sources Performed data masking and special character removal tasks in the data transformation using SPARK Been part of Design Reviews Daily Project Scrums and sprint planning based on Agile methodology Used Maven to build jar files and Accurev as software configuration management tool Used Rally as a work tracking tool and BMC for incident management Worked with different file formats such as Text Sequence files Avro ORC and Parquet Involved in Qlikview development of business analytic and visualization reports for DSC Management Dashboard SupportTroubleshoot hive programs running on the cluster and Involved in fixing issues arising out of duration testing Environment Hortonworks HDP 253 HadoopYARN HDFS Hive Sqoop Oozie Falcon Accurev LINUX Hue HBase Zookeeper Java Maven Autosys Mainframe Teradata Shell scripting Qlikview Rally Edureka November 2015 to April 2016 Project Details In Internet Telephony a call detail record is a data record that contains information related to a telephone call such as the origination and destination addresses of the call the time the call started and ended the duration of the call the time of day the call was made and any toll charges that were added through the network or charges for operator services among other details of the call This project is to find out the customers who are facing frequent call drops in Roaming This is a very important report which telecom companies uses to prevent customer churn out by calling them back and at the same time contacting their roaming partners to improve the connectivity issues in specific areas Responsibilities Developed Spark code using Scala and SparkSQL for faster testing and data processing as per the requirement Installed and configured Spark Hive Pig Sqoop and Oozie on the Hadoop cluster Imported millions of structured data from relational databases using Sqoop import to process using Spark and stored the data into HDFS in CSV format Used Spark SQL to process the huge amount of structured data Developed traits and case classes etc in Scala implemented business logic using Scala Exploring with the Spark for improving the performance and optimization of the existing algorithms in Hadoop using Spark Context SparkSQL Data Frame Pair RDDs Spark YARN Used DataFrame API in Scala for converting the distributed collection of data organized into named columns Registered the datasets as Hive Table Involved in converting HiveSQL queries into Spark transformations using Spark RDDs and Scala Used various spark Transformations and Actions for cleansing the input data Experienced in handling large datasets using Partitions Spark in Memory capabilities Broadcasts in Spark Effective efficient Joins Transformations and other during ingestion process itself Responsible in performing sort join aggregations filter and other transformations on the datasets using Spark Developed solutions to preprocess large sets of structured with different file formats Text file Avro data files Sequence files Xml and JSON files ORC and Parquet Experienced with batch processing of data sources using Apache Spark Environment HDFS YARN Sqoop Apache Spark SparkSQL Cloudera CDH 5X Sparkshell Hive Hue MYSQL JavaJ2EE Tata Consultancy Services February 2013 to May 2014 Developer Project Details Lloyds Banking Group is a major British financial institution formed through the acquisition of HBOS by Lloyds with millions of UK customers with a presence in nearly every community Its business is focused on retail and commercial financial services The project Savings ReEngineering project is one of the big milestone deliveries which went on live in three different group releases working in agile methodology The main requirements of this project were around Savings swimlanes It is the fundamental rethinking and radical redesign of business process to achieve dramatic improvements in critical measures of performance such as cost service and speed Responsibilities Involved in complete Software Development Life cycle SDLC of the project from Analysis Design Programming Testing and Deploying the application Used Java Server Faces JSF and Java Server Pages JSP for developing UI pages Developed HTML Java script prototypes for the Savings reengineering screens Developed web application using Chordiant MVC Framework Generated Web Service Client from a Web Services Description LanguageWSDL Used JUNIT for unit testing the code changes Involved in testing of application on various levels like Unit and Integration testing Provided support for SIT and UAT Used Rational Clear Case Clear Quest for source control and defect management Submitted the unit tested code for Review and rectified the concerns raised Interact with business people SMEs onshore partners and other supporting teams through mails voice calls video conference calls online communicator chats to ensure the delivery of an optimized solution to the customer Fixed the defects in SIT phase immediately and provided build with minimal turnaround time Used Quality Center QC for requirements and bug tracking Environment Core Java Chordiant MVC JSFJSP Servlets Oracle SQL Windows XPVista RAD TOAD WebSphereApplication Server Web Developer GEE Communication IN June 2012 to January 2013 Project Details Gee communication is part of a telecom company where they created a portal that helps to create and send EMail campaigns track leads automate salesforce activities and help in effective Customer Relationship Management CRM Responsibilities Involved in developing easytouse user interface and stepbystep process that helps to create eyecatching EMail campaign Design and developed webbased software using Java Server FacesJSF framework Data has been handled using the JDBC connection Java Beans were used to handle business logic as a Model and Servlets to control the flow of application as Controller Proficient in developing responsive Front End components using HTML CSS JSP tags JavaScript Used IBMs WebSphere Application Server to deploy code Involved in Code Reviews Defect Fixing and UAT support Familiarity with all aspects of Software Development Life Cycle Make code modifications for the assigned projects according to business specifications and application standards Perform unit and system testing for all coding changes Environment HTML CSS Servlets JSF JSP JUNIT Oracle 11g Eclipse JavaScript Core Java Education Bachelors Skills Apache hadoop hdfs 2 years Hadoop 2 years Hadoop distributed file system 2 years Java 3 years Sql 3 years Spark Less than 1 year Additional Information TECHNICAL SKILLS Hadoop Ecosystem HDFS MapReduce Hive Sqoop Oozie Zookeeper Spark Falcon Kafka Attunity Replicate Pig Spark Ecosystem Spark Core Spark SQL Spark MLlib Databases HBaseNoSQLMYSQL Teradata Programming Languages Java Scala Hive QL Shell Scripting Framework Hadoop API JSF Web Technologies HTML Java script CSS JSP Servlets XML IDEInterfaces Eclipse IntelliJ SparkShell PySpark JUNIT Maven Methodologies Agile DevOps Operating Systems Windows UNIX LINUX Cloud Platform AWS EMR",
    "unique_id": "64b7d39b-6640-487f-9f09-dc7ef228cb31"
}