{
    "clean_data": "Data engineer Data engineer Data engineer Verizon Atlanta GA 5 years of experience in various IT sectors which includes handson experience in Big Data technologies 4 years of experience as a Hadoop Developer in all phases of Hadoop and HDFS development Hands on experience with HDFS MapReduce and Hadoop Ecosystem Pig NiFi Hive Oozie Hbase Zookeeper Flume and Sqoop Well versed with developing and implementing MapReduce jobs using Hadoop to work with Big Data Have experience with Spark processing Framework such as Spark and Spark Sql Experience in NoSQL databases like HBase Cassandra and Mongodb Procedural knowledge in cleansing and analyzing data using HiveQL Pig Latin and custom MapReduce programs in Java Experienced in writing custom UDFs and UDAFs for extending Hive and Pig core functionalities Ability to develop Pig UDFS to preprocess the data for analysis Experience in importing and exporting data using Sqoop from HDFS to Relational Database Systems RDBMS Teradata and vice versa Skilled in creating workflows using Oozie for cron jobs Strong experience in Hadoop Administration and Linux Experienced with Java API and REST to access HBase data Worked extensively with Dimensional modeling Data migration Data cleansing Data profiling and ETL Processes features for data warehouses Hands on experience in application development using Java RDBMS and Linux shell scripting Hands on experience in PERL Scripting and Python Experience working with JAVA J2EE JDBC ODBC JSP Java Eclipse MS SQL Server Extensive experience with SQL PLSQL and database concepts Expertise in debugging and optimizing Oracle and java performance tuning with strong knowledge in Oracle 11g and SQL Good experience working with Distributions such as MAPR Horton works and Cloudera Experience in all stages of SDLC Agile Waterfall writing Technical Design document Development Testing and Implementation of Enterprise level Data mart and Data warehouses Having good knowledge on Hadoop Administration like Cluster configuration Single Node Configuration Multi Node Configuration Data Node Commissioning and Decommissioning Name Node Backup and Recovery HBase HDFS and Hive Configuration Monitoring clusters Access control List Good Inter personnel skills and ability to work as part of a team Exceptional ability to learn master new technologies and to deliver outputs in short deadlines Ability to work in highpressure environments delivering to and managing stakeholder expectations Application of structured methods to Project Scoping and Planning risks issues schedules and deliverables Strong analytical and Problem solving skills Good Inter personnel skills and ability to work as part of a team Exceptional ability to learn and master new technologies and to deliver outputs in short deadlines Work Experience Data engineer Verizon GA March 2017 to Present Responsibilities Core person in data ingestion team involved in designing data flow pipelines and Nifi administration Worked on various data sources and data formats to deliver the data with low latency and accuracy Developed python scripts to monitor and automate Nifi flows using Nifi api Expertise in Nifi to work with various ingestion sources and transforming data on the go Setup and maintain nifi registry to versioning and CICD of flows Worked on creation of hive tables managedexternal for various use cases Developed sqoop jobs to importexport data fromto oraclehdfs Responsible for analyzing and cleansing raw data by performing Hive queries and running Pig scripts on data Involved in writing flink jobs to parse near real time data and then push to hive Created Hive tables loaded data and wrote Hive queries that run within the map Implemented business logic by writing Pig UDFs in Java and used various UDFs from Piggybanks and other sources Used OOZIE Operational Services for batch processing and scheduling workflows dynamically Extensively worked on creating EndEnd data pipeline orchestration using Oozie Worked on Data Serialization formats for converting Complex objects into sequence bits by using AVRO PARQUET JSON CSV formats Integrated Druid with Hive for High availability and provide data for sla reporting on real time data Developed a framework to extractload data fromto databases as a substitute for sqoop Automated splunk indexing to report device and topology metrics Environment Map Reduce HDFS Hive Pig SQL Sqoop Oozie Shell scripting Cron Jobs Apache Nifi Splunk Python Apache Flink druid Apache Kafka J2EE Hadoop Developer Bed Bath Beyond Union NJ August 2015 to February 2017 Responsibilities Involved in loading and transforming large sets of structured semi structured and unstructured data from relational databases into HDFS using Sqoop imports Developed Sqoop scripts to import export data from relational sources and handled incremental loading on the customer transaction data by date Developed simple and complex MapReduce programs in Java for Data Analysis on different data formats Developed Spark code using scala and SparkSQLStreaming for faster testing and processing of data Import the data from different sources like HDFSHbase into Spark RDD Experienced with batch processing of data sources using Apache Spark and Elastic search Experienced in implementing Spark RDD transformations actions to implement business analysis Migrated Hive QL queries on structured into Spark QL to improve performance Optimized MapReduce Jobs to use HDFS efficiently by using various compression mechanisms Worked on partitioning HIVE tables and running the scripts in parallel to reduce runtime of the scripts Worked on Data Serialization formats for converting Complex objects into sequence bits by using AVRO PARQUET JSON CSV formats Responsible for analyzing and cleansing raw data by performing HiveImpala queries and running Pig scripts on data Created Hive tables loaded data and wrote Hive queries that run within the map Implemented business logic by writing Hive UDFs in Java Developed Shell scripts and some of Perl scripts based on the user requirement Wrote XML scripts to build OOZIE functionality Used OOZIE Operational Services for batch processing and scheduling workflows dynamically Extensively worked on creating EndEnd data pipeline orchestration using Oozie Evaluated suitability of Hadoop and its ecosystem to the above project and implementing validating with various proof of concept POC applications to eventually adopt them to benefit from the Big Data Hadoop initiative Environment Map Reduce HDFS Hive Pig SQL Sqoop Oozie Shell scripting Cron Jobs Perl scripting Apache Kafka J2EE Java Developer Frost Sullivan Chennai Tamil Nadu May 2012 to June 2013 Responsibilities Used AGILE methodology for developing the application As part of the lifecycle development prepared class model sequence model and flow diagrams by analyzing Use cases using Rational Tools Extensive use of SOA Framework for Controller components and view components Involved in writing the exception and validation classes using Struts validation rules Involved in writing the validation rules classes for general server side validations for implementing validation rules as part observer J2EE design pattern Used OR mapping tool Hibernate for the interaction with database Involved in writing Hibernate queries and Hibernate specific configuration and mapping files Involved in developing JSP pages and custom tag for presentation layer in Spring framework Developed web services using SOAP and WSDL with Apache Axis 2 Developed implemented and maintained an asynchronous AJAX based rich client for improved customer experience using XML data and XSLT templates Developed SQL stored procedures and prepared statements for updating and accessing data from database Development carried out under Eclipse Integrated Development Environment IDE Used JBoss for deploying various components of application Used JUNIT for testing and check API performance Involved in fixing bugs and minor enhancements for the frontend modules Responsible for troubleshooting issues monitoring and guiding team members to deploy and support the product Used SVN Version Control for Project Configuration Management Worked with the Android SDK and implemented Android Bluetooth and Location Connectivity components Worked with business and System Analyst to complete the development in time Implemented the presentation layer with HTML CSS and JavaScript Developed web components using JSP Servlets and JDBC Implemented secured cookies using Servlets Wrote complex SQL queries and stored procedures Implemented Persistent layer using Hibernate API Implemented Transaction and session handling using Hibernate Utils Implemented Search queries using Hibernate Criteria interface Provided support for loans reports for CBT Involved in fixing bugs and unit testing with test cases using Junit Maintained Jasper server on client server and resolved issues Actively involved in system testing Fine tuning SQL queries for maximum efficiency to improve the performance Designed Tables and indexes by following normalizations Involved in Unit testing Integration testing and User Acceptance testing Utilizes Java and SQL day to day to debug and fix issues with client processes Environment Java Servlets JSP Hibernate Junit Testing Oracle DB SQL Education BE in Computer Science BITS Pilani Pilani Rajasthan MS in Computer Science Indiana university Purdue University Skills J2EE 5 years SQL 5 years APACHE HADOOP HDFS 4 years APACHE HADOOP OOZIE 4 years APACHE HADOOP SQOOP 4 years Additional Information Technical Skills Technology Hadoop EcosystemJ2SEJ2EEOracle Operating Systems WindowsVistaXPNT2000Series UNIXLINUX Ubuntu CentOS Redhat AIXSolaris DBMSDatabases DB2 My SQL SQL PLSQL Programming Languages C C JSE XML Spring HTML JavaScript jQuery Web services Big Data Ecosystem HDFS Nifi Map Reduce Oozie HiveImpala Pig Sqoop Flume Zookeeper and Hbase Spark Scala Methodologies Agile Water Fall NOSQL Databases Cassandra MongoDb Hbase Version Control Tools SVN git",
    "entities": [
        "HDFSHbase",
        "AJAX",
        "Spark processing Framework",
        "EndEnd",
        "PERL Scripting and Python Experience",
        "Present Responsibilities Core",
        "Developed Spark",
        "ETL Processes",
        "Data mart",
        "Spark RDD Experienced",
        "Java Servlets JSP Hibernate Junit Testing",
        "User Acceptance",
        "Oozie Worked on Data Serialization",
        "UDAFs",
        "RDD",
        "Hadoop",
        "XML",
        "SOAP",
        "Spark QL",
        "Oracle DB SQL Education BE",
        "HBase",
        "Automated",
        "Hadoop Administration",
        "Apache Spark",
        "Developed Sqoop",
        "Piggybanks",
        "Computer Science BITS Pilani",
        "Developed",
        "HDFS MapReduce and Hadoop Ecosystem Pig NiFi Hive Oozie Hbase Zookeeper Flume",
        "Good Inter",
        "Rational Tools Extensive",
        "SDLC Agile Waterfall",
        "Eclipse Integrated Development Environment IDE",
        "AGILE",
        "Control Tools",
        "the Android SDK",
        "Hadoop Developer",
        "JSP",
        "My SQL SQL PLSQL",
        "Verizon Atlanta",
        "OOZIE Operational Services",
        "SOA Framework",
        "Spark",
        "Created Hive",
        "Implemented Persistent",
        "CSV",
        "HTML CSS",
        "API",
        "Purdue University Skills J2EE",
        "Sqoop",
        "HIVE",
        "CBT Involved",
        "Verizon GA",
        "Problem",
        "Perl",
        "Oracle",
        "Servlets Wrote",
        "Oozie Evaluated",
        "Computer Science Indiana university",
        "java",
        "Hibernate API Implemented Transaction",
        "Project Scoping and Planning",
        "SQL",
        "Relational Database Systems",
        "Worked on Data Serialization",
        "Big Data",
        "Hive",
        "CICD",
        "Java for Data Analysis",
        "OOZIE",
        "XSLT",
        "Cluster",
        "JSP Servlets",
        "Hibernate Criteria",
        "SVN Version Control for Project Configuration Management Worked",
        "Hive Configuration Monitoring",
        "HiveImpala",
        "Nifi",
        "REST",
        "Data",
        "MapReduce",
        "NoSQL",
        "Cron Jobs Apache Nifi Splunk Python Apache Flink",
        "Spark Sql",
        "Technical Design",
        "Hibernate Utils Implemented Search",
        "Integration",
        "Additional Information Technical Skills Technology Hadoop",
        "Node"
    ],
    "experience": "Experience in NoSQL databases like HBase Cassandra and Mongodb Procedural knowledge in cleansing and analyzing data using HiveQL Pig Latin and custom MapReduce programs in Java Experienced in writing custom UDFs and UDAFs for extending Hive and Pig core functionalities Ability to develop Pig UDFS to preprocess the data for analysis Experience in importing and exporting data using Sqoop from HDFS to Relational Database Systems RDBMS Teradata and vice versa Skilled in creating workflows using Oozie for cron jobs Strong experience in Hadoop Administration and Linux Experienced with Java API and REST to access HBase data Worked extensively with Dimensional modeling Data migration Data cleansing Data profiling and ETL Processes features for data warehouses Hands on experience in application development using Java RDBMS and Linux shell scripting Hands on experience in PERL Scripting and Python Experience working with JAVA J2EE JDBC ODBC JSP Java Eclipse MS SQL Server Extensive experience with SQL PLSQL and database concepts Expertise in debugging and optimizing Oracle and java performance tuning with strong knowledge in Oracle 11 g and SQL Good experience working with Distributions such as MAPR Horton works and Cloudera Experience in all stages of SDLC Agile Waterfall writing Technical Design document Development Testing and Implementation of Enterprise level Data mart and Data warehouses Having good knowledge on Hadoop Administration like Cluster configuration Single Node Configuration Multi Node Configuration Data Node Commissioning and Decommissioning Name Node Backup and Recovery HBase HDFS and Hive Configuration Monitoring clusters Access control List Good Inter personnel skills and ability to work as part of a team Exceptional ability to learn master new technologies and to deliver outputs in short deadlines Ability to work in highpressure environments delivering to and managing stakeholder expectations Application of structured methods to Project Scoping and Planning risks issues schedules and deliverables Strong analytical and Problem solving skills Good Inter personnel skills and ability to work as part of a team Exceptional ability to learn and master new technologies and to deliver outputs in short deadlines Work Experience Data engineer Verizon GA March 2017 to Present Responsibilities Core person in data ingestion team involved in designing data flow pipelines and Nifi administration Worked on various data sources and data formats to deliver the data with low latency and accuracy Developed python scripts to monitor and automate Nifi flows using Nifi api Expertise in Nifi to work with various ingestion sources and transforming data on the go Setup and maintain nifi registry to versioning and CICD of flows Worked on creation of hive tables managedexternal for various use cases Developed sqoop jobs to importexport data fromto oraclehdfs Responsible for analyzing and cleansing raw data by performing Hive queries and running Pig scripts on data Involved in writing flink jobs to parse near real time data and then push to hive Created Hive tables loaded data and wrote Hive queries that run within the map Implemented business logic by writing Pig UDFs in Java and used various UDFs from Piggybanks and other sources Used OOZIE Operational Services for batch processing and scheduling workflows dynamically Extensively worked on creating EndEnd data pipeline orchestration using Oozie Worked on Data Serialization formats for converting Complex objects into sequence bits by using AVRO PARQUET JSON CSV formats Integrated Druid with Hive for High availability and provide data for sla reporting on real time data Developed a framework to extractload data fromto databases as a substitute for sqoop Automated splunk indexing to report device and topology metrics Environment Map Reduce HDFS Hive Pig SQL Sqoop Oozie Shell scripting Cron Jobs Apache Nifi Splunk Python Apache Flink druid Apache Kafka J2EE Hadoop Developer Bed Bath Beyond Union NJ August 2015 to February 2017 Responsibilities Involved in loading and transforming large sets of structured semi structured and unstructured data from relational databases into HDFS using Sqoop imports Developed Sqoop scripts to import export data from relational sources and handled incremental loading on the customer transaction data by date Developed simple and complex MapReduce programs in Java for Data Analysis on different data formats Developed Spark code using scala and SparkSQLStreaming for faster testing and processing of data Import the data from different sources like HDFSHbase into Spark RDD Experienced with batch processing of data sources using Apache Spark and Elastic search Experienced in implementing Spark RDD transformations actions to implement business analysis Migrated Hive QL queries on structured into Spark QL to improve performance Optimized MapReduce Jobs to use HDFS efficiently by using various compression mechanisms Worked on partitioning HIVE tables and running the scripts in parallel to reduce runtime of the scripts Worked on Data Serialization formats for converting Complex objects into sequence bits by using AVRO PARQUET JSON CSV formats Responsible for analyzing and cleansing raw data by performing HiveImpala queries and running Pig scripts on data Created Hive tables loaded data and wrote Hive queries that run within the map Implemented business logic by writing Hive UDFs in Java Developed Shell scripts and some of Perl scripts based on the user requirement Wrote XML scripts to build OOZIE functionality Used OOZIE Operational Services for batch processing and scheduling workflows dynamically Extensively worked on creating EndEnd data pipeline orchestration using Oozie Evaluated suitability of Hadoop and its ecosystem to the above project and implementing validating with various proof of concept POC applications to eventually adopt them to benefit from the Big Data Hadoop initiative Environment Map Reduce HDFS Hive Pig SQL Sqoop Oozie Shell scripting Cron Jobs Perl scripting Apache Kafka J2EE Java Developer Frost Sullivan Chennai Tamil Nadu May 2012 to June 2013 Responsibilities Used AGILE methodology for developing the application As part of the lifecycle development prepared class model sequence model and flow diagrams by analyzing Use cases using Rational Tools Extensive use of SOA Framework for Controller components and view components Involved in writing the exception and validation classes using Struts validation rules Involved in writing the validation rules classes for general server side validations for implementing validation rules as part observer J2EE design pattern Used OR mapping tool Hibernate for the interaction with database Involved in writing Hibernate queries and Hibernate specific configuration and mapping files Involved in developing JSP pages and custom tag for presentation layer in Spring framework Developed web services using SOAP and WSDL with Apache Axis 2 Developed implemented and maintained an asynchronous AJAX based rich client for improved customer experience using XML data and XSLT templates Developed SQL stored procedures and prepared statements for updating and accessing data from database Development carried out under Eclipse Integrated Development Environment IDE Used JBoss for deploying various components of application Used JUNIT for testing and check API performance Involved in fixing bugs and minor enhancements for the frontend modules Responsible for troubleshooting issues monitoring and guiding team members to deploy and support the product Used SVN Version Control for Project Configuration Management Worked with the Android SDK and implemented Android Bluetooth and Location Connectivity components Worked with business and System Analyst to complete the development in time Implemented the presentation layer with HTML CSS and JavaScript Developed web components using JSP Servlets and JDBC Implemented secured cookies using Servlets Wrote complex SQL queries and stored procedures Implemented Persistent layer using Hibernate API Implemented Transaction and session handling using Hibernate Utils Implemented Search queries using Hibernate Criteria interface Provided support for loans reports for CBT Involved in fixing bugs and unit testing with test cases using Junit Maintained Jasper server on client server and resolved issues Actively involved in system testing Fine tuning SQL queries for maximum efficiency to improve the performance Designed Tables and indexes by following normalizations Involved in Unit testing Integration testing and User Acceptance testing Utilizes Java and SQL day to day to debug and fix issues with client processes Environment Java Servlets JSP Hibernate Junit Testing Oracle DB SQL Education BE in Computer Science BITS Pilani Pilani Rajasthan MS in Computer Science Indiana university Purdue University Skills J2EE 5 years SQL 5 years APACHE HADOOP HDFS 4 years APACHE HADOOP OOZIE 4 years APACHE HADOOP SQOOP 4 years Additional Information Technical Skills Technology Hadoop EcosystemJ2SEJ2EEOracle Operating Systems WindowsVistaXPNT2000Series UNIXLINUX Ubuntu CentOS Redhat AIXSolaris DBMSDatabases DB2 My SQL SQL PLSQL Programming Languages C C JSE XML Spring HTML JavaScript jQuery Web services Big Data Ecosystem HDFS Nifi Map Reduce Oozie HiveImpala Pig Sqoop Flume Zookeeper and Hbase Spark Scala Methodologies Agile Water Fall NOSQL Databases Cassandra MongoDb Hbase Version Control Tools SVN git",
    "extracted_keywords": [
        "Data",
        "engineer",
        "Data",
        "engineer",
        "Data",
        "engineer",
        "Verizon",
        "Atlanta",
        "GA",
        "years",
        "experience",
        "IT",
        "sectors",
        "handson",
        "experience",
        "Big",
        "Data",
        "technologies",
        "years",
        "experience",
        "Hadoop",
        "Developer",
        "phases",
        "Hadoop",
        "HDFS",
        "development",
        "Hands",
        "experience",
        "HDFS",
        "MapReduce",
        "Hadoop",
        "Ecosystem",
        "Pig",
        "NiFi",
        "Hive",
        "Oozie",
        "Hbase",
        "Zookeeper",
        "Flume",
        "Sqoop",
        "Well",
        "MapReduce",
        "jobs",
        "Hadoop",
        "Big",
        "Data",
        "experience",
        "Spark",
        "processing",
        "Framework",
        "Spark",
        "Spark",
        "Sql",
        "Experience",
        "NoSQL",
        "HBase",
        "Cassandra",
        "Mongodb",
        "Procedural",
        "knowledge",
        "cleansing",
        "data",
        "HiveQL",
        "Pig",
        "Latin",
        "MapReduce",
        "programs",
        "Java",
        "custom",
        "UDFs",
        "UDAFs",
        "Hive",
        "Pig",
        "core",
        "functionalities",
        "Ability",
        "Pig",
        "UDFS",
        "data",
        "analysis",
        "Experience",
        "data",
        "Sqoop",
        "HDFS",
        "Relational",
        "Database",
        "Systems",
        "RDBMS",
        "Teradata",
        "vice",
        "workflows",
        "Oozie",
        "cron",
        "jobs",
        "experience",
        "Hadoop",
        "Administration",
        "Linux",
        "Java",
        "API",
        "REST",
        "access",
        "HBase",
        "data",
        "modeling",
        "Data",
        "migration",
        "Data",
        "Data",
        "profiling",
        "ETL",
        "Processes",
        "features",
        "data",
        "Hands",
        "experience",
        "application",
        "development",
        "Java",
        "RDBMS",
        "Linux",
        "shell",
        "Hands",
        "experience",
        "PERL",
        "Scripting",
        "Python",
        "Experience",
        "JAVA",
        "J2EE",
        "JDBC",
        "ODBC",
        "JSP",
        "Java",
        "Eclipse",
        "MS",
        "SQL",
        "Server",
        "experience",
        "SQL",
        "PLSQL",
        "database",
        "concepts",
        "Expertise",
        "Oracle",
        "performance",
        "knowledge",
        "Oracle",
        "g",
        "SQL",
        "Good",
        "experience",
        "Distributions",
        "MAPR",
        "Horton",
        "Cloudera",
        "Experience",
        "stages",
        "SDLC",
        "Agile",
        "Waterfall",
        "Technical",
        "Design",
        "document",
        "Development",
        "Testing",
        "Implementation",
        "Enterprise",
        "level",
        "Data",
        "mart",
        "Data",
        "knowledge",
        "Hadoop",
        "Administration",
        "Cluster",
        "configuration",
        "Single",
        "Node",
        "Configuration",
        "Multi",
        "Node",
        "Configuration",
        "Data",
        "Node",
        "Commissioning",
        "Decommissioning",
        "Name",
        "Node",
        "Backup",
        "Recovery",
        "HBase",
        "HDFS",
        "Hive",
        "Configuration",
        "Monitoring",
        "Access",
        "control",
        "List",
        "Good",
        "Inter",
        "personnel",
        "skills",
        "ability",
        "part",
        "team",
        "ability",
        "master",
        "technologies",
        "outputs",
        "deadlines",
        "Ability",
        "environments",
        "stakeholder",
        "expectations",
        "Application",
        "methods",
        "Project",
        "Scoping",
        "Planning",
        "risks",
        "schedules",
        "Problem",
        "skills",
        "Good",
        "Inter",
        "personnel",
        "skills",
        "ability",
        "part",
        "team",
        "ability",
        "technologies",
        "outputs",
        "deadlines",
        "Work",
        "Experience",
        "Data",
        "engineer",
        "Verizon",
        "GA",
        "March",
        "Present",
        "Responsibilities",
        "Core",
        "person",
        "data",
        "ingestion",
        "team",
        "data",
        "flow",
        "pipelines",
        "Nifi",
        "administration",
        "data",
        "sources",
        "data",
        "formats",
        "data",
        "latency",
        "accuracy",
        "Developed",
        "python",
        "scripts",
        "Nifi",
        "Nifi",
        "api",
        "Expertise",
        "Nifi",
        "ingestion",
        "sources",
        "data",
        "go",
        "Setup",
        "registry",
        "CICD",
        "flows",
        "creation",
        "tables",
        "use",
        "cases",
        "sqoop",
        "jobs",
        "data",
        "fromto",
        "data",
        "Hive",
        "queries",
        "Pig",
        "scripts",
        "data",
        "flink",
        "jobs",
        "time",
        "data",
        "Hive",
        "tables",
        "data",
        "Hive",
        "queries",
        "map",
        "business",
        "logic",
        "Pig",
        "UDFs",
        "Java",
        "UDFs",
        "Piggybanks",
        "sources",
        "OOZIE",
        "Operational",
        "Services",
        "batch",
        "processing",
        "scheduling",
        "workflows",
        "EndEnd",
        "data",
        "pipeline",
        "orchestration",
        "Oozie",
        "Worked",
        "Data",
        "Serialization",
        "formats",
        "objects",
        "sequence",
        "bits",
        "AVRO",
        "PARQUET",
        "JSON",
        "CSV",
        "Integrated",
        "Druid",
        "Hive",
        "availability",
        "data",
        "sla",
        "time",
        "data",
        "framework",
        "extractload",
        "data",
        "fromto",
        "substitute",
        "sqoop",
        "splunk",
        "indexing",
        "device",
        "topology",
        "metrics",
        "Environment",
        "Map",
        "HDFS",
        "Hive",
        "Pig",
        "SQL",
        "Sqoop",
        "Oozie",
        "Shell",
        "Cron",
        "Jobs",
        "Apache",
        "Nifi",
        "Splunk",
        "Python",
        "Apache",
        "Flink",
        "druid",
        "Apache",
        "Kafka",
        "J2EE",
        "Hadoop",
        "Developer",
        "Bed",
        "Bath",
        "Union",
        "NJ",
        "August",
        "February",
        "Responsibilities",
        "loading",
        "sets",
        "data",
        "databases",
        "HDFS",
        "Sqoop",
        "imports",
        "Developed",
        "Sqoop",
        "scripts",
        "export",
        "data",
        "sources",
        "loading",
        "customer",
        "transaction",
        "data",
        "date",
        "MapReduce",
        "programs",
        "Java",
        "Data",
        "Analysis",
        "data",
        "formats",
        "Spark",
        "code",
        "scala",
        "SparkSQLStreaming",
        "testing",
        "processing",
        "data",
        "Import",
        "data",
        "sources",
        "HDFSHbase",
        "Spark",
        "RDD",
        "batch",
        "processing",
        "data",
        "sources",
        "Apache",
        "Spark",
        "search",
        "Spark",
        "RDD",
        "transformations",
        "actions",
        "business",
        "analysis",
        "Hive",
        "QL",
        "Spark",
        "QL",
        "performance",
        "MapReduce",
        "Jobs",
        "HDFS",
        "compression",
        "mechanisms",
        "HIVE",
        "tables",
        "scripts",
        "parallel",
        "runtime",
        "scripts",
        "Data",
        "Serialization",
        "formats",
        "objects",
        "sequence",
        "bits",
        "AVRO",
        "PARQUET",
        "JSON",
        "CSV",
        "formats",
        "data",
        "HiveImpala",
        "queries",
        "Pig",
        "scripts",
        "data",
        "Hive",
        "tables",
        "data",
        "Hive",
        "queries",
        "map",
        "business",
        "logic",
        "Hive",
        "UDFs",
        "Java",
        "Developed",
        "Shell",
        "scripts",
        "Perl",
        "scripts",
        "user",
        "requirement",
        "Wrote",
        "XML",
        "scripts",
        "OOZIE",
        "functionality",
        "OOZIE",
        "Operational",
        "Services",
        "batch",
        "processing",
        "scheduling",
        "workflows",
        "EndEnd",
        "data",
        "pipeline",
        "orchestration",
        "Oozie",
        "suitability",
        "Hadoop",
        "ecosystem",
        "project",
        "proof",
        "concept",
        "POC",
        "applications",
        "Big",
        "Data",
        "Hadoop",
        "initiative",
        "Environment",
        "Map",
        "HDFS",
        "Hive",
        "Pig",
        "SQL",
        "Sqoop",
        "Oozie",
        "Shell",
        "Cron",
        "Jobs",
        "Perl",
        "Apache",
        "Kafka",
        "J2EE",
        "Java",
        "Developer",
        "Frost",
        "Sullivan",
        "Chennai",
        "Tamil",
        "Nadu",
        "May",
        "June",
        "Responsibilities",
        "methodology",
        "application",
        "part",
        "lifecycle",
        "development",
        "class",
        "model",
        "sequence",
        "model",
        "flow",
        "diagrams",
        "Use",
        "cases",
        "Rational",
        "Tools",
        "use",
        "SOA",
        "Framework",
        "Controller",
        "components",
        "components",
        "exception",
        "validation",
        "classes",
        "Struts",
        "validation",
        "rules",
        "validation",
        "rules",
        "classes",
        "server",
        "side",
        "validations",
        "validation",
        "rules",
        "part",
        "observer",
        "J2EE",
        "design",
        "pattern",
        "mapping",
        "tool",
        "Hibernate",
        "interaction",
        "database",
        "Hibernate",
        "queries",
        "configuration",
        "mapping",
        "files",
        "JSP",
        "pages",
        "custom",
        "tag",
        "presentation",
        "layer",
        "Spring",
        "framework",
        "web",
        "services",
        "SOAP",
        "WSDL",
        "Apache",
        "Axis",
        "AJAX",
        "client",
        "customer",
        "experience",
        "XML",
        "data",
        "XSLT",
        "Developed",
        "SQL",
        "procedures",
        "statements",
        "data",
        "database",
        "Development",
        "Eclipse",
        "Integrated",
        "Development",
        "Environment",
        "IDE",
        "JBoss",
        "components",
        "application",
        "JUNIT",
        "testing",
        "API",
        "performance",
        "bugs",
        "enhancements",
        "frontend",
        "modules",
        "troubleshooting",
        "issues",
        "team",
        "members",
        "product",
        "SVN",
        "Version",
        "Control",
        "Project",
        "Configuration",
        "Management",
        "Android",
        "SDK",
        "Android",
        "Bluetooth",
        "Location",
        "Connectivity",
        "components",
        "business",
        "System",
        "Analyst",
        "development",
        "time",
        "presentation",
        "layer",
        "HTML",
        "CSS",
        "JavaScript",
        "Developed",
        "web",
        "components",
        "JSP",
        "Servlets",
        "JDBC",
        "cookies",
        "Servlets",
        "Wrote",
        "SQL",
        "queries",
        "procedures",
        "layer",
        "Hibernate",
        "API",
        "Transaction",
        "session",
        "handling",
        "Hibernate",
        "Utils",
        "Search",
        "queries",
        "Hibernate",
        "Criteria",
        "interface",
        "support",
        "loans",
        "reports",
        "CBT",
        "bugs",
        "unit",
        "testing",
        "test",
        "cases",
        "Junit",
        "Jasper",
        "server",
        "client",
        "server",
        "issues",
        "system",
        "tuning",
        "SQL",
        "efficiency",
        "performance",
        "Tables",
        "indexes",
        "normalizations",
        "Unit",
        "testing",
        "Integration",
        "testing",
        "User",
        "Acceptance",
        "testing",
        "Utilizes",
        "Java",
        "SQL",
        "day",
        "day",
        "issues",
        "client",
        "processes",
        "Environment",
        "Java",
        "Servlets",
        "JSP",
        "Hibernate",
        "Junit",
        "Testing",
        "Oracle",
        "DB",
        "SQL",
        "Education",
        "Computer",
        "Science",
        "BITS",
        "Pilani",
        "Pilani",
        "Rajasthan",
        "MS",
        "Computer",
        "Science",
        "Indiana",
        "university",
        "Purdue",
        "University",
        "Skills",
        "J2EE",
        "years",
        "SQL",
        "years",
        "APACHE",
        "HADOOP",
        "HDFS",
        "years",
        "APACHE",
        "HADOOP",
        "OOZIE",
        "years",
        "APACHE",
        "HADOOP",
        "years",
        "Additional",
        "Information",
        "Technical",
        "Skills",
        "Technology",
        "Hadoop",
        "EcosystemJ2SEJ2EEOracle",
        "Operating",
        "Systems",
        "WindowsVistaXPNT2000Series",
        "UNIXLINUX",
        "Ubuntu",
        "CentOS",
        "Redhat",
        "AIXSolaris",
        "DBMSDatabases",
        "DB2",
        "SQL",
        "SQL",
        "PLSQL",
        "Programming",
        "Languages",
        "C",
        "C",
        "JSE",
        "XML",
        "Spring",
        "HTML",
        "JavaScript",
        "jQuery",
        "Web",
        "services",
        "Big",
        "Data",
        "Ecosystem",
        "HDFS",
        "Nifi",
        "Map",
        "Reduce",
        "Oozie",
        "HiveImpala",
        "Pig",
        "Sqoop",
        "Flume",
        "Zookeeper",
        "Hbase",
        "Spark",
        "Scala",
        "Methodologies",
        "Agile",
        "Water",
        "Fall",
        "NOSQL",
        "Cassandra",
        "MongoDb",
        "Hbase",
        "Version",
        "Control",
        "Tools",
        "SVN",
        "git"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T22:11:14.078928",
    "resume_data": "Data engineer Data engineer Data engineer Verizon Atlanta GA 5 years of experience in various IT sectors which includes handson experience in Big Data technologies 4 years of experience as a Hadoop Developer in all phases of Hadoop and HDFS development Hands on experience with HDFS MapReduce and Hadoop Ecosystem Pig NiFi Hive Oozie Hbase Zookeeper Flume and Sqoop Well versed with developing and implementing MapReduce jobs using Hadoop to work with Big Data Have experience with Spark processing Framework such as Spark and Spark Sql Experience in NoSQL databases like HBase Cassandra and Mongodb Procedural knowledge in cleansing and analyzing data using HiveQL Pig Latin and custom MapReduce programs in Java Experienced in writing custom UDFs and UDAFs for extending Hive and Pig core functionalities Ability to develop Pig UDFS to preprocess the data for analysis Experience in importing and exporting data using Sqoop from HDFS to Relational Database Systems RDBMS Teradata and vice versa Skilled in creating workflows using Oozie for cron jobs Strong experience in Hadoop Administration and Linux Experienced with Java API and REST to access HBase data Worked extensively with Dimensional modeling Data migration Data cleansing Data profiling and ETL Processes features for data warehouses Hands on experience in application development using Java RDBMS and Linux shell scripting Hands on experience in PERL Scripting and Python Experience working with JAVA J2EE JDBC ODBC JSP Java Eclipse MS SQL Server Extensive experience with SQL PLSQL and database concepts Expertise in debugging and optimizing Oracle and java performance tuning with strong knowledge in Oracle 11g and SQL Good experience working with Distributions such as MAPR Horton works and Cloudera Experience in all stages of SDLC Agile Waterfall writing Technical Design document Development Testing and Implementation of Enterprise level Data mart and Data warehouses Having good knowledge on Hadoop Administration like Cluster configuration Single Node Configuration Multi Node Configuration Data Node Commissioning and Decommissioning Name Node Backup and Recovery HBase HDFS and Hive Configuration Monitoring clusters Access control List Good Inter personnel skills and ability to work as part of a team Exceptional ability to learn master new technologies and to deliver outputs in short deadlines Ability to work in highpressure environments delivering to and managing stakeholder expectations Application of structured methods to Project Scoping and Planning risks issues schedules and deliverables Strong analytical and Problem solving skills Good Inter personnel skills and ability to work as part of a team Exceptional ability to learn and master new technologies and to deliver outputs in short deadlines Work Experience Data engineer Verizon GA March 2017 to Present Responsibilities Core person in data ingestion team involved in designing data flow pipelines and Nifi administration Worked on various data sources and data formats to deliver the data with low latency and accuracy Developed python scripts to monitor and automate Nifi flows using Nifi api Expertise in Nifi to work with various ingestion sources and transforming data on the go Setup and maintain nifi registry to versioning and CICD of flows Worked on creation of hive tables managedexternal for various use cases Developed sqoop jobs to importexport data fromto oraclehdfs Responsible for analyzing and cleansing raw data by performing Hive queries and running Pig scripts on data Involved in writing flink jobs to parse near real time data and then push to hive Created Hive tables loaded data and wrote Hive queries that run within the map Implemented business logic by writing Pig UDFs in Java and used various UDFs from Piggybanks and other sources Used OOZIE Operational Services for batch processing and scheduling workflows dynamically Extensively worked on creating EndEnd data pipeline orchestration using Oozie Worked on Data Serialization formats for converting Complex objects into sequence bits by using AVRO PARQUET JSON CSV formats Integrated Druid with Hive for High availability and provide data for sla reporting on real time data Developed a framework to extractload data fromto databases as a substitute for sqoop Automated splunk indexing to report device and topology metrics Environment Map Reduce HDFS Hive Pig SQL Sqoop Oozie Shell scripting Cron Jobs Apache Nifi Splunk Python Apache Flink druid Apache Kafka J2EE Hadoop Developer Bed Bath Beyond Union NJ August 2015 to February 2017 Responsibilities Involved in loading and transforming large sets of structured semi structured and unstructured data from relational databases into HDFS using Sqoop imports Developed Sqoop scripts to import export data from relational sources and handled incremental loading on the customer transaction data by date Developed simple and complex MapReduce programs in Java for Data Analysis on different data formats Developed Spark code using scala and SparkSQLStreaming for faster testing and processing of data Import the data from different sources like HDFSHbase into Spark RDD Experienced with batch processing of data sources using Apache Spark and Elastic search Experienced in implementing Spark RDD transformations actions to implement business analysis Migrated Hive QL queries on structured into Spark QL to improve performance Optimized MapReduce Jobs to use HDFS efficiently by using various compression mechanisms Worked on partitioning HIVE tables and running the scripts in parallel to reduce runtime of the scripts Worked on Data Serialization formats for converting Complex objects into sequence bits by using AVRO PARQUET JSON CSV formats Responsible for analyzing and cleansing raw data by performing HiveImpala queries and running Pig scripts on data Created Hive tables loaded data and wrote Hive queries that run within the map Implemented business logic by writing Hive UDFs in Java Developed Shell scripts and some of Perl scripts based on the user requirement Wrote XML scripts to build OOZIE functionality Used OOZIE Operational Services for batch processing and scheduling workflows dynamically Extensively worked on creating EndEnd data pipeline orchestration using Oozie Evaluated suitability of Hadoop and its ecosystem to the above project and implementing validating with various proof of concept POC applications to eventually adopt them to benefit from the Big Data Hadoop initiative Environment Map Reduce HDFS Hive Pig SQL Sqoop Oozie Shell scripting Cron Jobs Perl scripting Apache Kafka J2EE Java Developer Frost Sullivan Chennai Tamil Nadu May 2012 to June 2013 Responsibilities Used AGILE methodology for developing the application As part of the lifecycle development prepared class model sequence model and flow diagrams by analyzing Use cases using Rational Tools Extensive use of SOA Framework for Controller components and view components Involved in writing the exception and validation classes using Struts validation rules Involved in writing the validation rules classes for general server side validations for implementing validation rules as part observer J2EE design pattern Used OR mapping tool Hibernate for the interaction with database Involved in writing Hibernate queries and Hibernate specific configuration and mapping files Involved in developing JSP pages and custom tag for presentation layer in Spring framework Developed web services using SOAP and WSDL with Apache Axis 2 Developed implemented and maintained an asynchronous AJAX based rich client for improved customer experience using XML data and XSLT templates Developed SQL stored procedures and prepared statements for updating and accessing data from database Development carried out under Eclipse Integrated Development Environment IDE Used JBoss for deploying various components of application Used JUNIT for testing and check API performance Involved in fixing bugs and minor enhancements for the frontend modules Responsible for troubleshooting issues monitoring and guiding team members to deploy and support the product Used SVN Version Control for Project Configuration Management Worked with the Android SDK and implemented Android Bluetooth and Location Connectivity components Worked with business and System Analyst to complete the development in time Implemented the presentation layer with HTML CSS and JavaScript Developed web components using JSP Servlets and JDBC Implemented secured cookies using Servlets Wrote complex SQL queries and stored procedures Implemented Persistent layer using Hibernate API Implemented Transaction and session handling using Hibernate Utils Implemented Search queries using Hibernate Criteria interface Provided support for loans reports for CBT Involved in fixing bugs and unit testing with test cases using Junit Maintained Jasper server on client server and resolved issues Actively involved in system testing Fine tuning SQL queries for maximum efficiency to improve the performance Designed Tables and indexes by following normalizations Involved in Unit testing Integration testing and User Acceptance testing Utilizes Java and SQL day to day to debug and fix issues with client processes Environment Java Servlets JSP Hibernate Junit Testing Oracle DB SQL Education BE in Computer Science BITS Pilani Pilani Rajasthan MS in Computer Science Indiana university Purdue University Skills J2EE 5 years SQL 5 years APACHE HADOOP HDFS 4 years APACHE HADOOP OOZIE 4 years APACHE HADOOP SQOOP 4 years Additional Information Technical Skills Technology Hadoop EcosystemJ2SEJ2EEOracle Operating Systems WindowsVistaXPNT2000Series UNIXLINUX Ubuntu CentOS Redhat AIXSolaris DBMSDatabases DB2 My SQL SQL PLSQL Programming Languages C C JSE XML Spring HTML JavaScript jQuery Web services Big Data Ecosystem HDFS Nifi Map Reduce Oozie HiveImpala Pig Sqoop Flume Zookeeper and Hbase Spark Scala Methodologies Agile Water Fall NOSQL Databases Cassandra MongoDb Hbase Version Control Tools SVN git",
    "unique_id": "d96e0308-c0a6-410a-9e22-ff1dd9443f1c"
}