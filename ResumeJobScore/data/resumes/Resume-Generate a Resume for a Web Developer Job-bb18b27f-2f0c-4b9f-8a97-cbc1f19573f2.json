{
    "clean_data": "Sr Hadoop Spark Developer Sr Hadoop Spark span lDeveloperspan Sr Hadoop Spark Developer American Express Palo Alto CA Over 6 years of professional IT experience which includes experience in Big data ecosystem experience in complete project life cycle design development unit testing and implementation of which over 3 years of work experience in ingestion storage querying processing and analysis of Big Data with hands on experience in Hadoop Ecosystem YARN HDFS and its components Hive Pig HBase Sqoop Hue Kafka Flume Oozie Zookeeper Spark Spark SQL and Spark Streaming Hands on experience in improving the performance and optimization of the existing algorithms in Hadoop using Spark context SparkSQL Data Frame pair RDDs Spark YARN and experience in application in Apache Spark using python Spark Working experience on building spark applications using build tools like SBT Maven and Gradle Good experience in dealing with different file formats like text Sequence RCFILE ORC Parquet Avro and JSON and different compression formats like GZip LZO BZip2 and snappy Good knowledge on relational databases like MySQL Oracle and NoSQL databases like HBase MongoDB Working experience in handling semiunstructured data from different data sources Working experience in developing Map side join Reducer side join Distributed Cache Compression techniques Multiple Input output Good working experience in performing adhoc analysis on structured data using HiveQL joins and Hive Generic UDFs good exposure to Counters Shuffle Sort parameters Dynamic Partitions Bucketing for performance improvement Good knowledge in using IDE like Net Beans Eclipse Intellij Authorized to work in the US for any employer Work Experience Sr Hadoop Spark Developer American Express Phoenix AZ January 2018 to Present Responsibilities Designed the solution using Storm Spouts to stream data from Kafka and Bolts connecting to Java APIs developed independently based on the application logic Imported bulk data into HBase Using Map Reduce programs Written Storm topology to accept the events from Kafka producer and emit into HBase Developed a data pipeline using Kafka and Strom to store data into HDFS Developed HDFS with huge amounts of data using Apache Kafka Implemented a proof of concept Pocs using Kafka Strom HBase for processing streaming data Created HBase tables to load large sets of structured semistructured and unstructured data coming from UNIX NoSQL and a variety of portfolios Development of software using core java with integration of Apache Storm Apache Kafka Integrated Oozie with the rest of the Hadoop stack supporting several types ofHadoop jobs out of the box such as MapReduce Pig Hive and Sqoop as well as system specific jobs such as Java programs and shell scripts Exploring with Spark improving the performance and optimization of the existing algorithms in Hadoop using Spark context SparkSQL Data Frame pair RDDs Spark YARN Developed Spark code and SparkSQLStreaming for faster testing and processing of data Experience in deploying data from various sources into HDFS and building reports using Tableau Performed real time analysis on the incoming data Load the data into Spark RDD and performed inmemory data computation to generate the output response Developed Spark scripts by using Python shell commands as per the requirement Developed Shell scripts and Python programs to automate tasks Environment Hadoop Map Reduce HDFS Spark Java Kafka Hive HBase maven Jenkins Pig UNIX Python Git Storm MapR Oozie HadoopSpark Developer East West Bank Palo Alto CA October 2016 to June 2017 Responsibilities Worked with lambda architecture in handling and processing batch and realtime data Using Sqoop ingested the Data from data warehouse to HDFS Using Kafka collected realtime streaming and log data from web applications and click stream data analyzing a part of data using spark streaming and rest stored into HDFS for future use Worked in creating External and Managed Hive tables Worked in writing Hive Queries for analyzing data in Hive warehouse using Hive Query Language HiveQL and worked with Hive Tables Hive queries Partitioning Bucketing Performed Data Profiling identify data quality and validating rules regarding data integrity and data quality as it relates to the impact on business requirements Build spark applications using SBT builds Used Spark SQL to process the huge amount of structured data Worked on Python plugin on MySQL workbench to upload CSV files Migrated python scikit learn machine learning to data framebased spark machine learning algorithms Created Rich dashboards using Tableau Dashboard Connected Tableau server to publish dashboard to a central location for portal integration Creation of metrics attributes filters reports and dashboards created advanced chart types visualizations and complex calculations to manipulate the data Environment Cloudera Manager Sqoop Java jdk18 Version Hive Spark SparkSQL Scala Hadoop Developer United Airlines Chicago IL May 2015 to September 2016 Responsibilities Worked using Apache Hadoop ecosystem components like HDFS Hive Sqoop and Worked with Spark Scala and Python Involved in data extraction from distributed RDBMS like Teradata and Oracle Involved in loading data from UNIX file system to HDFS Used Map Reduce JUnit for unit testing Good experience in handling data manipulation using python Scripts Developed data pipeline using Flume Sqoop Pig and Java map reduce and Spark to ingest customer behavioral data and purchase histories into HDFS for analysis Troubleshooting the cluster by reviewing Hadoop LOG files Involved in managing and reviewing Hadoop log files Installed and configured Pig for ETL jobs Experienced in migrating iterative map reduce programs into Spark transformations using Scala Import the data from different sources like HDFSHbase into Spark RDD Used Oozie to manage the Hadoop jobs Involved in running Hadoop streaming jobs to process terabytes of text data Load and transform large sets of structured semi structured and unstructured data Imported data using Sqoop from Teradata using Teradata connector Implemented Partitioning Dynamic Partitioning and Bucketing in HIVE Exported the result set from HIVE to MySQL using Shell scripts Used Zookeeper for various types of centralized configurations Involved in maintaining various Unix Shell scripts Implemented Fair schedulers on the Job tracker to share the resources of the Cluster for the Map Reduce jobs given by the users Automated all the jobs starting from pulling the Data from different Data Sources like MySQL to pushing the result set Data to Hadoop Distributed File System using Sqoop Used GIT for version control Maintain System integrity of all subcomponents primarily HDFS MR HBase and Flume Monitor System health and logs and respond accordingly to any warning or failure conditions Environment Hadoop HDFS Map Reduce Hive Pig Sqoop Hue Impala NoSQL Java 16 Mongo and Spark Sr Java Developer Fresenius Medical Care Waltham MA March 2014 to April 2015 Roles and Responsibilities Involved in various phases of Software Development such as modeling system analysis and design code generation and testing using AGILE Methodology Participated in daily Stand up meetings with Scrum Master Responsible for estimating the time for given task Service Request Designed developed and deployed application using Eclipse and Tomcat application Server Developed SQL queries and used JDBC to interact with the Oracle database Configured and customized logs using Log4J Involved in installing and configuring Eclipse and Maven for development Used SVN as a version management tool Written Test Cases for Unit Level Testing using JUnit Developed database objects like Stored Procedures Functions Triggers index views to maintain referential integrity of the database Responsible to monitor all the tasks assigned to offshore and ensure they are delivered with in SLA Part of the Maintenance and Enhancement Group which provides ongoing support for systems after implementation occurs Environment Spring Framework Hibernate PLSQL Tomcat 7x log4j JUnit Oracle 10g Eclipse AGILE Methodology SCRUM SOAP Webservices Unix Shell Scripting SVN Oracle 9i Maven 4 Java Web Developer Comcast Herndon VA October 2012 to February 2014 Responsibilities Involved in various phases of Software Development Life Cycle SDLC as design development and unit testing Developed and deployed UI layer logics of sites using JSP XML JavaScript HTMLDHTML and Ajax CSS and JavaScript were used to build rich internet pages Agile Scrum Methodology been followed for the development process Designed different design specifications for application development that includes frontend backend using design patterns Developed prototype test screens in HTML and JavaScript Involved in developing JSP for client data presentation and data validation on the client side within the forms Developed the application by using the Spring MVC framework Collection framework used to transfer objects between the different layers of the application Developed data mapping to create a communication bridge between various application interfaces using XML and XSL Spring IOC being used to inject the parameter values for the Dynamic parameters Developed JUnit testing framework for Unit level testing Actively involved in code review and bug fixing for improving the performance Documented application for its functionality and its enhanced features Created connection through JDBC and used JDBC statements to call stored procedures Environment Spring MVC J2EE Java JDBC Servlets JSP XML Design Patterns CSS HTML JavaScript Junit Apache Tomcat My SQL Server 2008 Education Bachelors Skills HDFS MAPREDUCE OOZIE SQOOP HBASE FLUME MONGODB NOSQL VISUAL STUDIO Git HBase Hive HTML JAVASCRIPT MapReduce Pig PYTHON XML ZooKeeper ECLIPSE Additional Information TECHNICAL SKILLS Operating Systems Windows Ubuntu UNIX Big Data HDFS MapReduce PIG Hive Zookeeper Sqoop Flume Oozie Programming Languages Python Java Databases Oracle 9i10g MySQL Sybase ASE 125 NoSQL HBase MongoDB Web Technologies Javascript HTML CSS XML Version Control Git IDEs Eclipse for Java Visual Studio",
    "entities": [
        "HDFSHbase",
        "Ajax CSS",
        "Scala Hadoop Developer",
        "SparkSQL Data Frame",
        "the Cluster for the Map Reduce",
        "Flume Monitor System",
        "UNIX",
        "East West Bank",
        "Data to Hadoop Distributed File System",
        "Data Sources",
        "Developed Spark",
        "Working",
        "Spark Streaming Hands",
        "Flume Sqoop Pig",
        "MapReduce Pig Hive",
        "Stored Procedures Functions Triggers",
        "Hadoop",
        "XML",
        "Sequence RCFILE ORC",
        "Sr Hadoop Spark Developer Sr Hadoop Spark",
        "Software Development Life Cycle SDLC",
        "Shell",
        "Palo Alto",
        "HBase",
        "Automated",
        "Apache Spark",
        "Created Rich",
        "Environment Hadoop HDFS Map Reduce Hive Pig",
        "Developed",
        "United Airlines",
        "Control Git",
        "HDFS Hive Sqoop",
        "Scripts Developed",
        "JavaScript Involved",
        "JSP",
        "Hive Queries",
        "Worked",
        "Hive Pig HBase Sqoop",
        "HDFS Developed HDFS",
        "Spark",
        "Apache Storm Apache Kafka Integrated Oozie",
        "GIT",
        "Implemented Fair",
        "CSV",
        "Creation of metrics",
        "US",
        "Sqoop",
        "HIVE",
        "Present Responsibilities Designed",
        "Work Experience Sr Hadoop Spark Developer American Express",
        "HTML",
        "HDFS MR HBase",
        "Spark RDD",
        "Software Development",
        "Chicago",
        "Oracle Involved",
        "Big Data",
        "Scala Import",
        "Hive",
        "Counters Shuffle Sort",
        "Python Involved",
        "ETL",
        "GZip LZO BZip2",
        "Maven",
        "Apache Hadoop",
        "JavaScript",
        "Spark Working",
        "SBT",
        "SVN",
        "Bolts",
        "Created HBase",
        "Written Test Cases for Unit Level Testing",
        "Tomcat",
        "Data",
        "jdk18 Version",
        "NoSQL",
        "Tableau",
        "Teradata",
        "HBase Developed"
    ],
    "experience": "Experience Sr Hadoop Spark Developer American Express Phoenix AZ January 2018 to Present Responsibilities Designed the solution using Storm Spouts to stream data from Kafka and Bolts connecting to Java APIs developed independently based on the application logic Imported bulk data into HBase Using Map Reduce programs Written Storm topology to accept the events from Kafka producer and emit into HBase Developed a data pipeline using Kafka and Strom to store data into HDFS Developed HDFS with huge amounts of data using Apache Kafka Implemented a proof of concept Pocs using Kafka Strom HBase for processing streaming data Created HBase tables to load large sets of structured semistructured and unstructured data coming from UNIX NoSQL and a variety of portfolios Development of software using core java with integration of Apache Storm Apache Kafka Integrated Oozie with the rest of the Hadoop stack supporting several types ofHadoop jobs out of the box such as MapReduce Pig Hive and Sqoop as well as system specific jobs such as Java programs and shell scripts Exploring with Spark improving the performance and optimization of the existing algorithms in Hadoop using Spark context SparkSQL Data Frame pair RDDs Spark YARN Developed Spark code and SparkSQLStreaming for faster testing and processing of data Experience in deploying data from various sources into HDFS and building reports using Tableau Performed real time analysis on the incoming data Load the data into Spark RDD and performed inmemory data computation to generate the output response Developed Spark scripts by using Python shell commands as per the requirement Developed Shell scripts and Python programs to automate tasks Environment Hadoop Map Reduce HDFS Spark Java Kafka Hive HBase maven Jenkins Pig UNIX Python Git Storm MapR Oozie HadoopSpark Developer East West Bank Palo Alto CA October 2016 to June 2017 Responsibilities Worked with lambda architecture in handling and processing batch and realtime data Using Sqoop ingested the Data from data warehouse to HDFS Using Kafka collected realtime streaming and log data from web applications and click stream data analyzing a part of data using spark streaming and rest stored into HDFS for future use Worked in creating External and Managed Hive tables Worked in writing Hive Queries for analyzing data in Hive warehouse using Hive Query Language HiveQL and worked with Hive Tables Hive queries Partitioning Bucketing Performed Data Profiling identify data quality and validating rules regarding data integrity and data quality as it relates to the impact on business requirements Build spark applications using SBT builds Used Spark SQL to process the huge amount of structured data Worked on Python plugin on MySQL workbench to upload CSV files Migrated python scikit learn machine learning to data framebased spark machine learning algorithms Created Rich dashboards using Tableau Dashboard Connected Tableau server to publish dashboard to a central location for portal integration Creation of metrics attributes filters reports and dashboards created advanced chart types visualizations and complex calculations to manipulate the data Environment Cloudera Manager Sqoop Java jdk18 Version Hive Spark SparkSQL Scala Hadoop Developer United Airlines Chicago IL May 2015 to September 2016 Responsibilities Worked using Apache Hadoop ecosystem components like HDFS Hive Sqoop and Worked with Spark Scala and Python Involved in data extraction from distributed RDBMS like Teradata and Oracle Involved in loading data from UNIX file system to HDFS Used Map Reduce JUnit for unit testing Good experience in handling data manipulation using python Scripts Developed data pipeline using Flume Sqoop Pig and Java map reduce and Spark to ingest customer behavioral data and purchase histories into HDFS for analysis Troubleshooting the cluster by reviewing Hadoop LOG files Involved in managing and reviewing Hadoop log files Installed and configured Pig for ETL jobs Experienced in migrating iterative map reduce programs into Spark transformations using Scala Import the data from different sources like HDFSHbase into Spark RDD Used Oozie to manage the Hadoop jobs Involved in running Hadoop streaming jobs to process terabytes of text data Load and transform large sets of structured semi structured and unstructured data Imported data using Sqoop from Teradata using Teradata connector Implemented Partitioning Dynamic Partitioning and Bucketing in HIVE Exported the result set from HIVE to MySQL using Shell scripts Used Zookeeper for various types of centralized configurations Involved in maintaining various Unix Shell scripts Implemented Fair schedulers on the Job tracker to share the resources of the Cluster for the Map Reduce jobs given by the users Automated all the jobs starting from pulling the Data from different Data Sources like MySQL to pushing the result set Data to Hadoop Distributed File System using Sqoop Used GIT for version control Maintain System integrity of all subcomponents primarily HDFS MR HBase and Flume Monitor System health and logs and respond accordingly to any warning or failure conditions Environment Hadoop HDFS Map Reduce Hive Pig Sqoop Hue Impala NoSQL Java 16 Mongo and Spark Sr Java Developer Fresenius Medical Care Waltham MA March 2014 to April 2015 Roles and Responsibilities Involved in various phases of Software Development such as modeling system analysis and design code generation and testing using AGILE Methodology Participated in daily Stand up meetings with Scrum Master Responsible for estimating the time for given task Service Request Designed developed and deployed application using Eclipse and Tomcat application Server Developed SQL queries and used JDBC to interact with the Oracle database Configured and customized logs using Log4J Involved in installing and configuring Eclipse and Maven for development Used SVN as a version management tool Written Test Cases for Unit Level Testing using JUnit Developed database objects like Stored Procedures Functions Triggers index views to maintain referential integrity of the database Responsible to monitor all the tasks assigned to offshore and ensure they are delivered with in SLA Part of the Maintenance and Enhancement Group which provides ongoing support for systems after implementation occurs Environment Spring Framework Hibernate PLSQL Tomcat 7x log4j JUnit Oracle 10 g Eclipse AGILE Methodology SCRUM SOAP Webservices Unix Shell Scripting SVN Oracle 9i Maven 4 Java Web Developer Comcast Herndon VA October 2012 to February 2014 Responsibilities Involved in various phases of Software Development Life Cycle SDLC as design development and unit testing Developed and deployed UI layer logics of sites using JSP XML JavaScript HTMLDHTML and Ajax CSS and JavaScript were used to build rich internet pages Agile Scrum Methodology been followed for the development process Designed different design specifications for application development that includes frontend backend using design patterns Developed prototype test screens in HTML and JavaScript Involved in developing JSP for client data presentation and data validation on the client side within the forms Developed the application by using the Spring MVC framework Collection framework used to transfer objects between the different layers of the application Developed data mapping to create a communication bridge between various application interfaces using XML and XSL Spring IOC being used to inject the parameter values for the Dynamic parameters Developed JUnit testing framework for Unit level testing Actively involved in code review and bug fixing for improving the performance Documented application for its functionality and its enhanced features Created connection through JDBC and used JDBC statements to call stored procedures Environment Spring MVC J2EE Java JDBC Servlets JSP XML Design Patterns CSS HTML JavaScript Junit Apache Tomcat My SQL Server 2008 Education Bachelors Skills HDFS MAPREDUCE OOZIE SQOOP HBASE FLUME MONGODB NOSQL VISUAL STUDIO Git HBase Hive HTML JAVASCRIPT MapReduce Pig PYTHON XML ZooKeeper ECLIPSE Additional Information TECHNICAL SKILLS Operating Systems Windows Ubuntu UNIX Big Data HDFS MapReduce PIG Hive Zookeeper Sqoop Flume Oozie Programming Languages Python Java Databases Oracle 9i10 g MySQL Sybase ASE 125 NoSQL HBase MongoDB Web Technologies Javascript HTML CSS XML Version Control Git IDEs Eclipse for Java Visual Studio",
    "extracted_keywords": [
        "Sr",
        "Hadoop",
        "Spark",
        "Developer",
        "Sr",
        "Hadoop",
        "Spark",
        "span",
        "lDeveloperspan",
        "Sr",
        "Hadoop",
        "Spark",
        "Developer",
        "American",
        "Express",
        "Palo",
        "Alto",
        "CA",
        "years",
        "IT",
        "experience",
        "experience",
        "data",
        "ecosystem",
        "experience",
        "project",
        "life",
        "cycle",
        "design",
        "development",
        "unit",
        "testing",
        "implementation",
        "years",
        "work",
        "experience",
        "ingestion",
        "storage",
        "processing",
        "analysis",
        "Big",
        "Data",
        "hands",
        "experience",
        "Hadoop",
        "Ecosystem",
        "YARN",
        "HDFS",
        "components",
        "Hive",
        "Pig",
        "HBase",
        "Sqoop",
        "Hue",
        "Kafka",
        "Flume",
        "Oozie",
        "Zookeeper",
        "Spark",
        "Spark",
        "SQL",
        "Spark",
        "Streaming",
        "Hands",
        "experience",
        "performance",
        "optimization",
        "algorithms",
        "Hadoop",
        "Spark",
        "context",
        "SparkSQL",
        "Data",
        "Frame",
        "pair",
        "Spark",
        "YARN",
        "experience",
        "application",
        "Apache",
        "Spark",
        "python",
        "Spark",
        "Working",
        "experience",
        "spark",
        "applications",
        "build",
        "tools",
        "SBT",
        "Maven",
        "Gradle",
        "Good",
        "experience",
        "file",
        "formats",
        "text",
        "Sequence",
        "RCFILE",
        "ORC",
        "Parquet",
        "Avro",
        "compression",
        "formats",
        "GZip",
        "LZO",
        "BZip2",
        "knowledge",
        "databases",
        "MySQL",
        "Oracle",
        "NoSQL",
        "HBase",
        "experience",
        "data",
        "data",
        "sources",
        "Working",
        "experience",
        "Map",
        "side",
        "join",
        "Reducer",
        "side",
        "join",
        "Cache",
        "Compression",
        "Input",
        "output",
        "working",
        "experience",
        "analysis",
        "data",
        "HiveQL",
        "joins",
        "Hive",
        "Generic",
        "UDFs",
        "exposure",
        "Counters",
        "Shuffle",
        "Sort",
        "parameters",
        "Dynamic",
        "Partitions",
        "Bucketing",
        "performance",
        "improvement",
        "knowledge",
        "IDE",
        "Net",
        "Beans",
        "Eclipse",
        "Intellij",
        "Authorized",
        "US",
        "employer",
        "Work",
        "Experience",
        "Sr",
        "Hadoop",
        "Spark",
        "Developer",
        "American",
        "Express",
        "Phoenix",
        "AZ",
        "January",
        "Present",
        "Responsibilities",
        "solution",
        "Storm",
        "Spouts",
        "data",
        "Kafka",
        "Bolts",
        "Java",
        "APIs",
        "application",
        "logic",
        "data",
        "HBase",
        "Map",
        "Reduce",
        "programs",
        "Storm",
        "topology",
        "events",
        "Kafka",
        "producer",
        "HBase",
        "data",
        "pipeline",
        "Kafka",
        "Strom",
        "data",
        "HDFS",
        "HDFS",
        "amounts",
        "data",
        "Apache",
        "Kafka",
        "proof",
        "concept",
        "Pocs",
        "Kafka",
        "Strom",
        "HBase",
        "streaming",
        "data",
        "Created",
        "HBase",
        "sets",
        "data",
        "UNIX",
        "NoSQL",
        "variety",
        "portfolios",
        "Development",
        "software",
        "core",
        "integration",
        "Apache",
        "Storm",
        "Apache",
        "Kafka",
        "Integrated",
        "Oozie",
        "rest",
        "Hadoop",
        "stack",
        "types",
        "ofHadoop",
        "jobs",
        "box",
        "MapReduce",
        "Pig",
        "Hive",
        "Sqoop",
        "system",
        "jobs",
        "Java",
        "programs",
        "scripts",
        "Spark",
        "performance",
        "optimization",
        "algorithms",
        "Hadoop",
        "Spark",
        "context",
        "SparkSQL",
        "Data",
        "Frame",
        "pair",
        "RDDs",
        "Spark",
        "YARN",
        "Spark",
        "code",
        "SparkSQLStreaming",
        "testing",
        "processing",
        "data",
        "Experience",
        "data",
        "sources",
        "HDFS",
        "building",
        "reports",
        "Tableau",
        "Performed",
        "time",
        "analysis",
        "data",
        "data",
        "Spark",
        "RDD",
        "data",
        "computation",
        "output",
        "response",
        "Developed",
        "Spark",
        "scripts",
        "Python",
        "shell",
        "commands",
        "requirement",
        "Developed",
        "Shell",
        "scripts",
        "Python",
        "programs",
        "tasks",
        "Environment",
        "Hadoop",
        "Map",
        "HDFS",
        "Spark",
        "Java",
        "Kafka",
        "Hive",
        "HBase",
        "Jenkins",
        "Pig",
        "UNIX",
        "Python",
        "Git",
        "Storm",
        "MapR",
        "Oozie",
        "HadoopSpark",
        "Developer",
        "East",
        "West",
        "Bank",
        "Palo",
        "Alto",
        "CA",
        "October",
        "June",
        "Responsibilities",
        "architecture",
        "processing",
        "batch",
        "data",
        "Sqoop",
        "Data",
        "data",
        "warehouse",
        "HDFS",
        "Kafka",
        "streaming",
        "data",
        "web",
        "applications",
        "stream",
        "data",
        "part",
        "data",
        "spark",
        "streaming",
        "rest",
        "HDFS",
        "use",
        "External",
        "Managed",
        "Hive",
        "tables",
        "Hive",
        "Queries",
        "data",
        "Hive",
        "warehouse",
        "Hive",
        "Query",
        "Language",
        "HiveQL",
        "Hive",
        "Tables",
        "Hive",
        "Partitioning",
        "Bucketing",
        "Performed",
        "Data",
        "Profiling",
        "data",
        "quality",
        "rules",
        "data",
        "integrity",
        "data",
        "quality",
        "impact",
        "business",
        "requirements",
        "spark",
        "applications",
        "SBT",
        "builds",
        "Spark",
        "SQL",
        "amount",
        "data",
        "Python",
        "plugin",
        "MySQL",
        "workbench",
        "CSV",
        "files",
        "scikit",
        "machine",
        "data",
        "spark",
        "machine",
        "learning",
        "algorithms",
        "Rich",
        "dashboards",
        "Tableau",
        "Dashboard",
        "Connected",
        "Tableau",
        "server",
        "dashboard",
        "location",
        "integration",
        "Creation",
        "metrics",
        "filters",
        "reports",
        "dashboards",
        "chart",
        "types",
        "visualizations",
        "calculations",
        "data",
        "Environment",
        "Cloudera",
        "Manager",
        "Sqoop",
        "Java",
        "jdk18",
        "Version",
        "Hive",
        "Spark",
        "SparkSQL",
        "Scala",
        "Hadoop",
        "Developer",
        "United",
        "Airlines",
        "Chicago",
        "IL",
        "May",
        "September",
        "Responsibilities",
        "Apache",
        "Hadoop",
        "ecosystem",
        "components",
        "HDFS",
        "Hive",
        "Sqoop",
        "Spark",
        "Scala",
        "Python",
        "data",
        "extraction",
        "RDBMS",
        "Teradata",
        "Oracle",
        "loading",
        "data",
        "UNIX",
        "file",
        "system",
        "Map",
        "JUnit",
        "unit",
        "experience",
        "data",
        "manipulation",
        "python",
        "Scripts",
        "data",
        "pipeline",
        "Flume",
        "Sqoop",
        "Pig",
        "Java",
        "map",
        "Spark",
        "customer",
        "data",
        "purchase",
        "histories",
        "HDFS",
        "analysis",
        "cluster",
        "Hadoop",
        "LOG",
        "files",
        "Hadoop",
        "log",
        "files",
        "Pig",
        "ETL",
        "jobs",
        "map",
        "programs",
        "Spark",
        "transformations",
        "Scala",
        "Import",
        "data",
        "sources",
        "HDFSHbase",
        "Spark",
        "RDD",
        "Oozie",
        "Hadoop",
        "jobs",
        "Hadoop",
        "streaming",
        "jobs",
        "terabytes",
        "text",
        "data",
        "Load",
        "sets",
        "data",
        "data",
        "Sqoop",
        "Teradata",
        "Teradata",
        "connector",
        "Partitioning",
        "Dynamic",
        "Partitioning",
        "Bucketing",
        "HIVE",
        "result",
        "HIVE",
        "MySQL",
        "Shell",
        "scripts",
        "Zookeeper",
        "types",
        "configurations",
        "Unix",
        "Shell",
        "scripts",
        "Fair",
        "schedulers",
        "Job",
        "tracker",
        "resources",
        "Cluster",
        "Map",
        "Reduce",
        "jobs",
        "users",
        "jobs",
        "Data",
        "Data",
        "Sources",
        "MySQL",
        "result",
        "Data",
        "Hadoop",
        "Distributed",
        "File",
        "System",
        "Sqoop",
        "Used",
        "GIT",
        "version",
        "control",
        "Maintain",
        "System",
        "integrity",
        "subcomponents",
        "MR",
        "HBase",
        "Flume",
        "Monitor",
        "System",
        "health",
        "logs",
        "warning",
        "failure",
        "conditions",
        "Environment",
        "Hadoop",
        "HDFS",
        "Map",
        "Reduce",
        "Hive",
        "Pig",
        "Sqoop",
        "Hue",
        "Impala",
        "NoSQL",
        "Java",
        "Mongo",
        "Spark",
        "Sr",
        "Java",
        "Developer",
        "Fresenius",
        "Medical",
        "Care",
        "Waltham",
        "MA",
        "March",
        "April",
        "Roles",
        "Responsibilities",
        "phases",
        "Software",
        "Development",
        "system",
        "analysis",
        "design",
        "code",
        "generation",
        "testing",
        "AGILE",
        "Methodology",
        "meetings",
        "Scrum",
        "Master",
        "Responsible",
        "time",
        "task",
        "Service",
        "Request",
        "application",
        "Eclipse",
        "Tomcat",
        "application",
        "Server",
        "SQL",
        "queries",
        "JDBC",
        "Oracle",
        "database",
        "Configured",
        "logs",
        "Log4J",
        "Eclipse",
        "Maven",
        "development",
        "SVN",
        "version",
        "management",
        "tool",
        "Written",
        "Test",
        "Cases",
        "Unit",
        "Level",
        "Testing",
        "JUnit",
        "database",
        "Stored",
        "Procedures",
        "Functions",
        "Triggers",
        "index",
        "views",
        "integrity",
        "database",
        "tasks",
        "SLA",
        "Part",
        "Maintenance",
        "Enhancement",
        "Group",
        "support",
        "systems",
        "implementation",
        "Environment",
        "Spring",
        "Framework",
        "Hibernate",
        "PLSQL",
        "Tomcat",
        "7x",
        "log4j",
        "JUnit",
        "Oracle",
        "g",
        "Eclipse",
        "AGILE",
        "Methodology",
        "SOAP",
        "Webservices",
        "Unix",
        "Shell",
        "Scripting",
        "SVN",
        "Oracle",
        "9i",
        "Maven",
        "Java",
        "Web",
        "Developer",
        "Comcast",
        "Herndon",
        "VA",
        "October",
        "February",
        "Responsibilities",
        "phases",
        "Software",
        "Development",
        "Life",
        "Cycle",
        "SDLC",
        "design",
        "development",
        "unit",
        "testing",
        "Developed",
        "UI",
        "layer",
        "logics",
        "sites",
        "JSP",
        "XML",
        "JavaScript",
        "HTMLDHTML",
        "CSS",
        "JavaScript",
        "internet",
        "pages",
        "Agile",
        "Scrum",
        "Methodology",
        "development",
        "process",
        "design",
        "specifications",
        "application",
        "development",
        "frontend",
        "design",
        "patterns",
        "prototype",
        "test",
        "screens",
        "HTML",
        "JavaScript",
        "JSP",
        "client",
        "data",
        "presentation",
        "data",
        "validation",
        "client",
        "side",
        "forms",
        "application",
        "Spring",
        "MVC",
        "framework",
        "Collection",
        "framework",
        "objects",
        "layers",
        "application",
        "data",
        "mapping",
        "communication",
        "bridge",
        "application",
        "interfaces",
        "XML",
        "XSL",
        "Spring",
        "IOC",
        "parameter",
        "values",
        "parameters",
        "JUnit",
        "testing",
        "framework",
        "Unit",
        "level",
        "testing",
        "code",
        "review",
        "bug",
        "performance",
        "application",
        "functionality",
        "features",
        "connection",
        "JDBC",
        "JDBC",
        "statements",
        "procedures",
        "Environment",
        "Spring",
        "MVC",
        "J2EE",
        "Java",
        "JDBC",
        "Servlets",
        "JSP",
        "XML",
        "Design",
        "Patterns",
        "CSS",
        "HTML",
        "JavaScript",
        "Junit",
        "Apache",
        "Tomcat",
        "SQL",
        "Server",
        "Education",
        "Bachelors",
        "Skills",
        "HDFS",
        "MAPREDUCE",
        "OOZIE",
        "SQOOP",
        "HBASE",
        "NOSQL",
        "VISUAL",
        "STUDIO",
        "Git",
        "HBase",
        "Hive",
        "HTML",
        "JAVASCRIPT",
        "MapReduce",
        "Pig",
        "PYTHON",
        "XML",
        "ZooKeeper",
        "Additional",
        "Information",
        "TECHNICAL",
        "SKILLS",
        "Operating",
        "Systems",
        "Windows",
        "Ubuntu",
        "UNIX",
        "Big",
        "Data",
        "HDFS",
        "MapReduce",
        "PIG",
        "Hive",
        "Zookeeper",
        "Sqoop",
        "Flume",
        "Oozie",
        "Programming",
        "Languages",
        "Python",
        "Java",
        "Databases",
        "Oracle",
        "g",
        "MySQL",
        "Sybase",
        "ASE",
        "NoSQL",
        "HBase",
        "MongoDB",
        "Web",
        "Technologies",
        "Javascript",
        "HTML",
        "CSS",
        "XML",
        "Version",
        "Control",
        "Git",
        "IDEs",
        "Eclipse",
        "Java",
        "Visual",
        "Studio"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T22:58:56.665963",
    "resume_data": "Sr Hadoop Spark Developer Sr Hadoop Spark span lDeveloperspan Sr Hadoop Spark Developer American Express Palo Alto CA Over 6 years of professional IT experience which includes experience in Big data ecosystem experience in complete project life cycle design development unit testing and implementation of which over 3 years of work experience in ingestion storage querying processing and analysis of Big Data with hands on experience in Hadoop Ecosystem YARN HDFS and its components Hive Pig HBase Sqoop Hue Kafka Flume Oozie Zookeeper Spark Spark SQL and Spark Streaming Hands on experience in improving the performance and optimization of the existing algorithms in Hadoop using Spark context SparkSQL Data Frame pair RDDs Spark YARN and experience in application in Apache Spark using python Spark Working experience on building spark applications using build tools like SBT Maven and Gradle Good experience in dealing with different file formats like text Sequence RCFILE ORC Parquet Avro and JSON and different compression formats like GZip LZO BZip2 and snappy Good knowledge on relational databases like MySQL Oracle and NoSQL databases like HBase MongoDB Working experience in handling semiunstructured data from different data sources Working experience in developing Map side join Reducer side join Distributed Cache Compression techniques Multiple Input output Good working experience in performing adhoc analysis on structured data using HiveQL joins and Hive Generic UDFs good exposure to Counters Shuffle Sort parameters Dynamic Partitions Bucketing for performance improvement Good knowledge in using IDE like Net Beans Eclipse Intellij Authorized to work in the US for any employer Work Experience Sr Hadoop Spark Developer American Express Phoenix AZ January 2018 to Present Responsibilities Designed the solution using Storm Spouts to stream data from Kafka and Bolts connecting to Java APIs developed independently based on the application logic Imported bulk data into HBase Using Map Reduce programs Written Storm topology to accept the events from Kafka producer and emit into HBase Developed a data pipeline using Kafka and Strom to store data into HDFS Developed HDFS with huge amounts of data using Apache Kafka Implemented a proof of concept Pocs using Kafka Strom HBase for processing streaming data Created HBase tables to load large sets of structured semistructured and unstructured data coming from UNIX NoSQL and a variety of portfolios Development of software using core java with integration of Apache Storm Apache Kafka Integrated Oozie with the rest of the Hadoop stack supporting several types ofHadoop jobs out of the box such as MapReduce Pig Hive and Sqoop as well as system specific jobs such as Java programs and shell scripts Exploring with Spark improving the performance and optimization of the existing algorithms in Hadoop using Spark context SparkSQL Data Frame pair RDDs Spark YARN Developed Spark code and SparkSQLStreaming for faster testing and processing of data Experience in deploying data from various sources into HDFS and building reports using Tableau Performed real time analysis on the incoming data Load the data into Spark RDD and performed inmemory data computation to generate the output response Developed Spark scripts by using Python shell commands as per the requirement Developed Shell scripts and Python programs to automate tasks Environment Hadoop Map Reduce HDFS Spark Java Kafka Hive HBase maven Jenkins Pig UNIX Python Git Storm MapR Oozie HadoopSpark Developer East West Bank Palo Alto CA October 2016 to June 2017 Responsibilities Worked with lambda architecture in handling and processing batch and realtime data Using Sqoop ingested the Data from data warehouse to HDFS Using Kafka collected realtime streaming and log data from web applications and click stream data analyzing a part of data using spark streaming and rest stored into HDFS for future use Worked in creating External and Managed Hive tables Worked in writing Hive Queries for analyzing data in Hive warehouse using Hive Query Language HiveQL and worked with Hive Tables Hive queries Partitioning Bucketing Performed Data Profiling identify data quality and validating rules regarding data integrity and data quality as it relates to the impact on business requirements Build spark applications using SBT builds Used Spark SQL to process the huge amount of structured data Worked on Python plugin on MySQL workbench to upload CSV files Migrated python scikit learn machine learning to data framebased spark machine learning algorithms Created Rich dashboards using Tableau Dashboard Connected Tableau server to publish dashboard to a central location for portal integration Creation of metrics attributes filters reports and dashboards created advanced chart types visualizations and complex calculations to manipulate the data Environment Cloudera Manager Sqoop Java jdk18 Version Hive Spark SparkSQL Scala Hadoop Developer United Airlines Chicago IL May 2015 to September 2016 Responsibilities Worked using Apache Hadoop ecosystem components like HDFS Hive Sqoop and Worked with Spark Scala and Python Involved in data extraction from distributed RDBMS like Teradata and Oracle Involved in loading data from UNIX file system to HDFS Used Map Reduce JUnit for unit testing Good experience in handling data manipulation using python Scripts Developed data pipeline using Flume Sqoop Pig and Java map reduce and Spark to ingest customer behavioral data and purchase histories into HDFS for analysis Troubleshooting the cluster by reviewing Hadoop LOG files Involved in managing and reviewing Hadoop log files Installed and configured Pig for ETL jobs Experienced in migrating iterative map reduce programs into Spark transformations using Scala Import the data from different sources like HDFSHbase into Spark RDD Used Oozie to manage the Hadoop jobs Involved in running Hadoop streaming jobs to process terabytes of text data Load and transform large sets of structured semi structured and unstructured data Imported data using Sqoop from Teradata using Teradata connector Implemented Partitioning Dynamic Partitioning and Bucketing in HIVE Exported the result set from HIVE to MySQL using Shell scripts Used Zookeeper for various types of centralized configurations Involved in maintaining various Unix Shell scripts Implemented Fair schedulers on the Job tracker to share the resources of the Cluster for the Map Reduce jobs given by the users Automated all the jobs starting from pulling the Data from different Data Sources like MySQL to pushing the result set Data to Hadoop Distributed File System using Sqoop Used GIT for version control Maintain System integrity of all subcomponents primarily HDFS MR HBase and Flume Monitor System health and logs and respond accordingly to any warning or failure conditions Environment Hadoop HDFS Map Reduce Hive Pig Sqoop Hue Impala NoSQL Java 16 Mongo and Spark Sr Java Developer Fresenius Medical Care Waltham MA March 2014 to April 2015 Roles and Responsibilities Involved in various phases of Software Development such as modeling system analysis and design code generation and testing using AGILE Methodology Participated in daily Stand up meetings with Scrum Master Responsible for estimating the time for given task Service Request Designed developed and deployed application using Eclipse and Tomcat application Server Developed SQL queries and used JDBC to interact with the Oracle database Configured and customized logs using Log4J Involved in installing and configuring Eclipse and Maven for development Used SVN as a version management tool Written Test Cases for Unit Level Testing using JUnit Developed database objects like Stored Procedures Functions Triggers index views to maintain referential integrity of the database Responsible to monitor all the tasks assigned to offshore and ensure they are delivered with in SLA Part of the Maintenance and Enhancement Group which provides ongoing support for systems after implementation occurs Environment Spring Framework Hibernate PLSQL Tomcat 7x log4j JUnit Oracle 10g Eclipse AGILE Methodology SCRUM SOAP Webservices Unix Shell Scripting SVN Oracle 9i Maven 4 Java Web Developer Comcast Herndon VA October 2012 to February 2014 Responsibilities Involved in various phases of Software Development Life Cycle SDLC as design development and unit testing Developed and deployed UI layer logics of sites using JSP XML JavaScript HTMLDHTML and Ajax CSS and JavaScript were used to build rich internet pages Agile Scrum Methodology been followed for the development process Designed different design specifications for application development that includes frontend backend using design patterns Developed prototype test screens in HTML and JavaScript Involved in developing JSP for client data presentation and data validation on the client side within the forms Developed the application by using the Spring MVC framework Collection framework used to transfer objects between the different layers of the application Developed data mapping to create a communication bridge between various application interfaces using XML and XSL Spring IOC being used to inject the parameter values for the Dynamic parameters Developed JUnit testing framework for Unit level testing Actively involved in code review and bug fixing for improving the performance Documented application for its functionality and its enhanced features Created connection through JDBC and used JDBC statements to call stored procedures Environment Spring MVC J2EE Java JDBC Servlets JSP XML Design Patterns CSS HTML JavaScript Junit Apache Tomcat My SQL Server 2008 Education Bachelors Skills HDFS MAPREDUCE OOZIE SQOOP HBASE FLUME MONGODB NOSQL VISUAL STUDIO Git HBase Hive HTML JAVASCRIPT MapReduce Pig PYTHON XML ZooKeeper ECLIPSE Additional Information TECHNICAL SKILLS Operating Systems Windows Ubuntu UNIX Big Data HDFS MapReduce PIG Hive Zookeeper Sqoop Flume Oozie Programming Languages Python Java Databases Oracle 9i10g MySQL Sybase ASE 125 NoSQL HBase MongoDB Web Technologies Javascript HTML CSS XML Version Control Git IDEs Eclipse for Java Visual Studio",
    "unique_id": "bb18b27f-2f0c-4b9f-8a97-cbc1f19573f2"
}