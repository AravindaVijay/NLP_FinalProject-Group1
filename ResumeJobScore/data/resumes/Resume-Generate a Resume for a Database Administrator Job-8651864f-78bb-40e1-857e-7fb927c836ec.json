{
    "clean_data": "Hadoopspark Developer Hadoopspark span lDeveloperspan Hadoopspark Developer BCBSA Morrisville NC Work Experience Hadoopspark Developer BCBSA Chicago IL January 2019 to Present Project Description This project is based on webservices and FHIR response to get the prior member coverage for members when provided with member id plan id and product id It utilizes the Hadoop cluster for data storage and uses Responsibilities Work closely with the business and analytics team in gathering the system requirements Data ingestion into HDFS from various mainframe Db2 table using Sqoop Skilled on migrating the data from different databases to Hadoop HDFS and Hive using Sqoop Analyzed large structured datasets using Hives data warehousing infrastructure Extensive Knowledge of creating manages tables and external tables in Eco system Implement Hive UDFs for evaluation filtering loading and storing of data Involved in writing the Hive scripts to reduce the job execution time Used all major ETL Transformations to load the tables from mapping Importing exporting historical data from Db2 to HDFS Hive Design managed and External tables in Hive to optimize performance to improve performance Environment HDFS Hive Sqoop ImportExport Spark Hue Oozie ETL Datawarehouse UNIX Cloudera spark Developer Hadoop Chicago IL December 2017 to November 2018 Project Description all scripts are specialized in designing and developing the highperformance production software with stateof art computer vision capabilities The increase of data made the existing databases un accommodable So all scripts Leap wants to move it to Hadoop where exactly we can handle massive amount of data by means of its cluster nodes and to satisfy the scaling needs of the business operation Responsibilities Extracted the data from the flat files and other RDBMS databases into staging area and ingested to Hadoop Installed and configured Hadoop MapReduce HDFS and developed multiple MapReduce jobs in Java for data cleansing and preprocessing Importing and exporting data into HDFS and Hive using Sqoop from MySQL Responsible for Coding batch pipelines Restful Service Map Reduce program Hive querys testing debugging Peer code review troubleshooting and maintain status report Involved in migrating tables from RDBMS into Hive tables using SQOOP and later generate visualizations using Tableau Involved in creating Hive tables loading with data and writing hive queries Installed Oozie workflow engine and scheduled it to run datatime dependent Hive and Pig jobs Environment HDFS MapReduce Cassandra Hive Pig Sqoop Tableau NoSQL Shell Scripting Git HDP Distribution Eclipse Linux Developer Hadoop Dearborn MI December 2016 to November 2017 Project Description This is a Web based project developed for providing services to their clients at different places This application gathers the information which is maintained in various service stations The information at a service center is technically processed based on the warranty The processed information is sent to the hub for placing the parts order for replacement This project is of two modules Customer Management and service providers management This project also monitors and maintains the manifesto regarding the details of the inventory Responsibilities Used all ETL transformations to load a table through Ab Initio mapping documents Used Hive as ETL tool to do transformations joins and some preaggregations before storing the data onto HDFS Worked on importing and exporting data from Oracle data into HDFS using SQOOP for analysis visualization and to generate reports Creating Hive external tables and partitioned tables using Hive Index and used HQL to make ease of data analytics Provided Oracle development solutions as SQL loader merge logic with insertupdate views Materialized views and generated PLSQL blocks with procedures functions and cursors Used TOAD to create execute and Optimized SQL queries to analyze the data and create various DDL and DML scripts on daily activities Worked on Hive for exposing data for further analysis and for generating transforming files from different analytical formats to text files Used forward engineering to create a Physical Data Model with DDL that best suits the requirements Worked with Sqoop to export analyzed data from HDFS environment into RDBMS for report generation and visualization purpose Maintaining and monitoring clusters Loaded data into the cluster from dynamically generated files using Flume and from relational database management systems using Sqoop Environment Cloudera Cloudera Manager HDFS Map Reduce Hive Impala Pig Python SQL Sqoop Flume Yarn Linux Centos HBase Java developer NebuLogic Technologies Hyderabad Telangana January 2014 to December 2015 Project Description Nebulogic is a provider of business outsourcing solutions They provide Services such as Human Capital Management Tax Compliance Electronic Payment Solutions Dealer Services and Medical Practice Services This project is focused on Human Capital Management and particularly on Payroll Services Human Resource Management Talent Management Benefits Administration Time Attendance and International Solutions This frontend application has interface with various Java applications which are used to send and receive information to third party vendors Responsibilities Involved in Full Life Cycle Development in Distributed Environment Using Java and J2EE framework Responsible for developing and modifying the existing service layer based on the business requirements Involved in designing developing webservices using SOAP and WSDL Involved in database design Created tables views triggers stored procedures in SQL for data manipulation and retrieval Developed Web Services for Payment Transaction and Payment Release Involved in Requirement Analysis Development and Documentation Developed frontend using JSP HTML CSS and JavaScript Coding for DAO Objects using JDBC using DAO pattern XML and XSDs are used to define data formats Implemented J2EE design patterns such as singleton DAO for the presentation tier business tier and Integration Tier layers of the project Involved in Bug fixing and functionality enhancements Followed coding and documentation standards and best practices Participated in project planning discussions and worked with team members to analyze the requirements and translate them into working software modules Environment Java J2EE JSP SOAP WSDL SQL PLSQL XML JDBC Eclipse Windows XP Oracle Education Masters Skills HDFS MAPREDUCE SQOOP HBASE HADOOP C CC C Git Hadoop HBase Hive JAVASCRIPT MapReduce Pig Ruby ZooKeeper DATABASE MICROSOFT SQL SERVER MICROSOFT SQL SERVER 2005 CertificationsLicenses Drivers License Additional Information TECHNICAL SKILLS Hadoop Hive HBase MFS HDFS MapReduce YARN Spark Sqoop Pig Cloudera Manager Zookeeper Tools Apache Tomcat 70 Maven Git Hibernate Microsoft SQL Server Management Studio SQL Developer MySQL Workbench Eclipse Languages Java 78 Scala C CC JavaScript ABAP4 Ruby HTML5 CSS3 Databases Oracle 10g11g12c MySQL Microsoft SQL Server 20052008",
    "entities": [
        "Hadoop Installed",
        "Installed Oozie",
        "Human Capital Management Tax Compliance Electronic Payment Solutions Dealer Services",
        "ETL Transformations",
        "Drivers License Additional Information TECHNICAL SKILLS Hadoop",
        "ETL",
        "DAO",
        "Developer Hadoop Chicago IL",
        "Sqoop",
        "MICROSOFT",
        "DAO Objects",
        "HDFS",
        "Git Hibernate",
        "Human Capital Management",
        "Sqoop Analyzed",
        "Hadoop MapReduce HDFS",
        "Medical Practice Services",
        "Responsibilities Involved",
        "Oracle",
        "DDL",
        "Microsoft",
        "Tableau Involved",
        "Payroll Services Human Resource Management Talent Management Benefits Administration Time Attendance and International Solutions",
        "Customer Management",
        "CSS",
        "FHIR",
        "Hadoopspark Developer Hadoopspark",
        "JSP",
        "HDFS Hive Design",
        "Ab Initio",
        "SQL",
        "External",
        "Hadoop",
        "Microsoft SQL Server Management Studio SQL Developer",
        "SOAP",
        "XML",
        "MapReduce",
        "DML",
        "CertificationsLicenses",
        "Maintaining",
        "Project Description",
        "Linux Developer Hadoop Dearborn",
        "TOAD",
        "Physical Data Model",
        "CC",
        "Developed Web Services for Payment Transaction",
        "Hive",
        "SQOOP",
        "NebuLogic Technologies Hyderabad",
        "Present Project Description",
        "JDBC"
    ],
    "experience": "Experience Hadoopspark Developer BCBSA Chicago IL January 2019 to Present Project Description This project is based on webservices and FHIR response to get the prior member coverage for members when provided with member i d plan i d and product i d It utilizes the Hadoop cluster for data storage and uses Responsibilities Work closely with the business and analytics team in gathering the system requirements Data ingestion into HDFS from various mainframe Db2 table using Sqoop Skilled on migrating the data from different databases to Hadoop HDFS and Hive using Sqoop Analyzed large structured datasets using Hives data warehousing infrastructure Extensive Knowledge of creating manages tables and external tables in Eco system Implement Hive UDFs for evaluation filtering loading and storing of data Involved in writing the Hive scripts to reduce the job execution time Used all major ETL Transformations to load the tables from mapping Importing exporting historical data from Db2 to HDFS Hive Design managed and External tables in Hive to optimize performance to improve performance Environment HDFS Hive Sqoop ImportExport Spark Hue Oozie ETL Datawarehouse UNIX Cloudera spark Developer Hadoop Chicago IL December 2017 to November 2018 Project Description all scripts are specialized in designing and developing the highperformance production software with stateof art computer vision capabilities The increase of data made the existing databases un accommodable So all scripts Leap wants to move it to Hadoop where exactly we can handle massive amount of data by means of its cluster nodes and to satisfy the scaling needs of the business operation Responsibilities Extracted the data from the flat files and other RDBMS databases into staging area and ingested to Hadoop Installed and configured Hadoop MapReduce HDFS and developed multiple MapReduce jobs in Java for data cleansing and preprocessing Importing and exporting data into HDFS and Hive using Sqoop from MySQL Responsible for Coding batch pipelines Restful Service Map Reduce program Hive querys testing debugging Peer code review troubleshooting and maintain status report Involved in migrating tables from RDBMS into Hive tables using SQOOP and later generate visualizations using Tableau Involved in creating Hive tables loading with data and writing hive queries Installed Oozie workflow engine and scheduled it to run datatime dependent Hive and Pig jobs Environment HDFS MapReduce Cassandra Hive Pig Sqoop Tableau NoSQL Shell Scripting Git HDP Distribution Eclipse Linux Developer Hadoop Dearborn MI December 2016 to November 2017 Project Description This is a Web based project developed for providing services to their clients at different places This application gathers the information which is maintained in various service stations The information at a service center is technically processed based on the warranty The processed information is sent to the hub for placing the parts order for replacement This project is of two modules Customer Management and service providers management This project also monitors and maintains the manifesto regarding the details of the inventory Responsibilities Used all ETL transformations to load a table through Ab Initio mapping documents Used Hive as ETL tool to do transformations joins and some preaggregations before storing the data onto HDFS Worked on importing and exporting data from Oracle data into HDFS using SQOOP for analysis visualization and to generate reports Creating Hive external tables and partitioned tables using Hive Index and used HQL to make ease of data analytics Provided Oracle development solutions as SQL loader merge logic with insertupdate views Materialized views and generated PLSQL blocks with procedures functions and cursors Used TOAD to create execute and Optimized SQL queries to analyze the data and create various DDL and DML scripts on daily activities Worked on Hive for exposing data for further analysis and for generating transforming files from different analytical formats to text files Used forward engineering to create a Physical Data Model with DDL that best suits the requirements Worked with Sqoop to export analyzed data from HDFS environment into RDBMS for report generation and visualization purpose Maintaining and monitoring clusters Loaded data into the cluster from dynamically generated files using Flume and from relational database management systems using Sqoop Environment Cloudera Cloudera Manager HDFS Map Reduce Hive Impala Pig Python SQL Sqoop Flume Yarn Linux Centos HBase Java developer NebuLogic Technologies Hyderabad Telangana January 2014 to December 2015 Project Description Nebulogic is a provider of business outsourcing solutions They provide Services such as Human Capital Management Tax Compliance Electronic Payment Solutions Dealer Services and Medical Practice Services This project is focused on Human Capital Management and particularly on Payroll Services Human Resource Management Talent Management Benefits Administration Time Attendance and International Solutions This frontend application has interface with various Java applications which are used to send and receive information to third party vendors Responsibilities Involved in Full Life Cycle Development in Distributed Environment Using Java and J2EE framework Responsible for developing and modifying the existing service layer based on the business requirements Involved in designing developing webservices using SOAP and WSDL Involved in database design Created tables views triggers stored procedures in SQL for data manipulation and retrieval Developed Web Services for Payment Transaction and Payment Release Involved in Requirement Analysis Development and Documentation Developed frontend using JSP HTML CSS and JavaScript Coding for DAO Objects using JDBC using DAO pattern XML and XSDs are used to define data formats Implemented J2EE design patterns such as singleton DAO for the presentation tier business tier and Integration Tier layers of the project Involved in Bug fixing and functionality enhancements Followed coding and documentation standards and best practices Participated in project planning discussions and worked with team members to analyze the requirements and translate them into working software modules Environment Java J2EE JSP SOAP WSDL SQL PLSQL XML JDBC Eclipse Windows XP Oracle Education Masters Skills HDFS MAPREDUCE SQOOP HBASE HADOOP C CC C Git Hadoop HBase Hive JAVASCRIPT MapReduce Pig Ruby ZooKeeper DATABASE MICROSOFT SQL SERVER MICROSOFT SQL SERVER 2005 CertificationsLicenses Drivers License Additional Information TECHNICAL SKILLS Hadoop Hive HBase MFS HDFS MapReduce YARN Spark Sqoop Pig Cloudera Manager Zookeeper Tools Apache Tomcat 70 Maven Git Hibernate Microsoft SQL Server Management Studio SQL Developer MySQL Workbench Eclipse Languages Java 78 Scala C CC JavaScript ABAP4 Ruby HTML5 CSS3 Databases Oracle 10g11g12c MySQL Microsoft SQL Server 20052008",
    "extracted_keywords": [
        "Hadoopspark",
        "Developer",
        "Hadoopspark",
        "span",
        "lDeveloperspan",
        "Hadoopspark",
        "Developer",
        "BCBSA",
        "Morrisville",
        "NC",
        "Work",
        "Experience",
        "Hadoopspark",
        "Developer",
        "BCBSA",
        "Chicago",
        "IL",
        "January",
        "Present",
        "Project",
        "Description",
        "project",
        "webservices",
        "FHIR",
        "response",
        "member",
        "coverage",
        "members",
        "member",
        "i",
        "d",
        "plan",
        "i",
        "d",
        "product",
        "i",
        "d",
        "Hadoop",
        "cluster",
        "data",
        "storage",
        "Responsibilities",
        "business",
        "analytics",
        "team",
        "system",
        "requirements",
        "Data",
        "ingestion",
        "HDFS",
        "mainframe",
        "Db2",
        "table",
        "Sqoop",
        "Skilled",
        "data",
        "databases",
        "Hadoop",
        "HDFS",
        "Hive",
        "Sqoop",
        "datasets",
        "Hives",
        "data",
        "warehousing",
        "infrastructure",
        "Knowledge",
        "manages",
        "tables",
        "tables",
        "Eco",
        "system",
        "Implement",
        "Hive",
        "UDFs",
        "evaluation",
        "loading",
        "storing",
        "data",
        "Hive",
        "scripts",
        "job",
        "execution",
        "time",
        "ETL",
        "Transformations",
        "tables",
        "mapping",
        "Importing",
        "data",
        "HDFS",
        "Hive",
        "Design",
        "tables",
        "Hive",
        "performance",
        "performance",
        "Environment",
        "HDFS",
        "Hive",
        "Sqoop",
        "ImportExport",
        "Spark",
        "Hue",
        "Oozie",
        "ETL",
        "Datawarehouse",
        "UNIX",
        "Cloudera",
        "spark",
        "Developer",
        "Hadoop",
        "Chicago",
        "IL",
        "December",
        "November",
        "Project",
        "Description",
        "scripts",
        "highperformance",
        "production",
        "software",
        "art",
        "computer",
        "vision",
        "increase",
        "data",
        "databases",
        "un",
        "scripts",
        "Leap",
        "Hadoop",
        "amount",
        "data",
        "means",
        "cluster",
        "nodes",
        "scaling",
        "needs",
        "business",
        "operation",
        "Responsibilities",
        "data",
        "files",
        "RDBMS",
        "databases",
        "staging",
        "area",
        "Hadoop",
        "Installed",
        "Hadoop",
        "MapReduce",
        "HDFS",
        "MapReduce",
        "jobs",
        "Java",
        "data",
        "cleansing",
        "Importing",
        "data",
        "HDFS",
        "Hive",
        "Sqoop",
        "MySQL",
        "batch",
        "pipelines",
        "Restful",
        "Service",
        "Map",
        "Reduce",
        "program",
        "Hive",
        "testing",
        "Peer",
        "code",
        "review",
        "troubleshooting",
        "status",
        "report",
        "tables",
        "RDBMS",
        "Hive",
        "tables",
        "SQOOP",
        "visualizations",
        "Tableau",
        "Hive",
        "tables",
        "data",
        "hive",
        "queries",
        "Installed",
        "Oozie",
        "workflow",
        "engine",
        "datatime",
        "Hive",
        "Pig",
        "jobs",
        "Environment",
        "HDFS",
        "MapReduce",
        "Cassandra",
        "Hive",
        "Pig",
        "Sqoop",
        "Tableau",
        "NoSQL",
        "Shell",
        "Scripting",
        "Git",
        "HDP",
        "Distribution",
        "Eclipse",
        "Linux",
        "Developer",
        "Hadoop",
        "Dearborn",
        "MI",
        "December",
        "November",
        "Project",
        "Description",
        "Web",
        "project",
        "services",
        "clients",
        "places",
        "application",
        "information",
        "service",
        "stations",
        "information",
        "service",
        "center",
        "warranty",
        "information",
        "hub",
        "parts",
        "order",
        "replacement",
        "project",
        "modules",
        "Customer",
        "Management",
        "service",
        "providers",
        "management",
        "project",
        "manifesto",
        "details",
        "inventory",
        "Responsibilities",
        "ETL",
        "transformations",
        "table",
        "Ab",
        "Initio",
        "mapping",
        "documents",
        "Hive",
        "ETL",
        "tool",
        "transformations",
        "joins",
        "preaggregations",
        "data",
        "HDFS",
        "data",
        "Oracle",
        "data",
        "HDFS",
        "SQOOP",
        "analysis",
        "visualization",
        "reports",
        "Hive",
        "tables",
        "tables",
        "Hive",
        "Index",
        "HQL",
        "ease",
        "data",
        "analytics",
        "Oracle",
        "development",
        "solutions",
        "SQL",
        "loader",
        "logic",
        "views",
        "views",
        "PLSQL",
        "blocks",
        "procedures",
        "functions",
        "cursors",
        "TOAD",
        "execute",
        "SQL",
        "data",
        "DDL",
        "DML",
        "scripts",
        "activities",
        "Hive",
        "data",
        "analysis",
        "transforming",
        "files",
        "formats",
        "files",
        "engineering",
        "Physical",
        "Data",
        "Model",
        "DDL",
        "requirements",
        "Sqoop",
        "data",
        "HDFS",
        "environment",
        "RDBMS",
        "report",
        "generation",
        "visualization",
        "purpose",
        "clusters",
        "data",
        "cluster",
        "files",
        "Flume",
        "database",
        "management",
        "systems",
        "Sqoop",
        "Environment",
        "Cloudera",
        "Cloudera",
        "Manager",
        "HDFS",
        "Map",
        "Reduce",
        "Hive",
        "Impala",
        "Pig",
        "Python",
        "SQL",
        "Sqoop",
        "Flume",
        "Yarn",
        "Linux",
        "Centos",
        "HBase",
        "Java",
        "developer",
        "NebuLogic",
        "Technologies",
        "Hyderabad",
        "Telangana",
        "January",
        "December",
        "Project",
        "Description",
        "Nebulogic",
        "provider",
        "business",
        "outsourcing",
        "solutions",
        "Services",
        "Human",
        "Capital",
        "Management",
        "Tax",
        "Compliance",
        "Electronic",
        "Payment",
        "Solutions",
        "Dealer",
        "Services",
        "Medical",
        "Practice",
        "Services",
        "project",
        "Human",
        "Capital",
        "Management",
        "Payroll",
        "Services",
        "Human",
        "Resource",
        "Management",
        "Talent",
        "Management",
        "Benefits",
        "Administration",
        "Time",
        "Attendance",
        "International",
        "Solutions",
        "frontend",
        "application",
        "interface",
        "Java",
        "applications",
        "information",
        "party",
        "vendors",
        "Responsibilities",
        "Full",
        "Life",
        "Cycle",
        "Development",
        "Distributed",
        "Environment",
        "Java",
        "J2EE",
        "framework",
        "service",
        "layer",
        "business",
        "requirements",
        "webservices",
        "SOAP",
        "WSDL",
        "database",
        "design",
        "tables",
        "views",
        "triggers",
        "procedures",
        "SQL",
        "data",
        "manipulation",
        "retrieval",
        "Developed",
        "Web",
        "Services",
        "Payment",
        "Transaction",
        "Payment",
        "Release",
        "Requirement",
        "Analysis",
        "Development",
        "Documentation",
        "frontend",
        "JSP",
        "HTML",
        "CSS",
        "JavaScript",
        "Coding",
        "DAO",
        "Objects",
        "JDBC",
        "DAO",
        "pattern",
        "XML",
        "XSDs",
        "data",
        "formats",
        "J2EE",
        "design",
        "patterns",
        "singleton",
        "DAO",
        "presentation",
        "tier",
        "business",
        "tier",
        "Integration",
        "Tier",
        "layers",
        "project",
        "Bug",
        "fixing",
        "functionality",
        "enhancements",
        "documentation",
        "standards",
        "practices",
        "project",
        "planning",
        "discussions",
        "team",
        "members",
        "requirements",
        "software",
        "modules",
        "Environment",
        "Java",
        "J2EE",
        "JSP",
        "SOAP",
        "WSDL",
        "SQL",
        "PLSQL",
        "XML",
        "JDBC",
        "Eclipse",
        "Windows",
        "XP",
        "Oracle",
        "Education",
        "Masters",
        "Skills",
        "HDFS",
        "MAPREDUCE",
        "SQOOP",
        "HBASE",
        "HADOOP",
        "C",
        "CC",
        "C",
        "Git",
        "Hadoop",
        "HBase",
        "Hive",
        "JAVASCRIPT",
        "MapReduce",
        "Pig",
        "Ruby",
        "ZooKeeper",
        "DATABASE",
        "MICROSOFT",
        "SQL",
        "SERVER",
        "MICROSOFT",
        "SQL",
        "SERVER",
        "CertificationsLicenses",
        "Drivers",
        "License",
        "Additional",
        "Information",
        "TECHNICAL",
        "SKILLS",
        "Hadoop",
        "Hive",
        "HBase",
        "MFS",
        "HDFS",
        "MapReduce",
        "YARN",
        "Spark",
        "Sqoop",
        "Pig",
        "Cloudera",
        "Manager",
        "Zookeeper",
        "Tools",
        "Apache",
        "Tomcat",
        "Maven",
        "Git",
        "Hibernate",
        "Microsoft",
        "SQL",
        "Server",
        "Management",
        "Studio",
        "SQL",
        "Developer",
        "MySQL",
        "Workbench",
        "Eclipse",
        "Languages",
        "Java",
        "Scala",
        "C",
        "CC",
        "JavaScript",
        "ABAP4",
        "Ruby",
        "HTML5",
        "CSS3",
        "Databases",
        "Oracle",
        "MySQL",
        "Microsoft",
        "SQL",
        "Server"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T19:54:49.012763",
    "resume_data": "Hadoopspark Developer Hadoopspark span lDeveloperspan Hadoopspark Developer BCBSA Morrisville NC Work Experience Hadoopspark Developer BCBSA Chicago IL January 2019 to Present Project Description This project is based on webservices and FHIR response to get the prior member coverage for members when provided with member id plan id and product id It utilizes the Hadoop cluster for data storage and uses Responsibilities Work closely with the business and analytics team in gathering the system requirements Data ingestion into HDFS from various mainframe Db2 table using Sqoop Skilled on migrating the data from different databases to Hadoop HDFS and Hive using Sqoop Analyzed large structured datasets using Hives data warehousing infrastructure Extensive Knowledge of creating manages tables and external tables in Eco system Implement Hive UDFs for evaluation filtering loading and storing of data Involved in writing the Hive scripts to reduce the job execution time Used all major ETL Transformations to load the tables from mapping Importing exporting historical data from Db2 to HDFS Hive Design managed and External tables in Hive to optimize performance to improve performance Environment HDFS Hive Sqoop ImportExport Spark Hue Oozie ETL Datawarehouse UNIX Cloudera spark Developer Hadoop Chicago IL December 2017 to November 2018 Project Description all scripts are specialized in designing and developing the highperformance production software with stateof art computer vision capabilities The increase of data made the existing databases un accommodable So all scripts Leap wants to move it to Hadoop where exactly we can handle massive amount of data by means of its cluster nodes and to satisfy the scaling needs of the business operation Responsibilities Extracted the data from the flat files and other RDBMS databases into staging area and ingested to Hadoop Installed and configured Hadoop MapReduce HDFS and developed multiple MapReduce jobs in Java for data cleansing and preprocessing Importing and exporting data into HDFS and Hive using Sqoop from MySQL Responsible for Coding batch pipelines Restful Service Map Reduce program Hive querys testing debugging Peer code review troubleshooting and maintain status report Involved in migrating tables from RDBMS into Hive tables using SQOOP and later generate visualizations using Tableau Involved in creating Hive tables loading with data and writing hive queries Installed Oozie workflow engine and scheduled it to run datatime dependent Hive and Pig jobs Environment HDFS MapReduce Cassandra Hive Pig Sqoop Tableau NoSQL Shell Scripting Git HDP Distribution Eclipse Linux Developer Hadoop Dearborn MI December 2016 to November 2017 Project Description This is a Web based project developed for providing services to their clients at different places This application gathers the information which is maintained in various service stations The information at a service center is technically processed based on the warranty The processed information is sent to the hub for placing the parts order for replacement This project is of two modules Customer Management and service providers management This project also monitors and maintains the manifesto regarding the details of the inventory Responsibilities Used all ETL transformations to load a table through Ab Initio mapping documents Used Hive as ETL tool to do transformations joins and some preaggregations before storing the data onto HDFS Worked on importing and exporting data from Oracle data into HDFS using SQOOP for analysis visualization and to generate reports Creating Hive external tables and partitioned tables using Hive Index and used HQL to make ease of data analytics Provided Oracle development solutions as SQL loader merge logic with insertupdate views Materialized views and generated PLSQL blocks with procedures functions and cursors Used TOAD to create execute and Optimized SQL queries to analyze the data and create various DDL and DML scripts on daily activities Worked on Hive for exposing data for further analysis and for generating transforming files from different analytical formats to text files Used forward engineering to create a Physical Data Model with DDL that best suits the requirements Worked with Sqoop to export analyzed data from HDFS environment into RDBMS for report generation and visualization purpose Maintaining and monitoring clusters Loaded data into the cluster from dynamically generated files using Flume and from relational database management systems using Sqoop Environment Cloudera Cloudera Manager HDFS Map Reduce Hive Impala Pig Python SQL Sqoop Flume Yarn Linux Centos HBase Java developer NebuLogic Technologies Hyderabad Telangana January 2014 to December 2015 Project Description Nebulogic is a provider of business outsourcing solutions They provide Services such as Human Capital Management Tax Compliance Electronic Payment Solutions Dealer Services and Medical Practice Services This project is focused on Human Capital Management and particularly on Payroll Services Human Resource Management Talent Management Benefits Administration Time Attendance and International Solutions This frontend application has interface with various Java applications which are used to send and receive information to third party vendors Responsibilities Involved in Full Life Cycle Development in Distributed Environment Using Java and J2EE framework Responsible for developing and modifying the existing service layer based on the business requirements Involved in designing developing webservices using SOAP and WSDL Involved in database design Created tables views triggers stored procedures in SQL for data manipulation and retrieval Developed Web Services for Payment Transaction and Payment Release Involved in Requirement Analysis Development and Documentation Developed frontend using JSP HTML CSS and JavaScript Coding for DAO Objects using JDBC using DAO pattern XML and XSDs are used to define data formats Implemented J2EE design patterns such as singleton DAO for the presentation tier business tier and Integration Tier layers of the project Involved in Bug fixing and functionality enhancements Followed coding and documentation standards and best practices Participated in project planning discussions and worked with team members to analyze the requirements and translate them into working software modules Environment Java J2EE JSP SOAP WSDL SQL PLSQL XML JDBC Eclipse Windows XP Oracle Education Masters Skills HDFS MAPREDUCE SQOOP HBASE HADOOP C CC C Git Hadoop HBase Hive JAVASCRIPT MapReduce Pig Ruby ZooKeeper DATABASE MICROSOFT SQL SERVER MICROSOFT SQL SERVER 2005 CertificationsLicenses Drivers License Additional Information TECHNICAL SKILLS Hadoop Hive HBase MFS HDFS MapReduce YARN Spark Sqoop Pig Cloudera Manager Zookeeper Tools Apache Tomcat 70 Maven Git Hibernate Microsoft SQL Server Management Studio SQL Developer MySQL Workbench Eclipse Languages Java 78 Scala C CC JavaScript ABAP4 Ruby HTML5 CSS3 Databases Oracle 10g11g12c MySQL Microsoft SQL Server 20052008",
    "unique_id": "8651864f-78bb-40e1-857e-7fb927c836ec"
}