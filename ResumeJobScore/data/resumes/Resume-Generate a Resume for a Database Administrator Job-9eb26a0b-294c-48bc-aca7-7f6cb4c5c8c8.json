{
    "clean_data": "Big Data Engineer Big Data Engineer Big Data Engineer Hadoop Houston TX Over 10 years of professional IT experience including 5 years of strong experience working on Apache Spark BigData and Hadoop ecosystem 5 years of strong endtoend experience in Python Java Programming involved in Design development and implementing various webbased applications using Python and Java Technologies Hands on experience in developing and deploying enterprisebased applications using major components in Hadoop ecosystem like Hadoop 2x YARN Hive Pig Spark Map Reduce Impala Kafka Oozie HBase FlumeSqoop and Zookeeper Involved in all phases of Software Development Life Cycle SDLC and Worked on all activities related to the development implementation administration and support for Hadoop Experience in Python programming language for framework and core java concepts Experience with onprem HortonWorks MapR and Google Cloud Platform Experience in monitoring tuning and administrating Hadoop cluster Experience in understanding Big Data business requirements and providing them Hadoop based solutions Experience in Importing and exporting data from different databases like MySQL Oracle Teradata into HDFS using Sqoop Worked on Spark 160 for data processing using RDDs and Dataframe API Experience in writing UDFS in Hive for processing and analyzing large datasets Experience in working with different file formats and compression techniques in Hadoop Experience in using NFS Network File Systems for backing up Name node metadata Experience in managing the cluster resources by implementing fair scheduler and capacity scheduler Experience in developing Pig Latin scripts for data processing on HDFS Excellent team player with good communication skills and effective time management Understand business process management and business requirements of the customers and translate them to specific software requirements Indepth understanding of Spark Architecture including Spark Core Spark SQL Spark Streaming Experience in using Scala to convert HiveSQL queries into RDD transformations in Spark Strong knowledge of real time data analytics using Spark Streaming Kafka amp Flume Proficient knowledge with kafka and spark with YARN Local Standalone modes Expertise in writing Spark RDD transformations Actions Case classes for input data and performing data transformations using SparkCore Implementing Scheduler using Azkaban Tidal Enterprise scheduler Crontab and Oozie Experience in using DStreams Broadcast Variables RDD caching for Spark Streaming Improving the performance and optimizing existing algorithms in Hadoop using Spark context SparkSQL DataFramesPair RDDs Spark YARN Hands on experience with ORC AVRO Sequence and Parquet file formats Experience in analyzing data using PIG Latin HiveQL Spark SQL Experience with Hadoop Distributions like Cloudera and Hortonworks Extensive knowledge on designing Hive ManagedExternal tables Views Hive Analytical functions Experience in tuning the performance of hive queries using Partitioning and Bucketing Experience working with FLUME to handle large volume of streaming data ingestion Experience in developing customized UDFs and UDAFs to extend core functionality if PIG and Hive Experience in various Big Data application phases like Data Ingestion Data analytics and Data visualization Proficient in working with NoSQL databases such as HBase and MongoDB Expertise in writing pig and hive queries for analyzing data to meet business requirements Experience in design and pipeline flows with Jenkins Tonomi and Azkaban Exposed to build tools like MAVEN SBT and bug tracking tool JIRA in the work environment Good Knowledge in scheduling JobWorkflow and monitoring tools like Azkaban and Cisco Tidal Scheduler Hands on Experience in ImportingExporting Data from RDBS to HDFS using SQOOP Excellent programming skills at high level abstraction using Java Scala Python SQL Coordinate patch upgrades bug fixes and new releases for the application within stipulated timelines Performing Team Lead Activities and Coordination with the team members and defining time estimations for deliverables of change requests patches and upgrades to the application Authorized to work in the US for any employer Work Experience Big Data Engineer Hadoop Houston TX June 2017 to Present Chevron is one of the largest oil and gas companies of standard oil based in Houston TX The main goal of this project is to process and analyze the logs of data generated in chevron gas stations This is a project in downstream used to analyze the logistics usage of fuel in different gas stations consumers sentimental analysis Data is imported into HDFS from DB2Oracle and Exadata Transformations are applied to create a single merge and flattened view As a Hadoop developer was responsible for Designing Developing and automating several ETL workflows and analytics Use cases Responsibilities Developed Map Reduce jobs in java for data cleansing and preprocessing Moving data from DB2 Oracle Exadata to HDFS and viceversa using SQOOP Collecting and aggregating large amounts of log data using Apache Flume and staging data in HDFS for further analysis Worked with different file formats and compression techniques to determine standards Developed hive queries and UDFS to analyzetransform the data in HDFS Developed hive scripts for implementing control tables logic in HDFS Designed and Implemented Partitioning Static Dynamic Buckets in HIVE Developed Pig scripts and UDFs as per the Business logic Developed user defined functions in pig using Python AnalyzingTransforming data with Hive and Pig Developed Oozie workflows and they are scheduled through a scheduler on a monthly basis Designed and developed read lock capability in HDFS Implemented Hadoop Float equivalent to the DB2 Decimal Involved in End to End implementation of ETL logic Effective coordination with offshore team and managed project deliverable on time Worked on QA support activities test data creation and Unit testing activities Environment CDH Hadoop HDFS MapReduce Hive Sqoop Pig XML ETL DB2 and QA Big Data Engineer Cisco San Jose CA January 2017 to May 2018 Cisco Systems Inc known as Cisco is an American multinational technology conglomerate headquartered in San Jos California in the center of Silicon Valley that develops manufactures and sells hardware telecommunications and other hightechnology services and products4 Through its numerous acquired subsidiaries such as OpenDNS WebEx and Jasper Cisco specializes into specific tech markets such as Internet IoT domain security and energy management Responsibilities 1 Monitoring Activities Review Tidal Enterprise Scheduler jobs verify if all the jobs are triggered at appropriate time and completed successfully Investigate RCA and fix the issue for any failed jobs Develop Scripts using Python for datalake framework Assist project teams for their queries on DataLake tables InvestigateFix if there are any datamismatch issues identified or raised by the project teams Coordinate with project teams for any datalake related changes and take initiatives like standardization of naming standards Any adhoc table refresh requests and assist project teams for their testing validating during GO Live We coordinate and help to fix the issues with data catalogue when it goes down Assist project teams for the approvals and access management related queries to the tables If there are any PVs to be loaded with huge data we load the data using TPT or with max_update_date and ensure for its success Coordinate with Teradata DBA team and Teradata support team for any TPT related queries issues Worked on Scala 210 jobs using Spark 160for data processing using RDDs and Dataframe API Performance tuning of Spark and Sqoop Job 2 Ingestion Activities Coordinate with project teams for their ingestion requests Review all the metadata provided by them database credentials Incremental columns and uniqueness of merge keys Discuss with Architect team for their approval to proceed with metadata creation Create metadata and get It reviewed by Architect team Load the tables take the audits and share it with the teams Scheduling jobs in TES and validate for their success Ingested data from AWS cloud to Hadoop datalake 3 Enterprise data and Analytics In the process of automating Hadoop Involved in requirement gathering discussions with project teams Joint Application Design meetings Buildupdate user stories Validate requirements Review screen design prototypes test the functionalities participate in User Acceptance Testing and actively involved in GO live activities Environment CDH Hadoop HDFS MapReduce Hive Sqoop Pig Jira XML DB2 and QA Big Data Engineer Kohls South Digital Center Milpitas CA April 2015 to December 2016 Kohls Digital Center in Milpitas California puts Kohls in the fastpaced technology industry and in close proximity to key technology partners The creative incubator is dedicated to seek and implement the latest and greatest technology to revolutionize the customer experience Kohls is one of the leading retail chain markets Develop Support and handle fixes related to project tracks in onprem and GCP Involve in Business interactions client management and coordination with the team Responsibilities Being in Retail domain Online and stores data Need to be Agile Aggressive and focused team player in monitoring debugging and Issue fix Any delay may impact propensities for top trending As a center of Excellence team Involve in any of the application issues triageinvestigate them build and fix the issues Using GCP Console monitor dataproc cluster and jobs Stack Driver to monitor Dashboards and do a performance tuning and optimization of jobs which are memory intensive and provide L3 support for the applications in production environment DAS Data as a Service in GCP google storage buckets Monitor Azkaban jobs in onprem Hortonworks distribution and GCP Google Cloud Platform Investigate RCA and fix the issue for any failed jobs As part of Production Engineering team keep the environment healthy 247 Monitor Azkaban Ambari splunk and Net Diagnostics for Hbase Timeouts Propensities GCP Stackdriver for monitoring logging compute engine and dataproc Involve in discussion with project teams understand the issue Investigate RCA and fix the issue Handle production issues assigned in JIRA tasks Incidents Requests through Remedy Involve in discussions take initiatives and work with Application teams for the smooth flow of projects Involve in all the Technical Discussions and scrum meetings Reviews project documents received from other Technical Specialists Business Technical Specialists and Project Managers to ensure quality completeness and adherence to documentation standard Environment Hadoop Java HDFS JiraAzkaban MapReduce Hive Sqoop Pig XML ETL DB2 and QA JavaHadoop Developer Samsung America Inc San Diego CA February 2013 to March 2015 Samsungs solutions build on the companys 35 years experience in developing and commercializing some of the worlds most popular communication services Based on its wide range of fieldexperience in any technology Samsung has already successfully partnered with dozens of mobile service providers around the world Responsibilities Installation configuration of a Hadoop cluster along with Hive Developed Map Reduce application using Hadoop Map reduce programming a framework for processing Large data sets in parallel across the Hadoop cluster for preprocessing Developed the code for Importing and exporting data into HDFS and Hive using Sqoop Responsible for writing Hive Queries for analyzing data in Hive warehouse using Hive Query Language HQL Responsible for designing Front end system using HTML CSS JSP Servlets and Ajax Transformed web application into compatible Mobile Tablet application by designing responsive designs using HTML CSS Used LDAP for user Authentication and authorization Created Stored Procedures Views Cursors and functions to support application Involved in defining job flows using Oozie for scheduling jobs to manage apache Hadoop jobs by directed Acyclic graph DAG of actions with control flows Developing Hive User Defined Functions in java compiling them into jars and adding them to the HDFS and executing them with Hive Queries Experienced in managing and reviewing Hadoop log files Responsible to manage data coming from different sources Assisted in monitoring the Hadoop cluster using Ganglia tool Dealing with high volume of data in the cluster Tested and reported defects in an Agile Methodology perspective Consolidate all defects report it to PMLeads for prompt fixes by development teams and drive it to closure Installed and configured Hadoop Cluster 3 Node Cluster in fully distributed mode Installed hadoop ecosystemsHive Pig Sqoop HBase Oozie on top of hadoop cluster Importing data from Oracle to HDFS Hive for analytical purpose Analyzing imported data in HDFS Hive using HiveQL and custom Map Reduce programs in Java Environment Java CDH Hadoop HDFS Map Reduce Hive and Sqoop JAVA Developer Value Labs July 2009 to December 2012 Project Description Eorder management project is part of initiatives to harness and leverage on the advancement of IT for better Health It will enable medical practitioners to electronically order supplies prescribe medicine and requisite services such as laboratory radiology referral and other clinical and therapy services Responsibilities Responsible and active in the analysis design implementation and deployment of full software development lifecycle SDLC of the project Designed and developed user interface using JSP HTML and JavaScript Developed JSP Custom Tag Libraries for Tree Structure and Grid using Pagination Logics Worked extensively with JSPs and Servlets to accommodate all presentation customizations on the front endUsed Building tools like Maven to build package test and deploy application in the application server Developed Struts action classes action forms and performed action mapping using Struts framework and Performed data validation in form beans and action classes Extensively used Struts framework as the controller to handle subsequent client requests and invoke the Model based upon user requests Defined the search criteria and pulled out the record of the customer from the database Make the required changes and save the updated record back to the database Validated the fields of user registration screen and login screen by writing JavaScript validations Involved in developing and coding the Interfaces and classes required for the application and created appropriate relationships between the system classes and the interfaces provided Developed build and deployed scripts using Apache ANT to customize WAR and EAR files Used DAO and JDBC for database access Developed stored procedures and triggers using PLSQL to calculate and update the tables to implement business logic Used SVN to maintain source and version management Using JIRA to manage the issuesproject work flow Involved in peer code reviews and performed integration testing of the modules Followed coding and documentation standards Involved in postproduction support and maintenance of the application Environment Oracle Java Struts Servlets HTML XML SQL J2EE JUnit Tomcat PLSQL JIRA SVN Education Masters Skills Ambari Hdfs Sqoop Hbase Apache kafka Kafka Flume Hadoop Teradata Apache spark C Hadoop Hbase Hive Javascript Json Perl Pig Scripting Zookeeper Additional Information Technical Skills Hadoop Ecosystem Development HDFS MapReduce Hive Pig TES HBase Sqoop Zookeeper Spark MCS Azkaban Ambari Hadoop Distribution Framework MapR HortonWorks Cloud Technologies Google Cloud Platform Languages Java C C Technologies Scripting Shell Script Perl JavaScript and PowerShell Hadoop Ingestion Apache Sqoop Apache Kafka Apache spark Apache Flume Storm Database Mongo Database Oracle 10g Oracle 11g MySQL Teradata Hbase Netezza Operating Systems Linux Unix Windows Development Tools Eclipse Putty Tectia Java Technologies JSONJDBCAJAX",
    "entities": [
        "FLUME",
        "Cisco Systems Inc",
        "Azkaban Tidal Enterprise",
        "MAVEN SBT",
        "onprem",
        "Consolidate",
        "Zookeeper Involved",
        "Java Technologies Hands",
        "Hadoop Experience",
        "Project Description Eorder",
        "HDFS",
        "Partitioning",
        "RDBS",
        "Ajax Transformed",
        "AnalyzingTransforming",
        "Interfaces",
        "ImportingExporting Data",
        "products4 Through",
        "Indepth",
        "UDAFs",
        "Project Managers",
        "RDD",
        "Hadoop",
        "Importing",
        "Agile Methodology",
        "EAR",
        "Software Development Life Cycle SDLC",
        "SQOOP Collecting",
        "Production Engineering",
        "Chevron",
        "Technical Specialists Business Technical Specialists",
        "Oracle to HDFS Hive",
        "HBase",
        "Milpitas",
        "Created Stored Procedures Views Cursors",
        "TX",
        "JobWorkflow",
        "DAS Data",
        "Authentication",
        "Python",
        "HDFS Hive",
        "Assisted",
        "Monitor Azkaban",
        "YARN Local Standalone",
        "SparkSQL",
        "Developed",
        "Samsung",
        "JavaScript Developed JSP Custom Tag Libraries for Tree Structure and Grid using",
        "DAO",
        "QA Big Data",
        "Jenkins Tonomi",
        "Big Data Engineer Big Data Engineer Big Data Engineer Hadoop",
        "Pig Sqoop HBase Oozie",
        "Servlets",
        "PMLeads",
        "TES",
        "Hadoop Distributions",
        "HTML CSS JSP Servlets",
        "San Jose",
        "Hadoop Involved",
        "JSP",
        "Ganglia",
        "Oracle Java Struts Servlets",
        "Hive Queries",
        "Hive Queries Experienced",
        "Work Experience Big Data",
        "User Acceptance Testing",
        "SparkCore Implementing Scheduler",
        "Parquet",
        "PowerShell Hadoop Ingestion Apache Sqoop",
        "Apache Spark BigData",
        "Spark",
        "Net Diagnostics for Hbase Timeouts Propensities GCP Stackdriver",
        "Validate",
        "TPT",
        "US",
        "Oozie Experience",
        "QA",
        "Digital Center",
        "Houston",
        "Spark Core Spark",
        "CA",
        "AWS",
        "Hadoop Cluster 3 Node Cluster",
        "Joint Application Design",
        "PIG",
        "log data",
        "java",
        "California",
        "Design",
        "HDFS Implemented Hadoop Float",
        "Spark RDD",
        "Incremental",
        "Azkaban",
        "Hive Query Language HQL Responsible",
        "Data Ingestion Data",
        "Exadata Transformations",
        "Big Data",
        "Hive",
        "SQOOP",
        "Buildupdate",
        "HiveQL",
        "DAG",
        "Environment CDH Hadoop HDFS MapReduce Hive Sqoop",
        "San Jos",
        "ETL",
        "Mobile Tablet",
        "Spark Strong",
        "Maven",
        "Performed",
        "Developed Struts",
        "GCP",
        "QA JavaHadoop Developer Samsung America Inc",
        "Cisco",
        "JavaScript",
        "Front",
        "Spark 160for",
        "InvestigateFix",
        "ORC AVRO Sequence",
        "NFS Network File Systems",
        "Hive ManagedExternal",
        "SVN",
        "Spark Streaming Kafka",
        "UDFS",
        "Responsibilities 1 Monitoring Activities Review Tidal",
        "Views Hive Analytical",
        "Data",
        "Model",
        "NoSQL",
        "Spark Architecture",
        "chevron",
        "GCP Involve",
        "Application",
        "Stack Driver",
        "Investigate RCA",
        "Oracle Exadata"
    ],
    "experience": "Experience in Python programming language for framework and core java concepts Experience with onprem HortonWorks MapR and Google Cloud Platform Experience in monitoring tuning and administrating Hadoop cluster Experience in understanding Big Data business requirements and providing them Hadoop based solutions Experience in Importing and exporting data from different databases like MySQL Oracle Teradata into HDFS using Sqoop Worked on Spark 160 for data processing using RDDs and Dataframe API Experience in writing UDFS in Hive for processing and analyzing large datasets Experience in working with different file formats and compression techniques in Hadoop Experience in using NFS Network File Systems for backing up Name node metadata Experience in managing the cluster resources by implementing fair scheduler and capacity scheduler Experience in developing Pig Latin scripts for data processing on HDFS Excellent team player with good communication skills and effective time management Understand business process management and business requirements of the customers and translate them to specific software requirements Indepth understanding of Spark Architecture including Spark Core Spark SQL Spark Streaming Experience in using Scala to convert HiveSQL queries into RDD transformations in Spark Strong knowledge of real time data analytics using Spark Streaming Kafka amp Flume Proficient knowledge with kafka and spark with YARN Local Standalone modes Expertise in writing Spark RDD transformations Actions Case classes for input data and performing data transformations using SparkCore Implementing Scheduler using Azkaban Tidal Enterprise scheduler Crontab and Oozie Experience in using DStreams Broadcast Variables RDD caching for Spark Streaming Improving the performance and optimizing existing algorithms in Hadoop using Spark context SparkSQL DataFramesPair RDDs Spark YARN Hands on experience with ORC AVRO Sequence and Parquet file formats Experience in analyzing data using PIG Latin HiveQL Spark SQL Experience with Hadoop Distributions like Cloudera and Hortonworks Extensive knowledge on designing Hive ManagedExternal tables Views Hive Analytical functions Experience in tuning the performance of hive queries using Partitioning and Bucketing Experience working with FLUME to handle large volume of streaming data ingestion Experience in developing customized UDFs and UDAFs to extend core functionality if PIG and Hive Experience in various Big Data application phases like Data Ingestion Data analytics and Data visualization Proficient in working with NoSQL databases such as HBase and MongoDB Expertise in writing pig and hive queries for analyzing data to meet business requirements Experience in design and pipeline flows with Jenkins Tonomi and Azkaban Exposed to build tools like MAVEN SBT and bug tracking tool JIRA in the work environment Good Knowledge in scheduling JobWorkflow and monitoring tools like Azkaban and Cisco Tidal Scheduler Hands on Experience in ImportingExporting Data from RDBS to HDFS using SQOOP Excellent programming skills at high level abstraction using Java Scala Python SQL Coordinate patch upgrades bug fixes and new releases for the application within stipulated timelines Performing Team Lead Activities and Coordination with the team members and defining time estimations for deliverables of change requests patches and upgrades to the application Authorized to work in the US for any employer Work Experience Big Data Engineer Hadoop Houston TX June 2017 to Present Chevron is one of the largest oil and gas companies of standard oil based in Houston TX The main goal of this project is to process and analyze the logs of data generated in chevron gas stations This is a project in downstream used to analyze the logistics usage of fuel in different gas stations consumers sentimental analysis Data is imported into HDFS from DB2Oracle and Exadata Transformations are applied to create a single merge and flattened view As a Hadoop developer was responsible for Designing Developing and automating several ETL workflows and analytics Use cases Responsibilities Developed Map Reduce jobs in java for data cleansing and preprocessing Moving data from DB2 Oracle Exadata to HDFS and viceversa using SQOOP Collecting and aggregating large amounts of log data using Apache Flume and staging data in HDFS for further analysis Worked with different file formats and compression techniques to determine standards Developed hive queries and UDFS to analyzetransform the data in HDFS Developed hive scripts for implementing control tables logic in HDFS Designed and Implemented Partitioning Static Dynamic Buckets in HIVE Developed Pig scripts and UDFs as per the Business logic Developed user defined functions in pig using Python AnalyzingTransforming data with Hive and Pig Developed Oozie workflows and they are scheduled through a scheduler on a monthly basis Designed and developed read lock capability in HDFS Implemented Hadoop Float equivalent to the DB2 Decimal Involved in End to End implementation of ETL logic Effective coordination with offshore team and managed project deliverable on time Worked on QA support activities test data creation and Unit testing activities Environment CDH Hadoop HDFS MapReduce Hive Sqoop Pig XML ETL DB2 and QA Big Data Engineer Cisco San Jose CA January 2017 to May 2018 Cisco Systems Inc known as Cisco is an American multinational technology conglomerate headquartered in San Jos California in the center of Silicon Valley that develops manufactures and sells hardware telecommunications and other hightechnology services and products4 Through its numerous acquired subsidiaries such as OpenDNS WebEx and Jasper Cisco specializes into specific tech markets such as Internet IoT domain security and energy management Responsibilities 1 Monitoring Activities Review Tidal Enterprise Scheduler jobs verify if all the jobs are triggered at appropriate time and completed successfully Investigate RCA and fix the issue for any failed jobs Develop Scripts using Python for datalake framework Assist project teams for their queries on DataLake tables InvestigateFix if there are any datamismatch issues identified or raised by the project teams Coordinate with project teams for any datalake related changes and take initiatives like standardization of naming standards Any adhoc table refresh requests and assist project teams for their testing validating during GO Live We coordinate and help to fix the issues with data catalogue when it goes down Assist project teams for the approvals and access management related queries to the tables If there are any PVs to be loaded with huge data we load the data using TPT or with max_update_date and ensure for its success Coordinate with Teradata DBA team and Teradata support team for any TPT related queries issues Worked on Scala 210 jobs using Spark 160for data processing using RDDs and Dataframe API Performance tuning of Spark and Sqoop Job 2 Ingestion Activities Coordinate with project teams for their ingestion requests Review all the metadata provided by them database credentials Incremental columns and uniqueness of merge keys Discuss with Architect team for their approval to proceed with metadata creation Create metadata and get It reviewed by Architect team Load the tables take the audits and share it with the teams Scheduling jobs in TES and validate for their success Ingested data from AWS cloud to Hadoop datalake 3 Enterprise data and Analytics In the process of automating Hadoop Involved in requirement gathering discussions with project teams Joint Application Design meetings Buildupdate user stories Validate requirements Review screen design prototypes test the functionalities participate in User Acceptance Testing and actively involved in GO live activities Environment CDH Hadoop HDFS MapReduce Hive Sqoop Pig Jira XML DB2 and QA Big Data Engineer Kohls South Digital Center Milpitas CA April 2015 to December 2016 Kohls Digital Center in Milpitas California puts Kohls in the fastpaced technology industry and in close proximity to key technology partners The creative incubator is dedicated to seek and implement the latest and greatest technology to revolutionize the customer experience Kohls is one of the leading retail chain markets Develop Support and handle fixes related to project tracks in onprem and GCP Involve in Business interactions client management and coordination with the team Responsibilities Being in Retail domain Online and stores data Need to be Agile Aggressive and focused team player in monitoring debugging and Issue fix Any delay may impact propensities for top trending As a center of Excellence team Involve in any of the application issues triageinvestigate them build and fix the issues Using GCP Console monitor dataproc cluster and jobs Stack Driver to monitor Dashboards and do a performance tuning and optimization of jobs which are memory intensive and provide L3 support for the applications in production environment DAS Data as a Service in GCP google storage buckets Monitor Azkaban jobs in onprem Hortonworks distribution and GCP Google Cloud Platform Investigate RCA and fix the issue for any failed jobs As part of Production Engineering team keep the environment healthy 247 Monitor Azkaban Ambari splunk and Net Diagnostics for Hbase Timeouts Propensities GCP Stackdriver for monitoring logging compute engine and dataproc Involve in discussion with project teams understand the issue Investigate RCA and fix the issue Handle production issues assigned in JIRA tasks Incidents Requests through Remedy Involve in discussions take initiatives and work with Application teams for the smooth flow of projects Involve in all the Technical Discussions and scrum meetings Reviews project documents received from other Technical Specialists Business Technical Specialists and Project Managers to ensure quality completeness and adherence to documentation standard Environment Hadoop Java HDFS JiraAzkaban MapReduce Hive Sqoop Pig XML ETL DB2 and QA JavaHadoop Developer Samsung America Inc San Diego CA February 2013 to March 2015 Samsungs solutions build on the companys 35 years experience in developing and commercializing some of the worlds most popular communication services Based on its wide range of fieldexperience in any technology Samsung has already successfully partnered with dozens of mobile service providers around the world Responsibilities Installation configuration of a Hadoop cluster along with Hive Developed Map Reduce application using Hadoop Map reduce programming a framework for processing Large data sets in parallel across the Hadoop cluster for preprocessing Developed the code for Importing and exporting data into HDFS and Hive using Sqoop Responsible for writing Hive Queries for analyzing data in Hive warehouse using Hive Query Language HQL Responsible for designing Front end system using HTML CSS JSP Servlets and Ajax Transformed web application into compatible Mobile Tablet application by designing responsive designs using HTML CSS Used LDAP for user Authentication and authorization Created Stored Procedures Views Cursors and functions to support application Involved in defining job flows using Oozie for scheduling jobs to manage apache Hadoop jobs by directed Acyclic graph DAG of actions with control flows Developing Hive User Defined Functions in java compiling them into jars and adding them to the HDFS and executing them with Hive Queries Experienced in managing and reviewing Hadoop log files Responsible to manage data coming from different sources Assisted in monitoring the Hadoop cluster using Ganglia tool Dealing with high volume of data in the cluster Tested and reported defects in an Agile Methodology perspective Consolidate all defects report it to PMLeads for prompt fixes by development teams and drive it to closure Installed and configured Hadoop Cluster 3 Node Cluster in fully distributed mode Installed hadoop ecosystemsHive Pig Sqoop HBase Oozie on top of hadoop cluster Importing data from Oracle to HDFS Hive for analytical purpose Analyzing imported data in HDFS Hive using HiveQL and custom Map Reduce programs in Java Environment Java CDH Hadoop HDFS Map Reduce Hive and Sqoop JAVA Developer Value Labs July 2009 to December 2012 Project Description Eorder management project is part of initiatives to harness and leverage on the advancement of IT for better Health It will enable medical practitioners to electronically order supplies prescribe medicine and requisite services such as laboratory radiology referral and other clinical and therapy services Responsibilities Responsible and active in the analysis design implementation and deployment of full software development lifecycle SDLC of the project Designed and developed user interface using JSP HTML and JavaScript Developed JSP Custom Tag Libraries for Tree Structure and Grid using Pagination Logics Worked extensively with JSPs and Servlets to accommodate all presentation customizations on the front endUsed Building tools like Maven to build package test and deploy application in the application server Developed Struts action classes action forms and performed action mapping using Struts framework and Performed data validation in form beans and action classes Extensively used Struts framework as the controller to handle subsequent client requests and invoke the Model based upon user requests Defined the search criteria and pulled out the record of the customer from the database Make the required changes and save the updated record back to the database Validated the fields of user registration screen and login screen by writing JavaScript validations Involved in developing and coding the Interfaces and classes required for the application and created appropriate relationships between the system classes and the interfaces provided Developed build and deployed scripts using Apache ANT to customize WAR and EAR files Used DAO and JDBC for database access Developed stored procedures and triggers using PLSQL to calculate and update the tables to implement business logic Used SVN to maintain source and version management Using JIRA to manage the issuesproject work flow Involved in peer code reviews and performed integration testing of the modules Followed coding and documentation standards Involved in postproduction support and maintenance of the application Environment Oracle Java Struts Servlets HTML XML SQL J2EE JUnit Tomcat PLSQL JIRA SVN Education Masters Skills Ambari Hdfs Sqoop Hbase Apache kafka Kafka Flume Hadoop Teradata Apache spark C Hadoop Hbase Hive Javascript Json Perl Pig Scripting Zookeeper Additional Information Technical Skills Hadoop Ecosystem Development HDFS MapReduce Hive Pig TES HBase Sqoop Zookeeper Spark MCS Azkaban Ambari Hadoop Distribution Framework MapR HortonWorks Cloud Technologies Google Cloud Platform Languages Java C C Technologies Scripting Shell Script Perl JavaScript and PowerShell Hadoop Ingestion Apache Sqoop Apache Kafka Apache spark Apache Flume Storm Database Mongo Database Oracle 10 g Oracle 11 g MySQL Teradata Hbase Netezza Operating Systems Linux Unix Windows Development Tools Eclipse Putty Tectia Java Technologies JSONJDBCAJAX",
    "extracted_keywords": [
        "Data",
        "Engineer",
        "Big",
        "Data",
        "Engineer",
        "Big",
        "Data",
        "Engineer",
        "Hadoop",
        "Houston",
        "TX",
        "years",
        "IT",
        "experience",
        "years",
        "experience",
        "Apache",
        "Spark",
        "BigData",
        "Hadoop",
        "ecosystem",
        "years",
        "endtoend",
        "experience",
        "Python",
        "Java",
        "Programming",
        "Design",
        "development",
        "applications",
        "Python",
        "Java",
        "Technologies",
        "Hands",
        "experience",
        "applications",
        "components",
        "Hadoop",
        "ecosystem",
        "Hadoop",
        "YARN",
        "Hive",
        "Pig",
        "Spark",
        "Map",
        "Reduce",
        "Impala",
        "Kafka",
        "Oozie",
        "HBase",
        "FlumeSqoop",
        "Zookeeper",
        "phases",
        "Software",
        "Development",
        "Life",
        "Cycle",
        "SDLC",
        "activities",
        "development",
        "implementation",
        "administration",
        "support",
        "Hadoop",
        "Experience",
        "Python",
        "programming",
        "language",
        "framework",
        "core",
        "java",
        "concepts",
        "Experience",
        "onprem",
        "HortonWorks",
        "MapR",
        "Google",
        "Cloud",
        "Platform",
        "Experience",
        "Hadoop",
        "cluster",
        "Experience",
        "Big",
        "Data",
        "business",
        "requirements",
        "Hadoop",
        "solutions",
        "Experience",
        "data",
        "databases",
        "MySQL",
        "Oracle",
        "Teradata",
        "HDFS",
        "Sqoop",
        "Worked",
        "Spark",
        "data",
        "processing",
        "RDDs",
        "API",
        "Experience",
        "UDFS",
        "Hive",
        "processing",
        "datasets",
        "Experience",
        "file",
        "formats",
        "compression",
        "techniques",
        "Hadoop",
        "Experience",
        "NFS",
        "Network",
        "File",
        "Systems",
        "Name",
        "node",
        "metadata",
        "Experience",
        "cluster",
        "resources",
        "scheduler",
        "capacity",
        "scheduler",
        "Experience",
        "Pig",
        "Latin",
        "scripts",
        "data",
        "processing",
        "HDFS",
        "team",
        "player",
        "communication",
        "skills",
        "time",
        "management",
        "Understand",
        "business",
        "process",
        "management",
        "business",
        "requirements",
        "customers",
        "software",
        "requirements",
        "understanding",
        "Spark",
        "Architecture",
        "Spark",
        "Core",
        "Spark",
        "SQL",
        "Spark",
        "Streaming",
        "Experience",
        "Scala",
        "HiveSQL",
        "queries",
        "RDD",
        "transformations",
        "Spark",
        "Strong",
        "knowledge",
        "time",
        "data",
        "analytics",
        "Spark",
        "Streaming",
        "Kafka",
        "amp",
        "Flume",
        "Proficient",
        "knowledge",
        "kafka",
        "YARN",
        "Standalone",
        "Expertise",
        "Spark",
        "RDD",
        "transformations",
        "Actions",
        "Case",
        "classes",
        "input",
        "data",
        "data",
        "transformations",
        "SparkCore",
        "Implementing",
        "Scheduler",
        "Azkaban",
        "Enterprise",
        "scheduler",
        "Crontab",
        "Oozie",
        "Experience",
        "DStreams",
        "Broadcast",
        "Variables",
        "RDD",
        "Spark",
        "Streaming",
        "performance",
        "algorithms",
        "Hadoop",
        "Spark",
        "context",
        "SparkSQL",
        "DataFramesPair",
        "RDDs",
        "Spark",
        "YARN",
        "Hands",
        "experience",
        "ORC",
        "AVRO",
        "Sequence",
        "Parquet",
        "file",
        "formats",
        "Experience",
        "data",
        "PIG",
        "Latin",
        "HiveQL",
        "Spark",
        "SQL",
        "Experience",
        "Hadoop",
        "Distributions",
        "Cloudera",
        "Hortonworks",
        "knowledge",
        "Hive",
        "ManagedExternal",
        "Views",
        "Hive",
        "Analytical",
        "functions",
        "Experience",
        "performance",
        "hive",
        "queries",
        "Partitioning",
        "Bucketing",
        "Experience",
        "FLUME",
        "volume",
        "data",
        "ingestion",
        "Experience",
        "UDFs",
        "UDAFs",
        "core",
        "functionality",
        "PIG",
        "Hive",
        "Experience",
        "Big",
        "Data",
        "application",
        "phases",
        "Data",
        "Ingestion",
        "Data",
        "analytics",
        "Data",
        "visualization",
        "Proficient",
        "NoSQL",
        "databases",
        "HBase",
        "MongoDB",
        "Expertise",
        "pig",
        "hive",
        "queries",
        "data",
        "business",
        "requirements",
        "Experience",
        "design",
        "pipeline",
        "Jenkins",
        "Tonomi",
        "Azkaban",
        "tools",
        "MAVEN",
        "SBT",
        "bug",
        "tool",
        "JIRA",
        "work",
        "environment",
        "Good",
        "Knowledge",
        "JobWorkflow",
        "tools",
        "Cisco",
        "Scheduler",
        "Hands",
        "Experience",
        "ImportingExporting",
        "Data",
        "RDBS",
        "HDFS",
        "SQOOP",
        "Excellent",
        "programming",
        "skills",
        "level",
        "abstraction",
        "Java",
        "Scala",
        "Python",
        "SQL",
        "Coordinate",
        "upgrades",
        "bug",
        "fixes",
        "releases",
        "application",
        "timelines",
        "Performing",
        "Team",
        "Lead",
        "Activities",
        "Coordination",
        "team",
        "members",
        "time",
        "estimations",
        "deliverables",
        "change",
        "requests",
        "patches",
        "upgrades",
        "application",
        "US",
        "employer",
        "Work",
        "Experience",
        "Big",
        "Data",
        "Engineer",
        "Hadoop",
        "Houston",
        "TX",
        "June",
        "Present",
        "Chevron",
        "oil",
        "gas",
        "companies",
        "oil",
        "Houston",
        "TX",
        "goal",
        "project",
        "logs",
        "data",
        "chevron",
        "gas",
        "stations",
        "project",
        "downstream",
        "logistics",
        "usage",
        "fuel",
        "gas",
        "stations",
        "analysis",
        "Data",
        "HDFS",
        "DB2Oracle",
        "Exadata",
        "Transformations",
        "merge",
        "view",
        "Hadoop",
        "developer",
        "Designing",
        "Developing",
        "ETL",
        "workflows",
        "analytics",
        "Use",
        "cases",
        "Responsibilities",
        "Map",
        "jobs",
        "java",
        "data",
        "cleansing",
        "Moving",
        "data",
        "DB2",
        "Oracle",
        "Exadata",
        "HDFS",
        "viceversa",
        "SQOOP",
        "Collecting",
        "amounts",
        "log",
        "data",
        "Apache",
        "Flume",
        "data",
        "HDFS",
        "analysis",
        "file",
        "formats",
        "compression",
        "techniques",
        "standards",
        "hive",
        "queries",
        "UDFS",
        "data",
        "HDFS",
        "hive",
        "scripts",
        "control",
        "tables",
        "logic",
        "HDFS",
        "Partitioning",
        "Static",
        "Buckets",
        "HIVE",
        "Developed",
        "Pig",
        "scripts",
        "UDFs",
        "Business",
        "logic",
        "user",
        "functions",
        "pig",
        "Python",
        "AnalyzingTransforming",
        "data",
        "Hive",
        "Pig",
        "Developed",
        "Oozie",
        "workflows",
        "scheduler",
        "basis",
        "read",
        "lock",
        "capability",
        "HDFS",
        "Implemented",
        "Hadoop",
        "Float",
        "DB2",
        "Decimal",
        "End",
        "End",
        "implementation",
        "ETL",
        "logic",
        "coordination",
        "team",
        "project",
        "time",
        "QA",
        "support",
        "activities",
        "data",
        "creation",
        "Unit",
        "testing",
        "activities",
        "Environment",
        "CDH",
        "Hadoop",
        "HDFS",
        "MapReduce",
        "Hive",
        "Sqoop",
        "Pig",
        "XML",
        "ETL",
        "DB2",
        "QA",
        "Big",
        "Data",
        "Engineer",
        "Cisco",
        "San",
        "Jose",
        "CA",
        "January",
        "May",
        "Cisco",
        "Systems",
        "Inc",
        "Cisco",
        "technology",
        "conglomerate",
        "San",
        "Jos",
        "California",
        "center",
        "Silicon",
        "Valley",
        "manufactures",
        "hardware",
        "telecommunications",
        "hightechnology",
        "services",
        "subsidiaries",
        "WebEx",
        "Jasper",
        "Cisco",
        "tech",
        "markets",
        "Internet",
        "IoT",
        "security",
        "energy",
        "management",
        "Responsibilities",
        "Monitoring",
        "Activities",
        "Review",
        "Tidal",
        "Enterprise",
        "Scheduler",
        "jobs",
        "jobs",
        "time",
        "Investigate",
        "RCA",
        "issue",
        "jobs",
        "Develop",
        "Scripts",
        "Python",
        "datalake",
        "framework",
        "project",
        "teams",
        "queries",
        "DataLake",
        "tables",
        "InvestigateFix",
        "datamismatch",
        "issues",
        "project",
        "teams",
        "project",
        "teams",
        "datalake",
        "changes",
        "initiatives",
        "standardization",
        "naming",
        "standards",
        "adhoc",
        "table",
        "requests",
        "project",
        "teams",
        "testing",
        "GO",
        "Live",
        "issues",
        "data",
        "catalogue",
        "project",
        "teams",
        "approvals",
        "access",
        "management",
        "queries",
        "tables",
        "PVs",
        "data",
        "data",
        "TPT",
        "max_update_date",
        "success",
        "Coordinate",
        "Teradata",
        "DBA",
        "team",
        "Teradata",
        "support",
        "team",
        "TPT",
        "queries",
        "issues",
        "Scala",
        "jobs",
        "Spark",
        "data",
        "processing",
        "RDDs",
        "API",
        "Performance",
        "tuning",
        "Spark",
        "Sqoop",
        "Job",
        "Ingestion",
        "Activities",
        "Coordinate",
        "project",
        "teams",
        "ingestion",
        "requests",
        "metadata",
        "database",
        "credentials",
        "columns",
        "uniqueness",
        "merge",
        "Discuss",
        "Architect",
        "team",
        "approval",
        "metadata",
        "creation",
        "metadata",
        "Architect",
        "team",
        "tables",
        "audits",
        "teams",
        "Scheduling",
        "jobs",
        "TES",
        "success",
        "data",
        "AWS",
        "cloud",
        "Hadoop",
        "Enterprise",
        "data",
        "Analytics",
        "process",
        "Hadoop",
        "requirement",
        "discussions",
        "project",
        "teams",
        "Joint",
        "Application",
        "Design",
        "meetings",
        "Buildupdate",
        "user",
        "stories",
        "Validate",
        "Review",
        "screen",
        "design",
        "prototypes",
        "test",
        "functionalities",
        "User",
        "Acceptance",
        "Testing",
        "GO",
        "activities",
        "Environment",
        "CDH",
        "Hadoop",
        "HDFS",
        "MapReduce",
        "Hive",
        "Sqoop",
        "Pig",
        "Jira",
        "XML",
        "DB2",
        "QA",
        "Big",
        "Data",
        "Engineer",
        "Kohls",
        "South",
        "Digital",
        "Center",
        "Milpitas",
        "CA",
        "April",
        "December",
        "Kohls",
        "Digital",
        "Center",
        "Milpitas",
        "California",
        "Kohls",
        "technology",
        "industry",
        "proximity",
        "technology",
        "partners",
        "incubator",
        "technology",
        "customer",
        "experience",
        "Kohls",
        "chain",
        "markets",
        "Develop",
        "Support",
        "fixes",
        "tracks",
        "onprem",
        "GCP",
        "Involve",
        "Business",
        "interactions",
        "client",
        "management",
        "coordination",
        "team",
        "Responsibilities",
        "domain",
        "Online",
        "stores",
        "data",
        "Agile",
        "Aggressive",
        "team",
        "player",
        "debugging",
        "Issue",
        "delay",
        "propensities",
        "top",
        "center",
        "Excellence",
        "team",
        "application",
        "issues",
        "issues",
        "GCP",
        "Console",
        "monitor",
        "dataproc",
        "cluster",
        "jobs",
        "Stack",
        "Driver",
        "Dashboards",
        "performance",
        "tuning",
        "optimization",
        "jobs",
        "memory",
        "L3",
        "support",
        "applications",
        "production",
        "environment",
        "DAS",
        "Data",
        "Service",
        "GCP",
        "google",
        "storage",
        "buckets",
        "jobs",
        "onprem",
        "Hortonworks",
        "distribution",
        "GCP",
        "Google",
        "Cloud",
        "Platform",
        "Investigate",
        "RCA",
        "issue",
        "jobs",
        "part",
        "Production",
        "Engineering",
        "team",
        "environment",
        "Monitor",
        "Azkaban",
        "Ambari",
        "splunk",
        "Diagnostics",
        "Hbase",
        "Timeouts",
        "Propensities",
        "GCP",
        "Stackdriver",
        "compute",
        "engine",
        "dataproc",
        "discussion",
        "project",
        "teams",
        "issue",
        "Investigate",
        "RCA",
        "issue",
        "Handle",
        "production",
        "issues",
        "JIRA",
        "tasks",
        "Incidents",
        "Requests",
        "Remedy",
        "Involve",
        "discussions",
        "initiatives",
        "work",
        "Application",
        "teams",
        "flow",
        "projects",
        "Technical",
        "Discussions",
        "meetings",
        "Reviews",
        "project",
        "documents",
        "Technical",
        "Specialists",
        "Business",
        "Technical",
        "Specialists",
        "Project",
        "Managers",
        "quality",
        "completeness",
        "adherence",
        "documentation",
        "Environment",
        "Hadoop",
        "Java",
        "HDFS",
        "JiraAzkaban",
        "MapReduce",
        "Hive",
        "Sqoop",
        "Pig",
        "XML",
        "ETL",
        "DB2",
        "QA",
        "JavaHadoop",
        "Developer",
        "Samsung",
        "America",
        "Inc",
        "San",
        "Diego",
        "CA",
        "February",
        "March",
        "Samsungs",
        "solutions",
        "companys",
        "years",
        "experience",
        "worlds",
        "communication",
        "services",
        "range",
        "fieldexperience",
        "technology",
        "Samsung",
        "dozens",
        "service",
        "providers",
        "world",
        "Responsibilities",
        "Installation",
        "configuration",
        "Hadoop",
        "cluster",
        "Hive",
        "Developed",
        "Map",
        "Reduce",
        "application",
        "Hadoop",
        "Map",
        "framework",
        "data",
        "sets",
        "parallel",
        "Hadoop",
        "cluster",
        "code",
        "data",
        "HDFS",
        "Hive",
        "Sqoop",
        "Responsible",
        "Hive",
        "Queries",
        "data",
        "Hive",
        "warehouse",
        "Hive",
        "Query",
        "Language",
        "HQL",
        "Front",
        "end",
        "system",
        "HTML",
        "CSS",
        "JSP",
        "Servlets",
        "Ajax",
        "Transformed",
        "web",
        "application",
        "Mobile",
        "Tablet",
        "application",
        "designs",
        "HTML",
        "CSS",
        "LDAP",
        "user",
        "Authentication",
        "authorization",
        "Stored",
        "Procedures",
        "Views",
        "Cursors",
        "functions",
        "application",
        "job",
        "flows",
        "Oozie",
        "scheduling",
        "jobs",
        "apache",
        "Hadoop",
        "jobs",
        "Acyclic",
        "graph",
        "DAG",
        "actions",
        "control",
        "Hive",
        "User",
        "Defined",
        "Functions",
        "jars",
        "HDFS",
        "Hive",
        "Queries",
        "Hadoop",
        "log",
        "files",
        "data",
        "sources",
        "Hadoop",
        "cluster",
        "Ganglia",
        "tool",
        "volume",
        "data",
        "cluster",
        "defects",
        "Agile",
        "Methodology",
        "perspective",
        "defects",
        "PMLeads",
        "fixes",
        "development",
        "teams",
        "Hadoop",
        "Cluster",
        "Node",
        "Cluster",
        "mode",
        "Installed",
        "hadoop",
        "ecosystemsHive",
        "Pig",
        "Sqoop",
        "HBase",
        "Oozie",
        "top",
        "hadoop",
        "cluster",
        "data",
        "Oracle",
        "HDFS",
        "Hive",
        "purpose",
        "data",
        "HDFS",
        "Hive",
        "HiveQL",
        "custom",
        "Map",
        "Reduce",
        "programs",
        "Java",
        "Environment",
        "Java",
        "CDH",
        "Hadoop",
        "HDFS",
        "Map",
        "Reduce",
        "Hive",
        "Sqoop",
        "JAVA",
        "Developer",
        "Value",
        "Labs",
        "July",
        "December",
        "Project",
        "Description",
        "Eorder",
        "management",
        "project",
        "part",
        "initiatives",
        "harness",
        "leverage",
        "advancement",
        "Health",
        "practitioners",
        "supplies",
        "medicine",
        "services",
        "laboratory",
        "radiology",
        "referral",
        "therapy",
        "services",
        "Responsibilities",
        "analysis",
        "design",
        "implementation",
        "deployment",
        "software",
        "development",
        "lifecycle",
        "SDLC",
        "project",
        "user",
        "interface",
        "JSP",
        "HTML",
        "JavaScript",
        "Developed",
        "JSP",
        "Custom",
        "Tag",
        "Libraries",
        "Tree",
        "Structure",
        "Grid",
        "Pagination",
        "Logics",
        "JSPs",
        "Servlets",
        "presentation",
        "customizations",
        "Building",
        "tools",
        "Maven",
        "package",
        "test",
        "application",
        "application",
        "server",
        "Struts",
        "action",
        "classes",
        "action",
        "forms",
        "action",
        "mapping",
        "Struts",
        "framework",
        "data",
        "validation",
        "form",
        "beans",
        "action",
        "classes",
        "Struts",
        "framework",
        "controller",
        "client",
        "requests",
        "Model",
        "user",
        "requests",
        "search",
        "criteria",
        "record",
        "customer",
        "database",
        "changes",
        "record",
        "database",
        "fields",
        "user",
        "registration",
        "screen",
        "login",
        "screen",
        "JavaScript",
        "validations",
        "Interfaces",
        "classes",
        "application",
        "relationships",
        "system",
        "classes",
        "interfaces",
        "build",
        "scripts",
        "Apache",
        "ANT",
        "WAR",
        "EAR",
        "files",
        "Used",
        "DAO",
        "JDBC",
        "database",
        "access",
        "procedures",
        "triggers",
        "PLSQL",
        "tables",
        "business",
        "logic",
        "SVN",
        "source",
        "version",
        "management",
        "JIRA",
        "issuesproject",
        "work",
        "flow",
        "peer",
        "code",
        "reviews",
        "integration",
        "testing",
        "modules",
        "documentation",
        "standards",
        "postproduction",
        "support",
        "maintenance",
        "application",
        "Environment",
        "Oracle",
        "Java",
        "Struts",
        "Servlets",
        "HTML",
        "XML",
        "SQL",
        "J2EE",
        "JUnit",
        "Tomcat",
        "PLSQL",
        "JIRA",
        "SVN",
        "Education",
        "Masters",
        "Skills",
        "Ambari",
        "Hdfs",
        "Sqoop",
        "Hbase",
        "Apache",
        "Kafka",
        "Flume",
        "Hadoop",
        "Teradata",
        "Apache",
        "spark",
        "C",
        "Hadoop",
        "Hbase",
        "Hive",
        "Javascript",
        "Json",
        "Perl",
        "Pig",
        "Scripting",
        "Zookeeper",
        "Additional",
        "Information",
        "Technical",
        "Skills",
        "Hadoop",
        "Ecosystem",
        "Development",
        "HDFS",
        "MapReduce",
        "Hive",
        "Pig",
        "TES",
        "HBase",
        "Sqoop",
        "Zookeeper",
        "Spark",
        "MCS",
        "Azkaban",
        "Ambari",
        "Hadoop",
        "Distribution",
        "Framework",
        "MapR",
        "HortonWorks",
        "Cloud",
        "Technologies",
        "Google",
        "Cloud",
        "Platform",
        "Languages",
        "Java",
        "C",
        "C",
        "Technologies",
        "Scripting",
        "Shell",
        "Script",
        "Perl",
        "JavaScript",
        "PowerShell",
        "Hadoop",
        "Ingestion",
        "Apache",
        "Sqoop",
        "Apache",
        "Kafka",
        "Apache",
        "spark",
        "Apache",
        "Flume",
        "Storm",
        "Database",
        "Mongo",
        "Database",
        "Oracle",
        "g",
        "Oracle",
        "g",
        "MySQL",
        "Teradata",
        "Hbase",
        "Netezza",
        "Operating",
        "Systems",
        "Linux",
        "Unix",
        "Windows",
        "Development",
        "Tools",
        "Eclipse",
        "Putty",
        "Tectia",
        "Java",
        "Technologies",
        "JSONJDBCAJAX"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T19:53:22.192040",
    "resume_data": "Big Data Engineer Big Data Engineer Big Data Engineer Hadoop Houston TX Over 10 years of professional IT experience including 5 years of strong experience working on Apache Spark BigData and Hadoop ecosystem 5 years of strong endtoend experience in Python Java Programming involved in Design development and implementing various webbased applications using Python and Java Technologies Hands on experience in developing and deploying enterprisebased applications using major components in Hadoop ecosystem like Hadoop 2x YARN Hive Pig Spark Map Reduce Impala Kafka Oozie HBase FlumeSqoop and Zookeeper Involved in all phases of Software Development Life Cycle SDLC and Worked on all activities related to the development implementation administration and support for Hadoop Experience in Python programming language for framework and core java concepts Experience with onprem HortonWorks MapR and Google Cloud Platform Experience in monitoring tuning and administrating Hadoop cluster Experience in understanding Big Data business requirements and providing them Hadoop based solutions Experience in Importing and exporting data from different databases like MySQL Oracle Teradata into HDFS using Sqoop Worked on Spark 160 for data processing using RDDs and Dataframe API Experience in writing UDFS in Hive for processing and analyzing large datasets Experience in working with different file formats and compression techniques in Hadoop Experience in using NFS Network File Systems for backing up Name node metadata Experience in managing the cluster resources by implementing fair scheduler and capacity scheduler Experience in developing Pig Latin scripts for data processing on HDFS Excellent team player with good communication skills and effective time management Understand business process management and business requirements of the customers and translate them to specific software requirements Indepth understanding of Spark Architecture including Spark Core Spark SQL Spark Streaming Experience in using Scala to convert HiveSQL queries into RDD transformations in Spark Strong knowledge of real time data analytics using Spark Streaming Kafka amp Flume Proficient knowledge with kafka and spark with YARN Local Standalone modes Expertise in writing Spark RDD transformations Actions Case classes for input data and performing data transformations using SparkCore Implementing Scheduler using Azkaban Tidal Enterprise scheduler Crontab and Oozie Experience in using DStreams Broadcast Variables RDD caching for Spark Streaming Improving the performance and optimizing existing algorithms in Hadoop using Spark context SparkSQL DataFramesPair RDDs Spark YARN Hands on experience with ORC AVRO Sequence and Parquet file formats Experience in analyzing data using PIG Latin HiveQL Spark SQL Experience with Hadoop Distributions like Cloudera and Hortonworks Extensive knowledge on designing Hive ManagedExternal tables Views Hive Analytical functions Experience in tuning the performance of hive queries using Partitioning and Bucketing Experience working with FLUME to handle large volume of streaming data ingestion Experience in developing customized UDFs and UDAFs to extend core functionality if PIG and Hive Experience in various Big Data application phases like Data Ingestion Data analytics and Data visualization Proficient in working with NoSQL databases such as HBase and MongoDB Expertise in writing pig and hive queries for analyzing data to meet business requirements Experience in design and pipeline flows with Jenkins Tonomi and Azkaban Exposed to build tools like MAVEN SBT and bug tracking tool JIRA in the work environment Good Knowledge in scheduling JobWorkflow and monitoring tools like Azkaban and Cisco Tidal Scheduler Hands on Experience in ImportingExporting Data from RDBS to HDFS using SQOOP Excellent programming skills at high level abstraction using Java Scala Python SQL Coordinate patch upgrades bug fixes and new releases for the application within stipulated timelines Performing Team Lead Activities and Coordination with the team members and defining time estimations for deliverables of change requests patches and upgrades to the application Authorized to work in the US for any employer Work Experience Big Data Engineer Hadoop Houston TX June 2017 to Present Chevron is one of the largest oil and gas companies of standard oil based in Houston TX The main goal of this project is to process and analyze the logs of data generated in chevron gas stations This is a project in downstream used to analyze the logistics usage of fuel in different gas stations consumers sentimental analysis Data is imported into HDFS from DB2Oracle and Exadata Transformations are applied to create a single merge and flattened view As a Hadoop developer was responsible for Designing Developing and automating several ETL workflows and analytics Use cases Responsibilities Developed Map Reduce jobs in java for data cleansing and preprocessing Moving data from DB2 Oracle Exadata to HDFS and viceversa using SQOOP Collecting and aggregating large amounts of log data using Apache Flume and staging data in HDFS for further analysis Worked with different file formats and compression techniques to determine standards Developed hive queries and UDFS to analyzetransform the data in HDFS Developed hive scripts for implementing control tables logic in HDFS Designed and Implemented Partitioning Static Dynamic Buckets in HIVE Developed Pig scripts and UDFs as per the Business logic Developed user defined functions in pig using Python AnalyzingTransforming data with Hive and Pig Developed Oozie workflows and they are scheduled through a scheduler on a monthly basis Designed and developed read lock capability in HDFS Implemented Hadoop Float equivalent to the DB2 Decimal Involved in End to End implementation of ETL logic Effective coordination with offshore team and managed project deliverable on time Worked on QA support activities test data creation and Unit testing activities Environment CDH Hadoop HDFS MapReduce Hive Sqoop Pig XML ETL DB2 and QA Big Data Engineer Cisco San Jose CA January 2017 to May 2018 Cisco Systems Inc known as Cisco is an American multinational technology conglomerate headquartered in San Jos California in the center of Silicon Valley that develops manufactures and sells hardware telecommunications and other hightechnology services and products4 Through its numerous acquired subsidiaries such as OpenDNS WebEx and Jasper Cisco specializes into specific tech markets such as Internet IoT domain security and energy management Responsibilities 1 Monitoring Activities Review Tidal Enterprise Scheduler jobs verify if all the jobs are triggered at appropriate time and completed successfully Investigate RCA and fix the issue for any failed jobs Develop Scripts using Python for datalake framework Assist project teams for their queries on DataLake tables InvestigateFix if there are any datamismatch issues identified or raised by the project teams Coordinate with project teams for any datalake related changes and take initiatives like standardization of naming standards Any adhoc table refresh requests and assist project teams for their testing validating during GO Live We coordinate and help to fix the issues with data catalogue when it goes down Assist project teams for the approvals and access management related queries to the tables If there are any PVs to be loaded with huge data we load the data using TPT or with max_update_date and ensure for its success Coordinate with Teradata DBA team and Teradata support team for any TPT related queries issues Worked on Scala 210 jobs using Spark 160for data processing using RDDs and Dataframe API Performance tuning of Spark and Sqoop Job 2 Ingestion Activities Coordinate with project teams for their ingestion requests Review all the metadata provided by them database credentials Incremental columns and uniqueness of merge keys Discuss with Architect team for their approval to proceed with metadata creation Create metadata and get It reviewed by Architect team Load the tables take the audits and share it with the teams Scheduling jobs in TES and validate for their success Ingested data from AWS cloud to Hadoop datalake 3 Enterprise data and Analytics In the process of automating Hadoop Involved in requirement gathering discussions with project teams Joint Application Design meetings Buildupdate user stories Validate requirements Review screen design prototypes test the functionalities participate in User Acceptance Testing and actively involved in GO live activities Environment CDH Hadoop HDFS MapReduce Hive Sqoop Pig Jira XML DB2 and QA Big Data Engineer Kohls South Digital Center Milpitas CA April 2015 to December 2016 Kohls Digital Center in Milpitas California puts Kohls in the fastpaced technology industry and in close proximity to key technology partners The creative incubator is dedicated to seek and implement the latest and greatest technology to revolutionize the customer experience Kohls is one of the leading retail chain markets Develop Support and handle fixes related to project tracks in onprem and GCP Involve in Business interactions client management and coordination with the team Responsibilities Being in Retail domain Online and stores data Need to be Agile Aggressive and focused team player in monitoring debugging and Issue fix Any delay may impact propensities for top trending As a center of Excellence team Involve in any of the application issues triageinvestigate them build and fix the issues Using GCP Console monitor dataproc cluster and jobs Stack Driver to monitor Dashboards and do a performance tuning and optimization of jobs which are memory intensive and provide L3 support for the applications in production environment DAS Data as a Service in GCP google storage buckets Monitor Azkaban jobs in onprem Hortonworks distribution and GCP Google Cloud Platform Investigate RCA and fix the issue for any failed jobs As part of Production Engineering team keep the environment healthy 247 Monitor Azkaban Ambari splunk and Net Diagnostics for Hbase Timeouts Propensities GCP Stackdriver for monitoring logging compute engine and dataproc Involve in discussion with project teams understand the issue Investigate RCA and fix the issue Handle production issues assigned in JIRA tasks Incidents Requests through Remedy Involve in discussions take initiatives and work with Application teams for the smooth flow of projects Involve in all the Technical Discussions and scrum meetings Reviews project documents received from other Technical Specialists Business Technical Specialists and Project Managers to ensure quality completeness and adherence to documentation standard Environment Hadoop Java HDFS JiraAzkaban MapReduce Hive Sqoop Pig XML ETL DB2 and QA JavaHadoop Developer Samsung America Inc San Diego CA February 2013 to March 2015 Samsungs solutions build on the companys 35 years experience in developing and commercializing some of the worlds most popular communication services Based on its wide range of fieldexperience in any technology Samsung has already successfully partnered with dozens of mobile service providers around the world Responsibilities Installation configuration of a Hadoop cluster along with Hive Developed Map Reduce application using Hadoop Map reduce programming a framework for processing Large data sets in parallel across the Hadoop cluster for preprocessing Developed the code for Importing and exporting data into HDFS and Hive using Sqoop Responsible for writing Hive Queries for analyzing data in Hive warehouse using Hive Query Language HQL Responsible for designing Front end system using HTML CSS JSP Servlets and Ajax Transformed web application into compatible Mobile Tablet application by designing responsive designs using HTML CSS Used LDAP for user Authentication and authorization Created Stored Procedures Views Cursors and functions to support application Involved in defining job flows using Oozie for scheduling jobs to manage apache Hadoop jobs by directed Acyclic graph DAG of actions with control flows Developing Hive User Defined Functions in java compiling them into jars and adding them to the HDFS and executing them with Hive Queries Experienced in managing and reviewing Hadoop log files Responsible to manage data coming from different sources Assisted in monitoring the Hadoop cluster using Ganglia tool Dealing with high volume of data in the cluster Tested and reported defects in an Agile Methodology perspective Consolidate all defects report it to PMLeads for prompt fixes by development teams and drive it to closure Installed and configured Hadoop Cluster 3 Node Cluster in fully distributed mode Installed hadoop ecosystemsHive Pig Sqoop HBase Oozie on top of hadoop cluster Importing data from Oracle to HDFS Hive for analytical purpose Analyzing imported data in HDFS Hive using HiveQL and custom Map Reduce programs in Java Environment Java CDH Hadoop HDFS Map Reduce Hive and Sqoop JAVA Developer Value Labs July 2009 to December 2012 Project Description Eorder management project is part of initiatives to harness and leverage on the advancement of IT for better Health It will enable medical practitioners to electronically order supplies prescribe medicine and requisite services such as laboratory radiology referral and other clinical and therapy services Responsibilities Responsible and active in the analysis design implementation and deployment of full software development lifecycle SDLC of the project Designed and developed user interface using JSP HTML and JavaScript Developed JSP Custom Tag Libraries for Tree Structure and Grid using Pagination Logics Worked extensively with JSPs and Servlets to accommodate all presentation customizations on the front endUsed Building tools like Maven to build package test and deploy application in the application server Developed Struts action classes action forms and performed action mapping using Struts framework and Performed data validation in form beans and action classes Extensively used Struts framework as the controller to handle subsequent client requests and invoke the Model based upon user requests Defined the search criteria and pulled out the record of the customer from the database Make the required changes and save the updated record back to the database Validated the fields of user registration screen and login screen by writing JavaScript validations Involved in developing and coding the Interfaces and classes required for the application and created appropriate relationships between the system classes and the interfaces provided Developed build and deployed scripts using Apache ANT to customize WAR and EAR files Used DAO and JDBC for database access Developed stored procedures and triggers using PLSQL to calculate and update the tables to implement business logic Used SVN to maintain source and version management Using JIRA to manage the issuesproject work flow Involved in peer code reviews and performed integration testing of the modules Followed coding and documentation standards Involved in postproduction support and maintenance of the application Environment Oracle Java Struts Servlets HTML XML SQL J2EE JUnit Tomcat PLSQL JIRA SVN Education Masters Skills Ambari Hdfs Sqoop Hbase Apache kafka Kafka Flume Hadoop Teradata Apache spark C Hadoop Hbase Hive Javascript Json Perl Pig Scripting Zookeeper Additional Information Technical Skills Hadoop Ecosystem Development HDFS MapReduce Hive Pig TES HBase Sqoop Zookeeper Spark MCS Azkaban Ambari Hadoop Distribution Framework MapR HortonWorks Cloud Technologies Google Cloud Platform Languages Java C C Technologies Scripting Shell Script Perl JavaScript and PowerShell Hadoop Ingestion Apache Sqoop Apache Kafka Apache spark Apache Flume Storm Database Mongo Database Oracle 10g Oracle 11g MySQL Teradata Hbase Netezza Operating Systems Linux Unix Windows Development Tools Eclipse Putty Tectia Java Technologies JSONJDBCAJAX",
    "unique_id": "9eb26a0b-294c-48bc-aca7-7f6cb4c5c8c8"
}