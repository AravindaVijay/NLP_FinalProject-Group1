{
    "clean_data": "Sr Big DataHadoop Developer Sr Big DataHadoop span lDeveloperspan Sr Big DataHadoop Developer VF Corporation Greensboro NV Greensboro NC Over 9 years of working experience as a Big DataHadoop Developer in designed and developed various applications like big data Hadoop JavaJ2EE opensource technologies Strong development skills in Hadoop HDFS Map Reduce Hive Sqoop HBase with solid understanding of Hadoop internals Experience in Programming and Development of java modules for an existing web portal based in Java using technologies like JSP Servlets JavaScript and HTML SOA with MVC architecture Expertise in ingesting real timenear real time data using Flume Kafka Storm Good knowledge of NO SQL databases like Mongo DB Cassandra and HBase Excellent knowledge of Hadoop Architecture and various components such as HDFS Job Tracker Task Tracker Name Node Data Node and MRA and MRv2 YARN Expertise in writing Hadoop Jobs to analyze data using MapReduce Apache Crunch Hive Pig and SOLR Splunk Hands on experience in installing configuring and using Apache Hadoop ecosystem components like Hadoop Distributed File System HDFS MapReduce Pig Hive HBase Apache Crunch Zookeeper Scoop Hue Scala AVRO Strong Programming Skills in designing and implementing of multitier applications using Java J2EE JDBC JSP JSTL HTML CSS JSF Struts JavaScript Servlets POJO EJB XSLT JAXB Extensive experience in SOAbased solutions Web Services Web API WCF SOAP including Restful APIs services Good Knowledge in Amazon Web Service AWS concepts like EMR and EC2 web services which provides fast and efficient processing of Teradata Big Data Analytics Experience working on EC2 Elastic Compute Cloud cluster instances setup data buckets on S3 Simple Storage Service set EMR Elastic MapReduce Experienced in collection of Log Data and JSON data into HDFS using Flume and processed the data using HivePig Expertise in developing a simple web based application using J2EE technologies like JSP Servlet and JDBC Work Extensively in Core Java Struts2 JSF22 Spring31 Hibernate Servlets JSP and Handson experience with PLSQL XML and SOAP In depth understandingknowledge of Hadoop Architecture and various components such as HDFS Job Tracker Task Tracker NameNode DataNode Well versed working with Relational Database Management Systems as Oracle 9i12c MS SQL MySQL Server Hands on experience in working on XML suite of technologies like XML XSL XSLT DTD XML Schema SAX DOM JAXB Hands on experience in advanced BigData technologies like Spark Ecosystem Spark SQL MLlib SparkR and Spark Streaming Kafka and Predictive analytics Knowledge of the software Development Life Cycle SDLC Agile and Waterfall Methodologies Experienced on applications using Java python and UNIX shell scripting Experience in consuming Web services with Apache Axis using JAXRSREST APIs Experienced in building tool Maven ANT and logging tool Log4J Experience in working with Web Servers like Apache Tomcat and Application Servers like IBM Web Sphere and JBOSS Good knowledge of NoSQL databases such as HBase MongoDB and Cassandra Experience in working with Eclipse IDE Net Beans and Rational Application Developer Experienced in Apache Flume for collecting aggregating and moving huge chunks of data from various sources such as web server telnet sources etc Extensively designed and executed SQL queries in order to ensure data integrity and consistency at the backend Experienced in working with different scripting technologies like Python UNIX shell scripts Strong experienced in working with UNIXLINUX environments writing shell scripts Expertise with frameworks like Angular JS jQuery in web presentation layer with servlets JSP Spring MVC at the web controller layer Experienced in deploying J2EE applications on Apache Tomcat web server and WebLogic WebSphere JBoss application server Technical ToolsTechnology Hadoop Ecosystem Hadoop2725 MapReduce Sqoop Hive Oozie Pig HDFS124 Zookeeper Flume Impala Spark20202 Storm Hadoop Cloudera Horton works and Pivotal Big Data Platforms Horton works Cloudera Amazon AWS and Apache Databases NOSQL Databases Oracle12c11g MYSQL Microsoft SQL Server20162014 MongoDB HBase and Cassandra Work Experience Sr Big DataHadoop Developer VF Corporation Greensboro NV May 2017 to Present Responsibilities Worked as a Sr BigHadoop Developer for providing solutions for big data problem Involved in full life cycle of the project from Design Analysis logical and physical architecture modelling development Implementation testing Utilized SDLC Methodology to help manage and organize a team of developers with regular code review sessions Involved in Agile methodologies daily scrum meetings spring planning Developed Pig Latin scripts for replacing the existing legacy process to the Hadoop and the data is fed to AWS S3 Worked on MongoDB by using CRUD Create Read Update and Delete Indexing Replication and Sharding features Created Talend jobs to read messages from Amazon AWS SQS queues download files from AWS S3 buckets Developed MapReduce YARN jobs for cleaning accessing and validating the data Worked on analyzing Hadoop cluster and different Big Data Components including Pig Hive Spark HBase Kafka Elastic Search database and SQOOP Written Hive jobs to parse the logs and structure them in tabular format to facilitate effective querying on the log data Wrote MapReduce jobs to filter and parse inventory data which was stored in the HDFS Configured Flume Hive Pig Sqoop and Oozie on the Hadoop cluster for data pipelining Imported and exported data into the HDFS from the Oracle database using Sqoop Integrated MapReduce with Cassandra to import bulk amount of logged data Converted ETL operations to the Hadoop system using Hive transformations and functions Conducted streaming jobs with basic Python to process terabytes of formatted data for machine learning purposes Used Flume to collect aggregate and store the web log data and loaded it into the HDFS Implemented Partitioning Dynamic Partitions Buckets in Hive Developed custom and Pig UDFs for product specific needs Implemented and configured workflows using Oozie to automate jobs Performed Hadoop cluster management and configuration of multiple nodes on AWS Involved in creating buckets to store the data in AWS and stored the data repository for future needs and reusability Worked along with Tableau developers to help performance tune the visualizations graphsanalytics Involved in the cluster coordination services through Zookeeper Participated in the managing and reviewing of the Hadoop log files Used Elastic Search MongoDB for storing and querying the offers and nonoffers data Created custom UDFsfor Spark and Kafkaprocedure for some of nonworking functionalities in custom UDF into Scala in production environment Import the data from different sources like HDFSHBase into Spark RDD and developed a data pipeline using Kafka and Storm to store data into HDFS Worked independently with Cloudera support and Hortonworks support for any issueconcerns with Hadoop cluster Responsible for building scalable distributed data solutions using Hadoop Cloudera Used Spark streaming to receive real time data from the Kafka and store the stream data to HDFS using Scala and NoSQL databases such as HBase and Cassandra Implementing Hadoop Security on Hortonworks Cluster using Kerberos and Twoway SSL Worked with teams in setting up AWS EC2 instances by using different AWS services like S3 EBS Elastic Load Balancer and Auto scaling groups VPC subnets and CloudWatch Implemented Hortonworks NiFi and recommended solution to inject data from multiple data sources to HDFS and Hiveusing NiFi Developed Restful web services using JAXRS and used DELETE PUT POST GET HTTP methods Created scalable and highperformance web services for data tracking and done Highspeed querying Used Java Messaging Services JMS for reliable and asynchronous exchange of important information such as payment status report on IBM WebSphere MQ messaging system Executed Hive queries on Parquet tables stored in Hive to perform data analysis to meet the business requirements Worked on migrating MapReduce programs into Spark transformations using Spark and Scala Developed NIFI flow to move data from different sources to HDFS and from HDFS to S3 bucketsIn current project Created and maintained various Shell and Python scripts for automating various processes and optimized MapReduce code pig scripts and performance tuning and analysis Worked on Oozie workflow engine for job scheduling Involved in Unit testing and delivered Unit test plans and results documents Involved with ingesting data received from various providers on HDFS for big data operations Wrote MapReduce jobs to perform big data analytics on ingested data using Java API Wrote MapReduce in Ruby using Hadoop Streaming to implement various functionalities Performed transformations cleaning and filtering on imported data using Hive Map Reduce and loaded final data into HDFS Conducted meetings with data analysts with basic Python and wrangled data for data repositories Environment Hadoop 30 Java MapReduce AWS HDFS Scala 212 Python 37 MongoDB 40 Spark 23 Hive 23 Pig 017 Linux XML Cloudera CDH45 Distribution Oracle 12c PLSQL EC2 Apache Flume 18 Zookeeper 34 Cassandra 311 Hortonworks Elastic search IBM WebSphere Big Data Engineer MFS Investments Boston MA January 2016 to April 2017 Responsibilities Architected Designed and Developed Business applications and Data marts to facilitate the reporting Involved in all phases of Software Development Life Cycle SDLC and Worked on all activities related to the development implementation and support for Hadoop Performed Requirements gathering Analysis Design Code development Testing using Agile methodologies Primary responsibilities include building scalable distributed data solutions using Hadoop ecosystem Worked on Hortonworks Data Platform Hadoop distribution for data querying using Hive to store and retrieve data Implemented Hive optimized joins to gather data from different sources and run adhoc queries on them Performed custom aggregate functions using Spark SQL and performed interactive querying Coordination with Hortonworks development and the operations team on the platform level issues Extensively worked on creating combiners partitioning and distributed cache to improve performance of MapReduce jobs Worked on Spark SQL and Data frames for faster execution of Hive queries using Spark Sql Context Used Sqoop transfer data between databases and HDFS and used Kafka to stream the log data from servers Used Pig to perform data transformations event joins filter and some preaggregations before storing the data onto HDFS Implemented different analytical algorithms using MapReduce programs to apply on top of HDFS data Worked on MongoDB database concepts such as locking transactions indexes sharding replication and schema design Implemented read references in MongoDB replica set Used Apache Tez for processing data and storing it in MongoDB Write concern in MongoDB to avoid loss of data during system failures Created HBase tables to load large sets of structured semistructured and unstructured data coming from Unix NoSQL and a variety of portfolios Extensively performed CRUD operations like put get scan delete update etc on HBase database Wrote Hive Generic UDFs to perform business logic operations at table level Developed workflow in Oozie to automate the tasks of loading the data into HDFS and preprocessing with Pig Hive and Sqoop Analyzed large amounts of data sets to determine optimal way to aggregate and report on it Used Hive join queries to join multiple tables of a source system and load them into Elastic Search Tables Used Apache Kafka as messaging system to load log data data from applications into HDFS system Developed POC using Scala and deployed on the Yarn cluster compared the performance of Spark with Hive and SQL Involved in converting Hive queries into Spark transformations using Spark RDDs Python and Scala Worked on various file formats and compression Text Avro Parquet file formats snappy bz2 gzip compression Implemented test scripts to support test driven development and continuous integration Scheduling Cron jobs for file system check using fsck and wrote shell scripts to generate alerts Data scrubbing and processing with Oozie Loading the analyzed Hive data into NOSQL databases like HBase MongoDB Provide Technical support for the Research in Information Technology program Manage and upgrade Linux and OS X server systems Responsible for installation configurations and management for Linux Systems Environment Hadoop 30 Java MapReduce HDFS Hive 23 Pig 017 Sqoop 14 Flume 18 Python 37 Spark 23 Impala Scala Kafka Shell Scripting Eclipse Cloudera MySQL Talend Cassandra 311 Sr JavaHadoop Developer Optum Eden Prairie MN February 2014 to December 2015 Responsibilities Involved in analysis design testing phases and responsible for documenting technical specifications Worked as part of the Agile Application Architecture A3 development team responsible for setting up the architectural components for different layers of the application Involved in end to end data processing like ingestion processing and quality checks and splitting Real time streaming the data using Spark Streaming with Kafka Developed Spark scripts by using Scala as per the requirement Load the data into Spark RDD and performed inmemory data computation to generate the output response Performed different types of transformations and actions on the RDD to meet the business requirements Extremely used plain JavaScript and JQuery JavaScript Library to do the client side validations Developed a data pipeline using Kafka Spark and Hive to ingest transform and analyzing data Developed Pig Scripts Pig UDFs and Hive Scripts Hive UDFs to analyze HDFS data Used Sqoop to export data from HDFS to RDBMS Performed ETL using Talend Worked on Hadoop eco system components HDFS MapReduce Hive Pig Sqoop and HBase Designed developed web based GUI architecture using HTML CSS AJAX JQuery AngularJS and JavaScript Developed Map Reduce programs for some refined queries on big data Involved in loading data from UNIX file system to HDFS Used Pig as ETL tool to do transformations event joins and some preaggregations before storing the data onto HDFS Worked in the cluster disaster recovery plan for the Hadoop cluster by implementing the cluster data backup in AmazonS3 buckets Implemented SparkSQL to access Hive tables into Spark for faster processing of data Extracted the data from Databases into HDFS using Sqoop Handled importing of data from various data sources performed transformations using Hive PIG and loaded data into HDFS Used PIG predefined functions to convert the fixed width file to delimited file Used HIVE join queries to join multiple tables of a source system and load them into Elastic Search Tables Manage and review Hadoop log files Implemented lambda architecture as a solution Adept at understanding Partitions bucketing concepts managed and created external tables in Hive to optimize performance Written Hadoop Jobs for analyzing data using HiveQL Queries Pig Latin Data flow language and custom MapReduce programs in Java Used Hadoop streaming jobs to process terabytes data in Hive Developed workflow in Oozie to automate the tasks of loading the data into HDFS and preprocessing with Pig Created reports for the BI team using Sqoop to import data into HDFS and Hive Responsible for the designing coding and developed the application in J2EE using MVC architecture Configured development environment using WebSphere application server for developers integration testing Used Web services for sending and getting data from different applications using SOAP messages Used ANT scripts to build the application and deployed on WebSphere Application Server Used JUnit framework for Unit testing of application Environment CDH Hadoop 28 HDFS MapReduce Yarn Hive 21 PIG 016 Oozie Sqoop 12 Linux Shell scripting Java SBT Amazon S3 JIRA Git Stash HDFS Eclipse SQL Oracle 11g Sr JavaJ2EE Developer Exchange Bank Santa Rosa CA October 2011 to January 2014 Responsibilities Responsible for system analysis design and development using J2EE architecture Actively participated in requirements gathering analysis design and testing phases Developed the application using Spring Framework that leverages classical Model View Controller MVC architecture Involved in Software Development Life cycle starting from requirements gathering and performed OOA and OOD Used Spring JDBC to execute database queries Created row mappers and query classes for DB operations Created a Transaction History Web Service using SOAP that is used for internal communication in the workflow process Designed and created components for companys object framework using best practices and design Patterns such as ModelViewController MVC Used DOM and DOM Functions using Firefox and IE Developer Tool bar for IE Debugged the application using Firebug to traverse the documents Involved in writing SQL Queries Stored Procedures and used JDBC for database connectivity with MySQL Server Developed the presentation layer using CSS and HTML taken from Bootstrap to develop for browsers Did core Java coding using JDK 13 Eclipse Integrated Development Environment IDE clear case and ANT Used Spring Core and Springweb framework Created a lot of classes for backend Involved in developing web pages using HTML and JSP Exposed business functionality to external systems Interoperable clients using Web Services WSDLSOAP Apache Axis Developed POJO classes and writing Hibernate query language HQL queries Used PLSQL for queries and stored procedures in SQL as the backend RDBMS Involved in the Analysis and Design of the frontend and middle tier using JSP Servlets and Ajax Implemented Spring IOC or Inversion of Control by way of Dependency Injection where a Factory class was written for creating and assembling the objects Implemented modules using Core Java APIs Java collection Threads XML and integrating the modules and used SOAP for Web Services by exchanging XML data between applications over HTTP Created EJB JPA and Hibernate component for the application Implemented XML parsers with SAX DOM and JAXB XML Parser Libraries to Modify User view of Products and Product information in Customized view with XML XSD XSTL in HTML XML PDF formats Established continuous integration with JIRA Jenkins Developed data mapping to create a communication bridge between various application interfaces using XML and XSL Used Hibernate to manage Transactions update delete along with writing complex SQL and HQL queries Used Microsoft VISIO for developing Use Case Diagrams Sequence Diagrams and Class Diagrams in the design phase Developed Restful Web services client to consume JSON messages using Spring JMS configuration Developed the message listener code Providing production support which includes handling tickets providing resolution Used BMC Remedy Tool to add issues update resolutions Create database objects like tables sequences views triggers stored procedures functions packages Used Maven as the build tool and Tortoise SVN as the Source version controller Environment Core Java UNIX J2EE XML Schemas XML JavaScript 2014 JSON CSS3 HTML5 spring Hibernate Design Patterns Servlets JUnit JMS MySQL Restful Web Services SOAP Tortoise SVN 15 Web Services Apache Tomcat 80 Windows XP JavaJ2EE Developer ITC Infotech July 2009 to September 2011 Responsibilities As a Java Developer involved in backend and frontend developing team Designed and implemented the User Interface using JavaScript HTML XHTML XML CSS JSP and AJAX Wrote web service client for tracking operations for the orders which is accessing web services API and utilizing in our web application Implemented data archiving and persistence of report generation metadata using Hibernate by creating Mapping files POJO classes and configuring hibernate to set up the data sources Developed Spring framework DAO Layer with JPA and EJB3 in Imaging Data model and Doc Import The business logic is developed using J2EE framework and deployed components on Application server where Eclipse was used for component building Actively involved in deployment EJB service jars Application war files in WebLogic Application server Developed GUI screens for login registration edit account forgot password and change password using Struts Used JUnit framework for unit testing of application and JUL logging to capture the log that includes runtime exceptions Wrote various SQL stored procedures and SQL commands to retrieve the data from the SQL server Used Spring MVC Framework to develop Action classes and Controllers along with validation framework and annotations Writing SQL queries for data access and manipulation using Oracle SQL Developer Developed Session Bean to encapsulate the business logic and Model and DAO classes using Hibernate Designed and coded JAXWS based Web Services used to access external financial information Implemented EJB Components using State less Session Bean and State full session beans Used spring framework with the help of Spring Configuration files to create the beans needed and injected dependency using Dependency Injection Utilized JPA for ObjectRelational Mapping purposes for transparent persistence onto the Oracle database Involved in creation of Test Cases for JUnit Testing Used Oracle as Database and used Toad for queries execution and also involved in writing SQL scripts PLSQL code for procedures and functions Used SOAP as a XMLbased protocol for web service operation invocation Packaged and deployed the application in IBM WebSphere Application server in different environments like Development testing etc Used Log4J to validate functionalities and JUnit for unit testing Environment Java Servlets JSP Struts 10 Hibernate 31 spring core Spring JDBC HTML JavaScript 2012 AJAX XSL XSLT XSD schema XML Beans Web logic Oracle9i Education Bachelors",
    "entities": [
        "Spring Framework",
        "Design Analysis",
        "Amazon AWS SQS",
        "GUI",
        "Imaging Data",
        "BI",
        "HDFS",
        "UNIX",
        "Spring Configuration",
        "AWS S3 Worked",
        "SQL Oracle",
        "UDFsfor Spark",
        "Written Hadoop Jobs",
        "Talend Worked on Hadoop",
        "HTTP Created EJB JPA",
        "JAXRSREST",
        "Modify User",
        "the Research in Information Technology",
        "ANT Used Spring Core",
        "AJAX Wrote",
        "S3 Simple Storage Service",
        "IBM",
        "RDD",
        "Hadoop",
        "SAX DOM",
        "SOAP",
        "XML",
        "Oozie Sqoop",
        "Spring MVC Framework",
        "Spark Ecosystem Spark",
        "NOSQL",
        "CRUD Create Read Update",
        "Developed Spring",
        "JAXWS",
        "Software Development Life Cycle SDLC",
        "HTML CSS AJAX",
        "Hive Developed",
        "parse inventory",
        "JSP Servlet",
        "Shell",
        "JUnit",
        "State",
        "HBase",
        "Involved in Software Development Life",
        "Test Cases for JUnit Testing Used Oracle",
        "HDFS Conducted",
        "Hive PIG",
        "RDBMS Performed ETL",
        "Spark with",
        "WebSphere",
        "SQL Involved",
        "Implemented Hive",
        "Oracle 9i12c MS",
        "Conducted",
        "Developed",
        "SparkSQL",
        "Hibernate Servlets JSP",
        "Kerberos",
        "AWS S3",
        "Node Data",
        "SOAbased",
        "UNIXLINUX",
        "HivePig Expertise",
        "JUL",
        "Sr JavaJ2EE Developer Exchange Bank",
        "Sqoop Analyzed",
        "DELETE PUT POST GET HTTP",
        "Sr Big DataHadoop Developer Sr Big DataHadoop",
        "Struts Used JUnit",
        "Hortonworks Elastic",
        "Hortonworks",
        "Technical ToolsTechnology Hadoop Ecosystem",
        "Teradata Big Data Analytics",
        "Firebug",
        "JSP",
        "Created a Transaction History Web Service",
        "Customized",
        "Apache Tomcat and Application Servers",
        "Spark Streaming",
        "MapReduce Apache Crunch Hive Pig",
        "DOM",
        "Hadoop Performed Requirements gathering Analysis Design Code",
        "BigData",
        "Maven ANT",
        "MVC",
        "Converted ETL",
        "Developed MapReduce YARN",
        "Spark",
        "AWS Involved",
        "EJB",
        "Log Data",
        "DOM Functions",
        "Hadoop Distributed File System HDFS MapReduce Pig Hive HBase",
        "Hadoop Jobs",
        "API",
        "Web Services",
        "Sqoop",
        "HIVE",
        "HTML XML",
        "PDF",
        "POJO",
        "Big Data Components",
        "Storm",
        "Created",
        "Pivotal Big Data Platforms",
        "AWS",
        "Hadoop Architecture",
        "HDFS Job Tracker Task Tracker NameNode DataNode",
        "PIG",
        "Doc Import",
        "HDFS Job Tracker Task Tracker",
        "HTML",
        "Oozie",
        "fed",
        "Created Talend",
        "SQL",
        "Spark Sql Context Used",
        "UDF",
        "Spark RDD",
        "SOLR Splunk Hands",
        "Cassandra Implementing Hadoop Security",
        "WebSphere Application Server Used",
        "Bootstrap",
        "EJB Components",
        "Development Life Cycle",
        "the Agile Application",
        "Cassandra Work",
        "Performed Hadoop",
        "Springweb",
        "Implemented XML",
        "JAXRS",
        "Kafkaprocedure",
        "Hive",
        "Dependency Injection Utilized JPA",
        "WebLogic Application",
        "WebLogic WebSphere JBoss",
        "Sr BigHadoop Developer",
        "Handson",
        "JIRA Jenkins Developed",
        "Hibernate Designed",
        "Zookeeper Participated",
        "Distribution Oracle",
        "Windows XP JavaJ2EE Developer ITC Infotech",
        "IBM WebSphere Application",
        "Ruby",
        "CRUD",
        "ETL",
        "JSP Spring",
        "CloudWatch Implemented Hortonworks NiFi",
        "Apache Hadoop",
        "Scheduling Cron",
        "VF Corporation Greensboro",
        "Performed",
        "XSLT",
        "Maven",
        "Impala",
        "Hibernate Design Patterns Servlets",
        "Spark SQL",
        "XML XSD XSTL",
        "Hadoop JavaJ2EE",
        "Created custom",
        "ANT",
        "Ajax Implemented",
        "JSP Servlets",
        "Microsoft",
        "IE Developer Tool",
        "Amazon Web Service AWS",
        "XSD",
        "Programming and Development",
        "MRA",
        "Linux Systems Environment Hadoop",
        "SVN",
        "Expertise",
        "Spark Streaming Kafka",
        "CSS",
        "Created HBase",
        "EJB3",
        "Imported",
        "Data",
        "MapReduce",
        "AmazonS3",
        "Provide Technical",
        "Inversion of Control",
        "Model",
        "NoSQL",
        "Tableau",
        "Application",
        "Oracle SQL Developer Developed Session Bean",
        "ObjectRelational Mapping",
        "Relational Database Management Systems",
        "Dependency Injection",
        "Used Elastic Search"
    ],
    "experience": "Experience in Programming and Development of java modules for an existing web portal based in Java using technologies like JSP Servlets JavaScript and HTML SOA with MVC architecture Expertise in ingesting real timenear real time data using Flume Kafka Storm Good knowledge of NO SQL databases like Mongo DB Cassandra and HBase Excellent knowledge of Hadoop Architecture and various components such as HDFS Job Tracker Task Tracker Name Node Data Node and MRA and MRv2 YARN Expertise in writing Hadoop Jobs to analyze data using MapReduce Apache Crunch Hive Pig and SOLR Splunk Hands on experience in installing configuring and using Apache Hadoop ecosystem components like Hadoop Distributed File System HDFS MapReduce Pig Hive HBase Apache Crunch Zookeeper Scoop Hue Scala AVRO Strong Programming Skills in designing and implementing of multitier applications using Java J2EE JDBC JSP JSTL HTML CSS JSF Struts JavaScript Servlets POJO EJB XSLT JAXB Extensive experience in SOAbased solutions Web Services Web API WCF SOAP including Restful APIs services Good Knowledge in Amazon Web Service AWS concepts like EMR and EC2 web services which provides fast and efficient processing of Teradata Big Data Analytics Experience working on EC2 Elastic Compute Cloud cluster instances setup data buckets on S3 Simple Storage Service set EMR Elastic MapReduce Experienced in collection of Log Data and JSON data into HDFS using Flume and processed the data using HivePig Expertise in developing a simple web based application using J2EE technologies like JSP Servlet and JDBC Work Extensively in Core Java Struts2 JSF22 Spring31 Hibernate Servlets JSP and Handson experience with PLSQL XML and SOAP In depth understandingknowledge of Hadoop Architecture and various components such as HDFS Job Tracker Task Tracker NameNode DataNode Well versed working with Relational Database Management Systems as Oracle 9i12c MS SQL MySQL Server Hands on experience in working on XML suite of technologies like XML XSL XSLT DTD XML Schema SAX DOM JAXB Hands on experience in advanced BigData technologies like Spark Ecosystem Spark SQL MLlib SparkR and Spark Streaming Kafka and Predictive analytics Knowledge of the software Development Life Cycle SDLC Agile and Waterfall Methodologies Experienced on applications using Java python and UNIX shell scripting Experience in consuming Web services with Apache Axis using JAXRSREST APIs Experienced in building tool Maven ANT and logging tool Log4J Experience in working with Web Servers like Apache Tomcat and Application Servers like IBM Web Sphere and JBOSS Good knowledge of NoSQL databases such as HBase MongoDB and Cassandra Experience in working with Eclipse IDE Net Beans and Rational Application Developer Experienced in Apache Flume for collecting aggregating and moving huge chunks of data from various sources such as web server telnet sources etc Extensively designed and executed SQL queries in order to ensure data integrity and consistency at the backend Experienced in working with different scripting technologies like Python UNIX shell scripts Strong experienced in working with UNIXLINUX environments writing shell scripts Expertise with frameworks like Angular JS jQuery in web presentation layer with servlets JSP Spring MVC at the web controller layer Experienced in deploying J2EE applications on Apache Tomcat web server and WebLogic WebSphere JBoss application server Technical ToolsTechnology Hadoop Ecosystem Hadoop2725 MapReduce Sqoop Hive Oozie Pig HDFS124 Zookeeper Flume Impala Spark20202 Storm Hadoop Cloudera Horton works and Pivotal Big Data Platforms Horton works Cloudera Amazon AWS and Apache Databases NOSQL Databases Oracle12c11 g MYSQL Microsoft SQL Server20162014 MongoDB HBase and Cassandra Work Experience Sr Big DataHadoop Developer VF Corporation Greensboro NV May 2017 to Present Responsibilities Worked as a Sr BigHadoop Developer for providing solutions for big data problem Involved in full life cycle of the project from Design Analysis logical and physical architecture modelling development Implementation testing Utilized SDLC Methodology to help manage and organize a team of developers with regular code review sessions Involved in Agile methodologies daily scrum meetings spring planning Developed Pig Latin scripts for replacing the existing legacy process to the Hadoop and the data is fed to AWS S3 Worked on MongoDB by using CRUD Create Read Update and Delete Indexing Replication and Sharding features Created Talend jobs to read messages from Amazon AWS SQS queues download files from AWS S3 buckets Developed MapReduce YARN jobs for cleaning accessing and validating the data Worked on analyzing Hadoop cluster and different Big Data Components including Pig Hive Spark HBase Kafka Elastic Search database and SQOOP Written Hive jobs to parse the logs and structure them in tabular format to facilitate effective querying on the log data Wrote MapReduce jobs to filter and parse inventory data which was stored in the HDFS Configured Flume Hive Pig Sqoop and Oozie on the Hadoop cluster for data pipelining Imported and exported data into the HDFS from the Oracle database using Sqoop Integrated MapReduce with Cassandra to import bulk amount of logged data Converted ETL operations to the Hadoop system using Hive transformations and functions Conducted streaming jobs with basic Python to process terabytes of formatted data for machine learning purposes Used Flume to collect aggregate and store the web log data and loaded it into the HDFS Implemented Partitioning Dynamic Partitions Buckets in Hive Developed custom and Pig UDFs for product specific needs Implemented and configured workflows using Oozie to automate jobs Performed Hadoop cluster management and configuration of multiple nodes on AWS Involved in creating buckets to store the data in AWS and stored the data repository for future needs and reusability Worked along with Tableau developers to help performance tune the visualizations graphsanalytics Involved in the cluster coordination services through Zookeeper Participated in the managing and reviewing of the Hadoop log files Used Elastic Search MongoDB for storing and querying the offers and nonoffers data Created custom UDFsfor Spark and Kafkaprocedure for some of nonworking functionalities in custom UDF into Scala in production environment Import the data from different sources like HDFSHBase into Spark RDD and developed a data pipeline using Kafka and Storm to store data into HDFS Worked independently with Cloudera support and Hortonworks support for any issueconcerns with Hadoop cluster Responsible for building scalable distributed data solutions using Hadoop Cloudera Used Spark streaming to receive real time data from the Kafka and store the stream data to HDFS using Scala and NoSQL databases such as HBase and Cassandra Implementing Hadoop Security on Hortonworks Cluster using Kerberos and Twoway SSL Worked with teams in setting up AWS EC2 instances by using different AWS services like S3 EBS Elastic Load Balancer and Auto scaling groups VPC subnets and CloudWatch Implemented Hortonworks NiFi and recommended solution to inject data from multiple data sources to HDFS and Hiveusing NiFi Developed Restful web services using JAXRS and used DELETE PUT POST GET HTTP methods Created scalable and highperformance web services for data tracking and done Highspeed querying Used Java Messaging Services JMS for reliable and asynchronous exchange of important information such as payment status report on IBM WebSphere MQ messaging system Executed Hive queries on Parquet tables stored in Hive to perform data analysis to meet the business requirements Worked on migrating MapReduce programs into Spark transformations using Spark and Scala Developed NIFI flow to move data from different sources to HDFS and from HDFS to S3 bucketsIn current project Created and maintained various Shell and Python scripts for automating various processes and optimized MapReduce code pig scripts and performance tuning and analysis Worked on Oozie workflow engine for job scheduling Involved in Unit testing and delivered Unit test plans and results documents Involved with ingesting data received from various providers on HDFS for big data operations Wrote MapReduce jobs to perform big data analytics on ingested data using Java API Wrote MapReduce in Ruby using Hadoop Streaming to implement various functionalities Performed transformations cleaning and filtering on imported data using Hive Map Reduce and loaded final data into HDFS Conducted meetings with data analysts with basic Python and wrangled data for data repositories Environment Hadoop 30 Java MapReduce AWS HDFS Scala 212 Python 37 MongoDB 40 Spark 23 Hive 23 Pig 017 Linux XML Cloudera CDH45 Distribution Oracle 12c PLSQL EC2 Apache Flume 18 Zookeeper 34 Cassandra 311 Hortonworks Elastic search IBM WebSphere Big Data Engineer MFS Investments Boston MA January 2016 to April 2017 Responsibilities Architected Designed and Developed Business applications and Data marts to facilitate the reporting Involved in all phases of Software Development Life Cycle SDLC and Worked on all activities related to the development implementation and support for Hadoop Performed Requirements gathering Analysis Design Code development Testing using Agile methodologies Primary responsibilities include building scalable distributed data solutions using Hadoop ecosystem Worked on Hortonworks Data Platform Hadoop distribution for data querying using Hive to store and retrieve data Implemented Hive optimized joins to gather data from different sources and run adhoc queries on them Performed custom aggregate functions using Spark SQL and performed interactive querying Coordination with Hortonworks development and the operations team on the platform level issues Extensively worked on creating combiners partitioning and distributed cache to improve performance of MapReduce jobs Worked on Spark SQL and Data frames for faster execution of Hive queries using Spark Sql Context Used Sqoop transfer data between databases and HDFS and used Kafka to stream the log data from servers Used Pig to perform data transformations event joins filter and some preaggregations before storing the data onto HDFS Implemented different analytical algorithms using MapReduce programs to apply on top of HDFS data Worked on MongoDB database concepts such as locking transactions indexes sharding replication and schema design Implemented read references in MongoDB replica set Used Apache Tez for processing data and storing it in MongoDB Write concern in MongoDB to avoid loss of data during system failures Created HBase tables to load large sets of structured semistructured and unstructured data coming from Unix NoSQL and a variety of portfolios Extensively performed CRUD operations like put get scan delete update etc on HBase database Wrote Hive Generic UDFs to perform business logic operations at table level Developed workflow in Oozie to automate the tasks of loading the data into HDFS and preprocessing with Pig Hive and Sqoop Analyzed large amounts of data sets to determine optimal way to aggregate and report on it Used Hive join queries to join multiple tables of a source system and load them into Elastic Search Tables Used Apache Kafka as messaging system to load log data data from applications into HDFS system Developed POC using Scala and deployed on the Yarn cluster compared the performance of Spark with Hive and SQL Involved in converting Hive queries into Spark transformations using Spark RDDs Python and Scala Worked on various file formats and compression Text Avro Parquet file formats snappy bz2 gzip compression Implemented test scripts to support test driven development and continuous integration Scheduling Cron jobs for file system check using fsck and wrote shell scripts to generate alerts Data scrubbing and processing with Oozie Loading the analyzed Hive data into NOSQL databases like HBase MongoDB Provide Technical support for the Research in Information Technology program Manage and upgrade Linux and OS X server systems Responsible for installation configurations and management for Linux Systems Environment Hadoop 30 Java MapReduce HDFS Hive 23 Pig 017 Sqoop 14 Flume 18 Python 37 Spark 23 Impala Scala Kafka Shell Scripting Eclipse Cloudera MySQL Talend Cassandra 311 Sr JavaHadoop Developer Optum Eden Prairie MN February 2014 to December 2015 Responsibilities Involved in analysis design testing phases and responsible for documenting technical specifications Worked as part of the Agile Application Architecture A3 development team responsible for setting up the architectural components for different layers of the application Involved in end to end data processing like ingestion processing and quality checks and splitting Real time streaming the data using Spark Streaming with Kafka Developed Spark scripts by using Scala as per the requirement Load the data into Spark RDD and performed inmemory data computation to generate the output response Performed different types of transformations and actions on the RDD to meet the business requirements Extremely used plain JavaScript and JQuery JavaScript Library to do the client side validations Developed a data pipeline using Kafka Spark and Hive to ingest transform and analyzing data Developed Pig Scripts Pig UDFs and Hive Scripts Hive UDFs to analyze HDFS data Used Sqoop to export data from HDFS to RDBMS Performed ETL using Talend Worked on Hadoop eco system components HDFS MapReduce Hive Pig Sqoop and HBase Designed developed web based GUI architecture using HTML CSS AJAX JQuery AngularJS and JavaScript Developed Map Reduce programs for some refined queries on big data Involved in loading data from UNIX file system to HDFS Used Pig as ETL tool to do transformations event joins and some preaggregations before storing the data onto HDFS Worked in the cluster disaster recovery plan for the Hadoop cluster by implementing the cluster data backup in AmazonS3 buckets Implemented SparkSQL to access Hive tables into Spark for faster processing of data Extracted the data from Databases into HDFS using Sqoop Handled importing of data from various data sources performed transformations using Hive PIG and loaded data into HDFS Used PIG predefined functions to convert the fixed width file to delimited file Used HIVE join queries to join multiple tables of a source system and load them into Elastic Search Tables Manage and review Hadoop log files Implemented lambda architecture as a solution Adept at understanding Partitions bucketing concepts managed and created external tables in Hive to optimize performance Written Hadoop Jobs for analyzing data using HiveQL Queries Pig Latin Data flow language and custom MapReduce programs in Java Used Hadoop streaming jobs to process terabytes data in Hive Developed workflow in Oozie to automate the tasks of loading the data into HDFS and preprocessing with Pig Created reports for the BI team using Sqoop to import data into HDFS and Hive Responsible for the designing coding and developed the application in J2EE using MVC architecture Configured development environment using WebSphere application server for developers integration testing Used Web services for sending and getting data from different applications using SOAP messages Used ANT scripts to build the application and deployed on WebSphere Application Server Used JUnit framework for Unit testing of application Environment CDH Hadoop 28 HDFS MapReduce Yarn Hive 21 PIG 016 Oozie Sqoop 12 Linux Shell scripting Java SBT Amazon S3 JIRA Git Stash HDFS Eclipse SQL Oracle 11 g Sr JavaJ2EE Developer Exchange Bank Santa Rosa CA October 2011 to January 2014 Responsibilities Responsible for system analysis design and development using J2EE architecture Actively participated in requirements gathering analysis design and testing phases Developed the application using Spring Framework that leverages classical Model View Controller MVC architecture Involved in Software Development Life cycle starting from requirements gathering and performed OOA and OOD Used Spring JDBC to execute database queries Created row mappers and query classes for DB operations Created a Transaction History Web Service using SOAP that is used for internal communication in the workflow process Designed and created components for companys object framework using best practices and design Patterns such as ModelViewController MVC Used DOM and DOM Functions using Firefox and IE Developer Tool bar for IE Debugged the application using Firebug to traverse the documents Involved in writing SQL Queries Stored Procedures and used JDBC for database connectivity with MySQL Server Developed the presentation layer using CSS and HTML taken from Bootstrap to develop for browsers Did core Java coding using JDK 13 Eclipse Integrated Development Environment IDE clear case and ANT Used Spring Core and Springweb framework Created a lot of classes for backend Involved in developing web pages using HTML and JSP Exposed business functionality to external systems Interoperable clients using Web Services WSDLSOAP Apache Axis Developed POJO classes and writing Hibernate query language HQL queries Used PLSQL for queries and stored procedures in SQL as the backend RDBMS Involved in the Analysis and Design of the frontend and middle tier using JSP Servlets and Ajax Implemented Spring IOC or Inversion of Control by way of Dependency Injection where a Factory class was written for creating and assembling the objects Implemented modules using Core Java APIs Java collection Threads XML and integrating the modules and used SOAP for Web Services by exchanging XML data between applications over HTTP Created EJB JPA and Hibernate component for the application Implemented XML parsers with SAX DOM and JAXB XML Parser Libraries to Modify User view of Products and Product information in Customized view with XML XSD XSTL in HTML XML PDF formats Established continuous integration with JIRA Jenkins Developed data mapping to create a communication bridge between various application interfaces using XML and XSL Used Hibernate to manage Transactions update delete along with writing complex SQL and HQL queries Used Microsoft VISIO for developing Use Case Diagrams Sequence Diagrams and Class Diagrams in the design phase Developed Restful Web services client to consume JSON messages using Spring JMS configuration Developed the message listener code Providing production support which includes handling tickets providing resolution Used BMC Remedy Tool to add issues update resolutions Create database objects like tables sequences views triggers stored procedures functions packages Used Maven as the build tool and Tortoise SVN as the Source version controller Environment Core Java UNIX J2EE XML Schemas XML JavaScript 2014 JSON CSS3 HTML5 spring Hibernate Design Patterns Servlets JUnit JMS MySQL Restful Web Services SOAP Tortoise SVN 15 Web Services Apache Tomcat 80 Windows XP JavaJ2EE Developer ITC Infotech July 2009 to September 2011 Responsibilities As a Java Developer involved in backend and frontend developing team Designed and implemented the User Interface using JavaScript HTML XHTML XML CSS JSP and AJAX Wrote web service client for tracking operations for the orders which is accessing web services API and utilizing in our web application Implemented data archiving and persistence of report generation metadata using Hibernate by creating Mapping files POJO classes and configuring hibernate to set up the data sources Developed Spring framework DAO Layer with JPA and EJB3 in Imaging Data model and Doc Import The business logic is developed using J2EE framework and deployed components on Application server where Eclipse was used for component building Actively involved in deployment EJB service jars Application war files in WebLogic Application server Developed GUI screens for login registration edit account forgot password and change password using Struts Used JUnit framework for unit testing of application and JUL logging to capture the log that includes runtime exceptions Wrote various SQL stored procedures and SQL commands to retrieve the data from the SQL server Used Spring MVC Framework to develop Action classes and Controllers along with validation framework and annotations Writing SQL queries for data access and manipulation using Oracle SQL Developer Developed Session Bean to encapsulate the business logic and Model and DAO classes using Hibernate Designed and coded JAXWS based Web Services used to access external financial information Implemented EJB Components using State less Session Bean and State full session beans Used spring framework with the help of Spring Configuration files to create the beans needed and injected dependency using Dependency Injection Utilized JPA for ObjectRelational Mapping purposes for transparent persistence onto the Oracle database Involved in creation of Test Cases for JUnit Testing Used Oracle as Database and used Toad for queries execution and also involved in writing SQL scripts PLSQL code for procedures and functions Used SOAP as a XMLbased protocol for web service operation invocation Packaged and deployed the application in IBM WebSphere Application server in different environments like Development testing etc Used Log4J to validate functionalities and JUnit for unit testing Environment Java Servlets JSP Struts 10 Hibernate 31 spring core Spring JDBC HTML JavaScript 2012 AJAX XSL XSLT XSD schema XML Beans Web logic Oracle9i Education Bachelors",
    "extracted_keywords": [
        "Sr",
        "Big",
        "DataHadoop",
        "Developer",
        "Sr",
        "Big",
        "DataHadoop",
        "span",
        "lDeveloperspan",
        "Sr",
        "Big",
        "DataHadoop",
        "Developer",
        "VF",
        "Corporation",
        "Greensboro",
        "NV",
        "Greensboro",
        "NC",
        "years",
        "working",
        "experience",
        "Big",
        "DataHadoop",
        "Developer",
        "applications",
        "data",
        "Hadoop",
        "JavaJ2EE",
        "opensource",
        "technologies",
        "development",
        "skills",
        "Hadoop",
        "HDFS",
        "Map",
        "Reduce",
        "Hive",
        "Sqoop",
        "HBase",
        "understanding",
        "Hadoop",
        "internals",
        "Experience",
        "Programming",
        "Development",
        "modules",
        "web",
        "portal",
        "Java",
        "technologies",
        "JSP",
        "Servlets",
        "JavaScript",
        "HTML",
        "SOA",
        "MVC",
        "architecture",
        "Expertise",
        "time",
        "data",
        "Flume",
        "Kafka",
        "Storm",
        "knowledge",
        "SQL",
        "Mongo",
        "DB",
        "Cassandra",
        "HBase",
        "Excellent",
        "knowledge",
        "Hadoop",
        "Architecture",
        "components",
        "HDFS",
        "Job",
        "Tracker",
        "Task",
        "Tracker",
        "Name",
        "Node",
        "Data",
        "Node",
        "MRA",
        "YARN",
        "Expertise",
        "Hadoop",
        "Jobs",
        "data",
        "MapReduce",
        "Apache",
        "Crunch",
        "Hive",
        "Pig",
        "SOLR",
        "Splunk",
        "Hands",
        "experience",
        "configuring",
        "Apache",
        "Hadoop",
        "ecosystem",
        "components",
        "Hadoop",
        "File",
        "System",
        "HDFS",
        "MapReduce",
        "Pig",
        "Hive",
        "HBase",
        "Apache",
        "Crunch",
        "Zookeeper",
        "Scoop",
        "Hue",
        "Scala",
        "AVRO",
        "Strong",
        "Programming",
        "Skills",
        "applications",
        "Java",
        "J2EE",
        "JDBC",
        "JSP",
        "JSTL",
        "HTML",
        "CSS",
        "JSF",
        "Struts",
        "JavaScript",
        "Servlets",
        "POJO",
        "EJB",
        "XSLT",
        "experience",
        "solutions",
        "Web",
        "Services",
        "Web",
        "API",
        "WCF",
        "SOAP",
        "APIs",
        "services",
        "Good",
        "Knowledge",
        "Amazon",
        "Web",
        "Service",
        "AWS",
        "concepts",
        "EMR",
        "EC2",
        "web",
        "services",
        "processing",
        "Teradata",
        "Big",
        "Data",
        "Analytics",
        "Experience",
        "EC2",
        "Elastic",
        "Compute",
        "Cloud",
        "cluster",
        "instances",
        "data",
        "buckets",
        "S3",
        "Simple",
        "Storage",
        "Service",
        "EMR",
        "Elastic",
        "MapReduce",
        "collection",
        "Log",
        "Data",
        "data",
        "HDFS",
        "Flume",
        "data",
        "HivePig",
        "Expertise",
        "web",
        "application",
        "J2EE",
        "technologies",
        "JSP",
        "Servlet",
        "JDBC",
        "Work",
        "Core",
        "Java",
        "Struts2",
        "JSF22",
        "Spring31",
        "Hibernate",
        "Servlets",
        "JSP",
        "Handson",
        "experience",
        "PLSQL",
        "XML",
        "depth",
        "understandingknowledge",
        "Hadoop",
        "Architecture",
        "components",
        "HDFS",
        "Job",
        "Tracker",
        "Task",
        "Tracker",
        "NameNode",
        "DataNode",
        "Relational",
        "Database",
        "Management",
        "Systems",
        "Oracle",
        "MS",
        "SQL",
        "MySQL",
        "Server",
        "Hands",
        "experience",
        "XML",
        "suite",
        "technologies",
        "XML",
        "XSL",
        "XSLT",
        "DTD",
        "XML",
        "Schema",
        "SAX",
        "DOM",
        "JAXB",
        "Hands",
        "experience",
        "BigData",
        "technologies",
        "Spark",
        "Ecosystem",
        "Spark",
        "SQL",
        "MLlib",
        "SparkR",
        "Spark",
        "Streaming",
        "Kafka",
        "Predictive",
        "analytics",
        "Knowledge",
        "software",
        "Development",
        "Life",
        "Cycle",
        "SDLC",
        "Agile",
        "Waterfall",
        "Methodologies",
        "applications",
        "Java",
        "python",
        "UNIX",
        "shell",
        "scripting",
        "Experience",
        "Web",
        "services",
        "Apache",
        "Axis",
        "JAXRSREST",
        "APIs",
        "building",
        "tool",
        "Maven",
        "ANT",
        "tool",
        "Log4J",
        "Experience",
        "Web",
        "Servers",
        "Apache",
        "Tomcat",
        "Application",
        "Servers",
        "IBM",
        "Web",
        "Sphere",
        "JBOSS",
        "knowledge",
        "NoSQL",
        "HBase",
        "MongoDB",
        "Cassandra",
        "Experience",
        "Eclipse",
        "IDE",
        "Net",
        "Beans",
        "Rational",
        "Application",
        "Developer",
        "Apache",
        "Flume",
        "chunks",
        "data",
        "sources",
        "web",
        "server",
        "telnet",
        "sources",
        "SQL",
        "queries",
        "order",
        "data",
        "integrity",
        "consistency",
        "backend",
        "scripting",
        "technologies",
        "Python",
        "UNIX",
        "shell",
        "scripts",
        "Strong",
        "UNIXLINUX",
        "environments",
        "shell",
        "scripts",
        "Expertise",
        "frameworks",
        "JS",
        "jQuery",
        "web",
        "presentation",
        "layer",
        "servlets",
        "JSP",
        "Spring",
        "MVC",
        "web",
        "controller",
        "layer",
        "J2EE",
        "applications",
        "Apache",
        "Tomcat",
        "web",
        "server",
        "WebLogic",
        "WebSphere",
        "JBoss",
        "application",
        "server",
        "Technical",
        "ToolsTechnology",
        "Hadoop",
        "Ecosystem",
        "Hadoop2725",
        "MapReduce",
        "Sqoop",
        "Hive",
        "Oozie",
        "Pig",
        "HDFS124",
        "Zookeeper",
        "Flume",
        "Impala",
        "Spark20202",
        "Storm",
        "Hadoop",
        "Cloudera",
        "Horton",
        "Big",
        "Data",
        "Platforms",
        "Horton",
        "Cloudera",
        "Amazon",
        "AWS",
        "Apache",
        "NOSQL",
        "g",
        "MYSQL",
        "Microsoft",
        "SQL",
        "Server20162014",
        "MongoDB",
        "HBase",
        "Cassandra",
        "Work",
        "Experience",
        "Sr",
        "Big",
        "DataHadoop",
        "Developer",
        "VF",
        "Corporation",
        "Greensboro",
        "NV",
        "May",
        "Present",
        "Responsibilities",
        "Sr",
        "BigHadoop",
        "Developer",
        "solutions",
        "data",
        "problem",
        "life",
        "cycle",
        "project",
        "Design",
        "Analysis",
        "architecture",
        "development",
        "Implementation",
        "testing",
        "SDLC",
        "Methodology",
        "team",
        "developers",
        "code",
        "review",
        "sessions",
        "methodologies",
        "meetings",
        "spring",
        "Developed",
        "Pig",
        "Latin",
        "scripts",
        "legacy",
        "process",
        "Hadoop",
        "data",
        "AWS",
        "S3",
        "MongoDB",
        "CRUD",
        "Create",
        "Read",
        "Update",
        "Delete",
        "Indexing",
        "Replication",
        "Sharding",
        "Talend",
        "jobs",
        "messages",
        "Amazon",
        "AWS",
        "SQS",
        "queues",
        "files",
        "AWS",
        "S3",
        "buckets",
        "MapReduce",
        "YARN",
        "jobs",
        "accessing",
        "data",
        "Hadoop",
        "cluster",
        "Big",
        "Data",
        "Components",
        "Pig",
        "Hive",
        "Spark",
        "HBase",
        "Kafka",
        "Elastic",
        "Search",
        "database",
        "SQOOP",
        "Written",
        "Hive",
        "jobs",
        "logs",
        "format",
        "querying",
        "log",
        "data",
        "Wrote",
        "MapReduce",
        "jobs",
        "inventory",
        "data",
        "HDFS",
        "Configured",
        "Flume",
        "Hive",
        "Pig",
        "Sqoop",
        "Oozie",
        "Hadoop",
        "cluster",
        "data",
        "data",
        "HDFS",
        "Oracle",
        "database",
        "Sqoop",
        "Integrated",
        "MapReduce",
        "Cassandra",
        "bulk",
        "amount",
        "data",
        "ETL",
        "operations",
        "Hadoop",
        "system",
        "Hive",
        "transformations",
        "functions",
        "streaming",
        "jobs",
        "Python",
        "terabytes",
        "data",
        "machine",
        "learning",
        "purposes",
        "Flume",
        "aggregate",
        "web",
        "log",
        "data",
        "Partitioning",
        "Dynamic",
        "Partitions",
        "Buckets",
        "Hive",
        "custom",
        "Pig",
        "UDFs",
        "product",
        "needs",
        "workflows",
        "Oozie",
        "jobs",
        "Performed",
        "Hadoop",
        "cluster",
        "management",
        "configuration",
        "nodes",
        "AWS",
        "buckets",
        "data",
        "AWS",
        "data",
        "repository",
        "needs",
        "reusability",
        "Tableau",
        "developers",
        "performance",
        "tune",
        "visualizations",
        "graphsanalytics",
        "cluster",
        "coordination",
        "services",
        "Zookeeper",
        "Participated",
        "managing",
        "reviewing",
        "Hadoop",
        "log",
        "files",
        "Elastic",
        "Search",
        "MongoDB",
        "offers",
        "nonoffers",
        "data",
        "custom",
        "UDFsfor",
        "Spark",
        "Kafkaprocedure",
        "functionalities",
        "custom",
        "UDF",
        "Scala",
        "production",
        "environment",
        "Import",
        "data",
        "sources",
        "HDFSHBase",
        "Spark",
        "RDD",
        "data",
        "pipeline",
        "Kafka",
        "Storm",
        "data",
        "HDFS",
        "Cloudera",
        "support",
        "Hortonworks",
        "support",
        "issueconcerns",
        "Hadoop",
        "cluster",
        "Responsible",
        "data",
        "solutions",
        "Hadoop",
        "Cloudera",
        "Spark",
        "streaming",
        "time",
        "data",
        "Kafka",
        "stream",
        "data",
        "HDFS",
        "Scala",
        "NoSQL",
        "HBase",
        "Cassandra",
        "Implementing",
        "Hadoop",
        "Security",
        "Hortonworks",
        "Cluster",
        "Kerberos",
        "Twoway",
        "SSL",
        "teams",
        "AWS",
        "EC2",
        "instances",
        "AWS",
        "services",
        "S3",
        "EBS",
        "Elastic",
        "Load",
        "Balancer",
        "Auto",
        "scaling",
        "groups",
        "subnets",
        "CloudWatch",
        "Hortonworks",
        "NiFi",
        "solution",
        "data",
        "data",
        "sources",
        "HDFS",
        "Hiveusing",
        "NiFi",
        "web",
        "services",
        "JAXRS",
        "DELETE",
        "PUT",
        "POST",
        "GET",
        "HTTP",
        "methods",
        "highperformance",
        "web",
        "services",
        "data",
        "tracking",
        "Highspeed",
        "Java",
        "Messaging",
        "Services",
        "JMS",
        "exchange",
        "information",
        "payment",
        "status",
        "report",
        "IBM",
        "WebSphere",
        "MQ",
        "system",
        "Hive",
        "queries",
        "Parquet",
        "tables",
        "Hive",
        "data",
        "analysis",
        "business",
        "requirements",
        "MapReduce",
        "programs",
        "Spark",
        "transformations",
        "Spark",
        "Scala",
        "NIFI",
        "flow",
        "data",
        "sources",
        "HDFS",
        "HDFS",
        "S3",
        "bucketsIn",
        "project",
        "Shell",
        "Python",
        "scripts",
        "processes",
        "MapReduce",
        "code",
        "pig",
        "scripts",
        "performance",
        "tuning",
        "analysis",
        "Oozie",
        "workflow",
        "engine",
        "job",
        "scheduling",
        "Unit",
        "testing",
        "Unit",
        "test",
        "plans",
        "documents",
        "data",
        "providers",
        "HDFS",
        "data",
        "operations",
        "Wrote",
        "MapReduce",
        "jobs",
        "data",
        "analytics",
        "data",
        "Java",
        "API",
        "Wrote",
        "MapReduce",
        "Ruby",
        "Hadoop",
        "Streaming",
        "functionalities",
        "Performed",
        "transformations",
        "filtering",
        "data",
        "Hive",
        "Map",
        "Reduce",
        "data",
        "HDFS",
        "Conducted",
        "meetings",
        "data",
        "analysts",
        "Python",
        "data",
        "data",
        "repositories",
        "Environment",
        "Hadoop",
        "Java",
        "MapReduce",
        "AWS",
        "HDFS",
        "Scala",
        "Python",
        "Spark",
        "Hive",
        "Pig",
        "Linux",
        "XML",
        "Cloudera",
        "CDH45",
        "Distribution",
        "Oracle",
        "12c",
        "PLSQL",
        "EC2",
        "Apache",
        "Flume",
        "Zookeeper",
        "Cassandra",
        "Hortonworks",
        "search",
        "IBM",
        "WebSphere",
        "Big",
        "Data",
        "Engineer",
        "MFS",
        "Investments",
        "Boston",
        "MA",
        "January",
        "April",
        "Responsibilities",
        "Developed",
        "Business",
        "applications",
        "Data",
        "marts",
        "reporting",
        "phases",
        "Software",
        "Development",
        "Life",
        "Cycle",
        "SDLC",
        "activities",
        "development",
        "implementation",
        "support",
        "Hadoop",
        "Performed",
        "Requirements",
        "Analysis",
        "Design",
        "Code",
        "development",
        "Testing",
        "methodologies",
        "responsibilities",
        "data",
        "solutions",
        "Hadoop",
        "ecosystem",
        "Hortonworks",
        "Data",
        "Platform",
        "Hadoop",
        "distribution",
        "data",
        "Hive",
        "data",
        "Hive",
        "joins",
        "data",
        "sources",
        "queries",
        "custom",
        "aggregate",
        "functions",
        "Spark",
        "SQL",
        "Coordination",
        "Hortonworks",
        "development",
        "operations",
        "team",
        "platform",
        "level",
        "issues",
        "combiners",
        "cache",
        "performance",
        "MapReduce",
        "jobs",
        "Spark",
        "SQL",
        "Data",
        "frames",
        "execution",
        "Hive",
        "queries",
        "Spark",
        "Sql",
        "Context",
        "Sqoop",
        "transfer",
        "data",
        "databases",
        "HDFS",
        "Kafka",
        "log",
        "data",
        "servers",
        "Used",
        "Pig",
        "data",
        "transformations",
        "event",
        "filter",
        "preaggregations",
        "data",
        "algorithms",
        "MapReduce",
        "programs",
        "top",
        "HDFS",
        "data",
        "MongoDB",
        "database",
        "concepts",
        "transactions",
        "indexes",
        "replication",
        "schema",
        "design",
        "references",
        "MongoDB",
        "replica",
        "Used",
        "Apache",
        "Tez",
        "data",
        "concern",
        "loss",
        "data",
        "system",
        "failures",
        "HBase",
        "sets",
        "data",
        "Unix",
        "NoSQL",
        "variety",
        "portfolios",
        "CRUD",
        "operations",
        "put",
        "update",
        "HBase",
        "database",
        "Wrote",
        "Hive",
        "Generic",
        "UDFs",
        "business",
        "logic",
        "operations",
        "table",
        "level",
        "workflow",
        "Oozie",
        "tasks",
        "data",
        "HDFS",
        "Pig",
        "Hive",
        "Sqoop",
        "amounts",
        "data",
        "sets",
        "way",
        "Hive",
        "join",
        "queries",
        "tables",
        "source",
        "system",
        "Elastic",
        "Search",
        "Tables",
        "Apache",
        "Kafka",
        "system",
        "log",
        "data",
        "data",
        "applications",
        "HDFS",
        "system",
        "Developed",
        "POC",
        "Scala",
        "Yarn",
        "cluster",
        "performance",
        "Spark",
        "Hive",
        "SQL",
        "Hive",
        "queries",
        "Spark",
        "transformations",
        "Spark",
        "RDDs",
        "Python",
        "Scala",
        "file",
        "formats",
        "compression",
        "Text",
        "Avro",
        "Parquet",
        "file",
        "formats",
        "bz2",
        "gzip",
        "compression",
        "test",
        "scripts",
        "test",
        "development",
        "integration",
        "Scheduling",
        "Cron",
        "jobs",
        "file",
        "system",
        "check",
        "fsck",
        "scripts",
        "alerts",
        "Data",
        "scrubbing",
        "processing",
        "Oozie",
        "Loading",
        "Hive",
        "data",
        "NOSQL",
        "databases",
        "HBase",
        "support",
        "Research",
        "Information",
        "Technology",
        "program",
        "Manage",
        "Linux",
        "OS",
        "X",
        "server",
        "systems",
        "installation",
        "configurations",
        "management",
        "Linux",
        "Systems",
        "Environment",
        "Hadoop",
        "Java",
        "MapReduce",
        "HDFS",
        "Hive",
        "Pig",
        "Sqoop",
        "Flume",
        "Python",
        "Spark",
        "Impala",
        "Scala",
        "Kafka",
        "Shell",
        "Scripting",
        "Eclipse",
        "Cloudera",
        "MySQL",
        "Talend",
        "Cassandra",
        "Sr",
        "JavaHadoop",
        "Developer",
        "Optum",
        "Eden",
        "Prairie",
        "MN",
        "February",
        "December",
        "Responsibilities",
        "analysis",
        "design",
        "testing",
        "phases",
        "specifications",
        "part",
        "Agile",
        "Application",
        "Architecture",
        "A3",
        "development",
        "team",
        "components",
        "layers",
        "application",
        "end",
        "data",
        "processing",
        "ingestion",
        "processing",
        "quality",
        "checks",
        "time",
        "data",
        "Spark",
        "Streaming",
        "Kafka",
        "Developed",
        "Spark",
        "scripts",
        "Scala",
        "requirement",
        "data",
        "Spark",
        "RDD",
        "data",
        "computation",
        "output",
        "response",
        "types",
        "transformations",
        "actions",
        "RDD",
        "business",
        "requirements",
        "JavaScript",
        "JQuery",
        "JavaScript",
        "Library",
        "client",
        "side",
        "validations",
        "data",
        "pipeline",
        "Kafka",
        "Spark",
        "Hive",
        "transform",
        "data",
        "Developed",
        "Pig",
        "Scripts",
        "Pig",
        "UDFs",
        "Hive",
        "Scripts",
        "Hive",
        "UDFs",
        "HDFS",
        "data",
        "Sqoop",
        "data",
        "HDFS",
        "Performed",
        "ETL",
        "Talend",
        "Worked",
        "Hadoop",
        "eco",
        "system",
        "HDFS",
        "MapReduce",
        "Hive",
        "Pig",
        "Sqoop",
        "HBase",
        "web",
        "GUI",
        "architecture",
        "HTML",
        "CSS",
        "AJAX",
        "JQuery",
        "AngularJS",
        "JavaScript",
        "Developed",
        "Map",
        "programs",
        "queries",
        "data",
        "loading",
        "data",
        "UNIX",
        "file",
        "system",
        "Pig",
        "ETL",
        "tool",
        "transformations",
        "event",
        "preaggregations",
        "data",
        "HDFS",
        "cluster",
        "disaster",
        "recovery",
        "plan",
        "Hadoop",
        "cluster",
        "cluster",
        "data",
        "backup",
        "AmazonS3",
        "buckets",
        "SparkSQL",
        "Hive",
        "tables",
        "Spark",
        "processing",
        "data",
        "data",
        "Databases",
        "HDFS",
        "Sqoop",
        "importing",
        "data",
        "data",
        "sources",
        "transformations",
        "Hive",
        "PIG",
        "data",
        "HDFS",
        "PIG",
        "functions",
        "width",
        "file",
        "file",
        "HIVE",
        "join",
        "queries",
        "tables",
        "source",
        "system",
        "Elastic",
        "Search",
        "Tables",
        "Manage",
        "Hadoop",
        "log",
        "files",
        "architecture",
        "solution",
        "Adept",
        "Partitions",
        "bucketing",
        "concepts",
        "tables",
        "Hive",
        "performance",
        "Written",
        "Hadoop",
        "Jobs",
        "data",
        "HiveQL",
        "Queries",
        "Pig",
        "Latin",
        "Data",
        "flow",
        "language",
        "custom",
        "MapReduce",
        "programs",
        "Java",
        "Used",
        "Hadoop",
        "streaming",
        "jobs",
        "terabytes",
        "data",
        "Hive",
        "Developed",
        "workflow",
        "Oozie",
        "tasks",
        "data",
        "HDFS",
        "Pig",
        "Created",
        "reports",
        "BI",
        "team",
        "Sqoop",
        "data",
        "HDFS",
        "Hive",
        "Responsible",
        "designing",
        "coding",
        "application",
        "J2EE",
        "MVC",
        "architecture",
        "Configured",
        "development",
        "environment",
        "WebSphere",
        "application",
        "server",
        "developers",
        "integration",
        "testing",
        "Web",
        "services",
        "data",
        "applications",
        "SOAP",
        "messages",
        "ANT",
        "scripts",
        "application",
        "WebSphere",
        "Application",
        "Server",
        "JUnit",
        "framework",
        "Unit",
        "testing",
        "application",
        "Environment",
        "CDH",
        "Hadoop",
        "HDFS",
        "MapReduce",
        "Yarn",
        "Hive",
        "PIG",
        "Oozie",
        "Sqoop",
        "Linux",
        "Shell",
        "Java",
        "SBT",
        "Amazon",
        "S3",
        "JIRA",
        "Git",
        "Stash",
        "HDFS",
        "Eclipse",
        "SQL",
        "Oracle",
        "g",
        "Sr",
        "JavaJ2EE",
        "Developer",
        "Exchange",
        "Bank",
        "Santa",
        "Rosa",
        "CA",
        "October",
        "January",
        "Responsibilities",
        "system",
        "analysis",
        "design",
        "development",
        "J2EE",
        "architecture",
        "requirements",
        "analysis",
        "design",
        "testing",
        "phases",
        "application",
        "Spring",
        "Framework",
        "Model",
        "View",
        "Controller",
        "MVC",
        "architecture",
        "Software",
        "Development",
        "Life",
        "cycle",
        "requirements",
        "OOA",
        "OOD",
        "Spring",
        "JDBC",
        "database",
        "queries",
        "row",
        "mappers",
        "query",
        "classes",
        "DB",
        "operations",
        "Transaction",
        "History",
        "Web",
        "Service",
        "SOAP",
        "communication",
        "workflow",
        "process",
        "components",
        "companys",
        "object",
        "framework",
        "practices",
        "Patterns",
        "ModelViewController",
        "MVC",
        "DOM",
        "DOM",
        "Functions",
        "Firefox",
        "IE",
        "Developer",
        "Tool",
        "bar",
        "IE",
        "application",
        "Firebug",
        "documents",
        "SQL",
        "Queries",
        "Stored",
        "Procedures",
        "JDBC",
        "database",
        "connectivity",
        "MySQL",
        "Server",
        "presentation",
        "layer",
        "CSS",
        "HTML",
        "Bootstrap",
        "browsers",
        "core",
        "Java",
        "JDK",
        "Eclipse",
        "Integrated",
        "Development",
        "Environment",
        "IDE",
        "case",
        "ANT",
        "Spring",
        "Core",
        "Springweb",
        "framework",
        "lot",
        "classes",
        "backend",
        "web",
        "pages",
        "HTML",
        "JSP",
        "business",
        "functionality",
        "systems",
        "clients",
        "Web",
        "Services",
        "WSDLSOAP",
        "Apache",
        "Axis",
        "POJO",
        "classes",
        "Hibernate",
        "query",
        "language",
        "HQL",
        "PLSQL",
        "queries",
        "procedures",
        "SQL",
        "RDBMS",
        "Analysis",
        "Design",
        "frontend",
        "tier",
        "JSP",
        "Servlets",
        "Ajax",
        "Spring",
        "IOC",
        "Inversion",
        "Control",
        "way",
        "Dependency",
        "Injection",
        "Factory",
        "class",
        "objects",
        "modules",
        "Core",
        "Java",
        "APIs",
        "Java",
        "collection",
        "Threads",
        "XML",
        "modules",
        "SOAP",
        "Web",
        "Services",
        "XML",
        "data",
        "applications",
        "HTTP",
        "Created",
        "EJB",
        "JPA",
        "Hibernate",
        "component",
        "application",
        "XML",
        "parsers",
        "SAX",
        "DOM",
        "JAXB",
        "XML",
        "Parser",
        "Libraries",
        "User",
        "view",
        "Products",
        "Product",
        "information",
        "view",
        "XML",
        "XSD",
        "XSTL",
        "HTML",
        "XML",
        "PDF",
        "formats",
        "integration",
        "JIRA",
        "Jenkins",
        "data",
        "mapping",
        "communication",
        "bridge",
        "application",
        "interfaces",
        "XML",
        "XSL",
        "Hibernate",
        "Transactions",
        "SQL",
        "HQL",
        "Microsoft",
        "VISIO",
        "Use",
        "Case",
        "Diagrams",
        "Sequence",
        "Diagrams",
        "Class",
        "Diagrams",
        "design",
        "phase",
        "Restful",
        "Web",
        "services",
        "client",
        "messages",
        "Spring",
        "JMS",
        "configuration",
        "message",
        "listener",
        "code",
        "production",
        "support",
        "tickets",
        "resolution",
        "BMC",
        "Remedy",
        "Tool",
        "issues",
        "update",
        "resolutions",
        "database",
        "objects",
        "tables",
        "sequences",
        "views",
        "triggers",
        "procedures",
        "functions",
        "packages",
        "Maven",
        "build",
        "tool",
        "Tortoise",
        "SVN",
        "Source",
        "version",
        "controller",
        "Environment",
        "Core",
        "Java",
        "UNIX",
        "J2EE",
        "XML",
        "Schemas",
        "XML",
        "JavaScript",
        "JSON",
        "CSS3",
        "HTML5",
        "spring",
        "Hibernate",
        "Design",
        "Patterns",
        "Servlets",
        "JUnit",
        "JMS",
        "MySQL",
        "Restful",
        "Web",
        "Services",
        "SOAP",
        "Tortoise",
        "SVN",
        "Web",
        "Services",
        "Apache",
        "Tomcat",
        "Windows",
        "XP",
        "JavaJ2EE",
        "Developer",
        "ITC",
        "Infotech",
        "July",
        "September",
        "Responsibilities",
        "Java",
        "Developer",
        "backend",
        "team",
        "User",
        "Interface",
        "JavaScript",
        "HTML",
        "XHTML",
        "XML",
        "CSS",
        "JSP",
        "AJAX",
        "Wrote",
        "web",
        "service",
        "client",
        "operations",
        "orders",
        "web",
        "services",
        "API",
        "web",
        "application",
        "data",
        "archiving",
        "persistence",
        "report",
        "generation",
        "metadata",
        "Hibernate",
        "Mapping",
        "files",
        "POJO",
        "classes",
        "hibernate",
        "data",
        "sources",
        "Spring",
        "framework",
        "DAO",
        "Layer",
        "JPA",
        "EJB3",
        "Imaging",
        "Data",
        "model",
        "Doc",
        "Import",
        "business",
        "logic",
        "J2EE",
        "framework",
        "components",
        "Application",
        "server",
        "Eclipse",
        "component",
        "building",
        "deployment",
        "EJB",
        "service",
        "jars",
        "Application",
        "war",
        "files",
        "WebLogic",
        "Application",
        "server",
        "GUI",
        "screens",
        "login",
        "registration",
        "edit",
        "account",
        "forgot",
        "password",
        "password",
        "Struts",
        "JUnit",
        "framework",
        "unit",
        "testing",
        "application",
        "JUL",
        "log",
        "runtime",
        "exceptions",
        "SQL",
        "procedures",
        "SQL",
        "data",
        "SQL",
        "server",
        "Spring",
        "MVC",
        "Framework",
        "Action",
        "classes",
        "Controllers",
        "validation",
        "framework",
        "annotations",
        "SQL",
        "data",
        "access",
        "manipulation",
        "Oracle",
        "SQL",
        "Developer",
        "Developed",
        "Session",
        "Bean",
        "business",
        "logic",
        "Model",
        "DAO",
        "classes",
        "Hibernate",
        "JAXWS",
        "Web",
        "Services",
        "information",
        "EJB",
        "Components",
        "State",
        "Session",
        "Bean",
        "State",
        "session",
        "beans",
        "spring",
        "framework",
        "help",
        "Spring",
        "Configuration",
        "files",
        "beans",
        "dependency",
        "Dependency",
        "Injection",
        "JPA",
        "ObjectRelational",
        "Mapping",
        "purposes",
        "persistence",
        "Oracle",
        "database",
        "creation",
        "Test",
        "Cases",
        "JUnit",
        "Testing",
        "Oracle",
        "Database",
        "Toad",
        "queries",
        "execution",
        "SQL",
        "scripts",
        "PLSQL",
        "code",
        "procedures",
        "functions",
        "SOAP",
        "protocol",
        "web",
        "service",
        "operation",
        "invocation",
        "Packaged",
        "application",
        "IBM",
        "WebSphere",
        "Application",
        "server",
        "environments",
        "Development",
        "testing",
        "Log4J",
        "functionalities",
        "JUnit",
        "unit",
        "testing",
        "Environment",
        "Java",
        "Servlets",
        "JSP",
        "Struts",
        "Hibernate",
        "spring",
        "core",
        "Spring",
        "JDBC",
        "HTML",
        "JavaScript",
        "AJAX",
        "XSL",
        "XSLT",
        "XSD",
        "schema",
        "XML",
        "Beans",
        "Web",
        "logic",
        "Oracle9i",
        "Education",
        "Bachelors"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T22:05:21.778091",
    "resume_data": "Sr Big DataHadoop Developer Sr Big DataHadoop span lDeveloperspan Sr Big DataHadoop Developer VF Corporation Greensboro NV Greensboro NC Over 9 years of working experience as a Big DataHadoop Developer in designed and developed various applications like big data Hadoop JavaJ2EE opensource technologies Strong development skills in Hadoop HDFS Map Reduce Hive Sqoop HBase with solid understanding of Hadoop internals Experience in Programming and Development of java modules for an existing web portal based in Java using technologies like JSP Servlets JavaScript and HTML SOA with MVC architecture Expertise in ingesting real timenear real time data using Flume Kafka Storm Good knowledge of NO SQL databases like Mongo DB Cassandra and HBase Excellent knowledge of Hadoop Architecture and various components such as HDFS Job Tracker Task Tracker Name Node Data Node and MRA and MRv2 YARN Expertise in writing Hadoop Jobs to analyze data using MapReduce Apache Crunch Hive Pig and SOLR Splunk Hands on experience in installing configuring and using Apache Hadoop ecosystem components like Hadoop Distributed File System HDFS MapReduce Pig Hive HBase Apache Crunch Zookeeper Scoop Hue Scala AVRO Strong Programming Skills in designing and implementing of multitier applications using Java J2EE JDBC JSP JSTL HTML CSS JSF Struts JavaScript Servlets POJO EJB XSLT JAXB Extensive experience in SOAbased solutions Web Services Web API WCF SOAP including Restful APIs services Good Knowledge in Amazon Web Service AWS concepts like EMR and EC2 web services which provides fast and efficient processing of Teradata Big Data Analytics Experience working on EC2 Elastic Compute Cloud cluster instances setup data buckets on S3 Simple Storage Service set EMR Elastic MapReduce Experienced in collection of Log Data and JSON data into HDFS using Flume and processed the data using HivePig Expertise in developing a simple web based application using J2EE technologies like JSP Servlet and JDBC Work Extensively in Core Java Struts2 JSF22 Spring31 Hibernate Servlets JSP and Handson experience with PLSQL XML and SOAP In depth understandingknowledge of Hadoop Architecture and various components such as HDFS Job Tracker Task Tracker NameNode DataNode Well versed working with Relational Database Management Systems as Oracle 9i12c MS SQL MySQL Server Hands on experience in working on XML suite of technologies like XML XSL XSLT DTD XML Schema SAX DOM JAXB Hands on experience in advanced BigData technologies like Spark Ecosystem Spark SQL MLlib SparkR and Spark Streaming Kafka and Predictive analytics Knowledge of the software Development Life Cycle SDLC Agile and Waterfall Methodologies Experienced on applications using Java python and UNIX shell scripting Experience in consuming Web services with Apache Axis using JAXRSREST APIs Experienced in building tool Maven ANT and logging tool Log4J Experience in working with Web Servers like Apache Tomcat and Application Servers like IBM Web Sphere and JBOSS Good knowledge of NoSQL databases such as HBase MongoDB and Cassandra Experience in working with Eclipse IDE Net Beans and Rational Application Developer Experienced in Apache Flume for collecting aggregating and moving huge chunks of data from various sources such as web server telnet sources etc Extensively designed and executed SQL queries in order to ensure data integrity and consistency at the backend Experienced in working with different scripting technologies like Python UNIX shell scripts Strong experienced in working with UNIXLINUX environments writing shell scripts Expertise with frameworks like Angular JS jQuery in web presentation layer with servlets JSP Spring MVC at the web controller layer Experienced in deploying J2EE applications on Apache Tomcat web server and WebLogic WebSphere JBoss application server Technical ToolsTechnology Hadoop Ecosystem Hadoop2725 MapReduce Sqoop Hive Oozie Pig HDFS124 Zookeeper Flume Impala Spark20202 Storm Hadoop Cloudera Horton works and Pivotal Big Data Platforms Horton works Cloudera Amazon AWS and Apache Databases NOSQL Databases Oracle12c11g MYSQL Microsoft SQL Server20162014 MongoDB HBase and Cassandra Work Experience Sr Big DataHadoop Developer VF Corporation Greensboro NV May 2017 to Present Responsibilities Worked as a Sr BigHadoop Developer for providing solutions for big data problem Involved in full life cycle of the project from Design Analysis logical and physical architecture modelling development Implementation testing Utilized SDLC Methodology to help manage and organize a team of developers with regular code review sessions Involved in Agile methodologies daily scrum meetings spring planning Developed Pig Latin scripts for replacing the existing legacy process to the Hadoop and the data is fed to AWS S3 Worked on MongoDB by using CRUD Create Read Update and Delete Indexing Replication and Sharding features Created Talend jobs to read messages from Amazon AWS SQS queues download files from AWS S3 buckets Developed MapReduce YARN jobs for cleaning accessing and validating the data Worked on analyzing Hadoop cluster and different Big Data Components including Pig Hive Spark HBase Kafka Elastic Search database and SQOOP Written Hive jobs to parse the logs and structure them in tabular format to facilitate effective querying on the log data Wrote MapReduce jobs to filter and parse inventory data which was stored in the HDFS Configured Flume Hive Pig Sqoop and Oozie on the Hadoop cluster for data pipelining Imported and exported data into the HDFS from the Oracle database using Sqoop Integrated MapReduce with Cassandra to import bulk amount of logged data Converted ETL operations to the Hadoop system using Hive transformations and functions Conducted streaming jobs with basic Python to process terabytes of formatted data for machine learning purposes Used Flume to collect aggregate and store the web log data and loaded it into the HDFS Implemented Partitioning Dynamic Partitions Buckets in Hive Developed custom and Pig UDFs for product specific needs Implemented and configured workflows using Oozie to automate jobs Performed Hadoop cluster management and configuration of multiple nodes on AWS Involved in creating buckets to store the data in AWS and stored the data repository for future needs and reusability Worked along with Tableau developers to help performance tune the visualizations graphsanalytics Involved in the cluster coordination services through Zookeeper Participated in the managing and reviewing of the Hadoop log files Used Elastic Search MongoDB for storing and querying the offers and nonoffers data Created custom UDFsfor Spark and Kafkaprocedure for some of nonworking functionalities in custom UDF into Scala in production environment Import the data from different sources like HDFSHBase into Spark RDD and developed a data pipeline using Kafka and Storm to store data into HDFS Worked independently with Cloudera support and Hortonworks support for any issueconcerns with Hadoop cluster Responsible for building scalable distributed data solutions using Hadoop Cloudera Used Spark streaming to receive real time data from the Kafka and store the stream data to HDFS using Scala and NoSQL databases such as HBase and Cassandra Implementing Hadoop Security on Hortonworks Cluster using Kerberos and Twoway SSL Worked with teams in setting up AWS EC2 instances by using different AWS services like S3 EBS Elastic Load Balancer and Auto scaling groups VPC subnets and CloudWatch Implemented Hortonworks NiFi and recommended solution to inject data from multiple data sources to HDFS and Hiveusing NiFi Developed Restful web services using JAXRS and used DELETE PUT POST GET HTTP methods Created scalable and highperformance web services for data tracking and done Highspeed querying Used Java Messaging Services JMS for reliable and asynchronous exchange of important information such as payment status report on IBM WebSphere MQ messaging system Executed Hive queries on Parquet tables stored in Hive to perform data analysis to meet the business requirements Worked on migrating MapReduce programs into Spark transformations using Spark and Scala Developed NIFI flow to move data from different sources to HDFS and from HDFS to S3 bucketsIn current project Created and maintained various Shell and Python scripts for automating various processes and optimized MapReduce code pig scripts and performance tuning and analysis Worked on Oozie workflow engine for job scheduling Involved in Unit testing and delivered Unit test plans and results documents Involved with ingesting data received from various providers on HDFS for big data operations Wrote MapReduce jobs to perform big data analytics on ingested data using Java API Wrote MapReduce in Ruby using Hadoop Streaming to implement various functionalities Performed transformations cleaning and filtering on imported data using Hive Map Reduce and loaded final data into HDFS Conducted meetings with data analysts with basic Python and wrangled data for data repositories Environment Hadoop 30 Java MapReduce AWS HDFS Scala 212 Python 37 MongoDB 40 Spark 23 Hive 23 Pig 017 Linux XML Cloudera CDH45 Distribution Oracle 12c PLSQL EC2 Apache Flume 18 Zookeeper 34 Cassandra 311 Hortonworks Elastic search IBM WebSphere Big Data Engineer MFS Investments Boston MA January 2016 to April 2017 Responsibilities Architected Designed and Developed Business applications and Data marts to facilitate the reporting Involved in all phases of Software Development Life Cycle SDLC and Worked on all activities related to the development implementation and support for Hadoop Performed Requirements gathering Analysis Design Code development Testing using Agile methodologies Primary responsibilities include building scalable distributed data solutions using Hadoop ecosystem Worked on Hortonworks Data Platform Hadoop distribution for data querying using Hive to store and retrieve data Implemented Hive optimized joins to gather data from different sources and run adhoc queries on them Performed custom aggregate functions using Spark SQL and performed interactive querying Coordination with Hortonworks development and the operations team on the platform level issues Extensively worked on creating combiners partitioning and distributed cache to improve performance of MapReduce jobs Worked on Spark SQL and Data frames for faster execution of Hive queries using Spark Sql Context Used Sqoop transfer data between databases and HDFS and used Kafka to stream the log data from servers Used Pig to perform data transformations event joins filter and some preaggregations before storing the data onto HDFS Implemented different analytical algorithms using MapReduce programs to apply on top of HDFS data Worked on MongoDB database concepts such as locking transactions indexes sharding replication and schema design Implemented read references in MongoDB replica set Used Apache Tez for processing data and storing it in MongoDB Write concern in MongoDB to avoid loss of data during system failures Created HBase tables to load large sets of structured semistructured and unstructured data coming from Unix NoSQL and a variety of portfolios Extensively performed CRUD operations like put get scan delete update etc on HBase database Wrote Hive Generic UDFs to perform business logic operations at table level Developed workflow in Oozie to automate the tasks of loading the data into HDFS and preprocessing with Pig Hive and Sqoop Analyzed large amounts of data sets to determine optimal way to aggregate and report on it Used Hive join queries to join multiple tables of a source system and load them into Elastic Search Tables Used Apache Kafka as messaging system to load log data data from applications into HDFS system Developed POC using Scala and deployed on the Yarn cluster compared the performance of Spark with Hive and SQL Involved in converting Hive queries into Spark transformations using Spark RDDs Python and Scala Worked on various file formats and compression Text Avro Parquet file formats snappy bz2 gzip compression Implemented test scripts to support test driven development and continuous integration Scheduling Cron jobs for file system check using fsck and wrote shell scripts to generate alerts Data scrubbing and processing with Oozie Loading the analyzed Hive data into NOSQL databases like HBase MongoDB Provide Technical support for the Research in Information Technology program Manage and upgrade Linux and OS X server systems Responsible for installation configurations and management for Linux Systems Environment Hadoop 30 Java MapReduce HDFS Hive 23 Pig 017 Sqoop 14 Flume 18 Python 37 Spark 23 Impala Scala Kafka Shell Scripting Eclipse Cloudera MySQL Talend Cassandra 311 Sr JavaHadoop Developer Optum Eden Prairie MN February 2014 to December 2015 Responsibilities Involved in analysis design testing phases and responsible for documenting technical specifications Worked as part of the Agile Application Architecture A3 development team responsible for setting up the architectural components for different layers of the application Involved in end to end data processing like ingestion processing and quality checks and splitting Real time streaming the data using Spark Streaming with Kafka Developed Spark scripts by using Scala as per the requirement Load the data into Spark RDD and performed inmemory data computation to generate the output response Performed different types of transformations and actions on the RDD to meet the business requirements Extremely used plain JavaScript and JQuery JavaScript Library to do the client side validations Developed a data pipeline using Kafka Spark and Hive to ingest transform and analyzing data Developed Pig Scripts Pig UDFs and Hive Scripts Hive UDFs to analyze HDFS data Used Sqoop to export data from HDFS to RDBMS Performed ETL using Talend Worked on Hadoop eco system components HDFS MapReduce Hive Pig Sqoop and HBase Designed developed web based GUI architecture using HTML CSS AJAX JQuery AngularJS and JavaScript Developed Map Reduce programs for some refined queries on big data Involved in loading data from UNIX file system to HDFS Used Pig as ETL tool to do transformations event joins and some preaggregations before storing the data onto HDFS Worked in the cluster disaster recovery plan for the Hadoop cluster by implementing the cluster data backup in AmazonS3 buckets Implemented SparkSQL to access Hive tables into Spark for faster processing of data Extracted the data from Databases into HDFS using Sqoop Handled importing of data from various data sources performed transformations using Hive PIG and loaded data into HDFS Used PIG predefined functions to convert the fixed width file to delimited file Used HIVE join queries to join multiple tables of a source system and load them into Elastic Search Tables Manage and review Hadoop log files Implemented lambda architecture as a solution Adept at understanding Partitions bucketing concepts managed and created external tables in Hive to optimize performance Written Hadoop Jobs for analyzing data using HiveQL Queries Pig Latin Data flow language and custom MapReduce programs in Java Used Hadoop streaming jobs to process terabytes data in Hive Developed workflow in Oozie to automate the tasks of loading the data into HDFS and preprocessing with Pig Created reports for the BI team using Sqoop to import data into HDFS and Hive Responsible for the designing coding and developed the application in J2EE using MVC architecture Configured development environment using WebSphere application server for developers integration testing Used Web services for sending and getting data from different applications using SOAP messages Used ANT scripts to build the application and deployed on WebSphere Application Server Used JUnit framework for Unit testing of application Environment CDH Hadoop 28 HDFS MapReduce Yarn Hive 21 PIG 016 Oozie Sqoop 12 Linux Shell scripting Java SBT Amazon S3 JIRA Git Stash HDFS Eclipse SQL Oracle 11g Sr JavaJ2EE Developer Exchange Bank Santa Rosa CA October 2011 to January 2014 Responsibilities Responsible for system analysis design and development using J2EE architecture Actively participated in requirements gathering analysis design and testing phases Developed the application using Spring Framework that leverages classical Model View Controller MVC architecture Involved in Software Development Life cycle starting from requirements gathering and performed OOA and OOD Used Spring JDBC to execute database queries Created row mappers and query classes for DB operations Created a Transaction History Web Service using SOAP that is used for internal communication in the workflow process Designed and created components for companys object framework using best practices and design Patterns such as ModelViewController MVC Used DOM and DOM Functions using Firefox and IE Developer Tool bar for IE Debugged the application using Firebug to traverse the documents Involved in writing SQL Queries Stored Procedures and used JDBC for database connectivity with MySQL Server Developed the presentation layer using CSS and HTML taken from Bootstrap to develop for browsers Did core Java coding using JDK 13 Eclipse Integrated Development Environment IDE clear case and ANT Used Spring Core and Springweb framework Created a lot of classes for backend Involved in developing web pages using HTML and JSP Exposed business functionality to external systems Interoperable clients using Web Services WSDLSOAP Apache Axis Developed POJO classes and writing Hibernate query language HQL queries Used PLSQL for queries and stored procedures in SQL as the backend RDBMS Involved in the Analysis and Design of the frontend and middle tier using JSP Servlets and Ajax Implemented Spring IOC or Inversion of Control by way of Dependency Injection where a Factory class was written for creating and assembling the objects Implemented modules using Core Java APIs Java collection Threads XML and integrating the modules and used SOAP for Web Services by exchanging XML data between applications over HTTP Created EJB JPA and Hibernate component for the application Implemented XML parsers with SAX DOM and JAXB XML Parser Libraries to Modify User view of Products and Product information in Customized view with XML XSD XSTL in HTML XML PDF formats Established continuous integration with JIRA Jenkins Developed data mapping to create a communication bridge between various application interfaces using XML and XSL Used Hibernate to manage Transactions update delete along with writing complex SQL and HQL queries Used Microsoft VISIO for developing Use Case Diagrams Sequence Diagrams and Class Diagrams in the design phase Developed Restful Web services client to consume JSON messages using Spring JMS configuration Developed the message listener code Providing production support which includes handling tickets providing resolution Used BMC Remedy Tool to add issues update resolutions Create database objects like tables sequences views triggers stored procedures functions packages Used Maven as the build tool and Tortoise SVN as the Source version controller Environment Core Java UNIX J2EE XML Schemas XML JavaScript 2014 JSON CSS3 HTML5 spring Hibernate Design Patterns Servlets JUnit JMS MySQL Restful Web Services SOAP Tortoise SVN 15 Web Services Apache Tomcat 80 Windows XP JavaJ2EE Developer ITC Infotech July 2009 to September 2011 Responsibilities As a Java Developer involved in backend and frontend developing team Designed and implemented the User Interface using JavaScript HTML XHTML XML CSS JSP and AJAX Wrote web service client for tracking operations for the orders which is accessing web services API and utilizing in our web application Implemented data archiving and persistence of report generation metadata using Hibernate by creating Mapping files POJO classes and configuring hibernate to set up the data sources Developed Spring framework DAO Layer with JPA and EJB3 in Imaging Data model and Doc Import The business logic is developed using J2EE framework and deployed components on Application server where Eclipse was used for component building Actively involved in deployment EJB service jars Application war files in WebLogic Application server Developed GUI screens for login registration edit account forgot password and change password using Struts Used JUnit framework for unit testing of application and JUL logging to capture the log that includes runtime exceptions Wrote various SQL stored procedures and SQL commands to retrieve the data from the SQL server Used Spring MVC Framework to develop Action classes and Controllers along with validation framework and annotations Writing SQL queries for data access and manipulation using Oracle SQL Developer Developed Session Bean to encapsulate the business logic and Model and DAO classes using Hibernate Designed and coded JAXWS based Web Services used to access external financial information Implemented EJB Components using State less Session Bean and State full session beans Used spring framework with the help of Spring Configuration files to create the beans needed and injected dependency using Dependency Injection Utilized JPA for ObjectRelational Mapping purposes for transparent persistence onto the Oracle database Involved in creation of Test Cases for JUnit Testing Used Oracle as Database and used Toad for queries execution and also involved in writing SQL scripts PLSQL code for procedures and functions Used SOAP as a XMLbased protocol for web service operation invocation Packaged and deployed the application in IBM WebSphere Application server in different environments like Development testing etc Used Log4J to validate functionalities and JUnit for unit testing Environment Java Servlets JSP Struts 10 Hibernate 31 spring core Spring JDBC HTML JavaScript 2012 AJAX XSL XSLT XSD schema XML Beans Web logic Oracle9i Education Bachelors",
    "unique_id": "c4c3068b-13be-478d-b7c6-ded9caba21e0"
}