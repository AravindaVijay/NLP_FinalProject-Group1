{
    "clean_data": "Sr Hadoop Developer Sr Hadoop span lDeveloperspan Sr Hadoop Developer CVS Health Los Angeles CA Around 8 years of IT experience in a variety of industries which includes 4 years of working as a Hadoop developer designing and implementing complete endtoend Hadoop infrastructure using MapReduce Pig Hive Sqoop Oozie Flume Spark HBase and Zookeeper Excellent knowledge of Hadoop Architecture and its various components such as HDFS Job Tracker Task Tracker Name Node Data Node and MapReduce Programming paradigm Hands on expertise with SQL PLSQL UNIX shell scripts and commands Experience in creating Hive tables and queries using HiveQL with a good understanding of Hive concepts such as Partitioning Bucketing and Joins Developed Pig Latin scripts to extract and load the data into HDFS Experience in migrating the data using Sqoop from Relational Database Systems to HDFS and viceversa Experience in loading unstructured data Log files Xml data into HDFS using Flume Hands on expertise with different file formats like JSON XML CSV etc Good exposure on setting up job streaming and scheduling with Oozie and working on messaging system such as Kafka integrated with Zookeeper Experience in monitoring Hadoop clusters on VM Horton Works Data Platform 21 and 22 CDH5 Cloudera Manager HDP on Linux Using Kafka HDFS connector to export data from Kafka topics to HDFS files in a variety of formats and integrate with Apache Hive to make data immediately available for querying with Hive QL Good exposure to NoSQL Databases and hands on work experience in using HBase Experience in using Oozie workflow scheduler to manage Hadoop jobs by Directed Acyclic Graph DAG of actions with control flows Experience in designing and developing applications in Spark using Python to compare the performance of Spark with Hive and SQLOracle Flexible with UnixLinux and Windows Environments working with Operating Systems like Cent OS 56 Ubuntu 1314 Experience in manipulating the streaming data to clusters through Kafka and Apache SparkStreaming Experience in Continuous Integration and Continuous Deployment by the tools like Jenkins Learning to use Amazon AWS EMR and EC2 for cloud big data processing Experience in all phases of Software Development Life Cycle including analysis design implementation integration deployment testing using different software methodologies like Agile Scrum Waterfall models Technically well versed in designing and developing business solutions in the field of Banking Insurance Ecommerce Manufacturing divisions Experienced with the Apache Spark improving the performance and optimization of the existing algorithms in Hadoop using Apache Spark Context Apache SparkSQL Data Frame Pair RDDs Apache Spark YARN Good knowledge on Hadoop MRV1 and Hadoop MRV2 or YARN Architecture An effective team player with excellent communication analytical and interpersonal skills with exceptional planning and execution skills with a systematic approach and quick adaptability Experience with databases such as Oracle  PostgreSQL MySQL Server with cluster setup and writing the SQL queries Triggers Stored Procedures Authorized to work in the US for any employer Work Experience Sr Hadoop Developer CVS Health Northbrook IL January 2019 to Present Responsibilities Designed and implemented scalable infrastructure and platform for large amounts of data ingestion aggregation and integration in Hadoop MapReduce Flume Kafka Spark Hive Loaded large sets of structured semistructured and unstructured data using Sqoop Flume Kafka Written Sqoop scripts to import export and update the data between HDFS and Relational databases Created Flume configure files to collect aggregate and store the web log data into HDFS Involved in designing and creating data model to load customer data into a NoSQL database like HBase For data enrichment we perform a lookup over the table present in HBase to retrieve the data After enriching the data we validate it and analyze the data by performing bucketing and partitioning in Hive Developed Pig Latin scripts to transform the data and load into HDFS Import data from different sources like HDFSHBase into Spark RDD Utilized Kafka to capture and process real time streaming data Worked with Oozie and Zookeeper to manage job workflow and job coordination in the cluster Worked closely with the business analysts to convert the Business Requirements into Technical Requirements and prepared low and highlevel documentation Played a key role in discussing about the requirements analysis of the entire system along with development and testing Exported data from HDFS into RDBMS using Sqoop to generate reports Involved in diagnosing different possible ways to optimize and improve the efficiency of the system Participated in peerreviews of solution designs and related code Developed workflow scheduler job scripts in Apache Oozie to extract the outcomes on a daytoday basis Environment Hadoop263 MapReduce HDFS Yarn Sqoop143 Oozie Pig011 Hive110 HBase098 Spark2x Java Eclipse UNIX shell scripting python351 Cloudera manager 59X Flume Kafka Hadoop Java Developer Grand Bank of Texas Dallas TX July 2016 to November 2018 Responsibilities Grand Bank of Texas is a locallyowned and operated independent community bank to serve the financial needs of the community and area Roles and Responsibilities Experience with the Hadoop ecosystem MapReduce Pig Hive HBase and NoSQL Analyze and determine the relationship of input keys to output keys in terms of both type and number identify the number type and value of emitted keys and values from the Mappers Reducers and the number and contents of the output files Developed MapReduce pipeline jobs to process the data and create necessary HFiles and loading the HFiles into HBase for faster access without taking performance hit Designed developed UI Screens with Spring MVC HTML5 CSS JavaScript AngularJS to provide interactive screens to display data Written multiple MapReduce programs in java for data extraction transformation and aggregation from multiple file formats including XML JSON CSV and other file formats Analyze to determine the correct InputFormat OutputFormat on order of MapReduce job requirements Created database tables and wrote TSQL Queries and stored procedures to create complex join tables and to perform CRUD operations By using AWS MapReduce job processed the data stored in the AWS Using Automated Build and continuous integration systems Jenkins and testdriven Unit Testing framework Junit Access control to users depending on logins using HTML jQuery for validations Created the web application using HTML CSS jQuery and JavaScript Used Eclipse as an IDE for developing the application Loaded the flat files data using Informatica to the staging area Used UIrouter in AngularJS to make this a single page application Developed unitassembly test cases and UNIX shell scripts to run along with dailyweeklymonthly batches to reduce or eliminate manual testing effort Developed mappings in Informatica to load the data including facts and dimensions from various sources into the Data Warehouse using different transformations like Source Qualifier Java Expression Lookup Aggregate Update Strategy and Joiner Environment Windows XPNT Java MapReduce Pig Hive Hbase NoSQL AWS Jenkins HTML CSS TSQL AngularJS UI jQuery Korn Shell Quality Center 10 Hadoop Developer East West Bank Pasadena CA March 2014 to June 2016 Responsibilities Installed and configured Apache Hadoop to test the maintenance of log files in Hadoop cluster Installed and configured Hive Pig Sqoop Flume and Oozie on the Hadoop cluster Developed Sqoop jobs to import and store massive volumes of data in HDFS and Hive Designed and developed PIG data transformation scripts to work against unstructured data from various data points and created a base line Setup and benchmarked Hadoop HBase clusters for internal use Developed Java MapReduce programs for the analysis of sample log file stored in cluster Developed Map Reduce Programs for data analysis and data cleaning Developed PIG UDFs like UTAFs UDAFs for manipulating the data according to business requirements and worked on developing custom PIG Loaders Implemented data pipeline by chaining multiple mappers by using Chained Mapper Used Hive and created Hive tables and involved in data loading and writing Hive UDFs Migration of processes from Oracle to Hive to test the easy data manipulation Configured deployed and maintained multinode Dev and Test Kafka Clusters Implemented data injection systems by creating Kafka brokers producers Consumers custom encoders Implemented Partitioning Dynamic Partitions and Bucketing in Hive for efficient data access Developed a data pipeline using Kafka and Strom to store data into HDFS Developed some utility helper classes to get data from HBase tables Good experience in troubleshooting performance issues and tuning Hadoop cluster Built components modules and plugins using Angular JS and Bootstrap Created Java Interfaces and Abstract classes for different functionalities Loaded and transformed large sets of structured semistructured and unstructured data in various formats like text sequence XML and JSON Written multiple MapReduce programs to power data for extraction transformation and aggregation from multiple file formats including XML JSON CSV other compressed file formats Environment Apache Hadoop HDFS Cloudera Manager Java MapReduce Eclipse Hive PIG Sqoop Oozie SQL Zookeeper CDH3 Cassandra Oracle NoSQL and UnixLinux Kafka Big Data Engineer Caesars Las Vegas NV August 2012 to February 2014 Responsibilities Responsible for creating sample Datasets required for testing various Map Reduce applications from various sources Developed Hive UDF to parse the staged raw data to get the item details from a specific store Built reusable Hive UDF libraries for business requirements which enabled users to use these UDFs in Hive querying Designed workflow by scheduling Hive processes for Log file data which is streamed into HDFS using Flume Developed SQL scripts to compare all the records for every field and table at each phase of the data movement process from the original source system to the final target Implemented map reduce jobs to process standard data in Hadoop cluster Involved in the performance enhancement by analyzing the workflows joins configuration parameters etc Collaborating with business usersproduct owners developers to contribute to the analysis of functional requirements Design Develop workflow using Oozie for business requirements which includes automating the extraction of data from MySQL database into HDFS using Sqoop scripts Worked on migration of Informatica Power center to Hadoop eco system Assisted with the admin department with issues related to migration Performed data analytics in Hive and then exported this metrics back to Oracle Database using Sqoop Provided adhoc queries and data metrics to the Business Users using Hive Pig Developed Map Reduce Programs in Java for applying business rules on the data Implemented Partitioning Dynamic Partitions Bucketing in Hive Document and manage failurerecovery steps for any production issues Involved in Minor and Major Release work activities Proactively involved in ongoing maintenance support and improvements in Hadoop cluster Import the data from various sources like HDFSHBase into Kafka Wrote MapReduce jobs using java and Pig Latin Worked on NoSQL databases including HBase and Cassandra Configured SQL Database to store Hive Teradata Participated in developmentimplementation of Cloudera impala Hadoop environment Imported data using Sqoop to load data from MySQL to HDFS on regular basis Used Pig as ETL Informatica tool to do Transformations even joins and some preaggregations before storing the data onto HDFS Developed Application components and APIs using Scala Created ETL Informatica jobs to generate and distribute reports from MySQL database Involved in loading data from LINUX file system to HDFS using Sqoop and exported the analyzed data to the relational databases using Sqoop for visualization and to generate reports for the Business intelligence BI team Installed and configured Flume Hive Pig Sqoop and Oozie on the Hadoop cluster Extracted the Teradata from Oracle into Hive using the Sqoop Worked on Agile Methodology Environment Hadoop HDFS Pig Zookeeper Sqoop HBase Shell Scripting Ubuntu Linux Red Hat Kafka Cassandra Senior Hadoop Developer AFLAC Columbus GA March 2011 to July 2012 Responsibilities Worked closely with the business analysts to convert the Business Requirements into Technical Requirements and preparing low and highlevel documentation Performing transformations using Hive MapReduce hands on experience in copying log snappy files into HDFS from Greenplum using Flume Kafka loaded data into HDFS and extracted the data into HDFS from MYSQL using Sqoop Involved in preparing the S2TM document as per the business requirement and worked with Source system SMEs in understanding the source data behavior Imported required tables from RDBMS to HDFS using Sqoop and used Storm Spark streaming and Kafka to get real time streaming of data into HBase Experience in Writing Map Reduce jobs for text mining and worked with predictive analysis team and Experience in working with Hadoop components such as HBase Spark Yarn Kafka Zookeeper PIG HIVE Sqoop Oozie Impala and Flume Wrote HIVE UDFs as per requirements and to handle different schemas and xml data Designing and developing MapReduce jobs to process data coming in different file formats like XML CSV JSON Involved in Apache SPARK testing Implemented ETL code to load data from multiple sources into HDFS using Pig Scripts Implemented MapReduce programs to handle semi unstructured data like XML JSON Avro data files and sequence files for log files Responsible to review the test cases in HP ALM Developed Spark applications using Scala for easy Hadoop transitions And Hands on experienced in writing Spark jobs and Spark streaming API using Scala and Python Used Spark API over Cloudera Hadoop YARN to perform analytics on data in Hive developed Spark code and SparkSQLStreaming for faster testing and processing of data Installed Oozie workflow engine to run multiple Hive and Pig jobs Designed and developed User Defined Function UDF for Hive and Developed the Pig UDFS to preprocess the data for analysis as well as experience in UDAFs for custom data specific processing Assisted in problem solving with Big Data technologies for integration of Hive with HBase and Sqoop with HBase Designed and developed the core data pipeline code involving work in Java and Python and built on Kafka and Storm Good knowledge on Partitions bucketing concepts in Hive and designed both Managed and External tables in Hive for optimized performance Performance tuning using Partitioning bucketing of IMPALA tables Hands on experience on fetching the live stream data from DB2 to HBase table using Spark Streaming and Apache Kafka Experience in job workflow scheduling and monitoring tools like Oozie and Zookeeper Worked on NoSQL databases including HBase and Cassandra Populated HDFS and Cassandra with huge amounts of data using Apache Kafka Created POC Proof of Concept to store Server Log data into Cassandra to identify System Alert Metrics Environment Map Reduce HDFS Hive Pig HBase Python SQL Sqoop Flume Oozie Impala Scala Spark Apache Kafka Zookeeper J2EE Linux Red Hat HPALM Eclipse Cassandra Talend Informatica Hadoop Developer Wellcare Health Plans Tampa FL June 2010 to February 2011 Responsibilities WellCare focuses exclusively on providing governmentsponsored managed care services primarily through Medicaid Medicare Advantage and Medicare Prescription Drug Plans to families children seniors and individuals with complex medical needs Roles and Responsibilities Experience in creating integration between Hive and HBase for effective usage and performed MR Unit testing for the Map Reduce jobs Created BI reports Tableau and dashboards from HDFS data using Hive Experience in importing and exporting the data from Relational Database Systems to HDFS by using Sqoop Developed a common framework to import the data from Teradata to HDFS and to export to Teradata using Sqoop Imported the log data from different servers into HDFS using Flume and developed MapReduce programs for analyzing the data Used Flume to handle the real time log processing for attribution reports Worked on tuning the performance of Pig queries Involved in loading data from UNIX file system to HDFS Performed operation using Partitioning pattern in MapReduce to move records into different categories Implemented Fair schedulers on the Job tracker to share the resources of the Cluster for the MapReduce jobs given by the users Involved in templates and screens in HTML and JavaScript Created HBase tables to load large sets of data coming from UNIX and NoSQL Implemented the Web Service client for the login authentication credit reports and applicant information using Apache Axis 2 Web Service Design develop test implement and support of Data Warehousing ETL using Talend and Hadoop Technologies Built and deployed Java applications into multiple Unix based environments and produced both unit and functional test results along with release notes Environment WebSphere 61 HTML XML ANT 16 MapReduce Sqoop UNIX NoSQL Java JavaScript MR Unit Teradata Nodejs JUnit 38 ETL Talend HDFS Hive HBase Education Bachelors Skills APACHE HADOOP MAPREDUCE 9 years APACHE HBASE 9 years Hadoop 9 years HADOOP 9 years HBase 9 years Additional Information TECHNICAL SKILLS Big Data Technologies Hadoop HDFS MapReduce Spark Hive Sqoop Oozie Flume HBase Pig Zookeeper Kafka Programming Languages Linux Programming Basics of Python language Hive SQL JAVA Pig Latin Web Development JavaScript JQuery HTML 50 CSS 30 AJAX JSON Tools CDH 591 Cloudera Navigator Cloudera Manager Basics VMware Web Servers Apache Tomcat Methodologies Agile Vmodel Waterfall model Operating Systems Linux CentOS Windows Databases MySQL Oracle 11g10g HBase Mongo DB Cassandra Couch MS SQL server",
    "entities": [
        "Zookeeper Experience",
        "the Business intelligence BI",
        "Data Frame Pair",
        "Apache Oozie",
        "Informatica",
        "Windows Environments",
        "Partitions",
        "Relational",
        "MR Unit",
        "Oracle  PostgreSQL MySQL Server",
        "HDFS",
        "UNIX",
        "the Sqoop Worked on Agile Methodology Environment Hadoop HDFS Pig Zookeeper Sqoop HBase",
        "IMPALA",
        "Informatica Power",
        "Partitioning",
        "HTML XML ANT 16",
        "Los Angeles",
        "MapReduce Pig Hive Sqoop Oozie Flume Spark HBase",
        "Work Experience Sr Hadoop Developer CVS Health Northbrook IL",
        "Operating Systems",
        "Hadoop HBase",
        "UDAFs",
        "Hadoop",
        "HDFS Involved",
        "Additional Information TECHNICAL SKILLS Big Data Technologies Hadoop HDFS MapReduce Spark Hive",
        "Pig Scripts Implemented MapReduce",
        "the Apache Spark",
        "Hive UDF",
        "HBase",
        "Sr Hadoop Developer Sr Hadoop",
        "TX",
        "Developed Sqoop",
        "Cloudera Hadoop",
        "Spark with",
        "UnixLinux Kafka Big Data Engineer Caesars",
        "Minor",
        "WebSphere",
        "Assisted",
        "Developed",
        "Medicare Prescription Drug Plans",
        "Banking Insurance Ecommerce Manufacturing",
        "Dallas",
        "Node Data",
        "the Cluster for the MapReduce",
        "HDFS Performed",
        "Responsibilities Grand Bank of Texas",
        "AWS MapReduce job",
        "Scala Created ETL Informatica",
        "User Defined Function UDF",
        "Apache Hive",
        "Hive Experience",
        "JavaScript Created HBase",
        "Built",
        "HTML CSS jQuery",
        "Oracle 11g10",
        "Spark Streaming",
        "Talend",
        "Spark RDD Utilized Kafka",
        "Roles",
        "XML CSV JSON Involved",
        "ETL Informatica",
        "Oracle Database",
        "MVC",
        "UnixLinux",
        "Spark",
        "Implemented Fair",
        "Created BI",
        "API",
        "US",
        "Sqoop",
        "Present Responsibilities Designed",
        "LINUX",
        "Hadoop Architecture",
        "Oracle",
        "Apache",
        "PIG",
        "Oracle to Hive",
        "HDFS Job Tracker Task Tracker",
        "java",
        "Consumers",
        "Las Vegas",
        "HTML",
        "Storm Spark",
        "SQL",
        "MapReduce Programming",
        "TSQL Queries",
        "the Business Users using Hive Pig Developed Map Reduce Programs",
        "MapReduce Pig Hive HBase",
        "Relational Database Systems",
        "VM Horton Works Data",
        "HDFS Developed Application",
        "UI jQuery Korn Shell Quality Center",
        "Big Data",
        "Hive",
        "CDH5",
        "Partitioning Bucketing",
        "the Business Requirements",
        "Created POC Proof",
        "Environment Apache Hadoop",
        "Continuous Deployment",
        "CRUD",
        "InputFormat OutputFormat",
        "HFiles",
        "Apache Hadoop",
        "Performed",
        "Developed Hive UDF",
        "Zookeeper Excellent",
        "Implemented Partitioning Dynamic Partitions",
        "Data Warehousing ETL",
        "Continuous Integration",
        "Cassandra Populated HDFS",
        "the Data Warehouse",
        "Amazon AWS EMR",
        "CSS",
        "MapReduce Sqoop",
        "the AWS Using Automated Build",
        "HDFS Import",
        "Implemented ETL",
        "Imported",
        "Medicaid Medicare Advantage",
        "MapReduce",
        "NoSQL",
        "Tableau",
        "Technical Requirements",
        "Directed Acyclic Graph DAG",
        "Setup",
        "Software Development Life Cycle",
        "Apache Spark Context"
    ],
    "experience": "Experience in creating Hive tables and queries using HiveQL with a good understanding of Hive concepts such as Partitioning Bucketing and Joins Developed Pig Latin scripts to extract and load the data into HDFS Experience in migrating the data using Sqoop from Relational Database Systems to HDFS and viceversa Experience in loading unstructured data Log files Xml data into HDFS using Flume Hands on expertise with different file formats like JSON XML CSV etc Good exposure on setting up job streaming and scheduling with Oozie and working on messaging system such as Kafka integrated with Zookeeper Experience in monitoring Hadoop clusters on VM Horton Works Data Platform 21 and 22 CDH5 Cloudera Manager HDP on Linux Using Kafka HDFS connector to export data from Kafka topics to HDFS files in a variety of formats and integrate with Apache Hive to make data immediately available for querying with Hive QL Good exposure to NoSQL Databases and hands on work experience in using HBase Experience in using Oozie workflow scheduler to manage Hadoop jobs by Directed Acyclic Graph DAG of actions with control flows Experience in designing and developing applications in Spark using Python to compare the performance of Spark with Hive and SQLOracle Flexible with UnixLinux and Windows Environments working with Operating Systems like Cent OS 56 Ubuntu 1314 Experience in manipulating the streaming data to clusters through Kafka and Apache SparkStreaming Experience in Continuous Integration and Continuous Deployment by the tools like Jenkins Learning to use Amazon AWS EMR and EC2 for cloud big data processing Experience in all phases of Software Development Life Cycle including analysis design implementation integration deployment testing using different software methodologies like Agile Scrum Waterfall models Technically well versed in designing and developing business solutions in the field of Banking Insurance Ecommerce Manufacturing divisions Experienced with the Apache Spark improving the performance and optimization of the existing algorithms in Hadoop using Apache Spark Context Apache SparkSQL Data Frame Pair RDDs Apache Spark YARN Good knowledge on Hadoop MRV1 and Hadoop MRV2 or YARN Architecture An effective team player with excellent communication analytical and interpersonal skills with exceptional planning and execution skills with a systematic approach and quick adaptability Experience with databases such as Oracle   PostgreSQL MySQL Server with cluster setup and writing the SQL queries Triggers Stored Procedures Authorized to work in the US for any employer Work Experience Sr Hadoop Developer CVS Health Northbrook IL January 2019 to Present Responsibilities Designed and implemented scalable infrastructure and platform for large amounts of data ingestion aggregation and integration in Hadoop MapReduce Flume Kafka Spark Hive Loaded large sets of structured semistructured and unstructured data using Sqoop Flume Kafka Written Sqoop scripts to import export and update the data between HDFS and Relational databases Created Flume configure files to collect aggregate and store the web log data into HDFS Involved in designing and creating data model to load customer data into a NoSQL database like HBase For data enrichment we perform a lookup over the table present in HBase to retrieve the data After enriching the data we validate it and analyze the data by performing bucketing and partitioning in Hive Developed Pig Latin scripts to transform the data and load into HDFS Import data from different sources like HDFSHBase into Spark RDD Utilized Kafka to capture and process real time streaming data Worked with Oozie and Zookeeper to manage job workflow and job coordination in the cluster Worked closely with the business analysts to convert the Business Requirements into Technical Requirements and prepared low and highlevel documentation Played a key role in discussing about the requirements analysis of the entire system along with development and testing Exported data from HDFS into RDBMS using Sqoop to generate reports Involved in diagnosing different possible ways to optimize and improve the efficiency of the system Participated in peerreviews of solution designs and related code Developed workflow scheduler job scripts in Apache Oozie to extract the outcomes on a daytoday basis Environment Hadoop263 MapReduce HDFS Yarn Sqoop143 Oozie Pig011 Hive110 HBase098 Spark2x Java Eclipse UNIX shell scripting python351 Cloudera manager 59X Flume Kafka Hadoop Java Developer Grand Bank of Texas Dallas TX July 2016 to November 2018 Responsibilities Grand Bank of Texas is a locallyowned and operated independent community bank to serve the financial needs of the community and area Roles and Responsibilities Experience with the Hadoop ecosystem MapReduce Pig Hive HBase and NoSQL Analyze and determine the relationship of input keys to output keys in terms of both type and number identify the number type and value of emitted keys and values from the Mappers Reducers and the number and contents of the output files Developed MapReduce pipeline jobs to process the data and create necessary HFiles and loading the HFiles into HBase for faster access without taking performance hit Designed developed UI Screens with Spring MVC HTML5 CSS JavaScript AngularJS to provide interactive screens to display data Written multiple MapReduce programs in java for data extraction transformation and aggregation from multiple file formats including XML JSON CSV and other file formats Analyze to determine the correct InputFormat OutputFormat on order of MapReduce job requirements Created database tables and wrote TSQL Queries and stored procedures to create complex join tables and to perform CRUD operations By using AWS MapReduce job processed the data stored in the AWS Using Automated Build and continuous integration systems Jenkins and testdriven Unit Testing framework Junit Access control to users depending on logins using HTML jQuery for validations Created the web application using HTML CSS jQuery and JavaScript Used Eclipse as an IDE for developing the application Loaded the flat files data using Informatica to the staging area Used UIrouter in AngularJS to make this a single page application Developed unitassembly test cases and UNIX shell scripts to run along with dailyweeklymonthly batches to reduce or eliminate manual testing effort Developed mappings in Informatica to load the data including facts and dimensions from various sources into the Data Warehouse using different transformations like Source Qualifier Java Expression Lookup Aggregate Update Strategy and Joiner Environment Windows XPNT Java MapReduce Pig Hive Hbase NoSQL AWS Jenkins HTML CSS TSQL AngularJS UI jQuery Korn Shell Quality Center 10 Hadoop Developer East West Bank Pasadena CA March 2014 to June 2016 Responsibilities Installed and configured Apache Hadoop to test the maintenance of log files in Hadoop cluster Installed and configured Hive Pig Sqoop Flume and Oozie on the Hadoop cluster Developed Sqoop jobs to import and store massive volumes of data in HDFS and Hive Designed and developed PIG data transformation scripts to work against unstructured data from various data points and created a base line Setup and benchmarked Hadoop HBase clusters for internal use Developed Java MapReduce programs for the analysis of sample log file stored in cluster Developed Map Reduce Programs for data analysis and data cleaning Developed PIG UDFs like UTAFs UDAFs for manipulating the data according to business requirements and worked on developing custom PIG Loaders Implemented data pipeline by chaining multiple mappers by using Chained Mapper Used Hive and created Hive tables and involved in data loading and writing Hive UDFs Migration of processes from Oracle to Hive to test the easy data manipulation Configured deployed and maintained multinode Dev and Test Kafka Clusters Implemented data injection systems by creating Kafka brokers producers Consumers custom encoders Implemented Partitioning Dynamic Partitions and Bucketing in Hive for efficient data access Developed a data pipeline using Kafka and Strom to store data into HDFS Developed some utility helper classes to get data from HBase tables Good experience in troubleshooting performance issues and tuning Hadoop cluster Built components modules and plugins using Angular JS and Bootstrap Created Java Interfaces and Abstract classes for different functionalities Loaded and transformed large sets of structured semistructured and unstructured data in various formats like text sequence XML and JSON Written multiple MapReduce programs to power data for extraction transformation and aggregation from multiple file formats including XML JSON CSV other compressed file formats Environment Apache Hadoop HDFS Cloudera Manager Java MapReduce Eclipse Hive PIG Sqoop Oozie SQL Zookeeper CDH3 Cassandra Oracle NoSQL and UnixLinux Kafka Big Data Engineer Caesars Las Vegas NV August 2012 to February 2014 Responsibilities Responsible for creating sample Datasets required for testing various Map Reduce applications from various sources Developed Hive UDF to parse the staged raw data to get the item details from a specific store Built reusable Hive UDF libraries for business requirements which enabled users to use these UDFs in Hive querying Designed workflow by scheduling Hive processes for Log file data which is streamed into HDFS using Flume Developed SQL scripts to compare all the records for every field and table at each phase of the data movement process from the original source system to the final target Implemented map reduce jobs to process standard data in Hadoop cluster Involved in the performance enhancement by analyzing the workflows joins configuration parameters etc Collaborating with business usersproduct owners developers to contribute to the analysis of functional requirements Design Develop workflow using Oozie for business requirements which includes automating the extraction of data from MySQL database into HDFS using Sqoop scripts Worked on migration of Informatica Power center to Hadoop eco system Assisted with the admin department with issues related to migration Performed data analytics in Hive and then exported this metrics back to Oracle Database using Sqoop Provided adhoc queries and data metrics to the Business Users using Hive Pig Developed Map Reduce Programs in Java for applying business rules on the data Implemented Partitioning Dynamic Partitions Bucketing in Hive Document and manage failurerecovery steps for any production issues Involved in Minor and Major Release work activities Proactively involved in ongoing maintenance support and improvements in Hadoop cluster Import the data from various sources like HDFSHBase into Kafka Wrote MapReduce jobs using java and Pig Latin Worked on NoSQL databases including HBase and Cassandra Configured SQL Database to store Hive Teradata Participated in developmentimplementation of Cloudera impala Hadoop environment Imported data using Sqoop to load data from MySQL to HDFS on regular basis Used Pig as ETL Informatica tool to do Transformations even joins and some preaggregations before storing the data onto HDFS Developed Application components and APIs using Scala Created ETL Informatica jobs to generate and distribute reports from MySQL database Involved in loading data from LINUX file system to HDFS using Sqoop and exported the analyzed data to the relational databases using Sqoop for visualization and to generate reports for the Business intelligence BI team Installed and configured Flume Hive Pig Sqoop and Oozie on the Hadoop cluster Extracted the Teradata from Oracle into Hive using the Sqoop Worked on Agile Methodology Environment Hadoop HDFS Pig Zookeeper Sqoop HBase Shell Scripting Ubuntu Linux Red Hat Kafka Cassandra Senior Hadoop Developer AFLAC Columbus GA March 2011 to July 2012 Responsibilities Worked closely with the business analysts to convert the Business Requirements into Technical Requirements and preparing low and highlevel documentation Performing transformations using Hive MapReduce hands on experience in copying log snappy files into HDFS from Greenplum using Flume Kafka loaded data into HDFS and extracted the data into HDFS from MYSQL using Sqoop Involved in preparing the S2TM document as per the business requirement and worked with Source system SMEs in understanding the source data behavior Imported required tables from RDBMS to HDFS using Sqoop and used Storm Spark streaming and Kafka to get real time streaming of data into HBase Experience in Writing Map Reduce jobs for text mining and worked with predictive analysis team and Experience in working with Hadoop components such as HBase Spark Yarn Kafka Zookeeper PIG HIVE Sqoop Oozie Impala and Flume Wrote HIVE UDFs as per requirements and to handle different schemas and xml data Designing and developing MapReduce jobs to process data coming in different file formats like XML CSV JSON Involved in Apache SPARK testing Implemented ETL code to load data from multiple sources into HDFS using Pig Scripts Implemented MapReduce programs to handle semi unstructured data like XML JSON Avro data files and sequence files for log files Responsible to review the test cases in HP ALM Developed Spark applications using Scala for easy Hadoop transitions And Hands on experienced in writing Spark jobs and Spark streaming API using Scala and Python Used Spark API over Cloudera Hadoop YARN to perform analytics on data in Hive developed Spark code and SparkSQLStreaming for faster testing and processing of data Installed Oozie workflow engine to run multiple Hive and Pig jobs Designed and developed User Defined Function UDF for Hive and Developed the Pig UDFS to preprocess the data for analysis as well as experience in UDAFs for custom data specific processing Assisted in problem solving with Big Data technologies for integration of Hive with HBase and Sqoop with HBase Designed and developed the core data pipeline code involving work in Java and Python and built on Kafka and Storm Good knowledge on Partitions bucketing concepts in Hive and designed both Managed and External tables in Hive for optimized performance Performance tuning using Partitioning bucketing of IMPALA tables Hands on experience on fetching the live stream data from DB2 to HBase table using Spark Streaming and Apache Kafka Experience in job workflow scheduling and monitoring tools like Oozie and Zookeeper Worked on NoSQL databases including HBase and Cassandra Populated HDFS and Cassandra with huge amounts of data using Apache Kafka Created POC Proof of Concept to store Server Log data into Cassandra to identify System Alert Metrics Environment Map Reduce HDFS Hive Pig HBase Python SQL Sqoop Flume Oozie Impala Scala Spark Apache Kafka Zookeeper J2EE Linux Red Hat HPALM Eclipse Cassandra Talend Informatica Hadoop Developer Wellcare Health Plans Tampa FL June 2010 to February 2011 Responsibilities WellCare focuses exclusively on providing governmentsponsored managed care services primarily through Medicaid Medicare Advantage and Medicare Prescription Drug Plans to families children seniors and individuals with complex medical needs Roles and Responsibilities Experience in creating integration between Hive and HBase for effective usage and performed MR Unit testing for the Map Reduce jobs Created BI reports Tableau and dashboards from HDFS data using Hive Experience in importing and exporting the data from Relational Database Systems to HDFS by using Sqoop Developed a common framework to import the data from Teradata to HDFS and to export to Teradata using Sqoop Imported the log data from different servers into HDFS using Flume and developed MapReduce programs for analyzing the data Used Flume to handle the real time log processing for attribution reports Worked on tuning the performance of Pig queries Involved in loading data from UNIX file system to HDFS Performed operation using Partitioning pattern in MapReduce to move records into different categories Implemented Fair schedulers on the Job tracker to share the resources of the Cluster for the MapReduce jobs given by the users Involved in templates and screens in HTML and JavaScript Created HBase tables to load large sets of data coming from UNIX and NoSQL Implemented the Web Service client for the login authentication credit reports and applicant information using Apache Axis 2 Web Service Design develop test implement and support of Data Warehousing ETL using Talend and Hadoop Technologies Built and deployed Java applications into multiple Unix based environments and produced both unit and functional test results along with release notes Environment WebSphere 61 HTML XML ANT 16 MapReduce Sqoop UNIX NoSQL Java JavaScript MR Unit Teradata Nodejs JUnit 38 ETL Talend HDFS Hive HBase Education Bachelors Skills APACHE HADOOP MAPREDUCE 9 years APACHE HBASE 9 years Hadoop 9 years HADOOP 9 years HBase 9 years Additional Information TECHNICAL SKILLS Big Data Technologies Hadoop HDFS MapReduce Spark Hive Sqoop Oozie Flume HBase Pig Zookeeper Kafka Programming Languages Linux Programming Basics of Python language Hive SQL JAVA Pig Latin Web Development JavaScript JQuery HTML 50 CSS 30 AJAX JSON Tools CDH 591 Cloudera Navigator Cloudera Manager Basics VMware Web Servers Apache Tomcat Methodologies Agile Vmodel Waterfall model Operating Systems Linux CentOS Windows Databases MySQL Oracle 11g10 g HBase Mongo DB Cassandra Couch MS SQL server",
    "extracted_keywords": [
        "Sr",
        "Hadoop",
        "Developer",
        "Sr",
        "Hadoop",
        "span",
        "lDeveloperspan",
        "Sr",
        "Hadoop",
        "Developer",
        "CVS",
        "Health",
        "Los",
        "Angeles",
        "CA",
        "years",
        "IT",
        "experience",
        "variety",
        "industries",
        "years",
        "Hadoop",
        "developer",
        "endtoend",
        "Hadoop",
        "infrastructure",
        "MapReduce",
        "Pig",
        "Hive",
        "Sqoop",
        "Oozie",
        "Flume",
        "Spark",
        "HBase",
        "Zookeeper",
        "Excellent",
        "knowledge",
        "Hadoop",
        "Architecture",
        "components",
        "HDFS",
        "Job",
        "Tracker",
        "Task",
        "Tracker",
        "Name",
        "Node",
        "Data",
        "Node",
        "MapReduce",
        "Programming",
        "paradigm",
        "Hands",
        "expertise",
        "SQL",
        "PLSQL",
        "UNIX",
        "shell",
        "scripts",
        "commands",
        "Experience",
        "Hive",
        "tables",
        "queries",
        "HiveQL",
        "understanding",
        "Hive",
        "concepts",
        "Partitioning",
        "Bucketing",
        "Joins",
        "Developed",
        "Pig",
        "Latin",
        "scripts",
        "data",
        "HDFS",
        "Experience",
        "data",
        "Sqoop",
        "Relational",
        "Database",
        "Systems",
        "HDFS",
        "viceversa",
        "Experience",
        "data",
        "Log",
        "files",
        "Xml",
        "data",
        "HDFS",
        "Flume",
        "Hands",
        "expertise",
        "file",
        "formats",
        "XML",
        "CSV",
        "exposure",
        "job",
        "streaming",
        "scheduling",
        "Oozie",
        "system",
        "Kafka",
        "Zookeeper",
        "Experience",
        "Hadoop",
        "clusters",
        "VM",
        "Horton",
        "Works",
        "Data",
        "Platform",
        "CDH5",
        "Cloudera",
        "Manager",
        "HDP",
        "Linux",
        "Kafka",
        "HDFS",
        "connector",
        "data",
        "Kafka",
        "topics",
        "files",
        "variety",
        "formats",
        "Apache",
        "Hive",
        "data",
        "Hive",
        "QL",
        "exposure",
        "NoSQL",
        "Databases",
        "hands",
        "work",
        "experience",
        "HBase",
        "Experience",
        "Oozie",
        "workflow",
        "scheduler",
        "Hadoop",
        "jobs",
        "Directed",
        "Acyclic",
        "Graph",
        "DAG",
        "actions",
        "control",
        "Experience",
        "applications",
        "Spark",
        "Python",
        "performance",
        "Spark",
        "Hive",
        "SQLOracle",
        "Flexible",
        "UnixLinux",
        "Windows",
        "Environments",
        "Operating",
        "Systems",
        "Cent",
        "Ubuntu",
        "Experience",
        "data",
        "clusters",
        "Kafka",
        "Apache",
        "SparkStreaming",
        "Experience",
        "Continuous",
        "Integration",
        "Continuous",
        "Deployment",
        "tools",
        "Jenkins",
        "Learning",
        "Amazon",
        "AWS",
        "EMR",
        "EC2",
        "data",
        "processing",
        "Experience",
        "phases",
        "Software",
        "Development",
        "Life",
        "Cycle",
        "analysis",
        "design",
        "implementation",
        "integration",
        "deployment",
        "testing",
        "software",
        "methodologies",
        "Agile",
        "Scrum",
        "Waterfall",
        "models",
        "business",
        "solutions",
        "field",
        "Banking",
        "Insurance",
        "Ecommerce",
        "Manufacturing",
        "divisions",
        "Apache",
        "Spark",
        "performance",
        "optimization",
        "algorithms",
        "Hadoop",
        "Apache",
        "Spark",
        "Context",
        "Apache",
        "SparkSQL",
        "Data",
        "Frame",
        "Pair",
        "Apache",
        "Spark",
        "YARN",
        "knowledge",
        "Hadoop",
        "MRV1",
        "Hadoop",
        "MRV2",
        "YARN",
        "Architecture",
        "team",
        "player",
        "communication",
        "skills",
        "planning",
        "execution",
        "skills",
        "approach",
        "adaptability",
        "Experience",
        "databases",
        "Oracle",
        "PostgreSQL",
        "MySQL",
        "Server",
        "cluster",
        "setup",
        "SQL",
        "Triggers",
        "Stored",
        "Procedures",
        "US",
        "employer",
        "Work",
        "Experience",
        "Sr",
        "Hadoop",
        "Developer",
        "CVS",
        "Health",
        "Northbrook",
        "IL",
        "January",
        "Present",
        "Responsibilities",
        "infrastructure",
        "platform",
        "amounts",
        "data",
        "ingestion",
        "aggregation",
        "integration",
        "Hadoop",
        "MapReduce",
        "Flume",
        "Kafka",
        "Spark",
        "Hive",
        "sets",
        "data",
        "Sqoop",
        "Flume",
        "Kafka",
        "Written",
        "Sqoop",
        "export",
        "data",
        "HDFS",
        "Relational",
        "databases",
        "Created",
        "Flume",
        "configure",
        "files",
        "aggregate",
        "web",
        "log",
        "data",
        "HDFS",
        "data",
        "model",
        "customer",
        "data",
        "NoSQL",
        "database",
        "HBase",
        "data",
        "enrichment",
        "lookup",
        "table",
        "HBase",
        "data",
        "data",
        "data",
        "bucketing",
        "Hive",
        "Developed",
        "Pig",
        "Latin",
        "scripts",
        "data",
        "load",
        "HDFS",
        "Import",
        "data",
        "sources",
        "HDFSHBase",
        "Spark",
        "RDD",
        "Kafka",
        "time",
        "data",
        "Oozie",
        "Zookeeper",
        "job",
        "workflow",
        "job",
        "coordination",
        "cluster",
        "business",
        "analysts",
        "Business",
        "Requirements",
        "Technical",
        "Requirements",
        "highlevel",
        "documentation",
        "role",
        "requirements",
        "analysis",
        "system",
        "development",
        "data",
        "HDFS",
        "RDBMS",
        "Sqoop",
        "reports",
        "ways",
        "efficiency",
        "system",
        "peerreviews",
        "solution",
        "designs",
        "code",
        "workflow",
        "scheduler",
        "job",
        "scripts",
        "Apache",
        "Oozie",
        "outcomes",
        "basis",
        "Environment",
        "MapReduce",
        "HDFS",
        "Yarn",
        "Sqoop143",
        "Oozie",
        "Pig011",
        "Hive110",
        "HBase098",
        "Spark2x",
        "Java",
        "Eclipse",
        "UNIX",
        "shell",
        "scripting",
        "Cloudera",
        "manager",
        "59X",
        "Flume",
        "Kafka",
        "Hadoop",
        "Java",
        "Developer",
        "Grand",
        "Bank",
        "Texas",
        "Dallas",
        "TX",
        "July",
        "November",
        "Responsibilities",
        "Grand",
        "Bank",
        "Texas",
        "community",
        "bank",
        "needs",
        "community",
        "area",
        "Roles",
        "Responsibilities",
        "Experience",
        "Hadoop",
        "ecosystem",
        "MapReduce",
        "Pig",
        "Hive",
        "HBase",
        "NoSQL",
        "relationship",
        "input",
        "keys",
        "keys",
        "terms",
        "type",
        "number",
        "number",
        "type",
        "value",
        "keys",
        "values",
        "Mappers",
        "Reducers",
        "number",
        "contents",
        "output",
        "MapReduce",
        "pipeline",
        "jobs",
        "data",
        "HFiles",
        "HFiles",
        "HBase",
        "access",
        "performance",
        "UI",
        "Screens",
        "Spring",
        "MVC",
        "HTML5",
        "CSS",
        "JavaScript",
        "AngularJS",
        "screens",
        "data",
        "MapReduce",
        "programs",
        "java",
        "data",
        "extraction",
        "transformation",
        "aggregation",
        "file",
        "formats",
        "XML",
        "CSV",
        "file",
        "formats",
        "InputFormat",
        "OutputFormat",
        "order",
        "MapReduce",
        "job",
        "requirements",
        "database",
        "tables",
        "TSQL",
        "Queries",
        "procedures",
        "join",
        "tables",
        "CRUD",
        "operations",
        "AWS",
        "MapReduce",
        "job",
        "data",
        "AWS",
        "Automated",
        "Build",
        "integration",
        "systems",
        "Jenkins",
        "testdriven",
        "Unit",
        "Testing",
        "framework",
        "Junit",
        "Access",
        "control",
        "users",
        "logins",
        "HTML",
        "jQuery",
        "validations",
        "web",
        "application",
        "HTML",
        "CSS",
        "jQuery",
        "JavaScript",
        "Eclipse",
        "IDE",
        "application",
        "files",
        "data",
        "Informatica",
        "staging",
        "area",
        "UIrouter",
        "page",
        "application",
        "cases",
        "UNIX",
        "shell",
        "scripts",
        "batches",
        "testing",
        "effort",
        "mappings",
        "Informatica",
        "data",
        "facts",
        "dimensions",
        "sources",
        "Data",
        "Warehouse",
        "transformations",
        "Source",
        "Qualifier",
        "Java",
        "Expression",
        "Lookup",
        "Aggregate",
        "Update",
        "Strategy",
        "Joiner",
        "Environment",
        "XPNT",
        "Java",
        "MapReduce",
        "Pig",
        "Hive",
        "Hbase",
        "NoSQL",
        "Jenkins",
        "HTML",
        "CSS",
        "TSQL",
        "UI",
        "jQuery",
        "Korn",
        "Shell",
        "Quality",
        "Center",
        "Hadoop",
        "Developer",
        "East",
        "West",
        "Bank",
        "Pasadena",
        "CA",
        "March",
        "June",
        "Responsibilities",
        "Apache",
        "Hadoop",
        "maintenance",
        "log",
        "files",
        "Hadoop",
        "cluster",
        "Installed",
        "Hive",
        "Pig",
        "Sqoop",
        "Flume",
        "Oozie",
        "Hadoop",
        "cluster",
        "Developed",
        "Sqoop",
        "jobs",
        "volumes",
        "data",
        "HDFS",
        "Hive",
        "PIG",
        "data",
        "transformation",
        "scripts",
        "data",
        "data",
        "points",
        "base",
        "line",
        "Setup",
        "Hadoop",
        "HBase",
        "clusters",
        "use",
        "Java",
        "MapReduce",
        "programs",
        "analysis",
        "sample",
        "log",
        "file",
        "cluster",
        "Developed",
        "Map",
        "Reduce",
        "Programs",
        "data",
        "analysis",
        "data",
        "PIG",
        "UDFs",
        "UTAFs",
        "UDAFs",
        "data",
        "business",
        "requirements",
        "custom",
        "PIG",
        "Loaders",
        "data",
        "pipeline",
        "mappers",
        "Chained",
        "Mapper",
        "Hive",
        "Hive",
        "tables",
        "data",
        "loading",
        "Hive",
        "UDFs",
        "Migration",
        "processes",
        "Oracle",
        "Hive",
        "data",
        "manipulation",
        "Configured",
        "multinode",
        "Dev",
        "Test",
        "Kafka",
        "Clusters",
        "data",
        "injection",
        "systems",
        "Kafka",
        "brokers",
        "producers",
        "Consumers",
        "custom",
        "encoders",
        "Partitioning",
        "Dynamic",
        "Partitions",
        "Bucketing",
        "Hive",
        "data",
        "access",
        "data",
        "pipeline",
        "Kafka",
        "Strom",
        "data",
        "HDFS",
        "utility",
        "helper",
        "classes",
        "data",
        "HBase",
        "experience",
        "troubleshooting",
        "performance",
        "issues",
        "Hadoop",
        "cluster",
        "components",
        "modules",
        "plugins",
        "Angular",
        "JS",
        "Bootstrap",
        "Java",
        "Interfaces",
        "classes",
        "functionalities",
        "sets",
        "data",
        "formats",
        "text",
        "sequence",
        "XML",
        "JSON",
        "MapReduce",
        "programs",
        "power",
        "data",
        "extraction",
        "transformation",
        "aggregation",
        "file",
        "formats",
        "XML",
        "CSV",
        "file",
        "formats",
        "Environment",
        "Apache",
        "Hadoop",
        "HDFS",
        "Cloudera",
        "Manager",
        "Java",
        "MapReduce",
        "Eclipse",
        "Hive",
        "PIG",
        "Sqoop",
        "Oozie",
        "SQL",
        "Zookeeper",
        "CDH3",
        "Cassandra",
        "Oracle",
        "NoSQL",
        "UnixLinux",
        "Kafka",
        "Big",
        "Data",
        "Engineer",
        "Caesars",
        "Las",
        "Vegas",
        "NV",
        "August",
        "February",
        "Responsibilities",
        "sample",
        "Datasets",
        "Map",
        "Reduce",
        "applications",
        "sources",
        "Hive",
        "UDF",
        "data",
        "item",
        "details",
        "store",
        "Hive",
        "UDF",
        "business",
        "requirements",
        "users",
        "UDFs",
        "Hive",
        "workflow",
        "scheduling",
        "Hive",
        "processes",
        "Log",
        "file",
        "data",
        "HDFS",
        "Flume",
        "Developed",
        "SQL",
        "scripts",
        "records",
        "field",
        "table",
        "phase",
        "data",
        "movement",
        "process",
        "source",
        "system",
        "target",
        "map",
        "jobs",
        "data",
        "Hadoop",
        "cluster",
        "performance",
        "enhancement",
        "workflows",
        "configuration",
        "parameters",
        "business",
        "usersproduct",
        "owners",
        "developers",
        "analysis",
        "requirements",
        "Design",
        "Develop",
        "workflow",
        "Oozie",
        "business",
        "requirements",
        "extraction",
        "data",
        "MySQL",
        "database",
        "HDFS",
        "Sqoop",
        "scripts",
        "migration",
        "Informatica",
        "Power",
        "center",
        "Hadoop",
        "eco",
        "system",
        "admin",
        "department",
        "issues",
        "migration",
        "Performed",
        "data",
        "analytics",
        "Hive",
        "metrics",
        "Oracle",
        "Database",
        "Sqoop",
        "queries",
        "data",
        "metrics",
        "Business",
        "Users",
        "Hive",
        "Pig",
        "Developed",
        "Map",
        "Reduce",
        "Programs",
        "Java",
        "business",
        "rules",
        "data",
        "Partitioning",
        "Dynamic",
        "Partitions",
        "Bucketing",
        "Hive",
        "Document",
        "failurerecovery",
        "steps",
        "production",
        "issues",
        "Minor",
        "Major",
        "Release",
        "work",
        "activities",
        "maintenance",
        "support",
        "improvements",
        "Hadoop",
        "cluster",
        "Import",
        "data",
        "sources",
        "HDFSHBase",
        "Kafka",
        "Wrote",
        "MapReduce",
        "jobs",
        "Pig",
        "Latin",
        "NoSQL",
        "databases",
        "HBase",
        "Cassandra",
        "Configured",
        "SQL",
        "Database",
        "Hive",
        "Teradata",
        "Participated",
        "developmentimplementation",
        "Cloudera",
        "impala",
        "Hadoop",
        "environment",
        "data",
        "Sqoop",
        "data",
        "MySQL",
        "HDFS",
        "basis",
        "Pig",
        "ETL",
        "Informatica",
        "tool",
        "Transformations",
        "preaggregations",
        "data",
        "HDFS",
        "Developed",
        "Application",
        "components",
        "APIs",
        "Scala",
        "Created",
        "ETL",
        "Informatica",
        "jobs",
        "reports",
        "MySQL",
        "database",
        "loading",
        "data",
        "LINUX",
        "file",
        "system",
        "HDFS",
        "Sqoop",
        "data",
        "databases",
        "Sqoop",
        "visualization",
        "reports",
        "Business",
        "intelligence",
        "BI",
        "team",
        "Flume",
        "Hive",
        "Pig",
        "Sqoop",
        "Oozie",
        "Hadoop",
        "cluster",
        "Teradata",
        "Oracle",
        "Hive",
        "Sqoop",
        "Worked",
        "Agile",
        "Methodology",
        "Environment",
        "Hadoop",
        "HDFS",
        "Pig",
        "Zookeeper",
        "Sqoop",
        "HBase",
        "Shell",
        "Scripting",
        "Ubuntu",
        "Linux",
        "Red",
        "Hat",
        "Kafka",
        "Cassandra",
        "Senior",
        "Hadoop",
        "Developer",
        "AFLAC",
        "Columbus",
        "GA",
        "March",
        "July",
        "Responsibilities",
        "business",
        "analysts",
        "Business",
        "Requirements",
        "Technical",
        "Requirements",
        "highlevel",
        "documentation",
        "transformations",
        "Hive",
        "MapReduce",
        "hands",
        "experience",
        "log",
        "files",
        "HDFS",
        "Greenplum",
        "Flume",
        "Kafka",
        "data",
        "HDFS",
        "data",
        "HDFS",
        "MYSQL",
        "Sqoop",
        "S2TM",
        "document",
        "business",
        "requirement",
        "Source",
        "system",
        "SMEs",
        "source",
        "data",
        "behavior",
        "tables",
        "RDBMS",
        "HDFS",
        "Sqoop",
        "Storm",
        "Spark",
        "streaming",
        "Kafka",
        "time",
        "streaming",
        "data",
        "HBase",
        "Experience",
        "Map",
        "jobs",
        "text",
        "mining",
        "analysis",
        "team",
        "Experience",
        "Hadoop",
        "components",
        "HBase",
        "Spark",
        "Yarn",
        "Kafka",
        "Zookeeper",
        "PIG",
        "HIVE",
        "Sqoop",
        "Oozie",
        "Impala",
        "Flume",
        "Wrote",
        "HIVE",
        "UDFs",
        "requirements",
        "schemas",
        "xml",
        "data",
        "Designing",
        "MapReduce",
        "jobs",
        "data",
        "file",
        "formats",
        "XML",
        "CSV",
        "JSON",
        "Apache",
        "SPARK",
        "testing",
        "ETL",
        "code",
        "data",
        "sources",
        "HDFS",
        "Pig",
        "Scripts",
        "MapReduce",
        "programs",
        "data",
        "XML",
        "JSON",
        "Avro",
        "data",
        "files",
        "sequence",
        "files",
        "log",
        "files",
        "test",
        "cases",
        "HP",
        "ALM",
        "Spark",
        "applications",
        "Scala",
        "Hadoop",
        "transitions",
        "Hands",
        "Spark",
        "jobs",
        "API",
        "Scala",
        "Python",
        "Spark",
        "API",
        "Cloudera",
        "Hadoop",
        "YARN",
        "analytics",
        "data",
        "Hive",
        "Spark",
        "code",
        "SparkSQLStreaming",
        "testing",
        "processing",
        "data",
        "Oozie",
        "workflow",
        "engine",
        "Hive",
        "Pig",
        "jobs",
        "User",
        "Defined",
        "Function",
        "UDF",
        "Hive",
        "Developed",
        "Pig",
        "UDFS",
        "data",
        "analysis",
        "experience",
        "UDAFs",
        "custom",
        "data",
        "processing",
        "problem",
        "Big",
        "Data",
        "technologies",
        "integration",
        "Hive",
        "HBase",
        "Sqoop",
        "HBase",
        "core",
        "data",
        "pipeline",
        "code",
        "work",
        "Java",
        "Python",
        "Kafka",
        "Storm",
        "knowledge",
        "Partitions",
        "bucketing",
        "concepts",
        "Hive",
        "Managed",
        "tables",
        "Hive",
        "performance",
        "Performance",
        "Partitioning",
        "bucketing",
        "IMPALA",
        "Hands",
        "experience",
        "stream",
        "data",
        "DB2",
        "HBase",
        "table",
        "Spark",
        "Streaming",
        "Apache",
        "Kafka",
        "Experience",
        "job",
        "workflow",
        "scheduling",
        "monitoring",
        "tools",
        "Oozie",
        "Zookeeper",
        "NoSQL",
        "databases",
        "HBase",
        "Cassandra",
        "Populated",
        "HDFS",
        "Cassandra",
        "amounts",
        "data",
        "Apache",
        "Kafka",
        "Created",
        "POC",
        "Proof",
        "Concept",
        "Server",
        "Log",
        "data",
        "Cassandra",
        "System",
        "Alert",
        "Metrics",
        "Environment",
        "Map",
        "HDFS",
        "Hive",
        "Pig",
        "HBase",
        "Python",
        "SQL",
        "Sqoop",
        "Flume",
        "Oozie",
        "Impala",
        "Scala",
        "Spark",
        "Apache",
        "Kafka",
        "Zookeeper",
        "J2EE",
        "Linux",
        "Red",
        "Hat",
        "HPALM",
        "Eclipse",
        "Cassandra",
        "Talend",
        "Informatica",
        "Hadoop",
        "Developer",
        "Wellcare",
        "Health",
        "Tampa",
        "FL",
        "June",
        "February",
        "Responsibilities",
        "WellCare",
        "care",
        "services",
        "Medicaid",
        "Medicare",
        "Advantage",
        "Medicare",
        "Prescription",
        "Drug",
        "Plans",
        "families",
        "children",
        "seniors",
        "individuals",
        "needs",
        "Roles",
        "Responsibilities",
        "Experience",
        "integration",
        "Hive",
        "HBase",
        "usage",
        "MR",
        "Unit",
        "testing",
        "Map",
        "Reduce",
        "jobs",
        "BI",
        "Tableau",
        "dashboards",
        "HDFS",
        "data",
        "Hive",
        "Experience",
        "data",
        "Relational",
        "Database",
        "Systems",
        "HDFS",
        "Sqoop",
        "framework",
        "data",
        "Teradata",
        "HDFS",
        "Teradata",
        "Sqoop",
        "log",
        "data",
        "servers",
        "HDFS",
        "Flume",
        "MapReduce",
        "programs",
        "data",
        "Flume",
        "time",
        "log",
        "processing",
        "attribution",
        "reports",
        "performance",
        "Pig",
        "queries",
        "loading",
        "data",
        "UNIX",
        "file",
        "system",
        "HDFS",
        "Performed",
        "operation",
        "Partitioning",
        "pattern",
        "MapReduce",
        "records",
        "categories",
        "Fair",
        "schedulers",
        "Job",
        "tracker",
        "resources",
        "Cluster",
        "MapReduce",
        "jobs",
        "users",
        "templates",
        "screens",
        "HTML",
        "JavaScript",
        "Created",
        "HBase",
        "sets",
        "data",
        "UNIX",
        "NoSQL",
        "Web",
        "Service",
        "client",
        "login",
        "authentication",
        "credit",
        "reports",
        "information",
        "Apache",
        "Axis",
        "Web",
        "Service",
        "Design",
        "test",
        "implement",
        "support",
        "Data",
        "Warehousing",
        "ETL",
        "Talend",
        "Hadoop",
        "Technologies",
        "Java",
        "applications",
        "Unix",
        "environments",
        "unit",
        "test",
        "results",
        "release",
        "notes",
        "Environment",
        "WebSphere",
        "HTML",
        "XML",
        "ANT",
        "MapReduce",
        "Sqoop",
        "UNIX",
        "NoSQL",
        "Java",
        "JavaScript",
        "MR",
        "Unit",
        "Teradata",
        "Nodejs",
        "JUnit",
        "ETL",
        "Talend",
        "HDFS",
        "Hive",
        "HBase",
        "Education",
        "Bachelors",
        "Skills",
        "APACHE",
        "HADOOP",
        "MAPREDUCE",
        "years",
        "APACHE",
        "HBASE",
        "years",
        "Hadoop",
        "years",
        "HADOOP",
        "years",
        "HBase",
        "years",
        "Information",
        "TECHNICAL",
        "SKILLS",
        "Big",
        "Data",
        "Technologies",
        "Hadoop",
        "HDFS",
        "MapReduce",
        "Spark",
        "Hive",
        "Sqoop",
        "Oozie",
        "Flume",
        "HBase",
        "Pig",
        "Zookeeper",
        "Kafka",
        "Programming",
        "Languages",
        "Linux",
        "Programming",
        "Basics",
        "Python",
        "language",
        "Hive",
        "SQL",
        "JAVA",
        "Pig",
        "Latin",
        "Web",
        "Development",
        "JavaScript",
        "JQuery",
        "HTML",
        "CSS",
        "AJAX",
        "JSON",
        "Tools",
        "CDH",
        "Cloudera",
        "Navigator",
        "Cloudera",
        "Manager",
        "Basics",
        "VMware",
        "Web",
        "Servers",
        "Apache",
        "Tomcat",
        "Methodologies",
        "Agile",
        "Vmodel",
        "Waterfall",
        "model",
        "Operating",
        "Systems",
        "Linux",
        "CentOS",
        "Windows",
        "MySQL",
        "Oracle",
        "g",
        "HBase",
        "Mongo",
        "DB",
        "Cassandra",
        "Couch",
        "MS",
        "SQL",
        "server"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T19:47:06.501070",
    "resume_data": "Sr Hadoop Developer Sr Hadoop span lDeveloperspan Sr Hadoop Developer CVS Health Los Angeles CA Around 8 years of IT experience in a variety of industries which includes 4 years of working as a Hadoop developer designing and implementing complete endtoend Hadoop infrastructure using MapReduce Pig Hive Sqoop Oozie Flume Spark HBase and Zookeeper Excellent knowledge of Hadoop Architecture and its various components such as HDFS Job Tracker Task Tracker Name Node Data Node and MapReduce Programming paradigm Hands on expertise with SQL PLSQL UNIX shell scripts and commands Experience in creating Hive tables and queries using HiveQL with a good understanding of Hive concepts such as Partitioning Bucketing and Joins Developed Pig Latin scripts to extract and load the data into HDFS Experience in migrating the data using Sqoop from Relational Database Systems to HDFS and viceversa Experience in loading unstructured data Log files Xml data into HDFS using Flume Hands on expertise with different file formats like JSON XML CSV etc Good exposure on setting up job streaming and scheduling with Oozie and working on messaging system such as Kafka integrated with Zookeeper Experience in monitoring Hadoop clusters on VM Horton Works Data Platform 21 and 22 CDH5 Cloudera Manager HDP on Linux Using Kafka HDFS connector to export data from Kafka topics to HDFS files in a variety of formats and integrate with Apache Hive to make data immediately available for querying with Hive QL Good exposure to NoSQL Databases and hands on work experience in using HBase Experience in using Oozie workflow scheduler to manage Hadoop jobs by Directed Acyclic Graph DAG of actions with control flows Experience in designing and developing applications in Spark using Python to compare the performance of Spark with Hive and SQLOracle Flexible with UnixLinux and Windows Environments working with Operating Systems like Cent OS 56 Ubuntu 1314 Experience in manipulating the streaming data to clusters through Kafka and Apache SparkStreaming Experience in Continuous Integration and Continuous Deployment by the tools like Jenkins Learning to use Amazon AWS EMR and EC2 for cloud big data processing Experience in all phases of Software Development Life Cycle including analysis design implementation integration deployment testing using different software methodologies like Agile Scrum Waterfall models Technically well versed in designing and developing business solutions in the field of Banking Insurance Ecommerce Manufacturing divisions Experienced with the Apache Spark improving the performance and optimization of the existing algorithms in Hadoop using Apache Spark Context Apache SparkSQL Data Frame Pair RDDs Apache Spark YARN Good knowledge on Hadoop MRV1 and Hadoop MRV2 or YARN Architecture An effective team player with excellent communication analytical and interpersonal skills with exceptional planning and execution skills with a systematic approach and quick adaptability Experience with databases such as Oracle 9i PostgreSQL MySQL Server with cluster setup and writing the SQL queries Triggers Stored Procedures Authorized to work in the US for any employer Work Experience Sr Hadoop Developer CVS Health Northbrook IL January 2019 to Present Responsibilities Designed and implemented scalable infrastructure and platform for large amounts of data ingestion aggregation and integration in Hadoop MapReduce Flume Kafka Spark Hive Loaded large sets of structured semistructured and unstructured data using Sqoop Flume Kafka Written Sqoop scripts to import export and update the data between HDFS and Relational databases Created Flume configure files to collect aggregate and store the web log data into HDFS Involved in designing and creating data model to load customer data into a NoSQL database like HBase For data enrichment we perform a lookup over the table present in HBase to retrieve the data After enriching the data we validate it and analyze the data by performing bucketing and partitioning in Hive Developed Pig Latin scripts to transform the data and load into HDFS Import data from different sources like HDFSHBase into Spark RDD Utilized Kafka to capture and process real time streaming data Worked with Oozie and Zookeeper to manage job workflow and job coordination in the cluster Worked closely with the business analysts to convert the Business Requirements into Technical Requirements and prepared low and highlevel documentation Played a key role in discussing about the requirements analysis of the entire system along with development and testing Exported data from HDFS into RDBMS using Sqoop to generate reports Involved in diagnosing different possible ways to optimize and improve the efficiency of the system Participated in peerreviews of solution designs and related code Developed workflow scheduler job scripts in Apache Oozie to extract the outcomes on a daytoday basis Environment Hadoop263 MapReduce HDFS Yarn Sqoop143 Oozie Pig011 Hive110 HBase098 Spark2x Java Eclipse UNIX shell scripting python351 Cloudera manager 59X Flume Kafka Hadoop Java Developer Grand Bank of Texas Dallas TX July 2016 to November 2018 Responsibilities Grand Bank of Texas is a locallyowned and operated independent community bank to serve the financial needs of the community and area Roles and Responsibilities Experience with the Hadoop ecosystem MapReduce Pig Hive HBase and NoSQL Analyze and determine the relationship of input keys to output keys in terms of both type and number identify the number type and value of emitted keys and values from the Mappers Reducers and the number and contents of the output files Developed MapReduce pipeline jobs to process the data and create necessary HFiles and loading the HFiles into HBase for faster access without taking performance hit Designed developed UI Screens with Spring MVC HTML5 CSS JavaScript AngularJS to provide interactive screens to display data Written multiple MapReduce programs in java for data extraction transformation and aggregation from multiple file formats including XML JSON CSV and other file formats Analyze to determine the correct InputFormat OutputFormat on order of MapReduce job requirements Created database tables and wrote TSQL Queries and stored procedures to create complex join tables and to perform CRUD operations By using AWS MapReduce job processed the data stored in the AWS Using Automated Build and continuous integration systems Jenkins and testdriven Unit Testing framework Junit Access control to users depending on logins using HTML jQuery for validations Created the web application using HTML CSS jQuery and JavaScript Used Eclipse as an IDE for developing the application Loaded the flat files data using Informatica to the staging area Used UIrouter in AngularJS to make this a single page application Developed unitassembly test cases and UNIX shell scripts to run along with dailyweeklymonthly batches to reduce or eliminate manual testing effort Developed mappings in Informatica to load the data including facts and dimensions from various sources into the Data Warehouse using different transformations like Source Qualifier Java Expression Lookup Aggregate Update Strategy and Joiner Environment Windows XPNT Java MapReduce Pig Hive Hbase NoSQL AWS Jenkins HTML CSS TSQL AngularJS UI jQuery Korn Shell Quality Center 10 Hadoop Developer East West Bank Pasadena CA March 2014 to June 2016 Responsibilities Installed and configured Apache Hadoop to test the maintenance of log files in Hadoop cluster Installed and configured Hive Pig Sqoop Flume and Oozie on the Hadoop cluster Developed Sqoop jobs to import and store massive volumes of data in HDFS and Hive Designed and developed PIG data transformation scripts to work against unstructured data from various data points and created a base line Setup and benchmarked Hadoop HBase clusters for internal use Developed Java MapReduce programs for the analysis of sample log file stored in cluster Developed Map Reduce Programs for data analysis and data cleaning Developed PIG UDFs like UTAFs UDAFs for manipulating the data according to business requirements and worked on developing custom PIG Loaders Implemented data pipeline by chaining multiple mappers by using Chained Mapper Used Hive and created Hive tables and involved in data loading and writing Hive UDFs Migration of processes from Oracle to Hive to test the easy data manipulation Configured deployed and maintained multinode Dev and Test Kafka Clusters Implemented data injection systems by creating Kafka brokers producers Consumers custom encoders Implemented Partitioning Dynamic Partitions and Bucketing in Hive for efficient data access Developed a data pipeline using Kafka and Strom to store data into HDFS Developed some utility helper classes to get data from HBase tables Good experience in troubleshooting performance issues and tuning Hadoop cluster Built components modules and plugins using Angular JS and Bootstrap Created Java Interfaces and Abstract classes for different functionalities Loaded and transformed large sets of structured semistructured and unstructured data in various formats like text sequence XML and JSON Written multiple MapReduce programs to power data for extraction transformation and aggregation from multiple file formats including XML JSON CSV other compressed file formats Environment Apache Hadoop HDFS Cloudera Manager Java MapReduce Eclipse Hive PIG Sqoop Oozie SQL Zookeeper CDH3 Cassandra Oracle NoSQL and UnixLinux Kafka Big Data Engineer Caesars Las Vegas NV August 2012 to February 2014 Responsibilities Responsible for creating sample Datasets required for testing various Map Reduce applications from various sources Developed Hive UDF to parse the staged raw data to get the item details from a specific store Built reusable Hive UDF libraries for business requirements which enabled users to use these UDFs in Hive querying Designed workflow by scheduling Hive processes for Log file data which is streamed into HDFS using Flume Developed SQL scripts to compare all the records for every field and table at each phase of the data movement process from the original source system to the final target Implemented map reduce jobs to process standard data in Hadoop cluster Involved in the performance enhancement by analyzing the workflows joins configuration parameters etc Collaborating with business usersproduct owners developers to contribute to the analysis of functional requirements Design Develop workflow using Oozie for business requirements which includes automating the extraction of data from MySQL database into HDFS using Sqoop scripts Worked on migration of Informatica Power center to Hadoop eco system Assisted with the admin department with issues related to migration Performed data analytics in Hive and then exported this metrics back to Oracle Database using Sqoop Provided adhoc queries and data metrics to the Business Users using Hive Pig Developed Map Reduce Programs in Java for applying business rules on the data Implemented Partitioning Dynamic Partitions Bucketing in Hive Document and manage failurerecovery steps for any production issues Involved in Minor and Major Release work activities Proactively involved in ongoing maintenance support and improvements in Hadoop cluster Import the data from various sources like HDFSHBase into Kafka Wrote MapReduce jobs using java and Pig Latin Worked on NoSQL databases including HBase and Cassandra Configured SQL Database to store Hive Teradata Participated in developmentimplementation of Cloudera impala Hadoop environment Imported data using Sqoop to load data from MySQL to HDFS on regular basis Used Pig as ETL Informatica tool to do Transformations even joins and some preaggregations before storing the data onto HDFS Developed Application components and APIs using Scala Created ETL Informatica jobs to generate and distribute reports from MySQL database Involved in loading data from LINUX file system to HDFS using Sqoop and exported the analyzed data to the relational databases using Sqoop for visualization and to generate reports for the Business intelligence BI team Installed and configured Flume Hive Pig Sqoop and Oozie on the Hadoop cluster Extracted the Teradata from Oracle into Hive using the Sqoop Worked on Agile Methodology Environment Hadoop HDFS Pig Zookeeper Sqoop HBase Shell Scripting Ubuntu Linux Red Hat Kafka Cassandra Senior Hadoop Developer AFLAC Columbus GA March 2011 to July 2012 Responsibilities Worked closely with the business analysts to convert the Business Requirements into Technical Requirements and preparing low and highlevel documentation Performing transformations using Hive MapReduce hands on experience in copying log snappy files into HDFS from Greenplum using Flume Kafka loaded data into HDFS and extracted the data into HDFS from MYSQL using Sqoop Involved in preparing the S2TM document as per the business requirement and worked with Source system SMEs in understanding the source data behavior Imported required tables from RDBMS to HDFS using Sqoop and used Storm Spark streaming and Kafka to get real time streaming of data into HBase Experience in Writing Map Reduce jobs for text mining and worked with predictive analysis team and Experience in working with Hadoop components such as HBase Spark Yarn Kafka Zookeeper PIG HIVE Sqoop Oozie Impala and Flume Wrote HIVE UDFs as per requirements and to handle different schemas and xml data Designing and developing MapReduce jobs to process data coming in different file formats like XML CSV JSON Involved in Apache SPARK testing Implemented ETL code to load data from multiple sources into HDFS using Pig Scripts Implemented MapReduce programs to handle semi unstructured data like XML JSON Avro data files and sequence files for log files Responsible to review the test cases in HP ALM Developed Spark applications using Scala for easy Hadoop transitions And Hands on experienced in writing Spark jobs and Spark streaming API using Scala and Python Used Spark API over Cloudera Hadoop YARN to perform analytics on data in Hive developed Spark code and SparkSQLStreaming for faster testing and processing of data Installed Oozie workflow engine to run multiple Hive and Pig jobs Designed and developed User Defined Function UDF for Hive and Developed the Pig UDFS to preprocess the data for analysis as well as experience in UDAFs for custom data specific processing Assisted in problem solving with Big Data technologies for integration of Hive with HBase and Sqoop with HBase Designed and developed the core data pipeline code involving work in Java and Python and built on Kafka and Storm Good knowledge on Partitions bucketing concepts in Hive and designed both Managed and External tables in Hive for optimized performance Performance tuning using Partitioning bucketing of IMPALA tables Hands on experience on fetching the live stream data from DB2 to HBase table using Spark Streaming and Apache Kafka Experience in job workflow scheduling and monitoring tools like Oozie and Zookeeper Worked on NoSQL databases including HBase and Cassandra Populated HDFS and Cassandra with huge amounts of data using Apache Kafka Created POC Proof of Concept to store Server Log data into Cassandra to identify System Alert Metrics Environment Map Reduce HDFS Hive Pig HBase Python SQL Sqoop Flume Oozie Impala Scala Spark Apache Kafka Zookeeper J2EE Linux Red Hat HPALM Eclipse Cassandra Talend Informatica Hadoop Developer Wellcare Health Plans Tampa FL June 2010 to February 2011 Responsibilities WellCare focuses exclusively on providing governmentsponsored managed care services primarily through Medicaid Medicare Advantage and Medicare Prescription Drug Plans to families children seniors and individuals with complex medical needs Roles and Responsibilities Experience in creating integration between Hive and HBase for effective usage and performed MR Unit testing for the Map Reduce jobs Created BI reports Tableau and dashboards from HDFS data using Hive Experience in importing and exporting the data from Relational Database Systems to HDFS by using Sqoop Developed a common framework to import the data from Teradata to HDFS and to export to Teradata using Sqoop Imported the log data from different servers into HDFS using Flume and developed MapReduce programs for analyzing the data Used Flume to handle the real time log processing for attribution reports Worked on tuning the performance of Pig queries Involved in loading data from UNIX file system to HDFS Performed operation using Partitioning pattern in MapReduce to move records into different categories Implemented Fair schedulers on the Job tracker to share the resources of the Cluster for the MapReduce jobs given by the users Involved in templates and screens in HTML and JavaScript Created HBase tables to load large sets of data coming from UNIX and NoSQL Implemented the Web Service client for the login authentication credit reports and applicant information using Apache Axis 2 Web Service Design develop test implement and support of Data Warehousing ETL using Talend and Hadoop Technologies Built and deployed Java applications into multiple Unix based environments and produced both unit and functional test results along with release notes Environment WebSphere 61 HTML XML ANT 16 MapReduce Sqoop UNIX NoSQL Java JavaScript MR Unit Teradata Nodejs JUnit 38 ETL Talend HDFS Hive HBase Education Bachelors Skills APACHE HADOOP MAPREDUCE 9 years APACHE HBASE 9 years Hadoop 9 years HADOOP 9 years HBase 9 years Additional Information TECHNICAL SKILLS Big Data Technologies Hadoop HDFS MapReduce Spark Hive Sqoop Oozie Flume HBase Pig Zookeeper Kafka Programming Languages Linux Programming Basics of Python language Hive SQL JAVA Pig Latin Web Development JavaScript JQuery HTML 50 CSS 30 AJAX JSON Tools CDH 591 Cloudera Navigator Cloudera Manager Basics VMware Web Servers Apache Tomcat Methodologies Agile Vmodel Waterfall model Operating Systems Linux CentOS Windows Databases MySQL Oracle 11g10g HBase Mongo DB Cassandra Couch MS SQL server",
    "unique_id": "76893eb1-1d2d-48bc-90bb-866da96847eb"
}