{
    "clean_data": "Sr Hadoop Developer Sr Hadoop span lDeveloperspan Sr Hadoop Developer United Technologies Corporation Farmington CT 7 Years of IT industry experience with 5 years of experience in dealing with Apache Hadoop components like HDFS MapReduce Spark Hive Pig Sqoop Oozie Zookeeper HBase Cassandra MongoDB and Amazon Web Services 2 years of experience in the Application Development and Maintenance of SDLC projects using Java technologies Developed applications for Distributed Environment using Hadoop MapReduce and Python Developed MapReduce jobs to automate transfer of data from HBase Developing and Maintenance the Web Applications using the Web Server Tomcat Experience in integrating Hadoop with Ganglia and have good understanding of Hadoop metrics and visualization using Ganglia Good experience working with Hortonworks Distribution Cloudera Distribution and MapR Distribution Very good understandingknowledge of Hadoop Architecture and various components such as HDFS JobTracker TaskTracker NameNode DataNode Secondary Namenode and MapReduce concepts Experience in data extraction and transformation using MapReduce jobs Proficient in working with Hadoop HDFS writing PIG scripts and Sqoop scripts Performed data analysis using Hive and Pig Expert in creating Pig and Hive UDFs using Java in order to analyze the data efficiently Experience in importing and exporting data using Sqoop from Relational Database Systems to HDFS and viceversa Strong understanding of NoSql databases like HBase MongoDB Cassandra Strong understanding of Spark real time streaming and SparkSQL and experience in loading data from external data sources like MySQL and Cassandra for Spark applications Experience in performing inmemory data processing and real time streaming analytics using Apache Spark with Scala Java and Python Created HBase tables to load large sets of structured semistructured and unstructured data coming from UNIX NoSQL and a variety of portfolios Intensive working experience with Amazon Web ServicesAWS using S3 for storage EC2 for computing and RDS EBS Well versed with job workflow scheduling and monitoring tools like Oozie Loaded streaming log data from various web servers into HDFS using Flume Experience in using Sqoop Oozie and Cloudera Manager Experience on Source control repositories like SVN CVS and GIT Experience in improving the search focus and quality in ElasticSearch by using aggregations and Python scripts Hands on experience in application development using RDBMS and Linux shell scripting Have experience with working on Amazon EMR and EC2 Spot instances Solid understanding of relational database concepts Extensively worked with Unified Modeling Tools UML in designing Use Cases Activity flow diagram Class diagrams Sequence and Object Diagrams using Rational Rose MSVisio Very good experience in complete project life cycle design development testing and implementation of Client Server and Web applications Good Knowledge on Hadoop Cluster architecture and monitoring the cluster Adequate knowledge and working experience in Agile Waterfall methodologies Support development testing and operations teams during new system deployments Practical knowledge on implementing Kafka with thirdparty systems such as Spark and Hadoop Good team player and can work efficiently in multiple team environments and multiple products Easily adaptable to the new systems and environments Possess excellent communication and analytical skills along with a cando attitude Work Experience Sr Hadoop Developer United Technologies Corporation Farmington CT February 2018 to Present United Technologies Corporation is a financial services and bank holding company in United States The objective should be analyzing the huge amount of data and producing largescale programs that integrate with technology for the clients to achieve customer satisfaction Implement and deploy custom applications on Hadoop clusters Providing information security customer satisfaction and fraud detection solutions using big data analytics The technical environment includes Spark Kafka Scala Hive and Oozie Responsibilities Involved in installation configuration and maintenance of Hadoop clusters for application development with Cloudera distribution Developed Kafka consumers API in Scala for consuming data from Kafka topics Developed endtoend scalable distributed data pipelines which receiving data using distributed messaging systems Kafka through persistence of data into HDFS with Apache Spark using Scala Involved in performance tuning of Spark jobs using Cache and using complete advantage of cluster environment In the framework we just need to mention the table names schemas and location of source file Sqoop parameters etc and the framework will generate the entire code which includes Workdlowxml Performed advanced operations like text analytics and processing using inmemory computing capabilities of Spark using Scala Experience in query data using Spark SQL on Spark to implement Spark RDDS in Scala Experienced in working with different scripting technologies like Python UNIX shell scripts Performed POC on writing the spark applications in Scala Python and R programming language Worked on Partitioning Bucketing Parallel execution Map side Joins for optimization of necessary hive queries Performed Hive QL to create Hive tables and to write Hive queries to perform the data analysis Experience in collecting log data from web servers and pushed to HDFS using Flume and NoSql database Cassandra Used Oozie workflow to Manage and scheduling Jobs on a Hadoop Cluster and used Zookeeper for cluster coordination services Used NIFI for the transformation of data from different components of Big data ecosystem Worked on different data sources like Oracle Netezza MySQL Flat files etc and experience with AWS components like Amazon Ec2 instances S3 buckets and Cloud Formation templates Used Qlik sense to build customized interactive reports worksheets and dashboards Worked on different file formats like Sequence files XML files and Map files using Map Reduce Programs Strong expertise on MapReduce programming model with XML JSON CSV file formats Involved in managing and organizing developers with regular code review sessions by utilizing Agile and Scrum Methodologies Experience in implementing Spark RDD transformations actions data frames case classes to required data by using Spark core Implemented Jira for bug tracking and Bitbucket to code and code review Implemented apache airflow DAG to find popular items in Redshift and ingest in the main PostgreSQL via a web service call Implemented Spark applications in data processing project to handle data from various sources and creating DStreams Data frames on input data which we get from streaming service like Kafka Environment Map Reduce HDFS Hive Spark SparkSQL Sqoop Apache Kafka Java 7 Cassandra Scala Apache Pig 0140 Apache Hive 100Oozie Linux AWS EC2 Agile development Oracle 11g10g UNIX Shell scripting Ambari TezEclipse and Qlik sense Cloudera Hadoop Developer Transamerica Dallas TX July 2016 to January 2018 The Transamerica Corporation is an American private holding company for various life insurance companies and investment firms Transamerica companies are recognized as leading providers of life insurance savings and retirement and investment solutions serving millions of customers throughout the United States and Canada Responsibilities Used Cassandra Query Language to design Cassandra database and tables with various configuration options Developed PIG UDFS for manipulating the data according to Business Requirements and also worked on developing custom PIG Loaders Experience in integrating Hadoop with Ganglia and have good understanding of Hadoop metrics and visualization using Ganglia Involved in the review of functional and nonfunctional requirements Practical experience in developing Spark applications in Eclipse with Maven Strong understanding of Spark real time streaming and SparkSQL Loading data from external data sources like MySQL and Cassandra for Spark applications Developed Python and Shell scripts to automate the endtoend implementation process of AI project Experience in selecting and configuring the right Amazon EC2 instances and access key AWS services using client tools and AWS SDKs Knowledge on using AWS identity and Access Management to secure access to EC2 instances and configure autoscaling groups using CloudWatch Firm understanding of optimizations and performancetuning practices while working with Spark Good knowledge on compression and serialization to improve performance in Spark applications Performed interactive querying using SparkSQL Worked on different file formats like Sequence files XML files and Map files using Map Reduce Programs Strong expertise on MapReduce programming model with XML JSON CSV file formats Designed and Implemented Partitioning static Dynamic and Bucketing in HIVEAWS Practical knowledge on Apache Sqoop to import datasets from MySQL to HDFS and viceversa Good knowledge on building predictive models focusing on customer service using R programming Experience in reviewing and managing Hadoop log files Experience in building batch and streaming applications with Apache Spark and Python Used the libraries built on Mlib to perform data cleaning and used R programming for dataset reorganizing Debug CQL queries and implement performance enhancement practices Strong knowledge on Apache Oozie for scheduling the tasks Practical knowledge on implementing Kafka with thirdparty systems such as Spark and Hadoop Experience in configuring Kafka brokers consumers and producers for optimal performance Knowledge of creating Apache Kafka consumers and producers in Java Involved in creating Hive tables loading with data and writing hive queries which will run internally in map reduce way Practical knowledge of monitoring a Hadoop cluster using Nagios and Ganglia Experience with GIT for version control system Involved in loading data from UNIX file system to HDFS and developed UNIX scripts for job scheduling process management and for handling logs from Hadoop Understanding technical specifications and documenting technical design documents Strong skills in agile development and TestDriven development Have practical knowledge on implementing Internet of Things IoT Environment Hadoop Cloudera Distribution CDH4 Java 7 Hadoop 252 Spark SparkSQL Mlib R programming Scala Cassandra IoT MapReduce Apache Pig 0140 Apache Hive 100 HDFS Sqoop Oozie Kafka Maven Eclipse Nagios Ganglia Zookeeper AWS EC2 GIT Ambari TezEclipse UNIX Shell scripting Oracle 11g10g Linux Agile development Hadoop Developer Pacific Life Newport Beach CA August 2015 to June 2016 Pacific Life a leading diversified international group of companies and is one of the top providers of Life insurance in United States ECommerce is a web application that facilitates customers to get a fast quote online originate a policy and service an account The company has many regional offices in the business of home and insurance Each regional office sends the details of monthly transactions to the central system Most of the feed is in the raw text format and the data is in fixed line format Responsibilities Analyzed large amounts of data sets to determine optimal way to aggregate and report on it Designed and implemented Incremental Imports into Hive tables Developed and written Apache PIG scripts and HIVE scripts to process the HDFS data Involved in defining job flows managing and reviewing log files Involved in Unit testing and delivered Unit test plans and results documents using Junit and MR unit Supported Map Reduce Programs those are running on the cluster As a Big Data Developer implemented solutions for ingesting data from various sources and processing the DataatRest utilizing Big Data technologies such as Hadoop MapReduce Frameworks HBase Hive Oozie Flume Sqoop etc Imported Bulk Data into HBase Using Map Reduce programs Perform analytics on Time Series Data exists in HBase using HBase API Involved in collecting aggregating and moving data from servers to HDFS using Apache Flume Written Hive jobs to parse the logs and structure them in tabular format to facilitate effective querying on the log data Responsible for continuous monitoring and managing Elastic MapReduce cluster through AWS console Wrote multiple java programs to pull data from HBase Involved with File Processing using Pig Latin Involved in creating Hive tables loading with data and writing hive queries that will run internally in map reduce way Experience in optimization of Map reduce algorithm using combiners and partitions to deliver the best results and worked on Application performance optimization for a HDFS cluster Worked on debugging performance tuning of Hive Pig Jobs Used Hive to find correlations between customers browser logs in different sites and analyzed them to build risk profile for such sites Implemented business logic by writing UDFs in Java and used various UDFs from Piggybanks and other sources Created and maintained Technical documentation for launching HADOOP Clusters and for executing Hive queries and Pig Scripts Environment Java Hadoop 210 Map Reduce2 Pig 0120 Hive 0130 Linux Sqoop 142 Flume 131 Eclipse AWS EC2 and Cloudera CDH 4 Java Developer Aegis Consulting Services IN December 2013 to May 2015 Aegis Consulting Services keeps abreast with the state of art technologies Knowledge of the technologies are carefully analyzed for suitability and appropriately deployed with the solution The company has indepth domain as well as functional knowledge The companys vision is to create a cohesive group of technology creative talent which holds the capacity to deliver effective results while deploying optional resources Responsibilities Involved in Analysis Design Implementation and Bug Fixing Activities Designing the initial WebWAP pages for a better UI as per the requirement Involved in design of basic Class Diagrams Sequence Diagrams and Event Diagrams as a part of Documentation Developed SQL queries and Stored Procedures using PLSQL to retrieve and insert into multiple database schemas Involved in Functional Technical Specification documents review and the code review Undergone training on the Domain Knowledge Discussions and meetings with the Business Analysts for understanding the functionality involved in Test Cases review Prepared the Support Guide containing the complete functionality Environment Core Java ApacheTomcat51 Oracle 9i Java Script HTML PLSQL Rational Rose Windows XP UNIX Software Engineer Wind Stream Communications IN June 2012 to November 2013 Wind Stream Communications Inc was established in 2008 with the goal of designing prototyping and manufacturing affordable and scalable renewable energy technologies for a global marketplace Responsibilities Designed developed and executed Data Migration from Db2 Database to Oracle Database using Linux scripts Java and SQL loader concepts A key member of the team and playing a key role in articulating the Design requirements for the Development of Automated tools that perform error free Configuration Developed UNIX and java utilities for Data migration from Db2 to Oracle Sole developer and POC for the migration Activity Developed JSP pages Servlets and HTML pages as per requirement Developed the necessary Java Beans PLSQL procedures for the implementation of business rules Developed user interface using JAVA Server Pages JSP HTML and Java Script for the Presentation Tier Developed JSP pages and clientside validation by java script tags Developed an own realm for Apache Tomcat Server for authenticating the users Developed front end controller in Servlet to handle all the requests Developed the web interface using JSP and developed struts action classes Responsible for both functional and nonfunctional requirements gathering performing impact analysis and testing the solutions build on build basis Coding using Java Java Script and HTML Used JDBC to provide database connectivity to database tables in Oracle Used WebSphere Application Server for application deployment Implemented Software Development Life Cycle Requirements Analysis Design Development Testing Deployment and Support Environment J2EE IBM DB2 IBM WebSphere Application Server EJB JSP Servlets HTML CSS JavaScript Oracle database Unix Scripting and Windows 2000 Skills CASSANDRA MAPREDUCE OOZIE SQOOP HBASE KAFKA ELASTICSEARCH FLUME HADOOP APPLICATION SERVER Git Hadoop HBase Hive HTML JAVASCRIPT MapReduce Pig PYTHON PYSPARK Additional Information TECHNICAL SKILLS Programming languages C Java Python Scala SQL HADOOPBIG DATA MapReduce Spark SparkSQL PySpark SparkR Pig Hive Sqoop HBase Flume Kafka Cassandra Yarn Oozie Zookeeper ElasticSearch Databases MySQL PLSQL Mongo DB HBase Cassandra Operating Systems Windows Unix Linux Ubuntu Web Development HTML JSP JavaScript JQuery CSS XML AJAX WebApplication Servers Apache Tomcat Sun Java Application Server Tools IntelliJ Eclipse Net Beans Nagios Ganglia Maven Scripting BASH JavaScript Version Controls GIT SVN",
    "entities": [
        "JAVA Server Pages",
        "Implemented Spark",
        "Unix Scripting",
        "JobTracker",
        "Agile Waterfall",
        "United Technologies Corporation Farmington CT",
        "Test Cases",
        "HDFS",
        "UNIX",
        "Pig 0120 Hive",
        "DataatRest",
        "BASH JavaScript Version",
        "the Domain Knowledge Discussions",
        "Maven Strong",
        "Nagios",
        "RDD",
        "Hadoop",
        "XML",
        "Sqoop Oozie",
        "Aegis Consulting Services",
        "Java Developer Aegis Consulting Services",
        "HBase Involved",
        "Responsibilities Involved in Analysis Design Implementation",
        "Shell",
        "the Application Development and Maintenance of SDLC",
        "HBase",
        "Spark Good",
        "Apache Spark",
        "Sr Hadoop Developer Sr Hadoop",
        "TX",
        "Amazon",
        "Piggybanks",
        "Implemented Software Development Life Cycle Requirements Analysis Design Development Testing Deployment and Support Environment J2EE IBM",
        "Cloudera Hadoop",
        "SparkSQL",
        "Developed",
        "Present United Technologies Corporation",
        "Dallas",
        "Time Series Data",
        "Python Created HBase",
        "NIFI",
        "Scala Cassandra IoT MapReduce Apache",
        "Sequence",
        "Git Hadoop HBase Hive HTML JAVASCRIPT MapReduce Pig PYTHON PYSPARK Additional Information TECHNICAL SKILLS Programming",
        "ElasticSearch",
        "Use Cases Activity",
        "Performed POC",
        "Hortonworks Distribution Cloudera Distribution",
        "Pacific Life",
        "Spark RDDS",
        "Ganglia Good",
        "Cache",
        "Ganglia",
        "Adequate",
        "JSP",
        "Work Experience Sr Hadoop Developer United Technologies Corporation Farmington",
        "Oracle 11g10",
        "the Web Server Tomcat Experience",
        "Hadoop Understanding",
        "Hadoop Developer Pacific Life Newport Beach",
        "Debug CQL",
        "Oracle Used WebSphere Application Server",
        "HDFS MapReduce Spark Hive Pig",
        "Ganglia Involved",
        "Oracle Database",
        "Hadoop Good",
        "Incremental Imports",
        "Spark",
        "Easily",
        "Agile",
        "Hadoop MapReduce Frameworks HBase",
        "GIT",
        "Amazon EMR",
        "API",
        "Sqoop",
        "Scala Involved",
        "HIVE",
        "NoSql",
        "Created",
        "AI",
        "AWS",
        "Hadoop Architecture",
        "MR",
        "PIG",
        "HBase Developing",
        "the Business Analysts",
        "HTML",
        "Functional Technical Specification",
        "java",
        "SQL",
        "Amazon Web Services",
        "Redshift",
        "Relational Database Systems",
        "Developed PIG UDFS",
        "HADOOP Clusters",
        "the United States",
        "Imported Bulk Data",
        "SVN CVS",
        "Hive",
        "Big Data",
        "DAG",
        "TestDriven",
        "Practical",
        "United States",
        "UNIX Shell",
        "The Transamerica Corporation",
        "Servlet",
        "Apache Hadoop",
        "Maven",
        "Performed",
        "Oracle Sole",
        "Data Migration",
        "Documentation Developed SQL",
        "Spark SQL",
        "Oozie Responsibilities Involved",
        "Responsibilities Analyzed",
        "UI",
        "Wind Stream Communications Inc",
        "Oracle Netezza MySQL Flat",
        "Sequence and Object Diagrams",
        "Business Requirements",
        "SVN",
        "Access Management",
        "HIVEAWS Practical",
        "CSS",
        "Oozie Loaded",
        "java utilities",
        "Data",
        "IBM WebSphere Application Server",
        "MapReduce",
        "NoSQL",
        "Application",
        "Linux AWS",
        "Map"
    ],
    "experience": "Experience in integrating Hadoop with Ganglia and have good understanding of Hadoop metrics and visualization using Ganglia Good experience working with Hortonworks Distribution Cloudera Distribution and MapR Distribution Very good understandingknowledge of Hadoop Architecture and various components such as HDFS JobTracker TaskTracker NameNode DataNode Secondary Namenode and MapReduce concepts Experience in data extraction and transformation using MapReduce jobs Proficient in working with Hadoop HDFS writing PIG scripts and Sqoop scripts Performed data analysis using Hive and Pig Expert in creating Pig and Hive UDFs using Java in order to analyze the data efficiently Experience in importing and exporting data using Sqoop from Relational Database Systems to HDFS and viceversa Strong understanding of NoSql databases like HBase MongoDB Cassandra Strong understanding of Spark real time streaming and SparkSQL and experience in loading data from external data sources like MySQL and Cassandra for Spark applications Experience in performing inmemory data processing and real time streaming analytics using Apache Spark with Scala Java and Python Created HBase tables to load large sets of structured semistructured and unstructured data coming from UNIX NoSQL and a variety of portfolios Intensive working experience with Amazon Web ServicesAWS using S3 for storage EC2 for computing and RDS EBS Well versed with job workflow scheduling and monitoring tools like Oozie Loaded streaming log data from various web servers into HDFS using Flume Experience in using Sqoop Oozie and Cloudera Manager Experience on Source control repositories like SVN CVS and GIT Experience in improving the search focus and quality in ElasticSearch by using aggregations and Python scripts Hands on experience in application development using RDBMS and Linux shell scripting Have experience with working on Amazon EMR and EC2 Spot instances Solid understanding of relational database concepts Extensively worked with Unified Modeling Tools UML in designing Use Cases Activity flow diagram Class diagrams Sequence and Object Diagrams using Rational Rose MSVisio Very good experience in complete project life cycle design development testing and implementation of Client Server and Web applications Good Knowledge on Hadoop Cluster architecture and monitoring the cluster Adequate knowledge and working experience in Agile Waterfall methodologies Support development testing and operations teams during new system deployments Practical knowledge on implementing Kafka with thirdparty systems such as Spark and Hadoop Good team player and can work efficiently in multiple team environments and multiple products Easily adaptable to the new systems and environments Possess excellent communication and analytical skills along with a cando attitude Work Experience Sr Hadoop Developer United Technologies Corporation Farmington CT February 2018 to Present United Technologies Corporation is a financial services and bank holding company in United States The objective should be analyzing the huge amount of data and producing largescale programs that integrate with technology for the clients to achieve customer satisfaction Implement and deploy custom applications on Hadoop clusters Providing information security customer satisfaction and fraud detection solutions using big data analytics The technical environment includes Spark Kafka Scala Hive and Oozie Responsibilities Involved in installation configuration and maintenance of Hadoop clusters for application development with Cloudera distribution Developed Kafka consumers API in Scala for consuming data from Kafka topics Developed endtoend scalable distributed data pipelines which receiving data using distributed messaging systems Kafka through persistence of data into HDFS with Apache Spark using Scala Involved in performance tuning of Spark jobs using Cache and using complete advantage of cluster environment In the framework we just need to mention the table names schemas and location of source file Sqoop parameters etc and the framework will generate the entire code which includes Workdlowxml Performed advanced operations like text analytics and processing using inmemory computing capabilities of Spark using Scala Experience in query data using Spark SQL on Spark to implement Spark RDDS in Scala Experienced in working with different scripting technologies like Python UNIX shell scripts Performed POC on writing the spark applications in Scala Python and R programming language Worked on Partitioning Bucketing Parallel execution Map side Joins for optimization of necessary hive queries Performed Hive QL to create Hive tables and to write Hive queries to perform the data analysis Experience in collecting log data from web servers and pushed to HDFS using Flume and NoSql database Cassandra Used Oozie workflow to Manage and scheduling Jobs on a Hadoop Cluster and used Zookeeper for cluster coordination services Used NIFI for the transformation of data from different components of Big data ecosystem Worked on different data sources like Oracle Netezza MySQL Flat files etc and experience with AWS components like Amazon Ec2 instances S3 buckets and Cloud Formation templates Used Qlik sense to build customized interactive reports worksheets and dashboards Worked on different file formats like Sequence files XML files and Map files using Map Reduce Programs Strong expertise on MapReduce programming model with XML JSON CSV file formats Involved in managing and organizing developers with regular code review sessions by utilizing Agile and Scrum Methodologies Experience in implementing Spark RDD transformations actions data frames case classes to required data by using Spark core Implemented Jira for bug tracking and Bitbucket to code and code review Implemented apache airflow DAG to find popular items in Redshift and ingest in the main PostgreSQL via a web service call Implemented Spark applications in data processing project to handle data from various sources and creating DStreams Data frames on input data which we get from streaming service like Kafka Environment Map Reduce HDFS Hive Spark SparkSQL Sqoop Apache Kafka Java 7 Cassandra Scala Apache Pig 0140 Apache Hive 100Oozie Linux AWS EC2 Agile development Oracle 11g10 g UNIX Shell scripting Ambari TezEclipse and Qlik sense Cloudera Hadoop Developer Transamerica Dallas TX July 2016 to January 2018 The Transamerica Corporation is an American private holding company for various life insurance companies and investment firms Transamerica companies are recognized as leading providers of life insurance savings and retirement and investment solutions serving millions of customers throughout the United States and Canada Responsibilities Used Cassandra Query Language to design Cassandra database and tables with various configuration options Developed PIG UDFS for manipulating the data according to Business Requirements and also worked on developing custom PIG Loaders Experience in integrating Hadoop with Ganglia and have good understanding of Hadoop metrics and visualization using Ganglia Involved in the review of functional and nonfunctional requirements Practical experience in developing Spark applications in Eclipse with Maven Strong understanding of Spark real time streaming and SparkSQL Loading data from external data sources like MySQL and Cassandra for Spark applications Developed Python and Shell scripts to automate the endtoend implementation process of AI project Experience in selecting and configuring the right Amazon EC2 instances and access key AWS services using client tools and AWS SDKs Knowledge on using AWS identity and Access Management to secure access to EC2 instances and configure autoscaling groups using CloudWatch Firm understanding of optimizations and performancetuning practices while working with Spark Good knowledge on compression and serialization to improve performance in Spark applications Performed interactive querying using SparkSQL Worked on different file formats like Sequence files XML files and Map files using Map Reduce Programs Strong expertise on MapReduce programming model with XML JSON CSV file formats Designed and Implemented Partitioning static Dynamic and Bucketing in HIVEAWS Practical knowledge on Apache Sqoop to import datasets from MySQL to HDFS and viceversa Good knowledge on building predictive models focusing on customer service using R programming Experience in reviewing and managing Hadoop log files Experience in building batch and streaming applications with Apache Spark and Python Used the libraries built on Mlib to perform data cleaning and used R programming for dataset reorganizing Debug CQL queries and implement performance enhancement practices Strong knowledge on Apache Oozie for scheduling the tasks Practical knowledge on implementing Kafka with thirdparty systems such as Spark and Hadoop Experience in configuring Kafka brokers consumers and producers for optimal performance Knowledge of creating Apache Kafka consumers and producers in Java Involved in creating Hive tables loading with data and writing hive queries which will run internally in map reduce way Practical knowledge of monitoring a Hadoop cluster using Nagios and Ganglia Experience with GIT for version control system Involved in loading data from UNIX file system to HDFS and developed UNIX scripts for job scheduling process management and for handling logs from Hadoop Understanding technical specifications and documenting technical design documents Strong skills in agile development and TestDriven development Have practical knowledge on implementing Internet of Things IoT Environment Hadoop Cloudera Distribution CDH4 Java 7 Hadoop 252 Spark SparkSQL Mlib R programming Scala Cassandra IoT MapReduce Apache Pig 0140 Apache Hive 100 HDFS Sqoop Oozie Kafka Maven Eclipse Nagios Ganglia Zookeeper AWS EC2 GIT Ambari TezEclipse UNIX Shell scripting Oracle 11g10 g Linux Agile development Hadoop Developer Pacific Life Newport Beach CA August 2015 to June 2016 Pacific Life a leading diversified international group of companies and is one of the top providers of Life insurance in United States ECommerce is a web application that facilitates customers to get a fast quote online originate a policy and service an account The company has many regional offices in the business of home and insurance Each regional office sends the details of monthly transactions to the central system Most of the feed is in the raw text format and the data is in fixed line format Responsibilities Analyzed large amounts of data sets to determine optimal way to aggregate and report on it Designed and implemented Incremental Imports into Hive tables Developed and written Apache PIG scripts and HIVE scripts to process the HDFS data Involved in defining job flows managing and reviewing log files Involved in Unit testing and delivered Unit test plans and results documents using Junit and MR unit Supported Map Reduce Programs those are running on the cluster As a Big Data Developer implemented solutions for ingesting data from various sources and processing the DataatRest utilizing Big Data technologies such as Hadoop MapReduce Frameworks HBase Hive Oozie Flume Sqoop etc Imported Bulk Data into HBase Using Map Reduce programs Perform analytics on Time Series Data exists in HBase using HBase API Involved in collecting aggregating and moving data from servers to HDFS using Apache Flume Written Hive jobs to parse the logs and structure them in tabular format to facilitate effective querying on the log data Responsible for continuous monitoring and managing Elastic MapReduce cluster through AWS console Wrote multiple java programs to pull data from HBase Involved with File Processing using Pig Latin Involved in creating Hive tables loading with data and writing hive queries that will run internally in map reduce way Experience in optimization of Map reduce algorithm using combiners and partitions to deliver the best results and worked on Application performance optimization for a HDFS cluster Worked on debugging performance tuning of Hive Pig Jobs Used Hive to find correlations between customers browser logs in different sites and analyzed them to build risk profile for such sites Implemented business logic by writing UDFs in Java and used various UDFs from Piggybanks and other sources Created and maintained Technical documentation for launching HADOOP Clusters and for executing Hive queries and Pig Scripts Environment Java Hadoop 210 Map Reduce2 Pig 0120 Hive 0130 Linux Sqoop 142 Flume 131 Eclipse AWS EC2 and Cloudera CDH 4 Java Developer Aegis Consulting Services IN December 2013 to May 2015 Aegis Consulting Services keeps abreast with the state of art technologies Knowledge of the technologies are carefully analyzed for suitability and appropriately deployed with the solution The company has indepth domain as well as functional knowledge The companys vision is to create a cohesive group of technology creative talent which holds the capacity to deliver effective results while deploying optional resources Responsibilities Involved in Analysis Design Implementation and Bug Fixing Activities Designing the initial WebWAP pages for a better UI as per the requirement Involved in design of basic Class Diagrams Sequence Diagrams and Event Diagrams as a part of Documentation Developed SQL queries and Stored Procedures using PLSQL to retrieve and insert into multiple database schemas Involved in Functional Technical Specification documents review and the code review Undergone training on the Domain Knowledge Discussions and meetings with the Business Analysts for understanding the functionality involved in Test Cases review Prepared the Support Guide containing the complete functionality Environment Core Java ApacheTomcat51 Oracle 9i Java Script HTML PLSQL Rational Rose Windows XP UNIX Software Engineer Wind Stream Communications IN June 2012 to November 2013 Wind Stream Communications Inc was established in 2008 with the goal of designing prototyping and manufacturing affordable and scalable renewable energy technologies for a global marketplace Responsibilities Designed developed and executed Data Migration from Db2 Database to Oracle Database using Linux scripts Java and SQL loader concepts A key member of the team and playing a key role in articulating the Design requirements for the Development of Automated tools that perform error free Configuration Developed UNIX and java utilities for Data migration from Db2 to Oracle Sole developer and POC for the migration Activity Developed JSP pages Servlets and HTML pages as per requirement Developed the necessary Java Beans PLSQL procedures for the implementation of business rules Developed user interface using JAVA Server Pages JSP HTML and Java Script for the Presentation Tier Developed JSP pages and clientside validation by java script tags Developed an own realm for Apache Tomcat Server for authenticating the users Developed front end controller in Servlet to handle all the requests Developed the web interface using JSP and developed struts action classes Responsible for both functional and nonfunctional requirements gathering performing impact analysis and testing the solutions build on build basis Coding using Java Java Script and HTML Used JDBC to provide database connectivity to database tables in Oracle Used WebSphere Application Server for application deployment Implemented Software Development Life Cycle Requirements Analysis Design Development Testing Deployment and Support Environment J2EE IBM DB2 IBM WebSphere Application Server EJB JSP Servlets HTML CSS JavaScript Oracle database Unix Scripting and Windows 2000 Skills CASSANDRA MAPREDUCE OOZIE SQOOP HBASE KAFKA ELASTICSEARCH FLUME HADOOP APPLICATION SERVER Git Hadoop HBase Hive HTML JAVASCRIPT MapReduce Pig PYTHON PYSPARK Additional Information TECHNICAL SKILLS Programming languages C Java Python Scala SQL HADOOPBIG DATA MapReduce Spark SparkSQL PySpark SparkR Pig Hive Sqoop HBase Flume Kafka Cassandra Yarn Oozie Zookeeper ElasticSearch Databases MySQL PLSQL Mongo DB HBase Cassandra Operating Systems Windows Unix Linux Ubuntu Web Development HTML JSP JavaScript JQuery CSS XML AJAX WebApplication Servers Apache Tomcat Sun Java Application Server Tools IntelliJ Eclipse Net Beans Nagios Ganglia Maven Scripting BASH JavaScript Version Controls GIT SVN",
    "extracted_keywords": [
        "Sr",
        "Hadoop",
        "Developer",
        "Sr",
        "Hadoop",
        "span",
        "lDeveloperspan",
        "Sr",
        "Hadoop",
        "Developer",
        "United",
        "Technologies",
        "Corporation",
        "Farmington",
        "CT",
        "Years",
        "IT",
        "industry",
        "experience",
        "years",
        "experience",
        "Apache",
        "Hadoop",
        "components",
        "MapReduce",
        "Spark",
        "Hive",
        "Pig",
        "Sqoop",
        "Oozie",
        "Zookeeper",
        "HBase",
        "Cassandra",
        "MongoDB",
        "Amazon",
        "Web",
        "Services",
        "years",
        "experience",
        "Application",
        "Development",
        "Maintenance",
        "SDLC",
        "projects",
        "Java",
        "technologies",
        "applications",
        "Distributed",
        "Environment",
        "Hadoop",
        "MapReduce",
        "Python",
        "MapReduce",
        "jobs",
        "transfer",
        "data",
        "HBase",
        "Developing",
        "Maintenance",
        "Web",
        "Applications",
        "Web",
        "Server",
        "Tomcat",
        "Experience",
        "Hadoop",
        "Ganglia",
        "understanding",
        "Hadoop",
        "metrics",
        "visualization",
        "Ganglia",
        "Good",
        "experience",
        "Hortonworks",
        "Distribution",
        "Cloudera",
        "Distribution",
        "MapR",
        "Distribution",
        "understandingknowledge",
        "Hadoop",
        "Architecture",
        "components",
        "HDFS",
        "JobTracker",
        "TaskTracker",
        "NameNode",
        "DataNode",
        "Secondary",
        "Namenode",
        "MapReduce",
        "concepts",
        "Experience",
        "data",
        "extraction",
        "transformation",
        "MapReduce",
        "jobs",
        "Proficient",
        "Hadoop",
        "HDFS",
        "PIG",
        "scripts",
        "Sqoop",
        "data",
        "analysis",
        "Hive",
        "Pig",
        "Expert",
        "Pig",
        "Hive",
        "UDFs",
        "Java",
        "order",
        "data",
        "data",
        "Sqoop",
        "Relational",
        "Database",
        "Systems",
        "HDFS",
        "viceversa",
        "Strong",
        "understanding",
        "NoSql",
        "HBase",
        "MongoDB",
        "Cassandra",
        "Strong",
        "understanding",
        "Spark",
        "time",
        "streaming",
        "SparkSQL",
        "experience",
        "loading",
        "data",
        "data",
        "sources",
        "MySQL",
        "Cassandra",
        "Spark",
        "applications",
        "Experience",
        "data",
        "processing",
        "time",
        "streaming",
        "analytics",
        "Apache",
        "Spark",
        "Scala",
        "Java",
        "Python",
        "Created",
        "HBase",
        "sets",
        "data",
        "UNIX",
        "NoSQL",
        "variety",
        "portfolios",
        "working",
        "experience",
        "Amazon",
        "Web",
        "ServicesAWS",
        "S3",
        "storage",
        "EC2",
        "computing",
        "RDS",
        "EBS",
        "job",
        "workflow",
        "scheduling",
        "monitoring",
        "tools",
        "Oozie",
        "Loaded",
        "streaming",
        "log",
        "data",
        "web",
        "servers",
        "HDFS",
        "Flume",
        "Experience",
        "Sqoop",
        "Oozie",
        "Cloudera",
        "Manager",
        "Experience",
        "Source",
        "control",
        "repositories",
        "SVN",
        "CVS",
        "GIT",
        "Experience",
        "search",
        "focus",
        "quality",
        "ElasticSearch",
        "aggregations",
        "Python",
        "Hands",
        "experience",
        "application",
        "development",
        "RDBMS",
        "Linux",
        "shell",
        "scripting",
        "experience",
        "Amazon",
        "EMR",
        "EC2",
        "Spot",
        "instances",
        "understanding",
        "database",
        "concepts",
        "Unified",
        "Modeling",
        "Tools",
        "UML",
        "Use",
        "Cases",
        "Activity",
        "flow",
        "diagram",
        "Class",
        "diagrams",
        "Sequence",
        "Object",
        "Diagrams",
        "Rational",
        "Rose",
        "MSVisio",
        "experience",
        "project",
        "life",
        "cycle",
        "design",
        "development",
        "testing",
        "implementation",
        "Client",
        "Server",
        "Web",
        "applications",
        "Good",
        "Knowledge",
        "Hadoop",
        "Cluster",
        "architecture",
        "cluster",
        "knowledge",
        "working",
        "experience",
        "Agile",
        "Waterfall",
        "methodologies",
        "Support",
        "development",
        "testing",
        "operations",
        "teams",
        "system",
        "deployments",
        "knowledge",
        "Kafka",
        "thirdparty",
        "systems",
        "Spark",
        "Hadoop",
        "Good",
        "team",
        "player",
        "team",
        "environments",
        "products",
        "systems",
        "environments",
        "communication",
        "skills",
        "cando",
        "attitude",
        "Work",
        "Experience",
        "Sr",
        "Hadoop",
        "Developer",
        "United",
        "Technologies",
        "Corporation",
        "Farmington",
        "CT",
        "February",
        "Present",
        "United",
        "Technologies",
        "Corporation",
        "services",
        "bank",
        "company",
        "United",
        "States",
        "objective",
        "amount",
        "data",
        "largescale",
        "programs",
        "technology",
        "clients",
        "customer",
        "satisfaction",
        "Implement",
        "custom",
        "applications",
        "Hadoop",
        "clusters",
        "information",
        "security",
        "customer",
        "satisfaction",
        "fraud",
        "detection",
        "solutions",
        "data",
        "analytics",
        "environment",
        "Spark",
        "Kafka",
        "Scala",
        "Hive",
        "Oozie",
        "Responsibilities",
        "installation",
        "configuration",
        "maintenance",
        "Hadoop",
        "clusters",
        "application",
        "development",
        "Cloudera",
        "distribution",
        "Developed",
        "Kafka",
        "consumers",
        "API",
        "Scala",
        "data",
        "Kafka",
        "topics",
        "data",
        "pipelines",
        "data",
        "systems",
        "Kafka",
        "persistence",
        "data",
        "HDFS",
        "Apache",
        "Spark",
        "Scala",
        "performance",
        "tuning",
        "Spark",
        "jobs",
        "Cache",
        "advantage",
        "cluster",
        "environment",
        "framework",
        "table",
        "names",
        "schemas",
        "location",
        "source",
        "file",
        "Sqoop",
        "parameters",
        "framework",
        "code",
        "Workdlowxml",
        "Performed",
        "operations",
        "text",
        "analytics",
        "processing",
        "inmemory",
        "computing",
        "capabilities",
        "Spark",
        "Scala",
        "Experience",
        "query",
        "data",
        "Spark",
        "SQL",
        "Spark",
        "Spark",
        "RDDS",
        "Scala",
        "scripting",
        "technologies",
        "Python",
        "UNIX",
        "shell",
        "Performed",
        "POC",
        "spark",
        "applications",
        "Scala",
        "Python",
        "R",
        "programming",
        "language",
        "Partitioning",
        "Bucketing",
        "execution",
        "Map",
        "side",
        "optimization",
        "hive",
        "Performed",
        "Hive",
        "QL",
        "Hive",
        "tables",
        "Hive",
        "queries",
        "data",
        "analysis",
        "Experience",
        "log",
        "data",
        "web",
        "servers",
        "HDFS",
        "Flume",
        "NoSql",
        "database",
        "Cassandra",
        "Oozie",
        "workflow",
        "scheduling",
        "Jobs",
        "Hadoop",
        "Cluster",
        "Zookeeper",
        "cluster",
        "coordination",
        "services",
        "NIFI",
        "transformation",
        "data",
        "components",
        "data",
        "ecosystem",
        "data",
        "sources",
        "Oracle",
        "Netezza",
        "MySQL",
        "files",
        "AWS",
        "components",
        "Amazon",
        "Ec2",
        "S3",
        "buckets",
        "Cloud",
        "Formation",
        "Qlik",
        "sense",
        "reports",
        "worksheets",
        "dashboards",
        "file",
        "formats",
        "Sequence",
        "files",
        "XML",
        "files",
        "Map",
        "files",
        "Map",
        "Reduce",
        "Programs",
        "expertise",
        "MapReduce",
        "programming",
        "model",
        "XML",
        "CSV",
        "file",
        "formats",
        "developers",
        "code",
        "review",
        "sessions",
        "Agile",
        "Scrum",
        "Methodologies",
        "Experience",
        "Spark",
        "RDD",
        "transformations",
        "actions",
        "data",
        "case",
        "classes",
        "data",
        "Spark",
        "core",
        "Jira",
        "bug",
        "tracking",
        "Bitbucket",
        "code",
        "review",
        "apache",
        "airflow",
        "DAG",
        "items",
        "Redshift",
        "PostgreSQL",
        "web",
        "service",
        "call",
        "Spark",
        "applications",
        "data",
        "processing",
        "project",
        "data",
        "sources",
        "DStreams",
        "Data",
        "frames",
        "input",
        "data",
        "streaming",
        "service",
        "Kafka",
        "Environment",
        "Map",
        "HDFS",
        "Hive",
        "Spark",
        "SparkSQL",
        "Sqoop",
        "Apache",
        "Kafka",
        "Java",
        "Cassandra",
        "Scala",
        "Apache",
        "Pig",
        "Apache",
        "Hive",
        "Linux",
        "AWS",
        "EC2",
        "Agile",
        "development",
        "Oracle",
        "g",
        "UNIX",
        "Shell",
        "Ambari",
        "TezEclipse",
        "Qlik",
        "sense",
        "Cloudera",
        "Hadoop",
        "Developer",
        "Transamerica",
        "Dallas",
        "TX",
        "July",
        "January",
        "Transamerica",
        "Corporation",
        "company",
        "life",
        "insurance",
        "companies",
        "investment",
        "firms",
        "Transamerica",
        "companies",
        "providers",
        "life",
        "insurance",
        "savings",
        "retirement",
        "investment",
        "solutions",
        "millions",
        "customers",
        "United",
        "States",
        "Canada",
        "Responsibilities",
        "Cassandra",
        "Query",
        "Language",
        "Cassandra",
        "database",
        "tables",
        "configuration",
        "options",
        "PIG",
        "UDFS",
        "data",
        "Business",
        "Requirements",
        "custom",
        "PIG",
        "Loaders",
        "Experience",
        "Hadoop",
        "Ganglia",
        "understanding",
        "Hadoop",
        "metrics",
        "visualization",
        "Ganglia",
        "review",
        "requirements",
        "experience",
        "Spark",
        "applications",
        "Eclipse",
        "Maven",
        "Strong",
        "understanding",
        "Spark",
        "time",
        "streaming",
        "SparkSQL",
        "Loading",
        "data",
        "data",
        "sources",
        "MySQL",
        "Cassandra",
        "Spark",
        "applications",
        "Python",
        "Shell",
        "scripts",
        "endtoend",
        "implementation",
        "process",
        "AI",
        "project",
        "Experience",
        "Amazon",
        "EC2",
        "instances",
        "AWS",
        "services",
        "client",
        "tools",
        "AWS",
        "SDKs",
        "Knowledge",
        "AWS",
        "identity",
        "Access",
        "Management",
        "access",
        "EC2",
        "instances",
        "configure",
        "groups",
        "CloudWatch",
        "Firm",
        "understanding",
        "optimizations",
        "performancetuning",
        "practices",
        "Spark",
        "knowledge",
        "compression",
        "serialization",
        "performance",
        "Spark",
        "applications",
        "querying",
        "SparkSQL",
        "Worked",
        "file",
        "formats",
        "Sequence",
        "files",
        "XML",
        "files",
        "Map",
        "files",
        "Map",
        "Reduce",
        "Programs",
        "expertise",
        "MapReduce",
        "programming",
        "model",
        "XML",
        "CSV",
        "file",
        "formats",
        "Partitioning",
        "Dynamic",
        "Bucketing",
        "HIVEAWS",
        "knowledge",
        "Apache",
        "Sqoop",
        "datasets",
        "MySQL",
        "HDFS",
        "viceversa",
        "knowledge",
        "models",
        "customer",
        "service",
        "R",
        "programming",
        "Experience",
        "Hadoop",
        "log",
        "Experience",
        "building",
        "batch",
        "streaming",
        "applications",
        "Apache",
        "Spark",
        "Python",
        "libraries",
        "Mlib",
        "data",
        "cleaning",
        "R",
        "programming",
        "dataset",
        "Debug",
        "CQL",
        "performance",
        "enhancement",
        "knowledge",
        "Apache",
        "Oozie",
        "tasks",
        "knowledge",
        "Kafka",
        "thirdparty",
        "systems",
        "Spark",
        "Hadoop",
        "Experience",
        "Kafka",
        "brokers",
        "consumers",
        "producers",
        "performance",
        "Knowledge",
        "Apache",
        "Kafka",
        "consumers",
        "producers",
        "Java",
        "Hive",
        "tables",
        "data",
        "hive",
        "queries",
        "map",
        "way",
        "knowledge",
        "Hadoop",
        "cluster",
        "Nagios",
        "Ganglia",
        "Experience",
        "GIT",
        "version",
        "control",
        "system",
        "loading",
        "data",
        "UNIX",
        "file",
        "system",
        "HDFS",
        "UNIX",
        "scripts",
        "job",
        "scheduling",
        "process",
        "management",
        "logs",
        "Hadoop",
        "Understanding",
        "specifications",
        "design",
        "documents",
        "skills",
        "development",
        "TestDriven",
        "development",
        "knowledge",
        "Internet",
        "Things",
        "IoT",
        "Environment",
        "Hadoop",
        "Cloudera",
        "Distribution",
        "CDH4",
        "Java",
        "Hadoop",
        "Spark",
        "SparkSQL",
        "Mlib",
        "R",
        "programming",
        "Scala",
        "Cassandra",
        "IoT",
        "MapReduce",
        "Apache",
        "Pig",
        "Apache",
        "Hive",
        "HDFS",
        "Sqoop",
        "Oozie",
        "Kafka",
        "Maven",
        "Eclipse",
        "Nagios",
        "Ganglia",
        "Zookeeper",
        "EC2",
        "GIT",
        "Ambari",
        "TezEclipse",
        "UNIX",
        "Shell",
        "Oracle",
        "g",
        "Linux",
        "Agile",
        "development",
        "Hadoop",
        "Developer",
        "Pacific",
        "Life",
        "Newport",
        "Beach",
        "CA",
        "August",
        "June",
        "Pacific",
        "Life",
        "group",
        "companies",
        "providers",
        "Life",
        "insurance",
        "United",
        "States",
        "ECommerce",
        "web",
        "application",
        "customers",
        "online",
        "policy",
        "service",
        "account",
        "company",
        "offices",
        "business",
        "home",
        "insurance",
        "office",
        "details",
        "transactions",
        "system",
        "feed",
        "text",
        "format",
        "data",
        "line",
        "format",
        "Responsibilities",
        "amounts",
        "data",
        "sets",
        "way",
        "Incremental",
        "Imports",
        "Hive",
        "tables",
        "Apache",
        "PIG",
        "scripts",
        "HIVE",
        "scripts",
        "HDFS",
        "data",
        "job",
        "log",
        "files",
        "Unit",
        "testing",
        "Unit",
        "test",
        "plans",
        "documents",
        "Junit",
        "MR",
        "unit",
        "Supported",
        "Map",
        "Reduce",
        "Programs",
        "cluster",
        "Big",
        "Data",
        "Developer",
        "solutions",
        "data",
        "sources",
        "DataatRest",
        "Big",
        "Data",
        "technologies",
        "Hadoop",
        "MapReduce",
        "Frameworks",
        "HBase",
        "Hive",
        "Oozie",
        "Flume",
        "Sqoop",
        "Imported",
        "Bulk",
        "Data",
        "HBase",
        "Map",
        "Reduce",
        "programs",
        "analytics",
        "Time",
        "Series",
        "Data",
        "HBase",
        "HBase",
        "API",
        "data",
        "servers",
        "HDFS",
        "Apache",
        "Flume",
        "Written",
        "Hive",
        "jobs",
        "logs",
        "format",
        "querying",
        "log",
        "data",
        "monitoring",
        "Elastic",
        "MapReduce",
        "cluster",
        "AWS",
        "console",
        "Wrote",
        "java",
        "programs",
        "data",
        "HBase",
        "File",
        "Processing",
        "Pig",
        "Latin",
        "Hive",
        "tables",
        "data",
        "hive",
        "queries",
        "map",
        "way",
        "Experience",
        "optimization",
        "Map",
        "algorithm",
        "combiners",
        "partitions",
        "results",
        "Application",
        "performance",
        "optimization",
        "HDFS",
        "cluster",
        "performance",
        "tuning",
        "Hive",
        "Pig",
        "Jobs",
        "Hive",
        "correlations",
        "customers",
        "browser",
        "logs",
        "sites",
        "risk",
        "profile",
        "sites",
        "business",
        "logic",
        "UDFs",
        "Java",
        "UDFs",
        "Piggybanks",
        "sources",
        "documentation",
        "HADOOP",
        "Clusters",
        "Hive",
        "queries",
        "Pig",
        "Scripts",
        "Environment",
        "Java",
        "Hadoop",
        "Map",
        "Reduce2",
        "Pig",
        "Hive",
        "Linux",
        "Sqoop",
        "Flume",
        "Eclipse",
        "AWS",
        "EC2",
        "Cloudera",
        "CDH",
        "Java",
        "Developer",
        "Aegis",
        "Consulting",
        "Services",
        "December",
        "May",
        "Aegis",
        "Consulting",
        "Services",
        "state",
        "art",
        "technologies",
        "Knowledge",
        "technologies",
        "suitability",
        "solution",
        "company",
        "domain",
        "knowledge",
        "companys",
        "vision",
        "group",
        "technology",
        "talent",
        "capacity",
        "results",
        "resources",
        "Responsibilities",
        "Analysis",
        "Design",
        "Implementation",
        "Bug",
        "Fixing",
        "Activities",
        "WebWAP",
        "pages",
        "UI",
        "requirement",
        "design",
        "Class",
        "Diagrams",
        "Sequence",
        "Diagrams",
        "Event",
        "Diagrams",
        "part",
        "Documentation",
        "SQL",
        "queries",
        "Procedures",
        "PLSQL",
        "database",
        "schemas",
        "Functional",
        "Technical",
        "Specification",
        "documents",
        "review",
        "code",
        "review",
        "Undergone",
        "training",
        "Domain",
        "Knowledge",
        "Discussions",
        "meetings",
        "Business",
        "Analysts",
        "functionality",
        "Test",
        "Cases",
        "review",
        "Support",
        "Guide",
        "functionality",
        "Environment",
        "Core",
        "Java",
        "Oracle",
        "9i",
        "Java",
        "Script",
        "HTML",
        "PLSQL",
        "Rational",
        "Rose",
        "Windows",
        "XP",
        "UNIX",
        "Software",
        "Engineer",
        "Wind",
        "Stream",
        "Communications",
        "June",
        "November",
        "Wind",
        "Stream",
        "Communications",
        "Inc",
        "goal",
        "manufacturing",
        "energy",
        "technologies",
        "marketplace",
        "Responsibilities",
        "Data",
        "Migration",
        "Database",
        "Oracle",
        "Database",
        "Linux",
        "scripts",
        "Java",
        "SQL",
        "loader",
        "member",
        "team",
        "role",
        "Design",
        "requirements",
        "Development",
        "tools",
        "error",
        "Configuration",
        "UNIX",
        "utilities",
        "Data",
        "migration",
        "Oracle",
        "Sole",
        "developer",
        "POC",
        "migration",
        "Activity",
        "JSP",
        "pages",
        "Servlets",
        "HTML",
        "pages",
        "requirement",
        "Java",
        "Beans",
        "PLSQL",
        "procedures",
        "implementation",
        "business",
        "rules",
        "user",
        "interface",
        "JAVA",
        "Server",
        "Pages",
        "JSP",
        "HTML",
        "Java",
        "Script",
        "Presentation",
        "Tier",
        "JSP",
        "pages",
        "validation",
        "script",
        "tags",
        "realm",
        "Apache",
        "Tomcat",
        "Server",
        "users",
        "end",
        "controller",
        "Servlet",
        "requests",
        "web",
        "interface",
        "JSP",
        "struts",
        "action",
        "classes",
        "requirements",
        "impact",
        "analysis",
        "solutions",
        "build",
        "basis",
        "Java",
        "Java",
        "Script",
        "HTML",
        "JDBC",
        "database",
        "connectivity",
        "tables",
        "Oracle",
        "Used",
        "WebSphere",
        "Application",
        "Server",
        "application",
        "deployment",
        "Implemented",
        "Software",
        "Development",
        "Life",
        "Cycle",
        "Requirements",
        "Analysis",
        "Design",
        "Development",
        "Testing",
        "Deployment",
        "Support",
        "Environment",
        "J2EE",
        "IBM",
        "DB2",
        "IBM",
        "WebSphere",
        "Application",
        "Server",
        "EJB",
        "JSP",
        "Servlets",
        "HTML",
        "CSS",
        "JavaScript",
        "Oracle",
        "database",
        "Unix",
        "Scripting",
        "Windows",
        "Skills",
        "CASSANDRA",
        "MAPREDUCE",
        "OOZIE",
        "SQOOP",
        "HBASE",
        "KAFKA",
        "ELASTICSEARCH",
        "FLUME",
        "HADOOP",
        "APPLICATION",
        "SERVER",
        "Git",
        "Hadoop",
        "HBase",
        "Hive",
        "HTML",
        "JAVASCRIPT",
        "MapReduce",
        "Pig",
        "PYTHON",
        "PYSPARK",
        "Additional",
        "Information",
        "TECHNICAL",
        "SKILLS",
        "Programming",
        "C",
        "Java",
        "Python",
        "Scala",
        "SQL",
        "HADOOPBIG",
        "DATA",
        "MapReduce",
        "Spark",
        "SparkSQL",
        "PySpark",
        "SparkR",
        "Pig",
        "Hive",
        "Sqoop",
        "HBase",
        "Flume",
        "Kafka",
        "Cassandra",
        "Yarn",
        "Oozie",
        "Zookeeper",
        "ElasticSearch",
        "MySQL",
        "PLSQL",
        "Mongo",
        "DB",
        "HBase",
        "Cassandra",
        "Operating",
        "Systems",
        "Windows",
        "Unix",
        "Linux",
        "Ubuntu",
        "Web",
        "Development",
        "HTML",
        "JSP",
        "JavaScript",
        "JQuery",
        "CSS",
        "XML",
        "AJAX",
        "WebApplication",
        "Servers",
        "Apache",
        "Tomcat",
        "Sun",
        "Java",
        "Application",
        "Server",
        "Tools",
        "IntelliJ",
        "Eclipse",
        "Net",
        "Beans",
        "Nagios",
        "Ganglia",
        "Maven",
        "Scripting",
        "BASH",
        "JavaScript",
        "Version",
        "Controls",
        "GIT",
        "SVN"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T20:49:07.341213",
    "resume_data": "Sr Hadoop Developer Sr Hadoop span lDeveloperspan Sr Hadoop Developer United Technologies Corporation Farmington CT 7 Years of IT industry experience with 5 years of experience in dealing with Apache Hadoop components like HDFS MapReduce Spark Hive Pig Sqoop Oozie Zookeeper HBase Cassandra MongoDB and Amazon Web Services 2 years of experience in the Application Development and Maintenance of SDLC projects using Java technologies Developed applications for Distributed Environment using Hadoop MapReduce and Python Developed MapReduce jobs to automate transfer of data from HBase Developing and Maintenance the Web Applications using the Web Server Tomcat Experience in integrating Hadoop with Ganglia and have good understanding of Hadoop metrics and visualization using Ganglia Good experience working with Hortonworks Distribution Cloudera Distribution and MapR Distribution Very good understandingknowledge of Hadoop Architecture and various components such as HDFS JobTracker TaskTracker NameNode DataNode Secondary Namenode and MapReduce concepts Experience in data extraction and transformation using MapReduce jobs Proficient in working with Hadoop HDFS writing PIG scripts and Sqoop scripts Performed data analysis using Hive and Pig Expert in creating Pig and Hive UDFs using Java in order to analyze the data efficiently Experience in importing and exporting data using Sqoop from Relational Database Systems to HDFS and viceversa Strong understanding of NoSql databases like HBase MongoDB Cassandra Strong understanding of Spark real time streaming and SparkSQL and experience in loading data from external data sources like MySQL and Cassandra for Spark applications Experience in performing inmemory data processing and real time streaming analytics using Apache Spark with Scala Java and Python Created HBase tables to load large sets of structured semistructured and unstructured data coming from UNIX NoSQL and a variety of portfolios Intensive working experience with Amazon Web ServicesAWS using S3 for storage EC2 for computing and RDS EBS Well versed with job workflow scheduling and monitoring tools like Oozie Loaded streaming log data from various web servers into HDFS using Flume Experience in using Sqoop Oozie and Cloudera Manager Experience on Source control repositories like SVN CVS and GIT Experience in improving the search focus and quality in ElasticSearch by using aggregations and Python scripts Hands on experience in application development using RDBMS and Linux shell scripting Have experience with working on Amazon EMR and EC2 Spot instances Solid understanding of relational database concepts Extensively worked with Unified Modeling Tools UML in designing Use Cases Activity flow diagram Class diagrams Sequence and Object Diagrams using Rational Rose MSVisio Very good experience in complete project life cycle design development testing and implementation of Client Server and Web applications Good Knowledge on Hadoop Cluster architecture and monitoring the cluster Adequate knowledge and working experience in Agile Waterfall methodologies Support development testing and operations teams during new system deployments Practical knowledge on implementing Kafka with thirdparty systems such as Spark and Hadoop Good team player and can work efficiently in multiple team environments and multiple products Easily adaptable to the new systems and environments Possess excellent communication and analytical skills along with a cando attitude Work Experience Sr Hadoop Developer United Technologies Corporation Farmington CT February 2018 to Present United Technologies Corporation is a financial services and bank holding company in United States The objective should be analyzing the huge amount of data and producing largescale programs that integrate with technology for the clients to achieve customer satisfaction Implement and deploy custom applications on Hadoop clusters Providing information security customer satisfaction and fraud detection solutions using big data analytics The technical environment includes Spark Kafka Scala Hive and Oozie Responsibilities Involved in installation configuration and maintenance of Hadoop clusters for application development with Cloudera distribution Developed Kafka consumers API in Scala for consuming data from Kafka topics Developed endtoend scalable distributed data pipelines which receiving data using distributed messaging systems Kafka through persistence of data into HDFS with Apache Spark using Scala Involved in performance tuning of Spark jobs using Cache and using complete advantage of cluster environment In the framework we just need to mention the table names schemas and location of source file Sqoop parameters etc and the framework will generate the entire code which includes Workdlowxml Performed advanced operations like text analytics and processing using inmemory computing capabilities of Spark using Scala Experience in query data using Spark SQL on Spark to implement Spark RDDS in Scala Experienced in working with different scripting technologies like Python UNIX shell scripts Performed POC on writing the spark applications in Scala Python and R programming language Worked on Partitioning Bucketing Parallel execution Map side Joins for optimization of necessary hive queries Performed Hive QL to create Hive tables and to write Hive queries to perform the data analysis Experience in collecting log data from web servers and pushed to HDFS using Flume and NoSql database Cassandra Used Oozie workflow to Manage and scheduling Jobs on a Hadoop Cluster and used Zookeeper for cluster coordination services Used NIFI for the transformation of data from different components of Big data ecosystem Worked on different data sources like Oracle Netezza MySQL Flat files etc and experience with AWS components like Amazon Ec2 instances S3 buckets and Cloud Formation templates Used Qlik sense to build customized interactive reports worksheets and dashboards Worked on different file formats like Sequence files XML files and Map files using Map Reduce Programs Strong expertise on MapReduce programming model with XML JSON CSV file formats Involved in managing and organizing developers with regular code review sessions by utilizing Agile and Scrum Methodologies Experience in implementing Spark RDD transformations actions data frames case classes to required data by using Spark core Implemented Jira for bug tracking and Bitbucket to code and code review Implemented apache airflow DAG to find popular items in Redshift and ingest in the main PostgreSQL via a web service call Implemented Spark applications in data processing project to handle data from various sources and creating DStreams Data frames on input data which we get from streaming service like Kafka Environment Map Reduce HDFS Hive Spark SparkSQL Sqoop Apache Kafka Java 7 Cassandra Scala Apache Pig 0140 Apache Hive 100Oozie Linux AWS EC2 Agile development Oracle 11g10g UNIX Shell scripting Ambari TezEclipse and Qlik sense Cloudera Hadoop Developer Transamerica Dallas TX July 2016 to January 2018 The Transamerica Corporation is an American private holding company for various life insurance companies and investment firms Transamerica companies are recognized as leading providers of life insurance savings and retirement and investment solutions serving millions of customers throughout the United States and Canada Responsibilities Used Cassandra Query Language to design Cassandra database and tables with various configuration options Developed PIG UDFS for manipulating the data according to Business Requirements and also worked on developing custom PIG Loaders Experience in integrating Hadoop with Ganglia and have good understanding of Hadoop metrics and visualization using Ganglia Involved in the review of functional and nonfunctional requirements Practical experience in developing Spark applications in Eclipse with Maven Strong understanding of Spark real time streaming and SparkSQL Loading data from external data sources like MySQL and Cassandra for Spark applications Developed Python and Shell scripts to automate the endtoend implementation process of AI project Experience in selecting and configuring the right Amazon EC2 instances and access key AWS services using client tools and AWS SDKs Knowledge on using AWS identity and Access Management to secure access to EC2 instances and configure autoscaling groups using CloudWatch Firm understanding of optimizations and performancetuning practices while working with Spark Good knowledge on compression and serialization to improve performance in Spark applications Performed interactive querying using SparkSQL Worked on different file formats like Sequence files XML files and Map files using Map Reduce Programs Strong expertise on MapReduce programming model with XML JSON CSV file formats Designed and Implemented Partitioning static Dynamic and Bucketing in HIVEAWS Practical knowledge on Apache Sqoop to import datasets from MySQL to HDFS and viceversa Good knowledge on building predictive models focusing on customer service using R programming Experience in reviewing and managing Hadoop log files Experience in building batch and streaming applications with Apache Spark and Python Used the libraries built on Mlib to perform data cleaning and used R programming for dataset reorganizing Debug CQL queries and implement performance enhancement practices Strong knowledge on Apache Oozie for scheduling the tasks Practical knowledge on implementing Kafka with thirdparty systems such as Spark and Hadoop Experience in configuring Kafka brokers consumers and producers for optimal performance Knowledge of creating Apache Kafka consumers and producers in Java Involved in creating Hive tables loading with data and writing hive queries which will run internally in map reduce way Practical knowledge of monitoring a Hadoop cluster using Nagios and Ganglia Experience with GIT for version control system Involved in loading data from UNIX file system to HDFS and developed UNIX scripts for job scheduling process management and for handling logs from Hadoop Understanding technical specifications and documenting technical design documents Strong skills in agile development and TestDriven development Have practical knowledge on implementing Internet of Things IoT Environment Hadoop Cloudera Distribution CDH4 Java 7 Hadoop 252 Spark SparkSQL Mlib R programming Scala Cassandra IoT MapReduce Apache Pig 0140 Apache Hive 100 HDFS Sqoop Oozie Kafka Maven Eclipse Nagios Ganglia Zookeeper AWS EC2 GIT Ambari TezEclipse UNIX Shell scripting Oracle 11g10g Linux Agile development Hadoop Developer Pacific Life Newport Beach CA August 2015 to June 2016 Pacific Life a leading diversified international group of companies and is one of the top providers of Life insurance in United States ECommerce is a web application that facilitates customers to get a fast quote online originate a policy and service an account The company has many regional offices in the business of home and insurance Each regional office sends the details of monthly transactions to the central system Most of the feed is in the raw text format and the data is in fixed line format Responsibilities Analyzed large amounts of data sets to determine optimal way to aggregate and report on it Designed and implemented Incremental Imports into Hive tables Developed and written Apache PIG scripts and HIVE scripts to process the HDFS data Involved in defining job flows managing and reviewing log files Involved in Unit testing and delivered Unit test plans and results documents using Junit and MR unit Supported Map Reduce Programs those are running on the cluster As a Big Data Developer implemented solutions for ingesting data from various sources and processing the DataatRest utilizing Big Data technologies such as Hadoop MapReduce Frameworks HBase Hive Oozie Flume Sqoop etc Imported Bulk Data into HBase Using Map Reduce programs Perform analytics on Time Series Data exists in HBase using HBase API Involved in collecting aggregating and moving data from servers to HDFS using Apache Flume Written Hive jobs to parse the logs and structure them in tabular format to facilitate effective querying on the log data Responsible for continuous monitoring and managing Elastic MapReduce cluster through AWS console Wrote multiple java programs to pull data from HBase Involved with File Processing using Pig Latin Involved in creating Hive tables loading with data and writing hive queries that will run internally in map reduce way Experience in optimization of Map reduce algorithm using combiners and partitions to deliver the best results and worked on Application performance optimization for a HDFS cluster Worked on debugging performance tuning of Hive Pig Jobs Used Hive to find correlations between customers browser logs in different sites and analyzed them to build risk profile for such sites Implemented business logic by writing UDFs in Java and used various UDFs from Piggybanks and other sources Created and maintained Technical documentation for launching HADOOP Clusters and for executing Hive queries and Pig Scripts Environment Java Hadoop 210 Map Reduce2 Pig 0120 Hive 0130 Linux Sqoop 142 Flume 131 Eclipse AWS EC2 and Cloudera CDH 4 Java Developer Aegis Consulting Services IN December 2013 to May 2015 Aegis Consulting Services keeps abreast with the state of art technologies Knowledge of the technologies are carefully analyzed for suitability and appropriately deployed with the solution The company has indepth domain as well as functional knowledge The companys vision is to create a cohesive group of technology creative talent which holds the capacity to deliver effective results while deploying optional resources Responsibilities Involved in Analysis Design Implementation and Bug Fixing Activities Designing the initial WebWAP pages for a better UI as per the requirement Involved in design of basic Class Diagrams Sequence Diagrams and Event Diagrams as a part of Documentation Developed SQL queries and Stored Procedures using PLSQL to retrieve and insert into multiple database schemas Involved in Functional Technical Specification documents review and the code review Undergone training on the Domain Knowledge Discussions and meetings with the Business Analysts for understanding the functionality involved in Test Cases review Prepared the Support Guide containing the complete functionality Environment Core Java ApacheTomcat51 Oracle 9i Java Script HTML PLSQL Rational Rose Windows XP UNIX Software Engineer Wind Stream Communications IN June 2012 to November 2013 Wind Stream Communications Inc was established in 2008 with the goal of designing prototyping and manufacturing affordable and scalable renewable energy technologies for a global marketplace Responsibilities Designed developed and executed Data Migration from Db2 Database to Oracle Database using Linux scripts Java and SQL loader concepts A key member of the team and playing a key role in articulating the Design requirements for the Development of Automated tools that perform error free Configuration Developed UNIX and java utilities for Data migration from Db2 to Oracle Sole developer and POC for the migration Activity Developed JSP pages Servlets and HTML pages as per requirement Developed the necessary Java Beans PLSQL procedures for the implementation of business rules Developed user interface using JAVA Server Pages JSP HTML and Java Script for the Presentation Tier Developed JSP pages and clientside validation by java script tags Developed an own realm for Apache Tomcat Server for authenticating the users Developed front end controller in Servlet to handle all the requests Developed the web interface using JSP and developed struts action classes Responsible for both functional and nonfunctional requirements gathering performing impact analysis and testing the solutions build on build basis Coding using Java Java Script and HTML Used JDBC to provide database connectivity to database tables in Oracle Used WebSphere Application Server for application deployment Implemented Software Development Life Cycle Requirements Analysis Design Development Testing Deployment and Support Environment J2EE IBM DB2 IBM WebSphere Application Server EJB JSP Servlets HTML CSS JavaScript Oracle database Unix Scripting and Windows 2000 Skills CASSANDRA MAPREDUCE OOZIE SQOOP HBASE KAFKA ELASTICSEARCH FLUME HADOOP APPLICATION SERVER Git Hadoop HBase Hive HTML JAVASCRIPT MapReduce Pig PYTHON PYSPARK Additional Information TECHNICAL SKILLS Programming languages C Java Python Scala SQL HADOOPBIG DATA MapReduce Spark SparkSQL PySpark SparkR Pig Hive Sqoop HBase Flume Kafka Cassandra Yarn Oozie Zookeeper ElasticSearch Databases MySQL PLSQL Mongo DB HBase Cassandra Operating Systems Windows Unix Linux Ubuntu Web Development HTML JSP JavaScript JQuery CSS XML AJAX WebApplication Servers Apache Tomcat Sun Java Application Server Tools IntelliJ Eclipse Net Beans Nagios Ganglia Maven Scripting BASH JavaScript Version Controls GIT SVN",
    "unique_id": "1be19e23-02e7-42ea-a0ba-06c5692597a9"
}