{
    "clean_data": "Data Scientist Data Scientist Data Scientist CUNA Mutual Group 7 years of Data Science experience in architecting and building comprehensive analytical solutions in Marketing Sales and Operations functions across Technology Banking Manufacturing Healthcare and Retail industries Extensive experience in Text Analytics developing different Statistical Machine Learning Data Mining solutions to various business problems and generating data visualizations using R Python Expertise in transforming business requirements into analytical models designing algorithms building models developing data mining and reporting solutions that scale across a massive volume of structured and unstructured data Expert knowledge in supervised and unsupervised learning algorithms such as Ensemble Methods Random forests Logistic Regression Regularized Linear Regression SVMs Deep Neural Networks Extreme Gradient Boosting Decision Trees KMeans Gaussian Mixture Models Hierarchical models and time series models ARIMA GARCH VARCH etc Expertise writing production quality code in SQL R Python and Spark Hands on experience building regression and classification models and other unsupervised learning algorithms with large datasets in distributed systems and resource constrained environments Familiar with predictive models using classification algorithms like KNN Naive base regression and decision trees Familiar with predictive models using numeric and classification prediction algorithms like support vector machines and neural networks and ensemble methods like bagging boosting and random forest to improve the efficiency of the predictive model Worked on Text Mining and Sentimental analysis for extracting the unstructured data from various social Media platforms like Facebook Twitter and Reddit Ability to translate analytic ideas into R Python production quality scripts Strong background handson in Data Science Big Data data structures statistics algorithms like Regression Classification etc Strong background handson of Supervised learning Decision Trees Random Forest Logistic Regression SVMs GBM etc and unsupervised learning KMeans KNN Strong background handson of Deep learning using Tensforflow Keras Theano H2o Sound understanding of Deep learning using CNN RNN ANN reinforcement learning transfer learning Strong background handson in Natural Language Processing and text analytics Experience and passion for solving analytical problems involving big data sets using quantitative approaches to generate insights from data Identify analyze and interpret trends or patterns in complex data sets Strong in Predictive and Prescriptive analytics approaches and experienced in operating tools like R and in programming using Python Has working experience in Data Science Machine Learning implementation in cloud platforms like Google Cloud Platform AWS Azure Bluemix Familiar with predictive models using different cloud based Machine learning tools like Microsoft Azure ML Oversee all activities related to data cleansing data quality and data consolidation using industry standards and processes Perform exploratory data analysis generate and test working hypothesis and prepare and analyze historical data and identify patterns Work closely with Cross Functional teams to encourage best practices for experimental design and data analysis Proficient with Python 3x including Numpy Scikitlearn Pandas Matplotlib and Seaborn Extensive experience in RDBMS such as SQL server 2012 MySQL 5x Experienced in Nonrelational database such as MongoDB 3x Experienced in Hadoop 2x ecosystem and Apache Spark 2x framework such as Hive Pig Pyspark Proficient at data visualization tools such as Tableau Power BI Python Matplotlib and Seaborn Experienced in Amazon Web Services AWS and Microsoft Azure such as AWS EC2 S3 RD3 Azure HDInsight Machine Learning Studio Azure Data Lake Apply various data modeling techniques to model user behavior and identify actionable levers for retaining and growing users Define key metrics conduct AB testing and oversee statistical measurement of new algorithms and approaches Expert at distilling questions wrangling data and driving decisions with data analytics Strong knowledge of relational databases and ability to write SQL code at an expert level Aptitude with numbers intellectual curiosity about metrics and measuring impact Present models findings and insights to senior management to catalyze business decisions Ability to solve complex problems by applying analytical techniques and predictive models to massive data sets and translate business needs into mathematical abstractions for algorithms to solve Research and prototype models and pipelines and work with engineers to put them into production at scale Raw data analysis such as assessing quality cleansing structuring for downstream processing Use mathematical statistical and programmatic knowledge to spec out design and build firstclass predictive models about customer behavior Design and prototype of accurate and scalable prediction algorithms Collaboration with engineering team to bring analytical prototypes to production Creative and pragmatic quantitatively minded individual with a passion for understanding location and human behavior Ability to identify issues quickly and rapidly determine root cause and effective resolution approach Very solid data analysis skills including application of analytical techniques such as statistical and machine learning Fundamental coding skills enabling the building of analytical pipelines and the development of prototypes for core company products and systems Excellent verbal and written communication skills ability to communicate technical topics to nontechnical individuals Ability to manage own time and work effectively with others on projects Work Experience Data Scientist CUNA Mutual Group Madison WI April 2017 to Present This project was to support auditing team and claim department to improve accounting accuracy and reduce risk of fraudulent activities via providing machine learning and modeling solutions to identify suspicious insurance claims Claims severity prediction in realtime Built classification models to predict the fraudulent claims by severity in realtime reducing the time for execution from 6 hours to 4 seconds Implemented the models as a predictive solution for finding the fraudulent claims for the credit disability and debt protection products Forwarded the highrisk claims for further investigation Text analytics for fraud prediction Executed topic modelling for finding different topics based on the notes made corresponding to the claimants claim Attributed the resulting topics to classify into fraud and not fraud categories Risk assessment prediction Incorporated models built in Python and R into the business processes using clustering techniques to assess the risk involved with a customer The models built are used in assessing the premiums amount required to be paid by the customer Customer churnattrition prediction Developed models that predict whether a customers propensity to churn leveraging the information related to insurance policies demographics claims related to the customer payment frequency home ownership status household tenure etc Responsibilities Analyze Data and Performed Data Preparation by applying historical model on the data set in AZURE ML Perform Data cleaning process applied Backward Forward filling methods on dataset for handling missing value Perform Data Transformation method for Rescaling and Normalizing Variables Develop a predictive model and validate KNN model for predict the feature label Plan develop and apply leadingedge analytic and quantitative tools and modeling techniques to help clients gain insights and improve decisionmaking Leverage the most appropriate algorithms and be prepared to justify your decisions Work closely with key stakeholders in product finance and operations to form deep understanding of growth and marketplace dynamics including product and pricing patterns outlier detection forecasting and imputation Collaborate with product and engineering to integrate various sources of data Apply strict sampling statistical inference and survey techniques to derive insights from small samples of data Utilize Sqoop to ingest realtime data Used analytics libraries SciKit Learn MLLIB and MLxtend Extensively use Pythons multiple data science packages like Pandas NumPy matplotlib Seaborn SciPy Scikitlearn and NLTK Performed Exploratory Data Analysis trying to find trends and clusters Develop rigorous data science models to aggregate inconsistent realtime signals into strong predictors of market trends Automate and own the endtoend process of modeling and data visualization Collaborate with Data Engineers and Software Developers to develop experiments and deploy solutions to production Work on data that was a combination of unstructured and structured data from multiple sources and automate the cleaning using Python scripts Extensively perform large data readwrites to and from csv and excel files using pandas Tasked with maintaining RDDs using SparkSQL Communicate and coordinate with other departments to collection business requirement Tackle highly imbalanced Fraud dataset using under sampling with ensemble methods oversampling and cost sensitive algorithms Improved fraud prediction performance by using random forest and gradient boosting for feature selection with Python Scikitlearn Implemented machine learning model logistic regression XGboost with Python Scikit learn Optimize algorithm with stochastic gradient descent algorithm Finetuned the algorithm parameter with manual tuning and automated tuning such as Bayesian Optimization Write research reports describing the experiment conducted results and findings and also make strategic recommendations to technology product and senior management Built executive Banco Santander SA 2000 to Present doing business as Santander Group is a Spanish banking group As its name suggests the company originated in Santander Spain The group has expanded since 2000 through a number of acquisitions with operations across Europe South America North America and Asia Santander has been ranked as 37th in the Forbes Global 2000 list of the worlds biggest public companies Santander is Spains largest bank KEY PROJECTS Credit History Predictive Modeling Analyzed and predicted the customers credit history and past bill payments based on the Credit card offers to create a predictive model and send offers to customers on the base of Model and past data A system was successfully created on the past data of Credit History and Payments activity of customers Ran model against the historical data and get predicted label if customers were eligible for credit card offer and on that basis send them an offer As a result customers actually got an offer they liked and increased the number of offer acceptance which lead to profit for the bank Forecasting Loan balance Forecasted bankwide loan balances under normal and stressed macroeconomic scenarios using R Performed variable reduction using the stepwise lasso and elastic net algorithms and tuned the models for accuracy using cross validation and grid search techniques Top down Models Commercial Real Estate Automated the scraping and cleaning of data from various data sources in R and Python Developed Banks loss forecasting process using relevant forecasting and regression algorithms in R The projected losses under stress conditions helped bank reserve enough funds per DFAST policies Loan Payment Default Prediction Built classification models using several features related to customer demographics macroeconomic dynamics historic payment behavior type and size of loan credit scores and loan to value ratios and with accuracy of 95 accuracy the model predicted the likelihood of default under various stressed conditions Marketing Campaign Measurement Built executive dashboards in Tableau that measured changes in customer behavior post campaign launch the ROI measurements helped to strategically select the effective campaigns Credit Risk Scorecards Built credit risk scorecards and marketing response models using SQL and SAS Evangelized the complex technical analysis into easily digestible reports for top executives in the company Developed several interactive dashboards in Tableau to visualize nearly 5 Terabytes of credit data by designing a scalable data cube structure Responsibilities Gathered analyzed documented and translated application requirements into data models supported standardization of documentation and the adoption of standards and practices related to data and applications Queried and aggregated data from Amazon Redshift to get the sample dataset Identified patterns data quality issues and leveraged insights by communicating with BI team In preprocessing phase used Pandas to remove or replace all the missing data and feature engineering to eliminate unrelated features Balanced the dataset with Oversampling the minority label class and Undersampling the majority label class In data exploration stage used correlation analysis and graphical techniques to get some insights about the claim data Tested classification algorithms such as Logistic Regression Gradient Boosting and Random Forest using Pandas and Scikitlearn and evaluated the performance Implemented tuned and tested the model on AWS EC2 with the best algorithm and parameters Set up data preprocessing pipeline to guarantee the consistency between the training data and new coming data Deployed the model on AWS Lambda collaborated with develop team to build the business solutions Collected the feedback after deployment retrained the model to improve the performance Discovered flaws in the methodology being used to calculate weather peril zone relativities designed and implemented a 3D algorithm based on kmeans clustering and Monte Carlo methods Observed groups of customers being neglected by the pricing algorithm used hierarchical clustering to improve customer segmentation and increase profits by 6 Designed developed and maintained daily and monthly summary trending and benchmark reports in Tableau Desktop Environment AWS EC2 S3 Redshift Lambda Linux Python ScikitLearnNumpyPandasMatplotlib Machine Learning Logistic RegressionGradient BoostingRandom Forest Tableau Data Scientist Santander Boston MA March 2016 to April 2017 Data Scientist Banco Santander SA Austin TX March 2014 to December 2015 Whole Foods Market Inc is an American supermarket chain that specializes in selling organic foods products without artificial additive products for growing foods colors flavors sweeteners and hydrogenated fats It has 473 stores in North America and the United Kingdom KEY PROJECTS Customer Purchase Propensity Modelling Built machine learning based regression models using scikitlearn python frameworks to estimate the customer propensity to purchase based on attributes such as customer verticals they operate in revenue historic purchases frequency and regency behaviours These predictions helped estimate propensities with higher accuracy improving the overall productivity of sales teams by accurately targeting the prospective clients Coupon Recommender System Developed a personalized coupon recommender system using recommender algorithms collaborative filtering low rank matrix factorization that recommended best offers to a user based on similar user profiles The recommendations enabled users to engage better and helped improving the overall customer retention rates Outlier Anomalous Pattern Detection Created interactive dashboard suite that illustrated outlier characteristics across several salesrelated dimensions and overall impact of outlier imputation in R Shiny Used iterative outlier detection and imputation algorithm using multiple densitybased clustering techniques DBSCAN kernel density estimation Cross Sell and Upsell Opportunity Analysis Implemented market basket algorithms from transactional data which helped identify coupons usedpurchased together frequently Discovering frequent coupon sets helped unearth cross sell and up selling opportunities and led to better pricing bundling and promotion strategies for sales and marketing teams Forecast Process Innovations Forecast Sales and improved accuracy by 1020 by implementing advanced forecasting algorithms that were effective in detecting seasonality and trends in the patterns in addition to incorporating exogenous covariates Increased accuracy helped business plan better with respect to budgeting and sales and operations planning Price Elasticity Analysis Measured the price elasticity for products that experienced price cuts and promotions using regression methods based on the elasticity Whole Foods made selective and cautious price cuts for certain categories Customer Churn Prediction Predicted the likelihood of customer churn based on customer attributes like customer size RFM loyalty metrics revenue type of industry competitor products and growth rates etc The models deployed in production environment helped detect churn in advance and aided salesmarketing teams plan for various retention strategies in advance like price discounts custom licensing plans etc Responsibilities Working as project technical lead paired with business lead in scoping researching and assessing project feasibility outcomes and product deliverables on knowledge graph project Hadoop ecosystem Spark ElasticSearch JanusGraph etc Working with business lead successfully launched endtoend knowledge graph MVP in 7 months time whereas previous attempts at knowledge graph project had languished for 34 years Using NLP and ML built and maintained a variety of cloudbased natural language web scrapers and parsers to augment existing data sources Python NumPy SciPy SpaCy etc Spark TensorFlow Jupyter notebooks Mentored junior teammates shared knowledge of NLP and information retrieval best practices and performed code reviews on a weekly basis Prototyped conducted and reported on data science experiments to technical and nontechnical audiences using supervised semisupervised and unsupervised learning techniques anomaly detection named entity recognition ontology creation etc Using Python and NLTK aggregated natural language data from online documents and developed pipelines to ingest scraped data into relational databases Performed adhoc data science analyses for departments across the business using R Python Natural Language Toolkit NLTK machine learning AnalystData Scientist Banco Santander SA Hyderabad Telangana April 2012 to March 2014 Bharti AXA General Insurance Company Ltd is a joint venture between Bharti Enterprises a leading Indian business group and AXA a world leader in financial protection Bharti AXAs offers insurance coverage across various categories Motor Health Travel Home student travel and more The objective was to load data analyze and provide monthly reports for the predictions on a claims potential of a thirdparty recovery Tableau and SSRS were used to build claim and recovery reports Responsibilities Assembled a Predictive Modeling module by using supervised learning for Subrogation Claim Prediction to identify which claims would be classified as having Subrogation potential Implemented models such as Logistic Regression and Nave Bayes in Python using scikitlearn to predict the claim potential outcome Dimensionality Reduction techniques applied to refine the attribute lists and feature selection applied to rank selected features to generate accurate results Gathered requirements and business rules from business users to implement Predictive Modeling Designed and developed ETL packages using SSIS to create Data Warehouses from different tables and file sources like Flat and Excel files with different methods in SSIS such as derived columns aggregations Merge joins count conditional split and more to transform the data Designed reporting solutions for different stakeholders from mockup till deployment in different areas such as Potential Subrogation claims Monthly Revenue from Subrogation Transactions Performed data visualization and designed dashboards with Tableau and provided complex reports including charts summaries and graphs to interpret the findings for Adjustors to view various claim information Optimized queries in TSQL by removing unnecessary columns and redundant data normalized tables established joins and indices developed complex SQL queries stored procedures views functions and reports that meet customer requirements Environment Python 3x Scikitlearn Matplotlib Jupyter SQL Server 2012 MS SQL Server Management Studio MS BI Suite SSISSSRS TSQL Visual Studio BIDS Tableau Jr Python Developer Banco Santander SA Hyderabad Telangana June 2010 to April 2012 Responsibilities Worked on development of customer support and complains registration system This is a Customer feedback and complains management system Design develop test deploy and maintain the website Coding and execution of scripts in PythonUnixVB Development of Application using Java and Python Recording of Scripts Web Web Services HTML using Vugen and SoapUI and script validation through co correlations parameterizations and other methods Scripting web and web services Data set up using SQLORACLETeradata Resolving Complexity in the scripts of the website due to the complex logic and correlations Script validation sometimes becomes challenging as it demanded many web based logic rather than correlation and parameterization Running loadendurance tests using Vugen ALM and controller server monitoring analysis using Dynatrace UNIX putty SQL logs and other tools and reporting the performance Analyzing errors and exceptions using putty logs UNIX etc Testing in citrix protocol with scripts and scenario Execution of batch jobs in Control M Perfmon and other tools Scripting and validation of scripts through correlation parameterization and web based logic testing Smoke test Load test Endurance using Controller for a duration further analysis checking response times CPU utilizations memory leaks of servers and other performance characteristics of the website through capturing Perfmon logs and creating PAPAL reports and creating test reports Designed and developed data management system using MySQL Rewrite existing PythonDjangoJava module to deliver certain format of data Used Django Database APIs to access database objects Wrote python scripts to parse XML documents and load the data in database Generated property list for every application dynamically using python Responsible for search engine optimization SEO to improve the visibility of the website Handled all the client side validation using JavaScript Creating unit testregression test framework for workingnew code Using Subversion version control tool to coordinate teamdevelopment Responsible for debugging and troubleshooting the web application Environment Python Putty SQL Teradata SoapUI  PerfMon MySQL Linux HTML XHTML CSS AJAX JavaScript Apache Web Server Education Bachelor of Technology in Computer Science Engineering Sreenidhi Institute of Science Technology Hyderabad Telangana Skills ALGORITHMS 10 years BI 10 years BUSINESS INTELLIGENCE 10 years LINUX 10 years LOGISTIC REGRESSION 10 years Additional Information TECHNICAL SKILLS Languages Java 8 Python R Python and R Numpy SciPy Pandas Scikitlearn Matplotlib Seaborn ggplot2 caret dplyr purrr readxl tidyr Rweka gmodels RCurl C50 twitter NLP Reshape2 rjson plyr Beautiful Soup Rpy2 Algorithms Kernel Density Estimation and Nonparametric Bayes Classifier KMeans Linear Regression Neighbors Nearest Farthest Range k Classification NonNegative Matrix Factorization Dimensionality Reduction Decision Tree Gaussian Processes Logistic Regression Nave Bayes Random Forest Ridge Regression Matrix FactorizationSVD NLPMachine LearningDeep Learning LDA Latent Dirichlet Allocation NLTK Apache OpenNLP Stanford NLP Sentiment Analysis SVMs ANN RNN CNN TensorFlow MXNet Caffe H2O Keras PyTorch Theano Azure ML Cloud Google Cloud Platform AWS Azure Bluemix Web Technologies JDBC HTML5 DHTML and XML CSS3 Web Services WSDL Data Modelling Tools Erwin r 96 95 91 8x Rational Rose ERStudio MS Visio SAP Power designer Big Data Technologies Hadoop Hive HDFS MapReduce Pig Kafka Databases SQL Hive Impala Pig Spark SQL Databases SQLServer My SQL MS Access HDFS HBase Teradata Netezza MongoDB Cassandra Reporting Tools MS Office WordExcelPower Point Visio Tableau Crystal reports XI Business Intelligence SSRS Business Objects 5x 6x Cognos7060 ETL Tools Informatica Power Centre SSIS Version Control Tools SVM GitHub BI Tools Tableau Tableau Server Tableau Reader SAP Business Objects OBIEE QlikView SAP Business Intelligence Amazon Redshift or Azure Data Warehouse Operating System Windows Linux Unix Macintosh HD Red Hat",
    "entities": [
        "Statistical Machine Learning Data Mining",
        "Whole Foods Market Inc",
        "NLTK",
        "Coupon Recommender System Developed",
        "Perform Data Transformation",
        "Nonrelational",
        "Whole Foods",
        "Dynatrace",
        "Identify",
        "Data Science Big Data",
        "Customer",
        "BI",
        "Santander Spain",
        "My SQL MS Access HDFS HBase Teradata Netezza",
        "Data Scientist Data Scientist Data Scientist CUNA Mutual Group",
        "Data Science",
        "Google Cloud Platform AWS",
        "Upsell Opportunity Analysis Implemented",
        "Forecast Process Innovations Forecast Sales",
        "KNN Naive",
        "Amazon Web Services AWS",
        "Undersampling",
        "Informatica Power Centre SSIS Version",
        "Hadoop",
        "XML",
        "Retail industries Extensive",
        "Flat and Excel",
        "Potential Subrogation",
        "Credit",
        "SSIS",
        "Data Engineers and Software Developers",
        "Bharti Enterprises",
        "Machine",
        "Dimensionality Reduction",
        "Developed",
        "PythonUnixVB Development of Application",
        "Perfmon",
        "Amazon Redshift",
        "Bharti AXA General Insurance Company Ltd",
        "AXA",
        "Marketing Sales",
        "SciKit Learn",
        "Develop",
        "Control Tools",
        "Predictive and Prescriptive",
        "Adjustors",
        "Built",
        "Automate",
        "CPU",
        "XI Business Intelligence SSRS Business Objects",
        "Subrogation Transactions Performed",
        "AnalystData",
        "the United Kingdom KEY PROJECTS Customer Purchase",
        "KEY PROJECTS Credit History Predictive Modeling Analyzed",
        "GitHub BI",
        "Tableau Power BI",
        "Perform",
        "Sqoop",
        "Responsibilities Analyze Data",
        "KNN",
        "Hive Pig Pyspark Proficient",
        "MVP",
        "AWS",
        "Technology Banking Manufacturing Healthcare",
        "Text Analytics",
        "Regression Classification etc Strong",
        "Coding",
        "Smoke",
        "Big Data Technologies Hadoop Hive HDFS MapReduce Pig Kafka",
        "Deep",
        "Tableau Desktop Environment AWS",
        "Data Science Machine Learning",
        "Random Forest",
        "SSRS",
        "Credit Risk Scorecards Built",
        "SQL",
        "Santander Group",
        "Customer Churn Prediction Predicted",
        "GBM",
        "NLP",
        "Pandas NumPy",
        "Responsibilities Working",
        "Models Hierarchical",
        "Macintosh",
        "Motor Health Travel Home",
        "SAP Power",
        "Logistic Regression and Nave Bayes",
        "Additional Information TECHNICAL SKILLS Languages",
        "Pandas",
        "Research",
        "ETL",
        "Python Scikitlearn Implemented",
        "Cross Functional",
        "Performed",
        "Credit History",
        "Responsibilities Assembled a Predictive Modeling",
        "Impala",
        "ROI",
        "Bayesian Optimization Write",
        "Forecasting Loan",
        "Python Developed Banks",
        "Pythons",
        "Control M Perfmon",
        "Microsoft",
        "SparkSQL Communicate",
        "Outlier Anomalous Pattern Detection Created",
        "CNN",
        "ML",
        "BoostingRandom Forest Tableau Data",
        "Stanford NLP Sentiment Analysis",
        "TSQL",
        "Data",
        "Performed Data Preparation",
        "Model",
        "MLxtend",
        "Tableau",
        "Operations",
        "SQL Server Management Studio MS BI",
        "anomaly detection",
        "Bharti",
        "Data Warehouses"
    ],
    "experience": "Experience and passion for solving analytical problems involving big data sets using quantitative approaches to generate insights from data Identify analyze and interpret trends or patterns in complex data sets Strong in Predictive and Prescriptive analytics approaches and experienced in operating tools like R and in programming using Python Has working experience in Data Science Machine Learning implementation in cloud platforms like Google Cloud Platform AWS Azure Bluemix Familiar with predictive models using different cloud based Machine learning tools like Microsoft Azure ML Oversee all activities related to data cleansing data quality and data consolidation using industry standards and processes Perform exploratory data analysis generate and test working hypothesis and prepare and analyze historical data and identify patterns Work closely with Cross Functional teams to encourage best practices for experimental design and data analysis Proficient with Python 3x including Numpy Scikitlearn Pandas Matplotlib and Seaborn Extensive experience in RDBMS such as SQL server 2012 MySQL 5x Experienced in Nonrelational database such as MongoDB 3x Experienced in Hadoop 2x ecosystem and Apache Spark 2x framework such as Hive Pig Pyspark Proficient at data visualization tools such as Tableau Power BI Python Matplotlib and Seaborn Experienced in Amazon Web Services AWS and Microsoft Azure such as AWS EC2 S3 RD3 Azure HDInsight Machine Learning Studio Azure Data Lake Apply various data modeling techniques to model user behavior and identify actionable levers for retaining and growing users Define key metrics conduct AB testing and oversee statistical measurement of new algorithms and approaches Expert at distilling questions wrangling data and driving decisions with data analytics Strong knowledge of relational databases and ability to write SQL code at an expert level Aptitude with numbers intellectual curiosity about metrics and measuring impact Present models findings and insights to senior management to catalyze business decisions Ability to solve complex problems by applying analytical techniques and predictive models to massive data sets and translate business needs into mathematical abstractions for algorithms to solve Research and prototype models and pipelines and work with engineers to put them into production at scale Raw data analysis such as assessing quality cleansing structuring for downstream processing Use mathematical statistical and programmatic knowledge to spec out design and build firstclass predictive models about customer behavior Design and prototype of accurate and scalable prediction algorithms Collaboration with engineering team to bring analytical prototypes to production Creative and pragmatic quantitatively minded individual with a passion for understanding location and human behavior Ability to identify issues quickly and rapidly determine root cause and effective resolution approach Very solid data analysis skills including application of analytical techniques such as statistical and machine learning Fundamental coding skills enabling the building of analytical pipelines and the development of prototypes for core company products and systems Excellent verbal and written communication skills ability to communicate technical topics to nontechnical individuals Ability to manage own time and work effectively with others on projects Work Experience Data Scientist CUNA Mutual Group Madison WI April 2017 to Present This project was to support auditing team and claim department to improve accounting accuracy and reduce risk of fraudulent activities via providing machine learning and modeling solutions to identify suspicious insurance claims Claims severity prediction in realtime Built classification models to predict the fraudulent claims by severity in realtime reducing the time for execution from 6 hours to 4 seconds Implemented the models as a predictive solution for finding the fraudulent claims for the credit disability and debt protection products Forwarded the highrisk claims for further investigation Text analytics for fraud prediction Executed topic modelling for finding different topics based on the notes made corresponding to the claimants claim Attributed the resulting topics to classify into fraud and not fraud categories Risk assessment prediction Incorporated models built in Python and R into the business processes using clustering techniques to assess the risk involved with a customer The models built are used in assessing the premiums amount required to be paid by the customer Customer churnattrition prediction Developed models that predict whether a customers propensity to churn leveraging the information related to insurance policies demographics claims related to the customer payment frequency home ownership status household tenure etc Responsibilities Analyze Data and Performed Data Preparation by applying historical model on the data set in AZURE ML Perform Data cleaning process applied Backward Forward filling methods on dataset for handling missing value Perform Data Transformation method for Rescaling and Normalizing Variables Develop a predictive model and validate KNN model for predict the feature label Plan develop and apply leadingedge analytic and quantitative tools and modeling techniques to help clients gain insights and improve decisionmaking Leverage the most appropriate algorithms and be prepared to justify your decisions Work closely with key stakeholders in product finance and operations to form deep understanding of growth and marketplace dynamics including product and pricing patterns outlier detection forecasting and imputation Collaborate with product and engineering to integrate various sources of data Apply strict sampling statistical inference and survey techniques to derive insights from small samples of data Utilize Sqoop to ingest realtime data Used analytics libraries SciKit Learn MLLIB and MLxtend Extensively use Pythons multiple data science packages like Pandas NumPy matplotlib Seaborn SciPy Scikitlearn and NLTK Performed Exploratory Data Analysis trying to find trends and clusters Develop rigorous data science models to aggregate inconsistent realtime signals into strong predictors of market trends Automate and own the endtoend process of modeling and data visualization Collaborate with Data Engineers and Software Developers to develop experiments and deploy solutions to production Work on data that was a combination of unstructured and structured data from multiple sources and automate the cleaning using Python scripts Extensively perform large data readwrites to and from csv and excel files using pandas Tasked with maintaining RDDs using SparkSQL Communicate and coordinate with other departments to collection business requirement Tackle highly imbalanced Fraud dataset using under sampling with ensemble methods oversampling and cost sensitive algorithms Improved fraud prediction performance by using random forest and gradient boosting for feature selection with Python Scikitlearn Implemented machine learning model logistic regression XGboost with Python Scikit learn Optimize algorithm with stochastic gradient descent algorithm Finetuned the algorithm parameter with manual tuning and automated tuning such as Bayesian Optimization Write research reports describing the experiment conducted results and findings and also make strategic recommendations to technology product and senior management Built executive Banco Santander SA 2000 to Present doing business as Santander Group is a Spanish banking group As its name suggests the company originated in Santander Spain The group has expanded since 2000 through a number of acquisitions with operations across Europe South America North America and Asia Santander has been ranked as 37th in the Forbes Global 2000 list of the worlds biggest public companies Santander is Spains largest bank KEY PROJECTS Credit History Predictive Modeling Analyzed and predicted the customers credit history and past bill payments based on the Credit card offers to create a predictive model and send offers to customers on the base of Model and past data A system was successfully created on the past data of Credit History and Payments activity of customers Ran model against the historical data and get predicted label if customers were eligible for credit card offer and on that basis send them an offer As a result customers actually got an offer they liked and increased the number of offer acceptance which lead to profit for the bank Forecasting Loan balance Forecasted bankwide loan balances under normal and stressed macroeconomic scenarios using R Performed variable reduction using the stepwise lasso and elastic net algorithms and tuned the models for accuracy using cross validation and grid search techniques Top down Models Commercial Real Estate Automated the scraping and cleaning of data from various data sources in R and Python Developed Banks loss forecasting process using relevant forecasting and regression algorithms in R The projected losses under stress conditions helped bank reserve enough funds per DFAST policies Loan Payment Default Prediction Built classification models using several features related to customer demographics macroeconomic dynamics historic payment behavior type and size of loan credit scores and loan to value ratios and with accuracy of 95 accuracy the model predicted the likelihood of default under various stressed conditions Marketing Campaign Measurement Built executive dashboards in Tableau that measured changes in customer behavior post campaign launch the ROI measurements helped to strategically select the effective campaigns Credit Risk Scorecards Built credit risk scorecards and marketing response models using SQL and SAS Evangelized the complex technical analysis into easily digestible reports for top executives in the company Developed several interactive dashboards in Tableau to visualize nearly 5 Terabytes of credit data by designing a scalable data cube structure Responsibilities Gathered analyzed documented and translated application requirements into data models supported standardization of documentation and the adoption of standards and practices related to data and applications Queried and aggregated data from Amazon Redshift to get the sample dataset Identified patterns data quality issues and leveraged insights by communicating with BI team In preprocessing phase used Pandas to remove or replace all the missing data and feature engineering to eliminate unrelated features Balanced the dataset with Oversampling the minority label class and Undersampling the majority label class In data exploration stage used correlation analysis and graphical techniques to get some insights about the claim data Tested classification algorithms such as Logistic Regression Gradient Boosting and Random Forest using Pandas and Scikitlearn and evaluated the performance Implemented tuned and tested the model on AWS EC2 with the best algorithm and parameters Set up data preprocessing pipeline to guarantee the consistency between the training data and new coming data Deployed the model on AWS Lambda collaborated with develop team to build the business solutions Collected the feedback after deployment retrained the model to improve the performance Discovered flaws in the methodology being used to calculate weather peril zone relativities designed and implemented a 3D algorithm based on kmeans clustering and Monte Carlo methods Observed groups of customers being neglected by the pricing algorithm used hierarchical clustering to improve customer segmentation and increase profits by 6 Designed developed and maintained daily and monthly summary trending and benchmark reports in Tableau Desktop Environment AWS EC2 S3 Redshift Lambda Linux Python ScikitLearnNumpyPandasMatplotlib Machine Learning Logistic RegressionGradient BoostingRandom Forest Tableau Data Scientist Santander Boston MA March 2016 to April 2017 Data Scientist Banco Santander SA Austin TX March 2014 to December 2015 Whole Foods Market Inc is an American supermarket chain that specializes in selling organic foods products without artificial additive products for growing foods colors flavors sweeteners and hydrogenated fats It has 473 stores in North America and the United Kingdom KEY PROJECTS Customer Purchase Propensity Modelling Built machine learning based regression models using scikitlearn python frameworks to estimate the customer propensity to purchase based on attributes such as customer verticals they operate in revenue historic purchases frequency and regency behaviours These predictions helped estimate propensities with higher accuracy improving the overall productivity of sales teams by accurately targeting the prospective clients Coupon Recommender System Developed a personalized coupon recommender system using recommender algorithms collaborative filtering low rank matrix factorization that recommended best offers to a user based on similar user profiles The recommendations enabled users to engage better and helped improving the overall customer retention rates Outlier Anomalous Pattern Detection Created interactive dashboard suite that illustrated outlier characteristics across several salesrelated dimensions and overall impact of outlier imputation in R Shiny Used iterative outlier detection and imputation algorithm using multiple densitybased clustering techniques DBSCAN kernel density estimation Cross Sell and Upsell Opportunity Analysis Implemented market basket algorithms from transactional data which helped identify coupons usedpurchased together frequently Discovering frequent coupon sets helped unearth cross sell and up selling opportunities and led to better pricing bundling and promotion strategies for sales and marketing teams Forecast Process Innovations Forecast Sales and improved accuracy by 1020 by implementing advanced forecasting algorithms that were effective in detecting seasonality and trends in the patterns in addition to incorporating exogenous covariates Increased accuracy helped business plan better with respect to budgeting and sales and operations planning Price Elasticity Analysis Measured the price elasticity for products that experienced price cuts and promotions using regression methods based on the elasticity Whole Foods made selective and cautious price cuts for certain categories Customer Churn Prediction Predicted the likelihood of customer churn based on customer attributes like customer size RFM loyalty metrics revenue type of industry competitor products and growth rates etc The models deployed in production environment helped detect churn in advance and aided salesmarketing teams plan for various retention strategies in advance like price discounts custom licensing plans etc Responsibilities Working as project technical lead paired with business lead in scoping researching and assessing project feasibility outcomes and product deliverables on knowledge graph project Hadoop ecosystem Spark ElasticSearch JanusGraph etc Working with business lead successfully launched endtoend knowledge graph MVP in 7 months time whereas previous attempts at knowledge graph project had languished for 34 years Using NLP and ML built and maintained a variety of cloudbased natural language web scrapers and parsers to augment existing data sources Python NumPy SciPy SpaCy etc Spark TensorFlow Jupyter notebooks Mentored junior teammates shared knowledge of NLP and information retrieval best practices and performed code reviews on a weekly basis Prototyped conducted and reported on data science experiments to technical and nontechnical audiences using supervised semisupervised and unsupervised learning techniques anomaly detection named entity recognition ontology creation etc Using Python and NLTK aggregated natural language data from online documents and developed pipelines to ingest scraped data into relational databases Performed adhoc data science analyses for departments across the business using R Python Natural Language Toolkit NLTK machine learning AnalystData Scientist Banco Santander SA Hyderabad Telangana April 2012 to March 2014 Bharti AXA General Insurance Company Ltd is a joint venture between Bharti Enterprises a leading Indian business group and AXA a world leader in financial protection Bharti AXAs offers insurance coverage across various categories Motor Health Travel Home student travel and more The objective was to load data analyze and provide monthly reports for the predictions on a claims potential of a thirdparty recovery Tableau and SSRS were used to build claim and recovery reports Responsibilities Assembled a Predictive Modeling module by using supervised learning for Subrogation Claim Prediction to identify which claims would be classified as having Subrogation potential Implemented models such as Logistic Regression and Nave Bayes in Python using scikitlearn to predict the claim potential outcome Dimensionality Reduction techniques applied to refine the attribute lists and feature selection applied to rank selected features to generate accurate results Gathered requirements and business rules from business users to implement Predictive Modeling Designed and developed ETL packages using SSIS to create Data Warehouses from different tables and file sources like Flat and Excel files with different methods in SSIS such as derived columns aggregations Merge joins count conditional split and more to transform the data Designed reporting solutions for different stakeholders from mockup till deployment in different areas such as Potential Subrogation claims Monthly Revenue from Subrogation Transactions Performed data visualization and designed dashboards with Tableau and provided complex reports including charts summaries and graphs to interpret the findings for Adjustors to view various claim information Optimized queries in TSQL by removing unnecessary columns and redundant data normalized tables established joins and indices developed complex SQL queries stored procedures views functions and reports that meet customer requirements Environment Python 3x Scikitlearn Matplotlib Jupyter SQL Server 2012 MS SQL Server Management Studio MS BI Suite SSISSSRS TSQL Visual Studio BIDS Tableau Jr Python Developer Banco Santander SA Hyderabad Telangana June 2010 to April 2012 Responsibilities Worked on development of customer support and complains registration system This is a Customer feedback and complains management system Design develop test deploy and maintain the website Coding and execution of scripts in PythonUnixVB Development of Application using Java and Python Recording of Scripts Web Web Services HTML using Vugen and SoapUI and script validation through co correlations parameterizations and other methods Scripting web and web services Data set up using SQLORACLETeradata Resolving Complexity in the scripts of the website due to the complex logic and correlations Script validation sometimes becomes challenging as it demanded many web based logic rather than correlation and parameterization Running loadendurance tests using Vugen ALM and controller server monitoring analysis using Dynatrace UNIX putty SQL logs and other tools and reporting the performance Analyzing errors and exceptions using putty logs UNIX etc Testing in citrix protocol with scripts and scenario Execution of batch jobs in Control M Perfmon and other tools Scripting and validation of scripts through correlation parameterization and web based logic testing Smoke test Load test Endurance using Controller for a duration further analysis checking response times CPU utilizations memory leaks of servers and other performance characteristics of the website through capturing Perfmon logs and creating PAPAL reports and creating test reports Designed and developed data management system using MySQL Rewrite existing PythonDjangoJava module to deliver certain format of data Used Django Database APIs to access database objects Wrote python scripts to parse XML documents and load the data in database Generated property list for every application dynamically using python Responsible for search engine optimization SEO to improve the visibility of the website Handled all the client side validation using JavaScript Creating unit testregression test framework for workingnew code Using Subversion version control tool to coordinate teamdevelopment Responsible for debugging and troubleshooting the web application Environment Python Putty SQL Teradata SoapUI   PerfMon MySQL Linux HTML XHTML CSS AJAX JavaScript Apache Web Server Education Bachelor of Technology in Computer Science Engineering Sreenidhi Institute of Science Technology Hyderabad Telangana Skills ALGORITHMS 10 years BI 10 years BUSINESS INTELLIGENCE 10 years LINUX 10 years LOGISTIC REGRESSION 10 years Additional Information TECHNICAL SKILLS Languages Java 8 Python R Python and R Numpy SciPy Pandas Scikitlearn Matplotlib Seaborn ggplot2 caret dplyr purrr readxl tidyr Rweka gmodels RCurl C50 twitter NLP Reshape2 rjson plyr Beautiful Soup Rpy2 Algorithms Kernel Density Estimation and Nonparametric Bayes Classifier KMeans Linear Regression Neighbors Nearest Farthest Range k Classification NonNegative Matrix Factorization Dimensionality Reduction Decision Tree Gaussian Processes Logistic Regression Nave Bayes Random Forest Ridge Regression Matrix FactorizationSVD NLPMachine LearningDeep Learning LDA Latent Dirichlet Allocation NLTK Apache OpenNLP Stanford NLP Sentiment Analysis SVMs ANN RNN CNN TensorFlow MXNet Caffe H2O Keras PyTorch Theano Azure ML Cloud Google Cloud Platform AWS Azure Bluemix Web Technologies JDBC HTML5 DHTML and XML CSS3 Web Services WSDL Data Modelling Tools Erwin r 96 95 91 8x Rational Rose ERStudio MS Visio SAP Power designer Big Data Technologies Hadoop Hive HDFS MapReduce Pig Kafka Databases SQL Hive Impala Pig Spark SQL Databases SQLServer My SQL MS Access HDFS HBase Teradata Netezza MongoDB Cassandra Reporting Tools MS Office WordExcelPower Point Visio Tableau Crystal reports XI Business Intelligence SSRS Business Objects 5x 6x Cognos7060 ETL Tools Informatica Power Centre SSIS Version Control Tools SVM GitHub BI Tools Tableau Tableau Server Tableau Reader SAP Business Objects OBIEE QlikView SAP Business Intelligence Amazon Redshift or Azure Data Warehouse Operating System Windows Linux Unix Macintosh HD Red Hat",
    "extracted_keywords": [
        "Data",
        "Scientist",
        "Data",
        "Scientist",
        "Data",
        "Scientist",
        "CUNA",
        "Mutual",
        "Group",
        "years",
        "Data",
        "Science",
        "experience",
        "solutions",
        "Marketing",
        "Sales",
        "Operations",
        "functions",
        "Technology",
        "Banking",
        "Manufacturing",
        "Healthcare",
        "Retail",
        "industries",
        "experience",
        "Text",
        "Analytics",
        "Machine",
        "Learning",
        "Data",
        "Mining",
        "solutions",
        "business",
        "problems",
        "data",
        "visualizations",
        "R",
        "Python",
        "Expertise",
        "business",
        "requirements",
        "models",
        "algorithms",
        "building",
        "models",
        "data",
        "mining",
        "reporting",
        "solutions",
        "volume",
        "data",
        "Expert",
        "knowledge",
        "learning",
        "algorithms",
        "Ensemble",
        "Methods",
        "Random",
        "forests",
        "Logistic",
        "Regression",
        "Linear",
        "Regression",
        "SVMs",
        "Deep",
        "Neural",
        "Networks",
        "Extreme",
        "Gradient",
        "Boosting",
        "Decision",
        "Trees",
        "KMeans",
        "Gaussian",
        "Mixture",
        "Models",
        "models",
        "time",
        "series",
        "models",
        "ARIMA",
        "GARCH",
        "VARCH",
        "Expertise",
        "production",
        "quality",
        "code",
        "SQL",
        "R",
        "Python",
        "Spark",
        "Hands",
        "experience",
        "building",
        "regression",
        "classification",
        "models",
        "learning",
        "algorithms",
        "datasets",
        "systems",
        "resource",
        "environments",
        "models",
        "classification",
        "algorithms",
        "KNN",
        "Naive",
        "base",
        "regression",
        "decision",
        "trees",
        "models",
        "classification",
        "prediction",
        "support",
        "vector",
        "machines",
        "networks",
        "methods",
        "forest",
        "efficiency",
        "model",
        "Text",
        "Mining",
        "Sentimental",
        "analysis",
        "data",
        "Media",
        "platforms",
        "Facebook",
        "Twitter",
        "Reddit",
        "Ability",
        "ideas",
        "R",
        "Python",
        "production",
        "quality",
        "background",
        "handson",
        "Data",
        "Science",
        "Big",
        "Data",
        "data",
        "structures",
        "algorithms",
        "Regression",
        "Classification",
        "background",
        "handson",
        "Supervised",
        "Decision",
        "Trees",
        "Random",
        "Forest",
        "Logistic",
        "Regression",
        "SVMs",
        "GBM",
        "KMeans",
        "KNN",
        "background",
        "handson",
        "Deep",
        "learning",
        "Tensforflow",
        "Keras",
        "Theano",
        "H2o",
        "Sound",
        "understanding",
        "Deep",
        "learning",
        "CNN",
        "RNN",
        "ANN",
        "reinforcement",
        "learning",
        "transfer",
        "background",
        "handson",
        "Natural",
        "Language",
        "Processing",
        "text",
        "analytics",
        "Experience",
        "passion",
        "problems",
        "data",
        "sets",
        "approaches",
        "insights",
        "data",
        "Identify",
        "trends",
        "patterns",
        "data",
        "Predictive",
        "Prescriptive",
        "analytics",
        "approaches",
        "operating",
        "tools",
        "R",
        "programming",
        "Python",
        "experience",
        "Data",
        "Science",
        "Machine",
        "Learning",
        "implementation",
        "cloud",
        "platforms",
        "Google",
        "Cloud",
        "Platform",
        "AWS",
        "Azure",
        "Bluemix",
        "models",
        "cloud",
        "Machine",
        "learning",
        "tools",
        "Microsoft",
        "Azure",
        "ML",
        "Oversee",
        "activities",
        "data",
        "cleansing",
        "data",
        "quality",
        "data",
        "consolidation",
        "industry",
        "standards",
        "processes",
        "data",
        "analysis",
        "generate",
        "working",
        "hypothesis",
        "data",
        "patterns",
        "Cross",
        "Functional",
        "teams",
        "practices",
        "design",
        "data",
        "analysis",
        "Python",
        "Numpy",
        "Scikitlearn",
        "Pandas",
        "Matplotlib",
        "Seaborn",
        "experience",
        "RDBMS",
        "SQL",
        "server",
        "MySQL",
        "5x",
        "database",
        "MongoDB",
        "3x",
        "Hadoop",
        "ecosystem",
        "Apache",
        "Spark",
        "framework",
        "Hive",
        "Pig",
        "Pyspark",
        "Proficient",
        "data",
        "visualization",
        "tools",
        "Tableau",
        "Power",
        "BI",
        "Python",
        "Matplotlib",
        "Seaborn",
        "Experienced",
        "Amazon",
        "Web",
        "Services",
        "AWS",
        "Microsoft",
        "Azure",
        "AWS",
        "EC2",
        "S3",
        "RD3",
        "Azure",
        "HDInsight",
        "Machine",
        "Learning",
        "Studio",
        "Azure",
        "Data",
        "Lake",
        "data",
        "techniques",
        "user",
        "behavior",
        "levers",
        "users",
        "metrics",
        "AB",
        "testing",
        "measurement",
        "algorithms",
        "Expert",
        "questions",
        "data",
        "decisions",
        "data",
        "analytics",
        "knowledge",
        "databases",
        "ability",
        "SQL",
        "code",
        "expert",
        "level",
        "Aptitude",
        "numbers",
        "curiosity",
        "metrics",
        "impact",
        "models",
        "findings",
        "insights",
        "management",
        "business",
        "decisions",
        "Ability",
        "problems",
        "techniques",
        "models",
        "data",
        "sets",
        "business",
        "needs",
        "abstractions",
        "algorithms",
        "Research",
        "prototype",
        "models",
        "pipelines",
        "work",
        "engineers",
        "production",
        "scale",
        "data",
        "analysis",
        "quality",
        "cleansing",
        "downstream",
        "Use",
        "knowledge",
        "design",
        "models",
        "customer",
        "behavior",
        "Design",
        "prototype",
        "prediction",
        "Collaboration",
        "engineering",
        "team",
        "prototypes",
        "individual",
        "passion",
        "location",
        "behavior",
        "Ability",
        "issues",
        "root",
        "cause",
        "resolution",
        "data",
        "analysis",
        "skills",
        "application",
        "techniques",
        "machine",
        "Fundamental",
        "skills",
        "building",
        "pipelines",
        "development",
        "prototypes",
        "core",
        "company",
        "products",
        "systems",
        "Excellent",
        "communication",
        "skills",
        "ability",
        "topics",
        "individuals",
        "Ability",
        "time",
        "others",
        "projects",
        "Work",
        "Experience",
        "Data",
        "Scientist",
        "CUNA",
        "Mutual",
        "Group",
        "Madison",
        "WI",
        "April",
        "Present",
        "project",
        "auditing",
        "team",
        "department",
        "accounting",
        "accuracy",
        "risk",
        "activities",
        "machine",
        "learning",
        "modeling",
        "solutions",
        "insurance",
        "claims",
        "Claims",
        "severity",
        "prediction",
        "classification",
        "models",
        "claims",
        "severity",
        "realtime",
        "time",
        "execution",
        "hours",
        "seconds",
        "models",
        "solution",
        "claims",
        "credit",
        "disability",
        "debt",
        "protection",
        "products",
        "highrisk",
        "claims",
        "investigation",
        "Text",
        "analytics",
        "fraud",
        "prediction",
        "topic",
        "topics",
        "notes",
        "claimants",
        "topics",
        "fraud",
        "fraud",
        "categories",
        "Risk",
        "assessment",
        "prediction",
        "models",
        "Python",
        "R",
        "business",
        "processes",
        "techniques",
        "risk",
        "customer",
        "models",
        "premiums",
        "amount",
        "customer",
        "Customer",
        "churnattrition",
        "prediction",
        "models",
        "customers",
        "propensity",
        "information",
        "insurance",
        "policies",
        "demographics",
        "claims",
        "customer",
        "payment",
        "frequency",
        "home",
        "ownership",
        "status",
        "household",
        "tenure",
        "Responsibilities",
        "Analyze",
        "Data",
        "Performed",
        "Data",
        "Preparation",
        "model",
        "data",
        "AZURE",
        "ML",
        "Perform",
        "Data",
        "cleaning",
        "process",
        "Backward",
        "Forward",
        "methods",
        "dataset",
        "value",
        "Perform",
        "Data",
        "Transformation",
        "method",
        "Rescaling",
        "Normalizing",
        "Variables",
        "model",
        "KNN",
        "model",
        "feature",
        "label",
        "Plan",
        "leadingedge",
        "tools",
        "techniques",
        "clients",
        "insights",
        "Leverage",
        "algorithms",
        "decisions",
        "stakeholders",
        "product",
        "finance",
        "operations",
        "understanding",
        "growth",
        "marketplace",
        "dynamics",
        "product",
        "pricing",
        "patterns",
        "detection",
        "forecasting",
        "imputation",
        "Collaborate",
        "product",
        "engineering",
        "sources",
        "data",
        "inference",
        "techniques",
        "insights",
        "samples",
        "data",
        "Utilize",
        "Sqoop",
        "data",
        "analytics",
        "SciKit",
        "MLLIB",
        "MLxtend",
        "Pythons",
        "data",
        "science",
        "packages",
        "Pandas",
        "NumPy",
        "matplotlib",
        "Seaborn",
        "SciPy",
        "Scikitlearn",
        "NLTK",
        "Performed",
        "Exploratory",
        "Data",
        "Analysis",
        "trends",
        "clusters",
        "data",
        "science",
        "models",
        "signals",
        "predictors",
        "market",
        "trends",
        "endtoend",
        "process",
        "modeling",
        "data",
        "visualization",
        "Collaborate",
        "Data",
        "Engineers",
        "Software",
        "Developers",
        "experiments",
        "solutions",
        "production",
        "Work",
        "data",
        "combination",
        "data",
        "sources",
        "cleaning",
        "Python",
        "scripts",
        "data",
        "csv",
        "files",
        "pandas",
        "RDDs",
        "SparkSQL",
        "Communicate",
        "departments",
        "collection",
        "business",
        "requirement",
        "Tackle",
        "Fraud",
        "dataset",
        "methods",
        "algorithms",
        "fraud",
        "prediction",
        "performance",
        "forest",
        "gradient",
        "feature",
        "selection",
        "Python",
        "Scikitlearn",
        "machine",
        "model",
        "regression",
        "XGboost",
        "Python",
        "Scikit",
        "Optimize",
        "algorithm",
        "descent",
        "algorithm",
        "algorithm",
        "parameter",
        "tuning",
        "tuning",
        "Bayesian",
        "Optimization",
        "Write",
        "research",
        "reports",
        "experiment",
        "results",
        "findings",
        "recommendations",
        "technology",
        "product",
        "management",
        "executive",
        "Banco",
        "Santander",
        "SA",
        "Present",
        "business",
        "Santander",
        "Group",
        "banking",
        "group",
        "name",
        "company",
        "Santander",
        "Spain",
        "group",
        "number",
        "acquisitions",
        "operations",
        "Europe",
        "South",
        "America",
        "North",
        "America",
        "Asia",
        "Santander",
        "37th",
        "Forbes",
        "Global",
        "list",
        "worlds",
        "companies",
        "Santander",
        "Spains",
        "bank",
        "KEY",
        "PROJECTS",
        "Credit",
        "History",
        "Predictive",
        "Modeling",
        "Analyzed",
        "customers",
        "credit",
        "history",
        "bill",
        "payments",
        "Credit",
        "card",
        "model",
        "offers",
        "customers",
        "base",
        "Model",
        "data",
        "system",
        "data",
        "Credit",
        "History",
        "Payments",
        "activity",
        "customers",
        "model",
        "data",
        "label",
        "customers",
        "credit",
        "card",
        "offer",
        "basis",
        "offer",
        "result",
        "customers",
        "offer",
        "number",
        "offer",
        "acceptance",
        "profit",
        "bank",
        "Forecasting",
        "Loan",
        "balance",
        "bankwide",
        "loan",
        "balances",
        "scenarios",
        "R",
        "reduction",
        "lasso",
        "algorithms",
        "models",
        "accuracy",
        "cross",
        "validation",
        "grid",
        "search",
        "techniques",
        "Models",
        "Commercial",
        "Real",
        "Estate",
        "scraping",
        "cleaning",
        "data",
        "data",
        "sources",
        "R",
        "Python",
        "Developed",
        "Banks",
        "loss",
        "forecasting",
        "process",
        "forecasting",
        "regression",
        "algorithms",
        "R",
        "losses",
        "stress",
        "conditions",
        "bank",
        "funds",
        "DFAST",
        "policies",
        "Loan",
        "Payment",
        "Default",
        "Prediction",
        "classification",
        "models",
        "features",
        "customer",
        "demographics",
        "macroeconomic",
        "payment",
        "behavior",
        "type",
        "size",
        "loan",
        "credit",
        "scores",
        "loan",
        "ratios",
        "accuracy",
        "accuracy",
        "model",
        "likelihood",
        "default",
        "conditions",
        "Marketing",
        "Campaign",
        "Measurement",
        "dashboards",
        "Tableau",
        "changes",
        "customer",
        "behavior",
        "post",
        "campaign",
        "launch",
        "ROI",
        "measurements",
        "campaigns",
        "Credit",
        "Risk",
        "Scorecards",
        "credit",
        "risk",
        "scorecards",
        "marketing",
        "response",
        "models",
        "SQL",
        "SAS",
        "analysis",
        "reports",
        "executives",
        "company",
        "dashboards",
        "Tableau",
        "Terabytes",
        "credit",
        "data",
        "data",
        "cube",
        "structure",
        "Responsibilities",
        "Gathered",
        "application",
        "requirements",
        "data",
        "models",
        "standardization",
        "documentation",
        "adoption",
        "standards",
        "practices",
        "data",
        "applications",
        "data",
        "Amazon",
        "Redshift",
        "sample",
        "patterns",
        "data",
        "quality",
        "issues",
        "insights",
        "BI",
        "team",
        "phase",
        "Pandas",
        "data",
        "feature",
        "engineering",
        "features",
        "dataset",
        "minority",
        "label",
        "class",
        "majority",
        "label",
        "class",
        "data",
        "exploration",
        "stage",
        "correlation",
        "analysis",
        "techniques",
        "insights",
        "claim",
        "data",
        "classification",
        "algorithms",
        "Logistic",
        "Regression",
        "Gradient",
        "Boosting",
        "Random",
        "Forest",
        "Pandas",
        "Scikitlearn",
        "performance",
        "model",
        "AWS",
        "EC2",
        "algorithm",
        "parameters",
        "data",
        "pipeline",
        "consistency",
        "training",
        "data",
        "data",
        "model",
        "AWS",
        "Lambda",
        "team",
        "business",
        "solutions",
        "feedback",
        "deployment",
        "model",
        "performance",
        "flaws",
        "methodology",
        "weather",
        "peril",
        "zone",
        "relativities",
        "algorithm",
        "kmeans",
        "Monte",
        "Carlo",
        "methods",
        "groups",
        "customers",
        "pricing",
        "algorithm",
        "clustering",
        "customer",
        "segmentation",
        "profits",
        "summary",
        "trending",
        "reports",
        "Tableau",
        "Desktop",
        "Environment",
        "EC2",
        "S3",
        "Redshift",
        "Lambda",
        "Linux",
        "Python",
        "Machine",
        "Learning",
        "Logistic",
        "RegressionGradient",
        "BoostingRandom",
        "Forest",
        "Tableau",
        "Data",
        "Scientist",
        "Santander",
        "Boston",
        "MA",
        "March",
        "April",
        "Data",
        "Scientist",
        "Banco",
        "Santander",
        "SA",
        "Austin",
        "TX",
        "March",
        "December",
        "Whole",
        "Foods",
        "Market",
        "Inc",
        "supermarket",
        "chain",
        "foods",
        "products",
        "products",
        "foods",
        "colors",
        "flavors",
        "sweeteners",
        "fats",
        "stores",
        "North",
        "America",
        "United",
        "Kingdom",
        "KEY",
        "PROJECTS",
        "Customer",
        "Purchase",
        "Propensity",
        "machine",
        "regression",
        "models",
        "frameworks",
        "customer",
        "propensity",
        "attributes",
        "customer",
        "verticals",
        "revenue",
        "purchases",
        "frequency",
        "regency",
        "predictions",
        "propensities",
        "accuracy",
        "productivity",
        "sales",
        "teams",
        "clients",
        "Coupon",
        "Recommender",
        "System",
        "coupon",
        "recommender",
        "system",
        "recommender",
        "algorithms",
        "filtering",
        "rank",
        "matrix",
        "factorization",
        "offers",
        "user",
        "user",
        "profiles",
        "recommendations",
        "users",
        "customer",
        "retention",
        "rates",
        "Outlier",
        "Anomalous",
        "Pattern",
        "Detection",
        "dashboard",
        "suite",
        "outlier",
        "characteristics",
        "dimensions",
        "impact",
        "imputation",
        "R",
        "Shiny",
        "outlier",
        "detection",
        "imputation",
        "algorithm",
        "techniques",
        "DBSCAN",
        "kernel",
        "density",
        "estimation",
        "Cross",
        "Sell",
        "Upsell",
        "Opportunity",
        "Analysis",
        "market",
        "basket",
        "algorithms",
        "data",
        "coupons",
        "coupon",
        "sets",
        "opportunities",
        "pricing",
        "bundling",
        "promotion",
        "strategies",
        "sales",
        "marketing",
        "teams",
        "Forecast",
        "Process",
        "Innovations",
        "Forecast",
        "Sales",
        "accuracy",
        "forecasting",
        "algorithms",
        "seasonality",
        "trends",
        "patterns",
        "addition",
        "covariates",
        "accuracy",
        "business",
        "plan",
        "respect",
        "budgeting",
        "sales",
        "operations",
        "Price",
        "Elasticity",
        "Analysis",
        "price",
        "elasticity",
        "products",
        "price",
        "cuts",
        "promotions",
        "regression",
        "methods",
        "elasticity",
        "Whole",
        "Foods",
        "price",
        "cuts",
        "categories",
        "Customer",
        "Churn",
        "Prediction",
        "likelihood",
        "customer",
        "churn",
        "customer",
        "attributes",
        "customer",
        "size",
        "RFM",
        "loyalty",
        "metrics",
        "revenue",
        "type",
        "industry",
        "competitor",
        "products",
        "growth",
        "rates",
        "models",
        "production",
        "environment",
        "churn",
        "advance",
        "teams",
        "retention",
        "strategies",
        "advance",
        "price",
        "discounts",
        "custom",
        "licensing",
        "plans",
        "Responsibilities",
        "project",
        "lead",
        "business",
        "lead",
        "project",
        "feasibility",
        "outcomes",
        "product",
        "deliverables",
        "knowledge",
        "graph",
        "project",
        "Hadoop",
        "ecosystem",
        "Spark",
        "ElasticSearch",
        "JanusGraph",
        "business",
        "lead",
        "knowledge",
        "graph",
        "MVP",
        "months",
        "time",
        "attempts",
        "knowledge",
        "graph",
        "project",
        "years",
        "NLP",
        "ML",
        "variety",
        "language",
        "web",
        "scrapers",
        "parsers",
        "data",
        "sources",
        "Python",
        "NumPy",
        "SciPy",
        "SpaCy",
        "Spark",
        "TensorFlow",
        "Jupyter",
        "Mentored",
        "teammates",
        "knowledge",
        "NLP",
        "information",
        "retrieval",
        "practices",
        "code",
        "reviews",
        "basis",
        "Prototyped",
        "data",
        "science",
        "experiments",
        "audiences",
        "techniques",
        "anomaly",
        "detection",
        "entity",
        "recognition",
        "ontology",
        "creation",
        "Python",
        "NLTK",
        "language",
        "data",
        "documents",
        "pipelines",
        "data",
        "databases",
        "data",
        "science",
        "analyses",
        "departments",
        "business",
        "R",
        "Python",
        "Natural",
        "Language",
        "Toolkit",
        "NLTK",
        "machine",
        "AnalystData",
        "Scientist",
        "Banco",
        "Santander",
        "SA",
        "Hyderabad",
        "Telangana",
        "April",
        "March",
        "Bharti",
        "AXA",
        "General",
        "Insurance",
        "Company",
        "Ltd",
        "venture",
        "Bharti",
        "Enterprises",
        "business",
        "group",
        "AXA",
        "world",
        "leader",
        "protection",
        "Bharti",
        "AXAs",
        "insurance",
        "coverage",
        "categories",
        "Motor",
        "Health",
        "Travel",
        "Home",
        "student",
        "travel",
        "objective",
        "data",
        "reports",
        "predictions",
        "claims",
        "potential",
        "thirdparty",
        "recovery",
        "Tableau",
        "SSRS",
        "claim",
        "recovery",
        "reports",
        "Responsibilities",
        "Predictive",
        "Modeling",
        "module",
        "learning",
        "Subrogation",
        "Claim",
        "Prediction",
        "claims",
        "Subrogation",
        "models",
        "Logistic",
        "Regression",
        "Nave",
        "Bayes",
        "Python",
        "scikitlearn",
        "claim",
        "outcome",
        "Dimensionality",
        "Reduction",
        "techniques",
        "attribute",
        "lists",
        "feature",
        "selection",
        "features",
        "results",
        "Gathered",
        "requirements",
        "business",
        "rules",
        "business",
        "users",
        "Predictive",
        "Modeling",
        "ETL",
        "packages",
        "SSIS",
        "Data",
        "Warehouses",
        "tables",
        "file",
        "sources",
        "Excel",
        "files",
        "methods",
        "SSIS",
        "columns",
        "aggregations",
        "Merge",
        "split",
        "data",
        "reporting",
        "solutions",
        "stakeholders",
        "mockup",
        "deployment",
        "areas",
        "Potential",
        "Subrogation",
        "Monthly",
        "Revenue",
        "Subrogation",
        "Transactions",
        "data",
        "visualization",
        "dashboards",
        "Tableau",
        "reports",
        "charts",
        "summaries",
        "graphs",
        "findings",
        "Adjustors",
        "claim",
        "information",
        "queries",
        "TSQL",
        "columns",
        "data",
        "tables",
        "joins",
        "indices",
        "SQL",
        "queries",
        "procedures",
        "views",
        "functions",
        "reports",
        "customer",
        "requirements",
        "Environment",
        "Python",
        "Scikitlearn",
        "Matplotlib",
        "Jupyter",
        "SQL",
        "Server",
        "MS",
        "SQL",
        "Server",
        "Management",
        "Studio",
        "MS",
        "BI",
        "Suite",
        "SSISSSRS",
        "TSQL",
        "Visual",
        "Studio",
        "BIDS",
        "Tableau",
        "Jr",
        "Python",
        "Developer",
        "Banco",
        "Santander",
        "SA",
        "Hyderabad",
        "Telangana",
        "June",
        "April",
        "Responsibilities",
        "development",
        "customer",
        "support",
        "registration",
        "system",
        "Customer",
        "feedback",
        "management",
        "system",
        "Design",
        "test",
        "deploy",
        "website",
        "Coding",
        "execution",
        "scripts",
        "PythonUnixVB",
        "Development",
        "Application",
        "Java",
        "Python",
        "Recording",
        "Scripts",
        "Web",
        "Web",
        "Services",
        "HTML",
        "Vugen",
        "SoapUI",
        "script",
        "validation",
        "co",
        "correlations",
        "parameterizations",
        "methods",
        "Scripting",
        "web",
        "web",
        "services",
        "Data",
        "SQLORACLETeradata",
        "Resolving",
        "Complexity",
        "scripts",
        "website",
        "logic",
        "correlations",
        "Script",
        "validation",
        "web",
        "logic",
        "correlation",
        "parameterization",
        "loadendurance",
        "tests",
        "Vugen",
        "ALM",
        "controller",
        "server",
        "monitoring",
        "analysis",
        "Dynatrace",
        "UNIX",
        "putty",
        "SQL",
        "logs",
        "tools",
        "performance",
        "errors",
        "exceptions",
        "putty",
        "logs",
        "UNIX",
        "Testing",
        "citrix",
        "protocol",
        "scripts",
        "scenario",
        "Execution",
        "batch",
        "jobs",
        "Control",
        "M",
        "Perfmon",
        "tools",
        "Scripting",
        "validation",
        "scripts",
        "correlation",
        "parameterization",
        "web",
        "logic",
        "testing",
        "Smoke",
        "test",
        "Load",
        "test",
        "Endurance",
        "Controller",
        "duration",
        "analysis",
        "response",
        "times",
        "CPU",
        "utilizations",
        "memory",
        "leaks",
        "servers",
        "performance",
        "characteristics",
        "website",
        "Perfmon",
        "logs",
        "PAPAL",
        "reports",
        "test",
        "reports",
        "data",
        "management",
        "system",
        "MySQL",
        "Rewrite",
        "module",
        "format",
        "data",
        "Django",
        "Database",
        "APIs",
        "database",
        "Wrote",
        "python",
        "scripts",
        "XML",
        "documents",
        "data",
        "database",
        "property",
        "list",
        "application",
        "python",
        "search",
        "engine",
        "optimization",
        "SEO",
        "visibility",
        "website",
        "client",
        "side",
        "validation",
        "JavaScript",
        "Creating",
        "unit",
        "testregression",
        "test",
        "framework",
        "code",
        "Subversion",
        "version",
        "control",
        "tool",
        "teamdevelopment",
        "web",
        "application",
        "Environment",
        "Python",
        "Putty",
        "SQL",
        "Teradata",
        "SoapUI",
        "PerfMon",
        "MySQL",
        "Linux",
        "HTML",
        "XHTML",
        "CSS",
        "AJAX",
        "JavaScript",
        "Apache",
        "Web",
        "Server",
        "Education",
        "Bachelor",
        "Technology",
        "Computer",
        "Science",
        "Engineering",
        "Sreenidhi",
        "Institute",
        "Science",
        "Technology",
        "Hyderabad",
        "Telangana",
        "Skills",
        "ALGORITHMS",
        "years",
        "BI",
        "years",
        "BUSINESS",
        "INTELLIGENCE",
        "years",
        "LINUX",
        "years",
        "LOGISTIC",
        "REGRESSION",
        "years",
        "Additional",
        "Information",
        "TECHNICAL",
        "SKILLS",
        "Languages",
        "Java",
        "Python",
        "R",
        "Python",
        "R",
        "Numpy",
        "SciPy",
        "Pandas",
        "Scikitlearn",
        "Matplotlib",
        "Seaborn",
        "ggplot2",
        "dplyr",
        "purrr",
        "readxl",
        "tidyr",
        "Rweka",
        "RCurl",
        "C50",
        "twitter",
        "NLP",
        "Reshape2",
        "rjson",
        "plyr",
        "Beautiful",
        "Soup",
        "Rpy2",
        "Algorithms",
        "Kernel",
        "Density",
        "Estimation",
        "Nonparametric",
        "Bayes",
        "Classifier",
        "KMeans",
        "Linear",
        "Regression",
        "Neighbors",
        "Nearest",
        "Farthest",
        "Range",
        "k",
        "Classification",
        "NonNegative",
        "Matrix",
        "Factorization",
        "Dimensionality",
        "Reduction",
        "Decision",
        "Tree",
        "Gaussian",
        "Processes",
        "Logistic",
        "Regression",
        "Nave",
        "Bayes",
        "Random",
        "Forest",
        "Ridge",
        "Regression",
        "Matrix",
        "FactorizationSVD",
        "NLPMachine",
        "LearningDeep",
        "Learning",
        "LDA",
        "Latent",
        "Dirichlet",
        "Allocation",
        "NLTK",
        "Apache",
        "OpenNLP",
        "Stanford",
        "NLP",
        "Sentiment",
        "Analysis",
        "SVMs",
        "ANN",
        "RNN",
        "CNN",
        "TensorFlow",
        "MXNet",
        "Caffe",
        "H2O",
        "Keras",
        "PyTorch",
        "Theano",
        "Azure",
        "ML",
        "Cloud",
        "Google",
        "Cloud",
        "Platform",
        "AWS",
        "Azure",
        "Bluemix",
        "Web",
        "Technologies",
        "JDBC",
        "HTML5",
        "DHTML",
        "XML",
        "CSS3",
        "Web",
        "Services",
        "WSDL",
        "Data",
        "Modelling",
        "Tools",
        "Erwin",
        "r",
        "8x",
        "Rational",
        "Rose",
        "ERStudio",
        "MS",
        "Visio",
        "SAP",
        "Power",
        "designer",
        "Big",
        "Data",
        "Technologies",
        "Hadoop",
        "Hive",
        "HDFS",
        "MapReduce",
        "Pig",
        "Kafka",
        "SQL",
        "Hive",
        "Impala",
        "Pig",
        "Spark",
        "SQL",
        "Databases",
        "SQLServer",
        "SQL",
        "MS",
        "Access",
        "HDFS",
        "HBase",
        "Teradata",
        "Netezza",
        "MongoDB",
        "Cassandra",
        "Reporting",
        "Tools",
        "MS",
        "Office",
        "WordExcelPower",
        "Point",
        "Visio",
        "Tableau",
        "Crystal",
        "XI",
        "Business",
        "Intelligence",
        "SSRS",
        "Business",
        "5x",
        "Cognos7060",
        "ETL",
        "Tools",
        "Informatica",
        "Power",
        "Centre",
        "SSIS",
        "Version",
        "Control",
        "Tools",
        "SVM",
        "GitHub",
        "BI",
        "Tools",
        "Tableau",
        "Tableau",
        "Server",
        "Tableau",
        "Reader",
        "SAP",
        "Business",
        "OBIEE",
        "QlikView",
        "SAP",
        "Business",
        "Intelligence",
        "Amazon",
        "Redshift",
        "Azure",
        "Data",
        "Warehouse",
        "Operating",
        "System",
        "Windows",
        "Linux",
        "Unix",
        "Macintosh",
        "HD",
        "Red",
        "Hat"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T21:29:17.527140",
    "resume_data": "Data Scientist Data Scientist Data Scientist CUNA Mutual Group 7 years of Data Science experience in architecting and building comprehensive analytical solutions in Marketing Sales and Operations functions across Technology Banking Manufacturing Healthcare and Retail industries Extensive experience in Text Analytics developing different Statistical Machine Learning Data Mining solutions to various business problems and generating data visualizations using R Python Expertise in transforming business requirements into analytical models designing algorithms building models developing data mining and reporting solutions that scale across a massive volume of structured and unstructured data Expert knowledge in supervised and unsupervised learning algorithms such as Ensemble Methods Random forests Logistic Regression Regularized Linear Regression SVMs Deep Neural Networks Extreme Gradient Boosting Decision Trees KMeans Gaussian Mixture Models Hierarchical models and time series models ARIMA GARCH VARCH etc Expertise writing production quality code in SQL R Python and Spark Hands on experience building regression and classification models and other unsupervised learning algorithms with large datasets in distributed systems and resource constrained environments Familiar with predictive models using classification algorithms like KNN Naive base regression and decision trees Familiar with predictive models using numeric and classification prediction algorithms like support vector machines and neural networks and ensemble methods like bagging boosting and random forest to improve the efficiency of the predictive model Worked on Text Mining and Sentimental analysis for extracting the unstructured data from various social Media platforms like Facebook Twitter and Reddit Ability to translate analytic ideas into R Python production quality scripts Strong background handson in Data Science Big Data data structures statistics algorithms like Regression Classification etc Strong background handson of Supervised learning Decision Trees Random Forest Logistic Regression SVMs GBM etc and unsupervised learning KMeans KNN Strong background handson of Deep learning using Tensforflow Keras Theano H2o Sound understanding of Deep learning using CNN RNN ANN reinforcement learning transfer learning Strong background handson in Natural Language Processing and text analytics Experience and passion for solving analytical problems involving big data sets using quantitative approaches to generate insights from data Identify analyze and interpret trends or patterns in complex data sets Strong in Predictive and Prescriptive analytics approaches and experienced in operating tools like R and in programming using Python Has working experience in Data Science Machine Learning implementation in cloud platforms like Google Cloud Platform AWS Azure Bluemix Familiar with predictive models using different cloud based Machine learning tools like Microsoft Azure ML Oversee all activities related to data cleansing data quality and data consolidation using industry standards and processes Perform exploratory data analysis generate and test working hypothesis and prepare and analyze historical data and identify patterns Work closely with Cross Functional teams to encourage best practices for experimental design and data analysis Proficient with Python 3x including Numpy Scikitlearn Pandas Matplotlib and Seaborn Extensive experience in RDBMS such as SQL server 2012 MySQL 5x Experienced in Nonrelational database such as MongoDB 3x Experienced in Hadoop 2x ecosystem and Apache Spark 2x framework such as Hive Pig Pyspark Proficient at data visualization tools such as Tableau Power BI Python Matplotlib and Seaborn Experienced in Amazon Web Services AWS and Microsoft Azure such as AWS EC2 S3 RD3 Azure HDInsight Machine Learning Studio Azure Data Lake Apply various data modeling techniques to model user behavior and identify actionable levers for retaining and growing users Define key metrics conduct AB testing and oversee statistical measurement of new algorithms and approaches Expert at distilling questions wrangling data and driving decisions with data analytics Strong knowledge of relational databases and ability to write SQL code at an expert level Aptitude with numbers intellectual curiosity about metrics and measuring impact Present models findings and insights to senior management to catalyze business decisions Ability to solve complex problems by applying analytical techniques and predictive models to massive data sets and translate business needs into mathematical abstractions for algorithms to solve Research and prototype models and pipelines and work with engineers to put them into production at scale Raw data analysis such as assessing quality cleansing structuring for downstream processing Use mathematical statistical and programmatic knowledge to spec out design and build firstclass predictive models about customer behavior Design and prototype of accurate and scalable prediction algorithms Collaboration with engineering team to bring analytical prototypes to production Creative and pragmatic quantitatively minded individual with a passion for understanding location and human behavior Ability to identify issues quickly and rapidly determine root cause and effective resolution approach Very solid data analysis skills including application of analytical techniques such as statistical and machine learning Fundamental coding skills enabling the building of analytical pipelines and the development of prototypes for core company products and systems Excellent verbal and written communication skills ability to communicate technical topics to nontechnical individuals Ability to manage own time and work effectively with others on projects Work Experience Data Scientist CUNA Mutual Group Madison WI April 2017 to Present This project was to support auditing team and claim department to improve accounting accuracy and reduce risk of fraudulent activities via providing machine learning and modeling solutions to identify suspicious insurance claims Claims severity prediction in realtime Built classification models to predict the fraudulent claims by severity in realtime reducing the time for execution from 6 hours to 4 seconds Implemented the models as a predictive solution for finding the fraudulent claims for the credit disability and debt protection products Forwarded the highrisk claims for further investigation Text analytics for fraud prediction Executed topic modelling for finding different topics based on the notes made corresponding to the claimants claim Attributed the resulting topics to classify into fraud and not fraud categories Risk assessment prediction Incorporated models built in Python and R into the business processes using clustering techniques to assess the risk involved with a customer The models built are used in assessing the premiums amount required to be paid by the customer Customer churnattrition prediction Developed models that predict whether a customers propensity to churn leveraging the information related to insurance policies demographics claims related to the customer payment frequency home ownership status household tenure etc Responsibilities Analyze Data and Performed Data Preparation by applying historical model on the data set in AZURE ML Perform Data cleaning process applied Backward Forward filling methods on dataset for handling missing value Perform Data Transformation method for Rescaling and Normalizing Variables Develop a predictive model and validate KNN model for predict the feature label Plan develop and apply leadingedge analytic and quantitative tools and modeling techniques to help clients gain insights and improve decisionmaking Leverage the most appropriate algorithms and be prepared to justify your decisions Work closely with key stakeholders in product finance and operations to form deep understanding of growth and marketplace dynamics including product and pricing patterns outlier detection forecasting and imputation Collaborate with product and engineering to integrate various sources of data Apply strict sampling statistical inference and survey techniques to derive insights from small samples of data Utilize Sqoop to ingest realtime data Used analytics libraries SciKit Learn MLLIB and MLxtend Extensively use Pythons multiple data science packages like Pandas NumPy matplotlib Seaborn SciPy Scikitlearn and NLTK Performed Exploratory Data Analysis trying to find trends and clusters Develop rigorous data science models to aggregate inconsistent realtime signals into strong predictors of market trends Automate and own the endtoend process of modeling and data visualization Collaborate with Data Engineers and Software Developers to develop experiments and deploy solutions to production Work on data that was a combination of unstructured and structured data from multiple sources and automate the cleaning using Python scripts Extensively perform large data readwrites to and from csv and excel files using pandas Tasked with maintaining RDDs using SparkSQL Communicate and coordinate with other departments to collection business requirement Tackle highly imbalanced Fraud dataset using under sampling with ensemble methods oversampling and cost sensitive algorithms Improved fraud prediction performance by using random forest and gradient boosting for feature selection with Python Scikitlearn Implemented machine learning model logistic regression XGboost with Python Scikit learn Optimize algorithm with stochastic gradient descent algorithm Finetuned the algorithm parameter with manual tuning and automated tuning such as Bayesian Optimization Write research reports describing the experiment conducted results and findings and also make strategic recommendations to technology product and senior management Built executive Banco Santander SA 2000 to Present doing business as Santander Group is a Spanish banking group As its name suggests the company originated in Santander Spain The group has expanded since 2000 through a number of acquisitions with operations across Europe South America North America and Asia Santander has been ranked as 37th in the Forbes Global 2000 list of the worlds biggest public companies Santander is Spains largest bank KEY PROJECTS Credit History Predictive Modeling Analyzed and predicted the customers credit history and past bill payments based on the Credit card offers to create a predictive model and send offers to customers on the base of Model and past data A system was successfully created on the past data of Credit History and Payments activity of customers Ran model against the historical data and get predicted label if customers were eligible for credit card offer and on that basis send them an offer As a result customers actually got an offer they liked and increased the number of offer acceptance which lead to profit for the bank Forecasting Loan balance Forecasted bankwide loan balances under normal and stressed macroeconomic scenarios using R Performed variable reduction using the stepwise lasso and elastic net algorithms and tuned the models for accuracy using cross validation and grid search techniques Top down Models Commercial Real Estate Automated the scraping and cleaning of data from various data sources in R and Python Developed Banks loss forecasting process using relevant forecasting and regression algorithms in R The projected losses under stress conditions helped bank reserve enough funds per DFAST policies Loan Payment Default Prediction Built classification models using several features related to customer demographics macroeconomic dynamics historic payment behavior type and size of loan credit scores and loan to value ratios and with accuracy of 95 accuracy the model predicted the likelihood of default under various stressed conditions Marketing Campaign Measurement Built executive dashboards in Tableau that measured changes in customer behavior post campaign launch the ROI measurements helped to strategically select the effective campaigns Credit Risk Scorecards Built credit risk scorecards and marketing response models using SQL and SAS Evangelized the complex technical analysis into easily digestible reports for top executives in the company Developed several interactive dashboards in Tableau to visualize nearly 5 Terabytes of credit data by designing a scalable data cube structure Responsibilities Gathered analyzed documented and translated application requirements into data models supported standardization of documentation and the adoption of standards and practices related to data and applications Queried and aggregated data from Amazon Redshift to get the sample dataset Identified patterns data quality issues and leveraged insights by communicating with BI team In preprocessing phase used Pandas to remove or replace all the missing data and feature engineering to eliminate unrelated features Balanced the dataset with Oversampling the minority label class and Undersampling the majority label class In data exploration stage used correlation analysis and graphical techniques to get some insights about the claim data Tested classification algorithms such as Logistic Regression Gradient Boosting and Random Forest using Pandas and Scikitlearn and evaluated the performance Implemented tuned and tested the model on AWS EC2 with the best algorithm and parameters Set up data preprocessing pipeline to guarantee the consistency between the training data and new coming data Deployed the model on AWS Lambda collaborated with develop team to build the business solutions Collected the feedback after deployment retrained the model to improve the performance Discovered flaws in the methodology being used to calculate weather peril zone relativities designed and implemented a 3D algorithm based on kmeans clustering and Monte Carlo methods Observed groups of customers being neglected by the pricing algorithm used hierarchical clustering to improve customer segmentation and increase profits by 6 Designed developed and maintained daily and monthly summary trending and benchmark reports in Tableau Desktop Environment AWS EC2 S3 Redshift Lambda Linux Python ScikitLearnNumpyPandasMatplotlib Machine Learning Logistic RegressionGradient BoostingRandom Forest Tableau Data Scientist Santander Boston MA March 2016 to April 2017 Data Scientist Banco Santander SA Austin TX March 2014 to December 2015 Whole Foods Market Inc is an American supermarket chain that specializes in selling organic foods products without artificial additive products for growing foods colors flavors sweeteners and hydrogenated fats It has 473 stores in North America and the United Kingdom KEY PROJECTS Customer Purchase Propensity Modelling Built machine learning based regression models using scikitlearn python frameworks to estimate the customer propensity to purchase based on attributes such as customer verticals they operate in revenue historic purchases frequency and regency behaviours These predictions helped estimate propensities with higher accuracy improving the overall productivity of sales teams by accurately targeting the prospective clients Coupon Recommender System Developed a personalized coupon recommender system using recommender algorithms collaborative filtering low rank matrix factorization that recommended best offers to a user based on similar user profiles The recommendations enabled users to engage better and helped improving the overall customer retention rates Outlier Anomalous Pattern Detection Created interactive dashboard suite that illustrated outlier characteristics across several salesrelated dimensions and overall impact of outlier imputation in R Shiny Used iterative outlier detection and imputation algorithm using multiple densitybased clustering techniques DBSCAN kernel density estimation Cross Sell and Upsell Opportunity Analysis Implemented market basket algorithms from transactional data which helped identify coupons usedpurchased together frequently Discovering frequent coupon sets helped unearth cross sell and up selling opportunities and led to better pricing bundling and promotion strategies for sales and marketing teams Forecast Process Innovations Forecast Sales and improved accuracy by 1020 by implementing advanced forecasting algorithms that were effective in detecting seasonality and trends in the patterns in addition to incorporating exogenous covariates Increased accuracy helped business plan better with respect to budgeting and sales and operations planning Price Elasticity Analysis Measured the price elasticity for products that experienced price cuts and promotions using regression methods based on the elasticity Whole Foods made selective and cautious price cuts for certain categories Customer Churn Prediction Predicted the likelihood of customer churn based on customer attributes like customer size RFM loyalty metrics revenue type of industry competitor products and growth rates etc The models deployed in production environment helped detect churn in advance and aided salesmarketing teams plan for various retention strategies in advance like price discounts custom licensing plans etc Responsibilities Working as project technical lead paired with business lead in scoping researching and assessing project feasibility outcomes and product deliverables on knowledge graph project Hadoop ecosystem Spark ElasticSearch JanusGraph etc Working with business lead successfully launched endtoend knowledge graph MVP in 7 months time whereas previous attempts at knowledge graph project had languished for 34 years Using NLP and ML built and maintained a variety of cloudbased natural language web scrapers and parsers to augment existing data sources Python NumPy SciPy SpaCy etc Spark TensorFlow Jupyter notebooks Mentored junior teammates shared knowledge of NLP and information retrieval best practices and performed code reviews on a weekly basis Prototyped conducted and reported on data science experiments to technical and nontechnical audiences using supervised semisupervised and unsupervised learning techniques anomaly detection named entity recognition ontology creation etc Using Python and NLTK aggregated natural language data from online documents and developed pipelines to ingest scraped data into relational databases Performed adhoc data science analyses for departments across the business using R Python Natural Language Toolkit NLTK machine learning AnalystData Scientist Banco Santander SA Hyderabad Telangana April 2012 to March 2014 Bharti AXA General Insurance Company Ltd is a joint venture between Bharti Enterprises a leading Indian business group and AXA a world leader in financial protection Bharti AXAs offers insurance coverage across various categories Motor Health Travel Home student travel and more The objective was to load data analyze and provide monthly reports for the predictions on a claims potential of a thirdparty recovery Tableau and SSRS were used to build claim and recovery reports Responsibilities Assembled a Predictive Modeling module by using supervised learning for Subrogation Claim Prediction to identify which claims would be classified as having Subrogation potential Implemented models such as Logistic Regression and Nave Bayes in Python using scikitlearn to predict the claim potential outcome Dimensionality Reduction techniques applied to refine the attribute lists and feature selection applied to rank selected features to generate accurate results Gathered requirements and business rules from business users to implement Predictive Modeling Designed and developed ETL packages using SSIS to create Data Warehouses from different tables and file sources like Flat and Excel files with different methods in SSIS such as derived columns aggregations Merge joins count conditional split and more to transform the data Designed reporting solutions for different stakeholders from mockup till deployment in different areas such as Potential Subrogation claims Monthly Revenue from Subrogation Transactions Performed data visualization and designed dashboards with Tableau and provided complex reports including charts summaries and graphs to interpret the findings for Adjustors to view various claim information Optimized queries in TSQL by removing unnecessary columns and redundant data normalized tables established joins and indices developed complex SQL queries stored procedures views functions and reports that meet customer requirements Environment Python 3x Scikitlearn Matplotlib Jupyter SQL Server 2012 MS SQL Server Management Studio MS BI Suite SSISSSRS TSQL Visual Studio BIDS Tableau Jr Python Developer Banco Santander SA Hyderabad Telangana June 2010 to April 2012 Responsibilities Worked on development of customer support and complains registration system This is a Customer feedback and complains management system Design develop test deploy and maintain the website Coding and execution of scripts in PythonUnixVB Development of Application using Java and Python Recording of Scripts Web Web Services HTML using Vugen and SoapUI and script validation through co correlations parameterizations and other methods Scripting web and web services Data set up using SQLORACLETeradata Resolving Complexity in the scripts of the website due to the complex logic and correlations Script validation sometimes becomes challenging as it demanded many web based logic rather than correlation and parameterization Running loadendurance tests using Vugen ALM and controller server monitoring analysis using Dynatrace UNIX putty SQL logs and other tools and reporting the performance Analyzing errors and exceptions using putty logs UNIX etc Testing in citrix protocol with scripts and scenario Execution of batch jobs in Control M Perfmon and other tools Scripting and validation of scripts through correlation parameterization and web based logic testing Smoke test Load test Endurance using Controller for a duration further analysis checking response times CPU utilizations memory leaks of servers and other performance characteristics of the website through capturing Perfmon logs and creating PAPAL reports and creating test reports Designed and developed data management system using MySQL Rewrite existing PythonDjangoJava module to deliver certain format of data Used Django Database APIs to access database objects Wrote python scripts to parse XML documents and load the data in database Generated property list for every application dynamically using python Responsible for search engine optimization SEO to improve the visibility of the website Handled all the client side validation using JavaScript Creating unit testregression test framework for workingnew code Using Subversion version control tool to coordinate teamdevelopment Responsible for debugging and troubleshooting the web application Environment Python Putty SQL Teradata SoapUI ControlM PerfMon MySQL Linux HTML XHTML CSS AJAX JavaScript Apache Web Server Education Bachelor of Technology in Computer Science Engineering Sreenidhi Institute of Science Technology Hyderabad Telangana Skills ALGORITHMS 10 years BI 10 years BUSINESS INTELLIGENCE 10 years LINUX 10 years LOGISTIC REGRESSION 10 years Additional Information TECHNICAL SKILLS Languages Java 8 Python R Python and R Numpy SciPy Pandas Scikitlearn Matplotlib Seaborn ggplot2 caret dplyr purrr readxl tidyr Rweka gmodels RCurl C50 twitter NLP Reshape2 rjson plyr Beautiful Soup Rpy2 Algorithms Kernel Density Estimation and Nonparametric Bayes Classifier KMeans Linear Regression Neighbors Nearest Farthest Range k Classification NonNegative Matrix Factorization Dimensionality Reduction Decision Tree Gaussian Processes Logistic Regression Nave Bayes Random Forest Ridge Regression Matrix FactorizationSVD NLPMachine LearningDeep Learning LDA Latent Dirichlet Allocation NLTK Apache OpenNLP Stanford NLP Sentiment Analysis SVMs ANN RNN CNN TensorFlow MXNet Caffe H2O Keras PyTorch Theano Azure ML Cloud Google Cloud Platform AWS Azure Bluemix Web Technologies JDBC HTML5 DHTML and XML CSS3 Web Services WSDL Data Modelling Tools Erwin r 96 95 91 8x Rational Rose ERStudio MS Visio SAP Power designer Big Data Technologies Hadoop Hive HDFS MapReduce Pig Kafka Databases SQL Hive Impala Pig Spark SQL Databases SQLServer My SQL MS Access HDFS HBase Teradata Netezza MongoDB Cassandra Reporting Tools MS Office WordExcelPower Point Visio Tableau Crystal reports XI Business Intelligence SSRS Business Objects 5x 6x Cognos7060 ETL Tools Informatica Power Centre SSIS Version Control Tools SVM GitHub BI Tools Tableau Tableau Server Tableau Reader SAP Business Objects OBIEE QlikView SAP Business Intelligence Amazon Redshift or Azure Data Warehouse Operating System Windows Linux Unix Macintosh HD Red Hat",
    "unique_id": "6d25d7b7-0ccc-4ce2-bb6d-4d970dcb28c3"
}