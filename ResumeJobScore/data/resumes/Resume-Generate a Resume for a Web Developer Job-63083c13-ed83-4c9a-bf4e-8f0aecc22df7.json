{
    "clean_data": "Sr Python Developer Sr span lPythonspan span lDeveloperspan Sr Python Developer Fitch Ratings 5 years of professional IT experience which includes Big data ecosystem related technologies like Hadoop HDFS Map Reduce Apache Pig Hive Sqoop Hbase Flume Oozie Spark Proficient in Hadoop Architecture and various components such as HDFS Job Tracker Task Tracker Data Node Name Node and MapReduce concepts Worked with Various Distributions like Cloudera Horton works and Amazon AWS Worked with large sets of structured semistructured and unstructured data Experience in working with tools like Attunity and having knowledge on ETL tools like Talend Extensively worked on Spark SQL Dataframes RDDs to improve the performance of the application Experience in using Apache Flume for collecting aggregating and moving large amounts of data from application servers Created Sqoop Jobs with incremental Loads to populate Hive External Table Designed and implemented Hive and Pig UDFs using Python for evaluation filtering loading and storing of data Developed web applications and Restful web services and APIs using Python Flask Pyramid and Django Good experience in Object oriented programming concepts in Python Django and Linux Experience in JSON based REST web services and SOAP for sending and getting data for the JSON format Experience in using HUE for scheduling and monitoring oozie workflow and coordinator Extensively worked on analyzing data using HiveQL Pig Latin and custom Map Reduce programs Worked on using different file formats like JSON Sequence files AVRO file Parquet file formats Used Spark SQL and Hive SQL to process structured and un structured data Extensive knowledge in writing and analyzing complex SQL queries stored procedures database tuning query optimization and resolving key performance issues Hands on Experience in Writing Python Scripts for Data Extract and Data Transfer from various data sources Utilized standard Python modules such as CSV Iter functions and pickle for development Worked with Panda data frames and MySQL queried MYSQL database queries from python using PythonMySQL connector and MYSQL db package to retrieve information Worked with several Python libraries like NumPy Pandas and MatplotLib Worked with Tableau to connect with Hive Data Warehouse to represent the data in dashboards Knowledge of working with Python Django Forms to record data of online users Experience in Scrum Agile models Highly motivated and versatile team player with the ability to work independently adapt quickly to new emerging technologies Authorized to work in the US for any employer Work Experience Sr Python Developer Fitch Ratings New York NY January 2016 to Present Responsibilities Analysis of requirements and implement different functions models according to design Extracted the data that are required for the segmentation using Pyspark environment Worked on reading multiple data formats on HDFS using PySpark Worked on creating custom ETL scripts using Python for business related data Created custom new columns depending up on the use case while ingesting the data into Hadoop Lake using Pyspark Performed Data Cleansing using python and loaded into the target tables Involved in transformations using various Spark Actions and Transformations by Creating RDDs from the required files in HDFS Worked on with spark dataframe operations that are required to develop a data format file Analysed the sql scripts and designed it by using PySpark SQL for faster performance Used python subprocess module to call UNIX shell commands to check directories or files exists Worked on storing the dataframe into hive as table using PySpark Developed data format file that is required by the Model to perform analytics using Spark SQL and Hive query language Used spark parameters like number of executors executor memory to execute the pipeline to increase the performance Developed other scripts that will be moving the data from HDFS to Amazon S3 The Data format file is provided as input to Model which performs the analytics and the output will be placed in HDFS location Created External Table in the HDFS location where the analytics data will be updating regularly Monitor the Spark jobs in Spark URL and Involved in Unit test and debugging after development Used Pandas API to put the data as time series and tabular format for easy timestamp data manipulation and retrieval Used Tableau to develop the data visualization and developed dashboards according to the requirement Involved in creating metrics attributes filters reports and dashboards with Tableau Environment Hortonworks24 Spark 16 Hive HiveQL Zookeeper Attunity Python Tableau Pandas Sr Python Developer Community Healthcare Network New York NY August 2014 to December 2015 Responsibilities Evaluated business requirements and prepared detailed specifications that follow project guidelines required to develop written programs Implemented python code for retrieving the Social Media data Analyzed large amounts of data sets to determine optimal way to aggregate and report on it Designed the data model for storing the data Creating Hive tables loading with data and writing Hive queries which will run internally in Map Reduce way Designed and developed the scripts to load the data into Hive Worked on Hive joins to aggregate social media data according to the requirement Created tables in Hive which are used as staging table for the data loaded from the inbound feeds Worked on different file formats like Text files and ORC files Created Hive queries that helped market analysts spot emerging trends by comparing fresh data with EDW reference tables and historical metrics Tested raw data and executed performance scripts Developed Spark Code using python for faster processing of data Segmentation and analysis of data from hive tables are done using Spark SQL Implemented Oozie workflow engine to run multiple Hive and Python jobs Developed Kafka where it will be receiving social media data and storing it in HDFS Used Tableau to connect to required data warehouse and perform visualization Environment Apache Hadoop Map Reduce Hive Pig Zookeeper Python Oozie Tableau Spark Kafka Hue HDFS Tez Sr Python Developer Community Healthcare Network New York NY January 2013 to July 2014 Responsibilities Requirement gathering and analysis phase of the project in documenting the business requirements by conducting workshopsmeetings with various business users Used Django Python webframework to develop application Developed the Python scripts in order to make the interaction between Server and MySQL Database Used MVC framework to build modular maintainable applications Designed and developed transactions and persistence layers to saveretrievemodify data for application functionalities using Django and PostgreSQL Automated data movements using python scripts Involved in splitting validating and processing of files Developed objectoriented programming to enhance product management Developed templates using HTML CSS BOOTSTRAP and JAVA Script based on clients request Created andor modified SQL Queries whenever required for change requestsenhancements Responsible for writing scripts for merging large datasets using Panda data frames and MySQL Developed the application using Python and MySQL Involved in writing the Python script files for processing data and loading to HDFS and writing CLI commands using HDFS shell commands Involved in building database Model and Views utilizing Python in order to build an interactive web based solution Responsible for debugging and troubleshooting the web application Designed and developed data management system using MySQL and built application logic in Python Environment Python 26 Django Framework 13 CSS SQL MySQL JavaScript JQuery Skills APACHE HADOOP HDFS 4 years HADOOP DISTRIBUTED FILE SYSTEM 4 years HDFS 4 years PYTHON 4 years SQL 4 years Additional Information SKILLSET Big Data HDFS Map Reduce Hive Pig Spark Kafka Sqoop Flume Oozie Cloud AWS Cloudera CDH Horton works Sandbox Programming Languages Python Java core Unix Shell Scripting IDES Eclipse MATLAB Scala IDE SQL Developer MS Office Database Oracle 11g SQL MySQL Operating Systems Linux UNIX Windows Tools Tableau Attunity Frameworks Django Web Technologies HTML CSS XML JavaScript",
    "entities": [
        "Created Hive",
        "IDE SQL Developer MS Office Database Oracle",
        "PySpark Worked",
        "ETL",
        "Developed",
        "Tableau Environment Hortonworks24 Spark",
        "US",
        "Hive External Table Designed",
        "HUE",
        "New York",
        "MatplotLib Worked",
        "Created External Table",
        "HDFS Used Tableau",
        "Attunity",
        "CSV Iter",
        "Present Responsibilities Analysis",
        "Utilized",
        "HDFS",
        "Hive Data Warehouse",
        "Created",
        "UNIX",
        "Spark SQL",
        "Server",
        "Created Sqoop Jobs",
        "Hadoop Architecture",
        "Model and Views",
        "Restful",
        "un",
        "Text",
        "Spark SQL Dataframes",
        "JSON",
        "Responsibilities Evaluated",
        "HDFS Job Tracker Task Tracker Data",
        "Pyspark Performed Data Cleansing",
        "Developed Spark Code",
        "CSS",
        "Work Experience Sr Python Developer Fitch Ratings New York",
        "Linux",
        "Panda",
        "Python Django Forms",
        "Spark Actions",
        "Social Media",
        "MVC",
        "SQL",
        "Spark URL and Involved",
        "EDW",
        "REST",
        "MapReduce",
        "Developer Community Healthcare Network",
        "lPythonspan",
        "Model",
        "Talend",
        "ORC",
        "Tableau",
        "Parquet",
        "Sandbox Programming Languages Python",
        "PySpark Developed",
        "Spark SQL Implemented Oozie",
        "CLI",
        "JAVA Script",
        "Amazon S3",
        "Hive",
        "Amazon AWS Worked",
        "Spark",
        "HTML CSS BOOTSTRAP",
        "Windows Tools Tableau",
        "PySpark SQL"
    ],
    "experience": "Experience in working with tools like Attunity and having knowledge on ETL tools like Talend Extensively worked on Spark SQL Dataframes RDDs to improve the performance of the application Experience in using Apache Flume for collecting aggregating and moving large amounts of data from application servers Created Sqoop Jobs with incremental Loads to populate Hive External Table Designed and implemented Hive and Pig UDFs using Python for evaluation filtering loading and storing of data Developed web applications and Restful web services and APIs using Python Flask Pyramid and Django Good experience in Object oriented programming concepts in Python Django and Linux Experience in JSON based REST web services and SOAP for sending and getting data for the JSON format Experience in using HUE for scheduling and monitoring oozie workflow and coordinator Extensively worked on analyzing data using HiveQL Pig Latin and custom Map Reduce programs Worked on using different file formats like JSON Sequence files AVRO file Parquet file formats Used Spark SQL and Hive SQL to process structured and un structured data Extensive knowledge in writing and analyzing complex SQL queries stored procedures database tuning query optimization and resolving key performance issues Hands on Experience in Writing Python Scripts for Data Extract and Data Transfer from various data sources Utilized standard Python modules such as CSV Iter functions and pickle for development Worked with Panda data frames and MySQL queried MYSQL database queries from python using PythonMySQL connector and MYSQL db package to retrieve information Worked with several Python libraries like NumPy Pandas and MatplotLib Worked with Tableau to connect with Hive Data Warehouse to represent the data in dashboards Knowledge of working with Python Django Forms to record data of online users Experience in Scrum Agile models Highly motivated and versatile team player with the ability to work independently adapt quickly to new emerging technologies Authorized to work in the US for any employer Work Experience Sr Python Developer Fitch Ratings New York NY January 2016 to Present Responsibilities Analysis of requirements and implement different functions models according to design Extracted the data that are required for the segmentation using Pyspark environment Worked on reading multiple data formats on HDFS using PySpark Worked on creating custom ETL scripts using Python for business related data Created custom new columns depending up on the use case while ingesting the data into Hadoop Lake using Pyspark Performed Data Cleansing using python and loaded into the target tables Involved in transformations using various Spark Actions and Transformations by Creating RDDs from the required files in HDFS Worked on with spark dataframe operations that are required to develop a data format file Analysed the sql scripts and designed it by using PySpark SQL for faster performance Used python subprocess module to call UNIX shell commands to check directories or files exists Worked on storing the dataframe into hive as table using PySpark Developed data format file that is required by the Model to perform analytics using Spark SQL and Hive query language Used spark parameters like number of executors executor memory to execute the pipeline to increase the performance Developed other scripts that will be moving the data from HDFS to Amazon S3 The Data format file is provided as input to Model which performs the analytics and the output will be placed in HDFS location Created External Table in the HDFS location where the analytics data will be updating regularly Monitor the Spark jobs in Spark URL and Involved in Unit test and debugging after development Used Pandas API to put the data as time series and tabular format for easy timestamp data manipulation and retrieval Used Tableau to develop the data visualization and developed dashboards according to the requirement Involved in creating metrics attributes filters reports and dashboards with Tableau Environment Hortonworks24 Spark 16 Hive HiveQL Zookeeper Attunity Python Tableau Pandas Sr Python Developer Community Healthcare Network New York NY August 2014 to December 2015 Responsibilities Evaluated business requirements and prepared detailed specifications that follow project guidelines required to develop written programs Implemented python code for retrieving the Social Media data Analyzed large amounts of data sets to determine optimal way to aggregate and report on it Designed the data model for storing the data Creating Hive tables loading with data and writing Hive queries which will run internally in Map Reduce way Designed and developed the scripts to load the data into Hive Worked on Hive joins to aggregate social media data according to the requirement Created tables in Hive which are used as staging table for the data loaded from the inbound feeds Worked on different file formats like Text files and ORC files Created Hive queries that helped market analysts spot emerging trends by comparing fresh data with EDW reference tables and historical metrics Tested raw data and executed performance scripts Developed Spark Code using python for faster processing of data Segmentation and analysis of data from hive tables are done using Spark SQL Implemented Oozie workflow engine to run multiple Hive and Python jobs Developed Kafka where it will be receiving social media data and storing it in HDFS Used Tableau to connect to required data warehouse and perform visualization Environment Apache Hadoop Map Reduce Hive Pig Zookeeper Python Oozie Tableau Spark Kafka Hue HDFS Tez Sr Python Developer Community Healthcare Network New York NY January 2013 to July 2014 Responsibilities Requirement gathering and analysis phase of the project in documenting the business requirements by conducting workshopsmeetings with various business users Used Django Python webframework to develop application Developed the Python scripts in order to make the interaction between Server and MySQL Database Used MVC framework to build modular maintainable applications Designed and developed transactions and persistence layers to saveretrievemodify data for application functionalities using Django and PostgreSQL Automated data movements using python scripts Involved in splitting validating and processing of files Developed objectoriented programming to enhance product management Developed templates using HTML CSS BOOTSTRAP and JAVA Script based on clients request Created andor modified SQL Queries whenever required for change requestsenhancements Responsible for writing scripts for merging large datasets using Panda data frames and MySQL Developed the application using Python and MySQL Involved in writing the Python script files for processing data and loading to HDFS and writing CLI commands using HDFS shell commands Involved in building database Model and Views utilizing Python in order to build an interactive web based solution Responsible for debugging and troubleshooting the web application Designed and developed data management system using MySQL and built application logic in Python Environment Python 26 Django Framework 13 CSS SQL MySQL JavaScript JQuery Skills APACHE HADOOP HDFS 4 years HADOOP DISTRIBUTED FILE SYSTEM 4 years HDFS 4 years PYTHON 4 years SQL 4 years Additional Information SKILLSET Big Data HDFS Map Reduce Hive Pig Spark Kafka Sqoop Flume Oozie Cloud AWS Cloudera CDH Horton works Sandbox Programming Languages Python Java core Unix Shell Scripting IDES Eclipse MATLAB Scala IDE SQL Developer MS Office Database Oracle 11 g SQL MySQL Operating Systems Linux UNIX Windows Tools Tableau Attunity Frameworks Django Web Technologies HTML CSS XML JavaScript",
    "extracted_keywords": [
        "Sr",
        "Python",
        "Developer",
        "Sr",
        "lPythonspan",
        "span",
        "lDeveloperspan",
        "Sr",
        "Python",
        "Developer",
        "Fitch",
        "Ratings",
        "years",
        "IT",
        "experience",
        "data",
        "ecosystem",
        "technologies",
        "Hadoop",
        "HDFS",
        "Map",
        "Reduce",
        "Apache",
        "Pig",
        "Hive",
        "Sqoop",
        "Hbase",
        "Flume",
        "Oozie",
        "Spark",
        "Proficient",
        "Hadoop",
        "Architecture",
        "components",
        "HDFS",
        "Job",
        "Tracker",
        "Task",
        "Tracker",
        "Data",
        "Node",
        "Name",
        "Node",
        "MapReduce",
        "concepts",
        "Distributions",
        "Cloudera",
        "Horton",
        "Amazon",
        "AWS",
        "sets",
        "data",
        "Experience",
        "tools",
        "Attunity",
        "knowledge",
        "ETL",
        "tools",
        "Talend",
        "Spark",
        "SQL",
        "Dataframes",
        "RDDs",
        "performance",
        "application",
        "Experience",
        "Apache",
        "Flume",
        "amounts",
        "data",
        "application",
        "servers",
        "Sqoop",
        "Jobs",
        "Loads",
        "Hive",
        "External",
        "Table",
        "Hive",
        "Pig",
        "UDFs",
        "Python",
        "evaluation",
        "loading",
        "storing",
        "data",
        "web",
        "applications",
        "web",
        "services",
        "APIs",
        "Python",
        "Flask",
        "Pyramid",
        "Django",
        "Good",
        "experience",
        "Object",
        "programming",
        "concepts",
        "Python",
        "Django",
        "Linux",
        "Experience",
        "JSON",
        "REST",
        "web",
        "services",
        "SOAP",
        "data",
        "format",
        "Experience",
        "HUE",
        "scheduling",
        "oozie",
        "workflow",
        "coordinator",
        "data",
        "HiveQL",
        "Pig",
        "Latin",
        "custom",
        "Map",
        "Reduce",
        "programs",
        "file",
        "formats",
        "JSON",
        "Sequence",
        "files",
        "AVRO",
        "file",
        "Parquet",
        "file",
        "formats",
        "Spark",
        "SQL",
        "Hive",
        "SQL",
        "un",
        "data",
        "knowledge",
        "writing",
        "SQL",
        "queries",
        "procedures",
        "database",
        "query",
        "optimization",
        "performance",
        "Hands",
        "Experience",
        "Python",
        "Scripts",
        "Data",
        "Extract",
        "Data",
        "Transfer",
        "data",
        "sources",
        "Python",
        "modules",
        "CSV",
        "Iter",
        "functions",
        "pickle",
        "development",
        "Panda",
        "data",
        "frames",
        "MySQL",
        "MYSQL",
        "database",
        "python",
        "PythonMySQL",
        "connector",
        "MYSQL",
        "package",
        "information",
        "Python",
        "libraries",
        "NumPy",
        "Pandas",
        "MatplotLib",
        "Tableau",
        "Hive",
        "Data",
        "Warehouse",
        "data",
        "dashboards",
        "Knowledge",
        "Python",
        "Django",
        "Forms",
        "data",
        "users",
        "Experience",
        "Scrum",
        "Agile",
        "models",
        "team",
        "player",
        "ability",
        "technologies",
        "US",
        "employer",
        "Work",
        "Experience",
        "Sr",
        "Python",
        "Developer",
        "Fitch",
        "Ratings",
        "New",
        "York",
        "NY",
        "January",
        "Present",
        "Responsibilities",
        "Analysis",
        "requirements",
        "functions",
        "models",
        "design",
        "data",
        "segmentation",
        "Pyspark",
        "environment",
        "data",
        "formats",
        "HDFS",
        "PySpark",
        "custom",
        "ETL",
        "scripts",
        "Python",
        "business",
        "data",
        "custom",
        "columns",
        "use",
        "case",
        "data",
        "Hadoop",
        "Lake",
        "Pyspark",
        "Performed",
        "Data",
        "Cleansing",
        "python",
        "target",
        "tables",
        "transformations",
        "Spark",
        "Actions",
        "Transformations",
        "RDDs",
        "files",
        "HDFS",
        "spark",
        "operations",
        "data",
        "format",
        "file",
        "scripts",
        "PySpark",
        "SQL",
        "performance",
        "module",
        "UNIX",
        "shell",
        "commands",
        "directories",
        "files",
        "dataframe",
        "hive",
        "table",
        "PySpark",
        "Developed",
        "data",
        "format",
        "file",
        "Model",
        "analytics",
        "Spark",
        "SQL",
        "Hive",
        "query",
        "language",
        "spark",
        "parameters",
        "number",
        "executors",
        "executor",
        "memory",
        "pipeline",
        "performance",
        "scripts",
        "data",
        "HDFS",
        "Amazon",
        "S3",
        "Data",
        "format",
        "file",
        "input",
        "Model",
        "analytics",
        "output",
        "HDFS",
        "location",
        "External",
        "Table",
        "HDFS",
        "location",
        "analytics",
        "data",
        "Monitor",
        "Spark",
        "jobs",
        "Spark",
        "URL",
        "Unit",
        "test",
        "development",
        "Pandas",
        "API",
        "data",
        "time",
        "series",
        "format",
        "timestamp",
        "data",
        "manipulation",
        "retrieval",
        "Tableau",
        "data",
        "visualization",
        "dashboards",
        "requirement",
        "metrics",
        "attributes",
        "filters",
        "reports",
        "dashboards",
        "Tableau",
        "Environment",
        "Hortonworks24",
        "Spark",
        "Hive",
        "HiveQL",
        "Zookeeper",
        "Attunity",
        "Python",
        "Tableau",
        "Pandas",
        "Sr",
        "Python",
        "Developer",
        "Community",
        "Healthcare",
        "Network",
        "New",
        "York",
        "NY",
        "August",
        "December",
        "Responsibilities",
        "business",
        "requirements",
        "specifications",
        "project",
        "guidelines",
        "programs",
        "python",
        "code",
        "Social",
        "Media",
        "data",
        "amounts",
        "data",
        "sets",
        "way",
        "data",
        "model",
        "data",
        "Hive",
        "tables",
        "data",
        "Hive",
        "queries",
        "Map",
        "Reduce",
        "way",
        "scripts",
        "data",
        "Hive",
        "Worked",
        "Hive",
        "media",
        "data",
        "requirement",
        "tables",
        "Hive",
        "table",
        "data",
        "feeds",
        "file",
        "formats",
        "Text",
        "files",
        "files",
        "Created",
        "Hive",
        "queries",
        "market",
        "analysts",
        "trends",
        "data",
        "EDW",
        "reference",
        "tables",
        "metrics",
        "data",
        "performance",
        "scripts",
        "Developed",
        "Spark",
        "Code",
        "python",
        "processing",
        "data",
        "Segmentation",
        "analysis",
        "data",
        "tables",
        "Spark",
        "SQL",
        "Oozie",
        "workflow",
        "engine",
        "Hive",
        "Python",
        "jobs",
        "Developed",
        "Kafka",
        "media",
        "data",
        "HDFS",
        "Tableau",
        "data",
        "warehouse",
        "visualization",
        "Environment",
        "Apache",
        "Hadoop",
        "Map",
        "Reduce",
        "Hive",
        "Pig",
        "Zookeeper",
        "Python",
        "Oozie",
        "Tableau",
        "Spark",
        "Kafka",
        "Hue",
        "HDFS",
        "Tez",
        "Sr",
        "Python",
        "Developer",
        "Community",
        "Healthcare",
        "Network",
        "New",
        "York",
        "NY",
        "January",
        "July",
        "Responsibilities",
        "Requirement",
        "gathering",
        "analysis",
        "phase",
        "project",
        "business",
        "requirements",
        "workshopsmeetings",
        "business",
        "users",
        "Django",
        "Python",
        "webframework",
        "application",
        "Python",
        "scripts",
        "order",
        "interaction",
        "Server",
        "MySQL",
        "Database",
        "MVC",
        "framework",
        "applications",
        "transactions",
        "persistence",
        "layers",
        "data",
        "application",
        "functionalities",
        "Django",
        "PostgreSQL",
        "data",
        "movements",
        "scripts",
        "splitting",
        "processing",
        "files",
        "programming",
        "product",
        "management",
        "templates",
        "HTML",
        "CSS",
        "BOOTSTRAP",
        "Script",
        "clients",
        "andor",
        "SQL",
        "Queries",
        "change",
        "requestsenhancements",
        "scripts",
        "datasets",
        "Panda",
        "data",
        "frames",
        "MySQL",
        "application",
        "Python",
        "MySQL",
        "Python",
        "script",
        "files",
        "data",
        "loading",
        "HDFS",
        "CLI",
        "commands",
        "HDFS",
        "shell",
        "commands",
        "database",
        "Model",
        "Views",
        "Python",
        "order",
        "web",
        "solution",
        "web",
        "application",
        "data",
        "management",
        "system",
        "MySQL",
        "application",
        "logic",
        "Python",
        "Environment",
        "Python",
        "Django",
        "Framework",
        "CSS",
        "SQL",
        "MySQL",
        "JavaScript",
        "JQuery",
        "Skills",
        "APACHE",
        "HADOOP",
        "HDFS",
        "years",
        "HADOOP",
        "FILE",
        "SYSTEM",
        "years",
        "years",
        "PYTHON",
        "years",
        "SQL",
        "years",
        "Additional",
        "Information",
        "SKILLSET",
        "Big",
        "Data",
        "HDFS",
        "Map",
        "Reduce",
        "Hive",
        "Pig",
        "Spark",
        "Kafka",
        "Sqoop",
        "Flume",
        "Oozie",
        "Cloud",
        "Cloudera",
        "CDH",
        "Horton",
        "Sandbox",
        "Programming",
        "Languages",
        "Python",
        "Java",
        "core",
        "Unix",
        "Shell",
        "Scripting",
        "IDES",
        "Eclipse",
        "MATLAB",
        "Scala",
        "IDE",
        "SQL",
        "Developer",
        "MS",
        "Office",
        "Database",
        "Oracle",
        "g",
        "SQL",
        "MySQL",
        "Operating",
        "Systems",
        "Linux",
        "UNIX",
        "Windows",
        "Tools",
        "Tableau",
        "Attunity",
        "Frameworks",
        "Django",
        "Web",
        "Technologies",
        "HTML",
        "CSS",
        "XML",
        "JavaScript"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T22:55:28.629634",
    "resume_data": "Sr Python Developer Sr span lPythonspan span lDeveloperspan Sr Python Developer Fitch Ratings 5 years of professional IT experience which includes Big data ecosystem related technologies like Hadoop HDFS Map Reduce Apache Pig Hive Sqoop Hbase Flume Oozie Spark Proficient in Hadoop Architecture and various components such as HDFS Job Tracker Task Tracker Data Node Name Node and MapReduce concepts Worked with Various Distributions like Cloudera Horton works and Amazon AWS Worked with large sets of structured semistructured and unstructured data Experience in working with tools like Attunity and having knowledge on ETL tools like Talend Extensively worked on Spark SQL Dataframes RDDs to improve the performance of the application Experience in using Apache Flume for collecting aggregating and moving large amounts of data from application servers Created Sqoop Jobs with incremental Loads to populate Hive External Table Designed and implemented Hive and Pig UDFs using Python for evaluation filtering loading and storing of data Developed web applications and Restful web services and APIs using Python Flask Pyramid and Django Good experience in Object oriented programming concepts in Python Django and Linux Experience in JSON based REST web services and SOAP for sending and getting data for the JSON format Experience in using HUE for scheduling and monitoring oozie workflow and coordinator Extensively worked on analyzing data using HiveQL Pig Latin and custom Map Reduce programs Worked on using different file formats like JSON Sequence files AVRO file Parquet file formats Used Spark SQL and Hive SQL to process structured and un structured data Extensive knowledge in writing and analyzing complex SQL queries stored procedures database tuning query optimization and resolving key performance issues Hands on Experience in Writing Python Scripts for Data Extract and Data Transfer from various data sources Utilized standard Python modules such as CSV Iter functions and pickle for development Worked with Panda data frames and MySQL queried MYSQL database queries from python using PythonMySQL connector and MYSQL db package to retrieve information Worked with several Python libraries like NumPy Pandas and MatplotLib Worked with Tableau to connect with Hive Data Warehouse to represent the data in dashboards Knowledge of working with Python Django Forms to record data of online users Experience in Scrum Agile models Highly motivated and versatile team player with the ability to work independently adapt quickly to new emerging technologies Authorized to work in the US for any employer Work Experience Sr Python Developer Fitch Ratings New York NY January 2016 to Present Responsibilities Analysis of requirements and implement different functions models according to design Extracted the data that are required for the segmentation using Pyspark environment Worked on reading multiple data formats on HDFS using PySpark Worked on creating custom ETL scripts using Python for business related data Created custom new columns depending up on the use case while ingesting the data into Hadoop Lake using Pyspark Performed Data Cleansing using python and loaded into the target tables Involved in transformations using various Spark Actions and Transformations by Creating RDDs from the required files in HDFS Worked on with spark dataframe operations that are required to develop a data format file Analysed the sql scripts and designed it by using PySpark SQL for faster performance Used python subprocess module to call UNIX shell commands to check directories or files exists Worked on storing the dataframe into hive as table using PySpark Developed data format file that is required by the Model to perform analytics using Spark SQL and Hive query language Used spark parameters like number of executors executor memory to execute the pipeline to increase the performance Developed other scripts that will be moving the data from HDFS to Amazon S3 The Data format file is provided as input to Model which performs the analytics and the output will be placed in HDFS location Created External Table in the HDFS location where the analytics data will be updating regularly Monitor the Spark jobs in Spark URL and Involved in Unit test and debugging after development Used Pandas API to put the data as time series and tabular format for easy timestamp data manipulation and retrieval Used Tableau to develop the data visualization and developed dashboards according to the requirement Involved in creating metrics attributes filters reports and dashboards with Tableau Environment Hortonworks24 Spark 16 Hive HiveQL Zookeeper Attunity Python Tableau Pandas Sr Python Developer Community Healthcare Network New York NY August 2014 to December 2015 Responsibilities Evaluated business requirements and prepared detailed specifications that follow project guidelines required to develop written programs Implemented python code for retrieving the Social Media data Analyzed large amounts of data sets to determine optimal way to aggregate and report on it Designed the data model for storing the data Creating Hive tables loading with data and writing Hive queries which will run internally in Map Reduce way Designed and developed the scripts to load the data into Hive Worked on Hive joins to aggregate social media data according to the requirement Created tables in Hive which are used as staging table for the data loaded from the inbound feeds Worked on different file formats like Text files and ORC files Created Hive queries that helped market analysts spot emerging trends by comparing fresh data with EDW reference tables and historical metrics Tested raw data and executed performance scripts Developed Spark Code using python for faster processing of data Segmentation and analysis of data from hive tables are done using Spark SQL Implemented Oozie workflow engine to run multiple Hive and Python jobs Developed Kafka where it will be receiving social media data and storing it in HDFS Used Tableau to connect to required data warehouse and perform visualization Environment Apache Hadoop Map Reduce Hive Pig Zookeeper Python Oozie Tableau Spark Kafka Hue HDFS Tez Sr Python Developer Community Healthcare Network New York NY January 2013 to July 2014 Responsibilities Requirement gathering and analysis phase of the project in documenting the business requirements by conducting workshopsmeetings with various business users Used Django Python webframework to develop application Developed the Python scripts in order to make the interaction between Server and MySQL Database Used MVC framework to build modular maintainable applications Designed and developed transactions and persistence layers to saveretrievemodify data for application functionalities using Django and PostgreSQL Automated data movements using python scripts Involved in splitting validating and processing of files Developed objectoriented programming to enhance product management Developed templates using HTML CSS BOOTSTRAP and JAVA Script based on clients request Created andor modified SQL Queries whenever required for change requestsenhancements Responsible for writing scripts for merging large datasets using Panda data frames and MySQL Developed the application using Python and MySQL Involved in writing the Python script files for processing data and loading to HDFS and writing CLI commands using HDFS shell commands Involved in building database Model and Views utilizing Python in order to build an interactive web based solution Responsible for debugging and troubleshooting the web application Designed and developed data management system using MySQL and built application logic in Python Environment Python 26 Django Framework 13 CSS SQL MySQL JavaScript JQuery Skills APACHE HADOOP HDFS 4 years HADOOP DISTRIBUTED FILE SYSTEM 4 years HDFS 4 years PYTHON 4 years SQL 4 years Additional Information SKILLSET Big Data HDFS Map Reduce Hive Pig Spark Kafka Sqoop Flume Oozie Cloud AWS Cloudera CDH Horton works Sandbox Programming Languages Python Java core Unix Shell Scripting IDES Eclipse MATLAB Scala IDE SQL Developer MS Office Database Oracle 11g SQL MySQL Operating Systems Linux UNIX Windows Tools Tableau Attunity Frameworks Django Web Technologies HTML CSS XML JavaScript",
    "unique_id": "63083c13-ed83-4c9a-bf4e-8f0aecc22df7"
}