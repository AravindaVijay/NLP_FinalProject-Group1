{
    "clean_data": "Sr Big Data Developer Sr Big Data span lDeveloperspan Sr Big Data Developer Nordstrom Inc Seattle WA 8 years of working experience in IT Industry including Big Data and JavaJ2EE Worked in various domains included Ecommerce telecommunication finance and investment Strong experience in Hadoop 23 Eco system technologies such as in HDFS YARN Map Reduce Spark 13 Hive 10 Pig 014 HBase 098 Zookeeper 34 Oozie 30 Flume 13 Sqoop 13 Impala 12 and Kafka 12 Experienced in building maintaining multiple Hadoop clusters of different sizes and configuration and setting up the rack topology for large clusters also in Hadoop AdministrationArchitecture Developer Developed Java 6 and Scala 210 applications on Hadoop and Spark Streaming for highvolume and realtime data processing Expertise in Spark Streaming and Spark SQL with Scala Exceptional skills with NoSQL databases such as HBase and Cassandra Experienced in writing Sqoop HiveQL and Pig scripts as well as the UDFs to do ELT process Utilized Kafka RabbitMQ and Flume to gain realtime data stream and save it in HDFS and HBase from the different data sources Experience other Hadoop ecosystem tools in jobs such as ZooKeeper Oozie Implala Experience in all the phases of Data warehouse life cycle involving requirement analysis design coding testing and deployment Strong in core Java data structure and Java components like Collections Framework Exception handling IO system and Multithreading Earned designation with J2EE development by using frameworks such as Spring MVC and Hibernate 3 4 as well as using Web services such as SOAP and REST Familiar in AgileScrum Development and Test Driven Development TDD Extensive Experienced in Unit Testing with JUnit MRUnit Pytest Worked in development environment tools such as Git JIRA Jenkins AgileScrum and Waterfall Selfmotivated teamwork working under high pressure working in several projects simultaneously dynamic problemsolving adept at working with minimal to supervision highlycaliber teams of professionals Authorized to work in the US for any employer Work Experience Sr Big Data Developer Nordstrom Inc Seattle WA February 2017 to Present Project Customer Behaviors Analysis System CBAS Nordstrom Inc is a leading fashion specialty retailer offering compelling clothing shoes and accessories for men women and children Nordstrom needs to maximize profits by reducing the overstock products Therefore the company decides to analyze the historical shopping transactions from the customers The purpose of this project is to process and analyze the transactions etc users shopping histories and eventually for data science team to generate reports Our team is responsible to process the data by using big data technologies with data modeling data transformation data analyzing and data visualization Responsibilities Design and build scalable infrastructure and platform for very large amounts of data ingestion aggregation integration and advanced analytics in Hadoop including Map Reduce Spark Hive HBase Pig Design the HBase schemes based on the requirements and HBase data migration and validation Design and implement the HBase query APIs in Java for BI teams Apply Spark Streaming to receive data from Kafka to do the continually data cleaning and aggregating then store the data in HBase Work on the core and Spark SQL modules of Spark Streaming extensively Write customized Spark SQL UDFs in Scala and Java Load the data from different sources such as HDFS or HBase into Spark RDD and implement in memory data computation to generate the output response Expertise at designing tables in Hive MySQL using Sqoop and processing data like importing and exporting of databases to the HDFS ETL Data extraction managing aggregation and loading into HBase Involve in converting HiveSQL queries into Spark transformations using Spark RDDs and Scala to improve the performance Develop multiple POCs using Scala and deploy on the Yarn cluster compared the performance of Spark with Hive and SQL Develop predictive analytic using Apache Spark Scala APIs HBase Spark Streaming performance tuning Configure Zookeeper to coordinate and support Kafka Spark Spark Streaming HBase and HDFS Environment Hadoop 26 HDFS YARN Spark 15 Spark SQL Spark Streaming Zookeeper 35 HBase 098 Cassandra Hive 12 Kafka 13 RabbitMQ 344 Oozie 41 Eclipse Scala UNIX Shell Scripting Sr Big Data Developer Avvo Inc Seattle WA March 2016 to January 2017 Project sWIG3 Avvo was founded in Seattle Washington by techsavvy lawyer Mark Britton to make legal easier and help people find a lawyer Avvo need to enhance the existing metric table because the requirements from business are updated In this project we need to load the raw data from MySQL to do delta difference Then we save the historical data in to source tables which will be used into dimension layer Finally the tables by applied business logics are save in the dimension layer Responsibilities Develop big data applications in CDH platform and maintain the data processing workflows Design the Hive schemes based on the requirements and Hive data migration and validation Apply Sqoop to extract data from MySQL to do the continually data cleaning and aggregating then store the data in HDFS Work on creating HiveQL scripts to process and load data into Hive tables Experienced performing data quality investigations writing interactive adhoc queries using Spark shell Expertise at designing tables in Hive MySQL using Sqoop and processing data like importing and exporting of databases to the HDFS ETL Data extraction managing aggregation and loading into Hive Extensive experience in data monitoring and integrating existing monitoring to maintain data workflows via DataDog Develop POC using Hive and deploy on the Test cluster compared the performance of the old data architecture Develop predictive analytic using Impala and check the data consistency Develop workflow automation in Oozie and define coordinator to connect the upstreaming workflow and generate trigger files for downstreaming workflow Environment Hadoop 26 MySQL HDFS YARN Impala DataDog Spark 16 Spark SQL Zookeeper 35 Hive 12 Kafka 13 Oozie 41 Eclipse Scala Python UNIX Shell Scripting Senior Data Engineer Groupon Seattle WA January 2015 to February 2016 Project Shopping Discount System SDS Groupon is an American worldwide ecommerce marketplace connecting millions of subscribers with local merchants by offering activities travel goods and services The Groupon is providing services which connects the suppliers and users with discount coupons of different products This project focused on analyzing the feedbacks from the suppliers based on the customers shopping behaviors and provides the best discount ranges of different coupons Our team worked on data ingestions from varieties data sources processing data with Hadoop related solutions and worked for designing statistical models for analyzing data with data science team Responsibilities Analyzed large data sets by running custom MapReduce and Hive queries Designed the HBase schemes based on the requirements Designed Cassandra schemes and connect it by Spark HBase data migration and validation Assisted in exporting analyzed data to relational databases using Sqoop Utilized Kafka and RabbitMQ to capture the data stream Processed data by using Spark Streaming and Kafka then stored the results in HBase Worked on the core and Spark SQL modules of Spark extensively Loaded the data from different source such as HDFS or HBase into Spark RDD and do in memory data computation to generate the output response Implemented POC to migrate MapReduce jobs into Spark RDD transformation using Scala IDE for IntelliJ Involved in converting HiveSQL queries into Spark transformations using Spark RDDs and Scala to improve the performance Developed predictive analytic using Apache Spark Scala APIs Scheduled workflow with Oozie Environment Hadoop 26 HDFS Spark 15 Spark SQL Spark Streaming Zookeeper 35 HBase 098 Hive 12 Kafka 13 RabbitMQ 344 Oozie 41 IntelliJ Scala UNIX Shell Scripting Senior Data Engineer Disney ABC Television Group Seattle WA September 2013 to December 2014 The DisneyABC Television Group is composed of The Walt Disney Companys global entertainment and news television properties owned television stations group as well as radio and publishing businesses The DisneyABC User Tracking Platform focused on development for the Hadoop related solutions and services to achieve better advertisement strategy We provide data processing and analytic solutions including streaming data ingestion RDBMS and log integration data transformationcleaning and data modeling Responsibilities Worked on Amazon EMR 4x with Agile methodology Developed Kafka consumer to receive and store real time data from Kafka to Amazon S3 Used Flume to collect aggregate and store web log data from different sources Used Sqoop to transfer data between RDBMS and HBase Extracted data from MongoDB through MongoDB Connector for Hadoop Involved in migrating of MapReduce programs into Spark using Spark and Scala Scheduled workflow with Oozie Use Spark with Scala and Spark SQL for testing and processing of data Worked with analytics team to build statistical model with MLlib and PySpark Worked with analytics team to prepare and visualize tables in Tableau for reporting Performed unit testing using JUnit and Pytest Used Git for version control JIRA for project tracking and Jenkins for continuous integration Environment Hadoop 26 Cloudera CDH 54 HDFS MapReduce HBase Sqoop Flume Zookeeper MongoDB Spark 14 Spark SQL Pyspark MLlib Tableau 92 JUnit Pytest Jr Data Engineer Starbucks Seattle WA June 2012 to August 2013 Starbucks Corporation is an American coffee company and coffeehouse chain The team contributes to detect mitigate and prevent loss and quickly and helps to easily identify fraudulent merchants to reduce risk Greater precision in search results and prioritization of the results is helping customers more quickly and easily evaluate a merchant without the cost of executing additional searches or further investigations Responsibilities Responsible for building scalable distributed data solutions using Hadoop Handled importing of data from various data sources performed transformations using Hive MapReduce loaded data into HDFS and Extracted the data from MySQL into HDFS using Sqoop Installed and configured Hive Pig Sqoop Flume and Oozie on the Hadoop cluster Developed simple to complex MapReduce jobs using Java and scripts using Hive and Pig Analyzed the data by performing Hive queries HiveQL and running Pig scripts Pig Latin for data ingestion and egress Implemented business logic by writing UDFs in Java and used various UDFs from other sources Experienced on loading and transforming of large sets of structured and semi structured data Managing and Reviewing Hadoop Log Files deploy and Maintaining Hadoop Cluster Export filtered data into HBase for fast query Environment Hadoop HBase Hive Pig Map Reduce Sqoop Oozie Eclipse Java Jr JavaJ2EEBig Data Developer UHGOptum Minnetonka MN July 2011 to May 2012 UnitedHealth Group Inc is an American forprofit managed health care company based in Minnetonka Minnesota It is sixth in the United States on the Fortune 500 UnitedHealth Group offers health care products and insurance services Responsibilities Developed Map Reduce programs in Java to help the analysis team to read write delete update and analyze the big data Developed a serious of Map Reduce algorithms to increase the performance Developed Web pages using Struts view component JSP JavaScript HTML jQuery AJAX to create the user interface views migration 3rd party applications Java Hibernate is used to access the database within the system Environment Eclipse Java Map Reduce Algorithm Hibernate Struts JavaScript JQuery CSS HTML MySQL and XML Jr Java Developer BBVA Compass Birmingham AL April 2009 to June 2011 This is an intranet application for stock screening This application is used by the customers and other client based users to build their query and send the result to the clients they cover The application has alerting capability which will alert users according to the criteria and alert level set for a stock in the form chosen The result set built is saved in downloadable viewable formats as preferred by the clients Responsibilities Involved in system design which is based on Spring Struts Hibernate framework Implemented the business logic in standalone Java classes using core Java Developed database MySQL applications Worked in Hibernate Template to access the MySQL database Involved in Unit testing of the components and created unit test cases and did unit test review Environment Eclipse MySQL Client 41 Spring HTML JavaScript Hibernate JSF Junit SDLC AgileScrum Education Bachelors Skills APACHE HADOOP OOZIE 6 years APACHE HADOOP SQOOP 6 years Hadoop 6 years HADOOP 6 years OOZIE 6 years Additional Information TECHNICAL SKILLS Hadoop Eco Systems Web Technologies Hadoop 270 MapReduce HBase 098 SOAP REST JSP 20 JavaScript 18 Spark 16 Hive 10 Pig 014 Kafka 12 Servlet 30 HTML 5 CSS 3 Sqoop 13 Flume 13 Impala 12 Oozie 30 Zookeeper 34 NoSQL Java Frameworks HBase 098 Cassandra 2 MangoDB 3 spring MVC Hibernate 3 4 Programming Languages Others Java Scala SQL SparkSQL JSON AVRO XML HiveQL PigLatin C C Python RabbitMQ 30",
    "entities": [
        "MLlib",
        "Nordstrom",
        "Environment Hadoop HBase Hive",
        "ABC Television",
        "BI",
        "HDFS",
        "Git JIRA Jenkins AgileScrum",
        "Oozie Use Spark",
        "Waterfall Selfmotivated",
        "Java",
        "Hadoop",
        "Hibernate 3 4",
        "SOAP",
        "JUnit",
        "HBase",
        "ELT",
        "Zookeeper 34 Oozie",
        "Oozie Environment Hadoop",
        "Amazon",
        "SQL Develop",
        "Spark with",
        "YARN Spark",
        "Assisted",
        "PySpark Worked",
        "Developed",
        "Additional Information TECHNICAL SKILLS Hadoop Eco Systems Web Technologies Hadoop",
        "Responsibilities Involved",
        "Develop",
        "Spark HBase",
        "Processed",
        "JSP",
        "Sr Big Data Developer Sr Big Data",
        "Spark Streaming",
        "MVC Hibernate 3 4 Programming Languages Others",
        "Data Developer",
        "Amazon EMR 4x",
        "Maintaining Hadoop Cluster Export",
        "MVC",
        "Spark",
        "MapReduce HBase",
        "Agile",
        "The DisneyABC Television Group",
        "ZooKeeper Oozie",
        "Scala Exceptional",
        "CDH",
        "US",
        "Sqoop",
        "Sqoop Installed",
        "Scala",
        "Work Experience Sr Big Data Developer Nordstrom Inc",
        "Hive MySQL",
        "Seattle",
        "AgileScrum Development",
        "Test",
        "Responsibilities Responsible",
        "Oozie",
        "SQL",
        "Spark RDD",
        "Hadoop AdministrationArchitecture Developer",
        "Responsibilities Worked",
        "the United States",
        "UnitedHealth Group Inc",
        "Big Data",
        "Hive",
        "Hive Extensive",
        "DataDog Develop",
        "PigLatin C C Python",
        "HDFS Environment Hadoop",
        "Servlet",
        "Performed",
        "Impala",
        "Spark SQL",
        "JavaScript",
        "Responsibilities Analyzed",
        "HBase Work",
        "Present Project Customer Behaviors Analysis System",
        "The Walt Disney Companys",
        "Expertise",
        "CSS",
        "the Fortune 500 UnitedHealth Group",
        "Responsibilities Develop",
        "MapReduce",
        "RDBMS",
        "NoSQL",
        "Tableau",
        "Washington",
        "Nordstrom Inc",
        "Responsibilities Design",
        "ETL Data"
    ],
    "experience": "Experience other Hadoop ecosystem tools in jobs such as ZooKeeper Oozie Implala Experience in all the phases of Data warehouse life cycle involving requirement analysis design coding testing and deployment Strong in core Java data structure and Java components like Collections Framework Exception handling IO system and Multithreading Earned designation with J2EE development by using frameworks such as Spring MVC and Hibernate 3 4 as well as using Web services such as SOAP and REST Familiar in AgileScrum Development and Test Driven Development TDD Extensive Experienced in Unit Testing with JUnit MRUnit Pytest Worked in development environment tools such as Git JIRA Jenkins AgileScrum and Waterfall Selfmotivated teamwork working under high pressure working in several projects simultaneously dynamic problemsolving adept at working with minimal to supervision highlycaliber teams of professionals Authorized to work in the US for any employer Work Experience Sr Big Data Developer Nordstrom Inc Seattle WA February 2017 to Present Project Customer Behaviors Analysis System CBAS Nordstrom Inc is a leading fashion specialty retailer offering compelling clothing shoes and accessories for men women and children Nordstrom needs to maximize profits by reducing the overstock products Therefore the company decides to analyze the historical shopping transactions from the customers The purpose of this project is to process and analyze the transactions etc users shopping histories and eventually for data science team to generate reports Our team is responsible to process the data by using big data technologies with data modeling data transformation data analyzing and data visualization Responsibilities Design and build scalable infrastructure and platform for very large amounts of data ingestion aggregation integration and advanced analytics in Hadoop including Map Reduce Spark Hive HBase Pig Design the HBase schemes based on the requirements and HBase data migration and validation Design and implement the HBase query APIs in Java for BI teams Apply Spark Streaming to receive data from Kafka to do the continually data cleaning and aggregating then store the data in HBase Work on the core and Spark SQL modules of Spark Streaming extensively Write customized Spark SQL UDFs in Scala and Java Load the data from different sources such as HDFS or HBase into Spark RDD and implement in memory data computation to generate the output response Expertise at designing tables in Hive MySQL using Sqoop and processing data like importing and exporting of databases to the HDFS ETL Data extraction managing aggregation and loading into HBase Involve in converting HiveSQL queries into Spark transformations using Spark RDDs and Scala to improve the performance Develop multiple POCs using Scala and deploy on the Yarn cluster compared the performance of Spark with Hive and SQL Develop predictive analytic using Apache Spark Scala APIs HBase Spark Streaming performance tuning Configure Zookeeper to coordinate and support Kafka Spark Spark Streaming HBase and HDFS Environment Hadoop 26 HDFS YARN Spark 15 Spark SQL Spark Streaming Zookeeper 35 HBase 098 Cassandra Hive 12 Kafka 13 RabbitMQ 344 Oozie 41 Eclipse Scala UNIX Shell Scripting Sr Big Data Developer Avvo Inc Seattle WA March 2016 to January 2017 Project sWIG3 Avvo was founded in Seattle Washington by techsavvy lawyer Mark Britton to make legal easier and help people find a lawyer Avvo need to enhance the existing metric table because the requirements from business are updated In this project we need to load the raw data from MySQL to do delta difference Then we save the historical data in to source tables which will be used into dimension layer Finally the tables by applied business logics are save in the dimension layer Responsibilities Develop big data applications in CDH platform and maintain the data processing workflows Design the Hive schemes based on the requirements and Hive data migration and validation Apply Sqoop to extract data from MySQL to do the continually data cleaning and aggregating then store the data in HDFS Work on creating HiveQL scripts to process and load data into Hive tables Experienced performing data quality investigations writing interactive adhoc queries using Spark shell Expertise at designing tables in Hive MySQL using Sqoop and processing data like importing and exporting of databases to the HDFS ETL Data extraction managing aggregation and loading into Hive Extensive experience in data monitoring and integrating existing monitoring to maintain data workflows via DataDog Develop POC using Hive and deploy on the Test cluster compared the performance of the old data architecture Develop predictive analytic using Impala and check the data consistency Develop workflow automation in Oozie and define coordinator to connect the upstreaming workflow and generate trigger files for downstreaming workflow Environment Hadoop 26 MySQL HDFS YARN Impala DataDog Spark 16 Spark SQL Zookeeper 35 Hive 12 Kafka 13 Oozie 41 Eclipse Scala Python UNIX Shell Scripting Senior Data Engineer Groupon Seattle WA January 2015 to February 2016 Project Shopping Discount System SDS Groupon is an American worldwide ecommerce marketplace connecting millions of subscribers with local merchants by offering activities travel goods and services The Groupon is providing services which connects the suppliers and users with discount coupons of different products This project focused on analyzing the feedbacks from the suppliers based on the customers shopping behaviors and provides the best discount ranges of different coupons Our team worked on data ingestions from varieties data sources processing data with Hadoop related solutions and worked for designing statistical models for analyzing data with data science team Responsibilities Analyzed large data sets by running custom MapReduce and Hive queries Designed the HBase schemes based on the requirements Designed Cassandra schemes and connect it by Spark HBase data migration and validation Assisted in exporting analyzed data to relational databases using Sqoop Utilized Kafka and RabbitMQ to capture the data stream Processed data by using Spark Streaming and Kafka then stored the results in HBase Worked on the core and Spark SQL modules of Spark extensively Loaded the data from different source such as HDFS or HBase into Spark RDD and do in memory data computation to generate the output response Implemented POC to migrate MapReduce jobs into Spark RDD transformation using Scala IDE for IntelliJ Involved in converting HiveSQL queries into Spark transformations using Spark RDDs and Scala to improve the performance Developed predictive analytic using Apache Spark Scala APIs Scheduled workflow with Oozie Environment Hadoop 26 HDFS Spark 15 Spark SQL Spark Streaming Zookeeper 35 HBase 098 Hive 12 Kafka 13 RabbitMQ 344 Oozie 41 IntelliJ Scala UNIX Shell Scripting Senior Data Engineer Disney ABC Television Group Seattle WA September 2013 to December 2014 The DisneyABC Television Group is composed of The Walt Disney Companys global entertainment and news television properties owned television stations group as well as radio and publishing businesses The DisneyABC User Tracking Platform focused on development for the Hadoop related solutions and services to achieve better advertisement strategy We provide data processing and analytic solutions including streaming data ingestion RDBMS and log integration data transformationcleaning and data modeling Responsibilities Worked on Amazon EMR 4x with Agile methodology Developed Kafka consumer to receive and store real time data from Kafka to Amazon S3 Used Flume to collect aggregate and store web log data from different sources Used Sqoop to transfer data between RDBMS and HBase Extracted data from MongoDB through MongoDB Connector for Hadoop Involved in migrating of MapReduce programs into Spark using Spark and Scala Scheduled workflow with Oozie Use Spark with Scala and Spark SQL for testing and processing of data Worked with analytics team to build statistical model with MLlib and PySpark Worked with analytics team to prepare and visualize tables in Tableau for reporting Performed unit testing using JUnit and Pytest Used Git for version control JIRA for project tracking and Jenkins for continuous integration Environment Hadoop 26 Cloudera CDH 54 HDFS MapReduce HBase Sqoop Flume Zookeeper MongoDB Spark 14 Spark SQL Pyspark MLlib Tableau 92 JUnit Pytest Jr Data Engineer Starbucks Seattle WA June 2012 to August 2013 Starbucks Corporation is an American coffee company and coffeehouse chain The team contributes to detect mitigate and prevent loss and quickly and helps to easily identify fraudulent merchants to reduce risk Greater precision in search results and prioritization of the results is helping customers more quickly and easily evaluate a merchant without the cost of executing additional searches or further investigations Responsibilities Responsible for building scalable distributed data solutions using Hadoop Handled importing of data from various data sources performed transformations using Hive MapReduce loaded data into HDFS and Extracted the data from MySQL into HDFS using Sqoop Installed and configured Hive Pig Sqoop Flume and Oozie on the Hadoop cluster Developed simple to complex MapReduce jobs using Java and scripts using Hive and Pig Analyzed the data by performing Hive queries HiveQL and running Pig scripts Pig Latin for data ingestion and egress Implemented business logic by writing UDFs in Java and used various UDFs from other sources Experienced on loading and transforming of large sets of structured and semi structured data Managing and Reviewing Hadoop Log Files deploy and Maintaining Hadoop Cluster Export filtered data into HBase for fast query Environment Hadoop HBase Hive Pig Map Reduce Sqoop Oozie Eclipse Java Jr JavaJ2EEBig Data Developer UHGOptum Minnetonka MN July 2011 to May 2012 UnitedHealth Group Inc is an American forprofit managed health care company based in Minnetonka Minnesota It is sixth in the United States on the Fortune 500 UnitedHealth Group offers health care products and insurance services Responsibilities Developed Map Reduce programs in Java to help the analysis team to read write delete update and analyze the big data Developed a serious of Map Reduce algorithms to increase the performance Developed Web pages using Struts view component JSP JavaScript HTML jQuery AJAX to create the user interface views migration 3rd party applications Java Hibernate is used to access the database within the system Environment Eclipse Java Map Reduce Algorithm Hibernate Struts JavaScript JQuery CSS HTML MySQL and XML Jr Java Developer BBVA Compass Birmingham AL April 2009 to June 2011 This is an intranet application for stock screening This application is used by the customers and other client based users to build their query and send the result to the clients they cover The application has alerting capability which will alert users according to the criteria and alert level set for a stock in the form chosen The result set built is saved in downloadable viewable formats as preferred by the clients Responsibilities Involved in system design which is based on Spring Struts Hibernate framework Implemented the business logic in standalone Java classes using core Java Developed database MySQL applications Worked in Hibernate Template to access the MySQL database Involved in Unit testing of the components and created unit test cases and did unit test review Environment Eclipse MySQL Client 41 Spring HTML JavaScript Hibernate JSF Junit SDLC AgileScrum Education Bachelors Skills APACHE HADOOP OOZIE 6 years APACHE HADOOP SQOOP 6 years Hadoop 6 years HADOOP 6 years OOZIE 6 years Additional Information TECHNICAL SKILLS Hadoop Eco Systems Web Technologies Hadoop 270 MapReduce HBase 098 SOAP REST JSP 20 JavaScript 18 Spark 16 Hive 10 Pig 014 Kafka 12 Servlet 30 HTML 5 CSS 3 Sqoop 13 Flume 13 Impala 12 Oozie 30 Zookeeper 34 NoSQL Java Frameworks HBase 098 Cassandra 2 MangoDB 3 spring MVC Hibernate 3 4 Programming Languages Others Java Scala SQL SparkSQL JSON AVRO XML HiveQL PigLatin C C Python RabbitMQ 30",
    "extracted_keywords": [
        "Sr",
        "Big",
        "Data",
        "Developer",
        "Sr",
        "Big",
        "Data",
        "span",
        "lDeveloperspan",
        "Sr",
        "Big",
        "Data",
        "Developer",
        "Nordstrom",
        "Inc",
        "Seattle",
        "WA",
        "years",
        "working",
        "experience",
        "IT",
        "Industry",
        "Big",
        "Data",
        "JavaJ2EE",
        "domains",
        "Ecommerce",
        "telecommunication",
        "finance",
        "investment",
        "experience",
        "Hadoop",
        "Eco",
        "system",
        "technologies",
        "HDFS",
        "YARN",
        "Map",
        "Spark",
        "Hive",
        "Pig",
        "HBase",
        "Zookeeper",
        "Oozie",
        "Flume",
        "Sqoop",
        "Impala",
        "Kafka",
        "Hadoop",
        "clusters",
        "sizes",
        "configuration",
        "rack",
        "topology",
        "clusters",
        "Hadoop",
        "AdministrationArchitecture",
        "Developer",
        "Java",
        "Scala",
        "applications",
        "Hadoop",
        "Spark",
        "Streaming",
        "highvolume",
        "data",
        "Expertise",
        "Spark",
        "Streaming",
        "Spark",
        "SQL",
        "Scala",
        "Exceptional",
        "skills",
        "databases",
        "HBase",
        "Cassandra",
        "Sqoop",
        "HiveQL",
        "Pig",
        "scripts",
        "UDFs",
        "ELT",
        "process",
        "Kafka",
        "RabbitMQ",
        "Flume",
        "data",
        "stream",
        "HDFS",
        "HBase",
        "data",
        "sources",
        "Hadoop",
        "ecosystem",
        "tools",
        "jobs",
        "ZooKeeper",
        "Oozie",
        "Implala",
        "Experience",
        "phases",
        "Data",
        "warehouse",
        "life",
        "cycle",
        "requirement",
        "analysis",
        "design",
        "testing",
        "deployment",
        "Strong",
        "core",
        "Java",
        "data",
        "structure",
        "Java",
        "components",
        "Collections",
        "Framework",
        "Exception",
        "IO",
        "system",
        "Multithreading",
        "designation",
        "J2EE",
        "development",
        "frameworks",
        "Spring",
        "MVC",
        "Hibernate",
        "Web",
        "services",
        "SOAP",
        "REST",
        "Familiar",
        "AgileScrum",
        "Development",
        "Test",
        "Driven",
        "Development",
        "TDD",
        "Extensive",
        "Experienced",
        "Unit",
        "Testing",
        "JUnit",
        "MRUnit",
        "Pytest",
        "development",
        "environment",
        "tools",
        "Git",
        "JIRA",
        "Jenkins",
        "AgileScrum",
        "Waterfall",
        "Selfmotivated",
        "teamwork",
        "pressure",
        "projects",
        "supervision",
        "highlycaliber",
        "teams",
        "professionals",
        "US",
        "employer",
        "Work",
        "Experience",
        "Sr",
        "Big",
        "Data",
        "Developer",
        "Nordstrom",
        "Inc",
        "Seattle",
        "WA",
        "February",
        "Present",
        "Project",
        "Customer",
        "Behaviors",
        "Analysis",
        "System",
        "CBAS",
        "Nordstrom",
        "Inc",
        "fashion",
        "specialty",
        "retailer",
        "clothing",
        "shoes",
        "accessories",
        "men",
        "women",
        "children",
        "Nordstrom",
        "profits",
        "overstock",
        "products",
        "company",
        "shopping",
        "transactions",
        "customers",
        "purpose",
        "project",
        "transactions",
        "users",
        "histories",
        "data",
        "science",
        "team",
        "reports",
        "team",
        "data",
        "data",
        "technologies",
        "data",
        "data",
        "transformation",
        "data",
        "data",
        "visualization",
        "Responsibilities",
        "Design",
        "infrastructure",
        "platform",
        "amounts",
        "data",
        "ingestion",
        "aggregation",
        "integration",
        "analytics",
        "Hadoop",
        "Map",
        "Reduce",
        "Spark",
        "Hive",
        "HBase",
        "Pig",
        "Design",
        "HBase",
        "schemes",
        "requirements",
        "HBase",
        "data",
        "migration",
        "validation",
        "Design",
        "HBase",
        "query",
        "APIs",
        "Java",
        "BI",
        "teams",
        "Spark",
        "Streaming",
        "data",
        "Kafka",
        "data",
        "cleaning",
        "data",
        "HBase",
        "Work",
        "core",
        "Spark",
        "SQL",
        "modules",
        "Spark",
        "Streaming",
        "Spark",
        "SQL",
        "UDFs",
        "Scala",
        "Java",
        "Load",
        "data",
        "sources",
        "HDFS",
        "HBase",
        "Spark",
        "RDD",
        "memory",
        "data",
        "computation",
        "output",
        "response",
        "Expertise",
        "tables",
        "Hive",
        "MySQL",
        "Sqoop",
        "processing",
        "data",
        "exporting",
        "databases",
        "HDFS",
        "ETL",
        "Data",
        "extraction",
        "aggregation",
        "loading",
        "HBase",
        "Involve",
        "HiveSQL",
        "queries",
        "Spark",
        "transformations",
        "Spark",
        "RDDs",
        "Scala",
        "performance",
        "POCs",
        "Scala",
        "Yarn",
        "cluster",
        "performance",
        "Spark",
        "Hive",
        "SQL",
        "Develop",
        "analytic",
        "Apache",
        "Spark",
        "Scala",
        "APIs",
        "HBase",
        "Spark",
        "Streaming",
        "performance",
        "Configure",
        "Zookeeper",
        "Kafka",
        "Spark",
        "Spark",
        "Streaming",
        "HBase",
        "HDFS",
        "Environment",
        "Hadoop",
        "HDFS",
        "YARN",
        "Spark",
        "Spark",
        "SQL",
        "Spark",
        "Streaming",
        "Zookeeper",
        "HBase",
        "Cassandra",
        "Hive",
        "Kafka",
        "RabbitMQ",
        "Oozie",
        "Eclipse",
        "Scala",
        "UNIX",
        "Shell",
        "Scripting",
        "Sr",
        "Big",
        "Data",
        "Developer",
        "Avvo",
        "Inc",
        "Seattle",
        "WA",
        "March",
        "January",
        "Project",
        "sWIG3",
        "Avvo",
        "Seattle",
        "Washington",
        "lawyer",
        "Mark",
        "Britton",
        "people",
        "lawyer",
        "Avvo",
        "table",
        "requirements",
        "business",
        "project",
        "data",
        "MySQL",
        "delta",
        "difference",
        "data",
        "source",
        "tables",
        "dimension",
        "layer",
        "tables",
        "business",
        "logics",
        "dimension",
        "layer",
        "Responsibilities",
        "data",
        "applications",
        "CDH",
        "platform",
        "data",
        "processing",
        "workflows",
        "Design",
        "Hive",
        "schemes",
        "requirements",
        "Hive",
        "data",
        "migration",
        "validation",
        "Apply",
        "Sqoop",
        "data",
        "MySQL",
        "data",
        "cleaning",
        "data",
        "HDFS",
        "Work",
        "scripts",
        "process",
        "data",
        "Hive",
        "tables",
        "data",
        "quality",
        "investigations",
        "queries",
        "Spark",
        "shell",
        "Expertise",
        "tables",
        "Hive",
        "MySQL",
        "Sqoop",
        "processing",
        "data",
        "exporting",
        "databases",
        "HDFS",
        "ETL",
        "Data",
        "extraction",
        "aggregation",
        "loading",
        "Hive",
        "experience",
        "data",
        "monitoring",
        "monitoring",
        "data",
        "workflows",
        "DataDog",
        "Develop",
        "POC",
        "Hive",
        "Test",
        "cluster",
        "performance",
        "data",
        "architecture",
        "analytic",
        "Impala",
        "data",
        "consistency",
        "automation",
        "Oozie",
        "coordinator",
        "upstreaming",
        "trigger",
        "files",
        "Environment",
        "Hadoop",
        "MySQL",
        "HDFS",
        "YARN",
        "Impala",
        "DataDog",
        "Spark",
        "Spark",
        "SQL",
        "Zookeeper",
        "Hive",
        "Kafka",
        "Oozie",
        "Eclipse",
        "Scala",
        "Python",
        "UNIX",
        "Shell",
        "Scripting",
        "Senior",
        "Data",
        "Engineer",
        "Groupon",
        "Seattle",
        "WA",
        "January",
        "February",
        "Project",
        "Shopping",
        "Discount",
        "System",
        "SDS",
        "Groupon",
        "ecommerce",
        "marketplace",
        "millions",
        "subscribers",
        "merchants",
        "activities",
        "goods",
        "services",
        "Groupon",
        "services",
        "suppliers",
        "users",
        "discount",
        "coupons",
        "products",
        "project",
        "feedbacks",
        "suppliers",
        "customers",
        "behaviors",
        "discount",
        "ranges",
        "coupons",
        "team",
        "data",
        "ingestions",
        "varieties",
        "data",
        "sources",
        "data",
        "Hadoop",
        "solutions",
        "models",
        "data",
        "data",
        "science",
        "team",
        "Responsibilities",
        "data",
        "sets",
        "custom",
        "MapReduce",
        "Hive",
        "queries",
        "HBase",
        "schemes",
        "requirements",
        "Cassandra",
        "schemes",
        "Spark",
        "HBase",
        "data",
        "migration",
        "validation",
        "Assisted",
        "data",
        "databases",
        "Sqoop",
        "Kafka",
        "data",
        "stream",
        "data",
        "Spark",
        "Streaming",
        "Kafka",
        "results",
        "HBase",
        "core",
        "Spark",
        "SQL",
        "modules",
        "Spark",
        "data",
        "source",
        "HDFS",
        "HBase",
        "Spark",
        "RDD",
        "memory",
        "data",
        "computation",
        "output",
        "response",
        "POC",
        "MapReduce",
        "jobs",
        "Spark",
        "RDD",
        "transformation",
        "Scala",
        "IDE",
        "IntelliJ",
        "HiveSQL",
        "queries",
        "Spark",
        "transformations",
        "Spark",
        "RDDs",
        "Scala",
        "performance",
        "analytic",
        "Apache",
        "Spark",
        "Scala",
        "APIs",
        "workflow",
        "Oozie",
        "Environment",
        "Hadoop",
        "HDFS",
        "Spark",
        "Spark",
        "SQL",
        "Spark",
        "Streaming",
        "Zookeeper",
        "HBase",
        "Hive",
        "Kafka",
        "RabbitMQ",
        "Oozie",
        "IntelliJ",
        "Scala",
        "UNIX",
        "Shell",
        "Scripting",
        "Senior",
        "Data",
        "Engineer",
        "Disney",
        "ABC",
        "Television",
        "Group",
        "Seattle",
        "WA",
        "September",
        "December",
        "DisneyABC",
        "Television",
        "Group",
        "Walt",
        "Disney",
        "Companys",
        "entertainment",
        "news",
        "television",
        "properties",
        "television",
        "stations",
        "group",
        "radio",
        "publishing",
        "businesses",
        "DisneyABC",
        "User",
        "Tracking",
        "Platform",
        "development",
        "Hadoop",
        "solutions",
        "services",
        "advertisement",
        "strategy",
        "data",
        "processing",
        "solutions",
        "data",
        "ingestion",
        "RDBMS",
        "integration",
        "data",
        "transformationcleaning",
        "data",
        "Responsibilities",
        "Amazon",
        "EMR",
        "4x",
        "methodology",
        "Kafka",
        "consumer",
        "time",
        "data",
        "Kafka",
        "Amazon",
        "S3",
        "Flume",
        "aggregate",
        "store",
        "web",
        "log",
        "data",
        "sources",
        "Sqoop",
        "data",
        "RDBMS",
        "HBase",
        "data",
        "Connector",
        "Hadoop",
        "migrating",
        "MapReduce",
        "programs",
        "Spark",
        "Spark",
        "Scala",
        "workflow",
        "Oozie",
        "Use",
        "Spark",
        "Scala",
        "Spark",
        "SQL",
        "testing",
        "processing",
        "data",
        "analytics",
        "team",
        "model",
        "MLlib",
        "PySpark",
        "analytics",
        "team",
        "tables",
        "Tableau",
        "Performed",
        "unit",
        "testing",
        "JUnit",
        "Pytest",
        "Git",
        "version",
        "control",
        "JIRA",
        "project",
        "tracking",
        "Jenkins",
        "integration",
        "Environment",
        "Hadoop",
        "Cloudera",
        "CDH",
        "HDFS",
        "MapReduce",
        "HBase",
        "Sqoop",
        "Flume",
        "Zookeeper",
        "MongoDB",
        "Spark",
        "Spark",
        "SQL",
        "Pyspark",
        "MLlib",
        "Tableau",
        "JUnit",
        "Pytest",
        "Jr",
        "Data",
        "Engineer",
        "Seattle",
        "WA",
        "June",
        "August",
        "Starbucks",
        "Corporation",
        "coffee",
        "company",
        "chain",
        "team",
        "mitigate",
        "loss",
        "merchants",
        "risk",
        "Greater",
        "precision",
        "search",
        "results",
        "prioritization",
        "results",
        "customers",
        "merchant",
        "cost",
        "searches",
        "investigations",
        "Responsibilities",
        "data",
        "solutions",
        "Hadoop",
        "importing",
        "data",
        "data",
        "sources",
        "transformations",
        "Hive",
        "MapReduce",
        "data",
        "HDFS",
        "data",
        "MySQL",
        "HDFS",
        "Sqoop",
        "Installed",
        "Hive",
        "Pig",
        "Sqoop",
        "Flume",
        "Oozie",
        "Hadoop",
        "cluster",
        "MapReduce",
        "jobs",
        "Java",
        "scripts",
        "Hive",
        "Pig",
        "data",
        "Hive",
        "queries",
        "Pig",
        "scripts",
        "Pig",
        "Latin",
        "data",
        "ingestion",
        "business",
        "logic",
        "UDFs",
        "Java",
        "UDFs",
        "sources",
        "loading",
        "transforming",
        "sets",
        "data",
        "Managing",
        "Reviewing",
        "Hadoop",
        "Log",
        "Files",
        "Maintaining",
        "Hadoop",
        "Cluster",
        "Export",
        "data",
        "HBase",
        "query",
        "Environment",
        "Hadoop",
        "HBase",
        "Hive",
        "Pig",
        "Map",
        "Reduce",
        "Sqoop",
        "Oozie",
        "Eclipse",
        "Java",
        "Jr",
        "JavaJ2EEBig",
        "Data",
        "Developer",
        "UHGOptum",
        "Minnetonka",
        "MN",
        "July",
        "May",
        "UnitedHealth",
        "Group",
        "Inc",
        "forprofit",
        "health",
        "care",
        "company",
        "Minnetonka",
        "Minnesota",
        "United",
        "States",
        "Fortune",
        "UnitedHealth",
        "Group",
        "health",
        "care",
        "products",
        "insurance",
        "services",
        "Responsibilities",
        "Map",
        "programs",
        "Java",
        "analysis",
        "team",
        "update",
        "data",
        "serious",
        "Map",
        "Reduce",
        "performance",
        "Web",
        "pages",
        "Struts",
        "view",
        "component",
        "JSP",
        "JavaScript",
        "HTML",
        "jQuery",
        "AJAX",
        "user",
        "interface",
        "migration",
        "3rd",
        "party",
        "applications",
        "Java",
        "Hibernate",
        "database",
        "system",
        "Environment",
        "Eclipse",
        "Java",
        "Map",
        "Reduce",
        "Algorithm",
        "Hibernate",
        "Struts",
        "JavaScript",
        "JQuery",
        "CSS",
        "HTML",
        "MySQL",
        "XML",
        "Jr",
        "Java",
        "Developer",
        "Compass",
        "Birmingham",
        "AL",
        "April",
        "June",
        "intranet",
        "application",
        "stock",
        "application",
        "customers",
        "client",
        "users",
        "query",
        "result",
        "clients",
        "application",
        "capability",
        "users",
        "criteria",
        "level",
        "stock",
        "form",
        "result",
        "formats",
        "clients",
        "Responsibilities",
        "system",
        "design",
        "Spring",
        "Struts",
        "Hibernate",
        "framework",
        "business",
        "logic",
        "Java",
        "classes",
        "core",
        "Java",
        "Developed",
        "database",
        "MySQL",
        "applications",
        "Hibernate",
        "Template",
        "MySQL",
        "database",
        "Unit",
        "testing",
        "components",
        "unit",
        "test",
        "cases",
        "unit",
        "test",
        "review",
        "Environment",
        "Eclipse",
        "MySQL",
        "Client",
        "Spring",
        "HTML",
        "JavaScript",
        "Hibernate",
        "JSF",
        "Junit",
        "SDLC",
        "AgileScrum",
        "Education",
        "Bachelors",
        "Skills",
        "APACHE",
        "HADOOP",
        "OOZIE",
        "years",
        "APACHE",
        "HADOOP",
        "SQOOP",
        "years",
        "Hadoop",
        "years",
        "HADOOP",
        "years",
        "OOZIE",
        "years",
        "Additional",
        "Information",
        "TECHNICAL",
        "SKILLS",
        "Hadoop",
        "Eco",
        "Systems",
        "Web",
        "Technologies",
        "Hadoop",
        "MapReduce",
        "HBase",
        "SOAP",
        "REST",
        "JSP",
        "JavaScript",
        "Spark",
        "Hive",
        "Pig",
        "Kafka",
        "Servlet",
        "HTML",
        "CSS",
        "Sqoop",
        "Flume",
        "Impala",
        "Oozie",
        "Zookeeper",
        "NoSQL",
        "Java",
        "Frameworks",
        "HBase",
        "Cassandra",
        "MangoDB",
        "spring",
        "MVC",
        "Hibernate",
        "Programming",
        "Languages",
        "Others",
        "Java",
        "Scala",
        "SQL",
        "SparkSQL",
        "JSON",
        "AVRO",
        "XML",
        "HiveQL",
        "PigLatin",
        "C",
        "C",
        "Python"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T22:59:42.056542",
    "resume_data": "Sr Big Data Developer Sr Big Data span lDeveloperspan Sr Big Data Developer Nordstrom Inc Seattle WA 8 years of working experience in IT Industry including Big Data and JavaJ2EE Worked in various domains included Ecommerce telecommunication finance and investment Strong experience in Hadoop 23 Eco system technologies such as in HDFS YARN Map Reduce Spark 13 Hive 10 Pig 014 HBase 098 Zookeeper 34 Oozie 30 Flume 13 Sqoop 13 Impala 12 and Kafka 12 Experienced in building maintaining multiple Hadoop clusters of different sizes and configuration and setting up the rack topology for large clusters also in Hadoop AdministrationArchitecture Developer Developed Java 6 and Scala 210 applications on Hadoop and Spark Streaming for highvolume and realtime data processing Expertise in Spark Streaming and Spark SQL with Scala Exceptional skills with NoSQL databases such as HBase and Cassandra Experienced in writing Sqoop HiveQL and Pig scripts as well as the UDFs to do ELT process Utilized Kafka RabbitMQ and Flume to gain realtime data stream and save it in HDFS and HBase from the different data sources Experience other Hadoop ecosystem tools in jobs such as ZooKeeper Oozie Implala Experience in all the phases of Data warehouse life cycle involving requirement analysis design coding testing and deployment Strong in core Java data structure and Java components like Collections Framework Exception handling IO system and Multithreading Earned designation with J2EE development by using frameworks such as Spring MVC and Hibernate 3 4 as well as using Web services such as SOAP and REST Familiar in AgileScrum Development and Test Driven Development TDD Extensive Experienced in Unit Testing with JUnit MRUnit Pytest Worked in development environment tools such as Git JIRA Jenkins AgileScrum and Waterfall Selfmotivated teamwork working under high pressure working in several projects simultaneously dynamic problemsolving adept at working with minimal to supervision highlycaliber teams of professionals Authorized to work in the US for any employer Work Experience Sr Big Data Developer Nordstrom Inc Seattle WA February 2017 to Present Project Customer Behaviors Analysis System CBAS Nordstrom Inc is a leading fashion specialty retailer offering compelling clothing shoes and accessories for men women and children Nordstrom needs to maximize profits by reducing the overstock products Therefore the company decides to analyze the historical shopping transactions from the customers The purpose of this project is to process and analyze the transactions etc users shopping histories and eventually for data science team to generate reports Our team is responsible to process the data by using big data technologies with data modeling data transformation data analyzing and data visualization Responsibilities Design and build scalable infrastructure and platform for very large amounts of data ingestion aggregation integration and advanced analytics in Hadoop including Map Reduce Spark Hive HBase Pig Design the HBase schemes based on the requirements and HBase data migration and validation Design and implement the HBase query APIs in Java for BI teams Apply Spark Streaming to receive data from Kafka to do the continually data cleaning and aggregating then store the data in HBase Work on the core and Spark SQL modules of Spark Streaming extensively Write customized Spark SQL UDFs in Scala and Java Load the data from different sources such as HDFS or HBase into Spark RDD and implement in memory data computation to generate the output response Expertise at designing tables in Hive MySQL using Sqoop and processing data like importing and exporting of databases to the HDFS ETL Data extraction managing aggregation and loading into HBase Involve in converting HiveSQL queries into Spark transformations using Spark RDDs and Scala to improve the performance Develop multiple POCs using Scala and deploy on the Yarn cluster compared the performance of Spark with Hive and SQL Develop predictive analytic using Apache Spark Scala APIs HBase Spark Streaming performance tuning Configure Zookeeper to coordinate and support Kafka Spark Spark Streaming HBase and HDFS Environment Hadoop 26 HDFS YARN Spark 15 Spark SQL Spark Streaming Zookeeper 35 HBase 098 Cassandra Hive 12 Kafka 13 RabbitMQ 344 Oozie 41 Eclipse Scala UNIX Shell Scripting Sr Big Data Developer Avvo Inc Seattle WA March 2016 to January 2017 Project sWIG3 Avvo was founded in Seattle Washington by techsavvy lawyer Mark Britton to make legal easier and help people find a lawyer Avvo need to enhance the existing metric table because the requirements from business are updated In this project we need to load the raw data from MySQL to do delta difference Then we save the historical data in to source tables which will be used into dimension layer Finally the tables by applied business logics are save in the dimension layer Responsibilities Develop big data applications in CDH platform and maintain the data processing workflows Design the Hive schemes based on the requirements and Hive data migration and validation Apply Sqoop to extract data from MySQL to do the continually data cleaning and aggregating then store the data in HDFS Work on creating HiveQL scripts to process and load data into Hive tables Experienced performing data quality investigations writing interactive adhoc queries using Spark shell Expertise at designing tables in Hive MySQL using Sqoop and processing data like importing and exporting of databases to the HDFS ETL Data extraction managing aggregation and loading into Hive Extensive experience in data monitoring and integrating existing monitoring to maintain data workflows via DataDog Develop POC using Hive and deploy on the Test cluster compared the performance of the old data architecture Develop predictive analytic using Impala and check the data consistency Develop workflow automation in Oozie and define coordinator to connect the upstreaming workflow and generate trigger files for downstreaming workflow Environment Hadoop 26 MySQL HDFS YARN Impala DataDog Spark 16 Spark SQL Zookeeper 35 Hive 12 Kafka 13 Oozie 41 Eclipse Scala Python UNIX Shell Scripting Senior Data Engineer Groupon Seattle WA January 2015 to February 2016 Project Shopping Discount System SDS Groupon is an American worldwide ecommerce marketplace connecting millions of subscribers with local merchants by offering activities travel goods and services The Groupon is providing services which connects the suppliers and users with discount coupons of different products This project focused on analyzing the feedbacks from the suppliers based on the customers shopping behaviors and provides the best discount ranges of different coupons Our team worked on data ingestions from varieties data sources processing data with Hadoop related solutions and worked for designing statistical models for analyzing data with data science team Responsibilities Analyzed large data sets by running custom MapReduce and Hive queries Designed the HBase schemes based on the requirements Designed Cassandra schemes and connect it by Spark HBase data migration and validation Assisted in exporting analyzed data to relational databases using Sqoop Utilized Kafka and RabbitMQ to capture the data stream Processed data by using Spark Streaming and Kafka then stored the results in HBase Worked on the core and Spark SQL modules of Spark extensively Loaded the data from different source such as HDFS or HBase into Spark RDD and do in memory data computation to generate the output response Implemented POC to migrate MapReduce jobs into Spark RDD transformation using Scala IDE for IntelliJ Involved in converting HiveSQL queries into Spark transformations using Spark RDDs and Scala to improve the performance Developed predictive analytic using Apache Spark Scala APIs Scheduled workflow with Oozie Environment Hadoop 26 HDFS Spark 15 Spark SQL Spark Streaming Zookeeper 35 HBase 098 Hive 12 Kafka 13 RabbitMQ 344 Oozie 41 IntelliJ Scala UNIX Shell Scripting Senior Data Engineer Disney ABC Television Group Seattle WA September 2013 to December 2014 The DisneyABC Television Group is composed of The Walt Disney Companys global entertainment and news television properties owned television stations group as well as radio and publishing businesses The DisneyABC User Tracking Platform focused on development for the Hadoop related solutions and services to achieve better advertisement strategy We provide data processing and analytic solutions including streaming data ingestion RDBMS and log integration data transformationcleaning and data modeling Responsibilities Worked on Amazon EMR 4x with Agile methodology Developed Kafka consumer to receive and store real time data from Kafka to Amazon S3 Used Flume to collect aggregate and store web log data from different sources Used Sqoop to transfer data between RDBMS and HBase Extracted data from MongoDB through MongoDB Connector for Hadoop Involved in migrating of MapReduce programs into Spark using Spark and Scala Scheduled workflow with Oozie Use Spark with Scala and Spark SQL for testing and processing of data Worked with analytics team to build statistical model with MLlib and PySpark Worked with analytics team to prepare and visualize tables in Tableau for reporting Performed unit testing using JUnit and Pytest Used Git for version control JIRA for project tracking and Jenkins for continuous integration Environment Hadoop 26 Cloudera CDH 54 HDFS MapReduce HBase Sqoop Flume Zookeeper MongoDB Spark 14 Spark SQL Pyspark MLlib Tableau 92 JUnit Pytest Jr Data Engineer Starbucks Seattle WA June 2012 to August 2013 Starbucks Corporation is an American coffee company and coffeehouse chain The team contributes to detect mitigate and prevent loss and quickly and helps to easily identify fraudulent merchants to reduce risk Greater precision in search results and prioritization of the results is helping customers more quickly and easily evaluate a merchant without the cost of executing additional searches or further investigations Responsibilities Responsible for building scalable distributed data solutions using Hadoop Handled importing of data from various data sources performed transformations using Hive MapReduce loaded data into HDFS and Extracted the data from MySQL into HDFS using Sqoop Installed and configured Hive Pig Sqoop Flume and Oozie on the Hadoop cluster Developed simple to complex MapReduce jobs using Java and scripts using Hive and Pig Analyzed the data by performing Hive queries HiveQL and running Pig scripts Pig Latin for data ingestion and egress Implemented business logic by writing UDFs in Java and used various UDFs from other sources Experienced on loading and transforming of large sets of structured and semi structured data Managing and Reviewing Hadoop Log Files deploy and Maintaining Hadoop Cluster Export filtered data into HBase for fast query Environment Hadoop HBase Hive Pig Map Reduce Sqoop Oozie Eclipse Java Jr JavaJ2EEBig Data Developer UHGOptum Minnetonka MN July 2011 to May 2012 UnitedHealth Group Inc is an American forprofit managed health care company based in Minnetonka Minnesota It is sixth in the United States on the Fortune 500 UnitedHealth Group offers health care products and insurance services Responsibilities Developed Map Reduce programs in Java to help the analysis team to read write delete update and analyze the big data Developed a serious of Map Reduce algorithms to increase the performance Developed Web pages using Struts view component JSP JavaScript HTML jQuery AJAX to create the user interface views migration 3rd party applications Java Hibernate is used to access the database within the system Environment Eclipse Java Map Reduce Algorithm Hibernate Struts JavaScript JQuery CSS HTML MySQL and XML Jr Java Developer BBVA Compass Birmingham AL April 2009 to June 2011 This is an intranet application for stock screening This application is used by the customers and other client based users to build their query and send the result to the clients they cover The application has alerting capability which will alert users according to the criteria and alert level set for a stock in the form chosen The result set built is saved in downloadable viewable formats as preferred by the clients Responsibilities Involved in system design which is based on Spring Struts Hibernate framework Implemented the business logic in standalone Java classes using core Java Developed database MySQL applications Worked in Hibernate Template to access the MySQL database Involved in Unit testing of the components and created unit test cases and did unit test review Environment Eclipse MySQL Client 41 Spring HTML JavaScript Hibernate JSF Junit SDLC AgileScrum Education Bachelors Skills APACHE HADOOP OOZIE 6 years APACHE HADOOP SQOOP 6 years Hadoop 6 years HADOOP 6 years OOZIE 6 years Additional Information TECHNICAL SKILLS Hadoop Eco Systems Web Technologies Hadoop 270 MapReduce HBase 098 SOAP REST JSP 20 JavaScript 18 Spark 16 Hive 10 Pig 014 Kafka 12 Servlet 30 HTML 5 CSS 3 Sqoop 13 Flume 13 Impala 12 Oozie 30 Zookeeper 34 NoSQL Java Frameworks HBase 098 Cassandra 2 MangoDB 3 spring MVC Hibernate 3 4 Programming Languages Others Java Scala SQL SparkSQL JSON AVRO XML HiveQL PigLatin C C Python RabbitMQ 30",
    "unique_id": "a59c1dca-d302-4db8-a584-dcd1e72dd3b7"
}