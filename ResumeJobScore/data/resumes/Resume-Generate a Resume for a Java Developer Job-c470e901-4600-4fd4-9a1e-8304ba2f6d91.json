{
    "clean_data": "Hadoop Developer Hadoop span lDeveloperspan Hadoop Developer Dow Chemicals Pasadena CA Eight years of experience with emphasis on Big Data Technologies Design and Development of Java based enterprise applications Three years of experience in Hadoop Development and five years of java application development Experience in installation configuration supporting and managing Hadoop clusters Up to date on evaluating new analytical tools and projects coming up big data space like Apache Spark and Apache Shark Datameer Platfora etc Implemented in setting up standards and processes for Hadoop based application design and implementation Responsible for writing MapReduce programs Logical Implementation and interaction with HBase Developed MapReduce jobs to automate transfer of data from HBase Perform data analysis using Hive and Pig Load log data into HDFS using Flume Gained good knowledge on creating strategies on risky transactions Assist with the addition of Hadoop processing to the IT infrastructure Support development testing and operations teams during new system deployments Evaluate and propose new tools and technologies to meet the needs of the organization Experience in using Scoop ZooKeeper and Cloudera Manager Good Knowledge on Hadoop Cluster architecture and monitoring the cluster 247 operational support to production servers and related infrastructure clusters Experience in Administering Performance Monitoring and Finetuning of Linux Redhat Worked on debugging tools such as Dtrace Struss and Top Expert in setting up SSH SCP SFTP connectivity between UNIX hosts Authorized to work in the US for any employer Work Experience Hadoop Developer Dow Chemicals Philadelphia PA May 2018 to Present Philadelphia is the home for Dows Advanced Materials Division which is the umbrella for some of the companys most important highgrowth specialty businesses Five businesses make up the division Coatings Building and Construction Paper and Textiles Specialty Packaging and Films and Separations Technologies and Electronic Materials that serves the fastestgrowing end markets such as water electronics food pharmaceuticals health care paints and more Responsibilities Involved in review of functional and nonfunctional requirements Facilitated knowledge transfer sessions Importing and exporting data into HDFS and Hive using Sqoop Experienced in defining job flows Experienced in managing and reviewing Hadoop log files Extracted files from CouchDB through Sqoop and placed in HDFS and processed Experienced in running Hadoop streaming jobs to process terabytes of xml format data Load and transform large sets of structured semi structured and unstructured data Responsible to manage data coming from different sources Got good experience with NOSQL database Supported Map Reduce Programs those are running on the cluster Involved in loading data from UNIX file system to HDFS Installed and configured Hive and also written Hive UDFs Involved in creating Hive tables loading with data and writing hive queries which will run internally in map reduce way Gained very good business knowledge on health insurance claim processing fraud suspect identification appeals process etc Developed a custom File System plug in for Hadoop so it can access files on Data Platform This plugin allows Hadoop MapReduce programs HBase Pig and Hive to work unmodified and access files directly Designed and implemented Mapreducebased largescale parallel relationlearning system Extracted feeds form social media sites such as Facebook Twitter using Python scripts Setup and benchmarked HadoopHBase clusters for internal use Setup Hadoop cluster on Amazon EC2 using whirr for POC Environment Java Eclipse Oracle Sub Version Hadoop Hive HBase Linux MapReduce HDFS Hive Java Hadoop Distribution of HortonWorks MapReduce DataStax IBM DataStage 81 Oracle PLSQL SQLPLUS Toad Windows NT UNIX Shell Scripting Hadoop Developer Kraft Foods Chicago IL November 2016 to April 2018 Kraft Foods Inc has approximately 140 000 diverse employees around the world and is mainly involved in creating food products all around United States Responsibilities Worked on analyzing Hadoop cluster using different big data analytic tools including Pig Hive and MapReduce Collecting and aggregating large amounts of log data using Apache Flume and staging data in HDFS for further analysis Implemented nine nodes CDH3 Hadoop cluster on Red hat LINUX Involved in loading data from LINUX file system to HDFS Created HBase tables to store variable data formats of PII data coming from different portfolios Implemented a script to transmit sysprin information from Oracle to Hbase using Sqoop Implemented best income logic using Pig scripts and UDFs Worked on debugging performance tuning of Hive Pig Jobs Created Hbase tables to store various data formats of PII data coming from different portfolios Implemented test scripts to support test driven development and continuous integration Worked on tuning the performance Pig queries Involved in loading data from LINUX file system to HDFS Importing and exporting data into HDFS and Hive using Sqoop Experience working on processing unstructured data using Pig and Hive Supported MapReduce Programs those are running on the cluster Gained experience in managing and reviewing Hadoop log files Involved in scheduling Oozie workflow engine to run multiple Hive and pig jobs Created and maintained Technical documentation for launching HADOOP Clusters and for executing Hive queries and Pig Scripts Environment Hadoop HDFS Pig Hive MapReduce Sqoop Oozie Cloudera LINUX and Big Data Hadoop and Java Developer Sheffield Financial Salem Tamil Nadu February 2015 to October 2016 Sheffield Financial offers the simplest quickest and easiest retail finance program Sheffield Financial finances major brand equipment like Honda and Nissan Sheffield is headquartered in North Carolina and has an extensive independent dealer base throughout the US Dealer suite is a web application that helps dealers to register and enter their selling products Borrowers can enter credit applications and equipments to which they want to take the finance Responsibilities Worked with several clients with day to day requests and responsibilities InstalledConfiguredMaintained Apache Hadoop clusters for application development and Hadoop tools like Hive Pig HBase Zookeeper and Sqoop Involved in analyzing system failures identifying root causes and recommended course of actions Worked on Hive for exposing data for further analysis and for generating transforming files from different analytical formats to text files Wrote the shell scripts to monitor the health check of Hadoop daemon services and respond accordingly to any warning or failure conditions Managing and scheduling Jobs on a Hadoop cluster Implemented and maintained various projects in Java Utilized Java and MySQL from day to day to debug and fix issues with client processes Developed tested and implemented financialservices application to bring multiple clients into standard database format Assisted in designing building and maintaining database to analyze life cycle of checking and debit transactions Excellent JAVA J2EE application development skills with strong experience in Object Oriented Analysis Extensively involved throughout Software Development Life Cycle SDLC Strong experience of J2SE XML Web Services WSDL SOAP UDDI TCP IP Strong experience of software and system development using JSP Servlet Java Server Face EJB JDBC JNDI Struts Maven Trac Subversion JUnit SQL language Rich experience of database design and handson experience of large database systems Oracle 8i and Oracle 9i DB2 PL SQL Handson experience of Sun One Application Server Web logic Application Server Web Sphere Application Server Web Sphere Portal Server and J2EE application deployment technology Environment Hive Pig HBase Zookeeper Sqoop Cloudera Java JDBC JNDI Struts Maven Trac Subversion JUnit SQL language spring Hibernate Junit Oracle XML Altova XmlSpy Putty and Eclipse JavaJEE Architect developer Pfizer Global Research Development New York NY January 2013 to January 2015 Pfizer Inc is an American multinational pharmaceutical corporation headquartered in New York City and with its research headquarters in Groton Connecticut United States It is one of the worlds largest pharmaceutical company by revenuesPfizer develops and produces medicines and vaccines for a wide range of conditions including in the areas of immunology and inflammation oncology cardiovascular and metabolic diseases neuroscience and pain Responsibilities Architected a JSF Web sphere Oracle spring and Hibernate based 24x7 Web application Built an end to end vertical slice for a JEE based billing application using popular frameworks like Spring Hibernate JSF Facelets XHTML Maven2 and Ajax by applying OO design concepts JEE GoF design patterns and best practices Integrated other subsystems like loans application equity markets online application system and documentation system with the structured products application through JMS Websphere MQ SOAP based Web services and XML Designed the logical and physical data model generated DDL scripts and wrote DML scripts for Oracle 9i database Tuned SQL statements Hibernate mapping and Websphere application server to improve performance and consequently met the SLAs Gathered business requirements and wrote functional specifications and detailed design documents Improved the build process by migrating it from Ant to Maven2 Built and deployed Java applications into multiple Unix based environments and produced both unit and functional test results along with release notes Environment Java 15 JSF Sun RI Facelets Ajax4JSF Richfaces Spring XML XSL XSD XHTML Hibernate Oracle 9i PLSQL MINA Springws SOAP Web service Websphere Oracle JMX ANT Maven2 Continuum JUnit SVN TDD and XP Java developer Apollo Healthcare Hyderabad Telangana September 2011 to December 2012 The Apollo PathLinks system is a patientcentered processoriented Webhost or selfhost information system that allows you to streamline your business processes by choosing from integrated modules that facilitate clinical financial and enterprise management You can start with a few applications and add others as requirement or implement the entire system for a total solution Responsibilities Developed Admission Census module which monitors a wide range of detailed information for each resident upon preadmission or admission to your facility Involved in development of Care Plans module which provides a comprehensive library of problems goals and approaches You have the option of tailoring adding deleting or editing problems goals and approaches these libraries and the disciplines you will use for your care plans Involved in development of General Ledger module which streamlines analysis reporting and recording of accounting information General Ledger automatically integrates with a powerful spreadsheet solution for budgeting comparative analysis and tracking facility information for flexible reporting Developed UI using HTML JavaScript and JSP and developed Business Logic and Interfacing components using Business Objects XML and JDBC Designed userinterface and checking validations using JavaScript Managed connectivity using JDBC for queryinginserting data management including triggers and stored procedures Developed various EJBs for handling business logic and data manipulations from database Involved in design of JSPs and Servlets for navigation among the modules Designed cascading style sheets and XML part of Order entry Module Product Search Module and did client side validations with java script Environment J2EE JavaJDK JDBC JSP Servlets JavaScript EJB JNDI JavaBeans XML XSLT Oracle 9i Eclipse HTML DHTML SVN Education Bachelors Skills ORACLE 7 years JAVA 6 years XML 6 years SQL 5 years APACHE HADOOP SQOOP 4 years Additional Information Technical Skills Programming Languages Java C SQL PIG PL SQL Java Technologies Java JAXP AJAX JFC Swing Log4j Java Help API J2EE Technologies JSP Servlets JDBC JNDI XML JAXP Java Beans Methodologies Agile UML Design Patterns Core J2EE Frame Works Jakarta Struts JUnit and JTest LDAP Databases Oracle NO SQL HBase MY SQL MS SQL server Application Server Apache Tomcat Jboss IDEs Utilities Eclipse and JCreator NetBeans Web Dev Technologies HTML Java Script XML DTD XSL XSLT XPath DOM XQuery Protocols TCPIP HTTP and HTTPS Operating Systems Linux MacOS WINDOWS Hadoop ecosystem Hadoop and MapReduce Spark Sqoop Hive PIG HBASE HDFS Flume Hue Zookeeper Lucene Sun Grid Engine Administration",
    "entities": [
        "OO",
        "Oracle 9i DB2",
        "MapReduce Spark",
        "Additional Information Technical Skills Programming Languages Java C",
        "HDFS",
        "UNIX",
        "Hadoop Developer Hadoop",
        "Spring Hibernate JSF Facelets XHTML Maven2",
        "Big Data Hadoop",
        "MapReduce Collecting",
        "PL SQL Handson",
        "Sun One Application",
        "Present Philadelphia",
        "Hadoop",
        "XML",
        "NOSQL",
        "Pfizer Inc",
        "XP Java",
        "Application Server Web Sphere Application",
        "Hive Pig Jobs Created Hbase",
        "Apache Spark",
        "File System",
        "Amazon",
        "HBase Pig",
        "HBase Perform",
        "Developed",
        "Oracle 8i",
        "Apollo Healthcare Hyderabad",
        "Responsibilities Involved",
        "Hadoop MapReduce",
        "Application Server Apache Tomcat Jboss IDEs Utilities",
        "DDL",
        "Setup Hadoop",
        "Kraft Foods Inc",
        "Coatings Building and Construction Paper",
        "Oracle 9i",
        "Pig Scripts Environment Hadoop HDFS Pig Hive MapReduce Sqoop",
        "JSP",
        "Evaluate",
        "Sheffield Financial",
        "Apollo PathLinks",
        "java application development Experience",
        "Hive Pig HBase Zookeeper",
        "Hibernate Junit Oracle XML",
        "Business Logic and Interfacing",
        "HDFS Created HBase",
        "Honda",
        "US",
        "Sqoop",
        "Work Experience Hadoop Developer Dow Chemicals",
        "LINUX",
        "Created",
        "JDBC JNDI XML",
        "Websphere",
        "Big Data Technologies Design and Development of Java",
        "JMS Websphere",
        "HadoopHBase",
        "CDH3 Hadoop",
        "log data",
        "IP Strong",
        "the US Dealer",
        "SQL",
        "Facilitated",
        "HBase Developed MapReduce",
        "DML",
        "HADOOP Clusters",
        "Maven Trac Subversion",
        "General Ledger",
        "Hive",
        "SQL MS",
        "Business Objects XML",
        "Nissan Sheffield",
        "Connecticut",
        "Object Oriented Analysis",
        "JTest LDAP Databases Oracle",
        "PII",
        "Kraft Foods",
        "Pfizer Global Research Development New York",
        "Order",
        "XSLT",
        "InstalledConfiguredMaintained Apache Hadoop",
        "Dows Advanced Materials Division",
        "New York City",
        "JEE GoF",
        "North Carolina",
        "Java Developer Sheffield Financial",
        "Responsibilities Developed Admission Census",
        "Administering Performance Monitoring and Finetuning of Linux Redhat Worked",
        "HTTPS",
        "MapReduce",
        "Oracle to Hbase",
        "JEE",
        "Developed UI",
        "Software Development Life Cycle",
        "Websphere Oracle JMX ANT Maven2",
        "Hadoop Development",
        "Groton"
    ],
    "experience": "Experience in installation configuration supporting and managing Hadoop clusters Up to date on evaluating new analytical tools and projects coming up big data space like Apache Spark and Apache Shark Datameer Platfora etc Implemented in setting up standards and processes for Hadoop based application design and implementation Responsible for writing MapReduce programs Logical Implementation and interaction with HBase Developed MapReduce jobs to automate transfer of data from HBase Perform data analysis using Hive and Pig Load log data into HDFS using Flume Gained good knowledge on creating strategies on risky transactions Assist with the addition of Hadoop processing to the IT infrastructure Support development testing and operations teams during new system deployments Evaluate and propose new tools and technologies to meet the needs of the organization Experience in using Scoop ZooKeeper and Cloudera Manager Good Knowledge on Hadoop Cluster architecture and monitoring the cluster 247 operational support to production servers and related infrastructure clusters Experience in Administering Performance Monitoring and Finetuning of Linux Redhat Worked on debugging tools such as Dtrace Struss and Top Expert in setting up SSH SCP SFTP connectivity between UNIX hosts Authorized to work in the US for any employer Work Experience Hadoop Developer Dow Chemicals Philadelphia PA May 2018 to Present Philadelphia is the home for Dows Advanced Materials Division which is the umbrella for some of the companys most important highgrowth specialty businesses Five businesses make up the division Coatings Building and Construction Paper and Textiles Specialty Packaging and Films and Separations Technologies and Electronic Materials that serves the fastestgrowing end markets such as water electronics food pharmaceuticals health care paints and more Responsibilities Involved in review of functional and nonfunctional requirements Facilitated knowledge transfer sessions Importing and exporting data into HDFS and Hive using Sqoop Experienced in defining job flows Experienced in managing and reviewing Hadoop log files Extracted files from CouchDB through Sqoop and placed in HDFS and processed Experienced in running Hadoop streaming jobs to process terabytes of xml format data Load and transform large sets of structured semi structured and unstructured data Responsible to manage data coming from different sources Got good experience with NOSQL database Supported Map Reduce Programs those are running on the cluster Involved in loading data from UNIX file system to HDFS Installed and configured Hive and also written Hive UDFs Involved in creating Hive tables loading with data and writing hive queries which will run internally in map reduce way Gained very good business knowledge on health insurance claim processing fraud suspect identification appeals process etc Developed a custom File System plug in for Hadoop so it can access files on Data Platform This plugin allows Hadoop MapReduce programs HBase Pig and Hive to work unmodified and access files directly Designed and implemented Mapreducebased largescale parallel relationlearning system Extracted feeds form social media sites such as Facebook Twitter using Python scripts Setup and benchmarked HadoopHBase clusters for internal use Setup Hadoop cluster on Amazon EC2 using whirr for POC Environment Java Eclipse Oracle Sub Version Hadoop Hive HBase Linux MapReduce HDFS Hive Java Hadoop Distribution of HortonWorks MapReduce DataStax IBM DataStage 81 Oracle PLSQL SQLPLUS Toad Windows NT UNIX Shell Scripting Hadoop Developer Kraft Foods Chicago IL November 2016 to April 2018 Kraft Foods Inc has approximately 140 000 diverse employees around the world and is mainly involved in creating food products all around United States Responsibilities Worked on analyzing Hadoop cluster using different big data analytic tools including Pig Hive and MapReduce Collecting and aggregating large amounts of log data using Apache Flume and staging data in HDFS for further analysis Implemented nine nodes CDH3 Hadoop cluster on Red hat LINUX Involved in loading data from LINUX file system to HDFS Created HBase tables to store variable data formats of PII data coming from different portfolios Implemented a script to transmit sysprin information from Oracle to Hbase using Sqoop Implemented best income logic using Pig scripts and UDFs Worked on debugging performance tuning of Hive Pig Jobs Created Hbase tables to store various data formats of PII data coming from different portfolios Implemented test scripts to support test driven development and continuous integration Worked on tuning the performance Pig queries Involved in loading data from LINUX file system to HDFS Importing and exporting data into HDFS and Hive using Sqoop Experience working on processing unstructured data using Pig and Hive Supported MapReduce Programs those are running on the cluster Gained experience in managing and reviewing Hadoop log files Involved in scheduling Oozie workflow engine to run multiple Hive and pig jobs Created and maintained Technical documentation for launching HADOOP Clusters and for executing Hive queries and Pig Scripts Environment Hadoop HDFS Pig Hive MapReduce Sqoop Oozie Cloudera LINUX and Big Data Hadoop and Java Developer Sheffield Financial Salem Tamil Nadu February 2015 to October 2016 Sheffield Financial offers the simplest quickest and easiest retail finance program Sheffield Financial finances major brand equipment like Honda and Nissan Sheffield is headquartered in North Carolina and has an extensive independent dealer base throughout the US Dealer suite is a web application that helps dealers to register and enter their selling products Borrowers can enter credit applications and equipments to which they want to take the finance Responsibilities Worked with several clients with day to day requests and responsibilities InstalledConfiguredMaintained Apache Hadoop clusters for application development and Hadoop tools like Hive Pig HBase Zookeeper and Sqoop Involved in analyzing system failures identifying root causes and recommended course of actions Worked on Hive for exposing data for further analysis and for generating transforming files from different analytical formats to text files Wrote the shell scripts to monitor the health check of Hadoop daemon services and respond accordingly to any warning or failure conditions Managing and scheduling Jobs on a Hadoop cluster Implemented and maintained various projects in Java Utilized Java and MySQL from day to day to debug and fix issues with client processes Developed tested and implemented financialservices application to bring multiple clients into standard database format Assisted in designing building and maintaining database to analyze life cycle of checking and debit transactions Excellent JAVA J2EE application development skills with strong experience in Object Oriented Analysis Extensively involved throughout Software Development Life Cycle SDLC Strong experience of J2SE XML Web Services WSDL SOAP UDDI TCP IP Strong experience of software and system development using JSP Servlet Java Server Face EJB JDBC JNDI Struts Maven Trac Subversion JUnit SQL language Rich experience of database design and handson experience of large database systems Oracle 8i and Oracle 9i DB2 PL SQL Handson experience of Sun One Application Server Web logic Application Server Web Sphere Application Server Web Sphere Portal Server and J2EE application deployment technology Environment Hive Pig HBase Zookeeper Sqoop Cloudera Java JDBC JNDI Struts Maven Trac Subversion JUnit SQL language spring Hibernate Junit Oracle XML Altova XmlSpy Putty and Eclipse JavaJEE Architect developer Pfizer Global Research Development New York NY January 2013 to January 2015 Pfizer Inc is an American multinational pharmaceutical corporation headquartered in New York City and with its research headquarters in Groton Connecticut United States It is one of the worlds largest pharmaceutical company by revenuesPfizer develops and produces medicines and vaccines for a wide range of conditions including in the areas of immunology and inflammation oncology cardiovascular and metabolic diseases neuroscience and pain Responsibilities Architected a JSF Web sphere Oracle spring and Hibernate based 24x7 Web application Built an end to end vertical slice for a JEE based billing application using popular frameworks like Spring Hibernate JSF Facelets XHTML Maven2 and Ajax by applying OO design concepts JEE GoF design patterns and best practices Integrated other subsystems like loans application equity markets online application system and documentation system with the structured products application through JMS Websphere MQ SOAP based Web services and XML Designed the logical and physical data model generated DDL scripts and wrote DML scripts for Oracle 9i database Tuned SQL statements Hibernate mapping and Websphere application server to improve performance and consequently met the SLAs Gathered business requirements and wrote functional specifications and detailed design documents Improved the build process by migrating it from Ant to Maven2 Built and deployed Java applications into multiple Unix based environments and produced both unit and functional test results along with release notes Environment Java 15 JSF Sun RI Facelets Ajax4JSF Richfaces Spring XML XSL XSD XHTML Hibernate Oracle 9i PLSQL MINA Springws SOAP Web service Websphere Oracle JMX ANT Maven2 Continuum JUnit SVN TDD and XP Java developer Apollo Healthcare Hyderabad Telangana September 2011 to December 2012 The Apollo PathLinks system is a patientcentered processoriented Webhost or selfhost information system that allows you to streamline your business processes by choosing from integrated modules that facilitate clinical financial and enterprise management You can start with a few applications and add others as requirement or implement the entire system for a total solution Responsibilities Developed Admission Census module which monitors a wide range of detailed information for each resident upon preadmission or admission to your facility Involved in development of Care Plans module which provides a comprehensive library of problems goals and approaches You have the option of tailoring adding deleting or editing problems goals and approaches these libraries and the disciplines you will use for your care plans Involved in development of General Ledger module which streamlines analysis reporting and recording of accounting information General Ledger automatically integrates with a powerful spreadsheet solution for budgeting comparative analysis and tracking facility information for flexible reporting Developed UI using HTML JavaScript and JSP and developed Business Logic and Interfacing components using Business Objects XML and JDBC Designed userinterface and checking validations using JavaScript Managed connectivity using JDBC for queryinginserting data management including triggers and stored procedures Developed various EJBs for handling business logic and data manipulations from database Involved in design of JSPs and Servlets for navigation among the modules Designed cascading style sheets and XML part of Order entry Module Product Search Module and did client side validations with java script Environment J2EE JavaJDK JDBC JSP Servlets JavaScript EJB JNDI JavaBeans XML XSLT Oracle 9i Eclipse HTML DHTML SVN Education Bachelors Skills ORACLE 7 years JAVA 6 years XML 6 years SQL 5 years APACHE HADOOP SQOOP 4 years Additional Information Technical Skills Programming Languages Java C SQL PIG PL SQL Java Technologies Java JAXP AJAX JFC Swing Log4j Java Help API J2EE Technologies JSP Servlets JDBC JNDI XML JAXP Java Beans Methodologies Agile UML Design Patterns Core J2EE Frame Works Jakarta Struts JUnit and JTest LDAP Databases Oracle NO SQL HBase MY SQL MS SQL server Application Server Apache Tomcat Jboss IDEs Utilities Eclipse and JCreator NetBeans Web Dev Technologies HTML Java Script XML DTD XSL XSLT XPath DOM XQuery Protocols TCPIP HTTP and HTTPS Operating Systems Linux MacOS WINDOWS Hadoop ecosystem Hadoop and MapReduce Spark Sqoop Hive PIG HBASE HDFS Flume Hue Zookeeper Lucene Sun Grid Engine Administration",
    "extracted_keywords": [
        "Hadoop",
        "Developer",
        "Hadoop",
        "span",
        "lDeveloperspan",
        "Hadoop",
        "Developer",
        "Dow",
        "Chemicals",
        "Pasadena",
        "CA",
        "years",
        "experience",
        "emphasis",
        "Big",
        "Data",
        "Technologies",
        "Design",
        "Development",
        "Java",
        "enterprise",
        "applications",
        "years",
        "experience",
        "Hadoop",
        "Development",
        "years",
        "application",
        "development",
        "Experience",
        "installation",
        "configuration",
        "Hadoop",
        "clusters",
        "date",
        "tools",
        "projects",
        "data",
        "space",
        "Apache",
        "Spark",
        "Apache",
        "Shark",
        "Datameer",
        "Platfora",
        "standards",
        "processes",
        "Hadoop",
        "application",
        "design",
        "implementation",
        "MapReduce",
        "programs",
        "Implementation",
        "interaction",
        "HBase",
        "MapReduce",
        "jobs",
        "transfer",
        "data",
        "HBase",
        "Perform",
        "data",
        "analysis",
        "Hive",
        "Pig",
        "Load",
        "data",
        "HDFS",
        "Flume",
        "knowledge",
        "strategies",
        "transactions",
        "addition",
        "Hadoop",
        "processing",
        "IT",
        "infrastructure",
        "Support",
        "development",
        "testing",
        "operations",
        "teams",
        "system",
        "deployments",
        "tools",
        "technologies",
        "needs",
        "organization",
        "Experience",
        "Scoop",
        "ZooKeeper",
        "Cloudera",
        "Manager",
        "Good",
        "Knowledge",
        "Hadoop",
        "Cluster",
        "architecture",
        "cluster",
        "support",
        "production",
        "servers",
        "infrastructure",
        "clusters",
        "Experience",
        "Administering",
        "Performance",
        "Monitoring",
        "Finetuning",
        "Linux",
        "Redhat",
        "tools",
        "Dtrace",
        "Struss",
        "Top",
        "Expert",
        "SSH",
        "SCP",
        "SFTP",
        "connectivity",
        "UNIX",
        "hosts",
        "US",
        "employer",
        "Work",
        "Experience",
        "Hadoop",
        "Developer",
        "Dow",
        "Chemicals",
        "Philadelphia",
        "PA",
        "May",
        "Present",
        "Philadelphia",
        "home",
        "Dows",
        "Advanced",
        "Materials",
        "Division",
        "umbrella",
        "companys",
        "highgrowth",
        "specialty",
        "businesses",
        "businesses",
        "division",
        "Coatings",
        "Building",
        "Construction",
        "Paper",
        "Textiles",
        "Specialty",
        "Packaging",
        "Films",
        "Separations",
        "Technologies",
        "Electronic",
        "Materials",
        "end",
        "markets",
        "water",
        "electronics",
        "food",
        "pharmaceuticals",
        "health",
        "care",
        "paints",
        "Responsibilities",
        "review",
        "requirements",
        "knowledge",
        "transfer",
        "sessions",
        "data",
        "HDFS",
        "Hive",
        "Sqoop",
        "Experienced",
        "job",
        "flows",
        "Hadoop",
        "log",
        "files",
        "CouchDB",
        "Sqoop",
        "HDFS",
        "Hadoop",
        "streaming",
        "jobs",
        "terabytes",
        "xml",
        "format",
        "data",
        "Load",
        "sets",
        "data",
        "data",
        "sources",
        "experience",
        "NOSQL",
        "database",
        "Supported",
        "Map",
        "Programs",
        "cluster",
        "loading",
        "data",
        "UNIX",
        "file",
        "system",
        "HDFS",
        "Installed",
        "Hive",
        "Hive",
        "UDFs",
        "Hive",
        "tables",
        "data",
        "hive",
        "queries",
        "map",
        "way",
        "business",
        "knowledge",
        "health",
        "insurance",
        "claim",
        "processing",
        "fraud",
        "suspect",
        "identification",
        "appeals",
        "process",
        "custom",
        "File",
        "System",
        "plug",
        "Hadoop",
        "files",
        "Data",
        "Platform",
        "plugin",
        "Hadoop",
        "MapReduce",
        "programs",
        "HBase",
        "Pig",
        "Hive",
        "access",
        "files",
        "Mapreducebased",
        "largescale",
        "system",
        "media",
        "sites",
        "Facebook",
        "Twitter",
        "Python",
        "scripts",
        "Setup",
        "HadoopHBase",
        "clusters",
        "use",
        "Setup",
        "Hadoop",
        "cluster",
        "Amazon",
        "EC2",
        "whirr",
        "POC",
        "Environment",
        "Java",
        "Eclipse",
        "Oracle",
        "Sub",
        "Version",
        "Hadoop",
        "Hive",
        "HBase",
        "Linux",
        "MapReduce",
        "HDFS",
        "Hive",
        "Java",
        "Hadoop",
        "Distribution",
        "HortonWorks",
        "MapReduce",
        "DataStax",
        "IBM",
        "DataStage",
        "Oracle",
        "PLSQL",
        "SQLPLUS",
        "Toad",
        "Windows",
        "NT",
        "UNIX",
        "Shell",
        "Scripting",
        "Hadoop",
        "Developer",
        "Kraft",
        "Foods",
        "Chicago",
        "IL",
        "November",
        "April",
        "Kraft",
        "Foods",
        "Inc",
        "employees",
        "world",
        "food",
        "products",
        "United",
        "States",
        "Responsibilities",
        "Hadoop",
        "cluster",
        "data",
        "tools",
        "Pig",
        "Hive",
        "MapReduce",
        "Collecting",
        "amounts",
        "log",
        "data",
        "Apache",
        "Flume",
        "data",
        "HDFS",
        "analysis",
        "nodes",
        "CDH3",
        "Hadoop",
        "cluster",
        "hat",
        "LINUX",
        "loading",
        "data",
        "LINUX",
        "file",
        "system",
        "HDFS",
        "Created",
        "HBase",
        "data",
        "formats",
        "PII",
        "data",
        "portfolios",
        "script",
        "sysprin",
        "information",
        "Oracle",
        "Hbase",
        "Sqoop",
        "income",
        "logic",
        "Pig",
        "scripts",
        "UDFs",
        "performance",
        "tuning",
        "Hive",
        "Pig",
        "Jobs",
        "Created",
        "Hbase",
        "data",
        "formats",
        "PII",
        "data",
        "portfolios",
        "test",
        "scripts",
        "test",
        "development",
        "integration",
        "performance",
        "Pig",
        "queries",
        "loading",
        "data",
        "LINUX",
        "file",
        "system",
        "HDFS",
        "Importing",
        "data",
        "HDFS",
        "Hive",
        "Sqoop",
        "Experience",
        "data",
        "Pig",
        "Hive",
        "Supported",
        "MapReduce",
        "Programs",
        "cluster",
        "experience",
        "Hadoop",
        "log",
        "files",
        "Oozie",
        "workflow",
        "engine",
        "Hive",
        "pig",
        "jobs",
        "documentation",
        "HADOOP",
        "Clusters",
        "Hive",
        "queries",
        "Pig",
        "Scripts",
        "Environment",
        "Hadoop",
        "HDFS",
        "Pig",
        "Hive",
        "MapReduce",
        "Sqoop",
        "Oozie",
        "Cloudera",
        "LINUX",
        "Big",
        "Data",
        "Hadoop",
        "Java",
        "Developer",
        "Sheffield",
        "Financial",
        "Salem",
        "Tamil",
        "Nadu",
        "February",
        "October",
        "Sheffield",
        "Financial",
        "finance",
        "program",
        "Sheffield",
        "Financial",
        "brand",
        "equipment",
        "Honda",
        "Nissan",
        "Sheffield",
        "North",
        "Carolina",
        "dealer",
        "base",
        "US",
        "Dealer",
        "suite",
        "web",
        "application",
        "dealers",
        "selling",
        "products",
        "Borrowers",
        "credit",
        "applications",
        "equipments",
        "finance",
        "Responsibilities",
        "clients",
        "day",
        "day",
        "requests",
        "responsibilities",
        "Apache",
        "Hadoop",
        "clusters",
        "application",
        "development",
        "Hadoop",
        "tools",
        "Hive",
        "Pig",
        "HBase",
        "Zookeeper",
        "Sqoop",
        "system",
        "failures",
        "root",
        "causes",
        "course",
        "actions",
        "Hive",
        "data",
        "analysis",
        "transforming",
        "files",
        "formats",
        "text",
        "files",
        "shell",
        "scripts",
        "health",
        "check",
        "Hadoop",
        "daemon",
        "services",
        "warning",
        "failure",
        "conditions",
        "scheduling",
        "Jobs",
        "Hadoop",
        "cluster",
        "projects",
        "Java",
        "Utilized",
        "Java",
        "MySQL",
        "day",
        "day",
        "issues",
        "client",
        "processes",
        "financialservices",
        "application",
        "clients",
        "database",
        "format",
        "building",
        "database",
        "life",
        "cycle",
        "debit",
        "transactions",
        "Excellent",
        "JAVA",
        "J2EE",
        "application",
        "development",
        "skills",
        "experience",
        "Object",
        "Oriented",
        "Analysis",
        "Software",
        "Development",
        "Life",
        "Cycle",
        "SDLC",
        "experience",
        "J2SE",
        "XML",
        "Web",
        "Services",
        "WSDL",
        "SOAP",
        "UDDI",
        "IP",
        "Strong",
        "experience",
        "software",
        "system",
        "development",
        "JSP",
        "Servlet",
        "Java",
        "Server",
        "Face",
        "EJB",
        "JDBC",
        "JNDI",
        "Struts",
        "Maven",
        "Trac",
        "Subversion",
        "JUnit",
        "SQL",
        "language",
        "Rich",
        "experience",
        "database",
        "design",
        "handson",
        "experience",
        "database",
        "systems",
        "Oracle",
        "Oracle",
        "9i",
        "DB2",
        "PL",
        "SQL",
        "Handson",
        "experience",
        "Sun",
        "Application",
        "Server",
        "Web",
        "logic",
        "Application",
        "Server",
        "Web",
        "Sphere",
        "Application",
        "Server",
        "Web",
        "Sphere",
        "Portal",
        "Server",
        "J2EE",
        "application",
        "deployment",
        "technology",
        "Environment",
        "Hive",
        "Pig",
        "HBase",
        "Zookeeper",
        "Sqoop",
        "Cloudera",
        "Java",
        "JDBC",
        "JNDI",
        "Struts",
        "Maven",
        "Trac",
        "Subversion",
        "JUnit",
        "SQL",
        "language",
        "spring",
        "Hibernate",
        "Junit",
        "Oracle",
        "XML",
        "Altova",
        "XmlSpy",
        "Putty",
        "Eclipse",
        "JavaJEE",
        "Architect",
        "developer",
        "Pfizer",
        "Global",
        "Research",
        "Development",
        "New",
        "York",
        "NY",
        "January",
        "January",
        "Pfizer",
        "Inc",
        "pharmaceutical",
        "corporation",
        "New",
        "York",
        "City",
        "research",
        "headquarters",
        "Groton",
        "Connecticut",
        "United",
        "States",
        "worlds",
        "company",
        "revenuesPfizer",
        "medicines",
        "vaccines",
        "range",
        "conditions",
        "areas",
        "immunology",
        "inflammation",
        "oncology",
        "metabolic",
        "diseases",
        "neuroscience",
        "pain",
        "Responsibilities",
        "JSF",
        "Web",
        "sphere",
        "Oracle",
        "spring",
        "Hibernate",
        "Web",
        "application",
        "end",
        "slice",
        "JEE",
        "billing",
        "application",
        "frameworks",
        "Spring",
        "Hibernate",
        "JSF",
        "XHTML",
        "Maven2",
        "OO",
        "design",
        "concepts",
        "JEE",
        "GoF",
        "design",
        "patterns",
        "practices",
        "subsystems",
        "loans",
        "application",
        "equity",
        "markets",
        "application",
        "system",
        "documentation",
        "system",
        "products",
        "application",
        "JMS",
        "Websphere",
        "MQ",
        "SOAP",
        "Web",
        "services",
        "XML",
        "data",
        "model",
        "DDL",
        "scripts",
        "DML",
        "scripts",
        "Oracle",
        "9i",
        "database",
        "SQL",
        "Hibernate",
        "mapping",
        "Websphere",
        "application",
        "server",
        "performance",
        "SLAs",
        "Gathered",
        "business",
        "requirements",
        "specifications",
        "design",
        "documents",
        "build",
        "process",
        "Ant",
        "Maven2",
        "Java",
        "applications",
        "Unix",
        "environments",
        "unit",
        "test",
        "results",
        "release",
        "notes",
        "Environment",
        "Java",
        "JSF",
        "Sun",
        "RI",
        "Facelets",
        "Ajax4JSF",
        "Richfaces",
        "Spring",
        "XML",
        "XSL",
        "XSD",
        "XHTML",
        "Hibernate",
        "Oracle",
        "9i",
        "PLSQL",
        "MINA",
        "SOAP",
        "Web",
        "service",
        "Websphere",
        "Oracle",
        "JMX",
        "ANT",
        "Maven2",
        "Continuum",
        "JUnit",
        "SVN",
        "TDD",
        "XP",
        "Java",
        "developer",
        "Apollo",
        "Healthcare",
        "Hyderabad",
        "Telangana",
        "September",
        "December",
        "Apollo",
        "PathLinks",
        "system",
        "Webhost",
        "information",
        "system",
        "business",
        "processes",
        "modules",
        "enterprise",
        "management",
        "applications",
        "others",
        "requirement",
        "system",
        "solution",
        "Responsibilities",
        "Admission",
        "Census",
        "module",
        "range",
        "information",
        "resident",
        "preadmission",
        "admission",
        "facility",
        "development",
        "Care",
        "Plans",
        "module",
        "library",
        "problems",
        "goals",
        "approaches",
        "option",
        "problems",
        "goals",
        "approaches",
        "libraries",
        "disciplines",
        "care",
        "plans",
        "development",
        "General",
        "Ledger",
        "module",
        "analysis",
        "reporting",
        "recording",
        "accounting",
        "information",
        "General",
        "Ledger",
        "spreadsheet",
        "solution",
        "analysis",
        "tracking",
        "facility",
        "information",
        "Developed",
        "UI",
        "HTML",
        "JavaScript",
        "JSP",
        "Business",
        "Logic",
        "components",
        "Business",
        "Objects",
        "XML",
        "JDBC",
        "userinterface",
        "validations",
        "JavaScript",
        "Managed",
        "connectivity",
        "JDBC",
        "data",
        "management",
        "triggers",
        "procedures",
        "EJBs",
        "business",
        "logic",
        "data",
        "manipulations",
        "database",
        "design",
        "JSPs",
        "Servlets",
        "navigation",
        "modules",
        "style",
        "sheets",
        "XML",
        "part",
        "Order",
        "entry",
        "Module",
        "Product",
        "Search",
        "Module",
        "client",
        "side",
        "validations",
        "java",
        "script",
        "Environment",
        "J2EE",
        "JDBC",
        "JSP",
        "Servlets",
        "JavaScript",
        "EJB",
        "JNDI",
        "JavaBeans",
        "XML",
        "XSLT",
        "Oracle",
        "9i",
        "Eclipse",
        "HTML",
        "DHTML",
        "SVN",
        "Education",
        "Bachelors",
        "Skills",
        "ORACLE",
        "years",
        "years",
        "XML",
        "years",
        "SQL",
        "years",
        "APACHE",
        "HADOOP",
        "years",
        "Additional",
        "Information",
        "Technical",
        "Skills",
        "Programming",
        "Languages",
        "Java",
        "C",
        "SQL",
        "PIG",
        "PL",
        "SQL",
        "Java",
        "Technologies",
        "Java",
        "JAXP",
        "AJAX",
        "JFC",
        "Swing",
        "Log4j",
        "Java",
        "Help",
        "API",
        "J2EE",
        "Technologies",
        "JSP",
        "Servlets",
        "JDBC",
        "JNDI",
        "XML",
        "JAXP",
        "Java",
        "Beans",
        "Methodologies",
        "Agile",
        "UML",
        "Design",
        "Patterns",
        "Core",
        "J2EE",
        "Frame",
        "Jakarta",
        "Struts",
        "JUnit",
        "JTest",
        "LDAP",
        "Oracle",
        "SQL",
        "HBase",
        "MY",
        "SQL",
        "MS",
        "SQL",
        "server",
        "Application",
        "Server",
        "Apache",
        "Tomcat",
        "Jboss",
        "IDEs",
        "Utilities",
        "Eclipse",
        "JCreator",
        "NetBeans",
        "Web",
        "Dev",
        "Technologies",
        "HTML",
        "Java",
        "Script",
        "XML",
        "DTD",
        "XSL",
        "XSLT",
        "XPath",
        "DOM",
        "XQuery",
        "Protocols",
        "HTTP",
        "HTTPS",
        "Operating",
        "Systems",
        "Linux",
        "MacOS",
        "WINDOWS",
        "Hadoop",
        "ecosystem",
        "Hadoop",
        "MapReduce",
        "Spark",
        "Sqoop",
        "Hive",
        "PIG",
        "HBASE",
        "HDFS",
        "Flume",
        "Hue",
        "Zookeeper",
        "Lucene",
        "Sun",
        "Grid",
        "Engine",
        "Administration"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T20:15:26.520843",
    "resume_data": "Hadoop Developer Hadoop span lDeveloperspan Hadoop Developer Dow Chemicals Pasadena CA Eight years of experience with emphasis on Big Data Technologies Design and Development of Java based enterprise applications Three years of experience in Hadoop Development and five years of java application development Experience in installation configuration supporting and managing Hadoop clusters Up to date on evaluating new analytical tools and projects coming up big data space like Apache Spark and Apache Shark Datameer Platfora etc Implemented in setting up standards and processes for Hadoop based application design and implementation Responsible for writing MapReduce programs Logical Implementation and interaction with HBase Developed MapReduce jobs to automate transfer of data from HBase Perform data analysis using Hive and Pig Load log data into HDFS using Flume Gained good knowledge on creating strategies on risky transactions Assist with the addition of Hadoop processing to the IT infrastructure Support development testing and operations teams during new system deployments Evaluate and propose new tools and technologies to meet the needs of the organization Experience in using Scoop ZooKeeper and Cloudera Manager Good Knowledge on Hadoop Cluster architecture and monitoring the cluster 247 operational support to production servers and related infrastructure clusters Experience in Administering Performance Monitoring and Finetuning of Linux Redhat Worked on debugging tools such as Dtrace Struss and Top Expert in setting up SSH SCP SFTP connectivity between UNIX hosts Authorized to work in the US for any employer Work Experience Hadoop Developer Dow Chemicals Philadelphia PA May 2018 to Present Philadelphia is the home for Dows Advanced Materials Division which is the umbrella for some of the companys most important highgrowth specialty businesses Five businesses make up the division Coatings Building and Construction Paper and Textiles Specialty Packaging and Films and Separations Technologies and Electronic Materials that serves the fastestgrowing end markets such as water electronics food pharmaceuticals health care paints and more Responsibilities Involved in review of functional and nonfunctional requirements Facilitated knowledge transfer sessions Importing and exporting data into HDFS and Hive using Sqoop Experienced in defining job flows Experienced in managing and reviewing Hadoop log files Extracted files from CouchDB through Sqoop and placed in HDFS and processed Experienced in running Hadoop streaming jobs to process terabytes of xml format data Load and transform large sets of structured semi structured and unstructured data Responsible to manage data coming from different sources Got good experience with NOSQL database Supported Map Reduce Programs those are running on the cluster Involved in loading data from UNIX file system to HDFS Installed and configured Hive and also written Hive UDFs Involved in creating Hive tables loading with data and writing hive queries which will run internally in map reduce way Gained very good business knowledge on health insurance claim processing fraud suspect identification appeals process etc Developed a custom File System plug in for Hadoop so it can access files on Data Platform This plugin allows Hadoop MapReduce programs HBase Pig and Hive to work unmodified and access files directly Designed and implemented Mapreducebased largescale parallel relationlearning system Extracted feeds form social media sites such as Facebook Twitter using Python scripts Setup and benchmarked HadoopHBase clusters for internal use Setup Hadoop cluster on Amazon EC2 using whirr for POC Environment Java Eclipse Oracle Sub Version Hadoop Hive HBase Linux MapReduce HDFS Hive Java Hadoop Distribution of HortonWorks MapReduce DataStax IBM DataStage 81 Oracle PLSQL SQLPLUS Toad Windows NT UNIX Shell Scripting Hadoop Developer Kraft Foods Chicago IL November 2016 to April 2018 Kraft Foods Inc has approximately 140 000 diverse employees around the world and is mainly involved in creating food products all around United States Responsibilities Worked on analyzing Hadoop cluster using different big data analytic tools including Pig Hive and MapReduce Collecting and aggregating large amounts of log data using Apache Flume and staging data in HDFS for further analysis Implemented nine nodes CDH3 Hadoop cluster on Red hat LINUX Involved in loading data from LINUX file system to HDFS Created HBase tables to store variable data formats of PII data coming from different portfolios Implemented a script to transmit sysprin information from Oracle to Hbase using Sqoop Implemented best income logic using Pig scripts and UDFs Worked on debugging performance tuning of Hive Pig Jobs Created Hbase tables to store various data formats of PII data coming from different portfolios Implemented test scripts to support test driven development and continuous integration Worked on tuning the performance Pig queries Involved in loading data from LINUX file system to HDFS Importing and exporting data into HDFS and Hive using Sqoop Experience working on processing unstructured data using Pig and Hive Supported MapReduce Programs those are running on the cluster Gained experience in managing and reviewing Hadoop log files Involved in scheduling Oozie workflow engine to run multiple Hive and pig jobs Created and maintained Technical documentation for launching HADOOP Clusters and for executing Hive queries and Pig Scripts Environment Hadoop HDFS Pig Hive MapReduce Sqoop Oozie Cloudera LINUX and Big Data Hadoop and Java Developer Sheffield Financial Salem Tamil Nadu February 2015 to October 2016 Sheffield Financial offers the simplest quickest and easiest retail finance program Sheffield Financial finances major brand equipment like Honda and Nissan Sheffield is headquartered in North Carolina and has an extensive independent dealer base throughout the US Dealer suite is a web application that helps dealers to register and enter their selling products Borrowers can enter credit applications and equipments to which they want to take the finance Responsibilities Worked with several clients with day to day requests and responsibilities InstalledConfiguredMaintained Apache Hadoop clusters for application development and Hadoop tools like Hive Pig HBase Zookeeper and Sqoop Involved in analyzing system failures identifying root causes and recommended course of actions Worked on Hive for exposing data for further analysis and for generating transforming files from different analytical formats to text files Wrote the shell scripts to monitor the health check of Hadoop daemon services and respond accordingly to any warning or failure conditions Managing and scheduling Jobs on a Hadoop cluster Implemented and maintained various projects in Java Utilized Java and MySQL from day to day to debug and fix issues with client processes Developed tested and implemented financialservices application to bring multiple clients into standard database format Assisted in designing building and maintaining database to analyze life cycle of checking and debit transactions Excellent JAVA J2EE application development skills with strong experience in Object Oriented Analysis Extensively involved throughout Software Development Life Cycle SDLC Strong experience of J2SE XML Web Services WSDL SOAP UDDI TCP IP Strong experience of software and system development using JSP Servlet Java Server Face EJB JDBC JNDI Struts Maven Trac Subversion JUnit SQL language Rich experience of database design and handson experience of large database systems Oracle 8i and Oracle 9i DB2 PL SQL Handson experience of Sun One Application Server Web logic Application Server Web Sphere Application Server Web Sphere Portal Server and J2EE application deployment technology Environment Hive Pig HBase Zookeeper Sqoop Cloudera Java JDBC JNDI Struts Maven Trac Subversion JUnit SQL language spring Hibernate Junit Oracle XML Altova XmlSpy Putty and Eclipse JavaJEE Architect developer Pfizer Global Research Development New York NY January 2013 to January 2015 Pfizer Inc is an American multinational pharmaceutical corporation headquartered in New York City and with its research headquarters in Groton Connecticut United States It is one of the worlds largest pharmaceutical company by revenuesPfizer develops and produces medicines and vaccines for a wide range of conditions including in the areas of immunology and inflammation oncology cardiovascular and metabolic diseases neuroscience and pain Responsibilities Architected a JSF Web sphere Oracle spring and Hibernate based 24x7 Web application Built an end to end vertical slice for a JEE based billing application using popular frameworks like Spring Hibernate JSF Facelets XHTML Maven2 and Ajax by applying OO design concepts JEE GoF design patterns and best practices Integrated other subsystems like loans application equity markets online application system and documentation system with the structured products application through JMS Websphere MQ SOAP based Web services and XML Designed the logical and physical data model generated DDL scripts and wrote DML scripts for Oracle 9i database Tuned SQL statements Hibernate mapping and Websphere application server to improve performance and consequently met the SLAs Gathered business requirements and wrote functional specifications and detailed design documents Improved the build process by migrating it from Ant to Maven2 Built and deployed Java applications into multiple Unix based environments and produced both unit and functional test results along with release notes Environment Java 15 JSF Sun RI Facelets Ajax4JSF Richfaces Spring XML XSL XSD XHTML Hibernate Oracle 9i PLSQL MINA Springws SOAP Web service Websphere Oracle JMX ANT Maven2 Continuum JUnit SVN TDD and XP Java developer Apollo Healthcare Hyderabad Telangana September 2011 to December 2012 The Apollo PathLinks system is a patientcentered processoriented Webhost or selfhost information system that allows you to streamline your business processes by choosing from integrated modules that facilitate clinical financial and enterprise management You can start with a few applications and add others as requirement or implement the entire system for a total solution Responsibilities Developed Admission Census module which monitors a wide range of detailed information for each resident upon preadmission or admission to your facility Involved in development of Care Plans module which provides a comprehensive library of problems goals and approaches You have the option of tailoring adding deleting or editing problems goals and approaches these libraries and the disciplines you will use for your care plans Involved in development of General Ledger module which streamlines analysis reporting and recording of accounting information General Ledger automatically integrates with a powerful spreadsheet solution for budgeting comparative analysis and tracking facility information for flexible reporting Developed UI using HTML JavaScript and JSP and developed Business Logic and Interfacing components using Business Objects XML and JDBC Designed userinterface and checking validations using JavaScript Managed connectivity using JDBC for queryinginserting data management including triggers and stored procedures Developed various EJBs for handling business logic and data manipulations from database Involved in design of JSPs and Servlets for navigation among the modules Designed cascading style sheets and XML part of Order entry Module Product Search Module and did client side validations with java script Environment J2EE JavaJDK JDBC JSP Servlets JavaScript EJB JNDI JavaBeans XML XSLT Oracle 9i Eclipse HTML DHTML SVN Education Bachelors Skills ORACLE 7 years JAVA 6 years XML 6 years SQL 5 years APACHE HADOOP SQOOP 4 years Additional Information Technical Skills Programming Languages Java C SQL PIG PL SQL Java Technologies Java JAXP AJAX JFC Swing Log4j Java Help API J2EE Technologies JSP Servlets JDBC JNDI XML JAXP Java Beans Methodologies Agile UML Design Patterns Core J2EE Frame Works Jakarta Struts JUnit and JTest LDAP Databases Oracle NO SQL HBase MY SQL MS SQL server Application Server Apache Tomcat Jboss IDEs Utilities Eclipse and JCreator NetBeans Web Dev Technologies HTML Java Script XML DTD XSL XSLT XPath DOM XQuery Protocols TCPIP HTTP and HTTPS Operating Systems Linux MacOS WINDOWS Hadoop ecosystem Hadoop and MapReduce Spark Sqoop Hive PIG HBASE HDFS Flume Hue Zookeeper Lucene Sun Grid Engine Administration",
    "unique_id": "c470e901-4600-4fd4-9a1e-8304ba2f6d91"
}