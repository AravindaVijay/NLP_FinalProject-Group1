{
    "clean_data": "Hadoop Developer Hadoop span lDeveloperspan Hadoop Developer American Express Phoenix AZ Over 7 years of total professional experience in IT field involving project development implementation deployment and maintenance using Hadoop ecosystem related technologies with domain knowledge in Finance Banking Communication Insurance Retail Industry and Health care 4 years of hands on experience in Hadoop Ecosystem technologies like HDFS MapReduce Yarn Spark Hive Pig Oozie Sqoop Flume Zookeeper HBase Over all two years of hands on experience using Spark framework with Scala 3 years of Java programming experience in developing web based applications and Client Server technologies In depth understanding of Hadoop Architecture and its various components such as Job Tracker Task Tracker Name Node Data Node Resource Manager and MapReduce concepts Proficient knowledge on Apache Spark and Apache Storm to process real time data Extensive knowledge in programming with Resilient Distributed Datasets RDDs Good exposure to performance tuning hive queries mapreduce jobs spark jobs Worked with various formats of files like delimited text files click stream log files Apache log filesAvro files JSON files XML Files Experience on installation and configuration of spark standalone mode for testing and development environments Developed simple to complex MapReduce jobs using Java language Worked on live 60 nodes Hadoop cluster running on Cloudera CDH4 Extensive experience in analyzing data using Hive QL Pig Latin and custom MapReduce programs in Java Developed UDF UDAF UDTF functions for Hive and Pig Good knowledge of Partitions Bucketing concepts designed and managed them and created external tables in Hive in order to optimize performance Good experience in Avro files RC files Combiners Counters for best practices and performance improvements Good knowledge on Joins group and aggregation concepts and resolved performance issues in Hive and Pig scripts by implementing them Experience with Big Data ML toolkits such as Mahout and Spark ML Experience in job work flow scheduling and monitoring tools like Oozie and Zookeeper Worked on NoSQL databases including HBase Cassandra and Mongo DB Experience in importing data from a Relational database management system RDBMS such as MySql and Oracle into HDFS Hive and exported the processed data back into RDBMS using Sqoop Experience in importing data from RDBMS to HBase and exporting data into RDBMS using Sqoop Implemented Flume for collecting aggregating and moving large amount of server logs and streaming data to HDFS Experience in HBase cluster setup and implementation Done Administration installing upgrading and managing distributions of Cassandra Good knowledge in performance troubleshooting and tunning Cassandra clusters and understanding of Cassandra Data Modeling based on applications Experience in setting up Hadoop in Pseudo distributed environment Experience in setting up Hive Pig HBase and Sqoop in Ubuntu operating system Good knowledge on Software development life cycleSDLC Experience as Java Developer in Web Client Server technologies using Java J2EE Servlets JSP EJB Hibernate framework and Spring framwork Good understanding of Software Development Life CycleSDLC and sound knowledge of project implementation methodologies including Waterfall and Agile Authorized to work in the US for any employer Work Experience Hadoop Developer American Express Phoenix AZ October 2016 to Present Responsibilities Worked on analyzing Hadoop cluster using different big data analytic tools including Kafka Pig Hive and Map Reduce Collecting and aggregating large amounts of log data using Apache Flume and staging data in HDFS for further analysis Real time streaming the data using Spark with Kafka Configured Spark streaming to receive real time data from the Kafka and store the stream data to HDFS using Scala Worked within the Apache Hadoop framework utilizing Opinion Lab statistics to ingest the data from a streaming application program interface API automate processes by creating Oozie workflows and draw conclusions about consumer sentiment based on data patterns found through the use of Hive for external client use Wrote the Storm topology with HDFS Bolt and Hive Bolts as destinations Expertise in writing Storm topology development maintenance and bug fixes Developed Hadoop streaming MapReduce works using Java Worked on debugging performance tuning of Hive Pig Jobs Implemented test scripts to support test driven development and continuous integration Worked on tuning the performance of Pig queries Involved in loading data from Linux file system to HDFS Importing and exporting data into HDFS using Sqoop Good knowledge on building Apache spark applications using Scala Experience working on processing unstructured data using Pig Implemented Partitioning Dynamic Partitions Buckets in Hive Implemented Spark using Scala and SparkSQL for faster testing and processing of data Good knowledge with NoSQL databases like HBase Cassandra Handled Administration installing upgrading and managing distributions of Cassandra Advanced knowledge in performance troubleshooting and tuning Cassandra clusters Done Scaling Cassandra cluster based on lead patterns Good understanding of Cassandra Data Modeling based on applications Experience with Cassandra Performance tuning Highly involved in developmentimplementation of Cassandra environment Plan deploy monitor and maintain Amazon AWS cloud infrastructure consisting of multiple EC2 nodes and VMWare Vms as required in the environment Expertise in AWS data migration between different database platforms like SQL Server to Amazon Aurora using RDS tool Supported Map Reduce Programs those are running on the cluster Gained experience in managing and reviewing Hadoop log files Involved in scheduling Oozie workflow engine to run multiple pig jobs Responsible for developing data pipeline using flume Sqoop and Pig to extract the data from weblogs and store in HDFS Data scrubbing and processing with Oozie Developed Pig Latin scripts to extract data from the web server output files to load into HDFS Involved in developing Hive DDLs to create alter and drop tables Created and maintained technical documentation for launching Hadoop clusters and for executing Hive queries and Pig Scripts Also exploring with the Spark improving the performance and optimization of the existing algorithms in Hadoop using Spark Context Spark SQL Data Frame Pair RDDs Storm Spark YARN Used many features like Parallelize Partitioned Caching both inmemory and disk Serialization Kryo Serialization etc Environment Hadoop Cloudera Big Data HDFS MapReduce Sqoop Spark Hive HBase Linux Java Eclipse Hadoop Distribution of Cloudera PLSQL Toad 96 Windows NT MongoDB Cassandra Tableau Unix shell scripting Putty and Eclipse Hadoop Developer Barclays New York NY August 2015 to September 2016 Responsibilities Developed simple to complex MapReduce streaming jobs using Java language for processing and validating the data Developed data pipeline using MapReduce Flume Sqoop and Pig to ingest customer behavioral data into HDFS for analysis Developed MapReduce and Spark jobs to discover trends in data usage by users Implemented Spark using Python and Spark SQL for faster processing of data Implemented algorithms for real time analysis in Spark Used Spark for interactive queries processing of streaming data and integration with popular NoSQL database for huge volume of data Used the Spark Cassandra Connector to load data to and from Cassandra Real time streaming the data using Spark with Kafka Handled importing data from different data sources into HDFS using Sqoop and also performing transformations using Hive MapReduce and then loading data into HDFS Exported the analyzed data to the relational databases using Sqoop to further visualize and generate reports for the BI team Collecting and aggregating large amounts of log data using Flume and staging data in HDFS for further analysis Analyzed the data by performing Hive queries HiveQL and running Pig scripts Pig Latin to study customer behavior Used Hive to analyze the partitioned and bucketed data and compute various metrics for reporting Developed Pig Latin scripts to perform Map Reduce jobs Developed product profiles using Pig and commodity UDFs Developed Hive scripts in HiveQL to DeNormalize and Aggregate the data Created HBase tables and column families to store the user event data Written automated HBase test cases for data quality checks using HBase command line tools Created UDFs to store specialized data structures in HBase and Cassandra Scheduled and executed work flows in Oozie to run Hive and Pig jobs Used Impala to read write and query the Hadoop data in HDFS from HBase or Cassandra Used Tez framework for building high performance jobs in Pig and Hive Configured Kafka to read and write messages from external programs Configured Kafka to handle real time data Developed end to end data processing pipelines that begin with receiving data using distributed messaging systems Kafka through persistence of data into HBase Written Storm topology to emit data into Cassandra DB Written Storm topology to accept data from Kafka producer and process the data Continuous monitoring and managing the Hadoop cluster using Cloudera Manager Used JUnit framework to perform Unit testing of the application Developed interactive shell scripts for scheduling various data cleansing and data loading process Performed data validation on the data ingested using MapReduce by building a custom model to filter all the invalid data and cleanse the data Experience with data wrangling and creating workable datasets Developed schemas to handle reporting requirements using Jaspersoft Environment Hadoop MapReduce Spark Pig Hive Sqoop Oozie HBase Zookeeper Kafka Flume Solr Storm Tez Impala Mahout Cassandra Cloudera manager MySQL Jaspersoft Multinode cluster with LinuxUbuntu Windows Unix Hadoop Developer Ally Financial Auburn Hills MI November 2014 to July 2015 Responsibilities Installed and configured Hadoop Ecosystem components and Cloudera manager using CDH distribution Frequent interactions with Business partners Designed and developed a MedicareMedicaid claims system using Modeldriven architecture on a customized framework built on spring Moved data from HDFS to Cassandra using MapReduce and BulkOutputFormat class Imported trading and derivatives data in Hadoop Distributed File System and Eco System MapReduce Pig Hive Sqoop Involved in loading and transforming large sets of Structured SemiStructured and Unstructured data and analyzed them by running Hive queries and Pig scripts Created tables in HBase and loading data into HBase tables Developed scripts to load data from HBase to Hive Meta store and perform Map Reduce jobs Was part of an activity to setup Hadoop ecosystem at dev QA Environment Managed and reviewed Hadoop Log files Responsible writing Pig Script and Hive queries for data processing Running Sqoop for importing data from Oracle Other Database Creation of shell script to collect raw logs from different machines Created Partition in a Hive as static and dynamic Implemented Pig Latin scripts using operators such as LOAD STORE DUMP FILTER DISTINCT FOREACH GENERATE GROUP COGROUP ORDER LIMIT AND UNION Optimized the Hive tables using optimization techniques like partitions and bucketing to provide better performance with HiveQL queries Defined some Pig UDF for some financial functions such as swap hedging Speculation and arbitrage Coded many MapReduce program to process unstructured logs file Worked on import and export data into HDFS and Hive using Sqoop Used different data formats Text format and Avro format while loading the data into HDFS Used parameterize Pig script and optimized script using illustrate and explain Involved in the process of configuring HA Kerberos security issues and name node failure restoration activity time to time as a part of zero downtime Implemented FAIR Scheduler as well Environment Hadoop Linux MapReduce HDFS Hbase Hive Pig Shell Scripting Sqoop CDH Distribution Windows Linux Java 6 Eclipse Ant Log4j and JUnit Hadoop Developer Hitachi America Chula Vista CA May 2014 to October 2014 Responsibilities Hands on Experience in joining raw data with the reference data using Pig scripting Written custom UDFs in Hive Hands on experience in extracting data from different databases and to copy into HDFS file system using Sqoop Written Sqoop incremental import job to move newupdated info From database to HDFS Created Oozie coordinated workflow to execute Sqoop incremental job daily Used Oozie workflow engine to run multiple Hive and Pig jobs Hands on experience in exporting the results into relational databases using Sqoop for visualization and to generate reports for the BI team Involved in Installing and configuring Hive Pig Sqoop Flume and Oozie on the Hadoop cluster Worked with application teams to install Operating System Hadoop updates patches versions upgrades as required Working with clients on requirements based on their business needs Communicate deliverables status to userstakeholders client and drive periodic review meetings On time completion of tasks and the projects per quality goals Environment Hadoop HDFS MapReduce Hive Pig Sqoop HBase Oozie MySql SVN Putty Zookeeper Ganglia UNIX and Shell scripting Hadoop Developer Florida Power Light Miami FL October 2013 to April 2014 Responsibilities Integrated Kafka with Storm for real time data processing and written some storm topologies to store the processed data directly to MongoDB and HDFS Experience in writing Spark SQL scripts Imported data from different sources into Spark RDD for processing Developed custom aggregate functions using Spark SQL and performed interactive querying Involved in loading data from edge node to HDFS using shell scripting Worked on installing cluster commissioning and decommissioning of Datanode Namenode high availability capacity planning and slots configuration Completion of unit testing for the new Hadoop jobs in standalone mode designated for Unit region using MR Unit Developed Spark scripts by using Scala and Python shell commands as per the requirement Experience in managing and reviewing Hadoop log files Experience in Hive partitioning bucketing and perform joins on Hive tables and implementing Hive SerDe like REGEX JSON and Avro Optimized Hive analytics Sql queries created tablesviews written custom UDFs and Hive based exception processing Involved in transforming the Teradata to legacy lables to HDFS and HBase tables using Sqoop and vice versa Configured Fair Scheduler to provide fair resources to all the applications across the cluster Environment Hortonworks Hadoop Ambari Spark Solr Kafka MongoDB Linux HDFS Hive Pig Sqoop Flume Zookeeper RDBMS JavaJ2EE Developer Mercury Insurance Roseville CA November 2012 to September 2013 Responsibilities Write design document based on requirements from MMSEA user guide Performed requirement gathering design coding testing implementation and deployment Worked on modeling of Dialog process Business Processes and coding Business Objects QueryMapper and JUnit files Involved in the design and creation of Class diagrams Sequence diagrams and Activity Diagrams using UML models Created the Business Objects methods using Java and integrating the activity diagrams Involved in developing JSP pages using Struts custom tags jQuery and Tiles Framework Used JavaScript to perform client side validations and StrutsValidator Framework for serverside validation Worked in web services using SOAP WSDL Wrote Query Mappers and MQ Experience in JUnit Test Cases Developed the UI using XSL and JavaScript Managed software configuration using ClearCase and SVN Design develop and test features and enhancements Performed error rate analysis of production issues and technical errors Developed test environment for testing all the Web Service exposed as part of the core module and their integration with partner services in Integration test Analyze user requirement document and develop test plan which includes test objectives test strategies test environment and test priorities Responsible for performing endtoend system testing of application writing JUnit test cases Perform Functional testing Performance testing Integration testing Regression testing Smoke testing and User Acceptance Testing UAT Converted Complex SQL queries running at mainframes into pig and Hive as a part of a migration from mainframes into Hadoop cluster Environment Shell Scripting Java 6 JEE Spring Hibernate Eclipse Oracle 10g JavaScript Servlets Nodejs JMS Ant Log4j and Junit Hadoop Pig Hive Java Developer Reliance Energy LTD June 2010 to August 2012 Responsibilities Involved in the design and implementation of the architecture for the project using OOAD UML design patterns Involved in design and development of server side layer using XML JSP JDBC JNDI EJB and DAO patterns using eclipse IDE Work involved extensive usage of HTML CSS Javascript and Ajax for client side development and validations Used parsers for the conversion of XML files to java objects and vice versa Developed screens using XML documents and XSL Developed Client programs for consuming the Web services published by the Country Defaults Department which keeps in track of the information regarding life span inflation rates retirement age etc using Apache Axis Developed Java Beans and JSPs by using spring and JSTL tag libs for supplements Development of EJBs Servlets and JSP files for implementing Business rules and Security options using IBM Web Sphere Involved in creating tables stored procedures in SQL for data manipulation and retrieval using SQL Server Oracle and DB2 Trained end users on developed application Environment Java JSF Framework Eclipse IDE Ajax Apache Axis OOAD Web Logic Java script HTML XML CSS SQL Server Oracle Web services Ajax Spring OOAD and UML Windows Education Bachelors Skills JAVA 6 years SQL 6 years Hadoop 5 years HADOOP 5 years Hive 5 years Hadoop Cloudera Big Data HDFS MapReduce Sqoop Spark Hive HBase Linux Java Eclipse Hadoop Distribution of Cloudera PLSQL Toad 96 Windows NT MongoDB Cassandra Tableau Unix shell scripting Putty and Eclipse 8 years Additional Information TECHNICAL SKILL Hadoop Ecosystem Hadoop HDFS YARN MapReduce Hive Pig Oozie Zookeeper Flume Sqoop Spark Mahout Kafka Storm Scala Spark ML Impala HBase Tez Languages C C Core Java PLSQL Scala JavaJ2EE Technologies Servlets JSP JDBC Java Beans EJB RMI Web Services Frameworks EJB Struts Hibernate and Spring Scripting Languages Python R SQL Unix Shell Scripting Hive QL Pig Latin Databases MySQL Oracle 10g SQL Server 2008 NoSQL Databases HBase Cassandra MongoDB Web Technologies HTML CSS XML Javascript Ajax Nodejs SOAP WebApplication Servers WebLogic WebSphere Apache Tomcat Development Tools Eclipse Ant Putty Version Control SVN GIT Cluster Management and Monitoring Tools Ganglia Ambari Visualization Tools Tableau Methodologies AgileScrum Rational Unified Process and Waterfall Environment Win 9598 Win NT Win XP Win 7 Unix LinuxUbuntu and CentOS",
    "entities": [
        "LinuxUbuntu Windows Unix Hadoop Developer Ally Financial Auburn Hills",
        "Implemented Spark",
        "Ant Putty Version",
        "Installing",
        "Done Administration",
        "Avro Optimized Hive",
        "Aggregate",
        "Relational",
        "BI",
        "HDFS",
        "Hadoop Developer Hadoop",
        "GIT Cluster Management",
        "Partitions Bucketing",
        "Working",
        "IBM",
        "Hadoop Ecosystem",
        "Created Partition",
        "Developed Hadoop",
        "Hadoop",
        "HDFS Involved",
        "Structured SemiStructured",
        "XML",
        "Cassandra using MapReduce",
        "Activity Diagrams",
        "Hadoop Distributed File System",
        "JUnit",
        "Phoenix AZ",
        "ClearCase",
        "UML Windows Education",
        "Shell",
        "HBase Cassandra",
        "HBase",
        "Parallelize Partitioned Caching",
        "Finance Banking Communication Insurance Retail Industry and Health",
        "Apache Spark",
        "Work Experience Hadoop Developer American Express",
        "Created the Business Objects",
        "Amazon",
        "jQuery and Tiles Framework Used",
        "Spark ML",
        "Hive Meta",
        "User Acceptance Testing UAT Converted Complex",
        "the Country Defaults Department",
        "HTML CSS Javascript",
        "Python",
        "SQL Server",
        "SparkSQL",
        "Developed",
        "DAO",
        "HDFS Created Oozie",
        "JSTL",
        "HDFS Bolt and Hive Bolts",
        "Hadoop Log",
        "UML",
        "Waterfall",
        "OOAD UML",
        "Perform Functional",
        "IDE Work",
        "Communicate",
        "Hive Pig HBase",
        "Oozie Developed",
        "HBase Written Storm",
        "Linux",
        "JSP",
        "Unstructured",
        "Big Data ML",
        "Cloudera CDH4 Extensive",
        "RDS",
        "Cassandra DB Written Storm",
        "HBase Cassandra Handled Administration",
        "Spark",
        "Mahout",
        "Running Sqoop",
        "Datanode Namenode",
        "CDH",
        "API",
        "Spark Context Spark",
        "US",
        "Sqoop",
        "Additional Information TECHNICAL SKILL Hadoop Ecosystem Hadoop",
        "Storm",
        "Created",
        "Scala",
        "Hadoop Architecture",
        "Hive Implemented Spark",
        "Coded",
        "UNION Optimized",
        "the Apache Hadoop",
        "Java Developed UDF UDAF",
        "BulkOutputFormat",
        "log data",
        "Pig Implemented Partitioning Dynamic Partitions Buckets",
        "java",
        "Impala HBase Tez Languages C C",
        "Oozie",
        "Software Development Life",
        "HDFS MapReduce Yarn Spark",
        "SQL",
        "American Express",
        "Spark RDD",
        "MQ Experience",
        "Hive",
        "HiveQL",
        "Amazon AWS",
        "Speculation",
        "MedicareMedicaid",
        "Jaspersoft Environment Hadoop MapReduce Spark",
        "Development of EJBs Servlets",
        "the Spark Cassandra Connector",
        "SQL Server Oracle",
        "Business Processes",
        "Performed",
        "Collecting",
        "Spark SQL",
        "XSL",
        "JavaScript",
        "Monitoring Tools Ganglia Ambari Visualization Tools Tableau Methodologies AgileScrum Rational Unified Process",
        "SVN Design",
        "Oracle Other Database Creation",
        "Job Tracker Task Tracker",
        "Developer Reliance Energy",
        "Present Responsibilities Worked",
        "MapReduce Flume Sqoop",
        "JUnit Hadoop Developer Hitachi America",
        "Hive Hands",
        "Created HBase",
        "Cassandra Data Modeling",
        "Imported",
        "Joins",
        "Miami",
        "MapReduce",
        "Scala JavaJ2EE Technologies",
        "RDBMS",
        "NoSQL",
        "HDFS Data",
        "JUnit Test Cases Developed",
        "Integration",
        "Node",
        "Java J2EE Servlets JSP EJB Hibernate",
        "MySql"
    ],
    "experience": "Experience on installation and configuration of spark standalone mode for testing and development environments Developed simple to complex MapReduce jobs using Java language Worked on live 60 nodes Hadoop cluster running on Cloudera CDH4 Extensive experience in analyzing data using Hive QL Pig Latin and custom MapReduce programs in Java Developed UDF UDAF UDTF functions for Hive and Pig Good knowledge of Partitions Bucketing concepts designed and managed them and created external tables in Hive in order to optimize performance Good experience in Avro files RC files Combiners Counters for best practices and performance improvements Good knowledge on Joins group and aggregation concepts and resolved performance issues in Hive and Pig scripts by implementing them Experience with Big Data ML toolkits such as Mahout and Spark ML Experience in job work flow scheduling and monitoring tools like Oozie and Zookeeper Worked on NoSQL databases including HBase Cassandra and Mongo DB Experience in importing data from a Relational database management system RDBMS such as MySql and Oracle into HDFS Hive and exported the processed data back into RDBMS using Sqoop Experience in importing data from RDBMS to HBase and exporting data into RDBMS using Sqoop Implemented Flume for collecting aggregating and moving large amount of server logs and streaming data to HDFS Experience in HBase cluster setup and implementation Done Administration installing upgrading and managing distributions of Cassandra Good knowledge in performance troubleshooting and tunning Cassandra clusters and understanding of Cassandra Data Modeling based on applications Experience in setting up Hadoop in Pseudo distributed environment Experience in setting up Hive Pig HBase and Sqoop in Ubuntu operating system Good knowledge on Software development life cycleSDLC Experience as Java Developer in Web Client Server technologies using Java J2EE Servlets JSP EJB Hibernate framework and Spring framwork Good understanding of Software Development Life CycleSDLC and sound knowledge of project implementation methodologies including Waterfall and Agile Authorized to work in the US for any employer Work Experience Hadoop Developer American Express Phoenix AZ October 2016 to Present Responsibilities Worked on analyzing Hadoop cluster using different big data analytic tools including Kafka Pig Hive and Map Reduce Collecting and aggregating large amounts of log data using Apache Flume and staging data in HDFS for further analysis Real time streaming the data using Spark with Kafka Configured Spark streaming to receive real time data from the Kafka and store the stream data to HDFS using Scala Worked within the Apache Hadoop framework utilizing Opinion Lab statistics to ingest the data from a streaming application program interface API automate processes by creating Oozie workflows and draw conclusions about consumer sentiment based on data patterns found through the use of Hive for external client use Wrote the Storm topology with HDFS Bolt and Hive Bolts as destinations Expertise in writing Storm topology development maintenance and bug fixes Developed Hadoop streaming MapReduce works using Java Worked on debugging performance tuning of Hive Pig Jobs Implemented test scripts to support test driven development and continuous integration Worked on tuning the performance of Pig queries Involved in loading data from Linux file system to HDFS Importing and exporting data into HDFS using Sqoop Good knowledge on building Apache spark applications using Scala Experience working on processing unstructured data using Pig Implemented Partitioning Dynamic Partitions Buckets in Hive Implemented Spark using Scala and SparkSQL for faster testing and processing of data Good knowledge with NoSQL databases like HBase Cassandra Handled Administration installing upgrading and managing distributions of Cassandra Advanced knowledge in performance troubleshooting and tuning Cassandra clusters Done Scaling Cassandra cluster based on lead patterns Good understanding of Cassandra Data Modeling based on applications Experience with Cassandra Performance tuning Highly involved in developmentimplementation of Cassandra environment Plan deploy monitor and maintain Amazon AWS cloud infrastructure consisting of multiple EC2 nodes and VMWare Vms as required in the environment Expertise in AWS data migration between different database platforms like SQL Server to Amazon Aurora using RDS tool Supported Map Reduce Programs those are running on the cluster Gained experience in managing and reviewing Hadoop log files Involved in scheduling Oozie workflow engine to run multiple pig jobs Responsible for developing data pipeline using flume Sqoop and Pig to extract the data from weblogs and store in HDFS Data scrubbing and processing with Oozie Developed Pig Latin scripts to extract data from the web server output files to load into HDFS Involved in developing Hive DDLs to create alter and drop tables Created and maintained technical documentation for launching Hadoop clusters and for executing Hive queries and Pig Scripts Also exploring with the Spark improving the performance and optimization of the existing algorithms in Hadoop using Spark Context Spark SQL Data Frame Pair RDDs Storm Spark YARN Used many features like Parallelize Partitioned Caching both inmemory and disk Serialization Kryo Serialization etc Environment Hadoop Cloudera Big Data HDFS MapReduce Sqoop Spark Hive HBase Linux Java Eclipse Hadoop Distribution of Cloudera PLSQL Toad 96 Windows NT MongoDB Cassandra Tableau Unix shell scripting Putty and Eclipse Hadoop Developer Barclays New York NY August 2015 to September 2016 Responsibilities Developed simple to complex MapReduce streaming jobs using Java language for processing and validating the data Developed data pipeline using MapReduce Flume Sqoop and Pig to ingest customer behavioral data into HDFS for analysis Developed MapReduce and Spark jobs to discover trends in data usage by users Implemented Spark using Python and Spark SQL for faster processing of data Implemented algorithms for real time analysis in Spark Used Spark for interactive queries processing of streaming data and integration with popular NoSQL database for huge volume of data Used the Spark Cassandra Connector to load data to and from Cassandra Real time streaming the data using Spark with Kafka Handled importing data from different data sources into HDFS using Sqoop and also performing transformations using Hive MapReduce and then loading data into HDFS Exported the analyzed data to the relational databases using Sqoop to further visualize and generate reports for the BI team Collecting and aggregating large amounts of log data using Flume and staging data in HDFS for further analysis Analyzed the data by performing Hive queries HiveQL and running Pig scripts Pig Latin to study customer behavior Used Hive to analyze the partitioned and bucketed data and compute various metrics for reporting Developed Pig Latin scripts to perform Map Reduce jobs Developed product profiles using Pig and commodity UDFs Developed Hive scripts in HiveQL to DeNormalize and Aggregate the data Created HBase tables and column families to store the user event data Written automated HBase test cases for data quality checks using HBase command line tools Created UDFs to store specialized data structures in HBase and Cassandra Scheduled and executed work flows in Oozie to run Hive and Pig jobs Used Impala to read write and query the Hadoop data in HDFS from HBase or Cassandra Used Tez framework for building high performance jobs in Pig and Hive Configured Kafka to read and write messages from external programs Configured Kafka to handle real time data Developed end to end data processing pipelines that begin with receiving data using distributed messaging systems Kafka through persistence of data into HBase Written Storm topology to emit data into Cassandra DB Written Storm topology to accept data from Kafka producer and process the data Continuous monitoring and managing the Hadoop cluster using Cloudera Manager Used JUnit framework to perform Unit testing of the application Developed interactive shell scripts for scheduling various data cleansing and data loading process Performed data validation on the data ingested using MapReduce by building a custom model to filter all the invalid data and cleanse the data Experience with data wrangling and creating workable datasets Developed schemas to handle reporting requirements using Jaspersoft Environment Hadoop MapReduce Spark Pig Hive Sqoop Oozie HBase Zookeeper Kafka Flume Solr Storm Tez Impala Mahout Cassandra Cloudera manager MySQL Jaspersoft Multinode cluster with LinuxUbuntu Windows Unix Hadoop Developer Ally Financial Auburn Hills MI November 2014 to July 2015 Responsibilities Installed and configured Hadoop Ecosystem components and Cloudera manager using CDH distribution Frequent interactions with Business partners Designed and developed a MedicareMedicaid claims system using Modeldriven architecture on a customized framework built on spring Moved data from HDFS to Cassandra using MapReduce and BulkOutputFormat class Imported trading and derivatives data in Hadoop Distributed File System and Eco System MapReduce Pig Hive Sqoop Involved in loading and transforming large sets of Structured SemiStructured and Unstructured data and analyzed them by running Hive queries and Pig scripts Created tables in HBase and loading data into HBase tables Developed scripts to load data from HBase to Hive Meta store and perform Map Reduce jobs Was part of an activity to setup Hadoop ecosystem at dev QA Environment Managed and reviewed Hadoop Log files Responsible writing Pig Script and Hive queries for data processing Running Sqoop for importing data from Oracle Other Database Creation of shell script to collect raw logs from different machines Created Partition in a Hive as static and dynamic Implemented Pig Latin scripts using operators such as LOAD STORE DUMP FILTER DISTINCT FOREACH GENERATE GROUP COGROUP ORDER LIMIT AND UNION Optimized the Hive tables using optimization techniques like partitions and bucketing to provide better performance with HiveQL queries Defined some Pig UDF for some financial functions such as swap hedging Speculation and arbitrage Coded many MapReduce program to process unstructured logs file Worked on import and export data into HDFS and Hive using Sqoop Used different data formats Text format and Avro format while loading the data into HDFS Used parameterize Pig script and optimized script using illustrate and explain Involved in the process of configuring HA Kerberos security issues and name node failure restoration activity time to time as a part of zero downtime Implemented FAIR Scheduler as well Environment Hadoop Linux MapReduce HDFS Hbase Hive Pig Shell Scripting Sqoop CDH Distribution Windows Linux Java 6 Eclipse Ant Log4j and JUnit Hadoop Developer Hitachi America Chula Vista CA May 2014 to October 2014 Responsibilities Hands on Experience in joining raw data with the reference data using Pig scripting Written custom UDFs in Hive Hands on experience in extracting data from different databases and to copy into HDFS file system using Sqoop Written Sqoop incremental import job to move newupdated info From database to HDFS Created Oozie coordinated workflow to execute Sqoop incremental job daily Used Oozie workflow engine to run multiple Hive and Pig jobs Hands on experience in exporting the results into relational databases using Sqoop for visualization and to generate reports for the BI team Involved in Installing and configuring Hive Pig Sqoop Flume and Oozie on the Hadoop cluster Worked with application teams to install Operating System Hadoop updates patches versions upgrades as required Working with clients on requirements based on their business needs Communicate deliverables status to userstakeholders client and drive periodic review meetings On time completion of tasks and the projects per quality goals Environment Hadoop HDFS MapReduce Hive Pig Sqoop HBase Oozie MySql SVN Putty Zookeeper Ganglia UNIX and Shell scripting Hadoop Developer Florida Power Light Miami FL October 2013 to April 2014 Responsibilities Integrated Kafka with Storm for real time data processing and written some storm topologies to store the processed data directly to MongoDB and HDFS Experience in writing Spark SQL scripts Imported data from different sources into Spark RDD for processing Developed custom aggregate functions using Spark SQL and performed interactive querying Involved in loading data from edge node to HDFS using shell scripting Worked on installing cluster commissioning and decommissioning of Datanode Namenode high availability capacity planning and slots configuration Completion of unit testing for the new Hadoop jobs in standalone mode designated for Unit region using MR Unit Developed Spark scripts by using Scala and Python shell commands as per the requirement Experience in managing and reviewing Hadoop log files Experience in Hive partitioning bucketing and perform joins on Hive tables and implementing Hive SerDe like REGEX JSON and Avro Optimized Hive analytics Sql queries created tablesviews written custom UDFs and Hive based exception processing Involved in transforming the Teradata to legacy lables to HDFS and HBase tables using Sqoop and vice versa Configured Fair Scheduler to provide fair resources to all the applications across the cluster Environment Hortonworks Hadoop Ambari Spark Solr Kafka MongoDB Linux HDFS Hive Pig Sqoop Flume Zookeeper RDBMS JavaJ2EE Developer Mercury Insurance Roseville CA November 2012 to September 2013 Responsibilities Write design document based on requirements from MMSEA user guide Performed requirement gathering design coding testing implementation and deployment Worked on modeling of Dialog process Business Processes and coding Business Objects QueryMapper and JUnit files Involved in the design and creation of Class diagrams Sequence diagrams and Activity Diagrams using UML models Created the Business Objects methods using Java and integrating the activity diagrams Involved in developing JSP pages using Struts custom tags jQuery and Tiles Framework Used JavaScript to perform client side validations and StrutsValidator Framework for serverside validation Worked in web services using SOAP WSDL Wrote Query Mappers and MQ Experience in JUnit Test Cases Developed the UI using XSL and JavaScript Managed software configuration using ClearCase and SVN Design develop and test features and enhancements Performed error rate analysis of production issues and technical errors Developed test environment for testing all the Web Service exposed as part of the core module and their integration with partner services in Integration test Analyze user requirement document and develop test plan which includes test objectives test strategies test environment and test priorities Responsible for performing endtoend system testing of application writing JUnit test cases Perform Functional testing Performance testing Integration testing Regression testing Smoke testing and User Acceptance Testing UAT Converted Complex SQL queries running at mainframes into pig and Hive as a part of a migration from mainframes into Hadoop cluster Environment Shell Scripting Java 6 JEE Spring Hibernate Eclipse Oracle 10 g JavaScript Servlets Nodejs JMS Ant Log4j and Junit Hadoop Pig Hive Java Developer Reliance Energy LTD June 2010 to August 2012 Responsibilities Involved in the design and implementation of the architecture for the project using OOAD UML design patterns Involved in design and development of server side layer using XML JSP JDBC JNDI EJB and DAO patterns using eclipse IDE Work involved extensive usage of HTML CSS Javascript and Ajax for client side development and validations Used parsers for the conversion of XML files to java objects and vice versa Developed screens using XML documents and XSL Developed Client programs for consuming the Web services published by the Country Defaults Department which keeps in track of the information regarding life span inflation rates retirement age etc using Apache Axis Developed Java Beans and JSPs by using spring and JSTL tag libs for supplements Development of EJBs Servlets and JSP files for implementing Business rules and Security options using IBM Web Sphere Involved in creating tables stored procedures in SQL for data manipulation and retrieval using SQL Server Oracle and DB2 Trained end users on developed application Environment Java JSF Framework Eclipse IDE Ajax Apache Axis OOAD Web Logic Java script HTML XML CSS SQL Server Oracle Web services Ajax Spring OOAD and UML Windows Education Bachelors Skills JAVA 6 years SQL 6 years Hadoop 5 years HADOOP 5 years Hive 5 years Hadoop Cloudera Big Data HDFS MapReduce Sqoop Spark Hive HBase Linux Java Eclipse Hadoop Distribution of Cloudera PLSQL Toad 96 Windows NT MongoDB Cassandra Tableau Unix shell scripting Putty and Eclipse 8 years Additional Information TECHNICAL SKILL Hadoop Ecosystem Hadoop HDFS YARN MapReduce Hive Pig Oozie Zookeeper Flume Sqoop Spark Mahout Kafka Storm Scala Spark ML Impala HBase Tez Languages C C Core Java PLSQL Scala JavaJ2EE Technologies Servlets JSP JDBC Java Beans EJB RMI Web Services Frameworks EJB Struts Hibernate and Spring Scripting Languages Python R SQL Unix Shell Scripting Hive QL Pig Latin Databases MySQL Oracle 10 g SQL Server 2008 NoSQL Databases HBase Cassandra MongoDB Web Technologies HTML CSS XML Javascript Ajax Nodejs SOAP WebApplication Servers WebLogic WebSphere Apache Tomcat Development Tools Eclipse Ant Putty Version Control SVN GIT Cluster Management and Monitoring Tools Ganglia Ambari Visualization Tools Tableau Methodologies AgileScrum Rational Unified Process and Waterfall Environment Win 9598 Win NT Win XP Win 7 Unix LinuxUbuntu and CentOS",
    "extracted_keywords": [
        "Hadoop",
        "Developer",
        "Hadoop",
        "span",
        "lDeveloperspan",
        "Hadoop",
        "Developer",
        "American",
        "Express",
        "Phoenix",
        "AZ",
        "years",
        "experience",
        "IT",
        "field",
        "project",
        "development",
        "implementation",
        "deployment",
        "maintenance",
        "Hadoop",
        "ecosystem",
        "technologies",
        "domain",
        "knowledge",
        "Finance",
        "Banking",
        "Communication",
        "Insurance",
        "Retail",
        "Industry",
        "Health",
        "care",
        "years",
        "hands",
        "experience",
        "Hadoop",
        "Ecosystem",
        "technologies",
        "MapReduce",
        "Yarn",
        "Spark",
        "Hive",
        "Pig",
        "Oozie",
        "Sqoop",
        "Flume",
        "Zookeeper",
        "HBase",
        "years",
        "hands",
        "experience",
        "Spark",
        "framework",
        "Scala",
        "years",
        "Java",
        "programming",
        "experience",
        "web",
        "applications",
        "Client",
        "Server",
        "technologies",
        "depth",
        "understanding",
        "Hadoop",
        "Architecture",
        "components",
        "Job",
        "Tracker",
        "Task",
        "Tracker",
        "Name",
        "Node",
        "Data",
        "Node",
        "Resource",
        "Manager",
        "MapReduce",
        "knowledge",
        "Apache",
        "Spark",
        "Apache",
        "Storm",
        "time",
        "data",
        "knowledge",
        "programming",
        "Resilient",
        "Distributed",
        "Datasets",
        "exposure",
        "performance",
        "hive",
        "queries",
        "jobs",
        "spark",
        "jobs",
        "formats",
        "files",
        "text",
        "files",
        "stream",
        "log",
        "files",
        "Apache",
        "files",
        "XML",
        "Files",
        "Experience",
        "installation",
        "configuration",
        "spark",
        "mode",
        "testing",
        "development",
        "environments",
        "MapReduce",
        "jobs",
        "Java",
        "language",
        "nodes",
        "Hadoop",
        "cluster",
        "Cloudera",
        "CDH4",
        "experience",
        "data",
        "Hive",
        "QL",
        "Pig",
        "Latin",
        "MapReduce",
        "programs",
        "Java",
        "Developed",
        "UDF",
        "UDTF",
        "functions",
        "Hive",
        "Pig",
        "knowledge",
        "Partitions",
        "Bucketing",
        "concepts",
        "tables",
        "Hive",
        "order",
        "performance",
        "experience",
        "Avro",
        "files",
        "RC",
        "Combiners",
        "Counters",
        "practices",
        "performance",
        "improvements",
        "knowledge",
        "Joins",
        "group",
        "aggregation",
        "concepts",
        "performance",
        "issues",
        "Hive",
        "Pig",
        "scripts",
        "Experience",
        "Big",
        "Data",
        "ML",
        "toolkits",
        "Mahout",
        "Spark",
        "ML",
        "Experience",
        "job",
        "work",
        "flow",
        "scheduling",
        "monitoring",
        "tools",
        "Oozie",
        "Zookeeper",
        "NoSQL",
        "databases",
        "HBase",
        "Cassandra",
        "Mongo",
        "DB",
        "Experience",
        "data",
        "database",
        "management",
        "system",
        "RDBMS",
        "MySql",
        "Oracle",
        "HDFS",
        "Hive",
        "data",
        "RDBMS",
        "Sqoop",
        "Experience",
        "data",
        "RDBMS",
        "HBase",
        "data",
        "RDBMS",
        "Sqoop",
        "Implemented",
        "Flume",
        "amount",
        "server",
        "logs",
        "streaming",
        "data",
        "HDFS",
        "Experience",
        "HBase",
        "cluster",
        "setup",
        "implementation",
        "Administration",
        "upgrading",
        "managing",
        "distributions",
        "Cassandra",
        "knowledge",
        "performance",
        "Cassandra",
        "clusters",
        "understanding",
        "Cassandra",
        "Data",
        "Modeling",
        "applications",
        "Experience",
        "Hadoop",
        "Pseudo",
        "environment",
        "Experience",
        "Hive",
        "Pig",
        "HBase",
        "Sqoop",
        "Ubuntu",
        "operating",
        "system",
        "knowledge",
        "Software",
        "development",
        "life",
        "cycleSDLC",
        "Experience",
        "Java",
        "Developer",
        "Web",
        "Client",
        "Server",
        "technologies",
        "Java",
        "J2EE",
        "Servlets",
        "JSP",
        "EJB",
        "Hibernate",
        "framework",
        "Spring",
        "framwork",
        "understanding",
        "Software",
        "Development",
        "Life",
        "knowledge",
        "project",
        "implementation",
        "methodologies",
        "Waterfall",
        "Agile",
        "Authorized",
        "US",
        "employer",
        "Work",
        "Experience",
        "Hadoop",
        "Developer",
        "American",
        "Express",
        "Phoenix",
        "AZ",
        "October",
        "Present",
        "Responsibilities",
        "Hadoop",
        "cluster",
        "data",
        "tools",
        "Kafka",
        "Pig",
        "Hive",
        "Map",
        "Collecting",
        "amounts",
        "log",
        "data",
        "Apache",
        "Flume",
        "data",
        "HDFS",
        "analysis",
        "time",
        "data",
        "Spark",
        "Kafka",
        "Configured",
        "Spark",
        "streaming",
        "time",
        "data",
        "Kafka",
        "stream",
        "data",
        "HDFS",
        "Scala",
        "Worked",
        "Apache",
        "Hadoop",
        "framework",
        "Opinion",
        "Lab",
        "statistics",
        "data",
        "application",
        "program",
        "interface",
        "API",
        "automate",
        "processes",
        "Oozie",
        "workflows",
        "conclusions",
        "consumer",
        "sentiment",
        "data",
        "patterns",
        "use",
        "Hive",
        "client",
        "use",
        "Storm",
        "topology",
        "HDFS",
        "Bolt",
        "Hive",
        "Bolts",
        "destinations",
        "Expertise",
        "Storm",
        "topology",
        "development",
        "maintenance",
        "bug",
        "fixes",
        "Developed",
        "Hadoop",
        "MapReduce",
        "works",
        "Java",
        "Worked",
        "performance",
        "tuning",
        "Hive",
        "Pig",
        "Jobs",
        "test",
        "scripts",
        "test",
        "development",
        "integration",
        "performance",
        "Pig",
        "queries",
        "loading",
        "data",
        "Linux",
        "file",
        "system",
        "HDFS",
        "Importing",
        "data",
        "HDFS",
        "Sqoop",
        "knowledge",
        "Apache",
        "spark",
        "applications",
        "Scala",
        "Experience",
        "data",
        "Pig",
        "Partitioning",
        "Dynamic",
        "Partitions",
        "Buckets",
        "Hive",
        "Spark",
        "Scala",
        "SparkSQL",
        "testing",
        "processing",
        "data",
        "knowledge",
        "databases",
        "HBase",
        "Cassandra",
        "Handled",
        "Administration",
        "upgrading",
        "managing",
        "distributions",
        "Cassandra",
        "knowledge",
        "performance",
        "Cassandra",
        "clusters",
        "Scaling",
        "Cassandra",
        "cluster",
        "lead",
        "patterns",
        "understanding",
        "Cassandra",
        "Data",
        "Modeling",
        "applications",
        "Experience",
        "Cassandra",
        "Performance",
        "developmentimplementation",
        "Cassandra",
        "environment",
        "Plan",
        "monitor",
        "Amazon",
        "AWS",
        "cloud",
        "infrastructure",
        "EC2",
        "nodes",
        "VMWare",
        "Vms",
        "environment",
        "Expertise",
        "AWS",
        "data",
        "migration",
        "database",
        "platforms",
        "SQL",
        "Server",
        "Amazon",
        "Aurora",
        "RDS",
        "tool",
        "Map",
        "Reduce",
        "Programs",
        "cluster",
        "experience",
        "Hadoop",
        "log",
        "files",
        "Oozie",
        "workflow",
        "engine",
        "pig",
        "jobs",
        "data",
        "pipeline",
        "flume",
        "Sqoop",
        "Pig",
        "data",
        "weblogs",
        "HDFS",
        "Data",
        "scrubbing",
        "processing",
        "Oozie",
        "Developed",
        "Pig",
        "Latin",
        "scripts",
        "data",
        "web",
        "server",
        "output",
        "files",
        "HDFS",
        "Hive",
        "DDLs",
        "alter",
        "tables",
        "documentation",
        "Hadoop",
        "clusters",
        "Hive",
        "queries",
        "Pig",
        "Scripts",
        "Spark",
        "performance",
        "optimization",
        "algorithms",
        "Hadoop",
        "Spark",
        "Context",
        "Spark",
        "SQL",
        "Data",
        "Frame",
        "Pair",
        "RDDs",
        "Storm",
        "Spark",
        "YARN",
        "features",
        "Parallelize",
        "Caching",
        "disk",
        "Serialization",
        "Kryo",
        "Serialization",
        "Environment",
        "Hadoop",
        "Cloudera",
        "Big",
        "Data",
        "HDFS",
        "MapReduce",
        "Sqoop",
        "Spark",
        "Hive",
        "HBase",
        "Linux",
        "Java",
        "Eclipse",
        "Hadoop",
        "Distribution",
        "Cloudera",
        "PLSQL",
        "Toad",
        "Windows",
        "NT",
        "MongoDB",
        "Cassandra",
        "Tableau",
        "Unix",
        "shell",
        "Putty",
        "Eclipse",
        "Hadoop",
        "Developer",
        "Barclays",
        "New",
        "York",
        "NY",
        "August",
        "September",
        "Responsibilities",
        "MapReduce",
        "streaming",
        "jobs",
        "Java",
        "language",
        "processing",
        "data",
        "data",
        "pipeline",
        "MapReduce",
        "Flume",
        "Sqoop",
        "Pig",
        "customer",
        "data",
        "HDFS",
        "analysis",
        "MapReduce",
        "Spark",
        "jobs",
        "trends",
        "data",
        "usage",
        "users",
        "Spark",
        "Python",
        "Spark",
        "SQL",
        "processing",
        "data",
        "algorithms",
        "time",
        "analysis",
        "Spark",
        "Spark",
        "queries",
        "processing",
        "data",
        "integration",
        "NoSQL",
        "database",
        "volume",
        "data",
        "Spark",
        "Cassandra",
        "Connector",
        "data",
        "Cassandra",
        "time",
        "data",
        "Spark",
        "Kafka",
        "data",
        "data",
        "sources",
        "HDFS",
        "Sqoop",
        "transformations",
        "Hive",
        "MapReduce",
        "data",
        "HDFS",
        "data",
        "databases",
        "Sqoop",
        "reports",
        "BI",
        "team",
        "amounts",
        "log",
        "data",
        "Flume",
        "data",
        "HDFS",
        "analysis",
        "data",
        "Hive",
        "queries",
        "Pig",
        "scripts",
        "Pig",
        "Latin",
        "customer",
        "behavior",
        "Hive",
        "data",
        "metrics",
        "Developed",
        "Pig",
        "Latin",
        "scripts",
        "Map",
        "Reduce",
        "jobs",
        "product",
        "profiles",
        "Pig",
        "commodity",
        "UDFs",
        "Hive",
        "scripts",
        "HiveQL",
        "DeNormalize",
        "data",
        "Created",
        "HBase",
        "tables",
        "column",
        "families",
        "user",
        "event",
        "data",
        "HBase",
        "test",
        "cases",
        "data",
        "quality",
        "checks",
        "HBase",
        "command",
        "line",
        "tools",
        "UDFs",
        "data",
        "structures",
        "HBase",
        "Cassandra",
        "Scheduled",
        "work",
        "flows",
        "Oozie",
        "Hive",
        "Pig",
        "jobs",
        "Impala",
        "Hadoop",
        "data",
        "HDFS",
        "HBase",
        "Cassandra",
        "Tez",
        "framework",
        "performance",
        "jobs",
        "Pig",
        "Hive",
        "Configured",
        "Kafka",
        "messages",
        "programs",
        "Configured",
        "Kafka",
        "time",
        "data",
        "end",
        "data",
        "processing",
        "pipelines",
        "data",
        "systems",
        "Kafka",
        "persistence",
        "data",
        "HBase",
        "Written",
        "Storm",
        "topology",
        "data",
        "Cassandra",
        "DB",
        "Written",
        "Storm",
        "topology",
        "data",
        "Kafka",
        "producer",
        "data",
        "monitoring",
        "Hadoop",
        "cluster",
        "Cloudera",
        "Manager",
        "JUnit",
        "framework",
        "Unit",
        "testing",
        "application",
        "shell",
        "scripts",
        "data",
        "cleansing",
        "data",
        "loading",
        "process",
        "data",
        "validation",
        "data",
        "MapReduce",
        "custom",
        "model",
        "data",
        "cleanse",
        "data",
        "Experience",
        "data",
        "datasets",
        "schemas",
        "reporting",
        "requirements",
        "Jaspersoft",
        "Environment",
        "Hadoop",
        "MapReduce",
        "Spark",
        "Pig",
        "Hive",
        "Sqoop",
        "Oozie",
        "HBase",
        "Zookeeper",
        "Kafka",
        "Flume",
        "Solr",
        "Storm",
        "Tez",
        "Impala",
        "Mahout",
        "Cassandra",
        "Cloudera",
        "manager",
        "MySQL",
        "Jaspersoft",
        "Multinode",
        "cluster",
        "LinuxUbuntu",
        "Windows",
        "Unix",
        "Hadoop",
        "Developer",
        "Ally",
        "Financial",
        "Auburn",
        "Hills",
        "MI",
        "November",
        "July",
        "Responsibilities",
        "Hadoop",
        "Ecosystem",
        "components",
        "Cloudera",
        "manager",
        "CDH",
        "distribution",
        "interactions",
        "Business",
        "partners",
        "MedicareMedicaid",
        "claims",
        "system",
        "Modeldriven",
        "architecture",
        "framework",
        "spring",
        "Moved",
        "data",
        "HDFS",
        "Cassandra",
        "MapReduce",
        "class",
        "trading",
        "derivatives",
        "data",
        "Hadoop",
        "Distributed",
        "File",
        "System",
        "Eco",
        "System",
        "MapReduce",
        "Pig",
        "Hive",
        "Sqoop",
        "loading",
        "sets",
        "Structured",
        "SemiStructured",
        "Unstructured",
        "data",
        "Hive",
        "queries",
        "Pig",
        "scripts",
        "tables",
        "HBase",
        "loading",
        "data",
        "HBase",
        "scripts",
        "data",
        "HBase",
        "Hive",
        "Meta",
        "store",
        "Map",
        "Reduce",
        "jobs",
        "part",
        "activity",
        "Hadoop",
        "ecosystem",
        "dev",
        "QA",
        "Environment",
        "Managed",
        "Hadoop",
        "Log",
        "Pig",
        "Script",
        "Hive",
        "queries",
        "data",
        "Running",
        "Sqoop",
        "data",
        "Oracle",
        "Other",
        "Database",
        "Creation",
        "shell",
        "script",
        "logs",
        "machines",
        "Partition",
        "Hive",
        "Pig",
        "Latin",
        "scripts",
        "operators",
        "STORE",
        "DUMP",
        "FILTER",
        "DISTINCT",
        "FOREACH",
        "GENERATE",
        "GROUP",
        "COGROUP",
        "ORDER",
        "LIMIT",
        "UNION",
        "Hive",
        "tables",
        "optimization",
        "techniques",
        "partitions",
        "bucketing",
        "performance",
        "queries",
        "Pig",
        "UDF",
        "functions",
        "swap",
        "Speculation",
        "arbitrage",
        "MapReduce",
        "program",
        "logs",
        "file",
        "import",
        "export",
        "data",
        "HDFS",
        "Hive",
        "Sqoop",
        "data",
        "formats",
        "Text",
        "format",
        "Avro",
        "format",
        "data",
        "HDFS",
        "parameterize",
        "Pig",
        "script",
        "script",
        "illustrate",
        "process",
        "HA",
        "security",
        "issues",
        "failure",
        "restoration",
        "activity",
        "time",
        "time",
        "part",
        "downtime",
        "Scheduler",
        "Environment",
        "Hadoop",
        "Linux",
        "MapReduce",
        "HDFS",
        "Hbase",
        "Hive",
        "Pig",
        "Shell",
        "Scripting",
        "Sqoop",
        "CDH",
        "Distribution",
        "Windows",
        "Linux",
        "Java",
        "Eclipse",
        "Ant",
        "Log4j",
        "JUnit",
        "Hadoop",
        "Developer",
        "Hitachi",
        "America",
        "Chula",
        "Vista",
        "CA",
        "May",
        "October",
        "Responsibilities",
        "Hands",
        "Experience",
        "data",
        "reference",
        "data",
        "Pig",
        "custom",
        "UDFs",
        "Hive",
        "Hands",
        "experience",
        "data",
        "databases",
        "HDFS",
        "file",
        "system",
        "Sqoop",
        "Written",
        "Sqoop",
        "import",
        "job",
        "info",
        "database",
        "HDFS",
        "Oozie",
        "workflow",
        "Sqoop",
        "job",
        "Oozie",
        "workflow",
        "engine",
        "Hive",
        "Pig",
        "jobs",
        "Hands",
        "experience",
        "results",
        "databases",
        "Sqoop",
        "visualization",
        "reports",
        "BI",
        "team",
        "Hive",
        "Pig",
        "Sqoop",
        "Flume",
        "Oozie",
        "Hadoop",
        "cluster",
        "application",
        "teams",
        "System",
        "Hadoop",
        "updates",
        "versions",
        "upgrades",
        "Working",
        "clients",
        "requirements",
        "business",
        "Communicate",
        "deliverables",
        "status",
        "userstakeholders",
        "client",
        "review",
        "meetings",
        "time",
        "completion",
        "tasks",
        "projects",
        "quality",
        "goals",
        "Environment",
        "Hadoop",
        "HDFS",
        "MapReduce",
        "Hive",
        "Pig",
        "Sqoop",
        "HBase",
        "Oozie",
        "MySql",
        "SVN",
        "Putty",
        "Zookeeper",
        "Ganglia",
        "UNIX",
        "Shell",
        "Hadoop",
        "Developer",
        "Florida",
        "Power",
        "Light",
        "Miami",
        "FL",
        "October",
        "April",
        "Responsibilities",
        "Integrated",
        "Kafka",
        "Storm",
        "time",
        "data",
        "processing",
        "storm",
        "topologies",
        "data",
        "MongoDB",
        "HDFS",
        "Experience",
        "Spark",
        "SQL",
        "data",
        "sources",
        "Spark",
        "RDD",
        "custom",
        "aggregate",
        "functions",
        "Spark",
        "SQL",
        "querying",
        "loading",
        "data",
        "edge",
        "node",
        "HDFS",
        "shell",
        "scripting",
        "cluster",
        "commissioning",
        "decommissioning",
        "Datanode",
        "Namenode",
        "availability",
        "capacity",
        "planning",
        "slots",
        "configuration",
        "Completion",
        "unit",
        "testing",
        "Hadoop",
        "jobs",
        "mode",
        "Unit",
        "region",
        "MR",
        "Unit",
        "Developed",
        "Spark",
        "scripts",
        "Scala",
        "Python",
        "shell",
        "commands",
        "requirement",
        "Experience",
        "Hadoop",
        "log",
        "Experience",
        "Hive",
        "bucketing",
        "joins",
        "Hive",
        "tables",
        "Hive",
        "SerDe",
        "REGEX",
        "JSON",
        "Avro",
        "Hive",
        "analytics",
        "Sql",
        "queries",
        "tablesviews",
        "custom",
        "UDFs",
        "Hive",
        "exception",
        "processing",
        "Teradata",
        "legacy",
        "lables",
        "HDFS",
        "HBase",
        "tables",
        "Sqoop",
        "vice",
        "versa",
        "Configured",
        "Fair",
        "Scheduler",
        "resources",
        "applications",
        "cluster",
        "Environment",
        "Hortonworks",
        "Hadoop",
        "Ambari",
        "Spark",
        "Solr",
        "Kafka",
        "MongoDB",
        "Linux",
        "HDFS",
        "Hive",
        "Pig",
        "Sqoop",
        "Flume",
        "Zookeeper",
        "RDBMS",
        "JavaJ2EE",
        "Developer",
        "Mercury",
        "Insurance",
        "Roseville",
        "CA",
        "November",
        "September",
        "Responsibilities",
        "design",
        "document",
        "requirements",
        "MMSEA",
        "user",
        "Performed",
        "requirement",
        "design",
        "testing",
        "implementation",
        "deployment",
        "modeling",
        "Dialog",
        "process",
        "Business",
        "Processes",
        "Business",
        "QueryMapper",
        "JUnit",
        "files",
        "design",
        "creation",
        "Class",
        "diagrams",
        "Sequence",
        "diagrams",
        "Activity",
        "Diagrams",
        "UML",
        "models",
        "Business",
        "Objects",
        "methods",
        "Java",
        "activity",
        "diagrams",
        "JSP",
        "pages",
        "Struts",
        "custom",
        "jQuery",
        "Tiles",
        "Framework",
        "JavaScript",
        "client",
        "side",
        "validations",
        "StrutsValidator",
        "Framework",
        "serverside",
        "validation",
        "web",
        "services",
        "SOAP",
        "WSDL",
        "Query",
        "Mappers",
        "MQ",
        "Experience",
        "JUnit",
        "Test",
        "Cases",
        "UI",
        "XSL",
        "JavaScript",
        "Managed",
        "software",
        "configuration",
        "ClearCase",
        "SVN",
        "Design",
        "test",
        "features",
        "Performed",
        "error",
        "rate",
        "analysis",
        "production",
        "issues",
        "errors",
        "test",
        "environment",
        "Web",
        "Service",
        "part",
        "core",
        "module",
        "integration",
        "partner",
        "services",
        "Integration",
        "test",
        "Analyze",
        "user",
        "requirement",
        "document",
        "test",
        "plan",
        "test",
        "objectives",
        "test",
        "strategies",
        "test",
        "environment",
        "test",
        "priorities",
        "endtoend",
        "system",
        "testing",
        "application",
        "JUnit",
        "test",
        "cases",
        "testing",
        "Performance",
        "testing",
        "Integration",
        "testing",
        "Regression",
        "testing",
        "Smoke",
        "testing",
        "User",
        "Acceptance",
        "Testing",
        "UAT",
        "Converted",
        "Complex",
        "SQL",
        "queries",
        "mainframes",
        "pig",
        "Hive",
        "part",
        "migration",
        "mainframes",
        "Hadoop",
        "cluster",
        "Environment",
        "Shell",
        "Scripting",
        "Java",
        "JEE",
        "Spring",
        "Hibernate",
        "Eclipse",
        "Oracle",
        "g",
        "JavaScript",
        "Servlets",
        "Nodejs",
        "JMS",
        "Ant",
        "Log4j",
        "Junit",
        "Hadoop",
        "Pig",
        "Hive",
        "Java",
        "Developer",
        "Reliance",
        "Energy",
        "LTD",
        "June",
        "August",
        "Responsibilities",
        "design",
        "implementation",
        "architecture",
        "project",
        "OOAD",
        "UML",
        "design",
        "patterns",
        "design",
        "development",
        "server",
        "side",
        "layer",
        "XML",
        "JSP",
        "JDBC",
        "JNDI",
        "EJB",
        "DAO",
        "patterns",
        "eclipse",
        "IDE",
        "Work",
        "usage",
        "HTML",
        "CSS",
        "Javascript",
        "Ajax",
        "client",
        "side",
        "development",
        "parsers",
        "conversion",
        "XML",
        "files",
        "objects",
        "vice",
        "screens",
        "XML",
        "documents",
        "XSL",
        "Client",
        "programs",
        "Web",
        "services",
        "Country",
        "Defaults",
        "Department",
        "track",
        "information",
        "life",
        "span",
        "inflation",
        "rates",
        "retirement",
        "age",
        "Apache",
        "Axis",
        "Java",
        "Beans",
        "JSPs",
        "spring",
        "JSTL",
        "tag",
        "supplements",
        "Development",
        "EJBs",
        "Servlets",
        "JSP",
        "files",
        "Business",
        "rules",
        "Security",
        "options",
        "IBM",
        "Web",
        "Sphere",
        "tables",
        "procedures",
        "SQL",
        "data",
        "manipulation",
        "retrieval",
        "SQL",
        "Server",
        "Oracle",
        "DB2",
        "Trained",
        "users",
        "application",
        "Environment",
        "Java",
        "JSF",
        "Framework",
        "Eclipse",
        "IDE",
        "Ajax",
        "Apache",
        "Axis",
        "OOAD",
        "Web",
        "Logic",
        "Java",
        "script",
        "HTML",
        "XML",
        "CSS",
        "SQL",
        "Server",
        "Oracle",
        "Web",
        "services",
        "Spring",
        "OOAD",
        "UML",
        "Windows",
        "Education",
        "Bachelors",
        "Skills",
        "JAVA",
        "years",
        "SQL",
        "years",
        "Hadoop",
        "years",
        "HADOOP",
        "years",
        "Hive",
        "years",
        "Hadoop",
        "Cloudera",
        "Big",
        "Data",
        "HDFS",
        "MapReduce",
        "Sqoop",
        "Spark",
        "Hive",
        "HBase",
        "Linux",
        "Java",
        "Eclipse",
        "Hadoop",
        "Distribution",
        "Cloudera",
        "PLSQL",
        "Toad",
        "Windows",
        "NT",
        "MongoDB",
        "Cassandra",
        "Tableau",
        "Unix",
        "shell",
        "Putty",
        "Eclipse",
        "years",
        "Additional",
        "Information",
        "TECHNICAL",
        "SKILL",
        "Hadoop",
        "Ecosystem",
        "Hadoop",
        "HDFS",
        "YARN",
        "MapReduce",
        "Hive",
        "Pig",
        "Oozie",
        "Zookeeper",
        "Flume",
        "Sqoop",
        "Spark",
        "Mahout",
        "Kafka",
        "Storm",
        "Scala",
        "Spark",
        "ML",
        "Impala",
        "HBase",
        "Tez",
        "Languages",
        "C",
        "C",
        "Core",
        "Java",
        "PLSQL",
        "Scala",
        "JavaJ2EE",
        "Technologies",
        "Servlets",
        "JSP",
        "JDBC",
        "Java",
        "Beans",
        "EJB",
        "RMI",
        "Web",
        "Services",
        "Frameworks",
        "EJB",
        "Struts",
        "Hibernate",
        "Spring",
        "Scripting",
        "Languages",
        "Python",
        "R",
        "SQL",
        "Unix",
        "Shell",
        "Scripting",
        "Hive",
        "QL",
        "Pig",
        "Latin",
        "MySQL",
        "Oracle",
        "g",
        "SQL",
        "Server",
        "NoSQL",
        "HBase",
        "Cassandra",
        "MongoDB",
        "Web",
        "Technologies",
        "HTML",
        "CSS",
        "XML",
        "Javascript",
        "Ajax",
        "Nodejs",
        "SOAP",
        "WebApplication",
        "Servers",
        "WebLogic",
        "WebSphere",
        "Apache",
        "Tomcat",
        "Development",
        "Tools",
        "Eclipse",
        "Ant",
        "Putty",
        "Version",
        "Control",
        "SVN",
        "GIT",
        "Cluster",
        "Management",
        "Monitoring",
        "Tools",
        "Ganglia",
        "Ambari",
        "Visualization",
        "Tools",
        "Tableau",
        "Methodologies",
        "AgileScrum",
        "Rational",
        "Unified",
        "Process",
        "Waterfall",
        "Environment",
        "Win",
        "Win",
        "NT",
        "Win",
        "XP",
        "Win",
        "Unix",
        "LinuxUbuntu",
        "CentOS"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T23:15:41.135268",
    "resume_data": "Hadoop Developer Hadoop span lDeveloperspan Hadoop Developer American Express Phoenix AZ Over 7 years of total professional experience in IT field involving project development implementation deployment and maintenance using Hadoop ecosystem related technologies with domain knowledge in Finance Banking Communication Insurance Retail Industry and Health care 4 years of hands on experience in Hadoop Ecosystem technologies like HDFS MapReduce Yarn Spark Hive Pig Oozie Sqoop Flume Zookeeper HBase Over all two years of hands on experience using Spark framework with Scala 3 years of Java programming experience in developing web based applications and Client Server technologies In depth understanding of Hadoop Architecture and its various components such as Job Tracker Task Tracker Name Node Data Node Resource Manager and MapReduce concepts Proficient knowledge on Apache Spark and Apache Storm to process real time data Extensive knowledge in programming with Resilient Distributed Datasets RDDs Good exposure to performance tuning hive queries mapreduce jobs spark jobs Worked with various formats of files like delimited text files click stream log files Apache log filesAvro files JSON files XML Files Experience on installation and configuration of spark standalone mode for testing and development environments Developed simple to complex MapReduce jobs using Java language Worked on live 60 nodes Hadoop cluster running on Cloudera CDH4 Extensive experience in analyzing data using Hive QL Pig Latin and custom MapReduce programs in Java Developed UDF UDAF UDTF functions for Hive and Pig Good knowledge of Partitions Bucketing concepts designed and managed them and created external tables in Hive in order to optimize performance Good experience in Avro files RC files Combiners Counters for best practices and performance improvements Good knowledge on Joins group and aggregation concepts and resolved performance issues in Hive and Pig scripts by implementing them Experience with Big Data ML toolkits such as Mahout and Spark ML Experience in job work flow scheduling and monitoring tools like Oozie and Zookeeper Worked on NoSQL databases including HBase Cassandra and Mongo DB Experience in importing data from a Relational database management system RDBMS such as MySql and Oracle into HDFS Hive and exported the processed data back into RDBMS using Sqoop Experience in importing data from RDBMS to HBase and exporting data into RDBMS using Sqoop Implemented Flume for collecting aggregating and moving large amount of server logs and streaming data to HDFS Experience in HBase cluster setup and implementation Done Administration installing upgrading and managing distributions of Cassandra Good knowledge in performance troubleshooting and tunning Cassandra clusters and understanding of Cassandra Data Modeling based on applications Experience in setting up Hadoop in Pseudo distributed environment Experience in setting up Hive Pig HBase and Sqoop in Ubuntu operating system Good knowledge on Software development life cycleSDLC Experience as Java Developer in Web Client Server technologies using Java J2EE Servlets JSP EJB Hibernate framework and Spring framwork Good understanding of Software Development Life CycleSDLC and sound knowledge of project implementation methodologies including Waterfall and Agile Authorized to work in the US for any employer Work Experience Hadoop Developer American Express Phoenix AZ October 2016 to Present Responsibilities Worked on analyzing Hadoop cluster using different big data analytic tools including Kafka Pig Hive and Map Reduce Collecting and aggregating large amounts of log data using Apache Flume and staging data in HDFS for further analysis Real time streaming the data using Spark with Kafka Configured Spark streaming to receive real time data from the Kafka and store the stream data to HDFS using Scala Worked within the Apache Hadoop framework utilizing Opinion Lab statistics to ingest the data from a streaming application program interface API automate processes by creating Oozie workflows and draw conclusions about consumer sentiment based on data patterns found through the use of Hive for external client use Wrote the Storm topology with HDFS Bolt and Hive Bolts as destinations Expertise in writing Storm topology development maintenance and bug fixes Developed Hadoop streaming MapReduce works using Java Worked on debugging performance tuning of Hive Pig Jobs Implemented test scripts to support test driven development and continuous integration Worked on tuning the performance of Pig queries Involved in loading data from Linux file system to HDFS Importing and exporting data into HDFS using Sqoop Good knowledge on building Apache spark applications using Scala Experience working on processing unstructured data using Pig Implemented Partitioning Dynamic Partitions Buckets in Hive Implemented Spark using Scala and SparkSQL for faster testing and processing of data Good knowledge with NoSQL databases like HBase Cassandra Handled Administration installing upgrading and managing distributions of Cassandra Advanced knowledge in performance troubleshooting and tuning Cassandra clusters Done Scaling Cassandra cluster based on lead patterns Good understanding of Cassandra Data Modeling based on applications Experience with Cassandra Performance tuning Highly involved in developmentimplementation of Cassandra environment Plan deploy monitor and maintain Amazon AWS cloud infrastructure consisting of multiple EC2 nodes and VMWare Vms as required in the environment Expertise in AWS data migration between different database platforms like SQL Server to Amazon Aurora using RDS tool Supported Map Reduce Programs those are running on the cluster Gained experience in managing and reviewing Hadoop log files Involved in scheduling Oozie workflow engine to run multiple pig jobs Responsible for developing data pipeline using flume Sqoop and Pig to extract the data from weblogs and store in HDFS Data scrubbing and processing with Oozie Developed Pig Latin scripts to extract data from the web server output files to load into HDFS Involved in developing Hive DDLs to create alter and drop tables Created and maintained technical documentation for launching Hadoop clusters and for executing Hive queries and Pig Scripts Also exploring with the Spark improving the performance and optimization of the existing algorithms in Hadoop using Spark Context Spark SQL Data Frame Pair RDDs Storm Spark YARN Used many features like Parallelize Partitioned Caching both inmemory and disk Serialization Kryo Serialization etc Environment Hadoop Cloudera Big Data HDFS MapReduce Sqoop Spark Hive HBase Linux Java Eclipse Hadoop Distribution of Cloudera PLSQL Toad 96 Windows NT MongoDB Cassandra Tableau Unix shell scripting Putty and Eclipse Hadoop Developer Barclays New York NY August 2015 to September 2016 Responsibilities Developed simple to complex MapReduce streaming jobs using Java language for processing and validating the data Developed data pipeline using MapReduce Flume Sqoop and Pig to ingest customer behavioral data into HDFS for analysis Developed MapReduce and Spark jobs to discover trends in data usage by users Implemented Spark using Python and Spark SQL for faster processing of data Implemented algorithms for real time analysis in Spark Used Spark for interactive queries processing of streaming data and integration with popular NoSQL database for huge volume of data Used the Spark Cassandra Connector to load data to and from Cassandra Real time streaming the data using Spark with Kafka Handled importing data from different data sources into HDFS using Sqoop and also performing transformations using Hive MapReduce and then loading data into HDFS Exported the analyzed data to the relational databases using Sqoop to further visualize and generate reports for the BI team Collecting and aggregating large amounts of log data using Flume and staging data in HDFS for further analysis Analyzed the data by performing Hive queries HiveQL and running Pig scripts Pig Latin to study customer behavior Used Hive to analyze the partitioned and bucketed data and compute various metrics for reporting Developed Pig Latin scripts to perform Map Reduce jobs Developed product profiles using Pig and commodity UDFs Developed Hive scripts in HiveQL to DeNormalize and Aggregate the data Created HBase tables and column families to store the user event data Written automated HBase test cases for data quality checks using HBase command line tools Created UDFs to store specialized data structures in HBase and Cassandra Scheduled and executed work flows in Oozie to run Hive and Pig jobs Used Impala to read write and query the Hadoop data in HDFS from HBase or Cassandra Used Tez framework for building high performance jobs in Pig and Hive Configured Kafka to read and write messages from external programs Configured Kafka to handle real time data Developed end to end data processing pipelines that begin with receiving data using distributed messaging systems Kafka through persistence of data into HBase Written Storm topology to emit data into Cassandra DB Written Storm topology to accept data from Kafka producer and process the data Continuous monitoring and managing the Hadoop cluster using Cloudera Manager Used JUnit framework to perform Unit testing of the application Developed interactive shell scripts for scheduling various data cleansing and data loading process Performed data validation on the data ingested using MapReduce by building a custom model to filter all the invalid data and cleanse the data Experience with data wrangling and creating workable datasets Developed schemas to handle reporting requirements using Jaspersoft Environment Hadoop MapReduce Spark Pig Hive Sqoop Oozie HBase Zookeeper Kafka Flume Solr Storm Tez Impala Mahout Cassandra Cloudera manager MySQL Jaspersoft Multinode cluster with LinuxUbuntu Windows Unix Hadoop Developer Ally Financial Auburn Hills MI November 2014 to July 2015 Responsibilities Installed and configured Hadoop Ecosystem components and Cloudera manager using CDH distribution Frequent interactions with Business partners Designed and developed a MedicareMedicaid claims system using Modeldriven architecture on a customized framework built on spring Moved data from HDFS to Cassandra using MapReduce and BulkOutputFormat class Imported trading and derivatives data in Hadoop Distributed File System and Eco System MapReduce Pig Hive Sqoop Involved in loading and transforming large sets of Structured SemiStructured and Unstructured data and analyzed them by running Hive queries and Pig scripts Created tables in HBase and loading data into HBase tables Developed scripts to load data from HBase to Hive Meta store and perform Map Reduce jobs Was part of an activity to setup Hadoop ecosystem at dev QA Environment Managed and reviewed Hadoop Log files Responsible writing Pig Script and Hive queries for data processing Running Sqoop for importing data from Oracle Other Database Creation of shell script to collect raw logs from different machines Created Partition in a Hive as static and dynamic Implemented Pig Latin scripts using operators such as LOAD STORE DUMP FILTER DISTINCT FOREACH GENERATE GROUP COGROUP ORDER LIMIT AND UNION Optimized the Hive tables using optimization techniques like partitions and bucketing to provide better performance with HiveQL queries Defined some Pig UDF for some financial functions such as swap hedging Speculation and arbitrage Coded many MapReduce program to process unstructured logs file Worked on import and export data into HDFS and Hive using Sqoop Used different data formats Text format and Avro format while loading the data into HDFS Used parameterize Pig script and optimized script using illustrate and explain Involved in the process of configuring HA Kerberos security issues and name node failure restoration activity time to time as a part of zero downtime Implemented FAIR Scheduler as well Environment Hadoop Linux MapReduce HDFS Hbase Hive Pig Shell Scripting Sqoop CDH Distribution Windows Linux Java 6 Eclipse Ant Log4j and JUnit Hadoop Developer Hitachi America Chula Vista CA May 2014 to October 2014 Responsibilities Hands on Experience in joining raw data with the reference data using Pig scripting Written custom UDFs in Hive Hands on experience in extracting data from different databases and to copy into HDFS file system using Sqoop Written Sqoop incremental import job to move newupdated info From database to HDFS Created Oozie coordinated workflow to execute Sqoop incremental job daily Used Oozie workflow engine to run multiple Hive and Pig jobs Hands on experience in exporting the results into relational databases using Sqoop for visualization and to generate reports for the BI team Involved in Installing and configuring Hive Pig Sqoop Flume and Oozie on the Hadoop cluster Worked with application teams to install Operating System Hadoop updates patches versions upgrades as required Working with clients on requirements based on their business needs Communicate deliverables status to userstakeholders client and drive periodic review meetings On time completion of tasks and the projects per quality goals Environment Hadoop HDFS MapReduce Hive Pig Sqoop HBase Oozie MySql SVN Putty Zookeeper Ganglia UNIX and Shell scripting Hadoop Developer Florida Power Light Miami FL October 2013 to April 2014 Responsibilities Integrated Kafka with Storm for real time data processing and written some storm topologies to store the processed data directly to MongoDB and HDFS Experience in writing Spark SQL scripts Imported data from different sources into Spark RDD for processing Developed custom aggregate functions using Spark SQL and performed interactive querying Involved in loading data from edge node to HDFS using shell scripting Worked on installing cluster commissioning and decommissioning of Datanode Namenode high availability capacity planning and slots configuration Completion of unit testing for the new Hadoop jobs in standalone mode designated for Unit region using MR Unit Developed Spark scripts by using Scala and Python shell commands as per the requirement Experience in managing and reviewing Hadoop log files Experience in Hive partitioning bucketing and perform joins on Hive tables and implementing Hive SerDe like REGEX JSON and Avro Optimized Hive analytics Sql queries created tablesviews written custom UDFs and Hive based exception processing Involved in transforming the Teradata to legacy lables to HDFS and HBase tables using Sqoop and vice versa Configured Fair Scheduler to provide fair resources to all the applications across the cluster Environment Hortonworks Hadoop Ambari Spark Solr Kafka MongoDB Linux HDFS Hive Pig Sqoop Flume Zookeeper RDBMS JavaJ2EE Developer Mercury Insurance Roseville CA November 2012 to September 2013 Responsibilities Write design document based on requirements from MMSEA user guide Performed requirement gathering design coding testing implementation and deployment Worked on modeling of Dialog process Business Processes and coding Business Objects QueryMapper and JUnit files Involved in the design and creation of Class diagrams Sequence diagrams and Activity Diagrams using UML models Created the Business Objects methods using Java and integrating the activity diagrams Involved in developing JSP pages using Struts custom tags jQuery and Tiles Framework Used JavaScript to perform client side validations and StrutsValidator Framework for serverside validation Worked in web services using SOAP WSDL Wrote Query Mappers and MQ Experience in JUnit Test Cases Developed the UI using XSL and JavaScript Managed software configuration using ClearCase and SVN Design develop and test features and enhancements Performed error rate analysis of production issues and technical errors Developed test environment for testing all the Web Service exposed as part of the core module and their integration with partner services in Integration test Analyze user requirement document and develop test plan which includes test objectives test strategies test environment and test priorities Responsible for performing endtoend system testing of application writing JUnit test cases Perform Functional testing Performance testing Integration testing Regression testing Smoke testing and User Acceptance Testing UAT Converted Complex SQL queries running at mainframes into pig and Hive as a part of a migration from mainframes into Hadoop cluster Environment Shell Scripting Java 6 JEE Spring Hibernate Eclipse Oracle 10g JavaScript Servlets Nodejs JMS Ant Log4j and Junit Hadoop Pig Hive Java Developer Reliance Energy LTD June 2010 to August 2012 Responsibilities Involved in the design and implementation of the architecture for the project using OOAD UML design patterns Involved in design and development of server side layer using XML JSP JDBC JNDI EJB and DAO patterns using eclipse IDE Work involved extensive usage of HTML CSS Javascript and Ajax for client side development and validations Used parsers for the conversion of XML files to java objects and vice versa Developed screens using XML documents and XSL Developed Client programs for consuming the Web services published by the Country Defaults Department which keeps in track of the information regarding life span inflation rates retirement age etc using Apache Axis Developed Java Beans and JSPs by using spring and JSTL tag libs for supplements Development of EJBs Servlets and JSP files for implementing Business rules and Security options using IBM Web Sphere Involved in creating tables stored procedures in SQL for data manipulation and retrieval using SQL Server Oracle and DB2 Trained end users on developed application Environment Java JSF Framework Eclipse IDE Ajax Apache Axis OOAD Web Logic Java script HTML XML CSS SQL Server Oracle Web services Ajax Spring OOAD and UML Windows Education Bachelors Skills JAVA 6 years SQL 6 years Hadoop 5 years HADOOP 5 years Hive 5 years Hadoop Cloudera Big Data HDFS MapReduce Sqoop Spark Hive HBase Linux Java Eclipse Hadoop Distribution of Cloudera PLSQL Toad 96 Windows NT MongoDB Cassandra Tableau Unix shell scripting Putty and Eclipse 8 years Additional Information TECHNICAL SKILL Hadoop Ecosystem Hadoop HDFS YARN MapReduce Hive Pig Oozie Zookeeper Flume Sqoop Spark Mahout Kafka Storm Scala Spark ML Impala HBase Tez Languages C C Core Java PLSQL Scala JavaJ2EE Technologies Servlets JSP JDBC Java Beans EJB RMI Web Services Frameworks EJB Struts Hibernate and Spring Scripting Languages Python R SQL Unix Shell Scripting Hive QL Pig Latin Databases MySQL Oracle 10g SQL Server 2008 NoSQL Databases HBase Cassandra MongoDB Web Technologies HTML CSS XML Javascript Ajax Nodejs SOAP WebApplication Servers WebLogic WebSphere Apache Tomcat Development Tools Eclipse Ant Putty Version Control SVN GIT Cluster Management and Monitoring Tools Ganglia Ambari Visualization Tools Tableau Methodologies AgileScrum Rational Unified Process and Waterfall Environment Win 9598 Win NT Win XP Win 7 Unix LinuxUbuntu and CentOS",
    "unique_id": "f8edb802-ab83-4182-b284-7f908a2595c5"
}