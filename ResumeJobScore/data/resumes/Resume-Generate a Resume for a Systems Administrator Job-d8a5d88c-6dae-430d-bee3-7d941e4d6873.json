{
    "clean_data": "Spark Python Developer Sparkspan lPythonspan span lDeveloperspan Spark Python Developer Lumity CA San Mateo CA Hadoop Developer with 4 years of overall IT experience in a variety of industries which includes hands on experience in Big Data tools and technologies Have 3 years of comprehensive experience in Big Data processing using Hadoop and its ecosystem MapReduce Pig Hive Sqoop Flume Spark Kafka and HBase Good working experience on Spark spark streaming spark SQL with python and Kafka Hands on experience with Spark Core Spark SQL and Data FramesData SetsRDD API Developed applications using Spark and python for data processing Strong experience and knowledge of real time data analytics using Flume and Spark Replaced existing mapreduce jobs and Hive scripts with Spark DataFrame transformation and actions Good knowledge on Spark architecture and realtime streaming using Spark Performed maintenance monitoring deployments and upgrades across infrastructure Involved in Debugging Pig and Hive scripts and used various optimization techniques in MapReduce jobs Wrote custom UDFs and UDAF for Hive and Pig core functionality Experience in importing and exporting data using Sqoop from Relational Database Systems to HDFS Good knowledge about YARN configuration Hands on experience in configuring and working with Flume to load the data from multiple web sources directly into HDFS Hands on experience working with NoSQL databases such as HBase MongoDB and Cassandra Used HBase in accordance with PIGHive as and when required for real time low latency queries Scheduled job workflow for FTP Sqoop and hive scripts using Oozie and Oozie coordinators Developed various shell scripts and python scripts to automate Spark jobs and hive scripts Generated Java APIs for retrieval and analysis on NoSQL database such as HBase and Cassandra Imported the data from different sources like AWS S3 Local file system into Spark RDD and worked on cloud Amazon Web Services EMR S3 EC2 RedShift Lambda Integrated clusters with Active Directory for Kerberos and User Authentication Authorization Experience with all stages of the SDLC and Agile Development model right from the requirement gathering to Deployment and production support Involved in daily SCRUM meetings to discuss the developmentprogress and was active in making scrum meetings more productive Work Experience Spark Python Developer Lumity CA October 2017 to Present Description Lumity is benefit management company where we provide environment to our clients and carriers Responsibilities Working on Big Data infrastructure for batch processing as well as realtime processing Responsible for building scalable distributed data solutions using Hadoop Used SparkStreaming APIs to perform necessary transformations and actions on the fly for building the common learner data model which gets the data from Kafka in near real time and Persists into Cassandra Developed Spark scripts by using Python shell commands as per the requirement Experience in designing and developing applications in Spark using Python to compare the performance of Spark with Hive and SQLOracle Involved in converting HiveSQL queries into Spark transformations using Spark RDDs Python Experience in both SQL Context and Spark Session and implementing Log Error Alarmed in Spark Optimized Hive QL pig scripts by using execution engine like TEZ Spark Tested Apache TEZ an extensible framework for building high performance batch and interactive data processing applications on Pig and Hive jobs Experience in managing nodes on Hadoop cluster and monitor Hadoop cluster job performance using Cloudera manager Managing and scheduling Jobs on a Hadoop Cloudera cluster using Oozie work flows and java schedulers Used Spark API over Cloudera Hadoop YARN to perform analytics on data in Hive and also involved in creating Hive Tables loading with data and writing Hive queries which will invoke and run Map Reduce jobs in the backend Designed and implemented Incremental Imports into Hive tables Created Partitions and Bucketing concepts in Hive and designed both Managed and External tables in Hive to optimize performance Experience in importing and exporting tera bytes of data using Sqoop from Relational Database Systems to HDFS Moved Relational Database data using Sqoop into Hive Dynamic partition tables using staging tables Involved in collecting aggregating and moving data from servers to HDFS using Apache Flume Involved in developing Pig Scripts for change data capture and delta record processing between newly arrived data and already existing data in HDFS Migrated ETL jobs to Pig scripts do transformations even joins and some preaggregations before storing the data onto HDFS Implemented the workflows using Apache Oozie framework to automate tasks Used Zookeeper to coordinate cluster services Worked on different file formats like Sequence files XML files and Map files using Map Reduce Programs Used Impala where ever possible to achieve faster results compared to Hive during data Analysis Configured deployed and maintained multinode Dev and Test Kafka Clusters and implemented data ingestion and handling clusters in real time processing using Kafka Worked on writing transformermapping MapReduce pipelines using Java Transform the logs data into data model using apache pig and written UDFs functions to format the logs data Setting up Confluent Kafka for ingesting large volumes of eventslogs into Hadoop Developed and Configured Kafka brokers to pipeline server logs data into spark streaming Used Teradata Data Mover to copy data and objects such as tables and statistics from one system to another involved in Analyzing building Teradata EDW using Teradata ETL utilities and Informatica Implemented usage of Amazon EMR for processing Big Data across a Hadoop Cluster of virtual servers on Amazon Elastic Compute Cloud EC2 and Amazon Simple Storage Service S3 Implemented installation and configuration of multinode cluster on the cloud using Amazon Web Services AWS on EC2 Used Amazon Web Services S3 to store large amount of data in identicalsimilar repository Environment Hadoop HDFS Pig Hive Sqoop Flume Kafka Spark TEZ Storm Shell Scripting HBase Python scripting Kerberos Agile Zookeeper Maven AWS AWS EMR MySQL HadoopBigdata Developer Apple CA January 2017 to September 2017 Description The purpose of the project is to analyze the data coming from the various sources into the Hadoop data center unit Created programs to process large volumes of data through a lot of prepay concepts which analyze produce suspect claims and it helps to generate Datasets for visualization These suspect claims verified again and it saves millions of dollars to the company every year Responsibilities Involved in implementation of Hadoop Cluster and Hive for Development and Test Environment Worked on analyzing Hadoop Cluster and different big data analytic tools including Pig Hive and MongoDB Extracted files from MongoDB through Sqoop and placed in HDFS for processed Designed and developed functionality to get JSON document from MongoDB document store and send it to client using RESTful web service Successfully loaded files to Hive and HDFS from MongoDB Exploring with the Spark for improving the performance and optimization of the existing algorithms in Hadoop using Spark Context SparkSQL Data Frame Pair RDDs Spark YARN Experienced with batch processing of data sources using Apache Spark and developing predictive analytic using Apache Spark Scala APIs Imported millions of structured data from relational databases using Sqoop import to process using Spark and stored the data into HDFS in CSV format and developed Spark streaming application to pull data from cloud to hive table Involved in importing data from Oracle into HDFS and Hive using Sqoop Created tables using Impala and involved in creating Queries which are stored in HBase Implemented complex scripts to support test driven development and continuous integration Improving the performance and optimization of existing algorithms in Hadoop using Spark context SparkSQL and Spark YARN using Scala Analyzed the data as per the business requirements using Hive queries Installed and configured Hadoop stack and different big data analytic tools export and imports from data preprocessing Collected and aggregated large amounts of log data using Apache Flume and staging data in HDFS for further analysis Hands on experience in writing custom UDFs and also custom input and output formats and created Hive Tables loaded values and generated ad hocreports using the table data Showcased strong understanding on Hadoop architecture including HDFS MapReduce Hive Pig Sqoop and Oozie Used spark with Yarn and got performance results compared with MapReduce Loaded existing data warehouse data from Oracle database to Hadoop Distributed File System HDFS Developed MapReduce programs in Java to search production logs and web analytics logs for use cases like application issues measure page download performance respectively Hadoop Developer Intern Watts Water Technologies AZ January 2016 to December 2016 Responsibilities Responsible for analyzing large data sets and derive customer usage patterns by developing new MapReduce programs Written MapReduce code to parse the data from various sources and storing parsed data into HBase and Hive Created HBase tables to store different formats of data as a backend for user portals Developed Kafka producer and consumers HBase clients Apache Spark and Hadoop MapReduce jobs along with components on HDFS Hive Worked on creating combiners partitions and distributed cache to improve the performance of MapReduce jobs Developed Shell Script to perform data profiling on the ingested data with the help of HIVE Bucketing Developed Hive UDF for performing Hashing mechanism on the Hive Column Involved in creating Hive tables loading with data and writing Hive queries which will run internally in map reduce way Used Hive to analyze the partitioned and bucketed data and compute various metrics for reporting Written Hive jobs to parse the logs and structure them in tabular format to facilitate effective querying on the log data Writing Hive join query to fetch info from multiple tables writing multiple Map Reduce jobs to collect output from Hive Ingest data into Hadoop HiveHDFS from different data resources Involved in loading and transforming large sets of Structured SemiStructured and Unstructured data and analyzed them by running Hive queries and Pig scripts Experienced in writing Hive validation scripts that are used in validation framework for daily analysis through graphs and presented to business users Developed workflow in Oozie to automate the tasks of loading data into HDFS and preprocessing with Pig and Hive Developed code in Python to use MapReduce framework by Hadoop streaming Used Pig as ETL tool to do transformations joins and some preaggregations before storing the data into HDFS Imported all the customer specific personal data to Hadoop using Sqoop component from various relational databases like Netezza and Oracle Develop testing scripts in Python and prepare test procedures analyze test results data and suggest improvements of the system and software Experience in streaming log data using Flume and data analytics using Hive Extracted the data from RDBMS Oracle MySQL Teradata to HDFS using Sqoop Environment Hadoop MapReduce HDFS Pig HiveQL Oozie Flume Impala Cloudera MySQL Shell Scripting HBase JavaJ2EE Developer Talent Nid Pvt Ltd Ind January 2014 to December 2014 Description This Project aims to implement the infrastructure of Java Message Service JMS This project developed in J2EE package using JMS APIs provides services for Exchange for message between components in a distributed environment It supports both Synchronous and Asynchronous messaging and the receiver receives the message according to selection of the message format The message will be stored in Database and it will be retrieved whenever sender or receiver requires Responsibilities Involved in Full Life Cycle Development in Distributed Environment using Java and J2EE framework Designed the application by implementing Struts Framework based on MVC Architecture Designed and developed the front end using JSP HTML and JavaScript and jQuery Implemented the Web Service client for the login authentication credit reports and applicant information Apache Axis 2 Web Service Extensively worked on User Interface for few modules using JSPs JavaScript and Ajax Developed framework for data processing using Design patterns Java XML Used the lightweight container of the Spring Framework to provide architectural flexibility for Inversion of Controller IOC Used Hibernate ORM framework with spring framework for data persistence and transaction management Designed and developed Session beans to implement the Business logic Developed EJB components that are deployed on Web logic Application Server Written unit tests using JUnit Framework and Logging is done using Log4J Framework Designed and developed various configuration files for Hibernate mappings Designed and documented RESTHTTP APIs including JSON data formats and API versioning strategy Developed Web Services for sending and getting data from different applications using SOAP messages Actively involved in code reviews and bug fixing Applied CSS Cascading style Sheets for entire site for standardization of the site Assisted QA Team in defining and implementing a defect resolution process including defect priority and severity Environment Java 50 Struts Spring 20 Hibernate 32 Web Logic 70 Eclipse 33 Oracle JUnit 42 Maven Windows XP HTML CSS JavaScript and XML Education Masters in Computer Science California State University January 2015 to December 2016 Bachelors in Computer Science Engineering in Computer Science Engineering Sri Padmavathi University August 2010 to May 2014",
    "entities": [
        "JUnit Framework",
        "Spark Context",
        "RedShift Lambda Integrated",
        "Oozie Used",
        "Sqoop Created",
        "User Authentication Authorization",
        "XML Education Masters",
        "Hadoop Developed",
        "Amazon Web Services S3",
        "Amazon Elastic Compute Cloud EC2",
        "Netezza",
        "Amazon Web Services AWS",
        "Hadoop",
        "XML",
        "Structured SemiStructured",
        "SOAP",
        "HBase",
        "Amazon Simple Storage Service",
        "Apache Spark",
        "Cloudera Hadoop",
        "Spark with",
        "HDFS MapReduce Hive Pig",
        "Python",
        "SparkSQL",
        "Developed",
        "UDAF for Hive and Pig",
        "Informatica Implemented",
        "Yarn",
        "Responsibilities Involved",
        "Hadoop MapReduce",
        "Oracle JUnit",
        "Hive Extracted",
        "Sequence",
        "Hive Ingest",
        "Applied CSS Cascading",
        "Responsibilities Working on Big Data",
        "Developed Web Services",
        "JSP",
        "Spark Performed",
        "Unstructured",
        "FTP Sqoop",
        "HBase Implemented",
        "Sqoop Environment Hadoop MapReduce HDFS Pig HiveQL",
        "jQuery Implemented",
        "Environment Hadoop HDFS Pig Hive Sqoop",
        "Hadoop Cluster",
        "Created Partitions",
        "HDFS Moved Relational Database",
        "Incremental Imports",
        "Spark",
        "EJB",
        "Agile Development",
        "Amazon EMR",
        "CSV",
        "API",
        "Ajax Developed",
        "Hadoop Developer Intern Watts Water Technologies",
        "Database",
        "Sqoop",
        "Computer Science Engineering",
        "SQLOracle Involved",
        "Created",
        "Spark Core Spark",
        "Assisted QA Team",
        "Oracle",
        "MapReduce Pig Hive Sqoop",
        "Shell Scripting HBase JavaJ2EE Developer Talent Nid Pvt Ltd Ind",
        "HBase Good",
        "CA Hadoop Developer",
        "MapReduce Loaded",
        "Teradata ETL",
        "log data",
        "Oozie",
        "Analysis Configured",
        "Collected",
        "SQL",
        "Amazon Web Services",
        "Computer Science Engineering Sri Padmavathi University",
        "Relational Database Systems",
        "Spark DataFrame",
        "Big Data",
        "Hive",
        "Used Spark API",
        "MVC Architecture Designed",
        "Hadoop Cloudera",
        "ETL",
        "the Spring Framework",
        "AWS S3 Local",
        "Application Server Written",
        "Work Experience Spark Python Developer Lumity",
        "HDFS Migrated ETL",
        "Impala",
        "Spark Session",
        "Hadoop Distributed File System HDFS Developed MapReduce",
        "Hive Created HBase",
        "Inversion of Controller IOC Used Hibernate",
        "Active Directory for Kerberos and",
        "Java Message Service",
        "Data",
        "Oracle Develop",
        "MapReduce",
        "NoSQL",
        "Map",
        "Bucketing",
        "Spark Python Developer Sparkspan"
    ],
    "experience": "Experience in importing and exporting data using Sqoop from Relational Database Systems to HDFS Good knowledge about YARN configuration Hands on experience in configuring and working with Flume to load the data from multiple web sources directly into HDFS Hands on experience working with NoSQL databases such as HBase MongoDB and Cassandra Used HBase in accordance with PIGHive as and when required for real time low latency queries Scheduled job workflow for FTP Sqoop and hive scripts using Oozie and Oozie coordinators Developed various shell scripts and python scripts to automate Spark jobs and hive scripts Generated Java APIs for retrieval and analysis on NoSQL database such as HBase and Cassandra Imported the data from different sources like AWS S3 Local file system into Spark RDD and worked on cloud Amazon Web Services EMR S3 EC2 RedShift Lambda Integrated clusters with Active Directory for Kerberos and User Authentication Authorization Experience with all stages of the SDLC and Agile Development model right from the requirement gathering to Deployment and production support Involved in daily SCRUM meetings to discuss the developmentprogress and was active in making scrum meetings more productive Work Experience Spark Python Developer Lumity CA October 2017 to Present Description Lumity is benefit management company where we provide environment to our clients and carriers Responsibilities Working on Big Data infrastructure for batch processing as well as realtime processing Responsible for building scalable distributed data solutions using Hadoop Used SparkStreaming APIs to perform necessary transformations and actions on the fly for building the common learner data model which gets the data from Kafka in near real time and Persists into Cassandra Developed Spark scripts by using Python shell commands as per the requirement Experience in designing and developing applications in Spark using Python to compare the performance of Spark with Hive and SQLOracle Involved in converting HiveSQL queries into Spark transformations using Spark RDDs Python Experience in both SQL Context and Spark Session and implementing Log Error Alarmed in Spark Optimized Hive QL pig scripts by using execution engine like TEZ Spark Tested Apache TEZ an extensible framework for building high performance batch and interactive data processing applications on Pig and Hive jobs Experience in managing nodes on Hadoop cluster and monitor Hadoop cluster job performance using Cloudera manager Managing and scheduling Jobs on a Hadoop Cloudera cluster using Oozie work flows and java schedulers Used Spark API over Cloudera Hadoop YARN to perform analytics on data in Hive and also involved in creating Hive Tables loading with data and writing Hive queries which will invoke and run Map Reduce jobs in the backend Designed and implemented Incremental Imports into Hive tables Created Partitions and Bucketing concepts in Hive and designed both Managed and External tables in Hive to optimize performance Experience in importing and exporting tera bytes of data using Sqoop from Relational Database Systems to HDFS Moved Relational Database data using Sqoop into Hive Dynamic partition tables using staging tables Involved in collecting aggregating and moving data from servers to HDFS using Apache Flume Involved in developing Pig Scripts for change data capture and delta record processing between newly arrived data and already existing data in HDFS Migrated ETL jobs to Pig scripts do transformations even joins and some preaggregations before storing the data onto HDFS Implemented the workflows using Apache Oozie framework to automate tasks Used Zookeeper to coordinate cluster services Worked on different file formats like Sequence files XML files and Map files using Map Reduce Programs Used Impala where ever possible to achieve faster results compared to Hive during data Analysis Configured deployed and maintained multinode Dev and Test Kafka Clusters and implemented data ingestion and handling clusters in real time processing using Kafka Worked on writing transformermapping MapReduce pipelines using Java Transform the logs data into data model using apache pig and written UDFs functions to format the logs data Setting up Confluent Kafka for ingesting large volumes of eventslogs into Hadoop Developed and Configured Kafka brokers to pipeline server logs data into spark streaming Used Teradata Data Mover to copy data and objects such as tables and statistics from one system to another involved in Analyzing building Teradata EDW using Teradata ETL utilities and Informatica Implemented usage of Amazon EMR for processing Big Data across a Hadoop Cluster of virtual servers on Amazon Elastic Compute Cloud EC2 and Amazon Simple Storage Service S3 Implemented installation and configuration of multinode cluster on the cloud using Amazon Web Services AWS on EC2 Used Amazon Web Services S3 to store large amount of data in identicalsimilar repository Environment Hadoop HDFS Pig Hive Sqoop Flume Kafka Spark TEZ Storm Shell Scripting HBase Python scripting Kerberos Agile Zookeeper Maven AWS AWS EMR MySQL HadoopBigdata Developer Apple CA January 2017 to September 2017 Description The purpose of the project is to analyze the data coming from the various sources into the Hadoop data center unit Created programs to process large volumes of data through a lot of prepay concepts which analyze produce suspect claims and it helps to generate Datasets for visualization These suspect claims verified again and it saves millions of dollars to the company every year Responsibilities Involved in implementation of Hadoop Cluster and Hive for Development and Test Environment Worked on analyzing Hadoop Cluster and different big data analytic tools including Pig Hive and MongoDB Extracted files from MongoDB through Sqoop and placed in HDFS for processed Designed and developed functionality to get JSON document from MongoDB document store and send it to client using RESTful web service Successfully loaded files to Hive and HDFS from MongoDB Exploring with the Spark for improving the performance and optimization of the existing algorithms in Hadoop using Spark Context SparkSQL Data Frame Pair RDDs Spark YARN Experienced with batch processing of data sources using Apache Spark and developing predictive analytic using Apache Spark Scala APIs Imported millions of structured data from relational databases using Sqoop import to process using Spark and stored the data into HDFS in CSV format and developed Spark streaming application to pull data from cloud to hive table Involved in importing data from Oracle into HDFS and Hive using Sqoop Created tables using Impala and involved in creating Queries which are stored in HBase Implemented complex scripts to support test driven development and continuous integration Improving the performance and optimization of existing algorithms in Hadoop using Spark context SparkSQL and Spark YARN using Scala Analyzed the data as per the business requirements using Hive queries Installed and configured Hadoop stack and different big data analytic tools export and imports from data preprocessing Collected and aggregated large amounts of log data using Apache Flume and staging data in HDFS for further analysis Hands on experience in writing custom UDFs and also custom input and output formats and created Hive Tables loaded values and generated ad hocreports using the table data Showcased strong understanding on Hadoop architecture including HDFS MapReduce Hive Pig Sqoop and Oozie Used spark with Yarn and got performance results compared with MapReduce Loaded existing data warehouse data from Oracle database to Hadoop Distributed File System HDFS Developed MapReduce programs in Java to search production logs and web analytics logs for use cases like application issues measure page download performance respectively Hadoop Developer Intern Watts Water Technologies AZ January 2016 to December 2016 Responsibilities Responsible for analyzing large data sets and derive customer usage patterns by developing new MapReduce programs Written MapReduce code to parse the data from various sources and storing parsed data into HBase and Hive Created HBase tables to store different formats of data as a backend for user portals Developed Kafka producer and consumers HBase clients Apache Spark and Hadoop MapReduce jobs along with components on HDFS Hive Worked on creating combiners partitions and distributed cache to improve the performance of MapReduce jobs Developed Shell Script to perform data profiling on the ingested data with the help of HIVE Bucketing Developed Hive UDF for performing Hashing mechanism on the Hive Column Involved in creating Hive tables loading with data and writing Hive queries which will run internally in map reduce way Used Hive to analyze the partitioned and bucketed data and compute various metrics for reporting Written Hive jobs to parse the logs and structure them in tabular format to facilitate effective querying on the log data Writing Hive join query to fetch info from multiple tables writing multiple Map Reduce jobs to collect output from Hive Ingest data into Hadoop HiveHDFS from different data resources Involved in loading and transforming large sets of Structured SemiStructured and Unstructured data and analyzed them by running Hive queries and Pig scripts Experienced in writing Hive validation scripts that are used in validation framework for daily analysis through graphs and presented to business users Developed workflow in Oozie to automate the tasks of loading data into HDFS and preprocessing with Pig and Hive Developed code in Python to use MapReduce framework by Hadoop streaming Used Pig as ETL tool to do transformations joins and some preaggregations before storing the data into HDFS Imported all the customer specific personal data to Hadoop using Sqoop component from various relational databases like Netezza and Oracle Develop testing scripts in Python and prepare test procedures analyze test results data and suggest improvements of the system and software Experience in streaming log data using Flume and data analytics using Hive Extracted the data from RDBMS Oracle MySQL Teradata to HDFS using Sqoop Environment Hadoop MapReduce HDFS Pig HiveQL Oozie Flume Impala Cloudera MySQL Shell Scripting HBase JavaJ2EE Developer Talent Nid Pvt Ltd Ind January 2014 to December 2014 Description This Project aims to implement the infrastructure of Java Message Service JMS This project developed in J2EE package using JMS APIs provides services for Exchange for message between components in a distributed environment It supports both Synchronous and Asynchronous messaging and the receiver receives the message according to selection of the message format The message will be stored in Database and it will be retrieved whenever sender or receiver requires Responsibilities Involved in Full Life Cycle Development in Distributed Environment using Java and J2EE framework Designed the application by implementing Struts Framework based on MVC Architecture Designed and developed the front end using JSP HTML and JavaScript and jQuery Implemented the Web Service client for the login authentication credit reports and applicant information Apache Axis 2 Web Service Extensively worked on User Interface for few modules using JSPs JavaScript and Ajax Developed framework for data processing using Design patterns Java XML Used the lightweight container of the Spring Framework to provide architectural flexibility for Inversion of Controller IOC Used Hibernate ORM framework with spring framework for data persistence and transaction management Designed and developed Session beans to implement the Business logic Developed EJB components that are deployed on Web logic Application Server Written unit tests using JUnit Framework and Logging is done using Log4J Framework Designed and developed various configuration files for Hibernate mappings Designed and documented RESTHTTP APIs including JSON data formats and API versioning strategy Developed Web Services for sending and getting data from different applications using SOAP messages Actively involved in code reviews and bug fixing Applied CSS Cascading style Sheets for entire site for standardization of the site Assisted QA Team in defining and implementing a defect resolution process including defect priority and severity Environment Java 50 Struts Spring 20 Hibernate 32 Web Logic 70 Eclipse 33 Oracle JUnit 42 Maven Windows XP HTML CSS JavaScript and XML Education Masters in Computer Science California State University January 2015 to December 2016 Bachelors in Computer Science Engineering in Computer Science Engineering Sri Padmavathi University August 2010 to May 2014",
    "extracted_keywords": [
        "Spark",
        "Python",
        "Developer",
        "Sparkspan",
        "lPythonspan",
        "span",
        "lDeveloperspan",
        "Spark",
        "Python",
        "Developer",
        "Lumity",
        "CA",
        "San",
        "Mateo",
        "CA",
        "Hadoop",
        "Developer",
        "years",
        "IT",
        "experience",
        "variety",
        "industries",
        "hands",
        "experience",
        "Big",
        "Data",
        "tools",
        "technologies",
        "years",
        "experience",
        "Big",
        "Data",
        "processing",
        "Hadoop",
        "ecosystem",
        "MapReduce",
        "Pig",
        "Hive",
        "Sqoop",
        "Flume",
        "Spark",
        "Kafka",
        "HBase",
        "working",
        "experience",
        "Spark",
        "spark",
        "streaming",
        "spark",
        "SQL",
        "python",
        "Kafka",
        "Hands",
        "experience",
        "Spark",
        "Core",
        "Spark",
        "SQL",
        "Data",
        "FramesData",
        "API",
        "applications",
        "Spark",
        "python",
        "data",
        "experience",
        "knowledge",
        "time",
        "data",
        "analytics",
        "Flume",
        "Spark",
        "mapreduce",
        "jobs",
        "Hive",
        "scripts",
        "Spark",
        "DataFrame",
        "transformation",
        "actions",
        "knowledge",
        "Spark",
        "architecture",
        "streaming",
        "Spark",
        "Performed",
        "maintenance",
        "monitoring",
        "deployments",
        "upgrades",
        "infrastructure",
        "Debugging",
        "Pig",
        "Hive",
        "scripts",
        "optimization",
        "techniques",
        "MapReduce",
        "jobs",
        "custom",
        "UDFs",
        "UDAF",
        "Hive",
        "Pig",
        "core",
        "functionality",
        "Experience",
        "data",
        "Sqoop",
        "Relational",
        "Database",
        "Systems",
        "knowledge",
        "YARN",
        "configuration",
        "Hands",
        "experience",
        "configuring",
        "Flume",
        "data",
        "web",
        "sources",
        "HDFS",
        "Hands",
        "experience",
        "NoSQL",
        "databases",
        "HBase",
        "MongoDB",
        "Cassandra",
        "Used",
        "HBase",
        "accordance",
        "PIGHive",
        "time",
        "latency",
        "job",
        "workflow",
        "FTP",
        "Sqoop",
        "hive",
        "scripts",
        "Oozie",
        "Oozie",
        "coordinators",
        "shell",
        "scripts",
        "scripts",
        "Spark",
        "jobs",
        "hive",
        "scripts",
        "Java",
        "APIs",
        "retrieval",
        "analysis",
        "NoSQL",
        "database",
        "HBase",
        "Cassandra",
        "data",
        "sources",
        "AWS",
        "S3",
        "file",
        "system",
        "Spark",
        "RDD",
        "cloud",
        "Amazon",
        "Web",
        "Services",
        "EMR",
        "S3",
        "EC2",
        "RedShift",
        "Lambda",
        "Integrated",
        "clusters",
        "Active",
        "Directory",
        "Kerberos",
        "User",
        "Authentication",
        "Authorization",
        "Experience",
        "stages",
        "SDLC",
        "Agile",
        "Development",
        "model",
        "requirement",
        "gathering",
        "Deployment",
        "production",
        "support",
        "SCRUM",
        "meetings",
        "developmentprogress",
        "scrum",
        "meetings",
        "Work",
        "Experience",
        "Spark",
        "Python",
        "Developer",
        "Lumity",
        "CA",
        "October",
        "Present",
        "Description",
        "Lumity",
        "benefit",
        "management",
        "company",
        "environment",
        "clients",
        "carriers",
        "Responsibilities",
        "Big",
        "Data",
        "infrastructure",
        "batch",
        "processing",
        "processing",
        "data",
        "solutions",
        "Hadoop",
        "Used",
        "SparkStreaming",
        "APIs",
        "transformations",
        "actions",
        "fly",
        "learner",
        "data",
        "model",
        "data",
        "Kafka",
        "time",
        "Persists",
        "Cassandra",
        "Developed",
        "Spark",
        "scripts",
        "Python",
        "shell",
        "commands",
        "requirement",
        "Experience",
        "applications",
        "Spark",
        "Python",
        "performance",
        "Spark",
        "Hive",
        "SQLOracle",
        "HiveSQL",
        "queries",
        "Spark",
        "transformations",
        "Spark",
        "RDDs",
        "Python",
        "Experience",
        "SQL",
        "Context",
        "Spark",
        "Session",
        "Log",
        "Error",
        "Alarmed",
        "Spark",
        "Hive",
        "QL",
        "pig",
        "scripts",
        "execution",
        "engine",
        "TEZ",
        "Spark",
        "Tested",
        "Apache",
        "TEZ",
        "framework",
        "performance",
        "batch",
        "data",
        "processing",
        "applications",
        "Pig",
        "Hive",
        "jobs",
        "Experience",
        "nodes",
        "Hadoop",
        "cluster",
        "Hadoop",
        "cluster",
        "job",
        "performance",
        "Cloudera",
        "manager",
        "Managing",
        "scheduling",
        "Jobs",
        "Hadoop",
        "Cloudera",
        "cluster",
        "Oozie",
        "work",
        "flows",
        "schedulers",
        "Spark",
        "API",
        "Cloudera",
        "Hadoop",
        "YARN",
        "analytics",
        "data",
        "Hive",
        "Hive",
        "Tables",
        "data",
        "Hive",
        "queries",
        "Map",
        "Reduce",
        "jobs",
        "backend",
        "Incremental",
        "Imports",
        "Hive",
        "tables",
        "Created",
        "Partitions",
        "Bucketing",
        "concepts",
        "Hive",
        "Managed",
        "tables",
        "Hive",
        "performance",
        "Experience",
        "tera",
        "bytes",
        "data",
        "Sqoop",
        "Relational",
        "Database",
        "Systems",
        "HDFS",
        "Moved",
        "Relational",
        "Database",
        "data",
        "Sqoop",
        "Hive",
        "partition",
        "tables",
        "staging",
        "tables",
        "data",
        "servers",
        "HDFS",
        "Apache",
        "Flume",
        "Pig",
        "Scripts",
        "change",
        "data",
        "capture",
        "delta",
        "record",
        "processing",
        "data",
        "data",
        "HDFS",
        "ETL",
        "jobs",
        "scripts",
        "transformations",
        "preaggregations",
        "data",
        "HDFS",
        "workflows",
        "Apache",
        "Oozie",
        "framework",
        "tasks",
        "Zookeeper",
        "cluster",
        "services",
        "file",
        "formats",
        "Sequence",
        "files",
        "XML",
        "files",
        "Map",
        "files",
        "Map",
        "Reduce",
        "Programs",
        "Impala",
        "results",
        "Hive",
        "data",
        "Analysis",
        "Configured",
        "multinode",
        "Dev",
        "Test",
        "Kafka",
        "Clusters",
        "data",
        "ingestion",
        "clusters",
        "time",
        "processing",
        "Kafka",
        "writing",
        "MapReduce",
        "pipelines",
        "Java",
        "Transform",
        "logs",
        "data",
        "data",
        "model",
        "apache",
        "pig",
        "UDFs",
        "functions",
        "logs",
        "data",
        "Confluent",
        "Kafka",
        "volumes",
        "eventslogs",
        "Hadoop",
        "Developed",
        "Configured",
        "Kafka",
        "brokers",
        "pipeline",
        "server",
        "logs",
        "data",
        "spark",
        "streaming",
        "Teradata",
        "Data",
        "Mover",
        "data",
        "objects",
        "tables",
        "statistics",
        "system",
        "building",
        "Teradata",
        "EDW",
        "Teradata",
        "ETL",
        "utilities",
        "Informatica",
        "usage",
        "Amazon",
        "EMR",
        "Big",
        "Data",
        "Hadoop",
        "Cluster",
        "servers",
        "Amazon",
        "Elastic",
        "Compute",
        "Cloud",
        "EC2",
        "Amazon",
        "Simple",
        "Storage",
        "Service",
        "S3",
        "installation",
        "configuration",
        "multinode",
        "cluster",
        "cloud",
        "Amazon",
        "Web",
        "Services",
        "AWS",
        "EC2",
        "Amazon",
        "Web",
        "Services",
        "S3",
        "amount",
        "data",
        "repository",
        "Environment",
        "Hadoop",
        "HDFS",
        "Pig",
        "Hive",
        "Sqoop",
        "Flume",
        "Kafka",
        "Spark",
        "TEZ",
        "Storm",
        "Shell",
        "Scripting",
        "HBase",
        "Python",
        "Kerberos",
        "Agile",
        "Zookeeper",
        "Maven",
        "AWS",
        "EMR",
        "MySQL",
        "HadoopBigdata",
        "Developer",
        "Apple",
        "CA",
        "January",
        "September",
        "Description",
        "purpose",
        "project",
        "data",
        "sources",
        "Hadoop",
        "data",
        "center",
        "unit",
        "Created",
        "programs",
        "volumes",
        "data",
        "lot",
        "prepay",
        "concepts",
        "claims",
        "Datasets",
        "visualization",
        "claims",
        "millions",
        "dollars",
        "company",
        "year",
        "Responsibilities",
        "implementation",
        "Hadoop",
        "Cluster",
        "Hive",
        "Development",
        "Test",
        "Environment",
        "Hadoop",
        "Cluster",
        "data",
        "tools",
        "Pig",
        "Hive",
        "files",
        "Sqoop",
        "HDFS",
        "functionality",
        "document",
        "MongoDB",
        "document",
        "store",
        "client",
        "web",
        "service",
        "files",
        "Hive",
        "HDFS",
        "Spark",
        "performance",
        "optimization",
        "algorithms",
        "Hadoop",
        "Spark",
        "Context",
        "SparkSQL",
        "Data",
        "Frame",
        "Pair",
        "RDDs",
        "Spark",
        "YARN",
        "batch",
        "processing",
        "data",
        "sources",
        "Apache",
        "Spark",
        "analytic",
        "Apache",
        "Spark",
        "Scala",
        "APIs",
        "millions",
        "data",
        "databases",
        "Sqoop",
        "import",
        "process",
        "Spark",
        "data",
        "HDFS",
        "CSV",
        "format",
        "Spark",
        "streaming",
        "application",
        "data",
        "cloud",
        "table",
        "data",
        "Oracle",
        "HDFS",
        "Hive",
        "Sqoop",
        "Created",
        "tables",
        "Impala",
        "Queries",
        "HBase",
        "scripts",
        "test",
        "development",
        "integration",
        "performance",
        "optimization",
        "algorithms",
        "Hadoop",
        "Spark",
        "context",
        "SparkSQL",
        "Spark",
        "YARN",
        "Scala",
        "data",
        "business",
        "requirements",
        "Hive",
        "queries",
        "Hadoop",
        "stack",
        "data",
        "tools",
        "export",
        "imports",
        "data",
        "Collected",
        "amounts",
        "log",
        "data",
        "Apache",
        "Flume",
        "data",
        "HDFS",
        "analysis",
        "Hands",
        "experience",
        "custom",
        "UDFs",
        "input",
        "output",
        "formats",
        "Hive",
        "Tables",
        "values",
        "ad",
        "hocreports",
        "table",
        "data",
        "understanding",
        "Hadoop",
        "architecture",
        "HDFS",
        "MapReduce",
        "Hive",
        "Pig",
        "Sqoop",
        "Oozie",
        "spark",
        "Yarn",
        "performance",
        "results",
        "MapReduce",
        "data",
        "warehouse",
        "data",
        "Oracle",
        "database",
        "Hadoop",
        "Distributed",
        "File",
        "System",
        "HDFS",
        "MapReduce",
        "programs",
        "Java",
        "production",
        "logs",
        "web",
        "analytics",
        "logs",
        "use",
        "cases",
        "application",
        "issues",
        "page",
        "download",
        "performance",
        "Hadoop",
        "Developer",
        "Intern",
        "Watts",
        "Water",
        "Technologies",
        "AZ",
        "January",
        "December",
        "Responsibilities",
        "data",
        "sets",
        "customer",
        "usage",
        "patterns",
        "MapReduce",
        "programs",
        "MapReduce",
        "code",
        "data",
        "sources",
        "data",
        "HBase",
        "Hive",
        "Created",
        "HBase",
        "formats",
        "data",
        "backend",
        "user",
        "Developed",
        "Kafka",
        "producer",
        "consumers",
        "HBase",
        "Apache",
        "Spark",
        "Hadoop",
        "MapReduce",
        "jobs",
        "components",
        "HDFS",
        "Hive",
        "Worked",
        "combiners",
        "partitions",
        "cache",
        "performance",
        "MapReduce",
        "jobs",
        "Developed",
        "Shell",
        "Script",
        "data",
        "profiling",
        "data",
        "help",
        "HIVE",
        "Bucketing",
        "Developed",
        "Hive",
        "UDF",
        "mechanism",
        "Hive",
        "Column",
        "Hive",
        "tables",
        "data",
        "Hive",
        "queries",
        "map",
        "way",
        "Hive",
        "data",
        "metrics",
        "Written",
        "Hive",
        "jobs",
        "logs",
        "format",
        "querying",
        "log",
        "data",
        "Hive",
        "query",
        "info",
        "tables",
        "Map",
        "Reduce",
        "jobs",
        "output",
        "Hive",
        "Ingest",
        "data",
        "Hadoop",
        "HiveHDFS",
        "data",
        "resources",
        "loading",
        "sets",
        "Structured",
        "SemiStructured",
        "Unstructured",
        "data",
        "Hive",
        "queries",
        "Pig",
        "scripts",
        "Hive",
        "validation",
        "scripts",
        "validation",
        "framework",
        "analysis",
        "graphs",
        "business",
        "users",
        "workflow",
        "Oozie",
        "tasks",
        "loading",
        "data",
        "HDFS",
        "Pig",
        "Hive",
        "Developed",
        "code",
        "Python",
        "MapReduce",
        "framework",
        "Hadoop",
        "streaming",
        "Pig",
        "ETL",
        "tool",
        "transformations",
        "joins",
        "preaggregations",
        "data",
        "HDFS",
        "customer",
        "data",
        "Hadoop",
        "Sqoop",
        "component",
        "databases",
        "Netezza",
        "Oracle",
        "Develop",
        "testing",
        "scripts",
        "Python",
        "test",
        "procedures",
        "test",
        "results",
        "data",
        "improvements",
        "system",
        "software",
        "Experience",
        "log",
        "data",
        "Flume",
        "data",
        "analytics",
        "Hive",
        "data",
        "RDBMS",
        "Oracle",
        "MySQL",
        "Teradata",
        "HDFS",
        "Sqoop",
        "Environment",
        "Hadoop",
        "MapReduce",
        "HDFS",
        "Pig",
        "HiveQL",
        "Oozie",
        "Flume",
        "Impala",
        "Cloudera",
        "MySQL",
        "Shell",
        "Scripting",
        "HBase",
        "JavaJ2EE",
        "Developer",
        "Talent",
        "Nid",
        "Pvt",
        "Ltd",
        "Ind",
        "January",
        "December",
        "Description",
        "Project",
        "infrastructure",
        "Java",
        "Message",
        "Service",
        "JMS",
        "project",
        "J2EE",
        "package",
        "JMS",
        "APIs",
        "services",
        "Exchange",
        "message",
        "components",
        "environment",
        "messaging",
        "receiver",
        "message",
        "selection",
        "message",
        "format",
        "message",
        "Database",
        "sender",
        "receiver",
        "Responsibilities",
        "Full",
        "Life",
        "Cycle",
        "Development",
        "Distributed",
        "Environment",
        "Java",
        "J2EE",
        "framework",
        "application",
        "Struts",
        "Framework",
        "MVC",
        "Architecture",
        "end",
        "JSP",
        "HTML",
        "JavaScript",
        "jQuery",
        "Web",
        "Service",
        "client",
        "login",
        "authentication",
        "credit",
        "reports",
        "information",
        "Apache",
        "Axis",
        "Web",
        "Service",
        "User",
        "Interface",
        "modules",
        "JSPs",
        "JavaScript",
        "framework",
        "data",
        "processing",
        "Design",
        "patterns",
        "Java",
        "XML",
        "container",
        "Spring",
        "Framework",
        "flexibility",
        "Inversion",
        "Controller",
        "IOC",
        "Hibernate",
        "ORM",
        "framework",
        "spring",
        "framework",
        "data",
        "persistence",
        "transaction",
        "management",
        "Session",
        "beans",
        "Business",
        "logic",
        "Developed",
        "EJB",
        "components",
        "Web",
        "logic",
        "Application",
        "Server",
        "Written",
        "unit",
        "tests",
        "JUnit",
        "Framework",
        "Logging",
        "Framework",
        "configuration",
        "files",
        "Hibernate",
        "mappings",
        "RESTHTTP",
        "APIs",
        "data",
        "formats",
        "API",
        "versioning",
        "strategy",
        "Developed",
        "Web",
        "Services",
        "data",
        "applications",
        "SOAP",
        "messages",
        "code",
        "reviews",
        "bug",
        "CSS",
        "style",
        "Sheets",
        "site",
        "standardization",
        "site",
        "Assisted",
        "QA",
        "Team",
        "resolution",
        "process",
        "defect",
        "priority",
        "severity",
        "Environment",
        "Java",
        "Struts",
        "Spring",
        "Hibernate",
        "Web",
        "Logic",
        "Eclipse",
        "Oracle",
        "JUnit",
        "Maven",
        "Windows",
        "XP",
        "HTML",
        "CSS",
        "JavaScript",
        "XML",
        "Education",
        "Masters",
        "Computer",
        "Science",
        "California",
        "State",
        "University",
        "January",
        "December",
        "Bachelors",
        "Computer",
        "Science",
        "Engineering",
        "Computer",
        "Science",
        "Engineering",
        "Sri",
        "Padmavathi",
        "University",
        "August",
        "May"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T22:26:47.403872",
    "resume_data": "Spark Python Developer Sparkspan lPythonspan span lDeveloperspan Spark Python Developer Lumity CA San Mateo CA Hadoop Developer with 4 years of overall IT experience in a variety of industries which includes hands on experience in Big Data tools and technologies Have 3 years of comprehensive experience in Big Data processing using Hadoop and its ecosystem MapReduce Pig Hive Sqoop Flume Spark Kafka and HBase Good working experience on Spark spark streaming spark SQL with python and Kafka Hands on experience with Spark Core Spark SQL and Data FramesData SetsRDD API Developed applications using Spark and python for data processing Strong experience and knowledge of real time data analytics using Flume and Spark Replaced existing mapreduce jobs and Hive scripts with Spark DataFrame transformation and actions Good knowledge on Spark architecture and realtime streaming using Spark Performed maintenance monitoring deployments and upgrades across infrastructure Involved in Debugging Pig and Hive scripts and used various optimization techniques in MapReduce jobs Wrote custom UDFs and UDAF for Hive and Pig core functionality Experience in importing and exporting data using Sqoop from Relational Database Systems to HDFS Good knowledge about YARN configuration Hands on experience in configuring and working with Flume to load the data from multiple web sources directly into HDFS Hands on experience working with NoSQL databases such as HBase MongoDB and Cassandra Used HBase in accordance with PIGHive as and when required for real time low latency queries Scheduled job workflow for FTP Sqoop and hive scripts using Oozie and Oozie coordinators Developed various shell scripts and python scripts to automate Spark jobs and hive scripts Generated Java APIs for retrieval and analysis on NoSQL database such as HBase and Cassandra Imported the data from different sources like AWS S3 Local file system into Spark RDD and worked on cloud Amazon Web Services EMR S3 EC2 RedShift Lambda Integrated clusters with Active Directory for Kerberos and User Authentication Authorization Experience with all stages of the SDLC and Agile Development model right from the requirement gathering to Deployment and production support Involved in daily SCRUM meetings to discuss the developmentprogress and was active in making scrum meetings more productive Work Experience Spark Python Developer Lumity CA October 2017 to Present Description Lumity is benefit management company where we provide environment to our clients and carriers Responsibilities Working on Big Data infrastructure for batch processing as well as realtime processing Responsible for building scalable distributed data solutions using Hadoop Used SparkStreaming APIs to perform necessary transformations and actions on the fly for building the common learner data model which gets the data from Kafka in near real time and Persists into Cassandra Developed Spark scripts by using Python shell commands as per the requirement Experience in designing and developing applications in Spark using Python to compare the performance of Spark with Hive and SQLOracle Involved in converting HiveSQL queries into Spark transformations using Spark RDDs Python Experience in both SQL Context and Spark Session and implementing Log Error Alarmed in Spark Optimized Hive QL pig scripts by using execution engine like TEZ Spark Tested Apache TEZ an extensible framework for building high performance batch and interactive data processing applications on Pig and Hive jobs Experience in managing nodes on Hadoop cluster and monitor Hadoop cluster job performance using Cloudera manager Managing and scheduling Jobs on a Hadoop Cloudera cluster using Oozie work flows and java schedulers Used Spark API over Cloudera Hadoop YARN to perform analytics on data in Hive and also involved in creating Hive Tables loading with data and writing Hive queries which will invoke and run Map Reduce jobs in the backend Designed and implemented Incremental Imports into Hive tables Created Partitions and Bucketing concepts in Hive and designed both Managed and External tables in Hive to optimize performance Experience in importing and exporting tera bytes of data using Sqoop from Relational Database Systems to HDFS Moved Relational Database data using Sqoop into Hive Dynamic partition tables using staging tables Involved in collecting aggregating and moving data from servers to HDFS using Apache Flume Involved in developing Pig Scripts for change data capture and delta record processing between newly arrived data and already existing data in HDFS Migrated ETL jobs to Pig scripts do transformations even joins and some preaggregations before storing the data onto HDFS Implemented the workflows using Apache Oozie framework to automate tasks Used Zookeeper to coordinate cluster services Worked on different file formats like Sequence files XML files and Map files using Map Reduce Programs Used Impala where ever possible to achieve faster results compared to Hive during data Analysis Configured deployed and maintained multinode Dev and Test Kafka Clusters and implemented data ingestion and handling clusters in real time processing using Kafka Worked on writing transformermapping MapReduce pipelines using Java Transform the logs data into data model using apache pig and written UDFs functions to format the logs data Setting up Confluent Kafka for ingesting large volumes of eventslogs into Hadoop Developed and Configured Kafka brokers to pipeline server logs data into spark streaming Used Teradata Data Mover to copy data and objects such as tables and statistics from one system to another involved in Analyzing building Teradata EDW using Teradata ETL utilities and Informatica Implemented usage of Amazon EMR for processing Big Data across a Hadoop Cluster of virtual servers on Amazon Elastic Compute Cloud EC2 and Amazon Simple Storage Service S3 Implemented installation and configuration of multinode cluster on the cloud using Amazon Web Services AWS on EC2 Used Amazon Web Services S3 to store large amount of data in identicalsimilar repository Environment Hadoop HDFS Pig Hive Sqoop Flume Kafka Spark TEZ Storm Shell Scripting HBase Python scripting Kerberos Agile Zookeeper Maven AWS AWS EMR MySQL HadoopBigdata Developer Apple CA January 2017 to September 2017 Description The purpose of the project is to analyze the data coming from the various sources into the Hadoop data center unit Created programs to process large volumes of data through a lot of prepay concepts which analyze produce suspect claims and it helps to generate Datasets for visualization These suspect claims verified again and it saves millions of dollars to the company every year Responsibilities Involved in implementation of Hadoop Cluster and Hive for Development and Test Environment Worked on analyzing Hadoop Cluster and different big data analytic tools including Pig Hive and MongoDB Extracted files from MongoDB through Sqoop and placed in HDFS for processed Designed and developed functionality to get JSON document from MongoDB document store and send it to client using RESTful web service Successfully loaded files to Hive and HDFS from MongoDB Exploring with the Spark for improving the performance and optimization of the existing algorithms in Hadoop using Spark Context SparkSQL Data Frame Pair RDDs Spark YARN Experienced with batch processing of data sources using Apache Spark and developing predictive analytic using Apache Spark Scala APIs Imported millions of structured data from relational databases using Sqoop import to process using Spark and stored the data into HDFS in CSV format and developed Spark streaming application to pull data from cloud to hive table Involved in importing data from Oracle into HDFS and Hive using Sqoop Created tables using Impala and involved in creating Queries which are stored in HBase Implemented complex scripts to support test driven development and continuous integration Improving the performance and optimization of existing algorithms in Hadoop using Spark context SparkSQL and Spark YARN using Scala Analyzed the data as per the business requirements using Hive queries Installed and configured Hadoop stack and different big data analytic tools export and imports from data preprocessing Collected and aggregated large amounts of log data using Apache Flume and staging data in HDFS for further analysis Hands on experience in writing custom UDFs and also custom input and output formats and created Hive Tables loaded values and generated ad hocreports using the table data Showcased strong understanding on Hadoop architecture including HDFS MapReduce Hive Pig Sqoop and Oozie Used spark with Yarn and got performance results compared with MapReduce Loaded existing data warehouse data from Oracle database to Hadoop Distributed File System HDFS Developed MapReduce programs in Java to search production logs and web analytics logs for use cases like application issues measure page download performance respectively Hadoop Developer Intern Watts Water Technologies AZ January 2016 to December 2016 Responsibilities Responsible for analyzing large data sets and derive customer usage patterns by developing new MapReduce programs Written MapReduce code to parse the data from various sources and storing parsed data into HBase and Hive Created HBase tables to store different formats of data as a backend for user portals Developed Kafka producer and consumers HBase clients Apache Spark and Hadoop MapReduce jobs along with components on HDFS Hive Worked on creating combiners partitions and distributed cache to improve the performance of MapReduce jobs Developed Shell Script to perform data profiling on the ingested data with the help of HIVE Bucketing Developed Hive UDF for performing Hashing mechanism on the Hive Column Involved in creating Hive tables loading with data and writing Hive queries which will run internally in map reduce way Used Hive to analyze the partitioned and bucketed data and compute various metrics for reporting Written Hive jobs to parse the logs and structure them in tabular format to facilitate effective querying on the log data Writing Hive join query to fetch info from multiple tables writing multiple Map Reduce jobs to collect output from Hive Ingest data into Hadoop HiveHDFS from different data resources Involved in loading and transforming large sets of Structured SemiStructured and Unstructured data and analyzed them by running Hive queries and Pig scripts Experienced in writing Hive validation scripts that are used in validation framework for daily analysis through graphs and presented to business users Developed workflow in Oozie to automate the tasks of loading data into HDFS and preprocessing with Pig and Hive Developed code in Python to use MapReduce framework by Hadoop streaming Used Pig as ETL tool to do transformations joins and some preaggregations before storing the data into HDFS Imported all the customer specific personal data to Hadoop using Sqoop component from various relational databases like Netezza and Oracle Develop testing scripts in Python and prepare test procedures analyze test results data and suggest improvements of the system and software Experience in streaming log data using Flume and data analytics using Hive Extracted the data from RDBMS Oracle MySQL Teradata to HDFS using Sqoop Environment Hadoop MapReduce HDFS Pig HiveQL Oozie Flume Impala Cloudera MySQL Shell Scripting HBase JavaJ2EE Developer Talent Nid Pvt Ltd Ind January 2014 to December 2014 Description This Project aims to implement the infrastructure of Java Message Service JMS This project developed in J2EE package using JMS APIs provides services for Exchange for message between components in a distributed environment It supports both Synchronous and Asynchronous messaging and the receiver receives the message according to selection of the message format The message will be stored in Database and it will be retrieved whenever sender or receiver requires Responsibilities Involved in Full Life Cycle Development in Distributed Environment using Java and J2EE framework Designed the application by implementing Struts Framework based on MVC Architecture Designed and developed the front end using JSP HTML and JavaScript and jQuery Implemented the Web Service client for the login authentication credit reports and applicant information Apache Axis 2 Web Service Extensively worked on User Interface for few modules using JSPs JavaScript and Ajax Developed framework for data processing using Design patterns Java XML Used the lightweight container of the Spring Framework to provide architectural flexibility for Inversion of Controller IOC Used Hibernate ORM framework with spring framework for data persistence and transaction management Designed and developed Session beans to implement the Business logic Developed EJB components that are deployed on Web logic Application Server Written unit tests using JUnit Framework and Logging is done using Log4J Framework Designed and developed various configuration files for Hibernate mappings Designed and documented RESTHTTP APIs including JSON data formats and API versioning strategy Developed Web Services for sending and getting data from different applications using SOAP messages Actively involved in code reviews and bug fixing Applied CSS Cascading style Sheets for entire site for standardization of the site Assisted QA Team in defining and implementing a defect resolution process including defect priority and severity Environment Java 50 Struts Spring 20 Hibernate 32 Web Logic 70 Eclipse 33 Oracle JUnit 42 Maven Windows XP HTML CSS JavaScript and XML Education Masters in Computer Science California State University January 2015 to December 2016 Bachelors in Computer Science Engineering in Computer Science Engineering Sri Padmavathi University August 2010 to May 2014",
    "unique_id": "d8a5d88c-6dae-430d-bee3-7d941e4d6873"
}