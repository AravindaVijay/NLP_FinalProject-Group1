{
    "clean_data": "Java Python Hadoop Developer Javaspan lPythonspan Hadoop span lDeveloperspan Senior Java Hadoop Developer Citi Bank New York NY Has almost 115 years including 6 years in US of extensive handson experience in latest Java and Python and its related technologies along with 3 years of experience in Hadoop related technologies Hive Spark Mapreduce and Oozie using Cloudera distribution of Hadoop Framework Has been working with a market leading Human Resources Company and previously worked with Retail Risk Investment and Auto Loan banking clients A selfstarter who takes ownership of tasks at hand with experience in architecting designing developing and testing complex custom applications in JavaJ2EE Spring Framework and development of Big Data technologies in highly scalable endtoend Hadoop Infrastructure Capable and proactive team player with excellent analytical problem solving communication and interpersonal skills with ability to interact with individuals Authorized to work in the US for any employer Work Experience Java Python Hadoop Developer ADP Roseland NJ November 2017 to Present Project Working on ADPs Ventures Development and its related Data Cloud Benchmark projects Data cloud benchmarking team is responsible to create data pipelines Machine learning models Data from the following sources Employee HR Payroll Largemedium business Workforce NowWFNMAS and small business SBS are cleaned normalized combined and aggregated and then go through benchmarking cubesmatrices process to build data pipelines and ML models are used by other downstream ADPs projects like Venture Next Gen PayrollPi Innovation lab etc Technologies Cloudera distribution of Hadoop Framework componentstechnologies HIVE Impala Spark HDFS MR Oozie workflow AVROPARQUET Machine learningML model creationdeployment Technology H2O distribution Driverless AI H2O server Core Java 8 and Python 273 for feature engineering for ML model Spring framework CORE spring MessagingJMS template springDAO JDBCtemplate Spring Integration Messaging and Spring BatchEh cache Oracle 11g Database PLSQL Queries Python Spark Python package Panda Frame REST Web service API java jaxwrs and python flask and sqlite3 packages Roles and Responsibilities Working on Python along with HiveImpala and Spark Python API PySpark to prepare Employee level client level and Payroll level data used for the predictable Analytic Models used PySpark RDD Data frame HiveSQL and Hive Context Creating REST web services to publish Machine learning models business function and earnings code prediction models via API as predictive analytic service using Java 8 and python flask codernity DB SQLitedb etc Creating python scripts to load data from HDFSCSV to Spark container for analysis and then save the outcome as textCSV table data to HDFS Using Panda Frame to convert spark data frame to panda frame for various type of quick data analysis and visualization like scatter plot area plot bar plot etc Extracting raw Hadoop data files from multiple data sourcesproducts like ADP Payroll Small business Workforce now and cleaning normalizing combining and aggregating using Hadoops Hive PythonSpark and Map Reducers program to create data pipeline for benchmarking ADP data Validating and testing benchmark numbers versus external sources like Federal Reserve data FRB using Hive python Creating feature engineering variables for Machine learning ML models using pyspark core java 8 Deploying Machine Learning Models built in H2O on Hadoop cluster Spark Application Using Apache AVRO Parquet format schema for the HIVE tables to persist data in HDFS Designed developed Spring Batch based batch jobs to invoke and execute the Hadoop Jobs Hive Jobs Crunch Jobs and Oozy work flow in Hadoop cluster Developing and maintaining use case diagrams class diagrams database tables and mapping between relational database tables and objectoriented java objects Mentoring team mates and help them with their technical solutions on daily basis Practicing Agile Scrum methodologies and scrum process etc Senior Java Hadoop Developer Citi Bank New York NY October 2015 to August 2017 NYC US Project Optima Retail is an enterprise wide initiative to automate the regulatory data submission process for CCAR and BASEL reports This involves building a system flexible and fast enough to process huge amount of data as the process actually involves consolidating data from each and every retail account of the organization Optima Retail project is responsible for extracting processing and generating report data required to be submitted to the regulatory bodies Technology Core Java Spring framework CORE spring MessagingJMS template springDAO JDBCtemplate Spring Integration Messaging and Spring Batch Eh cache Oracle 11g Database PLSQL Procedures Hadoop technologies HDFS HIVE Crunch MR Oozie workflow AVROPARQUET Python Spark Python API Panda Frame Roles and Responsibilities Designed developed Spring Batch based batch jobs to invoke and execute the Hadoop Jobs Hive Jobs Crunch Jobs and Oozie work flow in Hadoop cluster Developed common communication and decision making point of the systemInternal Message Handler to handle feed process using spring integration components like service router adapters to communicate between various upstream and downstream processes Created various type of Hadoop Jobs Hive jobs Crunch Jobs using MR jobs and invoked them using Oozie workflow Created Apache AVRO Parquet format schema for the HDFS tables to persist data in Hadoop cluster Developed use case diagrams class diagrams database tables and mapping between relational database tables and object oriented java objects Worked on Python and Spark Python API PySpark to prepare Account level data used for the predictable Analytic Models Used Pysparks RDD Dataframe HiveContext Functions to create various analytic functions like binning sort diff rank window groupby etc for loss forecast models Created python scripts to load data from HDFS to Spark container for analysis and then save the outcome as textCSV table data to HDFS Used PandaFrame to convert spark dataframe to pandaframe for various type of data visualization like scatter plot area plot bar plot etc Mentored team mates junior members and help them with technical solutions Participatedfollowed Agile Scrum methodologies and scrum process Senior Java Developer JPMorgan Chase September 2013 to September 2015 NJ US Project Worked in a suit of projects where customers Auto loan application creation and automatic underwriting manual under writing functionality has been built using latest JEE technologies Overview of each of the application in the project suit has been given below a Scorecard Its receives credit bureau or FICO score and its related data in JMS messages format from Mainframe system it calculates the score as per the firm scoring engine b Policy Engine This application process all the loan applications and decides if any application should be given loan Middle layer components have been developed using JMS pointtopoint messaging model and It interacts with ACAP Old Mainframe system using text format JMS messages c Credit Assist This application is used by business people to displaysee Dealer Data Policy Data which screen scrap main frame data from ACAP system and process them according to business rule and display them on the credit assist UI screen to business user Technology Core Java JEE Spring framework CORE spring MessagingJMS template springDAO JDBCtemplate Web Service SOAP and REST Hibernate JPA Eh cache Oracle 11g Database PLSQL Procedures Roles and Responsibilities Worked as the key person in terms of Project Architecture design development identifying inputsoutputs discussing with the JPMCs various Legacy Systems Extensively worked on middle tier and persistence tier using the Spring Full Stack and HibernateMybatis Developed C2A Middle layer components using Spring JMS pointtopoint messaging model with IBM MQ broker and it interacts with ACAP Old Mainframe system using text format JMS messages Used EHCache to achieved high performance of the reference data web services Developed reliable transaction system for the web services and JMS applications using spring transaction and AOP and secured the services using SSLTLS implementation Updated and reviewed design documents created various Test cases scenarios and Junit test cases to test persistence and service tiers Migrated the Legacy Mainframe system applications to JEE Application Worked with BAs and clients to identify the requirements in terms of interaction of the proposed applications with external cosupplier applications delivery phase data verification QA Production strategy Mentored team mates Senior Java Developer JPMorgan Chase June 2012 to September 2015 Senior Java Developer JPMorgan Chase New York NY October 2012 to September 2013 NYC US Technology Core Java JEE Spring Core Transaction JMS and JDBC Hibernate EHCACHE Web Service XML XSLT Junit SOAPUI Maven build Jenkins ABB client private cloud Web Service Vignette Application Portal SOA MULE Project Worked in Total Fund Repository TFR and Data integration a TFR a central repository for all the funds related data points TFR provides a web based solution to Fund management and fund related entities like Portfolio managers Benchmarks Share classes Disclosures etc b Data integration Near realtime data synchronization between SalesPage Retail CRM and Aprimo 3rd party data provider system bidirectionally using SOA MULE ESB Roles and Responsibilities Developed and maintained the data Integration application eliminating the existing manual data update process in multiple systems using MULE soft ESB framework Achieved near realtime data synchronization between SalesPage Retail CRM and Aprimo 3rd party data provider system bidirectionally Create multiple high performance Restful services using Ehcache and they were used inside ESB as part of the application Identified future system for data exchange and can be integrated using SOAMULE ESB Developed a web based solution to Fund management and fund related entities like Portfolio managers Benchmarks Share classes Disclosures etc Entities involved in JPMCs Total Fund Repository Worked in enhancement testing and bug fixing Performed code review Mentored and trained junior team members Java Developer JPMorgan Chase Kolkata West Bengal March 2011 to September 2012 Kolkata India and NYC US Project Worked in IM Core Party Data A group of applications created to feed Core Party Account data of JPMC Investment Management to downstream applications Multiple Web services and Messaging applications have been built to cater the downstream applications Technology Core Java JEE JSPServlet JMS Spring Core MVC Transaction JMS and JDBC Hibernate JPA EHCACHE Web Service Apache CXF Junit SOAPUI Quartz scheduler Maven build Jenkins ABB client private cloud Roles and Responsibilities Worked with client and BAs of the downstream to gather business requirements for Core Party Account data feeds of JPMC Investment Management Group Designed developed and maintained several core party SOAP and Restful web services and JMS applications Used EHCache to achieved high performance for the reference data web services Developed reliable transaction system for the web services and JMS applications using spring transaction and AOP Secured the services using SSLTLS implementation and deployed application in JPMC Private Cloud system Performed code review and Mentored and trained junior team members Offshoreonsite coordination Java Developer JPMorgan Chase August 2010 to May 2012 Java Developer JPMorgan Chase Kolkata West Bengal August 2010 to March 2011 Kolkata India Project Worked in IM Common Services Application Interaction among the various systems of JPMC Investment Management took place at real time using the various applications of IM Common Services IM CSA Application was a messaging system that was used to interact with different subsystems of Investment Management division of JPMorgan Chase Bank Technology Core Java JEE Spring Quartz Restful SOAP Web service XML WebSphere 6x Application server Sybase Unix Roles and Responsibilities Developed multiple RESTfulSOAP Web services and messaging system to build the instrument server which handles Interaction among the various systems of JPMC Investment Management group Migrated JEE componentsaround 30 small and big applications build from ant to maven ReArchitected and developed all existing EJB components to spring framework components to handle transaction of Indicative Data and Price Data Java Developer Pro Quest Noida Uttar Pradesh October 2008 to July 2010 Noida India Project It provides archives of sources such as newspapers journals periodicals dissertations and aggregated databases of various types and its content is estimated to be approximately 125 billion digital pages Technology Core java JEE JSP Servlet RESTful Web service Hibernate JPA ORM Oracle DB Apache Tapestry Apace Lucene API JSF DHTMLX java script component Roles and Responsibilities Implemented several high performance Restful web services that was used as archives of sources as newspapers journals periodicals dissertations and aggregated databases of various types Handled large number contents estimated to be approximately 125 billion digital pages in UIweb layer using apache tapestry and DHTMLX Followed and used the SCRUM work Plan view and Rational of IBM for Agile methodology Java Developer Proquest and IHG August 2007 to July 2010 Java Developer Intercontinental Hotels Grp Noida Uttar Pradesh August 2007 to October 2008 Noida India Project Worked in Expedia User Management This project was built to support IHG Third Party Intermediates TPI standards by providing an automated solution for pushing rate and availability status information message notification requested by its external partner IHG Web application Intercontinental Hotels Group IHG was a web based project for hotel reservations handling more than 4000 hotels worldwide and managing around 10 million PCR accounts Technology Core java Spring MVC JEE JSP Servlet Resin Pro 312 as application server ATG dynamo 70 as application server Genesis Framework Roles and Responsibilities Analyzed the requirements to support IHG TPI standards by providing an automated solution for pushing rate and availability status information message notification Developed and maintenance of the admin module and User Management module and handled more than 4000 hotels worldwide and managing around 10 million PCR accounts Used Spring MVC to refactor the legacy application Education Bachelor of Engineering in Information and Technology Jadavpur University 2003 to 2007 Skills JAVA 10 years JDBC 10 years JMS 6 years ORACLE 10 years APACHE 10 years Apache Spark 3 years Apache Hive 3 years MapReduce 3 years Avro 3 years Parquet 3 years Python 3 years Panda Frame 2 years Apache HDFS 3 years Flask C Links httpswwwlinkedincominmondalmithun Additional Information Technical Skills Technology Stack Language Core Java 7 SQL Python Technology JavaJ2EE Hadoop Hive Crunch Spark Panda Frame MapReduce Oozie HDFSAvro and Parquet Databases Oracle 11g Framework Spring 4x JMS Transaction JDBC Security AOP Batch Integration and MVC Middleware Technologies Hibernate 4x MyBatis 311 and IBM MQ 7x Front End Technologies JQuery JSON Javascript DHTMLX Web servers App servers Apache Tomcat 8x WAS Server 6x Continuous Integration Tools Jenkins Sonar Others Web service SOAP and RESTful Apache CXF and Jersey Client Distributed transaction JUnit Apache POI Jprofiler EhCache Maven build Java 7 Multithreading",
    "entities": [
        "REST Hibernate JPA Eh",
        "Spring BatchEh",
        "sqlite3",
        "Hadoop Infrastructure Capable",
        "AI H2O",
        "Spark Python API PySpark",
        "Dealer Data Policy Data",
        "UIweb",
        "New York",
        "AVROPARQUET Machine",
        "HDFS",
        "Spark Application Using Apache",
        "Expedia User Management",
        "JMS",
        "Noida",
        "Hadoop Framework",
        "IM Core Party Data",
        "User Management",
        "IBM",
        "WebSphere 6x",
        "Hadoop",
        "MULE",
        "SOAP",
        "HDFS Using Panda Frame",
        "JUnit",
        "MessagingJMS",
        "PySpark RDD Data",
        "Ehcache",
        "Intercontinental Hotels Group IHG",
        "Machine",
        "Aprimo 3rd",
        "Developed",
        "Kolkata India Project Worked",
        "Created Apache",
        "Java JEE Spring",
        "creationdeployment Technology H2O",
        "the Hadoop Jobs Hive Jobs Crunch Jobs",
        "ADP",
        "Spring",
        "Restful",
        "JavaJ2EE Spring Framework",
        "JPMC Investment Management",
        "AOP Secured",
        "ADP Payroll Small business Workforce",
        "Genesis Framework Roles",
        "Federal Reserve",
        "IM Common Services Application Interaction",
        "Payroll Largemedium",
        "HiveContext Functions",
        "Flask C Links httpswwwlinkedincominmondalmithun Additional Information Technical Skills Technology Stack Language Core",
        "TFR",
        "IHG Third Party Intermediates",
        "Indicative Data",
        "NYC US",
        "MVC",
        "Workforce NowWFNMAS",
        "Spark",
        "Agile",
        "EJB",
        "JPMC Investment Management Group Designed",
        "Core Party",
        "API",
        "US",
        "EHCache",
        "HIVE",
        "QA",
        "Present Project Working",
        "Technology Core",
        "Created",
        "Human Resources Company",
        "AOP",
        "MR",
        "Retail Risk Investment and Auto Loan",
        "Auto",
        "Benchmarks Share",
        "Credit Assist This",
        "java",
        "Noida India Project",
        "PCR",
        "Rational of IBM",
        "Kolkata",
        "Portfolio",
        "SSLTLS",
        "Mentored",
        "CORE",
        "Venture Next Gen PayrollPi Innovation",
        "Project Architecture",
        "Big Data",
        "Multiple Web services",
        "MVC Middleware Technologies Hibernate",
        "IM CSA Application",
        "JEE Application Worked",
        "India",
        "Maven",
        "ATG",
        "Front End Technologies JQuery JSON Javascript",
        "Impala",
        "Jenkins ABB",
        "Investment Management",
        "JDBCtemplate Spring Integration Messaging",
        "Responsibilities Analyzed",
        "FICO",
        "SOA MULE ESB Roles and Responsibilities Developed",
        "Hadoop Jobs Hive",
        "Payroll",
        "SOAMULE",
        "Panda Frame",
        "ML",
        "Technologies Cloudera",
        "JPMorgan Chase Bank Technology",
        "Hadoops Hive PythonSpark",
        "BASEL",
        "HiveImpala",
        "Map Reducers",
        "HDFSCSV",
        "JDBCtemplate Web Service SOAP",
        "Data",
        "IM Common Services",
        "JEE",
        "Oracle DB Apache Tapestry",
        "JPMCs Total Fund Repository Worked",
        "ACAP",
        "POI",
        "ESB",
        "H2O on Hadoop",
        "NJ US"
    ],
    "experience": "Experience Java Python Hadoop Developer ADP Roseland NJ November 2017 to Present Project Working on ADPs Ventures Development and its related Data Cloud Benchmark projects Data cloud benchmarking team is responsible to create data pipelines Machine learning models Data from the following sources Employee HR Payroll Largemedium business Workforce NowWFNMAS and small business SBS are cleaned normalized combined and aggregated and then go through benchmarking cubesmatrices process to build data pipelines and ML models are used by other downstream ADPs projects like Venture Next Gen PayrollPi Innovation lab etc Technologies Cloudera distribution of Hadoop Framework componentstechnologies HIVE Impala Spark HDFS MR Oozie workflow AVROPARQUET Machine learningML model creationdeployment Technology H2O distribution Driverless AI H2O server Core Java 8 and Python 273 for feature engineering for ML model Spring framework CORE spring MessagingJMS template springDAO JDBCtemplate Spring Integration Messaging and Spring BatchEh cache Oracle 11 g Database PLSQL Queries Python Spark Python package Panda Frame REST Web service API java jaxwrs and python flask and sqlite3 packages Roles and Responsibilities Working on Python along with HiveImpala and Spark Python API PySpark to prepare Employee level client level and Payroll level data used for the predictable Analytic Models used PySpark RDD Data frame HiveSQL and Hive Context Creating REST web services to publish Machine learning models business function and earnings code prediction models via API as predictive analytic service using Java 8 and python flask codernity DB SQLitedb etc Creating python scripts to load data from HDFSCSV to Spark container for analysis and then save the outcome as textCSV table data to HDFS Using Panda Frame to convert spark data frame to panda frame for various type of quick data analysis and visualization like scatter plot area plot bar plot etc Extracting raw Hadoop data files from multiple data sourcesproducts like ADP Payroll Small business Workforce now and cleaning normalizing combining and aggregating using Hadoops Hive PythonSpark and Map Reducers program to create data pipeline for benchmarking ADP data Validating and testing benchmark numbers versus external sources like Federal Reserve data FRB using Hive python Creating feature engineering variables for Machine learning ML models using pyspark core java 8 Deploying Machine Learning Models built in H2O on Hadoop cluster Spark Application Using Apache AVRO Parquet format schema for the HIVE tables to persist data in HDFS Designed developed Spring Batch based batch jobs to invoke and execute the Hadoop Jobs Hive Jobs Crunch Jobs and Oozy work flow in Hadoop cluster Developing and maintaining use case diagrams class diagrams database tables and mapping between relational database tables and objectoriented java objects Mentoring team mates and help them with their technical solutions on daily basis Practicing Agile Scrum methodologies and scrum process etc Senior Java Hadoop Developer Citi Bank New York NY October 2015 to August 2017 NYC US Project Optima Retail is an enterprise wide initiative to automate the regulatory data submission process for CCAR and BASEL reports This involves building a system flexible and fast enough to process huge amount of data as the process actually involves consolidating data from each and every retail account of the organization Optima Retail project is responsible for extracting processing and generating report data required to be submitted to the regulatory bodies Technology Core Java Spring framework CORE spring MessagingJMS template springDAO JDBCtemplate Spring Integration Messaging and Spring Batch Eh cache Oracle 11 g Database PLSQL Procedures Hadoop technologies HDFS HIVE Crunch MR Oozie workflow AVROPARQUET Python Spark Python API Panda Frame Roles and Responsibilities Designed developed Spring Batch based batch jobs to invoke and execute the Hadoop Jobs Hive Jobs Crunch Jobs and Oozie work flow in Hadoop cluster Developed common communication and decision making point of the systemInternal Message Handler to handle feed process using spring integration components like service router adapters to communicate between various upstream and downstream processes Created various type of Hadoop Jobs Hive jobs Crunch Jobs using MR jobs and invoked them using Oozie workflow Created Apache AVRO Parquet format schema for the HDFS tables to persist data in Hadoop cluster Developed use case diagrams class diagrams database tables and mapping between relational database tables and object oriented java objects Worked on Python and Spark Python API PySpark to prepare Account level data used for the predictable Analytic Models Used Pysparks RDD Dataframe HiveContext Functions to create various analytic functions like binning sort diff rank window groupby etc for loss forecast models Created python scripts to load data from HDFS to Spark container for analysis and then save the outcome as textCSV table data to HDFS Used PandaFrame to convert spark dataframe to pandaframe for various type of data visualization like scatter plot area plot bar plot etc Mentored team mates junior members and help them with technical solutions Participatedfollowed Agile Scrum methodologies and scrum process Senior Java Developer JPMorgan Chase September 2013 to September 2015 NJ US Project Worked in a suit of projects where customers Auto loan application creation and automatic underwriting manual under writing functionality has been built using latest JEE technologies Overview of each of the application in the project suit has been given below a Scorecard Its receives credit bureau or FICO score and its related data in JMS messages format from Mainframe system it calculates the score as per the firm scoring engine b Policy Engine This application process all the loan applications and decides if any application should be given loan Middle layer components have been developed using JMS pointtopoint messaging model and It interacts with ACAP Old Mainframe system using text format JMS messages c Credit Assist This application is used by business people to displaysee Dealer Data Policy Data which screen scrap main frame data from ACAP system and process them according to business rule and display them on the credit assist UI screen to business user Technology Core Java JEE Spring framework CORE spring MessagingJMS template springDAO JDBCtemplate Web Service SOAP and REST Hibernate JPA Eh cache Oracle 11 g Database PLSQL Procedures Roles and Responsibilities Worked as the key person in terms of Project Architecture design development identifying inputsoutputs discussing with the JPMCs various Legacy Systems Extensively worked on middle tier and persistence tier using the Spring Full Stack and HibernateMybatis Developed C2A Middle layer components using Spring JMS pointtopoint messaging model with IBM MQ broker and it interacts with ACAP Old Mainframe system using text format JMS messages Used EHCache to achieved high performance of the reference data web services Developed reliable transaction system for the web services and JMS applications using spring transaction and AOP and secured the services using SSLTLS implementation Updated and reviewed design documents created various Test cases scenarios and Junit test cases to test persistence and service tiers Migrated the Legacy Mainframe system applications to JEE Application Worked with BAs and clients to identify the requirements in terms of interaction of the proposed applications with external cosupplier applications delivery phase data verification QA Production strategy Mentored team mates Senior Java Developer JPMorgan Chase June 2012 to September 2015 Senior Java Developer JPMorgan Chase New York NY October 2012 to September 2013 NYC US Technology Core Java JEE Spring Core Transaction JMS and JDBC Hibernate EHCACHE Web Service XML XSLT Junit SOAPUI Maven build Jenkins ABB client private cloud Web Service Vignette Application Portal SOA MULE Project Worked in Total Fund Repository TFR and Data integration a TFR a central repository for all the funds related data points TFR provides a web based solution to Fund management and fund related entities like Portfolio managers Benchmarks Share classes Disclosures etc b Data integration Near realtime data synchronization between SalesPage Retail CRM and Aprimo 3rd party data provider system bidirectionally using SOA MULE ESB Roles and Responsibilities Developed and maintained the data Integration application eliminating the existing manual data update process in multiple systems using MULE soft ESB framework Achieved near realtime data synchronization between SalesPage Retail CRM and Aprimo 3rd party data provider system bidirectionally Create multiple high performance Restful services using Ehcache and they were used inside ESB as part of the application Identified future system for data exchange and can be integrated using SOAMULE ESB Developed a web based solution to Fund management and fund related entities like Portfolio managers Benchmarks Share classes Disclosures etc Entities involved in JPMCs Total Fund Repository Worked in enhancement testing and bug fixing Performed code review Mentored and trained junior team members Java Developer JPMorgan Chase Kolkata West Bengal March 2011 to September 2012 Kolkata India and NYC US Project Worked in IM Core Party Data A group of applications created to feed Core Party Account data of JPMC Investment Management to downstream applications Multiple Web services and Messaging applications have been built to cater the downstream applications Technology Core Java JEE JSPServlet JMS Spring Core MVC Transaction JMS and JDBC Hibernate JPA EHCACHE Web Service Apache CXF Junit SOAPUI Quartz scheduler Maven build Jenkins ABB client private cloud Roles and Responsibilities Worked with client and BAs of the downstream to gather business requirements for Core Party Account data feeds of JPMC Investment Management Group Designed developed and maintained several core party SOAP and Restful web services and JMS applications Used EHCache to achieved high performance for the reference data web services Developed reliable transaction system for the web services and JMS applications using spring transaction and AOP Secured the services using SSLTLS implementation and deployed application in JPMC Private Cloud system Performed code review and Mentored and trained junior team members Offshoreonsite coordination Java Developer JPMorgan Chase August 2010 to May 2012 Java Developer JPMorgan Chase Kolkata West Bengal August 2010 to March 2011 Kolkata India Project Worked in IM Common Services Application Interaction among the various systems of JPMC Investment Management took place at real time using the various applications of IM Common Services IM CSA Application was a messaging system that was used to interact with different subsystems of Investment Management division of JPMorgan Chase Bank Technology Core Java JEE Spring Quartz Restful SOAP Web service XML WebSphere 6x Application server Sybase Unix Roles and Responsibilities Developed multiple RESTfulSOAP Web services and messaging system to build the instrument server which handles Interaction among the various systems of JPMC Investment Management group Migrated JEE componentsaround 30 small and big applications build from ant to maven ReArchitected and developed all existing EJB components to spring framework components to handle transaction of Indicative Data and Price Data Java Developer Pro Quest Noida Uttar Pradesh October 2008 to July 2010 Noida India Project It provides archives of sources such as newspapers journals periodicals dissertations and aggregated databases of various types and its content is estimated to be approximately 125 billion digital pages Technology Core java JEE JSP Servlet RESTful Web service Hibernate JPA ORM Oracle DB Apache Tapestry Apace Lucene API JSF DHTMLX java script component Roles and Responsibilities Implemented several high performance Restful web services that was used as archives of sources as newspapers journals periodicals dissertations and aggregated databases of various types Handled large number contents estimated to be approximately 125 billion digital pages in UIweb layer using apache tapestry and DHTMLX Followed and used the SCRUM work Plan view and Rational of IBM for Agile methodology Java Developer Proquest and IHG August 2007 to July 2010 Java Developer Intercontinental Hotels Grp Noida Uttar Pradesh August 2007 to October 2008 Noida India Project Worked in Expedia User Management This project was built to support IHG Third Party Intermediates TPI standards by providing an automated solution for pushing rate and availability status information message notification requested by its external partner IHG Web application Intercontinental Hotels Group IHG was a web based project for hotel reservations handling more than 4000 hotels worldwide and managing around 10 million PCR accounts Technology Core java Spring MVC JEE JSP Servlet Resin Pro 312 as application server ATG dynamo 70 as application server Genesis Framework Roles and Responsibilities Analyzed the requirements to support IHG TPI standards by providing an automated solution for pushing rate and availability status information message notification Developed and maintenance of the admin module and User Management module and handled more than 4000 hotels worldwide and managing around 10 million PCR accounts Used Spring MVC to refactor the legacy application Education Bachelor of Engineering in Information and Technology Jadavpur University 2003 to 2007 Skills JAVA 10 years JDBC 10 years JMS 6 years ORACLE 10 years APACHE 10 years Apache Spark 3 years Apache Hive 3 years MapReduce 3 years Avro 3 years Parquet 3 years Python 3 years Panda Frame 2 years Apache HDFS 3 years Flask C Links httpswwwlinkedincominmondalmithun Additional Information Technical Skills Technology Stack Language Core Java 7 SQL Python Technology JavaJ2EE Hadoop Hive Crunch Spark Panda Frame MapReduce Oozie HDFSAvro and Parquet Databases Oracle 11 g Framework Spring 4x JMS Transaction JDBC Security AOP Batch Integration and MVC Middleware Technologies Hibernate 4x MyBatis 311 and IBM MQ 7x Front End Technologies JQuery JSON Javascript DHTMLX Web servers App servers Apache Tomcat 8x WAS Server 6x Continuous Integration Tools Jenkins Sonar Others Web service SOAP and RESTful Apache CXF and Jersey Client Distributed transaction JUnit Apache POI Jprofiler EhCache Maven build Java 7 Multithreading",
    "extracted_keywords": [
        "Java",
        "Python",
        "Hadoop",
        "Developer",
        "Javaspan",
        "lPythonspan",
        "Hadoop",
        "span",
        "lDeveloperspan",
        "Senior",
        "Java",
        "Hadoop",
        "Developer",
        "Citi",
        "Bank",
        "New",
        "York",
        "NY",
        "years",
        "years",
        "US",
        "handson",
        "experience",
        "Java",
        "Python",
        "technologies",
        "years",
        "experience",
        "Hadoop",
        "technologies",
        "Hive",
        "Spark",
        "Mapreduce",
        "Oozie",
        "Cloudera",
        "distribution",
        "Hadoop",
        "Framework",
        "market",
        "Human",
        "Resources",
        "Company",
        "Retail",
        "Risk",
        "Investment",
        "Auto",
        "Loan",
        "banking",
        "clients",
        "selfstarter",
        "ownership",
        "tasks",
        "hand",
        "experience",
        "custom",
        "applications",
        "JavaJ2EE",
        "Spring",
        "Framework",
        "development",
        "Big",
        "Data",
        "technologies",
        "endtoend",
        "Hadoop",
        "Infrastructure",
        "team",
        "player",
        "problem",
        "communication",
        "skills",
        "ability",
        "individuals",
        "US",
        "employer",
        "Work",
        "Experience",
        "Java",
        "Python",
        "Hadoop",
        "Developer",
        "ADP",
        "Roseland",
        "NJ",
        "November",
        "Present",
        "Project",
        "Working",
        "ADPs",
        "Ventures",
        "Development",
        "Data",
        "Cloud",
        "Benchmark",
        "Data",
        "cloud",
        "team",
        "data",
        "pipelines",
        "Machine",
        "learning",
        "models",
        "Data",
        "sources",
        "Employee",
        "HR",
        "Payroll",
        "Largemedium",
        "business",
        "Workforce",
        "NowWFNMAS",
        "business",
        "SBS",
        "cubesmatrices",
        "process",
        "data",
        "pipelines",
        "ML",
        "models",
        "ADPs",
        "projects",
        "Venture",
        "Next",
        "Gen",
        "PayrollPi",
        "Innovation",
        "lab",
        "Technologies",
        "Cloudera",
        "distribution",
        "Hadoop",
        "Framework",
        "componentstechnologies",
        "HIVE",
        "Impala",
        "Spark",
        "HDFS",
        "MR",
        "Oozie",
        "workflow",
        "AVROPARQUET",
        "Machine",
        "model",
        "creationdeployment",
        "Technology",
        "H2O",
        "distribution",
        "Driverless",
        "AI",
        "H2O",
        "server",
        "Core",
        "Java",
        "Python",
        "feature",
        "engineering",
        "ML",
        "model",
        "Spring",
        "framework",
        "CORE",
        "spring",
        "MessagingJMS",
        "template",
        "JDBCtemplate",
        "Spring",
        "Integration",
        "Messaging",
        "Spring",
        "BatchEh",
        "cache",
        "Oracle",
        "g",
        "Database",
        "PLSQL",
        "Python",
        "Spark",
        "Python",
        "package",
        "Panda",
        "Frame",
        "REST",
        "Web",
        "service",
        "API",
        "java",
        "jaxwrs",
        "python",
        "flask",
        "sqlite3",
        "packages",
        "Roles",
        "Responsibilities",
        "Python",
        "HiveImpala",
        "Spark",
        "Python",
        "API",
        "PySpark",
        "Employee",
        "level",
        "client",
        "level",
        "Payroll",
        "level",
        "data",
        "Analytic",
        "Models",
        "PySpark",
        "RDD",
        "Data",
        "frame",
        "HiveSQL",
        "Hive",
        "Context",
        "Creating",
        "REST",
        "web",
        "services",
        "Machine",
        "learning",
        "models",
        "business",
        "function",
        "earnings",
        "code",
        "prediction",
        "models",
        "API",
        "service",
        "Java",
        "python",
        "flask",
        "codernity",
        "DB",
        "SQLitedb",
        "scripts",
        "data",
        "HDFSCSV",
        "Spark",
        "container",
        "analysis",
        "outcome",
        "table",
        "data",
        "HDFS",
        "Panda",
        "Frame",
        "spark",
        "data",
        "frame",
        "panda",
        "frame",
        "type",
        "data",
        "analysis",
        "visualization",
        "scatter",
        "plot",
        "area",
        "plot",
        "bar",
        "plot",
        "Hadoop",
        "data",
        "files",
        "data",
        "sourcesproducts",
        "ADP",
        "Payroll",
        "business",
        "Workforce",
        "combining",
        "Hadoops",
        "Hive",
        "PythonSpark",
        "Map",
        "Reducers",
        "program",
        "data",
        "pipeline",
        "data",
        "testing",
        "benchmark",
        "numbers",
        "sources",
        "Federal",
        "Reserve",
        "data",
        "FRB",
        "Hive",
        "python",
        "feature",
        "engineering",
        "variables",
        "Machine",
        "ML",
        "models",
        "pyspark",
        "core",
        "Deploying",
        "Machine",
        "Learning",
        "Models",
        "H2O",
        "Hadoop",
        "cluster",
        "Spark",
        "Application",
        "Apache",
        "AVRO",
        "Parquet",
        "format",
        "schema",
        "HIVE",
        "tables",
        "data",
        "HDFS",
        "Spring",
        "Batch",
        "batch",
        "jobs",
        "Hadoop",
        "Jobs",
        "Hive",
        "Jobs",
        "Crunch",
        "Jobs",
        "Oozy",
        "work",
        "flow",
        "Hadoop",
        "cluster",
        "Developing",
        "use",
        "case",
        "diagrams",
        "class",
        "diagrams",
        "database",
        "tables",
        "mapping",
        "database",
        "tables",
        "objects",
        "Mentoring",
        "team",
        "mates",
        "solutions",
        "basis",
        "Agile",
        "Scrum",
        "methodologies",
        "scrum",
        "process",
        "Senior",
        "Java",
        "Hadoop",
        "Developer",
        "Citi",
        "Bank",
        "New",
        "York",
        "NY",
        "October",
        "August",
        "NYC",
        "US",
        "Project",
        "Optima",
        "Retail",
        "enterprise",
        "initiative",
        "data",
        "submission",
        "process",
        "CCAR",
        "reports",
        "system",
        "amount",
        "data",
        "process",
        "data",
        "account",
        "organization",
        "Optima",
        "Retail",
        "project",
        "processing",
        "generating",
        "report",
        "data",
        "bodies",
        "Technology",
        "Core",
        "Java",
        "Spring",
        "framework",
        "CORE",
        "spring",
        "MessagingJMS",
        "template",
        "JDBCtemplate",
        "Spring",
        "Integration",
        "Messaging",
        "Spring",
        "Batch",
        "cache",
        "Oracle",
        "g",
        "Database",
        "PLSQL",
        "Procedures",
        "Hadoop",
        "technologies",
        "HDFS",
        "HIVE",
        "Crunch",
        "MR",
        "Oozie",
        "workflow",
        "AVROPARQUET",
        "Python",
        "Spark",
        "Python",
        "API",
        "Panda",
        "Frame",
        "Roles",
        "Responsibilities",
        "Spring",
        "Batch",
        "batch",
        "jobs",
        "Hadoop",
        "Jobs",
        "Hive",
        "Jobs",
        "Crunch",
        "Jobs",
        "Oozie",
        "work",
        "flow",
        "Hadoop",
        "cluster",
        "communication",
        "decision",
        "making",
        "point",
        "Message",
        "Handler",
        "feed",
        "process",
        "spring",
        "integration",
        "components",
        "service",
        "router",
        "adapters",
        "processes",
        "type",
        "Hadoop",
        "Jobs",
        "Hive",
        "jobs",
        "Crunch",
        "Jobs",
        "MR",
        "jobs",
        "Oozie",
        "workflow",
        "Created",
        "Apache",
        "AVRO",
        "Parquet",
        "format",
        "schema",
        "HDFS",
        "tables",
        "data",
        "Hadoop",
        "cluster",
        "Developed",
        "use",
        "case",
        "diagrams",
        "class",
        "diagrams",
        "database",
        "tables",
        "mapping",
        "database",
        "tables",
        "object",
        "objects",
        "Python",
        "Spark",
        "Python",
        "API",
        "PySpark",
        "Account",
        "level",
        "data",
        "Analytic",
        "Models",
        "Used",
        "Pysparks",
        "RDD",
        "Dataframe",
        "HiveContext",
        "Functions",
        "functions",
        "sort",
        "diff",
        "rank",
        "window",
        "groupby",
        "loss",
        "forecast",
        "models",
        "python",
        "scripts",
        "data",
        "HDFS",
        "Spark",
        "container",
        "analysis",
        "outcome",
        "table",
        "data",
        "HDFS",
        "PandaFrame",
        "spark",
        "dataframe",
        "pandaframe",
        "type",
        "data",
        "visualization",
        "scatter",
        "plot",
        "area",
        "plot",
        "bar",
        "plot",
        "team",
        "members",
        "solutions",
        "Agile",
        "Scrum",
        "methodologies",
        "scrum",
        "process",
        "Senior",
        "Java",
        "Developer",
        "JPMorgan",
        "Chase",
        "September",
        "September",
        "NJ",
        "US",
        "Project",
        "suit",
        "projects",
        "customers",
        "Auto",
        "loan",
        "application",
        "creation",
        "underwriting",
        "manual",
        "functionality",
        "JEE",
        "technologies",
        "Overview",
        "application",
        "project",
        "suit",
        "Scorecard",
        "credit",
        "bureau",
        "FICO",
        "score",
        "data",
        "JMS",
        "messages",
        "format",
        "Mainframe",
        "system",
        "score",
        "scoring",
        "engine",
        "b",
        "Policy",
        "Engine",
        "application",
        "process",
        "loan",
        "applications",
        "application",
        "loan",
        "layer",
        "components",
        "JMS",
        "pointtopoint",
        "model",
        "ACAP",
        "Old",
        "Mainframe",
        "system",
        "text",
        "format",
        "JMS",
        "c",
        "Credit",
        "Assist",
        "application",
        "business",
        "people",
        "Dealer",
        "Data",
        "Policy",
        "Data",
        "screen",
        "scrap",
        "frame",
        "data",
        "system",
        "business",
        "rule",
        "credit",
        "assist",
        "UI",
        "screen",
        "business",
        "user",
        "Technology",
        "Core",
        "Java",
        "JEE",
        "Spring",
        "framework",
        "CORE",
        "spring",
        "MessagingJMS",
        "template",
        "JDBCtemplate",
        "Web",
        "Service",
        "SOAP",
        "REST",
        "Hibernate",
        "JPA",
        "cache",
        "Oracle",
        "g",
        "Database",
        "PLSQL",
        "Procedures",
        "Roles",
        "Responsibilities",
        "person",
        "terms",
        "Project",
        "Architecture",
        "design",
        "development",
        "inputsoutputs",
        "JPMCs",
        "Legacy",
        "Systems",
        "tier",
        "persistence",
        "tier",
        "Spring",
        "Full",
        "Stack",
        "HibernateMybatis",
        "Developed",
        "C2A",
        "layer",
        "components",
        "Spring",
        "JMS",
        "model",
        "IBM",
        "MQ",
        "broker",
        "ACAP",
        "Old",
        "Mainframe",
        "system",
        "text",
        "format",
        "JMS",
        "messages",
        "EHCache",
        "performance",
        "reference",
        "data",
        "web",
        "services",
        "transaction",
        "system",
        "web",
        "services",
        "JMS",
        "applications",
        "spring",
        "transaction",
        "AOP",
        "services",
        "SSLTLS",
        "implementation",
        "design",
        "documents",
        "Test",
        "cases",
        "scenarios",
        "Junit",
        "test",
        "cases",
        "persistence",
        "service",
        "tiers",
        "Legacy",
        "Mainframe",
        "system",
        "applications",
        "JEE",
        "Application",
        "BAs",
        "clients",
        "requirements",
        "terms",
        "interaction",
        "applications",
        "applications",
        "delivery",
        "phase",
        "data",
        "verification",
        "QA",
        "Production",
        "strategy",
        "Mentored",
        "team",
        "Senior",
        "Java",
        "Developer",
        "JPMorgan",
        "Chase",
        "June",
        "September",
        "Senior",
        "Java",
        "Developer",
        "JPMorgan",
        "Chase",
        "New",
        "York",
        "NY",
        "October",
        "September",
        "NYC",
        "US",
        "Technology",
        "Core",
        "Java",
        "JEE",
        "Spring",
        "Core",
        "Transaction",
        "JMS",
        "JDBC",
        "Hibernate",
        "EHCACHE",
        "Web",
        "Service",
        "XML",
        "XSLT",
        "Junit",
        "SOAPUI",
        "Maven",
        "Jenkins",
        "ABB",
        "client",
        "cloud",
        "Web",
        "Service",
        "Vignette",
        "Application",
        "Portal",
        "SOA",
        "MULE",
        "Project",
        "Total",
        "Fund",
        "Repository",
        "TFR",
        "Data",
        "integration",
        "TFR",
        "repository",
        "funds",
        "data",
        "points",
        "TFR",
        "web",
        "solution",
        "management",
        "entities",
        "Portfolio",
        "managers",
        "Benchmarks",
        "Share",
        "classes",
        "Disclosures",
        "Data",
        "integration",
        "data",
        "synchronization",
        "SalesPage",
        "Retail",
        "CRM",
        "Aprimo",
        "3rd",
        "party",
        "data",
        "provider",
        "system",
        "SOA",
        "MULE",
        "ESB",
        "Roles",
        "Responsibilities",
        "data",
        "Integration",
        "application",
        "data",
        "update",
        "process",
        "systems",
        "ESB",
        "framework",
        "data",
        "synchronization",
        "SalesPage",
        "Retail",
        "CRM",
        "Aprimo",
        "3rd",
        "party",
        "data",
        "provider",
        "system",
        "performance",
        "services",
        "Ehcache",
        "ESB",
        "part",
        "application",
        "system",
        "data",
        "exchange",
        "SOAMULE",
        "ESB",
        "web",
        "solution",
        "management",
        "entities",
        "Portfolio",
        "managers",
        "Benchmarks",
        "Share",
        "classes",
        "Disclosures",
        "Entities",
        "JPMCs",
        "Total",
        "Fund",
        "Repository",
        "enhancement",
        "testing",
        "bug",
        "Performed",
        "code",
        "review",
        "Mentored",
        "team",
        "members",
        "Java",
        "Developer",
        "JPMorgan",
        "Chase",
        "Kolkata",
        "West",
        "Bengal",
        "March",
        "September",
        "Kolkata",
        "India",
        "NYC",
        "US",
        "Project",
        "IM",
        "Core",
        "Party",
        "Data",
        "A",
        "group",
        "applications",
        "Core",
        "Party",
        "Account",
        "data",
        "JPMC",
        "Investment",
        "Management",
        "applications",
        "Multiple",
        "Web",
        "services",
        "applications",
        "applications",
        "Technology",
        "Core",
        "Java",
        "JEE",
        "JMS",
        "Spring",
        "Core",
        "MVC",
        "Transaction",
        "JMS",
        "JDBC",
        "Hibernate",
        "JPA",
        "EHCACHE",
        "Web",
        "Service",
        "Apache",
        "CXF",
        "Junit",
        "SOAPUI",
        "Quartz",
        "Maven",
        "Jenkins",
        "ABB",
        "client",
        "cloud",
        "Roles",
        "Responsibilities",
        "client",
        "BAs",
        "downstream",
        "business",
        "requirements",
        "Core",
        "Party",
        "Account",
        "data",
        "JPMC",
        "Investment",
        "Management",
        "Group",
        "core",
        "party",
        "SOAP",
        "Restful",
        "web",
        "services",
        "JMS",
        "applications",
        "EHCache",
        "performance",
        "reference",
        "data",
        "web",
        "services",
        "transaction",
        "system",
        "web",
        "services",
        "JMS",
        "applications",
        "spring",
        "transaction",
        "AOP",
        "Secured",
        "services",
        "SSLTLS",
        "implementation",
        "application",
        "JPMC",
        "Cloud",
        "system",
        "Performed",
        "code",
        "review",
        "Mentored",
        "team",
        "members",
        "Offshoreonsite",
        "coordination",
        "Java",
        "Developer",
        "JPMorgan",
        "Chase",
        "August",
        "May",
        "Java",
        "Developer",
        "JPMorgan",
        "Chase",
        "Kolkata",
        "West",
        "Bengal",
        "August",
        "March",
        "Kolkata",
        "India",
        "Project",
        "IM",
        "Common",
        "Services",
        "Application",
        "Interaction",
        "systems",
        "JPMC",
        "Investment",
        "Management",
        "place",
        "time",
        "applications",
        "IM",
        "Common",
        "Services",
        "IM",
        "CSA",
        "Application",
        "system",
        "subsystems",
        "Investment",
        "Management",
        "division",
        "JPMorgan",
        "Chase",
        "Bank",
        "Technology",
        "Core",
        "Java",
        "JEE",
        "Spring",
        "Quartz",
        "Restful",
        "SOAP",
        "Web",
        "service",
        "XML",
        "WebSphere",
        "6x",
        "Application",
        "server",
        "Sybase",
        "Unix",
        "Roles",
        "Responsibilities",
        "RESTfulSOAP",
        "Web",
        "services",
        "system",
        "server",
        "Interaction",
        "systems",
        "JPMC",
        "Investment",
        "Management",
        "group",
        "JEE",
        "componentsaround",
        "applications",
        "ReArchitected",
        "EJB",
        "components",
        "spring",
        "framework",
        "components",
        "transaction",
        "Indicative",
        "Data",
        "Price",
        "Data",
        "Java",
        "Developer",
        "Pro",
        "Quest",
        "Noida",
        "Uttar",
        "Pradesh",
        "October",
        "July",
        "Noida",
        "India",
        "Project",
        "archives",
        "sources",
        "newspapers",
        "journals",
        "periodicals",
        "dissertations",
        "databases",
        "types",
        "content",
        "pages",
        "Technology",
        "Core",
        "JEE",
        "JSP",
        "Servlet",
        "Web",
        "service",
        "Hibernate",
        "JPA",
        "ORM",
        "Oracle",
        "DB",
        "Apache",
        "Tapestry",
        "Apace",
        "Lucene",
        "API",
        "JSF",
        "DHTMLX",
        "script",
        "component",
        "Roles",
        "Responsibilities",
        "performance",
        "Restful",
        "web",
        "services",
        "archives",
        "sources",
        "newspapers",
        "periodicals",
        "dissertations",
        "databases",
        "types",
        "number",
        "contents",
        "pages",
        "UIweb",
        "layer",
        "apache",
        "tapestry",
        "DHTMLX",
        "SCRUM",
        "work",
        "Plan",
        "view",
        "Rational",
        "IBM",
        "methodology",
        "Java",
        "Developer",
        "Proquest",
        "IHG",
        "August",
        "July",
        "Java",
        "Developer",
        "Intercontinental",
        "Hotels",
        "Grp",
        "Noida",
        "Uttar",
        "Pradesh",
        "August",
        "October",
        "Noida",
        "India",
        "Project",
        "Expedia",
        "User",
        "Management",
        "project",
        "IHG",
        "Third",
        "Party",
        "TPI",
        "standards",
        "solution",
        "rate",
        "availability",
        "status",
        "information",
        "message",
        "notification",
        "partner",
        "IHG",
        "Web",
        "application",
        "Intercontinental",
        "Hotels",
        "Group",
        "IHG",
        "web",
        "project",
        "hotel",
        "reservations",
        "hotels",
        "PCR",
        "Technology",
        "Core",
        "Spring",
        "MVC",
        "JEE",
        "JSP",
        "Servlet",
        "Resin",
        "Pro",
        "application",
        "server",
        "ATG",
        "dynamo",
        "application",
        "server",
        "Genesis",
        "Framework",
        "Roles",
        "Responsibilities",
        "requirements",
        "IHG",
        "TPI",
        "standards",
        "solution",
        "rate",
        "availability",
        "status",
        "information",
        "message",
        "notification",
        "Developed",
        "maintenance",
        "admin",
        "module",
        "User",
        "Management",
        "module",
        "hotels",
        "PCR",
        "accounts",
        "Spring",
        "MVC",
        "legacy",
        "application",
        "Education",
        "Bachelor",
        "Engineering",
        "Information",
        "Technology",
        "Jadavpur",
        "University",
        "Skills",
        "JAVA",
        "years",
        "JDBC",
        "years",
        "JMS",
        "years",
        "ORACLE",
        "years",
        "APACHE",
        "years",
        "Apache",
        "Spark",
        "years",
        "Apache",
        "Hive",
        "years",
        "MapReduce",
        "years",
        "Avro",
        "years",
        "Parquet",
        "years",
        "Python",
        "years",
        "Panda",
        "Frame",
        "years",
        "Apache",
        "HDFS",
        "years",
        "Flask",
        "C",
        "Links",
        "httpswwwlinkedincominmondalmithun",
        "Additional",
        "Information",
        "Technical",
        "Skills",
        "Technology",
        "Stack",
        "Language",
        "Core",
        "Java",
        "SQL",
        "Python",
        "Technology",
        "JavaJ2EE",
        "Hadoop",
        "Hive",
        "Crunch",
        "Spark",
        "Panda",
        "Frame",
        "MapReduce",
        "Oozie",
        "HDFSAvro",
        "Parquet",
        "Databases",
        "Oracle",
        "g",
        "Framework",
        "Spring",
        "4x",
        "JMS",
        "Transaction",
        "JDBC",
        "Security",
        "AOP",
        "Batch",
        "Integration",
        "MVC",
        "Middleware",
        "Technologies",
        "Hibernate",
        "4x",
        "MyBatis",
        "IBM",
        "MQ",
        "7x",
        "Front",
        "End",
        "Technologies",
        "JQuery",
        "JSON",
        "Javascript",
        "DHTMLX",
        "Web",
        "servers",
        "App",
        "servers",
        "Apache",
        "Tomcat",
        "8x",
        "Server",
        "6x",
        "Continuous",
        "Integration",
        "Tools",
        "Jenkins",
        "Sonar",
        "Others",
        "Web",
        "service",
        "SOAP",
        "RESTful",
        "Apache",
        "CXF",
        "Jersey",
        "Client",
        "transaction",
        "JUnit",
        "Apache",
        "POI",
        "Jprofiler",
        "EhCache",
        "Maven",
        "Java",
        "Multithreading"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T23:09:54.836870",
    "resume_data": "Java Python Hadoop Developer Javaspan lPythonspan Hadoop span lDeveloperspan Senior Java Hadoop Developer Citi Bank New York NY Has almost 115 years including 6 years in US of extensive handson experience in latest Java and Python and its related technologies along with 3 years of experience in Hadoop related technologies Hive Spark Mapreduce and Oozie using Cloudera distribution of Hadoop Framework Has been working with a market leading Human Resources Company and previously worked with Retail Risk Investment and Auto Loan banking clients A selfstarter who takes ownership of tasks at hand with experience in architecting designing developing and testing complex custom applications in JavaJ2EE Spring Framework and development of Big Data technologies in highly scalable endtoend Hadoop Infrastructure Capable and proactive team player with excellent analytical problem solving communication and interpersonal skills with ability to interact with individuals Authorized to work in the US for any employer Work Experience Java Python Hadoop Developer ADP Roseland NJ November 2017 to Present Project Working on ADPs Ventures Development and its related Data Cloud Benchmark projects Data cloud benchmarking team is responsible to create data pipelines Machine learning models Data from the following sources Employee HR Payroll Largemedium business Workforce NowWFNMAS and small business SBS are cleaned normalized combined and aggregated and then go through benchmarking cubesmatrices process to build data pipelines and ML models are used by other downstream ADPs projects like Venture Next Gen PayrollPi Innovation lab etc Technologies Cloudera distribution of Hadoop Framework componentstechnologies HIVE Impala Spark HDFS MR Oozie workflow AVROPARQUET Machine learningML model creationdeployment Technology H2O distribution Driverless AI H2O server Core Java 8 and Python 273 for feature engineering for ML model Spring framework CORE spring MessagingJMS template springDAO JDBCtemplate Spring Integration Messaging and Spring BatchEh cache Oracle 11g Database PLSQL Queries Python Spark Python package Panda Frame REST Web service API java jaxwrs and python flask and sqlite3 packages Roles and Responsibilities Working on Python along with HiveImpala and Spark Python API PySpark to prepare Employee level client level and Payroll level data used for the predictable Analytic Models used PySpark RDD Data frame HiveSQL and Hive Context Creating REST web services to publish Machine learning models business function and earnings code prediction models via API as predictive analytic service using Java 8 and python flask codernity DB SQLitedb etc Creating python scripts to load data from HDFSCSV to Spark container for analysis and then save the outcome as textCSV table data to HDFS Using Panda Frame to convert spark data frame to panda frame for various type of quick data analysis and visualization like scatter plot area plot bar plot etc Extracting raw Hadoop data files from multiple data sourcesproducts like ADP Payroll Small business Workforce now and cleaning normalizing combining and aggregating using Hadoops Hive PythonSpark and Map Reducers program to create data pipeline for benchmarking ADP data Validating and testing benchmark numbers versus external sources like Federal Reserve data FRB using Hive python Creating feature engineering variables for Machine learning ML models using pyspark core java 8 Deploying Machine Learning Models built in H2O on Hadoop cluster Spark Application Using Apache AVRO Parquet format schema for the HIVE tables to persist data in HDFS Designed developed Spring Batch based batch jobs to invoke and execute the Hadoop Jobs Hive Jobs Crunch Jobs and Oozy work flow in Hadoop cluster Developing and maintaining use case diagrams class diagrams database tables and mapping between relational database tables and objectoriented java objects Mentoring team mates and help them with their technical solutions on daily basis Practicing Agile Scrum methodologies and scrum process etc Senior Java Hadoop Developer Citi Bank New York NY October 2015 to August 2017 NYC US Project Optima Retail is an enterprise wide initiative to automate the regulatory data submission process for CCAR and BASEL reports This involves building a system flexible and fast enough to process huge amount of data as the process actually involves consolidating data from each and every retail account of the organization Optima Retail project is responsible for extracting processing and generating report data required to be submitted to the regulatory bodies Technology Core Java Spring framework CORE spring MessagingJMS template springDAO JDBCtemplate Spring Integration Messaging and Spring Batch Eh cache Oracle 11g Database PLSQL Procedures Hadoop technologies HDFS HIVE Crunch MR Oozie workflow AVROPARQUET Python Spark Python API Panda Frame Roles and Responsibilities Designed developed Spring Batch based batch jobs to invoke and execute the Hadoop Jobs Hive Jobs Crunch Jobs and Oozie work flow in Hadoop cluster Developed common communication and decision making point of the systemInternal Message Handler to handle feed process using spring integration components like service router adapters to communicate between various upstream and downstream processes Created various type of Hadoop Jobs Hive jobs Crunch Jobs using MR jobs and invoked them using Oozie workflow Created Apache AVRO Parquet format schema for the HDFS tables to persist data in Hadoop cluster Developed use case diagrams class diagrams database tables and mapping between relational database tables and object oriented java objects Worked on Python and Spark Python API PySpark to prepare Account level data used for the predictable Analytic Models Used Pysparks RDD Dataframe HiveContext Functions to create various analytic functions like binning sort diff rank window groupby etc for loss forecast models Created python scripts to load data from HDFS to Spark container for analysis and then save the outcome as textCSV table data to HDFS Used PandaFrame to convert spark dataframe to pandaframe for various type of data visualization like scatter plot area plot bar plot etc Mentored team mates junior members and help them with technical solutions Participatedfollowed Agile Scrum methodologies and scrum process Senior Java Developer JPMorgan Chase September 2013 to September 2015 NJ US Project Worked in a suit of projects where customers Auto loan application creation and automatic underwriting manual under writing functionality has been built using latest JEE technologies Overview of each of the application in the project suit has been given below a Scorecard Its receives credit bureau or FICO score and its related data in JMS messages format from Mainframe system it calculates the score as per the firm scoring engine b Policy Engine This application process all the loan applications and decides if any application should be given loan Middle layer components have been developed using JMS pointtopoint messaging model and It interacts with ACAP Old Mainframe system using text format JMS messages c Credit Assist This application is used by business people to displaysee Dealer Data Policy Data which screen scrap main frame data from ACAP system and process them according to business rule and display them on the credit assist UI screen to business user Technology Core Java JEE Spring framework CORE spring MessagingJMS template springDAO JDBCtemplate Web Service SOAP and REST Hibernate JPA Eh cache Oracle 11g Database PLSQL Procedures Roles and Responsibilities Worked as the key person in terms of Project Architecture design development identifying inputsoutputs discussing with the JPMCs various Legacy Systems Extensively worked on middle tier and persistence tier using the Spring Full Stack and HibernateMybatis Developed C2A Middle layer components using Spring JMS pointtopoint messaging model with IBM MQ broker and it interacts with ACAP Old Mainframe system using text format JMS messages Used EHCache to achieved high performance of the reference data web services Developed reliable transaction system for the web services and JMS applications using spring transaction and AOP and secured the services using SSLTLS implementation Updated and reviewed design documents created various Test cases scenarios and Junit test cases to test persistence and service tiers Migrated the Legacy Mainframe system applications to JEE Application Worked with BAs and clients to identify the requirements in terms of interaction of the proposed applications with external cosupplier applications delivery phase data verification QA Production strategy Mentored team mates Senior Java Developer JPMorgan Chase June 2012 to September 2015 Senior Java Developer JPMorgan Chase New York NY October 2012 to September 2013 NYC US Technology Core Java JEE Spring Core Transaction JMS and JDBC Hibernate EHCACHE Web Service XML XSLT Junit SOAPUI Maven build Jenkins ABB client private cloud Web Service Vignette Application Portal SOA MULE Project Worked in Total Fund Repository TFR and Data integration a TFR a central repository for all the funds related data points TFR provides a web based solution to Fund management and fund related entities like Portfolio managers Benchmarks Share classes Disclosures etc b Data integration Near realtime data synchronization between SalesPage Retail CRM and Aprimo 3rd party data provider system bidirectionally using SOA MULE ESB Roles and Responsibilities Developed and maintained the data Integration application eliminating the existing manual data update process in multiple systems using MULE soft ESB framework Achieved near realtime data synchronization between SalesPage Retail CRM and Aprimo 3rd party data provider system bidirectionally Create multiple high performance Restful services using Ehcache and they were used inside ESB as part of the application Identified future system for data exchange and can be integrated using SOAMULE ESB Developed a web based solution to Fund management and fund related entities like Portfolio managers Benchmarks Share classes Disclosures etc Entities involved in JPMCs Total Fund Repository Worked in enhancement testing and bug fixing Performed code review Mentored and trained junior team members Java Developer JPMorgan Chase Kolkata West Bengal March 2011 to September 2012 Kolkata India and NYC US Project Worked in IM Core Party Data A group of applications created to feed Core Party Account data of JPMC Investment Management to downstream applications Multiple Web services and Messaging applications have been built to cater the downstream applications Technology Core Java JEE JSPServlet JMS Spring Core MVC Transaction JMS and JDBC Hibernate JPA EHCACHE Web Service Apache CXF Junit SOAPUI Quartz scheduler Maven build Jenkins ABB client private cloud Roles and Responsibilities Worked with client and BAs of the downstream to gather business requirements for Core Party Account data feeds of JPMC Investment Management Group Designed developed and maintained several core party SOAP and Restful web services and JMS applications Used EHCache to achieved high performance for the reference data web services Developed reliable transaction system for the web services and JMS applications using spring transaction and AOP Secured the services using SSLTLS implementation and deployed application in JPMC Private Cloud system Performed code review and Mentored and trained junior team members Offshoreonsite coordination Java Developer JPMorgan Chase August 2010 to May 2012 Java Developer JPMorgan Chase Kolkata West Bengal August 2010 to March 2011 Kolkata India Project Worked in IM Common Services Application Interaction among the various systems of JPMC Investment Management took place at real time using the various applications of IM Common Services IM CSA Application was a messaging system that was used to interact with different subsystems of Investment Management division of JPMorgan Chase Bank Technology Core Java JEE Spring Quartz Restful SOAP Web service XML WebSphere 6x Application server Sybase Unix Roles and Responsibilities Developed multiple RESTfulSOAP Web services and messaging system to build the instrument server which handles Interaction among the various systems of JPMC Investment Management group Migrated JEE componentsaround 30 small and big applications build from ant to maven ReArchitected and developed all existing EJB components to spring framework components to handle transaction of Indicative Data and Price Data Java Developer Pro Quest Noida Uttar Pradesh October 2008 to July 2010 Noida India Project It provides archives of sources such as newspapers journals periodicals dissertations and aggregated databases of various types and its content is estimated to be approximately 125 billion digital pages Technology Core java JEE JSP Servlet RESTful Web service Hibernate JPA ORM Oracle DB Apache Tapestry Apace Lucene API JSF DHTMLX java script component Roles and Responsibilities Implemented several high performance Restful web services that was used as archives of sources as newspapers journals periodicals dissertations and aggregated databases of various types Handled large number contents estimated to be approximately 125 billion digital pages in UIweb layer using apache tapestry and DHTMLX Followed and used the SCRUM work Plan view and Rational of IBM for Agile methodology Java Developer Proquest and IHG August 2007 to July 2010 Java Developer Intercontinental Hotels Grp Noida Uttar Pradesh August 2007 to October 2008 Noida India Project Worked in Expedia User Management This project was built to support IHG Third Party Intermediates TPI standards by providing an automated solution for pushing rate and availability status information message notification requested by its external partner IHG Web application Intercontinental Hotels Group IHG was a web based project for hotel reservations handling more than 4000 hotels worldwide and managing around 10 million PCR accounts Technology Core java Spring MVC JEE JSP Servlet Resin Pro 312 as application server ATG dynamo 70 as application server Genesis Framework Roles and Responsibilities Analyzed the requirements to support IHG TPI standards by providing an automated solution for pushing rate and availability status information message notification Developed and maintenance of the admin module and User Management module and handled more than 4000 hotels worldwide and managing around 10 million PCR accounts Used Spring MVC to refactor the legacy application Education Bachelor of Engineering in Information and Technology Jadavpur University 2003 to 2007 Skills JAVA 10 years JDBC 10 years JMS 6 years ORACLE 10 years APACHE 10 years Apache Spark 3 years Apache Hive 3 years MapReduce 3 years Avro 3 years Parquet 3 years Python 3 years Panda Frame 2 years Apache HDFS 3 years Flask C Links httpswwwlinkedincominmondalmithun Additional Information Technical Skills Technology Stack Language Core Java 7 SQL Python Technology JavaJ2EE Hadoop Hive Crunch Spark Panda Frame MapReduce Oozie HDFSAvro and Parquet Databases Oracle 11g Framework Spring 4x JMS Transaction JDBC Security AOP Batch Integration and MVC Middleware Technologies Hibernate 4x MyBatis 311 and IBM MQ 7x Front End Technologies JQuery JSON Javascript DHTMLX Web servers App servers Apache Tomcat 8x WAS Server 6x Continuous Integration Tools Jenkins Sonar Others Web service SOAP and RESTful Apache CXF and Jersey Client Distributed transaction JUnit Apache POI Jprofiler EhCache Maven build Java 7 Multithreading",
    "unique_id": "10496eda-c3fe-4cda-a878-9501826daf96"
}