{
    "clean_data": "Hadoop Developer Hadoop span lDeveloperspan Hadoop Developer Verizon Dallas TX Having more than 7 years of experience in IT industry involved in Developing Implementing testing and maintenance of various web based applications using J2EE technologies and Big Data ecosystems experience on Linux environment Including comprehensive experience as a Hadoop Big Data Analytics Developer Expertise on Hadoop architecture and ecosystems such as HDFS Map Reduce programming paradigm Experience in installation configuration supporting and monitoring Hadoop clusters using Apache Cloudera distributions and AWS Knowledge in installing configuring and using Hadoop ecosystem components like Hadoop Map Reduce HDFS HBase YARN Oozie Hive Sqoop Pig Zookeeper and Flume Good understanding of NoSQL databases like HBase Cassandra and MongoDB Experience in Spark and scala Experience in importing and exporting data using Apache Sqoop from HDFS to Relational Database Systems NonRelational Database Systems and viceversa Extending Hive and Pig core functionality by writing custom UDFs Experienced in analyzing data using HiveQL Pig Latin and custom Map Reduce programs in Java Experience in building maintaining multiple Hadoop clusters prod dev etc of different sizes and configuration and setting up the rack topology for large clusters Worked on NoSQL databases including HBase Cassandra and MongoDB Experienced in SYBASE Teradata Experienced in Datawarehousing systems Experienced in data integration and Datamodeling Experienced in job workflow scheduling and monitoring tools like Oozie and Zookeeper Strong experience with XML JSON SQL DB2 Oracle Experience on the Linux operating system Experienced in designing developing and implementing connectivity products that allow efficient exchange of data between the core database engine and the Hadoop ecosystem Experienced in Data warehousing and using ETL tools like Informatica and Pentaho Expert level skills in developing intranetinternet application using JAVAJ2EE technologies which includes Struts framework MVC design Patterns Chrodiant Servlets JSP JSLT XMLXLST Java Script AJAX EJB JDBC JMS JNDI RDMS SOAP Hibernate and custom tag Libraries Experience using XML XSD and XSLT Experience with webbased UI development using jQuery UI jQuery ExtJS CSS HTML HTML5 XHTML and JavaScript Extensive experience in middletier development using J2EE technologies like JNDI JSP Servlets JSF Struts Spring Hibernate JDBC EJB Possess excellent technical skills consistently outperformed schedules and acquired interpersonal and communication skills Authorized to work in the US for any employer Work Experience Hadoop Developer Verizon Dallas TX January 2017 to Present Verizons employee portal provides ability to search employee browsing habits at work To increase employee productivity and ensure security of company information Verizons employee portal tracks employee email and internet browsing collects various data like urls transmitting data size into HbaseHDFS Hadoop ecosystem was used collect the Big Data and analyze the data Flume servers were installed on the network proxy servers to collect the data and store into HDFS Responsibilities Involved in full lifecycle of the project from Design Analysis logical and physical architecture modeling development Implementation testing Responsible to manage data coming from different sources and involved in HDFS maintenance and loading of structured and unstructured data Collected data was analyzed using Map Reduce Jobs and presented to employee portal built using Spring MVC Processed Big Data using a Hadoop cluster consisting of 40 nodes Designed and configured Flume servers to collect data from the network proxy servers and store to HDFS Developed data pipeline using Flume and Java map reduce to ingest employee browsing data into HbaseHDFS for analysis Used agentE2EChain for reliability and failover in flume Designed and implemented Map Reduce jobs for analyzing the data collected by the flume server Actively involved in working with Hadoop Administration team to debugging various slow running MR Jobs and doing the necessary optimizations Designed and implemented RESTFul APIs to retrieve the data from Hadoop Platform to Employee Portal Web Application Optimized the full text search function by connecting MongoDB and Elastic Search Utilized AWS framework for content storage and Elastic Search for document search Creating Hive tables and working on them using Hive QL Created concurrent access for hive tables with shared and exclusive locking that can be enabled in hive with the help of Zookeeper implementation in the cluster Wrote the shell scripts to monitor the health check of Hadoop daemon services and respond accordingly to any warning or failure conditions Wrote MRUnit tests for unit testing the Map Reduce jobs Performed functional requirement review Worked closely with Risk Compliance Team and BA Facilitated Knowledge transfer sessions Worked in an agile environment Hadoop Developer Kroger Co Cincinnati OH April 2014 to December 2016 RetailLogistics is instore analytics program which provides comprehensive information on how to resolve the shoppers traffic It provides the detailed analysis to understand shoppers behavior purchase pattern By using these insights it is used to increase instore traffic optimize staffing improve merchandising and display performance Responsibilities Workings on bigdata infrastructure build out for batch processing as well as realtime processing Worked on Hadoop Hive Oozie and MySQL customization for batch data platform setup Worked on implementation of a log producer in SCALA that watches for application logs transforms incremental logs and sends them to a Kafka and Zookeeper based log collection platform Implemented a data export application to fetch processed data from these platforms to consuming application databases in a scalable manner Design deploy Manage cluster nodes for our data platform operations rackingstacking Install and configure cluster Setting up puppet for centralized configuration management Monitoring Cluster using various tools to see how the nodes are performing Developed Spark scripts by using Scala Shell commands as per the requirement Expertise in cluster task like adding Nodes Removing Nodes without any effect to running jobs and data Write scripts to automate application deployments and configurations Monitoring YARN applications Troubleshoot and resolve cluster related system problems Wrote map reduce programs to clean and preprocess the data coming from different sources Implemented various output formats like Sequence file and parquet format in Map reduce programs Also implemented multiple output formats in the same program to match the use cases Developed Hadoop streaming MapReduce works using Python Performed benchmarking of the NoSQL databases Cassandra and HBase Created data model for structuring and storing the data efficiently Implemented partitioning and bucketing of tables in Cassandra Implemented test scripts to support test driven development and continuous integration Converted text files into Avro then to parquet format for the file to be used with other Hadoop eco system tools Experienced on loading and transforming of large sets of structured semi structured and unstructured data Developed and implemented core API services using Scala and Spark Exported the analyzed data to HBase using Sqoop and to generate reports for the BI team Analyzed large amounts of data sets to determine optimal way to aggregate and report on it Participate in requirement gathering and analysis phase of the project in documenting the business requirements by conducting workshopsmeetings with various business users POC work is going on using Spark and Kafka for real time processing Developed a data pipeline using Kafka and Storm to store data into HDFS Populated HDFS and Cassandra with huge amounts of data using Apache Kafka POC work is going on comparing the Cassandra and HBase NoSQL databases Worked with NoSQL databases like Cassandra and Mongo DB for POC purpose Implement POC with Hadoop Extract data with Spark into HDFS Environment MapReduce HDFS Hive Pig Hue Oozie Core Java Eclipse Hbase Flume Spark Scala Kafka Cloudera Manager Cassandra Python Greenplum DB IDMS VSAM SQLPLUS Toad Putty Windows NT UNIX Shell Scripting Pentaho Talend Bigdata YARN Mongo DB Sr Java Developer Huntington Bank Columbus OH January 2012 to March 2014 Risk Rating is developed for calculating the risks on the loans given by the Huntington Bank to their customers This is the rating system for the loans which takes the borrower and customer type information and performs a series of ratings and calculations This system gives the rating of the risk associated with that loan Frontend is built using a Spring MVC framework in the presentation layer The back end functionality was built using Oracle as data persistence layer and Hibernate as data access layer Responsibilities Reviewed functional and nonfunctional requirements Implemented a highly visible POC using Hadoop ecosystem Responsible for Identifying risks from the discussions happens during onsite hours and high light the risk by articulating risk and mitigation option to project managers Responsible for leading performance test preparations and execution And leading the performance issues from identifying the root cause have discussions with offshore tech leads and architects to put a fix in place Developed application service components created Hibernate mapping files and generated database schema Designed and implemented UI module using JSPs and Spring MVC framework Implemented design patterns like MVC Factory Composite and Singleton Implemented persistence logic in ORM technology using Hibernate OR mappings Developed hibernate mapping file and Criteria queries for retrieving data from the database Created various Data Access Objects for Addition modification and deletion of records using various specification files and also using CRUD manager concept in Hibernate for the above operations Designed and implemented risk rating engine Used Log4J logging framework for logging messages Performed Unit Testing of Various Modules by generating the Test Cases Environment Java 16 Eclipse Indigo JBoss 50 JSP JQuery Maven JUnit Log4J Visio Oracle SQL Developer HDFS 020 SVN Unix Hibernate 331 Java Developer InfoTech Software solutions May 2009 to December 2011 The FNS is low cost internet and mobile banking based delivery channel solution which allows intuitions to greatly enhanced customer service levels and to streamline banking services offered to customers and allows customers the flexibility to take control of banking activities via the internet via leveraging the rich functionality and uninterrupted 247 availability of the core banking The rich functionality of Financial Connect include business banking payroll scheduling and international funds transfer together with standing paymentspay domestic funds transfers account enquiries online loan simulations and loan applications Responsibilities Developed Controllers for request handling using Spring framework Involved in Command controllers handler mappings and View Resolvers Designed and developed application components and architectural proof of concepts using Java EJB JSP JSF Struts and AJAX Participated in Enterprise Integration experience web services Configured JMS MQ EJB and Hibernate on Web sphere and JBoss Focused on Declarative transaction management Developed XML files for mapping requests to controllers Coded Spring Portlets to build portal pages for application using JSR 286 API Hibernate templates were used to access database Coded JDBC calls in the servlets to access the Oracle database tables Responsible for Integration unit testing system testing and stress testing for all the phases of project Prepared final guideline document that would serve as a tutorial for the users of this application Use the DAO in developing application code Developed stored procedures Extensively used Java Collection framework and Exception handling Environment Java J2EE5 Spring JSP XML Spring TLD JSP Servlets Hibernate Criteria API XSLT CSS JSF JSF RichFaces WASD Java Swing Web service AXIS Server WSDL XML Glassfish JSR 286 API UML EJB Java script JQuery Hibernate SQL CVS Agile JUnit Oracle SQL Developer Hadoop Developer RCFile March 2003 to May 2003 Environment Hadoop 0202 MR1 CDH3U6 HDFS Hbase  Flume 093 Sqoop 1x AWS EC2 S3 Elastic Search Hive 071 Java 16 Linux Spring 3x Eclipse Juno XML REST JSON Maven Avro RCFile SVN Education Bachelors Skills JAVA 8 years SQL 7 years APACHE HADOOP HDFS 6 years Hadoop 6 years HADOOP 6 years Hadoop 0202 MR1 CDH3U6 HDFS Hbase  Flume 093 Sqoop 1x AWS EC2 S3 Elastic Search Hive 071 Java 16 Linux Spring 3x Eclipse Juno XML REST JSON Maven Avro RCFile SVN 8 years Drupal Hybris Cobol Servicenow Additional Information TECHNICAL SKILLS HadoopBig Data HDFS Mapreduce HBase Pig Hive Sqoop Oozie YARN ZooKeeper NoSQL DB CDH3 CDH4 Apache Hadoop Java J2EE Technologies Core Java Servlets JSP JDBC JNDI Java Collections IDEs Eclipse Net beans Frameworks MVC Struts Hibernate Spring Programming languages Java Python Ant scripts Linux shell scripts Build Management Tools Maven Apache Ant Version control SVN github Databases Oracle 11g10g9i MySQL DB2 MSSQL Server Web Servers Web Logic Web Sphere Apache Tomcat Web Technologies HTML XML JavaScript AJAX SOAP WSDL Network Protocols TCPIP UDP HTTP DNS DHCP ETL Tools Informatica Pentaho Testing WinRunner QTP Selenium",
    "entities": [
        "API Hibernate",
        "Design Analysis",
        "Servicenow Additional Information TECHNICAL SKILLS HadoopBig Data HDFS Mapreduce HBase Pig Hive Sqoop Oozie",
        "Informatica",
        "ORM",
        "HDFS Environment MapReduce HDFS Hive Pig Hue Oozie Core",
        "Visio Oracle SQL Developer",
        "BI",
        "XML JSON",
        "Hadoop Developer Hadoop",
        "Criteria",
        "Apache Sqoop",
        "Developed Spark",
        "Responsible for Integration",
        "Build Management Tools",
        "Indigo",
        "Patterns Chrodiant Servlets JSP",
        "Maven Avro RCFile SVN",
        "the Huntington Bank",
        "Developed Hadoop",
        "Hadoop",
        "HDFS Responsibilities Involved",
        "Responsibilities Reviewed",
        "Maven Avro RCFile",
        "Cincinnati",
        "Responsible for Identifying",
        "HBase",
        "Hadoop Administration",
        "API UML",
        "TX",
        "JAVAJ2EE",
        "JQuery Hibernate SQL CVS",
        "Developed",
        "Dallas",
        "Ant Version",
        "Oracle 11g10g9i MySQL DB2 MSSQL Server Web Servers Web Logic Web Sphere Apache Tomcat Web Technologies HTML XML JavaScript AJAX SOAP WSDL",
        "Sequence",
        "Exception",
        "XML XSD",
        "FNS",
        "Linux",
        "JSP",
        "Responsibilities Developed Controllers",
        "TLD JSP Servlets",
        "Hive QL Created",
        "Oracle Experience on the",
        "MVC",
        "Java Collection",
        "Spark",
        "EJB Possess",
        "HbaseHDFS Hadoop",
        "API",
        "jQuery UI jQuery ExtJS",
        "Hadoop Big Data Analytics Developer Expertise on Hadoop",
        "US",
        "Sqoop",
        "RetailLogistics",
        "Storm",
        "View Resolvers Designed",
        "Hadoop Extract",
        "Oracle",
        "Singleton",
        "the Test Cases Environment",
        "DNS",
        "Developed XML",
        "Present Verizons",
        "Monitoring Cluster",
        "SQL",
        "Command",
        "Performed Unit Testing of Various Modules",
        "Relational Database Systems NonRelational Database Systems",
        "Cassandra Implemented",
        "MVC Factory Composite",
        "Big Data",
        "Coded Spring Portlets",
        "Hadoop Developer Kroger Co",
        "ETL",
        "CRUD",
        "Work Experience Hadoop",
        "Zookeeper",
        "Performed",
        "XSLT",
        "HBase Created",
        "UI",
        "Worked on Hadoop Hive Oozie",
        "Drupal",
        "SQL Developer Hadoop Developer RCFile",
        "SVN",
        "AWS Knowledge",
        "Financial Connect",
        "POC",
        "Data",
        "MapReduce",
        "SCALA",
        "NoSQL",
        "Implement POC",
        "JNDI JSP Servlets",
        "Risk Compliance Team",
        "Troubleshoot",
        "Data Access Objects for Addition",
        "AXIS Server"
    ],
    "experience": "Experience in installation configuration supporting and monitoring Hadoop clusters using Apache Cloudera distributions and AWS Knowledge in installing configuring and using Hadoop ecosystem components like Hadoop Map Reduce HDFS HBase YARN Oozie Hive Sqoop Pig Zookeeper and Flume Good understanding of NoSQL databases like HBase Cassandra and MongoDB Experience in Spark and scala Experience in importing and exporting data using Apache Sqoop from HDFS to Relational Database Systems NonRelational Database Systems and viceversa Extending Hive and Pig core functionality by writing custom UDFs Experienced in analyzing data using HiveQL Pig Latin and custom Map Reduce programs in Java Experience in building maintaining multiple Hadoop clusters prod dev etc of different sizes and configuration and setting up the rack topology for large clusters Worked on NoSQL databases including HBase Cassandra and MongoDB Experienced in SYBASE Teradata Experienced in Datawarehousing systems Experienced in data integration and Datamodeling Experienced in job workflow scheduling and monitoring tools like Oozie and Zookeeper Strong experience with XML JSON SQL DB2 Oracle Experience on the Linux operating system Experienced in designing developing and implementing connectivity products that allow efficient exchange of data between the core database engine and the Hadoop ecosystem Experienced in Data warehousing and using ETL tools like Informatica and Pentaho Expert level skills in developing intranetinternet application using JAVAJ2EE technologies which includes Struts framework MVC design Patterns Chrodiant Servlets JSP JSLT XMLXLST Java Script AJAX EJB JDBC JMS JNDI RDMS SOAP Hibernate and custom tag Libraries Experience using XML XSD and XSLT Experience with webbased UI development using jQuery UI jQuery ExtJS CSS HTML HTML5 XHTML and JavaScript Extensive experience in middletier development using J2EE technologies like JNDI JSP Servlets JSF Struts Spring Hibernate JDBC EJB Possess excellent technical skills consistently outperformed schedules and acquired interpersonal and communication skills Authorized to work in the US for any employer Work Experience Hadoop Developer Verizon Dallas TX January 2017 to Present Verizons employee portal provides ability to search employee browsing habits at work To increase employee productivity and ensure security of company information Verizons employee portal tracks employee email and internet browsing collects various data like urls transmitting data size into HbaseHDFS Hadoop ecosystem was used collect the Big Data and analyze the data Flume servers were installed on the network proxy servers to collect the data and store into HDFS Responsibilities Involved in full lifecycle of the project from Design Analysis logical and physical architecture modeling development Implementation testing Responsible to manage data coming from different sources and involved in HDFS maintenance and loading of structured and unstructured data Collected data was analyzed using Map Reduce Jobs and presented to employee portal built using Spring MVC Processed Big Data using a Hadoop cluster consisting of 40 nodes Designed and configured Flume servers to collect data from the network proxy servers and store to HDFS Developed data pipeline using Flume and Java map reduce to ingest employee browsing data into HbaseHDFS for analysis Used agentE2EChain for reliability and failover in flume Designed and implemented Map Reduce jobs for analyzing the data collected by the flume server Actively involved in working with Hadoop Administration team to debugging various slow running MR Jobs and doing the necessary optimizations Designed and implemented RESTFul APIs to retrieve the data from Hadoop Platform to Employee Portal Web Application Optimized the full text search function by connecting MongoDB and Elastic Search Utilized AWS framework for content storage and Elastic Search for document search Creating Hive tables and working on them using Hive QL Created concurrent access for hive tables with shared and exclusive locking that can be enabled in hive with the help of Zookeeper implementation in the cluster Wrote the shell scripts to monitor the health check of Hadoop daemon services and respond accordingly to any warning or failure conditions Wrote MRUnit tests for unit testing the Map Reduce jobs Performed functional requirement review Worked closely with Risk Compliance Team and BA Facilitated Knowledge transfer sessions Worked in an agile environment Hadoop Developer Kroger Co Cincinnati OH April 2014 to December 2016 RetailLogistics is instore analytics program which provides comprehensive information on how to resolve the shoppers traffic It provides the detailed analysis to understand shoppers behavior purchase pattern By using these insights it is used to increase instore traffic optimize staffing improve merchandising and display performance Responsibilities Workings on bigdata infrastructure build out for batch processing as well as realtime processing Worked on Hadoop Hive Oozie and MySQL customization for batch data platform setup Worked on implementation of a log producer in SCALA that watches for application logs transforms incremental logs and sends them to a Kafka and Zookeeper based log collection platform Implemented a data export application to fetch processed data from these platforms to consuming application databases in a scalable manner Design deploy Manage cluster nodes for our data platform operations rackingstacking Install and configure cluster Setting up puppet for centralized configuration management Monitoring Cluster using various tools to see how the nodes are performing Developed Spark scripts by using Scala Shell commands as per the requirement Expertise in cluster task like adding Nodes Removing Nodes without any effect to running jobs and data Write scripts to automate application deployments and configurations Monitoring YARN applications Troubleshoot and resolve cluster related system problems Wrote map reduce programs to clean and preprocess the data coming from different sources Implemented various output formats like Sequence file and parquet format in Map reduce programs Also implemented multiple output formats in the same program to match the use cases Developed Hadoop streaming MapReduce works using Python Performed benchmarking of the NoSQL databases Cassandra and HBase Created data model for structuring and storing the data efficiently Implemented partitioning and bucketing of tables in Cassandra Implemented test scripts to support test driven development and continuous integration Converted text files into Avro then to parquet format for the file to be used with other Hadoop eco system tools Experienced on loading and transforming of large sets of structured semi structured and unstructured data Developed and implemented core API services using Scala and Spark Exported the analyzed data to HBase using Sqoop and to generate reports for the BI team Analyzed large amounts of data sets to determine optimal way to aggregate and report on it Participate in requirement gathering and analysis phase of the project in documenting the business requirements by conducting workshopsmeetings with various business users POC work is going on using Spark and Kafka for real time processing Developed a data pipeline using Kafka and Storm to store data into HDFS Populated HDFS and Cassandra with huge amounts of data using Apache Kafka POC work is going on comparing the Cassandra and HBase NoSQL databases Worked with NoSQL databases like Cassandra and Mongo DB for POC purpose Implement POC with Hadoop Extract data with Spark into HDFS Environment MapReduce HDFS Hive Pig Hue Oozie Core Java Eclipse Hbase Flume Spark Scala Kafka Cloudera Manager Cassandra Python Greenplum DB IDMS VSAM SQLPLUS Toad Putty Windows NT UNIX Shell Scripting Pentaho Talend Bigdata YARN Mongo DB Sr Java Developer Huntington Bank Columbus OH January 2012 to March 2014 Risk Rating is developed for calculating the risks on the loans given by the Huntington Bank to their customers This is the rating system for the loans which takes the borrower and customer type information and performs a series of ratings and calculations This system gives the rating of the risk associated with that loan Frontend is built using a Spring MVC framework in the presentation layer The back end functionality was built using Oracle as data persistence layer and Hibernate as data access layer Responsibilities Reviewed functional and nonfunctional requirements Implemented a highly visible POC using Hadoop ecosystem Responsible for Identifying risks from the discussions happens during onsite hours and high light the risk by articulating risk and mitigation option to project managers Responsible for leading performance test preparations and execution And leading the performance issues from identifying the root cause have discussions with offshore tech leads and architects to put a fix in place Developed application service components created Hibernate mapping files and generated database schema Designed and implemented UI module using JSPs and Spring MVC framework Implemented design patterns like MVC Factory Composite and Singleton Implemented persistence logic in ORM technology using Hibernate OR mappings Developed hibernate mapping file and Criteria queries for retrieving data from the database Created various Data Access Objects for Addition modification and deletion of records using various specification files and also using CRUD manager concept in Hibernate for the above operations Designed and implemented risk rating engine Used Log4J logging framework for logging messages Performed Unit Testing of Various Modules by generating the Test Cases Environment Java 16 Eclipse Indigo JBoss 50 JSP JQuery Maven JUnit Log4J Visio Oracle SQL Developer HDFS 020 SVN Unix Hibernate 331 Java Developer InfoTech Software solutions May 2009 to December 2011 The FNS is low cost internet and mobile banking based delivery channel solution which allows intuitions to greatly enhanced customer service levels and to streamline banking services offered to customers and allows customers the flexibility to take control of banking activities via the internet via leveraging the rich functionality and uninterrupted 247 availability of the core banking The rich functionality of Financial Connect include business banking payroll scheduling and international funds transfer together with standing paymentspay domestic funds transfers account enquiries online loan simulations and loan applications Responsibilities Developed Controllers for request handling using Spring framework Involved in Command controllers handler mappings and View Resolvers Designed and developed application components and architectural proof of concepts using Java EJB JSP JSF Struts and AJAX Participated in Enterprise Integration experience web services Configured JMS MQ EJB and Hibernate on Web sphere and JBoss Focused on Declarative transaction management Developed XML files for mapping requests to controllers Coded Spring Portlets to build portal pages for application using JSR 286 API Hibernate templates were used to access database Coded JDBC calls in the servlets to access the Oracle database tables Responsible for Integration unit testing system testing and stress testing for all the phases of project Prepared final guideline document that would serve as a tutorial for the users of this application Use the DAO in developing application code Developed stored procedures Extensively used Java Collection framework and Exception handling Environment Java J2EE5 Spring JSP XML Spring TLD JSP Servlets Hibernate Criteria API XSLT CSS JSF JSF RichFaces WASD Java Swing Web service AXIS Server WSDL XML Glassfish JSR 286 API UML EJB Java script JQuery Hibernate SQL CVS Agile JUnit Oracle SQL Developer Hadoop Developer RCFile March 2003 to May 2003 Environment Hadoop 0202 MR1 CDH3U6 HDFS Hbase   Flume 093 Sqoop 1x AWS EC2 S3 Elastic Search Hive 071 Java 16 Linux Spring 3x Eclipse Juno XML REST JSON Maven Avro RCFile SVN Education Bachelors Skills JAVA 8 years SQL 7 years APACHE HADOOP HDFS 6 years Hadoop 6 years HADOOP 6 years Hadoop 0202 MR1 CDH3U6 HDFS Hbase   Flume 093 Sqoop 1x AWS EC2 S3 Elastic Search Hive 071 Java 16 Linux Spring 3x Eclipse Juno XML REST JSON Maven Avro RCFile SVN 8 years Drupal Hybris Cobol Servicenow Additional Information TECHNICAL SKILLS HadoopBig Data HDFS Mapreduce HBase Pig Hive Sqoop Oozie YARN ZooKeeper NoSQL DB CDH3 CDH4 Apache Hadoop Java J2EE Technologies Core Java Servlets JSP JDBC JNDI Java Collections IDEs Eclipse Net beans Frameworks MVC Struts Hibernate Spring Programming languages Java Python Ant scripts Linux shell scripts Build Management Tools Maven Apache Ant Version control SVN github Databases Oracle 11g10g9i MySQL DB2 MSSQL Server Web Servers Web Logic Web Sphere Apache Tomcat Web Technologies HTML XML JavaScript AJAX SOAP WSDL Network Protocols TCPIP UDP HTTP DNS DHCP ETL Tools Informatica Pentaho Testing WinRunner QTP Selenium",
    "extracted_keywords": [
        "Hadoop",
        "Developer",
        "Hadoop",
        "span",
        "lDeveloperspan",
        "Hadoop",
        "Developer",
        "Verizon",
        "Dallas",
        "TX",
        "years",
        "experience",
        "IT",
        "industry",
        "testing",
        "maintenance",
        "web",
        "applications",
        "J2EE",
        "technologies",
        "Big",
        "Data",
        "experience",
        "Linux",
        "environment",
        "experience",
        "Hadoop",
        "Big",
        "Data",
        "Analytics",
        "Developer",
        "Expertise",
        "Hadoop",
        "architecture",
        "ecosystems",
        "HDFS",
        "Map",
        "Reduce",
        "programming",
        "paradigm",
        "Experience",
        "installation",
        "configuration",
        "Hadoop",
        "clusters",
        "Apache",
        "Cloudera",
        "distributions",
        "AWS",
        "Knowledge",
        "configuring",
        "Hadoop",
        "ecosystem",
        "components",
        "Hadoop",
        "Map",
        "Reduce",
        "HDFS",
        "HBase",
        "YARN",
        "Oozie",
        "Hive",
        "Sqoop",
        "Pig",
        "Zookeeper",
        "Flume",
        "understanding",
        "HBase",
        "Cassandra",
        "Experience",
        "Spark",
        "Experience",
        "data",
        "Apache",
        "Sqoop",
        "HDFS",
        "Relational",
        "Database",
        "Systems",
        "NonRelational",
        "Database",
        "Systems",
        "viceversa",
        "Hive",
        "Pig",
        "core",
        "functionality",
        "custom",
        "UDFs",
        "data",
        "HiveQL",
        "Pig",
        "Latin",
        "custom",
        "Map",
        "Reduce",
        "programs",
        "Java",
        "Experience",
        "Hadoop",
        "clusters",
        "prod",
        "dev",
        "sizes",
        "configuration",
        "rack",
        "topology",
        "clusters",
        "NoSQL",
        "databases",
        "HBase",
        "Cassandra",
        "MongoDB",
        "SYBASE",
        "Teradata",
        "systems",
        "data",
        "integration",
        "Datamodeling",
        "job",
        "workflow",
        "scheduling",
        "monitoring",
        "tools",
        "Oozie",
        "Zookeeper",
        "Strong",
        "experience",
        "XML",
        "JSON",
        "SQL",
        "DB2",
        "Oracle",
        "Experience",
        "Linux",
        "operating",
        "system",
        "connectivity",
        "products",
        "exchange",
        "data",
        "core",
        "database",
        "engine",
        "Hadoop",
        "ecosystem",
        "Data",
        "warehousing",
        "ETL",
        "tools",
        "Informatica",
        "Pentaho",
        "Expert",
        "level",
        "skills",
        "intranetinternet",
        "application",
        "JAVAJ2EE",
        "technologies",
        "Struts",
        "framework",
        "MVC",
        "design",
        "Patterns",
        "Chrodiant",
        "Servlets",
        "JSP",
        "JSLT",
        "XMLXLST",
        "Java",
        "Script",
        "AJAX",
        "EJB",
        "JDBC",
        "JMS",
        "JNDI",
        "RDMS",
        "SOAP",
        "Hibernate",
        "custom",
        "tag",
        "Experience",
        "XML",
        "XSD",
        "XSLT",
        "Experience",
        "UI",
        "development",
        "jQuery",
        "UI",
        "jQuery",
        "ExtJS",
        "CSS",
        "HTML",
        "HTML5",
        "XHTML",
        "JavaScript",
        "experience",
        "development",
        "J2EE",
        "technologies",
        "JNDI",
        "JSP",
        "Servlets",
        "JSF",
        "Struts",
        "Spring",
        "Hibernate",
        "JDBC",
        "EJB",
        "Possess",
        "skills",
        "schedules",
        "communication",
        "skills",
        "US",
        "employer",
        "Work",
        "Experience",
        "Hadoop",
        "Developer",
        "Verizon",
        "Dallas",
        "TX",
        "January",
        "Present",
        "Verizons",
        "employee",
        "portal",
        "ability",
        "employee",
        "habits",
        "work",
        "employee",
        "productivity",
        "security",
        "company",
        "information",
        "Verizons",
        "employee",
        "tracks",
        "employee",
        "email",
        "internet",
        "browsing",
        "data",
        "urls",
        "data",
        "size",
        "HbaseHDFS",
        "Hadoop",
        "ecosystem",
        "Big",
        "Data",
        "data",
        "Flume",
        "servers",
        "network",
        "proxy",
        "servers",
        "data",
        "store",
        "HDFS",
        "Responsibilities",
        "lifecycle",
        "project",
        "Design",
        "Analysis",
        "architecture",
        "development",
        "Implementation",
        "testing",
        "data",
        "sources",
        "HDFS",
        "maintenance",
        "loading",
        "data",
        "Collected",
        "data",
        "Map",
        "Reduce",
        "Jobs",
        "employee",
        "portal",
        "Spring",
        "MVC",
        "Processed",
        "Big",
        "Data",
        "Hadoop",
        "cluster",
        "nodes",
        "Flume",
        "servers",
        "data",
        "network",
        "proxy",
        "servers",
        "store",
        "HDFS",
        "data",
        "pipeline",
        "Flume",
        "Java",
        "map",
        "employee",
        "data",
        "HbaseHDFS",
        "analysis",
        "reliability",
        "failover",
        "flume",
        "Map",
        "Reduce",
        "jobs",
        "data",
        "server",
        "Hadoop",
        "Administration",
        "team",
        "MR",
        "Jobs",
        "optimizations",
        "RESTFul",
        "APIs",
        "data",
        "Hadoop",
        "Platform",
        "Employee",
        "Portal",
        "Web",
        "Application",
        "text",
        "search",
        "function",
        "MongoDB",
        "Elastic",
        "Search",
        "AWS",
        "framework",
        "content",
        "storage",
        "Elastic",
        "Search",
        "document",
        "search",
        "Hive",
        "tables",
        "Hive",
        "QL",
        "access",
        "tables",
        "locking",
        "hive",
        "help",
        "implementation",
        "cluster",
        "shell",
        "scripts",
        "health",
        "check",
        "Hadoop",
        "daemon",
        "services",
        "warning",
        "failure",
        "conditions",
        "Wrote",
        "MRUnit",
        "unit",
        "Map",
        "Reduce",
        "jobs",
        "requirement",
        "review",
        "Risk",
        "Compliance",
        "Team",
        "BA",
        "Knowledge",
        "transfer",
        "sessions",
        "environment",
        "Hadoop",
        "Developer",
        "Kroger",
        "Co",
        "Cincinnati",
        "OH",
        "April",
        "December",
        "RetailLogistics",
        "analytics",
        "program",
        "information",
        "shoppers",
        "traffic",
        "analysis",
        "shoppers",
        "behavior",
        "purchase",
        "pattern",
        "insights",
        "traffic",
        "optimize",
        "merchandising",
        "display",
        "performance",
        "Responsibilities",
        "Workings",
        "bigdata",
        "infrastructure",
        "batch",
        "processing",
        "processing",
        "Hadoop",
        "Hive",
        "Oozie",
        "MySQL",
        "customization",
        "batch",
        "data",
        "platform",
        "setup",
        "implementation",
        "log",
        "producer",
        "SCALA",
        "application",
        "logs",
        "logs",
        "Kafka",
        "Zookeeper",
        "log",
        "collection",
        "platform",
        "data",
        "export",
        "application",
        "data",
        "platforms",
        "application",
        "databases",
        "manner",
        "Design",
        "Manage",
        "cluster",
        "nodes",
        "data",
        "platform",
        "operations",
        "Install",
        "configure",
        "cluster",
        "puppet",
        "configuration",
        "management",
        "Monitoring",
        "Cluster",
        "tools",
        "nodes",
        "Developed",
        "Spark",
        "scripts",
        "Scala",
        "Shell",
        "requirement",
        "Expertise",
        "cluster",
        "task",
        "Nodes",
        "Removing",
        "Nodes",
        "effect",
        "jobs",
        "data",
        "scripts",
        "application",
        "deployments",
        "configurations",
        "YARN",
        "applications",
        "Troubleshoot",
        "cluster",
        "system",
        "problems",
        "Wrote",
        "map",
        "programs",
        "data",
        "sources",
        "output",
        "formats",
        "Sequence",
        "file",
        "format",
        "Map",
        "programs",
        "output",
        "formats",
        "program",
        "use",
        "cases",
        "Developed",
        "Hadoop",
        "MapReduce",
        "Python",
        "Performed",
        "benchmarking",
        "NoSQL",
        "Cassandra",
        "HBase",
        "data",
        "model",
        "data",
        "bucketing",
        "tables",
        "Cassandra",
        "test",
        "scripts",
        "test",
        "development",
        "integration",
        "text",
        "files",
        "Avro",
        "format",
        "file",
        "Hadoop",
        "eco",
        "system",
        "tools",
        "loading",
        "transforming",
        "sets",
        "data",
        "core",
        "API",
        "services",
        "Scala",
        "Spark",
        "data",
        "HBase",
        "Sqoop",
        "reports",
        "BI",
        "team",
        "amounts",
        "data",
        "sets",
        "way",
        "requirement",
        "gathering",
        "analysis",
        "phase",
        "project",
        "business",
        "requirements",
        "workshopsmeetings",
        "business",
        "users",
        "POC",
        "work",
        "Spark",
        "Kafka",
        "time",
        "data",
        "pipeline",
        "Kafka",
        "Storm",
        "data",
        "HDFS",
        "Populated",
        "HDFS",
        "Cassandra",
        "amounts",
        "data",
        "Apache",
        "Kafka",
        "POC",
        "work",
        "Cassandra",
        "HBase",
        "NoSQL",
        "databases",
        "Cassandra",
        "Mongo",
        "DB",
        "POC",
        "purpose",
        "Implement",
        "POC",
        "Hadoop",
        "Extract",
        "data",
        "Spark",
        "HDFS",
        "Environment",
        "MapReduce",
        "HDFS",
        "Hive",
        "Pig",
        "Hue",
        "Oozie",
        "Core",
        "Java",
        "Eclipse",
        "Hbase",
        "Flume",
        "Spark",
        "Scala",
        "Kafka",
        "Cloudera",
        "Manager",
        "Cassandra",
        "Python",
        "Greenplum",
        "DB",
        "IDMS",
        "VSAM",
        "SQLPLUS",
        "Toad",
        "Putty",
        "Windows",
        "NT",
        "UNIX",
        "Shell",
        "Scripting",
        "Pentaho",
        "Talend",
        "Bigdata",
        "YARN",
        "Mongo",
        "DB",
        "Sr",
        "Java",
        "Developer",
        "Huntington",
        "Bank",
        "Columbus",
        "OH",
        "January",
        "March",
        "Risk",
        "Rating",
        "risks",
        "loans",
        "Huntington",
        "Bank",
        "customers",
        "rating",
        "system",
        "loans",
        "borrower",
        "customer",
        "type",
        "information",
        "series",
        "ratings",
        "calculations",
        "system",
        "rating",
        "risk",
        "loan",
        "Frontend",
        "Spring",
        "MVC",
        "framework",
        "presentation",
        "layer",
        "end",
        "functionality",
        "Oracle",
        "data",
        "persistence",
        "layer",
        "Hibernate",
        "data",
        "access",
        "layer",
        "Responsibilities",
        "requirements",
        "POC",
        "Hadoop",
        "ecosystem",
        "risks",
        "discussions",
        "hours",
        "light",
        "risk",
        "risk",
        "mitigation",
        "option",
        "managers",
        "performance",
        "test",
        "preparations",
        "execution",
        "performance",
        "issues",
        "root",
        "cause",
        "discussions",
        "tech",
        "leads",
        "architects",
        "fix",
        "place",
        "application",
        "service",
        "components",
        "Hibernate",
        "mapping",
        "files",
        "database",
        "schema",
        "UI",
        "module",
        "JSPs",
        "Spring",
        "MVC",
        "framework",
        "design",
        "patterns",
        "MVC",
        "Factory",
        "Composite",
        "Singleton",
        "persistence",
        "logic",
        "ORM",
        "technology",
        "Hibernate",
        "OR",
        "hibernate",
        "mapping",
        "file",
        "Criteria",
        "data",
        "database",
        "Data",
        "Access",
        "Objects",
        "Addition",
        "modification",
        "deletion",
        "records",
        "specification",
        "files",
        "CRUD",
        "manager",
        "concept",
        "Hibernate",
        "operations",
        "risk",
        "rating",
        "engine",
        "logging",
        "framework",
        "messages",
        "Performed",
        "Unit",
        "Testing",
        "Various",
        "Modules",
        "Test",
        "Cases",
        "Environment",
        "Java",
        "Eclipse",
        "Indigo",
        "JBoss",
        "JSP",
        "JQuery",
        "Maven",
        "JUnit",
        "Log4J",
        "Visio",
        "Oracle",
        "SQL",
        "Developer",
        "HDFS",
        "SVN",
        "Unix",
        "Hibernate",
        "Java",
        "Developer",
        "InfoTech",
        "Software",
        "solutions",
        "May",
        "December",
        "FNS",
        "cost",
        "internet",
        "banking",
        "delivery",
        "channel",
        "solution",
        "intuitions",
        "customer",
        "service",
        "levels",
        "banking",
        "services",
        "customers",
        "customers",
        "flexibility",
        "control",
        "banking",
        "activities",
        "internet",
        "functionality",
        "availability",
        "core",
        "banking",
        "functionality",
        "Financial",
        "Connect",
        "business",
        "banking",
        "payroll",
        "scheduling",
        "funds",
        "paymentspay",
        "funds",
        "transfers",
        "enquiries",
        "loan",
        "simulations",
        "loan",
        "applications",
        "Responsibilities",
        "Controllers",
        "request",
        "handling",
        "Spring",
        "framework",
        "Command",
        "handler",
        "mappings",
        "View",
        "Resolvers",
        "application",
        "components",
        "proof",
        "concepts",
        "Java",
        "EJB",
        "JSP",
        "JSF",
        "Struts",
        "AJAX",
        "Enterprise",
        "Integration",
        "experience",
        "web",
        "services",
        "JMS",
        "MQ",
        "EJB",
        "Hibernate",
        "Web",
        "sphere",
        "JBoss",
        "transaction",
        "management",
        "Developed",
        "XML",
        "files",
        "mapping",
        "requests",
        "controllers",
        "Spring",
        "Portlets",
        "pages",
        "application",
        "JSR",
        "API",
        "Hibernate",
        "templates",
        "database",
        "JDBC",
        "servlets",
        "Oracle",
        "database",
        "Integration",
        "unit",
        "testing",
        "system",
        "testing",
        "stress",
        "testing",
        "phases",
        "project",
        "Prepared",
        "guideline",
        "document",
        "tutorial",
        "users",
        "application",
        "DAO",
        "application",
        "code",
        "procedures",
        "Java",
        "Collection",
        "framework",
        "Exception",
        "Environment",
        "Java",
        "J2EE5",
        "Spring",
        "JSP",
        "XML",
        "Spring",
        "TLD",
        "JSP",
        "Servlets",
        "Hibernate",
        "Criteria",
        "API",
        "XSLT",
        "CSS",
        "JSF",
        "JSF",
        "RichFaces",
        "Java",
        "Swing",
        "Web",
        "service",
        "AXIS",
        "Server",
        "WSDL",
        "XML",
        "Glassfish",
        "JSR",
        "API",
        "UML",
        "EJB",
        "Java",
        "script",
        "JQuery",
        "Hibernate",
        "SQL",
        "CVS",
        "Agile",
        "JUnit",
        "Oracle",
        "SQL",
        "Developer",
        "Hadoop",
        "Developer",
        "RCFile",
        "March",
        "May",
        "Environment",
        "Hadoop",
        "0202",
        "MR1",
        "CDH3U6",
        "HDFS",
        "Hbase",
        "Flume",
        "Sqoop",
        "AWS",
        "EC2",
        "S3",
        "Elastic",
        "Search",
        "Hive",
        "Java",
        "Linux",
        "Spring",
        "Eclipse",
        "Juno",
        "XML",
        "REST",
        "JSON",
        "Maven",
        "Avro",
        "RCFile",
        "SVN",
        "Education",
        "Bachelors",
        "Skills",
        "JAVA",
        "years",
        "SQL",
        "years",
        "APACHE",
        "HADOOP",
        "HDFS",
        "years",
        "Hadoop",
        "years",
        "HADOOP",
        "years",
        "Hadoop",
        "0202",
        "MR1",
        "CDH3U6",
        "HDFS",
        "Hbase",
        "Flume",
        "Sqoop",
        "AWS",
        "EC2",
        "S3",
        "Elastic",
        "Search",
        "Hive",
        "Java",
        "Linux",
        "Spring",
        "Eclipse",
        "Juno",
        "XML",
        "REST",
        "JSON",
        "Maven",
        "Avro",
        "RCFile",
        "SVN",
        "years",
        "Drupal",
        "Hybris",
        "Cobol",
        "Servicenow",
        "Additional",
        "Information",
        "TECHNICAL",
        "SKILLS",
        "HadoopBig",
        "Data",
        "HDFS",
        "Mapreduce",
        "HBase",
        "Pig",
        "Hive",
        "Sqoop",
        "Oozie",
        "YARN",
        "ZooKeeper",
        "NoSQL",
        "DB",
        "CDH3",
        "CDH4",
        "Apache",
        "Hadoop",
        "Java",
        "J2EE",
        "Technologies",
        "Core",
        "Java",
        "Servlets",
        "JSP",
        "JDBC",
        "JNDI",
        "Java",
        "Collections",
        "IDEs",
        "Eclipse",
        "Net",
        "Frameworks",
        "MVC",
        "Struts",
        "Hibernate",
        "Spring",
        "Programming",
        "languages",
        "Java",
        "Python",
        "Ant",
        "Linux",
        "shell",
        "Build",
        "Management",
        "Tools",
        "Maven",
        "Apache",
        "Ant",
        "Version",
        "control",
        "SVN",
        "github",
        "Oracle",
        "MySQL",
        "DB2",
        "MSSQL",
        "Server",
        "Web",
        "Servers",
        "Web",
        "Logic",
        "Web",
        "Sphere",
        "Apache",
        "Tomcat",
        "Web",
        "Technologies",
        "HTML",
        "XML",
        "JavaScript",
        "AJAX",
        "SOAP",
        "WSDL",
        "Network",
        "Protocols",
        "UDP",
        "HTTP",
        "DNS",
        "ETL",
        "Tools",
        "Informatica",
        "Pentaho",
        "Testing",
        "WinRunner",
        "QTP",
        "Selenium"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T21:02:28.817401",
    "resume_data": "Hadoop Developer Hadoop span lDeveloperspan Hadoop Developer Verizon Dallas TX Having more than 7 years of experience in IT industry involved in Developing Implementing testing and maintenance of various web based applications using J2EE technologies and Big Data ecosystems experience on Linux environment Including comprehensive experience as a Hadoop Big Data Analytics Developer Expertise on Hadoop architecture and ecosystems such as HDFS Map Reduce programming paradigm Experience in installation configuration supporting and monitoring Hadoop clusters using Apache Cloudera distributions and AWS Knowledge in installing configuring and using Hadoop ecosystem components like Hadoop Map Reduce HDFS HBase YARN Oozie Hive Sqoop Pig Zookeeper and Flume Good understanding of NoSQL databases like HBase Cassandra and MongoDB Experience in Spark and scala Experience in importing and exporting data using Apache Sqoop from HDFS to Relational Database Systems NonRelational Database Systems and viceversa Extending Hive and Pig core functionality by writing custom UDFs Experienced in analyzing data using HiveQL Pig Latin and custom Map Reduce programs in Java Experience in building maintaining multiple Hadoop clusters prod dev etc of different sizes and configuration and setting up the rack topology for large clusters Worked on NoSQL databases including HBase Cassandra and MongoDB Experienced in SYBASE Teradata Experienced in Datawarehousing systems Experienced in data integration and Datamodeling Experienced in job workflow scheduling and monitoring tools like Oozie and Zookeeper Strong experience with XML JSON SQL DB2 Oracle Experience on the Linux operating system Experienced in designing developing and implementing connectivity products that allow efficient exchange of data between the core database engine and the Hadoop ecosystem Experienced in Data warehousing and using ETL tools like Informatica and Pentaho Expert level skills in developing intranetinternet application using JAVAJ2EE technologies which includes Struts framework MVC design Patterns Chrodiant Servlets JSP JSLT XMLXLST Java Script AJAX EJB JDBC JMS JNDI RDMS SOAP Hibernate and custom tag Libraries Experience using XML XSD and XSLT Experience with webbased UI development using jQuery UI jQuery ExtJS CSS HTML HTML5 XHTML and JavaScript Extensive experience in middletier development using J2EE technologies like JNDI JSP Servlets JSF Struts Spring Hibernate JDBC EJB Possess excellent technical skills consistently outperformed schedules and acquired interpersonal and communication skills Authorized to work in the US for any employer Work Experience Hadoop Developer Verizon Dallas TX January 2017 to Present Verizons employee portal provides ability to search employee browsing habits at work To increase employee productivity and ensure security of company information Verizons employee portal tracks employee email and internet browsing collects various data like urls transmitting data size into HbaseHDFS Hadoop ecosystem was used collect the Big Data and analyze the data Flume servers were installed on the network proxy servers to collect the data and store into HDFS Responsibilities Involved in full lifecycle of the project from Design Analysis logical and physical architecture modeling development Implementation testing Responsible to manage data coming from different sources and involved in HDFS maintenance and loading of structured and unstructured data Collected data was analyzed using Map Reduce Jobs and presented to employee portal built using Spring MVC Processed Big Data using a Hadoop cluster consisting of 40 nodes Designed and configured Flume servers to collect data from the network proxy servers and store to HDFS Developed data pipeline using Flume and Java map reduce to ingest employee browsing data into HbaseHDFS for analysis Used agentE2EChain for reliability and failover in flume Designed and implemented Map Reduce jobs for analyzing the data collected by the flume server Actively involved in working with Hadoop Administration team to debugging various slow running MR Jobs and doing the necessary optimizations Designed and implemented RESTFul APIs to retrieve the data from Hadoop Platform to Employee Portal Web Application Optimized the full text search function by connecting MongoDB and Elastic Search Utilized AWS framework for content storage and Elastic Search for document search Creating Hive tables and working on them using Hive QL Created concurrent access for hive tables with shared and exclusive locking that can be enabled in hive with the help of Zookeeper implementation in the cluster Wrote the shell scripts to monitor the health check of Hadoop daemon services and respond accordingly to any warning or failure conditions Wrote MRUnit tests for unit testing the Map Reduce jobs Performed functional requirement review Worked closely with Risk Compliance Team and BA Facilitated Knowledge transfer sessions Worked in an agile environment Hadoop Developer Kroger Co Cincinnati OH April 2014 to December 2016 RetailLogistics is instore analytics program which provides comprehensive information on how to resolve the shoppers traffic It provides the detailed analysis to understand shoppers behavior purchase pattern By using these insights it is used to increase instore traffic optimize staffing improve merchandising and display performance Responsibilities Workings on bigdata infrastructure build out for batch processing as well as realtime processing Worked on Hadoop Hive Oozie and MySQL customization for batch data platform setup Worked on implementation of a log producer in SCALA that watches for application logs transforms incremental logs and sends them to a Kafka and Zookeeper based log collection platform Implemented a data export application to fetch processed data from these platforms to consuming application databases in a scalable manner Design deploy Manage cluster nodes for our data platform operations rackingstacking Install and configure cluster Setting up puppet for centralized configuration management Monitoring Cluster using various tools to see how the nodes are performing Developed Spark scripts by using Scala Shell commands as per the requirement Expertise in cluster task like adding Nodes Removing Nodes without any effect to running jobs and data Write scripts to automate application deployments and configurations Monitoring YARN applications Troubleshoot and resolve cluster related system problems Wrote map reduce programs to clean and preprocess the data coming from different sources Implemented various output formats like Sequence file and parquet format in Map reduce programs Also implemented multiple output formats in the same program to match the use cases Developed Hadoop streaming MapReduce works using Python Performed benchmarking of the NoSQL databases Cassandra and HBase Created data model for structuring and storing the data efficiently Implemented partitioning and bucketing of tables in Cassandra Implemented test scripts to support test driven development and continuous integration Converted text files into Avro then to parquet format for the file to be used with other Hadoop eco system tools Experienced on loading and transforming of large sets of structured semi structured and unstructured data Developed and implemented core API services using Scala and Spark Exported the analyzed data to HBase using Sqoop and to generate reports for the BI team Analyzed large amounts of data sets to determine optimal way to aggregate and report on it Participate in requirement gathering and analysis phase of the project in documenting the business requirements by conducting workshopsmeetings with various business users POC work is going on using Spark and Kafka for real time processing Developed a data pipeline using Kafka and Storm to store data into HDFS Populated HDFS and Cassandra with huge amounts of data using Apache Kafka POC work is going on comparing the Cassandra and HBase NoSQL databases Worked with NoSQL databases like Cassandra and Mongo DB for POC purpose Implement POC with Hadoop Extract data with Spark into HDFS Environment MapReduce HDFS Hive Pig Hue Oozie Core Java Eclipse Hbase Flume Spark Scala Kafka Cloudera Manager Cassandra Python Greenplum DB IDMS VSAM SQLPLUS Toad Putty Windows NT UNIX Shell Scripting Pentaho Talend Bigdata YARN Mongo DB Sr Java Developer Huntington Bank Columbus OH January 2012 to March 2014 Risk Rating is developed for calculating the risks on the loans given by the Huntington Bank to their customers This is the rating system for the loans which takes the borrower and customer type information and performs a series of ratings and calculations This system gives the rating of the risk associated with that loan Frontend is built using a Spring MVC framework in the presentation layer The back end functionality was built using Oracle as data persistence layer and Hibernate as data access layer Responsibilities Reviewed functional and nonfunctional requirements Implemented a highly visible POC using Hadoop ecosystem Responsible for Identifying risks from the discussions happens during onsite hours and high light the risk by articulating risk and mitigation option to project managers Responsible for leading performance test preparations and execution And leading the performance issues from identifying the root cause have discussions with offshore tech leads and architects to put a fix in place Developed application service components created Hibernate mapping files and generated database schema Designed and implemented UI module using JSPs and Spring MVC framework Implemented design patterns like MVC Factory Composite and Singleton Implemented persistence logic in ORM technology using Hibernate OR mappings Developed hibernate mapping file and Criteria queries for retrieving data from the database Created various Data Access Objects for Addition modification and deletion of records using various specification files and also using CRUD manager concept in Hibernate for the above operations Designed and implemented risk rating engine Used Log4J logging framework for logging messages Performed Unit Testing of Various Modules by generating the Test Cases Environment Java 16 Eclipse Indigo JBoss 50 JSP JQuery Maven JUnit Log4J Visio Oracle SQL Developer HDFS 020 SVN Unix Hibernate 331 Java Developer InfoTech Software solutions May 2009 to December 2011 The FNS is low cost internet and mobile banking based delivery channel solution which allows intuitions to greatly enhanced customer service levels and to streamline banking services offered to customers and allows customers the flexibility to take control of banking activities via the internet via leveraging the rich functionality and uninterrupted 247 availability of the core banking The rich functionality of Financial Connect include business banking payroll scheduling and international funds transfer together with standing paymentspay domestic funds transfers account enquiries online loan simulations and loan applications Responsibilities Developed Controllers for request handling using Spring framework Involved in Command controllers handler mappings and View Resolvers Designed and developed application components and architectural proof of concepts using Java EJB JSP JSF Struts and AJAX Participated in Enterprise Integration experience web services Configured JMS MQ EJB and Hibernate on Web sphere and JBoss Focused on Declarative transaction management Developed XML files for mapping requests to controllers Coded Spring Portlets to build portal pages for application using JSR 286 API Hibernate templates were used to access database Coded JDBC calls in the servlets to access the Oracle database tables Responsible for Integration unit testing system testing and stress testing for all the phases of project Prepared final guideline document that would serve as a tutorial for the users of this application Use the DAO in developing application code Developed stored procedures Extensively used Java Collection framework and Exception handling Environment Java J2EE5 Spring JSP XML Spring TLD JSP Servlets Hibernate Criteria API XSLT CSS JSF JSF RichFaces WASD Java Swing Web service AXIS Server WSDL XML Glassfish JSR 286 API UML EJB Java script JQuery Hibernate SQL CVS Agile JUnit Oracle SQL Developer Hadoop Developer RCFile March 2003 to May 2003 Environment Hadoop 0202 MR1 CDH3U6 HDFS Hbase 090x Flume 093 Sqoop 1x AWS EC2 S3 Elastic Search Hive 071 Java 16 Linux Spring 3x Eclipse Juno XML REST JSON Maven Avro RCFile SVN Education Bachelors Skills JAVA 8 years SQL 7 years APACHE HADOOP HDFS 6 years Hadoop 6 years HADOOP 6 years Hadoop 0202 MR1 CDH3U6 HDFS Hbase 090x Flume 093 Sqoop 1x AWS EC2 S3 Elastic Search Hive 071 Java 16 Linux Spring 3x Eclipse Juno XML REST JSON Maven Avro RCFile SVN 8 years Drupal Hybris Cobol Servicenow Additional Information TECHNICAL SKILLS HadoopBig Data HDFS Mapreduce HBase Pig Hive Sqoop Oozie YARN ZooKeeper NoSQL DB CDH3 CDH4 Apache Hadoop Java J2EE Technologies Core Java Servlets JSP JDBC JNDI Java Collections IDEs Eclipse Net beans Frameworks MVC Struts Hibernate Spring Programming languages Java Python Ant scripts Linux shell scripts Build Management Tools Maven Apache Ant Version control SVN github Databases Oracle 11g10g9i MySQL DB2 MSSQL Server Web Servers Web Logic Web Sphere Apache Tomcat Web Technologies HTML XML JavaScript AJAX SOAP WSDL Network Protocols TCPIP UDP HTTP DNS DHCP ETL Tools Informatica Pentaho Testing WinRunner QTP Selenium",
    "unique_id": "36454634-695e-4595-9039-f45a88360a25"
}