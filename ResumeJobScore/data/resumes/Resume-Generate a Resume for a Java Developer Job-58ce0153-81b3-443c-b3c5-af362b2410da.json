{
    "clean_data": "Sr Hadoop BigData Engineer Sr Hadoop BigData Engineer Sr Hadoop BigData Engineer Liberty Mutual Insurance Dover NH Work Experience Sr Hadoop BigData Engineer Liberty Mutual Insurance Dover NH October 2017 to Present Roles Responsibilities Worked with business partners in discussing the requirements for new projects and enhancements to the existing applications Worked with application teams to install operating system Hadoop updates patches version upgrades as required Developed MapReduce programs to parse the raw data populate tables and store the refined data in partitioned tables Used Spark API over Cloudera Hadoop YARN to perform analytics on data in Hive Involved in creating Hive Tables loading with data and writing Hive queries which will invoke and run MapReduce jobs in the backend Developed Pig Latin Scripts to process data in a batch to perform trend analysis Wrote ETL jobs to read from web APIs using REST and HTTP calls and loaded into Cassandra using Java and Talend Analyzed Cassandra database and compared it with other opensource NoSQL databases to find which one of them better suites the current requirement Responsible for Cluster maintenance Monitoring commissioningdecommissioning Data Nodes troubleshooting manage and reviewing Data Backups and Log Files Ingested data from RDBMS and performed data transformations and then export the transformed data to Cassandra as per the business requirement Developed automated processes for flattening the upstream data from Cassandra which in JSON format Used Hive UDFs to flatten the JSON Data Configured Spark Streaming to receive real time data from the Kafka and store the stream data to the database Responsible for developing data pipeline by implementing Kafka producers and consumers Connected to AWS EC2 using SSH and ran Sparksubmit jobs Responsible for developing data pipeline with AWS to extract the data from weblogs and store in the database Analyzed user request patterns and implemented various performance optimization measures including implementing partitions and buckets in HiveQL Studied data by performing HiveQL running Pig Latin scripts to study customer behavior Environment Hadoop Hive MapReduce Pig Latin REST Java Cassandra JSON Spark AWS EC2 HiveQL Oozie Hadoop Developer NorthShore Medical Group Lincolnwood IL November 2015 to August 2017 Roles Responsibilities Worked on a product team using AgileSCRUM methodology to develop deploy and support solutions that leverage the Client big data platform Documented the systems processes and procedures for future references Supported technical team in management and review of Hadoop log files and data backups Involved in writing MapReduce program and Hive queries to load and process data in Hadoop File System Involved in creating Hive tables loading with data and writing hive queries which will run internally in MapReduce way Developing and running MapReduce jobs on YARN and Hadoop clusters to produce daily and monthly reports as per users need Maintain and schedule periodic jobs which range from updates on MapReduce jobs to creating adhoc jobs for the business users Scheduling and managing jobs on a Hadoop cluster using Oozie work flow Handled importing data from different data sources into HDFS using Sqoop and performing transformations using Hive MapReduce and then loading data into HDFS Exporting of result set from HIVE to MySQL using Sqoop export tool for further processing Collecting and aggregating large amounts of log data using Flume and staging data in HDFS for further analysis Developed several REST web services supporting both XML and JSON to perform task such as demand response management Created Maven builds to build and deploy Spring Boot microservices to internal enterprise Docker registry Designed target tables as per the requirement from the reporting team and designed Extraction Transformation and Loading using Talend Implemented File Transfer Protocol operations using Talend Studio to transfer files in between network folders Optimized MapReduce Jobs to use HDFS efficiently by using various compression mechanisms Responsible for continuous monitoring and managing Elastic MapReduce EMR cluster through AWS console Executed speedy reviews and first mover advantages by using workflows like Oozie in order to automate the data Involved in loading data from UNIX file system to HDFS Environment Hadoop MapReduce HDFS Hive Oracle Java AWS Servlets HTML XML SQL J2EE JUnit Tomcat UNIX Maven REST Hadoop Developer Barnes Noble Brentwood TN March 2014 to September 2015 Roles Responsibilities Involved in developing Hadoop Map Reduce jobs using Java Runtime Environment for the batch processing to search and match the scores Executed speedy reviews and first mover advantages by using workflows like Oozie in order to automate the data Loading process into the Hadoop distributed File System HDFS and Pig language in order to preprocess the data Integrated Oozie with the rest of the Hadoop stack supporting several types of Hadoop jobs out of the box such as MapReduce Pig Hive Sqoop Flume Wrote Hive jobs to parse the logs and structure them in tabular format to facilitate effective querying on the log data Worked with Sqoop to import export data from HDFS to Relational Database system Performed performance tuning and troubleshooting of Map Reduce jobs by analyzing and reviewing Hadoop log files Developed Pig scripts for analyzing large data sets in the HDFS Collecting and aggregating large amounts of log data using Apache Flume and staging data in HDFS for further analysis Designed and built the Reporting Application which uses the Spark SQL to fetch and generate reports on HBase table data Extracted data from the server into HDFS and Bulk Loaded the cleaned data into HBase Extracted data from the flat files and other RDBMS databases into staging area and populated onto Data warehouse Worked on JVM performance tuning to improve MapReduce jobs performance Developed programs to manipulate data perform CRUD operations on request to database Developed several REST web services supporting both XML and JSON to perform task such as demand response management Used message driven beans for asynchronous processing alerts to the customer Worked on developing Use Cases Class Diagrams Sequence diagrams and Data Models Performed performance tuning and troubleshooting of Map Reduce jobs by analyzing and reviewing Hadoop log files Environment Hadoop Hive HBase Linux MapReduce HDFS Hive Java JDK Cloudera MapReduce DataStax IBM DataStage UNIX Shell Scripting Java Developer Charter Communications St Louis MO February 2012 to December 2013 Roles Responsibilities Interacted with business analyst to understand the requirements to ensure correct modules been built to meet business requirements Participated in the daily SCRUM meetings to produce quality enhancements within time Developed UML using Case diagrams Class diagrams and Sequence diagrams using Rational Software Architect Spring MVC model integration for frontend request action controller Developed web screens in JSP JSTL CSS and clientside validation using jQuery Developed Web services to allow communication between application through SOAP over HTTP using Apache CXF Configured JMS on Web Sphere Server for asynchronous messaging through implementation of Message Driven Beans MDB Used Spring ORM module for integration with Hibernate for persistence layer Implemented the application using the concrete principles laid down by several design patterns such as Session Faade Business Delegate Singleton Data Access Object and Service Locator Developed the application in J2EE Application Server environment with IBM WebSphere as deployment server with RAD as development IDE Used JIRA for defect tracking and project management Developed and designed XML Schemas to transport and store data XML was used to simplify data and allow for Platform Changes as well as making data more available across the applications distributed platforms Extensively used XSLT to transform XML documents to HTML Wrote custom jQuery plugins and developed JavaScript functions to build a bleedingedge AJAXdriven user interface Developed unit and functional test cases using JUnit Maven and Jenkins used for the automatic build process Used Log4J utility to log error info and debug messages Used Rational Clear Case for version controlling Worked efficiently in a very tight schedule to meet the deadlines Environment Java J2EE Spring MVC Hibernate HTML CSS AJAX jQuery JavaScript JIRA XML JUnit Maven Jenkins Log4J Education Bachelors Degree in Business Administration in Business Administration Tribhuvan University 2018 MBA in Information Technology Johnson Wales University Providence RI 2012 Skills Cassandra Ambari Hdfs Impala Oozie Sqoop Hbase Kafka Db2 Flume Map reduce Apache spark Hbase Hive Javascript Json Pig Python Xml Zookeeper",
    "entities": [
        "Cloudera Hadoop",
        "Developed",
        "CRUD",
        "Sqoop",
        "HIVE",
        "Collecting",
        "jQuery Developed Web",
        "XSLT",
        "HDFS",
        "UNIX",
        "Platform Changes",
        "RAD",
        "Impala",
        "AWS",
        "MapReduce Pig Hive Sqoop Flume Wrote Hive",
        "Client",
        "HTTP",
        "Sequence",
        "JSON",
        "Used Rational Clear Case",
        "Rational Software Architect",
        "log data",
        "IBM",
        "HDFS Environment Hadoop MapReduce HDFS Hive Oracle Java AWS Servlets HTML XML SQL J2EE JUnit Tomcat",
        "Information Technology Johnson Wales University",
        "Hadoop File System Involved",
        "Session Faade Business Delegate Singleton",
        "Extraction Transformation",
        "Oozie",
        "MVC",
        "HDFS Exporting of result",
        "Created Maven",
        "Java Runtime Environment",
        "JSP",
        "jQuery",
        "Scheduling",
        "Elastic MapReduce EMR",
        "Hive Involved",
        "Hadoop",
        "REST",
        "XML",
        "Oozie Hadoop Developer NorthShore Medical Group",
        "Relational Database",
        "Developed UML",
        "SOAP",
        "MapReduce",
        "Business Administration in Business Administration Tribhuvan University",
        "YARN",
        "the Reporting Application",
        "RDBMS",
        "HTML Wrote",
        "JUnit",
        "Apache CXF Configured JMS",
        "J2EE Application Server",
        "SSH",
        "Sr Hadoop BigData Engineer Sr Hadoop BigData Engineer Sr Hadoop BigData Engineer Liberty Mutual Insurance Dover NH Work Experience Sr Hadoop BigData Engineer Liberty Mutual Insurance Dover NH",
        "HBase",
        "Executed",
        "Talend Studio",
        "Hive",
        "Data Models Performed",
        "Use Cases Class Diagrams Sequence",
        "Wrote"
    ],
    "experience": "Experience Sr Hadoop BigData Engineer Liberty Mutual Insurance Dover NH October 2017 to Present Roles Responsibilities Worked with business partners in discussing the requirements for new projects and enhancements to the existing applications Worked with application teams to install operating system Hadoop updates patches version upgrades as required Developed MapReduce programs to parse the raw data populate tables and store the refined data in partitioned tables Used Spark API over Cloudera Hadoop YARN to perform analytics on data in Hive Involved in creating Hive Tables loading with data and writing Hive queries which will invoke and run MapReduce jobs in the backend Developed Pig Latin Scripts to process data in a batch to perform trend analysis Wrote ETL jobs to read from web APIs using REST and HTTP calls and loaded into Cassandra using Java and Talend Analyzed Cassandra database and compared it with other opensource NoSQL databases to find which one of them better suites the current requirement Responsible for Cluster maintenance Monitoring commissioningdecommissioning Data Nodes troubleshooting manage and reviewing Data Backups and Log Files Ingested data from RDBMS and performed data transformations and then export the transformed data to Cassandra as per the business requirement Developed automated processes for flattening the upstream data from Cassandra which in JSON format Used Hive UDFs to flatten the JSON Data Configured Spark Streaming to receive real time data from the Kafka and store the stream data to the database Responsible for developing data pipeline by implementing Kafka producers and consumers Connected to AWS EC2 using SSH and ran Sparksubmit jobs Responsible for developing data pipeline with AWS to extract the data from weblogs and store in the database Analyzed user request patterns and implemented various performance optimization measures including implementing partitions and buckets in HiveQL Studied data by performing HiveQL running Pig Latin scripts to study customer behavior Environment Hadoop Hive MapReduce Pig Latin REST Java Cassandra JSON Spark AWS EC2 HiveQL Oozie Hadoop Developer NorthShore Medical Group Lincolnwood IL November 2015 to August 2017 Roles Responsibilities Worked on a product team using AgileSCRUM methodology to develop deploy and support solutions that leverage the Client big data platform Documented the systems processes and procedures for future references Supported technical team in management and review of Hadoop log files and data backups Involved in writing MapReduce program and Hive queries to load and process data in Hadoop File System Involved in creating Hive tables loading with data and writing hive queries which will run internally in MapReduce way Developing and running MapReduce jobs on YARN and Hadoop clusters to produce daily and monthly reports as per users need Maintain and schedule periodic jobs which range from updates on MapReduce jobs to creating adhoc jobs for the business users Scheduling and managing jobs on a Hadoop cluster using Oozie work flow Handled importing data from different data sources into HDFS using Sqoop and performing transformations using Hive MapReduce and then loading data into HDFS Exporting of result set from HIVE to MySQL using Sqoop export tool for further processing Collecting and aggregating large amounts of log data using Flume and staging data in HDFS for further analysis Developed several REST web services supporting both XML and JSON to perform task such as demand response management Created Maven builds to build and deploy Spring Boot microservices to internal enterprise Docker registry Designed target tables as per the requirement from the reporting team and designed Extraction Transformation and Loading using Talend Implemented File Transfer Protocol operations using Talend Studio to transfer files in between network folders Optimized MapReduce Jobs to use HDFS efficiently by using various compression mechanisms Responsible for continuous monitoring and managing Elastic MapReduce EMR cluster through AWS console Executed speedy reviews and first mover advantages by using workflows like Oozie in order to automate the data Involved in loading data from UNIX file system to HDFS Environment Hadoop MapReduce HDFS Hive Oracle Java AWS Servlets HTML XML SQL J2EE JUnit Tomcat UNIX Maven REST Hadoop Developer Barnes Noble Brentwood TN March 2014 to September 2015 Roles Responsibilities Involved in developing Hadoop Map Reduce jobs using Java Runtime Environment for the batch processing to search and match the scores Executed speedy reviews and first mover advantages by using workflows like Oozie in order to automate the data Loading process into the Hadoop distributed File System HDFS and Pig language in order to preprocess the data Integrated Oozie with the rest of the Hadoop stack supporting several types of Hadoop jobs out of the box such as MapReduce Pig Hive Sqoop Flume Wrote Hive jobs to parse the logs and structure them in tabular format to facilitate effective querying on the log data Worked with Sqoop to import export data from HDFS to Relational Database system Performed performance tuning and troubleshooting of Map Reduce jobs by analyzing and reviewing Hadoop log files Developed Pig scripts for analyzing large data sets in the HDFS Collecting and aggregating large amounts of log data using Apache Flume and staging data in HDFS for further analysis Designed and built the Reporting Application which uses the Spark SQL to fetch and generate reports on HBase table data Extracted data from the server into HDFS and Bulk Loaded the cleaned data into HBase Extracted data from the flat files and other RDBMS databases into staging area and populated onto Data warehouse Worked on JVM performance tuning to improve MapReduce jobs performance Developed programs to manipulate data perform CRUD operations on request to database Developed several REST web services supporting both XML and JSON to perform task such as demand response management Used message driven beans for asynchronous processing alerts to the customer Worked on developing Use Cases Class Diagrams Sequence diagrams and Data Models Performed performance tuning and troubleshooting of Map Reduce jobs by analyzing and reviewing Hadoop log files Environment Hadoop Hive HBase Linux MapReduce HDFS Hive Java JDK Cloudera MapReduce DataStax IBM DataStage UNIX Shell Scripting Java Developer Charter Communications St Louis MO February 2012 to December 2013 Roles Responsibilities Interacted with business analyst to understand the requirements to ensure correct modules been built to meet business requirements Participated in the daily SCRUM meetings to produce quality enhancements within time Developed UML using Case diagrams Class diagrams and Sequence diagrams using Rational Software Architect Spring MVC model integration for frontend request action controller Developed web screens in JSP JSTL CSS and clientside validation using jQuery Developed Web services to allow communication between application through SOAP over HTTP using Apache CXF Configured JMS on Web Sphere Server for asynchronous messaging through implementation of Message Driven Beans MDB Used Spring ORM module for integration with Hibernate for persistence layer Implemented the application using the concrete principles laid down by several design patterns such as Session Faade Business Delegate Singleton Data Access Object and Service Locator Developed the application in J2EE Application Server environment with IBM WebSphere as deployment server with RAD as development IDE Used JIRA for defect tracking and project management Developed and designed XML Schemas to transport and store data XML was used to simplify data and allow for Platform Changes as well as making data more available across the applications distributed platforms Extensively used XSLT to transform XML documents to HTML Wrote custom jQuery plugins and developed JavaScript functions to build a bleedingedge AJAXdriven user interface Developed unit and functional test cases using JUnit Maven and Jenkins used for the automatic build process Used Log4J utility to log error info and debug messages Used Rational Clear Case for version controlling Worked efficiently in a very tight schedule to meet the deadlines Environment Java J2EE Spring MVC Hibernate HTML CSS AJAX jQuery JavaScript JIRA XML JUnit Maven Jenkins Log4J Education Bachelors Degree in Business Administration in Business Administration Tribhuvan University 2018 MBA in Information Technology Johnson Wales University Providence RI 2012 Skills Cassandra Ambari Hdfs Impala Oozie Sqoop Hbase Kafka Db2 Flume Map reduce Apache spark Hbase Hive Javascript Json Pig Python Xml Zookeeper",
    "extracted_keywords": [
        "Sr",
        "Hadoop",
        "BigData",
        "Engineer",
        "Sr",
        "Hadoop",
        "BigData",
        "Engineer",
        "Sr",
        "Hadoop",
        "BigData",
        "Engineer",
        "Liberty",
        "Mutual",
        "Insurance",
        "Dover",
        "NH",
        "Work",
        "Experience",
        "Sr",
        "Hadoop",
        "BigData",
        "Engineer",
        "Liberty",
        "Mutual",
        "Insurance",
        "Dover",
        "NH",
        "October",
        "Present",
        "Roles",
        "Responsibilities",
        "business",
        "partners",
        "requirements",
        "projects",
        "enhancements",
        "applications",
        "application",
        "teams",
        "operating",
        "system",
        "Hadoop",
        "updates",
        "version",
        "upgrades",
        "Developed",
        "MapReduce",
        "programs",
        "data",
        "tables",
        "data",
        "tables",
        "Spark",
        "API",
        "Cloudera",
        "Hadoop",
        "YARN",
        "analytics",
        "data",
        "Hive",
        "Hive",
        "Tables",
        "data",
        "Hive",
        "queries",
        "MapReduce",
        "jobs",
        "Developed",
        "Pig",
        "Latin",
        "Scripts",
        "data",
        "batch",
        "trend",
        "analysis",
        "Wrote",
        "ETL",
        "jobs",
        "web",
        "APIs",
        "REST",
        "HTTP",
        "Cassandra",
        "Java",
        "Talend",
        "Cassandra",
        "database",
        "opensource",
        "NoSQL",
        "requirement",
        "Cluster",
        "maintenance",
        "Monitoring",
        "Data",
        "Nodes",
        "troubleshooting",
        "manage",
        "Data",
        "Backups",
        "Log",
        "Files",
        "data",
        "RDBMS",
        "data",
        "transformations",
        "data",
        "Cassandra",
        "business",
        "requirement",
        "Developed",
        "processes",
        "data",
        "Cassandra",
        "format",
        "Hive",
        "UDFs",
        "JSON",
        "Data",
        "Configured",
        "Spark",
        "Streaming",
        "time",
        "data",
        "Kafka",
        "stream",
        "data",
        "database",
        "data",
        "pipeline",
        "Kafka",
        "producers",
        "consumers",
        "AWS",
        "EC2",
        "SSH",
        "Sparksubmit",
        "jobs",
        "data",
        "pipeline",
        "AWS",
        "data",
        "weblogs",
        "database",
        "user",
        "request",
        "patterns",
        "performance",
        "optimization",
        "measures",
        "partitions",
        "buckets",
        "HiveQL",
        "data",
        "Pig",
        "Latin",
        "scripts",
        "customer",
        "behavior",
        "Environment",
        "Hadoop",
        "Hive",
        "MapReduce",
        "Pig",
        "Latin",
        "REST",
        "Java",
        "Cassandra",
        "JSON",
        "Spark",
        "EC2",
        "HiveQL",
        "Oozie",
        "Hadoop",
        "Developer",
        "NorthShore",
        "Medical",
        "Group",
        "Lincolnwood",
        "IL",
        "November",
        "August",
        "Roles",
        "Responsibilities",
        "product",
        "team",
        "AgileSCRUM",
        "methodology",
        "deploy",
        "solutions",
        "Client",
        "data",
        "platform",
        "systems",
        "processes",
        "procedures",
        "references",
        "team",
        "management",
        "review",
        "Hadoop",
        "log",
        "files",
        "data",
        "backups",
        "MapReduce",
        "program",
        "Hive",
        "process",
        "data",
        "Hadoop",
        "File",
        "System",
        "Hive",
        "tables",
        "data",
        "hive",
        "queries",
        "MapReduce",
        "way",
        "MapReduce",
        "jobs",
        "YARN",
        "Hadoop",
        "clusters",
        "reports",
        "users",
        "jobs",
        "updates",
        "MapReduce",
        "jobs",
        "jobs",
        "business",
        "users",
        "Scheduling",
        "managing",
        "jobs",
        "Hadoop",
        "cluster",
        "Oozie",
        "work",
        "flow",
        "data",
        "data",
        "sources",
        "HDFS",
        "Sqoop",
        "transformations",
        "Hive",
        "MapReduce",
        "data",
        "HDFS",
        "Exporting",
        "result",
        "HIVE",
        "MySQL",
        "Sqoop",
        "export",
        "tool",
        "Collecting",
        "amounts",
        "log",
        "data",
        "Flume",
        "data",
        "HDFS",
        "analysis",
        "REST",
        "web",
        "services",
        "XML",
        "JSON",
        "task",
        "demand",
        "response",
        "management",
        "Created",
        "Maven",
        "Spring",
        "Boot",
        "microservices",
        "enterprise",
        "Docker",
        "registry",
        "target",
        "tables",
        "requirement",
        "reporting",
        "team",
        "Extraction",
        "Transformation",
        "Loading",
        "Talend",
        "File",
        "Transfer",
        "Protocol",
        "operations",
        "Talend",
        "Studio",
        "files",
        "network",
        "folders",
        "MapReduce",
        "Jobs",
        "HDFS",
        "compression",
        "mechanisms",
        "monitoring",
        "Elastic",
        "MapReduce",
        "EMR",
        "cluster",
        "AWS",
        "console",
        "reviews",
        "advantages",
        "workflows",
        "Oozie",
        "order",
        "data",
        "loading",
        "data",
        "UNIX",
        "file",
        "system",
        "HDFS",
        "Environment",
        "Hadoop",
        "MapReduce",
        "HDFS",
        "Hive",
        "Oracle",
        "Java",
        "AWS",
        "Servlets",
        "HTML",
        "XML",
        "SQL",
        "J2EE",
        "JUnit",
        "Tomcat",
        "UNIX",
        "Maven",
        "REST",
        "Hadoop",
        "Developer",
        "Barnes",
        "Noble",
        "Brentwood",
        "TN",
        "March",
        "September",
        "Roles",
        "Responsibilities",
        "Hadoop",
        "Map",
        "Reduce",
        "jobs",
        "Java",
        "Runtime",
        "Environment",
        "batch",
        "processing",
        "search",
        "scores",
        "reviews",
        "advantages",
        "workflows",
        "Oozie",
        "order",
        "data",
        "Loading",
        "process",
        "Hadoop",
        "File",
        "System",
        "HDFS",
        "Pig",
        "language",
        "order",
        "data",
        "Integrated",
        "Oozie",
        "rest",
        "Hadoop",
        "stack",
        "types",
        "Hadoop",
        "jobs",
        "box",
        "MapReduce",
        "Pig",
        "Hive",
        "Sqoop",
        "Flume",
        "Wrote",
        "Hive",
        "jobs",
        "logs",
        "format",
        "querying",
        "log",
        "data",
        "Sqoop",
        "export",
        "data",
        "HDFS",
        "Relational",
        "Database",
        "system",
        "performance",
        "tuning",
        "troubleshooting",
        "Map",
        "Reduce",
        "jobs",
        "Hadoop",
        "log",
        "Developed",
        "Pig",
        "scripts",
        "data",
        "sets",
        "HDFS",
        "Collecting",
        "amounts",
        "log",
        "data",
        "Apache",
        "Flume",
        "data",
        "HDFS",
        "analysis",
        "Reporting",
        "Application",
        "Spark",
        "SQL",
        "reports",
        "HBase",
        "table",
        "data",
        "data",
        "server",
        "HDFS",
        "Bulk",
        "Loaded",
        "data",
        "HBase",
        "data",
        "files",
        "RDBMS",
        "databases",
        "staging",
        "area",
        "Data",
        "warehouse",
        "JVM",
        "performance",
        "MapReduce",
        "jobs",
        "programs",
        "data",
        "CRUD",
        "operations",
        "request",
        "REST",
        "web",
        "services",
        "XML",
        "JSON",
        "task",
        "demand",
        "response",
        "management",
        "message",
        "beans",
        "processing",
        "alerts",
        "customer",
        "Use",
        "Cases",
        "Class",
        "Diagrams",
        "Sequence",
        "diagrams",
        "Data",
        "Models",
        "performance",
        "tuning",
        "troubleshooting",
        "Map",
        "Reduce",
        "jobs",
        "Hadoop",
        "log",
        "files",
        "Environment",
        "Hadoop",
        "Hive",
        "HBase",
        "Linux",
        "MapReduce",
        "HDFS",
        "Hive",
        "Java",
        "JDK",
        "Cloudera",
        "MapReduce",
        "DataStax",
        "IBM",
        "DataStage",
        "UNIX",
        "Shell",
        "Scripting",
        "Java",
        "Developer",
        "Charter",
        "Communications",
        "St",
        "Louis",
        "MO",
        "February",
        "December",
        "Roles",
        "Responsibilities",
        "business",
        "analyst",
        "requirements",
        "modules",
        "business",
        "requirements",
        "SCRUM",
        "meetings",
        "quality",
        "enhancements",
        "time",
        "UML",
        "Case",
        "diagrams",
        "Class",
        "diagrams",
        "Sequence",
        "diagrams",
        "Rational",
        "Software",
        "Architect",
        "Spring",
        "MVC",
        "model",
        "integration",
        "frontend",
        "request",
        "action",
        "controller",
        "Developed",
        "web",
        "screens",
        "JSP",
        "JSTL",
        "CSS",
        "validation",
        "jQuery",
        "Developed",
        "Web",
        "services",
        "communication",
        "application",
        "SOAP",
        "HTTP",
        "Apache",
        "CXF",
        "JMS",
        "Web",
        "Sphere",
        "Server",
        "messaging",
        "implementation",
        "Message",
        "Driven",
        "Beans",
        "MDB",
        "Spring",
        "ORM",
        "module",
        "integration",
        "Hibernate",
        "persistence",
        "layer",
        "application",
        "principles",
        "design",
        "patterns",
        "Session",
        "Faade",
        "Business",
        "Delegate",
        "Singleton",
        "Data",
        "Access",
        "Object",
        "Service",
        "Locator",
        "application",
        "J2EE",
        "Application",
        "Server",
        "environment",
        "IBM",
        "WebSphere",
        "deployment",
        "server",
        "RAD",
        "development",
        "IDE",
        "JIRA",
        "tracking",
        "project",
        "management",
        "XML",
        "Schemas",
        "store",
        "data",
        "XML",
        "data",
        "Platform",
        "Changes",
        "data",
        "applications",
        "platforms",
        "XSLT",
        "XML",
        "documents",
        "HTML",
        "Wrote",
        "custom",
        "jQuery",
        "plugins",
        "JavaScript",
        "functions",
        "bleedingedge",
        "AJAXdriven",
        "user",
        "interface",
        "Developed",
        "unit",
        "test",
        "cases",
        "JUnit",
        "Maven",
        "Jenkins",
        "build",
        "process",
        "Log4J",
        "utility",
        "error",
        "info",
        "debug",
        "messages",
        "Rational",
        "Clear",
        "Case",
        "version",
        "Worked",
        "schedule",
        "deadlines",
        "Environment",
        "Java",
        "J2EE",
        "Spring",
        "MVC",
        "Hibernate",
        "HTML",
        "CSS",
        "AJAX",
        "jQuery",
        "JavaScript",
        "JIRA",
        "XML",
        "JUnit",
        "Maven",
        "Jenkins",
        "Log4J",
        "Education",
        "Bachelors",
        "Degree",
        "Business",
        "Administration",
        "Business",
        "Administration",
        "Tribhuvan",
        "University",
        "MBA",
        "Information",
        "Technology",
        "Johnson",
        "Wales",
        "University",
        "Providence",
        "RI",
        "Skills",
        "Cassandra",
        "Ambari",
        "Hdfs",
        "Impala",
        "Oozie",
        "Sqoop",
        "Hbase",
        "Kafka",
        "Flume",
        "Map",
        "Apache",
        "spark",
        "Hbase",
        "Hive",
        "Javascript",
        "Json",
        "Pig",
        "Python",
        "Xml",
        "Zookeeper"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T20:21:24.179519",
    "resume_data": "Sr Hadoop BigData Engineer Sr Hadoop BigData Engineer Sr Hadoop BigData Engineer Liberty Mutual Insurance Dover NH Work Experience Sr Hadoop BigData Engineer Liberty Mutual Insurance Dover NH October 2017 to Present Roles Responsibilities Worked with business partners in discussing the requirements for new projects and enhancements to the existing applications Worked with application teams to install operating system Hadoop updates patches version upgrades as required Developed MapReduce programs to parse the raw data populate tables and store the refined data in partitioned tables Used Spark API over Cloudera Hadoop YARN to perform analytics on data in Hive Involved in creating Hive Tables loading with data and writing Hive queries which will invoke and run MapReduce jobs in the backend Developed Pig Latin Scripts to process data in a batch to perform trend analysis Wrote ETL jobs to read from web APIs using REST and HTTP calls and loaded into Cassandra using Java and Talend Analyzed Cassandra database and compared it with other opensource NoSQL databases to find which one of them better suites the current requirement Responsible for Cluster maintenance Monitoring commissioningdecommissioning Data Nodes troubleshooting manage and reviewing Data Backups and Log Files Ingested data from RDBMS and performed data transformations and then export the transformed data to Cassandra as per the business requirement Developed automated processes for flattening the upstream data from Cassandra which in JSON format Used Hive UDFs to flatten the JSON Data Configured Spark Streaming to receive real time data from the Kafka and store the stream data to the database Responsible for developing data pipeline by implementing Kafka producers and consumers Connected to AWS EC2 using SSH and ran Sparksubmit jobs Responsible for developing data pipeline with AWS to extract the data from weblogs and store in the database Analyzed user request patterns and implemented various performance optimization measures including implementing partitions and buckets in HiveQL Studied data by performing HiveQL running Pig Latin scripts to study customer behavior Environment Hadoop Hive MapReduce Pig Latin REST Java Cassandra JSON Spark AWS EC2 HiveQL Oozie Hadoop Developer NorthShore Medical Group Lincolnwood IL November 2015 to August 2017 Roles Responsibilities Worked on a product team using AgileSCRUM methodology to develop deploy and support solutions that leverage the Client big data platform Documented the systems processes and procedures for future references Supported technical team in management and review of Hadoop log files and data backups Involved in writing MapReduce program and Hive queries to load and process data in Hadoop File System Involved in creating Hive tables loading with data and writing hive queries which will run internally in MapReduce way Developing and running MapReduce jobs on YARN and Hadoop clusters to produce daily and monthly reports as per users need Maintain and schedule periodic jobs which range from updates on MapReduce jobs to creating adhoc jobs for the business users Scheduling and managing jobs on a Hadoop cluster using Oozie work flow Handled importing data from different data sources into HDFS using Sqoop and performing transformations using Hive MapReduce and then loading data into HDFS Exporting of result set from HIVE to MySQL using Sqoop export tool for further processing Collecting and aggregating large amounts of log data using Flume and staging data in HDFS for further analysis Developed several REST web services supporting both XML and JSON to perform task such as demand response management Created Maven builds to build and deploy Spring Boot microservices to internal enterprise Docker registry Designed target tables as per the requirement from the reporting team and designed Extraction Transformation and Loading using Talend Implemented File Transfer Protocol operations using Talend Studio to transfer files in between network folders Optimized MapReduce Jobs to use HDFS efficiently by using various compression mechanisms Responsible for continuous monitoring and managing Elastic MapReduce EMR cluster through AWS console Executed speedy reviews and first mover advantages by using workflows like Oozie in order to automate the data Involved in loading data from UNIX file system to HDFS Environment Hadoop MapReduce HDFS Hive Oracle Java AWS Servlets HTML XML SQL J2EE JUnit Tomcat UNIX Maven REST Hadoop Developer Barnes Noble Brentwood TN March 2014 to September 2015 Roles Responsibilities Involved in developing Hadoop Map Reduce jobs using Java Runtime Environment for the batch processing to search and match the scores Executed speedy reviews and first mover advantages by using workflows like Oozie in order to automate the data Loading process into the Hadoop distributed File System HDFS and Pig language in order to preprocess the data Integrated Oozie with the rest of the Hadoop stack supporting several types of Hadoop jobs out of the box such as MapReduce Pig Hive Sqoop Flume Wrote Hive jobs to parse the logs and structure them in tabular format to facilitate effective querying on the log data Worked with Sqoop to import export data from HDFS to Relational Database system Performed performance tuning and troubleshooting of Map Reduce jobs by analyzing and reviewing Hadoop log files Developed Pig scripts for analyzing large data sets in the HDFS Collecting and aggregating large amounts of log data using Apache Flume and staging data in HDFS for further analysis Designed and built the Reporting Application which uses the Spark SQL to fetch and generate reports on HBase table data Extracted data from the server into HDFS and Bulk Loaded the cleaned data into HBase Extracted data from the flat files and other RDBMS databases into staging area and populated onto Data warehouse Worked on JVM performance tuning to improve MapReduce jobs performance Developed programs to manipulate data perform CRUD operations on request to database Developed several REST web services supporting both XML and JSON to perform task such as demand response management Used message driven beans for asynchronous processing alerts to the customer Worked on developing Use Cases Class Diagrams Sequence diagrams and Data Models Performed performance tuning and troubleshooting of Map Reduce jobs by analyzing and reviewing Hadoop log files Environment Hadoop Hive HBase Linux MapReduce HDFS Hive Java JDK Cloudera MapReduce DataStax IBM DataStage UNIX Shell Scripting Java Developer Charter Communications St Louis MO February 2012 to December 2013 Roles Responsibilities Interacted with business analyst to understand the requirements to ensure correct modules been built to meet business requirements Participated in the daily SCRUM meetings to produce quality enhancements within time Developed UML using Case diagrams Class diagrams and Sequence diagrams using Rational Software Architect Spring MVC model integration for frontend request action controller Developed web screens in JSP JSTL CSS and clientside validation using jQuery Developed Web services to allow communication between application through SOAP over HTTP using Apache CXF Configured JMS on Web Sphere Server for asynchronous messaging through implementation of Message Driven Beans MDB Used Spring ORM module for integration with Hibernate for persistence layer Implemented the application using the concrete principles laid down by several design patterns such as Session Faade Business Delegate Singleton Data Access Object and Service Locator Developed the application in J2EE Application Server environment with IBM WebSphere as deployment server with RAD as development IDE Used JIRA for defect tracking and project management Developed and designed XML Schemas to transport and store data XML was used to simplify data and allow for Platform Changes as well as making data more available across the applications distributed platforms Extensively used XSLT to transform XML documents to HTML Wrote custom jQuery plugins and developed JavaScript functions to build a bleedingedge AJAXdriven user interface Developed unit and functional test cases using JUnit Maven and Jenkins used for the automatic build process Used Log4J utility to log error info and debug messages Used Rational Clear Case for version controlling Worked efficiently in a very tight schedule to meet the deadlines Environment Java J2EE Spring MVC Hibernate HTML CSS AJAX jQuery JavaScript JIRA XML JUnit Maven Jenkins Log4J Education Bachelors Degree in Business Administration in Business Administration Tribhuvan University 2018 MBA in Information Technology Johnson Wales University Providence RI 2012 Skills Cassandra Ambari Hdfs Impala Oozie Sqoop Hbase Kafka Db2 Flume Map reduce Apache spark Hbase Hive Javascript Json Pig Python Xml Zookeeper",
    "unique_id": "58ce0153-81b3-443c-b3c5-af362b2410da"
}