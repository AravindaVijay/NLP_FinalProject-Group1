{
    "clean_data": "Sr Python Developer Srspan lPythonspan span lDeveloperspan Sr Python Developer Capital One Bank Plano TX Around 8 years of experience as a Python Developer and Data Scientist proficient IT professional with experience in Python Data Analytics Statistical Modeling Visualization Machine Learning and Deep learning REST API AWS C C and SQL Wrote python scripts to parse XML documents and load the data in database and developed webbased applications using Python CSS and HTML Worked on applications and developed them with XML JSON XSL PHP Django Python Rails Experienced in developing Web Services with Python programming language Experience in writing Sub Queries Stored Procedures Triggers Cursors and Functions on MySQL and PostgreSQL database Cleaned data and processed third party spending data into maneuverable deliverables within specific formats with Excel macros and python libraries Experienced in working with various Python IDEs using PyCharm PyScripter Spyder PyStudio and PyDev Good experience of software development in Python and IDEs pycharm sublime text Jupyter Notebook Experienced in web applications development using Django Python using HTMLCSS for serverside rendered applications Worked on Anaconda Python Environment Developed views and templates with Python and Djangos view controller and templating language to create a userfriendly website interface Experience in Data mining with large datasets of Structured and Unstructured data Data Acquisition Data Validation Predictive modeling Data Visualization Experience in integrating data profiling validating and data cleansing transformation and data visualization using R and Python Worked on MongoDB database concepts such as locking transactions indexes Sharding replication schema design Experience in writing Sub Queries Stored Procedures Triggers Cursors and Functions in MySQL Experienced in Agile Methodologies Scrum stories and sprints experience in a Python based environment along with data analytics data wrangling Good experience in using various Python libraries Beautiful Soup Numpy Scipy matplotlib pythontwitter Pandas MySQL dB for database connectivity Strong Experience in Big data technologies including Apache Spark HDFS Hive MongoDB Hands on experience of Git Sound understanding of Deep learning using CNN RNN ANN reinforcement learning transfer learning Theoretical foundations and practical handson projects related to i supervised learning linear and logistic regression boosted decision trees Support Vector Machines neural networks NLP ii unsupervised learning clustering dimensionality reduction recommender systems iii probability statistics experiment analysis confidence intervals AB testing iv algorithms and data structures Hands on experience in design management and visualization of databases using Oracle MySQL and SQL Server Experienced in Hadoop 2x ecosystem and Apache Spark 2x framework such as Hive Pig Scoop Pyspark Experience in manipulating the large data sets with R packages like tidyr tidyverse dplyr reshape lubridate Caret and visualizing the data using lattice and ggplot2 packages Experience in dimensionality reduction using techniques like PCA and LDA Intensive handson Boot camp on Data Analytics course spanning from Statistics to Programming including data engineering data visualization machine learning and programming in R SQL Experience in data analytics predictive analysis like Classification Regression Recommender Systems Good Exposure with Factor Analysis Bagging and Boosting algorithms Experience in Descriptive Analysis Problems like Frequent Pattern Mining Clustering Outlier Detection Worked on Machine Learning algorithms like Classification and Regression with KNN Model Decision Tree Model Nave Bayes Model Logistic Regression SVM Model and Latent Factor Model Handson experience on Python and libraries like Numpy Pandas Matplotlib Seaborn NLTK SciKit learn SciPy Expertise and knowledge in TensorFlow to do machine learningdeep learning package in python Good knowledge on Microsoft Azure SQL Machine Learning and HDInsight Good Knowledge on Natural Language Processing NLP and Time Series Analysis and Forecasting using ARIMA model in Python and R Good knowledge in Tableau Power BI for interactive data visualizations Indepth Understanding in NoSQL databases like MongoDB HBase Experienced in Amazon Web Services AWS and Microsoft Azure such as AWS EC2 S3 RD3 Azure HDInsight Machine Learning Studio Azure Data Lake Very good experience and knowledge in provisioning virtual clusters under AWS cloud which includes services like EC2 S3 and EMR Good exposure in creating pivot tables and charts in Excel Work Experience Sr Python Developer Capital One Bank Plano TX September 2018 to Present Capital One in Plano Texas second main office handling data source and their partner information in US based in Texas Plano and its principal work on data analytics and development Work has been assigned on multiple project with this company based on experience and client requirement to fulfill in given time As most of company moving toward cloud technology my work is mostly migration team to automate system from OnpermEDE to SFG AWS Cloud with latest technology Also part of internal web portal development using python powerful tool like pandas Django and Flask and Machine learning Automate different workflows which are initiated manually with Python scripts and Unix shell scripting Create activate and program in Anaconda environment Use Python unit and functional testing modules such as unit test unittest2 mock and custom frameworks inline with Agile Software Development methodologies Develop Sqoop scripts to handle change data capture for processing incremental records between new arrived and existing data in RDBMS tables Installed Hadoop Map Reduce HDFS AWS and developed multiple MapReduce jobs in PIG and Hive for data cleaning and preprocessing Manage datasets using Panda data frames and MySQL queried MYSQL database queries from python using PythonMySQL connector and MySQL dB package to retrieve information Involved in the WebApplication development using Python 35 HTML5 CSS3 AJAX JSON and Jquery Develop and tested many features for dashboard using Python Java Bootstrap CSS JavaScript and Jquery Generated Python Django forms to record data of online users and used PyTest for writing test cases Implemented and modified various SQL queries and Functions Cursors and Triggers as per the client requirements Clean data and processed third party spending data into maneuverable deliverables within specific format with Excel macros and python libraries such as NumPy SQLAlchemy and matplotlib Used Pandas as API to put the data as time series and tabular format for manipulation and retrieval of data Helped with the migration from the old server to Jira database Matching Fields with Python scripts for transferring and verifying the information Analyze Format data using Machine Learning algorithm by Python ScikitLearn Experience in python Jupyter Scientific computing stack numpy scipy pandasand matplotlib Perform troubleshooting fixed and deployed many Python bug fixes of the two main applications that were a main source of data for both customers and internal customer service team Write Python scripts to parse JSON documents and load the data in database Generating various capacity planning reports graphical using Python packages like Numpy matplotlib Analyzing various logs that are been generating and predictingforecasting next occurrence of event with various Python libraries Developed single page application by using Angular JS backed by MongoDB and NodeJS Design and maintain databases using Python and developed Python based API RESTful Web Service using Flask SQL Alchemy and PostgreSQL Manage code versioning with GitHub Bit Bucket and deployment to staging and production servers and implement MVC architecture in developing the web application with the help of Django framework Use Celery as task queue and RabbitMQ Redis as messaging broker to execute asynchronous tasks Design and manage API system deployment using fast http server and Amazon AWS architecture Develop remote integration with third party platforms by using RESTful web services and Successful implementation of Apache Spark and Spark Streaming applications for large scale data Built various graphs for business decision making using Python mat plotlib library Involved in development of Web Services using SOAP for sending and getting data from the external interface in the XML format Export Test case Scripts and modified the selenium scripts and executed in Selenium environment Wrote UNIX shell scripting for automation Developed views and templates with Django view controller and template Language to create a userfriendly website interface Create Individual Docker file for deployment for DevOps Teams whenever appropriate changes made Used JavaScript and JSON to update a portion of a webpage Develop consumerbased features using Django HTML and Test Driven Development TDD Developed Python web services for processing JSON and interfacing with the Data layer Increased the speed of preexisting search indexes through Django ORM optimizations Developed module to build Django ORM queries that can preload data to greatly reduce the number of databases queries needed to retrieve the same amount of data Environment Python 3 Django HTML5CSS PostgreSQL AWS AWSRDS Aurora Oracle MySQL JavaScript Jupyter Notebook VIM Pycharm Shell Scripting AngularJS JIRA Python DeveloperData Scientist Blue Shield of California Health Care Insurance Boston MA March 2017 to September 2018 As a Python DeveloperData Scientist at Blue Shield of California Health Care Insurance working on latest in distributed computing technologies and operating across massive datasets to unlock the big opportunities that help everyday people prescreen from possible future health issues The Enterprise Shared Services group is the wellrecognized group within Blue Cross Blue Shield of Massachusetts which provides the services to many consumers Disease Prediction Over the last few years BCBS has captured several health demographic and lifestyle details about patients This includes details such as age and gender along with several health parameters such as hypertension body mass index and lifestyle related variables like smoking occupation type alcohol consumption etc The project focuses on predicting the probability of certain diseases happening which can help make patients and doctors take a proactive approach towards certain health screenings EHR and EMR Healthcare Analytics The project uses data from EMR Electronic Medical Records and EHR Electronic Health Records to analyze disease trends in patients identifying the top five diseases amongst the patients from the Diagnosis code ICD9 The goal is to identify patterns of diseases relevant to certain conditions such as Hyper Tension and Diabetes and how they are spread across all the states and the prescriptionmedication these patients are receiving This also helps identify potential problems and disastrous sideeffects caused due to overprescriptionmedication Responsibilities Extracted the data from hive tables by writing efficient Hive queries Performed preliminary data analysis using descriptive statistics and handled anomalies such as removing duplicates and imputing missing values Analyze Data and Performed Data Preparation by applying historical model on the data set in AZUREML Recorded the online users data using Python Django forms and implemented test case using Pytest Used Django frameworks and Python to build dynamic webpages Generated Python Django Forms to record data of online users Application of various machine learning algorithms and statistical modeling like decision trees text analytics natural language processing NLP supervised and unsupervised regression models neural networks deep learning SVM clustering to identify Volume using scikitlearn package in python Matlab Expanded website functionality using Flask framework in Python to control the web application logic Develop SparkScala Python R for regular expression regex project in the HadoopHive environment with LinuxWindows for big data resources Used clustering technique KMeans to identify outliers and to classify unlabeled data Evaluate models using Cross Validation Log loss function ROC curves and used AUC for feature selection and elastic technologies like ElasticSearch Kibana etc Work with NLTK library to NLP data processing and finding the patterns Categorize comments into positive and negative clusters from different social networking sites using Sentiment Analysis and Text Analytics Ensure that the model has low False Positive Rate and Text classification and sentiment analysis for unstructured and semistructured data Addressed over fitting by implementing of the algorithm regularization methods like L2 and L1 Use Principal Component Analysis in feature engineering to analyze high dimensional data Create and design reports that will use gathered metrics to infer and draw logical conclusions of past and future behavior Perform Multinomial Logistic Regression Random forest Decision Tree SVM to classify package is going to deliver on time for the new route Implemented different models like Logistic Regression Random Forest and GradientBoost Trees to predict whether a given die will pass or fail the test Perform data analysis by using Hive to retrieve the data from Hadoop cluster SQL to retrieve data from the database and used ETL for data transformation Use MLlib Sparks Machine learning library to build and evaluate different models Perform Data Cleaning features scaling features engineering using pandas and numpy packages in python Communicate the results with operations team for taking best decisions Collect data needs and requirements by Interacting with the other departments Environment Python 2x R HDFS Hadoop 23 Hive Linux Spark IBM SPSS Tableau Desktop SQL Server 2012 Microsoft Excel Matlab Spark SQL Pyspark Data Scientist Bank of the West San Francisco CA January 2016 to February 2017 Bank of the West is a regional financial services company headquartered in San Francisco California It is a subsidiary of BNP Paribas It has more than 600 branches and offices in the Midwest and Western United States Credit History Predictive Modeling The project uses supervised machine learning to train a model with credit default data to determine the probability andor classification of defaulter or nondefaulter predictability Factors such as Education Level Marital Status Payment History and Income will return a classification The goal is to help the bank manage loanportfolio risk and determine if a neural network vs logistic regression is the better model for classification Logistic Regression Random Forest Model and Deep Neural Network Models were used Loan Payment Default Prediction Built classification models using several features related to customer demographics macroeconomic dynamics historic payment behavior type and size of loan credit scores and loan to value ratios and with accuracy of 95 accuracy the model predicted the likelihood of loan default under various stressed conditions Responsibilities Gathered analyzed documented and translated application requirements into data models supported standardization of documentation and the adoption of standards and practices related to data and applications Queried and aggregated data from Amazon Redshift to get the sample dataset Identified patterns data quality issues and leveraged insights by communicating with BI team In preprocessing phase used Pandas to remove or replace all the missing data and feature engineering to eliminate unrelated features Balanced the dataset with Oversampling the minority label class and Undersampling the majority label class In data exploration stage used correlation analysis and graphical techniques to get some insights about the claim data Applied machine learning techniques to tap into new markets new customers and put forth my recommendations to the top management which resulted in increase in customer base by 5 and customer portfolio by 9 Build mulitlayers Neural Networks to implement Deep Learning by using Tensorflow and Keras Perform hyperparameter tuning by doing Distributed Cross Validation in Spark to speed up the computation process Export trained models into Protobuf to be served by Tensorflow Serving and performed integration job with clients application Analyzed customer master data for the identification of prospective business to understand their business needs built client relationships and explored opportunities for crossselling of financial products 60 Increased from 40 of customers availed more than 6 products Improved fraud prediction performance by using random forest and gradient boosting for feature selection with Python Scikitlearn Tested classification algorithms such as Logistic Regression Gradient Boosting and Random Forest using Pandas and Scikitlearn and evaluated the performance Worked extensively with data governance team to maintain data models Metadata and dictionaries Developed advanced models using multivariate regression Logistic regression Random forests decision trees and clustering Applied predictive analysis and statistical modeling techniques to analyze customer behavior and offer customized products reduce delinquency rate and default rate Lead to fall in default rates from 5 to 2 Application of various machine learning algorithms and statistical modeling like decision trees regression models neural networks SVM clustering to identify Volume using scikitlearn package in python Matlab Implemented tuned and tested the model on AWS EC2 with the best algorithm and parameters Set up data preprocessing pipeline to guarantee the consistency between the training data and new coming data Deployed the model on AWS Lambda collaborated with develop team to build the business solutions Collected the feedback after deployment retrained the model to improve the performance Discovered flaws in the methodology being used to calculate weather peril zone relativities designed and implemented a 3D algorithm based on kmeans clustering and Monte Carlo methods Observed groups of customers being neglected by the pricing algorithm used hierarchical clustering to improve customer segmentation and increase profits by 6 Designed developed and maintained daily and monthly summary trending and benchmark reports in Tableau Desktop Environment Python 2x R HDFS Hadoop 23 Hive Linux Spark IBM SPSS Tableau Desktop SQL Server 2012 Microsoft Excel Matlab Spark SQL Pyspark Data ScientistData Analyst Dicks Sporting Goods Inc Pittsburgh PA November 2013 to December 2015 Dicks Sporting Goods Inc sometimes shortened to Dicks is a Fortune 500 American sporting goods retailing corporation headquartered in Coraopolis Pennsylvania in Greater Pittsburgh Dicks has 610 stores in 47 states primarily in the Eastern United States The company also owns Golf Galaxy Inc a golf specialty retailer with 82 stores in 30 states Responsibilities Statistical Modelling with ML to bring Insights in Data under guidance of Principal Data Scientist Data modeling with Pig Hive Impala Ingestion with Sqoop Flume Used SVN to commit the Changes into the main EMM application trunk Understanding and implementation of text mining concepts graph processing and semi structured and unstructured data processing Worked with Ajax API calls to communicate with Hadoop through Impala Connection and SQL to render the required data through it These API calls are similar to Microsoft Cognitive API calls Good grip on Cloudera and HDP ecosystem components Used ElasticSearch Big Data to retrieve data into application as required Performed Map Reduce Programs those are running on the cluster Developed multiple MapReduce jobs in java for data cleaning and preprocessing Developed scalable machine learning solutions within a distributed computation framework eg Hadoop Spark Storm etc Analyzed the partitioned and bucketed data and compute various metrics for reporting Involved in loading data from RDBMS and web logs into HDFS using Sqoop and Flume Worked on loading the data from MySQL to HBase where necessary using Sqoop Developed Hive queries for Analysis across different banners Extracted data from Twitter using Java and Twitter API Parsed JSON formatted twitter data and uploaded to database Launching Amazon EC2 Cloud Instances using Amazon Images Linux Ubuntu and Configuring launched instances with respect to specific applications to improve robustness Exported the result set from Hive to MySQL using Sqoop after processing the data Analyzed the data by performing Hive queries and running Pig scripts to study customer behavior Have hands on experience working on Sequence files AVRO HAR file formats and compression Used Hive to partition and bucket data Experience in writing MapReduce programs with Java API to cleanse Structured and unstructured data Wrote Pig Scripts to perform ETL procedures on the data in HDFS Created HBase tables to store various data formats of data coming from different portfolios Worked on improving performance of existing Pig and Hive Queries Environment SQLServer Oracle 9i MSOffice Teradata Informatica ER Studio XML Business Objects HDFS Teradata 141 JSON HADOOP HDFS MapReduce PIG Spark R Studio MAHOUT JAVA HIVE AWS Data AnalystData Scientist IDBI Bank Mumbai Maharashtra June 2011 to January 2013 Responsibilities Integrated data from multiple data sources or functional areas ensures data accuracy and integrity and updates data as need using SQL and Python Expertise leveraging SQL Excel and Tableau to manipulate analyze and present data Performs analyses of structured and unstructured data to solve multiple andor complex business problems utilizing advanced statistical techniques and mathematical analyses Developed advanced models using multivariate regression Logistic regression Random forests decision trees and clustering Used Pandas Numpy Seaborn Scikitlearn in Python for developing various machine learning algorithms Build and improve models using natural language processing NLP and machine learning to extract insights from unstructured data Experienced working with distributed computing technologies Apache Spark Hive Applied predictive analysis and statistical modeling techniques to analyze customer behavior and offer customized products reduce delinquency rate and default rate Lead to fall in default rates from 5 to 2 Applied machine learning techniques to tap into new markets new customers and put forth my recommendations to the top management which resulted in increase in customer base by 5 and customer portfolio by 9 Analyzed customer master data for the identification of prospective business to understand their business needs built client relationships and explored opportunities for crossselling of financial products 60 Increased from 40 of customers availed more than 6 products Collaborated with business partners to understand their problems and goals develop predictive modeling statistical analysis data reports and performance metrics Participate in the ongoing design and development of a consolidated data warehouse supporting key business metrics across the organization Designed developed and implemented data quality validation rules to inspect and monitor the health of the data Dashboard and report development experience using Tableau ETL Developer Responsibilities Involved in full SDLC of BI Project including Data Analysis Designing Development of Data Warehouse environment Used Oracle Data Integrator Designer to develop processes for extracting cleansing transforming integrating and loading data into data warehouse database Experience in Developing and customizing PLSQL packages procedures functions triggers and reports using Oracle SQL Developer Responsible for designing developing and testing of the ETL strategy to populate the data from various source systems Flat files Oracle Worked with the Business units to identify data quality rule requirements against identified anomalies Develop Data Mapping Join and queries Validation and addressingfixing data queries raised by project team in a timely manner Worked closely with Business analyst and interacted with the Business users to gather new business requirements and to understand the accurate business and current requirements Created Repositories Agent Contexts and both of Physical Logical Schema in Topology Manager for all the source and target schemas Data mapping logical data modeling created class diagrams and ER diagrams and used SQL queries to filter data within the Oracle database Installed and Setup ODI Master Repository Work Repository Execution Repository Used Topology Manager to manage the data describing the information systems physical and logical architecture Extensively worked and utilized ODI Knowledge Modules Reverse Engineering Loading Integration Check Journalizing and service Created various procedures and variables Created ODI Packages Jobs of various complexities and automated process data flow Configured and setup ODI Master repository Work repository Project Models sources targets packages Knowledge Modules Interfaces Scenarios filters condition metadata Environment PLSQL Tableau Oracle MSOffice Teradata SQL Excel Apache Spark Hive Data mapping Python Systems Analyst Coramandel Infrastructure Pvt Ltd Hyderabad Telangana November 2010 to April 2011 Responsibilities Participate in fulllife cycle of SQL database development Create conceptual logical and physical database models to support project requirements Design implement and maintain databases for Corporate Data Finance and Operations business units Responsible for maintaining the integrity of the SQL database and reporting any issues to the database architect Assisted in creating and presenting informational reports to Management based on SQL data Build manage and maintain all project documentation including Business Requirements Documents BRD technical specifications process flows and clientspecific user guides Developed and validated conceptual data models including implementing logical and physical data mart data models Work closely with clients and internal teams to elicit and document business and functional requirements Delivered projects on time using Agile and Waterfall methodologies for timely completion of projects Environment SQL database Agile and WaterfallBRD SQL dataMSOfficeOracle Education Bachelor of Engineering in Engineering Malla Reddy College of Engineering and Technology May 2011 Skills Hadoop Hbase Hdfs Hive Mapreduce Additional Information TECHNICAL SKILLS Languages Java 8 Python R Python and R Numpy SciPy Pandas Scikitlearn Matplotlib Seaborn ggplot2 caret dplyr purrr readxl tidyr Rweka gmodels RCurl C50 twitter NLP Reshape2 rjson plyr Beautiful Soup Rpy2 Algorithms Kernel Density Estimation and Nonparametric Bayes Classifier KMeans Linear Regression Neighbors Nearest Farthest Range k Classification NonNegative Matrix Factorization Dimensionality Reduction Decision Tree Gaussian Processes Logistic Regression Nave Bayes Random Forest Ridge Regression Matrix FactorizationSVD NLPMachine LearningDeep Learning LDA Latent Dirichlet Allocation NLTK Apache OpenNLP Stanford NLP Sentiment Analysis SVMs ANN RNN CNN TensorFlow MXNet Caffe H2O Keras PyTorch Theano Azure ML Cloud Google Cloud Platform AWS Azure Bluemix Web Technologies JDBC HTML5 DHTML and XML CSS3 Web Services WSDL Data Modeling Tools Erwin r 96 95 91 8x Rational Rose ERStudio MS Visio SAP Power designer Big Data Technologies Hadoop Hive HDFS MapReduce Pig Kafka Databases SQL Hive Impala Pig Spark SQL Databases SQLServer My SQL MS Access HDFS HBase Teradata Netezza MongoDB Cassandra Reporting Tools MS Office WordExcelPower Point Visio Tableau Crystal reports XI Business Intelligence SSRS Business Objects 5x 6x Cognos7060 ETL Tools Informatica Power Centre SSIS Version Control Tools SVM GitHub BI Tools Tableau Tableau Server Tableau Reader SAP Business Objects OBIEE QlikView SAP Business Intelligence Amazon Redshift or Azure Data Warehouse Operating System Windows Linux Unix Macintosh HD Red Hat",
    "entities": [
        "GradientBoost Trees",
        "Logistic Regression Random Forest Model",
        "Oracle MySQL",
        "Coraopolis Pennsylvania",
        "Oracle SQL Developer Responsible",
        "Python Developer and Data Scientist",
        "AUC",
        "EHR Electronic Health Records",
        "BI",
        "XML JSON",
        "Massachusetts",
        "Use MLlib Sparks Machine",
        "HAR",
        "Time Series Analysis and Forecasting",
        "Data Analysis Designing Development of Data",
        "My SQL MS Access HDFS HBase Teradata Netezza",
        "Applied",
        "BNP Paribas",
        "IBM",
        "Installed Hadoop Map Reduce HDFS AWS",
        "Amazon Web Services AWS",
        "Panda",
        "Pyspark Data Scientist Bank",
        "ER",
        "Undersampling",
        "Informatica Power Centre SSIS Version",
        "Hadoop",
        "Hadoop Spark Storm etc Analyzed",
        "XML",
        "SOAP",
        "Telangana",
        "Use Python",
        "Impala Connection",
        "EMM",
        "Apache Spark",
        "GitHub Bit Bucket",
        "Dicks Sporting Goods Inc",
        "Amazon",
        "AnalystData Scientist IDBI Bank",
        "Jquery Develop",
        "EMR Healthcare Analytics",
        "Protobuf",
        "Assisted",
        "Data Visualization Experience",
        "Create",
        "Python Data Analytics Statistical Modeling Visualization Machine Learning",
        "Developed",
        "Neural Networks",
        "OnpermEDE",
        "Vector Machines",
        "NodeJS Design",
        "Present Capital One",
        "Django",
        "Amazon Redshift",
        "San Francisco",
        "Waterfall",
        "SQL Excel",
        "Validation",
        "BCBS",
        "Sequence",
        "Hyper Tension",
        "The Enterprise Shared Services",
        "Worked on",
        "Descriptive Analysis Problems",
        "Develop",
        "Communicate",
        "Control Tools",
        "API RESTful Web Service",
        "Unstructured",
        "Flask",
        "Collaborated",
        "Evaluate",
        "Tableau Desktop",
        "Logistic Regression Random Forest",
        "Worked",
        "Automate",
        "Data Analytics",
        "Project Models",
        "Spark Streaming",
        "XI Business Intelligence SSRS Business Objects",
        "ROC",
        "HDP",
        "Created Repositories",
        "DevOps Teams",
        "MVC",
        "Flask SQL Alchemy",
        "BI Project",
        "GitHub BI",
        "Spark",
        "Agile",
        "California Health Care Insurance",
        "Skills Hadoop Hbase Hdfs Hive Mapreduce Additional Information TECHNICAL SKILLS Languages",
        "HDFS Created HBase",
        "Classification Regression Recommender Systems Good Exposure with",
        "Export Test",
        "API",
        "Tableau Power BI",
        "US",
        "Perform",
        "Sqoop",
        "Perform Data Cleaning",
        "LinuxWindows",
        "KNN",
        "Classification and Regression",
        "AWS",
        "Worked with Ajax API",
        "PCA",
        "EMR Electronic Medical Records",
        "Sub Queries Stored Procedures Triggers Cursors and Functions",
        "SQL Wrote",
        "Categorize",
        "Big Data Technologies Hadoop Hive HDFS MapReduce Pig Kafka",
        "PIG",
        "Perform Multinomial Logistic Regression Random",
        "Microsoft Cognitive API",
        "Random Forest",
        "java",
        "California",
        "Data Acquisition Data Validation Predictive",
        "SQL",
        "Caret",
        "Text Analytics Ensure",
        "NLP",
        "ARIMA",
        "Python Django",
        "Anaconda",
        "Create Individual Docker",
        "Hive",
        "Macintosh",
        "Amazon AWS",
        "SAP Power",
        "the Eastern United States",
        "Pandas",
        "ETL",
        "Agile Software Development",
        "Analyze Data",
        "Python Systems Analyst Coramandel Infrastructure Pvt Ltd Hyderabad",
        "Blue Cross Blue Shield",
        "Build",
        "unittest2",
        "Impala",
        "Djangos",
        "SQL Server Experienced",
        "Pig Hive Impala Ingestion with Sqoop Flume",
        "Microsoft",
        "Texas",
        "WebApplication",
        "HBase Experienced",
        "CNN",
        "Blue Shield",
        "ML",
        "PyCharm PyScripter Spyder PyStudio",
        "SVN",
        "CSS",
        "Stanford NLP Sentiment Analysis",
        "Oracle Data Integrator Designer",
        "Git Sound",
        "Data",
        "Structured",
        "Distributed Cross Validation",
        "Golf Galaxy Inc",
        "Corporate Data Finance",
        "Indepth Understanding",
        "MapReduce",
        "Developer Capital One Bank",
        "Performed Data Preparation",
        "RDBMS",
        "NoSQL",
        "Tableau",
        "Machine Learning",
        "Application",
        "Frequent Pattern Mining Clustering Outlier Detection Worked on",
        "PyTest",
        "Responsibilities Extracted",
        "Operations",
        "Business Requirements Documents BRD",
        "SVM",
        "Python ScikitLearn Experience",
        "Tableau Desktop Environment",
        "Cross Validation Log",
        "Principal Data Scientist Data"
    ],
    "experience": "Experience in writing Sub Queries Stored Procedures Triggers Cursors and Functions on MySQL and PostgreSQL database Cleaned data and processed third party spending data into maneuverable deliverables within specific formats with Excel macros and python libraries Experienced in working with various Python IDEs using PyCharm PyScripter Spyder PyStudio and PyDev Good experience of software development in Python and IDEs pycharm sublime text Jupyter Notebook Experienced in web applications development using Django Python using HTMLCSS for serverside rendered applications Worked on Anaconda Python Environment Developed views and templates with Python and Djangos view controller and templating language to create a userfriendly website interface Experience in Data mining with large datasets of Structured and Unstructured data Data Acquisition Data Validation Predictive modeling Data Visualization Experience in integrating data profiling validating and data cleansing transformation and data visualization using R and Python Worked on MongoDB database concepts such as locking transactions indexes Sharding replication schema design Experience in writing Sub Queries Stored Procedures Triggers Cursors and Functions in MySQL Experienced in Agile Methodologies Scrum stories and sprints experience in a Python based environment along with data analytics data wrangling Good experience in using various Python libraries Beautiful Soup Numpy Scipy matplotlib pythontwitter Pandas MySQL dB for database connectivity Strong Experience in Big data technologies including Apache Spark HDFS Hive MongoDB Hands on experience of Git Sound understanding of Deep learning using CNN RNN ANN reinforcement learning transfer learning Theoretical foundations and practical handson projects related to i supervised learning linear and logistic regression boosted decision trees Support Vector Machines neural networks NLP ii unsupervised learning clustering dimensionality reduction recommender systems iii probability statistics experiment analysis confidence intervals AB testing iv algorithms and data structures Hands on experience in design management and visualization of databases using Oracle MySQL and SQL Server Experienced in Hadoop 2x ecosystem and Apache Spark 2x framework such as Hive Pig Scoop Pyspark Experience in manipulating the large data sets with R packages like tidyr tidyverse dplyr reshape lubridate Caret and visualizing the data using lattice and ggplot2 packages Experience in dimensionality reduction using techniques like PCA and LDA Intensive handson Boot camp on Data Analytics course spanning from Statistics to Programming including data engineering data visualization machine learning and programming in R SQL Experience in data analytics predictive analysis like Classification Regression Recommender Systems Good Exposure with Factor Analysis Bagging and Boosting algorithms Experience in Descriptive Analysis Problems like Frequent Pattern Mining Clustering Outlier Detection Worked on Machine Learning algorithms like Classification and Regression with KNN Model Decision Tree Model Nave Bayes Model Logistic Regression SVM Model and Latent Factor Model Handson experience on Python and libraries like Numpy Pandas Matplotlib Seaborn NLTK SciKit learn SciPy Expertise and knowledge in TensorFlow to do machine learningdeep learning package in python Good knowledge on Microsoft Azure SQL Machine Learning and HDInsight Good Knowledge on Natural Language Processing NLP and Time Series Analysis and Forecasting using ARIMA model in Python and R Good knowledge in Tableau Power BI for interactive data visualizations Indepth Understanding in NoSQL databases like MongoDB HBase Experienced in Amazon Web Services AWS and Microsoft Azure such as AWS EC2 S3 RD3 Azure HDInsight Machine Learning Studio Azure Data Lake Very good experience and knowledge in provisioning virtual clusters under AWS cloud which includes services like EC2 S3 and EMR Good exposure in creating pivot tables and charts in Excel Work Experience Sr Python Developer Capital One Bank Plano TX September 2018 to Present Capital One in Plano Texas second main office handling data source and their partner information in US based in Texas Plano and its principal work on data analytics and development Work has been assigned on multiple project with this company based on experience and client requirement to fulfill in given time As most of company moving toward cloud technology my work is mostly migration team to automate system from OnpermEDE to SFG AWS Cloud with latest technology Also part of internal web portal development using python powerful tool like pandas Django and Flask and Machine learning Automate different workflows which are initiated manually with Python scripts and Unix shell scripting Create activate and program in Anaconda environment Use Python unit and functional testing modules such as unit test unittest2 mock and custom frameworks inline with Agile Software Development methodologies Develop Sqoop scripts to handle change data capture for processing incremental records between new arrived and existing data in RDBMS tables Installed Hadoop Map Reduce HDFS AWS and developed multiple MapReduce jobs in PIG and Hive for data cleaning and preprocessing Manage datasets using Panda data frames and MySQL queried MYSQL database queries from python using PythonMySQL connector and MySQL dB package to retrieve information Involved in the WebApplication development using Python 35 HTML5 CSS3 AJAX JSON and Jquery Develop and tested many features for dashboard using Python Java Bootstrap CSS JavaScript and Jquery Generated Python Django forms to record data of online users and used PyTest for writing test cases Implemented and modified various SQL queries and Functions Cursors and Triggers as per the client requirements Clean data and processed third party spending data into maneuverable deliverables within specific format with Excel macros and python libraries such as NumPy SQLAlchemy and matplotlib Used Pandas as API to put the data as time series and tabular format for manipulation and retrieval of data Helped with the migration from the old server to Jira database Matching Fields with Python scripts for transferring and verifying the information Analyze Format data using Machine Learning algorithm by Python ScikitLearn Experience in python Jupyter Scientific computing stack numpy scipy pandasand matplotlib Perform troubleshooting fixed and deployed many Python bug fixes of the two main applications that were a main source of data for both customers and internal customer service team Write Python scripts to parse JSON documents and load the data in database Generating various capacity planning reports graphical using Python packages like Numpy matplotlib Analyzing various logs that are been generating and predictingforecasting next occurrence of event with various Python libraries Developed single page application by using Angular JS backed by MongoDB and NodeJS Design and maintain databases using Python and developed Python based API RESTful Web Service using Flask SQL Alchemy and PostgreSQL Manage code versioning with GitHub Bit Bucket and deployment to staging and production servers and implement MVC architecture in developing the web application with the help of Django framework Use Celery as task queue and RabbitMQ Redis as messaging broker to execute asynchronous tasks Design and manage API system deployment using fast http server and Amazon AWS architecture Develop remote integration with third party platforms by using RESTful web services and Successful implementation of Apache Spark and Spark Streaming applications for large scale data Built various graphs for business decision making using Python mat plotlib library Involved in development of Web Services using SOAP for sending and getting data from the external interface in the XML format Export Test case Scripts and modified the selenium scripts and executed in Selenium environment Wrote UNIX shell scripting for automation Developed views and templates with Django view controller and template Language to create a userfriendly website interface Create Individual Docker file for deployment for DevOps Teams whenever appropriate changes made Used JavaScript and JSON to update a portion of a webpage Develop consumerbased features using Django HTML and Test Driven Development TDD Developed Python web services for processing JSON and interfacing with the Data layer Increased the speed of preexisting search indexes through Django ORM optimizations Developed module to build Django ORM queries that can preload data to greatly reduce the number of databases queries needed to retrieve the same amount of data Environment Python 3 Django HTML5CSS PostgreSQL AWS AWSRDS Aurora Oracle MySQL JavaScript Jupyter Notebook VIM Pycharm Shell Scripting AngularJS JIRA Python DeveloperData Scientist Blue Shield of California Health Care Insurance Boston MA March 2017 to September 2018 As a Python DeveloperData Scientist at Blue Shield of California Health Care Insurance working on latest in distributed computing technologies and operating across massive datasets to unlock the big opportunities that help everyday people prescreen from possible future health issues The Enterprise Shared Services group is the wellrecognized group within Blue Cross Blue Shield of Massachusetts which provides the services to many consumers Disease Prediction Over the last few years BCBS has captured several health demographic and lifestyle details about patients This includes details such as age and gender along with several health parameters such as hypertension body mass index and lifestyle related variables like smoking occupation type alcohol consumption etc The project focuses on predicting the probability of certain diseases happening which can help make patients and doctors take a proactive approach towards certain health screenings EHR and EMR Healthcare Analytics The project uses data from EMR Electronic Medical Records and EHR Electronic Health Records to analyze disease trends in patients identifying the top five diseases amongst the patients from the Diagnosis code ICD9 The goal is to identify patterns of diseases relevant to certain conditions such as Hyper Tension and Diabetes and how they are spread across all the states and the prescriptionmedication these patients are receiving This also helps identify potential problems and disastrous sideeffects caused due to overprescriptionmedication Responsibilities Extracted the data from hive tables by writing efficient Hive queries Performed preliminary data analysis using descriptive statistics and handled anomalies such as removing duplicates and imputing missing values Analyze Data and Performed Data Preparation by applying historical model on the data set in AZUREML Recorded the online users data using Python Django forms and implemented test case using Pytest Used Django frameworks and Python to build dynamic webpages Generated Python Django Forms to record data of online users Application of various machine learning algorithms and statistical modeling like decision trees text analytics natural language processing NLP supervised and unsupervised regression models neural networks deep learning SVM clustering to identify Volume using scikitlearn package in python Matlab Expanded website functionality using Flask framework in Python to control the web application logic Develop SparkScala Python R for regular expression regex project in the HadoopHive environment with LinuxWindows for big data resources Used clustering technique KMeans to identify outliers and to classify unlabeled data Evaluate models using Cross Validation Log loss function ROC curves and used AUC for feature selection and elastic technologies like ElasticSearch Kibana etc Work with NLTK library to NLP data processing and finding the patterns Categorize comments into positive and negative clusters from different social networking sites using Sentiment Analysis and Text Analytics Ensure that the model has low False Positive Rate and Text classification and sentiment analysis for unstructured and semistructured data Addressed over fitting by implementing of the algorithm regularization methods like L2 and L1 Use Principal Component Analysis in feature engineering to analyze high dimensional data Create and design reports that will use gathered metrics to infer and draw logical conclusions of past and future behavior Perform Multinomial Logistic Regression Random forest Decision Tree SVM to classify package is going to deliver on time for the new route Implemented different models like Logistic Regression Random Forest and GradientBoost Trees to predict whether a given die will pass or fail the test Perform data analysis by using Hive to retrieve the data from Hadoop cluster SQL to retrieve data from the database and used ETL for data transformation Use MLlib Sparks Machine learning library to build and evaluate different models Perform Data Cleaning features scaling features engineering using pandas and numpy packages in python Communicate the results with operations team for taking best decisions Collect data needs and requirements by Interacting with the other departments Environment Python 2x R HDFS Hadoop 23 Hive Linux Spark IBM SPSS Tableau Desktop SQL Server 2012 Microsoft Excel Matlab Spark SQL Pyspark Data Scientist Bank of the West San Francisco CA January 2016 to February 2017 Bank of the West is a regional financial services company headquartered in San Francisco California It is a subsidiary of BNP Paribas It has more than 600 branches and offices in the Midwest and Western United States Credit History Predictive Modeling The project uses supervised machine learning to train a model with credit default data to determine the probability andor classification of defaulter or nondefaulter predictability Factors such as Education Level Marital Status Payment History and Income will return a classification The goal is to help the bank manage loanportfolio risk and determine if a neural network vs logistic regression is the better model for classification Logistic Regression Random Forest Model and Deep Neural Network Models were used Loan Payment Default Prediction Built classification models using several features related to customer demographics macroeconomic dynamics historic payment behavior type and size of loan credit scores and loan to value ratios and with accuracy of 95 accuracy the model predicted the likelihood of loan default under various stressed conditions Responsibilities Gathered analyzed documented and translated application requirements into data models supported standardization of documentation and the adoption of standards and practices related to data and applications Queried and aggregated data from Amazon Redshift to get the sample dataset Identified patterns data quality issues and leveraged insights by communicating with BI team In preprocessing phase used Pandas to remove or replace all the missing data and feature engineering to eliminate unrelated features Balanced the dataset with Oversampling the minority label class and Undersampling the majority label class In data exploration stage used correlation analysis and graphical techniques to get some insights about the claim data Applied machine learning techniques to tap into new markets new customers and put forth my recommendations to the top management which resulted in increase in customer base by 5 and customer portfolio by 9 Build mulitlayers Neural Networks to implement Deep Learning by using Tensorflow and Keras Perform hyperparameter tuning by doing Distributed Cross Validation in Spark to speed up the computation process Export trained models into Protobuf to be served by Tensorflow Serving and performed integration job with clients application Analyzed customer master data for the identification of prospective business to understand their business needs built client relationships and explored opportunities for crossselling of financial products 60 Increased from 40 of customers availed more than 6 products Improved fraud prediction performance by using random forest and gradient boosting for feature selection with Python Scikitlearn Tested classification algorithms such as Logistic Regression Gradient Boosting and Random Forest using Pandas and Scikitlearn and evaluated the performance Worked extensively with data governance team to maintain data models Metadata and dictionaries Developed advanced models using multivariate regression Logistic regression Random forests decision trees and clustering Applied predictive analysis and statistical modeling techniques to analyze customer behavior and offer customized products reduce delinquency rate and default rate Lead to fall in default rates from 5 to 2 Application of various machine learning algorithms and statistical modeling like decision trees regression models neural networks SVM clustering to identify Volume using scikitlearn package in python Matlab Implemented tuned and tested the model on AWS EC2 with the best algorithm and parameters Set up data preprocessing pipeline to guarantee the consistency between the training data and new coming data Deployed the model on AWS Lambda collaborated with develop team to build the business solutions Collected the feedback after deployment retrained the model to improve the performance Discovered flaws in the methodology being used to calculate weather peril zone relativities designed and implemented a 3D algorithm based on kmeans clustering and Monte Carlo methods Observed groups of customers being neglected by the pricing algorithm used hierarchical clustering to improve customer segmentation and increase profits by 6 Designed developed and maintained daily and monthly summary trending and benchmark reports in Tableau Desktop Environment Python 2x R HDFS Hadoop 23 Hive Linux Spark IBM SPSS Tableau Desktop SQL Server 2012 Microsoft Excel Matlab Spark SQL Pyspark Data ScientistData Analyst Dicks Sporting Goods Inc Pittsburgh PA November 2013 to December 2015 Dicks Sporting Goods Inc sometimes shortened to Dicks is a Fortune 500 American sporting goods retailing corporation headquartered in Coraopolis Pennsylvania in Greater Pittsburgh Dicks has 610 stores in 47 states primarily in the Eastern United States The company also owns Golf Galaxy Inc a golf specialty retailer with 82 stores in 30 states Responsibilities Statistical Modelling with ML to bring Insights in Data under guidance of Principal Data Scientist Data modeling with Pig Hive Impala Ingestion with Sqoop Flume Used SVN to commit the Changes into the main EMM application trunk Understanding and implementation of text mining concepts graph processing and semi structured and unstructured data processing Worked with Ajax API calls to communicate with Hadoop through Impala Connection and SQL to render the required data through it These API calls are similar to Microsoft Cognitive API calls Good grip on Cloudera and HDP ecosystem components Used ElasticSearch Big Data to retrieve data into application as required Performed Map Reduce Programs those are running on the cluster Developed multiple MapReduce jobs in java for data cleaning and preprocessing Developed scalable machine learning solutions within a distributed computation framework eg Hadoop Spark Storm etc Analyzed the partitioned and bucketed data and compute various metrics for reporting Involved in loading data from RDBMS and web logs into HDFS using Sqoop and Flume Worked on loading the data from MySQL to HBase where necessary using Sqoop Developed Hive queries for Analysis across different banners Extracted data from Twitter using Java and Twitter API Parsed JSON formatted twitter data and uploaded to database Launching Amazon EC2 Cloud Instances using Amazon Images Linux Ubuntu and Configuring launched instances with respect to specific applications to improve robustness Exported the result set from Hive to MySQL using Sqoop after processing the data Analyzed the data by performing Hive queries and running Pig scripts to study customer behavior Have hands on experience working on Sequence files AVRO HAR file formats and compression Used Hive to partition and bucket data Experience in writing MapReduce programs with Java API to cleanse Structured and unstructured data Wrote Pig Scripts to perform ETL procedures on the data in HDFS Created HBase tables to store various data formats of data coming from different portfolios Worked on improving performance of existing Pig and Hive Queries Environment SQLServer Oracle 9i MSOffice Teradata Informatica ER Studio XML Business Objects HDFS Teradata 141 JSON HADOOP HDFS MapReduce PIG Spark R Studio MAHOUT JAVA HIVE AWS Data AnalystData Scientist IDBI Bank Mumbai Maharashtra June 2011 to January 2013 Responsibilities Integrated data from multiple data sources or functional areas ensures data accuracy and integrity and updates data as need using SQL and Python Expertise leveraging SQL Excel and Tableau to manipulate analyze and present data Performs analyses of structured and unstructured data to solve multiple andor complex business problems utilizing advanced statistical techniques and mathematical analyses Developed advanced models using multivariate regression Logistic regression Random forests decision trees and clustering Used Pandas Numpy Seaborn Scikitlearn in Python for developing various machine learning algorithms Build and improve models using natural language processing NLP and machine learning to extract insights from unstructured data Experienced working with distributed computing technologies Apache Spark Hive Applied predictive analysis and statistical modeling techniques to analyze customer behavior and offer customized products reduce delinquency rate and default rate Lead to fall in default rates from 5 to 2 Applied machine learning techniques to tap into new markets new customers and put forth my recommendations to the top management which resulted in increase in customer base by 5 and customer portfolio by 9 Analyzed customer master data for the identification of prospective business to understand their business needs built client relationships and explored opportunities for crossselling of financial products 60 Increased from 40 of customers availed more than 6 products Collaborated with business partners to understand their problems and goals develop predictive modeling statistical analysis data reports and performance metrics Participate in the ongoing design and development of a consolidated data warehouse supporting key business metrics across the organization Designed developed and implemented data quality validation rules to inspect and monitor the health of the data Dashboard and report development experience using Tableau ETL Developer Responsibilities Involved in full SDLC of BI Project including Data Analysis Designing Development of Data Warehouse environment Used Oracle Data Integrator Designer to develop processes for extracting cleansing transforming integrating and loading data into data warehouse database Experience in Developing and customizing PLSQL packages procedures functions triggers and reports using Oracle SQL Developer Responsible for designing developing and testing of the ETL strategy to populate the data from various source systems Flat files Oracle Worked with the Business units to identify data quality rule requirements against identified anomalies Develop Data Mapping Join and queries Validation and addressingfixing data queries raised by project team in a timely manner Worked closely with Business analyst and interacted with the Business users to gather new business requirements and to understand the accurate business and current requirements Created Repositories Agent Contexts and both of Physical Logical Schema in Topology Manager for all the source and target schemas Data mapping logical data modeling created class diagrams and ER diagrams and used SQL queries to filter data within the Oracle database Installed and Setup ODI Master Repository Work Repository Execution Repository Used Topology Manager to manage the data describing the information systems physical and logical architecture Extensively worked and utilized ODI Knowledge Modules Reverse Engineering Loading Integration Check Journalizing and service Created various procedures and variables Created ODI Packages Jobs of various complexities and automated process data flow Configured and setup ODI Master repository Work repository Project Models sources targets packages Knowledge Modules Interfaces Scenarios filters condition metadata Environment PLSQL Tableau Oracle MSOffice Teradata SQL Excel Apache Spark Hive Data mapping Python Systems Analyst Coramandel Infrastructure Pvt Ltd Hyderabad Telangana November 2010 to April 2011 Responsibilities Participate in fulllife cycle of SQL database development Create conceptual logical and physical database models to support project requirements Design implement and maintain databases for Corporate Data Finance and Operations business units Responsible for maintaining the integrity of the SQL database and reporting any issues to the database architect Assisted in creating and presenting informational reports to Management based on SQL data Build manage and maintain all project documentation including Business Requirements Documents BRD technical specifications process flows and clientspecific user guides Developed and validated conceptual data models including implementing logical and physical data mart data models Work closely with clients and internal teams to elicit and document business and functional requirements Delivered projects on time using Agile and Waterfall methodologies for timely completion of projects Environment SQL database Agile and WaterfallBRD SQL dataMSOfficeOracle Education Bachelor of Engineering in Engineering Malla Reddy College of Engineering and Technology May 2011 Skills Hadoop Hbase Hdfs Hive Mapreduce Additional Information TECHNICAL SKILLS Languages Java 8 Python R Python and R Numpy SciPy Pandas Scikitlearn Matplotlib Seaborn ggplot2 caret dplyr purrr readxl tidyr Rweka gmodels RCurl C50 twitter NLP Reshape2 rjson plyr Beautiful Soup Rpy2 Algorithms Kernel Density Estimation and Nonparametric Bayes Classifier KMeans Linear Regression Neighbors Nearest Farthest Range k Classification NonNegative Matrix Factorization Dimensionality Reduction Decision Tree Gaussian Processes Logistic Regression Nave Bayes Random Forest Ridge Regression Matrix FactorizationSVD NLPMachine LearningDeep Learning LDA Latent Dirichlet Allocation NLTK Apache OpenNLP Stanford NLP Sentiment Analysis SVMs ANN RNN CNN TensorFlow MXNet Caffe H2O Keras PyTorch Theano Azure ML Cloud Google Cloud Platform AWS Azure Bluemix Web Technologies JDBC HTML5 DHTML and XML CSS3 Web Services WSDL Data Modeling Tools Erwin r 96 95 91 8x Rational Rose ERStudio MS Visio SAP Power designer Big Data Technologies Hadoop Hive HDFS MapReduce Pig Kafka Databases SQL Hive Impala Pig Spark SQL Databases SQLServer My SQL MS Access HDFS HBase Teradata Netezza MongoDB Cassandra Reporting Tools MS Office WordExcelPower Point Visio Tableau Crystal reports XI Business Intelligence SSRS Business Objects 5x 6x Cognos7060 ETL Tools Informatica Power Centre SSIS Version Control Tools SVM GitHub BI Tools Tableau Tableau Server Tableau Reader SAP Business Objects OBIEE QlikView SAP Business Intelligence Amazon Redshift or Azure Data Warehouse Operating System Windows Linux Unix Macintosh HD Red Hat",
    "extracted_keywords": [
        "Sr",
        "Python",
        "Developer",
        "Srspan",
        "lPythonspan",
        "span",
        "lDeveloperspan",
        "Sr",
        "Python",
        "Developer",
        "Capital",
        "One",
        "Bank",
        "Plano",
        "TX",
        "years",
        "experience",
        "Python",
        "Developer",
        "Data",
        "Scientist",
        "proficient",
        "IT",
        "experience",
        "Python",
        "Data",
        "Analytics",
        "Statistical",
        "Modeling",
        "Visualization",
        "Machine",
        "Learning",
        "Deep",
        "REST",
        "API",
        "AWS",
        "C",
        "C",
        "SQL",
        "Wrote",
        "python",
        "scripts",
        "XML",
        "documents",
        "data",
        "database",
        "applications",
        "Python",
        "CSS",
        "HTML",
        "applications",
        "XML",
        "XSL",
        "PHP",
        "Django",
        "Python",
        "Rails",
        "Web",
        "Services",
        "Python",
        "programming",
        "language",
        "Experience",
        "Sub",
        "Queries",
        "Stored",
        "Procedures",
        "Triggers",
        "Cursors",
        "Functions",
        "MySQL",
        "PostgreSQL",
        "database",
        "data",
        "party",
        "spending",
        "data",
        "deliverables",
        "formats",
        "Excel",
        "macros",
        "python",
        "libraries",
        "Python",
        "IDEs",
        "PyCharm",
        "PyScripter",
        "Spyder",
        "PyStudio",
        "PyDev",
        "Good",
        "experience",
        "software",
        "development",
        "Python",
        "IDEs",
        "text",
        "Jupyter",
        "Notebook",
        "web",
        "applications",
        "development",
        "Django",
        "Python",
        "HTMLCSS",
        "serverside",
        "applications",
        "Anaconda",
        "Python",
        "Environment",
        "views",
        "templates",
        "Python",
        "Djangos",
        "controller",
        "templating",
        "language",
        "website",
        "interface",
        "Experience",
        "Data",
        "mining",
        "datasets",
        "Structured",
        "Unstructured",
        "data",
        "Data",
        "Acquisition",
        "Data",
        "Validation",
        "Predictive",
        "Data",
        "Visualization",
        "Experience",
        "data",
        "profiling",
        "data",
        "cleansing",
        "transformation",
        "data",
        "visualization",
        "R",
        "Python",
        "MongoDB",
        "database",
        "concepts",
        "transactions",
        "indexes",
        "Sharding",
        "replication",
        "schema",
        "Experience",
        "Sub",
        "Queries",
        "Stored",
        "Procedures",
        "Triggers",
        "Cursors",
        "Functions",
        "MySQL",
        "Agile",
        "Methodologies",
        "Scrum",
        "stories",
        "sprints",
        "experience",
        "Python",
        "environment",
        "data",
        "analytics",
        "data",
        "experience",
        "Python",
        "Beautiful",
        "Soup",
        "Numpy",
        "Scipy",
        "matplotlib",
        "pythontwitter",
        "Pandas",
        "MySQL",
        "database",
        "connectivity",
        "Strong",
        "Experience",
        "data",
        "technologies",
        "Apache",
        "Spark",
        "HDFS",
        "Hive",
        "MongoDB",
        "Hands",
        "experience",
        "Git",
        "Sound",
        "understanding",
        "Deep",
        "learning",
        "CNN",
        "RNN",
        "ANN",
        "reinforcement",
        "learning",
        "transfer",
        "foundations",
        "handson",
        "projects",
        "linear",
        "regression",
        "decision",
        "trees",
        "Support",
        "Vector",
        "Machines",
        "networks",
        "NLP",
        "ii",
        "dimensionality",
        "reduction",
        "recommender",
        "systems",
        "probability",
        "statistics",
        "experiment",
        "analysis",
        "confidence",
        "intervals",
        "AB",
        "testing",
        "algorithms",
        "data",
        "structures",
        "Hands",
        "experience",
        "design",
        "management",
        "visualization",
        "databases",
        "Oracle",
        "MySQL",
        "SQL",
        "Server",
        "Hadoop",
        "ecosystem",
        "Apache",
        "Spark",
        "framework",
        "Hive",
        "Pig",
        "Scoop",
        "Pyspark",
        "Experience",
        "data",
        "sets",
        "R",
        "packages",
        "tidyr",
        "reshape",
        "Caret",
        "data",
        "lattice",
        "ggplot2",
        "packages",
        "Experience",
        "dimensionality",
        "reduction",
        "techniques",
        "PCA",
        "LDA",
        "handson",
        "Boot",
        "camp",
        "Data",
        "Analytics",
        "course",
        "Statistics",
        "Programming",
        "data",
        "engineering",
        "data",
        "visualization",
        "machine",
        "learning",
        "programming",
        "R",
        "SQL",
        "Experience",
        "data",
        "analytics",
        "analysis",
        "Classification",
        "Regression",
        "Recommender",
        "Systems",
        "Good",
        "Exposure",
        "Factor",
        "Analysis",
        "Bagging",
        "algorithms",
        "Experience",
        "Descriptive",
        "Analysis",
        "Problems",
        "Frequent",
        "Pattern",
        "Mining",
        "Clustering",
        "Outlier",
        "Detection",
        "Machine",
        "Learning",
        "Classification",
        "Regression",
        "KNN",
        "Model",
        "Decision",
        "Tree",
        "Model",
        "Nave",
        "Bayes",
        "Model",
        "Logistic",
        "Regression",
        "SVM",
        "Model",
        "Latent",
        "Factor",
        "Model",
        "Handson",
        "experience",
        "Python",
        "Numpy",
        "Pandas",
        "Matplotlib",
        "Seaborn",
        "NLTK",
        "SciKit",
        "SciPy",
        "Expertise",
        "knowledge",
        "TensorFlow",
        "machine",
        "learningdeep",
        "package",
        "knowledge",
        "Microsoft",
        "Azure",
        "SQL",
        "Machine",
        "Learning",
        "HDInsight",
        "Good",
        "Knowledge",
        "Natural",
        "Language",
        "Processing",
        "NLP",
        "Time",
        "Series",
        "Analysis",
        "Forecasting",
        "ARIMA",
        "model",
        "Python",
        "R",
        "knowledge",
        "Tableau",
        "Power",
        "BI",
        "data",
        "visualizations",
        "Understanding",
        "NoSQL",
        "MongoDB",
        "HBase",
        "Amazon",
        "Web",
        "Services",
        "AWS",
        "Microsoft",
        "Azure",
        "AWS",
        "EC2",
        "S3",
        "RD3",
        "Azure",
        "HDInsight",
        "Machine",
        "Learning",
        "Studio",
        "Azure",
        "Data",
        "Lake",
        "experience",
        "knowledge",
        "clusters",
        "AWS",
        "cloud",
        "services",
        "EC2",
        "S3",
        "EMR",
        "exposure",
        "tables",
        "charts",
        "Excel",
        "Work",
        "Experience",
        "Sr",
        "Python",
        "Developer",
        "Capital",
        "One",
        "Bank",
        "Plano",
        "TX",
        "September",
        "Present",
        "Capital",
        "One",
        "Plano",
        "Texas",
        "office",
        "handling",
        "data",
        "source",
        "partner",
        "information",
        "US",
        "Texas",
        "Plano",
        "work",
        "data",
        "analytics",
        "development",
        "Work",
        "project",
        "company",
        "experience",
        "client",
        "requirement",
        "time",
        "company",
        "cloud",
        "technology",
        "work",
        "migration",
        "team",
        "system",
        "OnpermEDE",
        "SFG",
        "AWS",
        "Cloud",
        "technology",
        "part",
        "web",
        "development",
        "tool",
        "Django",
        "Flask",
        "Machine",
        "workflows",
        "Python",
        "scripts",
        "Unix",
        "shell",
        "scripting",
        "Create",
        "activate",
        "program",
        "Anaconda",
        "environment",
        "Use",
        "Python",
        "unit",
        "testing",
        "modules",
        "unit",
        "test",
        "custom",
        "frameworks",
        "Agile",
        "Software",
        "Development",
        "methodologies",
        "Develop",
        "Sqoop",
        "scripts",
        "change",
        "data",
        "capture",
        "records",
        "data",
        "tables",
        "Installed",
        "Hadoop",
        "Map",
        "HDFS",
        "AWS",
        "MapReduce",
        "jobs",
        "PIG",
        "Hive",
        "data",
        "Manage",
        "datasets",
        "Panda",
        "data",
        "frames",
        "MySQL",
        "MYSQL",
        "database",
        "python",
        "PythonMySQL",
        "connector",
        "MySQL",
        "package",
        "information",
        "WebApplication",
        "development",
        "Python",
        "HTML5",
        "CSS3",
        "AJAX",
        "JSON",
        "Jquery",
        "Develop",
        "features",
        "dashboard",
        "Python",
        "Java",
        "Bootstrap",
        "CSS",
        "JavaScript",
        "Jquery",
        "Python",
        "Django",
        "data",
        "users",
        "PyTest",
        "test",
        "cases",
        "SQL",
        "queries",
        "Functions",
        "Cursors",
        "Triggers",
        "client",
        "requirements",
        "data",
        "party",
        "spending",
        "data",
        "deliverables",
        "format",
        "Excel",
        "macros",
        "python",
        "libraries",
        "NumPy",
        "SQLAlchemy",
        "Pandas",
        "API",
        "data",
        "time",
        "series",
        "format",
        "manipulation",
        "retrieval",
        "data",
        "migration",
        "server",
        "Jira",
        "database",
        "Matching",
        "Fields",
        "Python",
        "scripts",
        "information",
        "Analyze",
        "Format",
        "data",
        "Machine",
        "Learning",
        "algorithm",
        "Python",
        "ScikitLearn",
        "Experience",
        "python",
        "Jupyter",
        "Scientific",
        "computing",
        "stack",
        "numpy",
        "pandasand",
        "Perform",
        "troubleshooting",
        "Python",
        "bug",
        "fixes",
        "applications",
        "source",
        "data",
        "customers",
        "customer",
        "service",
        "team",
        "Python",
        "scripts",
        "documents",
        "data",
        "database",
        "capacity",
        "planning",
        "reports",
        "Python",
        "packages",
        "Numpy",
        "matplotlib",
        "logs",
        "occurrence",
        "event",
        "Python",
        "page",
        "application",
        "Angular",
        "JS",
        "MongoDB",
        "NodeJS",
        "Design",
        "databases",
        "Python",
        "Python",
        "API",
        "RESTful",
        "Web",
        "Service",
        "Flask",
        "SQL",
        "Alchemy",
        "PostgreSQL",
        "Manage",
        "code",
        "GitHub",
        "Bit",
        "Bucket",
        "deployment",
        "staging",
        "production",
        "servers",
        "MVC",
        "architecture",
        "web",
        "application",
        "help",
        "Django",
        "framework",
        "Use",
        "Celery",
        "task",
        "queue",
        "Redis",
        "broker",
        "tasks",
        "Design",
        "API",
        "system",
        "deployment",
        "http",
        "server",
        "Amazon",
        "AWS",
        "architecture",
        "Develop",
        "integration",
        "party",
        "platforms",
        "web",
        "services",
        "implementation",
        "Apache",
        "Spark",
        "Spark",
        "Streaming",
        "applications",
        "scale",
        "data",
        "graphs",
        "business",
        "decision",
        "Python",
        "mat",
        "plotlib",
        "library",
        "development",
        "Web",
        "Services",
        "SOAP",
        "data",
        "interface",
        "XML",
        "format",
        "Export",
        "Test",
        "case",
        "Scripts",
        "selenium",
        "scripts",
        "Selenium",
        "environment",
        "UNIX",
        "shell",
        "scripting",
        "automation",
        "views",
        "templates",
        "Django",
        "controller",
        "template",
        "Language",
        "website",
        "interface",
        "Create",
        "Individual",
        "Docker",
        "file",
        "deployment",
        "DevOps",
        "Teams",
        "changes",
        "Used",
        "JavaScript",
        "JSON",
        "portion",
        "webpage",
        "Develop",
        "features",
        "Django",
        "HTML",
        "Test",
        "Driven",
        "Development",
        "TDD",
        "Developed",
        "Python",
        "web",
        "services",
        "JSON",
        "Data",
        "layer",
        "speed",
        "search",
        "indexes",
        "Django",
        "ORM",
        "optimizations",
        "module",
        "Django",
        "ORM",
        "queries",
        "data",
        "number",
        "databases",
        "queries",
        "amount",
        "data",
        "Environment",
        "Python",
        "Django",
        "HTML5CSS",
        "PostgreSQL",
        "AWS",
        "AWSRDS",
        "Aurora",
        "Oracle",
        "MySQL",
        "JavaScript",
        "Jupyter",
        "Notebook",
        "VIM",
        "Pycharm",
        "Shell",
        "Scripting",
        "JIRA",
        "Python",
        "DeveloperData",
        "Scientist",
        "Blue",
        "Shield",
        "California",
        "Health",
        "Care",
        "Insurance",
        "Boston",
        "MA",
        "March",
        "September",
        "Python",
        "DeveloperData",
        "Scientist",
        "Blue",
        "Shield",
        "California",
        "Health",
        "Care",
        "Insurance",
        "technologies",
        "datasets",
        "opportunities",
        "people",
        "health",
        "issues",
        "Enterprise",
        "Shared",
        "Services",
        "group",
        "group",
        "Blue",
        "Cross",
        "Blue",
        "Shield",
        "Massachusetts",
        "services",
        "consumers",
        "Disease",
        "Prediction",
        "years",
        "BCBS",
        "health",
        "lifestyle",
        "details",
        "patients",
        "details",
        "age",
        "gender",
        "health",
        "parameters",
        "hypertension",
        "body",
        "mass",
        "index",
        "lifestyle",
        "variables",
        "occupation",
        "type",
        "alcohol",
        "consumption",
        "project",
        "probability",
        "diseases",
        "patients",
        "doctors",
        "approach",
        "health",
        "screenings",
        "EHR",
        "EMR",
        "Healthcare",
        "Analytics",
        "project",
        "data",
        "EMR",
        "Electronic",
        "Medical",
        "Records",
        "EHR",
        "Electronic",
        "Health",
        "Records",
        "disease",
        "trends",
        "patients",
        "diseases",
        "patients",
        "Diagnosis",
        "code",
        "goal",
        "patterns",
        "diseases",
        "conditions",
        "Hyper",
        "Tension",
        "Diabetes",
        "states",
        "prescriptionmedication",
        "patients",
        "problems",
        "sideeffects",
        "Responsibilities",
        "data",
        "tables",
        "Hive",
        "queries",
        "data",
        "analysis",
        "statistics",
        "anomalies",
        "duplicates",
        "values",
        "Analyze",
        "Data",
        "Performed",
        "Data",
        "Preparation",
        "model",
        "data",
        "AZUREML",
        "users",
        "data",
        "Python",
        "Django",
        "forms",
        "test",
        "case",
        "Pytest",
        "Django",
        "frameworks",
        "Python",
        "webpages",
        "Python",
        "Django",
        "Forms",
        "data",
        "users",
        "Application",
        "machine",
        "algorithms",
        "modeling",
        "decision",
        "trees",
        "text",
        "analytics",
        "language",
        "processing",
        "NLP",
        "regression",
        "models",
        "networks",
        "SVM",
        "Volume",
        "package",
        "python",
        "Matlab",
        "website",
        "functionality",
        "Flask",
        "framework",
        "Python",
        "web",
        "application",
        "logic",
        "Develop",
        "SparkScala",
        "Python",
        "R",
        "expression",
        "regex",
        "project",
        "environment",
        "LinuxWindows",
        "data",
        "resources",
        "clustering",
        "technique",
        "KMeans",
        "outliers",
        "data",
        "Evaluate",
        "models",
        "Cross",
        "Validation",
        "Log",
        "loss",
        "function",
        "ROC",
        "curves",
        "AUC",
        "feature",
        "selection",
        "technologies",
        "ElasticSearch",
        "Kibana",
        "Work",
        "NLTK",
        "library",
        "data",
        "processing",
        "patterns",
        "Categorize",
        "comments",
        "clusters",
        "networking",
        "sites",
        "Sentiment",
        "Analysis",
        "Text",
        "Analytics",
        "Ensure",
        "model",
        "False",
        "Positive",
        "Rate",
        "Text",
        "classification",
        "sentiment",
        "analysis",
        "data",
        "algorithm",
        "regularization",
        "methods",
        "L2",
        "L1",
        "Use",
        "Principal",
        "Component",
        "Analysis",
        "feature",
        "engineering",
        "data",
        "design",
        "reports",
        "metrics",
        "conclusions",
        "behavior",
        "Perform",
        "Multinomial",
        "Logistic",
        "Regression",
        "Random",
        "forest",
        "Decision",
        "Tree",
        "SVM",
        "package",
        "time",
        "route",
        "models",
        "Logistic",
        "Regression",
        "Random",
        "Forest",
        "GradientBoost",
        "Trees",
        "die",
        "test",
        "Perform",
        "data",
        "analysis",
        "Hive",
        "data",
        "Hadoop",
        "cluster",
        "SQL",
        "data",
        "database",
        "ETL",
        "data",
        "transformation",
        "Use",
        "MLlib",
        "Sparks",
        "Machine",
        "library",
        "models",
        "Perform",
        "Data",
        "Cleaning",
        "features",
        "engineering",
        "pandas",
        "packages",
        "python",
        "Communicate",
        "results",
        "operations",
        "team",
        "decisions",
        "data",
        "needs",
        "requirements",
        "departments",
        "Environment",
        "Python",
        "R",
        "HDFS",
        "Hadoop",
        "Hive",
        "Linux",
        "Spark",
        "IBM",
        "SPSS",
        "Tableau",
        "Desktop",
        "SQL",
        "Server",
        "Microsoft",
        "Excel",
        "Matlab",
        "Spark",
        "SQL",
        "Pyspark",
        "Data",
        "Scientist",
        "Bank",
        "West",
        "San",
        "Francisco",
        "CA",
        "January",
        "February",
        "Bank",
        "West",
        "services",
        "company",
        "San",
        "Francisco",
        "California",
        "subsidiary",
        "BNP",
        "Paribas",
        "branches",
        "offices",
        "Midwest",
        "Western",
        "United",
        "States",
        "Credit",
        "History",
        "Predictive",
        "Modeling",
        "project",
        "machine",
        "model",
        "credit",
        "default",
        "data",
        "probability",
        "classification",
        "defaulter",
        "predictability",
        "Factors",
        "Education",
        "Level",
        "Marital",
        "Status",
        "Payment",
        "History",
        "Income",
        "classification",
        "goal",
        "bank",
        "loanportfolio",
        "risk",
        "network",
        "regression",
        "model",
        "classification",
        "Logistic",
        "Regression",
        "Random",
        "Forest",
        "Model",
        "Deep",
        "Neural",
        "Network",
        "Models",
        "Loan",
        "Payment",
        "Default",
        "Prediction",
        "classification",
        "models",
        "features",
        "customer",
        "demographics",
        "macroeconomic",
        "payment",
        "behavior",
        "type",
        "size",
        "loan",
        "credit",
        "scores",
        "loan",
        "ratios",
        "accuracy",
        "accuracy",
        "model",
        "likelihood",
        "loan",
        "default",
        "conditions",
        "Responsibilities",
        "Gathered",
        "application",
        "requirements",
        "data",
        "models",
        "standardization",
        "documentation",
        "adoption",
        "standards",
        "practices",
        "data",
        "applications",
        "data",
        "Amazon",
        "Redshift",
        "sample",
        "patterns",
        "data",
        "quality",
        "issues",
        "insights",
        "BI",
        "team",
        "phase",
        "Pandas",
        "data",
        "feature",
        "engineering",
        "features",
        "dataset",
        "minority",
        "label",
        "class",
        "majority",
        "label",
        "class",
        "data",
        "exploration",
        "stage",
        "correlation",
        "analysis",
        "techniques",
        "insights",
        "claim",
        "data",
        "machine",
        "techniques",
        "markets",
        "customers",
        "recommendations",
        "management",
        "increase",
        "customer",
        "base",
        "customer",
        "portfolio",
        "Build",
        "mulitlayers",
        "Neural",
        "Networks",
        "Deep",
        "Learning",
        "Tensorflow",
        "Keras",
        "Perform",
        "hyperparameter",
        "Cross",
        "Validation",
        "Spark",
        "computation",
        "process",
        "Export",
        "models",
        "Protobuf",
        "Tensorflow",
        "Serving",
        "integration",
        "job",
        "clients",
        "application",
        "customer",
        "master",
        "data",
        "identification",
        "business",
        "business",
        "needs",
        "client",
        "relationships",
        "opportunities",
        "crossselling",
        "products",
        "customers",
        "products",
        "fraud",
        "prediction",
        "performance",
        "forest",
        "gradient",
        "feature",
        "selection",
        "Python",
        "Scikitlearn",
        "classification",
        "algorithms",
        "Logistic",
        "Regression",
        "Gradient",
        "Boosting",
        "Random",
        "Forest",
        "Pandas",
        "Scikitlearn",
        "performance",
        "data",
        "governance",
        "team",
        "data",
        "models",
        "Metadata",
        "models",
        "multivariate",
        "regression",
        "regression",
        "Random",
        "forests",
        "decision",
        "trees",
        "analysis",
        "modeling",
        "techniques",
        "customer",
        "behavior",
        "products",
        "delinquency",
        "rate",
        "default",
        "rate",
        "Lead",
        "default",
        "rates",
        "Application",
        "machine",
        "algorithms",
        "modeling",
        "decision",
        "trees",
        "regression",
        "models",
        "networks",
        "SVM",
        "Volume",
        "package",
        "python",
        "Matlab",
        "model",
        "AWS",
        "EC2",
        "algorithm",
        "parameters",
        "data",
        "pipeline",
        "consistency",
        "training",
        "data",
        "data",
        "model",
        "AWS",
        "Lambda",
        "team",
        "business",
        "solutions",
        "feedback",
        "deployment",
        "model",
        "performance",
        "flaws",
        "methodology",
        "weather",
        "peril",
        "zone",
        "relativities",
        "algorithm",
        "kmeans",
        "Monte",
        "Carlo",
        "methods",
        "groups",
        "customers",
        "pricing",
        "algorithm",
        "clustering",
        "customer",
        "segmentation",
        "profits",
        "summary",
        "trending",
        "reports",
        "Tableau",
        "Desktop",
        "Environment",
        "Python",
        "R",
        "HDFS",
        "Hadoop",
        "Hive",
        "Linux",
        "Spark",
        "IBM",
        "SPSS",
        "Tableau",
        "Desktop",
        "SQL",
        "Server",
        "Microsoft",
        "Excel",
        "Matlab",
        "Spark",
        "SQL",
        "Pyspark",
        "Data",
        "ScientistData",
        "Analyst",
        "Dicks",
        "Sporting",
        "Goods",
        "Inc",
        "Pittsburgh",
        "PA",
        "November",
        "December",
        "Dicks",
        "Sporting",
        "Goods",
        "Inc",
        "Dicks",
        "Fortune",
        "sporting",
        "goods",
        "retailing",
        "corporation",
        "Coraopolis",
        "Pennsylvania",
        "Greater",
        "Pittsburgh",
        "Dicks",
        "stores",
        "states",
        "Eastern",
        "United",
        "States",
        "company",
        "Golf",
        "Galaxy",
        "Inc",
        "golf",
        "specialty",
        "retailer",
        "stores",
        "states",
        "Responsibilities",
        "Modelling",
        "ML",
        "Insights",
        "Data",
        "guidance",
        "Principal",
        "Data",
        "Scientist",
        "Data",
        "Pig",
        "Hive",
        "Impala",
        "Ingestion",
        "Sqoop",
        "Flume",
        "SVN",
        "Changes",
        "EMM",
        "application",
        "trunk",
        "Understanding",
        "implementation",
        "text",
        "mining",
        "concepts",
        "graph",
        "processing",
        "data",
        "processing",
        "API",
        "Hadoop",
        "Impala",
        "Connection",
        "SQL",
        "data",
        "API",
        "calls",
        "Microsoft",
        "API",
        "grip",
        "Cloudera",
        "HDP",
        "ecosystem",
        "components",
        "ElasticSearch",
        "Big",
        "Data",
        "data",
        "application",
        "Performed",
        "Map",
        "Reduce",
        "Programs",
        "cluster",
        "MapReduce",
        "jobs",
        "java",
        "data",
        "machine",
        "solutions",
        "computation",
        "framework",
        "eg",
        "Hadoop",
        "Spark",
        "Storm",
        "data",
        "metrics",
        "loading",
        "data",
        "RDBMS",
        "web",
        "logs",
        "HDFS",
        "Sqoop",
        "Flume",
        "data",
        "MySQL",
        "HBase",
        "Sqoop",
        "Developed",
        "Hive",
        "Analysis",
        "banners",
        "data",
        "Twitter",
        "Java",
        "Twitter",
        "API",
        "twitter",
        "data",
        "database",
        "Amazon",
        "EC2",
        "Cloud",
        "Instances",
        "Amazon",
        "Images",
        "Linux",
        "Ubuntu",
        "Configuring",
        "instances",
        "respect",
        "applications",
        "robustness",
        "result",
        "Hive",
        "MySQL",
        "Sqoop",
        "data",
        "data",
        "Hive",
        "queries",
        "Pig",
        "scripts",
        "customer",
        "behavior",
        "hands",
        "experience",
        "Sequence",
        "files",
        "AVRO",
        "HAR",
        "file",
        "formats",
        "compression",
        "Hive",
        "partition",
        "bucket",
        "data",
        "Experience",
        "MapReduce",
        "programs",
        "Java",
        "API",
        "Structured",
        "data",
        "Wrote",
        "Pig",
        "Scripts",
        "ETL",
        "procedures",
        "data",
        "HDFS",
        "Created",
        "HBase",
        "data",
        "formats",
        "data",
        "portfolios",
        "performance",
        "Pig",
        "Hive",
        "Queries",
        "Environment",
        "SQLServer",
        "Oracle",
        "9i",
        "MSOffice",
        "Teradata",
        "Informatica",
        "ER",
        "Studio",
        "XML",
        "Business",
        "HDFS",
        "Teradata",
        "JSON",
        "HADOOP",
        "HDFS",
        "MapReduce",
        "PIG",
        "Spark",
        "R",
        "Studio",
        "MAHOUT",
        "HIVE",
        "AWS",
        "Data",
        "AnalystData",
        "Scientist",
        "IDBI",
        "Bank",
        "Mumbai",
        "Maharashtra",
        "June",
        "January",
        "Responsibilities",
        "data",
        "data",
        "sources",
        "areas",
        "data",
        "accuracy",
        "integrity",
        "updates",
        "data",
        "need",
        "SQL",
        "Python",
        "Expertise",
        "SQL",
        "Excel",
        "Tableau",
        "data",
        "analyses",
        "data",
        "business",
        "problems",
        "techniques",
        "analyses",
        "models",
        "multivariate",
        "regression",
        "regression",
        "Random",
        "forests",
        "decision",
        "trees",
        "Pandas",
        "Numpy",
        "Seaborn",
        "Scikitlearn",
        "Python",
        "machine",
        "learning",
        "Build",
        "models",
        "language",
        "NLP",
        "machine",
        "learning",
        "insights",
        "data",
        "computing",
        "technologies",
        "Apache",
        "Spark",
        "Hive",
        "analysis",
        "modeling",
        "techniques",
        "customer",
        "behavior",
        "products",
        "delinquency",
        "rate",
        "default",
        "rate",
        "Lead",
        "default",
        "rates",
        "machine",
        "techniques",
        "markets",
        "customers",
        "recommendations",
        "management",
        "increase",
        "customer",
        "base",
        "customer",
        "portfolio",
        "customer",
        "master",
        "data",
        "identification",
        "business",
        "business",
        "needs",
        "client",
        "relationships",
        "opportunities",
        "crossselling",
        "products",
        "customers",
        "products",
        "business",
        "partners",
        "problems",
        "goals",
        "modeling",
        "analysis",
        "data",
        "reports",
        "performance",
        "metrics",
        "design",
        "development",
        "data",
        "warehouse",
        "business",
        "metrics",
        "organization",
        "data",
        "quality",
        "validation",
        "rules",
        "health",
        "data",
        "Dashboard",
        "development",
        "experience",
        "Tableau",
        "ETL",
        "Developer",
        "Responsibilities",
        "SDLC",
        "BI",
        "Project",
        "Data",
        "Analysis",
        "Designing",
        "Development",
        "Data",
        "Warehouse",
        "environment",
        "Oracle",
        "Data",
        "Integrator",
        "Designer",
        "processes",
        "cleansing",
        "loading",
        "data",
        "data",
        "warehouse",
        "database",
        "Experience",
        "Developing",
        "PLSQL",
        "packages",
        "procedures",
        "functions",
        "triggers",
        "reports",
        "Oracle",
        "SQL",
        "Developer",
        "Responsible",
        "testing",
        "ETL",
        "strategy",
        "data",
        "source",
        "systems",
        "files",
        "Oracle",
        "Worked",
        "Business",
        "units",
        "data",
        "quality",
        "rule",
        "requirements",
        "anomalies",
        "Develop",
        "Data",
        "Mapping",
        "Join",
        "Validation",
        "data",
        "queries",
        "project",
        "team",
        "manner",
        "Business",
        "analyst",
        "Business",
        "users",
        "business",
        "requirements",
        "business",
        "requirements",
        "Repositories",
        "Agent",
        "Contexts",
        "Physical",
        "Logical",
        "Schema",
        "Topology",
        "Manager",
        "source",
        "schemas",
        "Data",
        "mapping",
        "data",
        "class",
        "diagrams",
        "ER",
        "diagrams",
        "SQL",
        "queries",
        "data",
        "Oracle",
        "database",
        "Setup",
        "ODI",
        "Master",
        "Repository",
        "Work",
        "Repository",
        "Execution",
        "Repository",
        "Topology",
        "Manager",
        "data",
        "information",
        "systems",
        "architecture",
        "ODI",
        "Knowledge",
        "Modules",
        "Reverse",
        "Engineering",
        "Loading",
        "Integration",
        "Check",
        "Journalizing",
        "service",
        "procedures",
        "variables",
        "ODI",
        "Packages",
        "Jobs",
        "complexities",
        "process",
        "data",
        "Configured",
        "ODI",
        "Master",
        "repository",
        "Work",
        "repository",
        "Project",
        "Models",
        "sources",
        "packages",
        "Knowledge",
        "Modules",
        "Interfaces",
        "Scenarios",
        "condition",
        "metadata",
        "Environment",
        "PLSQL",
        "Tableau",
        "Oracle",
        "MSOffice",
        "Teradata",
        "SQL",
        "Excel",
        "Apache",
        "Spark",
        "Hive",
        "Data",
        "mapping",
        "Python",
        "Systems",
        "Analyst",
        "Coramandel",
        "Infrastructure",
        "Pvt",
        "Ltd",
        "Hyderabad",
        "Telangana",
        "November",
        "April",
        "Responsibilities",
        "fulllife",
        "cycle",
        "SQL",
        "database",
        "development",
        "database",
        "models",
        "project",
        "requirements",
        "Design",
        "databases",
        "Corporate",
        "Data",
        "Finance",
        "Operations",
        "business",
        "units",
        "integrity",
        "SQL",
        "database",
        "issues",
        "database",
        "architect",
        "reports",
        "Management",
        "SQL",
        "data",
        "Build",
        "manage",
        "project",
        "documentation",
        "Business",
        "Requirements",
        "Documents",
        "BRD",
        "specifications",
        "process",
        "flows",
        "user",
        "data",
        "models",
        "data",
        "mart",
        "data",
        "models",
        "clients",
        "teams",
        "document",
        "business",
        "requirements",
        "projects",
        "time",
        "Agile",
        "Waterfall",
        "methodologies",
        "completion",
        "projects",
        "Environment",
        "SQL",
        "database",
        "Agile",
        "WaterfallBRD",
        "SQL",
        "dataMSOfficeOracle",
        "Education",
        "Bachelor",
        "Engineering",
        "Engineering",
        "Malla",
        "Reddy",
        "College",
        "Engineering",
        "Technology",
        "May",
        "Skills",
        "Hadoop",
        "Hbase",
        "Hdfs",
        "Hive",
        "Mapreduce",
        "Additional",
        "Information",
        "TECHNICAL",
        "SKILLS",
        "Languages",
        "Java",
        "Python",
        "R",
        "Python",
        "R",
        "Numpy",
        "SciPy",
        "Pandas",
        "Scikitlearn",
        "Matplotlib",
        "Seaborn",
        "ggplot2",
        "dplyr",
        "purrr",
        "readxl",
        "tidyr",
        "Rweka",
        "RCurl",
        "C50",
        "twitter",
        "NLP",
        "Reshape2",
        "rjson",
        "plyr",
        "Beautiful",
        "Soup",
        "Rpy2",
        "Algorithms",
        "Kernel",
        "Density",
        "Estimation",
        "Nonparametric",
        "Bayes",
        "Classifier",
        "KMeans",
        "Linear",
        "Regression",
        "Neighbors",
        "Nearest",
        "Farthest",
        "Range",
        "k",
        "Classification",
        "NonNegative",
        "Matrix",
        "Factorization",
        "Dimensionality",
        "Reduction",
        "Decision",
        "Tree",
        "Gaussian",
        "Processes",
        "Logistic",
        "Regression",
        "Nave",
        "Bayes",
        "Random",
        "Forest",
        "Ridge",
        "Regression",
        "Matrix",
        "FactorizationSVD",
        "NLPMachine",
        "LearningDeep",
        "Learning",
        "LDA",
        "Latent",
        "Dirichlet",
        "Allocation",
        "NLTK",
        "Apache",
        "OpenNLP",
        "Stanford",
        "NLP",
        "Sentiment",
        "Analysis",
        "SVMs",
        "ANN",
        "RNN",
        "CNN",
        "TensorFlow",
        "MXNet",
        "Caffe",
        "H2O",
        "Keras",
        "PyTorch",
        "Theano",
        "Azure",
        "ML",
        "Cloud",
        "Google",
        "Cloud",
        "Platform",
        "AWS",
        "Azure",
        "Bluemix",
        "Web",
        "Technologies",
        "JDBC",
        "HTML5",
        "DHTML",
        "XML",
        "CSS3",
        "Web",
        "Services",
        "WSDL",
        "Data",
        "Modeling",
        "Tools",
        "Erwin",
        "r",
        "8x",
        "Rational",
        "Rose",
        "ERStudio",
        "MS",
        "Visio",
        "SAP",
        "Power",
        "designer",
        "Big",
        "Data",
        "Technologies",
        "Hadoop",
        "Hive",
        "HDFS",
        "MapReduce",
        "Pig",
        "Kafka",
        "SQL",
        "Hive",
        "Impala",
        "Pig",
        "Spark",
        "SQL",
        "Databases",
        "SQLServer",
        "SQL",
        "MS",
        "Access",
        "HDFS",
        "HBase",
        "Teradata",
        "Netezza",
        "MongoDB",
        "Cassandra",
        "Reporting",
        "Tools",
        "MS",
        "Office",
        "WordExcelPower",
        "Point",
        "Visio",
        "Tableau",
        "Crystal",
        "XI",
        "Business",
        "Intelligence",
        "SSRS",
        "Business",
        "5x",
        "Cognos7060",
        "ETL",
        "Tools",
        "Informatica",
        "Power",
        "Centre",
        "SSIS",
        "Version",
        "Control",
        "Tools",
        "SVM",
        "GitHub",
        "BI",
        "Tools",
        "Tableau",
        "Tableau",
        "Server",
        "Tableau",
        "Reader",
        "SAP",
        "Business",
        "OBIEE",
        "QlikView",
        "SAP",
        "Business",
        "Intelligence",
        "Amazon",
        "Redshift",
        "Azure",
        "Data",
        "Warehouse",
        "Operating",
        "System",
        "Windows",
        "Linux",
        "Unix",
        "Macintosh",
        "HD",
        "Red",
        "Hat"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T22:38:49.639430",
    "resume_data": "Sr Python Developer Srspan lPythonspan span lDeveloperspan Sr Python Developer Capital One Bank Plano TX Around 8 years of experience as a Python Developer and Data Scientist proficient IT professional with experience in Python Data Analytics Statistical Modeling Visualization Machine Learning and Deep learning REST API AWS C C and SQL Wrote python scripts to parse XML documents and load the data in database and developed webbased applications using Python CSS and HTML Worked on applications and developed them with XML JSON XSL PHP Django Python Rails Experienced in developing Web Services with Python programming language Experience in writing Sub Queries Stored Procedures Triggers Cursors and Functions on MySQL and PostgreSQL database Cleaned data and processed third party spending data into maneuverable deliverables within specific formats with Excel macros and python libraries Experienced in working with various Python IDEs using PyCharm PyScripter Spyder PyStudio and PyDev Good experience of software development in Python and IDEs pycharm sublime text Jupyter Notebook Experienced in web applications development using Django Python using HTMLCSS for serverside rendered applications Worked on Anaconda Python Environment Developed views and templates with Python and Djangos view controller and templating language to create a userfriendly website interface Experience in Data mining with large datasets of Structured and Unstructured data Data Acquisition Data Validation Predictive modeling Data Visualization Experience in integrating data profiling validating and data cleansing transformation and data visualization using R and Python Worked on MongoDB database concepts such as locking transactions indexes Sharding replication schema design Experience in writing Sub Queries Stored Procedures Triggers Cursors and Functions in MySQL Experienced in Agile Methodologies Scrum stories and sprints experience in a Python based environment along with data analytics data wrangling Good experience in using various Python libraries Beautiful Soup Numpy Scipy matplotlib pythontwitter Pandas MySQL dB for database connectivity Strong Experience in Big data technologies including Apache Spark HDFS Hive MongoDB Hands on experience of Git Sound understanding of Deep learning using CNN RNN ANN reinforcement learning transfer learning Theoretical foundations and practical handson projects related to i supervised learning linear and logistic regression boosted decision trees Support Vector Machines neural networks NLP ii unsupervised learning clustering dimensionality reduction recommender systems iii probability statistics experiment analysis confidence intervals AB testing iv algorithms and data structures Hands on experience in design management and visualization of databases using Oracle MySQL and SQL Server Experienced in Hadoop 2x ecosystem and Apache Spark 2x framework such as Hive Pig Scoop Pyspark Experience in manipulating the large data sets with R packages like tidyr tidyverse dplyr reshape lubridate Caret and visualizing the data using lattice and ggplot2 packages Experience in dimensionality reduction using techniques like PCA and LDA Intensive handson Boot camp on Data Analytics course spanning from Statistics to Programming including data engineering data visualization machine learning and programming in R SQL Experience in data analytics predictive analysis like Classification Regression Recommender Systems Good Exposure with Factor Analysis Bagging and Boosting algorithms Experience in Descriptive Analysis Problems like Frequent Pattern Mining Clustering Outlier Detection Worked on Machine Learning algorithms like Classification and Regression with KNN Model Decision Tree Model Nave Bayes Model Logistic Regression SVM Model and Latent Factor Model Handson experience on Python and libraries like Numpy Pandas Matplotlib Seaborn NLTK SciKit learn SciPy Expertise and knowledge in TensorFlow to do machine learningdeep learning package in python Good knowledge on Microsoft Azure SQL Machine Learning and HDInsight Good Knowledge on Natural Language Processing NLP and Time Series Analysis and Forecasting using ARIMA model in Python and R Good knowledge in Tableau Power BI for interactive data visualizations Indepth Understanding in NoSQL databases like MongoDB HBase Experienced in Amazon Web Services AWS and Microsoft Azure such as AWS EC2 S3 RD3 Azure HDInsight Machine Learning Studio Azure Data Lake Very good experience and knowledge in provisioning virtual clusters under AWS cloud which includes services like EC2 S3 and EMR Good exposure in creating pivot tables and charts in Excel Work Experience Sr Python Developer Capital One Bank Plano TX September 2018 to Present Capital One in Plano Texas second main office handling data source and their partner information in US based in Texas Plano and its principal work on data analytics and development Work has been assigned on multiple project with this company based on experience and client requirement to fulfill in given time As most of company moving toward cloud technology my work is mostly migration team to automate system from OnpermEDE to SFG AWS Cloud with latest technology Also part of internal web portal development using python powerful tool like pandas Django and Flask and Machine learning Automate different workflows which are initiated manually with Python scripts and Unix shell scripting Create activate and program in Anaconda environment Use Python unit and functional testing modules such as unit test unittest2 mock and custom frameworks inline with Agile Software Development methodologies Develop Sqoop scripts to handle change data capture for processing incremental records between new arrived and existing data in RDBMS tables Installed Hadoop Map Reduce HDFS AWS and developed multiple MapReduce jobs in PIG and Hive for data cleaning and preprocessing Manage datasets using Panda data frames and MySQL queried MYSQL database queries from python using PythonMySQL connector and MySQL dB package to retrieve information Involved in the WebApplication development using Python 35 HTML5 CSS3 AJAX JSON and Jquery Develop and tested many features for dashboard using Python Java Bootstrap CSS JavaScript and Jquery Generated Python Django forms to record data of online users and used PyTest for writing test cases Implemented and modified various SQL queries and Functions Cursors and Triggers as per the client requirements Clean data and processed third party spending data into maneuverable deliverables within specific format with Excel macros and python libraries such as NumPy SQLAlchemy and matplotlib Used Pandas as API to put the data as time series and tabular format for manipulation and retrieval of data Helped with the migration from the old server to Jira database Matching Fields with Python scripts for transferring and verifying the information Analyze Format data using Machine Learning algorithm by Python ScikitLearn Experience in python Jupyter Scientific computing stack numpy scipy pandasand matplotlib Perform troubleshooting fixed and deployed many Python bug fixes of the two main applications that were a main source of data for both customers and internal customer service team Write Python scripts to parse JSON documents and load the data in database Generating various capacity planning reports graphical using Python packages like Numpy matplotlib Analyzing various logs that are been generating and predictingforecasting next occurrence of event with various Python libraries Developed single page application by using Angular JS backed by MongoDB and NodeJS Design and maintain databases using Python and developed Python based API RESTful Web Service using Flask SQL Alchemy and PostgreSQL Manage code versioning with GitHub Bit Bucket and deployment to staging and production servers and implement MVC architecture in developing the web application with the help of Django framework Use Celery as task queue and RabbitMQ Redis as messaging broker to execute asynchronous tasks Design and manage API system deployment using fast http server and Amazon AWS architecture Develop remote integration with third party platforms by using RESTful web services and Successful implementation of Apache Spark and Spark Streaming applications for large scale data Built various graphs for business decision making using Python mat plotlib library Involved in development of Web Services using SOAP for sending and getting data from the external interface in the XML format Export Test case Scripts and modified the selenium scripts and executed in Selenium environment Wrote UNIX shell scripting for automation Developed views and templates with Django view controller and template Language to create a userfriendly website interface Create Individual Docker file for deployment for DevOps Teams whenever appropriate changes made Used JavaScript and JSON to update a portion of a webpage Develop consumerbased features using Django HTML and Test Driven Development TDD Developed Python web services for processing JSON and interfacing with the Data layer Increased the speed of preexisting search indexes through Django ORM optimizations Developed module to build Django ORM queries that can preload data to greatly reduce the number of databases queries needed to retrieve the same amount of data Environment Python 3 Django HTML5CSS PostgreSQL AWS AWSRDS Aurora Oracle MySQL JavaScript Jupyter Notebook VIM Pycharm Shell Scripting AngularJS JIRA Python DeveloperData Scientist Blue Shield of California Health Care Insurance Boston MA March 2017 to September 2018 As a Python DeveloperData Scientist at Blue Shield of California Health Care Insurance working on latest in distributed computing technologies and operating across massive datasets to unlock the big opportunities that help everyday people prescreen from possible future health issues The Enterprise Shared Services group is the wellrecognized group within Blue Cross Blue Shield of Massachusetts which provides the services to many consumers Disease Prediction Over the last few years BCBS has captured several health demographic and lifestyle details about patients This includes details such as age and gender along with several health parameters such as hypertension body mass index and lifestyle related variables like smoking occupation type alcohol consumption etc The project focuses on predicting the probability of certain diseases happening which can help make patients and doctors take a proactive approach towards certain health screenings EHR and EMR Healthcare Analytics The project uses data from EMR Electronic Medical Records and EHR Electronic Health Records to analyze disease trends in patients identifying the top five diseases amongst the patients from the Diagnosis code ICD9 The goal is to identify patterns of diseases relevant to certain conditions such as Hyper Tension and Diabetes and how they are spread across all the states and the prescriptionmedication these patients are receiving This also helps identify potential problems and disastrous sideeffects caused due to overprescriptionmedication Responsibilities Extracted the data from hive tables by writing efficient Hive queries Performed preliminary data analysis using descriptive statistics and handled anomalies such as removing duplicates and imputing missing values Analyze Data and Performed Data Preparation by applying historical model on the data set in AZUREML Recorded the online users data using Python Django forms and implemented test case using Pytest Used Django frameworks and Python to build dynamic webpages Generated Python Django Forms to record data of online users Application of various machine learning algorithms and statistical modeling like decision trees text analytics natural language processing NLP supervised and unsupervised regression models neural networks deep learning SVM clustering to identify Volume using scikitlearn package in python Matlab Expanded website functionality using Flask framework in Python to control the web application logic Develop SparkScala Python R for regular expression regex project in the HadoopHive environment with LinuxWindows for big data resources Used clustering technique KMeans to identify outliers and to classify unlabeled data Evaluate models using Cross Validation Log loss function ROC curves and used AUC for feature selection and elastic technologies like ElasticSearch Kibana etc Work with NLTK library to NLP data processing and finding the patterns Categorize comments into positive and negative clusters from different social networking sites using Sentiment Analysis and Text Analytics Ensure that the model has low False Positive Rate and Text classification and sentiment analysis for unstructured and semistructured data Addressed over fitting by implementing of the algorithm regularization methods like L2 and L1 Use Principal Component Analysis in feature engineering to analyze high dimensional data Create and design reports that will use gathered metrics to infer and draw logical conclusions of past and future behavior Perform Multinomial Logistic Regression Random forest Decision Tree SVM to classify package is going to deliver on time for the new route Implemented different models like Logistic Regression Random Forest and GradientBoost Trees to predict whether a given die will pass or fail the test Perform data analysis by using Hive to retrieve the data from Hadoop cluster SQL to retrieve data from the database and used ETL for data transformation Use MLlib Sparks Machine learning library to build and evaluate different models Perform Data Cleaning features scaling features engineering using pandas and numpy packages in python Communicate the results with operations team for taking best decisions Collect data needs and requirements by Interacting with the other departments Environment Python 2x R HDFS Hadoop 23 Hive Linux Spark IBM SPSS Tableau Desktop SQL Server 2012 Microsoft Excel Matlab Spark SQL Pyspark Data Scientist Bank of the West San Francisco CA January 2016 to February 2017 Bank of the West is a regional financial services company headquartered in San Francisco California It is a subsidiary of BNP Paribas It has more than 600 branches and offices in the Midwest and Western United States Credit History Predictive Modeling The project uses supervised machine learning to train a model with credit default data to determine the probability andor classification of defaulter or nondefaulter predictability Factors such as Education Level Marital Status Payment History and Income will return a classification The goal is to help the bank manage loanportfolio risk and determine if a neural network vs logistic regression is the better model for classification Logistic Regression Random Forest Model and Deep Neural Network Models were used Loan Payment Default Prediction Built classification models using several features related to customer demographics macroeconomic dynamics historic payment behavior type and size of loan credit scores and loan to value ratios and with accuracy of 95 accuracy the model predicted the likelihood of loan default under various stressed conditions Responsibilities Gathered analyzed documented and translated application requirements into data models supported standardization of documentation and the adoption of standards and practices related to data and applications Queried and aggregated data from Amazon Redshift to get the sample dataset Identified patterns data quality issues and leveraged insights by communicating with BI team In preprocessing phase used Pandas to remove or replace all the missing data and feature engineering to eliminate unrelated features Balanced the dataset with Oversampling the minority label class and Undersampling the majority label class In data exploration stage used correlation analysis and graphical techniques to get some insights about the claim data Applied machine learning techniques to tap into new markets new customers and put forth my recommendations to the top management which resulted in increase in customer base by 5 and customer portfolio by 9 Build mulitlayers Neural Networks to implement Deep Learning by using Tensorflow and Keras Perform hyperparameter tuning by doing Distributed Cross Validation in Spark to speed up the computation process Export trained models into Protobuf to be served by Tensorflow Serving and performed integration job with clients application Analyzed customer master data for the identification of prospective business to understand their business needs built client relationships and explored opportunities for crossselling of financial products 60 Increased from 40 of customers availed more than 6 products Improved fraud prediction performance by using random forest and gradient boosting for feature selection with Python Scikitlearn Tested classification algorithms such as Logistic Regression Gradient Boosting and Random Forest using Pandas and Scikitlearn and evaluated the performance Worked extensively with data governance team to maintain data models Metadata and dictionaries Developed advanced models using multivariate regression Logistic regression Random forests decision trees and clustering Applied predictive analysis and statistical modeling techniques to analyze customer behavior and offer customized products reduce delinquency rate and default rate Lead to fall in default rates from 5 to 2 Application of various machine learning algorithms and statistical modeling like decision trees regression models neural networks SVM clustering to identify Volume using scikitlearn package in python Matlab Implemented tuned and tested the model on AWS EC2 with the best algorithm and parameters Set up data preprocessing pipeline to guarantee the consistency between the training data and new coming data Deployed the model on AWS Lambda collaborated with develop team to build the business solutions Collected the feedback after deployment retrained the model to improve the performance Discovered flaws in the methodology being used to calculate weather peril zone relativities designed and implemented a 3D algorithm based on kmeans clustering and Monte Carlo methods Observed groups of customers being neglected by the pricing algorithm used hierarchical clustering to improve customer segmentation and increase profits by 6 Designed developed and maintained daily and monthly summary trending and benchmark reports in Tableau Desktop Environment Python 2x R HDFS Hadoop 23 Hive Linux Spark IBM SPSS Tableau Desktop SQL Server 2012 Microsoft Excel Matlab Spark SQL Pyspark Data ScientistData Analyst Dicks Sporting Goods Inc Pittsburgh PA November 2013 to December 2015 Dicks Sporting Goods Inc sometimes shortened to Dicks is a Fortune 500 American sporting goods retailing corporation headquartered in Coraopolis Pennsylvania in Greater Pittsburgh Dicks has 610 stores in 47 states primarily in the Eastern United States The company also owns Golf Galaxy Inc a golf specialty retailer with 82 stores in 30 states Responsibilities Statistical Modelling with ML to bring Insights in Data under guidance of Principal Data Scientist Data modeling with Pig Hive Impala Ingestion with Sqoop Flume Used SVN to commit the Changes into the main EMM application trunk Understanding and implementation of text mining concepts graph processing and semi structured and unstructured data processing Worked with Ajax API calls to communicate with Hadoop through Impala Connection and SQL to render the required data through it These API calls are similar to Microsoft Cognitive API calls Good grip on Cloudera and HDP ecosystem components Used ElasticSearch Big Data to retrieve data into application as required Performed Map Reduce Programs those are running on the cluster Developed multiple MapReduce jobs in java for data cleaning and preprocessing Developed scalable machine learning solutions within a distributed computation framework eg Hadoop Spark Storm etc Analyzed the partitioned and bucketed data and compute various metrics for reporting Involved in loading data from RDBMS and web logs into HDFS using Sqoop and Flume Worked on loading the data from MySQL to HBase where necessary using Sqoop Developed Hive queries for Analysis across different banners Extracted data from Twitter using Java and Twitter API Parsed JSON formatted twitter data and uploaded to database Launching Amazon EC2 Cloud Instances using Amazon Images Linux Ubuntu and Configuring launched instances with respect to specific applications to improve robustness Exported the result set from Hive to MySQL using Sqoop after processing the data Analyzed the data by performing Hive queries and running Pig scripts to study customer behavior Have hands on experience working on Sequence files AVRO HAR file formats and compression Used Hive to partition and bucket data Experience in writing MapReduce programs with Java API to cleanse Structured and unstructured data Wrote Pig Scripts to perform ETL procedures on the data in HDFS Created HBase tables to store various data formats of data coming from different portfolios Worked on improving performance of existing Pig and Hive Queries Environment SQLServer Oracle 9i MSOffice Teradata Informatica ER Studio XML Business Objects HDFS Teradata 141 JSON HADOOP HDFS MapReduce PIG Spark R Studio MAHOUT JAVA HIVE AWS Data AnalystData Scientist IDBI Bank Mumbai Maharashtra June 2011 to January 2013 Responsibilities Integrated data from multiple data sources or functional areas ensures data accuracy and integrity and updates data as need using SQL and Python Expertise leveraging SQL Excel and Tableau to manipulate analyze and present data Performs analyses of structured and unstructured data to solve multiple andor complex business problems utilizing advanced statistical techniques and mathematical analyses Developed advanced models using multivariate regression Logistic regression Random forests decision trees and clustering Used Pandas Numpy Seaborn Scikitlearn in Python for developing various machine learning algorithms Build and improve models using natural language processing NLP and machine learning to extract insights from unstructured data Experienced working with distributed computing technologies Apache Spark Hive Applied predictive analysis and statistical modeling techniques to analyze customer behavior and offer customized products reduce delinquency rate and default rate Lead to fall in default rates from 5 to 2 Applied machine learning techniques to tap into new markets new customers and put forth my recommendations to the top management which resulted in increase in customer base by 5 and customer portfolio by 9 Analyzed customer master data for the identification of prospective business to understand their business needs built client relationships and explored opportunities for crossselling of financial products 60 Increased from 40 of customers availed more than 6 products Collaborated with business partners to understand their problems and goals develop predictive modeling statistical analysis data reports and performance metrics Participate in the ongoing design and development of a consolidated data warehouse supporting key business metrics across the organization Designed developed and implemented data quality validation rules to inspect and monitor the health of the data Dashboard and report development experience using Tableau ETL Developer Responsibilities Involved in full SDLC of BI Project including Data Analysis Designing Development of Data Warehouse environment Used Oracle Data Integrator Designer to develop processes for extracting cleansing transforming integrating and loading data into data warehouse database Experience in Developing and customizing PLSQL packages procedures functions triggers and reports using Oracle SQL Developer Responsible for designing developing and testing of the ETL strategy to populate the data from various source systems Flat files Oracle Worked with the Business units to identify data quality rule requirements against identified anomalies Develop Data Mapping Join and queries Validation and addressingfixing data queries raised by project team in a timely manner Worked closely with Business analyst and interacted with the Business users to gather new business requirements and to understand the accurate business and current requirements Created Repositories Agent Contexts and both of Physical Logical Schema in Topology Manager for all the source and target schemas Data mapping logical data modeling created class diagrams and ER diagrams and used SQL queries to filter data within the Oracle database Installed and Setup ODI Master Repository Work Repository Execution Repository Used Topology Manager to manage the data describing the information systems physical and logical architecture Extensively worked and utilized ODI Knowledge Modules Reverse Engineering Loading Integration Check Journalizing and service Created various procedures and variables Created ODI Packages Jobs of various complexities and automated process data flow Configured and setup ODI Master repository Work repository Project Models sources targets packages Knowledge Modules Interfaces Scenarios filters condition metadata Environment PLSQL Tableau Oracle MSOffice Teradata SQL Excel Apache Spark Hive Data mapping Python Systems Analyst Coramandel Infrastructure Pvt Ltd Hyderabad Telangana November 2010 to April 2011 Responsibilities Participate in fulllife cycle of SQL database development Create conceptual logical and physical database models to support project requirements Design implement and maintain databases for Corporate Data Finance and Operations business units Responsible for maintaining the integrity of the SQL database and reporting any issues to the database architect Assisted in creating and presenting informational reports to Management based on SQL data Build manage and maintain all project documentation including Business Requirements Documents BRD technical specifications process flows and clientspecific user guides Developed and validated conceptual data models including implementing logical and physical data mart data models Work closely with clients and internal teams to elicit and document business and functional requirements Delivered projects on time using Agile and Waterfall methodologies for timely completion of projects Environment SQL database Agile and WaterfallBRD SQL dataMSOfficeOracle Education Bachelor of Engineering in Engineering Malla Reddy College of Engineering and Technology May 2011 Skills Hadoop Hbase Hdfs Hive Mapreduce Additional Information TECHNICAL SKILLS Languages Java 8 Python R Python and R Numpy SciPy Pandas Scikitlearn Matplotlib Seaborn ggplot2 caret dplyr purrr readxl tidyr Rweka gmodels RCurl C50 twitter NLP Reshape2 rjson plyr Beautiful Soup Rpy2 Algorithms Kernel Density Estimation and Nonparametric Bayes Classifier KMeans Linear Regression Neighbors Nearest Farthest Range k Classification NonNegative Matrix Factorization Dimensionality Reduction Decision Tree Gaussian Processes Logistic Regression Nave Bayes Random Forest Ridge Regression Matrix FactorizationSVD NLPMachine LearningDeep Learning LDA Latent Dirichlet Allocation NLTK Apache OpenNLP Stanford NLP Sentiment Analysis SVMs ANN RNN CNN TensorFlow MXNet Caffe H2O Keras PyTorch Theano Azure ML Cloud Google Cloud Platform AWS Azure Bluemix Web Technologies JDBC HTML5 DHTML and XML CSS3 Web Services WSDL Data Modeling Tools Erwin r 96 95 91 8x Rational Rose ERStudio MS Visio SAP Power designer Big Data Technologies Hadoop Hive HDFS MapReduce Pig Kafka Databases SQL Hive Impala Pig Spark SQL Databases SQLServer My SQL MS Access HDFS HBase Teradata Netezza MongoDB Cassandra Reporting Tools MS Office WordExcelPower Point Visio Tableau Crystal reports XI Business Intelligence SSRS Business Objects 5x 6x Cognos7060 ETL Tools Informatica Power Centre SSIS Version Control Tools SVM GitHub BI Tools Tableau Tableau Server Tableau Reader SAP Business Objects OBIEE QlikView SAP Business Intelligence Amazon Redshift or Azure Data Warehouse Operating System Windows Linux Unix Macintosh HD Red Hat",
    "unique_id": "159f6e1b-9718-4294-9468-4eca0c11f1c3"
}