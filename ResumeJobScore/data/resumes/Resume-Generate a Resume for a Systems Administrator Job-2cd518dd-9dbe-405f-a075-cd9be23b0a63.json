{
    "clean_data": "Python Developer span lPythonspan span lDeveloperspan Python Developer Cisco Charlotte NC Professional qualified Python developer with over 7 years of experience in python including Machine Learning Data Mining and Statistical Analysis Involved in the entire python project life cycle and actively involved in all the phases including data extraction data cleaning statistical modelling and data visualization with large data sets of structured and unstructured data In depth knowledge of different web frameworks Django Pyramid Good knowledge in GUI frameworks Tkinter WxPython PyQt Experienced with machine learning algorithm such as logistic regression random forest Xgboost KNN SVM neural network linear regression lasso regression and kmeans Implemented Bagging and Boosting to enhance the model performance Strong skills in statistical methodologies such as AB test experiment design hypothesis test ANOVA Extensively worked on Python 3527 Numpy Pandas Matplotlib NLTK and Scikitlearn Experienced in developing webbased applications using Python Django Flask XML JSON BSON CSS HTML DHTMLXHTML JavaScript and JQuery RESTful MVT architecture AJAX Experience in implementing data analysis with various analytic tools such as Anaconda 40Jupyter Notebook 4X R 30 ggplot2 Caret dplyr and Excel Solid ability to write and optimize diverse SQL queries working knowledge of RDBMS like SQLServer2008 NoSQL databases like MongoDB32 Strong experience in Bigdata technologies like Spark 16 Spark sql pySpark Hadoop 2X HDFS Hive 1X Experience in visualization tools like Tableau9X 10X for creating dashboards Excellent understanding Agile and Scrum development methodology Used the version control tools like Git 2X Passionate about gleaning insightful information from massive data assets and developing a culture of sound datadriven decision making Ability to maintain a fun casual professional and productive team atmosphere Experienced the full software life cycle in SDLC Agile and Scrum methodologies Skilled in Advanced Regression Modelling Correlation Multivariate Analysis Model Building Business Intelligence tools and application of Statistical Concepts Developed predictive models using Decision Tree Random Forest Nave Bayes Logistic Regression Cluster Analysis and Neural Networks Experienced in Machine Learning and Statistical Analysis with Python ScikitLearn Experienced in Python to manipulate data for data loading and extraction and worked with python libraries like Matplotlib Numpy Scipy and Pandas for data analysis Worked with complex applications such as R SAS Matlab and SPSS to develop neural network cluster analysis Expertise in transforming business requirements into analytical models designing algorithms building models developing data mining and reporting solutions that scales across massive volume of structured and unstructured data Worked with NoSQL Database including Hbase Cassandra and Mongo DB Experienced in Big Data with Hadoop HDFS Map Reduce and Spark Experienced in Data Integration Validation and Data Quality controls for ETL process and Data Warehousing using MS Visual Studio SSIS SSAS SSRS Proficient in Tableau and RShiny data visualization tools to analyze and obtain insights into large datasets create visually powerful and actionable interactive reports and dashboards Automated recurring reports using SQL and Python and visualized them on BI platform like Tableau Worked in development environment like Git and VM Excellent communication skills Successfully working in fastpaced multitasking environment both independently and in collaborative team a selfmotivated enthusiastic learner Work Experience Python Developer Cisco Charlotte NC October 2018 to Present Roles Responsibilities Extensively involved in all phases of data acquisition data collection data cleaning model development model validation and visualization to deliver python solutions Built machine learning models to identify whether a user is legitimate using realtime data analysis and prevent fraudulent transactions using the history of customer transactions with supervised learning Extracted data from SQL Server Database copied into HDFS File system and used Hadoop tools such as Hive and Pig Latin to retrieve the data required for building models Performed data cleaning including transforming variables and dealing with missing value and ensured data quality consistency integrity using Pandas NumPy Developed web based application using Django and flask framework with python concepts Generated Python Django forms to maintain the record of online users Tackled highly imbalanced Fraud dataset using sampling techniques like under sampling and oversampling with SMOTE Synthetic Minority OverSampling Technique using Python Scikitlearn Utilized PCA tSNE and other feature engineering techniques to reduce the high dimensional data applied feature scaling handled categorical attributes using one hot encoder of scikitlearn library Developed various machine learning models such as Logistic regression KNN and Gradient Boosting with Pandas NumPy Seaborn Matplotlib Scikitlearn in Python Experience in Installing JenkinsPlugins for GIT Repository Setup SCM Polling for Immediate Build with Maven and Maven Repository and Deployment of apps using custom modules through Puppet as a CICD Process Worked on Amazon Web Services AWS cloud services to do machine learning on big data Developed Spark Python modules for machine learning predictive analytics in Hadoop Implemented a Pythonbased distributed random forest via PySpark and MLlib Used crossvalidation to test the model with different batches of data to find the best parameters for the model and optimized which eventually boosted the performance Experimented with Ensemble methods to increase the accuracy of the training model with different Bagging and Boosting methods and deployed the model on AWS Created and maintained reports to display the status and performance of deployed model and algorithm with Tableau Technology Stack Machine Learning AWS AWS Redshift Python Scikitlearn SciPy NumPy Django Pandas Matplotlib Seaborn SQL Server Hadoop HDFS Hive Pig Latin Apache  GitHub Linux Tableau Python developer Florida Blue Jacksonville FL September 2017 to September 2018 Roles Responsibilities Provided Configuration Management and Build support for more than 5 different applications built and deployed to the production and lower environments Evaluated the performance of Various Classification and Regression algorithms using R language to predict the future power Worked with several R packages including knitr dplyr SparkR CausalInfer spacetime Involved in Detecting Patterns with Unsupervised Learning like KMeans Clustering Implemented endtoend systems for Data Analytics Data Automation and integrated with custom visualization tools using R Mahout Hadoop and MongoDB AWS ImportExport accelerates moving large amounts of data into and out of AWS using portable storage devices for transport Implemented business logic using PythonDjango and flask with jinja templating system Gathering all the data that is required from multiple data sources and creating datasets that will be used in analysis Developed views and templates with Python and Djangos view controller and templating language to created userfriendly website interface Implemented the frontend and backend using JavaScript Python Framework and Neo4j Performed Exploratory Data Analysis and Data Visualizations using R and Tableau Perform a proper EDA Univariate and bivariate analysis to understand the intrinsic effectcombined effects Worked with Data governance Data quality data lineage Data architect to design various models and processes Independently coded new programs and designed Tables to load and test the program effectively for the given POCs using with Big DataHadoop Designed data models and data flow diagrams using Erwin and MS Visio Hands on standard of SCM tools Git CICD JenkinsMaven process Developed triggers stored procedures functions and packages using cursors and ref cursor concepts associated with the project using PlSQL Created various types of data visualizations using R python and Tableau Used Python R SQL to create Statistical algorithms involving Multivariate Regression Linear Regression Logistic Regression PCA Random forest models Decision trees Support Vector Machine for estimating the risks of welfare dependency Identified and targeted welfare highrisk groups with Machine learning algorithms Conducted campaigns and run realtime trials to determine what works fast and track the impact of different initiatives Developed Tableau visualizations and dashboards using Tableau Desktop Used Graphical EntityRelationship Diagramming to create new database design via easy to use graphical interface Clean and analyze data by using Hive and Pig on AWS Hadoop cluster Created multiple custom SQL queries in Teradata SQL Workbench to prepare the right data sets for Tableau dashboards Perform analyses such as regression analysis logistic regression discriminant analysis cluster analysis using SAS programming Used Meta data tool for importing metadata from repository new job categories and creating new data elements Scheduled the task for weekly updates and running the model in workflow Automated the entire process flow in generating the analysis and reports Technology Stack Python Django Erwin 8 AWS AWS EC2 AWS Redshift Teradata 13 SQL Server 2008 Oracle 9i SQLLoader PLSQL ODS OLAP OLTP SSAS Informatica Power Center 81 Python Developer NSG India Pvt Ltd Hyderabad Telangana March 2016 to April 2017 Roles Responsibilities Analyzed the data using Python and Spark performed feature engineering to clean the data for further analysis Implemented exploratory analysis on complex data sets using SQL Performed correlation analysis to narrow down and choose the best attributes for building machine learning model Assisted in building a predictive model in Python using classification techniques such as decision trees to help identify which applicants would sign their loan application improving their sales by 31 Created customized reports in tableau for data visualization Worked closely with stakeholders and subject matter experts to elicit and gather business requirements Worked directly with the Dev team to ensure product backlog was understood to the level needed and to keep sprint on track Log monitoring and generating visual representations of logs using ELK stack Implement CICD tools Upgrade Backup Restore DNS LDAP and SSL setup Facilitated Agile team ceremonies including Daily Standup Backlog Grooming Sprint Review Sprint Planning etc Developed entire frontend and backend modules using Python on Django Web Framework Successfully migrated the Django database from SQLite to MySQL to PostgreSQL with complete data integrity Used Pandas Numpy seaborn SciPy Matplotlib in Python for developing various machine learning algorithms and utilized machine learning algorithms such as linear regression multivariate regression for dataanalysis Responsible for data extraction and transformation ETL from AWS DynamoDB with HIVE Impala and AWS EMR to conduct the analysis from billions of customer historical transaction records Assisted python team to gather the Requirements Develop Process Model and detailed Business Policies and modified the business requirement document Credibly challenge remediation data analytics to ensure reasonable complete accurate and consistent requirements strategies data logic and implementation Create business intelligence reports using Tableau as needed Analysis of detailed logical flow chart to objectoriented python language Bug fixing feature adding and code optimizations Created database using Postgre SQL wrote several queries to extract data from database Wrote scripts in Python for extracting data from HTML file Tracked Velocity Capacity Burn Down Charts and other metrics during iterations Using Python I have automated a process to extract data and various document types from a website save the documents to specified file path and upload documents into an excel template Technology Stack Python Django AWS Redshift SQLServer Oracle MSOffice Teradata Informatica ER Studio XML Business Objects HDFS Teradata 141 JSON HADOOP HDFS Python Map Reduce PIG Spark R Studio MAHOUT JAVA HIVE AWS Python Developer Hyderabad Telangana January 2014 to January 2016 Roles Responsibilities Involved in Design Development and Support phases of Software Development Life Cycle SDLC Performed data ETL by collecting exporting merging and massaging data from multiple sources and platforms including SSIS SQL Server Integration Services in SQL Server Used Python scripts to update content in the database and manipulate files Generated Python Django Forms to record data of online users Used Python and Django creating graphics XML processing data exchange and business logic implementation Worked with crossfunctional teams including data engineer team to extract data and rapidly execute from MongoDB through MongDB connector for Hadoop Rewrite existing PythonDjango module to deliver certain format of data Used Django and flask Database API to access database objects Performed data cleaning and feature selection using MLlib package in PySpark Performed partitional clustering into 100 by kmeans clustering using Scikitlearn package in Python where similar hotels for a search are grouped together Developed entire frontend and backend modules using Python on Django including Tastypie Web Framework using Git Created Unit test and regression test framework for working and new code Used Python to perform ANOVA test to analyze the differences among hotel clusters Implemented application of various machine learning algorithms and statistical modeling like Decision Tree Naive Bayes Logistic Regression and Linear Regression using Python to determine the accuracy rate of each model Determined the most accurately prediction model based on the accuracy rate Used textmining process of reviews to determine customers concentrations Delivered analysis support to hotel recommendation and providing an online AB test Designed Tableau bar graphs scattered plots and geographical maps to create detailed level summary reports and dashboards Developed hybrid model to improve the accuracy rate Delivered the results to operation team for better decisions and feedbacks Worked in PLSQL Programming Stored procedures Triggers Packages using Oracle SQL PLSQL SQL Server2008 and UNIX shell scripting to perform job scheduling Technology Stack Python 27 Django 16 HTML5 CSS XML MySQL MongoDB JavaScript Angular JS JQuery CSS Bootstrap JavaScript Eclipse Git GitHub Linux Redis Shell Scripting Data Analyst Gramener Hyderabad Telangana July 2012 to October 2013 Roles Responsibilities Worked with project team representatives to ensure that logical and physical ERStudio data models were developed in line with corporate standards and guidelines Involved in defining the source to target data mappings business rules data definitions Worked with BTEQ to submit SQL statements import and export data and generate reports in Teradata Responsible for defining the key identifiers for each mappinginterface Responsible for defining the functional requirement documents for each source to target interface Document clarify and communicate requests for change requests with the requestor and coordinate with the development and testing team Work with users to identify the most appropriate source of record and profile the data required for sales and service Used python flask framework to develop Restful APIs service Document the complete process flow to describe program development logic testing and implementation application integration coding Involved in defining the businesstransformation rules applied for sales and service data Define the list codes and code conversions between the source systems and the data mart Worked with internal architects and assisting in the development of current and target state data architectures Coordinate with the business users in providing appropriate effective and efficient way to design the new reporting needs based on the user with the existing functionality Remain knowledgeable in all areas of business operations in order to identify systems needs and requirements Technology Stack Python Django Erwin r70 SQL Server 20002005 Windows XPNT2000 Oracle 8i9i MSDTS UML UAT SQL Loader OOD OLTP PLSQL MS Visio Informatica Skills Ms visual studio Visual studio Amazon web services Cc C Django Hadoop Html Jenkins Json Perl Python Flask Matplotlib Numpy Pandas Scripting Software development Visio Xml Additional Information Skills DBMS SQLServer2008 NoSQL MongoDB32 Programming Languages CC SQL Java Perl Hadoop Oracle and Python ML Scripts Html XML shell scripting Advanced PLSQL SQL IDEs Splunk EclipseNet Beans SQL Developer IntelliJ Operating systems Windows XPNT2000 Red Hat Ubuntu Debian Fedora CentOS Solaris Mac OS Reporting Visualization MS Visual Studio SSIS SSAS SSRS Python Framework Django Flask Pyramid web2py Software Development Lifecycle Agile Methodology Scrum framework Analytics Tools JMP PRO SAS Tableau UCI NET NodeXL MVC3 Python Libraries Pandas NumPy urlilib matplotlib UnitTest JSON CSV XML XLS Cloud Microsoft Azure Amazon Web Services PCF OpenStack Continuous Integration Tools Jenkins Hudson AnthillPro BuildForge uBuildTeamCity Others Microsoft Office Microsoft Visio Microsoft Visual Studio2008",
    "entities": [
        "MLlib",
        "GUI",
        "Informatica Power Center",
        "Tableau9X 10X",
        "Cc C Django Hadoop",
        "AWS Hadoop",
        "CausalInfer",
        "BI",
        "UNIX",
        "Detecting Patterns",
        "Visio Xml Additional Information",
        "Teradata SQL Workbench",
        "GIT Repository Setup",
        "Teradata Responsible",
        "Amazon Web Services AWS",
        "Statistical Concepts Developed",
        "AWS EMR",
        "Decision Tree Naive Bayes Logistic Regression",
        "Hadoop",
        "XML",
        "Hadoop Rewrite",
        "Advanced PLSQL SQL IDEs",
        "Coordinate",
        "kmeans Implemented Bagging and Boosting",
        "SPSS",
        "PySpark Performed",
        "Amazon",
        "Maven Repository and Deployment",
        "ELK",
        "Python",
        "SQL Server",
        "Assisted",
        "Responsibilities Provided Configuration Management",
        "Developed",
        "Bagging and Boosting",
        "Conducted",
        "Upgrade Backup Restore DNS LDAP",
        "NC",
        "VM Excellent",
        "Microsoft Visual",
        "Puppet",
        "Facilitated Agile",
        "Tableau Worked",
        "Meta",
        "Software Development Life Cycle SDLC Performed",
        "Tableau Desktop Used Graphical EntityRelationship Diagramming",
        "Visual",
        "SMOTE Synthetic Minority OverSampling Technique",
        "SSL",
        "RShiny",
        "Spark Experienced",
        "Tables",
        "PythonDjango",
        "Spark",
        "linear",
        "SDLC Agile",
        "KNN",
        "Created",
        "AWS",
        "Florida",
        "Tackled",
        "Hadoop Implemented a Pythonbased",
        "PySpark",
        "EDA",
        "Developed Tableau",
        "Advanced Regression Modelling Correlation Multivariate Analysis Model Building Business Intelligence",
        "Work Experience Python Developer Cisco Charlotte",
        "Decision Tree Random Forest Nave Bayes Logistic Regression Cluster Analysis",
        "Credibly",
        "SAS",
        "HTML",
        "Data Integration Validation",
        "SQL",
        "Various Classification and Regression",
        "Git CICD JenkinsMaven",
        "Tkinter WxPython PyQt Experienced",
        "SSIS SQL Server Integration Services",
        "Amazon Web Services",
        "SQL Performed",
        "AWS Created",
        "lPythonspan",
        "Data Quality",
        "Oracle SQL PLSQL",
        "ANOVA",
        "SQL Server Database",
        "HDFS File system",
        "Anaconda",
        "Big Data",
        "Hive",
        "SQL Server Hadoop",
        "Solaris Mac",
        "GitHub Linux Tableau Python",
        "Pandas",
        "SQLite",
        "ETL",
        "SCM",
        "Multivariate Regression Linear Regression Logistic Regression PCA Random",
        "Maven",
        "DHTMLXHTML",
        "Performed",
        "Build",
        "Impala",
        "Unsupervised Learning",
        "Djangos",
        "Machine Learning Data Mining and Statistical Analysis Involved",
        "Python Scikitlearn Utilized",
        "Tableau Technology Stack Machine Learning AWS AWS Redshift Python",
        "ERStudio",
        "Python ScikitLearn Experienced",
        "Data Analytics Data Automation",
        "Microsoft",
        "Bigdata",
        "Git Created Unit",
        "JQuery RESTful MVT architecture AJAX Experience",
        "R Mahout Hadoop",
        "ref",
        "Data Warehousing",
        "Tableau",
        "Machine Learning",
        "JavaScript Python Framework",
        "NoSQL Database",
        "Ability"
    ],
    "experience": "Experience in implementing data analysis with various analytic tools such as Anaconda 40Jupyter Notebook 4X R 30 ggplot2 Caret dplyr and Excel Solid ability to write and optimize diverse SQL queries working knowledge of RDBMS like SQLServer2008 NoSQL databases like MongoDB32 Strong experience in Bigdata technologies like Spark 16 Spark sql pySpark Hadoop 2X HDFS Hive 1X Experience in visualization tools like Tableau9X 10X for creating dashboards Excellent understanding Agile and Scrum development methodology Used the version control tools like Git 2X Passionate about gleaning insightful information from massive data assets and developing a culture of sound datadriven decision making Ability to maintain a fun casual professional and productive team atmosphere Experienced the full software life cycle in SDLC Agile and Scrum methodologies Skilled in Advanced Regression Modelling Correlation Multivariate Analysis Model Building Business Intelligence tools and application of Statistical Concepts Developed predictive models using Decision Tree Random Forest Nave Bayes Logistic Regression Cluster Analysis and Neural Networks Experienced in Machine Learning and Statistical Analysis with Python ScikitLearn Experienced in Python to manipulate data for data loading and extraction and worked with python libraries like Matplotlib Numpy Scipy and Pandas for data analysis Worked with complex applications such as R SAS Matlab and SPSS to develop neural network cluster analysis Expertise in transforming business requirements into analytical models designing algorithms building models developing data mining and reporting solutions that scales across massive volume of structured and unstructured data Worked with NoSQL Database including Hbase Cassandra and Mongo DB Experienced in Big Data with Hadoop HDFS Map Reduce and Spark Experienced in Data Integration Validation and Data Quality controls for ETL process and Data Warehousing using MS Visual Studio SSIS SSAS SSRS Proficient in Tableau and RShiny data visualization tools to analyze and obtain insights into large datasets create visually powerful and actionable interactive reports and dashboards Automated recurring reports using SQL and Python and visualized them on BI platform like Tableau Worked in development environment like Git and VM Excellent communication skills Successfully working in fastpaced multitasking environment both independently and in collaborative team a selfmotivated enthusiastic learner Work Experience Python Developer Cisco Charlotte NC October 2018 to Present Roles Responsibilities Extensively involved in all phases of data acquisition data collection data cleaning model development model validation and visualization to deliver python solutions Built machine learning models to identify whether a user is legitimate using realtime data analysis and prevent fraudulent transactions using the history of customer transactions with supervised learning Extracted data from SQL Server Database copied into HDFS File system and used Hadoop tools such as Hive and Pig Latin to retrieve the data required for building models Performed data cleaning including transforming variables and dealing with missing value and ensured data quality consistency integrity using Pandas NumPy Developed web based application using Django and flask framework with python concepts Generated Python Django forms to maintain the record of online users Tackled highly imbalanced Fraud dataset using sampling techniques like under sampling and oversampling with SMOTE Synthetic Minority OverSampling Technique using Python Scikitlearn Utilized PCA tSNE and other feature engineering techniques to reduce the high dimensional data applied feature scaling handled categorical attributes using one hot encoder of scikitlearn library Developed various machine learning models such as Logistic regression KNN and Gradient Boosting with Pandas NumPy Seaborn Matplotlib Scikitlearn in Python Experience in Installing JenkinsPlugins for GIT Repository Setup SCM Polling for Immediate Build with Maven and Maven Repository and Deployment of apps using custom modules through Puppet as a CICD Process Worked on Amazon Web Services AWS cloud services to do machine learning on big data Developed Spark Python modules for machine learning predictive analytics in Hadoop Implemented a Pythonbased distributed random forest via PySpark and MLlib Used crossvalidation to test the model with different batches of data to find the best parameters for the model and optimized which eventually boosted the performance Experimented with Ensemble methods to increase the accuracy of the training model with different Bagging and Boosting methods and deployed the model on AWS Created and maintained reports to display the status and performance of deployed model and algorithm with Tableau Technology Stack Machine Learning AWS AWS Redshift Python Scikitlearn SciPy NumPy Django Pandas Matplotlib Seaborn SQL Server Hadoop HDFS Hive Pig Latin Apache   GitHub Linux Tableau Python developer Florida Blue Jacksonville FL September 2017 to September 2018 Roles Responsibilities Provided Configuration Management and Build support for more than 5 different applications built and deployed to the production and lower environments Evaluated the performance of Various Classification and Regression algorithms using R language to predict the future power Worked with several R packages including knitr dplyr SparkR CausalInfer spacetime Involved in Detecting Patterns with Unsupervised Learning like KMeans Clustering Implemented endtoend systems for Data Analytics Data Automation and integrated with custom visualization tools using R Mahout Hadoop and MongoDB AWS ImportExport accelerates moving large amounts of data into and out of AWS using portable storage devices for transport Implemented business logic using PythonDjango and flask with jinja templating system Gathering all the data that is required from multiple data sources and creating datasets that will be used in analysis Developed views and templates with Python and Djangos view controller and templating language to created userfriendly website interface Implemented the frontend and backend using JavaScript Python Framework and Neo4j Performed Exploratory Data Analysis and Data Visualizations using R and Tableau Perform a proper EDA Univariate and bivariate analysis to understand the intrinsic effectcombined effects Worked with Data governance Data quality data lineage Data architect to design various models and processes Independently coded new programs and designed Tables to load and test the program effectively for the given POCs using with Big DataHadoop Designed data models and data flow diagrams using Erwin and MS Visio Hands on standard of SCM tools Git CICD JenkinsMaven process Developed triggers stored procedures functions and packages using cursors and ref cursor concepts associated with the project using PlSQL Created various types of data visualizations using R python and Tableau Used Python R SQL to create Statistical algorithms involving Multivariate Regression Linear Regression Logistic Regression PCA Random forest models Decision trees Support Vector Machine for estimating the risks of welfare dependency Identified and targeted welfare highrisk groups with Machine learning algorithms Conducted campaigns and run realtime trials to determine what works fast and track the impact of different initiatives Developed Tableau visualizations and dashboards using Tableau Desktop Used Graphical EntityRelationship Diagramming to create new database design via easy to use graphical interface Clean and analyze data by using Hive and Pig on AWS Hadoop cluster Created multiple custom SQL queries in Teradata SQL Workbench to prepare the right data sets for Tableau dashboards Perform analyses such as regression analysis logistic regression discriminant analysis cluster analysis using SAS programming Used Meta data tool for importing metadata from repository new job categories and creating new data elements Scheduled the task for weekly updates and running the model in workflow Automated the entire process flow in generating the analysis and reports Technology Stack Python Django Erwin 8 AWS AWS EC2 AWS Redshift Teradata 13 SQL Server 2008 Oracle 9i SQLLoader PLSQL ODS OLAP OLTP SSAS Informatica Power Center 81 Python Developer NSG India Pvt Ltd Hyderabad Telangana March 2016 to April 2017 Roles Responsibilities Analyzed the data using Python and Spark performed feature engineering to clean the data for further analysis Implemented exploratory analysis on complex data sets using SQL Performed correlation analysis to narrow down and choose the best attributes for building machine learning model Assisted in building a predictive model in Python using classification techniques such as decision trees to help identify which applicants would sign their loan application improving their sales by 31 Created customized reports in tableau for data visualization Worked closely with stakeholders and subject matter experts to elicit and gather business requirements Worked directly with the Dev team to ensure product backlog was understood to the level needed and to keep sprint on track Log monitoring and generating visual representations of logs using ELK stack Implement CICD tools Upgrade Backup Restore DNS LDAP and SSL setup Facilitated Agile team ceremonies including Daily Standup Backlog Grooming Sprint Review Sprint Planning etc Developed entire frontend and backend modules using Python on Django Web Framework Successfully migrated the Django database from SQLite to MySQL to PostgreSQL with complete data integrity Used Pandas Numpy seaborn SciPy Matplotlib in Python for developing various machine learning algorithms and utilized machine learning algorithms such as linear regression multivariate regression for dataanalysis Responsible for data extraction and transformation ETL from AWS DynamoDB with HIVE Impala and AWS EMR to conduct the analysis from billions of customer historical transaction records Assisted python team to gather the Requirements Develop Process Model and detailed Business Policies and modified the business requirement document Credibly challenge remediation data analytics to ensure reasonable complete accurate and consistent requirements strategies data logic and implementation Create business intelligence reports using Tableau as needed Analysis of detailed logical flow chart to objectoriented python language Bug fixing feature adding and code optimizations Created database using Postgre SQL wrote several queries to extract data from database Wrote scripts in Python for extracting data from HTML file Tracked Velocity Capacity Burn Down Charts and other metrics during iterations Using Python I have automated a process to extract data and various document types from a website save the documents to specified file path and upload documents into an excel template Technology Stack Python Django AWS Redshift SQLServer Oracle MSOffice Teradata Informatica ER Studio XML Business Objects HDFS Teradata 141 JSON HADOOP HDFS Python Map Reduce PIG Spark R Studio MAHOUT JAVA HIVE AWS Python Developer Hyderabad Telangana January 2014 to January 2016 Roles Responsibilities Involved in Design Development and Support phases of Software Development Life Cycle SDLC Performed data ETL by collecting exporting merging and massaging data from multiple sources and platforms including SSIS SQL Server Integration Services in SQL Server Used Python scripts to update content in the database and manipulate files Generated Python Django Forms to record data of online users Used Python and Django creating graphics XML processing data exchange and business logic implementation Worked with crossfunctional teams including data engineer team to extract data and rapidly execute from MongoDB through MongDB connector for Hadoop Rewrite existing PythonDjango module to deliver certain format of data Used Django and flask Database API to access database objects Performed data cleaning and feature selection using MLlib package in PySpark Performed partitional clustering into 100 by kmeans clustering using Scikitlearn package in Python where similar hotels for a search are grouped together Developed entire frontend and backend modules using Python on Django including Tastypie Web Framework using Git Created Unit test and regression test framework for working and new code Used Python to perform ANOVA test to analyze the differences among hotel clusters Implemented application of various machine learning algorithms and statistical modeling like Decision Tree Naive Bayes Logistic Regression and Linear Regression using Python to determine the accuracy rate of each model Determined the most accurately prediction model based on the accuracy rate Used textmining process of reviews to determine customers concentrations Delivered analysis support to hotel recommendation and providing an online AB test Designed Tableau bar graphs scattered plots and geographical maps to create detailed level summary reports and dashboards Developed hybrid model to improve the accuracy rate Delivered the results to operation team for better decisions and feedbacks Worked in PLSQL Programming Stored procedures Triggers Packages using Oracle SQL PLSQL SQL Server2008 and UNIX shell scripting to perform job scheduling Technology Stack Python 27 Django 16 HTML5 CSS XML MySQL MongoDB JavaScript Angular JS JQuery CSS Bootstrap JavaScript Eclipse Git GitHub Linux Redis Shell Scripting Data Analyst Gramener Hyderabad Telangana July 2012 to October 2013 Roles Responsibilities Worked with project team representatives to ensure that logical and physical ERStudio data models were developed in line with corporate standards and guidelines Involved in defining the source to target data mappings business rules data definitions Worked with BTEQ to submit SQL statements import and export data and generate reports in Teradata Responsible for defining the key identifiers for each mappinginterface Responsible for defining the functional requirement documents for each source to target interface Document clarify and communicate requests for change requests with the requestor and coordinate with the development and testing team Work with users to identify the most appropriate source of record and profile the data required for sales and service Used python flask framework to develop Restful APIs service Document the complete process flow to describe program development logic testing and implementation application integration coding Involved in defining the businesstransformation rules applied for sales and service data Define the list codes and code conversions between the source systems and the data mart Worked with internal architects and assisting in the development of current and target state data architectures Coordinate with the business users in providing appropriate effective and efficient way to design the new reporting needs based on the user with the existing functionality Remain knowledgeable in all areas of business operations in order to identify systems needs and requirements Technology Stack Python Django Erwin r70 SQL Server 20002005 Windows XPNT2000 Oracle 8i9i MSDTS UML UAT SQL Loader OOD OLTP PLSQL MS Visio Informatica Skills Ms visual studio Visual studio Amazon web services Cc C Django Hadoop Html Jenkins Json Perl Python Flask Matplotlib Numpy Pandas Scripting Software development Visio Xml Additional Information Skills DBMS SQLServer2008 NoSQL MongoDB32 Programming Languages CC SQL Java Perl Hadoop Oracle and Python ML Scripts Html XML shell scripting Advanced PLSQL SQL IDEs Splunk EclipseNet Beans SQL Developer IntelliJ Operating systems Windows XPNT2000 Red Hat Ubuntu Debian Fedora CentOS Solaris Mac OS Reporting Visualization MS Visual Studio SSIS SSAS SSRS Python Framework Django Flask Pyramid web2py Software Development Lifecycle Agile Methodology Scrum framework Analytics Tools JMP PRO SAS Tableau UCI NET NodeXL MVC3 Python Libraries Pandas NumPy urlilib matplotlib UnitTest JSON CSV XML XLS Cloud Microsoft Azure Amazon Web Services PCF OpenStack Continuous Integration Tools Jenkins Hudson AnthillPro BuildForge uBuildTeamCity Others Microsoft Office Microsoft Visio Microsoft Visual Studio2008",
    "extracted_keywords": [
        "Python",
        "Developer",
        "lPythonspan",
        "span",
        "lDeveloperspan",
        "Python",
        "Developer",
        "Cisco",
        "Charlotte",
        "NC",
        "Professional",
        "Python",
        "developer",
        "years",
        "experience",
        "python",
        "Machine",
        "Learning",
        "Data",
        "Mining",
        "Statistical",
        "Analysis",
        "python",
        "project",
        "life",
        "cycle",
        "phases",
        "data",
        "extraction",
        "data",
        "modelling",
        "data",
        "visualization",
        "data",
        "sets",
        "data",
        "depth",
        "knowledge",
        "web",
        "frameworks",
        "Django",
        "Pyramid",
        "knowledge",
        "GUI",
        "frameworks",
        "Tkinter",
        "WxPython",
        "PyQt",
        "machine",
        "learning",
        "algorithm",
        "regression",
        "forest",
        "Xgboost",
        "KNN",
        "SVM",
        "network",
        "linear",
        "regression",
        "lasso",
        "regression",
        "kmeans",
        "Bagging",
        "model",
        "performance",
        "skills",
        "methodologies",
        "AB",
        "test",
        "experiment",
        "design",
        "hypothesis",
        "test",
        "ANOVA",
        "Python",
        "Numpy",
        "Pandas",
        "Matplotlib",
        "NLTK",
        "Scikitlearn",
        "Experienced",
        "applications",
        "Python",
        "Django",
        "Flask",
        "BSON",
        "CSS",
        "HTML",
        "DHTMLXHTML",
        "JavaScript",
        "JQuery",
        "RESTful",
        "MVT",
        "architecture",
        "AJAX",
        "Experience",
        "data",
        "analysis",
        "tools",
        "Anaconda",
        "Notebook",
        "4X",
        "R",
        "ggplot2",
        "Caret",
        "dplyr",
        "Excel",
        "ability",
        "SQL",
        "knowledge",
        "SQLServer2008",
        "NoSQL",
        "MongoDB32",
        "Strong",
        "experience",
        "Bigdata",
        "technologies",
        "Spark",
        "Spark",
        "sql",
        "pySpark",
        "Hadoop",
        "2X",
        "HDFS",
        "Hive",
        "1X",
        "Experience",
        "visualization",
        "tools",
        "Tableau9X",
        "10X",
        "dashboards",
        "understanding",
        "Agile",
        "Scrum",
        "development",
        "methodology",
        "version",
        "control",
        "tools",
        "Git",
        "2X",
        "Passionate",
        "information",
        "data",
        "assets",
        "culture",
        "decision",
        "Ability",
        "team",
        "atmosphere",
        "software",
        "life",
        "cycle",
        "SDLC",
        "Agile",
        "Scrum",
        "methodologies",
        "Regression",
        "Modelling",
        "Correlation",
        "Multivariate",
        "Analysis",
        "Model",
        "Building",
        "Business",
        "Intelligence",
        "tools",
        "application",
        "Statistical",
        "Concepts",
        "models",
        "Decision",
        "Tree",
        "Random",
        "Forest",
        "Nave",
        "Bayes",
        "Logistic",
        "Regression",
        "Cluster",
        "Analysis",
        "Neural",
        "Networks",
        "Machine",
        "Learning",
        "Statistical",
        "Analysis",
        "Python",
        "ScikitLearn",
        "Python",
        "data",
        "data",
        "loading",
        "extraction",
        "python",
        "libraries",
        "Matplotlib",
        "Numpy",
        "Scipy",
        "Pandas",
        "data",
        "analysis",
        "applications",
        "R",
        "SAS",
        "Matlab",
        "SPSS",
        "network",
        "cluster",
        "analysis",
        "Expertise",
        "business",
        "requirements",
        "models",
        "algorithms",
        "building",
        "models",
        "data",
        "mining",
        "reporting",
        "solutions",
        "volume",
        "data",
        "NoSQL",
        "Database",
        "Hbase",
        "Cassandra",
        "Mongo",
        "DB",
        "Big",
        "Data",
        "Hadoop",
        "HDFS",
        "Map",
        "Reduce",
        "Spark",
        "Data",
        "Integration",
        "Validation",
        "Data",
        "Quality",
        "ETL",
        "process",
        "Data",
        "Warehousing",
        "MS",
        "Visual",
        "Studio",
        "SSIS",
        "SSAS",
        "SSRS",
        "Proficient",
        "Tableau",
        "RShiny",
        "data",
        "visualization",
        "tools",
        "insights",
        "datasets",
        "reports",
        "dashboards",
        "reports",
        "SQL",
        "Python",
        "BI",
        "platform",
        "Tableau",
        "development",
        "environment",
        "Git",
        "VM",
        "Excellent",
        "communication",
        "skills",
        "environment",
        "team",
        "learner",
        "Work",
        "Experience",
        "Python",
        "Developer",
        "Cisco",
        "Charlotte",
        "NC",
        "October",
        "Present",
        "Roles",
        "Responsibilities",
        "phases",
        "data",
        "acquisition",
        "data",
        "collection",
        "data",
        "cleaning",
        "model",
        "development",
        "model",
        "validation",
        "visualization",
        "solutions",
        "machine",
        "learning",
        "models",
        "user",
        "data",
        "analysis",
        "transactions",
        "history",
        "customer",
        "transactions",
        "data",
        "SQL",
        "Server",
        "Database",
        "HDFS",
        "File",
        "system",
        "Hadoop",
        "tools",
        "Hive",
        "Pig",
        "Latin",
        "data",
        "building",
        "models",
        "data",
        "cleaning",
        "variables",
        "value",
        "data",
        "quality",
        "consistency",
        "integrity",
        "Pandas",
        "NumPy",
        "Developed",
        "web",
        "application",
        "Django",
        "flask",
        "framework",
        "concepts",
        "Python",
        "Django",
        "record",
        "users",
        "Fraud",
        "dataset",
        "techniques",
        "SMOTE",
        "Synthetic",
        "Minority",
        "OverSampling",
        "Technique",
        "Python",
        "Scikitlearn",
        "PCA",
        "tSNE",
        "feature",
        "engineering",
        "techniques",
        "data",
        "feature",
        "scaling",
        "attributes",
        "encoder",
        "library",
        "machine",
        "learning",
        "models",
        "regression",
        "KNN",
        "Gradient",
        "Boosting",
        "Pandas",
        "NumPy",
        "Seaborn",
        "Matplotlib",
        "Scikitlearn",
        "Python",
        "Experience",
        "Installing",
        "JenkinsPlugins",
        "GIT",
        "Repository",
        "Setup",
        "SCM",
        "Polling",
        "Immediate",
        "Build",
        "Maven",
        "Maven",
        "Repository",
        "Deployment",
        "apps",
        "custom",
        "modules",
        "Puppet",
        "CICD",
        "Process",
        "Amazon",
        "Web",
        "Services",
        "AWS",
        "cloud",
        "services",
        "machine",
        "data",
        "Developed",
        "Spark",
        "Python",
        "modules",
        "machine",
        "analytics",
        "Hadoop",
        "Pythonbased",
        "forest",
        "PySpark",
        "MLlib",
        "crossvalidation",
        "model",
        "batches",
        "data",
        "parameters",
        "model",
        "performance",
        "methods",
        "accuracy",
        "training",
        "model",
        "Bagging",
        "Boosting",
        "methods",
        "model",
        "AWS",
        "reports",
        "status",
        "performance",
        "model",
        "algorithm",
        "Tableau",
        "Technology",
        "Stack",
        "Machine",
        "Learning",
        "AWS",
        "AWS",
        "Redshift",
        "Python",
        "Scikitlearn",
        "SciPy",
        "NumPy",
        "Django",
        "Pandas",
        "Matplotlib",
        "Seaborn",
        "SQL",
        "Server",
        "Hadoop",
        "HDFS",
        "Hive",
        "Pig",
        "Latin",
        "Apache",
        "GitHub",
        "Linux",
        "Tableau",
        "Python",
        "developer",
        "Florida",
        "Blue",
        "Jacksonville",
        "FL",
        "September",
        "September",
        "Roles",
        "Responsibilities",
        "Configuration",
        "Management",
        "Build",
        "support",
        "applications",
        "production",
        "environments",
        "performance",
        "Various",
        "Classification",
        "Regression",
        "algorithms",
        "R",
        "language",
        "power",
        "R",
        "packages",
        "knitr",
        "dplyr",
        "CausalInfer",
        "spacetime",
        "Detecting",
        "Patterns",
        "Unsupervised",
        "Learning",
        "KMeans",
        "endtoend",
        "systems",
        "Data",
        "Analytics",
        "Data",
        "Automation",
        "custom",
        "visualization",
        "tools",
        "R",
        "Mahout",
        "Hadoop",
        "MongoDB",
        "AWS",
        "ImportExport",
        "amounts",
        "data",
        "AWS",
        "storage",
        "devices",
        "transport",
        "business",
        "logic",
        "PythonDjango",
        "flask",
        "templating",
        "system",
        "data",
        "data",
        "sources",
        "datasets",
        "analysis",
        "views",
        "templates",
        "Python",
        "Djangos",
        "controller",
        "templating",
        "language",
        "website",
        "interface",
        "frontend",
        "backend",
        "JavaScript",
        "Python",
        "Framework",
        "Neo4j",
        "Performed",
        "Exploratory",
        "Data",
        "Analysis",
        "Data",
        "Visualizations",
        "R",
        "Tableau",
        "EDA",
        "Univariate",
        "analysis",
        "effects",
        "Data",
        "governance",
        "Data",
        "quality",
        "data",
        "lineage",
        "Data",
        "architect",
        "models",
        "processes",
        "programs",
        "Tables",
        "program",
        "POCs",
        "Big",
        "DataHadoop",
        "data",
        "models",
        "data",
        "flow",
        "diagrams",
        "Erwin",
        "MS",
        "Visio",
        "Hands",
        "standard",
        "SCM",
        "Git",
        "CICD",
        "JenkinsMaven",
        "process",
        "Developed",
        "triggers",
        "procedures",
        "functions",
        "packages",
        "cursors",
        "ref",
        "cursor",
        "concepts",
        "project",
        "types",
        "data",
        "visualizations",
        "R",
        "python",
        "Tableau",
        "Python",
        "R",
        "SQL",
        "algorithms",
        "Multivariate",
        "Regression",
        "Linear",
        "Regression",
        "Logistic",
        "Regression",
        "PCA",
        "Random",
        "forest",
        "models",
        "Decision",
        "trees",
        "Support",
        "Vector",
        "Machine",
        "risks",
        "welfare",
        "dependency",
        "welfare",
        "highrisk",
        "groups",
        "Machine",
        "learning",
        "campaigns",
        "trials",
        "impact",
        "initiatives",
        "Tableau",
        "visualizations",
        "dashboards",
        "Tableau",
        "Desktop",
        "Used",
        "Graphical",
        "EntityRelationship",
        "database",
        "design",
        "interface",
        "data",
        "Hive",
        "Pig",
        "AWS",
        "Hadoop",
        "cluster",
        "custom",
        "SQL",
        "Teradata",
        "SQL",
        "Workbench",
        "data",
        "sets",
        "Tableau",
        "dashboards",
        "Perform",
        "analyses",
        "regression",
        "analysis",
        "regression",
        "discriminant",
        "analysis",
        "cluster",
        "analysis",
        "SAS",
        "programming",
        "Meta",
        "data",
        "tool",
        "metadata",
        "job",
        "categories",
        "data",
        "elements",
        "task",
        "updates",
        "model",
        "workflow",
        "process",
        "flow",
        "analysis",
        "Technology",
        "Stack",
        "Python",
        "Django",
        "Erwin",
        "AWS",
        "EC2",
        "AWS",
        "Redshift",
        "Teradata",
        "SQL",
        "Server",
        "Oracle",
        "9i",
        "SQLLoader",
        "PLSQL",
        "ODS",
        "OLTP",
        "SSAS",
        "Informatica",
        "Power",
        "Center",
        "Python",
        "Developer",
        "NSG",
        "India",
        "Pvt",
        "Ltd",
        "Hyderabad",
        "Telangana",
        "March",
        "April",
        "Roles",
        "Responsibilities",
        "data",
        "Python",
        "Spark",
        "feature",
        "engineering",
        "data",
        "analysis",
        "analysis",
        "data",
        "sets",
        "SQL",
        "Performed",
        "correlation",
        "analysis",
        "attributes",
        "machine",
        "learning",
        "model",
        "Assisted",
        "model",
        "Python",
        "classification",
        "techniques",
        "decision",
        "trees",
        "applicants",
        "loan",
        "application",
        "sales",
        "reports",
        "tableau",
        "data",
        "visualization",
        "stakeholders",
        "matter",
        "experts",
        "business",
        "requirements",
        "Dev",
        "team",
        "product",
        "backlog",
        "level",
        "sprint",
        "track",
        "Log",
        "monitoring",
        "representations",
        "logs",
        "ELK",
        "stack",
        "Implement",
        "CICD",
        "tools",
        "Upgrade",
        "Backup",
        "Restore",
        "DNS",
        "LDAP",
        "SSL",
        "setup",
        "team",
        "ceremonies",
        "Daily",
        "Standup",
        "Backlog",
        "Grooming",
        "Sprint",
        "Review",
        "Sprint",
        "Planning",
        "frontend",
        "modules",
        "Python",
        "Django",
        "Web",
        "Framework",
        "Django",
        "database",
        "SQLite",
        "PostgreSQL",
        "data",
        "integrity",
        "Pandas",
        "Numpy",
        "SciPy",
        "Matplotlib",
        "Python",
        "machine",
        "algorithms",
        "machine",
        "learning",
        "algorithms",
        "linear",
        "regression",
        "multivariate",
        "regression",
        "dataanalysis",
        "data",
        "extraction",
        "transformation",
        "ETL",
        "AWS",
        "HIVE",
        "Impala",
        "AWS",
        "EMR",
        "analysis",
        "billions",
        "customer",
        "transaction",
        "records",
        "Assisted",
        "team",
        "Requirements",
        "Develop",
        "Process",
        "Model",
        "Business",
        "Policies",
        "business",
        "requirement",
        "document",
        "Credibly",
        "challenge",
        "remediation",
        "data",
        "analytics",
        "requirements",
        "strategies",
        "data",
        "logic",
        "implementation",
        "business",
        "intelligence",
        "reports",
        "Tableau",
        "Analysis",
        "flow",
        "chart",
        "python",
        "language",
        "Bug",
        "feature",
        "adding",
        "code",
        "optimizations",
        "database",
        "Postgre",
        "SQL",
        "queries",
        "data",
        "database",
        "Wrote",
        "scripts",
        "Python",
        "data",
        "HTML",
        "file",
        "Velocity",
        "Capacity",
        "Charts",
        "metrics",
        "iterations",
        "Python",
        "process",
        "data",
        "document",
        "types",
        "website",
        "documents",
        "file",
        "path",
        "documents",
        "template",
        "Technology",
        "Stack",
        "Python",
        "Django",
        "SQLServer",
        "Oracle",
        "MSOffice",
        "Teradata",
        "Informatica",
        "ER",
        "Studio",
        "XML",
        "Business",
        "HDFS",
        "Teradata",
        "JSON",
        "HADOOP",
        "HDFS",
        "Python",
        "Map",
        "Reduce",
        "PIG",
        "Spark",
        "R",
        "Studio",
        "MAHOUT",
        "HIVE",
        "Python",
        "Developer",
        "Hyderabad",
        "Telangana",
        "January",
        "January",
        "Roles",
        "Responsibilities",
        "Design",
        "Development",
        "Support",
        "phases",
        "Software",
        "Development",
        "Life",
        "Cycle",
        "SDLC",
        "Performed",
        "data",
        "ETL",
        "merging",
        "data",
        "sources",
        "platforms",
        "SSIS",
        "SQL",
        "Server",
        "Integration",
        "Services",
        "SQL",
        "Server",
        "Python",
        "scripts",
        "content",
        "database",
        "manipulate",
        "files",
        "Python",
        "Django",
        "Forms",
        "data",
        "users",
        "Python",
        "Django",
        "graphics",
        "XML",
        "processing",
        "data",
        "exchange",
        "business",
        "logic",
        "implementation",
        "teams",
        "data",
        "engineer",
        "team",
        "data",
        "MongDB",
        "connector",
        "Hadoop",
        "Rewrite",
        "PythonDjango",
        "module",
        "format",
        "data",
        "Django",
        "flask",
        "Database",
        "API",
        "access",
        "database",
        "Performed",
        "data",
        "cleaning",
        "feature",
        "selection",
        "MLlib",
        "package",
        "PySpark",
        "Performed",
        "clustering",
        "kmeans",
        "Scikitlearn",
        "package",
        "Python",
        "hotels",
        "search",
        "frontend",
        "modules",
        "Python",
        "Django",
        "Tastypie",
        "Web",
        "Framework",
        "Git",
        "Created",
        "Unit",
        "test",
        "regression",
        "test",
        "framework",
        "code",
        "Python",
        "ANOVA",
        "test",
        "differences",
        "hotel",
        "clusters",
        "application",
        "machine",
        "algorithms",
        "modeling",
        "Decision",
        "Tree",
        "Naive",
        "Bayes",
        "Logistic",
        "Regression",
        "Linear",
        "Regression",
        "Python",
        "accuracy",
        "rate",
        "model",
        "prediction",
        "model",
        "accuracy",
        "rate",
        "textmining",
        "process",
        "reviews",
        "customers",
        "concentrations",
        "analysis",
        "support",
        "hotel",
        "recommendation",
        "AB",
        "test",
        "Tableau",
        "bar",
        "graphs",
        "plots",
        "maps",
        "level",
        "summary",
        "reports",
        "dashboards",
        "model",
        "accuracy",
        "rate",
        "results",
        "operation",
        "team",
        "decisions",
        "feedbacks",
        "PLSQL",
        "Programming",
        "procedures",
        "Triggers",
        "Packages",
        "Oracle",
        "SQL",
        "PLSQL",
        "SQL",
        "Server2008",
        "UNIX",
        "shell",
        "scripting",
        "job",
        "scheduling",
        "Technology",
        "Stack",
        "Python",
        "Django",
        "HTML5",
        "CSS",
        "XML",
        "MySQL",
        "MongoDB",
        "JavaScript",
        "Angular",
        "JS",
        "JQuery",
        "CSS",
        "Bootstrap",
        "JavaScript",
        "Eclipse",
        "Git",
        "GitHub",
        "Linux",
        "Redis",
        "Shell",
        "Scripting",
        "Data",
        "Analyst",
        "Gramener",
        "Hyderabad",
        "Telangana",
        "July",
        "October",
        "Roles",
        "Responsibilities",
        "project",
        "team",
        "representatives",
        "ERStudio",
        "data",
        "models",
        "line",
        "standards",
        "guidelines",
        "source",
        "data",
        "mappings",
        "business",
        "data",
        "definitions",
        "BTEQ",
        "SQL",
        "statements",
        "import",
        "export",
        "data",
        "reports",
        "Teradata",
        "Responsible",
        "identifiers",
        "mappinginterface",
        "requirement",
        "documents",
        "source",
        "interface",
        "Document",
        "clarify",
        "requests",
        "change",
        "requests",
        "requestor",
        "development",
        "testing",
        "team",
        "users",
        "source",
        "record",
        "data",
        "sales",
        "service",
        "python",
        "flask",
        "framework",
        "APIs",
        "service",
        "Document",
        "process",
        "flow",
        "program",
        "development",
        "logic",
        "testing",
        "implementation",
        "application",
        "integration",
        "businesstransformation",
        "rules",
        "sales",
        "service",
        "data",
        "list",
        "codes",
        "code",
        "conversions",
        "source",
        "systems",
        "data",
        "mart",
        "architects",
        "development",
        "state",
        "data",
        "Coordinate",
        "business",
        "users",
        "way",
        "reporting",
        "needs",
        "user",
        "functionality",
        "areas",
        "business",
        "operations",
        "order",
        "systems",
        "needs",
        "requirements",
        "Technology",
        "Stack",
        "Python",
        "Django",
        "Erwin",
        "r70",
        "SQL",
        "Server",
        "Windows",
        "XPNT2000",
        "Oracle",
        "8i9i",
        "MSDTS",
        "UML",
        "UAT",
        "SQL",
        "Loader",
        "OOD",
        "OLTP",
        "PLSQL",
        "MS",
        "Visio",
        "Informatica",
        "Skills",
        "Ms",
        "studio",
        "Visual",
        "studio",
        "Amazon",
        "web",
        "services",
        "Cc",
        "C",
        "Django",
        "Hadoop",
        "Html",
        "Jenkins",
        "Json",
        "Perl",
        "Python",
        "Flask",
        "Matplotlib",
        "Numpy",
        "Pandas",
        "Scripting",
        "Software",
        "development",
        "Visio",
        "Xml",
        "Additional",
        "Information",
        "Skills",
        "DBMS",
        "SQLServer2008",
        "NoSQL",
        "MongoDB32",
        "Programming",
        "Languages",
        "CC",
        "SQL",
        "Java",
        "Perl",
        "Hadoop",
        "Oracle",
        "Python",
        "ML",
        "Scripts",
        "Html",
        "XML",
        "shell",
        "Advanced",
        "PLSQL",
        "SQL",
        "IDEs",
        "Splunk",
        "EclipseNet",
        "Beans",
        "SQL",
        "Developer",
        "IntelliJ",
        "Operating",
        "systems",
        "Windows",
        "XPNT2000",
        "Red",
        "Hat",
        "Ubuntu",
        "Debian",
        "Fedora",
        "CentOS",
        "Solaris",
        "Mac",
        "OS",
        "Reporting",
        "Visualization",
        "MS",
        "Visual",
        "Studio",
        "SSIS",
        "SSAS",
        "SSRS",
        "Python",
        "Framework",
        "Django",
        "Flask",
        "Pyramid",
        "web2py",
        "Software",
        "Development",
        "Lifecycle",
        "Agile",
        "Methodology",
        "Scrum",
        "framework",
        "Analytics",
        "Tools",
        "JMP",
        "PRO",
        "SAS",
        "Tableau",
        "UCI",
        "NET",
        "NodeXL",
        "MVC3",
        "Python",
        "Pandas",
        "NumPy",
        "urlilib",
        "matplotlib",
        "UnitTest",
        "CSV",
        "XML",
        "XLS",
        "Cloud",
        "Microsoft",
        "Azure",
        "Amazon",
        "Web",
        "Services",
        "PCF",
        "OpenStack",
        "Continuous",
        "Integration",
        "Tools",
        "Jenkins",
        "Hudson",
        "AnthillPro",
        "BuildForge",
        "uBuildTeamCity",
        "Others",
        "Microsoft",
        "Office",
        "Microsoft",
        "Visio",
        "Microsoft",
        "Visual",
        "Studio2008"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T22:40:08.128818",
    "resume_data": "Python Developer span lPythonspan span lDeveloperspan Python Developer Cisco Charlotte NC Professional qualified Python developer with over 7 years of experience in python including Machine Learning Data Mining and Statistical Analysis Involved in the entire python project life cycle and actively involved in all the phases including data extraction data cleaning statistical modelling and data visualization with large data sets of structured and unstructured data In depth knowledge of different web frameworks Django Pyramid Good knowledge in GUI frameworks Tkinter WxPython PyQt Experienced with machine learning algorithm such as logistic regression random forest Xgboost KNN SVM neural network linear regression lasso regression and kmeans Implemented Bagging and Boosting to enhance the model performance Strong skills in statistical methodologies such as AB test experiment design hypothesis test ANOVA Extensively worked on Python 3527 Numpy Pandas Matplotlib NLTK and Scikitlearn Experienced in developing webbased applications using Python Django Flask XML JSON BSON CSS HTML DHTMLXHTML JavaScript and JQuery RESTful MVT architecture AJAX Experience in implementing data analysis with various analytic tools such as Anaconda 40Jupyter Notebook 4X R 30 ggplot2 Caret dplyr and Excel Solid ability to write and optimize diverse SQL queries working knowledge of RDBMS like SQLServer2008 NoSQL databases like MongoDB32 Strong experience in Bigdata technologies like Spark 16 Spark sql pySpark Hadoop 2X HDFS Hive 1X Experience in visualization tools like Tableau9X 10X for creating dashboards Excellent understanding Agile and Scrum development methodology Used the version control tools like Git 2X Passionate about gleaning insightful information from massive data assets and developing a culture of sound datadriven decision making Ability to maintain a fun casual professional and productive team atmosphere Experienced the full software life cycle in SDLC Agile and Scrum methodologies Skilled in Advanced Regression Modelling Correlation Multivariate Analysis Model Building Business Intelligence tools and application of Statistical Concepts Developed predictive models using Decision Tree Random Forest Nave Bayes Logistic Regression Cluster Analysis and Neural Networks Experienced in Machine Learning and Statistical Analysis with Python ScikitLearn Experienced in Python to manipulate data for data loading and extraction and worked with python libraries like Matplotlib Numpy Scipy and Pandas for data analysis Worked with complex applications such as R SAS Matlab and SPSS to develop neural network cluster analysis Expertise in transforming business requirements into analytical models designing algorithms building models developing data mining and reporting solutions that scales across massive volume of structured and unstructured data Worked with NoSQL Database including Hbase Cassandra and Mongo DB Experienced in Big Data with Hadoop HDFS Map Reduce and Spark Experienced in Data Integration Validation and Data Quality controls for ETL process and Data Warehousing using MS Visual Studio SSIS SSAS SSRS Proficient in Tableau and RShiny data visualization tools to analyze and obtain insights into large datasets create visually powerful and actionable interactive reports and dashboards Automated recurring reports using SQL and Python and visualized them on BI platform like Tableau Worked in development environment like Git and VM Excellent communication skills Successfully working in fastpaced multitasking environment both independently and in collaborative team a selfmotivated enthusiastic learner Work Experience Python Developer Cisco Charlotte NC October 2018 to Present Roles Responsibilities Extensively involved in all phases of data acquisition data collection data cleaning model development model validation and visualization to deliver python solutions Built machine learning models to identify whether a user is legitimate using realtime data analysis and prevent fraudulent transactions using the history of customer transactions with supervised learning Extracted data from SQL Server Database copied into HDFS File system and used Hadoop tools such as Hive and Pig Latin to retrieve the data required for building models Performed data cleaning including transforming variables and dealing with missing value and ensured data quality consistency integrity using Pandas NumPy Developed web based application using Django and flask framework with python concepts Generated Python Django forms to maintain the record of online users Tackled highly imbalanced Fraud dataset using sampling techniques like under sampling and oversampling with SMOTE Synthetic Minority OverSampling Technique using Python Scikitlearn Utilized PCA tSNE and other feature engineering techniques to reduce the high dimensional data applied feature scaling handled categorical attributes using one hot encoder of scikitlearn library Developed various machine learning models such as Logistic regression KNN and Gradient Boosting with Pandas NumPy Seaborn Matplotlib Scikitlearn in Python Experience in Installing JenkinsPlugins for GIT Repository Setup SCM Polling for Immediate Build with Maven and Maven Repository and Deployment of apps using custom modules through Puppet as a CICD Process Worked on Amazon Web Services AWS cloud services to do machine learning on big data Developed Spark Python modules for machine learning predictive analytics in Hadoop Implemented a Pythonbased distributed random forest via PySpark and MLlib Used crossvalidation to test the model with different batches of data to find the best parameters for the model and optimized which eventually boosted the performance Experimented with Ensemble methods to increase the accuracy of the training model with different Bagging and Boosting methods and deployed the model on AWS Created and maintained reports to display the status and performance of deployed model and algorithm with Tableau Technology Stack Machine Learning AWS AWS Redshift Python Scikitlearn SciPy NumPy Django Pandas Matplotlib Seaborn SQL Server Hadoop HDFS Hive Pig Latin Apache SparkPySparkMLlib GitHub Linux Tableau Python developer Florida Blue Jacksonville FL September 2017 to September 2018 Roles Responsibilities Provided Configuration Management and Build support for more than 5 different applications built and deployed to the production and lower environments Evaluated the performance of Various Classification and Regression algorithms using R language to predict the future power Worked with several R packages including knitr dplyr SparkR CausalInfer spacetime Involved in Detecting Patterns with Unsupervised Learning like KMeans Clustering Implemented endtoend systems for Data Analytics Data Automation and integrated with custom visualization tools using R Mahout Hadoop and MongoDB AWS ImportExport accelerates moving large amounts of data into and out of AWS using portable storage devices for transport Implemented business logic using PythonDjango and flask with jinja templating system Gathering all the data that is required from multiple data sources and creating datasets that will be used in analysis Developed views and templates with Python and Djangos view controller and templating language to created userfriendly website interface Implemented the frontend and backend using JavaScript Python Framework and Neo4j Performed Exploratory Data Analysis and Data Visualizations using R and Tableau Perform a proper EDA Univariate and bivariate analysis to understand the intrinsic effectcombined effects Worked with Data governance Data quality data lineage Data architect to design various models and processes Independently coded new programs and designed Tables to load and test the program effectively for the given POCs using with Big DataHadoop Designed data models and data flow diagrams using Erwin and MS Visio Hands on standard of SCM tools Git CICD JenkinsMaven process Developed triggers stored procedures functions and packages using cursors and ref cursor concepts associated with the project using PlSQL Created various types of data visualizations using R python and Tableau Used Python R SQL to create Statistical algorithms involving Multivariate Regression Linear Regression Logistic Regression PCA Random forest models Decision trees Support Vector Machine for estimating the risks of welfare dependency Identified and targeted welfare highrisk groups with Machine learning algorithms Conducted campaigns and run realtime trials to determine what works fast and track the impact of different initiatives Developed Tableau visualizations and dashboards using Tableau Desktop Used Graphical EntityRelationship Diagramming to create new database design via easy to use graphical interface Clean and analyze data by using Hive and Pig on AWS Hadoop cluster Created multiple custom SQL queries in Teradata SQL Workbench to prepare the right data sets for Tableau dashboards Perform analyses such as regression analysis logistic regression discriminant analysis cluster analysis using SAS programming Used Meta data tool for importing metadata from repository new job categories and creating new data elements Scheduled the task for weekly updates and running the model in workflow Automated the entire process flow in generating the analysis and reports Technology Stack Python Django Erwin 8 AWS AWS EC2 AWS Redshift Teradata 13 SQL Server 2008 Oracle 9i SQLLoader PLSQL ODS OLAP OLTP SSAS Informatica Power Center 81 Python Developer NSG India Pvt Ltd Hyderabad Telangana March 2016 to April 2017 Roles Responsibilities Analyzed the data using Python and Spark performed feature engineering to clean the data for further analysis Implemented exploratory analysis on complex data sets using SQL Performed correlation analysis to narrow down and choose the best attributes for building machine learning model Assisted in building a predictive model in Python using classification techniques such as decision trees to help identify which applicants would sign their loan application improving their sales by 31 Created customized reports in tableau for data visualization Worked closely with stakeholders and subject matter experts to elicit and gather business requirements Worked directly with the Dev team to ensure product backlog was understood to the level needed and to keep sprint on track Log monitoring and generating visual representations of logs using ELK stack Implement CICD tools Upgrade Backup Restore DNS LDAP and SSL setup Facilitated Agile team ceremonies including Daily Standup Backlog Grooming Sprint Review Sprint Planning etc Developed entire frontend and backend modules using Python on Django Web Framework Successfully migrated the Django database from SQLite to MySQL to PostgreSQL with complete data integrity Used Pandas Numpy seaborn SciPy Matplotlib in Python for developing various machine learning algorithms and utilized machine learning algorithms such as linear regression multivariate regression for dataanalysis Responsible for data extraction and transformation ETL from AWS DynamoDB with HIVE Impala and AWS EMR to conduct the analysis from billions of customer historical transaction records Assisted python team to gather the Requirements Develop Process Model and detailed Business Policies and modified the business requirement document Credibly challenge remediation data analytics to ensure reasonable complete accurate and consistent requirements strategies data logic and implementation Create business intelligence reports using Tableau as needed Analysis of detailed logical flow chart to objectoriented python language Bug fixing feature adding and code optimizations Created database using Postgre SQL wrote several queries to extract data from database Wrote scripts in Python for extracting data from HTML file Tracked Velocity Capacity Burn Down Charts and other metrics during iterations Using Python I have automated a process to extract data and various document types from a website save the documents to specified file path and upload documents into an excel template Technology Stack Python Django AWS Redshift SQLServer Oracle MSOffice Teradata Informatica ER Studio XML Business Objects HDFS Teradata 141 JSON HADOOP HDFS Python Map Reduce PIG Spark R Studio MAHOUT JAVA HIVE AWS Python Developer Hyderabad Telangana January 2014 to January 2016 Roles Responsibilities Involved in Design Development and Support phases of Software Development Life Cycle SDLC Performed data ETL by collecting exporting merging and massaging data from multiple sources and platforms including SSIS SQL Server Integration Services in SQL Server Used Python scripts to update content in the database and manipulate files Generated Python Django Forms to record data of online users Used Python and Django creating graphics XML processing data exchange and business logic implementation Worked with crossfunctional teams including data engineer team to extract data and rapidly execute from MongoDB through MongDB connector for Hadoop Rewrite existing PythonDjango module to deliver certain format of data Used Django and flask Database API to access database objects Performed data cleaning and feature selection using MLlib package in PySpark Performed partitional clustering into 100 by kmeans clustering using Scikitlearn package in Python where similar hotels for a search are grouped together Developed entire frontend and backend modules using Python on Django including Tastypie Web Framework using Git Created Unit test and regression test framework for working and new code Used Python to perform ANOVA test to analyze the differences among hotel clusters Implemented application of various machine learning algorithms and statistical modeling like Decision Tree Naive Bayes Logistic Regression and Linear Regression using Python to determine the accuracy rate of each model Determined the most accurately prediction model based on the accuracy rate Used textmining process of reviews to determine customers concentrations Delivered analysis support to hotel recommendation and providing an online AB test Designed Tableau bar graphs scattered plots and geographical maps to create detailed level summary reports and dashboards Developed hybrid model to improve the accuracy rate Delivered the results to operation team for better decisions and feedbacks Worked in PLSQL Programming Stored procedures Triggers Packages using Oracle SQL PLSQL SQL Server2008 and UNIX shell scripting to perform job scheduling Technology Stack Python 27 Django 16 HTML5 CSS XML MySQL MongoDB JavaScript Angular JS JQuery CSS Bootstrap JavaScript Eclipse Git GitHub Linux Redis Shell Scripting Data Analyst Gramener Hyderabad Telangana July 2012 to October 2013 Roles Responsibilities Worked with project team representatives to ensure that logical and physical ERStudio data models were developed in line with corporate standards and guidelines Involved in defining the source to target data mappings business rules data definitions Worked with BTEQ to submit SQL statements import and export data and generate reports in Teradata Responsible for defining the key identifiers for each mappinginterface Responsible for defining the functional requirement documents for each source to target interface Document clarify and communicate requests for change requests with the requestor and coordinate with the development and testing team Work with users to identify the most appropriate source of record and profile the data required for sales and service Used python flask framework to develop Restful APIs service Document the complete process flow to describe program development logic testing and implementation application integration coding Involved in defining the businesstransformation rules applied for sales and service data Define the list codes and code conversions between the source systems and the data mart Worked with internal architects and assisting in the development of current and target state data architectures Coordinate with the business users in providing appropriate effective and efficient way to design the new reporting needs based on the user with the existing functionality Remain knowledgeable in all areas of business operations in order to identify systems needs and requirements Technology Stack Python Django Erwin r70 SQL Server 20002005 Windows XPNT2000 Oracle 8i9i MSDTS UML UAT SQL Loader OOD OLTP PLSQL MS Visio Informatica Skills Ms visual studio Visual studio Amazon web services Cc C Django Hadoop Html Jenkins Json Perl Python Flask Matplotlib Numpy Pandas Scripting Software development Visio Xml Additional Information Skills DBMS SQLServer2008 NoSQL MongoDB32 Programming Languages CC SQL Java Perl Hadoop Oracle and Python ML Scripts Html XML shell scripting Advanced PLSQL SQL IDEs Splunk EclipseNet Beans SQL Developer IntelliJ Operating systems Windows XPNT2000 Red Hat Ubuntu Debian Fedora CentOS Solaris Mac OS Reporting Visualization MS Visual Studio SSIS SSAS SSRS Python Framework Django Flask Pyramid web2py Software Development Lifecycle Agile Methodology Scrum framework Analytics Tools JMP PRO SAS Tableau UCI NET NodeXL MVC3 Python Libraries Pandas NumPy urlilib matplotlib UnitTest JSON CSV XML XLS Cloud Microsoft Azure Amazon Web Services PCF OpenStack Continuous Integration Tools Jenkins Hudson AnthillPro BuildForge uBuildTeamCity Others Microsoft Office Microsoft Visio Microsoft Visual Studio2008",
    "unique_id": "2cd518dd-9dbe-405f-a075-cd9be23b0a63"
}