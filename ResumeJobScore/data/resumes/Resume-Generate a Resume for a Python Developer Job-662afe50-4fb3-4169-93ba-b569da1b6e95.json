{
    "clean_data": "DATA SCIENTIST DATA SCIENTIST DATA SCIENTIST CERNER KANSAS CITY Overland Park KS Around 6 years of experience in IT field with 3 years as Data Scientist with strong technical and business experience and communication skills to drive highimpact business outcomes through datadriven innovations and decisions Expertise in Statistical analysis Predictive modeling Text mining Supervised learning Unsupervised Learning and Reinforcement learning Strong mathematical background in Linear algebra Probability Statistics Differentiation and Integration Expertise in transforming business requirements into analytical models designing algorithms building models developing data mining and reporting solutions that scale across a massive volume of Structured and unstructured data Proficient in Data Mining Methods Factor Analysis ANOVA Hypothetical testing normal distribution and other advanced Statistical modeling both linear and nonlinear logistic linear Nave Bayes decision trees Random forest neural networks SVM clustering KNN Experience with Deep learning techniques such as Convolutional Neural Networks Recurrent Neural Networks by using Keras and Tensorflow Worked on several python packages like NumPy Pandas Scikit Learn Matplotlib Beautiful Soup Pickle SciPy Python PyTables etc Proficient in implementing Dimensionality Reduction Techniques like Principal Component Analysis tStochastics Neighborhood Embedding tSNE and Linear Discriminant Analysis LDA Expertise on Distributed Computing Hadoop Architecture and its Ecosystem components like HDFS Map Reduce HIVE IMPALA Spark PySpark and Kafka Experience in implementing data analysis with various analytic tools such as Anaconda 40 Jupyter Notebook 4X Expertise in all aspects of Software Development Lifecycle SDLC from requirement analysis Design Development Coding Testing Implementation and Maintenance Hands on advanced SQL experience summarizing transforming segmenting joining datasets Well experienced in Normalization DeNormalization techniques for optimum performance in relational and dimensional database environments Experience in visualization tools like Tableau 9X 10X for creating dashboards Experience in working on both Windows Linux platforms Experience in using GIT Version Control System Sponsorship required to work in the US Work Experience DATA SCIENTIST CERNER KANSAS CITY January 2018 to Present RESPONSIBILITIES Participated in all phases of Machine Learning and Data Mining data collection data cleaning developing models validation visualization Used Pandas NumPy seaborn scipy matplotlib scikitlearn nltk in python for developing various machine learning algorithms Implemented Bagging and Boosting to enhance the model performance Leveraged disparate data sources that provide deep customer insight including online transactional data web data payment orders history and marketing campaigns exposure data Implemented the endtoend platform for performing user behavior analytics using unsupervised machine learning Implemented Classification using supervised algorithms like Logistic Regression Decision trees KNN Naive Bayes Data transformation from various resources data organization features extraction from raw and stored Identified outliers and inconsistencies in data by conducting exploratory data analysis EDA using python NumPy and Seaborn to see the insights of data and validate each feature Validated models using crossvalidation and loss function to measure model performance Created Confusion Matrix and ROC Addressed overfitting by implementing of the algorithm regularization methods like L2 and L1 Performed price sensitivity and variation analysis across different marketing channels and conducted exploratory data analysis on variables such as lifetime value and profit score Built data pipelines implemented code modularization involving package creation and codeveloped REST APIs using Flask for production deployment Performed data discovery and build a stream that automatically retrieves data from multitude of sources SQL databases external data such as social network data user reviews to generate KPIs using Tableau ENVIRONMENT Anaconda Python R Studio Jupyter Notebook VS code Spyder Oracle SSMS Unix Tableau HDFS SPARK IMPALA HIVE Hue DATA ANALYTICS SPECALIST Equifax St Louis MO January 2016 to December 2017 RESPONSIBILITIES Analyzed the data using various machine learning algorithms whether to extendnot credit limit to an existing applicant and to approvenot new credit line to a new applicant will likely result in profit or loss based on various circumstances like credit history utilization rate income age location hard enquiries number of deliquesces Extracted terabytes of structured and unstructured data by using SQL queries and performed data mining tasks including handling missing data data wrangling feature scaling outlier analysis in python by importing pandas Conducted data investigation discovery mapping tools to scan every single data record Performed data analysis data validation data cleansing and data verification to identify data mismatch using Relational Data modeling 3NF and Dimensional Data Modeling Performed exploratory data analysis on all the features to understand feature importance and analyzed the behavior of features by using different statistical approaches Studied the feature distribution with the help of Probability Density Function Cumulative Distribution Function Percentiles Quantiles to draw some insights Developed automated model training testing deployment via machine learning continuous delivery pipelines Built decision tree model from the set of training data using the information entropy and the attribute with the highest normalized information gain is chosen to make the decision of credit approval Used ML algorithms logistic regression support vector machine k nearest neighbors Nave Bayes bagging boosting ensemble learning to analyze the data based on the features selected for datadriven decisions Performed text analysis on the reviews of the products using NLP techniques like Bag of Words Term FrequencyInverse Document Frequency Word2vec Average Word2vec with help of NLTK Beautiful soup libraries Used machine learning algorithms to forecast the companys shortterm and longterm growth in terms of revenue number of customers various costs stock changes etcetera Used Classified instances Relative Operating Characteristic curve ROC and Confusion Matrix to find the accuracy of the models built Acquired knowledge on designing iterating and finetuning neural network models architecture for runtime efficiency to achieve optimal performance Visualized results in python using Matplotlib Seaborn libraries of Scikitlearn and used Tableau to create the interactive dashboards to present results for team members management and clients ENVIRONMENT Anaconda Python R Studio Jupyter Notebook VS Code Spyder PyCharm SSMS Unix Tableau Jira HDFS SPARK IMPALA HIVE Hue PYTHON DEVELOPER MAVIN SOLUTION Hyderabad Telangana July 2012 to August 2015 INDIA RESPONSIBILITIES Involved in the design and development of different webbased applications based on clients requirements Designed use case diagrams class diagrams sequence diagrams and state diagrams Learned new technical skills as required for the system like Django Flask Frameworks and ModelViewController MVC design pattern Developed applications using Flask Python frameworks Designed email marketing campaigns and created responsive web forms that saved data into a database using Python Django Framework Developed Python scripts to read from Excel files generate XML configuration files and for generating IP access frequency lists in different data logs Deployed web applications to Google App Engine Learnt to deploy projects using Jenkins Utilized Pandas Python library for analyzing data and data structures Managed large datasets using Pandas data frames and SQLite Performed frontend development for web initiatives to ensure usability using HTML and CSS and enhanced quality feel and usability of consumerfacing website Tested all completed work to ensure proper and error free functionality Collaborated with a team of instructors and programmers to develop the curriculum and guidelines for workshops to teach the logic of programming Created and ran custom SQL queries stored procedures and created an application to store client phone calls and emails that were routed to various developers Performed data profiling and analysis applied various data cleansing rules designed data standards architecture and designed the relational models Maintained metadata data definitions of table structures and version controlling for the data model Environment Python Django Flask SQLite SSMS Google App Engine Jenkins Pandas HTML CSS PYTHON DEVELOPER SUTHERLAND Hyderabad Telangana May 2011 to June 2012 INDIA RESPONSIBILITIES Actively involved in interacting with front end users project lead and business analyst to gather user requirements and online system specifications Followed Agile Methodologies to manage full lifecycle development of the project Designed and developed communication between client and server using Secured Web services Written backend programming in Python and used the Django Framework to develop the application Participated in entire lifecycle of the projects including Design Development and Deployment Testing and Implementation and support Implemented user interface guidelines and standards throughout the development and maintenance of the website using the HTML5 CSS3 JavaScript Developed views and templates with Python and Djangos view controller and templating language to create a userfriendly website interface Developed RESTful services using Django Developed and tested many features for dashboard using Python CSS JavaScript Used JavaScript and XML to update a portion of a webpage Successfully migrated the Django database from SQLite3 to PostgreSQL with complete data integrity Worked on Jenkins continuous integration tool for deployment of project Created custom TSQL procedures to read data from flat files to dump to SQL Server database using SQL Server import and export data wizard Developed user defined functions based on the requirements and used various builtin functions Handled errors using Exception handling try catch extensively for the ease of debugging and displaying the error messages in the application Developed batch scripts for scheduling data migration scripts Created clustered nonclustered indexes and indexed views to optimize the queries performance Coordinated with onsite folks and mentored the offshore team Worked PLSQL in Oracle database for writing queries functions stored procedures and triggers ENVIRONMENT Python Django JavaScript HTML CSS XML MYSQL TSQL SSMS MS Excel MSword TSQL Windows Server TOOLS TECHNOLOGIES Languages Packages Python SQL TensorFlow PySpark Numpy Pandas Keras NLTK Caffe Languages Packages Python SQL Numpy Pandas ScikitLearn Matplotlib TensorFlow Keras NLTK Tableau MySQL Databases Hadoop Ecosystem SQL Server HDFS Map Reduce HIVE IMPALA Spark PySpark and Kafka Mathematical Matrix operations Differentiation Integration Probability Statistics Linear Algebra Geometry Machine Learning Algorithms Logistic Regression Linear Regression K Means Clustering Algorithm Decision Trees Support Vector Machines Nave Bayes Hierarchical Clustering Deep Learning Techniques Artificial Neural Networks Convolutional Neural Networks Multilayer Perceptrons Recurrent Neural Networks LSTM Back Propagation Chain rule Choosing Activation Functions Drop Out Optimization algorithms User Interfaces HTML CSS Java Script XML Version control Tools Git Visualization Tools Tableau Plotly Operating Systems Windows Linux Methodologies Agile Scrum Education Masters Skills Python Microsoft Office Excel SQL R programming Machine learning Deep Learning Hadoop MS Office Powerpoint access Links httpwwwlinkedincominmounikat23923a18a",
    "entities": [
        "Created custom TSQL",
        "Relational Data",
        "NLTK",
        "Python",
        "SQL Server",
        "Pandas",
        "Bag of Words Term FrequencyInverse Document",
        "Developed",
        "Linear",
        "Created Confusion Matrix",
        "US",
        "Normalization DeNormalization",
        "Implemented Classification",
        "Performed",
        "Random",
        "Data Mining Methods Factor Analysis",
        "Leveraged",
        "Google App Engine Learnt",
        "Secured Web services Written",
        "KNN",
        "Unsupervised Learning",
        "Created",
        "Djangos",
        "Multilayer Perceptrons Recurrent Neural Networks",
        "Linear Discriminant Analysis LDA Expertise on Distributed Computing Hadoop Architecture",
        "Convolutional Neural Networks Recurrent Neural Networks",
        "Dimensionality Reduction Techniques",
        "Oracle",
        "Software Development Lifecycle SDLC",
        "Logistic Regression Decision",
        "Exception",
        "Microsoft",
        "EDA",
        "KNN Naive Bayes Data",
        "Data Mining",
        "Overland Park",
        "DATA SCIENTIST DATA SCIENTIST DATA SCIENTIST CERNER",
        "Tools Git Visualization Tools Tableau",
        "Python Django Framework Developed Python",
        "ML",
        "Probability Density Function Cumulative Distribution Function Percentiles Quantiles",
        "ENVIRONMENT Anaconda Python",
        "HTML",
        "IP",
        "CSS",
        "Environment Python Django Flask",
        "User Interfaces HTML CSS Java Script XML Version",
        "MVC",
        "Dimensional Data Modeling Performed",
        "Spark PySpark",
        "Design Development and Deployment Testing and Implementation",
        "SQL",
        "Data Scientist",
        "Flask",
        "REST",
        "Structured",
        "Built",
        "XML",
        "Collaborated",
        "Deep Learning Hadoop MS Office Powerpoint",
        "NLP",
        "Confusion Matrix",
        "Tableau",
        "Maintained",
        "Machine Learning",
        "Flask Python",
        "ROC",
        "Design Development Coding Testing Implementation and Maintenance Hands",
        "Validated",
        "Principal Component Analysis",
        "GIT Version Control System Sponsorship",
        "Anaconda",
        "SVM",
        "SQLite Performed"
    ],
    "experience": "Experience with Deep learning techniques such as Convolutional Neural Networks Recurrent Neural Networks by using Keras and Tensorflow Worked on several python packages like NumPy Pandas Scikit Learn Matplotlib Beautiful Soup Pickle SciPy Python PyTables etc Proficient in implementing Dimensionality Reduction Techniques like Principal Component Analysis tStochastics Neighborhood Embedding tSNE and Linear Discriminant Analysis LDA Expertise on Distributed Computing Hadoop Architecture and its Ecosystem components like HDFS Map Reduce HIVE IMPALA Spark PySpark and Kafka Experience in implementing data analysis with various analytic tools such as Anaconda 40 Jupyter Notebook 4X Expertise in all aspects of Software Development Lifecycle SDLC from requirement analysis Design Development Coding Testing Implementation and Maintenance Hands on advanced SQL experience summarizing transforming segmenting joining datasets Well experienced in Normalization DeNormalization techniques for optimum performance in relational and dimensional database environments Experience in visualization tools like Tableau 9X 10X for creating dashboards Experience in working on both Windows Linux platforms Experience in using GIT Version Control System Sponsorship required to work in the US Work Experience DATA SCIENTIST CERNER KANSAS CITY January 2018 to Present RESPONSIBILITIES Participated in all phases of Machine Learning and Data Mining data collection data cleaning developing models validation visualization Used Pandas NumPy seaborn scipy matplotlib scikitlearn nltk in python for developing various machine learning algorithms Implemented Bagging and Boosting to enhance the model performance Leveraged disparate data sources that provide deep customer insight including online transactional data web data payment orders history and marketing campaigns exposure data Implemented the endtoend platform for performing user behavior analytics using unsupervised machine learning Implemented Classification using supervised algorithms like Logistic Regression Decision trees KNN Naive Bayes Data transformation from various resources data organization features extraction from raw and stored Identified outliers and inconsistencies in data by conducting exploratory data analysis EDA using python NumPy and Seaborn to see the insights of data and validate each feature Validated models using crossvalidation and loss function to measure model performance Created Confusion Matrix and ROC Addressed overfitting by implementing of the algorithm regularization methods like L2 and L1 Performed price sensitivity and variation analysis across different marketing channels and conducted exploratory data analysis on variables such as lifetime value and profit score Built data pipelines implemented code modularization involving package creation and codeveloped REST APIs using Flask for production deployment Performed data discovery and build a stream that automatically retrieves data from multitude of sources SQL databases external data such as social network data user reviews to generate KPIs using Tableau ENVIRONMENT Anaconda Python R Studio Jupyter Notebook VS code Spyder Oracle SSMS Unix Tableau HDFS SPARK IMPALA HIVE Hue DATA ANALYTICS SPECALIST Equifax St Louis MO January 2016 to December 2017 RESPONSIBILITIES Analyzed the data using various machine learning algorithms whether to extendnot credit limit to an existing applicant and to approvenot new credit line to a new applicant will likely result in profit or loss based on various circumstances like credit history utilization rate income age location hard enquiries number of deliquesces Extracted terabytes of structured and unstructured data by using SQL queries and performed data mining tasks including handling missing data data wrangling feature scaling outlier analysis in python by importing pandas Conducted data investigation discovery mapping tools to scan every single data record Performed data analysis data validation data cleansing and data verification to identify data mismatch using Relational Data modeling 3NF and Dimensional Data Modeling Performed exploratory data analysis on all the features to understand feature importance and analyzed the behavior of features by using different statistical approaches Studied the feature distribution with the help of Probability Density Function Cumulative Distribution Function Percentiles Quantiles to draw some insights Developed automated model training testing deployment via machine learning continuous delivery pipelines Built decision tree model from the set of training data using the information entropy and the attribute with the highest normalized information gain is chosen to make the decision of credit approval Used ML algorithms logistic regression support vector machine k nearest neighbors Nave Bayes bagging boosting ensemble learning to analyze the data based on the features selected for datadriven decisions Performed text analysis on the reviews of the products using NLP techniques like Bag of Words Term FrequencyInverse Document Frequency Word2vec Average Word2vec with help of NLTK Beautiful soup libraries Used machine learning algorithms to forecast the companys shortterm and longterm growth in terms of revenue number of customers various costs stock changes etcetera Used Classified instances Relative Operating Characteristic curve ROC and Confusion Matrix to find the accuracy of the models built Acquired knowledge on designing iterating and finetuning neural network models architecture for runtime efficiency to achieve optimal performance Visualized results in python using Matplotlib Seaborn libraries of Scikitlearn and used Tableau to create the interactive dashboards to present results for team members management and clients ENVIRONMENT Anaconda Python R Studio Jupyter Notebook VS Code Spyder PyCharm SSMS Unix Tableau Jira HDFS SPARK IMPALA HIVE Hue PYTHON DEVELOPER MAVIN SOLUTION Hyderabad Telangana July 2012 to August 2015 INDIA RESPONSIBILITIES Involved in the design and development of different webbased applications based on clients requirements Designed use case diagrams class diagrams sequence diagrams and state diagrams Learned new technical skills as required for the system like Django Flask Frameworks and ModelViewController MVC design pattern Developed applications using Flask Python frameworks Designed email marketing campaigns and created responsive web forms that saved data into a database using Python Django Framework Developed Python scripts to read from Excel files generate XML configuration files and for generating IP access frequency lists in different data logs Deployed web applications to Google App Engine Learnt to deploy projects using Jenkins Utilized Pandas Python library for analyzing data and data structures Managed large datasets using Pandas data frames and SQLite Performed frontend development for web initiatives to ensure usability using HTML and CSS and enhanced quality feel and usability of consumerfacing website Tested all completed work to ensure proper and error free functionality Collaborated with a team of instructors and programmers to develop the curriculum and guidelines for workshops to teach the logic of programming Created and ran custom SQL queries stored procedures and created an application to store client phone calls and emails that were routed to various developers Performed data profiling and analysis applied various data cleansing rules designed data standards architecture and designed the relational models Maintained metadata data definitions of table structures and version controlling for the data model Environment Python Django Flask SQLite SSMS Google App Engine Jenkins Pandas HTML CSS PYTHON DEVELOPER SUTHERLAND Hyderabad Telangana May 2011 to June 2012 INDIA RESPONSIBILITIES Actively involved in interacting with front end users project lead and business analyst to gather user requirements and online system specifications Followed Agile Methodologies to manage full lifecycle development of the project Designed and developed communication between client and server using Secured Web services Written backend programming in Python and used the Django Framework to develop the application Participated in entire lifecycle of the projects including Design Development and Deployment Testing and Implementation and support Implemented user interface guidelines and standards throughout the development and maintenance of the website using the HTML5 CSS3 JavaScript Developed views and templates with Python and Djangos view controller and templating language to create a userfriendly website interface Developed RESTful services using Django Developed and tested many features for dashboard using Python CSS JavaScript Used JavaScript and XML to update a portion of a webpage Successfully migrated the Django database from SQLite3 to PostgreSQL with complete data integrity Worked on Jenkins continuous integration tool for deployment of project Created custom TSQL procedures to read data from flat files to dump to SQL Server database using SQL Server import and export data wizard Developed user defined functions based on the requirements and used various builtin functions Handled errors using Exception handling try catch extensively for the ease of debugging and displaying the error messages in the application Developed batch scripts for scheduling data migration scripts Created clustered nonclustered indexes and indexed views to optimize the queries performance Coordinated with onsite folks and mentored the offshore team Worked PLSQL in Oracle database for writing queries functions stored procedures and triggers ENVIRONMENT Python Django JavaScript HTML CSS XML MYSQL TSQL SSMS MS Excel MSword TSQL Windows Server TOOLS TECHNOLOGIES Languages Packages Python SQL TensorFlow PySpark Numpy Pandas Keras NLTK Caffe Languages Packages Python SQL Numpy Pandas ScikitLearn Matplotlib TensorFlow Keras NLTK Tableau MySQL Databases Hadoop Ecosystem SQL Server HDFS Map Reduce HIVE IMPALA Spark PySpark and Kafka Mathematical Matrix operations Differentiation Integration Probability Statistics Linear Algebra Geometry Machine Learning Algorithms Logistic Regression Linear Regression K Means Clustering Algorithm Decision Trees Support Vector Machines Nave Bayes Hierarchical Clustering Deep Learning Techniques Artificial Neural Networks Convolutional Neural Networks Multilayer Perceptrons Recurrent Neural Networks LSTM Back Propagation Chain rule Choosing Activation Functions Drop Out Optimization algorithms User Interfaces HTML CSS Java Script XML Version control Tools Git Visualization Tools Tableau Plotly Operating Systems Windows Linux Methodologies Agile Scrum Education Masters Skills Python Microsoft Office Excel SQL R programming Machine learning Deep Learning Hadoop MS Office Powerpoint access Links httpwwwlinkedincominmounikat23923a18a",
    "extracted_keywords": [
        "DATA",
        "SCIENTIST",
        "DATA",
        "SCIENTIST",
        "DATA",
        "SCIENTIST",
        "CERNER",
        "KANSAS",
        "CITY",
        "Overland",
        "Park",
        "KS",
        "years",
        "experience",
        "IT",
        "field",
        "years",
        "Data",
        "Scientist",
        "business",
        "experience",
        "communication",
        "skills",
        "highimpact",
        "business",
        "outcomes",
        "innovations",
        "decisions",
        "Expertise",
        "analysis",
        "modeling",
        "Text",
        "mining",
        "Unsupervised",
        "Learning",
        "Reinforcement",
        "background",
        "Linear",
        "Probability",
        "Statistics",
        "Differentiation",
        "Integration",
        "Expertise",
        "business",
        "requirements",
        "models",
        "algorithms",
        "building",
        "models",
        "data",
        "mining",
        "reporting",
        "solutions",
        "volume",
        "data",
        "Proficient",
        "Data",
        "Mining",
        "Methods",
        "Factor",
        "Analysis",
        "ANOVA",
        "Hypothetical",
        "distribution",
        "modeling",
        "linear",
        "Nave",
        "Bayes",
        "decision",
        "trees",
        "Random",
        "forest",
        "networks",
        "SVM",
        "KNN",
        "Experience",
        "Deep",
        "techniques",
        "Convolutional",
        "Neural",
        "Networks",
        "Recurrent",
        "Neural",
        "Networks",
        "Keras",
        "Tensorflow",
        "python",
        "packages",
        "NumPy",
        "Pandas",
        "Scikit",
        "Learn",
        "Matplotlib",
        "Beautiful",
        "Soup",
        "Pickle",
        "SciPy",
        "Python",
        "PyTables",
        "Proficient",
        "Dimensionality",
        "Reduction",
        "Techniques",
        "Principal",
        "Component",
        "Analysis",
        "tStochastics",
        "Neighborhood",
        "tSNE",
        "Linear",
        "Discriminant",
        "Analysis",
        "LDA",
        "Expertise",
        "Computing",
        "Hadoop",
        "Architecture",
        "Ecosystem",
        "components",
        "Map",
        "HIVE",
        "IMPALA",
        "Spark",
        "PySpark",
        "Kafka",
        "Experience",
        "data",
        "analysis",
        "tools",
        "Anaconda",
        "Jupyter",
        "Notebook",
        "4X",
        "Expertise",
        "aspects",
        "Software",
        "Development",
        "Lifecycle",
        "SDLC",
        "requirement",
        "analysis",
        "Design",
        "Development",
        "Coding",
        "Testing",
        "Implementation",
        "Maintenance",
        "Hands",
        "SQL",
        "experience",
        "datasets",
        "Normalization",
        "DeNormalization",
        "techniques",
        "performance",
        "database",
        "Experience",
        "visualization",
        "tools",
        "Tableau",
        "9X",
        "dashboards",
        "Experience",
        "Windows",
        "Linux",
        "Experience",
        "GIT",
        "Version",
        "Control",
        "System",
        "Sponsorship",
        "US",
        "Work",
        "Experience",
        "DATA",
        "SCIENTIST",
        "CERNER",
        "KANSAS",
        "CITY",
        "January",
        "RESPONSIBILITIES",
        "phases",
        "Machine",
        "Learning",
        "Data",
        "Mining",
        "data",
        "collection",
        "data",
        "models",
        "validation",
        "visualization",
        "Pandas",
        "NumPy",
        "matplotlib",
        "nltk",
        "python",
        "machine",
        "learning",
        "algorithms",
        "Bagging",
        "model",
        "performance",
        "Leveraged",
        "data",
        "sources",
        "customer",
        "insight",
        "data",
        "web",
        "data",
        "payment",
        "orders",
        "history",
        "marketing",
        "campaigns",
        "exposure",
        "data",
        "endtoend",
        "platform",
        "user",
        "behavior",
        "analytics",
        "machine",
        "Classification",
        "algorithms",
        "Logistic",
        "Regression",
        "Decision",
        "KNN",
        "Naive",
        "Bayes",
        "Data",
        "transformation",
        "resources",
        "data",
        "organization",
        "extraction",
        "outliers",
        "inconsistencies",
        "data",
        "data",
        "analysis",
        "EDA",
        "NumPy",
        "Seaborn",
        "insights",
        "data",
        "feature",
        "models",
        "crossvalidation",
        "loss",
        "function",
        "model",
        "performance",
        "Created",
        "Confusion",
        "Matrix",
        "ROC",
        "algorithm",
        "regularization",
        "methods",
        "L2",
        "L1",
        "Performed",
        "price",
        "sensitivity",
        "variation",
        "analysis",
        "marketing",
        "channels",
        "data",
        "analysis",
        "variables",
        "lifetime",
        "value",
        "profit",
        "score",
        "data",
        "pipelines",
        "code",
        "modularization",
        "package",
        "creation",
        "REST",
        "APIs",
        "Flask",
        "production",
        "deployment",
        "Performed",
        "data",
        "discovery",
        "stream",
        "data",
        "multitude",
        "sources",
        "SQL",
        "data",
        "network",
        "data",
        "user",
        "reviews",
        "KPIs",
        "Tableau",
        "ENVIRONMENT",
        "Anaconda",
        "Python",
        "R",
        "Studio",
        "Jupyter",
        "Notebook",
        "VS",
        "code",
        "Spyder",
        "Oracle",
        "Unix",
        "Tableau",
        "HDFS",
        "SPARK",
        "IMPALA",
        "HIVE",
        "Hue",
        "DATA",
        "ANALYTICS",
        "SPECALIST",
        "Equifax",
        "St",
        "Louis",
        "MO",
        "January",
        "December",
        "RESPONSIBILITIES",
        "data",
        "machine",
        "learning",
        "credit",
        "limit",
        "applicant",
        "credit",
        "line",
        "applicant",
        "profit",
        "loss",
        "circumstances",
        "credit",
        "history",
        "utilization",
        "rate",
        "income",
        "age",
        "location",
        "enquiries",
        "number",
        "deliquesces",
        "terabytes",
        "data",
        "SQL",
        "queries",
        "data",
        "mining",
        "tasks",
        "data",
        "data",
        "feature",
        "analysis",
        "python",
        "pandas",
        "data",
        "investigation",
        "discovery",
        "mapping",
        "tools",
        "data",
        "record",
        "Performed",
        "data",
        "analysis",
        "data",
        "validation",
        "data",
        "cleansing",
        "data",
        "verification",
        "data",
        "mismatch",
        "Relational",
        "Data",
        "modeling",
        "Dimensional",
        "Data",
        "data",
        "analysis",
        "features",
        "feature",
        "importance",
        "behavior",
        "features",
        "approaches",
        "feature",
        "distribution",
        "help",
        "Probability",
        "Density",
        "Function",
        "Cumulative",
        "Distribution",
        "Function",
        "Percentiles",
        "Quantiles",
        "insights",
        "model",
        "training",
        "testing",
        "deployment",
        "machine",
        "delivery",
        "pipelines",
        "decision",
        "tree",
        "model",
        "set",
        "training",
        "data",
        "information",
        "entropy",
        "attribute",
        "information",
        "gain",
        "decision",
        "credit",
        "approval",
        "ML",
        "regression",
        "support",
        "vector",
        "machine",
        "k",
        "neighbors",
        "Nave",
        "Bayes",
        "learning",
        "data",
        "features",
        "decisions",
        "text",
        "analysis",
        "reviews",
        "products",
        "NLP",
        "techniques",
        "Bag",
        "Words",
        "Term",
        "FrequencyInverse",
        "Document",
        "Frequency",
        "Word2vec",
        "Average",
        "Word2vec",
        "help",
        "NLTK",
        "Beautiful",
        "soup",
        "machine",
        "learning",
        "algorithms",
        "companys",
        "growth",
        "terms",
        "revenue",
        "number",
        "customers",
        "costs",
        "stock",
        "changes",
        "etcetera",
        "instances",
        "Relative",
        "Characteristic",
        "curve",
        "ROC",
        "Confusion",
        "Matrix",
        "accuracy",
        "models",
        "knowledge",
        "iterating",
        "network",
        "models",
        "architecture",
        "runtime",
        "efficiency",
        "performance",
        "results",
        "python",
        "Matplotlib",
        "Seaborn",
        "libraries",
        "Scikitlearn",
        "Tableau",
        "dashboards",
        "results",
        "team",
        "members",
        "management",
        "clients",
        "Anaconda",
        "Python",
        "R",
        "Studio",
        "Jupyter",
        "Notebook",
        "VS",
        "Code",
        "Spyder",
        "PyCharm",
        "Unix",
        "Tableau",
        "Jira",
        "HDFS",
        "SPARK",
        "IMPALA",
        "HIVE",
        "Hue",
        "PYTHON",
        "DEVELOPER",
        "SOLUTION",
        "Hyderabad",
        "Telangana",
        "July",
        "August",
        "INDIA",
        "RESPONSIBILITIES",
        "design",
        "development",
        "applications",
        "clients",
        "requirements",
        "use",
        "case",
        "diagrams",
        "class",
        "diagrams",
        "sequence",
        "diagrams",
        "state",
        "diagrams",
        "skills",
        "system",
        "Django",
        "Flask",
        "Frameworks",
        "ModelViewController",
        "MVC",
        "design",
        "pattern",
        "applications",
        "Flask",
        "Python",
        "frameworks",
        "email",
        "marketing",
        "campaigns",
        "web",
        "forms",
        "data",
        "database",
        "Python",
        "Django",
        "Framework",
        "Python",
        "scripts",
        "Excel",
        "files",
        "XML",
        "configuration",
        "files",
        "IP",
        "access",
        "frequency",
        "lists",
        "data",
        "logs",
        "Deployed",
        "web",
        "applications",
        "Google",
        "App",
        "Engine",
        "Learnt",
        "projects",
        "Jenkins",
        "Pandas",
        "Python",
        "library",
        "data",
        "data",
        "structures",
        "datasets",
        "Pandas",
        "data",
        "frames",
        "SQLite",
        "Performed",
        "frontend",
        "development",
        "web",
        "initiatives",
        "usability",
        "HTML",
        "CSS",
        "quality",
        "feel",
        "usability",
        "website",
        "work",
        "error",
        "functionality",
        "team",
        "instructors",
        "programmers",
        "curriculum",
        "guidelines",
        "workshops",
        "logic",
        "programming",
        "custom",
        "SQL",
        "procedures",
        "application",
        "client",
        "phone",
        "calls",
        "emails",
        "developers",
        "data",
        "profiling",
        "analysis",
        "data",
        "cleansing",
        "rules",
        "data",
        "standards",
        "architecture",
        "models",
        "metadata",
        "data",
        "definitions",
        "table",
        "structures",
        "version",
        "data",
        "model",
        "Environment",
        "Python",
        "Django",
        "Flask",
        "SQLite",
        "Google",
        "App",
        "Engine",
        "Jenkins",
        "Pandas",
        "HTML",
        "CSS",
        "PYTHON",
        "DEVELOPER",
        "SUTHERLAND",
        "Hyderabad",
        "Telangana",
        "May",
        "June",
        "INDIA",
        "RESPONSIBILITIES",
        "end",
        "users",
        "project",
        "lead",
        "business",
        "analyst",
        "user",
        "requirements",
        "system",
        "specifications",
        "Agile",
        "Methodologies",
        "lifecycle",
        "development",
        "project",
        "communication",
        "client",
        "server",
        "Secured",
        "Web",
        "services",
        "programming",
        "Python",
        "Django",
        "Framework",
        "application",
        "lifecycle",
        "projects",
        "Design",
        "Development",
        "Deployment",
        "Testing",
        "Implementation",
        "user",
        "interface",
        "guidelines",
        "standards",
        "development",
        "maintenance",
        "website",
        "HTML5",
        "CSS3",
        "JavaScript",
        "views",
        "templates",
        "Python",
        "Djangos",
        "controller",
        "templating",
        "language",
        "website",
        "interface",
        "services",
        "Django",
        "Developed",
        "features",
        "dashboard",
        "Python",
        "CSS",
        "JavaScript",
        "JavaScript",
        "XML",
        "portion",
        "webpage",
        "Django",
        "database",
        "SQLite3",
        "PostgreSQL",
        "data",
        "integrity",
        "Jenkins",
        "integration",
        "tool",
        "deployment",
        "project",
        "custom",
        "TSQL",
        "procedures",
        "data",
        "files",
        "SQL",
        "Server",
        "database",
        "SQL",
        "Server",
        "import",
        "export",
        "data",
        "user",
        "functions",
        "requirements",
        "builtin",
        "functions",
        "errors",
        "Exception",
        "handling",
        "try",
        "ease",
        "error",
        "messages",
        "application",
        "batch",
        "scripts",
        "scheduling",
        "data",
        "migration",
        "scripts",
        "indexes",
        "views",
        "queries",
        "performance",
        "folks",
        "team",
        "Oracle",
        "database",
        "queries",
        "functions",
        "procedures",
        "triggers",
        "Python",
        "Django",
        "JavaScript",
        "HTML",
        "CSS",
        "XML",
        "MYSQL",
        "TSQL",
        "SSMS",
        "MS",
        "Excel",
        "MSword",
        "TSQL",
        "Windows",
        "Server",
        "TOOLS",
        "TECHNOLOGIES",
        "Languages",
        "Packages",
        "Python",
        "SQL",
        "TensorFlow",
        "PySpark",
        "Numpy",
        "Pandas",
        "Keras",
        "NLTK",
        "Caffe",
        "Languages",
        "Packages",
        "Python",
        "SQL",
        "Numpy",
        "Pandas",
        "ScikitLearn",
        "Matplotlib",
        "TensorFlow",
        "Keras",
        "NLTK",
        "Tableau",
        "MySQL",
        "Databases",
        "Hadoop",
        "Ecosystem",
        "SQL",
        "Server",
        "HDFS",
        "Map",
        "HIVE",
        "IMPALA",
        "Spark",
        "PySpark",
        "Kafka",
        "Mathematical",
        "Matrix",
        "operations",
        "Differentiation",
        "Integration",
        "Probability",
        "Statistics",
        "Linear",
        "Algebra",
        "Geometry",
        "Machine",
        "Learning",
        "Algorithms",
        "Logistic",
        "Regression",
        "Linear",
        "Regression",
        "K",
        "Clustering",
        "Algorithm",
        "Decision",
        "Trees",
        "Support",
        "Vector",
        "Machines",
        "Nave",
        "Bayes",
        "Hierarchical",
        "Clustering",
        "Deep",
        "Learning",
        "Techniques",
        "Artificial",
        "Neural",
        "Networks",
        "Convolutional",
        "Neural",
        "Networks",
        "Multilayer",
        "Perceptrons",
        "Recurrent",
        "Neural",
        "Networks",
        "LSTM",
        "Back",
        "Propagation",
        "Chain",
        "rule",
        "Activation",
        "Functions",
        "Drop",
        "Out",
        "Optimization",
        "User",
        "Interfaces",
        "HTML",
        "CSS",
        "Java",
        "Script",
        "XML",
        "Version",
        "control",
        "Tools",
        "Git",
        "Visualization",
        "Tools",
        "Tableau",
        "Operating",
        "Systems",
        "Windows",
        "Linux",
        "Methodologies",
        "Agile",
        "Scrum",
        "Education",
        "Masters",
        "Skills",
        "Python",
        "Microsoft",
        "Office",
        "Excel",
        "SQL",
        "R",
        "programming",
        "Machine",
        "Deep",
        "Learning",
        "Hadoop",
        "MS",
        "Office",
        "Powerpoint",
        "access",
        "Links"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T21:08:13.417217",
    "resume_data": "DATA SCIENTIST DATA SCIENTIST DATA SCIENTIST CERNER KANSAS CITY Overland Park KS Around 6 years of experience in IT field with 3 years as Data Scientist with strong technical and business experience and communication skills to drive highimpact business outcomes through datadriven innovations and decisions Expertise in Statistical analysis Predictive modeling Text mining Supervised learning Unsupervised Learning and Reinforcement learning Strong mathematical background in Linear algebra Probability Statistics Differentiation and Integration Expertise in transforming business requirements into analytical models designing algorithms building models developing data mining and reporting solutions that scale across a massive volume of Structured and unstructured data Proficient in Data Mining Methods Factor Analysis ANOVA Hypothetical testing normal distribution and other advanced Statistical modeling both linear and nonlinear logistic linear Nave Bayes decision trees Random forest neural networks SVM clustering KNN Experience with Deep learning techniques such as Convolutional Neural Networks Recurrent Neural Networks by using Keras and Tensorflow Worked on several python packages like NumPy Pandas Scikit Learn Matplotlib Beautiful Soup Pickle SciPy Python PyTables etc Proficient in implementing Dimensionality Reduction Techniques like Principal Component Analysis tStochastics Neighborhood Embedding tSNE and Linear Discriminant Analysis LDA Expertise on Distributed Computing Hadoop Architecture and its Ecosystem components like HDFS Map Reduce HIVE IMPALA Spark PySpark and Kafka Experience in implementing data analysis with various analytic tools such as Anaconda 40 Jupyter Notebook 4X Expertise in all aspects of Software Development Lifecycle SDLC from requirement analysis Design Development Coding Testing Implementation and Maintenance Hands on advanced SQL experience summarizing transforming segmenting joining datasets Well experienced in Normalization DeNormalization techniques for optimum performance in relational and dimensional database environments Experience in visualization tools like Tableau 9X 10X for creating dashboards Experience in working on both Windows Linux platforms Experience in using GIT Version Control System Sponsorship required to work in the US Work Experience DATA SCIENTIST CERNER KANSAS CITY January 2018 to Present RESPONSIBILITIES Participated in all phases of Machine Learning and Data Mining data collection data cleaning developing models validation visualization Used Pandas NumPy seaborn scipy matplotlib scikitlearn nltk in python for developing various machine learning algorithms Implemented Bagging and Boosting to enhance the model performance Leveraged disparate data sources that provide deep customer insight including online transactional data web data payment orders history and marketing campaigns exposure data Implemented the endtoend platform for performing user behavior analytics using unsupervised machine learning Implemented Classification using supervised algorithms like Logistic Regression Decision trees KNN Naive Bayes Data transformation from various resources data organization features extraction from raw and stored Identified outliers and inconsistencies in data by conducting exploratory data analysis EDA using python NumPy and Seaborn to see the insights of data and validate each feature Validated models using crossvalidation and loss function to measure model performance Created Confusion Matrix and ROC Addressed overfitting by implementing of the algorithm regularization methods like L2 and L1 Performed price sensitivity and variation analysis across different marketing channels and conducted exploratory data analysis on variables such as lifetime value and profit score Built data pipelines implemented code modularization involving package creation and codeveloped REST APIs using Flask for production deployment Performed data discovery and build a stream that automatically retrieves data from multitude of sources SQL databases external data such as social network data user reviews to generate KPIs using Tableau ENVIRONMENT Anaconda Python R Studio Jupyter Notebook VS code Spyder Oracle SSMS Unix Tableau HDFS SPARK IMPALA HIVE Hue DATA ANALYTICS SPECALIST Equifax St Louis MO January 2016 to December 2017 RESPONSIBILITIES Analyzed the data using various machine learning algorithms whether to extendnot credit limit to an existing applicant and to approvenot new credit line to a new applicant will likely result in profit or loss based on various circumstances like credit history utilization rate income age location hard enquiries number of deliquesces Extracted terabytes of structured and unstructured data by using SQL queries and performed data mining tasks including handling missing data data wrangling feature scaling outlier analysis in python by importing pandas Conducted data investigation discovery mapping tools to scan every single data record Performed data analysis data validation data cleansing and data verification to identify data mismatch using Relational Data modeling 3NF and Dimensional Data Modeling Performed exploratory data analysis on all the features to understand feature importance and analyzed the behavior of features by using different statistical approaches Studied the feature distribution with the help of Probability Density Function Cumulative Distribution Function Percentiles Quantiles to draw some insights Developed automated model training testing deployment via machine learning continuous delivery pipelines Built decision tree model from the set of training data using the information entropy and the attribute with the highest normalized information gain is chosen to make the decision of credit approval Used ML algorithms logistic regression support vector machine k nearest neighbors Nave Bayes bagging boosting ensemble learning to analyze the data based on the features selected for datadriven decisions Performed text analysis on the reviews of the products using NLP techniques like Bag of Words Term FrequencyInverse Document Frequency Word2vec Average Word2vec with help of NLTK Beautiful soup libraries Used machine learning algorithms to forecast the companys shortterm and longterm growth in terms of revenue number of customers various costs stock changes etcetera Used Classified instances Relative Operating Characteristic curve ROC and Confusion Matrix to find the accuracy of the models built Acquired knowledge on designing iterating and finetuning neural network models architecture for runtime efficiency to achieve optimal performance Visualized results in python using Matplotlib Seaborn libraries of Scikitlearn and used Tableau to create the interactive dashboards to present results for team members management and clients ENVIRONMENT Anaconda Python R Studio Jupyter Notebook VS Code Spyder PyCharm SSMS Unix Tableau Jira HDFS SPARK IMPALA HIVE Hue PYTHON DEVELOPER MAVIN SOLUTION Hyderabad Telangana July 2012 to August 2015 INDIA RESPONSIBILITIES Involved in the design and development of different webbased applications based on clients requirements Designed use case diagrams class diagrams sequence diagrams and state diagrams Learned new technical skills as required for the system like Django Flask Frameworks and ModelViewController MVC design pattern Developed applications using Flask Python frameworks Designed email marketing campaigns and created responsive web forms that saved data into a database using Python Django Framework Developed Python scripts to read from Excel files generate XML configuration files and for generating IP access frequency lists in different data logs Deployed web applications to Google App Engine Learnt to deploy projects using Jenkins Utilized Pandas Python library for analyzing data and data structures Managed large datasets using Pandas data frames and SQLite Performed frontend development for web initiatives to ensure usability using HTML and CSS and enhanced quality feel and usability of consumerfacing website Tested all completed work to ensure proper and error free functionality Collaborated with a team of instructors and programmers to develop the curriculum and guidelines for workshops to teach the logic of programming Created and ran custom SQL queries stored procedures and created an application to store client phone calls and emails that were routed to various developers Performed data profiling and analysis applied various data cleansing rules designed data standards architecture and designed the relational models Maintained metadata data definitions of table structures and version controlling for the data model Environment Python Django Flask SQLite SSMS Google App Engine Jenkins Pandas HTML CSS PYTHON DEVELOPER SUTHERLAND Hyderabad Telangana May 2011 to June 2012 INDIA RESPONSIBILITIES Actively involved in interacting with front end users project lead and business analyst to gather user requirements and online system specifications Followed Agile Methodologies to manage full lifecycle development of the project Designed and developed communication between client and server using Secured Web services Written backend programming in Python and used the Django Framework to develop the application Participated in entire lifecycle of the projects including Design Development and Deployment Testing and Implementation and support Implemented user interface guidelines and standards throughout the development and maintenance of the website using the HTML5 CSS3 JavaScript Developed views and templates with Python and Djangos view controller and templating language to create a userfriendly website interface Developed RESTful services using Django Developed and tested many features for dashboard using Python CSS JavaScript Used JavaScript and XML to update a portion of a webpage Successfully migrated the Django database from SQLite3 to PostgreSQL with complete data integrity Worked on Jenkins continuous integration tool for deployment of project Created custom TSQL procedures to read data from flat files to dump to SQL Server database using SQL Server import and export data wizard Developed user defined functions based on the requirements and used various builtin functions Handled errors using Exception handling try catch extensively for the ease of debugging and displaying the error messages in the application Developed batch scripts for scheduling data migration scripts Created clustered nonclustered indexes and indexed views to optimize the queries performance Coordinated with onsite folks and mentored the offshore team Worked PLSQL in Oracle database for writing queries functions stored procedures and triggers ENVIRONMENT Python Django JavaScript HTML CSS XML MYSQL TSQL SSMS MS Excel MSword TSQL Windows Server TOOLS TECHNOLOGIES Languages Packages Python SQL TensorFlow PySpark Numpy Pandas Keras NLTK Caffe Languages Packages Python SQL Numpy Pandas ScikitLearn Matplotlib TensorFlow Keras NLTK Tableau MySQL Databases Hadoop Ecosystem SQL Server HDFS Map Reduce HIVE IMPALA Spark PySpark and Kafka Mathematical Matrix operations Differentiation Integration Probability Statistics Linear Algebra Geometry Machine Learning Algorithms Logistic Regression Linear Regression K Means Clustering Algorithm Decision Trees Support Vector Machines Nave Bayes Hierarchical Clustering Deep Learning Techniques Artificial Neural Networks Convolutional Neural Networks Multilayer Perceptrons Recurrent Neural Networks LSTM Back Propagation Chain rule Choosing Activation Functions Drop Out Optimization algorithms User Interfaces HTML CSS Java Script XML Version control Tools Git Visualization Tools Tableau Plotly Operating Systems Windows Linux Methodologies Agile Scrum Education Masters Skills Python Microsoft Office Excel SQL R programming Machine learning Deep Learning Hadoop MS Office Powerpoint access Links httpwwwlinkedincominmounikat23923a18a",
    "unique_id": "662afe50-4fb3-4169-93ba-b569da1b6e95"
}