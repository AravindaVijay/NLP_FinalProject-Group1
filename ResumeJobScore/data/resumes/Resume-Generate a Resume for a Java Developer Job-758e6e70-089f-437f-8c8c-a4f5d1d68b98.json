{
    "clean_data": "SparkHadoop Developer SparkHadoop span lDeveloperspan SparkHadoop Developer CVS Health Marlborough MA Work Experience SparkHadoop Developer CVS Health Woonsocket RI October 2016 to Present Assisted in different data Modeling and Data Warehouse design and development Used Sqoop to connect to the Sql Server and move the pivoted data to Hive tables and stored in different file formats like Avro SerDe Text etc Using Spark API over Hortonworks Hadoop YARN to perform analytics on data in Hive Analyzing of large volumes of structured data using SparkSQL Enhanced and optimized product Spark code to aggregate group and run data mining tasks using the Spark framework Created internal and external Hive tables and defined static and dynamic partitions for optimized performance Creating Hive tables to import large data sets from various relational databases using Sqoop and export the analyzed data back for visualization and report Tableau generation by the BI team Wrote Python scripts to parse JSON documents and load the data in database Worked on Apache spark writing python applications to convert txt xls files and parse data into JSON format Developed the Shell Perl and Python Scripts Linux UNIX to execute jobs Exploring with the Spark improving the performance and optimization of the existing algorithms in Hadoop using Spark Context SparkSQL Data Frame Pair RDDs Spark YARN Loading data into Spark RDD and do in memory data Computation to generate the Output response Loading the data to HBASE by using bulk load and HBASE API Used PySpark to expose spark API to Python Developed Spark code and SparkSQL for faster testing and processing of data Experiencing in Oozie a workflow scheduler to manage Hadoop jobs by Direct Acyclic Graph DAG of actions with control flows Loading data into Spark RDD and do in memory data Computation to generate the Output response Developed Spark code using Python for faster processing of data on Hive Worked on Integrating Talend and SSIS with Hadoop and performed ETL operations Developed MapReduce jobs in Python for data cleaning and data processing Used spark cluster to manipulate RDDS Resilient Distributed Datasets and also used concepts of RDD partitions Connecting MySQL database through spark driver Develop applications that use MongoDB database and Pymong Wrote Python modules to extractload asset data from the MySQL source database Developed and maintain several batch jobs to run automatically depending on business requirements Involved in cluster maintenance commissioning decommissioning Data nodes Troubleshooting Manage and review data backups Manage and review Hadoop log files Environment Python Hortonworks Hadoop HDFS MapReduce YARN Spark Pig Hive Sqoop Kafka HBase Bedrock Oozie Flume Tableau SQL Scripting Linux Shell Scripting Sublime Hadoop Developer Eliza Corporation Danvers MA July 2015 to June 2016 Installed and configured Apache Hadoop Hive and Pig environment on AWS Handling Hadoop cluster setup involved in start to end process of installation configuration and monitoring Build servers using AWS Importing volumes launching EC2 RDS creating security groups autoscaling load balancers ELBs in the defined virtual private connection Migrating physical LinuxWindows servers to cloud AWS and testing Performed Export and import of data into simple storage service S3 Development Acceptance Integration and Production AWS Endpoints Developed complex MapReduce streaming jobs using Java language that are implemented Using Hive and Pig Loading data into Spark RDD and do in memory data Computation to generate the Output response Optimized MapReduce Jobs to use HDFS efficiently by using various compression mechanisms Collected and aggregated large amounts of web log data from different sources such as web servers mobile using Apache Flume and stored the data into HDFS for analysis Handled importing of data from various data sources performed transformations using Hive MapReduce loaded data into HDFS and Extracted the data from MySQL into HDFS using Sqoop Automating backups by the shell for Linux to transfer data in S3 bucket Designed and crafted HighPerformance Clusters in AWS along with AWS Red shift Experience in AWS EC2 configuring the servers for Auto scaling and Elastic load balancing AWS CLI Auto Scaling and Cloud Watch Monitoring creation and update Develop Hive scripts for end user analyst requirements to perform ad hoc analysis Plan deploy monitor and maintain Amazon AWS cloud infrastructure consisting of multiple EC2 nodes and VMware Vms as required in the environment Expertise in AWS data migration between different database platforms like SQL Server to Amazon Aurora using RDS tool Optimized MapReduce Jobs to use HDFS efficiently by using various compression mechanisms Loading Data into HBase using Bulk Load and Nonbulk load Experiencing in Oozie a workflow scheduler to manage Hadoop jobs by Direct Acyclic Graph DAG of actions with control flows Handled importing of data from various data sources performed transformations using Hive MapReduce loaded data into HDFS and Extracted the data from MySQL into HDFS using Sqoop Created Hive tables partitions and loaded the data to analyze using HiveQL queries Configuring EC2 instances in VPC network managing security through IAM and Monitoring servers health through Cloud Watch Development Acceptance Integration and Production AWS Endpoints Automating backups by the shell for Linux to transfer data in S3 bucket Designed and crafted HighPerformance Clusters in AWS along with AWS Red shift Environment Hadoop Python HDFS Amazon Web Services AWS YARN Spark Pig Hive Sqoop Oozie Flume Linux Shell Scripting SparkSQL MongoDB Data Engineer Hadoop ABS Pvt ltd Mumbai Maharashtra May 2014 to March 2015 Exported data to a MySQL from HDFS using Sqoop and NFS mount approach Moved data from HDFS to Cassandra using Map Reduce and Bulk Output Format class Developed Map Reduce programs for applying business rules on the data Developed and executed hive queries for demoralizing the data Works with ETL workflow analysis of Big Data and loaded them into Hadoop cluster Installed and configured Hadoop Cluster for development and testing environment Implemented Fair scheduler on the Job tracker to share the resources of the cluster for the map reduces jobs given by the users Automated the workflow using shell scripts Performance tuning of the Hive queries written by other developer Prototype various applications that utilize modern Big Data tools Environment Java Python Hadoop Map Reduce HDFS Hive Pig Sqoop Cloudera MongoDB HUE Linux Java Python Developer GBS Technologies January 2012 to March 2014 Responsible for understanding the scope of the project and requirement gathering Review and analyze the design and implementation of software componentsapplications and outline the development process strategies Used Spring JDBC to write some DAO classes to interact with the database to access account information Created entire application using Python Django MySQL and Linux Involved in creation of Test Cases for JUnit Testing Wrote python scripts to parse XML documents and load the data in database Designed and configured database and back end applications and programs Used Oracle as Database used Toad for queries execution and involved in writing SQL scripts PLSQL code for procedures and functions Used CVS Perforce as configuration management tool for code versioning and release Developed application using Eclipse and used build and deploy tool as Maven Environment Python Java15 J2EE Servlets JSP XML spring 30 Design Patterns Log4j CVS Maven Eclipse and Oracle 11g Education Master of Science in Computer Science Rivier University Skills LINUX 4 years MYSQL 4 years PYTHON 4 years JAVA 3 years SQL 3 years Additional Information Areas of Expertise Hadoop Ecosystems HDFS YARN Spark Map Reduce Hive Pig Zookeeper Sqoop Oozie Bedrock Flume Kafka Impala Ambari HUE MongoDB HBase Amazon Web Services Redshift EMR EC2 S3 RDS Cloud Search Data Pipeline Lambda Languages Python Java HiveQL Pig Latin Advanced PLSQL SQL Databases Oracle MSSQL Server MySQL MSAccess PostgreSQL NoSQL Teradata Tools Eclipse Net Beans SQL Developer IntelliJ MS Visual Studio Sublime BI Tools Tableau Qlikview Jinfonet JReport Hadoop Platforms Hortonworks Amazon Web services AWS Cloudera Platforms Linux Red HatUbuntuCentos Windows Macintosh Linux Solaris",
    "entities": [
        "SQL Server",
        "JReport Hadoop",
        "Implemented Fair",
        "Mumbai",
        "Spark Context",
        "CVS Maven Eclipse",
        "SparkSQL",
        "API",
        "Direct Acyclic Graph DAG",
        "ETL",
        "Data Warehouse",
        "Troubleshooting Manage",
        "Developed",
        "Sqoop",
        "LinuxWindows",
        "Build",
        "IAM",
        "Hortonworks Hadoop",
        "BI",
        "AWS",
        "the Sql Server",
        "Prototype",
        "Cloud Watch Development Acceptance Integration and Production AWS",
        "RDDS Resilient Distributed Datasets",
        "HBase Amazon Web Services Redshift EMR",
        "Data Engineer Hadoop",
        "Using Spark API",
        "Hive Worked on Integrating Talend",
        "Science in Computer Science Rivier University",
        "Loading Data",
        "Develop",
        "AWS Handling Hadoop",
        "Amazon",
        "AWS CLI Auto Scaling",
        "Review",
        "Developed MapReduce",
        "Python Hortonworks Hadoop HDFS MapReduce YARN Spark Pig Hive",
        "SparkHadoop Developer SparkHadoop",
        "AWS Red",
        "SQL",
        "RDD",
        "Hadoop",
        "Data",
        "Spark RDD",
        "lDeveloperspan SparkHadoop Developer CVS Health Marlborough MA Work",
        "Bulk Load",
        "AWS Red shift Environment Hadoop Python HDFS Amazon Web Services AWS YARN Spark Pig Hive Sqoop",
        "XML",
        "MapReduce",
        "NFS",
        "VPC",
        "S3 Development Acceptance Integration and Production AWS Endpoints Developed",
        "Test Cases for JUnit Testing Wrote",
        "Maven Environment Python",
        "Hadoop Cluster",
        "Sqoop Created Hive",
        "RDS",
        "Present Assisted",
        "Performed Export",
        "HighPerformance Clusters",
        "HBase",
        "Automated",
        "Sublime Hadoop Developer Eliza Corporation Danvers",
        "Big Data",
        "Hive",
        "Spark",
        "Amazon AWS",
        "SSIS"
    ],
    "experience": "Experience SparkHadoop Developer CVS Health Woonsocket RI October 2016 to Present Assisted in different data Modeling and Data Warehouse design and development Used Sqoop to connect to the Sql Server and move the pivoted data to Hive tables and stored in different file formats like Avro SerDe Text etc Using Spark API over Hortonworks Hadoop YARN to perform analytics on data in Hive Analyzing of large volumes of structured data using SparkSQL Enhanced and optimized product Spark code to aggregate group and run data mining tasks using the Spark framework Created internal and external Hive tables and defined static and dynamic partitions for optimized performance Creating Hive tables to import large data sets from various relational databases using Sqoop and export the analyzed data back for visualization and report Tableau generation by the BI team Wrote Python scripts to parse JSON documents and load the data in database Worked on Apache spark writing python applications to convert txt xls files and parse data into JSON format Developed the Shell Perl and Python Scripts Linux UNIX to execute jobs Exploring with the Spark improving the performance and optimization of the existing algorithms in Hadoop using Spark Context SparkSQL Data Frame Pair RDDs Spark YARN Loading data into Spark RDD and do in memory data Computation to generate the Output response Loading the data to HBASE by using bulk load and HBASE API Used PySpark to expose spark API to Python Developed Spark code and SparkSQL for faster testing and processing of data Experiencing in Oozie a workflow scheduler to manage Hadoop jobs by Direct Acyclic Graph DAG of actions with control flows Loading data into Spark RDD and do in memory data Computation to generate the Output response Developed Spark code using Python for faster processing of data on Hive Worked on Integrating Talend and SSIS with Hadoop and performed ETL operations Developed MapReduce jobs in Python for data cleaning and data processing Used spark cluster to manipulate RDDS Resilient Distributed Datasets and also used concepts of RDD partitions Connecting MySQL database through spark driver Develop applications that use MongoDB database and Pymong Wrote Python modules to extractload asset data from the MySQL source database Developed and maintain several batch jobs to run automatically depending on business requirements Involved in cluster maintenance commissioning decommissioning Data nodes Troubleshooting Manage and review data backups Manage and review Hadoop log files Environment Python Hortonworks Hadoop HDFS MapReduce YARN Spark Pig Hive Sqoop Kafka HBase Bedrock Oozie Flume Tableau SQL Scripting Linux Shell Scripting Sublime Hadoop Developer Eliza Corporation Danvers MA July 2015 to June 2016 Installed and configured Apache Hadoop Hive and Pig environment on AWS Handling Hadoop cluster setup involved in start to end process of installation configuration and monitoring Build servers using AWS Importing volumes launching EC2 RDS creating security groups autoscaling load balancers ELBs in the defined virtual private connection Migrating physical LinuxWindows servers to cloud AWS and testing Performed Export and import of data into simple storage service S3 Development Acceptance Integration and Production AWS Endpoints Developed complex MapReduce streaming jobs using Java language that are implemented Using Hive and Pig Loading data into Spark RDD and do in memory data Computation to generate the Output response Optimized MapReduce Jobs to use HDFS efficiently by using various compression mechanisms Collected and aggregated large amounts of web log data from different sources such as web servers mobile using Apache Flume and stored the data into HDFS for analysis Handled importing of data from various data sources performed transformations using Hive MapReduce loaded data into HDFS and Extracted the data from MySQL into HDFS using Sqoop Automating backups by the shell for Linux to transfer data in S3 bucket Designed and crafted HighPerformance Clusters in AWS along with AWS Red shift Experience in AWS EC2 configuring the servers for Auto scaling and Elastic load balancing AWS CLI Auto Scaling and Cloud Watch Monitoring creation and update Develop Hive scripts for end user analyst requirements to perform ad hoc analysis Plan deploy monitor and maintain Amazon AWS cloud infrastructure consisting of multiple EC2 nodes and VMware Vms as required in the environment Expertise in AWS data migration between different database platforms like SQL Server to Amazon Aurora using RDS tool Optimized MapReduce Jobs to use HDFS efficiently by using various compression mechanisms Loading Data into HBase using Bulk Load and Nonbulk load Experiencing in Oozie a workflow scheduler to manage Hadoop jobs by Direct Acyclic Graph DAG of actions with control flows Handled importing of data from various data sources performed transformations using Hive MapReduce loaded data into HDFS and Extracted the data from MySQL into HDFS using Sqoop Created Hive tables partitions and loaded the data to analyze using HiveQL queries Configuring EC2 instances in VPC network managing security through IAM and Monitoring servers health through Cloud Watch Development Acceptance Integration and Production AWS Endpoints Automating backups by the shell for Linux to transfer data in S3 bucket Designed and crafted HighPerformance Clusters in AWS along with AWS Red shift Environment Hadoop Python HDFS Amazon Web Services AWS YARN Spark Pig Hive Sqoop Oozie Flume Linux Shell Scripting SparkSQL MongoDB Data Engineer Hadoop ABS Pvt ltd Mumbai Maharashtra May 2014 to March 2015 Exported data to a MySQL from HDFS using Sqoop and NFS mount approach Moved data from HDFS to Cassandra using Map Reduce and Bulk Output Format class Developed Map Reduce programs for applying business rules on the data Developed and executed hive queries for demoralizing the data Works with ETL workflow analysis of Big Data and loaded them into Hadoop cluster Installed and configured Hadoop Cluster for development and testing environment Implemented Fair scheduler on the Job tracker to share the resources of the cluster for the map reduces jobs given by the users Automated the workflow using shell scripts Performance tuning of the Hive queries written by other developer Prototype various applications that utilize modern Big Data tools Environment Java Python Hadoop Map Reduce HDFS Hive Pig Sqoop Cloudera MongoDB HUE Linux Java Python Developer GBS Technologies January 2012 to March 2014 Responsible for understanding the scope of the project and requirement gathering Review and analyze the design and implementation of software componentsapplications and outline the development process strategies Used Spring JDBC to write some DAO classes to interact with the database to access account information Created entire application using Python Django MySQL and Linux Involved in creation of Test Cases for JUnit Testing Wrote python scripts to parse XML documents and load the data in database Designed and configured database and back end applications and programs Used Oracle as Database used Toad for queries execution and involved in writing SQL scripts PLSQL code for procedures and functions Used CVS Perforce as configuration management tool for code versioning and release Developed application using Eclipse and used build and deploy tool as Maven Environment Python Java15 J2EE Servlets JSP XML spring 30 Design Patterns Log4j CVS Maven Eclipse and Oracle 11 g Education Master of Science in Computer Science Rivier University Skills LINUX 4 years MYSQL 4 years PYTHON 4 years JAVA 3 years SQL 3 years Additional Information Areas of Expertise Hadoop Ecosystems HDFS YARN Spark Map Reduce Hive Pig Zookeeper Sqoop Oozie Bedrock Flume Kafka Impala Ambari HUE MongoDB HBase Amazon Web Services Redshift EMR EC2 S3 RDS Cloud Search Data Pipeline Lambda Languages Python Java HiveQL Pig Latin Advanced PLSQL SQL Databases Oracle MSSQL Server MySQL MSAccess PostgreSQL NoSQL Teradata Tools Eclipse Net Beans SQL Developer IntelliJ MS Visual Studio Sublime BI Tools Tableau Qlikview Jinfonet JReport Hadoop Platforms Hortonworks Amazon Web services AWS Cloudera Platforms Linux Red HatUbuntuCentos Windows Macintosh Linux Solaris",
    "extracted_keywords": [
        "SparkHadoop",
        "Developer",
        "SparkHadoop",
        "span",
        "lDeveloperspan",
        "SparkHadoop",
        "Developer",
        "CVS",
        "Health",
        "Marlborough",
        "MA",
        "Work",
        "Experience",
        "SparkHadoop",
        "Developer",
        "CVS",
        "Health",
        "Woonsocket",
        "RI",
        "October",
        "Present",
        "Assisted",
        "data",
        "Modeling",
        "Data",
        "Warehouse",
        "design",
        "development",
        "Used",
        "Sqoop",
        "Sql",
        "Server",
        "data",
        "Hive",
        "tables",
        "file",
        "formats",
        "Avro",
        "SerDe",
        "Text",
        "Spark",
        "API",
        "Hortonworks",
        "Hadoop",
        "YARN",
        "analytics",
        "data",
        "Hive",
        "Analyzing",
        "volumes",
        "data",
        "SparkSQL",
        "Enhanced",
        "product",
        "Spark",
        "code",
        "group",
        "data",
        "mining",
        "tasks",
        "Spark",
        "framework",
        "Hive",
        "tables",
        "partitions",
        "performance",
        "Hive",
        "tables",
        "data",
        "sets",
        "databases",
        "Sqoop",
        "data",
        "visualization",
        "Tableau",
        "generation",
        "BI",
        "team",
        "Wrote",
        "Python",
        "documents",
        "data",
        "database",
        "Apache",
        "spark",
        "python",
        "applications",
        "txt",
        "xls",
        "files",
        "data",
        "format",
        "Shell",
        "Perl",
        "Python",
        "Scripts",
        "Linux",
        "UNIX",
        "jobs",
        "Spark",
        "performance",
        "optimization",
        "algorithms",
        "Hadoop",
        "Spark",
        "Context",
        "SparkSQL",
        "Data",
        "Frame",
        "Pair",
        "Spark",
        "YARN",
        "Loading",
        "data",
        "Spark",
        "RDD",
        "memory",
        "data",
        "Computation",
        "Output",
        "response",
        "data",
        "HBASE",
        "load",
        "HBASE",
        "API",
        "PySpark",
        "spark",
        "API",
        "Python",
        "Developed",
        "Spark",
        "code",
        "SparkSQL",
        "testing",
        "processing",
        "data",
        "Oozie",
        "workflow",
        "scheduler",
        "Hadoop",
        "jobs",
        "Direct",
        "Acyclic",
        "Graph",
        "DAG",
        "actions",
        "control",
        "Loading",
        "data",
        "Spark",
        "RDD",
        "memory",
        "data",
        "Computation",
        "Output",
        "response",
        "Developed",
        "Spark",
        "code",
        "Python",
        "processing",
        "data",
        "Hive",
        "Worked",
        "Talend",
        "SSIS",
        "Hadoop",
        "ETL",
        "operations",
        "MapReduce",
        "jobs",
        "Python",
        "data",
        "cleaning",
        "data",
        "spark",
        "cluster",
        "RDDS",
        "Resilient",
        "Distributed",
        "Datasets",
        "concepts",
        "RDD",
        "partitions",
        "MySQL",
        "database",
        "spark",
        "driver",
        "Develop",
        "applications",
        "database",
        "Pymong",
        "Wrote",
        "Python",
        "modules",
        "extractload",
        "asset",
        "data",
        "MySQL",
        "source",
        "database",
        "batch",
        "jobs",
        "business",
        "requirements",
        "cluster",
        "maintenance",
        "Data",
        "nodes",
        "Troubleshooting",
        "Manage",
        "data",
        "backups",
        "Manage",
        "Hadoop",
        "log",
        "files",
        "Environment",
        "Python",
        "Hortonworks",
        "Hadoop",
        "HDFS",
        "MapReduce",
        "YARN",
        "Spark",
        "Pig",
        "Hive",
        "Sqoop",
        "Kafka",
        "HBase",
        "Bedrock",
        "Oozie",
        "Flume",
        "Tableau",
        "SQL",
        "Scripting",
        "Linux",
        "Shell",
        "Scripting",
        "Sublime",
        "Hadoop",
        "Developer",
        "Eliza",
        "Corporation",
        "MA",
        "July",
        "June",
        "Installed",
        "Apache",
        "Hadoop",
        "Hive",
        "Pig",
        "environment",
        "AWS",
        "Handling",
        "Hadoop",
        "cluster",
        "setup",
        "start",
        "process",
        "installation",
        "configuration",
        "Build",
        "servers",
        "AWS",
        "volumes",
        "EC2",
        "RDS",
        "security",
        "groups",
        "load",
        "balancers",
        "ELBs",
        "connection",
        "LinuxWindows",
        "servers",
        "AWS",
        "Performed",
        "Export",
        "import",
        "data",
        "storage",
        "service",
        "S3",
        "Development",
        "Acceptance",
        "Integration",
        "Production",
        "AWS",
        "Endpoints",
        "MapReduce",
        "streaming",
        "jobs",
        "Java",
        "language",
        "Hive",
        "Pig",
        "Loading",
        "data",
        "Spark",
        "RDD",
        "memory",
        "data",
        "Computation",
        "Output",
        "response",
        "MapReduce",
        "Jobs",
        "HDFS",
        "compression",
        "mechanisms",
        "amounts",
        "web",
        "log",
        "data",
        "sources",
        "web",
        "servers",
        "mobile",
        "Apache",
        "Flume",
        "data",
        "HDFS",
        "analysis",
        "importing",
        "data",
        "data",
        "sources",
        "transformations",
        "Hive",
        "MapReduce",
        "data",
        "HDFS",
        "data",
        "MySQL",
        "HDFS",
        "Sqoop",
        "Automating",
        "backups",
        "shell",
        "Linux",
        "data",
        "S3",
        "bucket",
        "HighPerformance",
        "Clusters",
        "AWS",
        "AWS",
        "shift",
        "Experience",
        "AWS",
        "EC2",
        "servers",
        "Auto",
        "scaling",
        "load",
        "AWS",
        "CLI",
        "Auto",
        "Scaling",
        "Cloud",
        "Watch",
        "creation",
        "Develop",
        "Hive",
        "scripts",
        "end",
        "user",
        "analyst",
        "requirements",
        "ad",
        "analysis",
        "Plan",
        "monitor",
        "Amazon",
        "AWS",
        "cloud",
        "infrastructure",
        "EC2",
        "nodes",
        "VMware",
        "Vms",
        "environment",
        "Expertise",
        "AWS",
        "data",
        "migration",
        "database",
        "platforms",
        "SQL",
        "Server",
        "Amazon",
        "Aurora",
        "RDS",
        "tool",
        "MapReduce",
        "Jobs",
        "HDFS",
        "compression",
        "mechanisms",
        "Loading",
        "Data",
        "HBase",
        "Bulk",
        "Load",
        "load",
        "Oozie",
        "workflow",
        "scheduler",
        "Hadoop",
        "jobs",
        "Direct",
        "Acyclic",
        "Graph",
        "DAG",
        "actions",
        "control",
        "importing",
        "data",
        "data",
        "sources",
        "transformations",
        "Hive",
        "MapReduce",
        "data",
        "HDFS",
        "data",
        "MySQL",
        "HDFS",
        "Sqoop",
        "Created",
        "Hive",
        "tables",
        "partitions",
        "data",
        "HiveQL",
        "queries",
        "Configuring",
        "EC2",
        "instances",
        "VPC",
        "network",
        "security",
        "IAM",
        "Monitoring",
        "health",
        "Cloud",
        "Watch",
        "Development",
        "Acceptance",
        "Integration",
        "Production",
        "AWS",
        "Endpoints",
        "Automating",
        "backups",
        "shell",
        "Linux",
        "data",
        "S3",
        "bucket",
        "HighPerformance",
        "Clusters",
        "AWS",
        "AWS",
        "shift",
        "Environment",
        "Hadoop",
        "Python",
        "HDFS",
        "Amazon",
        "Web",
        "Services",
        "AWS",
        "YARN",
        "Spark",
        "Pig",
        "Hive",
        "Sqoop",
        "Oozie",
        "Flume",
        "Linux",
        "Shell",
        "Scripting",
        "SparkSQL",
        "Data",
        "Engineer",
        "Hadoop",
        "ABS",
        "Pvt",
        "ltd",
        "Mumbai",
        "Maharashtra",
        "May",
        "March",
        "data",
        "MySQL",
        "HDFS",
        "Sqoop",
        "NFS",
        "mount",
        "approach",
        "Moved",
        "data",
        "HDFS",
        "Cassandra",
        "Map",
        "Reduce",
        "Bulk",
        "Output",
        "Format",
        "class",
        "Developed",
        "Map",
        "programs",
        "business",
        "rules",
        "data",
        "hive",
        "queries",
        "data",
        "ETL",
        "workflow",
        "analysis",
        "Big",
        "Data",
        "Hadoop",
        "cluster",
        "Installed",
        "Hadoop",
        "Cluster",
        "development",
        "testing",
        "environment",
        "Fair",
        "scheduler",
        "Job",
        "tracker",
        "resources",
        "cluster",
        "map",
        "jobs",
        "users",
        "workflow",
        "scripts",
        "Performance",
        "tuning",
        "Hive",
        "queries",
        "developer",
        "Prototype",
        "applications",
        "Big",
        "Data",
        "tools",
        "Environment",
        "Java",
        "Python",
        "Hadoop",
        "Map",
        "HDFS",
        "Hive",
        "Pig",
        "Sqoop",
        "Cloudera",
        "MongoDB",
        "HUE",
        "Linux",
        "Java",
        "Python",
        "Developer",
        "GBS",
        "Technologies",
        "January",
        "March",
        "scope",
        "project",
        "requirement",
        "Review",
        "design",
        "implementation",
        "software",
        "componentsapplications",
        "development",
        "process",
        "strategies",
        "Spring",
        "JDBC",
        "DAO",
        "classes",
        "database",
        "access",
        "account",
        "information",
        "application",
        "Python",
        "Django",
        "MySQL",
        "Linux",
        "creation",
        "Test",
        "Cases",
        "JUnit",
        "Testing",
        "python",
        "scripts",
        "XML",
        "documents",
        "data",
        "database",
        "database",
        "end",
        "applications",
        "programs",
        "Oracle",
        "Database",
        "Toad",
        "queries",
        "execution",
        "SQL",
        "scripts",
        "PLSQL",
        "code",
        "procedures",
        "functions",
        "CVS",
        "Perforce",
        "configuration",
        "management",
        "tool",
        "code",
        "versioning",
        "application",
        "Eclipse",
        "build",
        "tool",
        "Maven",
        "Environment",
        "Python",
        "Java15",
        "J2EE",
        "Servlets",
        "JSP",
        "XML",
        "spring",
        "Design",
        "Patterns",
        "Log4j",
        "CVS",
        "Maven",
        "Eclipse",
        "Oracle",
        "g",
        "Education",
        "Master",
        "Science",
        "Computer",
        "Science",
        "Rivier",
        "University",
        "Skills",
        "LINUX",
        "years",
        "MYSQL",
        "years",
        "PYTHON",
        "years",
        "years",
        "SQL",
        "years",
        "Additional",
        "Information",
        "Areas",
        "Expertise",
        "Hadoop",
        "Ecosystems",
        "HDFS",
        "YARN",
        "Spark",
        "Map",
        "Reduce",
        "Hive",
        "Pig",
        "Zookeeper",
        "Sqoop",
        "Oozie",
        "Bedrock",
        "Flume",
        "Kafka",
        "Impala",
        "Ambari",
        "HUE",
        "MongoDB",
        "HBase",
        "Amazon",
        "Web",
        "Services",
        "Redshift",
        "EMR",
        "EC2",
        "S3",
        "RDS",
        "Cloud",
        "Search",
        "Data",
        "Pipeline",
        "Lambda",
        "Languages",
        "Python",
        "Java",
        "HiveQL",
        "Pig",
        "Latin",
        "Advanced",
        "PLSQL",
        "SQL",
        "Databases",
        "Oracle",
        "MSSQL",
        "Server",
        "MySQL",
        "MSAccess",
        "PostgreSQL",
        "NoSQL",
        "Teradata",
        "Tools",
        "Eclipse",
        "Net",
        "Beans",
        "SQL",
        "Developer",
        "IntelliJ",
        "MS",
        "Visual",
        "Studio",
        "Sublime",
        "BI",
        "Tools",
        "Tableau",
        "Qlikview",
        "Jinfonet",
        "JReport",
        "Hadoop",
        "Platforms",
        "Hortonworks",
        "Amazon",
        "Web",
        "services",
        "AWS",
        "Cloudera",
        "Platforms",
        "Linux",
        "Red",
        "HatUbuntuCentos",
        "Windows",
        "Macintosh",
        "Linux",
        "Solaris"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T20:20:37.496455",
    "resume_data": "SparkHadoop Developer SparkHadoop span lDeveloperspan SparkHadoop Developer CVS Health Marlborough MA Work Experience SparkHadoop Developer CVS Health Woonsocket RI October 2016 to Present Assisted in different data Modeling and Data Warehouse design and development Used Sqoop to connect to the Sql Server and move the pivoted data to Hive tables and stored in different file formats like Avro SerDe Text etc Using Spark API over Hortonworks Hadoop YARN to perform analytics on data in Hive Analyzing of large volumes of structured data using SparkSQL Enhanced and optimized product Spark code to aggregate group and run data mining tasks using the Spark framework Created internal and external Hive tables and defined static and dynamic partitions for optimized performance Creating Hive tables to import large data sets from various relational databases using Sqoop and export the analyzed data back for visualization and report Tableau generation by the BI team Wrote Python scripts to parse JSON documents and load the data in database Worked on Apache spark writing python applications to convert txt xls files and parse data into JSON format Developed the Shell Perl and Python Scripts Linux UNIX to execute jobs Exploring with the Spark improving the performance and optimization of the existing algorithms in Hadoop using Spark Context SparkSQL Data Frame Pair RDDs Spark YARN Loading data into Spark RDD and do in memory data Computation to generate the Output response Loading the data to HBASE by using bulk load and HBASE API Used PySpark to expose spark API to Python Developed Spark code and SparkSQL for faster testing and processing of data Experiencing in Oozie a workflow scheduler to manage Hadoop jobs by Direct Acyclic Graph DAG of actions with control flows Loading data into Spark RDD and do in memory data Computation to generate the Output response Developed Spark code using Python for faster processing of data on Hive Worked on Integrating Talend and SSIS with Hadoop and performed ETL operations Developed MapReduce jobs in Python for data cleaning and data processing Used spark cluster to manipulate RDDS Resilient Distributed Datasets and also used concepts of RDD partitions Connecting MySQL database through spark driver Develop applications that use MongoDB database and Pymong Wrote Python modules to extractload asset data from the MySQL source database Developed and maintain several batch jobs to run automatically depending on business requirements Involved in cluster maintenance commissioning decommissioning Data nodes Troubleshooting Manage and review data backups Manage and review Hadoop log files Environment Python Hortonworks Hadoop HDFS MapReduce YARN Spark Pig Hive Sqoop Kafka HBase Bedrock Oozie Flume Tableau SQL Scripting Linux Shell Scripting Sublime Hadoop Developer Eliza Corporation Danvers MA July 2015 to June 2016 Installed and configured Apache Hadoop Hive and Pig environment on AWS Handling Hadoop cluster setup involved in start to end process of installation configuration and monitoring Build servers using AWS Importing volumes launching EC2 RDS creating security groups autoscaling load balancers ELBs in the defined virtual private connection Migrating physical LinuxWindows servers to cloud AWS and testing Performed Export and import of data into simple storage service S3 Development Acceptance Integration and Production AWS Endpoints Developed complex MapReduce streaming jobs using Java language that are implemented Using Hive and Pig Loading data into Spark RDD and do in memory data Computation to generate the Output response Optimized MapReduce Jobs to use HDFS efficiently by using various compression mechanisms Collected and aggregated large amounts of web log data from different sources such as web servers mobile using Apache Flume and stored the data into HDFS for analysis Handled importing of data from various data sources performed transformations using Hive MapReduce loaded data into HDFS and Extracted the data from MySQL into HDFS using Sqoop Automating backups by the shell for Linux to transfer data in S3 bucket Designed and crafted HighPerformance Clusters in AWS along with AWS Red shift Experience in AWS EC2 configuring the servers for Auto scaling and Elastic load balancing AWS CLI Auto Scaling and Cloud Watch Monitoring creation and update Develop Hive scripts for end user analyst requirements to perform ad hoc analysis Plan deploy monitor and maintain Amazon AWS cloud infrastructure consisting of multiple EC2 nodes and VMware Vms as required in the environment Expertise in AWS data migration between different database platforms like SQL Server to Amazon Aurora using RDS tool Optimized MapReduce Jobs to use HDFS efficiently by using various compression mechanisms Loading Data into HBase using Bulk Load and Nonbulk load Experiencing in Oozie a workflow scheduler to manage Hadoop jobs by Direct Acyclic Graph DAG of actions with control flows Handled importing of data from various data sources performed transformations using Hive MapReduce loaded data into HDFS and Extracted the data from MySQL into HDFS using Sqoop Created Hive tables partitions and loaded the data to analyze using HiveQL queries Configuring EC2 instances in VPC network managing security through IAM and Monitoring servers health through Cloud Watch Development Acceptance Integration and Production AWS Endpoints Automating backups by the shell for Linux to transfer data in S3 bucket Designed and crafted HighPerformance Clusters in AWS along with AWS Red shift Environment Hadoop Python HDFS Amazon Web Services AWS YARN Spark Pig Hive Sqoop Oozie Flume Linux Shell Scripting SparkSQL MongoDB Data Engineer Hadoop ABS Pvt ltd Mumbai Maharashtra May 2014 to March 2015 Exported data to a MySQL from HDFS using Sqoop and NFS mount approach Moved data from HDFS to Cassandra using Map Reduce and Bulk Output Format class Developed Map Reduce programs for applying business rules on the data Developed and executed hive queries for demoralizing the data Works with ETL workflow analysis of Big Data and loaded them into Hadoop cluster Installed and configured Hadoop Cluster for development and testing environment Implemented Fair scheduler on the Job tracker to share the resources of the cluster for the map reduces jobs given by the users Automated the workflow using shell scripts Performance tuning of the Hive queries written by other developer Prototype various applications that utilize modern Big Data tools Environment Java Python Hadoop Map Reduce HDFS Hive Pig Sqoop Cloudera MongoDB HUE Linux Java Python Developer GBS Technologies January 2012 to March 2014 Responsible for understanding the scope of the project and requirement gathering Review and analyze the design and implementation of software componentsapplications and outline the development process strategies Used Spring JDBC to write some DAO classes to interact with the database to access account information Created entire application using Python Django MySQL and Linux Involved in creation of Test Cases for JUnit Testing Wrote python scripts to parse XML documents and load the data in database Designed and configured database and back end applications and programs Used Oracle as Database used Toad for queries execution and involved in writing SQL scripts PLSQL code for procedures and functions Used CVS Perforce as configuration management tool for code versioning and release Developed application using Eclipse and used build and deploy tool as Maven Environment Python Java15 J2EE Servlets JSP XML spring 30 Design Patterns Log4j CVS Maven Eclipse and Oracle 11g Education Master of Science in Computer Science Rivier University Skills LINUX 4 years MYSQL 4 years PYTHON 4 years JAVA 3 years SQL 3 years Additional Information Areas of Expertise Hadoop Ecosystems HDFS YARN Spark Map Reduce Hive Pig Zookeeper Sqoop Oozie Bedrock Flume Kafka Impala Ambari HUE MongoDB HBase Amazon Web Services Redshift EMR EC2 S3 RDS Cloud Search Data Pipeline Lambda Languages Python Java HiveQL Pig Latin Advanced PLSQL SQL Databases Oracle MSSQL Server MySQL MSAccess PostgreSQL NoSQL Teradata Tools Eclipse Net Beans SQL Developer IntelliJ MS Visual Studio Sublime BI Tools Tableau Qlikview Jinfonet JReport Hadoop Platforms Hortonworks Amazon Web services AWS Cloudera Platforms Linux Red HatUbuntuCentos Windows Macintosh Linux Solaris",
    "unique_id": "758e6e70-089f-437f-8c8c-a4f5d1d68b98"
}