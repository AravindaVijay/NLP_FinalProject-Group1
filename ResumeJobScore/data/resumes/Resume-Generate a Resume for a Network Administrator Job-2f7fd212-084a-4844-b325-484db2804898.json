{
    "clean_data": "Sr HadoopScala Developer Sr HadoopScala span lDeveloperspan Sr HadoopScala Developer Safeway Phoenix AZ Having 8 years of Experience in IT industry in Designing Developing and Maintaining using Big data Technologies like Hadoop Spark Ecosystems and JavaJ2EE Technologies Extensively worked on Spark using Scala on cluster for computational analytics installed it on top of Hadoop performed advanced analytical application by making use of Sparkwith Hive and SQLOracle Excellent Programming skills at a higher level of abstraction using Scala Java and Python Extensive experience in working with various distributions of Hadoop enterprise versions of ClouderaCDH4CDH5 Horton works and good knowledge on MAPR distribution IBM Big Insights and Amazons EMR Elastic Map Reduce Working knowledge of Amazons Elastic Cloud ComputeEC2 infrastructure for computational tasks and Simple Storage Service S3 as Storage mechanism Experienced in implementing scheduler using Oozie Airflow Crontab and Shell scripts Good working experience in importing data using Sqoop SFTP from various sources like RDMS Teradata Mainframes Oracle Netezza to HDFS and performed transformations on it using Hive Pig and Spark Extensive experience in importing and exporting streaming data into HDFS using stream processing platforms like Flume and Kafka messaging system Strong experience and knowledge of real time data analytics using Spark Streaming Kafka and Flume Extensively worked on Spark streaming and Apache Kafka to fetch live stream data Expertise in writing SparkRDD transformations Actions Data Frames Case classes for the required input data and performed the data transformations using SparkCore Experience in integrating Hive queries into Spark environment using Spark SQL Expertise in performing real time analytics on big data using HBase and Cassandra Developed customized UDFs and UDAFs in java to extend Pig and Hive core functionality Proficient in NoSQL databases including HBase Cassandra MongoDB and its integration with Hadoop cluster Good experience in optimizing Map Reduce algorithms using Mappers Reducers combiners and practitioners to deliver the best results for the large datasets Extracted data from various data source including OLEDB Excel Flat files and XML Experienced in using build tools like Ant SBT Log4j Maven to build and deploy applications into the server Had competency in using Chef Puppet and Ansible configuration and automation tools Configured and administered CI tools like Jenkins Hudson Bambino for automated builds Proficient in developing deploying and managing the SOLR from development to production Experience in Enterprise search using SOLR to implement full text search with advanced text analysis faceted search filtering using advanced features like dismax extended dismax and grouping Worked on data warehousing and ETL tools like Informatica Talend and Pentaho Designed ETL workflows on Tableau Deployed data from various sources to HDFS Working experience on Test Data Management tools HP Quality Center HPALM Load Runner QTP and Selenium Worked on ELK stack like Elastic search Logstash Kibana for log management Worked on various programming languages using IDEs like Eclipse NetBeans and Intellij Experience in Software Design Development and Implementation of ClientServer Web based Applications using JSTL jQuery JavaScript Java Beans JDBC Struts PLSQL SQL HTML CSS PHP XML AJAX and had a birds eye view on React Java Script Library Used various Project Management services like JIRA for tracking issues bugs related to code and GitHub for various code reviews and Worked on various version control tools like CVS GIT PVCS SVN Experience with best practices of Web services development and Integration both REST and SOAP Generated various kinds of knowledge reports using Power BI and Qlik based on Business specification Experience in automated scripts using Unix shell scripting to perform database activities Experience in complete Software Development Life Cycle SDLC in both Waterfall and Agile methodologies Good understanding of all aspects of Testing such as Unit Regression Agile Whitebox Blackbox Authorized to work in the US for any employer Work Experience Sr HadoopScala Developer Safeway Phoenix AZ February 2016 to Present Responsibilities Experienced in designing and deployment of Hadoop cluster and different Big Data analytic tools including Pig Hive Cassandra Oozie Sqoop Kafka Spark Impala with Horton works distribution Performed source data transformations using Hive Supporting infrastructure environment comprising of RHEL and Solaris Involved in developing a Map Reduce framework that filters bad and unnecessary records Developed Spark scripts by using Scala shell commands as per the requirement Used Kafka to transfer data from different data systems to HDFS Created Spark jobs to see trends in data usage by users Responsible for generating actionable insights from complex data to drive real business results for various application teams Designed the Column families in Cassandra Ingested data from RDBMS and performed data transformations and then export the transformed data to Cassandra as per the business requirement Developed Spark code to using Scala and SparkSQL for faster processing and testing Experience in NoSQL ColumnOriented Databases like Cassandra and its Integration with Hadoop cluster Used Spark API over Hadoop YARN as execution engine for data analytics using Hive Exported the analyzed data to the relational databases using Sqoop to further visualize and generate reports for the BI team Good experience with Talend open studio for designing ETL Jobs for Processing of data Experience in processing large volume of data and skills in parallel execution of process using Talend functionality Worked on different file formats like Text files and Avro Created various kinds of reports using Power BI and Tableau based on the clients needs Worked on Agile Methodology projects extensively Experience designing and executing time driven and data driven Oozie workflows Setting up Kerberos principals and testing HDFS Hive Pig and MapReduce access for the new users Experienced in working with Spark eco system using SCALA and HIVE Queries on different data formats like Text file and parquet Collected the logs data from web servers and integrated in to HDFS using Flume Developed Hive scripts in Hive QL to denormalize and aggregate the data Implemented mapreduce counters to gather metrics of good records and bad records Work experience with cloud infrastructure like Amazon Web Services AWS Developed customized UDFs in java to extend Hive and Pig functionality Jobs via Zeppelin notebooks mentored and guided offshore team in troubleshooting and finetuning Spark Experience in importing data from various data sources like Mainframes Teradata Oracle and Netezza using Sqoop SFTP performed transformations using Hive Pig and Spark and loaded data into HDFS Extracted the data from Teradata into HDFSDatabasesDashboards using SPARK STREAMING Implemented best income logic using Pig scripts Worked on different file formats ORCFILE Parquet Avro and different Compression Codecs GZIP SNAPPY LZO Created applications using Kafka which monitors consumer lag within Apache Kafka clusters Used in production by multiple companies Using SparkStreaming APIs to perform transformations and actions on the fly for building the common learner data model which gets the data from Kafka in near real time and Persists into Cassandra Design and document RESTHTTP SOAP APIs including JSON data formats and API versioning strategy Experience in using Apache Kafka for collecting aggregating and moving large amounts of data from application servers Used Hibernate ORM framework with Spring framework for data persistence and transaction management Performance analysis of Spark streaming and batch jobs by using Spark tuning parameters Used React Bindings for embracing Redux Worked towards creating real time data streaming solutions using Apache SparkSpark Streaming Kafka Worked along with the Hadoop Operations team in Hadoop cluster planning installation maintenance monitoring and upgrades Used File System check FSCK to check the health of files in HDFS Worked in Agile development environment in sprint cycles of two weeks by dividing and organizing tasks Participated in daily scrum and other design related meetings Environment Hadoop Hive Map Reduce Sqoop Kafka Spark Yarn Pig Cassandra Oozie shell Scripting Scala Maven Java React JS JUnit agile methodologies Horton works Soap NIFI Teradata MySQL Sr Spark Developer Digital Realty Trust November 2014 to February 2016 Responsibilities Developed data pipeline using SPARK Apache Kafka to ingest customer behavioral data and financial histories into HADOOP cluster for analysis Collected data using SPARK STREAMING from AWS S3 bucket in nearrealtime and performed necessary Transformations and Aggregation to build the common learner data model and persist the data in HDFS Explored the usage of SPARK for improving the performance and optimization of the existing algorithms in HADOOP using SPARK Worked with SPARK STREAMING to ingest data into Spark Engine Imported the data from different sources like AWS S3 Local file system into Spark RDD Experience working with various services in Azure like Data lake to store and analyze the data Hands on experience configuring Hadoop cluster in professional environment and on Azure using HDInsight Developed Spark applications using SCALA with SPARKSQLSTREAMINGAPI for faster testing and processing of data Involved in converting HiveSQL queries into Spark Transformations using SPARK RDDs and SCALA Worked on the SPARK SQL and Spark Streaming modules of SPARKand used SCALA to write code for all Spark use cases Developed Spark code using Scala and Spark SQL for faster testing and data processing Worked on converting PLSQL code into Scala code and converted PLSQL queries into Hive queries Used IMPALA to give parallel processing database technology on top of Hadoop ecosystem Monitored cluster Health status and assisted in planed Hadoop cluster maintenance activities such as upgrades expansions configurations etc Worked closely with infrastructure network database and business intelligence and application teams to ensure business applications are highly available and performing within agreed on service levels Configure Flume to ingest log file data into HDFS Involved in using SQOOP for importing and exporting data between RDBMS and HDFS Handle data exchange between HDFS and RDBMS Write Spark applications in Scala to interact with MYSQL database using Spark SQL Extensively used Hive for ETL Transformations and optimized Hive Queries Experience in Hive partitioning bucketing and collections perform different types of joins on Hive tables Performed hive performance tuning aspects like Map Join Cost Based Optimization and Columnar Level Statistics Good Experience in loading data from LINUXfile system to HDFS Used ECLIPSE and Sbt to build the application Environment Spark Hive Hive UDFs SparkSql Spark Streaming Spark Yarn Spark Sql HBase Sqoop Kafka Aws Ec2 S3 Cloudera Scala IDEEclipse Linux Shell Scripting Hdfs Azure  Developer Novartis Corporation October 2013 to November 2014 Responsibilities Extensively involved in Installation and configuration of Cloudera distribution Hadoop Name Node Secondary Name Node Job Tracker Task Trackers and Data Nodes Developed Map Reduce programs in Java and Sqoop the data from ORACLE database Responsible for building scalable distributed data solutions using Hadoop Written various Hive and Pig scripts Moved data from HDFS to Cassandra using Map Reduce and Bulk Output Format class Experienced with different scripting language like Python and shell scripts Developed various Python scripts to find vulnerabilities with SQL Queries by doing SQL injection permission checks and performance analysis Installed Oozie workflow engine to run multiple Hive and Pig jobs which run independently with time and data availability Experienced with handling administration activations using Cloudera manager Expertise in understanding Partitions Bucketing concepts in Hive Used Oozie Scheduler system to automate the pipeline workflow and orchestrate the Map Reduces jobs that extract the data on a timely manner Responsible for loading data from UNIX file system to HDFS Analyzed the weblog data using the HiveQL integrated Oozie with the rest of the Hadoop stack Utilized cluster coordination services through Zookeeper Got good experience with various NoSQL databases and Comprehensive knowledge in process improvement normalizationdenormalization data extraction data cleansing data manipulation Experience with creating script for data modeling and data import and export Extensive experience in deploying managing and developing MongoDB clusters Created Partitioned Hive tables and worked on them using HiveQL Developed Shell scripts to automate routine DBA tasks Used Maven extensively for building jar files of MapReduce programs and deployed to Cluster Responsible for cluster maintenance adding and removing cluster nodes cluster monitoring and troubleshooting managing and reviewing data backups and Hadoop log files Environment HDFS Map Reduce Pig Hive Oozie Sqoop Flume HBase Java Maven Avro Cloudera Eclipse and Shell Scripting JAVAETL Developer ICICI Bank October 2011 to August 2013 Responsibilities Developed Maven scripts to build and deploy the application Developed Spring MVC controllers for all the modules SAS scripts on UNIX are run and the output datasets are exported into SAS Implemented jQuery validator components Extracted data from Oracle as one of the source databases Using Data stage ETL tool to copy data from Teradata to Netezza Created ETL Data mapping spreadsheets describing column level transformation details to load data from Teradata Landing zone tables to the tables in Party and Policy subject area of EDW based on SAS Insurance model Used JSON and XML documents with Mark logic NoSQL Database extensively REST API calls are made using NodeJS and Java API SAS data sets were constantly created and updated using the SET and UPDATE statements Built data transformation with SSIS including importing data from files Loaded the flat files data using Informatica to the staging area Created SHELL SCRIPTS for generic use Environment Java Spring MPP Windows XPNT Informatica Power center 9186 UNIX Teradata Oracle Designer Autosys Shell Quality Center 10 Java Developer Apollo Hospitals September 2010 to August 2011 Responsibilities Involved in the analysis design implementation and testing of the project Implemented the presentation layer with HTML XHTML and JavaScript Developed web components using JSP Servlets and JDBC Implemented database using SQL Server Implemented Spring IoC framework Developed Spring REST services for all the modules Developed custom SAML and SOAP integration for healthcare Validated the fields of user registration screen and login screen by writing JavaScript validations Used DAO and JDBC for database access Built responsive Web pages using Kendo UI mobile Designed dynamic and multibrowser compatible pages using HTML CSS jQuery JavaScript Require JS and Kendo UI Environment Oracle 11g Java 15 Struts Servlets HTML XML SQL J2EE JUnit Tomcat 6 Java JSP JDBC JavaScript MySQL Eclipse IDE Rest Jr Java Developer HR Portal June 2009 to July 2010 Responsibilities Development of two modules Leave Management and Performance Appraisal Management Developed the project using Spring MVC and Hibernate Framework Developed User management and different modules on requirement using HTML JavaScript AngularJS JSON and validations fixing the bugs and handling lot of Exceptions alone Participated in the enhancement and maintenance of Personal information Management Developed Excel based reports Leaves Status Appraisal status for the consumption of HR Manager Responsible for Implemented key functionalities in Leaves Management such as Auto Approval process if the Manager does not respond to the leave within 3 days of completion of the leave dates Implemented a workflowbased solution to pass the appraisal from the Employee to Manager untilHR Manager for final discussion Participation in Unit Testing and support for QA Testing Cycles Environment Java J2EE HTML Servlets Spring JSON Hibernate AngularJS JavaScript Oracle Web logic JMS Eclipse IDE Maven UNIX Junit Education Bachelors Skills JAVA 7 years ORACLE 7 years ETL 6 years EXTRACT TRANSFORM AND LOAD 6 years SQL 5 years Additional Information Technical Skills Big Data Ecosystem Hadoop MapReduce Pig Hive YARN Kafka Flume Sqoop Impala Oozie Zookeeper Spark Ambari Mahout MongoDB Cassandra Avro Storm Parquet and Snappy Hadoop Distributions Cloudera CDH3 CDH4 and CDH5 Horton works Map R and Apache Languages Java Python J ruby SQL HTML DHTML Scala JavaScript XML and CC No SQL Databases Cassandra MongoDB and HBase Java Technologies Servlets JavaBeans JSP JDBC JNDI EJB and struts Methodology Agile waterfall Development Build Tools Eclipse Ant Maven IntelliJ JUNIT and log4J DB Languages MySQL PLSQL PostgreSQL and Oracle RDBMS Teradata Oracle 9i10g11i MS SQL Server MySQL and DB2 Operating systems UNIX LINUX Mac OS and Windows Variants ETL Tools Talend Informatica Pentaho",
    "entities": [
        "Created Partitioned Hive",
        "Installed Oozie",
        "Spark Transformations",
        "Intellij Experience",
        "Responsibilities Developed",
        "Informatica",
        "Teradata Landing",
        "Horton",
        "Snappy Hadoop",
        "Mainframes Teradata Oracle",
        "SPARK",
        "Map Join Cost Based Optimization",
        "HADOOP",
        "BI",
        "FSCK",
        "HDFS",
        "UNIX",
        "IMPALA",
        "IDE Maven UNIX Junit Education",
        "SOLR",
        "Developed Spark",
        "Text",
        "Partitions Bucketing",
        "Party",
        "IBM",
        "JavaJ2EE Technologies",
        "UDAFs",
        "Test Data Management",
        "CVS GIT PVCS SVN",
        "SAS Insurance",
        "Hadoop",
        "SET",
        "HDFS Involved",
        "XML",
        "Hadoop Operations",
        "Developed Spring",
        "HDFS Analyzed",
        "Software Development Life Cycle SDLC",
        "Amazon Web Services AWS Developed",
        "Shell",
        "HBase",
        "CDH3",
        "ELK",
        "SSIS",
        "IDEEclipse",
        "Spark SQL Extensively",
        "SQL Server",
        "Netezza Created ETL Data",
        "SPARK Worked",
        "QA Testing Cycles Environment",
        "Avro Created",
        "SQLOracle Excellent Programming",
        "SparkSQL",
        "Informatica Talend",
        "Simple Storage Service S3",
        "Kerberos",
        "AWS S3",
        "HDFS Handle data exchange",
        "Cassandra Design",
        "Waterfall",
        "SparkCore",
        "HDFS Worked",
        "ORACLE",
        "XML Experienced",
        "Enterprise",
        "MAPR",
        "HDFS Created Spark",
        "Hibernate Framework Developed User",
        "HBase Java Technologies",
        "CDH5 Horton",
        "Hive Queries",
        "Hive Pig and",
        "Spark Experience",
        "SQL Queries",
        "Participation in Unit Testing",
        "Compression Codecs GZIP SNAPPY LZO Created",
        "Spark Streaming",
        "Talend",
        "Hadoop Written",
        "Additional Information Technical Skills Big Data Ecosystem Hadoop MapReduce Pig Hive",
        "SPARKand",
        "MVC",
        "Spark",
        "Chef Puppet",
        "SparkSql Spark Streaming Spark",
        "Sparkwith Hive",
        "API",
        "US",
        "Sqoop",
        "HIVE",
        "Power BI",
        "Created SHELL SCRIPTS",
        "Oracle",
        "Hive for ETL Transformations",
        "UPDATE",
        "Project Management",
        "HDFS Working",
        "Autosys Shell Quality Center",
        "java",
        "SAS",
        "SPARK STREAMING",
        "Ant SBT",
        "ClouderaCDH4CDH5 Horton",
        "Java Developer Apollo Hospitals",
        "Pig Hive Cassandra Oozie",
        "Leave Management",
        "Hive Supporting",
        "Collected",
        "CC No SQL Databases",
        "GitHub",
        "Spark RDD",
        "Hive Pig",
        "Oozie Airflow Crontab",
        "Spark Engine Imported",
        "Digital Realty Trust",
        "JDBC Implemented",
        "Present Responsibilities Experienced",
        "Used React Bindings",
        "CI",
        "Big Data",
        "Hive",
        "Used File System",
        "SQOOP",
        "JUNIT",
        "ETL Jobs for Processing of data",
        "HTML CSS jQuery JavaScript Require JS",
        "Qlik",
        "SPARK STREAMING Implemented",
        "ETL",
        "Kendo UI",
        "AWS S3 Local",
        "Cassandra Ingested",
        "Maven",
        "Performed",
        "Auto Approval",
        "Impala",
        "JavaScript",
        "the SPARK SQL",
        "SPARKSQLSTREAMINGAPI",
        "JSP Servlets",
        "Spark Streaming Kafka",
        "CSS",
        "Redux Worked",
        "Integration with Hadoop",
        "Zookeeper Got",
        "Cluster Responsible",
        "Leaves Management",
        "EDW",
        "REST",
        "MapReduce",
        "SCALA",
        "Spark Extensive",
        "RDBMS",
        "NoSQL",
        "Tableau",
        "Oozie Zookeeper Spark Ambari Mahout",
        "Integration",
        "Teradata",
        "Spark SQL Expertise",
        "NoSQL Database",
        "Software Design Development and Implementation of ClientServer",
        "Cloudera"
    ],
    "experience": "Experience in IT industry in Designing Developing and Maintaining using Big data Technologies like Hadoop Spark Ecosystems and JavaJ2EE Technologies Extensively worked on Spark using Scala on cluster for computational analytics installed it on top of Hadoop performed advanced analytical application by making use of Sparkwith Hive and SQLOracle Excellent Programming skills at a higher level of abstraction using Scala Java and Python Extensive experience in working with various distributions of Hadoop enterprise versions of ClouderaCDH4CDH5 Horton works and good knowledge on MAPR distribution IBM Big Insights and Amazons EMR Elastic Map Reduce Working knowledge of Amazons Elastic Cloud ComputeEC2 infrastructure for computational tasks and Simple Storage Service S3 as Storage mechanism Experienced in implementing scheduler using Oozie Airflow Crontab and Shell scripts Good working experience in importing data using Sqoop SFTP from various sources like RDMS Teradata Mainframes Oracle Netezza to HDFS and performed transformations on it using Hive Pig and Spark Extensive experience in importing and exporting streaming data into HDFS using stream processing platforms like Flume and Kafka messaging system Strong experience and knowledge of real time data analytics using Spark Streaming Kafka and Flume Extensively worked on Spark streaming and Apache Kafka to fetch live stream data Expertise in writing SparkRDD transformations Actions Data Frames Case classes for the required input data and performed the data transformations using SparkCore Experience in integrating Hive queries into Spark environment using Spark SQL Expertise in performing real time analytics on big data using HBase and Cassandra Developed customized UDFs and UDAFs in java to extend Pig and Hive core functionality Proficient in NoSQL databases including HBase Cassandra MongoDB and its integration with Hadoop cluster Good experience in optimizing Map Reduce algorithms using Mappers Reducers combiners and practitioners to deliver the best results for the large datasets Extracted data from various data source including OLEDB Excel Flat files and XML Experienced in using build tools like Ant SBT Log4j Maven to build and deploy applications into the server Had competency in using Chef Puppet and Ansible configuration and automation tools Configured and administered CI tools like Jenkins Hudson Bambino for automated builds Proficient in developing deploying and managing the SOLR from development to production Experience in Enterprise search using SOLR to implement full text search with advanced text analysis faceted search filtering using advanced features like dismax extended dismax and grouping Worked on data warehousing and ETL tools like Informatica Talend and Pentaho Designed ETL workflows on Tableau Deployed data from various sources to HDFS Working experience on Test Data Management tools HP Quality Center HPALM Load Runner QTP and Selenium Worked on ELK stack like Elastic search Logstash Kibana for log management Worked on various programming languages using IDEs like Eclipse NetBeans and Intellij Experience in Software Design Development and Implementation of ClientServer Web based Applications using JSTL jQuery JavaScript Java Beans JDBC Struts PLSQL SQL HTML CSS PHP XML AJAX and had a birds eye view on React Java Script Library Used various Project Management services like JIRA for tracking issues bugs related to code and GitHub for various code reviews and Worked on various version control tools like CVS GIT PVCS SVN Experience with best practices of Web services development and Integration both REST and SOAP Generated various kinds of knowledge reports using Power BI and Qlik based on Business specification Experience in automated scripts using Unix shell scripting to perform database activities Experience in complete Software Development Life Cycle SDLC in both Waterfall and Agile methodologies Good understanding of all aspects of Testing such as Unit Regression Agile Whitebox Blackbox Authorized to work in the US for any employer Work Experience Sr HadoopScala Developer Safeway Phoenix AZ February 2016 to Present Responsibilities Experienced in designing and deployment of Hadoop cluster and different Big Data analytic tools including Pig Hive Cassandra Oozie Sqoop Kafka Spark Impala with Horton works distribution Performed source data transformations using Hive Supporting infrastructure environment comprising of RHEL and Solaris Involved in developing a Map Reduce framework that filters bad and unnecessary records Developed Spark scripts by using Scala shell commands as per the requirement Used Kafka to transfer data from different data systems to HDFS Created Spark jobs to see trends in data usage by users Responsible for generating actionable insights from complex data to drive real business results for various application teams Designed the Column families in Cassandra Ingested data from RDBMS and performed data transformations and then export the transformed data to Cassandra as per the business requirement Developed Spark code to using Scala and SparkSQL for faster processing and testing Experience in NoSQL ColumnOriented Databases like Cassandra and its Integration with Hadoop cluster Used Spark API over Hadoop YARN as execution engine for data analytics using Hive Exported the analyzed data to the relational databases using Sqoop to further visualize and generate reports for the BI team Good experience with Talend open studio for designing ETL Jobs for Processing of data Experience in processing large volume of data and skills in parallel execution of process using Talend functionality Worked on different file formats like Text files and Avro Created various kinds of reports using Power BI and Tableau based on the clients needs Worked on Agile Methodology projects extensively Experience designing and executing time driven and data driven Oozie workflows Setting up Kerberos principals and testing HDFS Hive Pig and MapReduce access for the new users Experienced in working with Spark eco system using SCALA and HIVE Queries on different data formats like Text file and parquet Collected the logs data from web servers and integrated in to HDFS using Flume Developed Hive scripts in Hive QL to denormalize and aggregate the data Implemented mapreduce counters to gather metrics of good records and bad records Work experience with cloud infrastructure like Amazon Web Services AWS Developed customized UDFs in java to extend Hive and Pig functionality Jobs via Zeppelin notebooks mentored and guided offshore team in troubleshooting and finetuning Spark Experience in importing data from various data sources like Mainframes Teradata Oracle and Netezza using Sqoop SFTP performed transformations using Hive Pig and Spark and loaded data into HDFS Extracted the data from Teradata into HDFSDatabasesDashboards using SPARK STREAMING Implemented best income logic using Pig scripts Worked on different file formats ORCFILE Parquet Avro and different Compression Codecs GZIP SNAPPY LZO Created applications using Kafka which monitors consumer lag within Apache Kafka clusters Used in production by multiple companies Using SparkStreaming APIs to perform transformations and actions on the fly for building the common learner data model which gets the data from Kafka in near real time and Persists into Cassandra Design and document RESTHTTP SOAP APIs including JSON data formats and API versioning strategy Experience in using Apache Kafka for collecting aggregating and moving large amounts of data from application servers Used Hibernate ORM framework with Spring framework for data persistence and transaction management Performance analysis of Spark streaming and batch jobs by using Spark tuning parameters Used React Bindings for embracing Redux Worked towards creating real time data streaming solutions using Apache SparkSpark Streaming Kafka Worked along with the Hadoop Operations team in Hadoop cluster planning installation maintenance monitoring and upgrades Used File System check FSCK to check the health of files in HDFS Worked in Agile development environment in sprint cycles of two weeks by dividing and organizing tasks Participated in daily scrum and other design related meetings Environment Hadoop Hive Map Reduce Sqoop Kafka Spark Yarn Pig Cassandra Oozie shell Scripting Scala Maven Java React JS JUnit agile methodologies Horton works Soap NIFI Teradata MySQL Sr Spark Developer Digital Realty Trust November 2014 to February 2016 Responsibilities Developed data pipeline using SPARK Apache Kafka to ingest customer behavioral data and financial histories into HADOOP cluster for analysis Collected data using SPARK STREAMING from AWS S3 bucket in nearrealtime and performed necessary Transformations and Aggregation to build the common learner data model and persist the data in HDFS Explored the usage of SPARK for improving the performance and optimization of the existing algorithms in HADOOP using SPARK Worked with SPARK STREAMING to ingest data into Spark Engine Imported the data from different sources like AWS S3 Local file system into Spark RDD Experience working with various services in Azure like Data lake to store and analyze the data Hands on experience configuring Hadoop cluster in professional environment and on Azure using HDInsight Developed Spark applications using SCALA with SPARKSQLSTREAMINGAPI for faster testing and processing of data Involved in converting HiveSQL queries into Spark Transformations using SPARK RDDs and SCALA Worked on the SPARK SQL and Spark Streaming modules of SPARKand used SCALA to write code for all Spark use cases Developed Spark code using Scala and Spark SQL for faster testing and data processing Worked on converting PLSQL code into Scala code and converted PLSQL queries into Hive queries Used IMPALA to give parallel processing database technology on top of Hadoop ecosystem Monitored cluster Health status and assisted in planed Hadoop cluster maintenance activities such as upgrades expansions configurations etc Worked closely with infrastructure network database and business intelligence and application teams to ensure business applications are highly available and performing within agreed on service levels Configure Flume to ingest log file data into HDFS Involved in using SQOOP for importing and exporting data between RDBMS and HDFS Handle data exchange between HDFS and RDBMS Write Spark applications in Scala to interact with MYSQL database using Spark SQL Extensively used Hive for ETL Transformations and optimized Hive Queries Experience in Hive partitioning bucketing and collections perform different types of joins on Hive tables Performed hive performance tuning aspects like Map Join Cost Based Optimization and Columnar Level Statistics Good Experience in loading data from LINUXfile system to HDFS Used ECLIPSE and Sbt to build the application Environment Spark Hive Hive UDFs SparkSql Spark Streaming Spark Yarn Spark Sql HBase Sqoop Kafka Aws Ec2 S3 Cloudera Scala IDEEclipse Linux Shell Scripting Hdfs Azure   Developer Novartis Corporation October 2013 to November 2014 Responsibilities Extensively involved in Installation and configuration of Cloudera distribution Hadoop Name Node Secondary Name Node Job Tracker Task Trackers and Data Nodes Developed Map Reduce programs in Java and Sqoop the data from ORACLE database Responsible for building scalable distributed data solutions using Hadoop Written various Hive and Pig scripts Moved data from HDFS to Cassandra using Map Reduce and Bulk Output Format class Experienced with different scripting language like Python and shell scripts Developed various Python scripts to find vulnerabilities with SQL Queries by doing SQL injection permission checks and performance analysis Installed Oozie workflow engine to run multiple Hive and Pig jobs which run independently with time and data availability Experienced with handling administration activations using Cloudera manager Expertise in understanding Partitions Bucketing concepts in Hive Used Oozie Scheduler system to automate the pipeline workflow and orchestrate the Map Reduces jobs that extract the data on a timely manner Responsible for loading data from UNIX file system to HDFS Analyzed the weblog data using the HiveQL integrated Oozie with the rest of the Hadoop stack Utilized cluster coordination services through Zookeeper Got good experience with various NoSQL databases and Comprehensive knowledge in process improvement normalizationdenormalization data extraction data cleansing data manipulation Experience with creating script for data modeling and data import and export Extensive experience in deploying managing and developing MongoDB clusters Created Partitioned Hive tables and worked on them using HiveQL Developed Shell scripts to automate routine DBA tasks Used Maven extensively for building jar files of MapReduce programs and deployed to Cluster Responsible for cluster maintenance adding and removing cluster nodes cluster monitoring and troubleshooting managing and reviewing data backups and Hadoop log files Environment HDFS Map Reduce Pig Hive Oozie Sqoop Flume HBase Java Maven Avro Cloudera Eclipse and Shell Scripting JAVAETL Developer ICICI Bank October 2011 to August 2013 Responsibilities Developed Maven scripts to build and deploy the application Developed Spring MVC controllers for all the modules SAS scripts on UNIX are run and the output datasets are exported into SAS Implemented jQuery validator components Extracted data from Oracle as one of the source databases Using Data stage ETL tool to copy data from Teradata to Netezza Created ETL Data mapping spreadsheets describing column level transformation details to load data from Teradata Landing zone tables to the tables in Party and Policy subject area of EDW based on SAS Insurance model Used JSON and XML documents with Mark logic NoSQL Database extensively REST API calls are made using NodeJS and Java API SAS data sets were constantly created and updated using the SET and UPDATE statements Built data transformation with SSIS including importing data from files Loaded the flat files data using Informatica to the staging area Created SHELL SCRIPTS for generic use Environment Java Spring MPP Windows XPNT Informatica Power center 9186 UNIX Teradata Oracle Designer Autosys Shell Quality Center 10 Java Developer Apollo Hospitals September 2010 to August 2011 Responsibilities Involved in the analysis design implementation and testing of the project Implemented the presentation layer with HTML XHTML and JavaScript Developed web components using JSP Servlets and JDBC Implemented database using SQL Server Implemented Spring IoC framework Developed Spring REST services for all the modules Developed custom SAML and SOAP integration for healthcare Validated the fields of user registration screen and login screen by writing JavaScript validations Used DAO and JDBC for database access Built responsive Web pages using Kendo UI mobile Designed dynamic and multibrowser compatible pages using HTML CSS jQuery JavaScript Require JS and Kendo UI Environment Oracle 11 g Java 15 Struts Servlets HTML XML SQL J2EE JUnit Tomcat 6 Java JSP JDBC JavaScript MySQL Eclipse IDE Rest Jr Java Developer HR Portal June 2009 to July 2010 Responsibilities Development of two modules Leave Management and Performance Appraisal Management Developed the project using Spring MVC and Hibernate Framework Developed User management and different modules on requirement using HTML JavaScript AngularJS JSON and validations fixing the bugs and handling lot of Exceptions alone Participated in the enhancement and maintenance of Personal information Management Developed Excel based reports Leaves Status Appraisal status for the consumption of HR Manager Responsible for Implemented key functionalities in Leaves Management such as Auto Approval process if the Manager does not respond to the leave within 3 days of completion of the leave dates Implemented a workflowbased solution to pass the appraisal from the Employee to Manager untilHR Manager for final discussion Participation in Unit Testing and support for QA Testing Cycles Environment Java J2EE HTML Servlets Spring JSON Hibernate AngularJS JavaScript Oracle Web logic JMS Eclipse IDE Maven UNIX Junit Education Bachelors Skills JAVA 7 years ORACLE 7 years ETL 6 years EXTRACT TRANSFORM AND LOAD 6 years SQL 5 years Additional Information Technical Skills Big Data Ecosystem Hadoop MapReduce Pig Hive YARN Kafka Flume Sqoop Impala Oozie Zookeeper Spark Ambari Mahout MongoDB Cassandra Avro Storm Parquet and Snappy Hadoop Distributions Cloudera CDH3 CDH4 and CDH5 Horton works Map R and Apache Languages Java Python J ruby SQL HTML DHTML Scala JavaScript XML and CC No SQL Databases Cassandra MongoDB and HBase Java Technologies Servlets JavaBeans JSP JDBC JNDI EJB and struts Methodology Agile waterfall Development Build Tools Eclipse Ant Maven IntelliJ JUNIT and log4J DB Languages MySQL PLSQL PostgreSQL and Oracle RDBMS Teradata Oracle 9i10g11i MS SQL Server MySQL and DB2 Operating systems UNIX LINUX Mac OS and Windows Variants ETL Tools Talend Informatica Pentaho",
    "extracted_keywords": [
        "Sr",
        "HadoopScala",
        "Developer",
        "Sr",
        "HadoopScala",
        "span",
        "lDeveloperspan",
        "Sr",
        "HadoopScala",
        "Developer",
        "Safeway",
        "Phoenix",
        "AZ",
        "years",
        "Experience",
        "IT",
        "industry",
        "Designing",
        "Developing",
        "data",
        "Technologies",
        "Hadoop",
        "Spark",
        "Ecosystems",
        "JavaJ2EE",
        "Technologies",
        "Spark",
        "Scala",
        "cluster",
        "analytics",
        "top",
        "Hadoop",
        "application",
        "use",
        "Sparkwith",
        "Hive",
        "SQLOracle",
        "Excellent",
        "Programming",
        "skills",
        "level",
        "abstraction",
        "Scala",
        "Java",
        "Python",
        "experience",
        "distributions",
        "Hadoop",
        "enterprise",
        "versions",
        "ClouderaCDH4CDH5",
        "Horton",
        "knowledge",
        "MAPR",
        "distribution",
        "IBM",
        "Big",
        "Insights",
        "Amazons",
        "EMR",
        "Elastic",
        "Map",
        "Working",
        "knowledge",
        "Amazons",
        "Elastic",
        "Cloud",
        "ComputeEC2",
        "infrastructure",
        "tasks",
        "Simple",
        "Storage",
        "Service",
        "S3",
        "Storage",
        "mechanism",
        "scheduler",
        "Oozie",
        "Airflow",
        "Crontab",
        "Shell",
        "working",
        "experience",
        "data",
        "Sqoop",
        "SFTP",
        "sources",
        "RDMS",
        "Teradata",
        "Mainframes",
        "Oracle",
        "Netezza",
        "HDFS",
        "transformations",
        "Hive",
        "Pig",
        "Spark",
        "experience",
        "streaming",
        "data",
        "HDFS",
        "stream",
        "processing",
        "platforms",
        "Flume",
        "Kafka",
        "system",
        "experience",
        "knowledge",
        "time",
        "data",
        "analytics",
        "Spark",
        "Streaming",
        "Kafka",
        "Flume",
        "Spark",
        "streaming",
        "Apache",
        "Kafka",
        "stream",
        "data",
        "Expertise",
        "SparkRDD",
        "transformations",
        "Actions",
        "Data",
        "Frames",
        "Case",
        "classes",
        "input",
        "data",
        "data",
        "transformations",
        "SparkCore",
        "Experience",
        "Hive",
        "queries",
        "Spark",
        "environment",
        "Spark",
        "SQL",
        "Expertise",
        "time",
        "analytics",
        "data",
        "HBase",
        "Cassandra",
        "Developed",
        "UDFs",
        "UDAFs",
        "Pig",
        "Hive",
        "core",
        "functionality",
        "Proficient",
        "NoSQL",
        "databases",
        "HBase",
        "Cassandra",
        "MongoDB",
        "integration",
        "Hadoop",
        "cluster",
        "experience",
        "Map",
        "Reduce",
        "Mappers",
        "Reducers",
        "combiners",
        "practitioners",
        "results",
        "datasets",
        "data",
        "data",
        "source",
        "OLEDB",
        "Excel",
        "files",
        "XML",
        "build",
        "tools",
        "Ant",
        "SBT",
        "Log4j",
        "Maven",
        "applications",
        "server",
        "competency",
        "Chef",
        "Puppet",
        "configuration",
        "automation",
        "tools",
        "CI",
        "tools",
        "Jenkins",
        "Hudson",
        "Bambino",
        "builds",
        "Proficient",
        "SOLR",
        "development",
        "production",
        "Experience",
        "Enterprise",
        "search",
        "SOLR",
        "text",
        "search",
        "text",
        "analysis",
        "search",
        "filtering",
        "features",
        "dismax",
        "dismax",
        "Worked",
        "data",
        "warehousing",
        "ETL",
        "tools",
        "Informatica",
        "Talend",
        "Pentaho",
        "Designed",
        "ETL",
        "workflows",
        "Tableau",
        "Deployed",
        "data",
        "sources",
        "HDFS",
        "Working",
        "experience",
        "Test",
        "Data",
        "Management",
        "tools",
        "HP",
        "Quality",
        "Center",
        "HPALM",
        "Load",
        "Runner",
        "QTP",
        "Selenium",
        "ELK",
        "stack",
        "search",
        "Logstash",
        "Kibana",
        "log",
        "management",
        "programming",
        "languages",
        "IDEs",
        "Eclipse",
        "NetBeans",
        "Intellij",
        "Experience",
        "Software",
        "Design",
        "Development",
        "Implementation",
        "ClientServer",
        "Web",
        "Applications",
        "JSTL",
        "jQuery",
        "JavaScript",
        "Java",
        "Beans",
        "JDBC",
        "Struts",
        "PLSQL",
        "SQL",
        "HTML",
        "CSS",
        "PHP",
        "XML",
        "AJAX",
        "birds",
        "eye",
        "view",
        "React",
        "Java",
        "Script",
        "Library",
        "Project",
        "Management",
        "services",
        "JIRA",
        "tracking",
        "issues",
        "bugs",
        "code",
        "GitHub",
        "code",
        "reviews",
        "version",
        "control",
        "tools",
        "CVS",
        "GIT",
        "PVCS",
        "SVN",
        "Experience",
        "practices",
        "Web",
        "services",
        "development",
        "Integration",
        "REST",
        "SOAP",
        "kinds",
        "knowledge",
        "reports",
        "Power",
        "BI",
        "Qlik",
        "Business",
        "specification",
        "Experience",
        "scripts",
        "Unix",
        "shell",
        "scripting",
        "database",
        "activities",
        "Experience",
        "Software",
        "Development",
        "Life",
        "Cycle",
        "SDLC",
        "Waterfall",
        "methodologies",
        "understanding",
        "aspects",
        "Testing",
        "Unit",
        "Regression",
        "Agile",
        "Whitebox",
        "Blackbox",
        "Authorized",
        "US",
        "employer",
        "Work",
        "Experience",
        "Sr",
        "HadoopScala",
        "Developer",
        "Safeway",
        "Phoenix",
        "AZ",
        "February",
        "Present",
        "Responsibilities",
        "designing",
        "deployment",
        "Hadoop",
        "cluster",
        "Big",
        "Data",
        "tools",
        "Pig",
        "Hive",
        "Cassandra",
        "Oozie",
        "Sqoop",
        "Kafka",
        "Spark",
        "Impala",
        "Horton",
        "distribution",
        "Performed",
        "source",
        "data",
        "transformations",
        "Hive",
        "Supporting",
        "infrastructure",
        "environment",
        "comprising",
        "RHEL",
        "Solaris",
        "Map",
        "Reduce",
        "framework",
        "records",
        "Developed",
        "Spark",
        "scripts",
        "Scala",
        "shell",
        "commands",
        "requirement",
        "Kafka",
        "data",
        "data",
        "systems",
        "HDFS",
        "Created",
        "Spark",
        "jobs",
        "trends",
        "data",
        "usage",
        "users",
        "insights",
        "data",
        "business",
        "results",
        "application",
        "teams",
        "Column",
        "families",
        "Cassandra",
        "data",
        "RDBMS",
        "data",
        "transformations",
        "data",
        "Cassandra",
        "business",
        "requirement",
        "Developed",
        "Spark",
        "code",
        "Scala",
        "SparkSQL",
        "processing",
        "testing",
        "Experience",
        "NoSQL",
        "ColumnOriented",
        "Databases",
        "Cassandra",
        "Integration",
        "Hadoop",
        "cluster",
        "Spark",
        "API",
        "Hadoop",
        "YARN",
        "execution",
        "engine",
        "data",
        "analytics",
        "Hive",
        "data",
        "databases",
        "Sqoop",
        "reports",
        "BI",
        "team",
        "experience",
        "Talend",
        "studio",
        "ETL",
        "Jobs",
        "Processing",
        "data",
        "Experience",
        "volume",
        "data",
        "skills",
        "execution",
        "process",
        "Talend",
        "functionality",
        "file",
        "formats",
        "Text",
        "files",
        "Avro",
        "kinds",
        "reports",
        "Power",
        "BI",
        "Tableau",
        "clients",
        "Agile",
        "Methodology",
        "designing",
        "time",
        "data",
        "Oozie",
        "workflows",
        "Kerberos",
        "principals",
        "HDFS",
        "Hive",
        "Pig",
        "MapReduce",
        "access",
        "users",
        "Spark",
        "eco",
        "system",
        "SCALA",
        "HIVE",
        "Queries",
        "data",
        "formats",
        "Text",
        "file",
        "parquet",
        "logs",
        "data",
        "web",
        "servers",
        "HDFS",
        "Flume",
        "Developed",
        "Hive",
        "scripts",
        "Hive",
        "QL",
        "data",
        "mapreduce",
        "counters",
        "metrics",
        "records",
        "records",
        "Work",
        "experience",
        "infrastructure",
        "Amazon",
        "Web",
        "Services",
        "AWS",
        "UDFs",
        "Hive",
        "Pig",
        "functionality",
        "Jobs",
        "Zeppelin",
        "notebooks",
        "team",
        "troubleshooting",
        "Spark",
        "Experience",
        "data",
        "data",
        "sources",
        "Mainframes",
        "Teradata",
        "Oracle",
        "Netezza",
        "Sqoop",
        "SFTP",
        "transformations",
        "Hive",
        "Pig",
        "Spark",
        "data",
        "HDFS",
        "data",
        "Teradata",
        "HDFSDatabasesDashboards",
        "SPARK",
        "STREAMING",
        "income",
        "logic",
        "Pig",
        "scripts",
        "file",
        "formats",
        "Parquet",
        "Avro",
        "Compression",
        "Codecs",
        "GZIP",
        "LZO",
        "Created",
        "applications",
        "Kafka",
        "consumer",
        "lag",
        "Apache",
        "Kafka",
        "production",
        "companies",
        "SparkStreaming",
        "APIs",
        "transformations",
        "actions",
        "fly",
        "learner",
        "data",
        "model",
        "data",
        "Kafka",
        "time",
        "Persists",
        "Cassandra",
        "Design",
        "document",
        "RESTHTTP",
        "SOAP",
        "APIs",
        "data",
        "formats",
        "API",
        "strategy",
        "Experience",
        "Apache",
        "Kafka",
        "amounts",
        "data",
        "application",
        "servers",
        "Hibernate",
        "ORM",
        "framework",
        "Spring",
        "framework",
        "data",
        "persistence",
        "transaction",
        "management",
        "Performance",
        "analysis",
        "Spark",
        "streaming",
        "batch",
        "jobs",
        "Spark",
        "tuning",
        "parameters",
        "React",
        "Bindings",
        "Redux",
        "Worked",
        "time",
        "data",
        "streaming",
        "solutions",
        "Apache",
        "SparkSpark",
        "Streaming",
        "Kafka",
        "Hadoop",
        "Operations",
        "team",
        "Hadoop",
        "cluster",
        "planning",
        "installation",
        "maintenance",
        "monitoring",
        "upgrades",
        "File",
        "System",
        "FSCK",
        "health",
        "files",
        "HDFS",
        "Agile",
        "development",
        "environment",
        "sprint",
        "cycles",
        "weeks",
        "organizing",
        "tasks",
        "scrum",
        "design",
        "meetings",
        "Environment",
        "Hadoop",
        "Hive",
        "Map",
        "Reduce",
        "Sqoop",
        "Kafka",
        "Spark",
        "Yarn",
        "Pig",
        "Cassandra",
        "Oozie",
        "shell",
        "Scripting",
        "Scala",
        "Maven",
        "Java",
        "React",
        "JS",
        "JUnit",
        "methodologies",
        "Horton",
        "Soap",
        "NIFI",
        "Teradata",
        "MySQL",
        "Sr",
        "Spark",
        "Developer",
        "Digital",
        "Realty",
        "Trust",
        "November",
        "February",
        "Responsibilities",
        "data",
        "pipeline",
        "SPARK",
        "Apache",
        "Kafka",
        "customer",
        "data",
        "histories",
        "HADOOP",
        "cluster",
        "analysis",
        "data",
        "SPARK",
        "STREAMING",
        "AWS",
        "S3",
        "bucket",
        "nearrealtime",
        "Transformations",
        "Aggregation",
        "learner",
        "data",
        "model",
        "data",
        "HDFS",
        "usage",
        "SPARK",
        "performance",
        "optimization",
        "algorithms",
        "HADOOP",
        "SPARK",
        "Worked",
        "SPARK",
        "STREAMING",
        "data",
        "Spark",
        "Engine",
        "data",
        "sources",
        "AWS",
        "S3",
        "file",
        "system",
        "Spark",
        "RDD",
        "Experience",
        "services",
        "Azure",
        "Data",
        "lake",
        "data",
        "Hands",
        "experience",
        "Hadoop",
        "cluster",
        "environment",
        "Azure",
        "HDInsight",
        "Developed",
        "Spark",
        "applications",
        "SCALA",
        "SPARKSQLSTREAMINGAPI",
        "testing",
        "processing",
        "data",
        "HiveSQL",
        "queries",
        "Spark",
        "Transformations",
        "SPARK",
        "RDDs",
        "SCALA",
        "SPARK",
        "SQL",
        "Spark",
        "Streaming",
        "modules",
        "SPARKand",
        "SCALA",
        "code",
        "Spark",
        "use",
        "cases",
        "Spark",
        "code",
        "Scala",
        "Spark",
        "SQL",
        "testing",
        "data",
        "processing",
        "PLSQL",
        "code",
        "Scala",
        "code",
        "PLSQL",
        "queries",
        "Hive",
        "queries",
        "IMPALA",
        "processing",
        "database",
        "technology",
        "top",
        "Hadoop",
        "ecosystem",
        "cluster",
        "Health",
        "status",
        "Hadoop",
        "cluster",
        "maintenance",
        "activities",
        "upgrades",
        "expansions",
        "configurations",
        "infrastructure",
        "network",
        "database",
        "business",
        "intelligence",
        "application",
        "teams",
        "business",
        "applications",
        "service",
        "levels",
        "Configure",
        "Flume",
        "log",
        "file",
        "data",
        "HDFS",
        "SQOOP",
        "data",
        "RDBMS",
        "HDFS",
        "Handle",
        "data",
        "exchange",
        "HDFS",
        "RDBMS",
        "Write",
        "Spark",
        "applications",
        "Scala",
        "MYSQL",
        "database",
        "Spark",
        "SQL",
        "Hive",
        "ETL",
        "Transformations",
        "Hive",
        "Queries",
        "Experience",
        "Hive",
        "bucketing",
        "collections",
        "types",
        "joins",
        "Hive",
        "tables",
        "hive",
        "performance",
        "aspects",
        "Map",
        "Join",
        "Cost",
        "Based",
        "Optimization",
        "Columnar",
        "Level",
        "Statistics",
        "Good",
        "Experience",
        "loading",
        "data",
        "LINUXfile",
        "system",
        "ECLIPSE",
        "Sbt",
        "application",
        "Environment",
        "Spark",
        "Hive",
        "Hive",
        "UDFs",
        "SparkSql",
        "Spark",
        "Streaming",
        "Spark",
        "Yarn",
        "Spark",
        "Sql",
        "HBase",
        "Sqoop",
        "Kafka",
        "Aws",
        "Ec2",
        "S3",
        "Cloudera",
        "Scala",
        "IDEEclipse",
        "Linux",
        "Shell",
        "Scripting",
        "Hdfs",
        "Azure",
        "Developer",
        "Novartis",
        "Corporation",
        "October",
        "November",
        "Responsibilities",
        "Installation",
        "configuration",
        "Cloudera",
        "distribution",
        "Hadoop",
        "Name",
        "Node",
        "Secondary",
        "Name",
        "Node",
        "Job",
        "Tracker",
        "Task",
        "Trackers",
        "Data",
        "Nodes",
        "Developed",
        "Map",
        "programs",
        "Java",
        "Sqoop",
        "data",
        "database",
        "data",
        "solutions",
        "Hadoop",
        "Written",
        "Hive",
        "Pig",
        "scripts",
        "data",
        "HDFS",
        "Cassandra",
        "Map",
        "Reduce",
        "Bulk",
        "Output",
        "Format",
        "class",
        "scripting",
        "language",
        "Python",
        "scripts",
        "Python",
        "scripts",
        "vulnerabilities",
        "SQL",
        "Queries",
        "SQL",
        "injection",
        "permission",
        "checks",
        "performance",
        "analysis",
        "Oozie",
        "workflow",
        "engine",
        "Hive",
        "Pig",
        "jobs",
        "time",
        "data",
        "availability",
        "administration",
        "activations",
        "Cloudera",
        "manager",
        "Expertise",
        "Partitions",
        "Bucketing",
        "concepts",
        "Hive",
        "Oozie",
        "Scheduler",
        "system",
        "pipeline",
        "Map",
        "Reduces",
        "jobs",
        "data",
        "manner",
        "loading",
        "data",
        "UNIX",
        "file",
        "system",
        "HDFS",
        "weblog",
        "data",
        "HiveQL",
        "Oozie",
        "rest",
        "Hadoop",
        "stack",
        "cluster",
        "coordination",
        "services",
        "Zookeeper",
        "experience",
        "NoSQL",
        "databases",
        "knowledge",
        "process",
        "improvement",
        "normalizationdenormalization",
        "data",
        "extraction",
        "data",
        "cleansing",
        "data",
        "manipulation",
        "Experience",
        "script",
        "data",
        "modeling",
        "data",
        "import",
        "experience",
        "clusters",
        "Partitioned",
        "Hive",
        "tables",
        "HiveQL",
        "Developed",
        "Shell",
        "scripts",
        "DBA",
        "tasks",
        "Maven",
        "jar",
        "files",
        "MapReduce",
        "programs",
        "Cluster",
        "Responsible",
        "cluster",
        "maintenance",
        "cluster",
        "nodes",
        "cluster",
        "monitoring",
        "data",
        "backups",
        "Hadoop",
        "log",
        "Environment",
        "HDFS",
        "Map",
        "Reduce",
        "Pig",
        "Hive",
        "Oozie",
        "Sqoop",
        "Flume",
        "HBase",
        "Java",
        "Maven",
        "Avro",
        "Cloudera",
        "Eclipse",
        "Shell",
        "Scripting",
        "JAVAETL",
        "Developer",
        "ICICI",
        "Bank",
        "October",
        "August",
        "Responsibilities",
        "Maven",
        "scripts",
        "application",
        "Developed",
        "Spring",
        "MVC",
        "modules",
        "SAS",
        "scripts",
        "UNIX",
        "output",
        "datasets",
        "SAS",
        "jQuery",
        "validator",
        "components",
        "data",
        "Oracle",
        "source",
        "Data",
        "stage",
        "ETL",
        "tool",
        "data",
        "Teradata",
        "Netezza",
        "Created",
        "ETL",
        "Data",
        "mapping",
        "spreadsheets",
        "column",
        "level",
        "transformation",
        "details",
        "data",
        "Teradata",
        "Landing",
        "zone",
        "tables",
        "tables",
        "Party",
        "Policy",
        "subject",
        "area",
        "EDW",
        "SAS",
        "Insurance",
        "model",
        "JSON",
        "XML",
        "documents",
        "Mark",
        "logic",
        "NoSQL",
        "Database",
        "REST",
        "API",
        "calls",
        "NodeJS",
        "Java",
        "API",
        "SAS",
        "data",
        "sets",
        "SET",
        "UPDATE",
        "statements",
        "data",
        "transformation",
        "SSIS",
        "data",
        "files",
        "files",
        "data",
        "Informatica",
        "staging",
        "area",
        "Created",
        "SHELL",
        "SCRIPTS",
        "use",
        "Environment",
        "Java",
        "Spring",
        "MPP",
        "Windows",
        "XPNT",
        "Informatica",
        "Power",
        "center",
        "UNIX",
        "Teradata",
        "Oracle",
        "Designer",
        "Autosys",
        "Shell",
        "Quality",
        "Center",
        "Java",
        "Developer",
        "Apollo",
        "Hospitals",
        "September",
        "August",
        "Responsibilities",
        "analysis",
        "design",
        "implementation",
        "testing",
        "project",
        "presentation",
        "layer",
        "HTML",
        "XHTML",
        "JavaScript",
        "Developed",
        "web",
        "components",
        "JSP",
        "Servlets",
        "JDBC",
        "database",
        "SQL",
        "Server",
        "Spring",
        "IoC",
        "framework",
        "Developed",
        "Spring",
        "REST",
        "services",
        "modules",
        "custom",
        "SAML",
        "integration",
        "healthcare",
        "fields",
        "user",
        "registration",
        "screen",
        "login",
        "screen",
        "JavaScript",
        "validations",
        "DAO",
        "JDBC",
        "database",
        "access",
        "Web",
        "pages",
        "Kendo",
        "UI",
        "mobile",
        "dynamic",
        "multibrowser",
        "pages",
        "HTML",
        "CSS",
        "jQuery",
        "JavaScript",
        "Require",
        "JS",
        "Kendo",
        "UI",
        "Environment",
        "Oracle",
        "g",
        "Java",
        "Struts",
        "Servlets",
        "HTML",
        "XML",
        "SQL",
        "J2EE",
        "JUnit",
        "Tomcat",
        "Java",
        "JSP",
        "JDBC",
        "JavaScript",
        "MySQL",
        "Eclipse",
        "IDE",
        "Rest",
        "Jr",
        "Java",
        "Developer",
        "HR",
        "Portal",
        "June",
        "July",
        "Responsibilities",
        "Development",
        "modules",
        "Management",
        "Performance",
        "Appraisal",
        "Management",
        "project",
        "Spring",
        "MVC",
        "Hibernate",
        "Framework",
        "Developed",
        "User",
        "management",
        "modules",
        "requirement",
        "HTML",
        "JavaScript",
        "JSON",
        "validations",
        "bugs",
        "lot",
        "Exceptions",
        "enhancement",
        "maintenance",
        "information",
        "Management",
        "Developed",
        "Excel",
        "reports",
        "Leaves",
        "Status",
        "Appraisal",
        "status",
        "consumption",
        "HR",
        "Manager",
        "Responsible",
        "functionalities",
        "Leaves",
        "Management",
        "Auto",
        "Approval",
        "process",
        "Manager",
        "leave",
        "days",
        "completion",
        "leave",
        "dates",
        "solution",
        "appraisal",
        "Employee",
        "Manager",
        "untilHR",
        "Manager",
        "discussion",
        "Participation",
        "Unit",
        "Testing",
        "support",
        "QA",
        "Testing",
        "Cycles",
        "Environment",
        "Java",
        "J2EE",
        "HTML",
        "Servlets",
        "Spring",
        "JSON",
        "Hibernate",
        "JavaScript",
        "Oracle",
        "Web",
        "logic",
        "JMS",
        "Eclipse",
        "IDE",
        "Maven",
        "UNIX",
        "Junit",
        "Education",
        "Bachelors",
        "Skills",
        "JAVA",
        "years",
        "ORACLE",
        "years",
        "ETL",
        "years",
        "EXTRACT",
        "TRANSFORM",
        "years",
        "SQL",
        "years",
        "Additional",
        "Information",
        "Technical",
        "Skills",
        "Big",
        "Data",
        "Ecosystem",
        "Hadoop",
        "MapReduce",
        "Pig",
        "Hive",
        "YARN",
        "Kafka",
        "Flume",
        "Sqoop",
        "Impala",
        "Oozie",
        "Zookeeper",
        "Spark",
        "Ambari",
        "Mahout",
        "MongoDB",
        "Cassandra",
        "Avro",
        "Storm",
        "Parquet",
        "Snappy",
        "Hadoop",
        "Distributions",
        "Cloudera",
        "CDH3",
        "CDH4",
        "CDH5",
        "Horton",
        "Map",
        "R",
        "Apache",
        "Languages",
        "Java",
        "Python",
        "J",
        "ruby",
        "SQL",
        "HTML",
        "DHTML",
        "Scala",
        "JavaScript",
        "XML",
        "CC",
        "SQL",
        "Cassandra",
        "MongoDB",
        "HBase",
        "Java",
        "Technologies",
        "Servlets",
        "JavaBeans",
        "JSP",
        "JDBC",
        "JNDI",
        "EJB",
        "Methodology",
        "Agile",
        "waterfall",
        "Development",
        "Build",
        "Tools",
        "Eclipse",
        "Ant",
        "Maven",
        "IntelliJ",
        "JUNIT",
        "DB",
        "Languages",
        "MySQL",
        "PLSQL",
        "PostgreSQL",
        "Oracle",
        "RDBMS",
        "Teradata",
        "Oracle",
        "9i10g11i",
        "MS",
        "SQL",
        "Server",
        "MySQL",
        "DB2",
        "Operating",
        "systems",
        "UNIX",
        "LINUX",
        "Mac",
        "OS",
        "Windows",
        "Variants",
        "ETL",
        "Tools",
        "Talend",
        "Informatica",
        "Pentaho"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T20:32:51.808538",
    "resume_data": "Sr HadoopScala Developer Sr HadoopScala span lDeveloperspan Sr HadoopScala Developer Safeway Phoenix AZ Having 8 years of Experience in IT industry in Designing Developing and Maintaining using Big data Technologies like Hadoop Spark Ecosystems and JavaJ2EE Technologies Extensively worked on Spark using Scala on cluster for computational analytics installed it on top of Hadoop performed advanced analytical application by making use of Sparkwith Hive and SQLOracle Excellent Programming skills at a higher level of abstraction using Scala Java and Python Extensive experience in working with various distributions of Hadoop enterprise versions of ClouderaCDH4CDH5 Horton works and good knowledge on MAPR distribution IBM Big Insights and Amazons EMR Elastic Map Reduce Working knowledge of Amazons Elastic Cloud ComputeEC2 infrastructure for computational tasks and Simple Storage Service S3 as Storage mechanism Experienced in implementing scheduler using Oozie Airflow Crontab and Shell scripts Good working experience in importing data using Sqoop SFTP from various sources like RDMS Teradata Mainframes Oracle Netezza to HDFS and performed transformations on it using Hive Pig and Spark Extensive experience in importing and exporting streaming data into HDFS using stream processing platforms like Flume and Kafka messaging system Strong experience and knowledge of real time data analytics using Spark Streaming Kafka and Flume Extensively worked on Spark streaming and Apache Kafka to fetch live stream data Expertise in writing SparkRDD transformations Actions Data Frames Case classes for the required input data and performed the data transformations using SparkCore Experience in integrating Hive queries into Spark environment using Spark SQL Expertise in performing real time analytics on big data using HBase and Cassandra Developed customized UDFs and UDAFs in java to extend Pig and Hive core functionality Proficient in NoSQL databases including HBase Cassandra MongoDB and its integration with Hadoop cluster Good experience in optimizing Map Reduce algorithms using Mappers Reducers combiners and practitioners to deliver the best results for the large datasets Extracted data from various data source including OLEDB Excel Flat files and XML Experienced in using build tools like Ant SBT Log4j Maven to build and deploy applications into the server Had competency in using Chef Puppet and Ansible configuration and automation tools Configured and administered CI tools like Jenkins Hudson Bambino for automated builds Proficient in developing deploying and managing the SOLR from development to production Experience in Enterprise search using SOLR to implement full text search with advanced text analysis faceted search filtering using advanced features like dismax extended dismax and grouping Worked on data warehousing and ETL tools like Informatica Talend and Pentaho Designed ETL workflows on Tableau Deployed data from various sources to HDFS Working experience on Test Data Management tools HP Quality Center HPALM Load Runner QTP and Selenium Worked on ELK stack like Elastic search Logstash Kibana for log management Worked on various programming languages using IDEs like Eclipse NetBeans and Intellij Experience in Software Design Development and Implementation of ClientServer Web based Applications using JSTL jQuery JavaScript Java Beans JDBC Struts PLSQL SQL HTML CSS PHP XML AJAX and had a birds eye view on React Java Script Library Used various Project Management services like JIRA for tracking issues bugs related to code and GitHub for various code reviews and Worked on various version control tools like CVS GIT PVCS SVN Experience with best practices of Web services development and Integration both REST and SOAP Generated various kinds of knowledge reports using Power BI and Qlik based on Business specification Experience in automated scripts using Unix shell scripting to perform database activities Experience in complete Software Development Life Cycle SDLC in both Waterfall and Agile methodologies Good understanding of all aspects of Testing such as Unit Regression Agile Whitebox Blackbox Authorized to work in the US for any employer Work Experience Sr HadoopScala Developer Safeway Phoenix AZ February 2016 to Present Responsibilities Experienced in designing and deployment of Hadoop cluster and different Big Data analytic tools including Pig Hive Cassandra Oozie Sqoop Kafka Spark Impala with Horton works distribution Performed source data transformations using Hive Supporting infrastructure environment comprising of RHEL and Solaris Involved in developing a Map Reduce framework that filters bad and unnecessary records Developed Spark scripts by using Scala shell commands as per the requirement Used Kafka to transfer data from different data systems to HDFS Created Spark jobs to see trends in data usage by users Responsible for generating actionable insights from complex data to drive real business results for various application teams Designed the Column families in Cassandra Ingested data from RDBMS and performed data transformations and then export the transformed data to Cassandra as per the business requirement Developed Spark code to using Scala and SparkSQL for faster processing and testing Experience in NoSQL ColumnOriented Databases like Cassandra and its Integration with Hadoop cluster Used Spark API over Hadoop YARN as execution engine for data analytics using Hive Exported the analyzed data to the relational databases using Sqoop to further visualize and generate reports for the BI team Good experience with Talend open studio for designing ETL Jobs for Processing of data Experience in processing large volume of data and skills in parallel execution of process using Talend functionality Worked on different file formats like Text files and Avro Created various kinds of reports using Power BI and Tableau based on the clients needs Worked on Agile Methodology projects extensively Experience designing and executing time driven and data driven Oozie workflows Setting up Kerberos principals and testing HDFS Hive Pig and MapReduce access for the new users Experienced in working with Spark eco system using SCALA and HIVE Queries on different data formats like Text file and parquet Collected the logs data from web servers and integrated in to HDFS using Flume Developed Hive scripts in Hive QL to denormalize and aggregate the data Implemented mapreduce counters to gather metrics of good records and bad records Work experience with cloud infrastructure like Amazon Web Services AWS Developed customized UDFs in java to extend Hive and Pig functionality Jobs via Zeppelin notebooks mentored and guided offshore team in troubleshooting and finetuning Spark Experience in importing data from various data sources like Mainframes Teradata Oracle and Netezza using Sqoop SFTP performed transformations using Hive Pig and Spark and loaded data into HDFS Extracted the data from Teradata into HDFSDatabasesDashboards using SPARK STREAMING Implemented best income logic using Pig scripts Worked on different file formats ORCFILE Parquet Avro and different Compression Codecs GZIP SNAPPY LZO Created applications using Kafka which monitors consumer lag within Apache Kafka clusters Used in production by multiple companies Using SparkStreaming APIs to perform transformations and actions on the fly for building the common learner data model which gets the data from Kafka in near real time and Persists into Cassandra Design and document RESTHTTP SOAP APIs including JSON data formats and API versioning strategy Experience in using Apache Kafka for collecting aggregating and moving large amounts of data from application servers Used Hibernate ORM framework with Spring framework for data persistence and transaction management Performance analysis of Spark streaming and batch jobs by using Spark tuning parameters Used React Bindings for embracing Redux Worked towards creating real time data streaming solutions using Apache SparkSpark Streaming Kafka Worked along with the Hadoop Operations team in Hadoop cluster planning installation maintenance monitoring and upgrades Used File System check FSCK to check the health of files in HDFS Worked in Agile development environment in sprint cycles of two weeks by dividing and organizing tasks Participated in daily scrum and other design related meetings Environment Hadoop Hive Map Reduce Sqoop Kafka Spark Yarn Pig Cassandra Oozie shell Scripting Scala Maven Java React JS JUnit agile methodologies Horton works Soap NIFI Teradata MySQL Sr Spark Developer Digital Realty Trust November 2014 to February 2016 Responsibilities Developed data pipeline using SPARK Apache Kafka to ingest customer behavioral data and financial histories into HADOOP cluster for analysis Collected data using SPARK STREAMING from AWS S3 bucket in nearrealtime and performed necessary Transformations and Aggregation to build the common learner data model and persist the data in HDFS Explored the usage of SPARK for improving the performance and optimization of the existing algorithms in HADOOP using SPARK Worked with SPARK STREAMING to ingest data into Spark Engine Imported the data from different sources like AWS S3 Local file system into Spark RDD Experience working with various services in Azure like Data lake to store and analyze the data Hands on experience configuring Hadoop cluster in professional environment and on Azure using HDInsight Developed Spark applications using SCALA with SPARKSQLSTREAMINGAPI for faster testing and processing of data Involved in converting HiveSQL queries into Spark Transformations using SPARK RDDs and SCALA Worked on the SPARK SQL and Spark Streaming modules of SPARKand used SCALA to write code for all Spark use cases Developed Spark code using Scala and Spark SQL for faster testing and data processing Worked on converting PLSQL code into Scala code and converted PLSQL queries into Hive queries Used IMPALA to give parallel processing database technology on top of Hadoop ecosystem Monitored cluster Health status and assisted in planed Hadoop cluster maintenance activities such as upgrades expansions configurations etc Worked closely with infrastructure network database and business intelligence and application teams to ensure business applications are highly available and performing within agreed on service levels Configure Flume to ingest log file data into HDFS Involved in using SQOOP for importing and exporting data between RDBMS and HDFS Handle data exchange between HDFS and RDBMS Write Spark applications in Scala to interact with MYSQL database using Spark SQL Extensively used Hive for ETL Transformations and optimized Hive Queries Experience in Hive partitioning bucketing and collections perform different types of joins on Hive tables Performed hive performance tuning aspects like Map Join Cost Based Optimization and Columnar Level Statistics Good Experience in loading data from LINUXfile system to HDFS Used ECLIPSE and Sbt to build the application Environment Spark Hive Hive UDFs SparkSql Spark Streaming Spark Yarn Spark Sql HBase Sqoop Kafka Aws Ec2 S3 Cloudera Scala IDEEclipse Linux Shell Scripting Hdfs Azure HadoopETL Developer Novartis Corporation October 2013 to November 2014 Responsibilities Extensively involved in Installation and configuration of Cloudera distribution Hadoop Name Node Secondary Name Node Job Tracker Task Trackers and Data Nodes Developed Map Reduce programs in Java and Sqoop the data from ORACLE database Responsible for building scalable distributed data solutions using Hadoop Written various Hive and Pig scripts Moved data from HDFS to Cassandra using Map Reduce and Bulk Output Format class Experienced with different scripting language like Python and shell scripts Developed various Python scripts to find vulnerabilities with SQL Queries by doing SQL injection permission checks and performance analysis Installed Oozie workflow engine to run multiple Hive and Pig jobs which run independently with time and data availability Experienced with handling administration activations using Cloudera manager Expertise in understanding Partitions Bucketing concepts in Hive Used Oozie Scheduler system to automate the pipeline workflow and orchestrate the Map Reduces jobs that extract the data on a timely manner Responsible for loading data from UNIX file system to HDFS Analyzed the weblog data using the HiveQL integrated Oozie with the rest of the Hadoop stack Utilized cluster coordination services through Zookeeper Got good experience with various NoSQL databases and Comprehensive knowledge in process improvement normalizationdenormalization data extraction data cleansing data manipulation Experience with creating script for data modeling and data import and export Extensive experience in deploying managing and developing MongoDB clusters Created Partitioned Hive tables and worked on them using HiveQL Developed Shell scripts to automate routine DBA tasks Used Maven extensively for building jar files of MapReduce programs and deployed to Cluster Responsible for cluster maintenance adding and removing cluster nodes cluster monitoring and troubleshooting managing and reviewing data backups and Hadoop log files Environment HDFS Map Reduce Pig Hive Oozie Sqoop Flume HBase Java Maven Avro Cloudera Eclipse and Shell Scripting JAVAETL Developer ICICI Bank October 2011 to August 2013 Responsibilities Developed Maven scripts to build and deploy the application Developed Spring MVC controllers for all the modules SAS scripts on UNIX are run and the output datasets are exported into SAS Implemented jQuery validator components Extracted data from Oracle as one of the source databases Using Data stage ETL tool to copy data from Teradata to Netezza Created ETL Data mapping spreadsheets describing column level transformation details to load data from Teradata Landing zone tables to the tables in Party and Policy subject area of EDW based on SAS Insurance model Used JSON and XML documents with Mark logic NoSQL Database extensively REST API calls are made using NodeJS and Java API SAS data sets were constantly created and updated using the SET and UPDATE statements Built data transformation with SSIS including importing data from files Loaded the flat files data using Informatica to the staging area Created SHELL SCRIPTS for generic use Environment Java Spring MPP Windows XPNT Informatica Power center 9186 UNIX Teradata Oracle Designer Autosys Shell Quality Center 10 Java Developer Apollo Hospitals September 2010 to August 2011 Responsibilities Involved in the analysis design implementation and testing of the project Implemented the presentation layer with HTML XHTML and JavaScript Developed web components using JSP Servlets and JDBC Implemented database using SQL Server Implemented Spring IoC framework Developed Spring REST services for all the modules Developed custom SAML and SOAP integration for healthcare Validated the fields of user registration screen and login screen by writing JavaScript validations Used DAO and JDBC for database access Built responsive Web pages using Kendo UI mobile Designed dynamic and multibrowser compatible pages using HTML CSS jQuery JavaScript Require JS and Kendo UI Environment Oracle 11g Java 15 Struts Servlets HTML XML SQL J2EE JUnit Tomcat 6 Java JSP JDBC JavaScript MySQL Eclipse IDE Rest Jr Java Developer HR Portal June 2009 to July 2010 Responsibilities Development of two modules Leave Management and Performance Appraisal Management Developed the project using Spring MVC and Hibernate Framework Developed User management and different modules on requirement using HTML JavaScript AngularJS JSON and validations fixing the bugs and handling lot of Exceptions alone Participated in the enhancement and maintenance of Personal information Management Developed Excel based reports Leaves Status Appraisal status for the consumption of HR Manager Responsible for Implemented key functionalities in Leaves Management such as Auto Approval process if the Manager does not respond to the leave within 3 days of completion of the leave dates Implemented a workflowbased solution to pass the appraisal from the Employee to Manager untilHR Manager for final discussion Participation in Unit Testing and support for QA Testing Cycles Environment Java J2EE HTML Servlets Spring JSON Hibernate AngularJS JavaScript Oracle Web logic JMS Eclipse IDE Maven UNIX Junit Education Bachelors Skills JAVA 7 years ORACLE 7 years ETL 6 years EXTRACT TRANSFORM AND LOAD 6 years SQL 5 years Additional Information Technical Skills Big Data Ecosystem Hadoop MapReduce Pig Hive YARN Kafka Flume Sqoop Impala Oozie Zookeeper Spark Ambari Mahout MongoDB Cassandra Avro Storm Parquet and Snappy Hadoop Distributions Cloudera CDH3 CDH4 and CDH5 Horton works Map R and Apache Languages Java Python J ruby SQL HTML DHTML Scala JavaScript XML and CC No SQL Databases Cassandra MongoDB and HBase Java Technologies Servlets JavaBeans JSP JDBC JNDI EJB and struts Methodology Agile waterfall Development Build Tools Eclipse Ant Maven IntelliJ JUNIT and log4J DB Languages MySQL PLSQL PostgreSQL and Oracle RDBMS Teradata Oracle 9i10g11i MS SQL Server MySQL and DB2 Operating systems UNIX LINUX Mac OS and Windows Variants ETL Tools Talend Informatica Pentaho",
    "unique_id": "2f7fd212-084a-4844-b325-484db2804898"
}