{
    "clean_data": "Big Data Tech Lead Architect Big Data Tech Lead Architect Big Data Tech Lead Architect Hilton Worldwide Solid experience in Big data HadoopSpark and JavaJ2EE technologies development including requirements Analysis and Design Development implementation support maintenance and enhancements in Finance Insurance domains 5 years of experience as HadoopSpark Developer with good knowledge of Java Map Reduce Hive Pig Latin Scala and Spark Organizing data into tables performing transformations and simplifying complex queries with Hive Performing realtime interactive analysis on massive data sets stored in HDFS Strong knowledge and experience with Hadoop architecture and various components such as HDFS YARN Pig Hive Sqoop Oozie Flume Spark Kafka and Map Reduce programming paradigm Developed many MapReduce programs Experience in analyzing data using Spark SQL HIVEQL PIG Latin and experience in developing custom UDF s using Pig and Hive Experience in importing and exporting data using Sqoop from HDFS to Relational Database Systems and viceversa Good knowledge in using job scheduling tools like Oozie Experienced in using IDE Tool like Eclipse 3x IBM RAD 70 Experience in requirement gathering analysis planning designing coding and unit testing Strong work ethic with desire to succeed and make significant contributions to the organization Strong problem solving skills good communication interpersonal skills and a good team player Have the motivation to take independent responsibility as well as ability to contribute and be a productive team member Work Experience Big Data Tech Lead Architect Hilton Worldwide McLean VA September 2018 to Present Project Description Hilton is forward thinking global leader of hospitality Hilton is highly data driven company which uses Hadoop Big data architecture to make key business decisions The ingested data from different upstream applications will be transformed and loaded to Redshift which will be used by MicroStrategy reporting team to generate dashboards that aid in key business decisions Responsibilities Used NIFI as dataflow automation tool to ingest data into HDFS from different source systems Developed common process to bulk load raw HDFS files into dataframe Developed common process to persist dataframe into S3 Redshift HDFS Hive Prune the ingested data to remove duplicates by applying window functions and perform complex transformations to derive various metrics Used oozie scheduler to trigger spark jobs Performed POC on airflow scheduler and Involved in migration of oozie scheduler to airflow Created UDFs in spark to be used in spark sql Performed POC to consume requests from microstrategy dashboard which will in turn trigger the spark job The spark job dynamically generates spark sql based on the level of granularity business user requests and load data into Redshift to be used on dashboard Involved in migration of HDP to AWS and various proof of concepts to achieve it Guide junior developers in their day to day activities and ensure delivery of project Used Spark streaming on Kafka to achieve real time data analytics Created Dstreams dataframes from streaming data and performed transformations Performed performance tuning of spark jobs using broadcast joins correct level of Parallelism and memory tuning Analyze and define clients business strategy and determine system architecture requirements to achieve business goals Lead the Team to complete critical and measurable mile stones of the Project Environment Spark 22 Hadoop Hive 21 HDFS Java 18 Scala 211 HDP Elasticsearch AWS Redshift Oozie Intellij ORC Shell Scripting bitbucketairflowPython Pyspark Lead HadoopSpark Consultant Citibank Tampa FL November 2017 to September 2018 Responsibilities Used Spark API over Cloudera Hadoop YARN to perform analytics using ScalaPyspark programming Created dataframes and performed various transformations to generate recommendation strategy Created hive tables and views for business to access the recommendation strategy Used Spark streaming on Kafka to achieve real time data analytics Created Dstreams dataframes from streaming data and performed transformations Extract Real time feed using Kafka and Spark Streaming and convert it to RDD and process data in the form of Data Frame Persist the streaming data on Hbase Nosql database Wrote shell scripts to automate the jobs in UNIX Experienced in handling large datasets using Partitions Spark in Memory capabilities Broadcasts in Spark Efficient joins Transformations Used Spark SQL to save the data into multiple Hive tables Developed Hive queries to process the data for visualizing Environment Spark 16 Hadoop Hive 20 HDFS Kafka Sqoop 146 Java 18 Scala 211 CDH582 Oozie Eclipse Elasticsearch Parquet Shell Scripting bitbucket Bank of America Charlotte NC September 2015 to November 2017 Project Description Bank of America is American multinational banking and financial services corporation and is the second largest bank of United States of America The objective of the project is to pull data from upstream and apply smoothing transformation value add process to generate final basel views for federal reporting The project is being migrated from MainframeTeradata to Hadoop eco system Responsibilities Used Spark API to perform analytics on data in Hive using Scala programming Optimization of existing algorithms in Hadoop using Spark Context Data Frames Hive context Spark RDDs are created in Scala for all the data files which then undergo transformations The filtered RDDs are aggregated and transformed based on the business rules and converted into data frames and saved as temporary hive tables for intermediate processing The RDDs and data frames undergo various transformations and actions and are stored in HDFS as parquet Files to create Impala views Used Oozie scheduler to create workflows and scheduled jobs in Hadoop Cluster Written Hive UDFs to extract data from staging tables Involved in creating Hive tables views to load transform the data Involved in writing Pig scripts Supported and Monitored Map Reduce Programs running on the cluster Worked on data quality framework to generate reports to business on the quality of data processed in Hadoop Worked on developing Web UI using J2EE for reports generated on the final Basel views Environment Java 18 Scala 211Spark 16 Hadoop Pig012 Hive 11 Map Reduce HDFS MySQL Sqoop 146 CDH582 Oozie Eclipse Avro Parquet Toad Shell Scripting Teradata Impala J2EE SAS EG Hadoop Developer Prudential Financial NJ May 2013 to September 2015 Project Description Prudential is one of the largest insurance and financial services institutions in the United States of America This project is designed to extract raw data from different sources into Hadoop Eco system to create and populate the necessary Hive tables The main aim of the project is to centralize the source of data for report generation using historical database which otherwise are generated from multiple sources Responsibilities Worked on analyzing Hadoop cluster and different big data analytic tools including Pig Hbase Nosql database Importing and exporting data in HDFS and Hive using Sqoop Designed and developed Map Reduce jobs to process data from upstream Experience with NoSQL databases Worked on data ingestion to bring data to HDFS and Hive Written Hive UDFs to extract data from staging tables Involved in creating Hive tables loading with data Hands on writing Map Reduce code to make unstructured data as structured data and for inserting data into HBase from HDFS Experience in creating integration between Hive and HBase Used Oozie scheduler to submit workflows Review QA test cases with the QA team Environment Java 6 Eclipse Hadoop Pig012 Hive 013 Centos 64 Map Reduce HDFS My SQL Sqoop 144 CDH4 Hue Oozie Toad HBASE Sr Java Developer Prudential Financial Bengaluru Karnataka August 2010 to May 2013 Project Description Prudential is one of the largest insurance and financial service institutions in the United States of America PASSPORT Plan Admin Setup System Portal Application is built for plan Setup This helps to setup a plan and maintain tool that handles Vision and peripheral system setup for defined contribution plans Responsibilities Involved in software development on webbased frontend applications Involved in development of the CSV files using the Data load Performed unit testing of the developed modules Involved in bug fixing writing SQL queries unit test cases Used Rational Application Developer RAD Used Oracle as the Backend Database Involved in configuration and deployment of frontend application on RAD Involved in developing JSPs for graphical user interface Implemented code for validating the input fields and displaying the error messages Environment Java JSP Servlets Apache Struts framework WebSphere RAD Oracle PVCS TOAD Java Developer Danske Bank Bengaluru Karnataka September 2008 to August 2010 Project Description Worked on a key development project for Danske bank to develop a new IT system to automate its Trade services like Guarantee Letter of credit etc Responsibilities Participated in the implementation of efforts like coding unit testing Implemented a Web based application using SERVLETS JSP Client side validation has been done using Java Script Involved in Unit integration and bug fixing Involved in acceptance testing with test cases and code reviews Developed code for handling the exceptions using exceptional handing Involved in writing and executing queries in MySQL Developed the application on Eclipse Involved in deploying application on WebSphere server Prepared test case document and performed unit testing and system testing Environment Java JSP Servlets Java script WebSphere MySQL Eclipse TOAD Education Executive MBA IIM Kozhikode 2013 Bachelor of Engineering in Engineering National Institute of Technology Karnataka 2004 Skills Eclipse Java Jboss jquery Jsp Servlets Dynamodb Hdfs Impala Oozie Sqoop Hbase Kafka Cdh Db2 Hadoop Avro C Design patterns Hadoop Additional Information Technical Skills Hadoop Technologies Hadoop HDFS Hadoop MapReduce Hive HBase SQOOP Oozie AVRO PigLatin Hue CDH Parquet Impala Scala Spark Python KafkaAWSS3Trifacta DynamoDB EMR Apache Nifi No Sql HBase IDETools RAD Eclipse Web and Application Servers Web sphere JBOSS Tomcat Core Competency Technologies Java OOPS design patterns JSP servlets JDBC java 5 java 6 java 7 C C shell scripting Spark SAS EG Scala Spark Streaming Kafka Web presentation frameworks Java Script HTML AJAX jQuery CSS JSON Testing Issue Log tools JUnit 4 Bugzilla HP Quality Centre SCMVersion control tools PVCS CVS Sub Version Modeling tools Visio 2007 Build and continuous Integration Maven ANT Data base Oracle 8i9i10g DB2 MySQL 4x5x OS UNIX LINUX Windows Aix",
    "entities": [
        "Spark Efficient",
        "HDFS",
        "Project Description Worked",
        "MicroStrategy",
        "the Project Environment Spark",
        "Used Rational Application Developer RAD Used Oracle",
        "Spark Organizing",
        "IBM",
        "HadoopSpark",
        "Project Description Bank of America",
        "RDD",
        "Hadoop",
        "Environment Spark",
        "Bugzilla HP Quality Centre SCMVersion",
        "Hadoop Cluster Written Hive",
        "Modeling",
        "HBase",
        "JavaJ2EE",
        "Cloudera Hadoop",
        "JSP Client",
        "WebSphere",
        "ScalaPyspark",
        "Skills",
        "Oozie Experienced",
        "Hadoop Big",
        "Java Script Involved",
        "NIFI",
        "Responsibilities Involved",
        "the Backend Database Involved",
        "Spark Context Data",
        "Trade",
        "Hadoop Worked",
        "Performed POC",
        "Basel",
        "JSP",
        "Vision",
        "Work Experience Big Data",
        "Hadoop Additional Information Technical Skills Hadoop Technologies Hadoop HDFS Hadoop MapReduce Hive HBase",
        "Danske",
        "HDP",
        "Engineering National Institute of Technology",
        "Hadoop Eco",
        "Spark",
        "WebSphere RAD Oracle PVCS",
        "CSV",
        "Spark SAS",
        "Sqoop",
        "Created",
        "Bank of America Charlotte",
        "AWS",
        "HadoopSpark Developer",
        "Oracle",
        "Spark SQL HIVEQL",
        "UNIX Experienced",
        "Created Dstreams",
        "United States of America",
        "Oozie",
        "Finance Insurance",
        "SQL",
        "UDF",
        "Redshift",
        "Relational Database Systems",
        "S3 Redshift HDFS Hive Prune",
        "Present Project Description Hilton",
        "Hive Performing",
        "the United States of America",
        "Hive",
        "Partitions Spark",
        "Integration Maven ANT Data",
        "MainframeTeradata",
        "FL",
        "Project Description Prudential",
        "Performed",
        "RAD Involved",
        "Impala",
        "Data Frame Persist",
        "Scala programming Optimization",
        "Guarantee Letter",
        "Pig and Hive Experience",
        "IDE Tool",
        "Analysis and Design Development",
        "Data",
        "MapReduce",
        "Team",
        "NoSQL"
    ],
    "experience": "Experience in analyzing data using Spark SQL HIVEQL PIG Latin and experience in developing custom UDF s using Pig and Hive Experience in importing and exporting data using Sqoop from HDFS to Relational Database Systems and viceversa Good knowledge in using job scheduling tools like Oozie Experienced in using IDE Tool like Eclipse 3x IBM RAD 70 Experience in requirement gathering analysis planning designing coding and unit testing Strong work ethic with desire to succeed and make significant contributions to the organization Strong problem solving skills good communication interpersonal skills and a good team player Have the motivation to take independent responsibility as well as ability to contribute and be a productive team member Work Experience Big Data Tech Lead Architect Hilton Worldwide McLean VA September 2018 to Present Project Description Hilton is forward thinking global leader of hospitality Hilton is highly data driven company which uses Hadoop Big data architecture to make key business decisions The ingested data from different upstream applications will be transformed and loaded to Redshift which will be used by MicroStrategy reporting team to generate dashboards that aid in key business decisions Responsibilities Used NIFI as dataflow automation tool to ingest data into HDFS from different source systems Developed common process to bulk load raw HDFS files into dataframe Developed common process to persist dataframe into S3 Redshift HDFS Hive Prune the ingested data to remove duplicates by applying window functions and perform complex transformations to derive various metrics Used oozie scheduler to trigger spark jobs Performed POC on airflow scheduler and Involved in migration of oozie scheduler to airflow Created UDFs in spark to be used in spark sql Performed POC to consume requests from microstrategy dashboard which will in turn trigger the spark job The spark job dynamically generates spark sql based on the level of granularity business user requests and load data into Redshift to be used on dashboard Involved in migration of HDP to AWS and various proof of concepts to achieve it Guide junior developers in their day to day activities and ensure delivery of project Used Spark streaming on Kafka to achieve real time data analytics Created Dstreams dataframes from streaming data and performed transformations Performed performance tuning of spark jobs using broadcast joins correct level of Parallelism and memory tuning Analyze and define clients business strategy and determine system architecture requirements to achieve business goals Lead the Team to complete critical and measurable mile stones of the Project Environment Spark 22 Hadoop Hive 21 HDFS Java 18 Scala 211 HDP Elasticsearch AWS Redshift Oozie Intellij ORC Shell Scripting bitbucketairflowPython Pyspark Lead HadoopSpark Consultant Citibank Tampa FL November 2017 to September 2018 Responsibilities Used Spark API over Cloudera Hadoop YARN to perform analytics using ScalaPyspark programming Created dataframes and performed various transformations to generate recommendation strategy Created hive tables and views for business to access the recommendation strategy Used Spark streaming on Kafka to achieve real time data analytics Created Dstreams dataframes from streaming data and performed transformations Extract Real time feed using Kafka and Spark Streaming and convert it to RDD and process data in the form of Data Frame Persist the streaming data on Hbase Nosql database Wrote shell scripts to automate the jobs in UNIX Experienced in handling large datasets using Partitions Spark in Memory capabilities Broadcasts in Spark Efficient joins Transformations Used Spark SQL to save the data into multiple Hive tables Developed Hive queries to process the data for visualizing Environment Spark 16 Hadoop Hive 20 HDFS Kafka Sqoop 146 Java 18 Scala 211 CDH582 Oozie Eclipse Elasticsearch Parquet Shell Scripting bitbucket Bank of America Charlotte NC September 2015 to November 2017 Project Description Bank of America is American multinational banking and financial services corporation and is the second largest bank of United States of America The objective of the project is to pull data from upstream and apply smoothing transformation value add process to generate final basel views for federal reporting The project is being migrated from MainframeTeradata to Hadoop eco system Responsibilities Used Spark API to perform analytics on data in Hive using Scala programming Optimization of existing algorithms in Hadoop using Spark Context Data Frames Hive context Spark RDDs are created in Scala for all the data files which then undergo transformations The filtered RDDs are aggregated and transformed based on the business rules and converted into data frames and saved as temporary hive tables for intermediate processing The RDDs and data frames undergo various transformations and actions and are stored in HDFS as parquet Files to create Impala views Used Oozie scheduler to create workflows and scheduled jobs in Hadoop Cluster Written Hive UDFs to extract data from staging tables Involved in creating Hive tables views to load transform the data Involved in writing Pig scripts Supported and Monitored Map Reduce Programs running on the cluster Worked on data quality framework to generate reports to business on the quality of data processed in Hadoop Worked on developing Web UI using J2EE for reports generated on the final Basel views Environment Java 18 Scala 211Spark 16 Hadoop Pig012 Hive 11 Map Reduce HDFS MySQL Sqoop 146 CDH582 Oozie Eclipse Avro Parquet Toad Shell Scripting Teradata Impala J2EE SAS EG Hadoop Developer Prudential Financial NJ May 2013 to September 2015 Project Description Prudential is one of the largest insurance and financial services institutions in the United States of America This project is designed to extract raw data from different sources into Hadoop Eco system to create and populate the necessary Hive tables The main aim of the project is to centralize the source of data for report generation using historical database which otherwise are generated from multiple sources Responsibilities Worked on analyzing Hadoop cluster and different big data analytic tools including Pig Hbase Nosql database Importing and exporting data in HDFS and Hive using Sqoop Designed and developed Map Reduce jobs to process data from upstream Experience with NoSQL databases Worked on data ingestion to bring data to HDFS and Hive Written Hive UDFs to extract data from staging tables Involved in creating Hive tables loading with data Hands on writing Map Reduce code to make unstructured data as structured data and for inserting data into HBase from HDFS Experience in creating integration between Hive and HBase Used Oozie scheduler to submit workflows Review QA test cases with the QA team Environment Java 6 Eclipse Hadoop Pig012 Hive 013 Centos 64 Map Reduce HDFS My SQL Sqoop 144 CDH4 Hue Oozie Toad HBASE Sr Java Developer Prudential Financial Bengaluru Karnataka August 2010 to May 2013 Project Description Prudential is one of the largest insurance and financial service institutions in the United States of America PASSPORT Plan Admin Setup System Portal Application is built for plan Setup This helps to setup a plan and maintain tool that handles Vision and peripheral system setup for defined contribution plans Responsibilities Involved in software development on webbased frontend applications Involved in development of the CSV files using the Data load Performed unit testing of the developed modules Involved in bug fixing writing SQL queries unit test cases Used Rational Application Developer RAD Used Oracle as the Backend Database Involved in configuration and deployment of frontend application on RAD Involved in developing JSPs for graphical user interface Implemented code for validating the input fields and displaying the error messages Environment Java JSP Servlets Apache Struts framework WebSphere RAD Oracle PVCS TOAD Java Developer Danske Bank Bengaluru Karnataka September 2008 to August 2010 Project Description Worked on a key development project for Danske bank to develop a new IT system to automate its Trade services like Guarantee Letter of credit etc Responsibilities Participated in the implementation of efforts like coding unit testing Implemented a Web based application using SERVLETS JSP Client side validation has been done using Java Script Involved in Unit integration and bug fixing Involved in acceptance testing with test cases and code reviews Developed code for handling the exceptions using exceptional handing Involved in writing and executing queries in MySQL Developed the application on Eclipse Involved in deploying application on WebSphere server Prepared test case document and performed unit testing and system testing Environment Java JSP Servlets Java script WebSphere MySQL Eclipse TOAD Education Executive MBA IIM Kozhikode 2013 Bachelor of Engineering in Engineering National Institute of Technology Karnataka 2004 Skills Eclipse Java Jboss jquery Jsp Servlets Dynamodb Hdfs Impala Oozie Sqoop Hbase Kafka Cdh Db2 Hadoop Avro C Design patterns Hadoop Additional Information Technical Skills Hadoop Technologies Hadoop HDFS Hadoop MapReduce Hive HBase SQOOP Oozie AVRO PigLatin Hue CDH Parquet Impala Scala Spark Python KafkaAWSS3Trifacta DynamoDB EMR Apache Nifi No Sql HBase IDETools RAD Eclipse Web and Application Servers Web sphere JBOSS Tomcat Core Competency Technologies Java OOPS design patterns JSP servlets JDBC java 5 java 6 java 7 C C shell scripting Spark SAS EG Scala Spark Streaming Kafka Web presentation frameworks Java Script HTML AJAX jQuery CSS JSON Testing Issue Log tools JUnit 4 Bugzilla HP Quality Centre SCMVersion control tools PVCS CVS Sub Version Modeling tools Visio 2007 Build and continuous Integration Maven ANT Data base Oracle 8i9i10 g DB2 MySQL 4x5x OS UNIX LINUX Windows Aix",
    "extracted_keywords": [
        "Big",
        "Data",
        "Tech",
        "Lead",
        "Architect",
        "Big",
        "Data",
        "Tech",
        "Lead",
        "Architect",
        "Big",
        "Data",
        "Tech",
        "Lead",
        "Architect",
        "Hilton",
        "Worldwide",
        "experience",
        "data",
        "HadoopSpark",
        "JavaJ2EE",
        "technologies",
        "development",
        "requirements",
        "Analysis",
        "Design",
        "Development",
        "implementation",
        "support",
        "maintenance",
        "enhancements",
        "Finance",
        "Insurance",
        "domains",
        "years",
        "experience",
        "HadoopSpark",
        "Developer",
        "knowledge",
        "Java",
        "Map",
        "Reduce",
        "Hive",
        "Pig",
        "Latin",
        "Scala",
        "Spark",
        "data",
        "tables",
        "transformations",
        "queries",
        "Hive",
        "Performing",
        "analysis",
        "data",
        "sets",
        "HDFS",
        "knowledge",
        "experience",
        "Hadoop",
        "architecture",
        "components",
        "HDFS",
        "YARN",
        "Pig",
        "Hive",
        "Sqoop",
        "Oozie",
        "Flume",
        "Spark",
        "Kafka",
        "Map",
        "programming",
        "paradigm",
        "MapReduce",
        "programs",
        "data",
        "Spark",
        "SQL",
        "HIVEQL",
        "PIG",
        "Latin",
        "experience",
        "custom",
        "UDF",
        "Pig",
        "Hive",
        "Experience",
        "data",
        "Sqoop",
        "HDFS",
        "Relational",
        "Database",
        "Systems",
        "viceversa",
        "knowledge",
        "job",
        "scheduling",
        "tools",
        "Oozie",
        "IDE",
        "Tool",
        "Eclipse",
        "IBM",
        "RAD",
        "Experience",
        "requirement",
        "analysis",
        "designing",
        "coding",
        "unit",
        "work",
        "desire",
        "contributions",
        "organization",
        "Strong",
        "problem",
        "skills",
        "communication",
        "skills",
        "team",
        "player",
        "motivation",
        "responsibility",
        "ability",
        "team",
        "member",
        "Work",
        "Experience",
        "Big",
        "Data",
        "Tech",
        "Lead",
        "Architect",
        "Hilton",
        "Worldwide",
        "McLean",
        "VA",
        "September",
        "Present",
        "Project",
        "Description",
        "Hilton",
        "leader",
        "hospitality",
        "Hilton",
        "data",
        "company",
        "Hadoop",
        "Big",
        "data",
        "architecture",
        "business",
        "decisions",
        "data",
        "applications",
        "Redshift",
        "MicroStrategy",
        "reporting",
        "team",
        "dashboards",
        "aid",
        "business",
        "decisions",
        "Responsibilities",
        "NIFI",
        "automation",
        "tool",
        "data",
        "HDFS",
        "source",
        "systems",
        "process",
        "HDFS",
        "files",
        "process",
        "dataframe",
        "S3",
        "Redshift",
        "HDFS",
        "Hive",
        "Prune",
        "data",
        "duplicates",
        "window",
        "functions",
        "transformations",
        "metrics",
        "oozie",
        "scheduler",
        "spark",
        "jobs",
        "Performed",
        "POC",
        "airflow",
        "scheduler",
        "migration",
        "oozie",
        "scheduler",
        "UDFs",
        "spark",
        "spark",
        "sql",
        "Performed",
        "POC",
        "requests",
        "microstrategy",
        "dashboard",
        "turn",
        "spark",
        "job",
        "spark",
        "job",
        "spark",
        "sql",
        "level",
        "granularity",
        "business",
        "user",
        "requests",
        "load",
        "data",
        "Redshift",
        "dashboard",
        "migration",
        "HDP",
        "AWS",
        "proof",
        "concepts",
        "Guide",
        "developers",
        "day",
        "day",
        "activities",
        "delivery",
        "project",
        "Spark",
        "streaming",
        "Kafka",
        "time",
        "data",
        "analytics",
        "Created",
        "Dstreams",
        "data",
        "transformations",
        "performance",
        "tuning",
        "spark",
        "jobs",
        "broadcast",
        "level",
        "Parallelism",
        "memory",
        "tuning",
        "Analyze",
        "clients",
        "business",
        "strategy",
        "system",
        "architecture",
        "requirements",
        "business",
        "goals",
        "Team",
        "mile",
        "stones",
        "Project",
        "Environment",
        "Spark",
        "Hadoop",
        "Hive",
        "HDFS",
        "Java",
        "Scala",
        "HDP",
        "Elasticsearch",
        "AWS",
        "Redshift",
        "Oozie",
        "Intellij",
        "ORC",
        "Shell",
        "Scripting",
        "Pyspark",
        "Lead",
        "HadoopSpark",
        "Consultant",
        "Citibank",
        "Tampa",
        "FL",
        "November",
        "September",
        "Responsibilities",
        "Spark",
        "API",
        "Cloudera",
        "Hadoop",
        "YARN",
        "analytics",
        "ScalaPyspark",
        "programming",
        "Created",
        "dataframes",
        "transformations",
        "recommendation",
        "strategy",
        "hive",
        "tables",
        "views",
        "business",
        "recommendation",
        "strategy",
        "Spark",
        "streaming",
        "Kafka",
        "time",
        "data",
        "analytics",
        "Created",
        "Dstreams",
        "data",
        "transformations",
        "Extract",
        "time",
        "feed",
        "Kafka",
        "Spark",
        "Streaming",
        "process",
        "data",
        "form",
        "Data",
        "Frame",
        "data",
        "Hbase",
        "Nosql",
        "database",
        "Wrote",
        "shell",
        "scripts",
        "jobs",
        "UNIX",
        "datasets",
        "Partitions",
        "Spark",
        "Memory",
        "capabilities",
        "Broadcasts",
        "Spark",
        "Efficient",
        "Transformations",
        "Spark",
        "SQL",
        "data",
        "Hive",
        "tables",
        "Developed",
        "Hive",
        "data",
        "Environment",
        "Spark",
        "Hadoop",
        "Hive",
        "HDFS",
        "Kafka",
        "Sqoop",
        "Java",
        "Scala",
        "CDH582",
        "Oozie",
        "Eclipse",
        "Elasticsearch",
        "Parquet",
        "Shell",
        "Scripting",
        "bitbucket",
        "Bank",
        "America",
        "Charlotte",
        "NC",
        "September",
        "November",
        "Project",
        "Description",
        "Bank",
        "America",
        "banking",
        "services",
        "corporation",
        "bank",
        "United",
        "States",
        "America",
        "objective",
        "project",
        "data",
        "transformation",
        "value",
        "process",
        "basel",
        "views",
        "reporting",
        "project",
        "MainframeTeradata",
        "Hadoop",
        "eco",
        "system",
        "Responsibilities",
        "Spark",
        "API",
        "analytics",
        "data",
        "Hive",
        "Scala",
        "programming",
        "Optimization",
        "algorithms",
        "Hadoop",
        "Spark",
        "Context",
        "Data",
        "Frames",
        "Hive",
        "context",
        "Spark",
        "RDDs",
        "Scala",
        "data",
        "files",
        "transformations",
        "RDDs",
        "business",
        "rules",
        "data",
        "frames",
        "hive",
        "tables",
        "processing",
        "RDDs",
        "data",
        "frames",
        "transformations",
        "actions",
        "HDFS",
        "Files",
        "Impala",
        "views",
        "Oozie",
        "scheduler",
        "workflows",
        "jobs",
        "Hadoop",
        "Cluster",
        "Written",
        "Hive",
        "UDFs",
        "data",
        "tables",
        "Hive",
        "tables",
        "views",
        "data",
        "Pig",
        "scripts",
        "Monitored",
        "Map",
        "Reduce",
        "Programs",
        "cluster",
        "data",
        "quality",
        "framework",
        "reports",
        "business",
        "quality",
        "data",
        "Hadoop",
        "Worked",
        "Web",
        "UI",
        "J2EE",
        "reports",
        "Basel",
        "views",
        "Environment",
        "Java",
        "Scala",
        "211Spark",
        "Hadoop",
        "Pig012",
        "Hive",
        "Map",
        "Reduce",
        "HDFS",
        "MySQL",
        "Sqoop",
        "CDH582",
        "Oozie",
        "Eclipse",
        "Avro",
        "Parquet",
        "Toad",
        "Shell",
        "Scripting",
        "Teradata",
        "Impala",
        "J2EE",
        "SAS",
        "EG",
        "Hadoop",
        "Developer",
        "Prudential",
        "Financial",
        "NJ",
        "May",
        "September",
        "Project",
        "Description",
        "Prudential",
        "insurance",
        "services",
        "institutions",
        "United",
        "States",
        "America",
        "project",
        "data",
        "sources",
        "Hadoop",
        "Eco",
        "system",
        "Hive",
        "tables",
        "aim",
        "project",
        "source",
        "data",
        "report",
        "generation",
        "database",
        "sources",
        "Responsibilities",
        "Hadoop",
        "cluster",
        "data",
        "tools",
        "Pig",
        "Hbase",
        "Nosql",
        "database",
        "data",
        "HDFS",
        "Hive",
        "Sqoop",
        "Designed",
        "Map",
        "Reduce",
        "jobs",
        "data",
        "Experience",
        "NoSQL",
        "data",
        "ingestion",
        "data",
        "HDFS",
        "Hive",
        "Written",
        "Hive",
        "UDFs",
        "data",
        "tables",
        "Hive",
        "tables",
        "data",
        "Hands",
        "Map",
        "Reduce",
        "code",
        "data",
        "data",
        "data",
        "HBase",
        "HDFS",
        "Experience",
        "integration",
        "Hive",
        "HBase",
        "Oozie",
        "scheduler",
        "workflows",
        "Review",
        "QA",
        "test",
        "cases",
        "QA",
        "team",
        "Environment",
        "Java",
        "Eclipse",
        "Hadoop",
        "Pig012",
        "Hive",
        "Centos",
        "Map",
        "Reduce",
        "HDFS",
        "SQL",
        "Sqoop",
        "CDH4",
        "Hue",
        "Oozie",
        "Toad",
        "HBASE",
        "Sr",
        "Java",
        "Developer",
        "Prudential",
        "Financial",
        "Bengaluru",
        "Karnataka",
        "August",
        "May",
        "Project",
        "Description",
        "Prudential",
        "insurance",
        "service",
        "institutions",
        "United",
        "States",
        "America",
        "PASSPORT",
        "Plan",
        "Admin",
        "Setup",
        "System",
        "Portal",
        "Application",
        "plan",
        "Setup",
        "plan",
        "tool",
        "Vision",
        "system",
        "setup",
        "contribution",
        "Responsibilities",
        "software",
        "development",
        "frontend",
        "applications",
        "development",
        "CSV",
        "files",
        "Data",
        "load",
        "Performed",
        "unit",
        "testing",
        "modules",
        "bug",
        "SQL",
        "queries",
        "unit",
        "test",
        "cases",
        "Rational",
        "Application",
        "Developer",
        "RAD",
        "Oracle",
        "Backend",
        "Database",
        "configuration",
        "deployment",
        "frontend",
        "application",
        "RAD",
        "JSPs",
        "user",
        "interface",
        "code",
        "input",
        "fields",
        "error",
        "messages",
        "Environment",
        "Java",
        "JSP",
        "Servlets",
        "Apache",
        "Struts",
        "framework",
        "WebSphere",
        "RAD",
        "Oracle",
        "PVCS",
        "TOAD",
        "Java",
        "Developer",
        "Danske",
        "Bank",
        "Bengaluru",
        "Karnataka",
        "September",
        "August",
        "Project",
        "Description",
        "development",
        "project",
        "Danske",
        "bank",
        "IT",
        "system",
        "Trade",
        "services",
        "Guarantee",
        "Letter",
        "credit",
        "Responsibilities",
        "implementation",
        "efforts",
        "unit",
        "testing",
        "Web",
        "application",
        "SERVLETS",
        "JSP",
        "Client",
        "side",
        "validation",
        "Java",
        "Script",
        "Unit",
        "integration",
        "bug",
        "fixing",
        "acceptance",
        "testing",
        "test",
        "cases",
        "code",
        "reviews",
        "Developed",
        "code",
        "exceptions",
        "handing",
        "writing",
        "queries",
        "MySQL",
        "application",
        "Eclipse",
        "application",
        "WebSphere",
        "server",
        "test",
        "case",
        "document",
        "unit",
        "testing",
        "system",
        "testing",
        "Environment",
        "Java",
        "JSP",
        "Servlets",
        "Java",
        "script",
        "WebSphere",
        "MySQL",
        "Eclipse",
        "TOAD",
        "Education",
        "Executive",
        "MBA",
        "IIM",
        "Kozhikode",
        "Bachelor",
        "Engineering",
        "Engineering",
        "National",
        "Institute",
        "Technology",
        "Karnataka",
        "Skills",
        "Eclipse",
        "Java",
        "Jboss",
        "jquery",
        "Jsp",
        "Servlets",
        "Dynamodb",
        "Hdfs",
        "Impala",
        "Oozie",
        "Sqoop",
        "Hbase",
        "Kafka",
        "Cdh",
        "Db2",
        "Hadoop",
        "Avro",
        "C",
        "Design",
        "patterns",
        "Hadoop",
        "Additional",
        "Information",
        "Technical",
        "Skills",
        "Hadoop",
        "Technologies",
        "Hadoop",
        "HDFS",
        "Hadoop",
        "MapReduce",
        "Hive",
        "HBase",
        "SQOOP",
        "Oozie",
        "AVRO",
        "PigLatin",
        "Hue",
        "CDH",
        "Parquet",
        "Impala",
        "Scala",
        "Spark",
        "Python",
        "DynamoDB",
        "EMR",
        "Apache",
        "Nifi",
        "Sql",
        "HBase",
        "IDETools",
        "RAD",
        "Eclipse",
        "Web",
        "Application",
        "Servers",
        "Web",
        "sphere",
        "JBOSS",
        "Tomcat",
        "Core",
        "Competency",
        "Technologies",
        "Java",
        "design",
        "JSP",
        "JDBC",
        "C",
        "C",
        "shell",
        "Spark",
        "SAS",
        "EG",
        "Scala",
        "Spark",
        "Streaming",
        "Kafka",
        "Web",
        "presentation",
        "frameworks",
        "Java",
        "Script",
        "HTML",
        "AJAX",
        "jQuery",
        "CSS",
        "JSON",
        "Testing",
        "Issue",
        "Log",
        "tools",
        "JUnit",
        "Bugzilla",
        "HP",
        "Quality",
        "Centre",
        "SCMVersion",
        "control",
        "tools",
        "PVCS",
        "CVS",
        "Sub",
        "Version",
        "Modeling",
        "tools",
        "Visio",
        "Build",
        "Integration",
        "Maven",
        "ANT",
        "Data",
        "base",
        "Oracle",
        "g",
        "DB2",
        "MySQL",
        "UNIX",
        "LINUX",
        "Windows",
        "Aix"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T21:26:21.559679",
    "resume_data": "Big Data Tech Lead Architect Big Data Tech Lead Architect Big Data Tech Lead Architect Hilton Worldwide Solid experience in Big data HadoopSpark and JavaJ2EE technologies development including requirements Analysis and Design Development implementation support maintenance and enhancements in Finance Insurance domains 5 years of experience as HadoopSpark Developer with good knowledge of Java Map Reduce Hive Pig Latin Scala and Spark Organizing data into tables performing transformations and simplifying complex queries with Hive Performing realtime interactive analysis on massive data sets stored in HDFS Strong knowledge and experience with Hadoop architecture and various components such as HDFS YARN Pig Hive Sqoop Oozie Flume Spark Kafka and Map Reduce programming paradigm Developed many MapReduce programs Experience in analyzing data using Spark SQL HIVEQL PIG Latin and experience in developing custom UDF s using Pig and Hive Experience in importing and exporting data using Sqoop from HDFS to Relational Database Systems and viceversa Good knowledge in using job scheduling tools like Oozie Experienced in using IDE Tool like Eclipse 3x IBM RAD 70 Experience in requirement gathering analysis planning designing coding and unit testing Strong work ethic with desire to succeed and make significant contributions to the organization Strong problem solving skills good communication interpersonal skills and a good team player Have the motivation to take independent responsibility as well as ability to contribute and be a productive team member Work Experience Big Data Tech Lead Architect Hilton Worldwide McLean VA September 2018 to Present Project Description Hilton is forward thinking global leader of hospitality Hilton is highly data driven company which uses Hadoop Big data architecture to make key business decisions The ingested data from different upstream applications will be transformed and loaded to Redshift which will be used by MicroStrategy reporting team to generate dashboards that aid in key business decisions Responsibilities Used NIFI as dataflow automation tool to ingest data into HDFS from different source systems Developed common process to bulk load raw HDFS files into dataframe Developed common process to persist dataframe into S3 Redshift HDFS Hive Prune the ingested data to remove duplicates by applying window functions and perform complex transformations to derive various metrics Used oozie scheduler to trigger spark jobs Performed POC on airflow scheduler and Involved in migration of oozie scheduler to airflow Created UDFs in spark to be used in spark sql Performed POC to consume requests from microstrategy dashboard which will in turn trigger the spark job The spark job dynamically generates spark sql based on the level of granularity business user requests and load data into Redshift to be used on dashboard Involved in migration of HDP to AWS and various proof of concepts to achieve it Guide junior developers in their day to day activities and ensure delivery of project Used Spark streaming on Kafka to achieve real time data analytics Created Dstreams dataframes from streaming data and performed transformations Performed performance tuning of spark jobs using broadcast joins correct level of Parallelism and memory tuning Analyze and define clients business strategy and determine system architecture requirements to achieve business goals Lead the Team to complete critical and measurable mile stones of the Project Environment Spark 22 Hadoop Hive 21 HDFS Java 18 Scala 211 HDP Elasticsearch AWS Redshift Oozie Intellij ORC Shell Scripting bitbucketairflowPython Pyspark Lead HadoopSpark Consultant Citibank Tampa FL November 2017 to September 2018 Responsibilities Used Spark API over Cloudera Hadoop YARN to perform analytics using ScalaPyspark programming Created dataframes and performed various transformations to generate recommendation strategy Created hive tables and views for business to access the recommendation strategy Used Spark streaming on Kafka to achieve real time data analytics Created Dstreams dataframes from streaming data and performed transformations Extract Real time feed using Kafka and Spark Streaming and convert it to RDD and process data in the form of Data Frame Persist the streaming data on Hbase Nosql database Wrote shell scripts to automate the jobs in UNIX Experienced in handling large datasets using Partitions Spark in Memory capabilities Broadcasts in Spark Efficient joins Transformations Used Spark SQL to save the data into multiple Hive tables Developed Hive queries to process the data for visualizing Environment Spark 16 Hadoop Hive 20 HDFS Kafka Sqoop 146 Java 18 Scala 211 CDH582 Oozie Eclipse Elasticsearch Parquet Shell Scripting bitbucket Bank of America Charlotte NC September 2015 to November 2017 Project Description Bank of America is American multinational banking and financial services corporation and is the second largest bank of United States of America The objective of the project is to pull data from upstream and apply smoothing transformation value add process to generate final basel views for federal reporting The project is being migrated from MainframeTeradata to Hadoop eco system Responsibilities Used Spark API to perform analytics on data in Hive using Scala programming Optimization of existing algorithms in Hadoop using Spark Context Data Frames Hive context Spark RDDs are created in Scala for all the data files which then undergo transformations The filtered RDDs are aggregated and transformed based on the business rules and converted into data frames and saved as temporary hive tables for intermediate processing The RDDs and data frames undergo various transformations and actions and are stored in HDFS as parquet Files to create Impala views Used Oozie scheduler to create workflows and scheduled jobs in Hadoop Cluster Written Hive UDFs to extract data from staging tables Involved in creating Hive tables views to load transform the data Involved in writing Pig scripts Supported and Monitored Map Reduce Programs running on the cluster Worked on data quality framework to generate reports to business on the quality of data processed in Hadoop Worked on developing Web UI using J2EE for reports generated on the final Basel views Environment Java 18 Scala 211Spark 16 Hadoop Pig012 Hive 11 Map Reduce HDFS MySQL Sqoop 146 CDH582 Oozie Eclipse Avro Parquet Toad Shell Scripting Teradata Impala J2EE SAS EG Hadoop Developer Prudential Financial NJ May 2013 to September 2015 Project Description Prudential is one of the largest insurance and financial services institutions in the United States of America This project is designed to extract raw data from different sources into Hadoop Eco system to create and populate the necessary Hive tables The main aim of the project is to centralize the source of data for report generation using historical database which otherwise are generated from multiple sources Responsibilities Worked on analyzing Hadoop cluster and different big data analytic tools including Pig Hbase Nosql database Importing and exporting data in HDFS and Hive using Sqoop Designed and developed Map Reduce jobs to process data from upstream Experience with NoSQL databases Worked on data ingestion to bring data to HDFS and Hive Written Hive UDFs to extract data from staging tables Involved in creating Hive tables loading with data Hands on writing Map Reduce code to make unstructured data as structured data and for inserting data into HBase from HDFS Experience in creating integration between Hive and HBase Used Oozie scheduler to submit workflows Review QA test cases with the QA team Environment Java 6 Eclipse Hadoop Pig012 Hive 013 Centos 64 Map Reduce HDFS My SQL Sqoop 144 CDH4 Hue Oozie Toad HBASE Sr Java Developer Prudential Financial Bengaluru Karnataka August 2010 to May 2013 Project Description Prudential is one of the largest insurance and financial service institutions in the United States of America PASSPORT Plan Admin Setup System Portal Application is built for plan Setup This helps to setup a plan and maintain tool that handles Vision and peripheral system setup for defined contribution plans Responsibilities Involved in software development on webbased frontend applications Involved in development of the CSV files using the Data load Performed unit testing of the developed modules Involved in bug fixing writing SQL queries unit test cases Used Rational Application Developer RAD Used Oracle as the Backend Database Involved in configuration and deployment of frontend application on RAD Involved in developing JSPs for graphical user interface Implemented code for validating the input fields and displaying the error messages Environment Java JSP Servlets Apache Struts framework WebSphere RAD Oracle PVCS TOAD Java Developer Danske Bank Bengaluru Karnataka September 2008 to August 2010 Project Description Worked on a key development project for Danske bank to develop a new IT system to automate its Trade services like Guarantee Letter of credit etc Responsibilities Participated in the implementation of efforts like coding unit testing Implemented a Web based application using SERVLETS JSP Client side validation has been done using Java Script Involved in Unit integration and bug fixing Involved in acceptance testing with test cases and code reviews Developed code for handling the exceptions using exceptional handing Involved in writing and executing queries in MySQL Developed the application on Eclipse Involved in deploying application on WebSphere server Prepared test case document and performed unit testing and system testing Environment Java JSP Servlets Java script WebSphere MySQL Eclipse TOAD Education Executive MBA IIM Kozhikode 2013 Bachelor of Engineering in Engineering National Institute of Technology Karnataka 2004 Skills Eclipse Java Jboss jquery Jsp Servlets Dynamodb Hdfs Impala Oozie Sqoop Hbase Kafka Cdh Db2 Hadoop Avro C Design patterns Hadoop Additional Information Technical Skills Hadoop Technologies Hadoop HDFS Hadoop MapReduce Hive HBase SQOOP Oozie AVRO PigLatin Hue CDH Parquet Impala Scala Spark Python KafkaAWSS3Trifacta DynamoDB EMR Apache Nifi No Sql HBase IDETools RAD Eclipse Web and Application Servers Web sphere JBOSS Tomcat Core Competency Technologies Java OOPS design patterns JSP servlets JDBC java 5 java 6 java 7 C C shell scripting Spark SAS EG Scala Spark Streaming Kafka Web presentation frameworks Java Script HTML AJAX jQuery CSS JSON Testing Issue Log tools JUnit 4 Bugzilla HP Quality Centre SCMVersion control tools PVCS CVS Sub Version Modeling tools Visio 2007 Build and continuous Integration Maven ANT Data base Oracle 8i9i10g DB2 MySQL 4x5x OS UNIX LINUX Windows Aix",
    "unique_id": "b0d4c702-227e-482f-bb2b-761b85594d62"
}