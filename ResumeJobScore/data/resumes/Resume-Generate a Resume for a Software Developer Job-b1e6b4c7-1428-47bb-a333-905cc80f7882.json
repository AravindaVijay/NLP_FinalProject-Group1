{
    "clean_data": "SparkKafka Developer SparkKafka span lDeveloperspan SparkKafka Developer Citizens Property Insurance Corporation Florida City FL 10 years of experience in IT which includes experience in Bigdata Technologies Hadoop ecosystem Data Warehousing SQL related technologies in Retail Manufacturing Financial and Communication sectors 5 Years of experience in Big Data Analytics using Various Hadoop ecosystems tools and Spark Framework and Currently working on Spark and Spark Streaming frameworks extensively using Scala as the main programming dialect Experience installingconfiguringmaintaining Apache Hadoop clusters for application development and Hadoop tools like Sqoop Hive PIG Flume HBase Kafka Hue Storm Zoo Keeper Oozie Cassandra Sqoop Python Worked with major distributions like Cloudera CDH 34 Horton works Distributions and AWS Also worked on Unix and DWH in support for various Distributions Hands on experience in developing and deploying enterprisebased applications using major components in Hadoop ecosystem like Hadoop 2X YARN Hive Pig MapReduce Spark Kafka Storm Oozie HBase Flume Sqoop and Zookeeper Experience in handling large datasets using Partitions Spark in memory capabilities Broadcasts in Spark with Scala and python Effective and efficient Joins Transformations and other during ingestion process itself Experience in developing data pipeline using Pig Sqoop and Flume to extract the data from weblogs and store in HDFS and accomplished developing Pig Latin Scripts and using HiveQL for data analytics Extensively dealt with Spark Streaming and Apache Kafka to fetch live stream data Experience in converting HiveSQL queries into Spark transformations using Java and experience in ETL development using Kafka Flume and Sqoop Good experience in writing Spark applications using Scala and Java and used Scala set to develop Scala projects and executed using SparkSubmit Experience working on NoSQL databases including HBase Cassandra and MongoDB and experience using Sqoop to import data into HDFS from RDBMS and viceversa Developed Spark scripts by using Scala shell commands as per the requirement Good experience in writing Sqoop queries for transferring bulk data between Apache Hadoop and structured data stores Substantial experience in writing Map Reduce jobs in Java PIG Flume Zookeeper Hive and Storm Created multiple Map Reduce Jobs using Java API Pig and Hive for data extraction Strong expertise in troubleshooting and performance finetuning Spark Map Reduce and Hive applications Good experience on working with Amazon EMR framework for processing data on EMR and EC2 instances Created AWS VPC network for the installed Instances and configured security groups and Elastic IPs Accordingly Developed AWS Cloud formation templates to create custom sized VPC subnets EC2 instances ELB and security groups Extensive experience in developing applications that perform Data Processing tasks using Teradata Oracle SQL Server and MySQL database Worked on data warehousing and ETL tools like Informatica Tableau and Pentaho Experience in understanding the security requirements for Hadoop and integrate with Kerberos authentication and authorization infrastructure Acquaintance with Agile and Waterfall methodologies Responsible for handling several clients facing meetings with great communication skills Work Experience SparkKafka Developer Citizens Property Insurance Corporation February 2018 to Present Responsibilities Involved in complete Big Data flow of the application starting from data ingestion from upstream to HDFS processing and analyzing the data in HDFS Developed Spark API to import data into HDFS from Teradata and created Hive tables Developed Sqoop jobs to import data in Avro file format from Oracle database and created hive tables on top of it Created Partitioned and Bucketed Hive tables in Parquet File Formats with Snappy compression and then loaded data into Parquet hive tables from Avro hive tables Involved in running all the hive scripts through hive Impala Hive on Spark and some through Spark SQL Development and Review of spark code containing Airflow DAGs Databricks Notebooks Delta Tables in DDLs and Metadata SQLs other SQL scripts Deploying the Code to Dev QA PreProd and Prod Environments by adhering to GIT process flow and following the standards mentioned by the release management process Creating Technical Design Documentation and SupportOPS Turnover documentation by following the OPS checklist Raising Change Request once the code is PreProd Airflow Orchestration especially configuring the DAG start date and scheduled time and other parameters Worked on mainly developing Pyspark code in Databricks code using existing load patternsFull Incremental and Backfill for forecastingRegion and Country rawCustomerSales and pubCustomerSales Wrote Spark Dataframes that uses mainly CSV files Parquet Delta file formats Used Spark SQL Joins views partitioning extensively Validating the source data and generating the output data in the required format using Pyspark transformations Submitting Jobs for cluster administered by other Linux teams Environment Used Databricks Azure Data Lake storageGen1 Oracle EDW PySpark mainly Spark SQL Scala Spark occasionally Jenkins PyCharm Git Spark BDA server Putty for Tunneling into Airflow environments etc Involved in performance tuning of Hive from design storage and query perspectives Developed Flume ETL job for handling data from HTTP Source and Sink as HDFS Collected the Json data from HTTP Source and developed Spark APIs that helps to do inserts and updates in Hive tables Developed Spark scripts to import large files from Amazon S3 buckets Developed Spark core and Spark SQL scripts using Scala for faster data processing Developed Kafka consumers API in Scala for consuming data from Kafka topics Involved in designing and developing tables in HBase and storing aggregated data from Hive Table Integrated Hive and Tableau Desktop reports and published to Tableau Server Developed shell scripts for running Hive scripts in Hive and Impala Orchestrated number of Sqoop and Hive scripts using Oozie workflow and scheduled using Oozie coordinator Used Jira for bug tracking and Bit Bucket to checkin and checkout code changes Continuous monitoring and managing the Hadoop cluster through Cloudera Manager Environment HDFS Yarn Map Reduce Hive Sqoop Flume Oozie HBase Kafka Impala Spark SQL Spark Streaming Eclipse Oracle Teradata PLSQL UNIX Shell Scripting Cloudera Environment HDFS Yarn Map Reduce Hive Sqoop Flume Oozie HBase Kafka Impala SparkSQL Spark Streaming Eclipse Oracle Teradata PLSQL UNIX Shell Scripting Cloudera Hadoop Java Developer Experian Chicago IL March 2016 to January 2018 Responsibilities Responsible for architecting Hadoop clusters with CDH3 and involved in installation of CDH3 and up gradation to CDH4 from CDH3 Worked on creating Key space in Cassandra for saving the Spark Batch output Worked on Spark application to compact the small files present into hive ecosystem to make it equivalent to block size of HDFS Manage migration of onperm servers to AWS by creating golden images for upload and deployment Manage multiple AWS accounts with multiple VPCs for both production and nonproduction where primary objectives are automation build out integration and cost control Implemented the real time streaming ingestion using Kafka and Spark Streaming Loaded data using Sparkstreaming with Scala and Python Expertise in converting Map Reduce programs into Spark transformations using Spark RDDs Expertise in Spark Architecture including Spark Core Spark SQL Data Frames Spark Streaming and Spark MLlib Configured Spark streaming to receive real time data from the Kafka and store the stream data to HDFS using Scala Experience in implementing RealTime event processing and analytics using messaging systems like Spark Streaming Experience in using Kafka and Kafka brokers to initiate spark context and processing live streaming information with the help of RDD Good knowledge on Amazon AWS concepts like EMR and EC2 web services which provides fast and efficient processing of Big Data Experience with all flavor of Hadoop distributions including Cloudera Hortonworks Mapr and Apache Experience in installation configuration supporting and managing Hadoop Clusters using Apache Cloudera 5X distributions and on Amazon web services AWS Expertise in implementing SparkScala application using higher order functions for both batch and interactive analysis requirement Extensive experienced working with Spark tools like RDD transformations spark MLlib and spark QL Hands on experience in writing Hadoop Jobs for analyzing data using Hive QL Queries Pig Latin Data flow language and custom MapReduce programs in Java Experienced in working with structured data using HiveQL join operations Hive UDFs partitions bucketing and internalexternal tables Extensive experience in collecting and storing stream data like log data in HDFS using Apache Flume Experienced in using Pig scripts to do transformations event joins filters and some preaggregations before storing the data onto HDFS Involvement in creating custom UDFs for Pig and Hive to consolidate strategies and usefulness of Python Java into Pig Latin and HQL HiveQL Involved in requirement and design phase to implement Streaming Lambda Architecture to use real time streaming using Spark and Kafka and Scala Experience in loading the data into Spark RDD and performing inmemory data computation to generate the output responses Migrated complex map reduce programs into Inmemory Spark processing using Transformations and actions Developed full text search platform using NoSQL and Logstash Elastic Search engine allowing for much faster more scalable and more intuitive user searches Developed the Sqoop scripts to make the interaction between Pig and MySQL Database Worked on Performance Enhancement in Pig Hive and HBase on multiple nodes Worked with Distributed ntier architecture and ClientServer architecture Supported Map Reduce Programs those are running on the cluster and developed multiple Map Reduce jobs in Java for data cleaning and preprocessing Developed MapReduce application using Hadoop MapReduce programming and HBase Evaluated usage of Oozie for Workflow Orchestration and experienced in cluster coordination using Zookeeper Developing ETL jobs with organization and project defined standards and processes Experienced in enabling Kerberos authentication in ETL process Implemented data access using Hibernate persistence framework Design of GUI using Model View Controller Architecture STRUTS Framework Integrated Spring DAO for data access using Hibernate and involved in the Development of Spring Framework Controllers Environment Hadoop 2X HDFS MapReduce Hive Pig Sqoop Oozie HBase Java J2EE Eclipse HQL Hadoop Developer BMW Woodcliff Lake October 2013 to February 2016 Responsibilities Involved in the Complete Software development life cycle SDLC to develop the application Worked on analyzing Hadoop cluster using different big data analytic tools including Pig Hive and Map Reduce on EC2 Worked with the Data Science team to gather requirements for various data mining projects Worked with different source data file formats like JSON CSV and TSV etc Experience in importing data from various data sources like MySQL and Netezza using Sqoop SFTP performed transformations using Hive Pig and loaded data back into HDFS Performed transformations cleaning and filtering on imported data using Hive Map Reduce Import and export data between the environments like MySQL HDFS and deploying into productions Used Pig as ETL tool to do transformations event joins and some preaggregations before storing the data onto HDFS Worked on partitioning and used bucketing in HIVE tables and setting tuning parameters to improve the performance Involved in developing Impala scripts to do Adhoc queries Experience in Oozie workflow scheduler template to manage various jobs like Sqoop MR Pig Hive Shell scripts etc Involved in importing and exporting data from HBase using Spark Involved in POC for migrating ETLS from Hive to Spark in Spark on Yarn Environment Actively participating in the code reviews meetings and solving any technical issues Environment Apache Hadoop AWS EMR EC2 S3 Horton works Map Reduce Hive Pig Sqoop Apache Spark Zookeeper HBase Java Oozie Oracle MySQL Netezza and UNIX Shell Scripting Hadoop Developer WNS Global Services Pune Maharashtra June 2011 to September 2013 Responsibilities Installed and configured Hadoop MapReduce HDFS Developed multiple MapReduce jobs in java for data cleaning and preprocessing Installed and configured Apache Hadoop to test the maintenance of log files in Hadoop cluster Importing and exporting data into HDFS and Hive using Sqoop Experienced in defining job flows and managing and reviewing Hadoop log files Load and transform large sets of structured semi structured and unstructured data Responsible to manage data coming from different sources and for implementing MongoDB to store and analyze unstructured data Supported Map Reduce Programs those are running on the cluster and involved in loading data from UNIX file system to HDFS Installed and configured Hive and written Hive UDFs Involved in creating Hive tables loading with data and writing hive queries that will run internally in map reduce way Involved in Hadoop cluster task like Adding and Removing Nodes without any effect to running jobs and data Created HBase tables to store variable data formats of PII data coming from different portfolios Implemented best income logic using Pig scripts Load and transform large sets of structured semi structured and unstructured data Cluster coordination services through Zookeeper Exported the analyzed data to the relational databases using Sqoop for visualization and to generate reports for the BI team Supported in setting up QA environment and updating configurations for implementing scripts with Pig and Sqoop Continuous monitoring and managing the Hadoop cluster using Cloudera Manager Used Hibernate ORM framework with Spring framework for data persistence and transaction management and involved in templates and screens in HTML and JavaScript Environment Hadoop HDFS MapReduce Pig Sqoop UNIX HBase Java JavaScript HTML SQL Java Developer ILogix IT Services Pvt Ltd Hyderabad Telangana July 2009 to May 2011 Responsibilities Importing the data from the MySQL and Oracle into the HDFS using Sqoop Implemented CDH3 Hadoop cluster on Centos Worked on installing clusters commissioning decommissioning of data node name node recovery capacity planning and slots configuration Developing parser and loader map reduce application to retrieve data from HDFS and store to HBase and Hive Importing the unstructured data into the HDFS using Flume Written Map Reduce java programs to analyze the log data for largescale data sets Involved in creating Hive tables loading and analyzing data using hive queries Involved in using HBase Java API on Java application Automated all the jobs for extracting the data from different Data Sources like MySQL to pushing the result set data to Hadoop Distributed File System Vodafone is a British Multinational Telecommunications Services Company headquartered in Swindon UK Initially it has started with an infrastructure company which supplies infrastructure and cables to other companies After that it has started Telecommunications Company So they want to communicate with different components Project Description In this project all the Network elements for activating the network service will be stored in InventoryMangementSystemIMSUsing the procedure IMS will allocate deallocate modify cease the Service based on order request send by other components and request will be in the form of xmlResponse also will be sent as xml to other components InventoryMangementSystem GUI is used to add any new Network elements and more features are added to display the details of network elementsInventoryManagementGUI is designed with MVC Spring Hibernate framework architerature All the code changes are deployed in Weblogic server Responsibilities I was working as a Team Member Developed Pig Latin scripts to extract the data from the output files to load into HDFS Responsible for managing data from multiple sources Created and maintained Technical documentation for launching HADOOP Clusters and for executing Hive queries and Pig Scripts Designed and built many applications to deal with vast amounts of data flowing through multiple Hadoop clusters using Javabased mapreduce Worked with application teams to install operating system Hadoop updates patches version upgrades as required Environment Hadoop 100 Map Reduce Hive HBase Flume Sqoop Pig Zookeeper Java ETL SQL Centos Skills Cassandra Hdfs Impala Mapreduce Oozie Sqoop Hbase Kafka Flume Hadoop Jboss Mongodb Nosql Teradata Visual studio Apache spark Application server Git Hadoop Hbase Additional Information Skills Big Data Technologies HDFS MapReduce Hive Pig Sqoop Flume Oozie Zookeeper Kafka Cassandra Apache Spark Spark Streaming HBase Flume Impala Hadoop Distribution Cloudera Horton Works Apache AWS Languages Java SQL PLSQL Python Pig Latin HiveQL Scala Regular Expressions Web Technologies HTML CSS JavaScript XML JSP Restful SOAP Operating Systems Windows XP7810 UNIX LINUX UBUNTU CENTOS PortalsApplication servers WebLogic WebSphere Application server WebSphere Portal server JBOSS Build Automation tools SBT Ant Maven Version Control GIT IDE Build Tools Design Eclipse Visual Studio Net Beans Rational Application Developer Junit Databases Oracle SQL Server MySQL MS Access NoSQL Database HBase Cassandra MongoDB Teradata",
    "entities": [
        "Zookeeper Experience",
        "MLlib",
        "Hadoop Clusters",
        "Yarn Environment Actively",
        "xmlResponse",
        "Distributions and AWS Also",
        "Cassandra",
        "BI",
        "HDFS",
        "UNIX",
        "SparkSubmit",
        "WebLogic WebSphere Application",
        "Data Sources",
        "Developed Spark",
        "UK",
        "Parquet File Formats",
        "Jenkins PyCharm",
        "Putty for Tunneling",
        "Present Responsibilities Involved",
        "node",
        "Sparkstreaming",
        "RDD",
        "Hadoop",
        "PreProd Airflow Orchestration",
        "Informatica Tableau",
        "Spark Streaming Experience",
        "Project Description",
        "HBase",
        "Model View Controller Architecture",
        "Created AWS VPC",
        "Avro",
        "SparkScala",
        "Amazon",
        "Developed Sqoop",
        "CDH3",
        "Impala Hadoop Distribution",
        "Spark Streaming Loaded",
        "SparkSQL",
        "Kerberos",
        "InventoryMangementSystem GUI",
        "Databricks Notebooks Delta Tables",
        "Hadoop MapReduce",
        "Complete Software",
        "Spark Framework",
        "JavaScript Environment Hadoop HDFS MapReduce Pig Sqoop",
        "Linux",
        "Creating Technical Design Documentation",
        "Spark Streaming",
        "HDFS Involvement",
        "Wrote Spark Dataframes",
        "HBase Evaluated",
        "Parquet",
        "Adhoc",
        "WebSphere Portal",
        "PortalsApplication",
        "Spark",
        "Agile",
        "Service",
        "Swindon",
        "GIT",
        "Amazon EMR",
        "CSV",
        "Hadoop Jobs",
        "API",
        "HTTP Source",
        "Sqoop",
        "QA",
        "HIVE",
        "Maven Version",
        "Created Partitioned",
        "Created",
        "AWS",
        "Spark Core Spark",
        "Oracle",
        "Spark MLlib Configured Spark",
        "Inmemory Spark",
        "Big Data Analytics",
        "Teradata Oracle SQL Server",
        "Distributions Hands",
        "ClientServer",
        "Spark Involved",
        "ELB",
        "CDH3 Hadoop",
        "MVC Spring Hibernate",
        "Databricks",
        "HDFS Responsible",
        "Instances",
        "CDH3 Worked",
        "HTML",
        "the Data Science",
        "java",
        "Build Tools Design",
        "Oozie",
        "SQL",
        "Spark RDD",
        "Oozie for Workflow Orchestration",
        "Hive Pig",
        "HADOOP Clusters",
        "Big Data",
        "Hive",
        "Spark SQL Development",
        "Amazon AWS",
        "DAG",
        "Partitions Spark",
        "CDH4",
        "the Development of Spring Framework Controllers Environment Hadoop 2X HDFS MapReduce Hive Pig",
        "Git Hadoop Hbase Additional Information Skills Big Data Technologies HDFS MapReduce Hive Pig",
        "Zookeeper Developing ETL",
        "ETL",
        "PII",
        "Apache Hadoop",
        "HDFS Developed Spark API",
        "Impala",
        "Spark SQL",
        "Developer ILogix IT Services Pvt Ltd Hyderabad",
        "Bigdata Technologies Hadoop",
        "Network",
        "Created HBase",
        "Developed MapReduce",
        "Weblogic",
        "ETLS",
        "Storm Created",
        "Retail Manufacturing Financial and Communication",
        "Backfill",
        "MapReduce",
        "Hive Table Integrated Hive",
        "Team",
        "NoSQL",
        "Tableau",
        "Spark Architecture",
        "Environment Apache Hadoop AWS",
        "Application",
        "Citizens Property Insurance Corporation",
        "Teradata",
        "DWH",
        "British Multinational Telecommunications Services Company",
        "Hadoop Distributed File System Vodafone",
        "Data Processing",
        "Hadoop MapReduce HDFS Developed"
    ],
    "experience": "Experience installingconfiguringmaintaining Apache Hadoop clusters for application development and Hadoop tools like Sqoop Hive PIG Flume HBase Kafka Hue Storm Zoo Keeper Oozie Cassandra Sqoop Python Worked with major distributions like Cloudera CDH 34 Horton works Distributions and AWS Also worked on Unix and DWH in support for various Distributions Hands on experience in developing and deploying enterprisebased applications using major components in Hadoop ecosystem like Hadoop 2X YARN Hive Pig MapReduce Spark Kafka Storm Oozie HBase Flume Sqoop and Zookeeper Experience in handling large datasets using Partitions Spark in memory capabilities Broadcasts in Spark with Scala and python Effective and efficient Joins Transformations and other during ingestion process itself Experience in developing data pipeline using Pig Sqoop and Flume to extract the data from weblogs and store in HDFS and accomplished developing Pig Latin Scripts and using HiveQL for data analytics Extensively dealt with Spark Streaming and Apache Kafka to fetch live stream data Experience in converting HiveSQL queries into Spark transformations using Java and experience in ETL development using Kafka Flume and Sqoop Good experience in writing Spark applications using Scala and Java and used Scala set to develop Scala projects and executed using SparkSubmit Experience working on NoSQL databases including HBase Cassandra and MongoDB and experience using Sqoop to import data into HDFS from RDBMS and viceversa Developed Spark scripts by using Scala shell commands as per the requirement Good experience in writing Sqoop queries for transferring bulk data between Apache Hadoop and structured data stores Substantial experience in writing Map Reduce jobs in Java PIG Flume Zookeeper Hive and Storm Created multiple Map Reduce Jobs using Java API Pig and Hive for data extraction Strong expertise in troubleshooting and performance finetuning Spark Map Reduce and Hive applications Good experience on working with Amazon EMR framework for processing data on EMR and EC2 instances Created AWS VPC network for the installed Instances and configured security groups and Elastic IPs Accordingly Developed AWS Cloud formation templates to create custom sized VPC subnets EC2 instances ELB and security groups Extensive experience in developing applications that perform Data Processing tasks using Teradata Oracle SQL Server and MySQL database Worked on data warehousing and ETL tools like Informatica Tableau and Pentaho Experience in understanding the security requirements for Hadoop and integrate with Kerberos authentication and authorization infrastructure Acquaintance with Agile and Waterfall methodologies Responsible for handling several clients facing meetings with great communication skills Work Experience SparkKafka Developer Citizens Property Insurance Corporation February 2018 to Present Responsibilities Involved in complete Big Data flow of the application starting from data ingestion from upstream to HDFS processing and analyzing the data in HDFS Developed Spark API to import data into HDFS from Teradata and created Hive tables Developed Sqoop jobs to import data in Avro file format from Oracle database and created hive tables on top of it Created Partitioned and Bucketed Hive tables in Parquet File Formats with Snappy compression and then loaded data into Parquet hive tables from Avro hive tables Involved in running all the hive scripts through hive Impala Hive on Spark and some through Spark SQL Development and Review of spark code containing Airflow DAGs Databricks Notebooks Delta Tables in DDLs and Metadata SQLs other SQL scripts Deploying the Code to Dev QA PreProd and Prod Environments by adhering to GIT process flow and following the standards mentioned by the release management process Creating Technical Design Documentation and SupportOPS Turnover documentation by following the OPS checklist Raising Change Request once the code is PreProd Airflow Orchestration especially configuring the DAG start date and scheduled time and other parameters Worked on mainly developing Pyspark code in Databricks code using existing load patternsFull Incremental and Backfill for forecastingRegion and Country rawCustomerSales and pubCustomerSales Wrote Spark Dataframes that uses mainly CSV files Parquet Delta file formats Used Spark SQL Joins views partitioning extensively Validating the source data and generating the output data in the required format using Pyspark transformations Submitting Jobs for cluster administered by other Linux teams Environment Used Databricks Azure Data Lake storageGen1 Oracle EDW PySpark mainly Spark SQL Scala Spark occasionally Jenkins PyCharm Git Spark BDA server Putty for Tunneling into Airflow environments etc Involved in performance tuning of Hive from design storage and query perspectives Developed Flume ETL job for handling data from HTTP Source and Sink as HDFS Collected the Json data from HTTP Source and developed Spark APIs that helps to do inserts and updates in Hive tables Developed Spark scripts to import large files from Amazon S3 buckets Developed Spark core and Spark SQL scripts using Scala for faster data processing Developed Kafka consumers API in Scala for consuming data from Kafka topics Involved in designing and developing tables in HBase and storing aggregated data from Hive Table Integrated Hive and Tableau Desktop reports and published to Tableau Server Developed shell scripts for running Hive scripts in Hive and Impala Orchestrated number of Sqoop and Hive scripts using Oozie workflow and scheduled using Oozie coordinator Used Jira for bug tracking and Bit Bucket to checkin and checkout code changes Continuous monitoring and managing the Hadoop cluster through Cloudera Manager Environment HDFS Yarn Map Reduce Hive Sqoop Flume Oozie HBase Kafka Impala Spark SQL Spark Streaming Eclipse Oracle Teradata PLSQL UNIX Shell Scripting Cloudera Environment HDFS Yarn Map Reduce Hive Sqoop Flume Oozie HBase Kafka Impala SparkSQL Spark Streaming Eclipse Oracle Teradata PLSQL UNIX Shell Scripting Cloudera Hadoop Java Developer Experian Chicago IL March 2016 to January 2018 Responsibilities Responsible for architecting Hadoop clusters with CDH3 and involved in installation of CDH3 and up gradation to CDH4 from CDH3 Worked on creating Key space in Cassandra for saving the Spark Batch output Worked on Spark application to compact the small files present into hive ecosystem to make it equivalent to block size of HDFS Manage migration of onperm servers to AWS by creating golden images for upload and deployment Manage multiple AWS accounts with multiple VPCs for both production and nonproduction where primary objectives are automation build out integration and cost control Implemented the real time streaming ingestion using Kafka and Spark Streaming Loaded data using Sparkstreaming with Scala and Python Expertise in converting Map Reduce programs into Spark transformations using Spark RDDs Expertise in Spark Architecture including Spark Core Spark SQL Data Frames Spark Streaming and Spark MLlib Configured Spark streaming to receive real time data from the Kafka and store the stream data to HDFS using Scala Experience in implementing RealTime event processing and analytics using messaging systems like Spark Streaming Experience in using Kafka and Kafka brokers to initiate spark context and processing live streaming information with the help of RDD Good knowledge on Amazon AWS concepts like EMR and EC2 web services which provides fast and efficient processing of Big Data Experience with all flavor of Hadoop distributions including Cloudera Hortonworks Mapr and Apache Experience in installation configuration supporting and managing Hadoop Clusters using Apache Cloudera 5X distributions and on Amazon web services AWS Expertise in implementing SparkScala application using higher order functions for both batch and interactive analysis requirement Extensive experienced working with Spark tools like RDD transformations spark MLlib and spark QL Hands on experience in writing Hadoop Jobs for analyzing data using Hive QL Queries Pig Latin Data flow language and custom MapReduce programs in Java Experienced in working with structured data using HiveQL join operations Hive UDFs partitions bucketing and internalexternal tables Extensive experience in collecting and storing stream data like log data in HDFS using Apache Flume Experienced in using Pig scripts to do transformations event joins filters and some preaggregations before storing the data onto HDFS Involvement in creating custom UDFs for Pig and Hive to consolidate strategies and usefulness of Python Java into Pig Latin and HQL HiveQL Involved in requirement and design phase to implement Streaming Lambda Architecture to use real time streaming using Spark and Kafka and Scala Experience in loading the data into Spark RDD and performing inmemory data computation to generate the output responses Migrated complex map reduce programs into Inmemory Spark processing using Transformations and actions Developed full text search platform using NoSQL and Logstash Elastic Search engine allowing for much faster more scalable and more intuitive user searches Developed the Sqoop scripts to make the interaction between Pig and MySQL Database Worked on Performance Enhancement in Pig Hive and HBase on multiple nodes Worked with Distributed ntier architecture and ClientServer architecture Supported Map Reduce Programs those are running on the cluster and developed multiple Map Reduce jobs in Java for data cleaning and preprocessing Developed MapReduce application using Hadoop MapReduce programming and HBase Evaluated usage of Oozie for Workflow Orchestration and experienced in cluster coordination using Zookeeper Developing ETL jobs with organization and project defined standards and processes Experienced in enabling Kerberos authentication in ETL process Implemented data access using Hibernate persistence framework Design of GUI using Model View Controller Architecture STRUTS Framework Integrated Spring DAO for data access using Hibernate and involved in the Development of Spring Framework Controllers Environment Hadoop 2X HDFS MapReduce Hive Pig Sqoop Oozie HBase Java J2EE Eclipse HQL Hadoop Developer BMW Woodcliff Lake October 2013 to February 2016 Responsibilities Involved in the Complete Software development life cycle SDLC to develop the application Worked on analyzing Hadoop cluster using different big data analytic tools including Pig Hive and Map Reduce on EC2 Worked with the Data Science team to gather requirements for various data mining projects Worked with different source data file formats like JSON CSV and TSV etc Experience in importing data from various data sources like MySQL and Netezza using Sqoop SFTP performed transformations using Hive Pig and loaded data back into HDFS Performed transformations cleaning and filtering on imported data using Hive Map Reduce Import and export data between the environments like MySQL HDFS and deploying into productions Used Pig as ETL tool to do transformations event joins and some preaggregations before storing the data onto HDFS Worked on partitioning and used bucketing in HIVE tables and setting tuning parameters to improve the performance Involved in developing Impala scripts to do Adhoc queries Experience in Oozie workflow scheduler template to manage various jobs like Sqoop MR Pig Hive Shell scripts etc Involved in importing and exporting data from HBase using Spark Involved in POC for migrating ETLS from Hive to Spark in Spark on Yarn Environment Actively participating in the code reviews meetings and solving any technical issues Environment Apache Hadoop AWS EMR EC2 S3 Horton works Map Reduce Hive Pig Sqoop Apache Spark Zookeeper HBase Java Oozie Oracle MySQL Netezza and UNIX Shell Scripting Hadoop Developer WNS Global Services Pune Maharashtra June 2011 to September 2013 Responsibilities Installed and configured Hadoop MapReduce HDFS Developed multiple MapReduce jobs in java for data cleaning and preprocessing Installed and configured Apache Hadoop to test the maintenance of log files in Hadoop cluster Importing and exporting data into HDFS and Hive using Sqoop Experienced in defining job flows and managing and reviewing Hadoop log files Load and transform large sets of structured semi structured and unstructured data Responsible to manage data coming from different sources and for implementing MongoDB to store and analyze unstructured data Supported Map Reduce Programs those are running on the cluster and involved in loading data from UNIX file system to HDFS Installed and configured Hive and written Hive UDFs Involved in creating Hive tables loading with data and writing hive queries that will run internally in map reduce way Involved in Hadoop cluster task like Adding and Removing Nodes without any effect to running jobs and data Created HBase tables to store variable data formats of PII data coming from different portfolios Implemented best income logic using Pig scripts Load and transform large sets of structured semi structured and unstructured data Cluster coordination services through Zookeeper Exported the analyzed data to the relational databases using Sqoop for visualization and to generate reports for the BI team Supported in setting up QA environment and updating configurations for implementing scripts with Pig and Sqoop Continuous monitoring and managing the Hadoop cluster using Cloudera Manager Used Hibernate ORM framework with Spring framework for data persistence and transaction management and involved in templates and screens in HTML and JavaScript Environment Hadoop HDFS MapReduce Pig Sqoop UNIX HBase Java JavaScript HTML SQL Java Developer ILogix IT Services Pvt Ltd Hyderabad Telangana July 2009 to May 2011 Responsibilities Importing the data from the MySQL and Oracle into the HDFS using Sqoop Implemented CDH3 Hadoop cluster on Centos Worked on installing clusters commissioning decommissioning of data node name node recovery capacity planning and slots configuration Developing parser and loader map reduce application to retrieve data from HDFS and store to HBase and Hive Importing the unstructured data into the HDFS using Flume Written Map Reduce java programs to analyze the log data for largescale data sets Involved in creating Hive tables loading and analyzing data using hive queries Involved in using HBase Java API on Java application Automated all the jobs for extracting the data from different Data Sources like MySQL to pushing the result set data to Hadoop Distributed File System Vodafone is a British Multinational Telecommunications Services Company headquartered in Swindon UK Initially it has started with an infrastructure company which supplies infrastructure and cables to other companies After that it has started Telecommunications Company So they want to communicate with different components Project Description In this project all the Network elements for activating the network service will be stored in InventoryMangementSystemIMSUsing the procedure IMS will allocate deallocate modify cease the Service based on order request send by other components and request will be in the form of xmlResponse also will be sent as xml to other components InventoryMangementSystem GUI is used to add any new Network elements and more features are added to display the details of network elementsInventoryManagementGUI is designed with MVC Spring Hibernate framework architerature All the code changes are deployed in Weblogic server Responsibilities I was working as a Team Member Developed Pig Latin scripts to extract the data from the output files to load into HDFS Responsible for managing data from multiple sources Created and maintained Technical documentation for launching HADOOP Clusters and for executing Hive queries and Pig Scripts Designed and built many applications to deal with vast amounts of data flowing through multiple Hadoop clusters using Javabased mapreduce Worked with application teams to install operating system Hadoop updates patches version upgrades as required Environment Hadoop 100 Map Reduce Hive HBase Flume Sqoop Pig Zookeeper Java ETL SQL Centos Skills Cassandra Hdfs Impala Mapreduce Oozie Sqoop Hbase Kafka Flume Hadoop Jboss Mongodb Nosql Teradata Visual studio Apache spark Application server Git Hadoop Hbase Additional Information Skills Big Data Technologies HDFS MapReduce Hive Pig Sqoop Flume Oozie Zookeeper Kafka Cassandra Apache Spark Spark Streaming HBase Flume Impala Hadoop Distribution Cloudera Horton Works Apache AWS Languages Java SQL PLSQL Python Pig Latin HiveQL Scala Regular Expressions Web Technologies HTML CSS JavaScript XML JSP Restful SOAP Operating Systems Windows XP7810 UNIX LINUX UBUNTU CENTOS PortalsApplication servers WebLogic WebSphere Application server WebSphere Portal server JBOSS Build Automation tools SBT Ant Maven Version Control GIT IDE Build Tools Design Eclipse Visual Studio Net Beans Rational Application Developer Junit Databases Oracle SQL Server MySQL MS Access NoSQL Database HBase Cassandra MongoDB Teradata",
    "extracted_keywords": [
        "SparkKafka",
        "Developer",
        "SparkKafka",
        "span",
        "lDeveloperspan",
        "SparkKafka",
        "Developer",
        "Citizens",
        "Property",
        "Insurance",
        "Corporation",
        "Florida",
        "City",
        "FL",
        "years",
        "experience",
        "IT",
        "experience",
        "Bigdata",
        "Technologies",
        "Hadoop",
        "ecosystem",
        "Data",
        "Warehousing",
        "SQL",
        "technologies",
        "Retail",
        "Manufacturing",
        "Financial",
        "Communication",
        "Years",
        "experience",
        "Big",
        "Data",
        "Analytics",
        "Various",
        "Hadoop",
        "tools",
        "Spark",
        "Framework",
        "Spark",
        "Spark",
        "Streaming",
        "frameworks",
        "Scala",
        "programming",
        "dialect",
        "Experience",
        "Apache",
        "Hadoop",
        "clusters",
        "application",
        "development",
        "Hadoop",
        "tools",
        "Sqoop",
        "Hive",
        "PIG",
        "Flume",
        "HBase",
        "Kafka",
        "Hue",
        "Storm",
        "Zoo",
        "Keeper",
        "Oozie",
        "Cassandra",
        "Sqoop",
        "Python",
        "distributions",
        "Cloudera",
        "CDH",
        "Horton",
        "Distributions",
        "AWS",
        "Unix",
        "DWH",
        "support",
        "Distributions",
        "Hands",
        "experience",
        "applications",
        "components",
        "Hadoop",
        "ecosystem",
        "Hadoop",
        "2X",
        "YARN",
        "Hive",
        "Pig",
        "MapReduce",
        "Spark",
        "Kafka",
        "Storm",
        "Oozie",
        "HBase",
        "Flume",
        "Sqoop",
        "Zookeeper",
        "Experience",
        "datasets",
        "Partitions",
        "Spark",
        "memory",
        "capabilities",
        "Broadcasts",
        "Spark",
        "Scala",
        "python",
        "Joins",
        "Transformations",
        "ingestion",
        "process",
        "Experience",
        "data",
        "pipeline",
        "Pig",
        "Sqoop",
        "Flume",
        "data",
        "weblogs",
        "HDFS",
        "Pig",
        "Latin",
        "Scripts",
        "HiveQL",
        "data",
        "analytics",
        "Spark",
        "Streaming",
        "Apache",
        "Kafka",
        "stream",
        "data",
        "Experience",
        "HiveSQL",
        "queries",
        "Spark",
        "transformations",
        "Java",
        "experience",
        "ETL",
        "development",
        "Kafka",
        "Flume",
        "Sqoop",
        "Good",
        "experience",
        "Spark",
        "applications",
        "Scala",
        "Java",
        "Scala",
        "Scala",
        "projects",
        "SparkSubmit",
        "Experience",
        "NoSQL",
        "databases",
        "HBase",
        "Cassandra",
        "MongoDB",
        "experience",
        "Sqoop",
        "data",
        "HDFS",
        "RDBMS",
        "viceversa",
        "Developed",
        "Spark",
        "scripts",
        "Scala",
        "shell",
        "commands",
        "requirement",
        "experience",
        "Sqoop",
        "data",
        "Apache",
        "Hadoop",
        "data",
        "stores",
        "experience",
        "Map",
        "Reduce",
        "jobs",
        "Java",
        "PIG",
        "Flume",
        "Zookeeper",
        "Hive",
        "Storm",
        "Map",
        "Reduce",
        "Jobs",
        "Java",
        "API",
        "Pig",
        "Hive",
        "data",
        "extraction",
        "expertise",
        "troubleshooting",
        "performance",
        "Spark",
        "Map",
        "Reduce",
        "Hive",
        "applications",
        "experience",
        "Amazon",
        "EMR",
        "framework",
        "data",
        "EMR",
        "EC2",
        "instances",
        "AWS",
        "network",
        "Instances",
        "security",
        "groups",
        "IPs",
        "AWS",
        "Cloud",
        "formation",
        "templates",
        "custom",
        "VPC",
        "subnets",
        "EC2",
        "ELB",
        "security",
        "groups",
        "experience",
        "applications",
        "Data",
        "Processing",
        "tasks",
        "Teradata",
        "Oracle",
        "SQL",
        "Server",
        "MySQL",
        "database",
        "data",
        "warehousing",
        "ETL",
        "tools",
        "Informatica",
        "Tableau",
        "Pentaho",
        "Experience",
        "security",
        "requirements",
        "Hadoop",
        "authentication",
        "authorization",
        "infrastructure",
        "Acquaintance",
        "Agile",
        "Waterfall",
        "methodologies",
        "clients",
        "meetings",
        "communication",
        "skills",
        "Work",
        "Experience",
        "SparkKafka",
        "Developer",
        "Citizens",
        "Property",
        "Insurance",
        "Corporation",
        "February",
        "Present",
        "Responsibilities",
        "Big",
        "Data",
        "flow",
        "application",
        "data",
        "ingestion",
        "HDFS",
        "processing",
        "data",
        "HDFS",
        "Developed",
        "Spark",
        "API",
        "data",
        "HDFS",
        "Teradata",
        "Hive",
        "tables",
        "Developed",
        "Sqoop",
        "jobs",
        "data",
        "Avro",
        "file",
        "format",
        "Oracle",
        "database",
        "tables",
        "top",
        "Bucketed",
        "Hive",
        "tables",
        "Parquet",
        "File",
        "Formats",
        "compression",
        "data",
        "Parquet",
        "hive",
        "tables",
        "Avro",
        "hive",
        "tables",
        "hive",
        "scripts",
        "Impala",
        "Hive",
        "Spark",
        "Spark",
        "SQL",
        "Development",
        "Review",
        "spark",
        "code",
        "Airflow",
        "DAGs",
        "Databricks",
        "Notebooks",
        "Delta",
        "Tables",
        "DDLs",
        "Metadata",
        "SQLs",
        "SQL",
        "scripts",
        "Code",
        "Dev",
        "QA",
        "PreProd",
        "Prod",
        "Environments",
        "GIT",
        "process",
        "flow",
        "standards",
        "release",
        "management",
        "process",
        "Technical",
        "Design",
        "Documentation",
        "SupportOPS",
        "Turnover",
        "documentation",
        "OPS",
        "checklist",
        "Change",
        "Request",
        "code",
        "PreProd",
        "Airflow",
        "Orchestration",
        "DAG",
        "date",
        "time",
        "parameters",
        "Pyspark",
        "code",
        "Databricks",
        "code",
        "load",
        "Incremental",
        "Backfill",
        "forecastingRegion",
        "Country",
        "rawCustomerSales",
        "pubCustomerSales",
        "Wrote",
        "Spark",
        "Dataframes",
        "CSV",
        "files",
        "Parquet",
        "Delta",
        "file",
        "formats",
        "Spark",
        "SQL",
        "Joins",
        "views",
        "source",
        "data",
        "output",
        "data",
        "format",
        "Pyspark",
        "transformations",
        "Jobs",
        "cluster",
        "Linux",
        "teams",
        "Environment",
        "Databricks",
        "Azure",
        "Data",
        "Lake",
        "storageGen1",
        "Oracle",
        "EDW",
        "PySpark",
        "Spark",
        "SQL",
        "Scala",
        "Spark",
        "Jenkins",
        "PyCharm",
        "Git",
        "Spark",
        "BDA",
        "server",
        "Putty",
        "Tunneling",
        "Airflow",
        "environments",
        "performance",
        "tuning",
        "Hive",
        "design",
        "storage",
        "query",
        "perspectives",
        "Developed",
        "Flume",
        "ETL",
        "job",
        "data",
        "HTTP",
        "Source",
        "Sink",
        "HDFS",
        "Json",
        "data",
        "HTTP",
        "Source",
        "Spark",
        "APIs",
        "inserts",
        "updates",
        "Hive",
        "tables",
        "Developed",
        "Spark",
        "scripts",
        "files",
        "Amazon",
        "S3",
        "Spark",
        "core",
        "Spark",
        "SQL",
        "scripts",
        "Scala",
        "data",
        "Kafka",
        "consumers",
        "API",
        "Scala",
        "data",
        "Kafka",
        "topics",
        "tables",
        "HBase",
        "data",
        "Hive",
        "Table",
        "Integrated",
        "Hive",
        "Tableau",
        "Desktop",
        "reports",
        "Tableau",
        "Server",
        "shell",
        "scripts",
        "Hive",
        "scripts",
        "Hive",
        "Impala",
        "Orchestrated",
        "number",
        "Sqoop",
        "Hive",
        "scripts",
        "Oozie",
        "workflow",
        "Oozie",
        "coordinator",
        "Jira",
        "bug",
        "tracking",
        "Bit",
        "Bucket",
        "checkin",
        "checkout",
        "code",
        "monitoring",
        "Hadoop",
        "cluster",
        "Cloudera",
        "Manager",
        "Environment",
        "HDFS",
        "Yarn",
        "Map",
        "Reduce",
        "Hive",
        "Sqoop",
        "Flume",
        "Oozie",
        "HBase",
        "Kafka",
        "Impala",
        "Spark",
        "SQL",
        "Spark",
        "Streaming",
        "Eclipse",
        "Oracle",
        "Teradata",
        "PLSQL",
        "UNIX",
        "Shell",
        "Scripting",
        "Cloudera",
        "Environment",
        "HDFS",
        "Yarn",
        "Map",
        "Reduce",
        "Hive",
        "Sqoop",
        "Flume",
        "Oozie",
        "HBase",
        "Kafka",
        "Impala",
        "SparkSQL",
        "Spark",
        "Streaming",
        "Eclipse",
        "Oracle",
        "Teradata",
        "PLSQL",
        "UNIX",
        "Shell",
        "Scripting",
        "Cloudera",
        "Hadoop",
        "Java",
        "Developer",
        "Experian",
        "Chicago",
        "IL",
        "March",
        "January",
        "Responsibilities",
        "Hadoop",
        "clusters",
        "CDH3",
        "installation",
        "CDH3",
        "gradation",
        "CDH4",
        "CDH3",
        "space",
        "Cassandra",
        "Spark",
        "Batch",
        "output",
        "Spark",
        "application",
        "files",
        "hive",
        "ecosystem",
        "size",
        "HDFS",
        "Manage",
        "migration",
        "onperm",
        "servers",
        "AWS",
        "images",
        "upload",
        "deployment",
        "Manage",
        "AWS",
        "VPCs",
        "production",
        "nonproduction",
        "objectives",
        "automation",
        "integration",
        "cost",
        "control",
        "time",
        "ingestion",
        "Kafka",
        "Spark",
        "Streaming",
        "data",
        "Scala",
        "Python",
        "Expertise",
        "Map",
        "Reduce",
        "programs",
        "Spark",
        "transformations",
        "Spark",
        "RDDs",
        "Expertise",
        "Spark",
        "Architecture",
        "Spark",
        "Core",
        "Spark",
        "SQL",
        "Data",
        "Frames",
        "Spark",
        "Streaming",
        "Spark",
        "MLlib",
        "Configured",
        "Spark",
        "streaming",
        "time",
        "data",
        "Kafka",
        "stream",
        "data",
        "HDFS",
        "Scala",
        "Experience",
        "RealTime",
        "event",
        "processing",
        "analytics",
        "systems",
        "Spark",
        "Streaming",
        "Experience",
        "Kafka",
        "Kafka",
        "brokers",
        "spark",
        "context",
        "information",
        "help",
        "knowledge",
        "Amazon",
        "AWS",
        "concepts",
        "EMR",
        "EC2",
        "web",
        "services",
        "processing",
        "Big",
        "Data",
        "Experience",
        "flavor",
        "Hadoop",
        "distributions",
        "Cloudera",
        "Hortonworks",
        "Mapr",
        "Apache",
        "Experience",
        "installation",
        "configuration",
        "Hadoop",
        "Clusters",
        "Apache",
        "Cloudera",
        "5X",
        "distributions",
        "Amazon",
        "web",
        "services",
        "AWS",
        "Expertise",
        "SparkScala",
        "application",
        "order",
        "functions",
        "batch",
        "analysis",
        "requirement",
        "Extensive",
        "Spark",
        "tools",
        "RDD",
        "transformations",
        "MLlib",
        "QL",
        "Hands",
        "experience",
        "Hadoop",
        "Jobs",
        "data",
        "Hive",
        "QL",
        "Queries",
        "Pig",
        "Latin",
        "Data",
        "flow",
        "language",
        "custom",
        "MapReduce",
        "programs",
        "Java",
        "data",
        "join",
        "operations",
        "Hive",
        "UDFs",
        "partitions",
        "bucketing",
        "tables",
        "experience",
        "stream",
        "data",
        "log",
        "data",
        "HDFS",
        "Apache",
        "Flume",
        "Pig",
        "scripts",
        "transformations",
        "event",
        "filters",
        "preaggregations",
        "data",
        "HDFS",
        "Involvement",
        "custom",
        "UDFs",
        "Pig",
        "Hive",
        "strategies",
        "usefulness",
        "Python",
        "Java",
        "Pig",
        "Latin",
        "HQL",
        "HiveQL",
        "requirement",
        "design",
        "phase",
        "Streaming",
        "Lambda",
        "Architecture",
        "time",
        "streaming",
        "Spark",
        "Kafka",
        "Scala",
        "Experience",
        "data",
        "Spark",
        "RDD",
        "data",
        "computation",
        "output",
        "responses",
        "map",
        "programs",
        "Inmemory",
        "Spark",
        "processing",
        "Transformations",
        "actions",
        "text",
        "search",
        "platform",
        "NoSQL",
        "Logstash",
        "Elastic",
        "Search",
        "engine",
        "user",
        "searches",
        "Sqoop",
        "scripts",
        "interaction",
        "Pig",
        "MySQL",
        "Database",
        "Performance",
        "Enhancement",
        "Pig",
        "Hive",
        "HBase",
        "nodes",
        "ntier",
        "architecture",
        "ClientServer",
        "architecture",
        "Map",
        "Programs",
        "cluster",
        "Map",
        "Reduce",
        "jobs",
        "Java",
        "data",
        "MapReduce",
        "application",
        "Hadoop",
        "MapReduce",
        "programming",
        "HBase",
        "usage",
        "Oozie",
        "Workflow",
        "Orchestration",
        "cluster",
        "coordination",
        "Zookeeper",
        "ETL",
        "jobs",
        "organization",
        "project",
        "standards",
        "processes",
        "authentication",
        "ETL",
        "process",
        "data",
        "access",
        "Hibernate",
        "persistence",
        "framework",
        "Design",
        "GUI",
        "Model",
        "View",
        "Controller",
        "Architecture",
        "STRUTS",
        "Framework",
        "Integrated",
        "Spring",
        "DAO",
        "data",
        "access",
        "Hibernate",
        "Development",
        "Spring",
        "Framework",
        "Controllers",
        "Environment",
        "Hadoop",
        "2X",
        "HDFS",
        "MapReduce",
        "Hive",
        "Pig",
        "Sqoop",
        "Oozie",
        "HBase",
        "Java",
        "J2EE",
        "Eclipse",
        "HQL",
        "Hadoop",
        "Developer",
        "BMW",
        "Woodcliff",
        "Lake",
        "October",
        "February",
        "Responsibilities",
        "Complete",
        "Software",
        "development",
        "life",
        "cycle",
        "SDLC",
        "application",
        "Hadoop",
        "cluster",
        "data",
        "tools",
        "Pig",
        "Hive",
        "Map",
        "Reduce",
        "EC2",
        "Worked",
        "Data",
        "Science",
        "team",
        "requirements",
        "data",
        "mining",
        "projects",
        "source",
        "data",
        "file",
        "formats",
        "CSV",
        "TSV",
        "Experience",
        "data",
        "data",
        "sources",
        "MySQL",
        "Netezza",
        "Sqoop",
        "SFTP",
        "transformations",
        "Hive",
        "Pig",
        "data",
        "Performed",
        "transformations",
        "filtering",
        "data",
        "Hive",
        "Map",
        "Import",
        "export",
        "data",
        "environments",
        "MySQL",
        "HDFS",
        "productions",
        "Pig",
        "ETL",
        "tool",
        "transformations",
        "event",
        "preaggregations",
        "data",
        "HDFS",
        "Worked",
        "bucketing",
        "HIVE",
        "tables",
        "tuning",
        "parameters",
        "performance",
        "Impala",
        "scripts",
        "Adhoc",
        "Experience",
        "Oozie",
        "workflow",
        "scheduler",
        "template",
        "jobs",
        "Sqoop",
        "MR",
        "Pig",
        "Hive",
        "Shell",
        "scripts",
        "data",
        "HBase",
        "Spark",
        "POC",
        "ETLS",
        "Hive",
        "Spark",
        "Spark",
        "Yarn",
        "Environment",
        "code",
        "meetings",
        "issues",
        "Environment",
        "Apache",
        "Hadoop",
        "AWS",
        "EMR",
        "EC2",
        "S3",
        "Horton",
        "Map",
        "Reduce",
        "Hive",
        "Pig",
        "Sqoop",
        "Apache",
        "Spark",
        "Zookeeper",
        "HBase",
        "Java",
        "Oozie",
        "Oracle",
        "MySQL",
        "Netezza",
        "UNIX",
        "Shell",
        "Scripting",
        "Hadoop",
        "Developer",
        "WNS",
        "Global",
        "Services",
        "Pune",
        "Maharashtra",
        "June",
        "September",
        "Responsibilities",
        "Hadoop",
        "MapReduce",
        "MapReduce",
        "jobs",
        "java",
        "data",
        "Installed",
        "Apache",
        "Hadoop",
        "maintenance",
        "log",
        "files",
        "Hadoop",
        "cluster",
        "Importing",
        "data",
        "HDFS",
        "Hive",
        "Sqoop",
        "Experienced",
        "job",
        "flows",
        "Hadoop",
        "log",
        "Load",
        "sets",
        "data",
        "data",
        "sources",
        "data",
        "Map",
        "Reduce",
        "Programs",
        "cluster",
        "loading",
        "data",
        "UNIX",
        "file",
        "system",
        "HDFS",
        "Installed",
        "Hive",
        "Hive",
        "UDFs",
        "Hive",
        "tables",
        "data",
        "hive",
        "queries",
        "map",
        "way",
        "Hadoop",
        "cluster",
        "task",
        "Removing",
        "Nodes",
        "effect",
        "jobs",
        "data",
        "Created",
        "HBase",
        "data",
        "formats",
        "PII",
        "data",
        "portfolios",
        "income",
        "logic",
        "Pig",
        "scripts",
        "Load",
        "sets",
        "data",
        "Cluster",
        "coordination",
        "services",
        "Zookeeper",
        "data",
        "databases",
        "Sqoop",
        "visualization",
        "reports",
        "BI",
        "team",
        "QA",
        "environment",
        "configurations",
        "scripts",
        "Pig",
        "Sqoop",
        "Continuous",
        "monitoring",
        "Hadoop",
        "cluster",
        "Cloudera",
        "Manager",
        "Hibernate",
        "ORM",
        "framework",
        "Spring",
        "framework",
        "data",
        "persistence",
        "transaction",
        "management",
        "templates",
        "screens",
        "HTML",
        "JavaScript",
        "Environment",
        "Hadoop",
        "HDFS",
        "MapReduce",
        "Pig",
        "Sqoop",
        "UNIX",
        "HBase",
        "Java",
        "JavaScript",
        "HTML",
        "SQL",
        "Java",
        "Developer",
        "ILogix",
        "IT",
        "Services",
        "Pvt",
        "Ltd",
        "Hyderabad",
        "Telangana",
        "July",
        "May",
        "Responsibilities",
        "data",
        "MySQL",
        "Oracle",
        "HDFS",
        "Sqoop",
        "CDH3",
        "Hadoop",
        "cluster",
        "Centos",
        "clusters",
        "decommissioning",
        "data",
        "node",
        "name",
        "node",
        "recovery",
        "capacity",
        "planning",
        "slots",
        "configuration",
        "parser",
        "loader",
        "map",
        "application",
        "data",
        "HDFS",
        "store",
        "HBase",
        "Hive",
        "data",
        "HDFS",
        "Flume",
        "Written",
        "Map",
        "programs",
        "log",
        "data",
        "largescale",
        "data",
        "sets",
        "Hive",
        "tables",
        "data",
        "queries",
        "HBase",
        "Java",
        "API",
        "Java",
        "application",
        "jobs",
        "data",
        "Data",
        "Sources",
        "MySQL",
        "result",
        "data",
        "Hadoop",
        "Distributed",
        "File",
        "System",
        "Vodafone",
        "British",
        "Multinational",
        "Telecommunications",
        "Services",
        "Company",
        "Swindon",
        "UK",
        "Initially",
        "infrastructure",
        "company",
        "infrastructure",
        "cables",
        "companies",
        "Telecommunications",
        "Company",
        "components",
        "Project",
        "Description",
        "project",
        "Network",
        "elements",
        "network",
        "service",
        "procedure",
        "IMS",
        "Service",
        "order",
        "request",
        "components",
        "request",
        "form",
        "xmlResponse",
        "xml",
        "components",
        "InventoryMangementSystem",
        "GUI",
        "Network",
        "elements",
        "features",
        "details",
        "network",
        "elementsInventoryManagementGUI",
        "MVC",
        "Spring",
        "Hibernate",
        "framework",
        "code",
        "changes",
        "Weblogic",
        "server",
        "Responsibilities",
        "Team",
        "Member",
        "Developed",
        "Pig",
        "Latin",
        "scripts",
        "data",
        "output",
        "files",
        "HDFS",
        "data",
        "sources",
        "documentation",
        "HADOOP",
        "Clusters",
        "Hive",
        "queries",
        "Pig",
        "Scripts",
        "applications",
        "amounts",
        "data",
        "Hadoop",
        "clusters",
        "mapreduce",
        "application",
        "teams",
        "operating",
        "system",
        "Hadoop",
        "updates",
        "version",
        "upgrades",
        "Environment",
        "Hadoop",
        "Map",
        "Reduce",
        "Hive",
        "HBase",
        "Flume",
        "Sqoop",
        "Pig",
        "Zookeeper",
        "Java",
        "ETL",
        "SQL",
        "Centos",
        "Skills",
        "Cassandra",
        "Hdfs",
        "Impala",
        "Mapreduce",
        "Oozie",
        "Sqoop",
        "Hbase",
        "Kafka",
        "Flume",
        "Hadoop",
        "Jboss",
        "Mongodb",
        "Nosql",
        "Teradata",
        "Visual",
        "studio",
        "Apache",
        "spark",
        "Application",
        "server",
        "Git",
        "Hadoop",
        "Hbase",
        "Additional",
        "Information",
        "Skills",
        "Big",
        "Data",
        "Technologies",
        "HDFS",
        "MapReduce",
        "Hive",
        "Pig",
        "Sqoop",
        "Flume",
        "Oozie",
        "Zookeeper",
        "Kafka",
        "Cassandra",
        "Apache",
        "Spark",
        "Spark",
        "Streaming",
        "HBase",
        "Flume",
        "Impala",
        "Hadoop",
        "Distribution",
        "Cloudera",
        "Horton",
        "Apache",
        "AWS",
        "Languages",
        "Java",
        "SQL",
        "PLSQL",
        "Python",
        "Pig",
        "Latin",
        "HiveQL",
        "Scala",
        "Regular",
        "Expressions",
        "Web",
        "Technologies",
        "HTML",
        "CSS",
        "JavaScript",
        "XML",
        "JSP",
        "Restful",
        "SOAP",
        "Operating",
        "Systems",
        "Windows",
        "UNIX",
        "LINUX",
        "UBUNTU",
        "CENTOS",
        "PortalsApplication",
        "WebLogic",
        "WebSphere",
        "Application",
        "server",
        "WebSphere",
        "server",
        "JBOSS",
        "Build",
        "Automation",
        "tools",
        "SBT",
        "Ant",
        "Maven",
        "Version",
        "Control",
        "GIT",
        "IDE",
        "Build",
        "Tools",
        "Design",
        "Eclipse",
        "Visual",
        "Studio",
        "Net",
        "Beans",
        "Rational",
        "Application",
        "Developer",
        "Junit",
        "Oracle",
        "SQL",
        "Server",
        "MySQL",
        "MS",
        "Access",
        "NoSQL",
        "Database",
        "HBase",
        "Cassandra",
        "Teradata"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T22:10:01.201834",
    "resume_data": "SparkKafka Developer SparkKafka span lDeveloperspan SparkKafka Developer Citizens Property Insurance Corporation Florida City FL 10 years of experience in IT which includes experience in Bigdata Technologies Hadoop ecosystem Data Warehousing SQL related technologies in Retail Manufacturing Financial and Communication sectors 5 Years of experience in Big Data Analytics using Various Hadoop ecosystems tools and Spark Framework and Currently working on Spark and Spark Streaming frameworks extensively using Scala as the main programming dialect Experience installingconfiguringmaintaining Apache Hadoop clusters for application development and Hadoop tools like Sqoop Hive PIG Flume HBase Kafka Hue Storm Zoo Keeper Oozie Cassandra Sqoop Python Worked with major distributions like Cloudera CDH 34 Horton works Distributions and AWS Also worked on Unix and DWH in support for various Distributions Hands on experience in developing and deploying enterprisebased applications using major components in Hadoop ecosystem like Hadoop 2X YARN Hive Pig MapReduce Spark Kafka Storm Oozie HBase Flume Sqoop and Zookeeper Experience in handling large datasets using Partitions Spark in memory capabilities Broadcasts in Spark with Scala and python Effective and efficient Joins Transformations and other during ingestion process itself Experience in developing data pipeline using Pig Sqoop and Flume to extract the data from weblogs and store in HDFS and accomplished developing Pig Latin Scripts and using HiveQL for data analytics Extensively dealt with Spark Streaming and Apache Kafka to fetch live stream data Experience in converting HiveSQL queries into Spark transformations using Java and experience in ETL development using Kafka Flume and Sqoop Good experience in writing Spark applications using Scala and Java and used Scala set to develop Scala projects and executed using SparkSubmit Experience working on NoSQL databases including HBase Cassandra and MongoDB and experience using Sqoop to import data into HDFS from RDBMS and viceversa Developed Spark scripts by using Scala shell commands as per the requirement Good experience in writing Sqoop queries for transferring bulk data between Apache Hadoop and structured data stores Substantial experience in writing Map Reduce jobs in Java PIG Flume Zookeeper Hive and Storm Created multiple Map Reduce Jobs using Java API Pig and Hive for data extraction Strong expertise in troubleshooting and performance finetuning Spark Map Reduce and Hive applications Good experience on working with Amazon EMR framework for processing data on EMR and EC2 instances Created AWS VPC network for the installed Instances and configured security groups and Elastic IPs Accordingly Developed AWS Cloud formation templates to create custom sized VPC subnets EC2 instances ELB and security groups Extensive experience in developing applications that perform Data Processing tasks using Teradata Oracle SQL Server and MySQL database Worked on data warehousing and ETL tools like Informatica Tableau and Pentaho Experience in understanding the security requirements for Hadoop and integrate with Kerberos authentication and authorization infrastructure Acquaintance with Agile and Waterfall methodologies Responsible for handling several clients facing meetings with great communication skills Work Experience SparkKafka Developer Citizens Property Insurance Corporation February 2018 to Present Responsibilities Involved in complete Big Data flow of the application starting from data ingestion from upstream to HDFS processing and analyzing the data in HDFS Developed Spark API to import data into HDFS from Teradata and created Hive tables Developed Sqoop jobs to import data in Avro file format from Oracle database and created hive tables on top of it Created Partitioned and Bucketed Hive tables in Parquet File Formats with Snappy compression and then loaded data into Parquet hive tables from Avro hive tables Involved in running all the hive scripts through hive Impala Hive on Spark and some through Spark SQL Development and Review of spark code containing Airflow DAGs Databricks Notebooks Delta Tables in DDLs and Metadata SQLs other SQL scripts Deploying the Code to Dev QA PreProd and Prod Environments by adhering to GIT process flow and following the standards mentioned by the release management process Creating Technical Design Documentation and SupportOPS Turnover documentation by following the OPS checklist Raising Change Request once the code is PreProd Airflow Orchestration especially configuring the DAG start date and scheduled time and other parameters Worked on mainly developing Pyspark code in Databricks code using existing load patternsFull Incremental and Backfill for forecastingRegion and Country rawCustomerSales and pubCustomerSales Wrote Spark Dataframes that uses mainly CSV files Parquet Delta file formats Used Spark SQL Joins views partitioning extensively Validating the source data and generating the output data in the required format using Pyspark transformations Submitting Jobs for cluster administered by other Linux teams Environment Used Databricks Azure Data Lake storageGen1 Oracle EDW PySpark mainly Spark SQL Scala Spark occasionally Jenkins PyCharm Git Spark BDA server Putty for Tunneling into Airflow environments etc Involved in performance tuning of Hive from design storage and query perspectives Developed Flume ETL job for handling data from HTTP Source and Sink as HDFS Collected the Json data from HTTP Source and developed Spark APIs that helps to do inserts and updates in Hive tables Developed Spark scripts to import large files from Amazon S3 buckets Developed Spark core and Spark SQL scripts using Scala for faster data processing Developed Kafka consumers API in Scala for consuming data from Kafka topics Involved in designing and developing tables in HBase and storing aggregated data from Hive Table Integrated Hive and Tableau Desktop reports and published to Tableau Server Developed shell scripts for running Hive scripts in Hive and Impala Orchestrated number of Sqoop and Hive scripts using Oozie workflow and scheduled using Oozie coordinator Used Jira for bug tracking and Bit Bucket to checkin and checkout code changes Continuous monitoring and managing the Hadoop cluster through Cloudera Manager Environment HDFS Yarn Map Reduce Hive Sqoop Flume Oozie HBase Kafka Impala Spark SQL Spark Streaming Eclipse Oracle Teradata PLSQL UNIX Shell Scripting Cloudera Environment HDFS Yarn Map Reduce Hive Sqoop Flume Oozie HBase Kafka Impala SparkSQL Spark Streaming Eclipse Oracle Teradata PLSQL UNIX Shell Scripting Cloudera Hadoop Java Developer Experian Chicago IL March 2016 to January 2018 Responsibilities Responsible for architecting Hadoop clusters with CDH3 and involved in installation of CDH3 and up gradation to CDH4 from CDH3 Worked on creating Key space in Cassandra for saving the Spark Batch output Worked on Spark application to compact the small files present into hive ecosystem to make it equivalent to block size of HDFS Manage migration of onperm servers to AWS by creating golden images for upload and deployment Manage multiple AWS accounts with multiple VPCs for both production and nonproduction where primary objectives are automation build out integration and cost control Implemented the real time streaming ingestion using Kafka and Spark Streaming Loaded data using Sparkstreaming with Scala and Python Expertise in converting Map Reduce programs into Spark transformations using Spark RDDs Expertise in Spark Architecture including Spark Core Spark SQL Data Frames Spark Streaming and Spark MLlib Configured Spark streaming to receive real time data from the Kafka and store the stream data to HDFS using Scala Experience in implementing RealTime event processing and analytics using messaging systems like Spark Streaming Experience in using Kafka and Kafka brokers to initiate spark context and processing live streaming information with the help of RDD Good knowledge on Amazon AWS concepts like EMR and EC2 web services which provides fast and efficient processing of Big Data Experience with all flavor of Hadoop distributions including Cloudera Hortonworks Mapr and Apache Experience in installation configuration supporting and managing Hadoop Clusters using Apache Cloudera 5X distributions and on Amazon web services AWS Expertise in implementing SparkScala application using higher order functions for both batch and interactive analysis requirement Extensive experienced working with Spark tools like RDD transformations spark MLlib and spark QL Hands on experience in writing Hadoop Jobs for analyzing data using Hive QL Queries Pig Latin Data flow language and custom MapReduce programs in Java Experienced in working with structured data using HiveQL join operations Hive UDFs partitions bucketing and internalexternal tables Extensive experience in collecting and storing stream data like log data in HDFS using Apache Flume Experienced in using Pig scripts to do transformations event joins filters and some preaggregations before storing the data onto HDFS Involvement in creating custom UDFs for Pig and Hive to consolidate strategies and usefulness of Python Java into Pig Latin and HQL HiveQL Involved in requirement and design phase to implement Streaming Lambda Architecture to use real time streaming using Spark and Kafka and Scala Experience in loading the data into Spark RDD and performing inmemory data computation to generate the output responses Migrated complex map reduce programs into Inmemory Spark processing using Transformations and actions Developed full text search platform using NoSQL and Logstash Elastic Search engine allowing for much faster more scalable and more intuitive user searches Developed the Sqoop scripts to make the interaction between Pig and MySQL Database Worked on Performance Enhancement in Pig Hive and HBase on multiple nodes Worked with Distributed ntier architecture and ClientServer architecture Supported Map Reduce Programs those are running on the cluster and developed multiple Map Reduce jobs in Java for data cleaning and preprocessing Developed MapReduce application using Hadoop MapReduce programming and HBase Evaluated usage of Oozie for Workflow Orchestration and experienced in cluster coordination using Zookeeper Developing ETL jobs with organization and project defined standards and processes Experienced in enabling Kerberos authentication in ETL process Implemented data access using Hibernate persistence framework Design of GUI using Model View Controller Architecture STRUTS Framework Integrated Spring DAO for data access using Hibernate and involved in the Development of Spring Framework Controllers Environment Hadoop 2X HDFS MapReduce Hive Pig Sqoop Oozie HBase Java J2EE Eclipse HQL Hadoop Developer BMW Woodcliff Lake October 2013 to February 2016 Responsibilities Involved in the Complete Software development life cycle SDLC to develop the application Worked on analyzing Hadoop cluster using different big data analytic tools including Pig Hive and Map Reduce on EC2 Worked with the Data Science team to gather requirements for various data mining projects Worked with different source data file formats like JSON CSV and TSV etc Experience in importing data from various data sources like MySQL and Netezza using Sqoop SFTP performed transformations using Hive Pig and loaded data back into HDFS Performed transformations cleaning and filtering on imported data using Hive Map Reduce Import and export data between the environments like MySQL HDFS and deploying into productions Used Pig as ETL tool to do transformations event joins and some preaggregations before storing the data onto HDFS Worked on partitioning and used bucketing in HIVE tables and setting tuning parameters to improve the performance Involved in developing Impala scripts to do Adhoc queries Experience in Oozie workflow scheduler template to manage various jobs like Sqoop MR Pig Hive Shell scripts etc Involved in importing and exporting data from HBase using Spark Involved in POC for migrating ETLS from Hive to Spark in Spark on Yarn Environment Actively participating in the code reviews meetings and solving any technical issues Environment Apache Hadoop AWS EMR EC2 S3 Horton works Map Reduce Hive Pig Sqoop Apache Spark Zookeeper HBase Java Oozie Oracle MySQL Netezza and UNIX Shell Scripting Hadoop Developer WNS Global Services Pune Maharashtra June 2011 to September 2013 Responsibilities Installed and configured Hadoop MapReduce HDFS Developed multiple MapReduce jobs in java for data cleaning and preprocessing Installed and configured Apache Hadoop to test the maintenance of log files in Hadoop cluster Importing and exporting data into HDFS and Hive using Sqoop Experienced in defining job flows and managing and reviewing Hadoop log files Load and transform large sets of structured semi structured and unstructured data Responsible to manage data coming from different sources and for implementing MongoDB to store and analyze unstructured data Supported Map Reduce Programs those are running on the cluster and involved in loading data from UNIX file system to HDFS Installed and configured Hive and written Hive UDFs Involved in creating Hive tables loading with data and writing hive queries that will run internally in map reduce way Involved in Hadoop cluster task like Adding and Removing Nodes without any effect to running jobs and data Created HBase tables to store variable data formats of PII data coming from different portfolios Implemented best income logic using Pig scripts Load and transform large sets of structured semi structured and unstructured data Cluster coordination services through Zookeeper Exported the analyzed data to the relational databases using Sqoop for visualization and to generate reports for the BI team Supported in setting up QA environment and updating configurations for implementing scripts with Pig and Sqoop Continuous monitoring and managing the Hadoop cluster using Cloudera Manager Used Hibernate ORM framework with Spring framework for data persistence and transaction management and involved in templates and screens in HTML and JavaScript Environment Hadoop HDFS MapReduce Pig Sqoop UNIX HBase Java JavaScript HTML SQL Java Developer ILogix IT Services Pvt Ltd Hyderabad Telangana July 2009 to May 2011 Responsibilities Importing the data from the MySQL and Oracle into the HDFS using Sqoop Implemented CDH3 Hadoop cluster on Centos Worked on installing clusters commissioning decommissioning of data node name node recovery capacity planning and slots configuration Developing parser and loader map reduce application to retrieve data from HDFS and store to HBase and Hive Importing the unstructured data into the HDFS using Flume Written Map Reduce java programs to analyze the log data for largescale data sets Involved in creating Hive tables loading and analyzing data using hive queries Involved in using HBase Java API on Java application Automated all the jobs for extracting the data from different Data Sources like MySQL to pushing the result set data to Hadoop Distributed File System Vodafone is a British Multinational Telecommunications Services Company headquartered in Swindon UK Initially it has started with an infrastructure company which supplies infrastructure and cables to other companies After that it has started Telecommunications Company So they want to communicate with different components Project Description In this project all the Network elements for activating the network service will be stored in InventoryMangementSystemIMSUsing the procedure IMS will allocate deallocate modify cease the Service based on order request send by other components and request will be in the form of xmlResponse also will be sent as xml to other components InventoryMangementSystem GUI is used to add any new Network elements and more features are added to display the details of network elementsInventoryManagementGUI is designed with MVC Spring Hibernate framework architerature All the code changes are deployed in Weblogic server Responsibilities I was working as a Team Member Developed Pig Latin scripts to extract the data from the output files to load into HDFS Responsible for managing data from multiple sources Created and maintained Technical documentation for launching HADOOP Clusters and for executing Hive queries and Pig Scripts Designed and built many applications to deal with vast amounts of data flowing through multiple Hadoop clusters using Javabased mapreduce Worked with application teams to install operating system Hadoop updates patches version upgrades as required Environment Hadoop 100 Map Reduce Hive HBase Flume Sqoop Pig Zookeeper Java ETL SQL Centos Skills Cassandra Hdfs Impala Mapreduce Oozie Sqoop Hbase Kafka Flume Hadoop Jboss Mongodb Nosql Teradata Visual studio Apache spark Application server Git Hadoop Hbase Additional Information Skills Big Data Technologies HDFS MapReduce Hive Pig Sqoop Flume Oozie Zookeeper Kafka Cassandra Apache Spark Spark Streaming HBase Flume Impala Hadoop Distribution Cloudera Horton Works Apache AWS Languages Java SQL PLSQL Python Pig Latin HiveQL Scala Regular Expressions Web Technologies HTML CSS JavaScript XML JSP Restful SOAP Operating Systems Windows XP7810 UNIX LINUX UBUNTU CENTOS PortalsApplication servers WebLogic WebSphere Application server WebSphere Portal server JBOSS Build Automation tools SBT Ant Maven Version Control GIT IDE Build Tools Design Eclipse Visual Studio Net Beans Rational Application Developer Junit Databases Oracle SQL Server MySQL MS Access NoSQL Database HBase Cassandra MongoDB Teradata",
    "unique_id": "b1e6b4c7-1428-47bb-a333-905cc80f7882"
}