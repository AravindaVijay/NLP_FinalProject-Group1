{
    "clean_data": "Data Scientist Data Scientist Data Scientist LOreal USA Berkeley Heights NJ Data ScientistData Analyst around 8 years of Experience in Data Science and Analytics including Data Mining Statistical Analysis with domain knowledge in Retail Healthcare and Banking industries Involved in Data Science project life cycle including Data Cleaning Data extraction Visualization with large data sets of structured and unstructured data created ER diagrams and schema Experience with Machine Learning algorithms such as logistic regression KNN SVM random forest neural network linear regression lasso regression and kmeans Good experience in Text Analytics developing different Statistical Machine Learning Data Mining solutions to various business problems and generating data visualizations using R Python and Tableau Experience in implementing data analysis with various analytic tools such as Anaconda 40 Jupiter Notebook 4X R 30 ggplot2 dplyr Caret and Excel Experienced the full software lifecycle in SDLC Agile DevOps and Scrum methodologies including creating requirements test plans Strong skills in statistical methodologies such as AB test experiment design hypothesis test ANOVA Good Knowledge and experience in deep learning algorithms such as Artificial Neural network ANN Convolutional Neural Network CNN and Recurrent Neural Network RNN LSTM and RNN based speech recognition using TensorFlow Working Experience on Python 3527 such as NumPy SQLAlchemy Beautiful soup pickle Pyside Pymongo SciPy PyTables Ability to write and optimize diverse SQL queries working knowledge of RDBMS like SQL Server 2008 NoSQL databases like MongoDB 32 Experience in Big Data technologies like Spark 16 Spark SQL PySpark Hadoop 2X HDFS Hive 1X Experience in Data Warehousing including Data Modeling Data Architecture Data Integration ETLELT and Business Intelligence Good Experience in using various Python libraries Beautiful Soup NumPy Scipy matplotlib pythontwitter Pandas MySQL dB for database connectivity Having experienced in Big Data technologies including Apache Spark HDFS Hive MongoDB Used the version control tools like Git2X and build tools like Apache MavenAnt Worked on Machine Learning algorithms like Classification and Regression with KNN Model Decision Tree Model Nave Bayes Model Logistic Regression SVM Model and Latent Factor Model Experience and knowledge in provisioning virtual clusters under AWS cloud which includes services like EC2 S3 and EMR Good knowledge on Microsoft Azure Knowledge and understanding of DevopsDockers Experience in writing Sub Queries Stored Procedures Triggers Cursors and Functions on MySQL and PostgreSQL database Understanding of Python Best Practices PEP8 and package management system PIP in Python Extensive experience in Data visualization tools like Tableau 9X 10X for creating dashboards Experience in development and designing of ETL methodology for supporting data transformations and processing in a corporatewide environment using Teradata Mainframes and UNIX Shell Scripting Used SQL Queries and Stored Procedures extensively in retrieving the contents from MySQL Good in implementing SQL tuning techniques such as Join Indexes JI Aggregate Join Indexes AJIs Statistics and Table changes including Index SQL loader for direct and parallel load of data from raw file to database tables Experience in development of TSQL OLAP PLSQL Stored Procedures Triggers Functions Packages performance tuning and optimization for business logic implementation Strong SQL Server programming skills with experience in working with functions packages and triggers Good at handling complex processes using SAS Base SAS SQL SAS STAT SASGraph Merge Join and Set statements SAS ODS Good industry knowledge analytical problem solving skills and ability to work well with in a team as well as an individual Great team player and ability to work collaboratively and independently as required Experience with Biological data sets genomic transcriptomic microbiome etc Authorized to work in the US for any employer Work Experience Data Scientist LOreal USA Berkeley Heights NJ September 2017 to Present Python Description LOreal Group is one of the worlds largest cosmetics and Beauty company Online Product Trading B2B B2C brings dealers distributors manufacturers fleet owners and traders from all around the world together and facilitates trade in an easy secure transparent and cost efficient way Buyers can find sellers and sellers can find the best possible buyers This application can be used both by customers and manufacturers Customer can browse and select their choice of LOreal Product or put their specifications if not getting the matching choice so the manufacturers can make their choice of product and delivered to them by LOreal product Trader Responsibilities Communicated and coordinated with other departments to gather business requirements Gathering all the data that is required from multiple data sources and creating datasets that will be used in analysis Participated in the installation of SASEBI on Linux platform worked on Data Modeling tools Erwin Data Modeler to design the data models Designed tables and implemented the naming conventions for Logical and Physical Data Models in Erwin 70 Worked on development of data warehouse data Lake and ETL systems using relational and nonrelational tools like SQL No SQL Created SQL tables with referential integrity and developed queries using SQL SQLPLUS and PLSQL Design coding unit testing of ETL package source marts and subject marts using Informatica ETL processes for Oracle database Developed various QlikView Data Models by extracting and using the data from various sources files DB2 Excel Flat Files and Big data Handled importing data from various data sources performed transformations using Hive Map Reduce and loaded data into HDFS Interaction with Business Analyst SMEs and other Data Architects to understand Business needs and functionality for various project solutions Identifying and executing process improvements handson in various technologies such as Oracle Informatica business Objects Worked on data cleaning and ensured data quality consistency integrity using Pandas NumPy Participated in feature engineering such as feature intersection generating feature normalize and label encoding with Scikitlearn preprocessing Improved fraud prediction performance by using random forest and gradient boosting for feature selection with Python Scikitlearn Used Python NumPy Scipy Pandas ScikitLearn  and Spark 20 PySpark MLlib to develop variety of models and algorithms for analytic purposes Utilized spark Scala Hadoop HBase Kafka Spark Streaming MLlib Python a broad variety of machine learning methods including classifications regressions dimensionally reduction etc Implemented tuned and tested the model on AWS EC2 to get the best algorithm and parameters Setup storage and data analysis tools in Amazon Web Services cloud computing infrastructure Designed and developed machine learning models in Apache Spark MLlib Used NLTK in Python for developing various machine learning algorithms Implemented deep learning algorithms such as Artificial Neural network ANN and Recurrent Neural Network RNN tuned hyperparameter and improved models with Python packages TensorFlow Installed and used Caffe Deep Learning Framework Modified selected machine learning models with realtime data in in Spark PySpark Worked with architect to improve cloud Hadoop architecture as needed for Research Worked on different formats such as JSON XML and performed machine learning algorithms in Python Participated in all phases of datamining data collection data cleaning developing models validation visualization and performed Gap analysis Worked very close with Data Architects and DBA team to implement data model changes in the database in all environments Used Pandas library for statistical Analysis Communicated the results with operations team for taking best decisions Collected data needs and requirements by Interacting with the other departments Environment Python 3227 hive oozie Tableau Informatica 90 HTML5 CSS XML MySQL MS SQL Server 20082012 JavaScript AWS S3 EC2 Linux Jupyter Notebook RNN ANN Spark Hadoop Data Analyst CVS Health Boston MA September 2015 to August 2017 Description Worked with pharmacy chain client As the retail pharmacy it sells prescription drugs and a wide assortment of general merchandise including overthecounter drugs beauty products and cosmetics film and photo finishing services our goal of the project was to design develop and field data mining solutions that have direct impact to Patients and Janssen Responsibilities Investigated market sizing competitive analysis and positioning for product feasibility Conducted research on development and designing of sample methodologies and analyzed data for pricing of clients products Collaborated with database engineers to implement ETL process wrote and optimized SQL queries to perform data extraction and merging from SQL server database Worked on Business forecasting segmentation analysis and Data mining Developed Machine Learning algorithm to diagnose blood loss Generated graphs and reports using ggplot2 package in RStudio for analytical models Developed and implemented R and Shiny application which showcases machine learning for business forecasting Developed predictive models using Decision Tree Random Forest and Nave Bayes Performed time series analysis using Tableau Developed various workbooks in Tableau from multiple data sources Created dashboards and visualizations using Tableau desktop Later used Alteryx to blend the data Performed analysis using JMP Perform validation on machine learning output from R Written connectors to extract data from databases Environment R Python 2x Excel 2010 Machine Learning Tableau Quick View JMP Segmentation analysis Data Scientist The Cellular Connection July 2014 to August 2015 Description TCC is the largest Premium Wireless Retailer company I work with the lead Data scientist to perform statistical analyses and predictive models on datasets to address various business problems in Benefits and Benefit operations through leveraging advanced statistical modeling operations research machine learning or data mining techniques Responsibilities Involved in Data Profiling to learn about user behavior and merge data from multiple data sources Participated in big data processing applications to collect clean and normalization large volumes of open data using Hadoop ecosystems such as PIG Hive and HBase Designed the prototype of the Data Mart and documented possible outcome from it for enduser Worked as Analyst to generate Data Models using Erwin and developed a relational database system Designing and developing various machine learning frameworks using Python R and MATLAB Processed huge datasets over billion data points over 1 TB of datasets for data association pairing and provided insights into meaningful data association and trends Participated in all phases of data collection data cleaning developing models validation and visualization and performed Gap analysis Good knowledge of Hadoop Architecture and various components such as HDFS Job Tracker Task Tracker Name Node Data Node Secondary Name Node and MapReduce concepts Handled importing data from various data sources performed transformations using Hive MapReduce and loaded data into HDFS Collaborate with data engineers to implement ETL process write and optimized SQL queries to perform data extraction from Cloud and merging from Oracle 12c Collect unstructured data from MongoDB 33 and completed data aggregation Conducted analysis of assessing customer consuming behaviors and discover the value of customers with RMF analysis applied customer segmentation with clustering algorithms such as KMeans Clustering and Hierarchical Clustering Participate in features engineering such as feature intersection generating feature normalize and Label encoding with Scikitlearn preprocessing Used pandas NumPy  Scipy Matplotlib SKLearn and NLTK Natural Language Toolkit in Python for developing various machine learning algorithms Utilized machine learning algorithms such as Decision Tree linear regression multivariate regression Naive Bayes Random Forests Kmeans KNN Parsing data producing concise conclusions from raw data in a clean wellstructured and easily maintainable format Determine customer satisfaction and help enhance customer experience using NLP Developed various QlikView Data Models by extracting and using the data from various sources files DB2 Excel Flat Files and Big data Perform data integrity checks data cleaning exploratory analysis and feature engineer using R 340 Worked on different data formats such as JSON XML and performed machine learning algorithms in R Worked on MapReduceSpark Python modules for machine learning predictive analytics in Hadoop Perform data visualizations with Tableau 10 and generated dashboards to present the findings Work on Text Analytics Nave Bayes Sentiment analysis creating word clouds and retrieving data from Twitter and other social networking platforms Use Git26 to apply version control Tracked changes in files and coordinated work on the files among multiple team members Environment Python 3227 hive Tableau R QlikView MySQL MS SQL Server 20082012 AWS S3 EC2 Linux Jupyter Notebook RNN ANN Spark Hadoop Data Analyst Python PIMCO Manhattan NY May 2012 to June 2014 Description Pacific Investment Management Company is an American investment management firm provides mutual funds and other portfolio management and asset allocation solutions for millions of investors worldwide I was involving in developing statistical models and algorithms to predict classify quantify andor forecast business metrics Partner with business units and Analytics colleagues outside the workgroup to simulate and validate processes to maximize success metrics Responsibilities Used python libraries like Beautiful Soap NumPy Created various types of data visualizations using Python and Tableau Monitoring and tracking process performance using analytics tools like Tableau dashboard R Utilized standard Python modules such as csv robot parser iterators and pickle for development Created views in Tableau Desktop that were published to internal team for review and further data analysis and customization using filters and actions Worked on Python OpenStack APIs and used NumPy for Numerical analysis Used Python scripts to update content in the database and manipulate files Used Python creating graphics data exchange and business logic implementation Performed troubleshooting fixed and deployed many Python bug fixes of the applications and involved in fine tuning of existing processes followed advance patterns and methodologies Skilled in using collections in Python for manipulating and looping through different user defined objects Installed numerous python packages using pip and easy install Environment Python 27 Tableau R Windows XP UNIX HTML SQL server 2005 SQL Developer Johnson Johnson Bengaluru Karnataka May 2011 to 2012 Responsibilities Used DDL and DML for writing triggers stored procedures and data manipulation Interacted with Team and Analysis Design and Develop database using ER Diagram involved in Design Development and testing of the system Developed SQL Server Stored Procedures Tuned SQL Queries using Indexes Created Views to facilitate easy user interface implementation and Triggers on them to facilitate consistent data entry into the database Implemented Exceptional Handling Worked on client requirement and wrote Complex SQL Queries to generate Crystal Reports Created different Data sources and Datasets for the reports Tuned and Optimized SQL Queries using Execution Plan and Profiler Rebuilding Indexes and Tables as part of Performance Tuning Exercise Involved in performing database Backup and Recovery Documented end user requirements for SSRS Reports and database design Education Bachelors Skills Python 3227 hive oozie Tableau Informatica 90 HTML5 CSS XML MySQL MS SQL Server 20082012 JavaScript AWS S3 EC2 Linux Jupyter Notebook RNN ANN Spark Hadoop 8 years Additional Information SKILLS MATRIX Languages C C XML RR Studio SAS Enterprise Guide SAS R Python 2x3x Java C SQL Shell Scripting NO SQL Databases Cassandra HBase MongoDB Maria DB Statistics Hypothetical Testing ANOVA Confidence Intervals Bayes Law MLE Fish Information Principal Component Analysis PCA CrossValidation correlation BI Tools Tableau Tableau server Tableau Reader Splunk SAP Business Objects OBIEE SAP Business Intelligence QlikView Amazon Redshift or Azure Data Warehouse Algorithms Logistic regression random forest XG Boost KNN SVM neural network rk linear regression lasso regression kmeans Big Data Hadoop HDFS HIVE PuTTy Spark Scala Sqoop Reporting Tools MS Office  VisioOutlook Crystal Reports XI SSRS Cognos 7060 Database Design Tools and Data Modeling MS Visio ERWIN 4540 Star SchemaSnowflake Schema modeling Fact Dimensions tables physical logical data modeling Normalization and Denormalization techniques Kimball Inmon Methodologies",
    "entities": [
        "Apache MavenAnt Worked",
        "Data Profiling",
        "Statistical Machine Learning Data Mining",
        "Fact Dimensions",
        "XG Boost KNN SVM",
        "Informatica",
        "Indexes Created Views",
        "SDLC Agile DevOps",
        "Customer",
        "Tableau Reader Splunk SAP Business Objects OBIEE SAP Business Intelligence QlikView Amazon Redshift or Azure Data",
        "Data Scientist The Cellular Connection",
        "Banking industries Involved",
        "Cloud",
        "Erwin Data Modeler",
        "NLTK Natural Language Toolkit",
        "SQL No",
        "RNN",
        "ER",
        "Data Cleaning Data",
        "Soap NumPy Created",
        "Hadoop",
        "Tableau Developed",
        "Data Science and Analytics",
        "Retail Healthcare",
        "Oracle Informatica",
        "HBase",
        "DevopsDockers Experience",
        "Apache Spark",
        "Data Models",
        "Python",
        "Conducted",
        "Artificial Neural",
        "Utilized",
        "ER Diagram",
        "Complex SQL Queries",
        "Responsibilities Involved",
        "HDFS Collaborate",
        "SQL Created",
        "Analysis Communicated",
        "Research Worked",
        "kmeans Big Data Hadoop",
        "ANN",
        "SASEBI",
        "Linux",
        "Collaborated",
        "Tableau Desktop",
        "KMeans Clustering and Hierarchical Clustering Participate",
        "HDFS Interaction with Business Analyst",
        "Hadoop Perform",
        "MS",
        "the Data Mart",
        "Design Development",
        "Spark",
        "PIG Hive",
        "linear",
        "Developed Machine Learning",
        "JMP Segmentation",
        "Oracle 12c Collect",
        "Trader Responsibilities Communicated",
        "US",
        "Optimized SQL Queries",
        "Spark 16 Spark",
        "KNN",
        "Data Architects",
        "Classification and Regression",
        "Created",
        "AWS",
        "Hadoop Architecture",
        "Text Analytics",
        "Sub Queries Stored Procedures Triggers Cursors and Functions",
        "JMP Perform",
        "NLP Developed",
        "Interacted with Team and Analysis Design and Develop",
        "Join Indexes JI Aggregate",
        "Crystal Reports Created",
        "HDFS Job Tracker Task Tracker",
        "SAS",
        "SQL",
        "ANN Convolutional Neural Network",
        "Amazon Web Services",
        "Description Pacific Investment Management Company",
        "DML",
        "Manhattan",
        "Data Modeling Data Architecture Data Integration",
        "Profiler Rebuilding Indexes and",
        "PySpark MLlib",
        "Anaconda",
        "Big Data",
        "BI Tools Tableau Tableau",
        "Erwin",
        "Normalization and Denormalization",
        "Pandas",
        "RMF",
        "ETL",
        "Biological",
        "Python Scikitlearn Used Python NumPy Scipy Pandas ScikitLearn",
        "Studio SAS Enterprise Guide",
        "Recurrent Neural Network RNN",
        "Data Scientist Data Scientist Data Scientist LOreal USA Berkeley Heights NJ Data",
        "Microsoft",
        "CNN",
        "Data Mining Statistical Analysis",
        "Data Modeling",
        "Boston",
        "Data",
        "MapReduce",
        "Lake",
        "Premium Wireless Retailer",
        "Data Warehousing",
        "Tableau",
        "Machine Learning",
        "TensorFlow Working Experience on",
        "Present Python Description LOreal Group",
        "PIP",
        "Decision Tree Random Forest",
        "Data Modeling MS Visio ERWIN",
        "Spark Scala Sqoop Reporting Tools MS Office",
        "SVM",
        "Node",
        "LOreal",
        "JSON XML"
    ],
    "experience": "Experience in Data Science and Analytics including Data Mining Statistical Analysis with domain knowledge in Retail Healthcare and Banking industries Involved in Data Science project life cycle including Data Cleaning Data extraction Visualization with large data sets of structured and unstructured data created ER diagrams and schema Experience with Machine Learning algorithms such as logistic regression KNN SVM random forest neural network linear regression lasso regression and kmeans Good experience in Text Analytics developing different Statistical Machine Learning Data Mining solutions to various business problems and generating data visualizations using R Python and Tableau Experience in implementing data analysis with various analytic tools such as Anaconda 40 Jupiter Notebook 4X R 30 ggplot2 dplyr Caret and Excel Experienced the full software lifecycle in SDLC Agile DevOps and Scrum methodologies including creating requirements test plans Strong skills in statistical methodologies such as AB test experiment design hypothesis test ANOVA Good Knowledge and experience in deep learning algorithms such as Artificial Neural network ANN Convolutional Neural Network CNN and Recurrent Neural Network RNN LSTM and RNN based speech recognition using TensorFlow Working Experience on Python 3527 such as NumPy SQLAlchemy Beautiful soup pickle Pyside Pymongo SciPy PyTables Ability to write and optimize diverse SQL queries working knowledge of RDBMS like SQL Server 2008 NoSQL databases like MongoDB 32 Experience in Big Data technologies like Spark 16 Spark SQL PySpark Hadoop 2X HDFS Hive 1X Experience in Data Warehousing including Data Modeling Data Architecture Data Integration ETLELT and Business Intelligence Good Experience in using various Python libraries Beautiful Soup NumPy Scipy matplotlib pythontwitter Pandas MySQL dB for database connectivity Having experienced in Big Data technologies including Apache Spark HDFS Hive MongoDB Used the version control tools like Git2X and build tools like Apache MavenAnt Worked on Machine Learning algorithms like Classification and Regression with KNN Model Decision Tree Model Nave Bayes Model Logistic Regression SVM Model and Latent Factor Model Experience and knowledge in provisioning virtual clusters under AWS cloud which includes services like EC2 S3 and EMR Good knowledge on Microsoft Azure Knowledge and understanding of DevopsDockers Experience in writing Sub Queries Stored Procedures Triggers Cursors and Functions on MySQL and PostgreSQL database Understanding of Python Best Practices PEP8 and package management system PIP in Python Extensive experience in Data visualization tools like Tableau 9X 10X for creating dashboards Experience in development and designing of ETL methodology for supporting data transformations and processing in a corporatewide environment using Teradata Mainframes and UNIX Shell Scripting Used SQL Queries and Stored Procedures extensively in retrieving the contents from MySQL Good in implementing SQL tuning techniques such as Join Indexes JI Aggregate Join Indexes AJIs Statistics and Table changes including Index SQL loader for direct and parallel load of data from raw file to database tables Experience in development of TSQL OLAP PLSQL Stored Procedures Triggers Functions Packages performance tuning and optimization for business logic implementation Strong SQL Server programming skills with experience in working with functions packages and triggers Good at handling complex processes using SAS Base SAS SQL SAS STAT SASGraph Merge Join and Set statements SAS ODS Good industry knowledge analytical problem solving skills and ability to work well with in a team as well as an individual Great team player and ability to work collaboratively and independently as required Experience with Biological data sets genomic transcriptomic microbiome etc Authorized to work in the US for any employer Work Experience Data Scientist LOreal USA Berkeley Heights NJ September 2017 to Present Python Description LOreal Group is one of the worlds largest cosmetics and Beauty company Online Product Trading B2B B2C brings dealers distributors manufacturers fleet owners and traders from all around the world together and facilitates trade in an easy secure transparent and cost efficient way Buyers can find sellers and sellers can find the best possible buyers This application can be used both by customers and manufacturers Customer can browse and select their choice of LOreal Product or put their specifications if not getting the matching choice so the manufacturers can make their choice of product and delivered to them by LOreal product Trader Responsibilities Communicated and coordinated with other departments to gather business requirements Gathering all the data that is required from multiple data sources and creating datasets that will be used in analysis Participated in the installation of SASEBI on Linux platform worked on Data Modeling tools Erwin Data Modeler to design the data models Designed tables and implemented the naming conventions for Logical and Physical Data Models in Erwin 70 Worked on development of data warehouse data Lake and ETL systems using relational and nonrelational tools like SQL No SQL Created SQL tables with referential integrity and developed queries using SQL SQLPLUS and PLSQL Design coding unit testing of ETL package source marts and subject marts using Informatica ETL processes for Oracle database Developed various QlikView Data Models by extracting and using the data from various sources files DB2 Excel Flat Files and Big data Handled importing data from various data sources performed transformations using Hive Map Reduce and loaded data into HDFS Interaction with Business Analyst SMEs and other Data Architects to understand Business needs and functionality for various project solutions Identifying and executing process improvements handson in various technologies such as Oracle Informatica business Objects Worked on data cleaning and ensured data quality consistency integrity using Pandas NumPy Participated in feature engineering such as feature intersection generating feature normalize and label encoding with Scikitlearn preprocessing Improved fraud prediction performance by using random forest and gradient boosting for feature selection with Python Scikitlearn Used Python NumPy Scipy Pandas ScikitLearn   and Spark 20 PySpark MLlib to develop variety of models and algorithms for analytic purposes Utilized spark Scala Hadoop HBase Kafka Spark Streaming MLlib Python a broad variety of machine learning methods including classifications regressions dimensionally reduction etc Implemented tuned and tested the model on AWS EC2 to get the best algorithm and parameters Setup storage and data analysis tools in Amazon Web Services cloud computing infrastructure Designed and developed machine learning models in Apache Spark MLlib Used NLTK in Python for developing various machine learning algorithms Implemented deep learning algorithms such as Artificial Neural network ANN and Recurrent Neural Network RNN tuned hyperparameter and improved models with Python packages TensorFlow Installed and used Caffe Deep Learning Framework Modified selected machine learning models with realtime data in in Spark PySpark Worked with architect to improve cloud Hadoop architecture as needed for Research Worked on different formats such as JSON XML and performed machine learning algorithms in Python Participated in all phases of datamining data collection data cleaning developing models validation visualization and performed Gap analysis Worked very close with Data Architects and DBA team to implement data model changes in the database in all environments Used Pandas library for statistical Analysis Communicated the results with operations team for taking best decisions Collected data needs and requirements by Interacting with the other departments Environment Python 3227 hive oozie Tableau Informatica 90 HTML5 CSS XML MySQL MS SQL Server 20082012 JavaScript AWS S3 EC2 Linux Jupyter Notebook RNN ANN Spark Hadoop Data Analyst CVS Health Boston MA September 2015 to August 2017 Description Worked with pharmacy chain client As the retail pharmacy it sells prescription drugs and a wide assortment of general merchandise including overthecounter drugs beauty products and cosmetics film and photo finishing services our goal of the project was to design develop and field data mining solutions that have direct impact to Patients and Janssen Responsibilities Investigated market sizing competitive analysis and positioning for product feasibility Conducted research on development and designing of sample methodologies and analyzed data for pricing of clients products Collaborated with database engineers to implement ETL process wrote and optimized SQL queries to perform data extraction and merging from SQL server database Worked on Business forecasting segmentation analysis and Data mining Developed Machine Learning algorithm to diagnose blood loss Generated graphs and reports using ggplot2 package in RStudio for analytical models Developed and implemented R and Shiny application which showcases machine learning for business forecasting Developed predictive models using Decision Tree Random Forest and Nave Bayes Performed time series analysis using Tableau Developed various workbooks in Tableau from multiple data sources Created dashboards and visualizations using Tableau desktop Later used Alteryx to blend the data Performed analysis using JMP Perform validation on machine learning output from R Written connectors to extract data from databases Environment R Python 2x Excel 2010 Machine Learning Tableau Quick View JMP Segmentation analysis Data Scientist The Cellular Connection July 2014 to August 2015 Description TCC is the largest Premium Wireless Retailer company I work with the lead Data scientist to perform statistical analyses and predictive models on datasets to address various business problems in Benefits and Benefit operations through leveraging advanced statistical modeling operations research machine learning or data mining techniques Responsibilities Involved in Data Profiling to learn about user behavior and merge data from multiple data sources Participated in big data processing applications to collect clean and normalization large volumes of open data using Hadoop ecosystems such as PIG Hive and HBase Designed the prototype of the Data Mart and documented possible outcome from it for enduser Worked as Analyst to generate Data Models using Erwin and developed a relational database system Designing and developing various machine learning frameworks using Python R and MATLAB Processed huge datasets over billion data points over 1 TB of datasets for data association pairing and provided insights into meaningful data association and trends Participated in all phases of data collection data cleaning developing models validation and visualization and performed Gap analysis Good knowledge of Hadoop Architecture and various components such as HDFS Job Tracker Task Tracker Name Node Data Node Secondary Name Node and MapReduce concepts Handled importing data from various data sources performed transformations using Hive MapReduce and loaded data into HDFS Collaborate with data engineers to implement ETL process write and optimized SQL queries to perform data extraction from Cloud and merging from Oracle 12c Collect unstructured data from MongoDB 33 and completed data aggregation Conducted analysis of assessing customer consuming behaviors and discover the value of customers with RMF analysis applied customer segmentation with clustering algorithms such as KMeans Clustering and Hierarchical Clustering Participate in features engineering such as feature intersection generating feature normalize and Label encoding with Scikitlearn preprocessing Used pandas NumPy   Scipy Matplotlib SKLearn and NLTK Natural Language Toolkit in Python for developing various machine learning algorithms Utilized machine learning algorithms such as Decision Tree linear regression multivariate regression Naive Bayes Random Forests Kmeans KNN Parsing data producing concise conclusions from raw data in a clean wellstructured and easily maintainable format Determine customer satisfaction and help enhance customer experience using NLP Developed various QlikView Data Models by extracting and using the data from various sources files DB2 Excel Flat Files and Big data Perform data integrity checks data cleaning exploratory analysis and feature engineer using R 340 Worked on different data formats such as JSON XML and performed machine learning algorithms in R Worked on MapReduceSpark Python modules for machine learning predictive analytics in Hadoop Perform data visualizations with Tableau 10 and generated dashboards to present the findings Work on Text Analytics Nave Bayes Sentiment analysis creating word clouds and retrieving data from Twitter and other social networking platforms Use Git26 to apply version control Tracked changes in files and coordinated work on the files among multiple team members Environment Python 3227 hive Tableau R QlikView MySQL MS SQL Server 20082012 AWS S3 EC2 Linux Jupyter Notebook RNN ANN Spark Hadoop Data Analyst Python PIMCO Manhattan NY May 2012 to June 2014 Description Pacific Investment Management Company is an American investment management firm provides mutual funds and other portfolio management and asset allocation solutions for millions of investors worldwide I was involving in developing statistical models and algorithms to predict classify quantify andor forecast business metrics Partner with business units and Analytics colleagues outside the workgroup to simulate and validate processes to maximize success metrics Responsibilities Used python libraries like Beautiful Soap NumPy Created various types of data visualizations using Python and Tableau Monitoring and tracking process performance using analytics tools like Tableau dashboard R Utilized standard Python modules such as csv robot parser iterators and pickle for development Created views in Tableau Desktop that were published to internal team for review and further data analysis and customization using filters and actions Worked on Python OpenStack APIs and used NumPy for Numerical analysis Used Python scripts to update content in the database and manipulate files Used Python creating graphics data exchange and business logic implementation Performed troubleshooting fixed and deployed many Python bug fixes of the applications and involved in fine tuning of existing processes followed advance patterns and methodologies Skilled in using collections in Python for manipulating and looping through different user defined objects Installed numerous python packages using pip and easy install Environment Python 27 Tableau R Windows XP UNIX HTML SQL server 2005 SQL Developer Johnson Johnson Bengaluru Karnataka May 2011 to 2012 Responsibilities Used DDL and DML for writing triggers stored procedures and data manipulation Interacted with Team and Analysis Design and Develop database using ER Diagram involved in Design Development and testing of the system Developed SQL Server Stored Procedures Tuned SQL Queries using Indexes Created Views to facilitate easy user interface implementation and Triggers on them to facilitate consistent data entry into the database Implemented Exceptional Handling Worked on client requirement and wrote Complex SQL Queries to generate Crystal Reports Created different Data sources and Datasets for the reports Tuned and Optimized SQL Queries using Execution Plan and Profiler Rebuilding Indexes and Tables as part of Performance Tuning Exercise Involved in performing database Backup and Recovery Documented end user requirements for SSRS Reports and database design Education Bachelors Skills Python 3227 hive oozie Tableau Informatica 90 HTML5 CSS XML MySQL MS SQL Server 20082012 JavaScript AWS S3 EC2 Linux Jupyter Notebook RNN ANN Spark Hadoop 8 years Additional Information SKILLS MATRIX Languages C C XML RR Studio SAS Enterprise Guide SAS R Python 2x3x Java C SQL Shell Scripting NO SQL Databases Cassandra HBase MongoDB Maria DB Statistics Hypothetical Testing ANOVA Confidence Intervals Bayes Law MLE Fish Information Principal Component Analysis PCA CrossValidation correlation BI Tools Tableau Tableau server Tableau Reader Splunk SAP Business Objects OBIEE SAP Business Intelligence QlikView Amazon Redshift or Azure Data Warehouse Algorithms Logistic regression random forest XG Boost KNN SVM neural network rk linear regression lasso regression kmeans Big Data Hadoop HDFS HIVE PuTTy Spark Scala Sqoop Reporting Tools MS Office   VisioOutlook Crystal Reports XI SSRS Cognos 7060 Database Design Tools and Data Modeling MS Visio ERWIN 4540 Star SchemaSnowflake Schema modeling Fact Dimensions tables physical logical data modeling Normalization and Denormalization techniques Kimball Inmon Methodologies",
    "extracted_keywords": [
        "Data",
        "Scientist",
        "Data",
        "Scientist",
        "Data",
        "Scientist",
        "LOreal",
        "USA",
        "Berkeley",
        "Heights",
        "NJ",
        "Data",
        "ScientistData",
        "Analyst",
        "years",
        "Experience",
        "Data",
        "Science",
        "Analytics",
        "Data",
        "Mining",
        "Statistical",
        "Analysis",
        "domain",
        "knowledge",
        "Retail",
        "Healthcare",
        "Banking",
        "industries",
        "Data",
        "Science",
        "project",
        "life",
        "cycle",
        "Data",
        "Cleaning",
        "Data",
        "extraction",
        "Visualization",
        "data",
        "sets",
        "data",
        "ER",
        "diagrams",
        "schema",
        "Experience",
        "Machine",
        "Learning",
        "regression",
        "KNN",
        "SVM",
        "forest",
        "network",
        "linear",
        "regression",
        "lasso",
        "regression",
        "kmeans",
        "experience",
        "Text",
        "Analytics",
        "Machine",
        "Learning",
        "Data",
        "Mining",
        "solutions",
        "business",
        "problems",
        "data",
        "visualizations",
        "R",
        "Python",
        "Tableau",
        "Experience",
        "data",
        "analysis",
        "tools",
        "Anaconda",
        "Jupiter",
        "Notebook",
        "4X",
        "R",
        "ggplot2",
        "dplyr",
        "Caret",
        "Excel",
        "software",
        "lifecycle",
        "SDLC",
        "Agile",
        "DevOps",
        "Scrum",
        "methodologies",
        "requirements",
        "test",
        "skills",
        "methodologies",
        "AB",
        "test",
        "experiment",
        "design",
        "hypothesis",
        "test",
        "ANOVA",
        "Good",
        "Knowledge",
        "experience",
        "learning",
        "algorithms",
        "Artificial",
        "Neural",
        "network",
        "ANN",
        "Convolutional",
        "Neural",
        "Network",
        "CNN",
        "Recurrent",
        "Neural",
        "Network",
        "RNN",
        "LSTM",
        "RNN",
        "speech",
        "recognition",
        "TensorFlow",
        "Working",
        "Experience",
        "Python",
        "NumPy",
        "SQLAlchemy",
        "soup",
        "pickle",
        "Pyside",
        "Pymongo",
        "SciPy",
        "PyTables",
        "Ability",
        "SQL",
        "knowledge",
        "RDBMS",
        "SQL",
        "Server",
        "NoSQL",
        "MongoDB",
        "Experience",
        "Big",
        "Data",
        "technologies",
        "Spark",
        "Spark",
        "SQL",
        "PySpark",
        "Hadoop",
        "2X",
        "HDFS",
        "Hive",
        "1X",
        "Experience",
        "Data",
        "Warehousing",
        "Data",
        "Modeling",
        "Data",
        "Architecture",
        "Data",
        "Integration",
        "ETLELT",
        "Business",
        "Intelligence",
        "Good",
        "Experience",
        "Python",
        "Beautiful",
        "Soup",
        "NumPy",
        "Scipy",
        "matplotlib",
        "pythontwitter",
        "Pandas",
        "MySQL",
        "database",
        "connectivity",
        "Big",
        "Data",
        "technologies",
        "Apache",
        "Spark",
        "HDFS",
        "Hive",
        "MongoDB",
        "version",
        "control",
        "tools",
        "Git2X",
        "tools",
        "Apache",
        "MavenAnt",
        "Machine",
        "Learning",
        "Classification",
        "Regression",
        "KNN",
        "Model",
        "Decision",
        "Tree",
        "Model",
        "Nave",
        "Bayes",
        "Model",
        "Logistic",
        "Regression",
        "SVM",
        "Model",
        "Latent",
        "Factor",
        "Model",
        "Experience",
        "knowledge",
        "clusters",
        "AWS",
        "cloud",
        "services",
        "EC2",
        "S3",
        "EMR",
        "knowledge",
        "Microsoft",
        "Azure",
        "Knowledge",
        "understanding",
        "DevopsDockers",
        "Experience",
        "Sub",
        "Queries",
        "Stored",
        "Procedures",
        "Triggers",
        "Cursors",
        "Functions",
        "MySQL",
        "PostgreSQL",
        "database",
        "Understanding",
        "Python",
        "Best",
        "Practices",
        "PEP8",
        "package",
        "management",
        "system",
        "PIP",
        "Python",
        "experience",
        "Data",
        "visualization",
        "tools",
        "Tableau",
        "9X",
        "dashboards",
        "Experience",
        "development",
        "designing",
        "ETL",
        "methodology",
        "data",
        "transformations",
        "processing",
        "environment",
        "Teradata",
        "Mainframes",
        "UNIX",
        "Shell",
        "Scripting",
        "SQL",
        "Queries",
        "Stored",
        "Procedures",
        "contents",
        "MySQL",
        "Good",
        "SQL",
        "tuning",
        "techniques",
        "Join",
        "Indexes",
        "JI",
        "Aggregate",
        "Join",
        "Indexes",
        "AJIs",
        "Statistics",
        "Table",
        "changes",
        "Index",
        "SQL",
        "loader",
        "load",
        "data",
        "file",
        "database",
        "tables",
        "Experience",
        "development",
        "TSQL",
        "OLAP",
        "PLSQL",
        "Stored",
        "Procedures",
        "Triggers",
        "Functions",
        "Packages",
        "performance",
        "tuning",
        "optimization",
        "business",
        "logic",
        "implementation",
        "Strong",
        "SQL",
        "Server",
        "programming",
        "skills",
        "experience",
        "functions",
        "packages",
        "processes",
        "SAS",
        "Base",
        "SAS",
        "SQL",
        "SAS",
        "SASGraph",
        "Merge",
        "Join",
        "Set",
        "statements",
        "SAS",
        "ODS",
        "Good",
        "industry",
        "knowledge",
        "problem",
        "skills",
        "ability",
        "team",
        "team",
        "player",
        "ability",
        "Experience",
        "data",
        "microbiome",
        "US",
        "employer",
        "Work",
        "Experience",
        "Data",
        "Scientist",
        "LOreal",
        "USA",
        "Berkeley",
        "Heights",
        "NJ",
        "September",
        "Present",
        "Python",
        "Description",
        "LOreal",
        "Group",
        "worlds",
        "cosmetics",
        "Beauty",
        "company",
        "Online",
        "Product",
        "Trading",
        "B2B",
        "B2C",
        "dealers",
        "distributors",
        "manufacturers",
        "fleet",
        "owners",
        "traders",
        "world",
        "trade",
        "way",
        "Buyers",
        "sellers",
        "sellers",
        "buyers",
        "application",
        "customers",
        "manufacturers",
        "Customer",
        "choice",
        "LOreal",
        "Product",
        "specifications",
        "matching",
        "choice",
        "manufacturers",
        "choice",
        "product",
        "LOreal",
        "product",
        "Trader",
        "Responsibilities",
        "Communicated",
        "departments",
        "business",
        "requirements",
        "data",
        "data",
        "sources",
        "datasets",
        "analysis",
        "installation",
        "SASEBI",
        "Linux",
        "platform",
        "Data",
        "Modeling",
        "tools",
        "Erwin",
        "Data",
        "Modeler",
        "data",
        "models",
        "tables",
        "naming",
        "conventions",
        "Logical",
        "Physical",
        "Data",
        "Models",
        "Erwin",
        "development",
        "data",
        "warehouse",
        "data",
        "Lake",
        "ETL",
        "systems",
        "tools",
        "SQL",
        "SQL",
        "SQL",
        "tables",
        "integrity",
        "queries",
        "SQL",
        "SQLPLUS",
        "PLSQL",
        "Design",
        "unit",
        "testing",
        "ETL",
        "package",
        "source",
        "marts",
        "marts",
        "Informatica",
        "ETL",
        "processes",
        "Oracle",
        "database",
        "QlikView",
        "Data",
        "Models",
        "data",
        "sources",
        "DB2",
        "Excel",
        "Flat",
        "Files",
        "data",
        "data",
        "data",
        "sources",
        "transformations",
        "Hive",
        "Map",
        "Reduce",
        "data",
        "HDFS",
        "Interaction",
        "Business",
        "Analyst",
        "SMEs",
        "Data",
        "Architects",
        "Business",
        "needs",
        "functionality",
        "project",
        "solutions",
        "process",
        "improvements",
        "handson",
        "technologies",
        "Oracle",
        "Informatica",
        "business",
        "data",
        "cleaning",
        "data",
        "quality",
        "consistency",
        "integrity",
        "Pandas",
        "NumPy",
        "feature",
        "engineering",
        "feature",
        "intersection",
        "feature",
        "normalize",
        "label",
        "encoding",
        "Scikitlearn",
        "fraud",
        "prediction",
        "performance",
        "forest",
        "gradient",
        "feature",
        "selection",
        "Python",
        "Scikitlearn",
        "Python",
        "NumPy",
        "Scipy",
        "Pandas",
        "ScikitLearn",
        "Spark",
        "PySpark",
        "MLlib",
        "variety",
        "models",
        "algorithms",
        "purposes",
        "spark",
        "Scala",
        "Hadoop",
        "HBase",
        "Kafka",
        "Spark",
        "Streaming",
        "MLlib",
        "Python",
        "variety",
        "machine",
        "learning",
        "methods",
        "classifications",
        "regressions",
        "reduction",
        "model",
        "AWS",
        "EC2",
        "algorithm",
        "parameters",
        "Setup",
        "storage",
        "data",
        "analysis",
        "tools",
        "Amazon",
        "Web",
        "Services",
        "cloud",
        "infrastructure",
        "machine",
        "learning",
        "models",
        "Apache",
        "Spark",
        "MLlib",
        "NLTK",
        "Python",
        "machine",
        "learning",
        "algorithms",
        "learning",
        "algorithms",
        "Artificial",
        "Neural",
        "network",
        "ANN",
        "Recurrent",
        "Neural",
        "Network",
        "RNN",
        "hyperparameter",
        "models",
        "Python",
        "packages",
        "TensorFlow",
        "Caffe",
        "Deep",
        "Learning",
        "Framework",
        "Modified",
        "machine",
        "learning",
        "models",
        "data",
        "Spark",
        "PySpark",
        "architect",
        "cloud",
        "Hadoop",
        "architecture",
        "Research",
        "Worked",
        "formats",
        "JSON",
        "XML",
        "machine",
        "algorithms",
        "Python",
        "phases",
        "data",
        "collection",
        "data",
        "models",
        "validation",
        "visualization",
        "analysis",
        "Data",
        "Architects",
        "DBA",
        "team",
        "data",
        "model",
        "changes",
        "database",
        "environments",
        "Pandas",
        "library",
        "Analysis",
        "results",
        "operations",
        "team",
        "decisions",
        "data",
        "needs",
        "requirements",
        "departments",
        "Environment",
        "Python",
        "hive",
        "oozie",
        "Tableau",
        "Informatica",
        "HTML5",
        "CSS",
        "XML",
        "MySQL",
        "MS",
        "SQL",
        "Server",
        "JavaScript",
        "AWS",
        "S3",
        "EC2",
        "Linux",
        "Jupyter",
        "Notebook",
        "RNN",
        "ANN",
        "Spark",
        "Hadoop",
        "Data",
        "Analyst",
        "CVS",
        "Health",
        "Boston",
        "MA",
        "September",
        "August",
        "Description",
        "pharmacy",
        "chain",
        "client",
        "pharmacy",
        "prescription",
        "drugs",
        "assortment",
        "merchandise",
        "drugs",
        "beauty",
        "products",
        "cosmetics",
        "film",
        "photo",
        "services",
        "goal",
        "project",
        "field",
        "data",
        "mining",
        "solutions",
        "impact",
        "Patients",
        "Janssen",
        "Responsibilities",
        "market",
        "analysis",
        "positioning",
        "product",
        "feasibility",
        "research",
        "development",
        "designing",
        "sample",
        "methodologies",
        "data",
        "pricing",
        "clients",
        "products",
        "database",
        "engineers",
        "ETL",
        "process",
        "SQL",
        "queries",
        "data",
        "extraction",
        "SQL",
        "server",
        "database",
        "Business",
        "forecasting",
        "segmentation",
        "analysis",
        "Data",
        "mining",
        "Developed",
        "Machine",
        "Learning",
        "algorithm",
        "blood",
        "loss",
        "graphs",
        "reports",
        "package",
        "RStudio",
        "models",
        "R",
        "application",
        "machine",
        "business",
        "forecasting",
        "models",
        "Decision",
        "Tree",
        "Random",
        "Forest",
        "Nave",
        "Bayes",
        "Performed",
        "time",
        "series",
        "analysis",
        "Tableau",
        "workbooks",
        "Tableau",
        "data",
        "sources",
        "dashboards",
        "visualizations",
        "Tableau",
        "desktop",
        "Alteryx",
        "data",
        "analysis",
        "JMP",
        "Perform",
        "validation",
        "machine",
        "output",
        "R",
        "Written",
        "connectors",
        "data",
        "databases",
        "Environment",
        "R",
        "Python",
        "Excel",
        "Machine",
        "Learning",
        "Tableau",
        "Quick",
        "View",
        "JMP",
        "Segmentation",
        "analysis",
        "Data",
        "Scientist",
        "Cellular",
        "Connection",
        "July",
        "August",
        "Description",
        "TCC",
        "Premium",
        "Wireless",
        "Retailer",
        "company",
        "Data",
        "scientist",
        "analyses",
        "models",
        "datasets",
        "business",
        "problems",
        "Benefits",
        "Benefit",
        "operations",
        "modeling",
        "operations",
        "research",
        "machine",
        "learning",
        "data",
        "mining",
        "techniques",
        "Responsibilities",
        "Data",
        "Profiling",
        "user",
        "behavior",
        "data",
        "data",
        "sources",
        "data",
        "processing",
        "applications",
        "normalization",
        "volumes",
        "data",
        "Hadoop",
        "ecosystems",
        "PIG",
        "Hive",
        "HBase",
        "prototype",
        "Data",
        "Mart",
        "outcome",
        "enduser",
        "Worked",
        "Analyst",
        "Data",
        "Models",
        "Erwin",
        "database",
        "system",
        "Designing",
        "machine",
        "frameworks",
        "Python",
        "R",
        "MATLAB",
        "datasets",
        "data",
        "points",
        "TB",
        "datasets",
        "data",
        "association",
        "pairing",
        "insights",
        "data",
        "association",
        "trends",
        "phases",
        "data",
        "collection",
        "data",
        "models",
        "validation",
        "visualization",
        "analysis",
        "knowledge",
        "Hadoop",
        "Architecture",
        "components",
        "HDFS",
        "Job",
        "Tracker",
        "Task",
        "Tracker",
        "Name",
        "Node",
        "Data",
        "Node",
        "Secondary",
        "Name",
        "Node",
        "MapReduce",
        "concepts",
        "data",
        "data",
        "sources",
        "transformations",
        "Hive",
        "MapReduce",
        "data",
        "HDFS",
        "Collaborate",
        "data",
        "engineers",
        "ETL",
        "process",
        "SQL",
        "queries",
        "data",
        "extraction",
        "Cloud",
        "Oracle",
        "data",
        "MongoDB",
        "data",
        "aggregation",
        "analysis",
        "customer",
        "behaviors",
        "value",
        "customers",
        "RMF",
        "analysis",
        "customer",
        "segmentation",
        "algorithms",
        "KMeans",
        "Clustering",
        "Clustering",
        "Participate",
        "features",
        "engineering",
        "feature",
        "intersection",
        "feature",
        "normalize",
        "Label",
        "encoding",
        "Scikitlearn",
        "preprocessing",
        "pandas",
        "NumPy",
        "Scipy",
        "Matplotlib",
        "SKLearn",
        "NLTK",
        "Natural",
        "Language",
        "Toolkit",
        "Python",
        "machine",
        "learning",
        "machine",
        "learning",
        "algorithms",
        "Decision",
        "Tree",
        "linear",
        "regression",
        "multivariate",
        "regression",
        "Naive",
        "Bayes",
        "Random",
        "Forests",
        "Kmeans",
        "KNN",
        "data",
        "conclusions",
        "data",
        "format",
        "Determine",
        "customer",
        "satisfaction",
        "customer",
        "experience",
        "NLP",
        "QlikView",
        "Data",
        "Models",
        "data",
        "sources",
        "DB2",
        "Excel",
        "Flat",
        "Files",
        "data",
        "Perform",
        "data",
        "integrity",
        "data",
        "analysis",
        "feature",
        "engineer",
        "R",
        "data",
        "formats",
        "JSON",
        "XML",
        "machine",
        "algorithms",
        "R",
        "MapReduceSpark",
        "Python",
        "modules",
        "machine",
        "analytics",
        "Hadoop",
        "Perform",
        "data",
        "visualizations",
        "Tableau",
        "dashboards",
        "findings",
        "Text",
        "Analytics",
        "Nave",
        "Bayes",
        "Sentiment",
        "analysis",
        "word",
        "clouds",
        "data",
        "Twitter",
        "networking",
        "platforms",
        "Git26",
        "version",
        "control",
        "changes",
        "files",
        "work",
        "files",
        "team",
        "members",
        "Environment",
        "Python",
        "hive",
        "Tableau",
        "R",
        "QlikView",
        "MySQL",
        "MS",
        "SQL",
        "Server",
        "AWS",
        "S3",
        "EC2",
        "Linux",
        "Jupyter",
        "Notebook",
        "RNN",
        "ANN",
        "Spark",
        "Hadoop",
        "Data",
        "Analyst",
        "Python",
        "PIMCO",
        "Manhattan",
        "NY",
        "May",
        "June",
        "Description",
        "Pacific",
        "Investment",
        "Management",
        "Company",
        "investment",
        "management",
        "firm",
        "funds",
        "portfolio",
        "management",
        "asset",
        "allocation",
        "solutions",
        "millions",
        "investors",
        "models",
        "algorithms",
        "classify",
        "quantify",
        "forecast",
        "business",
        "metrics",
        "Partner",
        "business",
        "units",
        "Analytics",
        "colleagues",
        "workgroup",
        "processes",
        "success",
        "metrics",
        "Responsibilities",
        "python",
        "libraries",
        "Soap",
        "NumPy",
        "types",
        "data",
        "visualizations",
        "Python",
        "Tableau",
        "Monitoring",
        "tracking",
        "process",
        "performance",
        "analytics",
        "tools",
        "Tableau",
        "dashboard",
        "R",
        "Python",
        "modules",
        "csv",
        "robot",
        "parser",
        "iterators",
        "pickle",
        "development",
        "views",
        "Tableau",
        "Desktop",
        "team",
        "review",
        "data",
        "analysis",
        "customization",
        "filters",
        "actions",
        "Python",
        "OpenStack",
        "APIs",
        "NumPy",
        "analysis",
        "Python",
        "scripts",
        "content",
        "database",
        "manipulate",
        "files",
        "Python",
        "graphics",
        "data",
        "exchange",
        "business",
        "logic",
        "implementation",
        "Performed",
        "troubleshooting",
        "Python",
        "bug",
        "fixes",
        "applications",
        "tuning",
        "processes",
        "advance",
        "patterns",
        "methodologies",
        "collections",
        "Python",
        "user",
        "objects",
        "python",
        "packages",
        "pip",
        "install",
        "Environment",
        "Python",
        "Tableau",
        "R",
        "Windows",
        "XP",
        "UNIX",
        "HTML",
        "SQL",
        "server",
        "SQL",
        "Developer",
        "Johnson",
        "Johnson",
        "Bengaluru",
        "Karnataka",
        "May",
        "Responsibilities",
        "DDL",
        "DML",
        "triggers",
        "procedures",
        "data",
        "manipulation",
        "Team",
        "Analysis",
        "Design",
        "Develop",
        "database",
        "ER",
        "Diagram",
        "Design",
        "Development",
        "testing",
        "system",
        "SQL",
        "Server",
        "Stored",
        "Procedures",
        "SQL",
        "Queries",
        "Indexes",
        "Created",
        "Views",
        "user",
        "interface",
        "implementation",
        "Triggers",
        "data",
        "entry",
        "database",
        "Exceptional",
        "Handling",
        "Worked",
        "client",
        "requirement",
        "Complex",
        "SQL",
        "Queries",
        "Crystal",
        "Reports",
        "Data",
        "sources",
        "Datasets",
        "reports",
        "SQL",
        "Queries",
        "Execution",
        "Plan",
        "Profiler",
        "Rebuilding",
        "Indexes",
        "Tables",
        "part",
        "Performance",
        "Exercise",
        "database",
        "Backup",
        "Recovery",
        "end",
        "user",
        "requirements",
        "SSRS",
        "Reports",
        "database",
        "design",
        "Education",
        "Bachelors",
        "Skills",
        "Python",
        "hive",
        "oozie",
        "Tableau",
        "Informatica",
        "HTML5",
        "CSS",
        "XML",
        "MySQL",
        "MS",
        "SQL",
        "Server",
        "JavaScript",
        "AWS",
        "S3",
        "EC2",
        "Linux",
        "Jupyter",
        "Notebook",
        "RNN",
        "ANN",
        "Spark",
        "Hadoop",
        "years",
        "Information",
        "SKILLS",
        "MATRIX",
        "Languages",
        "C",
        "C",
        "XML",
        "RR",
        "Studio",
        "SAS",
        "Enterprise",
        "Guide",
        "SAS",
        "R",
        "Python",
        "2x3x",
        "Java",
        "C",
        "SQL",
        "Shell",
        "Scripting",
        "SQL",
        "Databases",
        "Cassandra",
        "HBase",
        "MongoDB",
        "Maria",
        "DB",
        "Statistics",
        "Hypothetical",
        "Testing",
        "ANOVA",
        "Confidence",
        "Intervals",
        "Bayes",
        "Law",
        "MLE",
        "Fish",
        "Information",
        "Principal",
        "Component",
        "Analysis",
        "PCA",
        "CrossValidation",
        "correlation",
        "BI",
        "Tools",
        "Tableau",
        "Tableau",
        "server",
        "Tableau",
        "Reader",
        "Splunk",
        "SAP",
        "Business",
        "OBIEE",
        "SAP",
        "Business",
        "Intelligence",
        "QlikView",
        "Amazon",
        "Redshift",
        "Azure",
        "Data",
        "Warehouse",
        "Algorithms",
        "regression",
        "forest",
        "XG",
        "Boost",
        "KNN",
        "SVM",
        "network",
        "rk",
        "regression",
        "lasso",
        "regression",
        "kmeans",
        "Big",
        "Data",
        "Hadoop",
        "HDFS",
        "HIVE",
        "Spark",
        "Scala",
        "Sqoop",
        "Reporting",
        "Tools",
        "MS",
        "Office",
        "VisioOutlook",
        "Crystal",
        "Reports",
        "XI",
        "SSRS",
        "Cognos",
        "Database",
        "Design",
        "Tools",
        "Data",
        "Modeling",
        "MS",
        "Visio",
        "ERWIN",
        "Star",
        "SchemaSnowflake",
        "Schema",
        "Fact",
        "Dimensions",
        "data",
        "Normalization",
        "Denormalization",
        "Kimball",
        "Inmon",
        "Methodologies"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T22:53:55.439970",
    "resume_data": "Data Scientist Data Scientist Data Scientist LOreal USA Berkeley Heights NJ Data ScientistData Analyst around 8 years of Experience in Data Science and Analytics including Data Mining Statistical Analysis with domain knowledge in Retail Healthcare and Banking industries Involved in Data Science project life cycle including Data Cleaning Data extraction Visualization with large data sets of structured and unstructured data created ER diagrams and schema Experience with Machine Learning algorithms such as logistic regression KNN SVM random forest neural network linear regression lasso regression and kmeans Good experience in Text Analytics developing different Statistical Machine Learning Data Mining solutions to various business problems and generating data visualizations using R Python and Tableau Experience in implementing data analysis with various analytic tools such as Anaconda 40 Jupiter Notebook 4X R 30 ggplot2 dplyr Caret and Excel Experienced the full software lifecycle in SDLC Agile DevOps and Scrum methodologies including creating requirements test plans Strong skills in statistical methodologies such as AB test experiment design hypothesis test ANOVA Good Knowledge and experience in deep learning algorithms such as Artificial Neural network ANN Convolutional Neural Network CNN and Recurrent Neural Network RNN LSTM and RNN based speech recognition using TensorFlow Working Experience on Python 3527 such as NumPy SQLAlchemy Beautiful soup pickle Pyside Pymongo SciPy PyTables Ability to write and optimize diverse SQL queries working knowledge of RDBMS like SQL Server 2008 NoSQL databases like MongoDB 32 Experience in Big Data technologies like Spark 16 Spark SQL PySpark Hadoop 2X HDFS Hive 1X Experience in Data Warehousing including Data Modeling Data Architecture Data Integration ETLELT and Business Intelligence Good Experience in using various Python libraries Beautiful Soup NumPy Scipy matplotlib pythontwitter Pandas MySQL dB for database connectivity Having experienced in Big Data technologies including Apache Spark HDFS Hive MongoDB Used the version control tools like Git2X and build tools like Apache MavenAnt Worked on Machine Learning algorithms like Classification and Regression with KNN Model Decision Tree Model Nave Bayes Model Logistic Regression SVM Model and Latent Factor Model Experience and knowledge in provisioning virtual clusters under AWS cloud which includes services like EC2 S3 and EMR Good knowledge on Microsoft Azure Knowledge and understanding of DevopsDockers Experience in writing Sub Queries Stored Procedures Triggers Cursors and Functions on MySQL and PostgreSQL database Understanding of Python Best Practices PEP8 and package management system PIP in Python Extensive experience in Data visualization tools like Tableau 9X 10X for creating dashboards Experience in development and designing of ETL methodology for supporting data transformations and processing in a corporatewide environment using Teradata Mainframes and UNIX Shell Scripting Used SQL Queries and Stored Procedures extensively in retrieving the contents from MySQL Good in implementing SQL tuning techniques such as Join Indexes JI Aggregate Join Indexes AJIs Statistics and Table changes including Index SQL loader for direct and parallel load of data from raw file to database tables Experience in development of TSQL OLAP PLSQL Stored Procedures Triggers Functions Packages performance tuning and optimization for business logic implementation Strong SQL Server programming skills with experience in working with functions packages and triggers Good at handling complex processes using SAS Base SAS SQL SAS STAT SASGraph Merge Join and Set statements SAS ODS Good industry knowledge analytical problem solving skills and ability to work well with in a team as well as an individual Great team player and ability to work collaboratively and independently as required Experience with Biological data sets genomic transcriptomic microbiome etc Authorized to work in the US for any employer Work Experience Data Scientist LOreal USA Berkeley Heights NJ September 2017 to Present Python Description LOreal Group is one of the worlds largest cosmetics and Beauty company Online Product Trading B2B B2C brings dealers distributors manufacturers fleet owners and traders from all around the world together and facilitates trade in an easy secure transparent and cost efficient way Buyers can find sellers and sellers can find the best possible buyers This application can be used both by customers and manufacturers Customer can browse and select their choice of LOreal Product or put their specifications if not getting the matching choice so the manufacturers can make their choice of product and delivered to them by LOreal product Trader Responsibilities Communicated and coordinated with other departments to gather business requirements Gathering all the data that is required from multiple data sources and creating datasets that will be used in analysis Participated in the installation of SASEBI on Linux platform worked on Data Modeling tools Erwin Data Modeler to design the data models Designed tables and implemented the naming conventions for Logical and Physical Data Models in Erwin 70 Worked on development of data warehouse data Lake and ETL systems using relational and nonrelational tools like SQL No SQL Created SQL tables with referential integrity and developed queries using SQL SQLPLUS and PLSQL Design coding unit testing of ETL package source marts and subject marts using Informatica ETL processes for Oracle database Developed various QlikView Data Models by extracting and using the data from various sources files DB2 Excel Flat Files and Big data Handled importing data from various data sources performed transformations using Hive Map Reduce and loaded data into HDFS Interaction with Business Analyst SMEs and other Data Architects to understand Business needs and functionality for various project solutions Identifying and executing process improvements handson in various technologies such as Oracle Informatica business Objects Worked on data cleaning and ensured data quality consistency integrity using Pandas NumPy Participated in feature engineering such as feature intersection generating feature normalize and label encoding with Scikitlearn preprocessing Improved fraud prediction performance by using random forest and gradient boosting for feature selection with Python Scikitlearn Used Python NumPy Scipy Pandas ScikitLearn Seaborn and Spark 20 PySpark MLlib to develop variety of models and algorithms for analytic purposes Utilized spark Scala Hadoop HBase Kafka Spark Streaming MLlib Python a broad variety of machine learning methods including classifications regressions dimensionally reduction etc Implemented tuned and tested the model on AWS EC2 to get the best algorithm and parameters Setup storage and data analysis tools in Amazon Web Services cloud computing infrastructure Designed and developed machine learning models in Apache Spark MLlib Used NLTK in Python for developing various machine learning algorithms Implemented deep learning algorithms such as Artificial Neural network ANN and Recurrent Neural Network RNN tuned hyperparameter and improved models with Python packages TensorFlow Installed and used Caffe Deep Learning Framework Modified selected machine learning models with realtime data in in Spark PySpark Worked with architect to improve cloud Hadoop architecture as needed for Research Worked on different formats such as JSON XML and performed machine learning algorithms in Python Participated in all phases of datamining data collection data cleaning developing models validation visualization and performed Gap analysis Worked very close with Data Architects and DBA team to implement data model changes in the database in all environments Used Pandas library for statistical Analysis Communicated the results with operations team for taking best decisions Collected data needs and requirements by Interacting with the other departments Environment Python 3227 hive oozie Tableau Informatica 90 HTML5 CSS XML MySQL MS SQL Server 20082012 JavaScript AWS S3 EC2 Linux Jupyter Notebook RNN ANN Spark Hadoop Data Analyst CVS Health Boston MA September 2015 to August 2017 Description Worked with pharmacy chain client As the retail pharmacy it sells prescription drugs and a wide assortment of general merchandise including overthecounter drugs beauty products and cosmetics film and photo finishing services our goal of the project was to design develop and field data mining solutions that have direct impact to Patients and Janssen Responsibilities Investigated market sizing competitive analysis and positioning for product feasibility Conducted research on development and designing of sample methodologies and analyzed data for pricing of clients products Collaborated with database engineers to implement ETL process wrote and optimized SQL queries to perform data extraction and merging from SQL server database Worked on Business forecasting segmentation analysis and Data mining Developed Machine Learning algorithm to diagnose blood loss Generated graphs and reports using ggplot2 package in RStudio for analytical models Developed and implemented R and Shiny application which showcases machine learning for business forecasting Developed predictive models using Decision Tree Random Forest and Nave Bayes Performed time series analysis using Tableau Developed various workbooks in Tableau from multiple data sources Created dashboards and visualizations using Tableau desktop Later used Alteryx to blend the data Performed analysis using JMP Perform validation on machine learning output from R Written connectors to extract data from databases Environment R Python 2x Excel 2010 Machine Learning Tableau Quick View JMP Segmentation analysis Data Scientist The Cellular Connection July 2014 to August 2015 Description TCC is the largest Premium Wireless Retailer company I work with the lead Data scientist to perform statistical analyses and predictive models on datasets to address various business problems in Benefits and Benefit operations through leveraging advanced statistical modeling operations research machine learning or data mining techniques Responsibilities Involved in Data Profiling to learn about user behavior and merge data from multiple data sources Participated in big data processing applications to collect clean and normalization large volumes of open data using Hadoop ecosystems such as PIG Hive and HBase Designed the prototype of the Data Mart and documented possible outcome from it for enduser Worked as Analyst to generate Data Models using Erwin and developed a relational database system Designing and developing various machine learning frameworks using Python R and MATLAB Processed huge datasets over billion data points over 1 TB of datasets for data association pairing and provided insights into meaningful data association and trends Participated in all phases of data collection data cleaning developing models validation and visualization and performed Gap analysis Good knowledge of Hadoop Architecture and various components such as HDFS Job Tracker Task Tracker Name Node Data Node Secondary Name Node and MapReduce concepts Handled importing data from various data sources performed transformations using Hive MapReduce and loaded data into HDFS Collaborate with data engineers to implement ETL process write and optimized SQL queries to perform data extraction from Cloud and merging from Oracle 12c Collect unstructured data from MongoDB 33 and completed data aggregation Conducted analysis of assessing customer consuming behaviors and discover the value of customers with RMF analysis applied customer segmentation with clustering algorithms such as KMeans Clustering and Hierarchical Clustering Participate in features engineering such as feature intersection generating feature normalize and Label encoding with Scikitlearn preprocessing Used pandas NumPy Seaborn Scipy Matplotlib SKLearn and NLTK Natural Language Toolkit in Python for developing various machine learning algorithms Utilized machine learning algorithms such as Decision Tree linear regression multivariate regression Naive Bayes Random Forests Kmeans KNN Parsing data producing concise conclusions from raw data in a clean wellstructured and easily maintainable format Determine customer satisfaction and help enhance customer experience using NLP Developed various QlikView Data Models by extracting and using the data from various sources files DB2 Excel Flat Files and Big data Perform data integrity checks data cleaning exploratory analysis and feature engineer using R 340 Worked on different data formats such as JSON XML and performed machine learning algorithms in R Worked on MapReduceSpark Python modules for machine learning predictive analytics in Hadoop Perform data visualizations with Tableau 10 and generated dashboards to present the findings Work on Text Analytics Nave Bayes Sentiment analysis creating word clouds and retrieving data from Twitter and other social networking platforms Use Git26 to apply version control Tracked changes in files and coordinated work on the files among multiple team members Environment Python 3227 hive Tableau R QlikView MySQL MS SQL Server 20082012 AWS S3 EC2 Linux Jupyter Notebook RNN ANN Spark Hadoop Data Analyst Python PIMCO Manhattan NY May 2012 to June 2014 Description Pacific Investment Management Company is an American investment management firm provides mutual funds and other portfolio management and asset allocation solutions for millions of investors worldwide I was involving in developing statistical models and algorithms to predict classify quantify andor forecast business metrics Partner with business units and Analytics colleagues outside the workgroup to simulate and validate processes to maximize success metrics Responsibilities Used python libraries like Beautiful Soap NumPy Created various types of data visualizations using Python and Tableau Monitoring and tracking process performance using analytics tools like Tableau dashboard R Utilized standard Python modules such as csv robot parser iterators and pickle for development Created views in Tableau Desktop that were published to internal team for review and further data analysis and customization using filters and actions Worked on Python OpenStack APIs and used NumPy for Numerical analysis Used Python scripts to update content in the database and manipulate files Used Python creating graphics data exchange and business logic implementation Performed troubleshooting fixed and deployed many Python bug fixes of the applications and involved in fine tuning of existing processes followed advance patterns and methodologies Skilled in using collections in Python for manipulating and looping through different user defined objects Installed numerous python packages using pip and easy install Environment Python 27 Tableau R Windows XP UNIX HTML SQL server 2005 SQL Developer Johnson Johnson Bengaluru Karnataka May 2011 to 2012 Responsibilities Used DDL and DML for writing triggers stored procedures and data manipulation Interacted with Team and Analysis Design and Develop database using ER Diagram involved in Design Development and testing of the system Developed SQL Server Stored Procedures Tuned SQL Queries using Indexes Created Views to facilitate easy user interface implementation and Triggers on them to facilitate consistent data entry into the database Implemented Exceptional Handling Worked on client requirement and wrote Complex SQL Queries to generate Crystal Reports Created different Data sources and Datasets for the reports Tuned and Optimized SQL Queries using Execution Plan and Profiler Rebuilding Indexes and Tables as part of Performance Tuning Exercise Involved in performing database Backup and Recovery Documented end user requirements for SSRS Reports and database design Education Bachelors Skills Python 3227 hive oozie Tableau Informatica 90 HTML5 CSS XML MySQL MS SQL Server 20082012 JavaScript AWS S3 EC2 Linux Jupyter Notebook RNN ANN Spark Hadoop 8 years Additional Information SKILLS MATRIX Languages C C XML RR Studio SAS Enterprise Guide SAS R Python 2x3x Java C SQL Shell Scripting NO SQL Databases Cassandra HBase MongoDB Maria DB Statistics Hypothetical Testing ANOVA Confidence Intervals Bayes Law MLE Fish Information Principal Component Analysis PCA CrossValidation correlation BI Tools Tableau Tableau server Tableau Reader Splunk SAP Business Objects OBIEE SAP Business Intelligence QlikView Amazon Redshift or Azure Data Warehouse Algorithms Logistic regression random forest XG Boost KNN SVM neural network rk linear regression lasso regression kmeans Big Data Hadoop HDFS HIVE PuTTy Spark Scala Sqoop Reporting Tools MS Office WordExcelPowerPoint VisioOutlook Crystal Reports XI SSRS Cognos 7060 Database Design Tools and Data Modeling MS Visio ERWIN 4540 Star SchemaSnowflake Schema modeling Fact Dimensions tables physical logical data modeling Normalization and Denormalization techniques Kimball Inmon Methodologies",
    "unique_id": "e2aa9b13-3cf4-4936-a772-22721bd502c3"
}