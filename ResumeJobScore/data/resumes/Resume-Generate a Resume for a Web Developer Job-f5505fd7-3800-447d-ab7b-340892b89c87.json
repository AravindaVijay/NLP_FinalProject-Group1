{
    "clean_data": "Big Data Developer Big Data span lDeveloperspan Big Data Developer Charlotte NC Over 8 years of professional IT experience which includes experience in Big data ecosystem Excellent Experience in Hadoop architecture and various components such as HDFS Job Tracker Task Tracker NameNode Data Node and MapReduce programming paradigm Have sound exposure to Retail market including Retail Delivery System Hands on experience in installing configuring and using Hadoop ecosystem components like Hadoop MapReduce HDFS HBase Hive Sqoop Pig Zookeeper and Flume Good Exposure on Apache Hadoop Map Reduce programming PIG Scripting and Distribute Application and HDFS Good Knowledge on Hadoop Cluster architecture and monitoring the cluster Indepth understanding of Data Structure and Algorithms Experience in managing and reviewing Hadoop log files Excellent understanding and knowledge of NOSQL databases like MongoDB HBase Cassandra Implemented in setting up standards and processes for Hadoop based application design and implementation Experience in importing and exporting data using Sqoop from HDFS to Relational Database Systems and viceversa Experience in Object Oriented Analysis Design OOAD and development of software using UML Methodology good knowledge of J2EE design patterns and Core Java design patterns Experience in managing Hadoop clusters using Cloudera Manager tool Very good experience in complete project life cycle design development testing and implementation of Client Server and Web applications Experience in Administering Installation configuration troubleshooting Security Backup Performance Monitoring and Finetuning of Linux Redhat Extensive experience working in Oracle DB2 SQL Server and My SQL database Hands on experience in VPN Putty WinSCP VNCviewer etc Scripting to deploy monitors checks and critical system admin functions automation Hands on experience in application development using Java RDBMS and Linux shell scripting Experience in Java JSP Servlets EJB WebLogic WebSphere Hibernate Spring JBoss JDBC RMI Java Script Ajax Jquery XML and HTML Ability to adapt to evolving technology strong sense of responsibility and accomplishment Authorized to work in the US for any employer Work Experience Big Data Developer MorganStanleyCapgemini Charlotte NC March 2018 to July 2019 Launched 5 Node Mapr Cluster 52 with 80 Cores 600G Ram on AWS to implement a data recommendation engine Installed Spark Hive Oozie Rstudio Zeepline on to Mapr Hadoop Cluster Wrote Python and Scala api from positecs rdbms to aws s3 and implemented on Spark Execution framework Worked Intensively on Spark Pyspark and  Created Spark Data Frames while loading the data in to Hive Tables Used Kafka Cluster to load IOT data on the Hadoop Cluster and designed a data lake for IOT on Kafka Cluster on Azure Created a VPG connection From Client Network to Hadoop cluster on top of aws Used Sqoop built the data lake From MSSQL TO Hadoop cluster scheduled jobs using crontab oozie Built a data pipeline from FTP Server to Hadoop using python api Created Hive tables in parquet format connected to Rstudio and zeepline through Sparkly JDBC Connection given Infrastructure access to data science team to run their data models on aws cloud Connected Hive tables on Hadoop cluster to visualization tools like PowerBI Tableau using ODBC and mapr drill connector Used s3 as data backup from the hadoop cluster Implemented POC on Launching Hadoop and Spark Cluster on Hdinsights Developed Spark for the recommendation engine and validated using python Environment AWS Spark Python Azure Hadoop Kafka Scala Hive Flume Hbase Sqoop PIG Java JDK 16 Eclipse MySQL Ubuntu Zookeeper Amazon EC2 SOLR Hadoop Developer PositecSyntelli Solutions Charlotee North Carolina March 2016 to February 2018 Created Hive HBase tables using ORC file format and Snappy compression Wrote different Pig scripts to clean up the ingested data and created partitions for the daily data Implemented Partitioning and bucketing for Hive tables based on the requirement Performed Various tasks on data Ingestion from DB2 to the HDFS using Sqoop JDBC Connector Created shell scripts to parameterize the Pig Hive actions in Oozie workflow Designed and implemented Incremental Imports into Hive tables Created different formats of Hive Tables Finally worked on ORC Avro and picked ORC as fit requirements Managed and reviewed the Hadoop log files Provisioning and managing multitenant Cassandra cluster on public cloud environment Created Hive External Managed Tables sorted out the best performance and voted to External Tables Created and maintained Technical documentation for launching Hadoop Clusters and for executing Pig scripts Worked on aggregating the data using advanced aspects of MapReduce Environment Hadoop MapReduce HDFS Hive Java Hadoop distribution of Horton Works Cloudera Pig Hbase Linux XML MySQL Java 6 Eclipse Oracle 10g PLSQL SQL PLUS Hadoop Developer KIOO LIMITED Hyderabad Telangana March 2012 to July 2014 Worked on a live 16 nodes Hadoop cluster running CDH44 Performed Flume Sqoop imports of data from Data warehouse platform to HDFS Continuous monitoring and managing the Hadoop cluster through Cloudera Manager Implemented Data classification algorithms using Map reduce design patterns Expertise in designing and deployment of Hadoop cluster and different Big Data analytic tools including Pig Hive HBase Oozie Sqoop Flume Spark Impala with Cloudera distribution Used Pig to do data transformations event joins filter and some preaggregations before storing the data into HDFS Supported MapReduce Programs those are running on the cluster Extracted the data from Teradata into HDFS using the Sqoop Environment Cloudera Hadoop MapReduce HDFS Hive Sqoop Hbase Linux XML MySQL Java 6 Eclipse Java J2EE Developer MUFINDI LIMITED Hyderabad Telangana April 2009 to March 2012 Developed web components using JSP Servlets and JDBC Designed tables and indexes Designed Implemented Tested and Deployed Enterprise Java Beans both Session and Entity using WebLogic as Application Serve Environment Spring305 Spring batch 217 Web Services SOAP Apache Axis 2 Tomcat v60 Eclipse Indigo 37 Ant 182 Log4j2 CVS ACORD SoapUI 453 DB Visualizer Pro 912 WebSphere MQ 7013 ACORD Visio 2013 BMC Blade Logic Server Admin console putty WinSCP Education Master in Electrical Engineering Computer Science Texas A M University 2014 Bachelors of Engineering in Engineering Electronics and Instrumentation Engineering BITS Pilani Pilani Rajasthan 2008 Skills Big Data Python ETL",
    "entities": [
        "Created Hive",
        "WebSphere",
        "Hadoop Clusters",
        "Created Spark Data Frames",
        "Distribute Application",
        "Created Hive HBase",
        "External Tables Created",
        "HDFS Job Tracker Task Tracker NameNode Data",
        "US",
        "Sqoop",
        "Installed Spark",
        "Cloudera Hadoop MapReduce HDFS Hive Sqoop",
        "NC",
        "Created Hive External Managed Tables",
        "HDFS",
        "IOT",
        "Impala",
        "PIG Scripting and",
        "SOLR Hadoop Developer",
        "Skills Big Data Python",
        "Connected Hive",
        "North Carolina",
        "JSP Servlets",
        "Electrical Engineering Computer Science Texas",
        "Horton Works",
        "Data Structure and Algorithms",
        "WebLogic WebSphere Hibernate",
        "Oracle DB2 SQL Server",
        "Sparkly",
        "Launching Hadoop",
        "Pig Hive",
        "Retail Delivery System Hands",
        "HDFS Supported MapReduce Programs",
        "Spark Cluster",
        "Worked Intensively",
        "ODBC",
        "Data",
        "Indepth",
        "Pig Hive HBase Oozie",
        "MapReduce Environment Hadoop MapReduce HDFS Hive Java Hadoop",
        "Hadoop",
        "Java JSP Servlets",
        "Rstudio",
        "Built",
        "Hadoop MapReduce HDFS HBase Hive",
        "Work Experience Big Data",
        "ORC Avro",
        "MapReduce",
        "NOSQL",
        "Mapr Hadoop Cluster Wrote Python",
        "FTP Server",
        "Environment AWS Spark Python",
        "UML Methodology",
        "ORC",
        "Relational Database Systems",
        "the Hadoop Cluster",
        "HTML Ability",
        "Tableau",
        "WebLogic",
        "Spark Pyspark",
        "Object Oriented Analysis Design OOAD",
        "Administering Installation",
        "Big Data Developer Big Data",
        "MSSQL TO Hadoop",
        "Teradata",
        "Big Data",
        "Spark Execution",
        "Incremental Imports",
        "Security Backup Performance Monitoring and Finetuning of Linux Redhat Extensive",
        "EJB"
    ],
    "experience": "Experience in Hadoop architecture and various components such as HDFS Job Tracker Task Tracker NameNode Data Node and MapReduce programming paradigm Have sound exposure to Retail market including Retail Delivery System Hands on experience in installing configuring and using Hadoop ecosystem components like Hadoop MapReduce HDFS HBase Hive Sqoop Pig Zookeeper and Flume Good Exposure on Apache Hadoop Map Reduce programming PIG Scripting and Distribute Application and HDFS Good Knowledge on Hadoop Cluster architecture and monitoring the cluster Indepth understanding of Data Structure and Algorithms Experience in managing and reviewing Hadoop log files Excellent understanding and knowledge of NOSQL databases like MongoDB HBase Cassandra Implemented in setting up standards and processes for Hadoop based application design and implementation Experience in importing and exporting data using Sqoop from HDFS to Relational Database Systems and viceversa Experience in Object Oriented Analysis Design OOAD and development of software using UML Methodology good knowledge of J2EE design patterns and Core Java design patterns Experience in managing Hadoop clusters using Cloudera Manager tool Very good experience in complete project life cycle design development testing and implementation of Client Server and Web applications Experience in Administering Installation configuration troubleshooting Security Backup Performance Monitoring and Finetuning of Linux Redhat Extensive experience working in Oracle DB2 SQL Server and My SQL database Hands on experience in VPN Putty WinSCP VNCviewer etc Scripting to deploy monitors checks and critical system admin functions automation Hands on experience in application development using Java RDBMS and Linux shell scripting Experience in Java JSP Servlets EJB WebLogic WebSphere Hibernate Spring JBoss JDBC RMI Java Script Ajax Jquery XML and HTML Ability to adapt to evolving technology strong sense of responsibility and accomplishment Authorized to work in the US for any employer Work Experience Big Data Developer MorganStanleyCapgemini Charlotte NC March 2018 to July 2019 Launched 5 Node Mapr Cluster 52 with 80 Cores 600 G Ram on AWS to implement a data recommendation engine Installed Spark Hive Oozie Rstudio Zeepline on to Mapr Hadoop Cluster Wrote Python and Scala api from positecs rdbms to aws s3 and implemented on Spark Execution framework Worked Intensively on Spark Pyspark and   Created Spark Data Frames while loading the data in to Hive Tables Used Kafka Cluster to load IOT data on the Hadoop Cluster and designed a data lake for IOT on Kafka Cluster on Azure Created a VPG connection From Client Network to Hadoop cluster on top of aws Used Sqoop built the data lake From MSSQL TO Hadoop cluster scheduled jobs using crontab oozie Built a data pipeline from FTP Server to Hadoop using python api Created Hive tables in parquet format connected to Rstudio and zeepline through Sparkly JDBC Connection given Infrastructure access to data science team to run their data models on aws cloud Connected Hive tables on Hadoop cluster to visualization tools like PowerBI Tableau using ODBC and mapr drill connector Used s3 as data backup from the hadoop cluster Implemented POC on Launching Hadoop and Spark Cluster on Hdinsights Developed Spark for the recommendation engine and validated using python Environment AWS Spark Python Azure Hadoop Kafka Scala Hive Flume Hbase Sqoop PIG Java JDK 16 Eclipse MySQL Ubuntu Zookeeper Amazon EC2 SOLR Hadoop Developer PositecSyntelli Solutions Charlotee North Carolina March 2016 to February 2018 Created Hive HBase tables using ORC file format and Snappy compression Wrote different Pig scripts to clean up the ingested data and created partitions for the daily data Implemented Partitioning and bucketing for Hive tables based on the requirement Performed Various tasks on data Ingestion from DB2 to the HDFS using Sqoop JDBC Connector Created shell scripts to parameterize the Pig Hive actions in Oozie workflow Designed and implemented Incremental Imports into Hive tables Created different formats of Hive Tables Finally worked on ORC Avro and picked ORC as fit requirements Managed and reviewed the Hadoop log files Provisioning and managing multitenant Cassandra cluster on public cloud environment Created Hive External Managed Tables sorted out the best performance and voted to External Tables Created and maintained Technical documentation for launching Hadoop Clusters and for executing Pig scripts Worked on aggregating the data using advanced aspects of MapReduce Environment Hadoop MapReduce HDFS Hive Java Hadoop distribution of Horton Works Cloudera Pig Hbase Linux XML MySQL Java 6 Eclipse Oracle 10 g PLSQL SQL PLUS Hadoop Developer KIOO LIMITED Hyderabad Telangana March 2012 to July 2014 Worked on a live 16 nodes Hadoop cluster running CDH44 Performed Flume Sqoop imports of data from Data warehouse platform to HDFS Continuous monitoring and managing the Hadoop cluster through Cloudera Manager Implemented Data classification algorithms using Map reduce design patterns Expertise in designing and deployment of Hadoop cluster and different Big Data analytic tools including Pig Hive HBase Oozie Sqoop Flume Spark Impala with Cloudera distribution Used Pig to do data transformations event joins filter and some preaggregations before storing the data into HDFS Supported MapReduce Programs those are running on the cluster Extracted the data from Teradata into HDFS using the Sqoop Environment Cloudera Hadoop MapReduce HDFS Hive Sqoop Hbase Linux XML MySQL Java 6 Eclipse Java J2EE Developer MUFINDI LIMITED Hyderabad Telangana April 2009 to March 2012 Developed web components using JSP Servlets and JDBC Designed tables and indexes Designed Implemented Tested and Deployed Enterprise Java Beans both Session and Entity using WebLogic as Application Serve Environment Spring305 Spring batch 217 Web Services SOAP Apache Axis 2 Tomcat v60 Eclipse Indigo 37 Ant 182 Log4j2 CVS ACORD SoapUI 453 DB Visualizer Pro 912 WebSphere MQ 7013 ACORD Visio 2013 BMC Blade Logic Server Admin console putty WinSCP Education Master in Electrical Engineering Computer Science Texas A M University 2014 Bachelors of Engineering in Engineering Electronics and Instrumentation Engineering BITS Pilani Pilani Rajasthan 2008 Skills Big Data Python ETL",
    "extracted_keywords": [
        "Data",
        "Developer",
        "Big",
        "Data",
        "span",
        "lDeveloperspan",
        "Big",
        "Data",
        "Developer",
        "Charlotte",
        "NC",
        "years",
        "IT",
        "experience",
        "experience",
        "data",
        "ecosystem",
        "Excellent",
        "Experience",
        "Hadoop",
        "architecture",
        "components",
        "HDFS",
        "Job",
        "Tracker",
        "Task",
        "Tracker",
        "NameNode",
        "Data",
        "Node",
        "MapReduce",
        "programming",
        "paradigm",
        "exposure",
        "market",
        "Retail",
        "Delivery",
        "System",
        "Hands",
        "experience",
        "configuring",
        "Hadoop",
        "ecosystem",
        "components",
        "Hadoop",
        "MapReduce",
        "HDFS",
        "HBase",
        "Hive",
        "Sqoop",
        "Pig",
        "Zookeeper",
        "Flume",
        "Good",
        "Exposure",
        "Apache",
        "Hadoop",
        "Map",
        "programming",
        "PIG",
        "Scripting",
        "Distribute",
        "Application",
        "HDFS",
        "Good",
        "Knowledge",
        "Hadoop",
        "Cluster",
        "architecture",
        "cluster",
        "understanding",
        "Data",
        "Structure",
        "Algorithms",
        "Experience",
        "Hadoop",
        "log",
        "understanding",
        "knowledge",
        "NOSQL",
        "MongoDB",
        "HBase",
        "Cassandra",
        "standards",
        "processes",
        "Hadoop",
        "application",
        "design",
        "implementation",
        "Experience",
        "data",
        "Sqoop",
        "HDFS",
        "Relational",
        "Database",
        "Systems",
        "viceversa",
        "Experience",
        "Object",
        "Oriented",
        "Analysis",
        "Design",
        "OOAD",
        "development",
        "software",
        "UML",
        "Methodology",
        "knowledge",
        "J2EE",
        "design",
        "patterns",
        "Core",
        "Java",
        "design",
        "patterns",
        "Experience",
        "Hadoop",
        "clusters",
        "Cloudera",
        "Manager",
        "tool",
        "experience",
        "project",
        "life",
        "cycle",
        "design",
        "development",
        "testing",
        "implementation",
        "Client",
        "Server",
        "Web",
        "applications",
        "Experience",
        "Administering",
        "Installation",
        "configuration",
        "Security",
        "Backup",
        "Performance",
        "Monitoring",
        "Finetuning",
        "Linux",
        "Redhat",
        "experience",
        "Oracle",
        "DB2",
        "SQL",
        "Server",
        "SQL",
        "database",
        "Hands",
        "experience",
        "VPN",
        "Putty",
        "WinSCP",
        "VNCviewer",
        "Scripting",
        "monitors",
        "checks",
        "system",
        "admin",
        "functions",
        "Hands",
        "experience",
        "application",
        "development",
        "Java",
        "RDBMS",
        "Linux",
        "shell",
        "Experience",
        "Java",
        "JSP",
        "Servlets",
        "EJB",
        "WebLogic",
        "WebSphere",
        "Hibernate",
        "Spring",
        "JBoss",
        "JDBC",
        "RMI",
        "Java",
        "Script",
        "Ajax",
        "Jquery",
        "XML",
        "HTML",
        "Ability",
        "technology",
        "sense",
        "responsibility",
        "accomplishment",
        "US",
        "employer",
        "Work",
        "Experience",
        "Big",
        "Data",
        "Developer",
        "Charlotte",
        "NC",
        "March",
        "July",
        "Node",
        "Mapr",
        "Cluster",
        "Cores",
        "G",
        "Ram",
        "AWS",
        "data",
        "recommendation",
        "engine",
        "Spark",
        "Hive",
        "Oozie",
        "Rstudio",
        "Zeepline",
        "Mapr",
        "Hadoop",
        "Cluster",
        "Wrote",
        "Python",
        "Scala",
        "api",
        "positecs",
        "rdbms",
        "aws",
        "s3",
        "Spark",
        "Execution",
        "framework",
        "Spark",
        "Pyspark",
        "Created",
        "Spark",
        "Data",
        "Frames",
        "data",
        "Hive",
        "Tables",
        "Kafka",
        "Cluster",
        "IOT",
        "data",
        "Hadoop",
        "Cluster",
        "data",
        "lake",
        "IOT",
        "Kafka",
        "Cluster",
        "Azure",
        "VPG",
        "connection",
        "Client",
        "Network",
        "Hadoop",
        "cluster",
        "top",
        "aws",
        "Used",
        "Sqoop",
        "data",
        "lake",
        "MSSQL",
        "Hadoop",
        "cluster",
        "jobs",
        "oozie",
        "data",
        "pipeline",
        "FTP",
        "Server",
        "Hadoop",
        "api",
        "Hive",
        "tables",
        "format",
        "Rstudio",
        "Sparkly",
        "JDBC",
        "Connection",
        "Infrastructure",
        "access",
        "data",
        "science",
        "team",
        "data",
        "models",
        "aws",
        "Connected",
        "Hive",
        "tables",
        "Hadoop",
        "cluster",
        "visualization",
        "tools",
        "PowerBI",
        "Tableau",
        "ODBC",
        "mapr",
        "connector",
        "Used",
        "s3",
        "data",
        "backup",
        "hadoop",
        "cluster",
        "Implemented",
        "POC",
        "Launching",
        "Hadoop",
        "Spark",
        "Cluster",
        "Hdinsights",
        "Developed",
        "Spark",
        "recommendation",
        "engine",
        "python",
        "Environment",
        "AWS",
        "Spark",
        "Python",
        "Azure",
        "Hadoop",
        "Kafka",
        "Scala",
        "Hive",
        "Flume",
        "Hbase",
        "Sqoop",
        "PIG",
        "Java",
        "JDK",
        "Eclipse",
        "MySQL",
        "Ubuntu",
        "Zookeeper",
        "Amazon",
        "EC2",
        "SOLR",
        "Hadoop",
        "Developer",
        "PositecSyntelli",
        "Solutions",
        "Charlotee",
        "North",
        "Carolina",
        "March",
        "February",
        "Created",
        "Hive",
        "HBase",
        "tables",
        "file",
        "format",
        "compression",
        "Pig",
        "scripts",
        "data",
        "partitions",
        "data",
        "Partitioning",
        "Hive",
        "tables",
        "requirement",
        "tasks",
        "data",
        "Ingestion",
        "DB2",
        "HDFS",
        "Sqoop",
        "JDBC",
        "Connector",
        "shell",
        "scripts",
        "Pig",
        "Hive",
        "actions",
        "Oozie",
        "workflow",
        "Incremental",
        "Imports",
        "Hive",
        "tables",
        "formats",
        "Hive",
        "Tables",
        "ORC",
        "Avro",
        "ORC",
        "requirements",
        "Hadoop",
        "log",
        "Provisioning",
        "Cassandra",
        "cluster",
        "cloud",
        "environment",
        "Hive",
        "External",
        "Managed",
        "Tables",
        "performance",
        "External",
        "Tables",
        "documentation",
        "Hadoop",
        "Clusters",
        "Pig",
        "scripts",
        "data",
        "aspects",
        "MapReduce",
        "Environment",
        "Hadoop",
        "MapReduce",
        "HDFS",
        "Hive",
        "Java",
        "Hadoop",
        "distribution",
        "Horton",
        "Works",
        "Cloudera",
        "Pig",
        "Hbase",
        "Linux",
        "XML",
        "MySQL",
        "Java",
        "Eclipse",
        "Oracle",
        "g",
        "PLSQL",
        "SQL",
        "PLUS",
        "Hadoop",
        "Developer",
        "KIOO",
        "LIMITED",
        "Hyderabad",
        "Telangana",
        "March",
        "July",
        "nodes",
        "Hadoop",
        "cluster",
        "CDH44",
        "Performed",
        "Flume",
        "Sqoop",
        "imports",
        "data",
        "Data",
        "warehouse",
        "platform",
        "HDFS",
        "monitoring",
        "Hadoop",
        "cluster",
        "Cloudera",
        "Manager",
        "Data",
        "classification",
        "algorithms",
        "Map",
        "design",
        "patterns",
        "Expertise",
        "designing",
        "deployment",
        "Hadoop",
        "cluster",
        "Big",
        "Data",
        "tools",
        "Pig",
        "Hive",
        "HBase",
        "Oozie",
        "Sqoop",
        "Flume",
        "Spark",
        "Impala",
        "Cloudera",
        "distribution",
        "Used",
        "Pig",
        "data",
        "transformations",
        "event",
        "filter",
        "preaggregations",
        "data",
        "MapReduce",
        "Programs",
        "cluster",
        "data",
        "Teradata",
        "HDFS",
        "Sqoop",
        "Environment",
        "Cloudera",
        "Hadoop",
        "MapReduce",
        "HDFS",
        "Hive",
        "Sqoop",
        "Hbase",
        "Linux",
        "XML",
        "MySQL",
        "Java",
        "Eclipse",
        "Java",
        "J2EE",
        "Developer",
        "MUFINDI",
        "LIMITED",
        "Hyderabad",
        "Telangana",
        "April",
        "March",
        "Developed",
        "web",
        "components",
        "JSP",
        "Servlets",
        "JDBC",
        "tables",
        "indexes",
        "Tested",
        "Deployed",
        "Enterprise",
        "Java",
        "Beans",
        "Session",
        "Entity",
        "WebLogic",
        "Application",
        "Serve",
        "Environment",
        "Spring305",
        "Spring",
        "Web",
        "Services",
        "SOAP",
        "Apache",
        "Axis",
        "Tomcat",
        "v60",
        "Eclipse",
        "Indigo",
        "Ant",
        "Log4j2",
        "CVS",
        "ACORD",
        "SoapUI",
        "DB",
        "Visualizer",
        "Pro",
        "WebSphere",
        "MQ",
        "ACORD",
        "Visio",
        "BMC",
        "Blade",
        "Logic",
        "Server",
        "Admin",
        "console",
        "putty",
        "WinSCP",
        "Education",
        "Master",
        "Electrical",
        "Engineering",
        "Computer",
        "Science",
        "Texas",
        "A",
        "M",
        "University",
        "Bachelors",
        "Engineering",
        "Engineering",
        "Electronics",
        "Instrumentation",
        "Engineering",
        "BITS",
        "Pilani",
        "Pilani",
        "Rajasthan",
        "Skills",
        "Big",
        "Data",
        "Python",
        "ETL"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T23:10:45.579146",
    "resume_data": "Big Data Developer Big Data span lDeveloperspan Big Data Developer Charlotte NC Over 8 years of professional IT experience which includes experience in Big data ecosystem Excellent Experience in Hadoop architecture and various components such as HDFS Job Tracker Task Tracker NameNode Data Node and MapReduce programming paradigm Have sound exposure to Retail market including Retail Delivery System Hands on experience in installing configuring and using Hadoop ecosystem components like Hadoop MapReduce HDFS HBase Hive Sqoop Pig Zookeeper and Flume Good Exposure on Apache Hadoop Map Reduce programming PIG Scripting and Distribute Application and HDFS Good Knowledge on Hadoop Cluster architecture and monitoring the cluster Indepth understanding of Data Structure and Algorithms Experience in managing and reviewing Hadoop log files Excellent understanding and knowledge of NOSQL databases like MongoDB HBase Cassandra Implemented in setting up standards and processes for Hadoop based application design and implementation Experience in importing and exporting data using Sqoop from HDFS to Relational Database Systems and viceversa Experience in Object Oriented Analysis Design OOAD and development of software using UML Methodology good knowledge of J2EE design patterns and Core Java design patterns Experience in managing Hadoop clusters using Cloudera Manager tool Very good experience in complete project life cycle design development testing and implementation of Client Server and Web applications Experience in Administering Installation configuration troubleshooting Security Backup Performance Monitoring and Finetuning of Linux Redhat Extensive experience working in Oracle DB2 SQL Server and My SQL database Hands on experience in VPN Putty WinSCP VNCviewer etc Scripting to deploy monitors checks and critical system admin functions automation Hands on experience in application development using Java RDBMS and Linux shell scripting Experience in Java JSP Servlets EJB WebLogic WebSphere Hibernate Spring JBoss JDBC RMI Java Script Ajax Jquery XML and HTML Ability to adapt to evolving technology strong sense of responsibility and accomplishment Authorized to work in the US for any employer Work Experience Big Data Developer MorganStanleyCapgemini Charlotte NC March 2018 to July 2019 Launched 5 Node Mapr Cluster 52 with 80 Cores 600G Ram on AWS to implement a data recommendation engine Installed Spark Hive Oozie Rstudio Zeepline on to Mapr Hadoop Cluster Wrote Python and Scala api from positecs rdbms to aws s3 and implemented on Spark Execution framework Worked Intensively on Spark Pyspark and SparkR Created Spark Data Frames while loading the data in to Hive Tables Used Kafka Cluster to load IOT data on the Hadoop Cluster and designed a data lake for IOT on Kafka Cluster on Azure Created a VPG connection From Client Network to Hadoop cluster on top of aws Used Sqoop built the data lake From MSSQL TO Hadoop cluster scheduled jobs using crontab oozie Built a data pipeline from FTP Server to Hadoop using python api Created Hive tables in parquet format connected to Rstudio and zeepline through Sparkly JDBC Connection given Infrastructure access to data science team to run their data models on aws cloud Connected Hive tables on Hadoop cluster to visualization tools like PowerBI Tableau using ODBC and mapr drill connector Used s3 as data backup from the hadoop cluster Implemented POC on Launching Hadoop and Spark Cluster on Hdinsights Developed Spark for the recommendation engine and validated using python Environment AWS Spark Python Azure Hadoop Kafka Scala Hive Flume Hbase Sqoop PIG Java JDK 16 Eclipse MySQL Ubuntu Zookeeper Amazon EC2 SOLR Hadoop Developer PositecSyntelli Solutions Charlotee North Carolina March 2016 to February 2018 Created Hive HBase tables using ORC file format and Snappy compression Wrote different Pig scripts to clean up the ingested data and created partitions for the daily data Implemented Partitioning and bucketing for Hive tables based on the requirement Performed Various tasks on data Ingestion from DB2 to the HDFS using Sqoop JDBC Connector Created shell scripts to parameterize the Pig Hive actions in Oozie workflow Designed and implemented Incremental Imports into Hive tables Created different formats of Hive Tables Finally worked on ORC Avro and picked ORC as fit requirements Managed and reviewed the Hadoop log files Provisioning and managing multitenant Cassandra cluster on public cloud environment Created Hive External Managed Tables sorted out the best performance and voted to External Tables Created and maintained Technical documentation for launching Hadoop Clusters and for executing Pig scripts Worked on aggregating the data using advanced aspects of MapReduce Environment Hadoop MapReduce HDFS Hive Java Hadoop distribution of Horton Works Cloudera Pig Hbase Linux XML MySQL Java 6 Eclipse Oracle 10g PLSQL SQL PLUS Hadoop Developer KIOO LIMITED Hyderabad Telangana March 2012 to July 2014 Worked on a live 16 nodes Hadoop cluster running CDH44 Performed Flume Sqoop imports of data from Data warehouse platform to HDFS Continuous monitoring and managing the Hadoop cluster through Cloudera Manager Implemented Data classification algorithms using Map reduce design patterns Expertise in designing and deployment of Hadoop cluster and different Big Data analytic tools including Pig Hive HBase Oozie Sqoop Flume Spark Impala with Cloudera distribution Used Pig to do data transformations event joins filter and some preaggregations before storing the data into HDFS Supported MapReduce Programs those are running on the cluster Extracted the data from Teradata into HDFS using the Sqoop Environment Cloudera Hadoop MapReduce HDFS Hive Sqoop Hbase Linux XML MySQL Java 6 Eclipse Java J2EE Developer MUFINDI LIMITED Hyderabad Telangana April 2009 to March 2012 Developed web components using JSP Servlets and JDBC Designed tables and indexes Designed Implemented Tested and Deployed Enterprise Java Beans both Session and Entity using WebLogic as Application Serve Environment Spring305 Spring batch 217 Web Services SOAP Apache Axis 2 Tomcat v60 Eclipse Indigo 37 Ant 182 Log4j2 CVS ACORD SoapUI 453 DB Visualizer Pro 912 WebSphere MQ 7013 ACORD Visio 2013 BMC Blade Logic Server Admin console putty WinSCP Education Master in Electrical Engineering Computer Science Texas A M University 2014 Bachelors of Engineering in Engineering Electronics and Instrumentation Engineering BITS Pilani Pilani Rajasthan 2008 Skills Big Data Python ETL",
    "unique_id": "f5505fd7-3800-447d-ab7b-340892b89c87"
}