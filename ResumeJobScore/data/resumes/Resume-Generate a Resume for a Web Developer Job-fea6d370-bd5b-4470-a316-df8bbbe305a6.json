{
    "clean_data": "Sr Data ScientistMachine Learning Engineer Sr Data ScientistMachine Learning Engineer Sr Data ScientistMachine Learning Engineer SEI Investments Developed Map ReduceSpark Python modules for machine learning predictive analytics in Hadoop on AWS Implemented a Pythonbased distributed random forest via Python streaming Perform Exploratory analysis hypothesis testing cluster analysis correlation ANOVA ROC Curve and build models in Supervised and Unsupervised Machine Learning algorithms Text Analytics Time Series forecasting Prepared the model data and built machine learning algorithms using Python Pandas scikit learn numPy keras etc libraries using Anaconda Jupyter Programming Linear Logistic Regressions KNN KMeans Clustering SentimentText Analytics NLP Nave Bayes Time Series forecasting using lm glm Arima Apriori Forecast Extracting data from Big Data Hadoop Data Lake Excel Analyzing Cleaning Sorting Merging Reporting and creating dashboards using Base SAS SAS Macros SQL Hive SAS VA SAS and Excel Developing MapReduceSpark Python modules for machine learning predictive analytics in Hadoop on AWS Implemented a Pythonbased distributed random forest via Python streaming Used pandas numpy seaborn scipy matplotlib scikitlearn NLTK in Python for developing various machine learning algorithms and utilized machine learning algorithms such as linear regression multivariate regression naive bayes Random Forests Kmeans KNN for data analysis Currently working on building clustering and predictive models using Mllib to predict fault code occurrences using Spark and Mllib Conducting studies rapid plots and using advance data mining and statistical modelling techniques to build solution that optimize the quality and performance of data Demonstrated experience in design and implementation of Statistical models Predictive models enterprise data model metadata solution and data life cycle management in both RDBMS Big Data environments Stored and retrieved data from datawarehouses using Amazon Redshift and designed and implemented system architecture for Amazon EC2 based cloudhosted solution for client Developed Simple to complex Map Reduce Jobs using Hive and Pig and developed multiple Map Reduce jobs in java for data cleaning and preprocessing Analyzing large data sets apply machine learning techniques and develop predictive models statistical models and developing and enhancing statistical models by leveraging bestinclass modeling techniques Developed Map ReduceSpark Python modules for machine learning predictive analytics in Hadoop on AWS Worked with various Teradata15 tools and utilities like Teradata Viewpoint Multi Load ARC Teradata Administrator BTEQ and other Teradata Utilities Utilized Spark Scala Hadoop HBase Kafka Spark Streaming Caffe TensorFlow  Python a broad variety of machine learning methods including classifications regressions dimensionally reduction etc Developed LINUX Shell scripts by using NZSQLNZLOAD utilities to load data from flat files to Netezza database Working on Information extraction from different kinds of text documents using NLP text mining and regular expressions Worked extensively on Tableau Desktop apply filters drill downs and generate Data visualizations interactive Dash Boards that can interact with views of data and worked on several options like query display analyze sort group drill down organize summarize and generate charts monitor and measure goals identify patterns Work Experience Sr Data ScientistMachine Learning Engineer SEI Investments Malvern PA March 2017 to Present Responsibilities Developed Map ReduceSpark Python modules for machine learning predictive analytics in Hadoop on AWS Implemented a Pythonbased distributed random forest via Python streaming Perform Exploratory analysis hypothesis testing cluster analysis correlation ANOVA ROC Curve and build models in Supervised and Unsupervised Machine Learning algorithms Text Analytics Time Series forecasting Prepared the model data and built machine learning algorithms using Python Pandas scikit learn numPy keras etc libraries using Anaconda Jupyter Programming Linear Logistic Regressions KNN KMeans Clustering SentimentText Analytics NLP Nave Bayes Time Series forecasting using lm glm Arima Apriori Forecast Extracting data from Big Data Hadoop Data Lake Excel Analyzing Cleaning Sorting Merging Reporting and creating dashboards using Base SAS SAS Macros SQL Hive SAS VA SAS and Excel Developing MapReduceSpark Python modules for machine learning predictive analytics in Hadoop on AWS Implemented a Pythonbased distributed random forest via Python streaming Used pandas numpy seaborn scipy matplotlib scikitlearn NLTK in Python for developing various machine learning algorithms and utilized machine learning algorithms such as linear regression multivariate regression naive bayes Random Forests Kmeans KNN for data analysis Currently working on building clustering and predictive models using Mllib to predict fault code occurrences using Spark and Mllib Conducting studies rapid plots and using advance data mining and statistical modelling techniques to build solution that optimize the quality and performance of data Demonstrated experience in design and implementation of Statistical models Predictive models enterprise data model metadata solution and data life cycle management in both RDBMS Big Data environments Stored and retrieved data from datawarehouses using Amazon Redshift and designed and implemented system architecture for Amazon EC2 based cloudhosted solution for client Developed Simple to complex Map Reduce Jobs using Hive and Pig and developed multiple MapReduce jobs in java for data cleaning and preprocessing Analyzing large data sets apply machine learning techniques and develop predictive models statistical models and developing and enhancing statistical models by leveraging bestinclass modeling techniques Developed MapReduceSpark Python modules for machine learning predictive analytics in Hadoop on AWS Worked with various Teradata15 tools and utilities like Teradata Viewpoint Multi Load ARC Teradata Administrator BTEQ and other Teradata Utilities Utilized Spark Scala Hadoop HBase Kafka Spark Streaming Caffe TensorFlow  Python a broad variety of machine learning methods including classifications regressions dimensionally reduction etc Developed LINUX Shell scripts by using NZSQLNZLOAD utilities to load data from flat files to Netezza database Working on Information extraction from different kinds of text documents using NLP text mining and regular expressions Worked extensively on Tableau Desktop apply filters drill downs and generate Data visualizations interactive Dash Boards that can interact with views of data and worked on several options like query display analyze sort group drill down organize summarize and generate charts monitor and measure goals identify patterns Environment Python SQL Oracle 12c Netezza SQL Server Informatica Java SSRS PLSQL TSQL Tableau  regression Scala NLP Spark Kafka MongoDB Workday logistic regression Hadoop Hive TensorFlow Teradata IDE random forest OLAP Azure MariaDB SAP CRM HDFS ODS NLTK SVM JSON Tableau XML AWS Redshift Pandas Cassandra MapReduce AWS Tableau Caffe Sr Data Scientist Machine Learning Engineer UHG Brooklyn NY January 2015 to February 2017 Responsibilities Implemented endtoend systems for Data Analytics Data Automation and integrated with custom visualization tools using R Hadoop and MongoDB Cassandra Building predictive models using tools such as SAS R with very granular data stored in big data platform Worked on different data formats such as JSON XML and performed machine learning algorithms in R and used Spark for test data analytics using  and Analyzed the performance to identify bottlenecks Involved working with Machine Learning Algorithms such as Decision Trees Random Forest Gradient Boosting Support Vector Machines K Mean Clustering Nave Bayes Bayesian Belief Networks and Artificial Neural Networks Developed Predictive models Machine learning Supervised and nonSupervised using R for Machine Motor Creating various B2B Predictive and descriptive analytics using R and Tableau and performed data cleaning and data preparation tasks to convert data into a meaningful data set using R Used R to verify the results of Mahout on small data sets Developed missing but important features of ML algorithms to the Mahout Utilized Spark Scala Hadoop HBase Kafka Spark Streaming  Python a broad variety of machine learning methods including classifications regressions dimensionally reduction etc Created partitioned and bucketed tables in Hive Involved in creating Hive internal and external tables loading with data and writing hive queries which involves multiple join scenarios Performed Kmeans clustering Multivariate analysis and Support Vector Machines in R Create analytical models using analytics algorithms like regression decision trees clustering text mining etc and leveraging tools like R Tableau etc to deliver actionable insights and recommendations Developed multiple Spark jobs using Scala for data cleaning and preprocessing Designed the schema configured and deployed AWS Redshift for optimal storage and fast retrieval of data Used External Loaders like Multi Load T Pump and Fast Load to load data into Teradata141Database Involved in Troubleshooting and quality control of data transformations and loading during migration from Oracle systems into Netezza EDW Used S3 Bucket to store the jars input datasets and used Dynamo DB to store the processed output from the input data set Worked on classificationscripting of multiple attribute models by applying textmining NLP SVM and Regular Expressions given product features like title description etc predicting product attribute values using PythonR Worked on different data formats such as JSON XML and performed machine learning algorithms in R Used Spark for test data analytics using  and Analyzed the performance to identify bottlenecks and used Supervised learning techniques such as classifiers and neural networks to identify patters in these data sets Developed Tableau visualizations and dashboards using Tableau Desktop Tableau workbooks from multiple data sources using Data Blending Developing new data warehousing system based on spark 2x and spark streaming utilizing Scala and Java 8 Strong Knowledge on concepts of DataModeling Star SchemaSnowflake modeling FACT Dimensions tables and LogicalPhysical data modeling Environment R3x Erwin 952 MDM QlikView  PLSQL Tableau Teradata 141 JSON HADOOP HDFS MapReduce SQL Server  Scala NLP SSMS ERP CRM Netezza Pandas SAS SPSS Java IDE Cassandra SQL PLSQL AWS SSRS Informatica PIG Spark Azure R Studio MongoDB MAHOUT JAVA HIVE AWS Redshift SQL Developer against ODS Western Digital Burlington NJ April 2012 to December 2014 Burlington NJ April 2012 to December 2014 Responsibilities Design database data models ETL processes data warehouse applications and business intelligence BI reports through the use of best practices and tools including Erwin SQL SSIS SSRS and OLAP OLTP Transformed Logical Data Model to Physical Data Model ensuring the Primary Key and Foreign Key relationships in PDM Consistency of definitions of Data Attributes and Primary Index Considerations Validated the data of reports by writing SQL queries in PLSQL Developer against ODS Developed MapReduce programs to parse the raw data populate staging tables and store the refined data in partitioned tables in the EDW Involved with Data Analysis primarily Identifying Data Sets Source Data Source Meta Data Data Definitions and Data Formats Data Analyst Macys Inc Duluth GA January 2010 to March 2012 Roles Responsibilities Analyzed Trading mechanism for realtime transactions and build collateral management tools Compiled data from various sources to perform complex analysis for actionable results Utilized machine learning algorithms such as linear regression multivariate regression naive bayes Random Forests Kmeans KNN for data analysis Measured Efficiency of HadoopHive environment ensuring SLA is met Developed Spark code using Scala and SparkSQLStreaming for faster processing of data Prepared Spark build from the source code and ran the PIG Scripts using Spark rather using MR jobs for better performance Analyzing the system for new enhancementsfunctionalities and perform Impact analysis of the application for implementing ETL changes Imported data using Sqoop to load data from MySQL to HDFS on regular basis Developed Scripts and Batch Job to schedule various Hadoop Program Used TensorFlow to train the model from insightful data and look at thousands of examples Designing developing and optimizing SQL code DDL DML Building performant scalable ETL processes to load cleanse and validate data Expertise in Data archival and Data migration adhoc reporting and code utilizing SAS on UNIX and Windows Environments Tested and debugged SAS programs against the test data Processed the data in SAS for the given requirement using SAS programming concepts Imported and Exported data files to and from SAS using Proc Import and Proc Export from Excel and various delimited textbased data files such as TXT tab delimited and CSV comma delimited files into SAS datasets for analysis Expertise in producing RTF PDF HTML files using SAS ODS facility Providing support for data processes This will involve monitoring data profiling database usage trouble shooting tuning and ensuring data integrity Participating in the full software development lifecycle with requirements solution design development QA implementation and product support using Scrum and other Agile methodologies Collaborate with team members and stakeholders in design and development of data environment Learning new tools and skillsets as needs arise Preparing associated documentation for specifications requirements and testing Optimizing the Tensorflow Model for an efficiency Used Tensorflow for text summarization Used Spark API over Cloudera Hadoop YARN to perform analytics on data in Hive Wrote Hive queries for data analysis to meet the business requirements Developed Kafka producer and consumers for message handling Responsible for analyzing multiplatform applications using python Used storm for an automatic mechanism to analyze large amounts of nonunique data points with low latency and high throughput Developed MapReduce jobs in Python for data cleaning and data processing Environment Machine learning AWS MS Azure Cassandra SAS Spark HDFS Hive Pig Linux Anaconda Python MySQL Eclipse PLSQL SQL connector SparkML Python Developer HCL India April 2008 to December 2009 Roles Responsibilities Worked on the project from gathering requirements to developing the entire application Worked on Anaconda Python Environment Created activated and programmed in Anaconda environment Wrote programs for performance calculations using NumPy and SQLAlchemy Wrote python routines to log into the websites and fetch data for selected options Used python modules of urllib urllib2 Requests for web crawling Experience using all these ML techniques clustering regression classification graphical models Extensive experience in Text Analytics developing different Statistical Machine Learning Data mining solutions to various business problems and generating data visualizations using R Python and Tableau Used with other packages such as Beautiful Soup for data parsing Involved in development of Web Services using SOAP for sending and getting data from the external interface in the XML format Used with other packages such as Beautiful Soup for data parsing Worked on development of SQL and stored procedures on MYSQL Analyzed the code completely and have reduced the code redundancy to the optimal level Design and build a text classification application using different text classification models Used Jira for defect tracking and project management Worked on writing and as well as read data from CSV and excel file formats Involved in Sprint planning sessions and participated in the daily Agile SCRUM meetings Conducted every day scrum as part of the SCRUM Master role Developed the project in Linux environment Worked on resulting reports of the application Performed QA testing on the application Held meetings with client and worked for the entire project with limited help from the client Environment Python Anaconda Sypder IDE Windows 7 Teradata Requests urllib urllib2 Beautiful Soup Tableau python libraries such as NumPy SQL Alchemy  Skills SQL 10 years APACHE HADOOP MAPREDUCE 8 years MapReduce 8 years MAPREDUCE 8 years PLSQL 8 years Additional Information Skills APACHE HADOOP MAPREDUCE 7 years MapReduce 7 years OLAP 7 years ONLINE ANALYTICAL PROCESSING 7 years PLSQL 7 years Technical Skills Data Analytics ToolsProgramming Python numpy scipy pandas Gensim Keras R Caret Weka ggplot MATLAB Microsoft SQL Server Oracle PLSQL Python SQL PLSQL TSQL UNIX shell scripting Java SAS Big Data Techs Hadoop Hive HDFS MapReduce Pig Kafka HBase Cassandra MongoDB Analysis and Modeling Tools Erwin Sybase Power Designer Oracle Designer BPwin Rational Rose ERStudio TOAD MS Visio ETL Tools Informatica Power Center Data Stage 75 Ab Initio Talend OLAP Tools MS SQL Analysis Manager DB2 OLAP CognosPowerplay Languages SQL PLSQL TSQL XML HTML UNIX Shell Scripting C C AWK Databases Oracle12c11g10g9i8i807x Teradata140 DB2 UDB 81 MS SQLServer 05 Netezaa and Sybase ASE 125315 Informix 9 HBase MongoDB Cassandra Amazon Redshift Operating Systems Windows 20078 UNIX SunSolaris HPUX Windows NTXPVista MSDOS Project Execution Methodologies Ralph Kimball and Bill Inmon data warehousing methodology Rational Unified Process RUP Rapid Application Development RAD Joint Application Development JAD Reporting Tools Business ObjectsXIR2655051 Cognos Impromptu 706050 Informatica Analytics Delivery Platform MicroStrategy Tableau Tools MSOffice suite Word Excel MS Project and Outlook VSS Others Spark  Scala NLP MariaDB Azure SAS IDE Microsoft Azure AWS Data ScientistMachine Learning Engineer Macys Inc Duluth GA Close to Eight years of expert involvement in IT in which I have 3 years of knowledge in Data Mining Machine Learning and Spark Development with big datasets of Structured and Unstructured Data Data Acquisition Data Validation Predictive demonstrating Data Visualization Capable in measurable programming languages like R and Python Proficient in managing entire data science project life cycle and actively involved in all the phases of project life cycle Extensive experience in Text Analytics developing different Statistical Machine Learning Data mining solutions to various business problems and generating data visualizations using R Python and Tableau Adept and deep understanding of Statistical modeling Multivariate Analysis model testing problem analysis model comparison and validation Skilled in performing data parsing data manipulation and data preparation with methods including describe data contents compute descriptive statistics of data regex split and combine Remap merge subset reindex melt and reshape Experience in using various packages in R and libraries in Python Working knowledge in Hadoop Hive and NOSQL databases like Cassandra and HBase Hands on experience in implementing LDA Naive Bayes and skilled in Random Forests Decision Trees Linear and Logistic Regression SVM Clustering neural networks Principle Component Analysis and good knowledge on Recommender Systems Good industry knowledge analytical and problemsolving skills and ability to work well within a team as well as an individual Highly creative innovative committed intellectually curious business savvy with effective communication and interpersonal skills I can be able to quickly adapt the new work pace and learning",
    "entities": [
        "Python Proficient",
        "Big Data Hadoop Data Lake Excel Analyzing Cleaning Sorting Merging Reporting",
        "BI",
        "UNIX",
        "Mllib Conducting",
        "MDM QlikView  PLSQL",
        "Outlook VSS Others Spark",
        "Beautiful Soup",
        "TensorFlow",
        "Netezza",
        "Hadoop",
        "SOAP",
        "XML",
        "MAHOUT",
        "IDE Cassandra SQL PLSQL AWS SSRS Informatica PIG Spark",
        "NOSQL",
        "Word Excel MS Project",
        "Python Working",
        "Artificial Neural Networks Developed Predictive",
        "Amazon",
        "Data Blending Developing",
        "Work Experience Sr Data ScientistMachine Learning Engineer SEI Investments Malvern PA",
        "the Mahout Utilized Spark",
        "Cloudera Hadoop",
        "Burlington NJ",
        "ODS NLTK",
        "Hadoop Program",
        "Utilized",
        "Amazon Redshift",
        "Hive Wrote Hive",
        "Data Attributes",
        "PDM",
        "Troubleshooting",
        "Processed",
        "PDF HTML",
        "LDA Naive Bayes",
        "Linux",
        "ANOVA ROC Curve",
        "Sr Data ScientistMachine Learning Engineer Sr Data ScientistMachine Learning Engineer Sr Data ScientistMachine Learning Engineer SEI Investments Developed Map ReduceSpark Python",
        "Mllib",
        "Tableau Desktop",
        "Teradata Utilities Utilized",
        "Support Vector Machines",
        "Python Developer HCL India",
        "Digital Burlington NJ",
        "Unsupervised Machine Learning",
        "Teradata Viewpoint Multi Load",
        "NZSQLNZLOAD",
        "Spark Development",
        "Optimizing the Tensorflow Model",
        "Spark",
        "Measured Efficiency of HadoopHive",
        "Cassandra Amazon Redshift Operating Systems",
        "linear",
        "Dash Boards",
        "CSV",
        "Tableau Adept",
        "Multivariate Analysis",
        "Sybase",
        "Hadoop Hive",
        "Sqoop",
        "QA",
        "Working on Information",
        "Identifying Data Sets Source Data Source Meta Data Data Definitions",
        "Windows NTXPVista MSDOS Project Execution Methodologies",
        "Worked on Anaconda Python Environment Created",
        "KNN",
        "Created",
        "Primary Index Considerations Validated",
        "Proc Import",
        "B2B Predictive",
        "MR",
        "Oracle",
        "Text Analytics",
        "Unstructured Data Data Acquisition Data Validation Predictive",
        "Data Formats Data",
        "Developed Tableau",
        "java",
        "SAS",
        "Techs Hadoop Hive HDFS MapReduce Pig",
        "SQL",
        "Hive Involved",
        "NLP",
        "Data Analysis",
        "Anaconda",
        "Erwin SQL SSIS",
        "Pandas",
        "Statistical Machine Learning Data",
        "ETL",
        "Learning",
        "NumPy SQL Alchemy",
        "Random Forests Decision Trees Linear and Logistic Regression SVM Clustering",
        "Tableau Desktop Tableau",
        "DataModeling Star SchemaSnowflake",
        "Data Analytics Data Automation",
        "Microsoft",
        "LogicalPhysical",
        "Principle Component Analysis",
        "Recommender Systems Good",
        "ML",
        "AWS Redshift",
        "Supervised",
        "EDW",
        "Data",
        "Structured",
        "MapReduce",
        "Data Mining Machine Learning",
        "Tableau",
        "SAP CRM",
        "Skills SQL",
        "Sprint",
        "Informatica Power Center Data Stage",
        "RTF",
        "SLA",
        "Data Visualization Capable",
        "OLAP OLTP Transformed Logical Data Model to Physical Data Model",
        "Informix",
        "TXT",
        "JSON XML"
    ],
    "experience": "Experience Sr Data ScientistMachine Learning Engineer SEI Investments Malvern PA March 2017 to Present Responsibilities Developed Map ReduceSpark Python modules for machine learning predictive analytics in Hadoop on AWS Implemented a Pythonbased distributed random forest via Python streaming Perform Exploratory analysis hypothesis testing cluster analysis correlation ANOVA ROC Curve and build models in Supervised and Unsupervised Machine Learning algorithms Text Analytics Time Series forecasting Prepared the model data and built machine learning algorithms using Python Pandas scikit learn numPy keras etc libraries using Anaconda Jupyter Programming Linear Logistic Regressions KNN KMeans Clustering SentimentText Analytics NLP Nave Bayes Time Series forecasting using lm glm Arima Apriori Forecast Extracting data from Big Data Hadoop Data Lake Excel Analyzing Cleaning Sorting Merging Reporting and creating dashboards using Base SAS SAS Macros SQL Hive SAS VA SAS and Excel Developing MapReduceSpark Python modules for machine learning predictive analytics in Hadoop on AWS Implemented a Pythonbased distributed random forest via Python streaming Used pandas numpy seaborn scipy matplotlib scikitlearn NLTK in Python for developing various machine learning algorithms and utilized machine learning algorithms such as linear regression multivariate regression naive bayes Random Forests Kmeans KNN for data analysis Currently working on building clustering and predictive models using Mllib to predict fault code occurrences using Spark and Mllib Conducting studies rapid plots and using advance data mining and statistical modelling techniques to build solution that optimize the quality and performance of data Demonstrated experience in design and implementation of Statistical models Predictive models enterprise data model metadata solution and data life cycle management in both RDBMS Big Data environments Stored and retrieved data from datawarehouses using Amazon Redshift and designed and implemented system architecture for Amazon EC2 based cloudhosted solution for client Developed Simple to complex Map Reduce Jobs using Hive and Pig and developed multiple MapReduce jobs in java for data cleaning and preprocessing Analyzing large data sets apply machine learning techniques and develop predictive models statistical models and developing and enhancing statistical models by leveraging bestinclass modeling techniques Developed MapReduceSpark Python modules for machine learning predictive analytics in Hadoop on AWS Worked with various Teradata15 tools and utilities like Teradata Viewpoint Multi Load ARC Teradata Administrator BTEQ and other Teradata Utilities Utilized Spark Scala Hadoop HBase Kafka Spark Streaming Caffe TensorFlow   Python a broad variety of machine learning methods including classifications regressions dimensionally reduction etc Developed LINUX Shell scripts by using NZSQLNZLOAD utilities to load data from flat files to Netezza database Working on Information extraction from different kinds of text documents using NLP text mining and regular expressions Worked extensively on Tableau Desktop apply filters drill downs and generate Data visualizations interactive Dash Boards that can interact with views of data and worked on several options like query display analyze sort group drill down organize summarize and generate charts monitor and measure goals identify patterns Environment Python SQL Oracle 12c Netezza SQL Server Informatica Java SSRS PLSQL TSQL Tableau   regression Scala NLP Spark Kafka MongoDB Workday logistic regression Hadoop Hive TensorFlow Teradata IDE random forest OLAP Azure MariaDB SAP CRM HDFS ODS NLTK SVM JSON Tableau XML AWS Redshift Pandas Cassandra MapReduce AWS Tableau Caffe Sr Data Scientist Machine Learning Engineer UHG Brooklyn NY January 2015 to February 2017 Responsibilities Implemented endtoend systems for Data Analytics Data Automation and integrated with custom visualization tools using R Hadoop and MongoDB Cassandra Building predictive models using tools such as SAS R with very granular data stored in big data platform Worked on different data formats such as JSON XML and performed machine learning algorithms in R and used Spark for test data analytics using   and Analyzed the performance to identify bottlenecks Involved working with Machine Learning Algorithms such as Decision Trees Random Forest Gradient Boosting Support Vector Machines K Mean Clustering Nave Bayes Bayesian Belief Networks and Artificial Neural Networks Developed Predictive models Machine learning Supervised and nonSupervised using R for Machine Motor Creating various B2B Predictive and descriptive analytics using R and Tableau and performed data cleaning and data preparation tasks to convert data into a meaningful data set using R Used R to verify the results of Mahout on small data sets Developed missing but important features of ML algorithms to the Mahout Utilized Spark Scala Hadoop HBase Kafka Spark Streaming   Python a broad variety of machine learning methods including classifications regressions dimensionally reduction etc Created partitioned and bucketed tables in Hive Involved in creating Hive internal and external tables loading with data and writing hive queries which involves multiple join scenarios Performed Kmeans clustering Multivariate analysis and Support Vector Machines in R Create analytical models using analytics algorithms like regression decision trees clustering text mining etc and leveraging tools like R Tableau etc to deliver actionable insights and recommendations Developed multiple Spark jobs using Scala for data cleaning and preprocessing Designed the schema configured and deployed AWS Redshift for optimal storage and fast retrieval of data Used External Loaders like Multi Load T Pump and Fast Load to load data into Teradata141Database Involved in Troubleshooting and quality control of data transformations and loading during migration from Oracle systems into Netezza EDW Used S3 Bucket to store the jars input datasets and used Dynamo DB to store the processed output from the input data set Worked on classificationscripting of multiple attribute models by applying textmining NLP SVM and Regular Expressions given product features like title description etc predicting product attribute values using PythonR Worked on different data formats such as JSON XML and performed machine learning algorithms in R Used Spark for test data analytics using   and Analyzed the performance to identify bottlenecks and used Supervised learning techniques such as classifiers and neural networks to identify patters in these data sets Developed Tableau visualizations and dashboards using Tableau Desktop Tableau workbooks from multiple data sources using Data Blending Developing new data warehousing system based on spark 2x and spark streaming utilizing Scala and Java 8 Strong Knowledge on concepts of DataModeling Star SchemaSnowflake modeling FACT Dimensions tables and LogicalPhysical data modeling Environment R3x Erwin 952 MDM QlikView   PLSQL Tableau Teradata 141 JSON HADOOP HDFS MapReduce SQL Server   Scala NLP SSMS ERP CRM Netezza Pandas SAS SPSS Java IDE Cassandra SQL PLSQL AWS SSRS Informatica PIG Spark Azure R Studio MongoDB MAHOUT JAVA HIVE AWS Redshift SQL Developer against ODS Western Digital Burlington NJ April 2012 to December 2014 Burlington NJ April 2012 to December 2014 Responsibilities Design database data models ETL processes data warehouse applications and business intelligence BI reports through the use of best practices and tools including Erwin SQL SSIS SSRS and OLAP OLTP Transformed Logical Data Model to Physical Data Model ensuring the Primary Key and Foreign Key relationships in PDM Consistency of definitions of Data Attributes and Primary Index Considerations Validated the data of reports by writing SQL queries in PLSQL Developer against ODS Developed MapReduce programs to parse the raw data populate staging tables and store the refined data in partitioned tables in the EDW Involved with Data Analysis primarily Identifying Data Sets Source Data Source Meta Data Data Definitions and Data Formats Data Analyst Macys Inc Duluth GA January 2010 to March 2012 Roles Responsibilities Analyzed Trading mechanism for realtime transactions and build collateral management tools Compiled data from various sources to perform complex analysis for actionable results Utilized machine learning algorithms such as linear regression multivariate regression naive bayes Random Forests Kmeans KNN for data analysis Measured Efficiency of HadoopHive environment ensuring SLA is met Developed Spark code using Scala and SparkSQLStreaming for faster processing of data Prepared Spark build from the source code and ran the PIG Scripts using Spark rather using MR jobs for better performance Analyzing the system for new enhancementsfunctionalities and perform Impact analysis of the application for implementing ETL changes Imported data using Sqoop to load data from MySQL to HDFS on regular basis Developed Scripts and Batch Job to schedule various Hadoop Program Used TensorFlow to train the model from insightful data and look at thousands of examples Designing developing and optimizing SQL code DDL DML Building performant scalable ETL processes to load cleanse and validate data Expertise in Data archival and Data migration adhoc reporting and code utilizing SAS on UNIX and Windows Environments Tested and debugged SAS programs against the test data Processed the data in SAS for the given requirement using SAS programming concepts Imported and Exported data files to and from SAS using Proc Import and Proc Export from Excel and various delimited textbased data files such as TXT tab delimited and CSV comma delimited files into SAS datasets for analysis Expertise in producing RTF PDF HTML files using SAS ODS facility Providing support for data processes This will involve monitoring data profiling database usage trouble shooting tuning and ensuring data integrity Participating in the full software development lifecycle with requirements solution design development QA implementation and product support using Scrum and other Agile methodologies Collaborate with team members and stakeholders in design and development of data environment Learning new tools and skillsets as needs arise Preparing associated documentation for specifications requirements and testing Optimizing the Tensorflow Model for an efficiency Used Tensorflow for text summarization Used Spark API over Cloudera Hadoop YARN to perform analytics on data in Hive Wrote Hive queries for data analysis to meet the business requirements Developed Kafka producer and consumers for message handling Responsible for analyzing multiplatform applications using python Used storm for an automatic mechanism to analyze large amounts of nonunique data points with low latency and high throughput Developed MapReduce jobs in Python for data cleaning and data processing Environment Machine learning AWS MS Azure Cassandra SAS Spark HDFS Hive Pig Linux Anaconda Python MySQL Eclipse PLSQL SQL connector SparkML Python Developer HCL India April 2008 to December 2009 Roles Responsibilities Worked on the project from gathering requirements to developing the entire application Worked on Anaconda Python Environment Created activated and programmed in Anaconda environment Wrote programs for performance calculations using NumPy and SQLAlchemy Wrote python routines to log into the websites and fetch data for selected options Used python modules of urllib urllib2 Requests for web crawling Experience using all these ML techniques clustering regression classification graphical models Extensive experience in Text Analytics developing different Statistical Machine Learning Data mining solutions to various business problems and generating data visualizations using R Python and Tableau Used with other packages such as Beautiful Soup for data parsing Involved in development of Web Services using SOAP for sending and getting data from the external interface in the XML format Used with other packages such as Beautiful Soup for data parsing Worked on development of SQL and stored procedures on MYSQL Analyzed the code completely and have reduced the code redundancy to the optimal level Design and build a text classification application using different text classification models Used Jira for defect tracking and project management Worked on writing and as well as read data from CSV and excel file formats Involved in Sprint planning sessions and participated in the daily Agile SCRUM meetings Conducted every day scrum as part of the SCRUM Master role Developed the project in Linux environment Worked on resulting reports of the application Performed QA testing on the application Held meetings with client and worked for the entire project with limited help from the client Environment Python Anaconda Sypder IDE Windows 7 Teradata Requests urllib urllib2 Beautiful Soup Tableau python libraries such as NumPy SQL Alchemy   Skills SQL 10 years APACHE HADOOP MAPREDUCE 8 years MapReduce 8 years MAPREDUCE 8 years PLSQL 8 years Additional Information Skills APACHE HADOOP MAPREDUCE 7 years MapReduce 7 years OLAP 7 years ONLINE ANALYTICAL PROCESSING 7 years PLSQL 7 years Technical Skills Data Analytics ToolsProgramming Python numpy scipy pandas Gensim Keras R Caret Weka ggplot MATLAB Microsoft SQL Server Oracle PLSQL Python SQL PLSQL TSQL UNIX shell scripting Java SAS Big Data Techs Hadoop Hive HDFS MapReduce Pig Kafka HBase Cassandra MongoDB Analysis and Modeling Tools Erwin Sybase Power Designer Oracle Designer BPwin Rational Rose ERStudio TOAD MS Visio ETL Tools Informatica Power Center Data Stage 75 Ab Initio Talend OLAP Tools MS SQL Analysis Manager DB2 OLAP CognosPowerplay Languages SQL PLSQL TSQL XML HTML UNIX Shell Scripting C C AWK Databases Oracle12c11g10g9i8i807x Teradata140 DB2 UDB 81 MS SQLServer 05 Netezaa and Sybase ASE 125315 Informix 9 HBase MongoDB Cassandra Amazon Redshift Operating Systems Windows 20078 UNIX SunSolaris HPUX Windows NTXPVista MSDOS Project Execution Methodologies Ralph Kimball and Bill Inmon data warehousing methodology Rational Unified Process RUP Rapid Application Development RAD Joint Application Development JAD Reporting Tools Business ObjectsXIR2655051 Cognos Impromptu 706050 Informatica Analytics Delivery Platform MicroStrategy Tableau Tools MSOffice suite Word Excel MS Project and Outlook VSS Others Spark   Scala NLP MariaDB Azure SAS IDE Microsoft Azure AWS Data ScientistMachine Learning Engineer Macys Inc Duluth GA Close to Eight years of expert involvement in IT in which I have 3 years of knowledge in Data Mining Machine Learning and Spark Development with big datasets of Structured and Unstructured Data Data Acquisition Data Validation Predictive demonstrating Data Visualization Capable in measurable programming languages like R and Python Proficient in managing entire data science project life cycle and actively involved in all the phases of project life cycle Extensive experience in Text Analytics developing different Statistical Machine Learning Data mining solutions to various business problems and generating data visualizations using R Python and Tableau Adept and deep understanding of Statistical modeling Multivariate Analysis model testing problem analysis model comparison and validation Skilled in performing data parsing data manipulation and data preparation with methods including describe data contents compute descriptive statistics of data regex split and combine Remap merge subset reindex melt and reshape Experience in using various packages in R and libraries in Python Working knowledge in Hadoop Hive and NOSQL databases like Cassandra and HBase Hands on experience in implementing LDA Naive Bayes and skilled in Random Forests Decision Trees Linear and Logistic Regression SVM Clustering neural networks Principle Component Analysis and good knowledge on Recommender Systems Good industry knowledge analytical and problemsolving skills and ability to work well within a team as well as an individual Highly creative innovative committed intellectually curious business savvy with effective communication and interpersonal skills I can be able to quickly adapt the new work pace and learning",
    "extracted_keywords": [
        "Sr",
        "Data",
        "ScientistMachine",
        "Learning",
        "Engineer",
        "Sr",
        "Data",
        "ScientistMachine",
        "Learning",
        "Engineer",
        "Sr",
        "Data",
        "ScientistMachine",
        "Learning",
        "Engineer",
        "SEI",
        "Investments",
        "Developed",
        "Map",
        "ReduceSpark",
        "Python",
        "modules",
        "machine",
        "analytics",
        "Hadoop",
        "AWS",
        "Pythonbased",
        "forest",
        "Python",
        "Perform",
        "analysis",
        "hypothesis",
        "testing",
        "cluster",
        "analysis",
        "correlation",
        "ANOVA",
        "ROC",
        "Curve",
        "models",
        "Supervised",
        "Unsupervised",
        "Machine",
        "Learning",
        "Text",
        "Analytics",
        "Time",
        "Series",
        "forecasting",
        "model",
        "data",
        "machine",
        "learning",
        "algorithms",
        "Python",
        "Pandas",
        "scikit",
        "numPy",
        "keras",
        "libraries",
        "Anaconda",
        "Jupyter",
        "Programming",
        "Linear",
        "Logistic",
        "Regressions",
        "KNN",
        "KMeans",
        "SentimentText",
        "Analytics",
        "NLP",
        "Nave",
        "Bayes",
        "Time",
        "Series",
        "forecasting",
        "glm",
        "Arima",
        "Apriori",
        "Forecast",
        "Extracting",
        "data",
        "Big",
        "Data",
        "Hadoop",
        "Data",
        "Lake",
        "Excel",
        "Analyzing",
        "Cleaning",
        "Sorting",
        "Merging",
        "Reporting",
        "dashboards",
        "Base",
        "SAS",
        "SAS",
        "Macros",
        "SQL",
        "Hive",
        "SAS",
        "VA",
        "SAS",
        "Excel",
        "MapReduceSpark",
        "Python",
        "modules",
        "machine",
        "analytics",
        "Hadoop",
        "AWS",
        "Pythonbased",
        "forest",
        "Python",
        "pandas",
        "numpy",
        "matplotlib",
        "scikitlearn",
        "NLTK",
        "Python",
        "machine",
        "algorithms",
        "machine",
        "learning",
        "algorithms",
        "linear",
        "regression",
        "multivariate",
        "regression",
        "bayes",
        "Random",
        "Forests",
        "Kmeans",
        "KNN",
        "data",
        "analysis",
        "clustering",
        "models",
        "Mllib",
        "fault",
        "code",
        "occurrences",
        "Spark",
        "Mllib",
        "Conducting",
        "studies",
        "plots",
        "advance",
        "data",
        "mining",
        "modelling",
        "techniques",
        "solution",
        "quality",
        "performance",
        "data",
        "experience",
        "design",
        "implementation",
        "models",
        "models",
        "enterprise",
        "data",
        "model",
        "metadata",
        "solution",
        "data",
        "life",
        "cycle",
        "management",
        "Big",
        "Data",
        "environments",
        "data",
        "datawarehouses",
        "Amazon",
        "Redshift",
        "system",
        "architecture",
        "Amazon",
        "EC2",
        "solution",
        "client",
        "Simple",
        "Map",
        "Reduce",
        "Jobs",
        "Hive",
        "Pig",
        "Map",
        "Reduce",
        "jobs",
        "java",
        "data",
        "data",
        "sets",
        "machine",
        "techniques",
        "models",
        "models",
        "models",
        "bestinclass",
        "modeling",
        "techniques",
        "Map",
        "ReduceSpark",
        "Python",
        "modules",
        "machine",
        "analytics",
        "Hadoop",
        "AWS",
        "Teradata15",
        "tools",
        "utilities",
        "Teradata",
        "Viewpoint",
        "Multi",
        "Load",
        "ARC",
        "Teradata",
        "Administrator",
        "BTEQ",
        "Teradata",
        "Utilities",
        "Spark",
        "Scala",
        "Hadoop",
        "HBase",
        "Kafka",
        "Spark",
        "Streaming",
        "Caffe",
        "TensorFlow",
        "Python",
        "variety",
        "machine",
        "learning",
        "methods",
        "classifications",
        "regressions",
        "reduction",
        "LINUX",
        "Shell",
        "scripts",
        "NZSQLNZLOAD",
        "utilities",
        "data",
        "files",
        "Netezza",
        "database",
        "Information",
        "extraction",
        "kinds",
        "text",
        "documents",
        "NLP",
        "text",
        "mining",
        "expressions",
        "Tableau",
        "Desktop",
        "filters",
        "downs",
        "Data",
        "visualizations",
        "Dash",
        "Boards",
        "views",
        "data",
        "options",
        "query",
        "display",
        "analyze",
        "sort",
        "group",
        "charts",
        "monitor",
        "measure",
        "goals",
        "patterns",
        "Work",
        "Experience",
        "Sr",
        "Data",
        "ScientistMachine",
        "Learning",
        "Engineer",
        "SEI",
        "Investments",
        "Malvern",
        "PA",
        "March",
        "Present",
        "Responsibilities",
        "Map",
        "ReduceSpark",
        "Python",
        "modules",
        "machine",
        "analytics",
        "Hadoop",
        "AWS",
        "Pythonbased",
        "forest",
        "Python",
        "Perform",
        "analysis",
        "hypothesis",
        "testing",
        "cluster",
        "analysis",
        "correlation",
        "ANOVA",
        "ROC",
        "Curve",
        "models",
        "Supervised",
        "Unsupervised",
        "Machine",
        "Learning",
        "Text",
        "Analytics",
        "Time",
        "Series",
        "forecasting",
        "model",
        "data",
        "machine",
        "learning",
        "algorithms",
        "Python",
        "Pandas",
        "scikit",
        "numPy",
        "keras",
        "libraries",
        "Anaconda",
        "Jupyter",
        "Programming",
        "Linear",
        "Logistic",
        "Regressions",
        "KNN",
        "KMeans",
        "SentimentText",
        "Analytics",
        "NLP",
        "Nave",
        "Bayes",
        "Time",
        "Series",
        "forecasting",
        "glm",
        "Arima",
        "Apriori",
        "Forecast",
        "Extracting",
        "data",
        "Big",
        "Data",
        "Hadoop",
        "Data",
        "Lake",
        "Excel",
        "Analyzing",
        "Cleaning",
        "Sorting",
        "Merging",
        "Reporting",
        "dashboards",
        "Base",
        "SAS",
        "SAS",
        "Macros",
        "SQL",
        "Hive",
        "SAS",
        "VA",
        "SAS",
        "Excel",
        "MapReduceSpark",
        "Python",
        "modules",
        "machine",
        "analytics",
        "Hadoop",
        "AWS",
        "Pythonbased",
        "forest",
        "Python",
        "pandas",
        "numpy",
        "matplotlib",
        "scikitlearn",
        "NLTK",
        "Python",
        "machine",
        "algorithms",
        "machine",
        "learning",
        "algorithms",
        "linear",
        "regression",
        "multivariate",
        "regression",
        "bayes",
        "Random",
        "Forests",
        "Kmeans",
        "KNN",
        "data",
        "analysis",
        "clustering",
        "models",
        "Mllib",
        "fault",
        "code",
        "occurrences",
        "Spark",
        "Mllib",
        "Conducting",
        "studies",
        "plots",
        "advance",
        "data",
        "mining",
        "modelling",
        "techniques",
        "solution",
        "quality",
        "performance",
        "data",
        "experience",
        "design",
        "implementation",
        "models",
        "models",
        "enterprise",
        "data",
        "model",
        "metadata",
        "solution",
        "data",
        "life",
        "cycle",
        "management",
        "Big",
        "Data",
        "environments",
        "data",
        "datawarehouses",
        "Amazon",
        "Redshift",
        "system",
        "architecture",
        "Amazon",
        "EC2",
        "solution",
        "client",
        "Simple",
        "Map",
        "Reduce",
        "Jobs",
        "Hive",
        "Pig",
        "MapReduce",
        "jobs",
        "java",
        "data",
        "data",
        "sets",
        "machine",
        "techniques",
        "models",
        "models",
        "models",
        "bestinclass",
        "modeling",
        "techniques",
        "MapReduceSpark",
        "Python",
        "modules",
        "machine",
        "analytics",
        "Hadoop",
        "AWS",
        "Teradata15",
        "tools",
        "utilities",
        "Teradata",
        "Viewpoint",
        "Multi",
        "Load",
        "ARC",
        "Teradata",
        "Administrator",
        "BTEQ",
        "Teradata",
        "Utilities",
        "Spark",
        "Scala",
        "Hadoop",
        "HBase",
        "Kafka",
        "Spark",
        "Streaming",
        "Caffe",
        "TensorFlow",
        "Python",
        "variety",
        "machine",
        "learning",
        "methods",
        "classifications",
        "regressions",
        "reduction",
        "LINUX",
        "Shell",
        "scripts",
        "NZSQLNZLOAD",
        "utilities",
        "data",
        "files",
        "Netezza",
        "database",
        "Information",
        "extraction",
        "kinds",
        "text",
        "documents",
        "NLP",
        "text",
        "mining",
        "expressions",
        "Tableau",
        "Desktop",
        "filters",
        "downs",
        "Data",
        "visualizations",
        "Dash",
        "Boards",
        "views",
        "data",
        "options",
        "query",
        "display",
        "analyze",
        "sort",
        "group",
        "charts",
        "monitor",
        "measure",
        "goals",
        "patterns",
        "Environment",
        "Python",
        "SQL",
        "Oracle",
        "12c",
        "Netezza",
        "SQL",
        "Server",
        "Informatica",
        "Java",
        "SSRS",
        "PLSQL",
        "TSQL",
        "Tableau",
        "regression",
        "Scala",
        "NLP",
        "Spark",
        "Kafka",
        "MongoDB",
        "Workday",
        "regression",
        "Hadoop",
        "Hive",
        "TensorFlow",
        "Teradata",
        "IDE",
        "forest",
        "OLAP",
        "Azure",
        "SAP",
        "CRM",
        "HDFS",
        "ODS",
        "NLTK",
        "SVM",
        "JSON",
        "Tableau",
        "XML",
        "AWS",
        "Pandas",
        "Cassandra",
        "MapReduce",
        "Tableau",
        "Caffe",
        "Sr",
        "Data",
        "Scientist",
        "Machine",
        "Learning",
        "Engineer",
        "UHG",
        "Brooklyn",
        "NY",
        "January",
        "February",
        "Responsibilities",
        "endtoend",
        "systems",
        "Data",
        "Analytics",
        "Data",
        "Automation",
        "custom",
        "visualization",
        "tools",
        "R",
        "Hadoop",
        "MongoDB",
        "Cassandra",
        "Building",
        "models",
        "tools",
        "SAS",
        "R",
        "data",
        "data",
        "platform",
        "data",
        "formats",
        "JSON",
        "XML",
        "machine",
        "algorithms",
        "R",
        "Spark",
        "test",
        "data",
        "analytics",
        "performance",
        "bottlenecks",
        "Machine",
        "Learning",
        "Algorithms",
        "Decision",
        "Trees",
        "Random",
        "Forest",
        "Gradient",
        "Boosting",
        "Support",
        "Vector",
        "Machines",
        "K",
        "Mean",
        "Clustering",
        "Nave",
        "Bayes",
        "Bayesian",
        "Belief",
        "Networks",
        "Artificial",
        "Neural",
        "Networks",
        "models",
        "Machine",
        "Supervised",
        "R",
        "Machine",
        "Motor",
        "B2B",
        "analytics",
        "R",
        "Tableau",
        "data",
        "cleaning",
        "data",
        "preparation",
        "tasks",
        "data",
        "data",
        "R",
        "R",
        "results",
        "Mahout",
        "data",
        "sets",
        "features",
        "ML",
        "Mahout",
        "Utilized",
        "Spark",
        "Scala",
        "Hadoop",
        "HBase",
        "Kafka",
        "Spark",
        "Streaming",
        "Python",
        "variety",
        "machine",
        "learning",
        "methods",
        "classifications",
        "regressions",
        "reduction",
        "tables",
        "Hive",
        "Hive",
        "tables",
        "data",
        "hive",
        "queries",
        "join",
        "scenarios",
        "Performed",
        "Kmeans",
        "Multivariate",
        "analysis",
        "Support",
        "Vector",
        "Machines",
        "R",
        "models",
        "analytics",
        "algorithms",
        "regression",
        "decision",
        "trees",
        "text",
        "mining",
        "tools",
        "R",
        "Tableau",
        "insights",
        "recommendations",
        "Spark",
        "jobs",
        "Scala",
        "data",
        "schema",
        "AWS",
        "Redshift",
        "storage",
        "retrieval",
        "data",
        "External",
        "Loaders",
        "Multi",
        "Load",
        "T",
        "Pump",
        "Fast",
        "Load",
        "data",
        "Teradata141Database",
        "Troubleshooting",
        "quality",
        "control",
        "data",
        "transformations",
        "loading",
        "migration",
        "Oracle",
        "systems",
        "Netezza",
        "EDW",
        "S3",
        "Bucket",
        "jars",
        "input",
        "datasets",
        "Dynamo",
        "DB",
        "output",
        "input",
        "data",
        "Worked",
        "attribute",
        "models",
        "NLP",
        "SVM",
        "Regular",
        "Expressions",
        "product",
        "features",
        "title",
        "description",
        "product",
        "attribute",
        "values",
        "PythonR",
        "data",
        "formats",
        "JSON",
        "XML",
        "machine",
        "algorithms",
        "R",
        "Spark",
        "test",
        "data",
        "analytics",
        "performance",
        "bottlenecks",
        "Supervised",
        "learning",
        "techniques",
        "classifiers",
        "networks",
        "patters",
        "data",
        "Tableau",
        "visualizations",
        "dashboards",
        "Tableau",
        "Desktop",
        "Tableau",
        "data",
        "sources",
        "Data",
        "Blending",
        "data",
        "warehousing",
        "system",
        "spark",
        "2x",
        "streaming",
        "Scala",
        "Java",
        "Strong",
        "Knowledge",
        "concepts",
        "DataModeling",
        "Star",
        "SchemaSnowflake",
        "FACT",
        "Dimensions",
        "tables",
        "data",
        "Environment",
        "R3x",
        "Erwin",
        "MDM",
        "QlikView",
        "PLSQL",
        "Tableau",
        "Teradata",
        "JSON",
        "HADOOP",
        "HDFS",
        "MapReduce",
        "SQL",
        "Server",
        "Scala",
        "NLP",
        "SSMS",
        "ERP",
        "CRM",
        "Netezza",
        "Pandas",
        "SAS",
        "SPSS",
        "Java",
        "IDE",
        "Cassandra",
        "SQL",
        "PLSQL",
        "AWS",
        "SSRS",
        "Informatica",
        "PIG",
        "Spark",
        "Azure",
        "R",
        "Studio",
        "MongoDB",
        "MAHOUT",
        "HIVE",
        "AWS",
        "SQL",
        "Developer",
        "ODS",
        "Western",
        "Digital",
        "Burlington",
        "NJ",
        "April",
        "December",
        "Burlington",
        "NJ",
        "April",
        "December",
        "Responsibilities",
        "Design",
        "database",
        "data",
        "models",
        "ETL",
        "data",
        "warehouse",
        "applications",
        "business",
        "intelligence",
        "BI",
        "use",
        "practices",
        "tools",
        "Erwin",
        "SQL",
        "SSIS",
        "SSRS",
        "OLAP",
        "OLTP",
        "Transformed",
        "Logical",
        "Data",
        "Model",
        "Physical",
        "Data",
        "Model",
        "Primary",
        "Key",
        "Foreign",
        "Key",
        "relationships",
        "PDM",
        "Consistency",
        "definitions",
        "Data",
        "Attributes",
        "Primary",
        "Index",
        "Considerations",
        "data",
        "reports",
        "SQL",
        "queries",
        "PLSQL",
        "Developer",
        "ODS",
        "MapReduce",
        "programs",
        "data",
        "staging",
        "tables",
        "data",
        "tables",
        "EDW",
        "Data",
        "Analysis",
        "Data",
        "Sets",
        "Source",
        "Data",
        "Source",
        "Meta",
        "Data",
        "Data",
        "Definitions",
        "Data",
        "Formats",
        "Data",
        "Analyst",
        "Macys",
        "Inc",
        "Duluth",
        "GA",
        "January",
        "March",
        "Roles",
        "Responsibilities",
        "Trading",
        "mechanism",
        "transactions",
        "management",
        "tools",
        "data",
        "sources",
        "analysis",
        "results",
        "machine",
        "learning",
        "algorithms",
        "linear",
        "regression",
        "multivariate",
        "regression",
        "bayes",
        "Random",
        "Forests",
        "Kmeans",
        "KNN",
        "data",
        "analysis",
        "Efficiency",
        "HadoopHive",
        "environment",
        "SLA",
        "Developed",
        "Spark",
        "code",
        "Scala",
        "SparkSQLStreaming",
        "processing",
        "data",
        "Prepared",
        "Spark",
        "source",
        "code",
        "PIG",
        "Scripts",
        "Spark",
        "MR",
        "jobs",
        "performance",
        "system",
        "enhancementsfunctionalities",
        "Impact",
        "analysis",
        "application",
        "ETL",
        "changes",
        "data",
        "Sqoop",
        "data",
        "MySQL",
        "HDFS",
        "basis",
        "Scripts",
        "Batch",
        "Job",
        "Hadoop",
        "Program",
        "TensorFlow",
        "model",
        "data",
        "thousands",
        "examples",
        "SQL",
        "code",
        "DDL",
        "DML",
        "ETL",
        "processes",
        "cleanse",
        "data",
        "Expertise",
        "Data",
        "archival",
        "Data",
        "migration",
        "adhoc",
        "reporting",
        "code",
        "SAS",
        "UNIX",
        "Windows",
        "Environments",
        "SAS",
        "programs",
        "test",
        "data",
        "data",
        "SAS",
        "requirement",
        "SAS",
        "programming",
        "concepts",
        "data",
        "files",
        "SAS",
        "Proc",
        "Import",
        "Proc",
        "Export",
        "Excel",
        "data",
        "files",
        "TXT",
        "tab",
        "CSV",
        "comma",
        "files",
        "SAS",
        "datasets",
        "analysis",
        "Expertise",
        "RTF",
        "PDF",
        "HTML",
        "files",
        "SAS",
        "ODS",
        "facility",
        "support",
        "data",
        "processes",
        "data",
        "profiling",
        "database",
        "usage",
        "trouble",
        "data",
        "integrity",
        "software",
        "development",
        "lifecycle",
        "requirements",
        "solution",
        "design",
        "development",
        "QA",
        "implementation",
        "product",
        "support",
        "Scrum",
        "methodologies",
        "team",
        "members",
        "stakeholders",
        "design",
        "development",
        "data",
        "environment",
        "tools",
        "skillsets",
        "needs",
        "documentation",
        "specifications",
        "requirements",
        "testing",
        "Tensorflow",
        "Model",
        "efficiency",
        "Tensorflow",
        "text",
        "summarization",
        "Spark",
        "API",
        "Cloudera",
        "Hadoop",
        "YARN",
        "analytics",
        "data",
        "Hive",
        "Wrote",
        "Hive",
        "data",
        "analysis",
        "business",
        "requirements",
        "Kafka",
        "producer",
        "consumers",
        "message",
        "multiplatform",
        "applications",
        "python",
        "storm",
        "mechanism",
        "amounts",
        "data",
        "points",
        "latency",
        "throughput",
        "Developed",
        "MapReduce",
        "jobs",
        "Python",
        "data",
        "cleaning",
        "data",
        "Environment",
        "Machine",
        "AWS",
        "MS",
        "Azure",
        "Cassandra",
        "SAS",
        "Spark",
        "HDFS",
        "Hive",
        "Pig",
        "Linux",
        "Anaconda",
        "Python",
        "MySQL",
        "Eclipse",
        "PLSQL",
        "SQL",
        "connector",
        "SparkML",
        "Python",
        "Developer",
        "HCL",
        "India",
        "April",
        "December",
        "Roles",
        "Responsibilities",
        "project",
        "requirements",
        "application",
        "Anaconda",
        "Python",
        "Environment",
        "Anaconda",
        "environment",
        "Wrote",
        "programs",
        "performance",
        "calculations",
        "NumPy",
        "SQLAlchemy",
        "python",
        "routines",
        "websites",
        "data",
        "options",
        "modules",
        "Requests",
        "web",
        "Experience",
        "ML",
        "techniques",
        "regression",
        "classification",
        "models",
        "experience",
        "Text",
        "Analytics",
        "Machine",
        "Learning",
        "Data",
        "mining",
        "solutions",
        "business",
        "problems",
        "data",
        "visualizations",
        "R",
        "Python",
        "Tableau",
        "packages",
        "Beautiful",
        "Soup",
        "data",
        "development",
        "Web",
        "Services",
        "SOAP",
        "data",
        "interface",
        "XML",
        "format",
        "packages",
        "Beautiful",
        "Soup",
        "data",
        "development",
        "SQL",
        "procedures",
        "MYSQL",
        "code",
        "code",
        "redundancy",
        "level",
        "Design",
        "text",
        "classification",
        "application",
        "text",
        "classification",
        "models",
        "Jira",
        "tracking",
        "project",
        "management",
        "writing",
        "data",
        "CSV",
        "file",
        "formats",
        "Sprint",
        "planning",
        "sessions",
        "Agile",
        "SCRUM",
        "meetings",
        "day",
        "part",
        "SCRUM",
        "Master",
        "role",
        "project",
        "Linux",
        "environment",
        "reports",
        "application",
        "Performed",
        "QA",
        "testing",
        "application",
        "meetings",
        "client",
        "project",
        "help",
        "client",
        "Environment",
        "Python",
        "Anaconda",
        "Sypder",
        "IDE",
        "Windows",
        "Teradata",
        "Requests",
        "Beautiful",
        "Soup",
        "Tableau",
        "NumPy",
        "SQL",
        "Alchemy",
        "Skills",
        "SQL",
        "years",
        "APACHE",
        "HADOOP",
        "MAPREDUCE",
        "years",
        "MapReduce",
        "years",
        "MAPREDUCE",
        "years",
        "PLSQL",
        "years",
        "Additional",
        "Information",
        "Skills",
        "APACHE",
        "HADOOP",
        "MAPREDUCE",
        "years",
        "MapReduce",
        "years",
        "years",
        "ANALYTICAL",
        "PROCESSING",
        "years",
        "years",
        "Technical",
        "Skills",
        "Data",
        "Analytics",
        "ToolsProgramming",
        "Python",
        "scipy",
        "Gensim",
        "Keras",
        "R",
        "Caret",
        "Weka",
        "ggplot",
        "MATLAB",
        "Microsoft",
        "SQL",
        "Server",
        "Oracle",
        "PLSQL",
        "Python",
        "SQL",
        "PLSQL",
        "TSQL",
        "UNIX",
        "shell",
        "scripting",
        "Java",
        "SAS",
        "Big",
        "Data",
        "Techs",
        "Hadoop",
        "Hive",
        "HDFS",
        "MapReduce",
        "Pig",
        "Kafka",
        "HBase",
        "Cassandra",
        "MongoDB",
        "Analysis",
        "Modeling",
        "Tools",
        "Erwin",
        "Sybase",
        "Power",
        "Designer",
        "Oracle",
        "Designer",
        "BPwin",
        "Rational",
        "Rose",
        "ERStudio",
        "TOAD",
        "MS",
        "Visio",
        "ETL",
        "Tools",
        "Informatica",
        "Power",
        "Center",
        "Data",
        "Stage",
        "Ab",
        "Initio",
        "Talend",
        "OLAP",
        "Tools",
        "MS",
        "SQL",
        "Analysis",
        "Manager",
        "DB2",
        "CognosPowerplay",
        "Languages",
        "SQL",
        "PLSQL",
        "TSQL",
        "XML",
        "HTML",
        "UNIX",
        "Shell",
        "Scripting",
        "C",
        "C",
        "AWK",
        "DB2",
        "UDB",
        "MS",
        "SQLServer",
        "Netezaa",
        "Sybase",
        "ASE",
        "Informix",
        "HBase",
        "MongoDB",
        "Cassandra",
        "Amazon",
        "Redshift",
        "Operating",
        "Systems",
        "Windows",
        "UNIX",
        "SunSolaris",
        "HPUX",
        "Windows",
        "NTXPVista",
        "MSDOS",
        "Project",
        "Execution",
        "Methodologies",
        "Ralph",
        "Kimball",
        "Bill",
        "Inmon",
        "warehousing",
        "methodology",
        "Rational",
        "Unified",
        "Process",
        "RUP",
        "Rapid",
        "Application",
        "Development",
        "RAD",
        "Joint",
        "Application",
        "Development",
        "JAD",
        "Reporting",
        "Tools",
        "Business",
        "ObjectsXIR2655051",
        "Cognos",
        "Impromptu",
        "Informatica",
        "Analytics",
        "Delivery",
        "Platform",
        "MicroStrategy",
        "Tableau",
        "Tools",
        "MSOffice",
        "suite",
        "Word",
        "Excel",
        "MS",
        "Project",
        "Outlook",
        "VSS",
        "Others",
        "Spark",
        "Scala",
        "NLP",
        "Azure",
        "SAS",
        "IDE",
        "Microsoft",
        "Azure",
        "AWS",
        "Data",
        "ScientistMachine",
        "Learning",
        "Engineer",
        "Macys",
        "Inc",
        "Duluth",
        "GA",
        "years",
        "expert",
        "involvement",
        "IT",
        "years",
        "knowledge",
        "Data",
        "Mining",
        "Machine",
        "Learning",
        "Spark",
        "Development",
        "datasets",
        "Structured",
        "Unstructured",
        "Data",
        "Data",
        "Acquisition",
        "Data",
        "Validation",
        "Predictive",
        "demonstrating",
        "Data",
        "Visualization",
        "Capable",
        "programming",
        "languages",
        "R",
        "Python",
        "Proficient",
        "data",
        "science",
        "project",
        "life",
        "cycle",
        "phases",
        "project",
        "life",
        "cycle",
        "experience",
        "Text",
        "Analytics",
        "Machine",
        "Learning",
        "Data",
        "mining",
        "solutions",
        "business",
        "problems",
        "data",
        "visualizations",
        "R",
        "Python",
        "Tableau",
        "Adept",
        "understanding",
        "modeling",
        "Multivariate",
        "Analysis",
        "model",
        "testing",
        "problem",
        "analysis",
        "model",
        "comparison",
        "validation",
        "data",
        "data",
        "manipulation",
        "data",
        "preparation",
        "methods",
        "describe",
        "data",
        "contents",
        "statistics",
        "data",
        "regex",
        "Remap",
        "merge",
        "subset",
        "reindex",
        "melt",
        "Experience",
        "packages",
        "R",
        "libraries",
        "Python",
        "Working",
        "knowledge",
        "Hadoop",
        "Hive",
        "NOSQL",
        "Cassandra",
        "HBase",
        "Hands",
        "experience",
        "LDA",
        "Naive",
        "Bayes",
        "Random",
        "Forests",
        "Decision",
        "Trees",
        "Linear",
        "Logistic",
        "Regression",
        "SVM",
        "networks",
        "Principle",
        "Component",
        "Analysis",
        "knowledge",
        "Recommender",
        "Systems",
        "Good",
        "industry",
        "knowledge",
        "skills",
        "ability",
        "team",
        "business",
        "communication",
        "skills",
        "work",
        "pace"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T23:06:13.426076",
    "resume_data": "Sr Data ScientistMachine Learning Engineer Sr Data ScientistMachine Learning Engineer Sr Data ScientistMachine Learning Engineer SEI Investments Developed Map ReduceSpark Python modules for machine learning predictive analytics in Hadoop on AWS Implemented a Pythonbased distributed random forest via Python streaming Perform Exploratory analysis hypothesis testing cluster analysis correlation ANOVA ROC Curve and build models in Supervised and Unsupervised Machine Learning algorithms Text Analytics Time Series forecasting Prepared the model data and built machine learning algorithms using Python Pandas scikit learn numPy keras etc libraries using Anaconda Jupyter Programming Linear Logistic Regressions KNN KMeans Clustering SentimentText Analytics NLP Nave Bayes Time Series forecasting using lm glm Arima Apriori Forecast Extracting data from Big Data Hadoop Data Lake Excel Analyzing Cleaning Sorting Merging Reporting and creating dashboards using Base SAS SAS Macros SQL Hive SAS VA SAS and Excel Developing MapReduceSpark Python modules for machine learning predictive analytics in Hadoop on AWS Implemented a Pythonbased distributed random forest via Python streaming Used pandas numpy seaborn scipy matplotlib scikitlearn NLTK in Python for developing various machine learning algorithms and utilized machine learning algorithms such as linear regression multivariate regression naive bayes Random Forests Kmeans KNN for data analysis Currently working on building clustering and predictive models using Mllib to predict fault code occurrences using Spark and Mllib Conducting studies rapid plots and using advance data mining and statistical modelling techniques to build solution that optimize the quality and performance of data Demonstrated experience in design and implementation of Statistical models Predictive models enterprise data model metadata solution and data life cycle management in both RDBMS Big Data environments Stored and retrieved data from datawarehouses using Amazon Redshift and designed and implemented system architecture for Amazon EC2 based cloudhosted solution for client Developed Simple to complex Map Reduce Jobs using Hive and Pig and developed multiple Map Reduce jobs in java for data cleaning and preprocessing Analyzing large data sets apply machine learning techniques and develop predictive models statistical models and developing and enhancing statistical models by leveraging bestinclass modeling techniques Developed Map ReduceSpark Python modules for machine learning predictive analytics in Hadoop on AWS Worked with various Teradata15 tools and utilities like Teradata Viewpoint Multi Load ARC Teradata Administrator BTEQ and other Teradata Utilities Utilized Spark Scala Hadoop HBase Kafka Spark Streaming Caffe TensorFlow MLLib Python a broad variety of machine learning methods including classifications regressions dimensionally reduction etc Developed LINUX Shell scripts by using NZSQLNZLOAD utilities to load data from flat files to Netezza database Working on Information extraction from different kinds of text documents using NLP text mining and regular expressions Worked extensively on Tableau Desktop apply filters drill downs and generate Data visualizations interactive Dash Boards that can interact with views of data and worked on several options like query display analyze sort group drill down organize summarize and generate charts monitor and measure goals identify patterns Work Experience Sr Data ScientistMachine Learning Engineer SEI Investments Malvern PA March 2017 to Present Responsibilities Developed Map ReduceSpark Python modules for machine learning predictive analytics in Hadoop on AWS Implemented a Pythonbased distributed random forest via Python streaming Perform Exploratory analysis hypothesis testing cluster analysis correlation ANOVA ROC Curve and build models in Supervised and Unsupervised Machine Learning algorithms Text Analytics Time Series forecasting Prepared the model data and built machine learning algorithms using Python Pandas scikit learn numPy keras etc libraries using Anaconda Jupyter Programming Linear Logistic Regressions KNN KMeans Clustering SentimentText Analytics NLP Nave Bayes Time Series forecasting using lm glm Arima Apriori Forecast Extracting data from Big Data Hadoop Data Lake Excel Analyzing Cleaning Sorting Merging Reporting and creating dashboards using Base SAS SAS Macros SQL Hive SAS VA SAS and Excel Developing MapReduceSpark Python modules for machine learning predictive analytics in Hadoop on AWS Implemented a Pythonbased distributed random forest via Python streaming Used pandas numpy seaborn scipy matplotlib scikitlearn NLTK in Python for developing various machine learning algorithms and utilized machine learning algorithms such as linear regression multivariate regression naive bayes Random Forests Kmeans KNN for data analysis Currently working on building clustering and predictive models using Mllib to predict fault code occurrences using Spark and Mllib Conducting studies rapid plots and using advance data mining and statistical modelling techniques to build solution that optimize the quality and performance of data Demonstrated experience in design and implementation of Statistical models Predictive models enterprise data model metadata solution and data life cycle management in both RDBMS Big Data environments Stored and retrieved data from datawarehouses using Amazon Redshift and designed and implemented system architecture for Amazon EC2 based cloudhosted solution for client Developed Simple to complex Map Reduce Jobs using Hive and Pig and developed multiple MapReduce jobs in java for data cleaning and preprocessing Analyzing large data sets apply machine learning techniques and develop predictive models statistical models and developing and enhancing statistical models by leveraging bestinclass modeling techniques Developed MapReduceSpark Python modules for machine learning predictive analytics in Hadoop on AWS Worked with various Teradata15 tools and utilities like Teradata Viewpoint Multi Load ARC Teradata Administrator BTEQ and other Teradata Utilities Utilized Spark Scala Hadoop HBase Kafka Spark Streaming Caffe TensorFlow MLLib Python a broad variety of machine learning methods including classifications regressions dimensionally reduction etc Developed LINUX Shell scripts by using NZSQLNZLOAD utilities to load data from flat files to Netezza database Working on Information extraction from different kinds of text documents using NLP text mining and regular expressions Worked extensively on Tableau Desktop apply filters drill downs and generate Data visualizations interactive Dash Boards that can interact with views of data and worked on several options like query display analyze sort group drill down organize summarize and generate charts monitor and measure goals identify patterns Environment Python SQL Oracle 12c Netezza SQL Server Informatica Java SSRS PLSQL TSQL Tableau MLLib regression Scala NLP Spark Kafka MongoDB Workday logistic regression Hadoop Hive TensorFlow Teradata IDE random forest OLAP Azure MariaDB SAP CRM HDFS ODS NLTK SVM JSON Tableau XML AWS Redshift Pandas Cassandra MapReduce AWS Tableau Caffe Sr Data Scientist Machine Learning Engineer UHG Brooklyn NY January 2015 to February 2017 Responsibilities Implemented endtoend systems for Data Analytics Data Automation and integrated with custom visualization tools using R Hadoop and MongoDB Cassandra Building predictive models using tools such as SAS R with very granular data stored in big data platform Worked on different data formats such as JSON XML and performed machine learning algorithms in R and used Spark for test data analytics using MLLib and Analyzed the performance to identify bottlenecks Involved working with Machine Learning Algorithms such as Decision Trees Random Forest Gradient Boosting Support Vector Machines K Mean Clustering Nave Bayes Bayesian Belief Networks and Artificial Neural Networks Developed Predictive models Machine learning Supervised and nonSupervised using R for Machine Motor Creating various B2B Predictive and descriptive analytics using R and Tableau and performed data cleaning and data preparation tasks to convert data into a meaningful data set using R Used R to verify the results of Mahout on small data sets Developed missing but important features of ML algorithms to the Mahout Utilized Spark Scala Hadoop HBase Kafka Spark Streaming MLLib Python a broad variety of machine learning methods including classifications regressions dimensionally reduction etc Created partitioned and bucketed tables in Hive Involved in creating Hive internal and external tables loading with data and writing hive queries which involves multiple join scenarios Performed Kmeans clustering Multivariate analysis and Support Vector Machines in R Create analytical models using analytics algorithms like regression decision trees clustering text mining etc and leveraging tools like R Tableau etc to deliver actionable insights and recommendations Developed multiple Spark jobs using Scala for data cleaning and preprocessing Designed the schema configured and deployed AWS Redshift for optimal storage and fast retrieval of data Used External Loaders like Multi Load T Pump and Fast Load to load data into Teradata141Database Involved in Troubleshooting and quality control of data transformations and loading during migration from Oracle systems into Netezza EDW Used S3 Bucket to store the jars input datasets and used Dynamo DB to store the processed output from the input data set Worked on classificationscripting of multiple attribute models by applying textmining NLP SVM and Regular Expressions given product features like title description etc predicting product attribute values using PythonR Worked on different data formats such as JSON XML and performed machine learning algorithms in R Used Spark for test data analytics using MLLib and Analyzed the performance to identify bottlenecks and used Supervised learning techniques such as classifiers and neural networks to identify patters in these data sets Developed Tableau visualizations and dashboards using Tableau Desktop Tableau workbooks from multiple data sources using Data Blending Developing new data warehousing system based on spark 2x and spark streaming utilizing Scala and Java 8 Strong Knowledge on concepts of DataModeling Star SchemaSnowflake modeling FACT Dimensions tables and LogicalPhysical data modeling Environment R3x Erwin 952 MDM QlikView MLLib PLSQL Tableau Teradata 141 JSON HADOOP HDFS MapReduce SQL Server MLLib Scala NLP SSMS ERP CRM Netezza Pandas SAS SPSS Java IDE Cassandra SQL PLSQL AWS SSRS Informatica PIG Spark Azure R Studio MongoDB MAHOUT JAVA HIVE AWS Redshift SQL Developer against ODS Western Digital Burlington NJ April 2012 to December 2014 Burlington NJ April 2012 to December 2014 Responsibilities Design database data models ETL processes data warehouse applications and business intelligence BI reports through the use of best practices and tools including Erwin SQL SSIS SSRS and OLAP OLTP Transformed Logical Data Model to Physical Data Model ensuring the Primary Key and Foreign Key relationships in PDM Consistency of definitions of Data Attributes and Primary Index Considerations Validated the data of reports by writing SQL queries in PLSQL Developer against ODS Developed MapReduce programs to parse the raw data populate staging tables and store the refined data in partitioned tables in the EDW Involved with Data Analysis primarily Identifying Data Sets Source Data Source Meta Data Data Definitions and Data Formats Data Analyst Macys Inc Duluth GA January 2010 to March 2012 Roles Responsibilities Analyzed Trading mechanism for realtime transactions and build collateral management tools Compiled data from various sources to perform complex analysis for actionable results Utilized machine learning algorithms such as linear regression multivariate regression naive bayes Random Forests Kmeans KNN for data analysis Measured Efficiency of HadoopHive environment ensuring SLA is met Developed Spark code using Scala and SparkSQLStreaming for faster processing of data Prepared Spark build from the source code and ran the PIG Scripts using Spark rather using MR jobs for better performance Analyzing the system for new enhancementsfunctionalities and perform Impact analysis of the application for implementing ETL changes Imported data using Sqoop to load data from MySQL to HDFS on regular basis Developed Scripts and Batch Job to schedule various Hadoop Program Used TensorFlow to train the model from insightful data and look at thousands of examples Designing developing and optimizing SQL code DDL DML Building performant scalable ETL processes to load cleanse and validate data Expertise in Data archival and Data migration adhoc reporting and code utilizing SAS on UNIX and Windows Environments Tested and debugged SAS programs against the test data Processed the data in SAS for the given requirement using SAS programming concepts Imported and Exported data files to and from SAS using Proc Import and Proc Export from Excel and various delimited textbased data files such as TXT tab delimited and CSV comma delimited files into SAS datasets for analysis Expertise in producing RTF PDF HTML files using SAS ODS facility Providing support for data processes This will involve monitoring data profiling database usage trouble shooting tuning and ensuring data integrity Participating in the full software development lifecycle with requirements solution design development QA implementation and product support using Scrum and other Agile methodologies Collaborate with team members and stakeholders in design and development of data environment Learning new tools and skillsets as needs arise Preparing associated documentation for specifications requirements and testing Optimizing the Tensorflow Model for an efficiency Used Tensorflow for text summarization Used Spark API over Cloudera Hadoop YARN to perform analytics on data in Hive Wrote Hive queries for data analysis to meet the business requirements Developed Kafka producer and consumers for message handling Responsible for analyzing multiplatform applications using python Used storm for an automatic mechanism to analyze large amounts of nonunique data points with low latency and high throughput Developed MapReduce jobs in Python for data cleaning and data processing Environment Machine learning AWS MS Azure Cassandra SAS Spark HDFS Hive Pig Linux Anaconda Python MySQL Eclipse PLSQL SQL connector SparkML Python Developer HCL India April 2008 to December 2009 Roles Responsibilities Worked on the project from gathering requirements to developing the entire application Worked on Anaconda Python Environment Created activated and programmed in Anaconda environment Wrote programs for performance calculations using NumPy and SQLAlchemy Wrote python routines to log into the websites and fetch data for selected options Used python modules of urllib urllib2 Requests for web crawling Experience using all these ML techniques clustering regression classification graphical models Extensive experience in Text Analytics developing different Statistical Machine Learning Data mining solutions to various business problems and generating data visualizations using R Python and Tableau Used with other packages such as Beautiful Soup for data parsing Involved in development of Web Services using SOAP for sending and getting data from the external interface in the XML format Used with other packages such as Beautiful Soup for data parsing Worked on development of SQL and stored procedures on MYSQL Analyzed the code completely and have reduced the code redundancy to the optimal level Design and build a text classification application using different text classification models Used Jira for defect tracking and project management Worked on writing and as well as read data from CSV and excel file formats Involved in Sprint planning sessions and participated in the daily Agile SCRUM meetings Conducted every day scrum as part of the SCRUM Master role Developed the project in Linux environment Worked on resulting reports of the application Performed QA testing on the application Held meetings with client and worked for the entire project with limited help from the client Environment Python Anaconda Sypder IDE Windows 7 Teradata Requests urllib urllib2 Beautiful Soup Tableau python libraries such as NumPy SQL Alchemy MySQLdb Skills SQL 10 years APACHE HADOOP MAPREDUCE 8 years MapReduce 8 years MAPREDUCE 8 years PLSQL 8 years Additional Information Skills APACHE HADOOP MAPREDUCE 7 years MapReduce 7 years OLAP 7 years ONLINE ANALYTICAL PROCESSING 7 years PLSQL 7 years Technical Skills Data Analytics ToolsProgramming Python numpy scipy pandas Gensim Keras R Caret Weka ggplot MATLAB Microsoft SQL Server Oracle PLSQL Python SQL PLSQL TSQL UNIX shell scripting Java SAS Big Data Techs Hadoop Hive HDFS MapReduce Pig Kafka HBase Cassandra MongoDB Analysis and Modeling Tools Erwin Sybase Power Designer Oracle Designer BPwin Rational Rose ERStudio TOAD MS Visio ETL Tools Informatica Power Center Data Stage 75 Ab Initio Talend OLAP Tools MS SQL Analysis Manager DB2 OLAP CognosPowerplay Languages SQL PLSQL TSQL XML HTML UNIX Shell Scripting C C AWK Databases Oracle12c11g10g9i8i807x Teradata140 DB2 UDB 81 MS SQLServer 201220082005 Netezaa and Sybase ASE 125315 Informix 9 HBase MongoDB Cassandra Amazon Redshift Operating Systems Windows 20078 UNIX SunSolaris HPUX Windows NTXPVista MSDOS Project Execution Methodologies Ralph Kimball and Bill Inmon data warehousing methodology Rational Unified Process RUP Rapid Application Development RAD Joint Application Development JAD Reporting Tools Business ObjectsXIR2655051 Cognos Impromptu 706050 Informatica Analytics Delivery Platform MicroStrategy Tableau Tools MSOffice suite Word Excel MS Project and Outlook VSS Others Spark MLLib Scala NLP MariaDB Azure SAS IDE Microsoft Azure AWS Data ScientistMachine Learning Engineer Macys Inc Duluth GA Close to Eight years of expert involvement in IT in which I have 3 years of knowledge in Data Mining Machine Learning and Spark Development with big datasets of Structured and Unstructured Data Data Acquisition Data Validation Predictive demonstrating Data Visualization Capable in measurable programming languages like R and Python Proficient in managing entire data science project life cycle and actively involved in all the phases of project life cycle Extensive experience in Text Analytics developing different Statistical Machine Learning Data mining solutions to various business problems and generating data visualizations using R Python and Tableau Adept and deep understanding of Statistical modeling Multivariate Analysis model testing problem analysis model comparison and validation Skilled in performing data parsing data manipulation and data preparation with methods including describe data contents compute descriptive statistics of data regex split and combine Remap merge subset reindex melt and reshape Experience in using various packages in R and libraries in Python Working knowledge in Hadoop Hive and NOSQL databases like Cassandra and HBase Hands on experience in implementing LDA Naive Bayes and skilled in Random Forests Decision Trees Linear and Logistic Regression SVM Clustering neural networks Principle Component Analysis and good knowledge on Recommender Systems Good industry knowledge analytical and problemsolving skills and ability to work well within a team as well as an individual Highly creative innovative committed intellectually curious business savvy with effective communication and interpersonal skills I can be able to quickly adapt the new work pace and learning",
    "unique_id": "fea6d370-bd5b-4470-a316-df8bbbe305a6"
}