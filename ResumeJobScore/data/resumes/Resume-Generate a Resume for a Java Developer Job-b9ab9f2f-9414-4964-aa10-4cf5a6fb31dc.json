{
    "clean_data": "Big data Developer Big data span lDeveloperspan Parsippany NJ Over 5 years of industrial experience in Application development and maintenance data management programming data analysis and data visualization Experience in dealing with Apache Hadoop components like HDFS MapReduce Hive HBase Pig Sqoop Oozie Python Spark Storm Cassandra MongoDB Big Data and Big Data Analytics Good understandingknowledge of Hadoop Architecture and various components such as HDFS Job Tracker Task Tracker Name Node Data Node Secondary Name node and MapReduce concepts Experienced managing NoSQL DB on large Hadoop distribution Systems such as Cloudera Hortonworks HDP Map R M series etc Experienced developing Hadoop integration for data ingestion data mapping and data process capabilities Good knowledge in ETL and hands on experience in ETL experience in RDBMS like Oracle MS SQL Server MySQL and DB2 and NoSQL databases like mongo DB and HBase Software development in Java Application Development ClientServer Applications InternetIntranet based database applications and developing testing and implementing application environment using J2EE JDBC JSP Servlets Web Services Oracle PLSQL and Relational Databases Exceptional ability to quickly master new concepts and capable of working in groups as well as independently Excellent interpersonal skills and the ability to work as a part of a team Has good knowledge of virtualization and worked on VMware Virtual Center Hands on experience in installing configuring and using Hadoop ecosystem components like Hadoop MapReduce HDFS HBase Hive Sqoop Pig Zookeeper Spark Kafka and Flume Good Knowledge on Hadoop Cluster architecture and monitoring the cluster Experience in managing Hadoop clusters using Cloudera Manager Experience in using the Impala usage for the highperformance SQL queries Very good experience in complete project life cycle design development testing and implementation of Client Server and Web applications Hands on experience in VPN Putty win SCP VNC viewer etc Hands on experience in application development using Java RDBMS and Linux shell scripting Performed data analysis using MySQL SQL Server Management Studio and Oracle Good knowledge on cloud computing platforms like Amazon Web ServicesAWS Ability to adapt to evolving technology strong sense of responsibility and accomplishment Work Experience Big data Developer Parsippany NJ January 2018 to Present Responsibilities Involved in installing Hadoop Ecosystem components and responsible to manage data coming from various sources Implemented Flume from relational database management systems using Sqoop Used Pig as ETL tool to do transformations event joins filter both traffic and some preaggregations before storing the data onto HDFS Involved in writing Flume and Hive scripts to extract transform and load the data into Database cluster capacity planning performance tuning cluster Monitoring Troubleshooting Developed Spark code using Java and SparkSQLStreaming for faster processing of data Develop ETL Process using SPARK SCALA HIVE and HBASE Extensively involved in writing ETL Specifications for Development and conversion projects Involved in converting HiveSQL queries into Spark transformations using Spark RDDS and Scala Developed various Map Reduce jobs in Java for data cleaning and preprocessing Developed Oozie workflow for scheduling orchestrating the ETL process Experience on loading and transforming of large sets of structured semi structured and optimizing of existing algorithms in Hadoop using Spark Context HiveSQL Data Frames Developed Spark scripts by using Scala shell commands as per the requirement Performed advanced procedures like text analytics and processing using the inmemory computing capabilities of Spark using Scala Created HBase tables to store variable data formats coming from different portfolios Performed real time analytics on HBase using Java API and Rest API Experienced in handling large datasets using Partitions Spark in Memory capabilities Broadcasts in Spark Effective efficient Joins Transformations and other during ingestion process itself Converted the text files and the csv files to parquet form for the analysis of data Prepared the mapping document as in which fields has be used from the HIVE DB and perform the transformations Implemented HDFS and Cassandra with vast amounts of data using Apache Kafka Performed the analytics over the data mining data visualization using Hive Worked in Agile environment and used JIRA as a bugreporting tool for updating the bug report Environment Cloudera Hadoop Hortonworks Linux ETL HDFS Hive Spark Sqoop Flume Zookeeper HBase Hadoop Developer Anthem Insurance Norfolk VA June 2016 to December 2017 Responsibilities Contributed to the development of key data integration and advanced analytics solutions leveraging Apache Hadoop and other big data technologies using major Hadoop Distributions like Hortonworks and Cloudera Worked on Amazon AWS EMR EC2 RDS S3 RedShift etc Tools Hadoop Hive Pig Sqoop Oozie HBase Flume Spark Worked on loading log data directly into HDFS using Flume in Cloudera CDH Involve in loading data from LINUX file system to HDFS in Cloudera CDH Experienced in running Hadoop streaming jobs to process terabytes of xml format data Experience in importing and exporting data into HDFS and assisted in exporting analyzed data to RDBMS using SQOOP in Cloudera Installed and configured MapReduce HIVE and the HDFS Developing Spark scripts by using Java per the requirement to readwrite JSON files Developed and implemented Java code according to MapReduce for the business requirements Worked on Importing and exporting data into HDFS and Hive using Sqoop Worked on Hadoop Administration development NoSQL in in Cloudera Load and transform large sets of structured semi structured and unstructured data Experienced in Hadoop Big Data Integration with TalenD ETL on performing data extract loading and transformation process Experience in development of extracting transforming and loading ETL maintain and support the enterprise data warehouse system and corresponding marts Involved in creating Hive tables loading with data and writing hive queries which will run internally in map Automate all the jobs for pulling data from FTP server to load data into Hive tables using Oozie workflows Created HBase tables to load large sets of structured semistructured and unstructured data coming from UNIX NoSQL and a variety of portfolios Supported codedesign analysis strategy development and project planning Created reports for the BI team using Sqoop to export data into HDFS and Hive Design Implemented Data Warehouse creating facts and dimension tables and loading them using Informatica Power Center Tools fetching data from the OLTP system to the Analytics Data Warehouse Coordinating with business user to gather the new requirements and working with existing issues worked on reading multiple data formats on HDFS using Scala Loading data into parquet files by applying transformation using Impala Involved in converting HiveSQL queries into Spark transformations using Spark RDDs and Scala Analyzed the SQL scripts and designed the solution to implement using Scala Developed multiple POCs using Scala and deployed on the Yarn cluster compared the performance of Spark with Hive and SQLTeradata Designed and Development the Integration APIs using various Data Structure concepts Java Collection Framework along with exception handling mechanism to return response within 500ms Usage of Java Thread concept to handle concurrent request Environment Hadoop 1x2x MR1 Cloudera CDH Hortonworks HDFS Spark Scala Impala HBase 090x Flume 093 Java Sqoop 2x Hive 071 Tableau Bigdata Developer Prime key software solutions Hyd April 2014 to June 2015 Responsibilities Worked on analyzing Hadoop cluster and different big data analytic tools including Pig HBase and Sqoop Responsible for building scalable distributed data solutions using Hadoop Configured a cluster by editing config files such as coresitexml mapredsitexml hdfssitexml and masters slaves Installed and configured Flume Hive Pig Sqoop and HBase on the Hadoop Cluster Responsible to monitor block Scanner Reports on data nodes Implemented 30 nodes CDH3CDH4 Hadoop cluster on CentOS Worked on installing cluster commissioning decommissioning of data node name node recovery capacity planning and slots configuration Involved in loading data from file system to HDFS and Implemented best offer logic using Pig scripts and Pig UDFs Implemented test scripts to support test driven development and continuous integration Responsible to manage data coming from different sources Installed and configured Hive and written Hive UDFs Developed simple and complex MapReduce programs in Java for Data Analysis on different data formats Experience in managing and reviewing Hadoop log files Exported the analyzed data to the relational databases using Sqoop for visualization and to generate reports for the BI team Analyzed large amounts of data sets to determine optimal way to aggregate and report on it Imported data using Sqoop to load data from MySQL to HDFS on regular basis Developing Scripts and Batch Job to schedule various Hadoop Program Responsible for writing Hive queries for data analysis to meet the business requirements Responsible for importing and exporting data into HDFS and Hive using Sqoop Involved in creating Hive tables loading with data and writing hive queries which will run internally in map reduce way Designed and implemented MapReduce based largescale parallel relationlearning system Environment Hadoop HDFS Pig Sqoop Storm VPN MapReduce CentOS Java J2ee Developer Logic solutions Pvt ltd Chennai Tamil Nadu September 2012 to March 2014 Responsibilities Responsible for developing Use Case Class diagrams and Sequence diagrams for the modules using UML and VISIO Used Spring Framework for dependency injection with the help of spring configuration files Developed the presentation layer using JSP HTML CSS and client validations using JavaScript Used HTML CSS JQUERY and JSP to created and UI Involved in Installation and configuration of Tomcat Server Involved in Dynamic form generation Auto completion of forms and user validation functionalities using AJAX Designed developed and maintained the data layer using Hibernate and performed configuration of Struts Application Framework Created stored procedures using SQL for data access layer Worked on tuning of backend stored procedures using TOAD Involved in the configuration management using CVS Developed various test cases and performed unit testing using JUnit Developed Unit test cases for the classes using JUnitEclipse Developed stored procedures to extract data from Oracle database Application developed with strict adherence to J2EE best practices Environment Java J2EE JSP Servlets Hibernate SQL Web Services SOAP WSDL JUnit Tomcat MySQL CVS and Windows Education Bachelor of Technology in Technology JNTU Hyderabad Telangana Skills Cassandra Hdfs Sqoop Hbase Db2 Etl Flume Hadoop Informatica Jms Map reduce Nosql C Hadoop Hbase Hive Html Javascript Perl Pig Additional Information Technical Skills Scripting Languages Python Perl Shell Big Data Technologies Hadoop HDFS Hive Map Reduce Pig Sqoop Flume Zookeeper AWS Programming Languages C Java Web Technologies HTML J2EE CSS JavaScript AJAX Servlets JSP DOM XML XSLT XPATH Java Frameworks Struts Spring Hibernate DB Languages SQL PLSQL Messaging Systems JMS  IBM MQ Databases ETL Oracle 9i10g11g MySQL 52 DB2 Informatica v 8x NoSQL Databases Hbase Cassandra Mango DB Operating Systems Windows Unix and Linux",
    "entities": [
        "Developer Big",
        "Spark Context",
        "Informatica",
        "SPARK",
        "Spark Effective",
        "Hibernate SQL Web Services SOAP WSDL JUnit Tomcat MySQL CVS",
        "BI",
        "HDFS",
        "UNIX",
        "MySQL SQL Server Management Studio",
        "HDFS MapReduce Hive HBase",
        "Present Responsibilities Involved",
        "IBM",
        "Hadoop Ecosystem",
        "Oracle MS",
        "node",
        "Hadoop",
        "Hadoop MapReduce HDFS HBase Hive",
        "CVS Developed",
        "HBase",
        "Developed Oozie",
        "Hadoop Administration",
        "Amazon",
        "the Analytics Data Warehouse Coordinating",
        "Zookeeper HBase Hadoop Developer Anthem Insurance",
        "Sqoop Responsible",
        "VMware Virtual Center Hands",
        "J2EE JDBC JSP Servlets Web Services",
        "UML",
        "Relational Databases",
        "Impala Involved",
        "Hadoop Program Responsible",
        "Sequence",
        "Data Structure",
        "Hadoop Distributions",
        "Hadoop Configured",
        "Linux",
        "Spark RDDS",
        "JSP",
        "HBase Software",
        "Automate",
        "Hadoop Big Data Integration",
        "JUnit Developed Unit",
        "Spark",
        "Java Application Development ClientServer Applications InternetIntranet",
        "Nosql C Hadoop Hbase",
        "500ms Usage of",
        "Database",
        "Sqoop",
        "LINUX",
        "Hadoop Architecture",
        "Oracle Good",
        "log data",
        "Java Collection Framework",
        "HDFS Job Tracker Task Tracker",
        "Oozie",
        "Data Frames Developed Spark",
        "SQL",
        "Cloudera CDH Experienced",
        "OLTP",
        "ETL Specifications for Development",
        "the Hadoop Cluster Responsible",
        "SCP",
        "Hive",
        "Informatica Power Center Tools",
        "SQOOP",
        "Amazon AWS",
        "FTP",
        "Java for Data Analysis",
        "Partitions Spark",
        "HBASE",
        "ETL",
        "DB",
        "JSP DOM XML",
        "ETL Oracle",
        "Apache Hadoop",
        "Performed",
        "Oracle database Application",
        "Impala",
        "TOAD Involved",
        "Struts Application Framework Created",
        "Tomcat Server Involved",
        "Created HBase",
        "MapReduce",
        "UI Involved",
        "NoSQL",
        "SQLTeradata Designed and Development the Integration",
        "Application",
        "Work Experience Big data",
        "Node"
    ],
    "experience": "Experience in dealing with Apache Hadoop components like HDFS MapReduce Hive HBase Pig Sqoop Oozie Python Spark Storm Cassandra MongoDB Big Data and Big Data Analytics Good understandingknowledge of Hadoop Architecture and various components such as HDFS Job Tracker Task Tracker Name Node Data Node Secondary Name node and MapReduce concepts Experienced managing NoSQL DB on large Hadoop distribution Systems such as Cloudera Hortonworks HDP Map R M series etc Experienced developing Hadoop integration for data ingestion data mapping and data process capabilities Good knowledge in ETL and hands on experience in ETL experience in RDBMS like Oracle MS SQL Server MySQL and DB2 and NoSQL databases like mongo DB and HBase Software development in Java Application Development ClientServer Applications InternetIntranet based database applications and developing testing and implementing application environment using J2EE JDBC JSP Servlets Web Services Oracle PLSQL and Relational Databases Exceptional ability to quickly master new concepts and capable of working in groups as well as independently Excellent interpersonal skills and the ability to work as a part of a team Has good knowledge of virtualization and worked on VMware Virtual Center Hands on experience in installing configuring and using Hadoop ecosystem components like Hadoop MapReduce HDFS HBase Hive Sqoop Pig Zookeeper Spark Kafka and Flume Good Knowledge on Hadoop Cluster architecture and monitoring the cluster Experience in managing Hadoop clusters using Cloudera Manager Experience in using the Impala usage for the highperformance SQL queries Very good experience in complete project life cycle design development testing and implementation of Client Server and Web applications Hands on experience in VPN Putty win SCP VNC viewer etc Hands on experience in application development using Java RDBMS and Linux shell scripting Performed data analysis using MySQL SQL Server Management Studio and Oracle Good knowledge on cloud computing platforms like Amazon Web ServicesAWS Ability to adapt to evolving technology strong sense of responsibility and accomplishment Work Experience Big data Developer Parsippany NJ January 2018 to Present Responsibilities Involved in installing Hadoop Ecosystem components and responsible to manage data coming from various sources Implemented Flume from relational database management systems using Sqoop Used Pig as ETL tool to do transformations event joins filter both traffic and some preaggregations before storing the data onto HDFS Involved in writing Flume and Hive scripts to extract transform and load the data into Database cluster capacity planning performance tuning cluster Monitoring Troubleshooting Developed Spark code using Java and SparkSQLStreaming for faster processing of data Develop ETL Process using SPARK SCALA HIVE and HBASE Extensively involved in writing ETL Specifications for Development and conversion projects Involved in converting HiveSQL queries into Spark transformations using Spark RDDS and Scala Developed various Map Reduce jobs in Java for data cleaning and preprocessing Developed Oozie workflow for scheduling orchestrating the ETL process Experience on loading and transforming of large sets of structured semi structured and optimizing of existing algorithms in Hadoop using Spark Context HiveSQL Data Frames Developed Spark scripts by using Scala shell commands as per the requirement Performed advanced procedures like text analytics and processing using the inmemory computing capabilities of Spark using Scala Created HBase tables to store variable data formats coming from different portfolios Performed real time analytics on HBase using Java API and Rest API Experienced in handling large datasets using Partitions Spark in Memory capabilities Broadcasts in Spark Effective efficient Joins Transformations and other during ingestion process itself Converted the text files and the csv files to parquet form for the analysis of data Prepared the mapping document as in which fields has be used from the HIVE DB and perform the transformations Implemented HDFS and Cassandra with vast amounts of data using Apache Kafka Performed the analytics over the data mining data visualization using Hive Worked in Agile environment and used JIRA as a bugreporting tool for updating the bug report Environment Cloudera Hadoop Hortonworks Linux ETL HDFS Hive Spark Sqoop Flume Zookeeper HBase Hadoop Developer Anthem Insurance Norfolk VA June 2016 to December 2017 Responsibilities Contributed to the development of key data integration and advanced analytics solutions leveraging Apache Hadoop and other big data technologies using major Hadoop Distributions like Hortonworks and Cloudera Worked on Amazon AWS EMR EC2 RDS S3 RedShift etc Tools Hadoop Hive Pig Sqoop Oozie HBase Flume Spark Worked on loading log data directly into HDFS using Flume in Cloudera CDH Involve in loading data from LINUX file system to HDFS in Cloudera CDH Experienced in running Hadoop streaming jobs to process terabytes of xml format data Experience in importing and exporting data into HDFS and assisted in exporting analyzed data to RDBMS using SQOOP in Cloudera Installed and configured MapReduce HIVE and the HDFS Developing Spark scripts by using Java per the requirement to readwrite JSON files Developed and implemented Java code according to MapReduce for the business requirements Worked on Importing and exporting data into HDFS and Hive using Sqoop Worked on Hadoop Administration development NoSQL in in Cloudera Load and transform large sets of structured semi structured and unstructured data Experienced in Hadoop Big Data Integration with TalenD ETL on performing data extract loading and transformation process Experience in development of extracting transforming and loading ETL maintain and support the enterprise data warehouse system and corresponding marts Involved in creating Hive tables loading with data and writing hive queries which will run internally in map Automate all the jobs for pulling data from FTP server to load data into Hive tables using Oozie workflows Created HBase tables to load large sets of structured semistructured and unstructured data coming from UNIX NoSQL and a variety of portfolios Supported codedesign analysis strategy development and project planning Created reports for the BI team using Sqoop to export data into HDFS and Hive Design Implemented Data Warehouse creating facts and dimension tables and loading them using Informatica Power Center Tools fetching data from the OLTP system to the Analytics Data Warehouse Coordinating with business user to gather the new requirements and working with existing issues worked on reading multiple data formats on HDFS using Scala Loading data into parquet files by applying transformation using Impala Involved in converting HiveSQL queries into Spark transformations using Spark RDDs and Scala Analyzed the SQL scripts and designed the solution to implement using Scala Developed multiple POCs using Scala and deployed on the Yarn cluster compared the performance of Spark with Hive and SQLTeradata Designed and Development the Integration APIs using various Data Structure concepts Java Collection Framework along with exception handling mechanism to return response within 500ms Usage of Java Thread concept to handle concurrent request Environment Hadoop 1x2x MR1 Cloudera CDH Hortonworks HDFS Spark Scala Impala HBase 090x Flume 093 Java Sqoop 2x Hive 071 Tableau Bigdata Developer Prime key software solutions Hyd April 2014 to June 2015 Responsibilities Worked on analyzing Hadoop cluster and different big data analytic tools including Pig HBase and Sqoop Responsible for building scalable distributed data solutions using Hadoop Configured a cluster by editing config files such as coresitexml mapredsitexml hdfssitexml and masters slaves Installed and configured Flume Hive Pig Sqoop and HBase on the Hadoop Cluster Responsible to monitor block Scanner Reports on data nodes Implemented 30 nodes CDH3CDH4 Hadoop cluster on CentOS Worked on installing cluster commissioning decommissioning of data node name node recovery capacity planning and slots configuration Involved in loading data from file system to HDFS and Implemented best offer logic using Pig scripts and Pig UDFs Implemented test scripts to support test driven development and continuous integration Responsible to manage data coming from different sources Installed and configured Hive and written Hive UDFs Developed simple and complex MapReduce programs in Java for Data Analysis on different data formats Experience in managing and reviewing Hadoop log files Exported the analyzed data to the relational databases using Sqoop for visualization and to generate reports for the BI team Analyzed large amounts of data sets to determine optimal way to aggregate and report on it Imported data using Sqoop to load data from MySQL to HDFS on regular basis Developing Scripts and Batch Job to schedule various Hadoop Program Responsible for writing Hive queries for data analysis to meet the business requirements Responsible for importing and exporting data into HDFS and Hive using Sqoop Involved in creating Hive tables loading with data and writing hive queries which will run internally in map reduce way Designed and implemented MapReduce based largescale parallel relationlearning system Environment Hadoop HDFS Pig Sqoop Storm VPN MapReduce CentOS Java J2ee Developer Logic solutions Pvt ltd Chennai Tamil Nadu September 2012 to March 2014 Responsibilities Responsible for developing Use Case Class diagrams and Sequence diagrams for the modules using UML and VISIO Used Spring Framework for dependency injection with the help of spring configuration files Developed the presentation layer using JSP HTML CSS and client validations using JavaScript Used HTML CSS JQUERY and JSP to created and UI Involved in Installation and configuration of Tomcat Server Involved in Dynamic form generation Auto completion of forms and user validation functionalities using AJAX Designed developed and maintained the data layer using Hibernate and performed configuration of Struts Application Framework Created stored procedures using SQL for data access layer Worked on tuning of backend stored procedures using TOAD Involved in the configuration management using CVS Developed various test cases and performed unit testing using JUnit Developed Unit test cases for the classes using JUnitEclipse Developed stored procedures to extract data from Oracle database Application developed with strict adherence to J2EE best practices Environment Java J2EE JSP Servlets Hibernate SQL Web Services SOAP WSDL JUnit Tomcat MySQL CVS and Windows Education Bachelor of Technology in Technology JNTU Hyderabad Telangana Skills Cassandra Hdfs Sqoop Hbase Db2 Etl Flume Hadoop Informatica Jms Map reduce Nosql C Hadoop Hbase Hive Html Javascript Perl Pig Additional Information Technical Skills Scripting Languages Python Perl Shell Big Data Technologies Hadoop HDFS Hive Map Reduce Pig Sqoop Flume Zookeeper AWS Programming Languages C Java Web Technologies HTML J2EE CSS JavaScript AJAX Servlets JSP DOM XML XSLT XPATH Java Frameworks Struts Spring Hibernate DB Languages SQL PLSQL Messaging Systems JMS   IBM MQ Databases ETL Oracle 9i10g11 g MySQL 52 DB2 Informatica v 8x NoSQL Databases Hbase Cassandra Mango DB Operating Systems Windows Unix and Linux",
    "extracted_keywords": [
        "data",
        "Developer",
        "data",
        "span",
        "lDeveloperspan",
        "Parsippany",
        "NJ",
        "years",
        "experience",
        "Application",
        "development",
        "maintenance",
        "data",
        "management",
        "programming",
        "data",
        "analysis",
        "data",
        "visualization",
        "Experience",
        "Apache",
        "Hadoop",
        "components",
        "MapReduce",
        "Hive",
        "HBase",
        "Pig",
        "Sqoop",
        "Oozie",
        "Python",
        "Spark",
        "Storm",
        "Cassandra",
        "MongoDB",
        "Big",
        "Data",
        "Big",
        "Data",
        "Analytics",
        "understandingknowledge",
        "Hadoop",
        "Architecture",
        "components",
        "HDFS",
        "Job",
        "Tracker",
        "Task",
        "Tracker",
        "Name",
        "Node",
        "Data",
        "Node",
        "Secondary",
        "Name",
        "node",
        "MapReduce",
        "concepts",
        "NoSQL",
        "DB",
        "Hadoop",
        "distribution",
        "Systems",
        "Cloudera",
        "Hortonworks",
        "HDP",
        "Map",
        "R",
        "M",
        "series",
        "Hadoop",
        "integration",
        "data",
        "ingestion",
        "data",
        "mapping",
        "data",
        "process",
        "knowledge",
        "ETL",
        "hands",
        "experience",
        "ETL",
        "experience",
        "RDBMS",
        "Oracle",
        "MS",
        "SQL",
        "Server",
        "MySQL",
        "DB2",
        "NoSQL",
        "mongo",
        "DB",
        "HBase",
        "Software",
        "development",
        "Java",
        "Application",
        "Development",
        "ClientServer",
        "Applications",
        "InternetIntranet",
        "database",
        "applications",
        "testing",
        "application",
        "environment",
        "J2EE",
        "JDBC",
        "JSP",
        "Servlets",
        "Web",
        "Services",
        "Oracle",
        "PLSQL",
        "Relational",
        "ability",
        "concepts",
        "groups",
        "skills",
        "ability",
        "part",
        "team",
        "knowledge",
        "virtualization",
        "VMware",
        "Virtual",
        "Center",
        "Hands",
        "experience",
        "configuring",
        "Hadoop",
        "ecosystem",
        "components",
        "Hadoop",
        "MapReduce",
        "HDFS",
        "HBase",
        "Hive",
        "Sqoop",
        "Pig",
        "Zookeeper",
        "Spark",
        "Kafka",
        "Flume",
        "Good",
        "Knowledge",
        "Hadoop",
        "Cluster",
        "architecture",
        "cluster",
        "Experience",
        "Hadoop",
        "clusters",
        "Cloudera",
        "Manager",
        "Experience",
        "Impala",
        "usage",
        "highperformance",
        "SQL",
        "experience",
        "project",
        "life",
        "cycle",
        "design",
        "development",
        "testing",
        "implementation",
        "Client",
        "Server",
        "Web",
        "applications",
        "Hands",
        "experience",
        "VPN",
        "Putty",
        "SCP",
        "VNC",
        "viewer",
        "Hands",
        "experience",
        "application",
        "development",
        "Java",
        "RDBMS",
        "Linux",
        "shell",
        "Performed",
        "data",
        "analysis",
        "MySQL",
        "SQL",
        "Server",
        "Management",
        "Studio",
        "Oracle",
        "knowledge",
        "cloud",
        "computing",
        "platforms",
        "Amazon",
        "Web",
        "ServicesAWS",
        "Ability",
        "technology",
        "sense",
        "responsibility",
        "accomplishment",
        "Work",
        "Experience",
        "data",
        "Developer",
        "Parsippany",
        "NJ",
        "January",
        "Present",
        "Responsibilities",
        "Hadoop",
        "Ecosystem",
        "components",
        "data",
        "sources",
        "Flume",
        "database",
        "management",
        "systems",
        "Sqoop",
        "Used",
        "Pig",
        "ETL",
        "tool",
        "transformations",
        "event",
        "filter",
        "traffic",
        "preaggregations",
        "data",
        "HDFS",
        "Flume",
        "Hive",
        "scripts",
        "transform",
        "data",
        "Database",
        "cluster",
        "capacity",
        "planning",
        "performance",
        "cluster",
        "Monitoring",
        "Troubleshooting",
        "Developed",
        "Spark",
        "code",
        "Java",
        "SparkSQLStreaming",
        "processing",
        "data",
        "Develop",
        "ETL",
        "Process",
        "SPARK",
        "SCALA",
        "HIVE",
        "HBASE",
        "ETL",
        "Specifications",
        "Development",
        "conversion",
        "projects",
        "HiveSQL",
        "queries",
        "Spark",
        "transformations",
        "Spark",
        "RDDS",
        "Scala",
        "Map",
        "Reduce",
        "jobs",
        "Java",
        "data",
        "Developed",
        "Oozie",
        "workflow",
        "scheduling",
        "ETL",
        "process",
        "Experience",
        "loading",
        "transforming",
        "sets",
        "semi",
        "algorithms",
        "Hadoop",
        "Spark",
        "Context",
        "HiveSQL",
        "Data",
        "Frames",
        "Spark",
        "scripts",
        "Scala",
        "shell",
        "commands",
        "requirement",
        "Performed",
        "procedures",
        "text",
        "analytics",
        "processing",
        "inmemory",
        "computing",
        "capabilities",
        "Spark",
        "Scala",
        "Created",
        "HBase",
        "data",
        "formats",
        "portfolios",
        "time",
        "analytics",
        "HBase",
        "Java",
        "API",
        "Rest",
        "API",
        "datasets",
        "Partitions",
        "Spark",
        "Memory",
        "capabilities",
        "Broadcasts",
        "Spark",
        "Effective",
        "Joins",
        "Transformations",
        "ingestion",
        "process",
        "text",
        "files",
        "csv",
        "files",
        "form",
        "analysis",
        "data",
        "mapping",
        "document",
        "fields",
        "HIVE",
        "DB",
        "transformations",
        "HDFS",
        "Cassandra",
        "amounts",
        "data",
        "Apache",
        "Kafka",
        "analytics",
        "data",
        "mining",
        "data",
        "visualization",
        "Hive",
        "Worked",
        "environment",
        "JIRA",
        "tool",
        "bug",
        "report",
        "Environment",
        "Cloudera",
        "Hadoop",
        "Hortonworks",
        "Linux",
        "ETL",
        "HDFS",
        "Hive",
        "Spark",
        "Sqoop",
        "Flume",
        "Zookeeper",
        "HBase",
        "Hadoop",
        "Developer",
        "Anthem",
        "Insurance",
        "Norfolk",
        "VA",
        "June",
        "December",
        "Responsibilities",
        "development",
        "data",
        "integration",
        "analytics",
        "solutions",
        "Apache",
        "Hadoop",
        "data",
        "technologies",
        "Hadoop",
        "Distributions",
        "Hortonworks",
        "Cloudera",
        "Amazon",
        "AWS",
        "EMR",
        "EC2",
        "RDS",
        "S3",
        "RedShift",
        "Tools",
        "Hadoop",
        "Hive",
        "Pig",
        "Sqoop",
        "Oozie",
        "HBase",
        "Flume",
        "Spark",
        "loading",
        "log",
        "data",
        "HDFS",
        "Flume",
        "Cloudera",
        "CDH",
        "Involve",
        "loading",
        "data",
        "LINUX",
        "file",
        "system",
        "HDFS",
        "Cloudera",
        "CDH",
        "Hadoop",
        "streaming",
        "jobs",
        "terabytes",
        "xml",
        "format",
        "data",
        "Experience",
        "data",
        "HDFS",
        "data",
        "SQOOP",
        "Cloudera",
        "Installed",
        "MapReduce",
        "HIVE",
        "HDFS",
        "Spark",
        "scripts",
        "Java",
        "requirement",
        "files",
        "Java",
        "code",
        "MapReduce",
        "business",
        "requirements",
        "data",
        "HDFS",
        "Hive",
        "Sqoop",
        "Worked",
        "Hadoop",
        "Administration",
        "development",
        "NoSQL",
        "Cloudera",
        "Load",
        "sets",
        "data",
        "Hadoop",
        "Big",
        "Data",
        "Integration",
        "TalenD",
        "ETL",
        "data",
        "extract",
        "loading",
        "transformation",
        "process",
        "Experience",
        "development",
        "transforming",
        "loading",
        "ETL",
        "enterprise",
        "data",
        "warehouse",
        "system",
        "marts",
        "Hive",
        "tables",
        "data",
        "hive",
        "queries",
        "map",
        "jobs",
        "data",
        "FTP",
        "server",
        "data",
        "Hive",
        "tables",
        "Oozie",
        "workflows",
        "Created",
        "HBase",
        "sets",
        "data",
        "UNIX",
        "NoSQL",
        "variety",
        "portfolios",
        "codedesign",
        "analysis",
        "strategy",
        "development",
        "project",
        "reports",
        "BI",
        "team",
        "Sqoop",
        "data",
        "HDFS",
        "Hive",
        "Design",
        "Data",
        "Warehouse",
        "facts",
        "dimension",
        "tables",
        "Informatica",
        "Power",
        "Center",
        "Tools",
        "data",
        "OLTP",
        "system",
        "Analytics",
        "Data",
        "Warehouse",
        "business",
        "user",
        "requirements",
        "issues",
        "data",
        "formats",
        "HDFS",
        "Scala",
        "Loading",
        "data",
        "files",
        "transformation",
        "Impala",
        "HiveSQL",
        "queries",
        "Spark",
        "transformations",
        "Spark",
        "RDDs",
        "Scala",
        "SQL",
        "scripts",
        "solution",
        "Scala",
        "POCs",
        "Scala",
        "Yarn",
        "cluster",
        "performance",
        "Spark",
        "Hive",
        "SQLTeradata",
        "Designed",
        "Development",
        "Integration",
        "APIs",
        "Data",
        "Structure",
        "Java",
        "Collection",
        "Framework",
        "exception",
        "handling",
        "mechanism",
        "response",
        "Usage",
        "Java",
        "Thread",
        "concept",
        "request",
        "Environment",
        "Hadoop",
        "1x2x",
        "MR1",
        "Cloudera",
        "CDH",
        "Hortonworks",
        "HDFS",
        "Spark",
        "Scala",
        "Impala",
        "HBase",
        "090x",
        "Flume",
        "Java",
        "Sqoop",
        "Hive",
        "Tableau",
        "Bigdata",
        "Developer",
        "Prime",
        "software",
        "solutions",
        "Hyd",
        "April",
        "June",
        "Responsibilities",
        "Hadoop",
        "cluster",
        "data",
        "tools",
        "Pig",
        "HBase",
        "Sqoop",
        "Responsible",
        "data",
        "solutions",
        "Hadoop",
        "Configured",
        "cluster",
        "files",
        "coresitexml",
        "mapredsitexml",
        "hdfssitexml",
        "masters",
        "slaves",
        "Flume",
        "Hive",
        "Pig",
        "Sqoop",
        "HBase",
        "Hadoop",
        "Cluster",
        "Responsible",
        "block",
        "Scanner",
        "Reports",
        "data",
        "nodes",
        "nodes",
        "CDH3CDH4",
        "Hadoop",
        "cluster",
        "CentOS",
        "cluster",
        "decommissioning",
        "data",
        "node",
        "name",
        "node",
        "recovery",
        "capacity",
        "planning",
        "slots",
        "configuration",
        "loading",
        "data",
        "file",
        "system",
        "HDFS",
        "offer",
        "logic",
        "Pig",
        "scripts",
        "Pig",
        "UDFs",
        "test",
        "scripts",
        "test",
        "development",
        "integration",
        "data",
        "sources",
        "Hive",
        "Hive",
        "UDFs",
        "MapReduce",
        "programs",
        "Java",
        "Data",
        "Analysis",
        "data",
        "formats",
        "Experience",
        "Hadoop",
        "log",
        "files",
        "data",
        "databases",
        "Sqoop",
        "visualization",
        "reports",
        "BI",
        "team",
        "amounts",
        "data",
        "sets",
        "way",
        "data",
        "Sqoop",
        "data",
        "MySQL",
        "HDFS",
        "basis",
        "Scripts",
        "Batch",
        "Job",
        "Hadoop",
        "Program",
        "Responsible",
        "Hive",
        "queries",
        "data",
        "analysis",
        "business",
        "requirements",
        "data",
        "HDFS",
        "Hive",
        "Sqoop",
        "Hive",
        "tables",
        "data",
        "hive",
        "queries",
        "map",
        "way",
        "MapReduce",
        "largescale",
        "system",
        "Environment",
        "Hadoop",
        "HDFS",
        "Pig",
        "Sqoop",
        "Storm",
        "VPN",
        "MapReduce",
        "CentOS",
        "Java",
        "J2ee",
        "Developer",
        "Logic",
        "solutions",
        "Pvt",
        "ltd",
        "Chennai",
        "Tamil",
        "Nadu",
        "September",
        "March",
        "Responsibilities",
        "Use",
        "Case",
        "Class",
        "diagrams",
        "Sequence",
        "diagrams",
        "modules",
        "UML",
        "VISIO",
        "Spring",
        "Framework",
        "dependency",
        "injection",
        "help",
        "spring",
        "configuration",
        "files",
        "presentation",
        "layer",
        "JSP",
        "HTML",
        "CSS",
        "client",
        "validations",
        "JavaScript",
        "HTML",
        "CSS",
        "JQUERY",
        "JSP",
        "UI",
        "Installation",
        "configuration",
        "Tomcat",
        "Server",
        "form",
        "generation",
        "Auto",
        "completion",
        "forms",
        "user",
        "validation",
        "functionalities",
        "AJAX",
        "data",
        "layer",
        "Hibernate",
        "configuration",
        "Struts",
        "Application",
        "Framework",
        "procedures",
        "SQL",
        "data",
        "access",
        "layer",
        "tuning",
        "procedures",
        "TOAD",
        "configuration",
        "management",
        "CVS",
        "test",
        "cases",
        "unit",
        "testing",
        "JUnit",
        "Developed",
        "Unit",
        "test",
        "cases",
        "classes",
        "JUnitEclipse",
        "procedures",
        "data",
        "Oracle",
        "database",
        "Application",
        "adherence",
        "J2EE",
        "practices",
        "Environment",
        "Java",
        "J2EE",
        "JSP",
        "Servlets",
        "Hibernate",
        "SQL",
        "Web",
        "Services",
        "SOAP",
        "WSDL",
        "JUnit",
        "Tomcat",
        "MySQL",
        "CVS",
        "Windows",
        "Education",
        "Bachelor",
        "Technology",
        "Technology",
        "JNTU",
        "Hyderabad",
        "Telangana",
        "Skills",
        "Cassandra",
        "Hdfs",
        "Sqoop",
        "Hbase",
        "Db2",
        "Etl",
        "Flume",
        "Hadoop",
        "Informatica",
        "Jms",
        "Map",
        "Nosql",
        "C",
        "Hadoop",
        "Hbase",
        "Hive",
        "Html",
        "Javascript",
        "Perl",
        "Pig",
        "Additional",
        "Information",
        "Technical",
        "Skills",
        "Scripting",
        "Languages",
        "Python",
        "Perl",
        "Shell",
        "Big",
        "Data",
        "Technologies",
        "Hadoop",
        "HDFS",
        "Hive",
        "Map",
        "Reduce",
        "Pig",
        "Sqoop",
        "Flume",
        "Zookeeper",
        "AWS",
        "Programming",
        "Languages",
        "C",
        "Java",
        "Web",
        "Technologies",
        "HTML",
        "J2EE",
        "CSS",
        "JavaScript",
        "AJAX",
        "Servlets",
        "JSP",
        "DOM",
        "XML",
        "XSLT",
        "XPATH",
        "Java",
        "Frameworks",
        "Struts",
        "Spring",
        "Hibernate",
        "DB",
        "Languages",
        "SQL",
        "PLSQL",
        "Messaging",
        "Systems",
        "JMS",
        "IBM",
        "MQ",
        "Databases",
        "ETL",
        "Oracle",
        "9i10g11",
        "g",
        "MySQL",
        "DB2",
        "Informatica",
        "8x",
        "NoSQL",
        "Hbase",
        "Cassandra",
        "Mango",
        "DB",
        "Operating",
        "Systems",
        "Windows",
        "Unix",
        "Linux"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T20:10:03.149155",
    "resume_data": "Big data Developer Big data span lDeveloperspan Parsippany NJ Over 5 years of industrial experience in Application development and maintenance data management programming data analysis and data visualization Experience in dealing with Apache Hadoop components like HDFS MapReduce Hive HBase Pig Sqoop Oozie Python Spark Storm Cassandra MongoDB Big Data and Big Data Analytics Good understandingknowledge of Hadoop Architecture and various components such as HDFS Job Tracker Task Tracker Name Node Data Node Secondary Name node and MapReduce concepts Experienced managing NoSQL DB on large Hadoop distribution Systems such as Cloudera Hortonworks HDP Map R M series etc Experienced developing Hadoop integration for data ingestion data mapping and data process capabilities Good knowledge in ETL and hands on experience in ETL experience in RDBMS like Oracle MS SQL Server MySQL and DB2 and NoSQL databases like mongo DB and HBase Software development in Java Application Development ClientServer Applications InternetIntranet based database applications and developing testing and implementing application environment using J2EE JDBC JSP Servlets Web Services Oracle PLSQL and Relational Databases Exceptional ability to quickly master new concepts and capable of working in groups as well as independently Excellent interpersonal skills and the ability to work as a part of a team Has good knowledge of virtualization and worked on VMware Virtual Center Hands on experience in installing configuring and using Hadoop ecosystem components like Hadoop MapReduce HDFS HBase Hive Sqoop Pig Zookeeper Spark Kafka and Flume Good Knowledge on Hadoop Cluster architecture and monitoring the cluster Experience in managing Hadoop clusters using Cloudera Manager Experience in using the Impala usage for the highperformance SQL queries Very good experience in complete project life cycle design development testing and implementation of Client Server and Web applications Hands on experience in VPN Putty win SCP VNC viewer etc Hands on experience in application development using Java RDBMS and Linux shell scripting Performed data analysis using MySQL SQL Server Management Studio and Oracle Good knowledge on cloud computing platforms like Amazon Web ServicesAWS Ability to adapt to evolving technology strong sense of responsibility and accomplishment Work Experience Big data Developer Parsippany NJ January 2018 to Present Responsibilities Involved in installing Hadoop Ecosystem components and responsible to manage data coming from various sources Implemented Flume from relational database management systems using Sqoop Used Pig as ETL tool to do transformations event joins filter both traffic and some preaggregations before storing the data onto HDFS Involved in writing Flume and Hive scripts to extract transform and load the data into Database cluster capacity planning performance tuning cluster Monitoring Troubleshooting Developed Spark code using Java and SparkSQLStreaming for faster processing of data Develop ETL Process using SPARK SCALA HIVE and HBASE Extensively involved in writing ETL Specifications for Development and conversion projects Involved in converting HiveSQL queries into Spark transformations using Spark RDDS and Scala Developed various Map Reduce jobs in Java for data cleaning and preprocessing Developed Oozie workflow for scheduling orchestrating the ETL process Experience on loading and transforming of large sets of structured semi structured and optimizing of existing algorithms in Hadoop using Spark Context HiveSQL Data Frames Developed Spark scripts by using Scala shell commands as per the requirement Performed advanced procedures like text analytics and processing using the inmemory computing capabilities of Spark using Scala Created HBase tables to store variable data formats coming from different portfolios Performed real time analytics on HBase using Java API and Rest API Experienced in handling large datasets using Partitions Spark in Memory capabilities Broadcasts in Spark Effective efficient Joins Transformations and other during ingestion process itself Converted the text files and the csv files to parquet form for the analysis of data Prepared the mapping document as in which fields has be used from the HIVE DB and perform the transformations Implemented HDFS and Cassandra with vast amounts of data using Apache Kafka Performed the analytics over the data mining data visualization using Hive Worked in Agile environment and used JIRA as a bugreporting tool for updating the bug report Environment Cloudera Hadoop Hortonworks Linux ETL HDFS Hive Spark Sqoop Flume Zookeeper HBase Hadoop Developer Anthem Insurance Norfolk VA June 2016 to December 2017 Responsibilities Contributed to the development of key data integration and advanced analytics solutions leveraging Apache Hadoop and other big data technologies using major Hadoop Distributions like Hortonworks and Cloudera Worked on Amazon AWS EMR EC2 RDS S3 RedShift etc Tools Hadoop Hive Pig Sqoop Oozie HBase Flume Spark Worked on loading log data directly into HDFS using Flume in Cloudera CDH Involve in loading data from LINUX file system to HDFS in Cloudera CDH Experienced in running Hadoop streaming jobs to process terabytes of xml format data Experience in importing and exporting data into HDFS and assisted in exporting analyzed data to RDBMS using SQOOP in Cloudera Installed and configured MapReduce HIVE and the HDFS Developing Spark scripts by using Java per the requirement to readwrite JSON files Developed and implemented Java code according to MapReduce for the business requirements Worked on Importing and exporting data into HDFS and Hive using Sqoop Worked on Hadoop Administration development NoSQL in in Cloudera Load and transform large sets of structured semi structured and unstructured data Experienced in Hadoop Big Data Integration with TalenD ETL on performing data extract loading and transformation process Experience in development of extracting transforming and loading ETL maintain and support the enterprise data warehouse system and corresponding marts Involved in creating Hive tables loading with data and writing hive queries which will run internally in map Automate all the jobs for pulling data from FTP server to load data into Hive tables using Oozie workflows Created HBase tables to load large sets of structured semistructured and unstructured data coming from UNIX NoSQL and a variety of portfolios Supported codedesign analysis strategy development and project planning Created reports for the BI team using Sqoop to export data into HDFS and Hive Design Implemented Data Warehouse creating facts and dimension tables and loading them using Informatica Power Center Tools fetching data from the OLTP system to the Analytics Data Warehouse Coordinating with business user to gather the new requirements and working with existing issues worked on reading multiple data formats on HDFS using Scala Loading data into parquet files by applying transformation using Impala Involved in converting HiveSQL queries into Spark transformations using Spark RDDs and Scala Analyzed the SQL scripts and designed the solution to implement using Scala Developed multiple POCs using Scala and deployed on the Yarn cluster compared the performance of Spark with Hive and SQLTeradata Designed and Development the Integration APIs using various Data Structure concepts Java Collection Framework along with exception handling mechanism to return response within 500ms Usage of Java Thread concept to handle concurrent request Environment Hadoop 1x2x MR1 Cloudera CDH Hortonworks HDFS Spark Scala Impala HBase 090x Flume 093 Java Sqoop 2x Hive 071 Tableau Bigdata Developer Prime key software solutions Hyd April 2014 to June 2015 Responsibilities Worked on analyzing Hadoop cluster and different big data analytic tools including Pig HBase and Sqoop Responsible for building scalable distributed data solutions using Hadoop Configured a cluster by editing config files such as coresitexml mapredsitexml hdfssitexml and masters slaves Installed and configured Flume Hive Pig Sqoop and HBase on the Hadoop Cluster Responsible to monitor block Scanner Reports on data nodes Implemented 30 nodes CDH3CDH4 Hadoop cluster on CentOS Worked on installing cluster commissioning decommissioning of data node name node recovery capacity planning and slots configuration Involved in loading data from file system to HDFS and Implemented best offer logic using Pig scripts and Pig UDFs Implemented test scripts to support test driven development and continuous integration Responsible to manage data coming from different sources Installed and configured Hive and written Hive UDFs Developed simple and complex MapReduce programs in Java for Data Analysis on different data formats Experience in managing and reviewing Hadoop log files Exported the analyzed data to the relational databases using Sqoop for visualization and to generate reports for the BI team Analyzed large amounts of data sets to determine optimal way to aggregate and report on it Imported data using Sqoop to load data from MySQL to HDFS on regular basis Developing Scripts and Batch Job to schedule various Hadoop Program Responsible for writing Hive queries for data analysis to meet the business requirements Responsible for importing and exporting data into HDFS and Hive using Sqoop Involved in creating Hive tables loading with data and writing hive queries which will run internally in map reduce way Designed and implemented MapReduce based largescale parallel relationlearning system Environment Hadoop HDFS Pig Sqoop Storm VPN MapReduce CentOS Java J2ee Developer Logic solutions Pvt ltd Chennai Tamil Nadu September 2012 to March 2014 Responsibilities Responsible for developing Use Case Class diagrams and Sequence diagrams for the modules using UML and VISIO Used Spring Framework for dependency injection with the help of spring configuration files Developed the presentation layer using JSP HTML CSS and client validations using JavaScript Used HTML CSS JQUERY and JSP to created and UI Involved in Installation and configuration of Tomcat Server Involved in Dynamic form generation Auto completion of forms and user validation functionalities using AJAX Designed developed and maintained the data layer using Hibernate and performed configuration of Struts Application Framework Created stored procedures using SQL for data access layer Worked on tuning of backend stored procedures using TOAD Involved in the configuration management using CVS Developed various test cases and performed unit testing using JUnit Developed Unit test cases for the classes using JUnitEclipse Developed stored procedures to extract data from Oracle database Application developed with strict adherence to J2EE best practices Environment Java J2EE JSP Servlets Hibernate SQL Web Services SOAP WSDL JUnit Tomcat MySQL CVS and Windows Education Bachelor of Technology in Technology JNTU Hyderabad Telangana Skills Cassandra Hdfs Sqoop Hbase Db2 Etl Flume Hadoop Informatica Jms Map reduce Nosql C Hadoop Hbase Hive Html Javascript Perl Pig Additional Information Technical Skills Scripting Languages Python Perl Shell Big Data Technologies Hadoop HDFS Hive Map Reduce Pig Sqoop Flume Zookeeper AWS Programming Languages C Java Web Technologies HTML J2EE CSS JavaScript AJAX Servlets JSP DOM XML XSLT XPATH Java Frameworks Struts Spring Hibernate DB Languages SQL PLSQL Messaging Systems JMS ActiveMQ IBM MQ Databases ETL Oracle 9i10g11g MySQL 52 DB2 Informatica v 8x NoSQL Databases Hbase Cassandra Mango DB Operating Systems Windows Unix and Linux",
    "unique_id": "b9ab9f2f-9414-4964-aa10-4cf5a6fb31dc"
}