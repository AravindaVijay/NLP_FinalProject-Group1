{
    "clean_data": "HadoopSpark Developer HadoopSpark span lDeveloperspan HadoopSpark Developer Ford Southfield MI Talented and accomplished Software Engineer with 8 years of IT experience in developing applications using BigData AWS JavaSQL and Spark 3 years of experience with Big Data tools like MapReduce YARN HDFS Hbase ImpalaHive Pig OozieAWS ApacheSpark for ingestion storage querying processing and analysis of data Performance tuning in HiveImpala using multiple methods limited to dynamic partitioning bucketing indexing files compressions Hands on experience withdata ingestion tools Kafka Flume and workflow management tools Oozie and Zena Hands on experience handling different file formats like JSON AVRO ORC Parquet and compression techniques like snappy zlib and lzo Hands on experience in Hadoop Ecosystem components such as Hadoop Spark HDFS YARN TEZ Hive Sqoop Flume MapReduce SCALA Pig OOZIE Kafka NIFI Storm HBASE Experience on analyzing data in NOSQL databases like Hbase and Cassandraand its Integration with Hadoop cluster Hands on experience with Spark Core Spark SQL and Data FramesData SetsRDD API Experience in using Kafka and Kafka brokers to initiate spark context and processing live streaming information with the help of RDD Developed Java applications using various IDEs like Spring Tool Suite and Eclipse Good knowledge in using Hibernate for mapping Java classes with database and using Hibernate Query Language HQL Operated on JavaJ2EE systems with different databases which include Oracle MySQL and DB2 Knowledge on implementing Big Data in Amazon Elastic MapReduce Amazon EMR for processing managing Hadoop framework dynamically scalable Amazon EC2 instances Capable of processing large sets of structured semistructured and unstructureddata and supporting systems application architecture Extensive development experience in sparkapplications for datatransformations and loading into HDFS using RDD DataFrames and Datasets Extensive knowledge on performance tuning of Spark applications and converting HiveSQL queries into Sparktransformations Handson experience with AWS AmazonWebServices using ElasticMapReduce EMR creating and storing data in S3buckets and creating ElasticLoadBalancersELB for Hadoop front end WebUIs Extensive knowledge on creating Hadoop cluster on multiple EC2 instances in AWS and configuring them through ambari and using IAM Identity and AccessManagement for creating groups users and assigning permissions Extensive programming experience in JavaCore concepts like OOPS Multithreading Collections and IO Experience using Jira for ticketing issues and Jenkins for continuous integration Extensive experience with UNIX commands shellscripting and setting up CRON jobs Experience in software configuration management using Git Good experience in using Relational databases OracleMySQL Able to assess businessrules collaborate with stakeholders and perform sourcetotarget datamapping design Successfully working in fastpaced environment both independently and in collaborative team environments Work Experience HadoopSpark Developer Ford Southfield MI May 2018 to Present Responsibilities Implemented Hive UDFs and did performance tuning for better results Analyzed the data by performing Hive queries and running Pig Scripts Implemented optimized map joins to get data from different sources to perform cleaning operations before applying the algorithms MonitoringMaintaining daytoday batch jobs using Event Engine Interacting with SOR team to fix production data issues Performing data transformation using Pig and Hive Performing Snapshot full refresh and Load append data refresh Providing support for CMDL raw data and ODL ingestion EFS Big Data applications Experience in using Sqoop to import and export the data from Netezza and Oracle DB into HDFS and HIVE Involved in loading data from UNIX file system to HDFS Handled importing of data from various data sources performed transformations using Hive MapReduce and loaded data into HDFS Providing weeklymonthly status reports to customer Worked on analyzing data with Hive and Pig and real time analytical operations using Hbase Worked on loading and transforming of large sets of structured semi structured and unstructured data Participated in all the meetings with the Database owners for the approvals and extraction of the data Retrieving data from external web APIs and indexing it into our distributed SolrCloud platform Also maintained and deployed the production and development SolrCloud setup using Ansible Other responsibilities included building proofs of concept in C for transforming incoming data into schema compatible with the existing SQL data as well as building monitoring scripts in Python Resolve missing fields in DataFrame rows using filtering and imputation Integrate visualizations into a Spark application using Databricks and popular visualization libraries ggplot matplotlib Faster processing and testing of data is achieved by implementing Spark SQL and Spark using Scala Experienced on addinginstallation of new components and removal of them through Ambari Experience with data wrangling and creating workable datasets Monitoring systems and services through Ambari dashboard to make the clusters available for the business Responsible for building scalable distributed data solutions using Hadoop Working as Hadoop Developer and admin in Hortonworks HDP 2242 distribution for 10 clusters ranges from POC to PROD Prepared the Business and high level design documents Created and maintained Technical documentation for launching HADOOP Clusters and for executing Hive queries and Pig Scripts Installed Oozie workflow engine to run multiple Hive and Pig jobs which run independently with time and data availability Assisted application teams in installing Hadoop updates operating system patches and version upgrades when required Implemented Map reduce secondary sorting to get better performance for sorting results in Map Reduce programs Worked on User Defined Functions in Hive to load the data from HDFS to run aggregation function on multiple rows Involved in moving all log files generated from various sources to HDFS for further processing through Flume Coordinated with the testing team for bug fixes and created documentation for recorded data agent usage and release cycle notes Used Teradata system Priority Schedule in controlling the load of the system Created different UDFs and UDAFs to analyze the partitioned and bucketed data and compute various metrics for reporting on the dashboard and stored them in different summary tables Implemented POC to introduce Spark Transformations Environment Hadoop HDFS hive Sqoop Kafka Spark Scala MapReduce Cloudera Kafka Zookeeper HBase Shell Scripting Python Mongo DB AWS UNIX Shell Scripting Hadoop Developer T Mobile Bellevue WA January 2017 to April 2018 AWS with Spark Responsibilities Worked on improving the performance of existing Pig and Hive Queries Developed Oozie workflow engines to automate Hive and Pig jobs Worked on performing Join operations Exported the result set from Hive to MySQL using Sqoop after processing the data Analyzed the data by performing Hive queries and running Pig scripts to study customer behavior Used Hive to partition and bucket data Performed various source data ingestions cleansing and transformation in Hadoop Design and developed many Spark Programs using pyspark Produce unit tests for Spark transformations and helper methods Creating RDDs and Pair RDDs for Spark Programming Implement Joins Grouping and Aggregations for the Pair RDDs Write Scaladocstyle documentation with all code Experienced in performance tuning of Spark Applications for setting right Batch Interval time correct level of Parallelism and memory tuning Developed Pig Scripts to perform ETL procedures on the data in HDFS Analyzed the partitioned and bucketed data and compute various metrics for reporting Created HBase tables to store various data formats of data coming from different systems Advanced knowledge in performance troubleshooting and tuning Cassandra clusters Analyzing the source data to know the quality of data by using Talend Data Quality Created ScalaSpark jobs for data transformation and aggregation Involved in creating Hive tables loading with data and writing hive queries Used Impala to read write and query the Hadoop data in HDFS from Cassandra and configured Kafka to read and write messages from external programs Preparation of Technical architecture and Lowlevel design documents Tested raw data and executed performance scripts Environment eclipse jdk180 Hadoop28 HDFS MapReduceSpark 20 Pig0150 Hive20 HBase ApacheMaven3 JavaHadoop Developer Charles Schwab Austin TX January 2015 to December 2016 Responsibilities Implemented J2EEDesignPatterns like DAO Singleton and Factory Managed connectivity using JDBC for queryinginserting data management including triggers and stored procedures Used SpringMVC framework to enable the interactions between JSPView layer and implemented different design patterns with J2EE and XML technology Worked on Python and build the custom ingest framework Implemented application using MVC architecture integrating Hibernate and spring frameworks Utilized various JavaScript and JQuery libraries Bootstrap Ajax for form validation and other interactive features Extensively worked on Hadoop ecosystems including Hive Spark Streaming with MapRdistribution Upgraded the Hadoop Cluster from CDH3 to CDH4 setting up High Availability Cluster and integrating Hive with existing applications Worked on storing data in HDFS either directly or through Hbase Worked on NoSQL support enterprise production and loading data into HBase using Impala and Sqoop Performed multiple MapReduce jobs in Pig and Hive for data cleaning and preprocessing Build Hadoop solutions for big data problems using MR1 and MR2 in YARN Handled importing of data from various data sources performed transformations using Hive PIG and loaded data into HDFS Worked on data using Sqoop from HDFS to Relational Database Systems and viceversa Maintaining and troubleshooting Exploring with Spark to improve the performance and optimization of the existing algorithms in Hadoop using Spark context SparkSQL Data Frame pair RDDs Created Hive Tables loaded claims data from Oracle using Sqoop and loaded the processed data into target database Worked on Delete printer module using python Involved in PLSQL query optimization to reduce the overall run time of stored procedures Exported data from HDFS to RDBMS via Sqoop for Business Intelligence visualization and user report generation Implemented the J2EE design patterns Data Access Object DAO Session Faade and Business Delegate Developed Nififlows dealing with various kinds of data formats such as XML JSON and Avro Implemented MapReduce jobs in HIVE by querying the available data Proactively involved in ongoing maintenance support and improvements in Hadoop cluster Developed Spark code using Scala and SparkSQLStreaming for faster testing and processing of data Used Cloudera Manager for installation and management of Hadoop Cluster Collaborated with business usersproduct owners developers to contribute to the analysis of functional requirements Involved in converting HiveQL into Spark transformations using Spark RDD and through Scala programming Integrated KafkaSparkstreaming for high efficiency throughput and reliability Worked in tuning HivePig to improve performance and solved performance issues in both scripts Environment Hadoop 30 PythonHive 21 J2EE Hbase JDBC Pig 016 HBase 11 Sqoop NoSQL Impala Java Spring MVC XML Spark 19 PLSQL HDFS JSON Hibernate Bootstrap JQuery JavaScript Ajax Java Developer Catalyte Chicago IL October 2013 to December 2014 Responsibilities As a Java Developer involved in backend and frontend developing team Involved in the Software Development Life Cycle SDLC including Analysis Design Implementation Responsible for use case diagrams class diagrams and sequence diagrams using Rational Rose in the Design phase Developed ANT scripts that checkout code from SVN repository build EAR files Used XML Web Services using SOAP to transfer information to the supply chain and domain expertise Monitoring Systems Use Eclipse and Tomcat web server for developing deploying the applications Developed REST Web Services clients to consume those Web Services as well other enterprise wide Web Services Used JavaScript and AJAXtechnologies for front end user input validations and Spring validation framework for backend validation for the User Interface Used both annotation based configuration and XML based Developed application service components and configured beans using  Spring IOC Implemented persistence mechanism using Hibernate ORM Mapping Developed the DAO layer for the application using Spring Hibernate Template support Used WebLogic workshop Eclipse IDE to develop the application Performed the code build and deployment using Maven Implementation of Spring Restful web services which produces JSON Responsible for maintaining the code quality coding and implementation standards by code reviews Developed the front end of the application using HTML CSS JSP and JavaScript Created RESTFULL APIs using Spring MVC Used SVN version controller to maintain the code versions Worked on web applications using open source MVCframeworks Developed Web interface using JSP Standard Tag Libraries JSTL and SpringFramework Implemented logger for debugging and testing purposes using Log4j Environment JSON HTML 4 CSS XML Hibernate 36 Eclipse Maven JUnit JDBC ANT SOAP Log4j Java Developer Paychex Rochester NY October 2011 to September 2013 Responsibilities Individually worked on all the stages of a SoftwareDevelopmentLifeCycle SDLC Responsible for design and implementation of various modules of the application using StrutsSpringHibernate architecture Created userfriendly GUI interface and Web pages using HTML CSS and JSP Developed web components using MVC pattern under Struts framework Wrote JSPs Servlets and deployed them on Weblogic Application server Used JSPs HTML on front end Servlets as Front Controllers and JavaScript for client side validations Wrote the Hibernatemapping XML files to define java classesdatabase tables mapping Developed the UI using JSP HTML CSS and AJAX and learned how to implement JQuery JSP and client server validations using JavaScript Implemented MVC architecture by using spring to send and receive the data from frontend to business layer Designed developed and maintained the data layer using JDBC and performed configuration of JavaApplication Framework Extensively used Hibernate in data access layer to access and update information in the database Migrated the Servlets to the Spring Controllers and developed Spring Interceptors worked on JSPs JSTL and JSP Custom Tags Used Jenkins for continuous integration purpose in using SVN JUnit and Mockito as version control and Unit testing by Creating design documents and test cases for development work Worked on Eclipse IDE for front end development environment for insertions updating and retrieval operations of data from oracle database by writing stored procedures Responsible for writing Struts action classes Hibernate POJO classes and integrating Struts and Hibernate with spring for processing business needs Developed the application using Servlets and JSP for the presentation layer along with JavaScript for the client side validations Wrote Hibernate classes DAOs to retrieve store data configured Hibernate files Used Web Logic for application deployment and Log4J used for Loggingdebugging Used CVSversion controlling tool and project build tool using ANT Used various Core Java concepts such as multithreading Exception Handling Collection APIs to implement various features and enhancements Wrote and debugged the MavenScripts for building the entire web application Designed and developed Ajax calls to populate screens parts on demand Environment Struts HTML CSS JSP MVC Hibernate JSP AJAX JQuery Java Jenkins ANT Maven Education Bachelors Skills Hdfs Impala Mapreduce Oozie Sqoop",
    "entities": [
        "BigData AWS",
        "MVCframeworks Developed",
        "Produce",
        "AJAX",
        "SparkSQL Data Frame",
        "the Spring Controllers",
        "ODL",
        "GUI",
        "Oracle MySQL",
        "Relational",
        "HDFS",
        "UNIX",
        "Present Responsibilities Implemented Hive",
        "Hive and Pig",
        "Analysis Design Implementation Responsible",
        "Spark Programs",
        "JSP Standard Tag Libraries JSTL",
        "Hadoop Ecosystem",
        "Netezza",
        "Ambari",
        "StrutsSpringHibernate",
        "HadoopSpark",
        "Ajax",
        "UDAFs",
        "Sqoop for Business Intelligence",
        "MapRdistribution Upgraded",
        "RDD",
        "Hadoop",
        "Software Engineer",
        "XML",
        "SOAP",
        "JavaScript Implemented",
        "CRON",
        "EAR",
        "NOSQL",
        "Hadoop Cluster Collaborated",
        "RESTFULL",
        "HDFS Analyzed",
        "the Hadoop Cluster",
        "WebLogic",
        "MonitoringMaintaining",
        "HBase",
        "JavaJ2EE",
        "Amazon",
        "TX",
        "Hive PIG",
        "CDH3",
        "Data Access Object DAO Session Faade and Business Delegate Developed Nififlows",
        "SOR",
        "PROD Prepared the Business",
        "Assisted",
        "HTML CSS JSP",
        "Developed",
        "Scala Experienced",
        "Utilized",
        "Mockito",
        "Created Hive Tables",
        "JSTL",
        "CSS XML Hibernate",
        "Servlets",
        "Build Hadoop",
        "CMDL",
        "Lowlevel",
        "MVC XML Spark",
        "DataFrame",
        "Hadoop Developer",
        "JavaApplication Framework",
        "JSP",
        "the Software Development Life Cycle",
        "JSPView",
        "Rational Rose",
        "Integrated KafkaSparkstreaming",
        "JSON Responsible",
        "MVC",
        "Monitoring Systems Use Eclipse",
        "Spark",
        "Wrote Hibernate",
        "JavaHadoop",
        "HTML CSS",
        "Hibernate Query Language HQL Operated",
        "Sqoop",
        "HIVE",
        "Exception Handling Collection",
        "Maven Implementation of Spring Restful",
        "Created",
        "Spark Programming Implement Joins Grouping and Aggregations for the Pair",
        "Spark Core Spark",
        "AWS",
        "Spark Transformations Environment Hadoop HDFS",
        "Oracle",
        "SpringFramework Implemented",
        "Oracle DB",
        "Preparation of Technical",
        "Databricks",
        "OOPS Multithreading Collections",
        "Ford Southfield MI Talented",
        "SQL",
        "Spark RDD",
        "RDD DataFrames",
        "Relational Database Systems",
        "DAO Singleton",
        "HADOOP Clusters",
        "Big Data",
        "Hive",
        "Hadoop Working",
        "HiveQL",
        "SolrCloud",
        "CDH4",
        "IAM Identity",
        "Ford",
        "Spark Responsibilities Worked",
        "Hadoop Design",
        "ETL",
        "S3buckets",
        "Talend Data Quality Created ScalaSpark",
        "JavaScript Created",
        "Hibernate",
        "Impala",
        "Impala Mapreduce Oozie Sqoop",
        "JavaScript",
        "ANT",
        "XML Web Services",
        "UI",
        "Hibernatemapping XML",
        "AJAXtechnologies",
        "Sparktransformations Handson",
        "Delete",
        "HadoopSpark Developer HadoopSpark",
        "JSP Custom Tags Used Jenkins",
        "Python Resolve",
        "SVN",
        "Created HBase",
        "Hadoop Spark",
        "HiveImpala",
        "MavenScripts",
        "Tomcat",
        "Data",
        "Avro Implemented MapReduce",
        "MapReduce YARN HDFS Hbase ImpalaHive Pig OozieAWS ApacheSpark",
        "MapReduce",
        "NoSQL",
        "JSP HTML CSS",
        "Weblogic Application",
        "Front Controllers",
        "Spring Interceptors"
    ],
    "experience": "Experience on analyzing data in NOSQL databases like Hbase and Cassandraand its Integration with Hadoop cluster Hands on experience with Spark Core Spark SQL and Data FramesData SetsRDD API Experience in using Kafka and Kafka brokers to initiate spark context and processing live streaming information with the help of RDD Developed Java applications using various IDEs like Spring Tool Suite and Eclipse Good knowledge in using Hibernate for mapping Java classes with database and using Hibernate Query Language HQL Operated on JavaJ2EE systems with different databases which include Oracle MySQL and DB2 Knowledge on implementing Big Data in Amazon Elastic MapReduce Amazon EMR for processing managing Hadoop framework dynamically scalable Amazon EC2 instances Capable of processing large sets of structured semistructured and unstructureddata and supporting systems application architecture Extensive development experience in sparkapplications for datatransformations and loading into HDFS using RDD DataFrames and Datasets Extensive knowledge on performance tuning of Spark applications and converting HiveSQL queries into Sparktransformations Handson experience with AWS AmazonWebServices using ElasticMapReduce EMR creating and storing data in S3buckets and creating ElasticLoadBalancersELB for Hadoop front end WebUIs Extensive knowledge on creating Hadoop cluster on multiple EC2 instances in AWS and configuring them through ambari and using IAM Identity and AccessManagement for creating groups users and assigning permissions Extensive programming experience in JavaCore concepts like OOPS Multithreading Collections and IO Experience using Jira for ticketing issues and Jenkins for continuous integration Extensive experience with UNIX commands shellscripting and setting up CRON jobs Experience in software configuration management using Git Good experience in using Relational databases OracleMySQL Able to assess businessrules collaborate with stakeholders and perform sourcetotarget datamapping design Successfully working in fastpaced environment both independently and in collaborative team environments Work Experience HadoopSpark Developer Ford Southfield MI May 2018 to Present Responsibilities Implemented Hive UDFs and did performance tuning for better results Analyzed the data by performing Hive queries and running Pig Scripts Implemented optimized map joins to get data from different sources to perform cleaning operations before applying the algorithms MonitoringMaintaining daytoday batch jobs using Event Engine Interacting with SOR team to fix production data issues Performing data transformation using Pig and Hive Performing Snapshot full refresh and Load append data refresh Providing support for CMDL raw data and ODL ingestion EFS Big Data applications Experience in using Sqoop to import and export the data from Netezza and Oracle DB into HDFS and HIVE Involved in loading data from UNIX file system to HDFS Handled importing of data from various data sources performed transformations using Hive MapReduce and loaded data into HDFS Providing weeklymonthly status reports to customer Worked on analyzing data with Hive and Pig and real time analytical operations using Hbase Worked on loading and transforming of large sets of structured semi structured and unstructured data Participated in all the meetings with the Database owners for the approvals and extraction of the data Retrieving data from external web APIs and indexing it into our distributed SolrCloud platform Also maintained and deployed the production and development SolrCloud setup using Ansible Other responsibilities included building proofs of concept in C for transforming incoming data into schema compatible with the existing SQL data as well as building monitoring scripts in Python Resolve missing fields in DataFrame rows using filtering and imputation Integrate visualizations into a Spark application using Databricks and popular visualization libraries ggplot matplotlib Faster processing and testing of data is achieved by implementing Spark SQL and Spark using Scala Experienced on addinginstallation of new components and removal of them through Ambari Experience with data wrangling and creating workable datasets Monitoring systems and services through Ambari dashboard to make the clusters available for the business Responsible for building scalable distributed data solutions using Hadoop Working as Hadoop Developer and admin in Hortonworks HDP 2242 distribution for 10 clusters ranges from POC to PROD Prepared the Business and high level design documents Created and maintained Technical documentation for launching HADOOP Clusters and for executing Hive queries and Pig Scripts Installed Oozie workflow engine to run multiple Hive and Pig jobs which run independently with time and data availability Assisted application teams in installing Hadoop updates operating system patches and version upgrades when required Implemented Map reduce secondary sorting to get better performance for sorting results in Map Reduce programs Worked on User Defined Functions in Hive to load the data from HDFS to run aggregation function on multiple rows Involved in moving all log files generated from various sources to HDFS for further processing through Flume Coordinated with the testing team for bug fixes and created documentation for recorded data agent usage and release cycle notes Used Teradata system Priority Schedule in controlling the load of the system Created different UDFs and UDAFs to analyze the partitioned and bucketed data and compute various metrics for reporting on the dashboard and stored them in different summary tables Implemented POC to introduce Spark Transformations Environment Hadoop HDFS hive Sqoop Kafka Spark Scala MapReduce Cloudera Kafka Zookeeper HBase Shell Scripting Python Mongo DB AWS UNIX Shell Scripting Hadoop Developer T Mobile Bellevue WA January 2017 to April 2018 AWS with Spark Responsibilities Worked on improving the performance of existing Pig and Hive Queries Developed Oozie workflow engines to automate Hive and Pig jobs Worked on performing Join operations Exported the result set from Hive to MySQL using Sqoop after processing the data Analyzed the data by performing Hive queries and running Pig scripts to study customer behavior Used Hive to partition and bucket data Performed various source data ingestions cleansing and transformation in Hadoop Design and developed many Spark Programs using pyspark Produce unit tests for Spark transformations and helper methods Creating RDDs and Pair RDDs for Spark Programming Implement Joins Grouping and Aggregations for the Pair RDDs Write Scaladocstyle documentation with all code Experienced in performance tuning of Spark Applications for setting right Batch Interval time correct level of Parallelism and memory tuning Developed Pig Scripts to perform ETL procedures on the data in HDFS Analyzed the partitioned and bucketed data and compute various metrics for reporting Created HBase tables to store various data formats of data coming from different systems Advanced knowledge in performance troubleshooting and tuning Cassandra clusters Analyzing the source data to know the quality of data by using Talend Data Quality Created ScalaSpark jobs for data transformation and aggregation Involved in creating Hive tables loading with data and writing hive queries Used Impala to read write and query the Hadoop data in HDFS from Cassandra and configured Kafka to read and write messages from external programs Preparation of Technical architecture and Lowlevel design documents Tested raw data and executed performance scripts Environment eclipse jdk180 Hadoop28 HDFS MapReduceSpark 20 Pig0150 Hive20 HBase ApacheMaven3 JavaHadoop Developer Charles Schwab Austin TX January 2015 to December 2016 Responsibilities Implemented J2EEDesignPatterns like DAO Singleton and Factory Managed connectivity using JDBC for queryinginserting data management including triggers and stored procedures Used SpringMVC framework to enable the interactions between JSPView layer and implemented different design patterns with J2EE and XML technology Worked on Python and build the custom ingest framework Implemented application using MVC architecture integrating Hibernate and spring frameworks Utilized various JavaScript and JQuery libraries Bootstrap Ajax for form validation and other interactive features Extensively worked on Hadoop ecosystems including Hive Spark Streaming with MapRdistribution Upgraded the Hadoop Cluster from CDH3 to CDH4 setting up High Availability Cluster and integrating Hive with existing applications Worked on storing data in HDFS either directly or through Hbase Worked on NoSQL support enterprise production and loading data into HBase using Impala and Sqoop Performed multiple MapReduce jobs in Pig and Hive for data cleaning and preprocessing Build Hadoop solutions for big data problems using MR1 and MR2 in YARN Handled importing of data from various data sources performed transformations using Hive PIG and loaded data into HDFS Worked on data using Sqoop from HDFS to Relational Database Systems and viceversa Maintaining and troubleshooting Exploring with Spark to improve the performance and optimization of the existing algorithms in Hadoop using Spark context SparkSQL Data Frame pair RDDs Created Hive Tables loaded claims data from Oracle using Sqoop and loaded the processed data into target database Worked on Delete printer module using python Involved in PLSQL query optimization to reduce the overall run time of stored procedures Exported data from HDFS to RDBMS via Sqoop for Business Intelligence visualization and user report generation Implemented the J2EE design patterns Data Access Object DAO Session Faade and Business Delegate Developed Nififlows dealing with various kinds of data formats such as XML JSON and Avro Implemented MapReduce jobs in HIVE by querying the available data Proactively involved in ongoing maintenance support and improvements in Hadoop cluster Developed Spark code using Scala and SparkSQLStreaming for faster testing and processing of data Used Cloudera Manager for installation and management of Hadoop Cluster Collaborated with business usersproduct owners developers to contribute to the analysis of functional requirements Involved in converting HiveQL into Spark transformations using Spark RDD and through Scala programming Integrated KafkaSparkstreaming for high efficiency throughput and reliability Worked in tuning HivePig to improve performance and solved performance issues in both scripts Environment Hadoop 30 PythonHive 21 J2EE Hbase JDBC Pig 016 HBase 11 Sqoop NoSQL Impala Java Spring MVC XML Spark 19 PLSQL HDFS JSON Hibernate Bootstrap JQuery JavaScript Ajax Java Developer Catalyte Chicago IL October 2013 to December 2014 Responsibilities As a Java Developer involved in backend and frontend developing team Involved in the Software Development Life Cycle SDLC including Analysis Design Implementation Responsible for use case diagrams class diagrams and sequence diagrams using Rational Rose in the Design phase Developed ANT scripts that checkout code from SVN repository build EAR files Used XML Web Services using SOAP to transfer information to the supply chain and domain expertise Monitoring Systems Use Eclipse and Tomcat web server for developing deploying the applications Developed REST Web Services clients to consume those Web Services as well other enterprise wide Web Services Used JavaScript and AJAXtechnologies for front end user input validations and Spring validation framework for backend validation for the User Interface Used both annotation based configuration and XML based Developed application service components and configured beans using   Spring IOC Implemented persistence mechanism using Hibernate ORM Mapping Developed the DAO layer for the application using Spring Hibernate Template support Used WebLogic workshop Eclipse IDE to develop the application Performed the code build and deployment using Maven Implementation of Spring Restful web services which produces JSON Responsible for maintaining the code quality coding and implementation standards by code reviews Developed the front end of the application using HTML CSS JSP and JavaScript Created RESTFULL APIs using Spring MVC Used SVN version controller to maintain the code versions Worked on web applications using open source MVCframeworks Developed Web interface using JSP Standard Tag Libraries JSTL and SpringFramework Implemented logger for debugging and testing purposes using Log4j Environment JSON HTML 4 CSS XML Hibernate 36 Eclipse Maven JUnit JDBC ANT SOAP Log4j Java Developer Paychex Rochester NY October 2011 to September 2013 Responsibilities Individually worked on all the stages of a SoftwareDevelopmentLifeCycle SDLC Responsible for design and implementation of various modules of the application using StrutsSpringHibernate architecture Created userfriendly GUI interface and Web pages using HTML CSS and JSP Developed web components using MVC pattern under Struts framework Wrote JSPs Servlets and deployed them on Weblogic Application server Used JSPs HTML on front end Servlets as Front Controllers and JavaScript for client side validations Wrote the Hibernatemapping XML files to define java classesdatabase tables mapping Developed the UI using JSP HTML CSS and AJAX and learned how to implement JQuery JSP and client server validations using JavaScript Implemented MVC architecture by using spring to send and receive the data from frontend to business layer Designed developed and maintained the data layer using JDBC and performed configuration of JavaApplication Framework Extensively used Hibernate in data access layer to access and update information in the database Migrated the Servlets to the Spring Controllers and developed Spring Interceptors worked on JSPs JSTL and JSP Custom Tags Used Jenkins for continuous integration purpose in using SVN JUnit and Mockito as version control and Unit testing by Creating design documents and test cases for development work Worked on Eclipse IDE for front end development environment for insertions updating and retrieval operations of data from oracle database by writing stored procedures Responsible for writing Struts action classes Hibernate POJO classes and integrating Struts and Hibernate with spring for processing business needs Developed the application using Servlets and JSP for the presentation layer along with JavaScript for the client side validations Wrote Hibernate classes DAOs to retrieve store data configured Hibernate files Used Web Logic for application deployment and Log4J used for Loggingdebugging Used CVSversion controlling tool and project build tool using ANT Used various Core Java concepts such as multithreading Exception Handling Collection APIs to implement various features and enhancements Wrote and debugged the MavenScripts for building the entire web application Designed and developed Ajax calls to populate screens parts on demand Environment Struts HTML CSS JSP MVC Hibernate JSP AJAX JQuery Java Jenkins ANT Maven Education Bachelors Skills Hdfs Impala Mapreduce Oozie Sqoop",
    "extracted_keywords": [
        "HadoopSpark",
        "Developer",
        "HadoopSpark",
        "span",
        "lDeveloperspan",
        "HadoopSpark",
        "Developer",
        "Ford",
        "Southfield",
        "MI",
        "Talented",
        "Software",
        "Engineer",
        "years",
        "IT",
        "experience",
        "applications",
        "BigData",
        "AWS",
        "JavaSQL",
        "Spark",
        "years",
        "experience",
        "Big",
        "Data",
        "tools",
        "MapReduce",
        "YARN",
        "HDFS",
        "Hbase",
        "ImpalaHive",
        "Pig",
        "ApacheSpark",
        "ingestion",
        "storage",
        "processing",
        "analysis",
        "data",
        "Performance",
        "HiveImpala",
        "methods",
        "partitioning",
        "bucketing",
        "indexing",
        "files",
        "compressions",
        "Hands",
        "experience",
        "ingestion",
        "tools",
        "Kafka",
        "Flume",
        "management",
        "tools",
        "Oozie",
        "Zena",
        "Hands",
        "experience",
        "file",
        "formats",
        "AVRO",
        "ORC",
        "Parquet",
        "compression",
        "techniques",
        "zlib",
        "lzo",
        "Hands",
        "experience",
        "Hadoop",
        "Ecosystem",
        "components",
        "Hadoop",
        "Spark",
        "HDFS",
        "YARN",
        "TEZ",
        "Hive",
        "Sqoop",
        "Flume",
        "MapReduce",
        "SCALA",
        "Pig",
        "OOZIE",
        "Kafka",
        "NIFI",
        "Storm",
        "HBASE",
        "data",
        "NOSQL",
        "Hbase",
        "Cassandraand",
        "Integration",
        "Hadoop",
        "cluster",
        "Hands",
        "experience",
        "Spark",
        "Core",
        "Spark",
        "SQL",
        "Data",
        "FramesData",
        "API",
        "Experience",
        "Kafka",
        "Kafka",
        "brokers",
        "spark",
        "context",
        "information",
        "help",
        "RDD",
        "Java",
        "applications",
        "IDEs",
        "Spring",
        "Tool",
        "Suite",
        "Eclipse",
        "knowledge",
        "Hibernate",
        "mapping",
        "Java",
        "classes",
        "database",
        "Hibernate",
        "Query",
        "Language",
        "HQL",
        "JavaJ2EE",
        "systems",
        "databases",
        "Oracle",
        "MySQL",
        "DB2",
        "Knowledge",
        "Big",
        "Data",
        "Amazon",
        "Elastic",
        "MapReduce",
        "Amazon",
        "EMR",
        "Hadoop",
        "framework",
        "Amazon",
        "EC2",
        "sets",
        "semistructured",
        "unstructureddata",
        "systems",
        "application",
        "architecture",
        "development",
        "experience",
        "sparkapplications",
        "datatransformations",
        "loading",
        "HDFS",
        "RDD",
        "DataFrames",
        "Datasets",
        "knowledge",
        "performance",
        "tuning",
        "Spark",
        "applications",
        "HiveSQL",
        "queries",
        "Sparktransformations",
        "Handson",
        "experience",
        "AWS",
        "AmazonWebServices",
        "ElasticMapReduce",
        "EMR",
        "data",
        "S3buckets",
        "ElasticLoadBalancersELB",
        "Hadoop",
        "front",
        "knowledge",
        "Hadoop",
        "cluster",
        "EC2",
        "instances",
        "AWS",
        "IAM",
        "Identity",
        "AccessManagement",
        "groups",
        "users",
        "permissions",
        "programming",
        "experience",
        "JavaCore",
        "concepts",
        "Multithreading",
        "Collections",
        "IO",
        "Experience",
        "Jira",
        "issues",
        "Jenkins",
        "integration",
        "experience",
        "UNIX",
        "commands",
        "CRON",
        "jobs",
        "Experience",
        "software",
        "configuration",
        "management",
        "Git",
        "Good",
        "experience",
        "Relational",
        "databases",
        "OracleMySQL",
        "businessrules",
        "stakeholders",
        "sourcetotarget",
        "datamapping",
        "design",
        "environment",
        "team",
        "Work",
        "Experience",
        "HadoopSpark",
        "Developer",
        "Ford",
        "Southfield",
        "MI",
        "May",
        "Present",
        "Responsibilities",
        "Hive",
        "UDFs",
        "performance",
        "results",
        "data",
        "Hive",
        "queries",
        "Pig",
        "Scripts",
        "map",
        "data",
        "sources",
        "operations",
        "algorithms",
        "MonitoringMaintaining",
        "daytoday",
        "batch",
        "jobs",
        "Event",
        "Engine",
        "Interacting",
        "SOR",
        "team",
        "production",
        "data",
        "issues",
        "data",
        "transformation",
        "Pig",
        "Hive",
        "Performing",
        "Snapshot",
        "refresh",
        "Load",
        "append",
        "data",
        "support",
        "CMDL",
        "data",
        "ODL",
        "ingestion",
        "EFS",
        "Big",
        "Data",
        "applications",
        "Experience",
        "Sqoop",
        "data",
        "Netezza",
        "Oracle",
        "DB",
        "HDFS",
        "HIVE",
        "loading",
        "data",
        "UNIX",
        "file",
        "system",
        "importing",
        "data",
        "data",
        "sources",
        "transformations",
        "Hive",
        "MapReduce",
        "data",
        "HDFS",
        "Providing",
        "status",
        "reports",
        "customer",
        "data",
        "Hive",
        "Pig",
        "time",
        "operations",
        "Hbase",
        "Worked",
        "loading",
        "transforming",
        "sets",
        "data",
        "meetings",
        "Database",
        "owners",
        "approvals",
        "extraction",
        "data",
        "data",
        "web",
        "APIs",
        "SolrCloud",
        "platform",
        "production",
        "development",
        "SolrCloud",
        "setup",
        "responsibilities",
        "building",
        "proofs",
        "concept",
        "C",
        "data",
        "schema",
        "SQL",
        "data",
        "monitoring",
        "scripts",
        "Python",
        "Resolve",
        "fields",
        "DataFrame",
        "rows",
        "filtering",
        "imputation",
        "Integrate",
        "visualizations",
        "Spark",
        "application",
        "Databricks",
        "visualization",
        "libraries",
        "ggplot",
        "matplotlib",
        "processing",
        "testing",
        "data",
        "Spark",
        "SQL",
        "Spark",
        "Scala",
        "addinginstallation",
        "components",
        "removal",
        "Ambari",
        "Experience",
        "data",
        "datasets",
        "Monitoring",
        "systems",
        "services",
        "dashboard",
        "clusters",
        "business",
        "data",
        "solutions",
        "Hadoop",
        "Working",
        "Hadoop",
        "Developer",
        "admin",
        "Hortonworks",
        "HDP",
        "distribution",
        "clusters",
        "POC",
        "PROD",
        "Prepared",
        "Business",
        "level",
        "design",
        "documents",
        "documentation",
        "HADOOP",
        "Clusters",
        "Hive",
        "queries",
        "Pig",
        "Scripts",
        "Installed",
        "Oozie",
        "workflow",
        "engine",
        "Hive",
        "Pig",
        "jobs",
        "time",
        "data",
        "availability",
        "Assisted",
        "application",
        "teams",
        "Hadoop",
        "updates",
        "operating",
        "system",
        "patches",
        "version",
        "upgrades",
        "Implemented",
        "Map",
        "sorting",
        "performance",
        "results",
        "Map",
        "Reduce",
        "programs",
        "User",
        "Defined",
        "Functions",
        "Hive",
        "data",
        "HDFS",
        "aggregation",
        "function",
        "rows",
        "log",
        "files",
        "sources",
        "HDFS",
        "processing",
        "Flume",
        "Coordinated",
        "testing",
        "team",
        "bug",
        "fixes",
        "documentation",
        "data",
        "agent",
        "usage",
        "release",
        "cycle",
        "notes",
        "Teradata",
        "system",
        "Priority",
        "Schedule",
        "load",
        "system",
        "UDFs",
        "UDAFs",
        "data",
        "metrics",
        "dashboard",
        "summary",
        "tables",
        "POC",
        "Spark",
        "Transformations",
        "Environment",
        "Hadoop",
        "HDFS",
        "hive",
        "Sqoop",
        "Kafka",
        "Spark",
        "Scala",
        "MapReduce",
        "Cloudera",
        "Kafka",
        "Zookeeper",
        "HBase",
        "Shell",
        "Scripting",
        "Python",
        "Mongo",
        "DB",
        "AWS",
        "UNIX",
        "Shell",
        "Scripting",
        "Hadoop",
        "Developer",
        "T",
        "Mobile",
        "Bellevue",
        "WA",
        "January",
        "April",
        "AWS",
        "Spark",
        "Responsibilities",
        "performance",
        "Pig",
        "Hive",
        "Queries",
        "Oozie",
        "workflow",
        "engines",
        "Hive",
        "Pig",
        "jobs",
        "Join",
        "operations",
        "result",
        "Hive",
        "MySQL",
        "Sqoop",
        "data",
        "data",
        "Hive",
        "queries",
        "Pig",
        "scripts",
        "customer",
        "behavior",
        "Hive",
        "partition",
        "bucket",
        "data",
        "source",
        "data",
        "ingestions",
        "cleansing",
        "transformation",
        "Hadoop",
        "Design",
        "Spark",
        "Programs",
        "pyspark",
        "Produce",
        "unit",
        "tests",
        "Spark",
        "transformations",
        "helper",
        "methods",
        "RDDs",
        "Pair",
        "RDDs",
        "Spark",
        "Programming",
        "Implement",
        "Joins",
        "Grouping",
        "Aggregations",
        "Pair",
        "RDDs",
        "Scaladocstyle",
        "documentation",
        "code",
        "performance",
        "tuning",
        "Spark",
        "Applications",
        "Batch",
        "Interval",
        "time",
        "level",
        "Parallelism",
        "memory",
        "Developed",
        "Pig",
        "Scripts",
        "ETL",
        "procedures",
        "data",
        "HDFS",
        "data",
        "metrics",
        "Created",
        "HBase",
        "data",
        "formats",
        "data",
        "systems",
        "knowledge",
        "performance",
        "Cassandra",
        "clusters",
        "source",
        "data",
        "quality",
        "data",
        "Talend",
        "Data",
        "Quality",
        "Created",
        "ScalaSpark",
        "jobs",
        "data",
        "transformation",
        "aggregation",
        "Hive",
        "tables",
        "data",
        "hive",
        "queries",
        "Impala",
        "Hadoop",
        "data",
        "HDFS",
        "Cassandra",
        "Kafka",
        "messages",
        "programs",
        "Preparation",
        "architecture",
        "Lowlevel",
        "design",
        "documents",
        "data",
        "performance",
        "scripts",
        "Environment",
        "eclipse",
        "jdk180",
        "Hadoop28",
        "HDFS",
        "MapReduceSpark",
        "Pig0150",
        "Hive20",
        "HBase",
        "ApacheMaven3",
        "JavaHadoop",
        "Developer",
        "Charles",
        "Schwab",
        "Austin",
        "TX",
        "January",
        "December",
        "Responsibilities",
        "J2EEDesignPatterns",
        "DAO",
        "Singleton",
        "Factory",
        "Managed",
        "connectivity",
        "JDBC",
        "data",
        "management",
        "triggers",
        "procedures",
        "framework",
        "interactions",
        "JSPView",
        "layer",
        "design",
        "patterns",
        "J2EE",
        "XML",
        "technology",
        "Python",
        "custom",
        "ingest",
        "framework",
        "application",
        "MVC",
        "architecture",
        "Hibernate",
        "spring",
        "frameworks",
        "JavaScript",
        "JQuery",
        "Bootstrap",
        "Ajax",
        "form",
        "validation",
        "features",
        "Hadoop",
        "ecosystems",
        "Hive",
        "Spark",
        "Streaming",
        "MapRdistribution",
        "Hadoop",
        "Cluster",
        "CDH3",
        "CDH4",
        "High",
        "Availability",
        "Cluster",
        "Hive",
        "applications",
        "data",
        "HDFS",
        "Hbase",
        "Worked",
        "NoSQL",
        "support",
        "enterprise",
        "production",
        "loading",
        "data",
        "HBase",
        "Impala",
        "Sqoop",
        "MapReduce",
        "jobs",
        "Pig",
        "Hive",
        "data",
        "Build",
        "Hadoop",
        "solutions",
        "data",
        "problems",
        "MR1",
        "MR2",
        "YARN",
        "importing",
        "data",
        "data",
        "sources",
        "transformations",
        "Hive",
        "PIG",
        "data",
        "HDFS",
        "data",
        "Sqoop",
        "HDFS",
        "Relational",
        "Database",
        "Systems",
        "viceversa",
        "Maintaining",
        "Spark",
        "performance",
        "optimization",
        "algorithms",
        "Hadoop",
        "Spark",
        "context",
        "SparkSQL",
        "Data",
        "Frame",
        "pair",
        "RDDs",
        "Hive",
        "Tables",
        "claims",
        "data",
        "Oracle",
        "Sqoop",
        "data",
        "target",
        "database",
        "Delete",
        "printer",
        "module",
        "python",
        "PLSQL",
        "query",
        "optimization",
        "time",
        "procedures",
        "data",
        "HDFS",
        "Sqoop",
        "Business",
        "Intelligence",
        "visualization",
        "user",
        "report",
        "generation",
        "J2EE",
        "design",
        "patterns",
        "Data",
        "Access",
        "Object",
        "DAO",
        "Session",
        "Faade",
        "Business",
        "Delegate",
        "Developed",
        "Nififlows",
        "kinds",
        "data",
        "formats",
        "XML",
        "JSON",
        "Avro",
        "MapReduce",
        "jobs",
        "HIVE",
        "data",
        "maintenance",
        "support",
        "improvements",
        "Hadoop",
        "cluster",
        "Developed",
        "Spark",
        "code",
        "Scala",
        "SparkSQLStreaming",
        "testing",
        "processing",
        "data",
        "Cloudera",
        "Manager",
        "installation",
        "management",
        "Hadoop",
        "Cluster",
        "Collaborated",
        "business",
        "usersproduct",
        "owners",
        "developers",
        "analysis",
        "requirements",
        "HiveQL",
        "Spark",
        "transformations",
        "Spark",
        "RDD",
        "Scala",
        "programming",
        "Integrated",
        "KafkaSparkstreaming",
        "efficiency",
        "throughput",
        "reliability",
        "HivePig",
        "performance",
        "performance",
        "issues",
        "scripts",
        "Environment",
        "Hadoop",
        "PythonHive",
        "J2EE",
        "Hbase",
        "JDBC",
        "Pig",
        "HBase",
        "Sqoop",
        "NoSQL",
        "Impala",
        "Java",
        "Spring",
        "MVC",
        "XML",
        "Spark",
        "PLSQL",
        "JSON",
        "Hibernate",
        "Bootstrap",
        "JQuery",
        "JavaScript",
        "Ajax",
        "Java",
        "Developer",
        "Catalyte",
        "Chicago",
        "IL",
        "October",
        "December",
        "Responsibilities",
        "Java",
        "Developer",
        "backend",
        "team",
        "Software",
        "Development",
        "Life",
        "Cycle",
        "SDLC",
        "Analysis",
        "Design",
        "Implementation",
        "Responsible",
        "use",
        "case",
        "diagrams",
        "class",
        "diagrams",
        "sequence",
        "diagrams",
        "Rational",
        "Rose",
        "Design",
        "phase",
        "ANT",
        "scripts",
        "checkout",
        "code",
        "SVN",
        "repository",
        "build",
        "EAR",
        "files",
        "XML",
        "Web",
        "Services",
        "SOAP",
        "information",
        "supply",
        "chain",
        "domain",
        "expertise",
        "Monitoring",
        "Systems",
        "Use",
        "Eclipse",
        "Tomcat",
        "web",
        "server",
        "applications",
        "REST",
        "Web",
        "Services",
        "clients",
        "Web",
        "Services",
        "enterprise",
        "Web",
        "Services",
        "JavaScript",
        "AJAXtechnologies",
        "end",
        "user",
        "input",
        "validations",
        "Spring",
        "validation",
        "framework",
        "validation",
        "User",
        "Interface",
        "annotation",
        "configuration",
        "XML",
        "application",
        "service",
        "components",
        "beans",
        "Spring",
        "IOC",
        "persistence",
        "mechanism",
        "Hibernate",
        "ORM",
        "Mapping",
        "DAO",
        "layer",
        "application",
        "Spring",
        "Hibernate",
        "Template",
        "support",
        "WebLogic",
        "workshop",
        "Eclipse",
        "IDE",
        "application",
        "code",
        "build",
        "deployment",
        "Maven",
        "Implementation",
        "Spring",
        "Restful",
        "web",
        "services",
        "JSON",
        "code",
        "quality",
        "coding",
        "implementation",
        "standards",
        "code",
        "reviews",
        "end",
        "application",
        "HTML",
        "CSS",
        "JSP",
        "JavaScript",
        "Created",
        "RESTFULL",
        "APIs",
        "Spring",
        "MVC",
        "SVN",
        "version",
        "controller",
        "code",
        "versions",
        "web",
        "applications",
        "source",
        "MVCframeworks",
        "Developed",
        "Web",
        "interface",
        "JSP",
        "Standard",
        "Tag",
        "Libraries",
        "JSTL",
        "SpringFramework",
        "testing",
        "purposes",
        "Log4j",
        "Environment",
        "JSON",
        "HTML",
        "CSS",
        "XML",
        "Hibernate",
        "Eclipse",
        "Maven",
        "JUnit",
        "JDBC",
        "ANT",
        "SOAP",
        "Log4j",
        "Java",
        "Developer",
        "Paychex",
        "Rochester",
        "NY",
        "October",
        "September",
        "Responsibilities",
        "stages",
        "SoftwareDevelopmentLifeCycle",
        "SDLC",
        "design",
        "implementation",
        "modules",
        "application",
        "StrutsSpringHibernate",
        "architecture",
        "GUI",
        "interface",
        "Web",
        "pages",
        "HTML",
        "CSS",
        "JSP",
        "Developed",
        "web",
        "components",
        "MVC",
        "pattern",
        "Struts",
        "framework",
        "JSPs",
        "Servlets",
        "Weblogic",
        "Application",
        "server",
        "JSPs",
        "HTML",
        "end",
        "Servlets",
        "Front",
        "Controllers",
        "JavaScript",
        "client",
        "side",
        "validations",
        "Hibernatemapping",
        "XML",
        "files",
        "java",
        "classesdatabase",
        "mapping",
        "UI",
        "JSP",
        "HTML",
        "CSS",
        "AJAX",
        "JQuery",
        "JSP",
        "client",
        "server",
        "validations",
        "JavaScript",
        "MVC",
        "architecture",
        "spring",
        "data",
        "frontend",
        "business",
        "layer",
        "data",
        "layer",
        "JDBC",
        "configuration",
        "JavaApplication",
        "Framework",
        "Hibernate",
        "data",
        "access",
        "layer",
        "information",
        "database",
        "Servlets",
        "Spring",
        "Controllers",
        "Spring",
        "Interceptors",
        "JSPs",
        "JSTL",
        "JSP",
        "Custom",
        "Tags",
        "Jenkins",
        "integration",
        "purpose",
        "SVN",
        "JUnit",
        "Mockito",
        "version",
        "control",
        "Unit",
        "testing",
        "design",
        "documents",
        "test",
        "cases",
        "development",
        "work",
        "Eclipse",
        "IDE",
        "end",
        "development",
        "environment",
        "insertions",
        "retrieval",
        "operations",
        "data",
        "oracle",
        "database",
        "procedures",
        "Struts",
        "action",
        "classes",
        "Hibernate",
        "POJO",
        "classes",
        "Struts",
        "Hibernate",
        "spring",
        "business",
        "application",
        "Servlets",
        "JSP",
        "presentation",
        "layer",
        "JavaScript",
        "client",
        "side",
        "validations",
        "Wrote",
        "Hibernate",
        "DAOs",
        "store",
        "data",
        "Hibernate",
        "Web",
        "Logic",
        "application",
        "deployment",
        "Log4J",
        "CVSversion",
        "tool",
        "project",
        "build",
        "tool",
        "ANT",
        "Core",
        "Java",
        "concepts",
        "Exception",
        "Handling",
        "Collection",
        "APIs",
        "features",
        "enhancements",
        "Wrote",
        "MavenScripts",
        "web",
        "application",
        "calls",
        "screens",
        "parts",
        "demand",
        "Environment",
        "Struts",
        "HTML",
        "CSS",
        "JSP",
        "MVC",
        "Hibernate",
        "JSP",
        "AJAX",
        "JQuery",
        "Java",
        "Jenkins",
        "ANT",
        "Maven",
        "Education",
        "Bachelors",
        "Skills",
        "Hdfs",
        "Impala",
        "Mapreduce",
        "Oozie",
        "Sqoop"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T20:46:47.593812",
    "resume_data": "HadoopSpark Developer HadoopSpark span lDeveloperspan HadoopSpark Developer Ford Southfield MI Talented and accomplished Software Engineer with 8 years of IT experience in developing applications using BigData AWS JavaSQL and Spark 3 years of experience with Big Data tools like MapReduce YARN HDFS Hbase ImpalaHive Pig OozieAWS ApacheSpark for ingestion storage querying processing and analysis of data Performance tuning in HiveImpala using multiple methods limited to dynamic partitioning bucketing indexing files compressions Hands on experience withdata ingestion tools Kafka Flume and workflow management tools Oozie and Zena Hands on experience handling different file formats like JSON AVRO ORC Parquet and compression techniques like snappy zlib and lzo Hands on experience in Hadoop Ecosystem components such as Hadoop Spark HDFS YARN TEZ Hive Sqoop Flume MapReduce SCALA Pig OOZIE Kafka NIFI Storm HBASE Experience on analyzing data in NOSQL databases like Hbase and Cassandraand its Integration with Hadoop cluster Hands on experience with Spark Core Spark SQL and Data FramesData SetsRDD API Experience in using Kafka and Kafka brokers to initiate spark context and processing live streaming information with the help of RDD Developed Java applications using various IDEs like Spring Tool Suite and Eclipse Good knowledge in using Hibernate for mapping Java classes with database and using Hibernate Query Language HQL Operated on JavaJ2EE systems with different databases which include Oracle MySQL and DB2 Knowledge on implementing Big Data in Amazon Elastic MapReduce Amazon EMR for processing managing Hadoop framework dynamically scalable Amazon EC2 instances Capable of processing large sets of structured semistructured and unstructureddata and supporting systems application architecture Extensive development experience in sparkapplications for datatransformations and loading into HDFS using RDD DataFrames and Datasets Extensive knowledge on performance tuning of Spark applications and converting HiveSQL queries into Sparktransformations Handson experience with AWS AmazonWebServices using ElasticMapReduce EMR creating and storing data in S3buckets and creating ElasticLoadBalancersELB for Hadoop front end WebUIs Extensive knowledge on creating Hadoop cluster on multiple EC2 instances in AWS and configuring them through ambari and using IAM Identity and AccessManagement for creating groups users and assigning permissions Extensive programming experience in JavaCore concepts like OOPS Multithreading Collections and IO Experience using Jira for ticketing issues and Jenkins for continuous integration Extensive experience with UNIX commands shellscripting and setting up CRON jobs Experience in software configuration management using Git Good experience in using Relational databases OracleMySQL Able to assess businessrules collaborate with stakeholders and perform sourcetotarget datamapping design Successfully working in fastpaced environment both independently and in collaborative team environments Work Experience HadoopSpark Developer Ford Southfield MI May 2018 to Present Responsibilities Implemented Hive UDFs and did performance tuning for better results Analyzed the data by performing Hive queries and running Pig Scripts Implemented optimized map joins to get data from different sources to perform cleaning operations before applying the algorithms MonitoringMaintaining daytoday batch jobs using Event Engine Interacting with SOR team to fix production data issues Performing data transformation using Pig and Hive Performing Snapshot full refresh and Load append data refresh Providing support for CMDL raw data and ODL ingestion EFS Big Data applications Experience in using Sqoop to import and export the data from Netezza and Oracle DB into HDFS and HIVE Involved in loading data from UNIX file system to HDFS Handled importing of data from various data sources performed transformations using Hive MapReduce and loaded data into HDFS Providing weeklymonthly status reports to customer Worked on analyzing data with Hive and Pig and real time analytical operations using Hbase Worked on loading and transforming of large sets of structured semi structured and unstructured data Participated in all the meetings with the Database owners for the approvals and extraction of the data Retrieving data from external web APIs and indexing it into our distributed SolrCloud platform Also maintained and deployed the production and development SolrCloud setup using Ansible Other responsibilities included building proofs of concept in C for transforming incoming data into schema compatible with the existing SQL data as well as building monitoring scripts in Python Resolve missing fields in DataFrame rows using filtering and imputation Integrate visualizations into a Spark application using Databricks and popular visualization libraries ggplot matplotlib Faster processing and testing of data is achieved by implementing Spark SQL and Spark using Scala Experienced on addinginstallation of new components and removal of them through Ambari Experience with data wrangling and creating workable datasets Monitoring systems and services through Ambari dashboard to make the clusters available for the business Responsible for building scalable distributed data solutions using Hadoop Working as Hadoop Developer and admin in Hortonworks HDP 2242 distribution for 10 clusters ranges from POC to PROD Prepared the Business and high level design documents Created and maintained Technical documentation for launching HADOOP Clusters and for executing Hive queries and Pig Scripts Installed Oozie workflow engine to run multiple Hive and Pig jobs which run independently with time and data availability Assisted application teams in installing Hadoop updates operating system patches and version upgrades when required Implemented Map reduce secondary sorting to get better performance for sorting results in Map Reduce programs Worked on User Defined Functions in Hive to load the data from HDFS to run aggregation function on multiple rows Involved in moving all log files generated from various sources to HDFS for further processing through Flume Coordinated with the testing team for bug fixes and created documentation for recorded data agent usage and release cycle notes Used Teradata system Priority Schedule in controlling the load of the system Created different UDFs and UDAFs to analyze the partitioned and bucketed data and compute various metrics for reporting on the dashboard and stored them in different summary tables Implemented POC to introduce Spark Transformations Environment Hadoop HDFS hive Sqoop Kafka Spark Scala MapReduce Cloudera Kafka Zookeeper HBase Shell Scripting Python Mongo DB AWS UNIX Shell Scripting Hadoop Developer T Mobile Bellevue WA January 2017 to April 2018 AWS with Spark Responsibilities Worked on improving the performance of existing Pig and Hive Queries Developed Oozie workflow engines to automate Hive and Pig jobs Worked on performing Join operations Exported the result set from Hive to MySQL using Sqoop after processing the data Analyzed the data by performing Hive queries and running Pig scripts to study customer behavior Used Hive to partition and bucket data Performed various source data ingestions cleansing and transformation in Hadoop Design and developed many Spark Programs using pyspark Produce unit tests for Spark transformations and helper methods Creating RDDs and Pair RDDs for Spark Programming Implement Joins Grouping and Aggregations for the Pair RDDs Write Scaladocstyle documentation with all code Experienced in performance tuning of Spark Applications for setting right Batch Interval time correct level of Parallelism and memory tuning Developed Pig Scripts to perform ETL procedures on the data in HDFS Analyzed the partitioned and bucketed data and compute various metrics for reporting Created HBase tables to store various data formats of data coming from different systems Advanced knowledge in performance troubleshooting and tuning Cassandra clusters Analyzing the source data to know the quality of data by using Talend Data Quality Created ScalaSpark jobs for data transformation and aggregation Involved in creating Hive tables loading with data and writing hive queries Used Impala to read write and query the Hadoop data in HDFS from Cassandra and configured Kafka to read and write messages from external programs Preparation of Technical architecture and Lowlevel design documents Tested raw data and executed performance scripts Environment eclipse jdk180 Hadoop28 HDFS MapReduceSpark 20 Pig0150 Hive20 HBase ApacheMaven3 JavaHadoop Developer Charles Schwab Austin TX January 2015 to December 2016 Responsibilities Implemented J2EEDesignPatterns like DAO Singleton and Factory Managed connectivity using JDBC for queryinginserting data management including triggers and stored procedures Used SpringMVC framework to enable the interactions between JSPView layer and implemented different design patterns with J2EE and XML technology Worked on Python and build the custom ingest framework Implemented application using MVC architecture integrating Hibernate and spring frameworks Utilized various JavaScript and JQuery libraries Bootstrap Ajax for form validation and other interactive features Extensively worked on Hadoop ecosystems including Hive Spark Streaming with MapRdistribution Upgraded the Hadoop Cluster from CDH3 to CDH4 setting up High Availability Cluster and integrating Hive with existing applications Worked on storing data in HDFS either directly or through Hbase Worked on NoSQL support enterprise production and loading data into HBase using Impala and Sqoop Performed multiple MapReduce jobs in Pig and Hive for data cleaning and preprocessing Build Hadoop solutions for big data problems using MR1 and MR2 in YARN Handled importing of data from various data sources performed transformations using Hive PIG and loaded data into HDFS Worked on data using Sqoop from HDFS to Relational Database Systems and viceversa Maintaining and troubleshooting Exploring with Spark to improve the performance and optimization of the existing algorithms in Hadoop using Spark context SparkSQL Data Frame pair RDDs Created Hive Tables loaded claims data from Oracle using Sqoop and loaded the processed data into target database Worked on Delete printer module using python Involved in PLSQL query optimization to reduce the overall run time of stored procedures Exported data from HDFS to RDBMS via Sqoop for Business Intelligence visualization and user report generation Implemented the J2EE design patterns Data Access Object DAO Session Faade and Business Delegate Developed Nififlows dealing with various kinds of data formats such as XML JSON and Avro Implemented MapReduce jobs in HIVE by querying the available data Proactively involved in ongoing maintenance support and improvements in Hadoop cluster Developed Spark code using Scala and SparkSQLStreaming for faster testing and processing of data Used Cloudera Manager for installation and management of Hadoop Cluster Collaborated with business usersproduct owners developers to contribute to the analysis of functional requirements Involved in converting HiveQL into Spark transformations using Spark RDD and through Scala programming Integrated KafkaSparkstreaming for high efficiency throughput and reliability Worked in tuning HivePig to improve performance and solved performance issues in both scripts Environment Hadoop 30 PythonHive 21 J2EE Hbase JDBC Pig 016 HBase 11 Sqoop NoSQL Impala Java Spring MVC XML Spark 19 PLSQL HDFS JSON Hibernate Bootstrap JQuery JavaScript Ajax Java Developer Catalyte Chicago IL October 2013 to December 2014 Responsibilities As a Java Developer involved in backend and frontend developing team Involved in the Software Development Life Cycle SDLC including Analysis Design Implementation Responsible for use case diagrams class diagrams and sequence diagrams using Rational Rose in the Design phase Developed ANT scripts that checkout code from SVN repository build EAR files Used XML Web Services using SOAP to transfer information to the supply chain and domain expertise Monitoring Systems Use Eclipse and Tomcat web server for developing deploying the applications Developed REST Web Services clients to consume those Web Services as well other enterprise wide Web Services Used JavaScript and AJAXtechnologies for front end user input validations and Spring validation framework for backend validation for the User Interface Used both annotation based configuration and XML based Developed application service components and configured beans using applicationContextxml Spring IOC Implemented persistence mechanism using Hibernate ORM Mapping Developed the DAO layer for the application using Spring Hibernate Template support Used WebLogic workshop Eclipse IDE to develop the application Performed the code build and deployment using Maven Implementation of Spring Restful web services which produces JSON Responsible for maintaining the code quality coding and implementation standards by code reviews Developed the front end of the application using HTML CSS JSP and JavaScript Created RESTFULL APIs using Spring MVC Used SVN version controller to maintain the code versions Worked on web applications using open source MVCframeworks Developed Web interface using JSP Standard Tag Libraries JSTL and SpringFramework Implemented logger for debugging and testing purposes using Log4j Environment JSON HTML 4 CSS XML Hibernate 36 Eclipse Maven JUnit JDBC ANT SOAP Log4j Java Developer Paychex Rochester NY October 2011 to September 2013 Responsibilities Individually worked on all the stages of a SoftwareDevelopmentLifeCycle SDLC Responsible for design and implementation of various modules of the application using StrutsSpringHibernate architecture Created userfriendly GUI interface and Web pages using HTML CSS and JSP Developed web components using MVC pattern under Struts framework Wrote JSPs Servlets and deployed them on Weblogic Application server Used JSPs HTML on front end Servlets as Front Controllers and JavaScript for client side validations Wrote the Hibernatemapping XML files to define java classesdatabase tables mapping Developed the UI using JSP HTML CSS and AJAX and learned how to implement JQuery JSP and client server validations using JavaScript Implemented MVC architecture by using spring to send and receive the data from frontend to business layer Designed developed and maintained the data layer using JDBC and performed configuration of JavaApplication Framework Extensively used Hibernate in data access layer to access and update information in the database Migrated the Servlets to the Spring Controllers and developed Spring Interceptors worked on JSPs JSTL and JSP Custom Tags Used Jenkins for continuous integration purpose in using SVN JUnit and Mockito as version control and Unit testing by Creating design documents and test cases for development work Worked on Eclipse IDE for front end development environment for insertions updating and retrieval operations of data from oracle database by writing stored procedures Responsible for writing Struts action classes Hibernate POJO classes and integrating Struts and Hibernate with spring for processing business needs Developed the application using Servlets and JSP for the presentation layer along with JavaScript for the client side validations Wrote Hibernate classes DAOs to retrieve store data configured Hibernate files Used Web Logic for application deployment and Log4J used for Loggingdebugging Used CVSversion controlling tool and project build tool using ANT Used various Core Java concepts such as multithreading Exception Handling Collection APIs to implement various features and enhancements Wrote and debugged the MavenScripts for building the entire web application Designed and developed Ajax calls to populate screens parts on demand Environment Struts HTML CSS JSP MVC Hibernate JSP AJAX JQuery Java Jenkins ANT Maven Education Bachelors Skills Hdfs Impala Mapreduce Oozie Sqoop",
    "unique_id": "b8e30830-3003-41b9-8fd4-6becddaae164"
}