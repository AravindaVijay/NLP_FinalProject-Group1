{
    "clean_data": "Spark Developer Spark span lDeveloperspan Spark Developer State Farm Work Experience Spark Developer State Farm Bloomington IL October 2018 to Present Responsibilities Worked under the Cloudera distribution CDH 513 version Worked on Ingesting weblog data into HDFS using Kafka Used Spark SQL to process JSON data Performed Cleansing the data to get a desired format Wrote Spark Sql Data frames into Parquet Files Worked on Tuning Spark Jobs for optimal Efficiency Wrote the Python functions procedures Constructors and Traits Created Hive tables to load the transformed Data Performed partitions and bucketing in hive for easy data classification Involved in Analyzing data by writing queries using HiveQL for faster data processing Involved in working with Sqoop for loading the data into RDBMS Created a data pipeline using oozie which runs on daily basis Involved in Persisting Metadata into HDFS for further data processing Loading data from Linux Filesystems to HDFS and viceversa Involved in creating tables partitioning bucketing of table and creating UDFs along with fine tuning in Hive Loaded the Cleaned Data into the hive tables and performed some analysis based on the requirements Responsible in performing sort join aggregations filter and other transformations on the datasets Utilized Agile and Scrum Methodology to help manage and organize a team of developers with regular code review sessions Environment HDFS Apache Spark Apache Hive Python Oozie Flume Kafka Agile Methodology Cloudera Cassandra HadoopSpark Developer Caterpillar Chicago IL August 2017 to November 2018 Responsibilities Worked on Cluster size of 150200 nodes Responsible for building scalable distributed data solutions using Hadoop Worked on migrating Map Reduce programs into Spark transformations using Spark and Python Using SparkStreaming APIs to perform transformations and actions on the fly for building the common learner data model which gets the data from Kafka in near real time and Persists into Cassandra Developed Spark scripts by using Python shell commands as per the requirement Used Spark API over Cloudera Hadoop YARN to perform analytics on data in Hive Developed Python scripts UDFFs using both Data framesSQL and RDDMapReduce in Spark 16 for Data Aggregation queries and writing data back into OLTP system through Sqoop And Developed enterprise application using Python as well Expertise in performance tuning of Spark Applications for setting right Batch Interval time correct level of Parallelism and memory tuning Loaded the data into Spark RDD and do in memory data Computation to generate the Output response Experience and handson knowledge in Akka and LIFT Framework Used PostgreSQL and NoSQL database and integrated with Hadoop to develop datasets on HDFS Involved in creating partitioned Hive tables and loading and analysing data using hive queries Implemented Partitioning and bucketing in Hive Worked on a POC to compare processing time of Impala with Apache Hive for batch applications to implement the former in project Developed Hive queries to process the data and generate the data cubes for visualizing Implemented schema extraction for Parquet and Avro file Formats in Hive Good experience with Talend open studio for designing ETL Jobs for Processing of data Experience designing reviewing implementing and optimizing data transformation processes in the Hadoop and Talend Informatica ecosystems Implemented Partitioning Dynamic Partitions Buckets in HIVE Coordinated with admins and Sr Technical staff for migrating Teradata to Hadoop and Ab Initio to Hadoop as well Configured Hadoop clusters and coordinated with Big Data Admins for cluster maintenance Environment Hadoop YARN SparkCore SparkStreaming SparkSQL Python Kafka Hive Sqoop Amazon AWS Elastic Search Impala Cassandra Tableau Informatica Cloudera Oracle 10g Linux Hadoop Developer Tachyon Technologies LLC October 2015 to November 2016 Responsibilities Used Flume as a data pipeline system to ingest the unstructured events from various web servers to HDFS We altered the unstructured events from web servers on the fly using various flume interceptors Wrote various spark transformations using Python to perform data cleansing validation and summarization activities on user behavioral data Parsed the unstructured data into semistructured format by writing complex algorithms in spark using Python Developed generic parser to transform any format of unstructured data into a consisted data model Configured Flume using Python with the Spark Streaming to transfer the data into HDFS at regular intervals of time from web servers to process the data Implemented the persistence of frequently used transformed data from data frames for faster processing Build hive tables on the transformed data and used different SERDEs to store the data in HDFS in different formats Loaded the transformed Data into the hive tables and perform some analysis based on the requirements Implemented portioning on the Hive data to increase the performance of the processing of data Analyzed the data by performing Hive queries Hive QL to study customer behavior Created Pig Latin scripts to sort group join and filter to transform the data Worked on various performance optimizations like using distributed cache for small datasets Partitioning Bucketing in Hive and Map Side joins Exported the analyzed data to the relational databases using Sqoop to further visualize and generate reports for the BI team Implemented custom workflow to automate the jobs on daily basis Created custom workflows to automate Sqoop jobs weekly and monthly Environment HDFS Python Hive Sqoop Flume Spark MapReduce Oracle 11g YARN UNIX Shell Scripting Agile Methodology Cloudera Python Developer Tachyon Technologies LLC May 2014 to September 2015 Responsibilities Worked with Open stack Commandline client Created backend database TSQL stored procedures and Jasper Reports Created a Git repository and added the project to GitHub Used Python modules such as requests urllib urllib2 for web crawling Used other packages such as Beautiful soup for data parsing Worked on writing and as well as read data from csv and excel file formats Worked on resulting reports of the application and Tableau reports Performed QA testing on the application Held meetings with client and worked all alone for the entire project with limited help from the client Utilize PyUnit the Python unit test framework for all Python applications ExportedImported data between different data sources using SQL Server Management Studio Designed and developed the UI of the website using HTML XHTML AJAX CSS and JavaScript Developed views and templates with Python and Djangos view controller and templating language to create a userfriendly website interface Working with Database procedures Triggers PLSQL statements for data retrievingand also for migration purpose Environment Python 2730 PLSQL C Redshift XML Agile SCRUM PyUnit MYSQL Apache CSS MySQL DHTML HTML JavaScript Shell Scripts Git Linux Unix and Windows Education Masters Skills Apache 3 years Apache hadoop hdfs 3 years Apache hadoop oozie Less than 1 year Apache hadoop sqoop 3 years Apache kafka 1 year databases 1 year Flume 1 year Hadoop 2 years Hadoop distributed file system 3 years Hdfs 3 years Hive 3 years Kafka 1 year Linux 3 years map reduce 1 year Mysql 1 year Oozie Less than 1 year Oracle 2 years Pig 1 year Python 4 years Sqoop 3 years Additional Information TECHNICAL SKILLS Big Data Technologies Apache Spark Apache Hadoop Map Reduce Apache Hive Apache Pig Apache Sqoop Apache Kafka Apache Flume Apache oozie Hue Apache Zookeeper HDFS amazon S3 EC2 EMR Languages Scala Python Java Databases MySQL Oracle 11g Operating Systems Mac OS Windows 710 Linux Cent OS Redhat Ubuntu Development Tools IntelliJ Maven Scala Test GitHub Jenkins",
    "entities": [
        "Wrote Spark",
        "Cloudera Hadoop",
        "ETL Jobs for Processing of data",
        "SQL Server Management Studio Designed",
        "CDH",
        "ExportedImported",
        "Developed",
        "Talend Informatica",
        "Sqoop",
        "sort group join",
        "Build",
        "BI",
        "Impala",
        "HDFS",
        "Created",
        "Developer Tachyon Technologies",
        "Djangos",
        "Additional Information TECHNICAL SKILLS Big Data Technologies Apache Spark",
        "Data Aggregation",
        "HadoopSpark Developer",
        "Data Performed",
        "Git",
        "Akka",
        "Apache Sqoop",
        "Hadoop Worked",
        "RDBMS Created",
        "Linux Filesystems",
        "Apache",
        "Spark",
        "Developed Hive",
        "Big Data Admins",
        "Constructors and Traits Created Hive",
        "Linux",
        "Spark Developer Spark",
        "Ab Initio",
        "S3 EC2",
        "Hadoop",
        "Data",
        "OLTP",
        "Spark RDD",
        "HDFS Involved",
        "GitHub Used Python",
        "Talend",
        "NoSQL",
        "Utilized Agile",
        "Tableau",
        "Linux Hadoop Developer Tachyon Technologies",
        "IL",
        "Python Developed",
        "csv",
        "Jasper Reports Created",
        "Hive",
        "Sr Technical",
        "Windows Education Masters Skills",
        "the Cleaned Data"
    ],
    "experience": "Experience Spark Developer State Farm Bloomington IL October 2018 to Present Responsibilities Worked under the Cloudera distribution CDH 513 version Worked on Ingesting weblog data into HDFS using Kafka Used Spark SQL to process JSON data Performed Cleansing the data to get a desired format Wrote Spark Sql Data frames into Parquet Files Worked on Tuning Spark Jobs for optimal Efficiency Wrote the Python functions procedures Constructors and Traits Created Hive tables to load the transformed Data Performed partitions and bucketing in hive for easy data classification Involved in Analyzing data by writing queries using HiveQL for faster data processing Involved in working with Sqoop for loading the data into RDBMS Created a data pipeline using oozie which runs on daily basis Involved in Persisting Metadata into HDFS for further data processing Loading data from Linux Filesystems to HDFS and viceversa Involved in creating tables partitioning bucketing of table and creating UDFs along with fine tuning in Hive Loaded the Cleaned Data into the hive tables and performed some analysis based on the requirements Responsible in performing sort join aggregations filter and other transformations on the datasets Utilized Agile and Scrum Methodology to help manage and organize a team of developers with regular code review sessions Environment HDFS Apache Spark Apache Hive Python Oozie Flume Kafka Agile Methodology Cloudera Cassandra HadoopSpark Developer Caterpillar Chicago IL August 2017 to November 2018 Responsibilities Worked on Cluster size of 150200 nodes Responsible for building scalable distributed data solutions using Hadoop Worked on migrating Map Reduce programs into Spark transformations using Spark and Python Using SparkStreaming APIs to perform transformations and actions on the fly for building the common learner data model which gets the data from Kafka in near real time and Persists into Cassandra Developed Spark scripts by using Python shell commands as per the requirement Used Spark API over Cloudera Hadoop YARN to perform analytics on data in Hive Developed Python scripts UDFFs using both Data framesSQL and RDDMapReduce in Spark 16 for Data Aggregation queries and writing data back into OLTP system through Sqoop And Developed enterprise application using Python as well Expertise in performance tuning of Spark Applications for setting right Batch Interval time correct level of Parallelism and memory tuning Loaded the data into Spark RDD and do in memory data Computation to generate the Output response Experience and handson knowledge in Akka and LIFT Framework Used PostgreSQL and NoSQL database and integrated with Hadoop to develop datasets on HDFS Involved in creating partitioned Hive tables and loading and analysing data using hive queries Implemented Partitioning and bucketing in Hive Worked on a POC to compare processing time of Impala with Apache Hive for batch applications to implement the former in project Developed Hive queries to process the data and generate the data cubes for visualizing Implemented schema extraction for Parquet and Avro file Formats in Hive Good experience with Talend open studio for designing ETL Jobs for Processing of data Experience designing reviewing implementing and optimizing data transformation processes in the Hadoop and Talend Informatica ecosystems Implemented Partitioning Dynamic Partitions Buckets in HIVE Coordinated with admins and Sr Technical staff for migrating Teradata to Hadoop and Ab Initio to Hadoop as well Configured Hadoop clusters and coordinated with Big Data Admins for cluster maintenance Environment Hadoop YARN SparkCore SparkStreaming SparkSQL Python Kafka Hive Sqoop Amazon AWS Elastic Search Impala Cassandra Tableau Informatica Cloudera Oracle 10 g Linux Hadoop Developer Tachyon Technologies LLC October 2015 to November 2016 Responsibilities Used Flume as a data pipeline system to ingest the unstructured events from various web servers to HDFS We altered the unstructured events from web servers on the fly using various flume interceptors Wrote various spark transformations using Python to perform data cleansing validation and summarization activities on user behavioral data Parsed the unstructured data into semistructured format by writing complex algorithms in spark using Python Developed generic parser to transform any format of unstructured data into a consisted data model Configured Flume using Python with the Spark Streaming to transfer the data into HDFS at regular intervals of time from web servers to process the data Implemented the persistence of frequently used transformed data from data frames for faster processing Build hive tables on the transformed data and used different SERDEs to store the data in HDFS in different formats Loaded the transformed Data into the hive tables and perform some analysis based on the requirements Implemented portioning on the Hive data to increase the performance of the processing of data Analyzed the data by performing Hive queries Hive QL to study customer behavior Created Pig Latin scripts to sort group join and filter to transform the data Worked on various performance optimizations like using distributed cache for small datasets Partitioning Bucketing in Hive and Map Side joins Exported the analyzed data to the relational databases using Sqoop to further visualize and generate reports for the BI team Implemented custom workflow to automate the jobs on daily basis Created custom workflows to automate Sqoop jobs weekly and monthly Environment HDFS Python Hive Sqoop Flume Spark MapReduce Oracle 11 g YARN UNIX Shell Scripting Agile Methodology Cloudera Python Developer Tachyon Technologies LLC May 2014 to September 2015 Responsibilities Worked with Open stack Commandline client Created backend database TSQL stored procedures and Jasper Reports Created a Git repository and added the project to GitHub Used Python modules such as requests urllib urllib2 for web crawling Used other packages such as Beautiful soup for data parsing Worked on writing and as well as read data from csv and excel file formats Worked on resulting reports of the application and Tableau reports Performed QA testing on the application Held meetings with client and worked all alone for the entire project with limited help from the client Utilize PyUnit the Python unit test framework for all Python applications ExportedImported data between different data sources using SQL Server Management Studio Designed and developed the UI of the website using HTML XHTML AJAX CSS and JavaScript Developed views and templates with Python and Djangos view controller and templating language to create a userfriendly website interface Working with Database procedures Triggers PLSQL statements for data retrievingand also for migration purpose Environment Python 2730 PLSQL C Redshift XML Agile SCRUM PyUnit MYSQL Apache CSS MySQL DHTML HTML JavaScript Shell Scripts Git Linux Unix and Windows Education Masters Skills Apache 3 years Apache hadoop hdfs 3 years Apache hadoop oozie Less than 1 year Apache hadoop sqoop 3 years Apache kafka 1 year databases 1 year Flume 1 year Hadoop 2 years Hadoop distributed file system 3 years Hdfs 3 years Hive 3 years Kafka 1 year Linux 3 years map reduce 1 year Mysql 1 year Oozie Less than 1 year Oracle 2 years Pig 1 year Python 4 years Sqoop 3 years Additional Information TECHNICAL SKILLS Big Data Technologies Apache Spark Apache Hadoop Map Reduce Apache Hive Apache Pig Apache Sqoop Apache Kafka Apache Flume Apache oozie Hue Apache Zookeeper HDFS amazon S3 EC2 EMR Languages Scala Python Java Databases MySQL Oracle 11 g Operating Systems Mac OS Windows 710 Linux Cent OS Redhat Ubuntu Development Tools IntelliJ Maven Scala Test GitHub Jenkins",
    "extracted_keywords": [
        "Spark",
        "Developer",
        "Spark",
        "span",
        "lDeveloperspan",
        "Spark",
        "Developer",
        "State",
        "Farm",
        "Work",
        "Experience",
        "Spark",
        "Developer",
        "State",
        "Farm",
        "Bloomington",
        "IL",
        "October",
        "Present",
        "Responsibilities",
        "Cloudera",
        "distribution",
        "CDH",
        "version",
        "weblog",
        "data",
        "HDFS",
        "Kafka",
        "Spark",
        "SQL",
        "JSON",
        "data",
        "data",
        "format",
        "Wrote",
        "Spark",
        "Sql",
        "Data",
        "frames",
        "Parquet",
        "Files",
        "Spark",
        "Jobs",
        "Efficiency",
        "Python",
        "functions",
        "Constructors",
        "Traits",
        "Hive",
        "tables",
        "transformed",
        "Data",
        "Performed",
        "partitions",
        "bucketing",
        "hive",
        "data",
        "classification",
        "data",
        "queries",
        "HiveQL",
        "data",
        "processing",
        "Sqoop",
        "data",
        "RDBMS",
        "data",
        "pipeline",
        "oozie",
        "basis",
        "Persisting",
        "Metadata",
        "HDFS",
        "data",
        "Loading",
        "data",
        "Linux",
        "Filesystems",
        "HDFS",
        "viceversa",
        "tables",
        "bucketing",
        "table",
        "UDFs",
        "tuning",
        "Hive",
        "Loaded",
        "Cleaned",
        "Data",
        "tables",
        "analysis",
        "requirements",
        "sort",
        "join",
        "aggregations",
        "filter",
        "transformations",
        "datasets",
        "Agile",
        "Scrum",
        "Methodology",
        "team",
        "developers",
        "code",
        "review",
        "sessions",
        "Environment",
        "HDFS",
        "Apache",
        "Spark",
        "Apache",
        "Hive",
        "Python",
        "Oozie",
        "Flume",
        "Kafka",
        "Agile",
        "Methodology",
        "Cloudera",
        "Cassandra",
        "HadoopSpark",
        "Developer",
        "Caterpillar",
        "Chicago",
        "IL",
        "August",
        "November",
        "Responsibilities",
        "Cluster",
        "size",
        "nodes",
        "data",
        "solutions",
        "Hadoop",
        "Worked",
        "Map",
        "Reduce",
        "programs",
        "Spark",
        "transformations",
        "Spark",
        "Python",
        "SparkStreaming",
        "APIs",
        "transformations",
        "actions",
        "fly",
        "learner",
        "data",
        "model",
        "data",
        "Kafka",
        "time",
        "Persists",
        "Cassandra",
        "Developed",
        "Spark",
        "scripts",
        "Python",
        "shell",
        "commands",
        "requirement",
        "Spark",
        "API",
        "Cloudera",
        "Hadoop",
        "YARN",
        "analytics",
        "data",
        "Hive",
        "Developed",
        "Python",
        "UDFFs",
        "Data",
        "framesSQL",
        "RDDMapReduce",
        "Spark",
        "Data",
        "Aggregation",
        "data",
        "OLTP",
        "system",
        "Sqoop",
        "enterprise",
        "application",
        "Python",
        "Expertise",
        "performance",
        "tuning",
        "Spark",
        "Applications",
        "Batch",
        "Interval",
        "time",
        "level",
        "Parallelism",
        "memory",
        "data",
        "Spark",
        "RDD",
        "memory",
        "data",
        "Computation",
        "Output",
        "response",
        "Experience",
        "knowledge",
        "Akka",
        "LIFT",
        "Framework",
        "PostgreSQL",
        "NoSQL",
        "database",
        "Hadoop",
        "datasets",
        "HDFS",
        "Hive",
        "tables",
        "loading",
        "data",
        "queries",
        "Partitioning",
        "Hive",
        "Worked",
        "POC",
        "processing",
        "time",
        "Impala",
        "Apache",
        "Hive",
        "batch",
        "applications",
        "project",
        "Developed",
        "Hive",
        "queries",
        "data",
        "data",
        "cubes",
        "Implemented",
        "schema",
        "extraction",
        "Parquet",
        "Avro",
        "file",
        "Formats",
        "Hive",
        "Good",
        "experience",
        "Talend",
        "studio",
        "ETL",
        "Jobs",
        "Processing",
        "data",
        "Experience",
        "data",
        "transformation",
        "processes",
        "Hadoop",
        "Talend",
        "Informatica",
        "Dynamic",
        "Partitions",
        "Buckets",
        "HIVE",
        "Coordinated",
        "admins",
        "Sr",
        "Technical",
        "staff",
        "Teradata",
        "Hadoop",
        "Ab",
        "Initio",
        "Hadoop",
        "Configured",
        "Hadoop",
        "clusters",
        "Big",
        "Data",
        "Admins",
        "cluster",
        "maintenance",
        "Environment",
        "Hadoop",
        "YARN",
        "SparkCore",
        "SparkStreaming",
        "SparkSQL",
        "Python",
        "Kafka",
        "Hive",
        "Sqoop",
        "Amazon",
        "AWS",
        "Elastic",
        "Search",
        "Impala",
        "Cassandra",
        "Tableau",
        "Informatica",
        "Cloudera",
        "Oracle",
        "g",
        "Linux",
        "Hadoop",
        "Developer",
        "Tachyon",
        "Technologies",
        "LLC",
        "October",
        "November",
        "Responsibilities",
        "Flume",
        "data",
        "pipeline",
        "system",
        "events",
        "web",
        "servers",
        "HDFS",
        "events",
        "web",
        "servers",
        "fly",
        "flume",
        "interceptors",
        "spark",
        "transformations",
        "Python",
        "data",
        "cleansing",
        "validation",
        "summarization",
        "activities",
        "user",
        "data",
        "data",
        "format",
        "algorithms",
        "spark",
        "Python",
        "parser",
        "format",
        "data",
        "data",
        "model",
        "Configured",
        "Flume",
        "Python",
        "Spark",
        "Streaming",
        "data",
        "HDFS",
        "intervals",
        "time",
        "web",
        "servers",
        "data",
        "persistence",
        "data",
        "data",
        "frames",
        "Build",
        "hive",
        "tables",
        "data",
        "SERDEs",
        "data",
        "HDFS",
        "formats",
        "transformed",
        "Data",
        "tables",
        "analysis",
        "requirements",
        "Hive",
        "data",
        "performance",
        "processing",
        "data",
        "data",
        "Hive",
        "queries",
        "Hive",
        "QL",
        "customer",
        "behavior",
        "Created",
        "Pig",
        "Latin",
        "scripts",
        "group",
        "join",
        "data",
        "performance",
        "optimizations",
        "cache",
        "datasets",
        "Bucketing",
        "Hive",
        "Map",
        "Side",
        "data",
        "databases",
        "Sqoop",
        "reports",
        "BI",
        "team",
        "custom",
        "jobs",
        "basis",
        "custom",
        "workflows",
        "Sqoop",
        "jobs",
        "Environment",
        "HDFS",
        "Python",
        "Hive",
        "Sqoop",
        "Flume",
        "Spark",
        "MapReduce",
        "Oracle",
        "g",
        "YARN",
        "UNIX",
        "Shell",
        "Scripting",
        "Agile",
        "Methodology",
        "Cloudera",
        "Python",
        "Developer",
        "Tachyon",
        "Technologies",
        "LLC",
        "May",
        "September",
        "Responsibilities",
        "stack",
        "Commandline",
        "client",
        "backend",
        "database",
        "TSQL",
        "procedures",
        "Jasper",
        "Reports",
        "Git",
        "repository",
        "project",
        "GitHub",
        "Python",
        "modules",
        "requests",
        "urllib2",
        "web",
        "packages",
        "soup",
        "data",
        "writing",
        "data",
        "csv",
        "file",
        "formats",
        "reports",
        "application",
        "Tableau",
        "Performed",
        "QA",
        "testing",
        "application",
        "meetings",
        "client",
        "project",
        "help",
        "client",
        "Utilize",
        "PyUnit",
        "Python",
        "unit",
        "test",
        "framework",
        "Python",
        "applications",
        "ExportedImported",
        "data",
        "data",
        "sources",
        "SQL",
        "Server",
        "Management",
        "Studio",
        "UI",
        "website",
        "HTML",
        "XHTML",
        "AJAX",
        "CSS",
        "JavaScript",
        "views",
        "templates",
        "Python",
        "Djangos",
        "controller",
        "templating",
        "language",
        "website",
        "interface",
        "Working",
        "Database",
        "procedures",
        "Triggers",
        "PLSQL",
        "statements",
        "data",
        "retrievingand",
        "migration",
        "purpose",
        "Environment",
        "Python",
        "PLSQL",
        "C",
        "Redshift",
        "XML",
        "Agile",
        "SCRUM",
        "PyUnit",
        "MYSQL",
        "Apache",
        "CSS",
        "MySQL",
        "DHTML",
        "HTML",
        "JavaScript",
        "Shell",
        "Scripts",
        "Git",
        "Linux",
        "Unix",
        "Windows",
        "Education",
        "Masters",
        "Skills",
        "Apache",
        "years",
        "Apache",
        "hadoop",
        "hdfs",
        "years",
        "Apache",
        "hadoop",
        "oozie",
        "year",
        "Apache",
        "hadoop",
        "sqoop",
        "years",
        "Apache",
        "year",
        "year",
        "Flume",
        "year",
        "Hadoop",
        "years",
        "Hadoop",
        "file",
        "system",
        "years",
        "Hdfs",
        "years",
        "Hive",
        "years",
        "Kafka",
        "year",
        "Linux",
        "years",
        "map",
        "year",
        "Mysql",
        "year",
        "Oozie",
        "year",
        "Oracle",
        "years",
        "Pig",
        "year",
        "Python",
        "years",
        "Sqoop",
        "years",
        "Information",
        "TECHNICAL",
        "SKILLS",
        "Big",
        "Data",
        "Technologies",
        "Apache",
        "Spark",
        "Apache",
        "Hadoop",
        "Map",
        "Reduce",
        "Apache",
        "Hive",
        "Apache",
        "Pig",
        "Apache",
        "Sqoop",
        "Apache",
        "Kafka",
        "Apache",
        "Flume",
        "Apache",
        "oozie",
        "Hue",
        "Apache",
        "Zookeeper",
        "HDFS",
        "amazon",
        "S3",
        "EC2",
        "EMR",
        "Languages",
        "Scala",
        "Python",
        "Java",
        "MySQL",
        "Oracle",
        "g",
        "Operating",
        "Systems",
        "Mac",
        "OS",
        "Linux",
        "Cent",
        "OS",
        "Redhat",
        "Ubuntu",
        "Development",
        "Tools",
        "IntelliJ",
        "Maven",
        "Scala",
        "Test",
        "GitHub",
        "Jenkins"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T21:18:33.446116",
    "resume_data": "Spark Developer Spark span lDeveloperspan Spark Developer State Farm Work Experience Spark Developer State Farm Bloomington IL October 2018 to Present Responsibilities Worked under the Cloudera distribution CDH 513 version Worked on Ingesting weblog data into HDFS using Kafka Used Spark SQL to process JSON data Performed Cleansing the data to get a desired format Wrote Spark Sql Data frames into Parquet Files Worked on Tuning Spark Jobs for optimal Efficiency Wrote the Python functions procedures Constructors and Traits Created Hive tables to load the transformed Data Performed partitions and bucketing in hive for easy data classification Involved in Analyzing data by writing queries using HiveQL for faster data processing Involved in working with Sqoop for loading the data into RDBMS Created a data pipeline using oozie which runs on daily basis Involved in Persisting Metadata into HDFS for further data processing Loading data from Linux Filesystems to HDFS and viceversa Involved in creating tables partitioning bucketing of table and creating UDFs along with fine tuning in Hive Loaded the Cleaned Data into the hive tables and performed some analysis based on the requirements Responsible in performing sort join aggregations filter and other transformations on the datasets Utilized Agile and Scrum Methodology to help manage and organize a team of developers with regular code review sessions Environment HDFS Apache Spark Apache Hive Python Oozie Flume Kafka Agile Methodology Cloudera Cassandra HadoopSpark Developer Caterpillar Chicago IL August 2017 to November 2018 Responsibilities Worked on Cluster size of 150200 nodes Responsible for building scalable distributed data solutions using Hadoop Worked on migrating Map Reduce programs into Spark transformations using Spark and Python Using SparkStreaming APIs to perform transformations and actions on the fly for building the common learner data model which gets the data from Kafka in near real time and Persists into Cassandra Developed Spark scripts by using Python shell commands as per the requirement Used Spark API over Cloudera Hadoop YARN to perform analytics on data in Hive Developed Python scripts UDFFs using both Data framesSQL and RDDMapReduce in Spark 16 for Data Aggregation queries and writing data back into OLTP system through Sqoop And Developed enterprise application using Python as well Expertise in performance tuning of Spark Applications for setting right Batch Interval time correct level of Parallelism and memory tuning Loaded the data into Spark RDD and do in memory data Computation to generate the Output response Experience and handson knowledge in Akka and LIFT Framework Used PostgreSQL and NoSQL database and integrated with Hadoop to develop datasets on HDFS Involved in creating partitioned Hive tables and loading and analysing data using hive queries Implemented Partitioning and bucketing in Hive Worked on a POC to compare processing time of Impala with Apache Hive for batch applications to implement the former in project Developed Hive queries to process the data and generate the data cubes for visualizing Implemented schema extraction for Parquet and Avro file Formats in Hive Good experience with Talend open studio for designing ETL Jobs for Processing of data Experience designing reviewing implementing and optimizing data transformation processes in the Hadoop and Talend Informatica ecosystems Implemented Partitioning Dynamic Partitions Buckets in HIVE Coordinated with admins and Sr Technical staff for migrating Teradata to Hadoop and Ab Initio to Hadoop as well Configured Hadoop clusters and coordinated with Big Data Admins for cluster maintenance Environment Hadoop YARN SparkCore SparkStreaming SparkSQL Python Kafka Hive Sqoop Amazon AWS Elastic Search Impala Cassandra Tableau Informatica Cloudera Oracle 10g Linux Hadoop Developer Tachyon Technologies LLC October 2015 to November 2016 Responsibilities Used Flume as a data pipeline system to ingest the unstructured events from various web servers to HDFS We altered the unstructured events from web servers on the fly using various flume interceptors Wrote various spark transformations using Python to perform data cleansing validation and summarization activities on user behavioral data Parsed the unstructured data into semistructured format by writing complex algorithms in spark using Python Developed generic parser to transform any format of unstructured data into a consisted data model Configured Flume using Python with the Spark Streaming to transfer the data into HDFS at regular intervals of time from web servers to process the data Implemented the persistence of frequently used transformed data from data frames for faster processing Build hive tables on the transformed data and used different SERDEs to store the data in HDFS in different formats Loaded the transformed Data into the hive tables and perform some analysis based on the requirements Implemented portioning on the Hive data to increase the performance of the processing of data Analyzed the data by performing Hive queries Hive QL to study customer behavior Created Pig Latin scripts to sort group join and filter to transform the data Worked on various performance optimizations like using distributed cache for small datasets Partitioning Bucketing in Hive and Map Side joins Exported the analyzed data to the relational databases using Sqoop to further visualize and generate reports for the BI team Implemented custom workflow to automate the jobs on daily basis Created custom workflows to automate Sqoop jobs weekly and monthly Environment HDFS Python Hive Sqoop Flume Spark MapReduce Oracle 11g YARN UNIX Shell Scripting Agile Methodology Cloudera Python Developer Tachyon Technologies LLC May 2014 to September 2015 Responsibilities Worked with Open stack Commandline client Created backend database TSQL stored procedures and Jasper Reports Created a Git repository and added the project to GitHub Used Python modules such as requests urllib urllib2 for web crawling Used other packages such as Beautiful soup for data parsing Worked on writing and as well as read data from csv and excel file formats Worked on resulting reports of the application and Tableau reports Performed QA testing on the application Held meetings with client and worked all alone for the entire project with limited help from the client Utilize PyUnit the Python unit test framework for all Python applications ExportedImported data between different data sources using SQL Server Management Studio Designed and developed the UI of the website using HTML XHTML AJAX CSS and JavaScript Developed views and templates with Python and Djangos view controller and templating language to create a userfriendly website interface Working with Database procedures Triggers PLSQL statements for data retrievingand also for migration purpose Environment Python 2730 PLSQL C Redshift XML Agile SCRUM PyUnit MYSQL Apache CSS MySQL DHTML HTML JavaScript Shell Scripts Git Linux Unix and Windows Education Masters Skills Apache 3 years Apache hadoop hdfs 3 years Apache hadoop oozie Less than 1 year Apache hadoop sqoop 3 years Apache kafka 1 year databases 1 year Flume 1 year Hadoop 2 years Hadoop distributed file system 3 years Hdfs 3 years Hive 3 years Kafka 1 year Linux 3 years map reduce 1 year Mysql 1 year Oozie Less than 1 year Oracle 2 years Pig 1 year Python 4 years Sqoop 3 years Additional Information TECHNICAL SKILLS Big Data Technologies Apache Spark Apache Hadoop Map Reduce Apache Hive Apache Pig Apache Sqoop Apache Kafka Apache Flume Apache oozie Hue Apache Zookeeper HDFS amazon S3 EC2 EMR Languages Scala Python Java Databases MySQL Oracle 11g Operating Systems Mac OS Windows 710 Linux Cent OS Redhat Ubuntu Development Tools IntelliJ Maven Scala Test GitHub Jenkins",
    "unique_id": "db5943fd-c97b-4f23-93af-170ca6dc1ba3"
}