{
    "clean_data": "Sr Python Developer Sr span lPythonspan span lDeveloperspan Sr Python Developer Tiaacref CaryNC Work Experience Sr Python Developer Tiaacref Cary NC US August 2017 to Present Responsibilities Implemented Data Exploration to analyze patterns and to select features using Python SciPy Built Factor Analysis and Cluster Analysis models using Python SciPy to classify customers into different target groups Using R and Python withggplot2 packages performed an extensive graphical visualization of overall data including customized graphical representation of revenue reports specific item sales statistics and visualization Evaluated business requirements and prepared detailed specifications that follow project guidelines required to develop written programs Used Oozie workflow engine to run multiple Hive and Pig jobs Participated in Data Acquisition with Data Engineer team to extract historical and realtime data by using python Library Pandas Analyzed the partitioned and bucketed data and compute various metrics for reporting Involved in loading data from RDBMS and web logs into SQL and  Extracted data from Twitter using Java and Twitter API Parsed JSON formatted twitter data and uploaded to database Launching Amazon EC2 Cloud Instances using Amazon Images Linux Ubuntu and Configuring launched instances with respect to specific applications Developed Hive queries for analysis and exported the result set from Hive to MySQL using Sqoop after processing the data Analyzed the data by performing Hive queries and running Pig scripts to study customer behavior Created HBase tables to store various data formats of data coming from different portfolios Worked on improving performance of existing Pig and Hive Queries Created reports and dashboards by using D3js and Tableau 9x to explain and communicate data insights significant features models scores and performance of new recommendation system to both technical and business teams Utilize SQL Excel and several MarketingWeb Analytics tools Google Analytics AdWords in order to complete business marketing analysis and assessment Used Git 2x for version control with Data Engineer team and Data Scientists colleagues Used Agile methodology and SCRUM process for project developing Environment Python 27 Django HTML5CSS PostgreSQL MS SQL Server 2013 MySQL JavaScript Jupyter Notebook VIM Pycharm Shell Scripting AngularJS JIRA Python Developer Amneal Long Island April 2015 to June 2017 Responsibilities Perform Data Profiling to learn about behavior with various features such as traffic pattern location Date and Time etc Extracted the data from hive tables by writing efficient Hive queries Performed preliminary data analysis using descriptive statistics and handled anomalies such as removing duplicates and imputing missing values Analyze Data and Performed Data Preparation by applying historical model on the data set in Tablue Performed data cleaning and feature selection using MLlib package in PySpark and working with deep learning frameworks such as Caffe Neon etc Conducted a hybrid of Hierarchical and Kmeans Cluster Analysis using IBM SPSS and identified meaningful segments of customers through a discovery approach Develop SparkScala Python R for regular expression regex project in the HadoopHive environment with LinuxWindows for big data resources Used clustering technique KMeans to identify outliers and to classify unlabeled data Evaluate models using Cross Validation Log loss function ROC curves and used AUC for feature selection and elastic technologies like Elastic Search Kibana etc Work with NLTK library to NLP data processing and finding the patterns Categorize comments into positive and negative clusters from different social networking sites using Sentiment Analysis and Text Analytics Analyze traffic patterns by calculating autocorrelation with different time lags Ensure that the model has low False Positive Rate and Text classification and sentiment analysis for unstructured and semistructured data Addressed over fitting by implementing of the algorithm regularization methods like L2 and L1 Use Principal Component Analysis in feature engineering to analyze high dimensional data Create and design reports that will use gathered metrics to infer and draw logical conclusions of past and future behavior Perform Multinomial Logistic Regression Random forest Decision Tree SVM to classify package is going to deliver on time for the new route Implemented different models like Logistic Regression Random Forest and GradientBoost Trees to predict whether a given die will pass or fail the test Perform data analysis by using Hive to retrieve the data from Sql to retrieve datafrom Oracle database and used ETL for data transformation Perform Data Cleaning features scaling features engineering using pandas and numpy packages in python Develop Map Reduce pipeline for feature extraction using Hive and Pig Create Data Quality Scripts using SQL and Hive to validate successful data load and quality of the data Create various types of data visualizations using Python and Tableau Communicate the results with operations team for taking best decisions Collect data needs and requirements by Interacting with the other departments Environment Python 2x R HDFS Hadoop 23 Hive Linux Spark IBM SPSS Tableau Desktop SQL Server 2012 Microsoft Excel Matlab Spark SQL Pyspark Python Developer Rite Aid Richmond VA US September 2012 to March 2015 Responsibilities Worked in comprehending and examining the clients business requirements Used Django frameworks and Python to build dynamic webpages Developed tools for monitoring and notification using Python Enhanced the application by using HTML and Java script for design and development Used data structures like directories tuples Object Oriented class based inheritance features for making complex algorithms of networks Created PHPMySQL backend for data entry from Flash and worked in tandem with the Flash developer to obtain the correct data through query string Involved in designing database Model APIs Views using python to build an interactive web based solution Generated Python Django Forms to record data of online users Implemented Data tables to add delete update and display patient records and policy information using PyQt Implemented a module to connect and view the status of an Apache Cassandra instance using python Developed MVC prototype replacement of current product with Django Improved the Data Security and generated report efficiently by caching and reusing data Created UI using JavaScript and HTML5CSS3 Managed datasets using Panda data frames and MYSQL Queried the database queries using PythonMySQL connector and retrieved information using MySQLdb Recorded the online users data using Python Django forms and implemented test case using Pytest Developed the application using the Testdriven methodology and designed the unit tests using Python Unit test framework Created web application prototype usingJQuery and Angular JS Deployed the project into Heroku using GIT version control system Maintained and Updated the application in accordance to the clienteles requirement Environment Python 3 Django 16 Tableau 82 Beautiful soup HTML5 CSSCSS3 Bootstrap XML JSON JavaScript JQuery Angular JS Backbone JS Restful Web services Apache spark Linux Git Amazon s3 Jenkins MySQL Mongo DB TSQL Eclipse Education Bachelors Skills HTML5 3 years Java 3 years Linux 6 years Python 6 years SQL 6 years Additional Information Technical Skills Languages Java 8 Python R Packages ggplot2 caret dplyr Rweka gmodels RCurl C50 twitter NLP Reshape2 rjson plyr pandas numPy Seaborn sciPy matplot lib scikitlearn Beautiful Soup Rpy2 Web Technologies JDBC HTML5 DHTML and XML CSS3 Web Services WSDL Data Modelling Tools Erwin r 96 95 91 8x Rational Rose ERStudio MS Visio SAP Power designer Big Data Technologies Hadoop Hive HDFS MapReduce Pig Kafka Databases SQL Databases SQLServer My SQL MS Access Reporting Tools MS Office WordExcelPower Point Visio Tableau Crystal reports XI Business Intelligence SSRS Business Objects 5x 6x Cognos7060 ETL Tools Informatica Power Centre SSIS Version Control Tools SVM GitHub BI Tools Tableau Tableau Server Tableau Reader Amazon Redshift Operating System Windows Linux Unix Macintosh HD Red Hat",
    "entities": [
        "Data Scientists",
        "MLlib",
        "GIT",
        "Python",
        "Data Acquisition",
        "GradientBoost Trees",
        "ETL",
        "Developed",
        "Utilize SQL Excel",
        "Macintosh",
        "US",
        "Analyze Data",
        "Sqoop",
        "Perform",
        "LinuxWindows",
        "Perform Data Cleaning",
        "Model APIs Views",
        "Developed MVC",
        "Tablue Performed",
        "AUC",
        "Additional Information Technical Skills Languages",
        "Created UI",
        "Python Unit",
        "JavaScript",
        "Oracle",
        "PySpark",
        "Microsoft",
        "Categorize",
        "Perform Multinomial Logistic Regression Random",
        "Hive",
        "Django Improved",
        "IBM",
        "Sql",
        "Big Data Technologies Hadoop Hive HDFS MapReduce Pig Kafka Databases SQL Databases",
        "HTML",
        "Python SciPy Built Factor Analysis",
        "Present Responsibilities Implemented Data Exploration",
        "Pig Create Data Quality Scripts",
        "Implemented Data",
        "Created HBase",
        "Panda",
        "Heroku",
        "Informatica Power Centre SSIS Version",
        "Control Tools",
        "Google Analytics AdWords",
        "SQL",
        "Evaluate",
        "Tableau Desktop",
        "Logistic Regression Random Forest",
        "Data Engineer",
        "NLP",
        "lPythonspan",
        "Performed Data Preparation",
        "RDBMS",
        "Kmeans Cluster Analysis",
        "Tableau",
        "the Data Security",
        "Python Django",
        "Maintained",
        "XI Business Intelligence SSRS Business Objects",
        "ROC",
        "Testdriven",
        "Cluster Analysis",
        "PyQt",
        "Amazon",
        "MarketingWeb Analytics",
        "GitHub BI",
        "SAP Power",
        "Cross Validation Log",
        "Created PHPMySQL"
    ],
    "experience": "Experience Sr Python Developer Tiaacref Cary NC US August 2017 to Present Responsibilities Implemented Data Exploration to analyze patterns and to select features using Python SciPy Built Factor Analysis and Cluster Analysis models using Python SciPy to classify customers into different target groups Using R and Python withggplot2 packages performed an extensive graphical visualization of overall data including customized graphical representation of revenue reports specific item sales statistics and visualization Evaluated business requirements and prepared detailed specifications that follow project guidelines required to develop written programs Used Oozie workflow engine to run multiple Hive and Pig jobs Participated in Data Acquisition with Data Engineer team to extract historical and realtime data by using python Library Pandas Analyzed the partitioned and bucketed data and compute various metrics for reporting Involved in loading data from RDBMS and web logs into SQL and   Extracted data from Twitter using Java and Twitter API Parsed JSON formatted twitter data and uploaded to database Launching Amazon EC2 Cloud Instances using Amazon Images Linux Ubuntu and Configuring launched instances with respect to specific applications Developed Hive queries for analysis and exported the result set from Hive to MySQL using Sqoop after processing the data Analyzed the data by performing Hive queries and running Pig scripts to study customer behavior Created HBase tables to store various data formats of data coming from different portfolios Worked on improving performance of existing Pig and Hive Queries Created reports and dashboards by using D3js and Tableau 9x to explain and communicate data insights significant features models scores and performance of new recommendation system to both technical and business teams Utilize SQL Excel and several MarketingWeb Analytics tools Google Analytics AdWords in order to complete business marketing analysis and assessment Used Git 2x for version control with Data Engineer team and Data Scientists colleagues Used Agile methodology and SCRUM process for project developing Environment Python 27 Django HTML5CSS PostgreSQL MS SQL Server 2013 MySQL JavaScript Jupyter Notebook VIM Pycharm Shell Scripting AngularJS JIRA Python Developer Amneal Long Island April 2015 to June 2017 Responsibilities Perform Data Profiling to learn about behavior with various features such as traffic pattern location Date and Time etc Extracted the data from hive tables by writing efficient Hive queries Performed preliminary data analysis using descriptive statistics and handled anomalies such as removing duplicates and imputing missing values Analyze Data and Performed Data Preparation by applying historical model on the data set in Tablue Performed data cleaning and feature selection using MLlib package in PySpark and working with deep learning frameworks such as Caffe Neon etc Conducted a hybrid of Hierarchical and Kmeans Cluster Analysis using IBM SPSS and identified meaningful segments of customers through a discovery approach Develop SparkScala Python R for regular expression regex project in the HadoopHive environment with LinuxWindows for big data resources Used clustering technique KMeans to identify outliers and to classify unlabeled data Evaluate models using Cross Validation Log loss function ROC curves and used AUC for feature selection and elastic technologies like Elastic Search Kibana etc Work with NLTK library to NLP data processing and finding the patterns Categorize comments into positive and negative clusters from different social networking sites using Sentiment Analysis and Text Analytics Analyze traffic patterns by calculating autocorrelation with different time lags Ensure that the model has low False Positive Rate and Text classification and sentiment analysis for unstructured and semistructured data Addressed over fitting by implementing of the algorithm regularization methods like L2 and L1 Use Principal Component Analysis in feature engineering to analyze high dimensional data Create and design reports that will use gathered metrics to infer and draw logical conclusions of past and future behavior Perform Multinomial Logistic Regression Random forest Decision Tree SVM to classify package is going to deliver on time for the new route Implemented different models like Logistic Regression Random Forest and GradientBoost Trees to predict whether a given die will pass or fail the test Perform data analysis by using Hive to retrieve the data from Sql to retrieve datafrom Oracle database and used ETL for data transformation Perform Data Cleaning features scaling features engineering using pandas and numpy packages in python Develop Map Reduce pipeline for feature extraction using Hive and Pig Create Data Quality Scripts using SQL and Hive to validate successful data load and quality of the data Create various types of data visualizations using Python and Tableau Communicate the results with operations team for taking best decisions Collect data needs and requirements by Interacting with the other departments Environment Python 2x R HDFS Hadoop 23 Hive Linux Spark IBM SPSS Tableau Desktop SQL Server 2012 Microsoft Excel Matlab Spark SQL Pyspark Python Developer Rite Aid Richmond VA US September 2012 to March 2015 Responsibilities Worked in comprehending and examining the clients business requirements Used Django frameworks and Python to build dynamic webpages Developed tools for monitoring and notification using Python Enhanced the application by using HTML and Java script for design and development Used data structures like directories tuples Object Oriented class based inheritance features for making complex algorithms of networks Created PHPMySQL backend for data entry from Flash and worked in tandem with the Flash developer to obtain the correct data through query string Involved in designing database Model APIs Views using python to build an interactive web based solution Generated Python Django Forms to record data of online users Implemented Data tables to add delete update and display patient records and policy information using PyQt Implemented a module to connect and view the status of an Apache Cassandra instance using python Developed MVC prototype replacement of current product with Django Improved the Data Security and generated report efficiently by caching and reusing data Created UI using JavaScript and HTML5CSS3 Managed datasets using Panda data frames and MYSQL Queried the database queries using PythonMySQL connector and retrieved information using MySQLdb Recorded the online users data using Python Django forms and implemented test case using Pytest Developed the application using the Testdriven methodology and designed the unit tests using Python Unit test framework Created web application prototype usingJQuery and Angular JS Deployed the project into Heroku using GIT version control system Maintained and Updated the application in accordance to the clienteles requirement Environment Python 3 Django 16 Tableau 82 Beautiful soup HTML5 CSSCSS3 Bootstrap XML JSON JavaScript JQuery Angular JS Backbone JS Restful Web services Apache spark Linux Git Amazon s3 Jenkins MySQL Mongo DB TSQL Eclipse Education Bachelors Skills HTML5 3 years Java 3 years Linux 6 years Python 6 years SQL 6 years Additional Information Technical Skills Languages Java 8 Python R Packages ggplot2 caret dplyr Rweka gmodels RCurl C50 twitter NLP Reshape2 rjson plyr pandas numPy Seaborn sciPy matplot lib scikitlearn Beautiful Soup Rpy2 Web Technologies JDBC HTML5 DHTML and XML CSS3 Web Services WSDL Data Modelling Tools Erwin r 96 95 91 8x Rational Rose ERStudio MS Visio SAP Power designer Big Data Technologies Hadoop Hive HDFS MapReduce Pig Kafka Databases SQL Databases SQLServer My SQL MS Access Reporting Tools MS Office WordExcelPower Point Visio Tableau Crystal reports XI Business Intelligence SSRS Business Objects 5x 6x Cognos7060 ETL Tools Informatica Power Centre SSIS Version Control Tools SVM GitHub BI Tools Tableau Tableau Server Tableau Reader Amazon Redshift Operating System Windows Linux Unix Macintosh HD Red Hat",
    "extracted_keywords": [
        "Sr",
        "Python",
        "Developer",
        "Sr",
        "lPythonspan",
        "span",
        "lDeveloperspan",
        "Sr",
        "Python",
        "Developer",
        "Tiaacref",
        "CaryNC",
        "Work",
        "Experience",
        "Sr",
        "Python",
        "Developer",
        "Tiaacref",
        "Cary",
        "NC",
        "US",
        "August",
        "Present",
        "Responsibilities",
        "Data",
        "Exploration",
        "patterns",
        "features",
        "Python",
        "SciPy",
        "Built",
        "Factor",
        "Analysis",
        "Cluster",
        "Analysis",
        "models",
        "Python",
        "SciPy",
        "customers",
        "target",
        "groups",
        "R",
        "Python",
        "withggplot2",
        "packages",
        "visualization",
        "data",
        "representation",
        "revenue",
        "item",
        "sales",
        "statistics",
        "business",
        "requirements",
        "specifications",
        "project",
        "guidelines",
        "programs",
        "Oozie",
        "workflow",
        "engine",
        "Hive",
        "Pig",
        "jobs",
        "Data",
        "Acquisition",
        "Data",
        "Engineer",
        "team",
        "data",
        "python",
        "Library",
        "Pandas",
        "data",
        "metrics",
        "loading",
        "data",
        "RDBMS",
        "web",
        "logs",
        "SQL",
        "data",
        "Twitter",
        "Java",
        "Twitter",
        "API",
        "twitter",
        "data",
        "database",
        "Amazon",
        "EC2",
        "Cloud",
        "Instances",
        "Amazon",
        "Images",
        "Linux",
        "Ubuntu",
        "Configuring",
        "instances",
        "respect",
        "applications",
        "Hive",
        "analysis",
        "result",
        "Hive",
        "MySQL",
        "Sqoop",
        "data",
        "data",
        "Hive",
        "queries",
        "Pig",
        "scripts",
        "customer",
        "behavior",
        "Created",
        "HBase",
        "data",
        "formats",
        "data",
        "portfolios",
        "performance",
        "Pig",
        "Hive",
        "Queries",
        "reports",
        "dashboards",
        "D3js",
        "Tableau",
        "data",
        "insights",
        "features",
        "models",
        "scores",
        "performance",
        "recommendation",
        "system",
        "business",
        "teams",
        "Utilize",
        "SQL",
        "Excel",
        "MarketingWeb",
        "Analytics",
        "tools",
        "Google",
        "Analytics",
        "AdWords",
        "order",
        "business",
        "marketing",
        "analysis",
        "assessment",
        "Git",
        "version",
        "control",
        "Data",
        "Engineer",
        "team",
        "Data",
        "Scientists",
        "colleagues",
        "methodology",
        "SCRUM",
        "process",
        "project",
        "Environment",
        "Python",
        "Django",
        "HTML5CSS",
        "PostgreSQL",
        "MS",
        "SQL",
        "Server",
        "MySQL",
        "JavaScript",
        "Jupyter",
        "Notebook",
        "VIM",
        "Pycharm",
        "Shell",
        "Scripting",
        "JIRA",
        "Python",
        "Developer",
        "Amneal",
        "Long",
        "Island",
        "April",
        "June",
        "Responsibilities",
        "Perform",
        "Data",
        "Profiling",
        "behavior",
        "features",
        "traffic",
        "pattern",
        "location",
        "Date",
        "Time",
        "data",
        "tables",
        "Hive",
        "queries",
        "data",
        "analysis",
        "statistics",
        "anomalies",
        "duplicates",
        "values",
        "Analyze",
        "Data",
        "Performed",
        "Data",
        "Preparation",
        "model",
        "data",
        "Tablue",
        "Performed",
        "data",
        "cleaning",
        "feature",
        "selection",
        "MLlib",
        "package",
        "PySpark",
        "frameworks",
        "Caffe",
        "Neon",
        "hybrid",
        "Kmeans",
        "Cluster",
        "Analysis",
        "IBM",
        "SPSS",
        "segments",
        "customers",
        "discovery",
        "approach",
        "Develop",
        "SparkScala",
        "Python",
        "R",
        "expression",
        "regex",
        "project",
        "environment",
        "LinuxWindows",
        "data",
        "resources",
        "clustering",
        "technique",
        "KMeans",
        "outliers",
        "data",
        "Evaluate",
        "models",
        "Cross",
        "Validation",
        "Log",
        "loss",
        "function",
        "ROC",
        "curves",
        "AUC",
        "feature",
        "selection",
        "technologies",
        "Elastic",
        "Search",
        "Kibana",
        "Work",
        "NLTK",
        "library",
        "data",
        "processing",
        "patterns",
        "Categorize",
        "comments",
        "clusters",
        "networking",
        "sites",
        "Sentiment",
        "Analysis",
        "Text",
        "Analytics",
        "Analyze",
        "traffic",
        "patterns",
        "autocorrelation",
        "time",
        "lags",
        "model",
        "False",
        "Positive",
        "Rate",
        "Text",
        "classification",
        "sentiment",
        "analysis",
        "data",
        "algorithm",
        "regularization",
        "methods",
        "L2",
        "L1",
        "Use",
        "Principal",
        "Component",
        "Analysis",
        "feature",
        "engineering",
        "data",
        "design",
        "reports",
        "metrics",
        "conclusions",
        "behavior",
        "Perform",
        "Multinomial",
        "Logistic",
        "Regression",
        "Random",
        "forest",
        "Decision",
        "Tree",
        "SVM",
        "package",
        "time",
        "route",
        "models",
        "Logistic",
        "Regression",
        "Random",
        "Forest",
        "GradientBoost",
        "Trees",
        "die",
        "test",
        "Perform",
        "data",
        "analysis",
        "Hive",
        "data",
        "Sql",
        "Oracle",
        "database",
        "ETL",
        "data",
        "transformation",
        "Perform",
        "Data",
        "Cleaning",
        "features",
        "engineering",
        "pandas",
        "packages",
        "python",
        "Develop",
        "Map",
        "Reduce",
        "pipeline",
        "feature",
        "extraction",
        "Hive",
        "Pig",
        "Create",
        "Data",
        "Quality",
        "Scripts",
        "SQL",
        "Hive",
        "data",
        "load",
        "quality",
        "data",
        "types",
        "data",
        "visualizations",
        "Python",
        "Tableau",
        "Communicate",
        "results",
        "operations",
        "team",
        "decisions",
        "data",
        "needs",
        "requirements",
        "departments",
        "Environment",
        "Python",
        "R",
        "HDFS",
        "Hadoop",
        "Hive",
        "Linux",
        "Spark",
        "IBM",
        "SPSS",
        "Tableau",
        "Desktop",
        "SQL",
        "Server",
        "Microsoft",
        "Excel",
        "Matlab",
        "Spark",
        "SQL",
        "Pyspark",
        "Python",
        "Developer",
        "Rite",
        "Aid",
        "Richmond",
        "VA",
        "US",
        "September",
        "March",
        "Responsibilities",
        "clients",
        "business",
        "requirements",
        "Django",
        "frameworks",
        "Python",
        "webpages",
        "tools",
        "monitoring",
        "notification",
        "Python",
        "application",
        "HTML",
        "Java",
        "script",
        "design",
        "development",
        "data",
        "structures",
        "directories",
        "Object",
        "class",
        "inheritance",
        "algorithms",
        "networks",
        "PHPMySQL",
        "backend",
        "data",
        "entry",
        "Flash",
        "tandem",
        "Flash",
        "developer",
        "data",
        "query",
        "string",
        "database",
        "Model",
        "APIs",
        "Views",
        "python",
        "web",
        "solution",
        "Python",
        "Django",
        "Forms",
        "data",
        "users",
        "Data",
        "tables",
        "update",
        "patient",
        "records",
        "policy",
        "information",
        "PyQt",
        "module",
        "status",
        "Apache",
        "Cassandra",
        "instance",
        "python",
        "Developed",
        "MVC",
        "prototype",
        "replacement",
        "product",
        "Django",
        "Data",
        "Security",
        "report",
        "data",
        "UI",
        "JavaScript",
        "HTML5CSS3",
        "Managed",
        "datasets",
        "Panda",
        "data",
        "frames",
        "MYSQL",
        "database",
        "connector",
        "information",
        "users",
        "data",
        "Python",
        "Django",
        "forms",
        "test",
        "case",
        "Pytest",
        "application",
        "Testdriven",
        "methodology",
        "unit",
        "tests",
        "Python",
        "Unit",
        "test",
        "framework",
        "web",
        "application",
        "prototype",
        "usingJQuery",
        "JS",
        "project",
        "Heroku",
        "GIT",
        "version",
        "control",
        "system",
        "application",
        "accordance",
        "clienteles",
        "requirement",
        "Environment",
        "Python",
        "Django",
        "Tableau",
        "Beautiful",
        "soup",
        "HTML5",
        "CSSCSS3",
        "Bootstrap",
        "XML",
        "JSON",
        "JavaScript",
        "JQuery",
        "Angular",
        "JS",
        "Backbone",
        "JS",
        "Restful",
        "Web",
        "services",
        "Apache",
        "spark",
        "Linux",
        "Git",
        "Amazon",
        "s3",
        "Jenkins",
        "MySQL",
        "Mongo",
        "DB",
        "TSQL",
        "Eclipse",
        "Education",
        "Bachelors",
        "Skills",
        "HTML5",
        "years",
        "Java",
        "years",
        "Linux",
        "years",
        "Python",
        "years",
        "SQL",
        "years",
        "Additional",
        "Information",
        "Technical",
        "Skills",
        "Languages",
        "Java",
        "Python",
        "R",
        "Packages",
        "ggplot2",
        "dplyr",
        "Rweka",
        "RCurl",
        "C50",
        "twitter",
        "NLP",
        "Reshape2",
        "rjson",
        "plyr",
        "numPy",
        "Seaborn",
        "sciPy",
        "matplot",
        "lib",
        "Beautiful",
        "Soup",
        "Rpy2",
        "Web",
        "Technologies",
        "JDBC",
        "HTML5",
        "DHTML",
        "XML",
        "CSS3",
        "Web",
        "Services",
        "WSDL",
        "Data",
        "Modelling",
        "Tools",
        "Erwin",
        "r",
        "8x",
        "Rational",
        "Rose",
        "ERStudio",
        "MS",
        "Visio",
        "SAP",
        "Power",
        "designer",
        "Big",
        "Data",
        "Technologies",
        "Hadoop",
        "Hive",
        "HDFS",
        "MapReduce",
        "Pig",
        "Kafka",
        "SQL",
        "Databases",
        "SQLServer",
        "SQL",
        "MS",
        "Access",
        "Reporting",
        "Tools",
        "MS",
        "Office",
        "WordExcelPower",
        "Point",
        "Visio",
        "Tableau",
        "Crystal",
        "XI",
        "Business",
        "Intelligence",
        "SSRS",
        "Business",
        "5x",
        "Cognos7060",
        "ETL",
        "Tools",
        "Informatica",
        "Power",
        "Centre",
        "SSIS",
        "Version",
        "Control",
        "Tools",
        "SVM",
        "GitHub",
        "BI",
        "Tools",
        "Tableau",
        "Tableau",
        "Server",
        "Tableau",
        "Reader",
        "Amazon",
        "Redshift",
        "Operating",
        "System",
        "Linux",
        "Unix",
        "Macintosh",
        "HD",
        "Red",
        "Hat"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T22:51:31.442395",
    "resume_data": "Sr Python Developer Sr span lPythonspan span lDeveloperspan Sr Python Developer Tiaacref CaryNC Work Experience Sr Python Developer Tiaacref Cary NC US August 2017 to Present Responsibilities Implemented Data Exploration to analyze patterns and to select features using Python SciPy Built Factor Analysis and Cluster Analysis models using Python SciPy to classify customers into different target groups Using R and Python withggplot2 packages performed an extensive graphical visualization of overall data including customized graphical representation of revenue reports specific item sales statistics and visualization Evaluated business requirements and prepared detailed specifications that follow project guidelines required to develop written programs Used Oozie workflow engine to run multiple Hive and Pig jobs Participated in Data Acquisition with Data Engineer team to extract historical and realtime data by using python Library Pandas Analyzed the partitioned and bucketed data and compute various metrics for reporting Involved in loading data from RDBMS and web logs into SQL and NOSql Extracted data from Twitter using Java and Twitter API Parsed JSON formatted twitter data and uploaded to database Launching Amazon EC2 Cloud Instances using Amazon Images Linux Ubuntu and Configuring launched instances with respect to specific applications Developed Hive queries for analysis and exported the result set from Hive to MySQL using Sqoop after processing the data Analyzed the data by performing Hive queries and running Pig scripts to study customer behavior Created HBase tables to store various data formats of data coming from different portfolios Worked on improving performance of existing Pig and Hive Queries Created reports and dashboards by using D3js and Tableau 9x to explain and communicate data insights significant features models scores and performance of new recommendation system to both technical and business teams Utilize SQL Excel and several MarketingWeb Analytics tools Google Analytics AdWords in order to complete business marketing analysis and assessment Used Git 2x for version control with Data Engineer team and Data Scientists colleagues Used Agile methodology and SCRUM process for project developing Environment Python 27 Django HTML5CSS PostgreSQL MS SQL Server 2013 MySQL JavaScript Jupyter Notebook VIM Pycharm Shell Scripting AngularJS JIRA Python Developer Amneal Long Island April 2015 to June 2017 Responsibilities Perform Data Profiling to learn about behavior with various features such as traffic pattern location Date and Time etc Extracted the data from hive tables by writing efficient Hive queries Performed preliminary data analysis using descriptive statistics and handled anomalies such as removing duplicates and imputing missing values Analyze Data and Performed Data Preparation by applying historical model on the data set in Tablue Performed data cleaning and feature selection using MLlib package in PySpark and working with deep learning frameworks such as Caffe Neon etc Conducted a hybrid of Hierarchical and Kmeans Cluster Analysis using IBM SPSS and identified meaningful segments of customers through a discovery approach Develop SparkScala Python R for regular expression regex project in the HadoopHive environment with LinuxWindows for big data resources Used clustering technique KMeans to identify outliers and to classify unlabeled data Evaluate models using Cross Validation Log loss function ROC curves and used AUC for feature selection and elastic technologies like Elastic Search Kibana etc Work with NLTK library to NLP data processing and finding the patterns Categorize comments into positive and negative clusters from different social networking sites using Sentiment Analysis and Text Analytics Analyze traffic patterns by calculating autocorrelation with different time lags Ensure that the model has low False Positive Rate and Text classification and sentiment analysis for unstructured and semistructured data Addressed over fitting by implementing of the algorithm regularization methods like L2 and L1 Use Principal Component Analysis in feature engineering to analyze high dimensional data Create and design reports that will use gathered metrics to infer and draw logical conclusions of past and future behavior Perform Multinomial Logistic Regression Random forest Decision Tree SVM to classify package is going to deliver on time for the new route Implemented different models like Logistic Regression Random Forest and GradientBoost Trees to predict whether a given die will pass or fail the test Perform data analysis by using Hive to retrieve the data from Sql to retrieve datafrom Oracle database and used ETL for data transformation Perform Data Cleaning features scaling features engineering using pandas and numpy packages in python Develop Map Reduce pipeline for feature extraction using Hive and Pig Create Data Quality Scripts using SQL and Hive to validate successful data load and quality of the data Create various types of data visualizations using Python and Tableau Communicate the results with operations team for taking best decisions Collect data needs and requirements by Interacting with the other departments Environment Python 2x R HDFS Hadoop 23 Hive Linux Spark IBM SPSS Tableau Desktop SQL Server 2012 Microsoft Excel Matlab Spark SQL Pyspark Python Developer Rite Aid Richmond VA US September 2012 to March 2015 Responsibilities Worked in comprehending and examining the clients business requirements Used Django frameworks and Python to build dynamic webpages Developed tools for monitoring and notification using Python Enhanced the application by using HTML and Java script for design and development Used data structures like directories tuples Object Oriented class based inheritance features for making complex algorithms of networks Created PHPMySQL backend for data entry from Flash and worked in tandem with the Flash developer to obtain the correct data through query string Involved in designing database Model APIs Views using python to build an interactive web based solution Generated Python Django Forms to record data of online users Implemented Data tables to add delete update and display patient records and policy information using PyQt Implemented a module to connect and view the status of an Apache Cassandra instance using python Developed MVC prototype replacement of current product with Django Improved the Data Security and generated report efficiently by caching and reusing data Created UI using JavaScript and HTML5CSS3 Managed datasets using Panda data frames and MYSQL Queried the database queries using PythonMySQL connector and retrieved information using MySQLdb Recorded the online users data using Python Django forms and implemented test case using Pytest Developed the application using the Testdriven methodology and designed the unit tests using Python Unit test framework Created web application prototype usingJQuery and Angular JS Deployed the project into Heroku using GIT version control system Maintained and Updated the application in accordance to the clienteles requirement Environment Python 3 Django 16 Tableau 82 Beautiful soup HTML5 CSSCSS3 Bootstrap XML JSON JavaScript JQuery Angular JS Backbone JS Restful Web services Apache spark Linux Git Amazon s3 Jenkins MySQL Mongo DB TSQL Eclipse Education Bachelors Skills HTML5 3 years Java 3 years Linux 6 years Python 6 years SQL 6 years Additional Information Technical Skills Languages Java 8 Python R Packages ggplot2 caret dplyr Rweka gmodels RCurl C50 twitter NLP Reshape2 rjson plyr pandas numPy Seaborn sciPy matplot lib scikitlearn Beautiful Soup Rpy2 Web Technologies JDBC HTML5 DHTML and XML CSS3 Web Services WSDL Data Modelling Tools Erwin r 96 95 91 8x Rational Rose ERStudio MS Visio SAP Power designer Big Data Technologies Hadoop Hive HDFS MapReduce Pig Kafka Databases SQL Databases SQLServer My SQL MS Access Reporting Tools MS Office WordExcelPower Point Visio Tableau Crystal reports XI Business Intelligence SSRS Business Objects 5x 6x Cognos7060 ETL Tools Informatica Power Centre SSIS Version Control Tools SVM GitHub BI Tools Tableau Tableau Server Tableau Reader Amazon Redshift Operating System Windows Linux Unix Macintosh HD Red Hat",
    "unique_id": "9e5aa57d-34f6-42ab-a980-c57128423b98"
}