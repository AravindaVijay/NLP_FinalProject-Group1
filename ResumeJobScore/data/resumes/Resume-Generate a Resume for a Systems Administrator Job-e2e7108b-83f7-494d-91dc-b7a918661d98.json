{
    "clean_data": "Data ScientistMachine Learning Data ScientistMachine Learning Data ScientistMachine Learning Norcross GA 8years of experience in DataScience and Analytics including MachineLearning DataMining  Over 5Experience with MachineLearning techniques and algorithms such as kNN NaiveBayes etc Experience in AWS AmazonWebServicesEC2 VPC IAM IAM S3 CloudFront CloudWatch CloudFormation Glacier RDSConfig Route53 SNS SQS ElasticCache AzureCloud Extensive full cycle CloudAzure experience with full BigData Elasticsearch and SOLR MachineLearning and DeepLearning development and deployment Experienced with machinelearningalgorithms such as logisticregression randomforest XGboost KNN SVM neuralnetwork linearregression lassoregressionandkmeans Expertise in synthesizing Machinelearning PredictiveAnalytics and BigData technologies like Hadoop Hive Pig Strong skills in statistical methodologies such as ABtest experimentdesign hypothesistest ANOVA Experience in implementing data analysis with various analytic tools such as Anaconda 40R 30 ggplot2 Caret dplyr and Excel Solid ability to write and optimize diverse SQLqueries working knowledge of RDBMS like SQLServer2008 Experience in BigData technologies like Spark16 Sparksql pySpark Hadoop2X HDFS Hive1X Experience in visualization tools likeTableau9X 10X DataBlendingfor creating dashboards Proficient in PredictiveModeling DataMiningMethods FactorAnalysis ANOVA Hypotheticaltesting normal distribution and other advanced statistical and econometric techniques Developed predictive models using DecisionTree RandomForest NaiveBayes LogisticRegression ClusterAnalysis and NeuralNetworks Excellent knowledge of Machine Learning Mathematical Modeling and Operations Research Comfortable with R Python SAS MATLAB Relationaldatabases Deep understanding exposure of BigDataEcosystem Expert in creating PLSQLSchemaobjects like Packages Procedures Functions Subprograms Triggers Views MaterializedViews Indexes Constraints Sequences ExceptionHandling DynamicSQLCursors NativeCompilation CollectionTypes RecordType ObjectType using SQLDeveloper Hands on Experience in implementing ModelViewControl MVC architecture using Spring JDK CoreJava Collections OOPSConcepts JSP Servlets Struts springs Hibernate JDBCand provided ServerAdministrator duties LogicalPosition Worked with complex applications such as R SAS Matlaband SPSS to developeda neuralnetwork cluster analysis Experienced in DataIntegrationValidation and DataQuality controls for ETL process and DataWarehousing using MSVisualStudioSSIS SSAS SSRS Proficient in Tableau and RShiny datavisualization tools to analyze and obtain insights into large datasets create visually powerful and actionable interactive reports and dashboards Automated recurring reports using SQL and R and visualized them on BI platform like Tableau Worked in a development environment like Git and VM Excellent communication skills Successfully working in fastpaced multitasking environment both independently and in a collaborative team a selfmotivated enthusiastic learner Authorized to work in the US for any employer Work Experience Data ScientistMachine Learning FleetCor Technologies Inc Norcross GA May 2017 to May 2017 Responsibilities Analyzed Trading mechanism for realtime transactions and build collateral management tools Compiled data from various sources to perform complex analysis for actionable results Utilized machine learning algorithms such as linear regression multivariate regression naive bayes Random Forests Kmeans KNN for data analysis Measured Efficiency of HadoopHive environment ensuring SLA is met Developed Spark code using Scala and SparkSQLStreaming for faster processing of data Prepared Spark build from the source code and ran the PIG Scripts using Spark rather using MR jobs for better performance Analyzing the system for new enhancementsfunctionalities and perform Impact analysis of the application for implementing ETL changes Imported data using Sqoop to load data from MySQL to HDFS on regular basis Developed Scripts and Batch Job to schedule various Hadoop Program Used TensorFlow to train the model from insightful data and look at thousands of examples Designing developing and optimizing SQL code DDL DML Building performant scalable ETL processes to load cleanse and validate data Expertise in Data archival and Data migration adhoc reporting and code utilizing SAS on UNIX and Windows Environments Tested and debugged SAS programs against the test data Processed the data in SAS for the given requirement using SAS programming concepts Imported and Exported data files to and from SAS using Proc Import and Proc Export from Excel and various delimited textbased data files such as TXT tab delimited and CSV comma delimited files into SAS datasets for analysis Expertise in producing RTF PDF HTML files using SAS ODS facility Providing support for data processes This will involve monitoring data profiling database usage trouble shooting tuning and ensuring data integrity Participating in the full software development lifecycle with requirements solution design development QA implementation and product support using Scrum and other Agile methodologies Collaborate with team members and stakeholders in design and development of data environment Learning new tools and skillsets as needs arise Preparing associated documentation for specifications requirements and testing Optimizing the Tensorflow Model for an efficiency Used Tensorflow for text summarization Used Spark API over Cloudera Hadoop YARN to perform analytics on data in Hive Wrote Hive queries for data analysis to meet the business requirements Developed Kafka producer and consumers for message handling Responsible for analyzing multiplatform applications using python Used storm for an automatic mechanism to analyze large amounts of nonunique data points with low latency and high throughput Developed MapReduce jobs in Python for data cleaning and data processing Environment Machine learning AWS MS Azure Cassandra SAS Spark HDFS Hive Pig Linux Anaconda Python MySQL Eclipse PLSQL SQL connector SparkML Data ScientistMachine Learning Ford Motor Dearborn MI December 2015 to April 2017 Responsibilities Provided the architectural leadership in shaping strategic business technology projects with an emphasis on application architecture Utilized domain knowledge and application portfolio knowledge to play a key role in defining the future state of large business technology programs Participated in all phases of datamining datacollection datacleaning developingmodels validation and visualization and performed Gapanalysis Developed MapReduceSpark R modules for machine learning predictive analytics in Hadoop on AWS Implemented a Rbased distributed randomforest Utilized Spark Scala Hadoop SparkStreaming MLLib Rabroadvariety of machinelearning methods includingclassifications regressions dimensionallyreduction etc and utilized the engine to increase user lifetime by 45 and triple user conversations for target categories Well versed with CloudIaaS and PaaS implementations in both private and publicclouds like VMware Openstack AmazonAWS and Cloudfoundry Pivotal and HPStackato Used R and h2oai libraries for developing various machinelearningalgorithms and utilizedmachinelearningalgorithms such as linearregression multivariateregression naiveBayes RandomForests Kmeans KNN for dataanalysis Worked on databasedesign relationalintegrityconstraints OLAP OLTP Cubes andNormalization 3N0F and DeNormalization of the database Worked on customersegmentation using an unsupervised learning techniqueclustering Utilized Spark Scala Hadoop HBase Kafka SparkStreaming MLlib Python a broad variety of machinelearningmethods including classifications regressions dimensionallyreduction etc Data analysis reporting using TableauPerform numerous data pulling requests using SQLserver2012 Designed and implemented system architecture for AmazonEC2 based cloudhosted solution for the client Tested Complex ETLMappings and Sessions based on business user requirements and business rules to load data from source flat files and RDBMS tables to target tables Hands on experience in Hadoopecosystem with componentsHadoopMapReduce HDFS Oozie HiveQL Sqoop HBase MongoDB Zookeeper Pig andFlume Environment R SQL Oracle 12c Netezza SQL Server Informatica Java SSRS PLSQL TSQL Tableau MLlib regression Cluster analysis Scala NLP Spark Kafka MongoDB logistic regression Hadoop Hive Teradata random forest OLAP Azure MariaDB SAP CRM HDFS ODS NLTK SVM JSON Tableau XML Cassandra MapReduce AWS Data ScientistR Developer LOral Shanghai CN June 2014 to November 2015 Responsibilities The conducted analysis in assessing customer consuming behaviors and discover the value of customers with RMFanalysis applied customer segmentation with clustering algorithms such as KMeansClustering and HierarchicalClustering Collaborated with data engineers to implement the ETLprocess wrote and optimized SQLqueries to perform dataextraction and merging from Oracle Used Rand Spark to develop a variety of models and algorithms for analytic purposes Performed dataintegritychecks datacleaning exploratory analysis and feature engineer using R Developed personalized product recommendation with Machinelearningalgorithms including GradientBoostingTree and Collaborative filtering to better meet the needs of existing customers and acquire new customers Used R and Spark to implement different machinelearningalgorithms including GeneralizedLinearModel RandomForest SVM Boosting and NeuralNetwork Evaluated parameters with KFoldCrossValidation and optimized performance of models A highly immersive Data Science program involving DataManipulation and Visualization WebScraping MachineLearning GIT SQL UNIXCommands Rprogramming NoSQL Identified risk level and eligibility of new insurance applicants with MachineLearning algorithms Utilized SQL and HiveQL to query manipulate data from variety data sources including Oracle and HDFS while maintaining data integrity Performed datavisualization and Designeddashboards with Tableau and D3js and provided complexreports includingcharts summaries and graphs to interpret the findings to the team and stakeholders Environment R MATLAB MongoDB exploratory analysis feature engineering KMeans Clustering Hierarchical Clustering Machine Learning Python Spark MLlib PySpark Tableau Micro Strategy SAS Tensor Flow regression logistic regression Hadoop 27 OLTP random forest OLAP HDFS ODS NLTK SVM JSON XML and MapReduce Data Analyst C Client Chicago IL June 2012 to August 2012 Responsibilities Participated in JAD sessions gathered information from BusinessAnalysts end users and other stakeholders to determine the requirements Hands on Experience in CloudComputing such asAWSstorage Compute DatabasesSQL Designed the DataWarehouse and MDMhubConceptual Logical and Physicaldatamodels Performed DailyMonitoring of Oracle instances using OracleEnterpriseManager ADDM TOAD monitorusers tablespaces memorystructures rollbacksegments logs andalerts Used Normalization methods up to 3NF and Denormalization techniques for effective performance in OLTP and OLAP systems Generated DDL scripts using ForwardEngineering technique to create objects and deploy them into the databases Worked on database testing writing complex SQLqueries to verify the transactions and business logic like identifying the duplicate rows by using SQLDeveloper and PLSQL Developer Used TeradataSQLAssistant TeradataAdministrator PMON and data loadexport utilities like BTEQ FastLoad MultiLoad FastExport TPump on UNIXWindows environments and running the batch process for Teradata Developed SQLQueries to fetch complex data from different tables in remote databases using joins database links and Bulkcollects The migrated database from legacy systems SQL server to Oracle and Netezza Used SSIS to create ETL packages to validate extract transform and load data to pull data from Source servers to staging database Worked on SQLServerconceptsSSIS SQLServerIntegrationServices SSAS SQLServerAnalysisServices and SSRS SQLServerReportingServices Python Developer Aspect Software July 2010 to May 2011 Environment ER Studio  SQL PLSQL BTEQ DB2 Oracle MDM Netezza ETL RTF UNIX SQL Server2010 Informatica SSRS SSIS SSAS SAS Aginity Client Aspect Software INDIA July 2010 May 2011 July 2012 Role Python Developer Responsibilities Worked on the project from gathering requirements to developing the entire application Worked on Anaconda Python Environment Created activated and programmed in Anaconda environment Wrote programs for performance calculations using NumPy and SQLAlchemy Wrote python routines to log into the websites and fetch data for selected options Used python modules of urllib urllib2 Requests for web crawling Experience using all these ML techniques clustering regression classification graphical models Extensive experience in Text Analytics developing different Statistical Machine Learning Data mining solutions to various business problems and generating data visualizations using R Python and Tableau Used with other packages such as Beautiful Soup for data parsing Involved in development of Web Services using SOAP for sending and getting data from the external interface in the XML format Used with other packages such as Beautiful Soup for data parsing Worked on development of SQL and stored procedures on MYSQL Analyzed the code completely and have reduced the code redundancy to the optimal level Design and build a text classification application using different text classification models Used Jira for defect tracking and project management Worked on writing and as well as read data from CSV and excel file formats Involved in Sprint planning sessions and participated in the daily Agile SCRUM meetings Conducted every day scrum as part of the SCRUM Master role Developed the project in Linux environment Worked on resulting reports of the application Performed QA testing on the application Held meetings with client and worked for the entire project with limited help from the client Environment Python Anaconda Sypder IDE Windows 7 Teradata Requests urllib urllib2 Beautiful Soup Tableau python libraries such as NumPy SQL Alchemy MySQLdb Education Bachelors Skills APACHE HADOOP HDFS 3 years Oracle 3 years python 3 years SQL 4 years XML 3 years Additional Information Skills Big DataHadoop Technologies Hadoop HDFS YARN MapReduce Hive Pig Impala Sqoop Flume Spark Kafka Zookeeper and Oozie Languages C C HTML5 DHTML WSDL css3 XML RR Studio SAS Enterprise Guide SAS R R Caret Weka ggplot Python NumPy SciPy Pandas SQL PLSQL Pig Latin HiveQL Shell Scripting Cloud Computing Tools Amazon AWS Azure Databases Microsoft SQL Server 2008 MySQL 4x5x Oracle 10g 11g 12c DB2 Teradata Netezza NO SQL Databases HBase Cassandra MongoDB MariaDB Build Tools Maven ANT Toad SQL Loader RTC RSA ControlM Oozie Hue SOAP UI Development Tools Microsoft SQL Studio Eclipse NetBeans IntelliJ Development Methodologies AgileScrum Waterfall UML Design Patterns Version Control Tools and Testing API Git SVM GitHub SVN and JUNIT ETL Tools Informatica Power Centre SSIS Reporting Tools MS Office  VisioOutlook Crystal Reports XI SSRS Cognos7060 Operating Systems All versions of UNIX Windows LINUX Macintosh HD Sun Solaris",
    "entities": [
        "BTEQ FastLoad MultiLoad",
        "SQLqueries",
        "Machine Learning Mathematical Modeling and Operations Research Comfortable",
        "RMFanalysis",
        "Informatica",
        "BI",
        "TableauPerform",
        "UNIX",
        "ForwardEngineering",
        "KMeans Clustering Hierarchical Clustering Machine Learning Python Spark MLlib PySpark Tableau Micro Strategy",
        "HierarchicalClustering Collaborated",
        "Data Science",
        "Beautiful Soup",
        "TensorFlow",
        "Spring JDK CoreJava Collections OOPSConcepts JSP Servlets Struts",
        "Hadoop",
        "HPStackato",
        "SOAP",
        "XML",
        "Gapanalysis Developed",
        "ETLprocess",
        "MapReduce Data Analyst C",
        "JAD",
        "Designeddashboards",
        "BusinessAnalysts",
        "SPSS",
        "NeuralNetwork Evaluated",
        "DataWarehouse",
        "Oracle and HDFS",
        "Sparksql",
        "MaterializedViews Indexes Constraints Sequences ExceptionHandling",
        "PredictiveModeling DataMiningMethods FactorAnalysis ANOVA Hypotheticaltesting",
        "SSIS",
        "Cloudera Hadoop",
        "Utilized Spark Scala Hadoop SparkStreaming MLLib Rabroadvariety",
        "BigData Elasticsearch",
        "Informatica Power Centre SSIS Reporting Tools MS Office",
        "ODS NLTK",
        "Hadoop Program",
        "VM Excellent",
        "Utilized",
        "Tableau Worked",
        "Hive Wrote Hive",
        "DDL",
        "SQLDeveloper Hands on Experience",
        "Processed",
        "PDF HTML",
        "Control Tools",
        "Linux",
        "DataManipulation",
        "Oozie HiveQL Sqoop HBase",
        "SOLR MachineLearning",
        "RTC",
        "Cloudfoundry Pivotal",
        "Utilized Spark",
        "BigData",
        "ANOVA Experience",
        "MVC",
        "Utilized SQL",
        "Optimizing the Tensorflow Model",
        "Spark",
        "Measured Efficiency of HadoopHive",
        "Oracle Used Rand Spark",
        "linear",
        "CSV",
        "US",
        "Sqoop",
        "QA",
        "Worked on Anaconda Python Environment Created",
        "KNN",
        "Data ScientistMachine Learning Data ScientistMachine Learning Data ScientistMachine Learning Norcross",
        "Proc Import",
        "AWS",
        "UNIXWindows",
        "MR",
        "Text Analytics",
        "Data ScientistMachine Learning Ford Motor Dearborn",
        "SAS",
        "SSRS",
        "GeneralizedLinearModel RandomForest SVM Boosting",
        "SQL",
        "OracleEnterpriseManager ADDM",
        "OLTP",
        "DataQuality",
        "Oozie Languages",
        "Chicago",
        "ServerAdministrator",
        "Anaconda",
        "Macintosh",
        "JUNIT",
        "Sun Solaris",
        "Statistical Machine Learning Data",
        "ETL",
        "Learning",
        "Performed",
        "OLAP",
        "Impala",
        "DataScience",
        "Studio SAS Enterprise Guide",
        "DecisionTree RandomForest NaiveBayes LogisticRegression ClusterAnalysis",
        "Microsoft",
        "ML",
        "Work Experience Data ScientistMachine Learning FleetCor Technologies Inc Norcross",
        "NumPy SQL Alchemy MySQLdb",
        "Machinelearning PredictiveAnalytics",
        "Data",
        "SQLDeveloper",
        "NetBeans",
        "Shanghai",
        "Tableau",
        "NeuralNetworks",
        "SAP CRM",
        "Sprint",
        "RTF",
        "Denormalization",
        "SLA",
        "CloudComputing",
        "TXT"
    ],
    "experience": "Experience in AWS AmazonWebServicesEC2 VPC IAM IAM S3 CloudFront CloudWatch CloudFormation Glacier RDSConfig Route53 SNS SQS ElasticCache AzureCloud Extensive full cycle CloudAzure experience with full BigData Elasticsearch and SOLR MachineLearning and DeepLearning development and deployment Experienced with machinelearningalgorithms such as logisticregression randomforest XGboost KNN SVM neuralnetwork linearregression lassoregressionandkmeans Expertise in synthesizing Machinelearning PredictiveAnalytics and BigData technologies like Hadoop Hive Pig Strong skills in statistical methodologies such as ABtest experimentdesign hypothesistest ANOVA Experience in implementing data analysis with various analytic tools such as Anaconda 40R 30 ggplot2 Caret dplyr and Excel Solid ability to write and optimize diverse SQLqueries working knowledge of RDBMS like SQLServer2008 Experience in BigData technologies like Spark16 Sparksql pySpark Hadoop2X HDFS Hive1X Experience in visualization tools likeTableau9X 10X DataBlendingfor creating dashboards Proficient in PredictiveModeling DataMiningMethods FactorAnalysis ANOVA Hypotheticaltesting normal distribution and other advanced statistical and econometric techniques Developed predictive models using DecisionTree RandomForest NaiveBayes LogisticRegression ClusterAnalysis and NeuralNetworks Excellent knowledge of Machine Learning Mathematical Modeling and Operations Research Comfortable with R Python SAS MATLAB Relationaldatabases Deep understanding exposure of BigDataEcosystem Expert in creating PLSQLSchemaobjects like Packages Procedures Functions Subprograms Triggers Views MaterializedViews Indexes Constraints Sequences ExceptionHandling DynamicSQLCursors NativeCompilation CollectionTypes RecordType ObjectType using SQLDeveloper Hands on Experience in implementing ModelViewControl MVC architecture using Spring JDK CoreJava Collections OOPSConcepts JSP Servlets Struts springs Hibernate JDBCand provided ServerAdministrator duties LogicalPosition Worked with complex applications such as R SAS Matlaband SPSS to developeda neuralnetwork cluster analysis Experienced in DataIntegrationValidation and DataQuality controls for ETL process and DataWarehousing using MSVisualStudioSSIS SSAS SSRS Proficient in Tableau and RShiny datavisualization tools to analyze and obtain insights into large datasets create visually powerful and actionable interactive reports and dashboards Automated recurring reports using SQL and R and visualized them on BI platform like Tableau Worked in a development environment like Git and VM Excellent communication skills Successfully working in fastpaced multitasking environment both independently and in a collaborative team a selfmotivated enthusiastic learner Authorized to work in the US for any employer Work Experience Data ScientistMachine Learning FleetCor Technologies Inc Norcross GA May 2017 to May 2017 Responsibilities Analyzed Trading mechanism for realtime transactions and build collateral management tools Compiled data from various sources to perform complex analysis for actionable results Utilized machine learning algorithms such as linear regression multivariate regression naive bayes Random Forests Kmeans KNN for data analysis Measured Efficiency of HadoopHive environment ensuring SLA is met Developed Spark code using Scala and SparkSQLStreaming for faster processing of data Prepared Spark build from the source code and ran the PIG Scripts using Spark rather using MR jobs for better performance Analyzing the system for new enhancementsfunctionalities and perform Impact analysis of the application for implementing ETL changes Imported data using Sqoop to load data from MySQL to HDFS on regular basis Developed Scripts and Batch Job to schedule various Hadoop Program Used TensorFlow to train the model from insightful data and look at thousands of examples Designing developing and optimizing SQL code DDL DML Building performant scalable ETL processes to load cleanse and validate data Expertise in Data archival and Data migration adhoc reporting and code utilizing SAS on UNIX and Windows Environments Tested and debugged SAS programs against the test data Processed the data in SAS for the given requirement using SAS programming concepts Imported and Exported data files to and from SAS using Proc Import and Proc Export from Excel and various delimited textbased data files such as TXT tab delimited and CSV comma delimited files into SAS datasets for analysis Expertise in producing RTF PDF HTML files using SAS ODS facility Providing support for data processes This will involve monitoring data profiling database usage trouble shooting tuning and ensuring data integrity Participating in the full software development lifecycle with requirements solution design development QA implementation and product support using Scrum and other Agile methodologies Collaborate with team members and stakeholders in design and development of data environment Learning new tools and skillsets as needs arise Preparing associated documentation for specifications requirements and testing Optimizing the Tensorflow Model for an efficiency Used Tensorflow for text summarization Used Spark API over Cloudera Hadoop YARN to perform analytics on data in Hive Wrote Hive queries for data analysis to meet the business requirements Developed Kafka producer and consumers for message handling Responsible for analyzing multiplatform applications using python Used storm for an automatic mechanism to analyze large amounts of nonunique data points with low latency and high throughput Developed MapReduce jobs in Python for data cleaning and data processing Environment Machine learning AWS MS Azure Cassandra SAS Spark HDFS Hive Pig Linux Anaconda Python MySQL Eclipse PLSQL SQL connector SparkML Data ScientistMachine Learning Ford Motor Dearborn MI December 2015 to April 2017 Responsibilities Provided the architectural leadership in shaping strategic business technology projects with an emphasis on application architecture Utilized domain knowledge and application portfolio knowledge to play a key role in defining the future state of large business technology programs Participated in all phases of datamining datacollection datacleaning developingmodels validation and visualization and performed Gapanalysis Developed MapReduceSpark R modules for machine learning predictive analytics in Hadoop on AWS Implemented a Rbased distributed randomforest Utilized Spark Scala Hadoop SparkStreaming MLLib Rabroadvariety of machinelearning methods includingclassifications regressions dimensionallyreduction etc and utilized the engine to increase user lifetime by 45 and triple user conversations for target categories Well versed with CloudIaaS and PaaS implementations in both private and publicclouds like VMware Openstack AmazonAWS and Cloudfoundry Pivotal and HPStackato Used R and h2oai libraries for developing various machinelearningalgorithms and utilizedmachinelearningalgorithms such as linearregression multivariateregression naiveBayes RandomForests Kmeans KNN for dataanalysis Worked on databasedesign relationalintegrityconstraints OLAP OLTP Cubes andNormalization 3N0F and DeNormalization of the database Worked on customersegmentation using an unsupervised learning techniqueclustering Utilized Spark Scala Hadoop HBase Kafka SparkStreaming MLlib Python a broad variety of machinelearningmethods including classifications regressions dimensionallyreduction etc Data analysis reporting using TableauPerform numerous data pulling requests using SQLserver2012 Designed and implemented system architecture for AmazonEC2 based cloudhosted solution for the client Tested Complex ETLMappings and Sessions based on business user requirements and business rules to load data from source flat files and RDBMS tables to target tables Hands on experience in Hadoopecosystem with componentsHadoopMapReduce HDFS Oozie HiveQL Sqoop HBase MongoDB Zookeeper Pig andFlume Environment R SQL Oracle 12c Netezza SQL Server Informatica Java SSRS PLSQL TSQL Tableau MLlib regression Cluster analysis Scala NLP Spark Kafka MongoDB logistic regression Hadoop Hive Teradata random forest OLAP Azure MariaDB SAP CRM HDFS ODS NLTK SVM JSON Tableau XML Cassandra MapReduce AWS Data ScientistR Developer LOral Shanghai CN June 2014 to November 2015 Responsibilities The conducted analysis in assessing customer consuming behaviors and discover the value of customers with RMFanalysis applied customer segmentation with clustering algorithms such as KMeansClustering and HierarchicalClustering Collaborated with data engineers to implement the ETLprocess wrote and optimized SQLqueries to perform dataextraction and merging from Oracle Used Rand Spark to develop a variety of models and algorithms for analytic purposes Performed dataintegritychecks datacleaning exploratory analysis and feature engineer using R Developed personalized product recommendation with Machinelearningalgorithms including GradientBoostingTree and Collaborative filtering to better meet the needs of existing customers and acquire new customers Used R and Spark to implement different machinelearningalgorithms including GeneralizedLinearModel RandomForest SVM Boosting and NeuralNetwork Evaluated parameters with KFoldCrossValidation and optimized performance of models A highly immersive Data Science program involving DataManipulation and Visualization WebScraping MachineLearning GIT SQL UNIXCommands Rprogramming NoSQL Identified risk level and eligibility of new insurance applicants with MachineLearning algorithms Utilized SQL and HiveQL to query manipulate data from variety data sources including Oracle and HDFS while maintaining data integrity Performed datavisualization and Designeddashboards with Tableau and D3js and provided complexreports includingcharts summaries and graphs to interpret the findings to the team and stakeholders Environment R MATLAB MongoDB exploratory analysis feature engineering KMeans Clustering Hierarchical Clustering Machine Learning Python Spark MLlib PySpark Tableau Micro Strategy SAS Tensor Flow regression logistic regression Hadoop 27 OLTP random forest OLAP HDFS ODS NLTK SVM JSON XML and MapReduce Data Analyst C Client Chicago IL June 2012 to August 2012 Responsibilities Participated in JAD sessions gathered information from BusinessAnalysts end users and other stakeholders to determine the requirements Hands on Experience in CloudComputing such asAWSstorage Compute DatabasesSQL Designed the DataWarehouse and MDMhubConceptual Logical and Physicaldatamodels Performed DailyMonitoring of Oracle instances using OracleEnterpriseManager ADDM TOAD monitorusers tablespaces memorystructures rollbacksegments logs andalerts Used Normalization methods up to 3NF and Denormalization techniques for effective performance in OLTP and OLAP systems Generated DDL scripts using ForwardEngineering technique to create objects and deploy them into the databases Worked on database testing writing complex SQLqueries to verify the transactions and business logic like identifying the duplicate rows by using SQLDeveloper and PLSQL Developer Used TeradataSQLAssistant TeradataAdministrator PMON and data loadexport utilities like BTEQ FastLoad MultiLoad FastExport TPump on UNIXWindows environments and running the batch process for Teradata Developed SQLQueries to fetch complex data from different tables in remote databases using joins database links and Bulkcollects The migrated database from legacy systems SQL server to Oracle and Netezza Used SSIS to create ETL packages to validate extract transform and load data to pull data from Source servers to staging database Worked on SQLServerconceptsSSIS SQLServerIntegrationServices SSAS SQLServerAnalysisServices and SSRS SQLServerReportingServices Python Developer Aspect Software July 2010 to May 2011 Environment ER Studio   SQL PLSQL BTEQ DB2 Oracle MDM Netezza ETL RTF UNIX SQL Server2010 Informatica SSRS SSIS SSAS SAS Aginity Client Aspect Software INDIA July 2010 May 2011 July 2012 Role Python Developer Responsibilities Worked on the project from gathering requirements to developing the entire application Worked on Anaconda Python Environment Created activated and programmed in Anaconda environment Wrote programs for performance calculations using NumPy and SQLAlchemy Wrote python routines to log into the websites and fetch data for selected options Used python modules of urllib urllib2 Requests for web crawling Experience using all these ML techniques clustering regression classification graphical models Extensive experience in Text Analytics developing different Statistical Machine Learning Data mining solutions to various business problems and generating data visualizations using R Python and Tableau Used with other packages such as Beautiful Soup for data parsing Involved in development of Web Services using SOAP for sending and getting data from the external interface in the XML format Used with other packages such as Beautiful Soup for data parsing Worked on development of SQL and stored procedures on MYSQL Analyzed the code completely and have reduced the code redundancy to the optimal level Design and build a text classification application using different text classification models Used Jira for defect tracking and project management Worked on writing and as well as read data from CSV and excel file formats Involved in Sprint planning sessions and participated in the daily Agile SCRUM meetings Conducted every day scrum as part of the SCRUM Master role Developed the project in Linux environment Worked on resulting reports of the application Performed QA testing on the application Held meetings with client and worked for the entire project with limited help from the client Environment Python Anaconda Sypder IDE Windows 7 Teradata Requests urllib urllib2 Beautiful Soup Tableau python libraries such as NumPy SQL Alchemy MySQLdb Education Bachelors Skills APACHE HADOOP HDFS 3 years Oracle 3 years python 3 years SQL 4 years XML 3 years Additional Information Skills Big DataHadoop Technologies Hadoop HDFS YARN MapReduce Hive Pig Impala Sqoop Flume Spark Kafka Zookeeper and Oozie Languages C C HTML5 DHTML WSDL css3 XML RR Studio SAS Enterprise Guide SAS R R Caret Weka ggplot Python NumPy SciPy Pandas SQL PLSQL Pig Latin HiveQL Shell Scripting Cloud Computing Tools Amazon AWS Azure Databases Microsoft SQL Server 2008 MySQL 4x5x Oracle 10 g 11 g 12c DB2 Teradata Netezza NO SQL Databases HBase Cassandra MongoDB MariaDB Build Tools Maven ANT Toad SQL Loader RTC RSA ControlM Oozie Hue SOAP UI Development Tools Microsoft SQL Studio Eclipse NetBeans IntelliJ Development Methodologies AgileScrum Waterfall UML Design Patterns Version Control Tools and Testing API Git SVM GitHub SVN and JUNIT ETL Tools Informatica Power Centre SSIS Reporting Tools MS Office   VisioOutlook Crystal Reports XI SSRS Cognos7060 Operating Systems All versions of UNIX Windows LINUX Macintosh HD Sun Solaris",
    "extracted_keywords": [
        "Data",
        "ScientistMachine",
        "Learning",
        "Data",
        "ScientistMachine",
        "Learning",
        "Data",
        "ScientistMachine",
        "Learning",
        "Norcross",
        "GA",
        "8years",
        "experience",
        "DataScience",
        "Analytics",
        "MachineLearning",
        "DataMining",
        "5Experience",
        "MachineLearning",
        "techniques",
        "algorithms",
        "NaiveBayes",
        "Experience",
        "AWS",
        "VPC",
        "IAM",
        "IAM",
        "S3",
        "CloudFront",
        "CloudWatch",
        "CloudFormation",
        "Glacier",
        "RDSConfig",
        "Route53",
        "SNS",
        "SQS",
        "ElasticCache",
        "AzureCloud",
        "cycle",
        "CloudAzure",
        "experience",
        "BigData",
        "Elasticsearch",
        "SOLR",
        "MachineLearning",
        "DeepLearning",
        "development",
        "deployment",
        "machinelearningalgorithms",
        "logisticregression",
        "XGboost",
        "KNN",
        "SVM",
        "neuralnetwork",
        "linearregression",
        "lassoregressionandkmeans",
        "Expertise",
        "Machinelearning",
        "PredictiveAnalytics",
        "BigData",
        "technologies",
        "Hadoop",
        "Hive",
        "Pig",
        "Strong",
        "skills",
        "methodologies",
        "ABtest",
        "ANOVA",
        "Experience",
        "data",
        "analysis",
        "tools",
        "Anaconda",
        "ggplot2",
        "Caret",
        "dplyr",
        "Excel",
        "ability",
        "SQLqueries",
        "knowledge",
        "SQLServer2008",
        "Experience",
        "BigData",
        "technologies",
        "Spark16",
        "Sparksql",
        "Hadoop2X",
        "HDFS",
        "Hive1X",
        "Experience",
        "visualization",
        "tools",
        "likeTableau9X",
        "10X",
        "DataBlendingfor",
        "dashboards",
        "Proficient",
        "PredictiveModeling",
        "DataMiningMethods",
        "FactorAnalysis",
        "ANOVA",
        "distribution",
        "techniques",
        "models",
        "DecisionTree",
        "RandomForest",
        "NaiveBayes",
        "LogisticRegression",
        "ClusterAnalysis",
        "NeuralNetworks",
        "Excellent",
        "knowledge",
        "Machine",
        "Learning",
        "Mathematical",
        "Modeling",
        "Operations",
        "Research",
        "Comfortable",
        "R",
        "Python",
        "SAS",
        "MATLAB",
        "Relationaldatabases",
        "Deep",
        "understanding",
        "exposure",
        "BigDataEcosystem",
        "Expert",
        "PLSQLSchemaobjects",
        "Packages",
        "Procedures",
        "Functions",
        "Subprograms",
        "Triggers",
        "Views",
        "MaterializedViews",
        "Indexes",
        "Constraints",
        "Sequences",
        "ExceptionHandling",
        "DynamicSQLCursors",
        "NativeCompilation",
        "CollectionTypes",
        "RecordType",
        "ObjectType",
        "SQLDeveloper",
        "Hands",
        "Experience",
        "ModelViewControl",
        "MVC",
        "architecture",
        "Spring",
        "JDK",
        "CoreJava",
        "Collections",
        "JSP",
        "Servlets",
        "Struts",
        "springs",
        "Hibernate",
        "JDBCand",
        "ServerAdministrator",
        "duties",
        "LogicalPosition",
        "applications",
        "R",
        "SAS",
        "Matlaband",
        "SPSS",
        "developeda",
        "neuralnetwork",
        "cluster",
        "analysis",
        "DataIntegrationValidation",
        "DataQuality",
        "ETL",
        "process",
        "DataWarehousing",
        "MSVisualStudioSSIS",
        "SSAS",
        "SSRS",
        "Proficient",
        "Tableau",
        "RShiny",
        "datavisualization",
        "tools",
        "insights",
        "datasets",
        "reports",
        "dashboards",
        "reports",
        "SQL",
        "R",
        "BI",
        "platform",
        "Tableau",
        "development",
        "environment",
        "Git",
        "VM",
        "Excellent",
        "communication",
        "skills",
        "environment",
        "team",
        "learner",
        "US",
        "employer",
        "Work",
        "Experience",
        "Data",
        "ScientistMachine",
        "Learning",
        "FleetCor",
        "Technologies",
        "Inc",
        "Norcross",
        "GA",
        "May",
        "May",
        "Responsibilities",
        "Trading",
        "mechanism",
        "transactions",
        "management",
        "tools",
        "data",
        "sources",
        "analysis",
        "results",
        "machine",
        "learning",
        "algorithms",
        "linear",
        "regression",
        "multivariate",
        "regression",
        "bayes",
        "Random",
        "Forests",
        "Kmeans",
        "KNN",
        "data",
        "analysis",
        "Efficiency",
        "HadoopHive",
        "environment",
        "SLA",
        "Developed",
        "Spark",
        "code",
        "Scala",
        "SparkSQLStreaming",
        "processing",
        "data",
        "Prepared",
        "Spark",
        "source",
        "code",
        "PIG",
        "Scripts",
        "Spark",
        "MR",
        "jobs",
        "performance",
        "system",
        "enhancementsfunctionalities",
        "Impact",
        "analysis",
        "application",
        "ETL",
        "changes",
        "data",
        "Sqoop",
        "data",
        "MySQL",
        "HDFS",
        "basis",
        "Scripts",
        "Batch",
        "Job",
        "Hadoop",
        "Program",
        "TensorFlow",
        "model",
        "data",
        "thousands",
        "examples",
        "SQL",
        "code",
        "DDL",
        "DML",
        "ETL",
        "processes",
        "cleanse",
        "data",
        "Expertise",
        "Data",
        "archival",
        "Data",
        "migration",
        "adhoc",
        "reporting",
        "code",
        "SAS",
        "UNIX",
        "Windows",
        "Environments",
        "SAS",
        "programs",
        "test",
        "data",
        "data",
        "SAS",
        "requirement",
        "SAS",
        "programming",
        "concepts",
        "data",
        "files",
        "SAS",
        "Proc",
        "Import",
        "Proc",
        "Export",
        "Excel",
        "data",
        "files",
        "TXT",
        "tab",
        "CSV",
        "comma",
        "files",
        "SAS",
        "datasets",
        "analysis",
        "Expertise",
        "RTF",
        "PDF",
        "HTML",
        "files",
        "SAS",
        "ODS",
        "facility",
        "support",
        "data",
        "processes",
        "data",
        "profiling",
        "database",
        "usage",
        "trouble",
        "data",
        "integrity",
        "software",
        "development",
        "lifecycle",
        "requirements",
        "solution",
        "design",
        "development",
        "QA",
        "implementation",
        "product",
        "support",
        "Scrum",
        "methodologies",
        "team",
        "members",
        "stakeholders",
        "design",
        "development",
        "data",
        "environment",
        "tools",
        "skillsets",
        "needs",
        "documentation",
        "specifications",
        "requirements",
        "testing",
        "Tensorflow",
        "Model",
        "efficiency",
        "Tensorflow",
        "text",
        "summarization",
        "Spark",
        "API",
        "Cloudera",
        "Hadoop",
        "YARN",
        "analytics",
        "data",
        "Hive",
        "Wrote",
        "Hive",
        "data",
        "analysis",
        "business",
        "requirements",
        "Kafka",
        "producer",
        "consumers",
        "message",
        "multiplatform",
        "applications",
        "python",
        "storm",
        "mechanism",
        "amounts",
        "data",
        "points",
        "latency",
        "throughput",
        "Developed",
        "MapReduce",
        "jobs",
        "Python",
        "data",
        "cleaning",
        "data",
        "Environment",
        "Machine",
        "AWS",
        "MS",
        "Azure",
        "Cassandra",
        "SAS",
        "Spark",
        "HDFS",
        "Hive",
        "Pig",
        "Linux",
        "Anaconda",
        "Python",
        "MySQL",
        "Eclipse",
        "PLSQL",
        "SQL",
        "connector",
        "SparkML",
        "Data",
        "ScientistMachine",
        "Learning",
        "Ford",
        "Motor",
        "Dearborn",
        "MI",
        "December",
        "April",
        "Responsibilities",
        "leadership",
        "business",
        "technology",
        "projects",
        "emphasis",
        "application",
        "architecture",
        "domain",
        "knowledge",
        "application",
        "portfolio",
        "knowledge",
        "role",
        "state",
        "business",
        "technology",
        "programs",
        "phases",
        "datacollection",
        "datacleaning",
        "validation",
        "visualization",
        "Gapanalysis",
        "Developed",
        "MapReduceSpark",
        "R",
        "modules",
        "machine",
        "analytics",
        "Hadoop",
        "AWS",
        "Rbased",
        "Spark",
        "Scala",
        "Hadoop",
        "SparkStreaming",
        "MLLib",
        "Rabroadvariety",
        "machinelearning",
        "methods",
        "includingclassifications",
        "regressions",
        "dimensionallyreduction",
        "engine",
        "user",
        "lifetime",
        "user",
        "conversations",
        "target",
        "categories",
        "CloudIaaS",
        "PaaS",
        "implementations",
        "publicclouds",
        "VMware",
        "Openstack",
        "AmazonAWS",
        "Cloudfoundry",
        "Pivotal",
        "HPStackato",
        "Used",
        "R",
        "h2oai",
        "libraries",
        "machinelearningalgorithms",
        "utilizedmachinelearningalgorithms",
        "linearregression",
        "multivariateregression",
        "naiveBayes",
        "RandomForests",
        "Kmeans",
        "KNN",
        "dataanalysis",
        "relationalintegrityconstraints",
        "OLTP",
        "Cubes",
        "andNormalization",
        "3N0F",
        "DeNormalization",
        "database",
        "customersegmentation",
        "learning",
        "techniqueclustering",
        "Spark",
        "Scala",
        "Hadoop",
        "HBase",
        "Kafka",
        "SparkStreaming",
        "MLlib",
        "Python",
        "variety",
        "machinelearningmethods",
        "classifications",
        "regressions",
        "dimensionallyreduction",
        "Data",
        "analysis",
        "TableauPerform",
        "data",
        "requests",
        "SQLserver2012",
        "system",
        "architecture",
        "AmazonEC2",
        "solution",
        "client",
        "Complex",
        "ETLMappings",
        "Sessions",
        "business",
        "user",
        "requirements",
        "business",
        "rules",
        "data",
        "source",
        "files",
        "RDBMS",
        "tables",
        "tables",
        "Hands",
        "experience",
        "Hadoopecosystem",
        "componentsHadoopMapReduce",
        "HDFS",
        "Oozie",
        "HiveQL",
        "Sqoop",
        "HBase",
        "MongoDB",
        "Zookeeper",
        "Pig",
        "andFlume",
        "Environment",
        "R",
        "SQL",
        "Oracle",
        "12c",
        "Netezza",
        "SQL",
        "Server",
        "Informatica",
        "Java",
        "SSRS",
        "PLSQL",
        "TSQL",
        "Tableau",
        "MLlib",
        "regression",
        "Cluster",
        "analysis",
        "Scala",
        "NLP",
        "Spark",
        "Kafka",
        "MongoDB",
        "regression",
        "Hadoop",
        "Hive",
        "Teradata",
        "forest",
        "OLAP",
        "Azure",
        "SAP",
        "CRM",
        "HDFS",
        "ODS",
        "NLTK",
        "SVM",
        "JSON",
        "Tableau",
        "XML",
        "Cassandra",
        "MapReduce",
        "AWS",
        "Data",
        "Developer",
        "LOral",
        "Shanghai",
        "CN",
        "June",
        "November",
        "Responsibilities",
        "analysis",
        "customer",
        "behaviors",
        "value",
        "customers",
        "RMFanalysis",
        "customer",
        "segmentation",
        "algorithms",
        "KMeansClustering",
        "HierarchicalClustering",
        "Collaborated",
        "data",
        "engineers",
        "ETLprocess",
        "SQLqueries",
        "dataextraction",
        "Oracle",
        "Used",
        "Rand",
        "Spark",
        "variety",
        "models",
        "algorithms",
        "purposes",
        "dataintegritychecks",
        "analysis",
        "feature",
        "engineer",
        "R",
        "product",
        "recommendation",
        "Machinelearningalgorithms",
        "GradientBoostingTree",
        "Collaborative",
        "filtering",
        "needs",
        "customers",
        "customers",
        "R",
        "Spark",
        "machinelearningalgorithms",
        "GeneralizedLinearModel",
        "RandomForest",
        "SVM",
        "Boosting",
        "NeuralNetwork",
        "parameters",
        "KFoldCrossValidation",
        "performance",
        "models",
        "Data",
        "Science",
        "program",
        "DataManipulation",
        "Visualization",
        "WebScraping",
        "MachineLearning",
        "GIT",
        "SQL",
        "UNIXCommands",
        "Rprogramming",
        "NoSQL",
        "risk",
        "level",
        "eligibility",
        "insurance",
        "applicants",
        "MachineLearning",
        "SQL",
        "HiveQL",
        "manipulate",
        "data",
        "variety",
        "data",
        "sources",
        "Oracle",
        "HDFS",
        "data",
        "integrity",
        "Performed",
        "datavisualization",
        "Designeddashboards",
        "Tableau",
        "D3js",
        "complexreports",
        "summaries",
        "graphs",
        "findings",
        "team",
        "stakeholders",
        "Environment",
        "R",
        "MATLAB",
        "MongoDB",
        "analysis",
        "feature",
        "engineering",
        "KMeans",
        "Clustering",
        "Machine",
        "Learning",
        "Python",
        "Spark",
        "MLlib",
        "PySpark",
        "Tableau",
        "Micro",
        "Strategy",
        "SAS",
        "Tensor",
        "Flow",
        "regression",
        "regression",
        "Hadoop",
        "OLTP",
        "forest",
        "OLAP",
        "HDFS",
        "ODS",
        "NLTK",
        "SVM",
        "JSON",
        "XML",
        "MapReduce",
        "Data",
        "Analyst",
        "C",
        "Client",
        "Chicago",
        "IL",
        "June",
        "August",
        "Responsibilities",
        "JAD",
        "sessions",
        "information",
        "BusinessAnalysts",
        "users",
        "stakeholders",
        "requirements",
        "Hands",
        "Experience",
        "CloudComputing",
        "asAWSstorage",
        "Compute",
        "DatabasesSQL",
        "DataWarehouse",
        "Logical",
        "Physicaldatamodels",
        "Performed",
        "DailyMonitoring",
        "Oracle",
        "instances",
        "OracleEnterpriseManager",
        "ADDM",
        "TOAD",
        "monitorusers",
        "tablespaces",
        "memorystructures",
        "logs",
        "andalerts",
        "Normalization",
        "methods",
        "Denormalization",
        "techniques",
        "performance",
        "OLTP",
        "OLAP",
        "systems",
        "DDL",
        "scripts",
        "ForwardEngineering",
        "technique",
        "objects",
        "databases",
        "database",
        "testing",
        "SQLqueries",
        "transactions",
        "business",
        "logic",
        "rows",
        "SQLDeveloper",
        "PLSQL",
        "Developer",
        "TeradataSQLAssistant",
        "TeradataAdministrator",
        "PMON",
        "data",
        "loadexport",
        "utilities",
        "BTEQ",
        "FastLoad",
        "MultiLoad",
        "FastExport",
        "TPump",
        "UNIXWindows",
        "environments",
        "batch",
        "process",
        "Teradata",
        "Developed",
        "SQLQueries",
        "data",
        "tables",
        "databases",
        "joins",
        "database",
        "links",
        "Bulkcollects",
        "database",
        "legacy",
        "systems",
        "SQL",
        "server",
        "Oracle",
        "Netezza",
        "SSIS",
        "ETL",
        "packages",
        "extract",
        "transform",
        "data",
        "data",
        "Source",
        "servers",
        "database",
        "SQLServerconceptsSSIS",
        "SQLServerIntegrationServices",
        "SSAS",
        "SQLServerAnalysisServices",
        "SSRS",
        "SQLServerReportingServices",
        "Python",
        "Developer",
        "Aspect",
        "Software",
        "July",
        "May",
        "Environment",
        "ER",
        "Studio",
        "SQL",
        "PLSQL",
        "BTEQ",
        "DB2",
        "Oracle",
        "MDM",
        "Netezza",
        "ETL",
        "RTF",
        "UNIX",
        "SQL",
        "Server2010",
        "Informatica",
        "SSRS",
        "SSIS",
        "SSAS",
        "SAS",
        "Aginity",
        "Client",
        "Aspect",
        "Software",
        "INDIA",
        "July",
        "May",
        "July",
        "Role",
        "Python",
        "Developer",
        "Responsibilities",
        "project",
        "requirements",
        "application",
        "Anaconda",
        "Python",
        "Environment",
        "Anaconda",
        "environment",
        "Wrote",
        "programs",
        "performance",
        "calculations",
        "NumPy",
        "SQLAlchemy",
        "python",
        "routines",
        "websites",
        "data",
        "options",
        "modules",
        "Requests",
        "web",
        "Experience",
        "ML",
        "techniques",
        "regression",
        "classification",
        "models",
        "experience",
        "Text",
        "Analytics",
        "Machine",
        "Learning",
        "Data",
        "mining",
        "solutions",
        "business",
        "problems",
        "data",
        "visualizations",
        "R",
        "Python",
        "Tableau",
        "packages",
        "Beautiful",
        "Soup",
        "data",
        "development",
        "Web",
        "Services",
        "SOAP",
        "data",
        "interface",
        "XML",
        "format",
        "packages",
        "Beautiful",
        "Soup",
        "data",
        "development",
        "SQL",
        "procedures",
        "MYSQL",
        "code",
        "code",
        "redundancy",
        "level",
        "Design",
        "text",
        "classification",
        "application",
        "text",
        "classification",
        "models",
        "Jira",
        "tracking",
        "project",
        "management",
        "writing",
        "data",
        "CSV",
        "file",
        "formats",
        "Sprint",
        "planning",
        "sessions",
        "Agile",
        "SCRUM",
        "meetings",
        "day",
        "part",
        "SCRUM",
        "Master",
        "role",
        "project",
        "Linux",
        "environment",
        "reports",
        "application",
        "Performed",
        "QA",
        "testing",
        "application",
        "meetings",
        "client",
        "project",
        "help",
        "client",
        "Environment",
        "Python",
        "Anaconda",
        "Sypder",
        "IDE",
        "Windows",
        "Teradata",
        "Requests",
        "Beautiful",
        "Soup",
        "Tableau",
        "NumPy",
        "SQL",
        "Alchemy",
        "MySQLdb",
        "Education",
        "Bachelors",
        "Skills",
        "APACHE",
        "HADOOP",
        "HDFS",
        "years",
        "Oracle",
        "years",
        "years",
        "SQL",
        "years",
        "XML",
        "years",
        "Additional",
        "Information",
        "Skills",
        "Big",
        "DataHadoop",
        "Technologies",
        "Hadoop",
        "HDFS",
        "YARN",
        "MapReduce",
        "Hive",
        "Pig",
        "Impala",
        "Sqoop",
        "Flume",
        "Spark",
        "Kafka",
        "Zookeeper",
        "Oozie",
        "Languages",
        "C",
        "C",
        "HTML5",
        "DHTML",
        "WSDL",
        "css3",
        "XML",
        "RR",
        "Studio",
        "SAS",
        "Enterprise",
        "Guide",
        "SAS",
        "R",
        "R",
        "Caret",
        "Weka",
        "ggplot",
        "Python",
        "NumPy",
        "SciPy",
        "Pandas",
        "SQL",
        "PLSQL",
        "Pig",
        "Latin",
        "HiveQL",
        "Shell",
        "Scripting",
        "Cloud",
        "Computing",
        "Tools",
        "Amazon",
        "AWS",
        "Azure",
        "Microsoft",
        "SQL",
        "Server",
        "MySQL",
        "Oracle",
        "g",
        "g",
        "DB2",
        "Teradata",
        "Netezza",
        "SQL",
        "HBase",
        "Cassandra",
        "MongoDB",
        "Build",
        "Tools",
        "Maven",
        "ANT",
        "Toad",
        "SQL",
        "Loader",
        "RTC",
        "RSA",
        "ControlM",
        "Oozie",
        "Hue",
        "SOAP",
        "UI",
        "Development",
        "Tools",
        "Microsoft",
        "SQL",
        "Studio",
        "Eclipse",
        "NetBeans",
        "Development",
        "Methodologies",
        "AgileScrum",
        "Waterfall",
        "UML",
        "Design",
        "Patterns",
        "Version",
        "Control",
        "Tools",
        "Testing",
        "API",
        "Git",
        "SVM",
        "GitHub",
        "SVN",
        "JUNIT",
        "ETL",
        "Tools",
        "Informatica",
        "Power",
        "Centre",
        "SSIS",
        "Reporting",
        "Tools",
        "MS",
        "Office",
        "VisioOutlook",
        "Crystal",
        "Reports",
        "XI",
        "SSRS",
        "Cognos7060",
        "Operating",
        "Systems",
        "versions",
        "UNIX",
        "Windows",
        "LINUX",
        "Macintosh",
        "HD",
        "Sun",
        "Solaris"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T22:43:41.699696",
    "resume_data": "Data ScientistMachine Learning Data ScientistMachine Learning Data ScientistMachine Learning Norcross GA 8years of experience in DataScience and Analytics including MachineLearning DataMining DataBlendingStatisticalAnalysis Over 5Experience with MachineLearning techniques and algorithms such as kNN NaiveBayes etc Experience in AWS AmazonWebServicesEC2 VPC IAM IAM S3 CloudFront CloudWatch CloudFormation Glacier RDSConfig Route53 SNS SQS ElasticCache AzureCloud Extensive full cycle CloudAzure experience with full BigData Elasticsearch and SOLR MachineLearning and DeepLearning development and deployment Experienced with machinelearningalgorithms such as logisticregression randomforest XGboost KNN SVM neuralnetwork linearregression lassoregressionandkmeans Expertise in synthesizing Machinelearning PredictiveAnalytics and BigData technologies like Hadoop Hive Pig Strong skills in statistical methodologies such as ABtest experimentdesign hypothesistest ANOVA Experience in implementing data analysis with various analytic tools such as Anaconda 40R 30 ggplot2 Caret dplyr and Excel Solid ability to write and optimize diverse SQLqueries working knowledge of RDBMS like SQLServer2008 Experience in BigData technologies like Spark16 Sparksql pySpark Hadoop2X HDFS Hive1X Experience in visualization tools likeTableau9X 10X DataBlendingfor creating dashboards Proficient in PredictiveModeling DataMiningMethods FactorAnalysis ANOVA Hypotheticaltesting normal distribution and other advanced statistical and econometric techniques Developed predictive models using DecisionTree RandomForest NaiveBayes LogisticRegression ClusterAnalysis and NeuralNetworks Excellent knowledge of Machine Learning Mathematical Modeling and Operations Research Comfortable with R Python SAS MATLAB Relationaldatabases Deep understanding exposure of BigDataEcosystem Expert in creating PLSQLSchemaobjects like Packages Procedures Functions Subprograms Triggers Views MaterializedViews Indexes Constraints Sequences ExceptionHandling DynamicSQLCursors NativeCompilation CollectionTypes RecordType ObjectType using SQLDeveloper Hands on Experience in implementing ModelViewControl MVC architecture using Spring JDK CoreJava Collections OOPSConcepts JSP Servlets Struts springs Hibernate JDBCand provided ServerAdministrator duties LogicalPosition Worked with complex applications such as R SAS Matlaband SPSS to developeda neuralnetwork cluster analysis Experienced in DataIntegrationValidation and DataQuality controls for ETL process and DataWarehousing using MSVisualStudioSSIS SSAS SSRS Proficient in Tableau and RShiny datavisualization tools to analyze and obtain insights into large datasets create visually powerful and actionable interactive reports and dashboards Automated recurring reports using SQL and R and visualized them on BI platform like Tableau Worked in a development environment like Git and VM Excellent communication skills Successfully working in fastpaced multitasking environment both independently and in a collaborative team a selfmotivated enthusiastic learner Authorized to work in the US for any employer Work Experience Data ScientistMachine Learning FleetCor Technologies Inc Norcross GA May 2017 to May 2017 Responsibilities Analyzed Trading mechanism for realtime transactions and build collateral management tools Compiled data from various sources to perform complex analysis for actionable results Utilized machine learning algorithms such as linear regression multivariate regression naive bayes Random Forests Kmeans KNN for data analysis Measured Efficiency of HadoopHive environment ensuring SLA is met Developed Spark code using Scala and SparkSQLStreaming for faster processing of data Prepared Spark build from the source code and ran the PIG Scripts using Spark rather using MR jobs for better performance Analyzing the system for new enhancementsfunctionalities and perform Impact analysis of the application for implementing ETL changes Imported data using Sqoop to load data from MySQL to HDFS on regular basis Developed Scripts and Batch Job to schedule various Hadoop Program Used TensorFlow to train the model from insightful data and look at thousands of examples Designing developing and optimizing SQL code DDL DML Building performant scalable ETL processes to load cleanse and validate data Expertise in Data archival and Data migration adhoc reporting and code utilizing SAS on UNIX and Windows Environments Tested and debugged SAS programs against the test data Processed the data in SAS for the given requirement using SAS programming concepts Imported and Exported data files to and from SAS using Proc Import and Proc Export from Excel and various delimited textbased data files such as TXT tab delimited and CSV comma delimited files into SAS datasets for analysis Expertise in producing RTF PDF HTML files using SAS ODS facility Providing support for data processes This will involve monitoring data profiling database usage trouble shooting tuning and ensuring data integrity Participating in the full software development lifecycle with requirements solution design development QA implementation and product support using Scrum and other Agile methodologies Collaborate with team members and stakeholders in design and development of data environment Learning new tools and skillsets as needs arise Preparing associated documentation for specifications requirements and testing Optimizing the Tensorflow Model for an efficiency Used Tensorflow for text summarization Used Spark API over Cloudera Hadoop YARN to perform analytics on data in Hive Wrote Hive queries for data analysis to meet the business requirements Developed Kafka producer and consumers for message handling Responsible for analyzing multiplatform applications using python Used storm for an automatic mechanism to analyze large amounts of nonunique data points with low latency and high throughput Developed MapReduce jobs in Python for data cleaning and data processing Environment Machine learning AWS MS Azure Cassandra SAS Spark HDFS Hive Pig Linux Anaconda Python MySQL Eclipse PLSQL SQL connector SparkML Data ScientistMachine Learning Ford Motor Dearborn MI December 2015 to April 2017 Responsibilities Provided the architectural leadership in shaping strategic business technology projects with an emphasis on application architecture Utilized domain knowledge and application portfolio knowledge to play a key role in defining the future state of large business technology programs Participated in all phases of datamining datacollection datacleaning developingmodels validation and visualization and performed Gapanalysis Developed MapReduceSpark R modules for machine learning predictive analytics in Hadoop on AWS Implemented a Rbased distributed randomforest Utilized Spark Scala Hadoop SparkStreaming MLLib Rabroadvariety of machinelearning methods includingclassifications regressions dimensionallyreduction etc and utilized the engine to increase user lifetime by 45 and triple user conversations for target categories Well versed with CloudIaaS and PaaS implementations in both private and publicclouds like VMware Openstack AmazonAWS and Cloudfoundry Pivotal and HPStackato Used R and h2oai libraries for developing various machinelearningalgorithms and utilizedmachinelearningalgorithms such as linearregression multivariateregression naiveBayes RandomForests Kmeans KNN for dataanalysis Worked on databasedesign relationalintegrityconstraints OLAP OLTP Cubes andNormalization 3N0F and DeNormalization of the database Worked on customersegmentation using an unsupervised learning techniqueclustering Utilized Spark Scala Hadoop HBase Kafka SparkStreaming MLlib Python a broad variety of machinelearningmethods including classifications regressions dimensionallyreduction etc Data analysis reporting using TableauPerform numerous data pulling requests using SQLserver2012 Designed and implemented system architecture for AmazonEC2 based cloudhosted solution for the client Tested Complex ETLMappings and Sessions based on business user requirements and business rules to load data from source flat files and RDBMS tables to target tables Hands on experience in Hadoopecosystem with componentsHadoopMapReduce HDFS Oozie HiveQL Sqoop HBase MongoDB Zookeeper Pig andFlume Environment R SQL Oracle 12c Netezza SQL Server Informatica Java SSRS PLSQL TSQL Tableau MLlib regression Cluster analysis Scala NLP Spark Kafka MongoDB logistic regression Hadoop Hive Teradata random forest OLAP Azure MariaDB SAP CRM HDFS ODS NLTK SVM JSON Tableau XML Cassandra MapReduce AWS Data ScientistR Developer LOral Shanghai CN June 2014 to November 2015 Responsibilities The conducted analysis in assessing customer consuming behaviors and discover the value of customers with RMFanalysis applied customer segmentation with clustering algorithms such as KMeansClustering and HierarchicalClustering Collaborated with data engineers to implement the ETLprocess wrote and optimized SQLqueries to perform dataextraction and merging from Oracle Used Rand Spark to develop a variety of models and algorithms for analytic purposes Performed dataintegritychecks datacleaning exploratory analysis and feature engineer using R Developed personalized product recommendation with Machinelearningalgorithms including GradientBoostingTree and Collaborative filtering to better meet the needs of existing customers and acquire new customers Used R and Spark to implement different machinelearningalgorithms including GeneralizedLinearModel RandomForest SVM Boosting and NeuralNetwork Evaluated parameters with KFoldCrossValidation and optimized performance of models A highly immersive Data Science program involving DataManipulation and Visualization WebScraping MachineLearning GIT SQL UNIXCommands Rprogramming NoSQL Identified risk level and eligibility of new insurance applicants with MachineLearning algorithms Utilized SQL and HiveQL to query manipulate data from variety data sources including Oracle and HDFS while maintaining data integrity Performed datavisualization and Designeddashboards with Tableau and D3js and provided complexreports includingcharts summaries and graphs to interpret the findings to the team and stakeholders Environment R MATLAB MongoDB exploratory analysis feature engineering KMeans Clustering Hierarchical Clustering Machine Learning Python Spark MLlib PySpark Tableau Micro Strategy SAS Tensor Flow regression logistic regression Hadoop 27 OLTP random forest OLAP HDFS ODS NLTK SVM JSON XML and MapReduce Data Analyst C Client Chicago IL June 2012 to August 2012 Responsibilities Participated in JAD sessions gathered information from BusinessAnalysts end users and other stakeholders to determine the requirements Hands on Experience in CloudComputing such asAWSstorage Compute DatabasesSQL Designed the DataWarehouse and MDMhubConceptual Logical and Physicaldatamodels Performed DailyMonitoring of Oracle instances using OracleEnterpriseManager ADDM TOAD monitorusers tablespaces memorystructures rollbacksegments logs andalerts Used Normalization methods up to 3NF and Denormalization techniques for effective performance in OLTP and OLAP systems Generated DDL scripts using ForwardEngineering technique to create objects and deploy them into the databases Worked on database testing writing complex SQLqueries to verify the transactions and business logic like identifying the duplicate rows by using SQLDeveloper and PLSQL Developer Used TeradataSQLAssistant TeradataAdministrator PMON and data loadexport utilities like BTEQ FastLoad MultiLoad FastExport TPump on UNIXWindows environments and running the batch process for Teradata Developed SQLQueries to fetch complex data from different tables in remote databases using joins database links and Bulkcollects The migrated database from legacy systems SQL server to Oracle and Netezza Used SSIS to create ETL packages to validate extract transform and load data to pull data from Source servers to staging database Worked on SQLServerconceptsSSIS SQLServerIntegrationServices SSAS SQLServerAnalysisServices and SSRS SQLServerReportingServices Python Developer Aspect Software July 2010 to May 2011 Environment ER Studio Teradata131 SQL PLSQL BTEQ DB2 Oracle MDM Netezza ETL RTF UNIX SQL Server2010 Informatica SSRS SSIS SSAS SAS Aginity Client Aspect Software INDIA July 2010 May 2011 July 2012 Role Python Developer Responsibilities Worked on the project from gathering requirements to developing the entire application Worked on Anaconda Python Environment Created activated and programmed in Anaconda environment Wrote programs for performance calculations using NumPy and SQLAlchemy Wrote python routines to log into the websites and fetch data for selected options Used python modules of urllib urllib2 Requests for web crawling Experience using all these ML techniques clustering regression classification graphical models Extensive experience in Text Analytics developing different Statistical Machine Learning Data mining solutions to various business problems and generating data visualizations using R Python and Tableau Used with other packages such as Beautiful Soup for data parsing Involved in development of Web Services using SOAP for sending and getting data from the external interface in the XML format Used with other packages such as Beautiful Soup for data parsing Worked on development of SQL and stored procedures on MYSQL Analyzed the code completely and have reduced the code redundancy to the optimal level Design and build a text classification application using different text classification models Used Jira for defect tracking and project management Worked on writing and as well as read data from CSV and excel file formats Involved in Sprint planning sessions and participated in the daily Agile SCRUM meetings Conducted every day scrum as part of the SCRUM Master role Developed the project in Linux environment Worked on resulting reports of the application Performed QA testing on the application Held meetings with client and worked for the entire project with limited help from the client Environment Python Anaconda Sypder IDE Windows 7 Teradata Requests urllib urllib2 Beautiful Soup Tableau python libraries such as NumPy SQL Alchemy MySQLdb Education Bachelors Skills APACHE HADOOP HDFS 3 years Oracle 3 years python 3 years SQL 4 years XML 3 years Additional Information Skills Big DataHadoop Technologies Hadoop HDFS YARN MapReduce Hive Pig Impala Sqoop Flume Spark Kafka Zookeeper and Oozie Languages C C HTML5 DHTML WSDL css3 XML RR Studio SAS Enterprise Guide SAS R R Caret Weka ggplot Python NumPy SciPy Pandas SQL PLSQL Pig Latin HiveQL Shell Scripting Cloud Computing Tools Amazon AWS Azure Databases Microsoft SQL Server 2008 MySQL 4x5x Oracle 10g 11g 12c DB2 Teradata Netezza NO SQL Databases HBase Cassandra MongoDB MariaDB Build Tools Maven ANT Toad SQL Loader RTC RSA ControlM Oozie Hue SOAP UI Development Tools Microsoft SQL Studio Eclipse NetBeans IntelliJ Development Methodologies AgileScrum Waterfall UML Design Patterns Version Control Tools and Testing API Git SVM GitHub SVN and JUNIT ETL Tools Informatica Power Centre SSIS Reporting Tools MS Office WordExcelPowerPoint VisioOutlook Crystal Reports XI SSRS Cognos7060 Operating Systems All versions of UNIX Windows LINUX Macintosh HD Sun Solaris",
    "unique_id": "e2e7108b-83f7-494d-91dc-b7a918661d98"
}