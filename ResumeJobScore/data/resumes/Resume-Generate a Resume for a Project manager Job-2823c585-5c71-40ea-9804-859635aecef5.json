{
    "clean_data": "SR BIGDATA DEVELOPERENGINEER SR BIGDATA span lDEVELOPERspanENGINEER SR BIGDATA DEVELOPERENGINEER MeganSoft Austin TX Around 11 years of overall experience in software development with extensive experience with Big Data technologies Good knowledge of Hadoop ecosystem such as HDFS YARN and MapReduce Experience in build scripts using Maven and do continuous integrations systems like Jenkins Expertise in using Kafka as a messaging system to implement realtime Streaming solutions and implemented Sqoop for large data transfers from RDMS to HDFSHBaseHive and viceversa Experience in migrating ETL process into Hadoop Designing Hive data model and wrote Pig Latin scripts to load data into Hadoop Good Knowledge on Cloudera distributions and in Amazon simple storage service Amazon S3 AWS and Amazon EC2 Amazon EMR Experience working with structured semistructured and unstructured data ingestion technologies such as Sqoop Flume and Kafka Experience working with relational databases such as MySQL 57 Oracle 10g and Postgres 96 and NoSQL databases such as MongoDB Cassandra and HBase Indepth knowledge of Apache Cassandra architecture and extensive experience designing Cassandra data models and working with Cassandra Query Language CQL Experience writing Hive QL queries and Pig Latin scripts for ETL Expertise in processing and analyzing archived and realtime data using Core Spark SparkSQL and Spark Streaming Experience writing workflows in Oozie to schedule various jobs on Hadoop and with Expertise in Core Java features such as multithreading exception handling Generics garbage collection Collections lambda expressions serialization and deserialization Experience with writing unit tests using JUnit and Mockito frameworks Good Knowledge in Amazon Web Service AWS concepts like EMR and EC2 web services which provides fast and efficient processing of Teradata Big Data Analytics Expertise in Data Development in Hortonworks HDP platform Hadoop ecosystem tools like Hadoop HDFS Spark Zeppelin Hive HBase SQOOP flume Atlas SOLR Pig Falcon Oozie Hue Tez  Kafka Have very good experience in Apache Spark Spark Streaming Spark SQL and No SQL databases like Cassandra and Hbase Expert in Amazon EMR Spark Kinesis S3 Boto3 Bean Stalk ECS Cloud watch Lambda ELB VPC Elastic Cache Dynamo DB Redshit RDS Aethna Zeppelin Airflow Strong knowledge on creating and monitoring Hadoop clusters onAmazon EC2 VM HortonworksData Platform 21 22 CDH3 CDH4Cloudera Manager on Linux Ubuntu OS etc Expertise in Java Script JavaScript MVC patterns Object Oriented JavaScript Design Patterns and AJAX and developed core modules in large crossplatform applications using JAVA JSP Servlets JDBC JavaScript XML and HTML Zookeeper for managing and coordinating the cluster Experience with multiple Hadoop distributions such as Cloudera Hortonworks and AWS Experience with VMWare VirtualBox Docker and Vagrant Experience with Java SE 8 and Java EE frameworks such as Spring MVC 40 Spring Indepth understanding of Spark Architecture including Spark Core Spark SQL Data Frames Spark Streaming Spark MLlib Expertise in writing Spark RDD transformations actions Data Frames case classes for the required input data and performed the data transformations using SparkCore Good knowledge of Hadoop Architecture and various components such as YARN HDFS NodeManager ResourceManager JobTracker TaskTracker NameNode DataNode and MapReduce concepts Strong knowledge in NOSQL column oriented databases like HBase Cassandra MongoDB and its integration with Hadoop cluster Experience in installation configuration supporting and managing HortonworksClouderas Hadoop platform along with CDH34 clusters Solid SQL skills can write complex SQL queries functions triggers and stored procedures for Backend testing Database Testing and EndtoEnd testing Experienced on Hadoop cluster on Azure HD Insight Platform and deployed Data analytic solutions using tools like Spark and BI reporting tools Authorized to work in the US for any employer Work Experience SR BIGDATA DEVELOPERENGINEER MeganSoft Dearborn MI January 2017 to Present RESPONSIBILITIES Worked on loading disparate data sets coming from different sources to BDpaas HADOOP environment using Spark Developed UNIX scripts in creating Batch load for bringing huge amount of data from Relational databases to BIG DATA platform Delivery experience on major Hadoop ecosystem Components such as Pig Hive Spark Kafka Elastic Search HBase and monitoring with Cloudera Manager Used AWS Data Pipeline to schedule an Amazon EMR cluster to clean and process web server logs stored in Amazon S3 bucket Involved in gathering and analyzing system requirements and played key role in the highlevel design for the implementation of this application Mavenized the existing applications using Maven tool and added the required jar files to the application as dependencies to the pomXML file and used JSF Struts frameworks to interact with the front end Utilized SwingJFC framework to develop client side components and developed J2EE components on Eclipse IDE Implemented the Machine learning algorithms using Spark with Python and worked on Spark Storm Apache and Apex and python Involved in analyzing data coming from various sources and creating Metafiles and control files to ingest the data in to the Data Lake Involved in configuring batch job to perform ingestion of the source files in to the Data Lake and developed Pig queries to load data to HBase Leveraged Hive queries to create ORC tables and developed HIVE scripts for analyst requirements for analysis Implemented Kafka consumers to move data from Kafka partitions into Cassandra for near realtime analysis and worked extensively on Hive to create alter and drop tables and involved in writing hive queries Developed workflow in Oozie to automate the tasks of loading the data into HDFS and preprocessing with Pig and parsed highlevel design spec to simple ETL coding and mapping standards Created and altered HBase tables on top of data residing in Data Lake and Created external Hive tables on the Blobs to showcase the data to the Hive Meta Store Involved in requirement and design phase to implement Streaming Architecture to use real time streaming using Spark and Kafka Use Spark API for Machine learning Translate a predictive model from SAS code to Spark and used Spark API over Cloudera Hadoop YARN to perform analytics on data in Hive Created Reports with different Selection Criteria from Hive Tables on the data residing in Data Lake Worked on Hadoop Architecture and various components such as YARN HDFS NodeManager Resource Manager JobTracker TaskTracker NameNode DataNode and MapReduce concepts Deployed Hadoop components on the Cluster like Hive HBase Spark Scala and others with respect to the requirement Uploaded and processed terabytes of data from various structured and unstructured sources into HDFS AWS cloud using Sqoop Implemented the Business Rules in Spark SCALA to get the business logic in place to run the Rating Engine Used Spark UI to observe the running of a submitted Spark Job at the node level and used Spark to do Property Bag Parsing of the data to get the required fields of data Extensively used ETL methodology for supporting Data Extraction transformations and loading processing using Hadoop Used both Hive context as well as SQL context of Spark to do the initial testing of the Spark job and used WINSCP and FTP to view the data storage structure in the server and to upload JARs which were used to do the Spark Submit Developed code from scratch in Spark using SCALA according to the technical requirements Environment Hadoop Hive HDFS Pig Sqoop Python SparkSQL Machine Learning MongoDB AWS AWS S3 AWS EC2 AWS EMR Oozie ETL Tableau Spark SparkStreaming KAFKA Netezza Apache Solr Cassandra Cloudera Distribution Java Impala Web Servers Maven Build MySQL Grafana AWS AgileScrum SR BIGDATA ENGINEER DEVELOPER Prabhav Services Inc Newark NJ September 2014 to December 2016 RESPONSIBILITIES Exploring with the Spark improving the performance and optimization of the existing algorithms in Hadoop using Spark Context Spark SQL Data Frame Spark Yarn Worked on cloud computing infrastructure eg Amazon Web Services EC2 and considerations for scalable distributed systems Worked on Gocd cicd tool to deploy application and have experience with Munin frame work for big data Testing Involved in file movements between HDFS and AWS S3 and extensively worked with S3 bucket in AWS and converted all Hadoop jobs to run in EMR by configuring the cluster according to the data size Documented and established policies and procedures for GIS technologies including servers workstations storage server virtualization GIS data security web based maps and service Extensively worked with Avro and Parquet files and converted the data from either format Parsed Semi Structured JSON data and converted to Parquet using Data Frames in Spark Involved in converting HiveSQL queries into Spark Transformations using Spark RDDs and Scala and involved in using SQOOP for importing and exporting data between RDBMS and HDFS Imported the data from different sources like AWS S3 Local file system into Spark RDD Collected data using Spark Streaming from AWS S3 bucket in nearrealtime and performs necessary Transformations and Aggregations on the fly to build the common learner data model and persistence the data in HDFS Involved in transforming the relational database to legacy labels to HDFS and HBASE tables using Sqoop and vice versa Processed the web server logs by developing Multihop flume agents by using Avro Sink and loaded into MongoDB for further analysis and worked on MongoDB NoSQL data modeling tuning disaster recovery and backup Developed data pipeline using Spark Hive and HBase to ingest customer behavioral data and financial histories into Hadoop cluster for analysis Developed a Python Script to load the CSV files into the S3 buckets and created AWS S3 buckets performed folder management in each bucket managed logs and objects within each bucket Worked with different file formats like JSon AVRO and parquet and compression techniques like snappy and developed python code for different tasks dependencies SLA watcher and time sensor for each job for workflow management and automation using Airflow tool Developed shell scripts for dynamic partitions adding to hive stage table verifying JSON schema change of source files and verifying duplicate files in source location Monitor and Troubleshoot Hadoop jobs using Yarn Resource Manager and EMR job logs using Genie and Kibana Worked on CICD Automation using tools like Jenkins Salt stack Git Vagrant Docker Elastic Search Grafana Data Management Data Access Data Governance and Integration Security and Operations performed by using Hortonworks Data Platform HDP Worked with importing metadata into Hive using Python and migrated existing tables and applications to work on AWS cloud S3 Involved with writing scripts in Oracle SQL Server and Netezza databases to extract data for reporting and analysis and Worked in importing and cleansing of data from various sources like DB2 Oracle flat files onto SQL Server with high volume data Worked extensively with importing metadata into Hive and migrated existing tables and applications to work on Hive and AWS cloud and making the data available in Athena and Snowflake Extensively used Stash GitBucket for Code Control and Worked on AWS Components such as Airflow Elastic Map Reduce EMR Athena and SnowFlake ENVIRONMENT FileNet IBM RAD 60 Scala Java 15 JSP Servlets Core Java Spring Swing Hibernate JSF ICE Faces Hibernate HTML CSS Jenkins JavaScript NodeJs UNIX Web Services SOAP WAS 61 XML IBM WebSphere 61 Rational Clear Case Log 4j IBM DB2 HADOOP DEVELOPER Infinite Computer Solutions Irving TX US July 2012 to August 2014 RESPONSIBILITIES Demonstrated Hadoop practices and broad knowledge of technical solutions design patterns and code for mediumcomplex applications deployed in Hadoop production Wrote Spark applications for Data validation cleansing transformations and custom aggregations and imported data from different sources into Spark RDD for processing and developed custom aggregate functions using Spark SQL and performed interactive querying Responsible for developing data pipeline with Amazon AWS to extract the data from weblogs and store in HDFS Worked on data pipeline creation to convert incoming data to a common format prepare data for analysis and visualization Migrate between databases share data processing logic across web apps batch jobs and APIs Consume large XML CSV and fixedwidth files and created data pipelines in kafka to Replace batch jobs with realtime data Involved in data pipeline using Pig Sqoop to ingest cargo data and customer histories into HDFS for analysis Developed Pig scripts to help perform analytics on JSON and XML data and created Hive tables external internal with static and dynamic partitions and performed bucketing on the tables to provide efficiency Used Hive QL to analyze the partitioned and bucketed data and compute various metrics for reporting and performed data transformations by writing MapReduce and Pig jobs as per business requirements Used Apache Kafka to aggregate web log data from multiple servers and make them available in Downstream systems for analysis and used Kafka Streams to Configure Spark streaming to get information and then store it in HDFS Design Architecture of data pipelineingestion as well as optimization of ETL workflows and developed syllabusCurriculum data pipelines from SyllabusCurriculum Web Services to HBASE and Hive tables Performed data analysis feature selection feature extraction using Apache Spark Machine Learning streaming libraries in Python Leading managing planning the development and implementation of the wide Geographic Information Systems GIS program Worked on setting up and configuring AWSs EMR Clusters and Used Amazon IAM to grant finegrained access to AWS resources to users Enable and configure Hadoop services such as HDFS YARN Hive Ranger Hbase Kafka Sqoop Zeppeline Notebook and SparkSpark2 and involved in analyzing log data to predict the errors by using Apache Spark Evaluate deep learning algorithms for text summarization using Python Keras TensorFlow and Theano on Cloudera Hadoop system Designed Database Schema and created Data Model to store realtime Tick Data with NoSQL store Extracting real time data using Kafka and spark streaming by Creating DStreams and converting them into RDD processing it and stored it into Cassandra Used AWS Data Pipeline to schedule an Amazon EMR cluster to clean and process web server logs stored in Amazon S3 bucket Used DataStax SparkCassandra connector to load data into Cassandra and used CQL to analyze data from Cassandra tables for quick searching sorting and grouping and involved in analyzing log data to predict the errors by using Apache Spark Worked on migrating MapReduce programs into Spark transformations using Spark and Scala initially done using python PySpark Worked and learned a great deal from Amazon Web Services AWS Cloud services like EC2 S3 EBS RDS and VPC Integrated MapReduce with HBase to import bulk amount of data into HBase using MapReduce programs Used Impala and Written Queries for fetching Data from Hive tables and developed Several MapReduce jobs using Java API Worked with Apache SOLR to implement indexing and wrote Custom SOLR query segments to optimize the search Created kafka spark streaming data pipelines for consuming the data from external source and performing the transformations in scala and contributed towards developing a Data Pipeline to load data from different sources like Web RDBMS NoSQL to Apache Kafka or Spark cluster Worked with xmls extracting tag information using xpaths and Scala XML libraries from compressed blob data types Involved in creating Data Lake by extracting customers Big Data from various data sources into Hadoop HDFS This included data from Excel Flat Files Oracle SQL Server MongoDb Cassandra HBase Teradata Netezza and also log data from servers Define data governance rules and administrating the rights depending on job profile of users Developed Pig and Hive UDFs to implement business logic for processing the data as per requirements and developed Pig UDFs in Java and used UDFs from PiggyBank for sorting and preparing the data Developed Spark scripts by using Scala IDEas per the business requirement Configured and optimized the Cassandra cluster and developed realtime java based application to work along with the Cassandra database Involved in file movements between HDFS and AWS S3 and extensively worked with S3 bucket in AWS Developed Spark jobs using Scala on top of  for interactive and Batch Analysis and involved in querying data using SparkSQL on top of Spark engine for faster data sets processing and worked on implementing Spark Framework a Java based Web Frame work Created Hive tables loaded data and wrote Hive queries that helped market analysts spot emerging trends by comparing fresh data with EDW reference tables and historical metrics Developed Spark code by using Scala and SparkSQL for faster processing and testing and performed complex HiveQL queries on Hive tables Creating Nagios Grafana and Graphite dashboard for infrastructure monitoring Environment Spark AWS EC2 EMR Hive SQL Workbench Genie Logs Kibana Sqoop Spark SQL Spark Streaming Scala Python Hadoop Cloudera Stack Hue Spark Netezza Kafka HBase HDFS Hive Pig Sqoop Oracle ETL AWS S3 AWS EMR GIT Grafana SR JAVA DEVELOPER HiTech Solutions Inc Chicago IL US January 2010 to June 2012 RESPONSIBILITIES Involved in the design and development phases of Agile Software Development and analyzed current Mainframe system and designed new GUI screens Used Sqoop to ingest from DBMS and Python to ingest logs from client data centers Develop Python and bash scripts for automation and implemented Map Reduce jobs using Java API and Python using Spark Imported data from RDBMS systems like MySQL into HDFS using Sqoop and developed Sqoop jobs to perform incremental imports into Hive tables Demonstrated experience in managing the collection of geospatial data and understanding of data systems managed policies concerning the compilation of information and coordination of data through the GIS program coordinating and overseeing the implementation of policies Involved in loading and transforming of large sets of structured and semi structured data and created Data Pipelines as per the business requirements and scheduled it using Oozie Coordinators Developed the application using 3 Tier Architecture ie Presentation Business and Data Integration layers in accordance with the customerclient standards Played a vital role in Scala framework for web based applications and used File net for Content Management and for streamlining Business Processes Created Responsive Layouts for multiple devices and platforms using foundation framework and implemented printable chart report using HTML CSS and jQuery Applied JavaScript for client side form validation and worked on UNIX LINUX to move the project into production environment Created Managed Beans for handling JSF pages and include logic for processing of the data on the page and Created simple user interface for applications configuration system using MVC design patterns and swing framework Used ObjectRelational mapping tool Hibernate to achieve object to database table persistency Worked with Core Java to develop automated solutions to include web interfaces using HTML CSS JavaScript and Web services Developed web GUI involving HTML Java Script under MVC architecture and creation of WebLogic domains and setup Admin Managed servers for JAVAJ2EE applications on Non Production and Production environments Implemented code according to coding standards and Created AngularJS Controller Which Isolate scopes perform operations and used GWT GUICE JavaScript and Angular JS for client side implementation and extensively used Core Java such as Exceptions and Collections Configured the Web sphere application server to connect with DB2 Oracle and SQL Server in the back end by creating JDBC data source and configured MQ Series with IBM RAD and WAS to create new connection factories and queues Extensively worked on TOAD for interacting with data base developing the stored procedures and promoting SQL changes to QA and Production Environments Used Apache Maven for project management and building the application and CVS was used for project management and version management Creating and updating existing build scripts using Ant for deployment Tested and implementeddeployed application on WAS server and used Rational Clear Case for Version Control Environment Hadoop Map Reduce Yarn Hive Pig HBase Sqoop Spark Scala MapR Core Java R Language SQL Python Eclipse Linux Unix HDFS Map Reduce Impala Cloudera SQOOP Kafka Apache Cassandra Oozie Impala Zookeeper MySQL Eclipse PLSQL JAVA DEVELOPER APLOMB Technologies Princeton NJ US May 2007 to December 2009 RESPONSIBILITIES Involved in the configuration of Spring Framework and Hibernate mapping tool and monitoring WebLogicJBoss Server health and security Creation of Connection Pools Data Sources in WebLogic console and implemented Hibernate for Database Transactions on DB2 Implemented CI CD using Jenkins for continuous development and delivery Involved in configuring hibernate to access database and retrieve data from the database and written Web Services JAXWS for external system via SOAPHTTP call Used Log4j framework to logtrack application and involved in developing SQL queries stored procedures and functions Developed a new CR screen from the existing screen for the LTL loads Low Truck Load using JSF Used spring framework configuration files to manage objects and to achieve dependency injection Implemented cross cutting concerns like logging and monitoring mechanism using Spring AOP Implemented SOA architecture with web services using SOAP WSDL UDDI and XML and made screen changes to the existing screen for the LTL Low Truck Load Accessories using Struts Developed desktop interface using Java Swing for maintaining and tracking products Used JAXWS to access the external web services get the xml response and convert it back to java objects Developed the application using Eclipse IDE and worked under Agile Environment and worked with Web admin and the admin team to configure the application on development training test and stress environments Web logic server Executed and coordinated the installation for the project and worked on webbased reporting system with HTML JavaScript and JSP Build PLSQL functions stored procedures views and configured Oracle Database with JNDI data source with connection pooling enabled Developed the Training and Appraisal modules using Java JSP Servlets and JavaScript Used Hibernate based persistence classes at data access tier and adopted J2EE design patterns like Service Locator Session Facade and Singleton Worked on Spring Core layer Spring ORM Spring AOP in developing the application components Modified web pages using JSP and Used Struts Validation Framework for form input validation Created the WSDL and used Apache Axis 20 for publishing the WSDL and creating PDF files for storing the data required for module Used custom components using JSTL tags and Tag libraries implementing struts and used Web Logic server for deploying the war files and used Toad for the DB2 database changes ENVIRONMENT Java J2EE JSF Hibernate Struts Spring SwingJFC JSP HTML XML Web Logic iText DB2 Eclipse IDE SOAP Maven JSTL TOAD DB2 JDK Web Logic Server WSDL JAXWS Apache Axis Education Bachelors Skills Apache 9 years APACHE HADOOP HDFS 8 years APACHE HADOOP SQOOP 8 years Java 10 years Oracle 9 years Javascript Additional Information Technical Skills Bigdata Ecosystem HDFS and Map Reduce Pig Hive Impala YARN HUE Oozie ZookeeperSolr Apache Spark Apache STORM Apache Kafka Sqoop Flume Flink Elasticsearch NoSQL Databases HBase Cassandra and MongoDB Hadoop Distributions Cloudera Hortonworks Programming languages Java C SCALA Pig Latin HiveQL PySpark Scripting Languages Shell Scripting Databases MySQL oracle Teradata DB2 Build Tools Maven Ant sbt Reporting Tool Tableau Version control Tools SVN Git GitHub Cloud AWS Azure S3 EC2 EMR AppWeb servers WebSphere WebLogic JBoss and Tomcat Web Design Tools HTML AJAX JavaScript JQuery CSS and JSON Operating Systems WINDOWS 108Vista XP Development IDEs NetBeans Eclipse IDE PythonIDLE Packages Microsoft Office putty MS Visual Studio",
    "entities": [
        "Prabhav Services Inc",
        "Spark Transformations",
        "Oracle SQL Server",
        "AJAX",
        "HortonworksClouderas Hadoop",
        "GUI",
        "HBase Indepth",
        "Troubleshoot Hadoop",
        "Work Experience SR BIGDATA DEVELOPERENGINEER MeganSoft Dearborn",
        "Relational",
        "BI",
        "HDFS",
        "UNIX",
        "Linux Ubuntu OS etc Expertise",
        "HiTech Solutions Inc",
        "Define",
        "Hadoop Good Knowledge",
        "Data Lake",
        "Exceptions and Collections Configured",
        "JSON",
        "Developed Pig and",
        "IBM",
        "CVS",
        "Netezza",
        "JAVA JSP Servlets",
        "Worked on AWS Components",
        "Hadoop HDFS Spark",
        "JSON Operating Systems",
        "Hive Created Reports",
        "Hadoop",
        "the Cluster like Hive HBase Spark",
        "HDFS Involved",
        "XML",
        "Amazon S3 AWS",
        "RDD",
        "Presentation Business and Data Integration",
        "NOSQL",
        "Kibana Sqoop Spark",
        "Used Struts Validation Framework",
        "WebLogic",
        "Downstream",
        "JUnit",
        "JAXWS",
        "HBase",
        "Atlas SOLR Pig Falcon Oozie Hue",
        "Amazon",
        "MQ Series",
        "CDH3",
        "MS Visual Studio",
        "Cloudera Hadoop",
        "SQL Server",
        "Non Production and Production",
        "LTL",
        "Machine",
        "Amazon Web Services EC2",
        "PySpark Worked",
        "SparkSQL",
        "Used Amazon IAM",
        "HBase Leveraged Hive",
        "Munin",
        "WebSphere 61 Rational",
        "Developed",
        "AWS S3",
        "RDMS",
        "Cassandra Query Language CQL",
        "Mockito",
        "Utilized",
        "SnowFlake ENVIRONMENT",
        "BDpaas HADOOP",
        "HDFS Worked",
        "Agile Environment",
        "Cassandra Used AWS Data Pipeline",
        "Processed",
        "JNDI",
        "Translate",
        "JSP",
        "Teradata Big Data Analytics Expertise",
        "the Data Lake Involved",
        "Airflow",
        "Worked",
        "Spark RDD Collected",
        "the Spark Submit Developed",
        "Spark Streaming",
        "ORC",
        "Parquet",
        "ETL Expertise",
        "FileNet IBM",
        "SR BIGDATA DEVELOPERENGINEER MeganSoft",
        "Deployed Hadoop",
        "Oracle Database",
        "MVC",
        "Controller Which Isolate",
        "AWS Data Pipeline",
        "Apex",
        "Spark",
        "EndtoEnd",
        "SparkCassandra",
        "Created Hive",
        "Created the WSDL",
        "Amazon EMR",
        "Data Extraction",
        "Lambda ELB VPC Elastic",
        "Spark Developed",
        "CSV",
        "HTML CSS",
        "Jenkins Expertise",
        "Oozie Coordinators Developed",
        "Spark Context Spark",
        "US",
        "HTML CSS JavaScript",
        "Sqoop",
        "QA",
        "HIVE",
        "Geographic Information Systems",
        "PDF",
        "Web Frame",
        "Created",
        "Database Testing",
        "SyllabusCurriculum Web Services",
        "JSF Struts",
        "Spark Core Spark",
        "AWS",
        "SR BIGDATA DEVELOPERENGINEER SR BIGDATA",
        "Hadoop Architecture",
        "Scala",
        "Core Spark",
        "YARN HDFS NodeManager Resource",
        "Created Managed Beans",
        "Oracle",
        "Singleton",
        "JSF",
        "the Rating Engine Used Spark UI",
        "Apache",
        "Spark Involved",
        "jQuery Applied JavaScript",
        "DBMS",
        "Data Pipelines",
        "Struts Developed",
        "SAS",
        "Oozie",
        "SQL",
        "Rational Clear Case for",
        "Stash GitBucket for Code Control",
        "Spark RDD",
        "Microsoft Office",
        "Yarn Resource",
        "Reporting Tool Tableau Version",
        "Apache Spark Machine Learning",
        "Business Processes Created Responsive Layouts",
        "WebSphere WebLogic JBoss",
        "Grafana AWS AgileScrum SR BIGDATA",
        "Demonstrated Hadoop",
        "Chicago",
        "PiggyBank",
        "IDE PythonIDLE",
        "Amazon Web Services AWS Cloud",
        "Big Data",
        "Hive",
        "SQOOP",
        "Tomcat Web Design Tools",
        "Amazon EMR Spark",
        "VMWare",
        "Amazon AWS",
        "GIS",
        "FTP",
        "PySpark Scripting Languages Shell Scripting",
        "Data Development",
        "Spark Hive",
        "AWS Developed Spark",
        "ETL",
        "Agile Software Development",
        "Backend",
        "AWS S3 Local",
        "a Data Pipeline",
        "Maven",
        "Data Frames",
        "WINSCP",
        "Performed",
        "Scala XML",
        "Impala",
        "GWT",
        "Spark SQL",
        "XP Development",
        "HDFS Design Architecture",
        "Grafana Data Management Data Access Data Governance and Integration Security",
        "Blobs",
        "Spark Storm Apache",
        "CQL",
        "Strong",
        "Python Leading",
        "Sqoop Implemented the Business Rules",
        "Amazon Web Service AWS",
        "Graphite",
        "Javascript Additional Information Technical Skills Bigdata Ecosystem",
        "Grafana SR JAVA",
        "Written Queries",
        "Content Management",
        "CR",
        "Generics",
        "Hadoop HDFS This",
        "Spark Imported",
        "Creation of Connection Pools Data Sources",
        "EDW",
        "Environment Spark AWS EC2",
        "Java JSP Servlets",
        "Eclipse IDE Implemented",
        "MapReduce",
        "NetBeans",
        "SCALA",
        "RDBMS",
        "NoSQL",
        "Spark Architecture",
        "Consume",
        "TOAD",
        "Hadoop Designing Hive",
        "Infinite Computer",
        "Operations",
        "pomXML",
        "the Hive Meta Store Involved",
        "Athena",
        "Spring Framework and Hibernate",
        "Cloudera"
    ],
    "experience": "Experience in build scripts using Maven and do continuous integrations systems like Jenkins Expertise in using Kafka as a messaging system to implement realtime Streaming solutions and implemented Sqoop for large data transfers from RDMS to HDFSHBaseHive and viceversa Experience in migrating ETL process into Hadoop Designing Hive data model and wrote Pig Latin scripts to load data into Hadoop Good Knowledge on Cloudera distributions and in Amazon simple storage service Amazon S3 AWS and Amazon EC2 Amazon EMR Experience working with structured semistructured and unstructured data ingestion technologies such as Sqoop Flume and Kafka Experience working with relational databases such as MySQL 57 Oracle 10 g and Postgres 96 and NoSQL databases such as MongoDB Cassandra and HBase Indepth knowledge of Apache Cassandra architecture and extensive experience designing Cassandra data models and working with Cassandra Query Language CQL Experience writing Hive QL queries and Pig Latin scripts for ETL Expertise in processing and analyzing archived and realtime data using Core Spark SparkSQL and Spark Streaming Experience writing workflows in Oozie to schedule various jobs on Hadoop and with Expertise in Core Java features such as multithreading exception handling Generics garbage collection Collections lambda expressions serialization and deserialization Experience with writing unit tests using JUnit and Mockito frameworks Good Knowledge in Amazon Web Service AWS concepts like EMR and EC2 web services which provides fast and efficient processing of Teradata Big Data Analytics Expertise in Data Development in Hortonworks HDP platform Hadoop ecosystem tools like Hadoop HDFS Spark Zeppelin Hive HBase SQOOP flume Atlas SOLR Pig Falcon Oozie Hue Tez   Kafka Have very good experience in Apache Spark Spark Streaming Spark SQL and No SQL databases like Cassandra and Hbase Expert in Amazon EMR Spark Kinesis S3 Boto3 Bean Stalk ECS Cloud watch Lambda ELB VPC Elastic Cache Dynamo DB Redshit RDS Aethna Zeppelin Airflow Strong knowledge on creating and monitoring Hadoop clusters onAmazon EC2 VM HortonworksData Platform 21 22 CDH3 CDH4Cloudera Manager on Linux Ubuntu OS etc Expertise in Java Script JavaScript MVC patterns Object Oriented JavaScript Design Patterns and AJAX and developed core modules in large crossplatform applications using JAVA JSP Servlets JDBC JavaScript XML and HTML Zookeeper for managing and coordinating the cluster Experience with multiple Hadoop distributions such as Cloudera Hortonworks and AWS Experience with VMWare VirtualBox Docker and Vagrant Experience with Java SE 8 and Java EE frameworks such as Spring MVC 40 Spring Indepth understanding of Spark Architecture including Spark Core Spark SQL Data Frames Spark Streaming Spark MLlib Expertise in writing Spark RDD transformations actions Data Frames case classes for the required input data and performed the data transformations using SparkCore Good knowledge of Hadoop Architecture and various components such as YARN HDFS NodeManager ResourceManager JobTracker TaskTracker NameNode DataNode and MapReduce concepts Strong knowledge in NOSQL column oriented databases like HBase Cassandra MongoDB and its integration with Hadoop cluster Experience in installation configuration supporting and managing HortonworksClouderas Hadoop platform along with CDH34 clusters Solid SQL skills can write complex SQL queries functions triggers and stored procedures for Backend testing Database Testing and EndtoEnd testing Experienced on Hadoop cluster on Azure HD Insight Platform and deployed Data analytic solutions using tools like Spark and BI reporting tools Authorized to work in the US for any employer Work Experience SR BIGDATA DEVELOPERENGINEER MeganSoft Dearborn MI January 2017 to Present RESPONSIBILITIES Worked on loading disparate data sets coming from different sources to BDpaas HADOOP environment using Spark Developed UNIX scripts in creating Batch load for bringing huge amount of data from Relational databases to BIG DATA platform Delivery experience on major Hadoop ecosystem Components such as Pig Hive Spark Kafka Elastic Search HBase and monitoring with Cloudera Manager Used AWS Data Pipeline to schedule an Amazon EMR cluster to clean and process web server logs stored in Amazon S3 bucket Involved in gathering and analyzing system requirements and played key role in the highlevel design for the implementation of this application Mavenized the existing applications using Maven tool and added the required jar files to the application as dependencies to the pomXML file and used JSF Struts frameworks to interact with the front end Utilized SwingJFC framework to develop client side components and developed J2EE components on Eclipse IDE Implemented the Machine learning algorithms using Spark with Python and worked on Spark Storm Apache and Apex and python Involved in analyzing data coming from various sources and creating Metafiles and control files to ingest the data in to the Data Lake Involved in configuring batch job to perform ingestion of the source files in to the Data Lake and developed Pig queries to load data to HBase Leveraged Hive queries to create ORC tables and developed HIVE scripts for analyst requirements for analysis Implemented Kafka consumers to move data from Kafka partitions into Cassandra for near realtime analysis and worked extensively on Hive to create alter and drop tables and involved in writing hive queries Developed workflow in Oozie to automate the tasks of loading the data into HDFS and preprocessing with Pig and parsed highlevel design spec to simple ETL coding and mapping standards Created and altered HBase tables on top of data residing in Data Lake and Created external Hive tables on the Blobs to showcase the data to the Hive Meta Store Involved in requirement and design phase to implement Streaming Architecture to use real time streaming using Spark and Kafka Use Spark API for Machine learning Translate a predictive model from SAS code to Spark and used Spark API over Cloudera Hadoop YARN to perform analytics on data in Hive Created Reports with different Selection Criteria from Hive Tables on the data residing in Data Lake Worked on Hadoop Architecture and various components such as YARN HDFS NodeManager Resource Manager JobTracker TaskTracker NameNode DataNode and MapReduce concepts Deployed Hadoop components on the Cluster like Hive HBase Spark Scala and others with respect to the requirement Uploaded and processed terabytes of data from various structured and unstructured sources into HDFS AWS cloud using Sqoop Implemented the Business Rules in Spark SCALA to get the business logic in place to run the Rating Engine Used Spark UI to observe the running of a submitted Spark Job at the node level and used Spark to do Property Bag Parsing of the data to get the required fields of data Extensively used ETL methodology for supporting Data Extraction transformations and loading processing using Hadoop Used both Hive context as well as SQL context of Spark to do the initial testing of the Spark job and used WINSCP and FTP to view the data storage structure in the server and to upload JARs which were used to do the Spark Submit Developed code from scratch in Spark using SCALA according to the technical requirements Environment Hadoop Hive HDFS Pig Sqoop Python SparkSQL Machine Learning MongoDB AWS AWS S3 AWS EC2 AWS EMR Oozie ETL Tableau Spark SparkStreaming KAFKA Netezza Apache Solr Cassandra Cloudera Distribution Java Impala Web Servers Maven Build MySQL Grafana AWS AgileScrum SR BIGDATA ENGINEER DEVELOPER Prabhav Services Inc Newark NJ September 2014 to December 2016 RESPONSIBILITIES Exploring with the Spark improving the performance and optimization of the existing algorithms in Hadoop using Spark Context Spark SQL Data Frame Spark Yarn Worked on cloud computing infrastructure eg Amazon Web Services EC2 and considerations for scalable distributed systems Worked on Gocd cicd tool to deploy application and have experience with Munin frame work for big data Testing Involved in file movements between HDFS and AWS S3 and extensively worked with S3 bucket in AWS and converted all Hadoop jobs to run in EMR by configuring the cluster according to the data size Documented and established policies and procedures for GIS technologies including servers workstations storage server virtualization GIS data security web based maps and service Extensively worked with Avro and Parquet files and converted the data from either format Parsed Semi Structured JSON data and converted to Parquet using Data Frames in Spark Involved in converting HiveSQL queries into Spark Transformations using Spark RDDs and Scala and involved in using SQOOP for importing and exporting data between RDBMS and HDFS Imported the data from different sources like AWS S3 Local file system into Spark RDD Collected data using Spark Streaming from AWS S3 bucket in nearrealtime and performs necessary Transformations and Aggregations on the fly to build the common learner data model and persistence the data in HDFS Involved in transforming the relational database to legacy labels to HDFS and HBASE tables using Sqoop and vice versa Processed the web server logs by developing Multihop flume agents by using Avro Sink and loaded into MongoDB for further analysis and worked on MongoDB NoSQL data modeling tuning disaster recovery and backup Developed data pipeline using Spark Hive and HBase to ingest customer behavioral data and financial histories into Hadoop cluster for analysis Developed a Python Script to load the CSV files into the S3 buckets and created AWS S3 buckets performed folder management in each bucket managed logs and objects within each bucket Worked with different file formats like JSon AVRO and parquet and compression techniques like snappy and developed python code for different tasks dependencies SLA watcher and time sensor for each job for workflow management and automation using Airflow tool Developed shell scripts for dynamic partitions adding to hive stage table verifying JSON schema change of source files and verifying duplicate files in source location Monitor and Troubleshoot Hadoop jobs using Yarn Resource Manager and EMR job logs using Genie and Kibana Worked on CICD Automation using tools like Jenkins Salt stack Git Vagrant Docker Elastic Search Grafana Data Management Data Access Data Governance and Integration Security and Operations performed by using Hortonworks Data Platform HDP Worked with importing metadata into Hive using Python and migrated existing tables and applications to work on AWS cloud S3 Involved with writing scripts in Oracle SQL Server and Netezza databases to extract data for reporting and analysis and Worked in importing and cleansing of data from various sources like DB2 Oracle flat files onto SQL Server with high volume data Worked extensively with importing metadata into Hive and migrated existing tables and applications to work on Hive and AWS cloud and making the data available in Athena and Snowflake Extensively used Stash GitBucket for Code Control and Worked on AWS Components such as Airflow Elastic Map Reduce EMR Athena and SnowFlake ENVIRONMENT FileNet IBM RAD 60 Scala Java 15 JSP Servlets Core Java Spring Swing Hibernate JSF ICE Faces Hibernate HTML CSS Jenkins JavaScript NodeJs UNIX Web Services SOAP WAS 61 XML IBM WebSphere 61 Rational Clear Case Log 4j IBM DB2 HADOOP DEVELOPER Infinite Computer Solutions Irving TX US July 2012 to August 2014 RESPONSIBILITIES Demonstrated Hadoop practices and broad knowledge of technical solutions design patterns and code for mediumcomplex applications deployed in Hadoop production Wrote Spark applications for Data validation cleansing transformations and custom aggregations and imported data from different sources into Spark RDD for processing and developed custom aggregate functions using Spark SQL and performed interactive querying Responsible for developing data pipeline with Amazon AWS to extract the data from weblogs and store in HDFS Worked on data pipeline creation to convert incoming data to a common format prepare data for analysis and visualization Migrate between databases share data processing logic across web apps batch jobs and APIs Consume large XML CSV and fixedwidth files and created data pipelines in kafka to Replace batch jobs with realtime data Involved in data pipeline using Pig Sqoop to ingest cargo data and customer histories into HDFS for analysis Developed Pig scripts to help perform analytics on JSON and XML data and created Hive tables external internal with static and dynamic partitions and performed bucketing on the tables to provide efficiency Used Hive QL to analyze the partitioned and bucketed data and compute various metrics for reporting and performed data transformations by writing MapReduce and Pig jobs as per business requirements Used Apache Kafka to aggregate web log data from multiple servers and make them available in Downstream systems for analysis and used Kafka Streams to Configure Spark streaming to get information and then store it in HDFS Design Architecture of data pipelineingestion as well as optimization of ETL workflows and developed syllabusCurriculum data pipelines from SyllabusCurriculum Web Services to HBASE and Hive tables Performed data analysis feature selection feature extraction using Apache Spark Machine Learning streaming libraries in Python Leading managing planning the development and implementation of the wide Geographic Information Systems GIS program Worked on setting up and configuring AWSs EMR Clusters and Used Amazon IAM to grant finegrained access to AWS resources to users Enable and configure Hadoop services such as HDFS YARN Hive Ranger Hbase Kafka Sqoop Zeppeline Notebook and SparkSpark2 and involved in analyzing log data to predict the errors by using Apache Spark Evaluate deep learning algorithms for text summarization using Python Keras TensorFlow and Theano on Cloudera Hadoop system Designed Database Schema and created Data Model to store realtime Tick Data with NoSQL store Extracting real time data using Kafka and spark streaming by Creating DStreams and converting them into RDD processing it and stored it into Cassandra Used AWS Data Pipeline to schedule an Amazon EMR cluster to clean and process web server logs stored in Amazon S3 bucket Used DataStax SparkCassandra connector to load data into Cassandra and used CQL to analyze data from Cassandra tables for quick searching sorting and grouping and involved in analyzing log data to predict the errors by using Apache Spark Worked on migrating MapReduce programs into Spark transformations using Spark and Scala initially done using python PySpark Worked and learned a great deal from Amazon Web Services AWS Cloud services like EC2 S3 EBS RDS and VPC Integrated MapReduce with HBase to import bulk amount of data into HBase using MapReduce programs Used Impala and Written Queries for fetching Data from Hive tables and developed Several MapReduce jobs using Java API Worked with Apache SOLR to implement indexing and wrote Custom SOLR query segments to optimize the search Created kafka spark streaming data pipelines for consuming the data from external source and performing the transformations in scala and contributed towards developing a Data Pipeline to load data from different sources like Web RDBMS NoSQL to Apache Kafka or Spark cluster Worked with xmls extracting tag information using xpaths and Scala XML libraries from compressed blob data types Involved in creating Data Lake by extracting customers Big Data from various data sources into Hadoop HDFS This included data from Excel Flat Files Oracle SQL Server MongoDb Cassandra HBase Teradata Netezza and also log data from servers Define data governance rules and administrating the rights depending on job profile of users Developed Pig and Hive UDFs to implement business logic for processing the data as per requirements and developed Pig UDFs in Java and used UDFs from PiggyBank for sorting and preparing the data Developed Spark scripts by using Scala IDEas per the business requirement Configured and optimized the Cassandra cluster and developed realtime java based application to work along with the Cassandra database Involved in file movements between HDFS and AWS S3 and extensively worked with S3 bucket in AWS Developed Spark jobs using Scala on top of   for interactive and Batch Analysis and involved in querying data using SparkSQL on top of Spark engine for faster data sets processing and worked on implementing Spark Framework a Java based Web Frame work Created Hive tables loaded data and wrote Hive queries that helped market analysts spot emerging trends by comparing fresh data with EDW reference tables and historical metrics Developed Spark code by using Scala and SparkSQL for faster processing and testing and performed complex HiveQL queries on Hive tables Creating Nagios Grafana and Graphite dashboard for infrastructure monitoring Environment Spark AWS EC2 EMR Hive SQL Workbench Genie Logs Kibana Sqoop Spark SQL Spark Streaming Scala Python Hadoop Cloudera Stack Hue Spark Netezza Kafka HBase HDFS Hive Pig Sqoop Oracle ETL AWS S3 AWS EMR GIT Grafana SR JAVA DEVELOPER HiTech Solutions Inc Chicago IL US January 2010 to June 2012 RESPONSIBILITIES Involved in the design and development phases of Agile Software Development and analyzed current Mainframe system and designed new GUI screens Used Sqoop to ingest from DBMS and Python to ingest logs from client data centers Develop Python and bash scripts for automation and implemented Map Reduce jobs using Java API and Python using Spark Imported data from RDBMS systems like MySQL into HDFS using Sqoop and developed Sqoop jobs to perform incremental imports into Hive tables Demonstrated experience in managing the collection of geospatial data and understanding of data systems managed policies concerning the compilation of information and coordination of data through the GIS program coordinating and overseeing the implementation of policies Involved in loading and transforming of large sets of structured and semi structured data and created Data Pipelines as per the business requirements and scheduled it using Oozie Coordinators Developed the application using 3 Tier Architecture ie Presentation Business and Data Integration layers in accordance with the customerclient standards Played a vital role in Scala framework for web based applications and used File net for Content Management and for streamlining Business Processes Created Responsive Layouts for multiple devices and platforms using foundation framework and implemented printable chart report using HTML CSS and jQuery Applied JavaScript for client side form validation and worked on UNIX LINUX to move the project into production environment Created Managed Beans for handling JSF pages and include logic for processing of the data on the page and Created simple user interface for applications configuration system using MVC design patterns and swing framework Used ObjectRelational mapping tool Hibernate to achieve object to database table persistency Worked with Core Java to develop automated solutions to include web interfaces using HTML CSS JavaScript and Web services Developed web GUI involving HTML Java Script under MVC architecture and creation of WebLogic domains and setup Admin Managed servers for JAVAJ2EE applications on Non Production and Production environments Implemented code according to coding standards and Created AngularJS Controller Which Isolate scopes perform operations and used GWT GUICE JavaScript and Angular JS for client side implementation and extensively used Core Java such as Exceptions and Collections Configured the Web sphere application server to connect with DB2 Oracle and SQL Server in the back end by creating JDBC data source and configured MQ Series with IBM RAD and WAS to create new connection factories and queues Extensively worked on TOAD for interacting with data base developing the stored procedures and promoting SQL changes to QA and Production Environments Used Apache Maven for project management and building the application and CVS was used for project management and version management Creating and updating existing build scripts using Ant for deployment Tested and implementeddeployed application on WAS server and used Rational Clear Case for Version Control Environment Hadoop Map Reduce Yarn Hive Pig HBase Sqoop Spark Scala MapR Core Java R Language SQL Python Eclipse Linux Unix HDFS Map Reduce Impala Cloudera SQOOP Kafka Apache Cassandra Oozie Impala Zookeeper MySQL Eclipse PLSQL JAVA DEVELOPER APLOMB Technologies Princeton NJ US May 2007 to December 2009 RESPONSIBILITIES Involved in the configuration of Spring Framework and Hibernate mapping tool and monitoring WebLogicJBoss Server health and security Creation of Connection Pools Data Sources in WebLogic console and implemented Hibernate for Database Transactions on DB2 Implemented CI CD using Jenkins for continuous development and delivery Involved in configuring hibernate to access database and retrieve data from the database and written Web Services JAXWS for external system via SOAPHTTP call Used Log4j framework to logtrack application and involved in developing SQL queries stored procedures and functions Developed a new CR screen from the existing screen for the LTL loads Low Truck Load using JSF Used spring framework configuration files to manage objects and to achieve dependency injection Implemented cross cutting concerns like logging and monitoring mechanism using Spring AOP Implemented SOA architecture with web services using SOAP WSDL UDDI and XML and made screen changes to the existing screen for the LTL Low Truck Load Accessories using Struts Developed desktop interface using Java Swing for maintaining and tracking products Used JAXWS to access the external web services get the xml response and convert it back to java objects Developed the application using Eclipse IDE and worked under Agile Environment and worked with Web admin and the admin team to configure the application on development training test and stress environments Web logic server Executed and coordinated the installation for the project and worked on webbased reporting system with HTML JavaScript and JSP Build PLSQL functions stored procedures views and configured Oracle Database with JNDI data source with connection pooling enabled Developed the Training and Appraisal modules using Java JSP Servlets and JavaScript Used Hibernate based persistence classes at data access tier and adopted J2EE design patterns like Service Locator Session Facade and Singleton Worked on Spring Core layer Spring ORM Spring AOP in developing the application components Modified web pages using JSP and Used Struts Validation Framework for form input validation Created the WSDL and used Apache Axis 20 for publishing the WSDL and creating PDF files for storing the data required for module Used custom components using JSTL tags and Tag libraries implementing struts and used Web Logic server for deploying the war files and used Toad for the DB2 database changes ENVIRONMENT Java J2EE JSF Hibernate Struts Spring SwingJFC JSP HTML XML Web Logic iText DB2 Eclipse IDE SOAP Maven JSTL TOAD DB2 JDK Web Logic Server WSDL JAXWS Apache Axis Education Bachelors Skills Apache 9 years APACHE HADOOP HDFS 8 years APACHE HADOOP SQOOP 8 years Java 10 years Oracle 9 years Javascript Additional Information Technical Skills Bigdata Ecosystem HDFS and Map Reduce Pig Hive Impala YARN HUE Oozie ZookeeperSolr Apache Spark Apache STORM Apache Kafka Sqoop Flume Flink Elasticsearch NoSQL Databases HBase Cassandra and MongoDB Hadoop Distributions Cloudera Hortonworks Programming languages Java C SCALA Pig Latin HiveQL PySpark Scripting Languages Shell Scripting Databases MySQL oracle Teradata DB2 Build Tools Maven Ant sbt Reporting Tool Tableau Version control Tools SVN Git GitHub Cloud AWS Azure S3 EC2 EMR AppWeb servers WebSphere WebLogic JBoss and Tomcat Web Design Tools HTML AJAX JavaScript JQuery CSS and JSON Operating Systems WINDOWS 108Vista XP Development IDEs NetBeans Eclipse IDE PythonIDLE Packages Microsoft Office putty MS Visual Studio",
    "extracted_keywords": [
        "SR",
        "BIGDATA",
        "DEVELOPERENGINEER",
        "SR",
        "BIGDATA",
        "span",
        "lDEVELOPERspanENGINEER",
        "SR",
        "BIGDATA",
        "DEVELOPERENGINEER",
        "MeganSoft",
        "Austin",
        "TX",
        "years",
        "experience",
        "software",
        "development",
        "experience",
        "Big",
        "Data",
        "technologies",
        "knowledge",
        "Hadoop",
        "ecosystem",
        "HDFS",
        "YARN",
        "MapReduce",
        "Experience",
        "build",
        "scripts",
        "Maven",
        "integrations",
        "systems",
        "Jenkins",
        "Expertise",
        "Kafka",
        "system",
        "realtime",
        "Streaming",
        "solutions",
        "Sqoop",
        "data",
        "transfers",
        "RDMS",
        "viceversa",
        "Experience",
        "ETL",
        "process",
        "Hadoop",
        "Designing",
        "Hive",
        "data",
        "model",
        "Pig",
        "Latin",
        "scripts",
        "data",
        "Hadoop",
        "Good",
        "Knowledge",
        "Cloudera",
        "distributions",
        "Amazon",
        "storage",
        "service",
        "Amazon",
        "S3",
        "AWS",
        "Amazon",
        "EC2",
        "Amazon",
        "EMR",
        "Experience",
        "data",
        "ingestion",
        "technologies",
        "Sqoop",
        "Flume",
        "Kafka",
        "Experience",
        "databases",
        "MySQL",
        "Oracle",
        "g",
        "Postgres",
        "NoSQL",
        "MongoDB",
        "Cassandra",
        "HBase",
        "knowledge",
        "Apache",
        "Cassandra",
        "architecture",
        "experience",
        "Cassandra",
        "data",
        "models",
        "Cassandra",
        "Query",
        "Language",
        "CQL",
        "Experience",
        "Hive",
        "QL",
        "Pig",
        "Latin",
        "scripts",
        "ETL",
        "Expertise",
        "processing",
        "data",
        "Core",
        "Spark",
        "SparkSQL",
        "Spark",
        "Streaming",
        "Experience",
        "workflows",
        "Oozie",
        "jobs",
        "Hadoop",
        "Expertise",
        "Core",
        "Java",
        "exception",
        "Generics",
        "garbage",
        "collection",
        "Collections",
        "lambda",
        "expressions",
        "serialization",
        "deserialization",
        "Experience",
        "writing",
        "unit",
        "tests",
        "JUnit",
        "Mockito",
        "frameworks",
        "Good",
        "Knowledge",
        "Amazon",
        "Web",
        "Service",
        "AWS",
        "concepts",
        "EMR",
        "EC2",
        "web",
        "services",
        "processing",
        "Teradata",
        "Big",
        "Data",
        "Analytics",
        "Expertise",
        "Data",
        "Development",
        "Hortonworks",
        "HDP",
        "platform",
        "Hadoop",
        "ecosystem",
        "tools",
        "Hadoop",
        "HDFS",
        "Spark",
        "Zeppelin",
        "Hive",
        "HBase",
        "SQOOP",
        "Atlas",
        "SOLR",
        "Pig",
        "Falcon",
        "Oozie",
        "Hue",
        "Tez",
        "Kafka",
        "experience",
        "Apache",
        "Spark",
        "Spark",
        "Streaming",
        "Spark",
        "SQL",
        "SQL",
        "Cassandra",
        "Hbase",
        "Expert",
        "Amazon",
        "EMR",
        "Spark",
        "Kinesis",
        "S3",
        "Boto3",
        "Bean",
        "Stalk",
        "ECS",
        "Cloud",
        "Lambda",
        "ELB",
        "VPC",
        "Elastic",
        "Cache",
        "Dynamo",
        "DB",
        "Redshit",
        "RDS",
        "Aethna",
        "Zeppelin",
        "Airflow",
        "knowledge",
        "Hadoop",
        "clusters",
        "onAmazon",
        "EC2",
        "VM",
        "HortonworksData",
        "Platform",
        "CDH3",
        "CDH4Cloudera",
        "Manager",
        "Linux",
        "Ubuntu",
        "Expertise",
        "Java",
        "Script",
        "JavaScript",
        "MVC",
        "Object",
        "Oriented",
        "JavaScript",
        "Design",
        "Patterns",
        "AJAX",
        "core",
        "modules",
        "crossplatform",
        "applications",
        "JSP",
        "Servlets",
        "JDBC",
        "JavaScript",
        "XML",
        "HTML",
        "Zookeeper",
        "cluster",
        "Experience",
        "Hadoop",
        "distributions",
        "Cloudera",
        "Hortonworks",
        "AWS",
        "Experience",
        "VMWare",
        "VirtualBox",
        "Docker",
        "Vagrant",
        "Experience",
        "Java",
        "SE",
        "Java",
        "EE",
        "frameworks",
        "Spring",
        "MVC",
        "Spring",
        "Indepth",
        "understanding",
        "Spark",
        "Architecture",
        "Spark",
        "Core",
        "Spark",
        "SQL",
        "Data",
        "Frames",
        "Spark",
        "Streaming",
        "Spark",
        "MLlib",
        "Expertise",
        "Spark",
        "RDD",
        "transformations",
        "actions",
        "Data",
        "Frames",
        "case",
        "classes",
        "input",
        "data",
        "data",
        "transformations",
        "SparkCore",
        "knowledge",
        "Hadoop",
        "Architecture",
        "components",
        "YARN",
        "HDFS",
        "NodeManager",
        "ResourceManager",
        "JobTracker",
        "TaskTracker",
        "NameNode",
        "DataNode",
        "MapReduce",
        "knowledge",
        "NOSQL",
        "column",
        "databases",
        "HBase",
        "Cassandra",
        "MongoDB",
        "integration",
        "Hadoop",
        "cluster",
        "Experience",
        "installation",
        "configuration",
        "HortonworksClouderas",
        "Hadoop",
        "platform",
        "CDH34",
        "clusters",
        "SQL",
        "skills",
        "SQL",
        "queries",
        "functions",
        "triggers",
        "procedures",
        "Backend",
        "testing",
        "Database",
        "Testing",
        "EndtoEnd",
        "Hadoop",
        "cluster",
        "Azure",
        "HD",
        "Insight",
        "Platform",
        "Data",
        "solutions",
        "tools",
        "Spark",
        "BI",
        "reporting",
        "tools",
        "US",
        "employer",
        "Work",
        "Experience",
        "SR",
        "BIGDATA",
        "DEVELOPERENGINEER",
        "MeganSoft",
        "Dearborn",
        "MI",
        "January",
        "RESPONSIBILITIES",
        "data",
        "sets",
        "sources",
        "BDpaas",
        "HADOOP",
        "environment",
        "Spark",
        "UNIX",
        "scripts",
        "Batch",
        "load",
        "amount",
        "data",
        "Relational",
        "databases",
        "DATA",
        "platform",
        "Delivery",
        "experience",
        "Hadoop",
        "ecosystem",
        "Components",
        "Pig",
        "Hive",
        "Spark",
        "Kafka",
        "Elastic",
        "Search",
        "HBase",
        "Cloudera",
        "Manager",
        "AWS",
        "Data",
        "Pipeline",
        "Amazon",
        "EMR",
        "cluster",
        "process",
        "web",
        "server",
        "logs",
        "Amazon",
        "S3",
        "bucket",
        "system",
        "requirements",
        "role",
        "highlevel",
        "design",
        "implementation",
        "application",
        "applications",
        "Maven",
        "tool",
        "jar",
        "files",
        "application",
        "dependencies",
        "pomXML",
        "file",
        "JSF",
        "Struts",
        "frameworks",
        "end",
        "framework",
        "client",
        "side",
        "components",
        "J2EE",
        "components",
        "Eclipse",
        "IDE",
        "Machine",
        "learning",
        "algorithms",
        "Spark",
        "Python",
        "Spark",
        "Storm",
        "Apache",
        "Apex",
        "python",
        "data",
        "sources",
        "Metafiles",
        "files",
        "data",
        "Data",
        "Lake",
        "configuring",
        "batch",
        "job",
        "ingestion",
        "source",
        "Data",
        "Lake",
        "Pig",
        "queries",
        "data",
        "HBase",
        "Leveraged",
        "Hive",
        "tables",
        "HIVE",
        "scripts",
        "analyst",
        "requirements",
        "analysis",
        "Kafka",
        "consumers",
        "data",
        "Kafka",
        "partitions",
        "Cassandra",
        "analysis",
        "Hive",
        "alter",
        "tables",
        "hive",
        "queries",
        "workflow",
        "Oozie",
        "tasks",
        "data",
        "HDFS",
        "Pig",
        "highlevel",
        "design",
        "spec",
        "ETL",
        "mapping",
        "standards",
        "HBase",
        "tables",
        "top",
        "data",
        "Data",
        "Lake",
        "Hive",
        "tables",
        "Blobs",
        "data",
        "Hive",
        "Meta",
        "Store",
        "requirement",
        "design",
        "phase",
        "Streaming",
        "Architecture",
        "time",
        "streaming",
        "Spark",
        "Kafka",
        "Use",
        "Spark",
        "API",
        "Machine",
        "learning",
        "model",
        "SAS",
        "code",
        "Spark",
        "Spark",
        "API",
        "Cloudera",
        "Hadoop",
        "YARN",
        "analytics",
        "data",
        "Hive",
        "Created",
        "Reports",
        "Selection",
        "Criteria",
        "Hive",
        "Tables",
        "data",
        "Data",
        "Lake",
        "Worked",
        "Hadoop",
        "Architecture",
        "components",
        "YARN",
        "HDFS",
        "NodeManager",
        "Resource",
        "Manager",
        "JobTracker",
        "TaskTracker",
        "NameNode",
        "DataNode",
        "MapReduce",
        "concepts",
        "Deployed",
        "Hadoop",
        "components",
        "Cluster",
        "Hive",
        "HBase",
        "Spark",
        "Scala",
        "others",
        "respect",
        "requirement",
        "terabytes",
        "data",
        "sources",
        "HDFS",
        "AWS",
        "cloud",
        "Sqoop",
        "Business",
        "Rules",
        "Spark",
        "SCALA",
        "business",
        "logic",
        "place",
        "Rating",
        "Engine",
        "Spark",
        "UI",
        "running",
        "Spark",
        "Job",
        "node",
        "level",
        "Spark",
        "Property",
        "Bag",
        "Parsing",
        "data",
        "fields",
        "data",
        "ETL",
        "methodology",
        "Data",
        "Extraction",
        "transformations",
        "loading",
        "processing",
        "Hadoop",
        "Hive",
        "context",
        "SQL",
        "context",
        "Spark",
        "testing",
        "Spark",
        "job",
        "WINSCP",
        "FTP",
        "data",
        "storage",
        "structure",
        "server",
        "JARs",
        "Spark",
        "Submit",
        "Developed",
        "code",
        "scratch",
        "Spark",
        "SCALA",
        "requirements",
        "Environment",
        "Hadoop",
        "Hive",
        "HDFS",
        "Pig",
        "Sqoop",
        "Python",
        "SparkSQL",
        "Machine",
        "Learning",
        "MongoDB",
        "AWS",
        "S3",
        "AWS",
        "EC2",
        "AWS",
        "EMR",
        "Oozie",
        "ETL",
        "Tableau",
        "Spark",
        "SparkStreaming",
        "KAFKA",
        "Netezza",
        "Apache",
        "Solr",
        "Cassandra",
        "Cloudera",
        "Distribution",
        "Java",
        "Impala",
        "Web",
        "Servers",
        "Maven",
        "Build",
        "MySQL",
        "Grafana",
        "AWS",
        "AgileScrum",
        "SR",
        "BIGDATA",
        "ENGINEER",
        "DEVELOPER",
        "Prabhav",
        "Services",
        "Inc",
        "Newark",
        "NJ",
        "September",
        "December",
        "RESPONSIBILITIES",
        "Spark",
        "performance",
        "optimization",
        "algorithms",
        "Hadoop",
        "Spark",
        "Context",
        "Spark",
        "SQL",
        "Data",
        "Frame",
        "Spark",
        "Yarn",
        "cloud",
        "computing",
        "infrastructure",
        "eg",
        "Amazon",
        "Web",
        "Services",
        "EC2",
        "considerations",
        "systems",
        "Gocd",
        "cicd",
        "tool",
        "application",
        "experience",
        "Munin",
        "frame",
        "work",
        "data",
        "Testing",
        "file",
        "movements",
        "HDFS",
        "AWS",
        "S3",
        "S3",
        "bucket",
        "AWS",
        "Hadoop",
        "jobs",
        "EMR",
        "cluster",
        "data",
        "size",
        "policies",
        "procedures",
        "GIS",
        "technologies",
        "servers",
        "workstations",
        "storage",
        "server",
        "virtualization",
        "GIS",
        "data",
        "security",
        "web",
        "maps",
        "service",
        "Avro",
        "Parquet",
        "files",
        "data",
        "format",
        "Semi",
        "JSON",
        "data",
        "Parquet",
        "Data",
        "Frames",
        "Spark",
        "HiveSQL",
        "queries",
        "Spark",
        "Transformations",
        "Spark",
        "RDDs",
        "Scala",
        "SQOOP",
        "data",
        "RDBMS",
        "HDFS",
        "data",
        "sources",
        "AWS",
        "S3",
        "file",
        "system",
        "Spark",
        "RDD",
        "Collected",
        "data",
        "Spark",
        "Streaming",
        "AWS",
        "S3",
        "bucket",
        "nearrealtime",
        "Transformations",
        "Aggregations",
        "fly",
        "learner",
        "data",
        "model",
        "data",
        "HDFS",
        "database",
        "legacy",
        "labels",
        "HDFS",
        "HBASE",
        "tables",
        "Sqoop",
        "vice",
        "versa",
        "web",
        "server",
        "logs",
        "Multihop",
        "flume",
        "agents",
        "Avro",
        "Sink",
        "analysis",
        "MongoDB",
        "NoSQL",
        "data",
        "modeling",
        "disaster",
        "recovery",
        "data",
        "pipeline",
        "Spark",
        "Hive",
        "HBase",
        "customer",
        "data",
        "histories",
        "Hadoop",
        "cluster",
        "analysis",
        "Python",
        "Script",
        "CSV",
        "files",
        "S3",
        "buckets",
        "AWS",
        "S3",
        "buckets",
        "folder",
        "management",
        "bucket",
        "logs",
        "bucket",
        "file",
        "formats",
        "JSon",
        "AVRO",
        "parquet",
        "compression",
        "techniques",
        "code",
        "tasks",
        "dependencies",
        "SLA",
        "watcher",
        "time",
        "sensor",
        "job",
        "workflow",
        "management",
        "automation",
        "Airflow",
        "tool",
        "shell",
        "scripts",
        "partitions",
        "stage",
        "table",
        "JSON",
        "schema",
        "change",
        "source",
        "files",
        "files",
        "source",
        "location",
        "Monitor",
        "Troubleshoot",
        "Hadoop",
        "jobs",
        "Yarn",
        "Resource",
        "Manager",
        "EMR",
        "job",
        "logs",
        "Genie",
        "Kibana",
        "CICD",
        "Automation",
        "tools",
        "Jenkins",
        "Salt",
        "Git",
        "Vagrant",
        "Docker",
        "Elastic",
        "Search",
        "Grafana",
        "Data",
        "Management",
        "Data",
        "Access",
        "Data",
        "Governance",
        "Integration",
        "Security",
        "Operations",
        "Hortonworks",
        "Data",
        "Platform",
        "HDP",
        "metadata",
        "Hive",
        "Python",
        "tables",
        "applications",
        "AWS",
        "cloud",
        "S3",
        "writing",
        "scripts",
        "Oracle",
        "SQL",
        "Server",
        "Netezza",
        "data",
        "reporting",
        "analysis",
        "cleansing",
        "data",
        "sources",
        "DB2",
        "Oracle",
        "files",
        "SQL",
        "Server",
        "volume",
        "data",
        "metadata",
        "Hive",
        "tables",
        "applications",
        "Hive",
        "AWS",
        "cloud",
        "data",
        "Athena",
        "Snowflake",
        "Stash",
        "GitBucket",
        "Code",
        "Control",
        "AWS",
        "Components",
        "Airflow",
        "Elastic",
        "Map",
        "EMR",
        "Athena",
        "SnowFlake",
        "FileNet",
        "IBM",
        "RAD",
        "Scala",
        "Java",
        "JSP",
        "Servlets",
        "Core",
        "Java",
        "Spring",
        "Swing",
        "Hibernate",
        "JSF",
        "ICE",
        "Hibernate",
        "HTML",
        "CSS",
        "Jenkins",
        "JavaScript",
        "UNIX",
        "Web",
        "Services",
        "SOAP",
        "XML",
        "IBM",
        "WebSphere",
        "Rational",
        "Clear",
        "Case",
        "Log",
        "IBM",
        "DB2",
        "HADOOP",
        "DEVELOPER",
        "Infinite",
        "Computer",
        "Solutions",
        "Irving",
        "TX",
        "US",
        "July",
        "August",
        "RESPONSIBILITIES",
        "Hadoop",
        "practices",
        "knowledge",
        "solutions",
        "design",
        "patterns",
        "code",
        "mediumcomplex",
        "applications",
        "Hadoop",
        "production",
        "Wrote",
        "Spark",
        "applications",
        "Data",
        "validation",
        "cleansing",
        "transformations",
        "custom",
        "aggregations",
        "data",
        "sources",
        "Spark",
        "RDD",
        "processing",
        "custom",
        "aggregate",
        "functions",
        "Spark",
        "SQL",
        "data",
        "pipeline",
        "Amazon",
        "AWS",
        "data",
        "weblogs",
        "HDFS",
        "data",
        "pipeline",
        "creation",
        "data",
        "format",
        "data",
        "analysis",
        "visualization",
        "Migrate",
        "databases",
        "share",
        "data",
        "processing",
        "logic",
        "web",
        "apps",
        "jobs",
        "APIs",
        "XML",
        "CSV",
        "files",
        "data",
        "pipelines",
        "kafka",
        "batch",
        "jobs",
        "data",
        "data",
        "pipeline",
        "Pig",
        "Sqoop",
        "cargo",
        "data",
        "customer",
        "histories",
        "HDFS",
        "analysis",
        "Developed",
        "Pig",
        "scripts",
        "analytics",
        "XML",
        "data",
        "Hive",
        "tables",
        "partitions",
        "bucketing",
        "tables",
        "efficiency",
        "Hive",
        "QL",
        "data",
        "metrics",
        "reporting",
        "data",
        "transformations",
        "MapReduce",
        "Pig",
        "jobs",
        "business",
        "requirements",
        "Apache",
        "Kafka",
        "web",
        "log",
        "data",
        "servers",
        "Downstream",
        "systems",
        "analysis",
        "Kafka",
        "Streams",
        "Spark",
        "streaming",
        "information",
        "HDFS",
        "Design",
        "Architecture",
        "data",
        "pipelineingestion",
        "optimization",
        "ETL",
        "workflows",
        "data",
        "pipelines",
        "SyllabusCurriculum",
        "Web",
        "Services",
        "HBASE",
        "Hive",
        "data",
        "analysis",
        "feature",
        "selection",
        "feature",
        "extraction",
        "Apache",
        "Spark",
        "Machine",
        "Learning",
        "streaming",
        "libraries",
        "Python",
        "development",
        "implementation",
        "Geographic",
        "Information",
        "Systems",
        "GIS",
        "program",
        "AWSs",
        "EMR",
        "Clusters",
        "Amazon",
        "IAM",
        "access",
        "AWS",
        "resources",
        "users",
        "Enable",
        "configure",
        "Hadoop",
        "services",
        "HDFS",
        "YARN",
        "Hive",
        "Ranger",
        "Hbase",
        "Kafka",
        "Sqoop",
        "Zeppeline",
        "Notebook",
        "SparkSpark2",
        "log",
        "data",
        "errors",
        "Apache",
        "Spark",
        "Evaluate",
        "algorithms",
        "text",
        "summarization",
        "Python",
        "Keras",
        "TensorFlow",
        "Theano",
        "Cloudera",
        "Hadoop",
        "system",
        "Database",
        "Schema",
        "Data",
        "Model",
        "realtime",
        "Tick",
        "Data",
        "NoSQL",
        "store",
        "time",
        "data",
        "Kafka",
        "streaming",
        "DStreams",
        "RDD",
        "Cassandra",
        "AWS",
        "Data",
        "Pipeline",
        "Amazon",
        "EMR",
        "cluster",
        "process",
        "web",
        "server",
        "logs",
        "Amazon",
        "S3",
        "bucket",
        "DataStax",
        "SparkCassandra",
        "connector",
        "data",
        "Cassandra",
        "CQL",
        "data",
        "Cassandra",
        "tables",
        "grouping",
        "log",
        "data",
        "errors",
        "Apache",
        "Spark",
        "MapReduce",
        "programs",
        "Spark",
        "transformations",
        "Spark",
        "Scala",
        "python",
        "PySpark",
        "Worked",
        "deal",
        "Amazon",
        "Web",
        "Services",
        "AWS",
        "Cloud",
        "services",
        "EC2",
        "S3",
        "EBS",
        "RDS",
        "VPC",
        "Integrated",
        "MapReduce",
        "HBase",
        "bulk",
        "amount",
        "data",
        "HBase",
        "MapReduce",
        "programs",
        "Impala",
        "Written",
        "Queries",
        "Data",
        "Hive",
        "tables",
        "MapReduce",
        "jobs",
        "Java",
        "API",
        "Apache",
        "SOLR",
        "indexing",
        "Custom",
        "SOLR",
        "query",
        "segments",
        "search",
        "kafka",
        "streaming",
        "data",
        "pipelines",
        "data",
        "source",
        "transformations",
        "scala",
        "Data",
        "Pipeline",
        "data",
        "sources",
        "Web",
        "RDBMS",
        "NoSQL",
        "Apache",
        "Kafka",
        "Spark",
        "cluster",
        "xmls",
        "tag",
        "information",
        "xpaths",
        "Scala",
        "XML",
        "libraries",
        "blob",
        "data",
        "types",
        "Data",
        "Lake",
        "customers",
        "Big",
        "Data",
        "data",
        "sources",
        "Hadoop",
        "HDFS",
        "data",
        "Excel",
        "Flat",
        "Files",
        "Oracle",
        "SQL",
        "Server",
        "MongoDb",
        "Cassandra",
        "HBase",
        "Teradata",
        "Netezza",
        "data",
        "servers",
        "Define",
        "data",
        "governance",
        "rules",
        "rights",
        "job",
        "profile",
        "users",
        "Developed",
        "Pig",
        "Hive",
        "UDFs",
        "business",
        "logic",
        "data",
        "requirements",
        "Pig",
        "UDFs",
        "Java",
        "UDFs",
        "PiggyBank",
        "data",
        "Spark",
        "scripts",
        "Scala",
        "IDEas",
        "business",
        "requirement",
        "Configured",
        "Cassandra",
        "cluster",
        "realtime",
        "application",
        "Cassandra",
        "database",
        "file",
        "movements",
        "HDFS",
        "AWS",
        "S3",
        "S3",
        "bucket",
        "AWS",
        "Spark",
        "jobs",
        "Scala",
        "top",
        "Batch",
        "Analysis",
        "data",
        "SparkSQL",
        "top",
        "Spark",
        "engine",
        "data",
        "sets",
        "processing",
        "Spark",
        "Framework",
        "Java",
        "Web",
        "Frame",
        "work",
        "Hive",
        "tables",
        "data",
        "Hive",
        "queries",
        "market",
        "analysts",
        "trends",
        "data",
        "EDW",
        "reference",
        "tables",
        "metrics",
        "Spark",
        "code",
        "Scala",
        "SparkSQL",
        "processing",
        "testing",
        "HiveQL",
        "queries",
        "Hive",
        "tables",
        "Nagios",
        "Grafana",
        "Graphite",
        "dashboard",
        "infrastructure",
        "Environment",
        "Spark",
        "AWS",
        "EC2",
        "EMR",
        "Hive",
        "SQL",
        "Workbench",
        "Genie",
        "Logs",
        "Kibana",
        "Sqoop",
        "Spark",
        "SQL",
        "Spark",
        "Streaming",
        "Scala",
        "Python",
        "Hadoop",
        "Cloudera",
        "Stack",
        "Hue",
        "Spark",
        "Netezza",
        "Kafka",
        "HBase",
        "HDFS",
        "Hive",
        "Pig",
        "Sqoop",
        "Oracle",
        "ETL",
        "AWS",
        "S3",
        "AWS",
        "EMR",
        "GIT",
        "Grafana",
        "SR",
        "DEVELOPER",
        "HiTech",
        "Solutions",
        "Inc",
        "Chicago",
        "IL",
        "US",
        "January",
        "June",
        "RESPONSIBILITIES",
        "design",
        "development",
        "phases",
        "Agile",
        "Software",
        "Development",
        "Mainframe",
        "system",
        "GUI",
        "screens",
        "Sqoop",
        "DBMS",
        "Python",
        "logs",
        "client",
        "data",
        "centers",
        "Develop",
        "Python",
        "scripts",
        "automation",
        "Map",
        "Reduce",
        "jobs",
        "Java",
        "API",
        "Python",
        "Spark",
        "Imported",
        "data",
        "RDBMS",
        "systems",
        "MySQL",
        "HDFS",
        "Sqoop",
        "Sqoop",
        "jobs",
        "imports",
        "Hive",
        "tables",
        "experience",
        "collection",
        "data",
        "understanding",
        "data",
        "systems",
        "policies",
        "compilation",
        "information",
        "coordination",
        "data",
        "GIS",
        "program",
        "implementation",
        "policies",
        "loading",
        "transforming",
        "sets",
        "data",
        "Data",
        "Pipelines",
        "business",
        "requirements",
        "Oozie",
        "Coordinators",
        "application",
        "Tier",
        "Architecture",
        "Presentation",
        "Business",
        "Data",
        "Integration",
        "layers",
        "accordance",
        "customerclient",
        "standards",
        "role",
        "Scala",
        "framework",
        "web",
        "applications",
        "File",
        "net",
        "Content",
        "Management",
        "Business",
        "Processes",
        "Responsive",
        "Layouts",
        "devices",
        "platforms",
        "foundation",
        "framework",
        "chart",
        "report",
        "HTML",
        "CSS",
        "jQuery",
        "Applied",
        "JavaScript",
        "client",
        "side",
        "form",
        "validation",
        "UNIX",
        "LINUX",
        "project",
        "production",
        "environment",
        "Managed",
        "Beans",
        "JSF",
        "pages",
        "logic",
        "processing",
        "data",
        "page",
        "user",
        "interface",
        "applications",
        "configuration",
        "system",
        "MVC",
        "design",
        "patterns",
        "swing",
        "framework",
        "mapping",
        "tool",
        "Hibernate",
        "object",
        "database",
        "table",
        "persistency",
        "Core",
        "Java",
        "solutions",
        "web",
        "interfaces",
        "HTML",
        "CSS",
        "JavaScript",
        "Web",
        "services",
        "web",
        "GUI",
        "HTML",
        "Java",
        "Script",
        "MVC",
        "architecture",
        "creation",
        "WebLogic",
        "domains",
        "setup",
        "Admin",
        "Managed",
        "servers",
        "JAVAJ2EE",
        "applications",
        "Non",
        "Production",
        "Production",
        "environments",
        "code",
        "standards",
        "Controller",
        "Isolate",
        "scopes",
        "operations",
        "GWT",
        "GUICE",
        "JavaScript",
        "Angular",
        "JS",
        "client",
        "side",
        "implementation",
        "Core",
        "Java",
        "Exceptions",
        "Collections",
        "Web",
        "sphere",
        "application",
        "server",
        "DB2",
        "Oracle",
        "SQL",
        "Server",
        "end",
        "JDBC",
        "data",
        "source",
        "MQ",
        "Series",
        "IBM",
        "RAD",
        "connection",
        "factories",
        "queues",
        "TOAD",
        "data",
        "base",
        "procedures",
        "SQL",
        "changes",
        "QA",
        "Production",
        "Environments",
        "Apache",
        "Maven",
        "project",
        "management",
        "application",
        "CVS",
        "project",
        "management",
        "version",
        "management",
        "build",
        "scripts",
        "Ant",
        "deployment",
        "application",
        "server",
        "Rational",
        "Clear",
        "Case",
        "Version",
        "Control",
        "Environment",
        "Hadoop",
        "Map",
        "Reduce",
        "Yarn",
        "Hive",
        "Pig",
        "HBase",
        "Sqoop",
        "Spark",
        "Scala",
        "MapR",
        "Core",
        "Java",
        "R",
        "Language",
        "SQL",
        "Python",
        "Eclipse",
        "Linux",
        "Unix",
        "HDFS",
        "Map",
        "Reduce",
        "Impala",
        "Cloudera",
        "SQOOP",
        "Kafka",
        "Apache",
        "Cassandra",
        "Oozie",
        "Impala",
        "Zookeeper",
        "MySQL",
        "Eclipse",
        "PLSQL",
        "DEVELOPER",
        "APLOMB",
        "Technologies",
        "Princeton",
        "NJ",
        "US",
        "May",
        "December",
        "RESPONSIBILITIES",
        "configuration",
        "Spring",
        "Framework",
        "Hibernate",
        "mapping",
        "tool",
        "WebLogicJBoss",
        "Server",
        "health",
        "security",
        "Creation",
        "Connection",
        "Pools",
        "Data",
        "Sources",
        "WebLogic",
        "console",
        "Hibernate",
        "Database",
        "Transactions",
        "DB2",
        "CI",
        "CD",
        "Jenkins",
        "development",
        "delivery",
        "hibernate",
        "database",
        "retrieve",
        "data",
        "database",
        "Web",
        "Services",
        "JAXWS",
        "system",
        "SOAPHTTP",
        "Log4j",
        "framework",
        "application",
        "SQL",
        "queries",
        "procedures",
        "functions",
        "CR",
        "screen",
        "screen",
        "LTL",
        "Low",
        "Truck",
        "Load",
        "JSF",
        "spring",
        "framework",
        "configuration",
        "files",
        "objects",
        "dependency",
        "injection",
        "cross",
        "concerns",
        "mechanism",
        "Spring",
        "AOP",
        "SOA",
        "architecture",
        "web",
        "services",
        "SOAP",
        "WSDL",
        "UDDI",
        "XML",
        "screen",
        "changes",
        "screen",
        "LTL",
        "Low",
        "Truck",
        "Load",
        "Accessories",
        "Struts",
        "desktop",
        "interface",
        "Java",
        "Swing",
        "tracking",
        "products",
        "JAXWS",
        "web",
        "services",
        "xml",
        "response",
        "java",
        "application",
        "Eclipse",
        "IDE",
        "Agile",
        "Environment",
        "Web",
        "admin",
        "admin",
        "team",
        "application",
        "development",
        "training",
        "test",
        "stress",
        "Web",
        "logic",
        "server",
        "installation",
        "project",
        "reporting",
        "system",
        "HTML",
        "JavaScript",
        "JSP",
        "Build",
        "PLSQL",
        "functions",
        "procedures",
        "views",
        "Oracle",
        "Database",
        "JNDI",
        "data",
        "source",
        "connection",
        "pooling",
        "Developed",
        "Training",
        "Appraisal",
        "modules",
        "Java",
        "JSP",
        "Servlets",
        "JavaScript",
        "Hibernate",
        "persistence",
        "classes",
        "data",
        "access",
        "tier",
        "J2EE",
        "design",
        "patterns",
        "Service",
        "Locator",
        "Session",
        "Facade",
        "Singleton",
        "Spring",
        "Core",
        "layer",
        "Spring",
        "ORM",
        "Spring",
        "AOP",
        "application",
        "components",
        "Modified",
        "web",
        "pages",
        "JSP",
        "Struts",
        "Validation",
        "Framework",
        "form",
        "input",
        "validation",
        "WSDL",
        "Apache",
        "Axis",
        "WSDL",
        "PDF",
        "files",
        "data",
        "module",
        "custom",
        "components",
        "JSTL",
        "tags",
        "Tag",
        "struts",
        "Web",
        "Logic",
        "server",
        "war",
        "files",
        "Toad",
        "DB2",
        "database",
        "ENVIRONMENT",
        "Java",
        "J2EE",
        "JSF",
        "Hibernate",
        "Struts",
        "Spring",
        "SwingJFC",
        "JSP",
        "HTML",
        "XML",
        "Web",
        "Logic",
        "iText",
        "DB2",
        "Eclipse",
        "IDE",
        "SOAP",
        "Maven",
        "JSTL",
        "TOAD",
        "DB2",
        "JDK",
        "Web",
        "Logic",
        "Server",
        "WSDL",
        "JAXWS",
        "Apache",
        "Axis",
        "Education",
        "Bachelors",
        "Skills",
        "Apache",
        "years",
        "APACHE",
        "HADOOP",
        "HDFS",
        "years",
        "APACHE",
        "HADOOP",
        "SQOOP",
        "years",
        "Java",
        "years",
        "Oracle",
        "years",
        "Javascript",
        "Additional",
        "Information",
        "Technical",
        "Skills",
        "Bigdata",
        "Ecosystem",
        "HDFS",
        "Map",
        "Reduce",
        "Pig",
        "Hive",
        "Impala",
        "YARN",
        "HUE",
        "Oozie",
        "ZookeeperSolr",
        "Apache",
        "Spark",
        "Apache",
        "STORM",
        "Apache",
        "Kafka",
        "Sqoop",
        "Flume",
        "Flink",
        "Elasticsearch",
        "NoSQL",
        "HBase",
        "Cassandra",
        "MongoDB",
        "Hadoop",
        "Distributions",
        "Cloudera",
        "Hortonworks",
        "Programming",
        "languages",
        "Java",
        "C",
        "SCALA",
        "Pig",
        "Latin",
        "HiveQL",
        "PySpark",
        "Scripting",
        "Languages",
        "Shell",
        "Scripting",
        "MySQL",
        "oracle",
        "Teradata",
        "DB2",
        "Build",
        "Tools",
        "Maven",
        "Ant",
        "sbt",
        "Reporting",
        "Tool",
        "Tableau",
        "Version",
        "control",
        "Tools",
        "SVN",
        "Git",
        "GitHub",
        "Cloud",
        "S3",
        "EC2",
        "EMR",
        "AppWeb",
        "servers",
        "WebSphere",
        "WebLogic",
        "JBoss",
        "Tomcat",
        "Web",
        "Design",
        "Tools",
        "HTML",
        "AJAX",
        "JavaScript",
        "JQuery",
        "CSS",
        "JSON",
        "Operating",
        "Systems",
        "WINDOWS",
        "108Vista",
        "XP",
        "Development",
        "IDEs",
        "NetBeans",
        "IDE",
        "PythonIDLE",
        "Packages",
        "Microsoft",
        "Office",
        "putty",
        "MS",
        "Visual",
        "Studio"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T20:48:50.263528",
    "resume_data": "SR BIGDATA DEVELOPERENGINEER SR BIGDATA span lDEVELOPERspanENGINEER SR BIGDATA DEVELOPERENGINEER MeganSoft Austin TX Around 11 years of overall experience in software development with extensive experience with Big Data technologies Good knowledge of Hadoop ecosystem such as HDFS YARN and MapReduce Experience in build scripts using Maven and do continuous integrations systems like Jenkins Expertise in using Kafka as a messaging system to implement realtime Streaming solutions and implemented Sqoop for large data transfers from RDMS to HDFSHBaseHive and viceversa Experience in migrating ETL process into Hadoop Designing Hive data model and wrote Pig Latin scripts to load data into Hadoop Good Knowledge on Cloudera distributions and in Amazon simple storage service Amazon S3 AWS and Amazon EC2 Amazon EMR Experience working with structured semistructured and unstructured data ingestion technologies such as Sqoop Flume and Kafka Experience working with relational databases such as MySQL 57 Oracle 10g and Postgres 96 and NoSQL databases such as MongoDB Cassandra and HBase Indepth knowledge of Apache Cassandra architecture and extensive experience designing Cassandra data models and working with Cassandra Query Language CQL Experience writing Hive QL queries and Pig Latin scripts for ETL Expertise in processing and analyzing archived and realtime data using Core Spark SparkSQL and Spark Streaming Experience writing workflows in Oozie to schedule various jobs on Hadoop and with Expertise in Core Java features such as multithreading exception handling Generics garbage collection Collections lambda expressions serialization and deserialization Experience with writing unit tests using JUnit and Mockito frameworks Good Knowledge in Amazon Web Service AWS concepts like EMR and EC2 web services which provides fast and efficient processing of Teradata Big Data Analytics Expertise in Data Development in Hortonworks HDP platform Hadoop ecosystem tools like Hadoop HDFS Spark Zeppelin Hive HBase SQOOP flume Atlas SOLR Pig Falcon Oozie Hue Tez ApacheNiFi Kafka Have very good experience in Apache Spark Spark Streaming Spark SQL and No SQL databases like Cassandra and Hbase Expert in Amazon EMR Spark Kinesis S3 Boto3 Bean Stalk ECS Cloud watch Lambda ELB VPC Elastic Cache Dynamo DB Redshit RDS Aethna Zeppelin Airflow Strong knowledge on creating and monitoring Hadoop clusters onAmazon EC2 VM HortonworksData Platform 21 22 CDH3 CDH4Cloudera Manager on Linux Ubuntu OS etc Expertise in Java Script JavaScript MVC patterns Object Oriented JavaScript Design Patterns and AJAX and developed core modules in large crossplatform applications using JAVA JSP Servlets JDBC JavaScript XML and HTML Zookeeper for managing and coordinating the cluster Experience with multiple Hadoop distributions such as Cloudera Hortonworks and AWS Experience with VMWare VirtualBox Docker and Vagrant Experience with Java SE 8 and Java EE frameworks such as Spring MVC 40 Spring Indepth understanding of Spark Architecture including Spark Core Spark SQL Data Frames Spark Streaming Spark MLlib Expertise in writing Spark RDD transformations actions Data Frames case classes for the required input data and performed the data transformations using SparkCore Good knowledge of Hadoop Architecture and various components such as YARN HDFS NodeManager ResourceManager JobTracker TaskTracker NameNode DataNode and MapReduce concepts Strong knowledge in NOSQL column oriented databases like HBase Cassandra MongoDB and its integration with Hadoop cluster Experience in installation configuration supporting and managing HortonworksClouderas Hadoop platform along with CDH34 clusters Solid SQL skills can write complex SQL queries functions triggers and stored procedures for Backend testing Database Testing and EndtoEnd testing Experienced on Hadoop cluster on Azure HD Insight Platform and deployed Data analytic solutions using tools like Spark and BI reporting tools Authorized to work in the US for any employer Work Experience SR BIGDATA DEVELOPERENGINEER MeganSoft Dearborn MI January 2017 to Present RESPONSIBILITIES Worked on loading disparate data sets coming from different sources to BDpaas HADOOP environment using Spark Developed UNIX scripts in creating Batch load for bringing huge amount of data from Relational databases to BIG DATA platform Delivery experience on major Hadoop ecosystem Components such as Pig Hive Spark Kafka Elastic Search HBase and monitoring with Cloudera Manager Used AWS Data Pipeline to schedule an Amazon EMR cluster to clean and process web server logs stored in Amazon S3 bucket Involved in gathering and analyzing system requirements and played key role in the highlevel design for the implementation of this application Mavenized the existing applications using Maven tool and added the required jar files to the application as dependencies to the pomXML file and used JSF Struts frameworks to interact with the front end Utilized SwingJFC framework to develop client side components and developed J2EE components on Eclipse IDE Implemented the Machine learning algorithms using Spark with Python and worked on Spark Storm Apache and Apex and python Involved in analyzing data coming from various sources and creating Metafiles and control files to ingest the data in to the Data Lake Involved in configuring batch job to perform ingestion of the source files in to the Data Lake and developed Pig queries to load data to HBase Leveraged Hive queries to create ORC tables and developed HIVE scripts for analyst requirements for analysis Implemented Kafka consumers to move data from Kafka partitions into Cassandra for near realtime analysis and worked extensively on Hive to create alter and drop tables and involved in writing hive queries Developed workflow in Oozie to automate the tasks of loading the data into HDFS and preprocessing with Pig and parsed highlevel design spec to simple ETL coding and mapping standards Created and altered HBase tables on top of data residing in Data Lake and Created external Hive tables on the Blobs to showcase the data to the Hive Meta Store Involved in requirement and design phase to implement Streaming Architecture to use real time streaming using Spark and Kafka Use Spark API for Machine learning Translate a predictive model from SAS code to Spark and used Spark API over Cloudera Hadoop YARN to perform analytics on data in Hive Created Reports with different Selection Criteria from Hive Tables on the data residing in Data Lake Worked on Hadoop Architecture and various components such as YARN HDFS NodeManager Resource Manager JobTracker TaskTracker NameNode DataNode and MapReduce concepts Deployed Hadoop components on the Cluster like Hive HBase Spark Scala and others with respect to the requirement Uploaded and processed terabytes of data from various structured and unstructured sources into HDFS AWS cloud using Sqoop Implemented the Business Rules in Spark SCALA to get the business logic in place to run the Rating Engine Used Spark UI to observe the running of a submitted Spark Job at the node level and used Spark to do Property Bag Parsing of the data to get the required fields of data Extensively used ETL methodology for supporting Data Extraction transformations and loading processing using Hadoop Used both Hive context as well as SQL context of Spark to do the initial testing of the Spark job and used WINSCP and FTP to view the data storage structure in the server and to upload JARs which were used to do the Spark Submit Developed code from scratch in Spark using SCALA according to the technical requirements Environment Hadoop Hive HDFS Pig Sqoop Python SparkSQL Machine Learning MongoDB AWS AWS S3 AWS EC2 AWS EMR Oozie ETL Tableau Spark SparkStreaming KAFKA Netezza Apache Solr Cassandra Cloudera Distribution Java Impala Web Servers Maven Build MySQL Grafana AWS AgileScrum SR BIGDATA ENGINEER DEVELOPER Prabhav Services Inc Newark NJ September 2014 to December 2016 RESPONSIBILITIES Exploring with the Spark improving the performance and optimization of the existing algorithms in Hadoop using Spark Context Spark SQL Data Frame Spark Yarn Worked on cloud computing infrastructure eg Amazon Web Services EC2 and considerations for scalable distributed systems Worked on Gocd cicd tool to deploy application and have experience with Munin frame work for big data Testing Involved in file movements between HDFS and AWS S3 and extensively worked with S3 bucket in AWS and converted all Hadoop jobs to run in EMR by configuring the cluster according to the data size Documented and established policies and procedures for GIS technologies including servers workstations storage server virtualization GIS data security web based maps and service Extensively worked with Avro and Parquet files and converted the data from either format Parsed Semi Structured JSON data and converted to Parquet using Data Frames in Spark Involved in converting HiveSQL queries into Spark Transformations using Spark RDDs and Scala and involved in using SQOOP for importing and exporting data between RDBMS and HDFS Imported the data from different sources like AWS S3 Local file system into Spark RDD Collected data using Spark Streaming from AWS S3 bucket in nearrealtime and performs necessary Transformations and Aggregations on the fly to build the common learner data model and persistence the data in HDFS Involved in transforming the relational database to legacy labels to HDFS and HBASE tables using Sqoop and vice versa Processed the web server logs by developing Multihop flume agents by using Avro Sink and loaded into MongoDB for further analysis and worked on MongoDB NoSQL data modeling tuning disaster recovery and backup Developed data pipeline using Spark Hive and HBase to ingest customer behavioral data and financial histories into Hadoop cluster for analysis Developed a Python Script to load the CSV files into the S3 buckets and created AWS S3 buckets performed folder management in each bucket managed logs and objects within each bucket Worked with different file formats like JSon AVRO and parquet and compression techniques like snappy and developed python code for different tasks dependencies SLA watcher and time sensor for each job for workflow management and automation using Airflow tool Developed shell scripts for dynamic partitions adding to hive stage table verifying JSON schema change of source files and verifying duplicate files in source location Monitor and Troubleshoot Hadoop jobs using Yarn Resource Manager and EMR job logs using Genie and Kibana Worked on CICD Automation using tools like Jenkins Salt stack Git Vagrant Docker Elastic Search Grafana Data Management Data Access Data Governance and Integration Security and Operations performed by using Hortonworks Data Platform HDP Worked with importing metadata into Hive using Python and migrated existing tables and applications to work on AWS cloud S3 Involved with writing scripts in Oracle SQL Server and Netezza databases to extract data for reporting and analysis and Worked in importing and cleansing of data from various sources like DB2 Oracle flat files onto SQL Server with high volume data Worked extensively with importing metadata into Hive and migrated existing tables and applications to work on Hive and AWS cloud and making the data available in Athena and Snowflake Extensively used Stash GitBucket for Code Control and Worked on AWS Components such as Airflow Elastic Map Reduce EMR Athena and SnowFlake ENVIRONMENT FileNet IBM RAD 60 Scala Java 15 JSP Servlets Core Java Spring Swing Hibernate JSF ICE Faces Hibernate HTML CSS Jenkins JavaScript NodeJs UNIX Web Services SOAP WAS 61 XML IBM WebSphere 61 Rational Clear Case Log 4j IBM DB2 HADOOP DEVELOPER Infinite Computer Solutions Irving TX US July 2012 to August 2014 RESPONSIBILITIES Demonstrated Hadoop practices and broad knowledge of technical solutions design patterns and code for mediumcomplex applications deployed in Hadoop production Wrote Spark applications for Data validation cleansing transformations and custom aggregations and imported data from different sources into Spark RDD for processing and developed custom aggregate functions using Spark SQL and performed interactive querying Responsible for developing data pipeline with Amazon AWS to extract the data from weblogs and store in HDFS Worked on data pipeline creation to convert incoming data to a common format prepare data for analysis and visualization Migrate between databases share data processing logic across web apps batch jobs and APIs Consume large XML CSV and fixedwidth files and created data pipelines in kafka to Replace batch jobs with realtime data Involved in data pipeline using Pig Sqoop to ingest cargo data and customer histories into HDFS for analysis Developed Pig scripts to help perform analytics on JSON and XML data and created Hive tables external internal with static and dynamic partitions and performed bucketing on the tables to provide efficiency Used Hive QL to analyze the partitioned and bucketed data and compute various metrics for reporting and performed data transformations by writing MapReduce and Pig jobs as per business requirements Used Apache Kafka to aggregate web log data from multiple servers and make them available in Downstream systems for analysis and used Kafka Streams to Configure Spark streaming to get information and then store it in HDFS Design Architecture of data pipelineingestion as well as optimization of ETL workflows and developed syllabusCurriculum data pipelines from SyllabusCurriculum Web Services to HBASE and Hive tables Performed data analysis feature selection feature extraction using Apache Spark Machine Learning streaming libraries in Python Leading managing planning the development and implementation of the wide Geographic Information Systems GIS program Worked on setting up and configuring AWSs EMR Clusters and Used Amazon IAM to grant finegrained access to AWS resources to users Enable and configure Hadoop services such as HDFS YARN Hive Ranger Hbase Kafka Sqoop Zeppeline Notebook and SparkSpark2 and involved in analyzing log data to predict the errors by using Apache Spark Evaluate deep learning algorithms for text summarization using Python Keras TensorFlow and Theano on Cloudera Hadoop system Designed Database Schema and created Data Model to store realtime Tick Data with NoSQL store Extracting real time data using Kafka and spark streaming by Creating DStreams and converting them into RDD processing it and stored it into Cassandra Used AWS Data Pipeline to schedule an Amazon EMR cluster to clean and process web server logs stored in Amazon S3 bucket Used DataStax SparkCassandra connector to load data into Cassandra and used CQL to analyze data from Cassandra tables for quick searching sorting and grouping and involved in analyzing log data to predict the errors by using Apache Spark Worked on migrating MapReduce programs into Spark transformations using Spark and Scala initially done using python PySpark Worked and learned a great deal from Amazon Web Services AWS Cloud services like EC2 S3 EBS RDS and VPC Integrated MapReduce with HBase to import bulk amount of data into HBase using MapReduce programs Used Impala and Written Queries for fetching Data from Hive tables and developed Several MapReduce jobs using Java API Worked with Apache SOLR to implement indexing and wrote Custom SOLR query segments to optimize the search Created kafka spark streaming data pipelines for consuming the data from external source and performing the transformations in scala and contributed towards developing a Data Pipeline to load data from different sources like Web RDBMS NoSQL to Apache Kafka or Spark cluster Worked with xmls extracting tag information using xpaths and Scala XML libraries from compressed blob data types Involved in creating Data Lake by extracting customers Big Data from various data sources into Hadoop HDFS This included data from Excel Flat Files Oracle SQL Server MongoDb Cassandra HBase Teradata Netezza and also log data from servers Define data governance rules and administrating the rights depending on job profile of users Developed Pig and Hive UDFs to implement business logic for processing the data as per requirements and developed Pig UDFs in Java and used UDFs from PiggyBank for sorting and preparing the data Developed Spark scripts by using Scala IDEas per the business requirement Configured and optimized the Cassandra cluster and developed realtime java based application to work along with the Cassandra database Involved in file movements between HDFS and AWS S3 and extensively worked with S3 bucket in AWS Developed Spark jobs using Scala on top of YarnMRv2 for interactive and Batch Analysis and involved in querying data using SparkSQL on top of Spark engine for faster data sets processing and worked on implementing Spark Framework a Java based Web Frame work Created Hive tables loaded data and wrote Hive queries that helped market analysts spot emerging trends by comparing fresh data with EDW reference tables and historical metrics Developed Spark code by using Scala and SparkSQL for faster processing and testing and performed complex HiveQL queries on Hive tables Creating Nagios Grafana and Graphite dashboard for infrastructure monitoring Environment Spark AWS EC2 EMR Hive SQL Workbench Genie Logs Kibana Sqoop Spark SQL Spark Streaming Scala Python Hadoop Cloudera Stack Hue Spark Netezza Kafka HBase HDFS Hive Pig Sqoop Oracle ETL AWS S3 AWS EMR GIT Grafana SR JAVA DEVELOPER HiTech Solutions Inc Chicago IL US January 2010 to June 2012 RESPONSIBILITIES Involved in the design and development phases of Agile Software Development and analyzed current Mainframe system and designed new GUI screens Used Sqoop to ingest from DBMS and Python to ingest logs from client data centers Develop Python and bash scripts for automation and implemented Map Reduce jobs using Java API and Python using Spark Imported data from RDBMS systems like MySQL into HDFS using Sqoop and developed Sqoop jobs to perform incremental imports into Hive tables Demonstrated experience in managing the collection of geospatial data and understanding of data systems managed policies concerning the compilation of information and coordination of data through the GIS program coordinating and overseeing the implementation of policies Involved in loading and transforming of large sets of structured and semi structured data and created Data Pipelines as per the business requirements and scheduled it using Oozie Coordinators Developed the application using 3 Tier Architecture ie Presentation Business and Data Integration layers in accordance with the customerclient standards Played a vital role in Scala framework for web based applications and used File net for Content Management and for streamlining Business Processes Created Responsive Layouts for multiple devices and platforms using foundation framework and implemented printable chart report using HTML CSS and jQuery Applied JavaScript for client side form validation and worked on UNIX LINUX to move the project into production environment Created Managed Beans for handling JSF pages and include logic for processing of the data on the page and Created simple user interface for applications configuration system using MVC design patterns and swing framework Used ObjectRelational mapping tool Hibernate to achieve object to database table persistency Worked with Core Java to develop automated solutions to include web interfaces using HTML CSS JavaScript and Web services Developed web GUI involving HTML Java Script under MVC architecture and creation of WebLogic domains and setup Admin Managed servers for JAVAJ2EE applications on Non Production and Production environments Implemented code according to coding standards and Created AngularJS Controller Which Isolate scopes perform operations and used GWT GUICE JavaScript and Angular JS for client side implementation and extensively used Core Java such as Exceptions and Collections Configured the Web sphere application server to connect with DB2 Oracle and SQL Server in the back end by creating JDBC data source and configured MQ Series with IBM RAD and WAS to create new connection factories and queues Extensively worked on TOAD for interacting with data base developing the stored procedures and promoting SQL changes to QA and Production Environments Used Apache Maven for project management and building the application and CVS was used for project management and version management Creating and updating existing build scripts using Ant for deployment Tested and implementeddeployed application on WAS server and used Rational Clear Case for Version Control Environment Hadoop Map Reduce Yarn Hive Pig HBase Sqoop Spark Scala MapR Core Java R Language SQL Python Eclipse Linux Unix HDFS Map Reduce Impala Cloudera SQOOP Kafka Apache Cassandra Oozie Impala Zookeeper MySQL Eclipse PLSQL JAVA DEVELOPER APLOMB Technologies Princeton NJ US May 2007 to December 2009 RESPONSIBILITIES Involved in the configuration of Spring Framework and Hibernate mapping tool and monitoring WebLogicJBoss Server health and security Creation of Connection Pools Data Sources in WebLogic console and implemented Hibernate for Database Transactions on DB2 Implemented CI CD using Jenkins for continuous development and delivery Involved in configuring hibernate to access database and retrieve data from the database and written Web Services JAXWS for external system via SOAPHTTP call Used Log4j framework to logtrack application and involved in developing SQL queries stored procedures and functions Developed a new CR screen from the existing screen for the LTL loads Low Truck Load using JSF Used spring framework configuration files to manage objects and to achieve dependency injection Implemented cross cutting concerns like logging and monitoring mechanism using Spring AOP Implemented SOA architecture with web services using SOAP WSDL UDDI and XML and made screen changes to the existing screen for the LTL Low Truck Load Accessories using Struts Developed desktop interface using Java Swing for maintaining and tracking products Used JAXWS to access the external web services get the xml response and convert it back to java objects Developed the application using Eclipse IDE and worked under Agile Environment and worked with Web admin and the admin team to configure the application on development training test and stress environments Web logic server Executed and coordinated the installation for the project and worked on webbased reporting system with HTML JavaScript and JSP Build PLSQL functions stored procedures views and configured Oracle Database with JNDI data source with connection pooling enabled Developed the Training and Appraisal modules using Java JSP Servlets and JavaScript Used Hibernate based persistence classes at data access tier and adopted J2EE design patterns like Service Locator Session Facade and Singleton Worked on Spring Core layer Spring ORM Spring AOP in developing the application components Modified web pages using JSP and Used Struts Validation Framework for form input validation Created the WSDL and used Apache Axis 20 for publishing the WSDL and creating PDF files for storing the data required for module Used custom components using JSTL tags and Tag libraries implementing struts and used Web Logic server for deploying the war files and used Toad for the DB2 database changes ENVIRONMENT Java J2EE JSF Hibernate Struts Spring SwingJFC JSP HTML XML Web Logic iText DB2 Eclipse IDE SOAP Maven JSTL TOAD DB2 JDK Web Logic Server WSDL JAXWS Apache Axis Education Bachelors Skills Apache 9 years APACHE HADOOP HDFS 8 years APACHE HADOOP SQOOP 8 years Java 10 years Oracle 9 years Javascript Additional Information Technical Skills Bigdata Ecosystem HDFS and Map Reduce Pig Hive Impala YARN HUE Oozie ZookeeperSolr Apache Spark Apache STORM Apache Kafka Sqoop Flume Flink Elasticsearch NoSQL Databases HBase Cassandra and MongoDB Hadoop Distributions Cloudera Hortonworks Programming languages Java C SCALA Pig Latin HiveQL PySpark Scripting Languages Shell Scripting Databases MySQL oracle Teradata DB2 Build Tools Maven Ant sbt Reporting Tool Tableau Version control Tools SVN Git GitHub Cloud AWS Azure S3 EC2 EMR AppWeb servers WebSphere WebLogic JBoss and Tomcat Web Design Tools HTML AJAX JavaScript JQuery CSS and JSON Operating Systems WINDOWS 108Vista XP Development IDEs NetBeans Eclipse IDE PythonIDLE Packages Microsoft Office putty MS Visual Studio",
    "unique_id": "2823c585-5c71-40ea-9804-859635aecef5"
}