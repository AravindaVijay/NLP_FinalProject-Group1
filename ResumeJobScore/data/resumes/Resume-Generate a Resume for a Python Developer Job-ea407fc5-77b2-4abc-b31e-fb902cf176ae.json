{
    "clean_data": "Developer Admin span lDeveloperspan amp Admin Developer Admin Hadoop Cleveland OH 5 years of IT experience that includes Data Analysis and Hadoop Ecosystem Experience in components of Hadoop ecosystem including HDFS MapReduce Sqoop Hive Pig HBase Oozie Flume Kafka Zookeeper and Spark Expertise in Hadoop Ecosystem HDFS Architecture and Cluster technologies such as YARN Management HDFS HBase MapReduce Hive Pig Flume Oozie Sqoop Zookeeper and Ranger Experience in developing software solutions to build out capabilities on a Big Data Platform Experience in configuring cluster and installing the services monitoring the cluster by eliminating the compatibility errors Experience in different Hadoop distributions like Cloudera and HortonWorks Distributions HDP Highly capable of processing large sets of Structured Semistructured and Unstructured datasets and supporting Big Data applications Experience with NoSQL databases like HBase MapR and Cassandra as well as other ecosystems like Zookeeper Oozie Impala Storm Spark  Kafka Hypertable Flume Expertise in transferring data between a Hadoop ecosystem and structured data storage in a RDBMS such as MYSQL Oracle Teradata and DB2 using Sqoop Good experience with Hive concepts like staticdynamic partitioning bucketing managed and external tables join operations on tables Proficient in building user defined functions UDFs in Hive and Pig to analyze data and extended HiveQL and Pig Latin functionality Experience in implementing unified data ingestion platform using Kafka producers and consumers Proficient with Flume topologies for data ingestion from streaming sources into Hadoop Ability to adapt to evolving technology strong sense of responsibility and accomplishment Has very good development experience with Agile Methodology Strong experience in distinct phases of Software Development Life cycle SDLC including Planning Design Development and Testing during the development of software applications Excellent leadership interpersonal problem solving and time management skills Excellent communication skills both written documentation and verbal presentation Very responsible and good team player Can work independently with minimal supervision Work Experience Developer Admin Hadoop Cleveland OH June 2018 to Present Williams generates a large amount of unstructured data through RD As the part of a team worked within processing of huge unstructured datasets of the chemically formulated paints analyzing the large data transforming the data and loading data to the RD Responsibilities Understanding the scope of the project and requirements gathering Using MapReduce to Index the large amount of data to easily access specific records Loading log data into HDFS using Flume Installed and configured Hadoop cluster of Hortonworks Data Platform using Ambari Server and maintained it Maintaining the Operations installation configuration of clusters with MapR distribution Creating MapReduce jobs to power data for search and aggregation Writing Apache PIG scripts to process the HDFS data Writing Mapreduce Code for filtering data Creating Hive tables to store the processed results in a tabular format Developing Sqoop scripts to make the interaction between Pig and Oracle Writing script files for processing data and loading to HDFS Working with Sqoop for importing data from Oracle Utilizing Apache Hadoop ecosystem tools like HDFS Hive and Pig for large datasets analysis Developing Pig and Hive UDF to analyze the complex data to find specific user behavior Using Pig for data cleansing and developed Pig Latin scripts to extract the data from web server output files to load into HDFS Developing MapReduce ETL in JavaPig and data validation using HIVE Working on Hive by creating external and internal tables loading it with data and writing Hive queries Creating HBase tables to store data from various sources Developing workflow in Oozie to automate the tasks of loading data into HDFS and preprocessing with Pig and Hive Working with various Hadoop file formats including Text Sequence File RCFILE and ORC File Configured Zookeeper for Cluster coordination services Environment Hadoop MapReduce HDFS Pig Hive HBase Zookeeper Flume Kafka Spark Elastic Search Oozie Javajdk16 Cloudera Oracle 11g10g Windows UNIX Shell Scripting Graduate Assistant Long Island University New York NY September 2017 to May 2018 Worked on a research project on Social Media Analytics Performed data analytics through Rprograming Performed various techniques to get the accurate results using Twitter Data Responsibilities Conducted research in Social Media Analytics Involved in collecting processing analyzing and reporting social media data of specific research topic Worked on Tracking Community Development from Social Media using R Explored Social Media Analysis on Community Development Practices based on the results from R Performed data mining data cleaning explored data visualization techniques on a variety of data stored in spreadsheets and text files using R and plotting the same using with R packages Handson statistical coding using R and Advanced Excel Environment RStudio RPubs Java v18 ShinyApps Excel Hadoop Developer Eventcy IN October 2013 to August 2015 India Responsibilities Worked on Hadoop Ecosystem using different big data analytic tools including Hive Pig Involved in loading data from LINUX file system to HDFS Importing and exporting data into HDFS and Hive using Sqoop Implemented Partitioning Bucketing in Hive Worked on different file formats ORCFILE TEXTFILE and different Compression Codecs GZIP SNAPPY LZO Worked with multiple Input Formats such as Text File Key Value and Sequence File Input Format Experienced in running Hadoop Streaming jobs to process terabytes of json format data Involved in scheduling Oozie workflow engine to run multiple Hive and Pig jobs Executed Hive queries on Parquet tables stored in Hive to perform data analysis to meet the business requirements Created HBase tables to store various data formats of incoming data from different portfolios Created Pig Latin scripts to sort group join and filter the enterprise wise data Developed the verification and control process for daily load Experience in Daily production support to monitor and trouble shoots HadoopHive jobs Worked collaboratively with different teams to smoothly slide the project to production Environment HDFS Pig Hive Sqoop Shell Scripting HBase Zoo Keeper MySQL Software Developer Aetins IN September 2011 to July 2013 India Responsibilities Performed analysis for the client requirements based on detailed design documents Developed Use Cases Class Diagrams Sequence Diagrams and Data Models using Microsoft Visio Developed STRUTS forms and actions for validation of user request data and application functionality Developed a WebService using SOAP WSDL XML and SoapUI Developed JSP with STRUTS custom tags and implemented JavaScript validation of data Involved in developing business tier using stateless session bean Used JavaScript for the web page validation and Struts Valuator for server side validation Designing the database and coding of SQL PLSQL Triggers and Views using IBMDB Design patterns of Delegates Data Transfer Objects and Data Access Objects Developed Message Driven Beans for asynchronous processing of alerts Used ClearCase for source code control and JUNIT for unit testing The networks are simulated in realtime using an ns3 network simulator modified for multithreading across multiple cores which is implemented on generic Linux machine Involved in peer code reviews and performed integration testing of the modules Environment Struts JSP with Struts JDBC Struts Valuator SQL PLSQL IBMDB JUNIT Java JSP Servlets EJB 20 SQL Server Oracle 9i JBoss WebLogic Server 6 JavaScript Education Master in Computer Science in Computer Science Long Island University 2016 to 2018 Bachelors in Production Engineering in Production Engineering Osmania University 2007 to 2011 Skills CASSANDRA MAPREDUCE OOZIE SQOOP HBASE KAFKA DATA VISUALIZATION FLUME HADOOP NOSQL POWER BI C Hadoop HBase Hive HTML JAVASCRIPT MapReduce PHP Pig SQL XML Sql Server CertificationsLicenses Drivers License Additional Information SKILLS Languages C C Java Core J2EE AspNet Python Scala UNIX Shell Scripting Scripting HTML PHP JavaScript CSS Hadoop Ecosystem MapReduce HBASE HIVE PIG SQOOP Zookeeper OOZIE Flume HUE Kafka SPARKSQL Hadoop Distributions Cloudera Hortonworks MapR Database MySQL NoSQL Oracle DB Cassandra Virtualization Cloud Amazon AWS VMware Virtualbox Data Visualization Power BI Tableau IDE Eclipse Net Beans VisualStudio Methodologies Agile SDLC",
    "entities": [
        "JAVASCRIPT MapReduce PHP",
        "HortonWorks",
        "STRUTS",
        "HDFS",
        "the RD Responsibilities Understanding",
        "json format data",
        "Hadoop Streaming",
        "NOSQL POWER BI C Hadoop",
        "Compression Codecs GZIP SNAPPY LZO Worked",
        "Hadoop",
        "R Performed",
        "WebLogic",
        "ClearCase",
        "HBase",
        "Hadoop Ability",
        "Data Models",
        "YARN Management HDFS HBase MapReduce Hive Pig",
        "Developed Use Cases Class Diagrams",
        "Structured Semistructured",
        "Planning Design Development and Testing",
        "ORC File Configured Zookeeper for Cluster coordination services Environment Hadoop MapReduce HDFS Pig Hive HBase",
        "Text Sequence File RCFILE",
        "RD",
        "JavaScript Education Master",
        "Visualization Power BI",
        "Linux",
        "Unstructured",
        "Cloudera Oracle 11g10",
        "Text File Key Value",
        "Views",
        "Admin Developer Admin Hadoop",
        "Worked on Tracking Community Development",
        "Created Pig Latin",
        "HIVE Working on Hive",
        "EJB",
        "HDFS MapReduce Sqoop Hive Pig HBase Oozie Flume",
        "Writing Apache",
        "Data Access Objects",
        "Sqoop",
        "sort group join",
        "Executed Hive",
        "LINUX",
        "HDFS Hive and Pig",
        "Spark Expertise",
        "PIG",
        "HDFS Developing MapReduce ETL",
        "Oozie",
        "Software Development Life",
        "Sequence File Input Format Experienced",
        "Social Media",
        "SQL",
        "Big Data",
        "Hive",
        "JUNIT",
        "Handson",
        "HDP Highly",
        "Data Analysis and Hadoop Ecosystem",
        "Long Island University",
        "HDFS Working with",
        "Social Media Analytics Involved",
        "Social Media Analytics Performed",
        "lDeveloperspan",
        "Oracle Writing",
        "Rprograming Performed",
        "Microsoft",
        "Oracle Utilizing Apache Hadoop",
        "Input Formats",
        "Created HBase",
        "Agile Methodology Strong",
        "NoSQL",
        "Twitter Data Responsibilities Conducted",
        "Delegates Data Transfer Objects",
        "Production Engineering Osmania University",
        "KAFKA",
        "Operations",
        "WebService",
        "Computer Science in Computer Science Long Island University"
    ],
    "experience": "Experience in components of Hadoop ecosystem including HDFS MapReduce Sqoop Hive Pig HBase Oozie Flume Kafka Zookeeper and Spark Expertise in Hadoop Ecosystem HDFS Architecture and Cluster technologies such as YARN Management HDFS HBase MapReduce Hive Pig Flume Oozie Sqoop Zookeeper and Ranger Experience in developing software solutions to build out capabilities on a Big Data Platform Experience in configuring cluster and installing the services monitoring the cluster by eliminating the compatibility errors Experience in different Hadoop distributions like Cloudera and HortonWorks Distributions HDP Highly capable of processing large sets of Structured Semistructured and Unstructured datasets and supporting Big Data applications Experience with NoSQL databases like HBase MapR and Cassandra as well as other ecosystems like Zookeeper Oozie Impala Storm Spark   Kafka Hypertable Flume Expertise in transferring data between a Hadoop ecosystem and structured data storage in a RDBMS such as MYSQL Oracle Teradata and DB2 using Sqoop Good experience with Hive concepts like staticdynamic partitioning bucketing managed and external tables join operations on tables Proficient in building user defined functions UDFs in Hive and Pig to analyze data and extended HiveQL and Pig Latin functionality Experience in implementing unified data ingestion platform using Kafka producers and consumers Proficient with Flume topologies for data ingestion from streaming sources into Hadoop Ability to adapt to evolving technology strong sense of responsibility and accomplishment Has very good development experience with Agile Methodology Strong experience in distinct phases of Software Development Life cycle SDLC including Planning Design Development and Testing during the development of software applications Excellent leadership interpersonal problem solving and time management skills Excellent communication skills both written documentation and verbal presentation Very responsible and good team player Can work independently with minimal supervision Work Experience Developer Admin Hadoop Cleveland OH June 2018 to Present Williams generates a large amount of unstructured data through RD As the part of a team worked within processing of huge unstructured datasets of the chemically formulated paints analyzing the large data transforming the data and loading data to the RD Responsibilities Understanding the scope of the project and requirements gathering Using MapReduce to Index the large amount of data to easily access specific records Loading log data into HDFS using Flume Installed and configured Hadoop cluster of Hortonworks Data Platform using Ambari Server and maintained it Maintaining the Operations installation configuration of clusters with MapR distribution Creating MapReduce jobs to power data for search and aggregation Writing Apache PIG scripts to process the HDFS data Writing Mapreduce Code for filtering data Creating Hive tables to store the processed results in a tabular format Developing Sqoop scripts to make the interaction between Pig and Oracle Writing script files for processing data and loading to HDFS Working with Sqoop for importing data from Oracle Utilizing Apache Hadoop ecosystem tools like HDFS Hive and Pig for large datasets analysis Developing Pig and Hive UDF to analyze the complex data to find specific user behavior Using Pig for data cleansing and developed Pig Latin scripts to extract the data from web server output files to load into HDFS Developing MapReduce ETL in JavaPig and data validation using HIVE Working on Hive by creating external and internal tables loading it with data and writing Hive queries Creating HBase tables to store data from various sources Developing workflow in Oozie to automate the tasks of loading data into HDFS and preprocessing with Pig and Hive Working with various Hadoop file formats including Text Sequence File RCFILE and ORC File Configured Zookeeper for Cluster coordination services Environment Hadoop MapReduce HDFS Pig Hive HBase Zookeeper Flume Kafka Spark Elastic Search Oozie Javajdk16 Cloudera Oracle 11g10 g Windows UNIX Shell Scripting Graduate Assistant Long Island University New York NY September 2017 to May 2018 Worked on a research project on Social Media Analytics Performed data analytics through Rprograming Performed various techniques to get the accurate results using Twitter Data Responsibilities Conducted research in Social Media Analytics Involved in collecting processing analyzing and reporting social media data of specific research topic Worked on Tracking Community Development from Social Media using R Explored Social Media Analysis on Community Development Practices based on the results from R Performed data mining data cleaning explored data visualization techniques on a variety of data stored in spreadsheets and text files using R and plotting the same using with R packages Handson statistical coding using R and Advanced Excel Environment RStudio RPubs Java v18 ShinyApps Excel Hadoop Developer Eventcy IN October 2013 to August 2015 India Responsibilities Worked on Hadoop Ecosystem using different big data analytic tools including Hive Pig Involved in loading data from LINUX file system to HDFS Importing and exporting data into HDFS and Hive using Sqoop Implemented Partitioning Bucketing in Hive Worked on different file formats ORCFILE TEXTFILE and different Compression Codecs GZIP SNAPPY LZO Worked with multiple Input Formats such as Text File Key Value and Sequence File Input Format Experienced in running Hadoop Streaming jobs to process terabytes of json format data Involved in scheduling Oozie workflow engine to run multiple Hive and Pig jobs Executed Hive queries on Parquet tables stored in Hive to perform data analysis to meet the business requirements Created HBase tables to store various data formats of incoming data from different portfolios Created Pig Latin scripts to sort group join and filter the enterprise wise data Developed the verification and control process for daily load Experience in Daily production support to monitor and trouble shoots HadoopHive jobs Worked collaboratively with different teams to smoothly slide the project to production Environment HDFS Pig Hive Sqoop Shell Scripting HBase Zoo Keeper MySQL Software Developer Aetins IN September 2011 to July 2013 India Responsibilities Performed analysis for the client requirements based on detailed design documents Developed Use Cases Class Diagrams Sequence Diagrams and Data Models using Microsoft Visio Developed STRUTS forms and actions for validation of user request data and application functionality Developed a WebService using SOAP WSDL XML and SoapUI Developed JSP with STRUTS custom tags and implemented JavaScript validation of data Involved in developing business tier using stateless session bean Used JavaScript for the web page validation and Struts Valuator for server side validation Designing the database and coding of SQL PLSQL Triggers and Views using IBMDB Design patterns of Delegates Data Transfer Objects and Data Access Objects Developed Message Driven Beans for asynchronous processing of alerts Used ClearCase for source code control and JUNIT for unit testing The networks are simulated in realtime using an ns3 network simulator modified for multithreading across multiple cores which is implemented on generic Linux machine Involved in peer code reviews and performed integration testing of the modules Environment Struts JSP with Struts JDBC Struts Valuator SQL PLSQL IBMDB JUNIT Java JSP Servlets EJB 20 SQL Server Oracle 9i JBoss WebLogic Server 6 JavaScript Education Master in Computer Science in Computer Science Long Island University 2016 to 2018 Bachelors in Production Engineering in Production Engineering Osmania University 2007 to 2011 Skills CASSANDRA MAPREDUCE OOZIE SQOOP HBASE KAFKA DATA VISUALIZATION FLUME HADOOP NOSQL POWER BI C Hadoop HBase Hive HTML JAVASCRIPT MapReduce PHP Pig SQL XML Sql Server CertificationsLicenses Drivers License Additional Information SKILLS Languages C C Java Core J2EE AspNet Python Scala UNIX Shell Scripting Scripting HTML PHP JavaScript CSS Hadoop Ecosystem MapReduce HBASE HIVE PIG SQOOP Zookeeper OOZIE Flume HUE Kafka SPARKSQL Hadoop Distributions Cloudera Hortonworks MapR Database MySQL NoSQL Oracle DB Cassandra Virtualization Cloud Amazon AWS VMware Virtualbox Data Visualization Power BI Tableau IDE Eclipse Net Beans VisualStudio Methodologies Agile SDLC",
    "extracted_keywords": [
        "Developer",
        "Admin",
        "span",
        "lDeveloperspan",
        "amp",
        "Admin",
        "Developer",
        "Admin",
        "Hadoop",
        "Cleveland",
        "OH",
        "years",
        "IT",
        "experience",
        "Data",
        "Analysis",
        "Hadoop",
        "Ecosystem",
        "Experience",
        "components",
        "Hadoop",
        "ecosystem",
        "HDFS",
        "MapReduce",
        "Sqoop",
        "Hive",
        "Pig",
        "HBase",
        "Oozie",
        "Flume",
        "Kafka",
        "Zookeeper",
        "Spark",
        "Expertise",
        "Hadoop",
        "Ecosystem",
        "HDFS",
        "Architecture",
        "Cluster",
        "technologies",
        "YARN",
        "Management",
        "HDFS",
        "HBase",
        "MapReduce",
        "Hive",
        "Pig",
        "Flume",
        "Oozie",
        "Sqoop",
        "Zookeeper",
        "Ranger",
        "Experience",
        "software",
        "solutions",
        "capabilities",
        "Big",
        "Data",
        "Platform",
        "Experience",
        "cluster",
        "services",
        "cluster",
        "compatibility",
        "errors",
        "Experience",
        "Hadoop",
        "distributions",
        "Cloudera",
        "HortonWorks",
        "Distributions",
        "HDP",
        "sets",
        "Structured",
        "Semistructured",
        "datasets",
        "Big",
        "Data",
        "applications",
        "Experience",
        "databases",
        "HBase",
        "MapR",
        "Cassandra",
        "ecosystems",
        "Zookeeper",
        "Oozie",
        "Impala",
        "Storm",
        "Spark",
        "Kafka",
        "Hypertable",
        "Flume",
        "Expertise",
        "data",
        "Hadoop",
        "ecosystem",
        "data",
        "storage",
        "RDBMS",
        "MYSQL",
        "Oracle",
        "Teradata",
        "DB2",
        "Sqoop",
        "Good",
        "experience",
        "Hive",
        "concepts",
        "partitioning",
        "bucketing",
        "tables",
        "operations",
        "tables",
        "user",
        "functions",
        "UDFs",
        "Hive",
        "Pig",
        "data",
        "HiveQL",
        "Pig",
        "Latin",
        "functionality",
        "Experience",
        "data",
        "ingestion",
        "platform",
        "Kafka",
        "producers",
        "consumers",
        "Proficient",
        "Flume",
        "topologies",
        "data",
        "ingestion",
        "sources",
        "Hadoop",
        "Ability",
        "technology",
        "sense",
        "responsibility",
        "accomplishment",
        "development",
        "experience",
        "Agile",
        "Methodology",
        "Strong",
        "experience",
        "phases",
        "Software",
        "Development",
        "Life",
        "cycle",
        "SDLC",
        "Planning",
        "Design",
        "Development",
        "Testing",
        "development",
        "software",
        "applications",
        "leadership",
        "problem",
        "time",
        "management",
        "communication",
        "skills",
        "documentation",
        "presentation",
        "team",
        "player",
        "supervision",
        "Work",
        "Experience",
        "Developer",
        "Admin",
        "Hadoop",
        "Cleveland",
        "OH",
        "June",
        "Present",
        "Williams",
        "amount",
        "data",
        "RD",
        "part",
        "team",
        "processing",
        "datasets",
        "paints",
        "data",
        "data",
        "loading",
        "data",
        "RD",
        "Responsibilities",
        "scope",
        "project",
        "requirements",
        "MapReduce",
        "amount",
        "data",
        "records",
        "Loading",
        "log",
        "data",
        "HDFS",
        "Flume",
        "Installed",
        "Hadoop",
        "cluster",
        "Hortonworks",
        "Data",
        "Platform",
        "Ambari",
        "Server",
        "Operations",
        "installation",
        "configuration",
        "clusters",
        "MapR",
        "distribution",
        "Creating",
        "MapReduce",
        "jobs",
        "power",
        "data",
        "search",
        "aggregation",
        "Apache",
        "PIG",
        "scripts",
        "HDFS",
        "data",
        "Mapreduce",
        "Code",
        "data",
        "Hive",
        "tables",
        "results",
        "format",
        "Sqoop",
        "scripts",
        "interaction",
        "Pig",
        "Oracle",
        "Writing",
        "script",
        "files",
        "data",
        "loading",
        "HDFS",
        "Working",
        "Sqoop",
        "data",
        "Oracle",
        "Utilizing",
        "Apache",
        "Hadoop",
        "ecosystem",
        "tools",
        "HDFS",
        "Hive",
        "Pig",
        "datasets",
        "analysis",
        "Pig",
        "Hive",
        "UDF",
        "data",
        "user",
        "behavior",
        "Pig",
        "data",
        "cleansing",
        "Pig",
        "Latin",
        "scripts",
        "data",
        "web",
        "server",
        "output",
        "files",
        "HDFS",
        "MapReduce",
        "ETL",
        "JavaPig",
        "data",
        "validation",
        "HIVE",
        "Working",
        "Hive",
        "tables",
        "data",
        "Hive",
        "queries",
        "Creating",
        "HBase",
        "tables",
        "data",
        "sources",
        "workflow",
        "Oozie",
        "tasks",
        "loading",
        "data",
        "HDFS",
        "Pig",
        "Hive",
        "Working",
        "Hadoop",
        "file",
        "formats",
        "Text",
        "Sequence",
        "File",
        "RCFILE",
        "ORC",
        "File",
        "Configured",
        "Zookeeper",
        "Cluster",
        "coordination",
        "services",
        "Environment",
        "Hadoop",
        "MapReduce",
        "HDFS",
        "Pig",
        "Hive",
        "HBase",
        "Zookeeper",
        "Flume",
        "Kafka",
        "Spark",
        "Elastic",
        "Search",
        "Oozie",
        "Javajdk16",
        "Cloudera",
        "Oracle",
        "g",
        "Windows",
        "UNIX",
        "Shell",
        "Scripting",
        "Graduate",
        "Assistant",
        "Long",
        "Island",
        "University",
        "New",
        "York",
        "NY",
        "September",
        "May",
        "research",
        "project",
        "Social",
        "Media",
        "Analytics",
        "Performed",
        "data",
        "analytics",
        "techniques",
        "results",
        "Twitter",
        "Data",
        "Responsibilities",
        "research",
        "Social",
        "Media",
        "Analytics",
        "processing",
        "media",
        "data",
        "research",
        "topic",
        "Tracking",
        "Community",
        "Development",
        "Social",
        "Media",
        "R",
        "Social",
        "Media",
        "Analysis",
        "Community",
        "Development",
        "Practices",
        "results",
        "R",
        "Performed",
        "data",
        "mining",
        "data",
        "data",
        "visualization",
        "techniques",
        "variety",
        "data",
        "spreadsheets",
        "text",
        "files",
        "R",
        "R",
        "packages",
        "Handson",
        "coding",
        "R",
        "Advanced",
        "Excel",
        "Environment",
        "RStudio",
        "RPubs",
        "Java",
        "v18",
        "ShinyApps",
        "Excel",
        "Hadoop",
        "Developer",
        "Eventcy",
        "October",
        "August",
        "India",
        "Responsibilities",
        "Hadoop",
        "Ecosystem",
        "data",
        "tools",
        "Hive",
        "Pig",
        "loading",
        "data",
        "LINUX",
        "file",
        "system",
        "HDFS",
        "Importing",
        "data",
        "HDFS",
        "Hive",
        "Sqoop",
        "Partitioning",
        "Bucketing",
        "Hive",
        "Worked",
        "file",
        "formats",
        "TEXTFILE",
        "Compression",
        "Codecs",
        "GZIP",
        "LZO",
        "Worked",
        "Input",
        "Formats",
        "Text",
        "File",
        "Key",
        "Value",
        "Sequence",
        "File",
        "Input",
        "Format",
        "Hadoop",
        "Streaming",
        "jobs",
        "terabytes",
        "json",
        "format",
        "data",
        "Oozie",
        "workflow",
        "engine",
        "Hive",
        "Pig",
        "jobs",
        "Hive",
        "queries",
        "Parquet",
        "tables",
        "Hive",
        "data",
        "analysis",
        "business",
        "requirements",
        "Created",
        "HBase",
        "data",
        "formats",
        "data",
        "portfolios",
        "Pig",
        "Latin",
        "scripts",
        "group",
        "join",
        "enterprise",
        "data",
        "verification",
        "control",
        "process",
        "load",
        "Experience",
        "Daily",
        "production",
        "support",
        "trouble",
        "jobs",
        "teams",
        "project",
        "Environment",
        "HDFS",
        "Pig",
        "Hive",
        "Sqoop",
        "Shell",
        "Scripting",
        "HBase",
        "Zoo",
        "Keeper",
        "MySQL",
        "Software",
        "Developer",
        "Aetins",
        "September",
        "July",
        "India",
        "Responsibilities",
        "analysis",
        "client",
        "requirements",
        "design",
        "documents",
        "Developed",
        "Use",
        "Cases",
        "Class",
        "Diagrams",
        "Sequence",
        "Diagrams",
        "Data",
        "Models",
        "Microsoft",
        "Visio",
        "STRUTS",
        "forms",
        "actions",
        "validation",
        "user",
        "request",
        "data",
        "application",
        "functionality",
        "WebService",
        "SOAP",
        "WSDL",
        "XML",
        "SoapUI",
        "Developed",
        "JSP",
        "STRUTS",
        "custom",
        "tags",
        "JavaScript",
        "validation",
        "data",
        "business",
        "tier",
        "session",
        "bean",
        "JavaScript",
        "web",
        "page",
        "validation",
        "Struts",
        "Valuator",
        "server",
        "side",
        "validation",
        "database",
        "coding",
        "SQL",
        "PLSQL",
        "Triggers",
        "Views",
        "IBMDB",
        "Design",
        "patterns",
        "Delegates",
        "Data",
        "Transfer",
        "Objects",
        "Data",
        "Access",
        "Message",
        "Driven",
        "Beans",
        "processing",
        "alerts",
        "ClearCase",
        "source",
        "code",
        "control",
        "JUNIT",
        "unit",
        "networks",
        "realtime",
        "ns3",
        "network",
        "simulator",
        "cores",
        "Linux",
        "machine",
        "peer",
        "code",
        "reviews",
        "integration",
        "testing",
        "modules",
        "Environment",
        "Struts",
        "JSP",
        "Struts",
        "JDBC",
        "Struts",
        "Valuator",
        "SQL",
        "PLSQL",
        "IBMDB",
        "JUNIT",
        "Java",
        "JSP",
        "Servlets",
        "EJB",
        "SQL",
        "Server",
        "Oracle",
        "9i",
        "JBoss",
        "WebLogic",
        "Server",
        "JavaScript",
        "Education",
        "Master",
        "Computer",
        "Science",
        "Computer",
        "Science",
        "Long",
        "Island",
        "University",
        "Bachelors",
        "Production",
        "Engineering",
        "Production",
        "Engineering",
        "Osmania",
        "University",
        "Skills",
        "CASSANDRA",
        "MAPREDUCE",
        "OOZIE",
        "SQOOP",
        "HBASE",
        "KAFKA",
        "DATA",
        "VISUALIZATION",
        "FLUME",
        "HADOOP",
        "NOSQL",
        "POWER",
        "BI",
        "C",
        "Hadoop",
        "HBase",
        "Hive",
        "HTML",
        "JAVASCRIPT",
        "MapReduce",
        "PHP",
        "Pig",
        "SQL",
        "XML",
        "Sql",
        "Server",
        "CertificationsLicenses",
        "Drivers",
        "License",
        "Additional",
        "Information",
        "SKILLS",
        "Languages",
        "C",
        "C",
        "Java",
        "Core",
        "J2EE",
        "AspNet",
        "Python",
        "Scala",
        "UNIX",
        "Shell",
        "Scripting",
        "Scripting",
        "HTML",
        "PHP",
        "JavaScript",
        "CSS",
        "Hadoop",
        "Ecosystem",
        "MapReduce",
        "HBASE",
        "PIG",
        "SQOOP",
        "Zookeeper",
        "OOZIE",
        "Flume",
        "HUE",
        "Kafka",
        "SPARKSQL",
        "Hadoop",
        "Distributions",
        "Cloudera",
        "Hortonworks",
        "MapR",
        "Database",
        "MySQL",
        "NoSQL",
        "Oracle",
        "DB",
        "Cassandra",
        "Virtualization",
        "Cloud",
        "Amazon",
        "AWS",
        "VMware",
        "Virtualbox",
        "Data",
        "Visualization",
        "Power",
        "BI",
        "Tableau",
        "IDE",
        "Eclipse",
        "Net",
        "Beans",
        "VisualStudio",
        "Methodologies",
        "Agile",
        "SDLC"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T21:13:50.572428",
    "resume_data": "Developer Admin span lDeveloperspan amp Admin Developer Admin Hadoop Cleveland OH 5 years of IT experience that includes Data Analysis and Hadoop Ecosystem Experience in components of Hadoop ecosystem including HDFS MapReduce Sqoop Hive Pig HBase Oozie Flume Kafka Zookeeper and Spark Expertise in Hadoop Ecosystem HDFS Architecture and Cluster technologies such as YARN Management HDFS HBase MapReduce Hive Pig Flume Oozie Sqoop Zookeeper and Ranger Experience in developing software solutions to build out capabilities on a Big Data Platform Experience in configuring cluster and installing the services monitoring the cluster by eliminating the compatibility errors Experience in different Hadoop distributions like Cloudera and HortonWorks Distributions HDP Highly capable of processing large sets of Structured Semistructured and Unstructured datasets and supporting Big Data applications Experience with NoSQL databases like HBase MapR and Cassandra as well as other ecosystems like Zookeeper Oozie Impala Storm Spark StreamingSQL Kafka Hypertable Flume Expertise in transferring data between a Hadoop ecosystem and structured data storage in a RDBMS such as MYSQL Oracle Teradata and DB2 using Sqoop Good experience with Hive concepts like staticdynamic partitioning bucketing managed and external tables join operations on tables Proficient in building user defined functions UDFs in Hive and Pig to analyze data and extended HiveQL and Pig Latin functionality Experience in implementing unified data ingestion platform using Kafka producers and consumers Proficient with Flume topologies for data ingestion from streaming sources into Hadoop Ability to adapt to evolving technology strong sense of responsibility and accomplishment Has very good development experience with Agile Methodology Strong experience in distinct phases of Software Development Life cycle SDLC including Planning Design Development and Testing during the development of software applications Excellent leadership interpersonal problem solving and time management skills Excellent communication skills both written documentation and verbal presentation Very responsible and good team player Can work independently with minimal supervision Work Experience Developer Admin Hadoop Cleveland OH June 2018 to Present Williams generates a large amount of unstructured data through RD As the part of a team worked within processing of huge unstructured datasets of the chemically formulated paints analyzing the large data transforming the data and loading data to the RD Responsibilities Understanding the scope of the project and requirements gathering Using MapReduce to Index the large amount of data to easily access specific records Loading log data into HDFS using Flume Installed and configured Hadoop cluster of Hortonworks Data Platform using Ambari Server and maintained it Maintaining the Operations installation configuration of clusters with MapR distribution Creating MapReduce jobs to power data for search and aggregation Writing Apache PIG scripts to process the HDFS data Writing Mapreduce Code for filtering data Creating Hive tables to store the processed results in a tabular format Developing Sqoop scripts to make the interaction between Pig and Oracle Writing script files for processing data and loading to HDFS Working with Sqoop for importing data from Oracle Utilizing Apache Hadoop ecosystem tools like HDFS Hive and Pig for large datasets analysis Developing Pig and Hive UDF to analyze the complex data to find specific user behavior Using Pig for data cleansing and developed Pig Latin scripts to extract the data from web server output files to load into HDFS Developing MapReduce ETL in JavaPig and data validation using HIVE Working on Hive by creating external and internal tables loading it with data and writing Hive queries Creating HBase tables to store data from various sources Developing workflow in Oozie to automate the tasks of loading data into HDFS and preprocessing with Pig and Hive Working with various Hadoop file formats including Text Sequence File RCFILE and ORC File Configured Zookeeper for Cluster coordination services Environment Hadoop MapReduce HDFS Pig Hive HBase Zookeeper Flume Kafka Spark Elastic Search Oozie Javajdk16 Cloudera Oracle 11g10g Windows UNIX Shell Scripting Graduate Assistant Long Island University New York NY September 2017 to May 2018 Worked on a research project on Social Media Analytics Performed data analytics through Rprograming Performed various techniques to get the accurate results using Twitter Data Responsibilities Conducted research in Social Media Analytics Involved in collecting processing analyzing and reporting social media data of specific research topic Worked on Tracking Community Development from Social Media using R Explored Social Media Analysis on Community Development Practices based on the results from R Performed data mining data cleaning explored data visualization techniques on a variety of data stored in spreadsheets and text files using R and plotting the same using with R packages Handson statistical coding using R and Advanced Excel Environment RStudio RPubs Java v18 ShinyApps Excel Hadoop Developer Eventcy IN October 2013 to August 2015 India Responsibilities Worked on Hadoop Ecosystem using different big data analytic tools including Hive Pig Involved in loading data from LINUX file system to HDFS Importing and exporting data into HDFS and Hive using Sqoop Implemented Partitioning Bucketing in Hive Worked on different file formats ORCFILE TEXTFILE and different Compression Codecs GZIP SNAPPY LZO Worked with multiple Input Formats such as Text File Key Value and Sequence File Input Format Experienced in running Hadoop Streaming jobs to process terabytes of json format data Involved in scheduling Oozie workflow engine to run multiple Hive and Pig jobs Executed Hive queries on Parquet tables stored in Hive to perform data analysis to meet the business requirements Created HBase tables to store various data formats of incoming data from different portfolios Created Pig Latin scripts to sort group join and filter the enterprise wise data Developed the verification and control process for daily load Experience in Daily production support to monitor and trouble shoots HadoopHive jobs Worked collaboratively with different teams to smoothly slide the project to production Environment HDFS Pig Hive Sqoop Shell Scripting HBase Zoo Keeper MySQL Software Developer Aetins IN September 2011 to July 2013 India Responsibilities Performed analysis for the client requirements based on detailed design documents Developed Use Cases Class Diagrams Sequence Diagrams and Data Models using Microsoft Visio Developed STRUTS forms and actions for validation of user request data and application functionality Developed a WebService using SOAP WSDL XML and SoapUI Developed JSP with STRUTS custom tags and implemented JavaScript validation of data Involved in developing business tier using stateless session bean Used JavaScript for the web page validation and Struts Valuator for server side validation Designing the database and coding of SQL PLSQL Triggers and Views using IBMDB Design patterns of Delegates Data Transfer Objects and Data Access Objects Developed Message Driven Beans for asynchronous processing of alerts Used ClearCase for source code control and JUNIT for unit testing The networks are simulated in realtime using an ns3 network simulator modified for multithreading across multiple cores which is implemented on generic Linux machine Involved in peer code reviews and performed integration testing of the modules Environment Struts JSP with Struts JDBC Struts Valuator SQL PLSQL IBMDB JUNIT Java JSP Servlets EJB 20 SQL Server Oracle 9i JBoss WebLogic Server 6 JavaScript Education Master in Computer Science in Computer Science Long Island University 2016 to 2018 Bachelors in Production Engineering in Production Engineering Osmania University 2007 to 2011 Skills CASSANDRA MAPREDUCE OOZIE SQOOP HBASE KAFKA DATA VISUALIZATION FLUME HADOOP NOSQL POWER BI C Hadoop HBase Hive HTML JAVASCRIPT MapReduce PHP Pig SQL XML Sql Server CertificationsLicenses Drivers License Additional Information SKILLS Languages C C Java Core J2EE AspNet Python Scala UNIX Shell Scripting Scripting HTML PHP JavaScript CSS Hadoop Ecosystem MapReduce HBASE HIVE PIG SQOOP Zookeeper OOZIE Flume HUE Kafka SPARKSQL Hadoop Distributions Cloudera Hortonworks MapR Database MySQL NoSQL Oracle DB Cassandra Virtualization Cloud Amazon AWS VMware Virtualbox Data Visualization Power BI Tableau IDE Eclipse Net Beans VisualStudio Methodologies Agile SDLC",
    "unique_id": "ea407fc5-77b2-4abc-b31e-fb902cf176ae"
}