{
    "clean_data": "Hadoop Spark Developer Hadoop amp Spark span lDeveloperspan Hadoop Spark Developer Mindtree An IT professional with around 3 years of experience in Software Development and Implementation of Big Data and Big Data related technologies Efficient on working with Big Data and Hadoop Distributed File System HDFS Good working knowledge with Hive and Sqoop Written Sqoop scripts In Order to Integrate with Hive and MYSQL Database Expert in data ingestion data transformations data preprocessing data exploration and data integration using Sqoop Hive Scripts Performed Hive operations on large datasets with proficiency in writing HiveQL queries using transactional and performance efficient concepts Partitioning Bucketing efficient and effective Join operations Indepth understanding of Apache Spark Architecture and performed querying using Spark Core SQL RDDs Data frames Datasets Experience and strong knowledge on implementation of Spark Core Spark SQL Implemented Spark using Scala and Spark SQL for faster testing and processing of data Experience in Scala programming Experience in working with Echo systems like Sqoop Hive Hbase Knowledge with message broker such as Apache Kafka Worked with Sqoop to Import and Export data from a relational database into Hadoop Experienced with different file formats like Parquet ORC Avro Sequence CSV XML JSON Text files Scheduled jobs and automated workflows using Oozie Experience in Agile Development and Scrum process Work Experience Hadoop Spark Developer Mindtree US May 2017 to Present USA Environment Hadoop Hive SQOOP MySQL Spark Core Spark Sql Duration May 2017 to till date Role Hadoop Spark Developer Description HCA Data Repository is a cross platform patient medical records suite for capturing monitoring information such as measurements for hypertension or blood sugar patient reminders symptoms reporting issues reporting We will process the metadata datasets coming thousands of records of patients data from various hospitals The purpose of publishing this dataset is to figure out the patients depending upon various constraints like type of disease based on gender age seasonal diseases on location base Roles Responsibilities Written Hive Scripts to process the HDFS data Created Hive tables to Store the Processed results Devoloped Sqoop Scripts in order to make the interaction between Hive and Mysql databases Implemented Spark using Scala and Spark SQL for faster testing and Processing of data Invovled in Conversion of SQL to HQL and My SQL procedures to Scala Code to run on Spark Engine Creating RDD to load the Unstructured data Involved in gathering the requirements designing and development Writing the Script files for Processing data and loading in HDFS Writing CLI commands using HDFS Creating Data frames to get tables format Using count and show actions after creation of Data frames Involved in requirement analysis phase Project 2 Project Name Target ECommerce Analysis Client Target Software Engineer Mindtree Bengaluru Karnataka March 2016 to Present Technical Skills Distribution Cloudera CDH5 Hadoop Big Data HDFS Sqoop Hive HBase Sparkcore and SparkSQl Operating Systems LinuxUNIX Windows Programming Languages Scala RDBMSNoSQL SQL server HBase Scripting Shell Scripting IDE Eclipse IntelliJ Idea Project Details Project 1 Project Name HCA Data Repository Education BTECH in MECH NOVA COLLEGE OF ENGINEERINGTECHNOLOGY Hyderabad Telangana Skills APACHE HADOOP HDFS 3 years APACHE HADOOP SQOOP 3 years Hadoop 3 years HADOOP 3 years HADOOP DISTRIBUTED FILE SYSTEM 3 years Additional Information Technologies CDH HDFS Hive Sqoop Spark Duration May 2017 to till date Role Hadoop Spark Developer Description Target is the Second Largest ECommerce Company in USA Target is using Bigdata technologies to analyze their Customer transactions and draw some useful out sights off it which will be useful for their business development Roles Responsibilities Used sparksql to read the data from csv files and create tables in hive using scala API Used various spark transformations and actions for cleansing the input data and transformation Experience in Spark RDD and Spark SQL Programing using Scala Executed Queries on tables in hive to perform data analysis Responsible for writing hive queries Devoloping Hive Scripts for processing and transforming the data stored in HDFS Worked with Hive partition and Bucketing Hands on storing the data in different file formats in Hive Hands on with various data formats using SparkCore and SparkSql",
    "entities": [
        "Agile Development",
        "Created Hive",
        "Implemented Spark",
        "Present Technical Skills Distribution",
        "Data frames Involved",
        "SparkSQl Operating Systems",
        "API",
        "Sqoop",
        "Oozie Experience",
        "Hadoop Experienced",
        "Spark Core",
        "Data frames Datasets Experience",
        "Customer",
        "HDFS",
        "Spark Core Spark",
        "Role Hadoop Spark Developer Description HCA Data Repository",
        "Hadoop Spark Developer Hadoop",
        "Order to Integrate with Hive",
        "HDFS Worked with Hive partition",
        "Bucketing Hands",
        "SparkCore",
        "Project 2 Project Name Target ECommerce Analysis Client Target Software Engineer Mindtree Bengaluru",
        "Bigdata",
        "Software Development and Implementation of Big Data",
        "SparkSql",
        "Processed",
        "Hive Hands",
        "Role Hadoop Spark Developer Description Target",
        "Unstructured",
        "SQL",
        "RDD",
        "Import and Export",
        "Spark RDD",
        "Apache Spark Architecture",
        "Idea Project Details Project 1 Project Name HCA Data Repository Education",
        "the Second Largest ECommerce Company",
        "USA Target",
        "LinuxUNIX Windows Programming Languages",
        "Present USA Environment Hadoop",
        "ENGINEERINGTECHNOLOGY Hyderabad",
        "Spark SQL Programing",
        "Scala Code",
        "csv",
        "Scala Executed Queries",
        "HBase",
        "CLI",
        "Additional Information Technologies CDH HDFS Hive Sqoop Spark",
        "Echo",
        "Big Data",
        "Hive",
        "Partitioning Bucketing",
        "Spark",
        "Devoloped Sqoop Scripts"
    ],
    "experience": "Experience and strong knowledge on implementation of Spark Core Spark SQL Implemented Spark using Scala and Spark SQL for faster testing and processing of data Experience in Scala programming Experience in working with Echo systems like Sqoop Hive Hbase Knowledge with message broker such as Apache Kafka Worked with Sqoop to Import and Export data from a relational database into Hadoop Experienced with different file formats like Parquet ORC Avro Sequence CSV XML JSON Text files Scheduled jobs and automated workflows using Oozie Experience in Agile Development and Scrum process Work Experience Hadoop Spark Developer Mindtree US May 2017 to Present USA Environment Hadoop Hive SQOOP MySQL Spark Core Spark Sql Duration May 2017 to till date Role Hadoop Spark Developer Description HCA Data Repository is a cross platform patient medical records suite for capturing monitoring information such as measurements for hypertension or blood sugar patient reminders symptoms reporting issues reporting We will process the metadata datasets coming thousands of records of patients data from various hospitals The purpose of publishing this dataset is to figure out the patients depending upon various constraints like type of disease based on gender age seasonal diseases on location base Roles Responsibilities Written Hive Scripts to process the HDFS data Created Hive tables to Store the Processed results Devoloped Sqoop Scripts in order to make the interaction between Hive and Mysql databases Implemented Spark using Scala and Spark SQL for faster testing and Processing of data Invovled in Conversion of SQL to HQL and My SQL procedures to Scala Code to run on Spark Engine Creating RDD to load the Unstructured data Involved in gathering the requirements designing and development Writing the Script files for Processing data and loading in HDFS Writing CLI commands using HDFS Creating Data frames to get tables format Using count and show actions after creation of Data frames Involved in requirement analysis phase Project 2 Project Name Target ECommerce Analysis Client Target Software Engineer Mindtree Bengaluru Karnataka March 2016 to Present Technical Skills Distribution Cloudera CDH5 Hadoop Big Data HDFS Sqoop Hive HBase Sparkcore and SparkSQl Operating Systems LinuxUNIX Windows Programming Languages Scala RDBMSNoSQL SQL server HBase Scripting Shell Scripting IDE Eclipse IntelliJ Idea Project Details Project 1 Project Name HCA Data Repository Education BTECH in MECH NOVA COLLEGE OF ENGINEERINGTECHNOLOGY Hyderabad Telangana Skills APACHE HADOOP HDFS 3 years APACHE HADOOP SQOOP 3 years Hadoop 3 years HADOOP 3 years HADOOP DISTRIBUTED FILE SYSTEM 3 years Additional Information Technologies CDH HDFS Hive Sqoop Spark Duration May 2017 to till date Role Hadoop Spark Developer Description Target is the Second Largest ECommerce Company in USA Target is using Bigdata technologies to analyze their Customer transactions and draw some useful out sights off it which will be useful for their business development Roles Responsibilities Used sparksql to read the data from csv files and create tables in hive using scala API Used various spark transformations and actions for cleansing the input data and transformation Experience in Spark RDD and Spark SQL Programing using Scala Executed Queries on tables in hive to perform data analysis Responsible for writing hive queries Devoloping Hive Scripts for processing and transforming the data stored in HDFS Worked with Hive partition and Bucketing Hands on storing the data in different file formats in Hive Hands on with various data formats using SparkCore and SparkSql",
    "extracted_keywords": [
        "Hadoop",
        "Spark",
        "Developer",
        "Hadoop",
        "amp",
        "Spark",
        "span",
        "lDeveloperspan",
        "Hadoop",
        "Spark",
        "Developer",
        "Mindtree",
        "IT",
        "years",
        "experience",
        "Software",
        "Development",
        "Implementation",
        "Big",
        "Data",
        "Big",
        "Data",
        "technologies",
        "Big",
        "Data",
        "Hadoop",
        "Distributed",
        "File",
        "System",
        "HDFS",
        "knowledge",
        "Hive",
        "Sqoop",
        "Written",
        "Sqoop",
        "Order",
        "Hive",
        "MYSQL",
        "Database",
        "Expert",
        "data",
        "ingestion",
        "data",
        "transformations",
        "data",
        "data",
        "exploration",
        "data",
        "integration",
        "Sqoop",
        "Hive",
        "Scripts",
        "Performed",
        "Hive",
        "operations",
        "datasets",
        "proficiency",
        "queries",
        "performance",
        "concepts",
        "Partitioning",
        "Bucketing",
        "operations",
        "understanding",
        "Apache",
        "Spark",
        "Architecture",
        "Spark",
        "Core",
        "SQL",
        "RDDs",
        "Data",
        "Datasets",
        "Experience",
        "knowledge",
        "implementation",
        "Spark",
        "Core",
        "Spark",
        "SQL",
        "Spark",
        "Scala",
        "Spark",
        "SQL",
        "testing",
        "processing",
        "data",
        "Experience",
        "Scala",
        "programming",
        "Experience",
        "Echo",
        "systems",
        "Sqoop",
        "Hive",
        "Hbase",
        "Knowledge",
        "message",
        "broker",
        "Apache",
        "Kafka",
        "Sqoop",
        "Import",
        "Export",
        "data",
        "database",
        "Hadoop",
        "Experienced",
        "file",
        "formats",
        "Parquet",
        "ORC",
        "Avro",
        "Sequence",
        "CSV",
        "XML",
        "JSON",
        "Text",
        "jobs",
        "workflows",
        "Oozie",
        "Experience",
        "Agile",
        "Development",
        "Scrum",
        "process",
        "Work",
        "Experience",
        "Hadoop",
        "Spark",
        "Developer",
        "Mindtree",
        "US",
        "May",
        "Present",
        "USA",
        "Environment",
        "Hadoop",
        "Hive",
        "SQOOP",
        "MySQL",
        "Spark",
        "Core",
        "Spark",
        "Sql",
        "Duration",
        "May",
        "date",
        "Role",
        "Hadoop",
        "Spark",
        "Developer",
        "Description",
        "HCA",
        "Data",
        "Repository",
        "cross",
        "platform",
        "records",
        "suite",
        "monitoring",
        "information",
        "measurements",
        "hypertension",
        "blood",
        "sugar",
        "patient",
        "reminders",
        "symptoms",
        "issues",
        "datasets",
        "thousands",
        "records",
        "patients",
        "data",
        "hospitals",
        "purpose",
        "dataset",
        "patients",
        "constraints",
        "type",
        "disease",
        "gender",
        "age",
        "diseases",
        "location",
        "base",
        "Roles",
        "Responsibilities",
        "Written",
        "Hive",
        "Scripts",
        "HDFS",
        "data",
        "Hive",
        "tables",
        "results",
        "Devoloped",
        "Sqoop",
        "Scripts",
        "order",
        "interaction",
        "Hive",
        "Mysql",
        "Spark",
        "Scala",
        "Spark",
        "SQL",
        "testing",
        "Processing",
        "data",
        "Conversion",
        "SQL",
        "HQL",
        "SQL",
        "procedures",
        "Scala",
        "Code",
        "Spark",
        "Engine",
        "Creating",
        "RDD",
        "data",
        "requirements",
        "development",
        "Script",
        "files",
        "Processing",
        "data",
        "loading",
        "HDFS",
        "Writing",
        "CLI",
        "commands",
        "HDFS",
        "Creating",
        "Data",
        "frames",
        "tables",
        "format",
        "count",
        "actions",
        "creation",
        "Data",
        "frames",
        "requirement",
        "analysis",
        "phase",
        "Project",
        "Project",
        "Name",
        "Target",
        "ECommerce",
        "Analysis",
        "Client",
        "Target",
        "Software",
        "Engineer",
        "Mindtree",
        "Bengaluru",
        "Karnataka",
        "March",
        "Present",
        "Technical",
        "Skills",
        "Distribution",
        "Cloudera",
        "CDH5",
        "Hadoop",
        "Big",
        "Data",
        "HDFS",
        "Sqoop",
        "Hive",
        "HBase",
        "Sparkcore",
        "SparkSQl",
        "Operating",
        "Systems",
        "Windows",
        "Programming",
        "Languages",
        "Scala",
        "RDBMSNoSQL",
        "SQL",
        "server",
        "HBase",
        "Scripting",
        "Shell",
        "Scripting",
        "IDE",
        "Eclipse",
        "IntelliJ",
        "Idea",
        "Project",
        "Details",
        "Project",
        "Project",
        "Name",
        "HCA",
        "Data",
        "Repository",
        "Education",
        "BTECH",
        "MECH",
        "NOVA",
        "COLLEGE",
        "Hyderabad",
        "Telangana",
        "Skills",
        "APACHE",
        "HADOOP",
        "HDFS",
        "years",
        "APACHE",
        "HADOOP",
        "SQOOP",
        "years",
        "Hadoop",
        "years",
        "HADOOP",
        "years",
        "HADOOP",
        "FILE",
        "SYSTEM",
        "years",
        "Additional",
        "Information",
        "Technologies",
        "CDH",
        "HDFS",
        "Hive",
        "Sqoop",
        "Spark",
        "Duration",
        "May",
        "date",
        "Role",
        "Hadoop",
        "Spark",
        "Developer",
        "Description",
        "Target",
        "Second",
        "Largest",
        "ECommerce",
        "Company",
        "USA",
        "Target",
        "Bigdata",
        "technologies",
        "Customer",
        "transactions",
        "sights",
        "business",
        "development",
        "Roles",
        "Responsibilities",
        "sparksql",
        "data",
        "files",
        "tables",
        "hive",
        "scala",
        "API",
        "spark",
        "transformations",
        "actions",
        "input",
        "data",
        "transformation",
        "Experience",
        "Spark",
        "RDD",
        "Spark",
        "SQL",
        "Programing",
        "Scala",
        "Executed",
        "Queries",
        "tables",
        "hive",
        "data",
        "analysis",
        "hive",
        "queries",
        "Devoloping",
        "Hive",
        "Scripts",
        "processing",
        "data",
        "HDFS",
        "Hive",
        "partition",
        "Bucketing",
        "Hands",
        "data",
        "file",
        "formats",
        "Hive",
        "Hands",
        "data",
        "formats",
        "SparkCore",
        "SparkSql"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T21:35:45.319867",
    "resume_data": "Hadoop Spark Developer Hadoop amp Spark span lDeveloperspan Hadoop Spark Developer Mindtree An IT professional with around 3 years of experience in Software Development and Implementation of Big Data and Big Data related technologies Efficient on working with Big Data and Hadoop Distributed File System HDFS Good working knowledge with Hive and Sqoop Written Sqoop scripts In Order to Integrate with Hive and MYSQL Database Expert in data ingestion data transformations data preprocessing data exploration and data integration using Sqoop Hive Scripts Performed Hive operations on large datasets with proficiency in writing HiveQL queries using transactional and performance efficient concepts Partitioning Bucketing efficient and effective Join operations Indepth understanding of Apache Spark Architecture and performed querying using Spark Core SQL RDDs Data frames Datasets Experience and strong knowledge on implementation of Spark Core Spark SQL Implemented Spark using Scala and Spark SQL for faster testing and processing of data Experience in Scala programming Experience in working with Echo systems like Sqoop Hive Hbase Knowledge with message broker such as Apache Kafka Worked with Sqoop to Import and Export data from a relational database into Hadoop Experienced with different file formats like Parquet ORC Avro Sequence CSV XML JSON Text files Scheduled jobs and automated workflows using Oozie Experience in Agile Development and Scrum process Work Experience Hadoop Spark Developer Mindtree US May 2017 to Present USA Environment Hadoop Hive SQOOP MySQL Spark Core Spark Sql Duration May 2017 to till date Role Hadoop Spark Developer Description HCA Data Repository is a cross platform patient medical records suite for capturing monitoring information such as measurements for hypertension or blood sugar patient reminders symptoms reporting issues reporting We will process the metadata datasets coming thousands of records of patients data from various hospitals The purpose of publishing this dataset is to figure out the patients depending upon various constraints like type of disease based on gender age seasonal diseases on location base Roles Responsibilities Written Hive Scripts to process the HDFS data Created Hive tables to Store the Processed results Devoloped Sqoop Scripts in order to make the interaction between Hive and Mysql databases Implemented Spark using Scala and Spark SQL for faster testing and Processing of data Invovled in Conversion of SQL to HQL and My SQL procedures to Scala Code to run on Spark Engine Creating RDD to load the Unstructured data Involved in gathering the requirements designing and development Writing the Script files for Processing data and loading in HDFS Writing CLI commands using HDFS Creating Data frames to get tables format Using count and show actions after creation of Data frames Involved in requirement analysis phase Project 2 Project Name Target ECommerce Analysis Client Target Software Engineer Mindtree Bengaluru Karnataka March 2016 to Present Technical Skills Distribution Cloudera CDH5 Hadoop Big Data HDFS Sqoop Hive HBase Sparkcore and SparkSQl Operating Systems LinuxUNIX Windows Programming Languages Scala RDBMSNoSQL SQL server HBase Scripting Shell Scripting IDE Eclipse IntelliJ Idea Project Details Project 1 Project Name HCA Data Repository Education BTECH in MECH NOVA COLLEGE OF ENGINEERINGTECHNOLOGY Hyderabad Telangana Skills APACHE HADOOP HDFS 3 years APACHE HADOOP SQOOP 3 years Hadoop 3 years HADOOP 3 years HADOOP DISTRIBUTED FILE SYSTEM 3 years Additional Information Technologies CDH HDFS Hive Sqoop Spark Duration May 2017 to till date Role Hadoop Spark Developer Description Target is the Second Largest ECommerce Company in USA Target is using Bigdata technologies to analyze their Customer transactions and draw some useful out sights off it which will be useful for their business development Roles Responsibilities Used sparksql to read the data from csv files and create tables in hive using scala API Used various spark transformations and actions for cleansing the input data and transformation Experience in Spark RDD and Spark SQL Programing using Scala Executed Queries on tables in hive to perform data analysis Responsible for writing hive queries Devoloping Hive Scripts for processing and transforming the data stored in HDFS Worked with Hive partition and Bucketing Hands on storing the data in different file formats in Hive Hands on with various data formats using SparkCore and SparkSql",
    "unique_id": "8237e887-2895-4434-ac5d-bea5aa958b55"
}