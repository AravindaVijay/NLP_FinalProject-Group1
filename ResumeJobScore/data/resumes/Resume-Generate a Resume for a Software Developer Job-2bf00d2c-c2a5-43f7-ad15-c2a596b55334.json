{
    "clean_data": "Data Engineer Data Engineer Data Engineer Capital One Richmond VA Data Engineer with 2 years of IT Industry experience in designing developing and maintaining largescale systemsapplications with significant expertise in Python 27 32 and 36 Experienced with full software development lifecycle architecting scalable platforms objectoriented programming database design and agile methodologies Expert knowledge of and experience in Object Oriented Design and Programming concepts Experience in writing data processing frameworks for large scale applications Hands on experience in using Spark ecosystem components like S3 RDS Snowflake REST Kafka Experience in closely working with data analysis and Data scientists for converting POCs into production grade software Experience in consuming data from Kafka into spark micro batches Worked on spark application tuning and resource allocations based on use case E xperience in Shell Scripting SQL Server UNIX and Linux Experience in building applications in AWS infrastructure Cloud Formation Templates Cloud Watch Alarms S3 RDS Security Groups VPCs EC2 Familiar with JSON based REST Web services and Amazon Web services Automated the continuous integration and deployments using Jenkins and AWS Cloud Templates deployment services Lambda Experience with web scrapping using beautiful soup Created own flight logistic application using information from scrapped data Experience in project deployment using Jenkins and using web services like Amazon Web Services AWS EC2 Cloud Formation Templates AWS S3 and Cloud watch Exposure to MLDL ecosystem algorithmic approaches along with design and performance constraints Exposure to Tensor flow and PyToch frameworks hardware for AI Work Experience Data Engineer Capital One Richmond VA April 2018 to Present Capital One maintains an ecosystem of internal data processing applications designed to prevent fraud of various types including transactional and application fraud with dynamic fraud patterns Ecosystem of highly available multiple spark clusters running rule on kafka streams and batch data Developed multiplatform PySpark framework used to create spark jobs and provide SQL like interface for data analysts This application can be used for both creating spark jobs and productionize them Responsible for migrating findings from Data Scientist and fraud investigators into existing fraud defenses And over seeing Data Science Approaches so that functionalities can be converted to production environment Designed data flow for new business needs and participated in architectural and workflow discussions Responsible for maintaining allocating and tuning resources for faster performance for heavy data loads Resolved Big Data small files issue in the organization using spark Hadoop configurations and dependency configurations As part of Capital One Fraud Prevention team worked on maintaining old and developing new rule engines along with application resiliency strategies Created new Fraud defenses using existing data processing patterns Responsible for developmental and production data security aspects along with upgrading the system with new software and infrastructural features Converted exiting traditional Teradata fraud defenses into cloud architecture and spark SQL Create QA data by performing data analysis using Databricks notebooks with maintained entropy Configured Apache Arrow for columnar data processing at processors in pyspark 23 Migrate exiting fraud defenses from Teradata to PySpark environment Tune exiting spark jobs for performance Created modules for switching cluster stack active and inactive using Jenkins AWS lambda to EC2 cluster Converted monolithic application into a pip installable package and incorporated it to schedule Jupyter Notebooks using papermill Built CICD pipeline using Jenkins and AWS lambda Build secret management system Built redundancy catch up in fraud case creation Upgraded Spark Hadoop Version and participated in building Custom Assume Role Credential Provider in jar for spark session assume role auto renewal Environment Linux Rel7 PySpark Data Lake Hadoop Teradata PSQL Python JSON AWS CICD RISK management Apache Arrow Credit Card Fraud Prevention REST GitHub Notebooks Python Developer Data Analyst WAFTS Solutions January 2017 to March 2018 Provide insights into data for Small to Midrange businesses and present regular findings from data Develop SQL code to be used with automated processes to identify revenue opportunities and financial issues Data mine large datasets in Relational Databases to find emerging issues and rootcause in provisioning marketing and billing systems Drive timely resolution of marketing issues for a seamless customer experience Proactively monitor daily processes and results to ensure consistent coverage Create REST APIs for web services using python Flask Django frameworks Environment Ubuntu Python SQL JSON AWS REST Jupyter Notebooks GitHub Jr Web Developer Repulsor Technologies IN May 2015 to July 2015 Developed static websites for local businesses and organizations Designed cover books and posters for technology events in undergraduate collages Environment Windows HTML CSS XML Adobe Photoshop Education Master of Science in Internet and Web Design in Internet and Web Design Wilmington University Wilmington DE 2017 Additional Information Professional skills Programming Languages Python Java JavaScript Web services RESTful Data bases Oracle 1011g MySQL SQL Server IDEs and tools Eclipse Pycharm NetBeans OS Environment XP windows Linux Unix Ubuntu Unix Shell Scripting Unix Shell Scripting Version control GitHub Development Methodologies Agile Scrum Hadoop HDFS MapReduce Spark Machine Learning KNN Gradient Descent Back Propagation",
    "entities": [
        "Fraud",
        "SQL Create",
        "WAFTS Solutions",
        "Drive",
        "spark micro",
        "Object Oriented Design",
        "Present Capital",
        "QA",
        "GitHub Development Methodologies Agile Scrum Hadoop HDFS MapReduce Spark Machine Learning",
        "Build",
        "Relational",
        "Upgraded Spark Hadoop Version",
        "Created",
        "AWS",
        "Data Science Approaches",
        "Small to Midrange",
        "PySpark Data Lake Hadoop Teradata",
        "Richmond VA Data Engineer",
        "Databricks",
        "Built CICD",
        "Linux",
        "SQL",
        "Data Scientist",
        "AI Work Experience Data Engineer Capital",
        "Data",
        "REST",
        "Hadoop",
        "Built",
        "Jenkins AWS",
        "Shell Scripting",
        "Data Engineer Data Engineer Data Engineer Capital",
        "Amazon Web Services AWS EC2",
        "Shell Scripting Version",
        "Amazon",
        "Capital One Fraud Prevention",
        "Spark",
        "Custom Assume Role Credential Provider"
    ],
    "experience": "Experience in writing data processing frameworks for large scale applications Hands on experience in using Spark ecosystem components like S3 RDS Snowflake REST Kafka Experience in closely working with data analysis and Data scientists for converting POCs into production grade software Experience in consuming data from Kafka into spark micro batches Worked on spark application tuning and resource allocations based on use case E xperience in Shell Scripting SQL Server UNIX and Linux Experience in building applications in AWS infrastructure Cloud Formation Templates Cloud Watch Alarms S3 RDS Security Groups VPCs EC2 Familiar with JSON based REST Web services and Amazon Web services Automated the continuous integration and deployments using Jenkins and AWS Cloud Templates deployment services Lambda Experience with web scrapping using beautiful soup Created own flight logistic application using information from scrapped data Experience in project deployment using Jenkins and using web services like Amazon Web Services AWS EC2 Cloud Formation Templates AWS S3 and Cloud watch Exposure to MLDL ecosystem algorithmic approaches along with design and performance constraints Exposure to Tensor flow and PyToch frameworks hardware for AI Work Experience Data Engineer Capital One Richmond VA April 2018 to Present Capital One maintains an ecosystem of internal data processing applications designed to prevent fraud of various types including transactional and application fraud with dynamic fraud patterns Ecosystem of highly available multiple spark clusters running rule on kafka streams and batch data Developed multiplatform PySpark framework used to create spark jobs and provide SQL like interface for data analysts This application can be used for both creating spark jobs and productionize them Responsible for migrating findings from Data Scientist and fraud investigators into existing fraud defenses And over seeing Data Science Approaches so that functionalities can be converted to production environment Designed data flow for new business needs and participated in architectural and workflow discussions Responsible for maintaining allocating and tuning resources for faster performance for heavy data loads Resolved Big Data small files issue in the organization using spark Hadoop configurations and dependency configurations As part of Capital One Fraud Prevention team worked on maintaining old and developing new rule engines along with application resiliency strategies Created new Fraud defenses using existing data processing patterns Responsible for developmental and production data security aspects along with upgrading the system with new software and infrastructural features Converted exiting traditional Teradata fraud defenses into cloud architecture and spark SQL Create QA data by performing data analysis using Databricks notebooks with maintained entropy Configured Apache Arrow for columnar data processing at processors in pyspark 23 Migrate exiting fraud defenses from Teradata to PySpark environment Tune exiting spark jobs for performance Created modules for switching cluster stack active and inactive using Jenkins AWS lambda to EC2 cluster Converted monolithic application into a pip installable package and incorporated it to schedule Jupyter Notebooks using papermill Built CICD pipeline using Jenkins and AWS lambda Build secret management system Built redundancy catch up in fraud case creation Upgraded Spark Hadoop Version and participated in building Custom Assume Role Credential Provider in jar for spark session assume role auto renewal Environment Linux Rel7 PySpark Data Lake Hadoop Teradata PSQL Python JSON AWS CICD RISK management Apache Arrow Credit Card Fraud Prevention REST GitHub Notebooks Python Developer Data Analyst WAFTS Solutions January 2017 to March 2018 Provide insights into data for Small to Midrange businesses and present regular findings from data Develop SQL code to be used with automated processes to identify revenue opportunities and financial issues Data mine large datasets in Relational Databases to find emerging issues and rootcause in provisioning marketing and billing systems Drive timely resolution of marketing issues for a seamless customer experience Proactively monitor daily processes and results to ensure consistent coverage Create REST APIs for web services using python Flask Django frameworks Environment Ubuntu Python SQL JSON AWS REST Jupyter Notebooks GitHub Jr Web Developer Repulsor Technologies IN May 2015 to July 2015 Developed static websites for local businesses and organizations Designed cover books and posters for technology events in undergraduate collages Environment Windows HTML CSS XML Adobe Photoshop Education Master of Science in Internet and Web Design in Internet and Web Design Wilmington University Wilmington DE 2017 Additional Information Professional skills Programming Languages Python Java JavaScript Web services RESTful Data bases Oracle 1011 g MySQL SQL Server IDEs and tools Eclipse Pycharm NetBeans OS Environment XP windows Linux Unix Ubuntu Unix Shell Scripting Unix Shell Scripting Version control GitHub Development Methodologies Agile Scrum Hadoop HDFS MapReduce Spark Machine Learning KNN Gradient Descent Back Propagation",
    "extracted_keywords": [
        "Data",
        "Engineer",
        "Data",
        "Engineer",
        "Data",
        "Engineer",
        "Capital",
        "Richmond",
        "VA",
        "Data",
        "Engineer",
        "years",
        "IT",
        "Industry",
        "experience",
        "largescale",
        "systemsapplications",
        "expertise",
        "Python",
        "software",
        "development",
        "lifecycle",
        "platforms",
        "programming",
        "database",
        "design",
        "methodologies",
        "Expert",
        "knowledge",
        "Object",
        "Oriented",
        "Design",
        "Programming",
        "concepts",
        "Experience",
        "data",
        "processing",
        "frameworks",
        "scale",
        "applications",
        "Hands",
        "experience",
        "Spark",
        "ecosystem",
        "components",
        "S3",
        "RDS",
        "Snowflake",
        "REST",
        "Kafka",
        "Experience",
        "data",
        "analysis",
        "Data",
        "scientists",
        "POCs",
        "production",
        "grade",
        "software",
        "Experience",
        "data",
        "Kafka",
        "spark",
        "micro",
        "batches",
        "spark",
        "application",
        "tuning",
        "resource",
        "allocations",
        "use",
        "case",
        "E",
        "xperience",
        "Shell",
        "Scripting",
        "SQL",
        "Server",
        "UNIX",
        "Linux",
        "Experience",
        "building",
        "applications",
        "AWS",
        "infrastructure",
        "Cloud",
        "Formation",
        "Templates",
        "Cloud",
        "Watch",
        "Alarms",
        "S3",
        "RDS",
        "Security",
        "Groups",
        "EC2",
        "Familiar",
        "JSON",
        "REST",
        "Web",
        "services",
        "Amazon",
        "Web",
        "services",
        "integration",
        "deployments",
        "Jenkins",
        "AWS",
        "Cloud",
        "Templates",
        "deployment",
        "services",
        "Lambda",
        "Experience",
        "web",
        "soup",
        "Created",
        "flight",
        "application",
        "information",
        "data",
        "Experience",
        "project",
        "deployment",
        "Jenkins",
        "web",
        "services",
        "Amazon",
        "Web",
        "Services",
        "AWS",
        "EC2",
        "Cloud",
        "Formation",
        "Templates",
        "S3",
        "Cloud",
        "Exposure",
        "MLDL",
        "ecosystem",
        "approaches",
        "design",
        "performance",
        "Exposure",
        "Tensor",
        "flow",
        "PyToch",
        "frameworks",
        "hardware",
        "AI",
        "Work",
        "Experience",
        "Data",
        "Engineer",
        "Capital",
        "Richmond",
        "VA",
        "April",
        "Present",
        "Capital",
        "One",
        "ecosystem",
        "data",
        "processing",
        "applications",
        "fraud",
        "types",
        "application",
        "fraud",
        "fraud",
        "patterns",
        "Ecosystem",
        "spark",
        "clusters",
        "rule",
        "kafka",
        "streams",
        "batch",
        "data",
        "multiplatform",
        "PySpark",
        "framework",
        "spark",
        "jobs",
        "SQL",
        "interface",
        "data",
        "analysts",
        "application",
        "spark",
        "jobs",
        "findings",
        "Data",
        "Scientist",
        "fraud",
        "investigators",
        "fraud",
        "defenses",
        "Data",
        "Science",
        "Approaches",
        "functionalities",
        "production",
        "environment",
        "data",
        "flow",
        "business",
        "needs",
        "discussions",
        "resources",
        "performance",
        "data",
        "loads",
        "Resolved",
        "Big",
        "Data",
        "files",
        "issue",
        "organization",
        "spark",
        "Hadoop",
        "configurations",
        "dependency",
        "configurations",
        "part",
        "Capital",
        "One",
        "Fraud",
        "Prevention",
        "team",
        "rule",
        "engines",
        "application",
        "resiliency",
        "strategies",
        "Fraud",
        "defenses",
        "data",
        "processing",
        "patterns",
        "production",
        "data",
        "security",
        "aspects",
        "system",
        "software",
        "features",
        "Teradata",
        "fraud",
        "defenses",
        "architecture",
        "spark",
        "SQL",
        "Create",
        "QA",
        "data",
        "data",
        "analysis",
        "Databricks",
        "notebooks",
        "entropy",
        "Configured",
        "Apache",
        "Arrow",
        "data",
        "processing",
        "processors",
        "pyspark",
        "Migrate",
        "exiting",
        "fraud",
        "defenses",
        "Teradata",
        "PySpark",
        "environment",
        "Tune",
        "spark",
        "jobs",
        "performance",
        "modules",
        "cluster",
        "stack",
        "Jenkins",
        "AWS",
        "lambda",
        "EC2",
        "cluster",
        "application",
        "package",
        "Jupyter",
        "Notebooks",
        "papermill",
        "CICD",
        "pipeline",
        "Jenkins",
        "AWS",
        "Build",
        "management",
        "system",
        "redundancy",
        "fraud",
        "case",
        "creation",
        "Spark",
        "Hadoop",
        "Version",
        "Custom",
        "Assume",
        "Role",
        "Credential",
        "Provider",
        "jar",
        "spark",
        "session",
        "role",
        "auto",
        "renewal",
        "Environment",
        "Linux",
        "PySpark",
        "Data",
        "Lake",
        "Hadoop",
        "Teradata",
        "PSQL",
        "Python",
        "JSON",
        "AWS",
        "RISK",
        "management",
        "Apache",
        "Arrow",
        "Credit",
        "Card",
        "Fraud",
        "Prevention",
        "REST",
        "GitHub",
        "Notebooks",
        "Python",
        "Developer",
        "Data",
        "Analyst",
        "WAFTS",
        "Solutions",
        "January",
        "March",
        "Provide",
        "insights",
        "data",
        "Midrange",
        "businesses",
        "findings",
        "data",
        "Develop",
        "SQL",
        "code",
        "processes",
        "revenue",
        "opportunities",
        "issues",
        "Data",
        "datasets",
        "Relational",
        "Databases",
        "issues",
        "rootcause",
        "marketing",
        "billing",
        "systems",
        "resolution",
        "marketing",
        "issues",
        "customer",
        "experience",
        "processes",
        "results",
        "coverage",
        "REST",
        "APIs",
        "web",
        "services",
        "python",
        "Flask",
        "Django",
        "frameworks",
        "Environment",
        "Ubuntu",
        "Python",
        "SQL",
        "JSON",
        "AWS",
        "Jupyter",
        "Notebooks",
        "GitHub",
        "Jr",
        "Web",
        "Developer",
        "Repulsor",
        "Technologies",
        "May",
        "July",
        "websites",
        "businesses",
        "organizations",
        "cover",
        "books",
        "posters",
        "technology",
        "events",
        "collages",
        "Environment",
        "Windows",
        "HTML",
        "CSS",
        "XML",
        "Adobe",
        "Photoshop",
        "Education",
        "Master",
        "Science",
        "Internet",
        "Web",
        "Design",
        "Internet",
        "Web",
        "Design",
        "Wilmington",
        "University",
        "Wilmington",
        "DE",
        "Additional",
        "Information",
        "Professional",
        "skills",
        "Programming",
        "Languages",
        "Python",
        "Java",
        "JavaScript",
        "Web",
        "services",
        "Data",
        "bases",
        "Oracle",
        "g",
        "MySQL",
        "SQL",
        "Server",
        "IDEs",
        "tools",
        "Eclipse",
        "Pycharm",
        "NetBeans",
        "Environment",
        "XP",
        "Linux",
        "Unix",
        "Ubuntu",
        "Unix",
        "Shell",
        "Scripting",
        "Unix",
        "Shell",
        "Scripting",
        "Version",
        "control",
        "GitHub",
        "Development",
        "Methodologies",
        "Agile",
        "Scrum",
        "Hadoop",
        "HDFS",
        "MapReduce",
        "Spark",
        "Machine",
        "Learning",
        "KNN",
        "Gradient",
        "Descent",
        "Propagation"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T22:05:07.467797",
    "resume_data": "Data Engineer Data Engineer Data Engineer Capital One Richmond VA Data Engineer with 2 years of IT Industry experience in designing developing and maintaining largescale systemsapplications with significant expertise in Python 27 32 and 36 Experienced with full software development lifecycle architecting scalable platforms objectoriented programming database design and agile methodologies Expert knowledge of and experience in Object Oriented Design and Programming concepts Experience in writing data processing frameworks for large scale applications Hands on experience in using Spark ecosystem components like S3 RDS Snowflake REST Kafka Experience in closely working with data analysis and Data scientists for converting POCs into production grade software Experience in consuming data from Kafka into spark micro batches Worked on spark application tuning and resource allocations based on use case E xperience in Shell Scripting SQL Server UNIX and Linux Experience in building applications in AWS infrastructure Cloud Formation Templates Cloud Watch Alarms S3 RDS Security Groups VPCs EC2 Familiar with JSON based REST Web services and Amazon Web services Automated the continuous integration and deployments using Jenkins and AWS Cloud Templates deployment services Lambda Experience with web scrapping using beautiful soup Created own flight logistic application using information from scrapped data Experience in project deployment using Jenkins and using web services like Amazon Web Services AWS EC2 Cloud Formation Templates AWS S3 and Cloud watch Exposure to MLDL ecosystem algorithmic approaches along with design and performance constraints Exposure to Tensor flow and PyToch frameworks hardware for AI Work Experience Data Engineer Capital One Richmond VA April 2018 to Present Capital One maintains an ecosystem of internal data processing applications designed to prevent fraud of various types including transactional and application fraud with dynamic fraud patterns Ecosystem of highly available multiple spark clusters running rule on kafka streams and batch data Developed multiplatform PySpark framework used to create spark jobs and provide SQL like interface for data analysts This application can be used for both creating spark jobs and productionize them Responsible for migrating findings from Data Scientist and fraud investigators into existing fraud defenses And over seeing Data Science Approaches so that functionalities can be converted to production environment Designed data flow for new business needs and participated in architectural and workflow discussions Responsible for maintaining allocating and tuning resources for faster performance for heavy data loads Resolved Big Data small files issue in the organization using spark Hadoop configurations and dependency configurations As part of Capital One Fraud Prevention team worked on maintaining old and developing new rule engines along with application resiliency strategies Created new Fraud defenses using existing data processing patterns Responsible for developmental and production data security aspects along with upgrading the system with new software and infrastructural features Converted exiting traditional Teradata fraud defenses into cloud architecture and spark SQL Create QA data by performing data analysis using Databricks notebooks with maintained entropy Configured Apache Arrow for columnar data processing at processors in pyspark 23 Migrate exiting fraud defenses from Teradata to PySpark environment Tune exiting spark jobs for performance Created modules for switching cluster stack active and inactive using Jenkins AWS lambda to EC2 cluster Converted monolithic application into a pip installable package and incorporated it to schedule Jupyter Notebooks using papermill Built CICD pipeline using Jenkins and AWS lambda Build secret management system Built redundancy catch up in fraud case creation Upgraded Spark Hadoop Version and participated in building Custom Assume Role Credential Provider in jar for spark session assume role auto renewal Environment Linux Rel7 PySpark Data Lake Hadoop Teradata PSQL Python JSON AWS CICD RISK management Apache Arrow Credit Card Fraud Prevention REST GitHub Notebooks Python Developer Data Analyst WAFTS Solutions January 2017 to March 2018 Provide insights into data for Small to Midrange businesses and present regular findings from data Develop SQL code to be used with automated processes to identify revenue opportunities and financial issues Data mine large datasets in Relational Databases to find emerging issues and rootcause in provisioning marketing and billing systems Drive timely resolution of marketing issues for a seamless customer experience Proactively monitor daily processes and results to ensure consistent coverage Create REST APIs for web services using python Flask Django frameworks Environment Ubuntu Python SQL JSON AWS REST Jupyter Notebooks GitHub Jr Web Developer Repulsor Technologies IN May 2015 to July 2015 Developed static websites for local businesses and organizations Designed cover books and posters for technology events in undergraduate collages Environment Windows HTML CSS XML Adobe Photoshop Education Master of Science in Internet and Web Design in Internet and Web Design Wilmington University Wilmington DE 2017 Additional Information Professional skills Programming Languages Python Java JavaScript Web services RESTful Data bases Oracle 1011g MySQL SQL Server IDEs and tools Eclipse Pycharm NetBeans OS Environment XP windows Linux Unix Ubuntu Unix Shell Scripting Unix Shell Scripting Version control GitHub Development Methodologies Agile Scrum Hadoop HDFS MapReduce Spark Machine Learning KNN Gradient Descent Back Propagation",
    "unique_id": "2bf00d2c-c2a5-43f7-ad15-c2a596b55334"
}