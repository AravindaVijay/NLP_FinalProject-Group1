{
    "clean_data": "Hadoop Developer Hadoop span lDeveloperspan Hadoop Developer Vanguard Group Charlotte NC Over 5 years of experience in software design development testing and maintenance of web and enterprise applications using Java python AWS and Hadoop Experienced on Hadoop ecosystem using several components line HDFS MapReduce Hive Kafka Pig Hive Sqoop Oozie and Spark Worked on several AWS components like EMR S3 EC2 and also familiar with Lamda functions kinesis streaming Easily adoptable to any new technology and try to explore whenever a situation is needed Can easily coordinate with the small or big team size Authorized to work in the US for any employer Work Experience Hadoop Developer Vanguard Group Charlotte NC October 2017 to Present Responsibilities Built Data Lake in the AWS cloud Worked on creating an end to end data ingestion pipeline to transfer data on premise to AWS cloud under AgileScrum methodology Worked on AWS services like EMR Cloud Watch Cloud Formation EC2 S3 Lambda Big data technologies like Sqoop Oozie Hive Hue Tez Yarn Zepplin Developed Scala scripts using Dataframessql in spark 2X for data aggregation queries and writing data back into OLTP system through Sqoop Developed shell scripts to automate the workflows to perform data ingestion python scripts to generate Json Oozie workflows Experienced in handling large data sets using partitions Spark in memory capabilities Effective and efficient joins Transformations and other during ingestion process Used troposphere in python to auto generate cloud formation templates in JSON format to deploy AWS big data environment Used reporting tool Tableau to connect with Hive for generating daily reports of data Worked with different file formats like TEXTFILE SEQUENCE FILE AVRO FILE ORC and PARQUET for hive querying and processing Worked in migration of several microservices Webapps and web services from OnPrem PCF to AWS PCF Worked on setting up Dashboards monitoring and alerts in Splunk for webapps and web services on AWS Worked on Atlassian CICD stack namely Bit Bucket JIRA Bamboo Confluence Hadoop Developer Thales Melbourne FL July 2017 to September 2017 Responsibilities Worked on loading and transforming structured semi structured and unstructured data Implemented Spark using Scala and Spark SQL for faster testing and processing of data Exploring with Spark improving performance and optimization of the existing algorithms in Hadoop using Spark Context SparkSQL Data Frame and Pair RDDs Created Hive tables partitions and implemented incremental imports to perform adhoc queries on structured data Developed jobs to move inbound files to HDFS file location based on monthly weekly daily and hourly partitioning Analyzed and filtered the logs using Logstash Experienced on tableau desktopserver to visualize the data from csv files and hive server 2X Java Hadoop Developer Veritis Group Inc Irving TX May 2016 to May 2017 Responsibilities Ingested files to HDFS using java kafka publisher and consumer APIs for streaming application Experienced in using HDFS Java API to put the files to HDFS Implemented cucumber test cases to compare data before and after transformations Used Pig as to do transformations event joins filters and some preaggregations before storing the data onto HDFS Created external and internal tables partitions and data storage format in hive Worked on Hibernate framework to insert and update the data in MySQL Database Used Log4j for logging purpose and write it to a file for auditing purpose Written shell scripts for file Automation process Used git for code repository and maven to build the project JavaJ2EE Developer Adeptros Bengaluru Karnataka January 2014 to June 2014 Responsibilities Involved in various phases of Software Development Life Cycle SDLC such as requirements gathering modeling analysis design development and testing Used J2EE design patterns for adding new functionality to existing applications Create and execute test cases in JUnit for unit testing of application Wrote SQL queries to retrieve data from the database using JDBC Utilized frameworks such as Hibernate and Spring for persistence and application layers Utilized objectoriented programming and Java for creating business logic Used Log4J for writing into different logs files Application Log and Error Log Implemented Representational state transfer REST Web services for distributed systems to retrieve data from client side Developed UNIX Shell scripts for automating project management tasks To keep track of issues and tasks on individuals used JIRA ticketing system Using Eclipse integrated IDE to build the application and git to maintain the version of the files Involved in Bug fixing of various modules that were raised by the testing teams in the application during integration testing phase Education M S in Computer Information Systems Florida Institute of Technology Melbourne FL Technology GITAM University Visakhapatnam Andhra Pradesh Skills Hive 1 year JIRA 2 years Scala 1 year sql 2 years web services 2 years",
    "entities": [
        "Implemented Spark",
        "Florida Institute of Technology Melbourne",
        "Spark Context",
        "Developed",
        "2X Java Hadoop Developer Veritis Group Inc",
        "US",
        "AWS Worked",
        "Hadoop Experienced",
        "Developed UNIX Shell",
        "NC",
        "Utilized",
        "HDFS",
        "Worked on Hibernate",
        "AWS",
        "Hadoop Developer Hadoop",
        "JavaJ2EE Developer Adeptros Bengaluru",
        "PARQUET",
        "FL Technology GITAM University",
        "JSON",
        "Work Experience Hadoop Developer Vanguard Group",
        "OnPrem PCF",
        "2X",
        "SQL",
        "Hadoop",
        "OLTP",
        "Dashboards",
        "AWS PCF Worked",
        "Skills Hive",
        "Dataframessql",
        "Present Responsibilities Built Data Lake",
        "Tableau",
        "JUnit",
        "csv",
        "Software Development Life Cycle",
        "Computer Information Systems",
        "Data Frame",
        "TEXTFILE SEQUENCE FILE AVRO FILE ORC",
        "HDFS Created",
        "Spark",
        "Sqoop Developed"
    ],
    "experience": "Experience Hadoop Developer Vanguard Group Charlotte NC October 2017 to Present Responsibilities Built Data Lake in the AWS cloud Worked on creating an end to end data ingestion pipeline to transfer data on premise to AWS cloud under AgileScrum methodology Worked on AWS services like EMR Cloud Watch Cloud Formation EC2 S3 Lambda Big data technologies like Sqoop Oozie Hive Hue Tez Yarn Zepplin Developed Scala scripts using Dataframessql in spark 2X for data aggregation queries and writing data back into OLTP system through Sqoop Developed shell scripts to automate the workflows to perform data ingestion python scripts to generate Json Oozie workflows Experienced in handling large data sets using partitions Spark in memory capabilities Effective and efficient joins Transformations and other during ingestion process Used troposphere in python to auto generate cloud formation templates in JSON format to deploy AWS big data environment Used reporting tool Tableau to connect with Hive for generating daily reports of data Worked with different file formats like TEXTFILE SEQUENCE FILE AVRO FILE ORC and PARQUET for hive querying and processing Worked in migration of several microservices Webapps and web services from OnPrem PCF to AWS PCF Worked on setting up Dashboards monitoring and alerts in Splunk for webapps and web services on AWS Worked on Atlassian CICD stack namely Bit Bucket JIRA Bamboo Confluence Hadoop Developer Thales Melbourne FL July 2017 to September 2017 Responsibilities Worked on loading and transforming structured semi structured and unstructured data Implemented Spark using Scala and Spark SQL for faster testing and processing of data Exploring with Spark improving performance and optimization of the existing algorithms in Hadoop using Spark Context SparkSQL Data Frame and Pair RDDs Created Hive tables partitions and implemented incremental imports to perform adhoc queries on structured data Developed jobs to move inbound files to HDFS file location based on monthly weekly daily and hourly partitioning Analyzed and filtered the logs using Logstash Experienced on tableau desktopserver to visualize the data from csv files and hive server 2X Java Hadoop Developer Veritis Group Inc Irving TX May 2016 to May 2017 Responsibilities Ingested files to HDFS using java kafka publisher and consumer APIs for streaming application Experienced in using HDFS Java API to put the files to HDFS Implemented cucumber test cases to compare data before and after transformations Used Pig as to do transformations event joins filters and some preaggregations before storing the data onto HDFS Created external and internal tables partitions and data storage format in hive Worked on Hibernate framework to insert and update the data in MySQL Database Used Log4j for logging purpose and write it to a file for auditing purpose Written shell scripts for file Automation process Used git for code repository and maven to build the project JavaJ2EE Developer Adeptros Bengaluru Karnataka January 2014 to June 2014 Responsibilities Involved in various phases of Software Development Life Cycle SDLC such as requirements gathering modeling analysis design development and testing Used J2EE design patterns for adding new functionality to existing applications Create and execute test cases in JUnit for unit testing of application Wrote SQL queries to retrieve data from the database using JDBC Utilized frameworks such as Hibernate and Spring for persistence and application layers Utilized objectoriented programming and Java for creating business logic Used Log4J for writing into different logs files Application Log and Error Log Implemented Representational state transfer REST Web services for distributed systems to retrieve data from client side Developed UNIX Shell scripts for automating project management tasks To keep track of issues and tasks on individuals used JIRA ticketing system Using Eclipse integrated IDE to build the application and git to maintain the version of the files Involved in Bug fixing of various modules that were raised by the testing teams in the application during integration testing phase Education M S in Computer Information Systems Florida Institute of Technology Melbourne FL Technology GITAM University Visakhapatnam Andhra Pradesh Skills Hive 1 year JIRA 2 years Scala 1 year sql 2 years web services 2 years",
    "extracted_keywords": [
        "Hadoop",
        "Developer",
        "Hadoop",
        "span",
        "lDeveloperspan",
        "Hadoop",
        "Developer",
        "Vanguard",
        "Group",
        "Charlotte",
        "NC",
        "years",
        "experience",
        "software",
        "design",
        "development",
        "testing",
        "maintenance",
        "web",
        "enterprise",
        "applications",
        "Java",
        "python",
        "AWS",
        "Hadoop",
        "Hadoop",
        "ecosystem",
        "components",
        "line",
        "HDFS",
        "MapReduce",
        "Hive",
        "Kafka",
        "Pig",
        "Hive",
        "Sqoop",
        "Oozie",
        "Spark",
        "AWS",
        "components",
        "EMR",
        "S3",
        "EC2",
        "Lamda",
        "functions",
        "kinesis",
        "technology",
        "situation",
        "team",
        "size",
        "US",
        "employer",
        "Work",
        "Experience",
        "Hadoop",
        "Developer",
        "Vanguard",
        "Group",
        "Charlotte",
        "NC",
        "October",
        "Present",
        "Responsibilities",
        "Data",
        "Lake",
        "AWS",
        "cloud",
        "end",
        "data",
        "ingestion",
        "pipeline",
        "data",
        "premise",
        "AWS",
        "cloud",
        "AgileScrum",
        "methodology",
        "AWS",
        "services",
        "EMR",
        "Cloud",
        "Watch",
        "Cloud",
        "Formation",
        "EC2",
        "S3",
        "Lambda",
        "data",
        "technologies",
        "Sqoop",
        "Oozie",
        "Hive",
        "Hue",
        "Tez",
        "Yarn",
        "Zepplin",
        "Developed",
        "Scala",
        "scripts",
        "Dataframessql",
        "spark",
        "2X",
        "data",
        "aggregation",
        "queries",
        "data",
        "OLTP",
        "system",
        "Sqoop",
        "Developed",
        "shell",
        "scripts",
        "workflows",
        "data",
        "ingestion",
        "python",
        "scripts",
        "Json",
        "Oozie",
        "workflows",
        "data",
        "sets",
        "partitions",
        "Spark",
        "memory",
        "capabilities",
        "Transformations",
        "ingestion",
        "process",
        "python",
        "auto",
        "generate",
        "cloud",
        "formation",
        "templates",
        "format",
        "AWS",
        "data",
        "environment",
        "reporting",
        "tool",
        "Tableau",
        "Hive",
        "reports",
        "data",
        "file",
        "formats",
        "TEXTFILE",
        "SEQUENCE",
        "FILE",
        "AVRO",
        "FILE",
        "ORC",
        "PARQUET",
        "hive",
        "querying",
        "migration",
        "microservices",
        "Webapps",
        "web",
        "services",
        "OnPrem",
        "PCF",
        "AWS",
        "PCF",
        "Worked",
        "Dashboards",
        "monitoring",
        "alerts",
        "Splunk",
        "webapps",
        "web",
        "services",
        "AWS",
        "CICD",
        "Bit",
        "Bucket",
        "JIRA",
        "Bamboo",
        "Confluence",
        "Hadoop",
        "Developer",
        "Thales",
        "Melbourne",
        "FL",
        "July",
        "September",
        "Responsibilities",
        "loading",
        "data",
        "Spark",
        "Scala",
        "Spark",
        "SQL",
        "testing",
        "processing",
        "data",
        "Spark",
        "performance",
        "optimization",
        "algorithms",
        "Hadoop",
        "Spark",
        "Context",
        "SparkSQL",
        "Data",
        "Frame",
        "Pair",
        "RDDs",
        "Hive",
        "tables",
        "partitions",
        "imports",
        "queries",
        "data",
        "jobs",
        "files",
        "HDFS",
        "file",
        "location",
        "Analyzed",
        "logs",
        "Logstash",
        "Experienced",
        "tableau",
        "desktopserver",
        "data",
        "csv",
        "files",
        "server",
        "2X",
        "Java",
        "Hadoop",
        "Developer",
        "Veritis",
        "Group",
        "Inc",
        "Irving",
        "TX",
        "May",
        "May",
        "Responsibilities",
        "files",
        "HDFS",
        "java",
        "kafka",
        "publisher",
        "consumer",
        "APIs",
        "streaming",
        "application",
        "HDFS",
        "Java",
        "API",
        "files",
        "cucumber",
        "test",
        "cases",
        "data",
        "transformations",
        "Pig",
        "transformations",
        "event",
        "filters",
        "preaggregations",
        "data",
        "tables",
        "partitions",
        "data",
        "storage",
        "format",
        "hive",
        "Worked",
        "Hibernate",
        "framework",
        "data",
        "MySQL",
        "Database",
        "Log4j",
        "purpose",
        "file",
        "auditing",
        "purpose",
        "Written",
        "shell",
        "scripts",
        "file",
        "Automation",
        "process",
        "git",
        "code",
        "repository",
        "project",
        "JavaJ2EE",
        "Developer",
        "Adeptros",
        "Bengaluru",
        "Karnataka",
        "January",
        "June",
        "Responsibilities",
        "phases",
        "Software",
        "Development",
        "Life",
        "Cycle",
        "SDLC",
        "requirements",
        "analysis",
        "design",
        "development",
        "testing",
        "J2EE",
        "design",
        "patterns",
        "functionality",
        "applications",
        "test",
        "cases",
        "JUnit",
        "unit",
        "testing",
        "application",
        "Wrote",
        "SQL",
        "data",
        "database",
        "JDBC",
        "frameworks",
        "Hibernate",
        "Spring",
        "persistence",
        "application",
        "layers",
        "programming",
        "Java",
        "business",
        "logic",
        "Log4J",
        "logs",
        "files",
        "Application",
        "Log",
        "Error",
        "Log",
        "state",
        "transfer",
        "REST",
        "Web",
        "services",
        "systems",
        "data",
        "client",
        "side",
        "Developed",
        "UNIX",
        "Shell",
        "scripts",
        "project",
        "management",
        "tasks",
        "track",
        "issues",
        "tasks",
        "individuals",
        "JIRA",
        "ticketing",
        "system",
        "Eclipse",
        "IDE",
        "application",
        "git",
        "version",
        "files",
        "Bug",
        "fixing",
        "modules",
        "testing",
        "teams",
        "application",
        "integration",
        "testing",
        "phase",
        "Education",
        "M",
        "S",
        "Computer",
        "Information",
        "Systems",
        "Florida",
        "Institute",
        "Technology",
        "Melbourne",
        "FL",
        "Technology",
        "GITAM",
        "University",
        "Visakhapatnam",
        "Andhra",
        "Pradesh",
        "Skills",
        "Hive",
        "year",
        "JIRA",
        "years",
        "Scala",
        "year",
        "sql",
        "years",
        "web",
        "services",
        "years"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T19:50:11.557883",
    "resume_data": "Hadoop Developer Hadoop span lDeveloperspan Hadoop Developer Vanguard Group Charlotte NC Over 5 years of experience in software design development testing and maintenance of web and enterprise applications using Java python AWS and Hadoop Experienced on Hadoop ecosystem using several components line HDFS MapReduce Hive Kafka Pig Hive Sqoop Oozie and Spark Worked on several AWS components like EMR S3 EC2 and also familiar with Lamda functions kinesis streaming Easily adoptable to any new technology and try to explore whenever a situation is needed Can easily coordinate with the small or big team size Authorized to work in the US for any employer Work Experience Hadoop Developer Vanguard Group Charlotte NC October 2017 to Present Responsibilities Built Data Lake in the AWS cloud Worked on creating an end to end data ingestion pipeline to transfer data on premise to AWS cloud under AgileScrum methodology Worked on AWS services like EMR Cloud Watch Cloud Formation EC2 S3 Lambda Big data technologies like Sqoop Oozie Hive Hue Tez Yarn Zepplin Developed Scala scripts using Dataframessql in spark 2X for data aggregation queries and writing data back into OLTP system through Sqoop Developed shell scripts to automate the workflows to perform data ingestion python scripts to generate Json Oozie workflows Experienced in handling large data sets using partitions Spark in memory capabilities Effective and efficient joins Transformations and other during ingestion process Used troposphere in python to auto generate cloud formation templates in JSON format to deploy AWS big data environment Used reporting tool Tableau to connect with Hive for generating daily reports of data Worked with different file formats like TEXTFILE SEQUENCE FILE AVRO FILE ORC and PARQUET for hive querying and processing Worked in migration of several microservices Webapps and web services from OnPrem PCF to AWS PCF Worked on setting up Dashboards monitoring and alerts in Splunk for webapps and web services on AWS Worked on Atlassian CICD stack namely Bit Bucket JIRA Bamboo Confluence Hadoop Developer Thales Melbourne FL July 2017 to September 2017 Responsibilities Worked on loading and transforming structured semi structured and unstructured data Implemented Spark using Scala and Spark SQL for faster testing and processing of data Exploring with Spark improving performance and optimization of the existing algorithms in Hadoop using Spark Context SparkSQL Data Frame and Pair RDDs Created Hive tables partitions and implemented incremental imports to perform adhoc queries on structured data Developed jobs to move inbound files to HDFS file location based on monthly weekly daily and hourly partitioning Analyzed and filtered the logs using Logstash Experienced on tableau desktopserver to visualize the data from csv files and hive server 2X Java Hadoop Developer Veritis Group Inc Irving TX May 2016 to May 2017 Responsibilities Ingested files to HDFS using java kafka publisher and consumer APIs for streaming application Experienced in using HDFS Java API to put the files to HDFS Implemented cucumber test cases to compare data before and after transformations Used Pig as to do transformations event joins filters and some preaggregations before storing the data onto HDFS Created external and internal tables partitions and data storage format in hive Worked on Hibernate framework to insert and update the data in MySQL Database Used Log4j for logging purpose and write it to a file for auditing purpose Written shell scripts for file Automation process Used git for code repository and maven to build the project JavaJ2EE Developer Adeptros Bengaluru Karnataka January 2014 to June 2014 Responsibilities Involved in various phases of Software Development Life Cycle SDLC such as requirements gathering modeling analysis design development and testing Used J2EE design patterns for adding new functionality to existing applications Create and execute test cases in JUnit for unit testing of application Wrote SQL queries to retrieve data from the database using JDBC Utilized frameworks such as Hibernate and Spring for persistence and application layers Utilized objectoriented programming and Java for creating business logic Used Log4J for writing into different logs files Application Log and Error Log Implemented Representational state transfer REST Web services for distributed systems to retrieve data from client side Developed UNIX Shell scripts for automating project management tasks To keep track of issues and tasks on individuals used JIRA ticketing system Using Eclipse integrated IDE to build the application and git to maintain the version of the files Involved in Bug fixing of various modules that were raised by the testing teams in the application during integration testing phase Education M S in Computer Information Systems Florida Institute of Technology Melbourne FL Technology GITAM University Visakhapatnam Andhra Pradesh Skills Hive 1 year JIRA 2 years Scala 1 year sql 2 years web services 2 years",
    "unique_id": "498018a0-a201-4467-bf2b-2784604ff697"
}