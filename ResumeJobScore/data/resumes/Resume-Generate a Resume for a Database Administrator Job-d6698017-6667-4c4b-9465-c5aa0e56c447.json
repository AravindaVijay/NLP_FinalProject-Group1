{
    "clean_data": "Python Full Stack Developer span lPythonspan Full Stack span lDeveloperspan Python Full Stack Developer US Global Mail Around 5 years working experience in Programming languages like python java ruby JavaScript Around 3 years of software experience in Analysis Design Development of web applications using python frameworks Django Flask Around 2 years working experience in cloud services like AWS and Build tools like Jenkins Maven Experience using python libraries including Numpy Pandas Scipy Matplotlib HTTPLib2 Urllib2 Pickle and Beautiful Soup during the software lifecycle development Excellent Knowledge on using Python Deep learning library ScikitLearn and Scipy machine learning libraries for python Experience with Frontend technologies like HTML CSS Bootstrap jQuery Angular and Angular 2 Extensive Experience in Analysis designing and developing web based applications and clientserver application using HTML5 JavaScript jQuery Ajax Angular Python and java Experience working with relational databases including MySQL Oracle PostgreSQL and NoSQL databases like MongoDB Experience in developing applications with SOAP and REST web services Experience Developing RESTAPIs Using Swagger and nodeJs Experience in developing applications using LAMP WAMP application stack Experience writing Sub Queries and working with features like Cursors Triggers Procedures Functions relational databases Experience in Unit Testing Integration Testing and Load Testing using libraries like Pytest unit test mock and nose Experience working with the version control systems like SVN GIT In depth knowledge in Creating managing and deploying applications using container Engine like Docker Experience deploying multicontainer python based docker applications using docker compose Experience building maintaining and deploying Docker based images on enterprise scale AWS Elastic Container Service Excellent understanding of continuous integrations and continuous delivery pipeline and its implementation using Jenkins as well as AWS Code Pipeline Experience in building Virtual Private Cloud and restricting access using NACL and Security Groups 1 year experience in creating server less web application using AWS services like Lambda DynamoDB API Gateway S3 Cognito 2 year experience in writing Hadoop MapReduce jobs Apache Pig Scripts and Apache Spark jobs in both python and java Experience in managing and reviewing Hadoop Log files using FLUME and Kafka Efficient in working with Hive data warehouse tool creating tables data distributing by implementing Partitioning and Bucketing strategy writing and optimizing the HiveQL queries Extending HIVE and PIG core functionality by using custom User Define Functions UDF User Defined TableGenerating Functions UDTF and User Defined Aggregating Functions UDAF for Hive and Pig Implemented advanced procedures like text analytics and processing using the inmemory computing capabilities like Apache Spark written in Scala Worked with Spark engine to process large scale data and experience to create Spark RDD and developing Spark Streaming jobs by using RDDs and leverage SparkShell Having experience on RDD architecture and implementing Spark operations on RDD and also optimizing transformations and actions in Spark Hands on experience in Apache Spark jobs using Scala in test environment for faster data processing and used SparkSQL for querying Work Experience Python Full Stack Developer US Global Mail Houston TX May 2018 to Present Responsibilities Designed and developed new front end for Django based Web application using Django CMS JavaScript Jquery Bootstrap Containerized companys existing web applications using Docker and deployed the docker images using a 6 machine cluster using Docker Swarm Has written specs for new features using BDD framework Gherkin and has done smoke testing regression testing using behave and behaving modules in python Integrated new website with the existing Django Backend Deployed the web application on Azure and Aws and have experience working with Continuous Integration tools like Jenkins Developing new backend for the proposed site using Django Framework and Python from scratch Have used Json Rest api and Django Rest Framework to implement shipping tracking for carriers like fedex dhl etc and for the warehouse operations Used Git for Version Control Developed prototype for the current web application in invision app Migrated entire mysql database to Postgres Suppoted companies existing web application with hotfixes and new feature development in python and Django Migrated Django cms based frontend to Wordpress cms Project lead for new site front end Was part of 4 major releases Environment HTML JavaScript CSS Bootstrap Jquery Python Django Aws Azure Git Jira Agile Docker GRADUATE ASSISTANT Wright State University January 2017 to December 2017 Role SparkHadoop Developer Responsibilities Hands on experience in Spark and Spark Streaming creating RDD applying operations transformations and Developed Spark applications using Scala for easy Hadoop transitions Used Spark and SparkSQL to read the parquet data and create the tables in hive using the Scala API Performed advanced procedures like text analytics and processing using the inmemory computing capabilities of Spark using Scala Developed Spark code using Scala and SparkSQL for faster processing and testing Implemented Spark sample programs in python using pyspark Analyzed the SQL scripts and designed the solution to implement using pyspark Developed pyspark code to mimic the transformations performed in the on premise environment Used SparkStreaming APIs to perform necessary transformations and actions on the fly for building the common learner data model which gets the data from Kafka in near real time Experienced with different scripting language like Python and shell scripts Developed various Python scripts to find vulnerabilities with SQL Queries by doing SQL injection permission checks and performance analysis Involved in converting HiveSQL queries into Spark transformations using Spark RDD Scala and Python Worked on Spark SQL created Data frames by loading data from Hive tables and created prep data and stored in AWS S3 Using SparkStreaming APIs to perform transformations and actions on the fly for building the common learner data model which gets the data from Kafka in near real time and Persists into Cassandra Involvement in creating custom UDFs for Pig and Hive to consolidated strategies and usefulness of Python into Pig Latin and HQL HiveQL Involved in converting HiveSQL queries into Spark transformations using Spark RDD Scala and Python Worked on Spark SQL created Data frames by loading data from Hive tables and created prep data and stored in AWS S3 Developed various data loading strategies and performed various transformations for analyzing the datasets by using Hortonworks Distribution for Hadoop ecosystem Python developer KHUB Khammam November 2015 to December 2016 Responsibilities Performed the requirement specifications and designing of modules Followed best practices quality and consistency in project development Actively Involved in Design Development Testing Deployment of web applications Design and Develop webservices using Django and Flask Frameworks Created Views Forms Templates and developing database Models APIs using python Connected different python model classes to NoSQL database using MongoDB Developed single page web application using Angular HTML5 CSS3 Bootstrap jQuery for interactive crossbrowser functionality and for validating the user information according to business rules and requirements Used AJAX in the user interface to avoid reloading the entire page by simply changing a small portion of data on web page Develop stored procedures functions and triggers in SQL and PLSQL Responsible in developing Code snippets and Queries for parsing the data in required data format and store them into the databases Developed RESTful web services for receiving and sending the data from an external source like web APIs in JSON format Modified existing modules of PythonDjango to generate a data in specific form Extracted unstructured data by web scraping different web sources using Beautiful Soup module to use the data for further manipulations Developed application in LAMP Apache Linux MySQL PythonPHP architecture Used Jira for tracking bug defects and management of project Used GIT for version control Experience in Unit Testing Integration testing and Load Testing using pythons testing libraries Involved in troubleshooting the application as well as debugging Environment Python Django HTML CSS JavaScript Bootstrap AngularJS Linux JSON MySQL Jira REST College Student Intern Associate SW Developer EMC Corporation January 2015 to November 2015 Python developer Responsibilities Worked in DPAD as python developer in Core Technical Division for Bangalore office Worked for Emc product Avamar which is an enterprise level backup and recovery software Developed automation tools for product Avamar using Python wrote many code snippets in Linux environment for automating the process and to fetch the information from server side following Data Deduplication strategies Migrated the entire database from SQLLite3 to PostgreSQL with the data integration Created database tables and generated EntityRelationship diagrams across different databases Developed the Frontend that is Avamar GUI using HTML CSS JavaScript AngularJS and applications using Django Framework Has automated installation of many avamar servers by writing code snippets in python both physical and virtual and by checking the avamar version build and replacing it with the latest build if available from repository using SFTP Telnet SSH etc Has written numerous test cases and validated for bugs in Avamars MSI GUI interface Experienced in automation using Python Shell scripting Responsible for windows functionality testing with Automation script in python using selenium webdriver package Responsible for testing various Avamar Functionalities like Backup Recover Clone Client Push Disaster Recovery Cluster HA testing offline and online recovery Responsible for multiple releases like Cumulative hot fixes service packs and main releases Internship HCL Technologies Chennai Tamil Nadu May 2013 to November 2013 Responsibilities Involved in Designing and writing unit test cases in python as well as in code reviews Developed UI using HTML CSS JavaScript AJAX Involved in writing python code snippets to perform manipulations on files Used various python modules numpy matplotlib ploty for data visualization Used the pythons in built modules pandas for Statistical analysis of data Created property list for individual applications dynamically in python Migrated entire application framework from java to python Has written numerous MySQL queries and executed them using pure python module MySQL connector and mysqldb Developed scripts in SQL for validating the data loading Performed various merge operations in python for retrieving and loading the data into the databases Developed Functions triggers and stored procedures Performed system testing unit testing and integration testing Environment Python MySQL PLSQL HTML CSS JavaScript AJAX Git Linux Apache Web Server Education Masters in Computer Science in Computer Science Wright State University Dayton OH January 2016 to December 2017 Bachelors in Computer Science in Computer Science SRM University Chennai Tamil Nadu Skills Apache spark C Django Docker Git Javascript Bootstrap Jenkins Json Mapreduce Php Pig Prolog Python Flask Numpy Pandas Pyspark Ruby Scripting Additional Information TECHNICAL SKILLS Programming Python Java C C SQL SCHEME MapReduce Languages Ruby Pig scripting pcap programming shell scripting Prolog Web technologies HTML5 CSS php JavaScript jQuery AJAX AngularJS Bootstrap AngularJS 2 XML Json Build Tools and IDEs Jenkins Maven Eclipse PyCharm Sublime text Notepad Spyder Microsoft Visual studioWireshark Databases MySQL PLSQL PostgreSQL AWS Aurora DynamoDB MongoDB Version Control Tools CVS GIT Bit Bucket Python Libraries NumPy Pandas Beautiful Soup SciKitlearn urllib2 pycopg2 PySpark Operating System Windows Linux MacOS Unix Big Data MapReduce Apache Pig Apache Spark Cloud Services Aws Testing Tools SeleniumQtp Unittest pytest Web Services Soap Rest Web Frameworks Django Flask Spring Hibernate Deployment Tools Chef AWS Code Commit Docker Docker Compose Docker Swarm Serverless Technology Aws Lambda Monitoring and Logging Cloud Watch and Cloud Trail Messaging Queues AWS SQS",
    "entities": [
        "Implemented Spark",
        "FLUME",
        "Designing",
        "MapReduce Languages Ruby Pig",
        "Sub Queries",
        "Developed Spark",
        "Postgres Suppoted",
        "JSON",
        "Beautiful Soup",
        "Pandas Pyspark Ruby Scripting Additional Information TECHNICAL SKILLS Programming Python",
        "RDD",
        "Hadoop",
        "SOAP",
        "Actively Involved in Design Development Testing Deployment",
        "AWS S3 Developed",
        "Creating",
        "Backup Recover Clone Client",
        "Shell",
        "HTML CSS Bootstrap jQuery Angular and Angular 2 Extensive",
        "Apache Spark",
        "LAMP",
        "Angular HTML5 CSS3 Bootstrap jQuery",
        "Python Worked on",
        "SparkSQL",
        "Developed",
        "Cumulative",
        "Unit Testing Integration",
        "Hadoop MapReduce",
        "Hadoop Log",
        "Cassandra Involvement",
        "Django Framework",
        "Spark Hands",
        "Django Backend Deployed",
        "ScikitLearn",
        "PythonPHP",
        "Linux",
        "Wright State University",
        "SQL Queries",
        "Spark Streaming",
        "PythonDjango",
        "Spark",
        "SparkShell",
        "GIT",
        "AWS Elastic Container Service",
        "Programming",
        "NACL",
        "Data Deduplication",
        "Hortonworks Distribution for Hadoop",
        "HTML CSS JavaScript",
        "HIVE",
        "Analysis",
        "Present Responsibilities Designed",
        "Created",
        "BDD",
        "AWS",
        "Wordpress",
        "PIG",
        "java",
        "AWS Code Pipeline",
        "SQL",
        "Responsibilities Worked",
        "Analysis Design Development",
        "Hive",
        "Core Technical Division",
        "HTML5 CSS",
        "SQS",
        "Build",
        "Maven",
        "Performed",
        "Spark SQL",
        "Migrated",
        "Notepad Spyder Microsoft Visual",
        "Virtual Private Cloud",
        "SVN",
        "Pytest",
        "Scala Developed Spark",
        "REST",
        "Data",
        "Unit Testing Integration Testing and Load Testing",
        "Oracle PostgreSQL",
        "NoSQL",
        "HTML CSS JavaScript AJAX Involved",
        "Used Git for Version Control Developed",
        "Internship HCL Technologies",
        "Control Tools CVS",
        "Engine",
        "Bucketing",
        "Connected"
    ],
    "experience": "Experience using python libraries including Numpy Pandas Scipy Matplotlib HTTPLib2 Urllib2 Pickle and Beautiful Soup during the software lifecycle development Excellent Knowledge on using Python Deep learning library ScikitLearn and Scipy machine learning libraries for python Experience with Frontend technologies like HTML CSS Bootstrap jQuery Angular and Angular 2 Extensive Experience in Analysis designing and developing web based applications and clientserver application using HTML5 JavaScript jQuery Ajax Angular Python and java Experience working with relational databases including MySQL Oracle PostgreSQL and NoSQL databases like MongoDB Experience in developing applications with SOAP and REST web services Experience Developing RESTAPIs Using Swagger and nodeJs Experience in developing applications using LAMP WAMP application stack Experience writing Sub Queries and working with features like Cursors Triggers Procedures Functions relational databases Experience in Unit Testing Integration Testing and Load Testing using libraries like Pytest unit test mock and nose Experience working with the version control systems like SVN GIT In depth knowledge in Creating managing and deploying applications using container Engine like Docker Experience deploying multicontainer python based docker applications using docker compose Experience building maintaining and deploying Docker based images on enterprise scale AWS Elastic Container Service Excellent understanding of continuous integrations and continuous delivery pipeline and its implementation using Jenkins as well as AWS Code Pipeline Experience in building Virtual Private Cloud and restricting access using NACL and Security Groups 1 year experience in creating server less web application using AWS services like Lambda DynamoDB API Gateway S3 Cognito 2 year experience in writing Hadoop MapReduce jobs Apache Pig Scripts and Apache Spark jobs in both python and java Experience in managing and reviewing Hadoop Log files using FLUME and Kafka Efficient in working with Hive data warehouse tool creating tables data distributing by implementing Partitioning and Bucketing strategy writing and optimizing the HiveQL queries Extending HIVE and PIG core functionality by using custom User Define Functions UDF User Defined TableGenerating Functions UDTF and User Defined Aggregating Functions UDAF for Hive and Pig Implemented advanced procedures like text analytics and processing using the inmemory computing capabilities like Apache Spark written in Scala Worked with Spark engine to process large scale data and experience to create Spark RDD and developing Spark Streaming jobs by using RDDs and leverage SparkShell Having experience on RDD architecture and implementing Spark operations on RDD and also optimizing transformations and actions in Spark Hands on experience in Apache Spark jobs using Scala in test environment for faster data processing and used SparkSQL for querying Work Experience Python Full Stack Developer US Global Mail Houston TX May 2018 to Present Responsibilities Designed and developed new front end for Django based Web application using Django CMS JavaScript Jquery Bootstrap Containerized companys existing web applications using Docker and deployed the docker images using a 6 machine cluster using Docker Swarm Has written specs for new features using BDD framework Gherkin and has done smoke testing regression testing using behave and behaving modules in python Integrated new website with the existing Django Backend Deployed the web application on Azure and Aws and have experience working with Continuous Integration tools like Jenkins Developing new backend for the proposed site using Django Framework and Python from scratch Have used Json Rest api and Django Rest Framework to implement shipping tracking for carriers like fedex dhl etc and for the warehouse operations Used Git for Version Control Developed prototype for the current web application in invision app Migrated entire mysql database to Postgres Suppoted companies existing web application with hotfixes and new feature development in python and Django Migrated Django cms based frontend to Wordpress cms Project lead for new site front end Was part of 4 major releases Environment HTML JavaScript CSS Bootstrap Jquery Python Django Aws Azure Git Jira Agile Docker GRADUATE ASSISTANT Wright State University January 2017 to December 2017 Role SparkHadoop Developer Responsibilities Hands on experience in Spark and Spark Streaming creating RDD applying operations transformations and Developed Spark applications using Scala for easy Hadoop transitions Used Spark and SparkSQL to read the parquet data and create the tables in hive using the Scala API Performed advanced procedures like text analytics and processing using the inmemory computing capabilities of Spark using Scala Developed Spark code using Scala and SparkSQL for faster processing and testing Implemented Spark sample programs in python using pyspark Analyzed the SQL scripts and designed the solution to implement using pyspark Developed pyspark code to mimic the transformations performed in the on premise environment Used SparkStreaming APIs to perform necessary transformations and actions on the fly for building the common learner data model which gets the data from Kafka in near real time Experienced with different scripting language like Python and shell scripts Developed various Python scripts to find vulnerabilities with SQL Queries by doing SQL injection permission checks and performance analysis Involved in converting HiveSQL queries into Spark transformations using Spark RDD Scala and Python Worked on Spark SQL created Data frames by loading data from Hive tables and created prep data and stored in AWS S3 Using SparkStreaming APIs to perform transformations and actions on the fly for building the common learner data model which gets the data from Kafka in near real time and Persists into Cassandra Involvement in creating custom UDFs for Pig and Hive to consolidated strategies and usefulness of Python into Pig Latin and HQL HiveQL Involved in converting HiveSQL queries into Spark transformations using Spark RDD Scala and Python Worked on Spark SQL created Data frames by loading data from Hive tables and created prep data and stored in AWS S3 Developed various data loading strategies and performed various transformations for analyzing the datasets by using Hortonworks Distribution for Hadoop ecosystem Python developer KHUB Khammam November 2015 to December 2016 Responsibilities Performed the requirement specifications and designing of modules Followed best practices quality and consistency in project development Actively Involved in Design Development Testing Deployment of web applications Design and Develop webservices using Django and Flask Frameworks Created Views Forms Templates and developing database Models APIs using python Connected different python model classes to NoSQL database using MongoDB Developed single page web application using Angular HTML5 CSS3 Bootstrap jQuery for interactive crossbrowser functionality and for validating the user information according to business rules and requirements Used AJAX in the user interface to avoid reloading the entire page by simply changing a small portion of data on web page Develop stored procedures functions and triggers in SQL and PLSQL Responsible in developing Code snippets and Queries for parsing the data in required data format and store them into the databases Developed RESTful web services for receiving and sending the data from an external source like web APIs in JSON format Modified existing modules of PythonDjango to generate a data in specific form Extracted unstructured data by web scraping different web sources using Beautiful Soup module to use the data for further manipulations Developed application in LAMP Apache Linux MySQL PythonPHP architecture Used Jira for tracking bug defects and management of project Used GIT for version control Experience in Unit Testing Integration testing and Load Testing using pythons testing libraries Involved in troubleshooting the application as well as debugging Environment Python Django HTML CSS JavaScript Bootstrap AngularJS Linux JSON MySQL Jira REST College Student Intern Associate SW Developer EMC Corporation January 2015 to November 2015 Python developer Responsibilities Worked in DPAD as python developer in Core Technical Division for Bangalore office Worked for Emc product Avamar which is an enterprise level backup and recovery software Developed automation tools for product Avamar using Python wrote many code snippets in Linux environment for automating the process and to fetch the information from server side following Data Deduplication strategies Migrated the entire database from SQLLite3 to PostgreSQL with the data integration Created database tables and generated EntityRelationship diagrams across different databases Developed the Frontend that is Avamar GUI using HTML CSS JavaScript AngularJS and applications using Django Framework Has automated installation of many avamar servers by writing code snippets in python both physical and virtual and by checking the avamar version build and replacing it with the latest build if available from repository using SFTP Telnet SSH etc Has written numerous test cases and validated for bugs in Avamars MSI GUI interface Experienced in automation using Python Shell scripting Responsible for windows functionality testing with Automation script in python using selenium webdriver package Responsible for testing various Avamar Functionalities like Backup Recover Clone Client Push Disaster Recovery Cluster HA testing offline and online recovery Responsible for multiple releases like Cumulative hot fixes service packs and main releases Internship HCL Technologies Chennai Tamil Nadu May 2013 to November 2013 Responsibilities Involved in Designing and writing unit test cases in python as well as in code reviews Developed UI using HTML CSS JavaScript AJAX Involved in writing python code snippets to perform manipulations on files Used various python modules numpy matplotlib ploty for data visualization Used the pythons in built modules pandas for Statistical analysis of data Created property list for individual applications dynamically in python Migrated entire application framework from java to python Has written numerous MySQL queries and executed them using pure python module MySQL connector and mysqldb Developed scripts in SQL for validating the data loading Performed various merge operations in python for retrieving and loading the data into the databases Developed Functions triggers and stored procedures Performed system testing unit testing and integration testing Environment Python MySQL PLSQL HTML CSS JavaScript AJAX Git Linux Apache Web Server Education Masters in Computer Science in Computer Science Wright State University Dayton OH January 2016 to December 2017 Bachelors in Computer Science in Computer Science SRM University Chennai Tamil Nadu Skills Apache spark C Django Docker Git Javascript Bootstrap Jenkins Json Mapreduce Php Pig Prolog Python Flask Numpy Pandas Pyspark Ruby Scripting Additional Information TECHNICAL SKILLS Programming Python Java C C SQL SCHEME MapReduce Languages Ruby Pig scripting pcap programming shell scripting Prolog Web technologies HTML5 CSS php JavaScript jQuery AJAX AngularJS Bootstrap AngularJS 2 XML Json Build Tools and IDEs Jenkins Maven Eclipse PyCharm Sublime text Notepad Spyder Microsoft Visual studioWireshark Databases MySQL PLSQL PostgreSQL AWS Aurora DynamoDB MongoDB Version Control Tools CVS GIT Bit Bucket Python Libraries NumPy Pandas Beautiful Soup SciKitlearn urllib2 pycopg2 PySpark Operating System Windows Linux MacOS Unix Big Data MapReduce Apache Pig Apache Spark Cloud Services Aws Testing Tools SeleniumQtp Unittest pytest Web Services Soap Rest Web Frameworks Django Flask Spring Hibernate Deployment Tools Chef AWS Code Commit Docker Docker Compose Docker Swarm Serverless Technology Aws Lambda Monitoring and Logging Cloud Watch and Cloud Trail Messaging Queues AWS SQS",
    "extracted_keywords": [
        "Python",
        "Stack",
        "Developer",
        "lPythonspan",
        "Full",
        "Stack",
        "span",
        "lDeveloperspan",
        "Python",
        "Stack",
        "Developer",
        "US",
        "Global",
        "Mail",
        "years",
        "working",
        "experience",
        "Programming",
        "languages",
        "python",
        "java",
        "ruby",
        "JavaScript",
        "years",
        "software",
        "experience",
        "Analysis",
        "Design",
        "Development",
        "web",
        "applications",
        "frameworks",
        "Django",
        "Flask",
        "years",
        "working",
        "experience",
        "cloud",
        "services",
        "AWS",
        "Build",
        "tools",
        "Jenkins",
        "Maven",
        "Experience",
        "libraries",
        "Numpy",
        "Pandas",
        "Scipy",
        "Matplotlib",
        "HTTPLib2",
        "Urllib2",
        "Pickle",
        "Beautiful",
        "Soup",
        "software",
        "lifecycle",
        "development",
        "Excellent",
        "Knowledge",
        "Python",
        "Deep",
        "library",
        "ScikitLearn",
        "Scipy",
        "machine",
        "learning",
        "libraries",
        "python",
        "Experience",
        "Frontend",
        "technologies",
        "HTML",
        "CSS",
        "Bootstrap",
        "jQuery",
        "Angular",
        "Angular",
        "Experience",
        "Analysis",
        "web",
        "applications",
        "application",
        "HTML5",
        "JavaScript",
        "jQuery",
        "Ajax",
        "Angular",
        "Python",
        "java",
        "Experience",
        "databases",
        "MySQL",
        "Oracle",
        "PostgreSQL",
        "NoSQL",
        "MongoDB",
        "Experience",
        "applications",
        "SOAP",
        "REST",
        "web",
        "services",
        "Experience",
        "RESTAPIs",
        "Swagger",
        "nodeJs",
        "Experience",
        "applications",
        "LAMP",
        "WAMP",
        "application",
        "stack",
        "Experience",
        "Sub",
        "Queries",
        "features",
        "Cursors",
        "Triggers",
        "Procedures",
        "Functions",
        "Experience",
        "Unit",
        "Testing",
        "Integration",
        "Testing",
        "Load",
        "Testing",
        "libraries",
        "unit",
        "test",
        "nose",
        "Experience",
        "version",
        "control",
        "systems",
        "SVN",
        "GIT",
        "depth",
        "knowledge",
        "applications",
        "container",
        "Engine",
        "Docker",
        "Experience",
        "multicontainer",
        "python",
        "docker",
        "applications",
        "docker",
        "Experience",
        "Docker",
        "images",
        "enterprise",
        "scale",
        "AWS",
        "Elastic",
        "Container",
        "Service",
        "Excellent",
        "understanding",
        "integrations",
        "delivery",
        "pipeline",
        "implementation",
        "Jenkins",
        "AWS",
        "Code",
        "Pipeline",
        "Experience",
        "Virtual",
        "Cloud",
        "access",
        "NACL",
        "Security",
        "Groups",
        "year",
        "experience",
        "server",
        "web",
        "application",
        "AWS",
        "services",
        "Lambda",
        "DynamoDB",
        "API",
        "Gateway",
        "S3",
        "Cognito",
        "year",
        "experience",
        "Hadoop",
        "MapReduce",
        "jobs",
        "Apache",
        "Pig",
        "Scripts",
        "Apache",
        "Spark",
        "jobs",
        "python",
        "Experience",
        "Hadoop",
        "Log",
        "files",
        "FLUME",
        "Kafka",
        "Efficient",
        "Hive",
        "data",
        "warehouse",
        "tool",
        "tables",
        "data",
        "Partitioning",
        "strategy",
        "writing",
        "HiveQL",
        "queries",
        "HIVE",
        "PIG",
        "core",
        "functionality",
        "custom",
        "User",
        "Define",
        "Functions",
        "UDF",
        "User",
        "Defined",
        "TableGenerating",
        "Functions",
        "UDTF",
        "User",
        "Defined",
        "Aggregating",
        "Functions",
        "UDAF",
        "Hive",
        "Pig",
        "procedures",
        "text",
        "analytics",
        "processing",
        "inmemory",
        "computing",
        "capabilities",
        "Apache",
        "Spark",
        "Scala",
        "Spark",
        "engine",
        "scale",
        "data",
        "experience",
        "Spark",
        "RDD",
        "Spark",
        "Streaming",
        "jobs",
        "RDDs",
        "leverage",
        "SparkShell",
        "experience",
        "RDD",
        "architecture",
        "Spark",
        "operations",
        "RDD",
        "transformations",
        "actions",
        "Spark",
        "Hands",
        "experience",
        "Apache",
        "Spark",
        "jobs",
        "Scala",
        "test",
        "environment",
        "data",
        "processing",
        "SparkSQL",
        "Work",
        "Experience",
        "Python",
        "Stack",
        "Developer",
        "US",
        "Global",
        "Mail",
        "Houston",
        "TX",
        "May",
        "Present",
        "Responsibilities",
        "end",
        "Django",
        "Web",
        "application",
        "Django",
        "CMS",
        "JavaScript",
        "Jquery",
        "Bootstrap",
        "Containerized",
        "companys",
        "web",
        "applications",
        "Docker",
        "docker",
        "images",
        "machine",
        "cluster",
        "Docker",
        "Swarm",
        "specs",
        "features",
        "BDD",
        "framework",
        "Gherkin",
        "smoke",
        "testing",
        "regression",
        "testing",
        "behave",
        "modules",
        "python",
        "website",
        "Django",
        "Backend",
        "web",
        "application",
        "Azure",
        "Aws",
        "experience",
        "Continuous",
        "Integration",
        "tools",
        "Jenkins",
        "backend",
        "site",
        "Django",
        "Framework",
        "Python",
        "scratch",
        "Json",
        "Rest",
        "api",
        "Django",
        "Rest",
        "Framework",
        "shipping",
        "tracking",
        "carriers",
        "fedex",
        "dhl",
        "warehouse",
        "operations",
        "Git",
        "Version",
        "Control",
        "Developed",
        "prototype",
        "web",
        "application",
        "invision",
        "app",
        "mysql",
        "database",
        "Postgres",
        "companies",
        "web",
        "application",
        "hotfixes",
        "feature",
        "development",
        "python",
        "Django",
        "Django",
        "cms",
        "frontend",
        "cms",
        "Project",
        "lead",
        "site",
        "end",
        "part",
        "releases",
        "Environment",
        "HTML",
        "JavaScript",
        "CSS",
        "Bootstrap",
        "Jquery",
        "Python",
        "Django",
        "Aws",
        "Azure",
        "Git",
        "Jira",
        "Agile",
        "Docker",
        "GRADUATE",
        "ASSISTANT",
        "Wright",
        "State",
        "University",
        "January",
        "December",
        "Role",
        "SparkHadoop",
        "Developer",
        "Hands",
        "experience",
        "Spark",
        "Spark",
        "Streaming",
        "RDD",
        "operations",
        "transformations",
        "Spark",
        "applications",
        "Scala",
        "Hadoop",
        "transitions",
        "Spark",
        "SparkSQL",
        "parquet",
        "data",
        "tables",
        "hive",
        "Scala",
        "API",
        "procedures",
        "text",
        "analytics",
        "processing",
        "inmemory",
        "computing",
        "capabilities",
        "Spark",
        "Scala",
        "Developed",
        "Spark",
        "code",
        "Scala",
        "SparkSQL",
        "processing",
        "testing",
        "Spark",
        "sample",
        "programs",
        "python",
        "pyspark",
        "SQL",
        "scripts",
        "solution",
        "pyspark",
        "pyspark",
        "code",
        "transformations",
        "on",
        "premise",
        "environment",
        "SparkStreaming",
        "APIs",
        "transformations",
        "actions",
        "fly",
        "learner",
        "data",
        "model",
        "data",
        "Kafka",
        "time",
        "scripting",
        "language",
        "Python",
        "scripts",
        "Python",
        "scripts",
        "vulnerabilities",
        "SQL",
        "Queries",
        "SQL",
        "injection",
        "permission",
        "checks",
        "performance",
        "analysis",
        "HiveSQL",
        "queries",
        "Spark",
        "transformations",
        "Spark",
        "RDD",
        "Scala",
        "Python",
        "Spark",
        "SQL",
        "Data",
        "frames",
        "data",
        "Hive",
        "tables",
        "prep",
        "data",
        "AWS",
        "S3",
        "SparkStreaming",
        "APIs",
        "transformations",
        "actions",
        "fly",
        "learner",
        "data",
        "model",
        "data",
        "Kafka",
        "time",
        "Persists",
        "Cassandra",
        "Involvement",
        "custom",
        "UDFs",
        "Pig",
        "Hive",
        "strategies",
        "usefulness",
        "Python",
        "Pig",
        "Latin",
        "HQL",
        "HiveQL",
        "HiveSQL",
        "queries",
        "Spark",
        "transformations",
        "Spark",
        "RDD",
        "Scala",
        "Python",
        "Spark",
        "SQL",
        "Data",
        "frames",
        "data",
        "Hive",
        "tables",
        "prep",
        "data",
        "AWS",
        "S3",
        "data",
        "loading",
        "strategies",
        "transformations",
        "datasets",
        "Hortonworks",
        "Distribution",
        "Hadoop",
        "ecosystem",
        "Python",
        "developer",
        "KHUB",
        "Khammam",
        "November",
        "December",
        "Responsibilities",
        "requirement",
        "specifications",
        "designing",
        "modules",
        "practices",
        "quality",
        "consistency",
        "project",
        "development",
        "Design",
        "Development",
        "Testing",
        "Deployment",
        "web",
        "applications",
        "Design",
        "Develop",
        "webservices",
        "Django",
        "Flask",
        "Frameworks",
        "Views",
        "Forms",
        "Templates",
        "database",
        "Models",
        "APIs",
        "model",
        "classes",
        "NoSQL",
        "database",
        "page",
        "web",
        "application",
        "Angular",
        "HTML5",
        "CSS3",
        "Bootstrap",
        "jQuery",
        "crossbrowser",
        "functionality",
        "user",
        "information",
        "business",
        "rules",
        "requirements",
        "AJAX",
        "user",
        "interface",
        "page",
        "portion",
        "data",
        "web",
        "page",
        "Develop",
        "procedures",
        "functions",
        "triggers",
        "SQL",
        "PLSQL",
        "Code",
        "snippets",
        "Queries",
        "data",
        "data",
        "format",
        "databases",
        "web",
        "services",
        "data",
        "source",
        "web",
        "APIs",
        "format",
        "modules",
        "PythonDjango",
        "data",
        "form",
        "data",
        "web",
        "web",
        "sources",
        "Beautiful",
        "Soup",
        "module",
        "data",
        "manipulations",
        "application",
        "LAMP",
        "Apache",
        "Linux",
        "MySQL",
        "PythonPHP",
        "architecture",
        "Jira",
        "bug",
        "defects",
        "management",
        "project",
        "GIT",
        "version",
        "control",
        "Experience",
        "Unit",
        "Testing",
        "Integration",
        "testing",
        "Load",
        "Testing",
        "pythons",
        "testing",
        "libraries",
        "application",
        "Environment",
        "Python",
        "Django",
        "HTML",
        "CSS",
        "JavaScript",
        "Bootstrap",
        "AngularJS",
        "Linux",
        "JSON",
        "MySQL",
        "Jira",
        "REST",
        "College",
        "Student",
        "Intern",
        "Associate",
        "SW",
        "Developer",
        "EMC",
        "Corporation",
        "January",
        "November",
        "Python",
        "developer",
        "Responsibilities",
        "DPAD",
        "python",
        "developer",
        "Core",
        "Technical",
        "Division",
        "Bangalore",
        "office",
        "Emc",
        "product",
        "Avamar",
        "enterprise",
        "level",
        "backup",
        "recovery",
        "software",
        "automation",
        "tools",
        "product",
        "Avamar",
        "Python",
        "code",
        "snippets",
        "Linux",
        "environment",
        "process",
        "information",
        "server",
        "side",
        "Data",
        "Deduplication",
        "strategies",
        "database",
        "SQLLite3",
        "PostgreSQL",
        "data",
        "integration",
        "database",
        "tables",
        "EntityRelationship",
        "diagrams",
        "databases",
        "Frontend",
        "Avamar",
        "GUI",
        "HTML",
        "CSS",
        "JavaScript",
        "AngularJS",
        "applications",
        "Django",
        "Framework",
        "installation",
        "avamar",
        "servers",
        "code",
        "snippets",
        "python",
        "avamar",
        "version",
        "build",
        "build",
        "repository",
        "SFTP",
        "Telnet",
        "SSH",
        "test",
        "cases",
        "bugs",
        "Avamars",
        "MSI",
        "GUI",
        "interface",
        "automation",
        "Python",
        "Shell",
        "windows",
        "functionality",
        "testing",
        "Automation",
        "script",
        "python",
        "selenium",
        "webdriver",
        "package",
        "Avamar",
        "Functionalities",
        "Backup",
        "Recover",
        "Clone",
        "Client",
        "Push",
        "Disaster",
        "Recovery",
        "Cluster",
        "HA",
        "testing",
        "offline",
        "recovery",
        "releases",
        "fixes",
        "service",
        "packs",
        "releases",
        "Internship",
        "HCL",
        "Technologies",
        "Chennai",
        "Tamil",
        "Nadu",
        "May",
        "November",
        "Responsibilities",
        "Designing",
        "writing",
        "unit",
        "test",
        "cases",
        "python",
        "code",
        "reviews",
        "UI",
        "HTML",
        "CSS",
        "JavaScript",
        "AJAX",
        "python",
        "code",
        "snippets",
        "manipulations",
        "files",
        "python",
        "modules",
        "numpy",
        "matplotlib",
        "data",
        "visualization",
        "pythons",
        "modules",
        "analysis",
        "data",
        "property",
        "list",
        "applications",
        "application",
        "framework",
        "python",
        "MySQL",
        "queries",
        "module",
        "MySQL",
        "connector",
        "scripts",
        "SQL",
        "data",
        "merge",
        "operations",
        "python",
        "data",
        "databases",
        "Developed",
        "Functions",
        "triggers",
        "procedures",
        "system",
        "testing",
        "unit",
        "testing",
        "integration",
        "testing",
        "Environment",
        "Python",
        "MySQL",
        "PLSQL",
        "HTML",
        "CSS",
        "JavaScript",
        "AJAX",
        "Git",
        "Linux",
        "Apache",
        "Web",
        "Server",
        "Education",
        "Masters",
        "Computer",
        "Science",
        "Computer",
        "Science",
        "Wright",
        "State",
        "University",
        "Dayton",
        "OH",
        "January",
        "December",
        "Bachelors",
        "Computer",
        "Science",
        "Computer",
        "Science",
        "SRM",
        "University",
        "Chennai",
        "Tamil",
        "Nadu",
        "Skills",
        "Apache",
        "spark",
        "C",
        "Django",
        "Docker",
        "Git",
        "Javascript",
        "Bootstrap",
        "Jenkins",
        "Json",
        "Mapreduce",
        "Php",
        "Pig",
        "Prolog",
        "Python",
        "Flask",
        "Numpy",
        "Pandas",
        "Pyspark",
        "Ruby",
        "Scripting",
        "Additional",
        "Information",
        "TECHNICAL",
        "SKILLS",
        "Programming",
        "Python",
        "Java",
        "C",
        "C",
        "SQL",
        "SCHEME",
        "MapReduce",
        "Languages",
        "Ruby",
        "Pig",
        "pcap",
        "programming",
        "shell",
        "Prolog",
        "Web",
        "technologies",
        "HTML5",
        "CSS",
        "php",
        "JavaScript",
        "jQuery",
        "AJAX",
        "Bootstrap",
        "AngularJS",
        "XML",
        "Json",
        "Build",
        "Tools",
        "IDEs",
        "Jenkins",
        "Maven",
        "Eclipse",
        "PyCharm",
        "Sublime",
        "text",
        "Notepad",
        "Spyder",
        "Microsoft",
        "Visual",
        "MySQL",
        "PLSQL",
        "PostgreSQL",
        "AWS",
        "Aurora",
        "DynamoDB",
        "MongoDB",
        "Version",
        "Control",
        "Tools",
        "CVS",
        "GIT",
        "Bit",
        "Bucket",
        "Python",
        "NumPy",
        "Pandas",
        "Beautiful",
        "Soup",
        "SciKitlearn",
        "urllib2",
        "PySpark",
        "System",
        "Windows",
        "Linux",
        "MacOS",
        "Unix",
        "Big",
        "Data",
        "MapReduce",
        "Apache",
        "Pig",
        "Apache",
        "Spark",
        "Cloud",
        "Services",
        "Aws",
        "Testing",
        "Tools",
        "SeleniumQtp",
        "Unittest",
        "Web",
        "Services",
        "Soap",
        "Rest",
        "Web",
        "Frameworks",
        "Django",
        "Flask",
        "Spring",
        "Hibernate",
        "Deployment",
        "Tools",
        "Chef",
        "AWS",
        "Code",
        "Commit",
        "Docker",
        "Docker",
        "Compose",
        "Docker",
        "Swarm",
        "Serverless",
        "Technology",
        "Aws",
        "Lambda",
        "Monitoring",
        "Logging",
        "Cloud",
        "Watch",
        "Cloud",
        "Trail",
        "Queues",
        "AWS",
        "SQS"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T19:46:48.523378",
    "resume_data": "Python Full Stack Developer span lPythonspan Full Stack span lDeveloperspan Python Full Stack Developer US Global Mail Around 5 years working experience in Programming languages like python java ruby JavaScript Around 3 years of software experience in Analysis Design Development of web applications using python frameworks Django Flask Around 2 years working experience in cloud services like AWS and Build tools like Jenkins Maven Experience using python libraries including Numpy Pandas Scipy Matplotlib HTTPLib2 Urllib2 Pickle and Beautiful Soup during the software lifecycle development Excellent Knowledge on using Python Deep learning library ScikitLearn and Scipy machine learning libraries for python Experience with Frontend technologies like HTML CSS Bootstrap jQuery Angular and Angular 2 Extensive Experience in Analysis designing and developing web based applications and clientserver application using HTML5 JavaScript jQuery Ajax Angular Python and java Experience working with relational databases including MySQL Oracle PostgreSQL and NoSQL databases like MongoDB Experience in developing applications with SOAP and REST web services Experience Developing RESTAPIs Using Swagger and nodeJs Experience in developing applications using LAMP WAMP application stack Experience writing Sub Queries and working with features like Cursors Triggers Procedures Functions relational databases Experience in Unit Testing Integration Testing and Load Testing using libraries like Pytest unit test mock and nose Experience working with the version control systems like SVN GIT In depth knowledge in Creating managing and deploying applications using container Engine like Docker Experience deploying multicontainer python based docker applications using docker compose Experience building maintaining and deploying Docker based images on enterprise scale AWS Elastic Container Service Excellent understanding of continuous integrations and continuous delivery pipeline and its implementation using Jenkins as well as AWS Code Pipeline Experience in building Virtual Private Cloud and restricting access using NACL and Security Groups 1 year experience in creating server less web application using AWS services like Lambda DynamoDB API Gateway S3 Cognito 2 year experience in writing Hadoop MapReduce jobs Apache Pig Scripts and Apache Spark jobs in both python and java Experience in managing and reviewing Hadoop Log files using FLUME and Kafka Efficient in working with Hive data warehouse tool creating tables data distributing by implementing Partitioning and Bucketing strategy writing and optimizing the HiveQL queries Extending HIVE and PIG core functionality by using custom User Define Functions UDF User Defined TableGenerating Functions UDTF and User Defined Aggregating Functions UDAF for Hive and Pig Implemented advanced procedures like text analytics and processing using the inmemory computing capabilities like Apache Spark written in Scala Worked with Spark engine to process large scale data and experience to create Spark RDD and developing Spark Streaming jobs by using RDDs and leverage SparkShell Having experience on RDD architecture and implementing Spark operations on RDD and also optimizing transformations and actions in Spark Hands on experience in Apache Spark jobs using Scala in test environment for faster data processing and used SparkSQL for querying Work Experience Python Full Stack Developer US Global Mail Houston TX May 2018 to Present Responsibilities Designed and developed new front end for Django based Web application using Django CMS JavaScript Jquery Bootstrap Containerized companys existing web applications using Docker and deployed the docker images using a 6 machine cluster using Docker Swarm Has written specs for new features using BDD framework Gherkin and has done smoke testing regression testing using behave and behaving modules in python Integrated new website with the existing Django Backend Deployed the web application on Azure and Aws and have experience working with Continuous Integration tools like Jenkins Developing new backend for the proposed site using Django Framework and Python from scratch Have used Json Rest api and Django Rest Framework to implement shipping tracking for carriers like fedex dhl etc and for the warehouse operations Used Git for Version Control Developed prototype for the current web application in invision app Migrated entire mysql database to Postgres Suppoted companies existing web application with hotfixes and new feature development in python and Django Migrated Django cms based frontend to Wordpress cms Project lead for new site front end Was part of 4 major releases Environment HTML JavaScript CSS Bootstrap Jquery Python Django Aws Azure Git Jira Agile Docker GRADUATE ASSISTANT Wright State University January 2017 to December 2017 Role SparkHadoop Developer Responsibilities Hands on experience in Spark and Spark Streaming creating RDD applying operations transformations and Developed Spark applications using Scala for easy Hadoop transitions Used Spark and SparkSQL to read the parquet data and create the tables in hive using the Scala API Performed advanced procedures like text analytics and processing using the inmemory computing capabilities of Spark using Scala Developed Spark code using Scala and SparkSQL for faster processing and testing Implemented Spark sample programs in python using pyspark Analyzed the SQL scripts and designed the solution to implement using pyspark Developed pyspark code to mimic the transformations performed in the on premise environment Used SparkStreaming APIs to perform necessary transformations and actions on the fly for building the common learner data model which gets the data from Kafka in near real time Experienced with different scripting language like Python and shell scripts Developed various Python scripts to find vulnerabilities with SQL Queries by doing SQL injection permission checks and performance analysis Involved in converting HiveSQL queries into Spark transformations using Spark RDD Scala and Python Worked on Spark SQL created Data frames by loading data from Hive tables and created prep data and stored in AWS S3 Using SparkStreaming APIs to perform transformations and actions on the fly for building the common learner data model which gets the data from Kafka in near real time and Persists into Cassandra Involvement in creating custom UDFs for Pig and Hive to consolidated strategies and usefulness of Python into Pig Latin and HQL HiveQL Involved in converting HiveSQL queries into Spark transformations using Spark RDD Scala and Python Worked on Spark SQL created Data frames by loading data from Hive tables and created prep data and stored in AWS S3 Developed various data loading strategies and performed various transformations for analyzing the datasets by using Hortonworks Distribution for Hadoop ecosystem Python developer KHUB Khammam November 2015 to December 2016 Responsibilities Performed the requirement specifications and designing of modules Followed best practices quality and consistency in project development Actively Involved in Design Development Testing Deployment of web applications Design and Develop webservices using Django and Flask Frameworks Created Views Forms Templates and developing database Models APIs using python Connected different python model classes to NoSQL database using MongoDB Developed single page web application using Angular HTML5 CSS3 Bootstrap jQuery for interactive crossbrowser functionality and for validating the user information according to business rules and requirements Used AJAX in the user interface to avoid reloading the entire page by simply changing a small portion of data on web page Develop stored procedures functions and triggers in SQL and PLSQL Responsible in developing Code snippets and Queries for parsing the data in required data format and store them into the databases Developed RESTful web services for receiving and sending the data from an external source like web APIs in JSON format Modified existing modules of PythonDjango to generate a data in specific form Extracted unstructured data by web scraping different web sources using Beautiful Soup module to use the data for further manipulations Developed application in LAMP Apache Linux MySQL PythonPHP architecture Used Jira for tracking bug defects and management of project Used GIT for version control Experience in Unit Testing Integration testing and Load Testing using pythons testing libraries Involved in troubleshooting the application as well as debugging Environment Python Django HTML CSS JavaScript Bootstrap AngularJS Linux JSON MySQL Jira REST College Student Intern Associate SW Developer EMC Corporation January 2015 to November 2015 Python developer Responsibilities Worked in DPAD as python developer in Core Technical Division for Bangalore office Worked for Emc product Avamar which is an enterprise level backup and recovery software Developed automation tools for product Avamar using Python wrote many code snippets in Linux environment for automating the process and to fetch the information from server side following Data Deduplication strategies Migrated the entire database from SQLLite3 to PostgreSQL with the data integration Created database tables and generated EntityRelationship diagrams across different databases Developed the Frontend that is Avamar GUI using HTML CSS JavaScript AngularJS and applications using Django Framework Has automated installation of many avamar servers by writing code snippets in python both physical and virtual and by checking the avamar version build and replacing it with the latest build if available from repository using SFTP Telnet SSH etc Has written numerous test cases and validated for bugs in Avamars MSI GUI interface Experienced in automation using Python Shell scripting Responsible for windows functionality testing with Automation script in python using selenium webdriver package Responsible for testing various Avamar Functionalities like Backup Recover Clone Client Push Disaster Recovery Cluster HA testing offline and online recovery Responsible for multiple releases like Cumulative hot fixes service packs and main releases Internship HCL Technologies Chennai Tamil Nadu May 2013 to November 2013 Responsibilities Involved in Designing and writing unit test cases in python as well as in code reviews Developed UI using HTML CSS JavaScript AJAX Involved in writing python code snippets to perform manipulations on files Used various python modules numpy matplotlib ploty for data visualization Used the pythons in built modules pandas for Statistical analysis of data Created property list for individual applications dynamically in python Migrated entire application framework from java to python Has written numerous MySQL queries and executed them using pure python module MySQL connector and mysqldb Developed scripts in SQL for validating the data loading Performed various merge operations in python for retrieving and loading the data into the databases Developed Functions triggers and stored procedures Performed system testing unit testing and integration testing Environment Python MySQL PLSQL HTML CSS JavaScript AJAX Git Linux Apache Web Server Education Masters in Computer Science in Computer Science Wright State University Dayton OH January 2016 to December 2017 Bachelors in Computer Science in Computer Science SRM University Chennai Tamil Nadu Skills Apache spark C Django Docker Git Javascript Bootstrap Jenkins Json Mapreduce Php Pig Prolog Python Flask Numpy Pandas Pyspark Ruby Scripting Additional Information TECHNICAL SKILLS Programming Python Java C C SQL SCHEME MapReduce Languages Ruby Pig scripting pcap programming shell scripting Prolog Web technologies HTML5 CSS php JavaScript jQuery AJAX AngularJS Bootstrap AngularJS 2 XML Json Build Tools and IDEs Jenkins Maven Eclipse PyCharm Sublime text Notepad Spyder Microsoft Visual studioWireshark Databases MySQL PLSQL PostgreSQL AWS Aurora DynamoDB MongoDB Version Control Tools CVS GIT Bit Bucket Python Libraries NumPy Pandas Beautiful Soup SciKitlearn urllib2 pycopg2 PySpark Operating System Windows Linux MacOS Unix Big Data MapReduce Apache Pig Apache Spark Cloud Services Aws Testing Tools SeleniumQtp Unittest pytest Web Services Soap Rest Web Frameworks Django Flask Spring Hibernate Deployment Tools Chef AWS Code Commit Docker Docker Compose Docker Swarm Serverless Technology Aws Lambda Monitoring and Logging Cloud Watch and Cloud Trail Messaging Queues AWS SQS",
    "unique_id": "d6698017-6667-4c4b-9465-c5aa0e56c447"
}