{
    "clean_data": "Hadoop Developer Hadoop span lDeveloperspan Hadoop Developer Infor Grenville Grenville SD Having 8 years of experience in Consulting Analysis Implementation of Java and Big Data solutions for various project assignments Lead contributor for Big Data Centre of excellence in various emerging technologies like Big Data Texts analytics No SQL databases and other related areas Focus on designing and delivering most optimum and critical business solutions for Big Data Technologies Keen in building knowledge on emerging technologies in the Analytics Information Management Big data Data science and related areas and in providing best business solutions Experienced in individual phases of project lifecycle with emphasis on Planning Designing and Coding Experienced in Hadoop environment technologiesPlatforms like Pig Hive Sqoop Consulting for Java Big Data Technologies Evaluation of new technologies in Big data Analytics and NO Sql space Exclusive experience in Hadoop and its components like HDFS Map Reduce Apache Pig Hive Sqoop HBase Cassandra and Oozie Extensive Experience in Setting Hadoop Cluster Good working knowledge with Map Reduce and Apache Pig Involved in writing the Pig scripts to reduce the job execution time Having experience with processing real time streamed data using Strom and Spark streaming Experience with configuration of Hadoop Ecosystem components Map Reduce Hive Hbase Pig Sqoop Oozie Flume Storm Spark Yarn and Tez Have executed projects using JavaJ2EE technologies such as Core Java Servlets Jsp JDBC Ext JS Struts Experience in application development frameworks like spring Hibernate and also on validation plugins like Validator frameworks Strong experience with version control tools such as Subversion Clear Case and CVS Experienced in Developing J2EE Application on IDE tools like Eclipse and Net Beans Expertise in build scripts like ANT and Maven and build automation Strong Experience in working with Databases like Oracle SQL Server EDB and proficiency in writing complex SQL PLSQL for creating tables views indexes stored procedures and functions Experience with all stages of the SDLC and Agile Development model right from the requirement gathering to Deployment and production support Also have experience in understanding of existing systems maintenance and production support on technologies such as Java J2EE and various databases oracle SQL Server Highly Capable in learning things quickly and good at good time management Excellent communication skills interpersonal hardworking and ability to proficiently communicate with all levels of the organization and work as a part of the team as well as independently Knowledge on FLUME NOSQL Spark EcosystemSpark Core SQL Streaming ML Lib Graph XData warehouse and BI technologies Authorized to work in the US for any employer Work Experience Hadoop Developer Infor Grenville October 2018 to Present Description The American Express was initiated with the vision of delivering additional value to the customers seeking Credit line enhancement in corporate category This Program has acquired extensive knowledge of customers account history The key intent of the program is to calculate the new credit limit for the customer based on various calculations involving his past purchase behaviors from big data platform and provide the results nearreal time Responsibilities Installed and configured Hadoop Map Reduce HDFS and developed multiple Map Reduce jobs in Java for data cleaning and preprocessing Collaborate with subject matter experts various stakeholders and fellow developers to design develop implement and support data analytics Importing and exporting data into HDFS and Hive using Sqoop Spark Core and Spark SQL Created Hive tables and loading and analyzing data using hive queries Having exposure to Teradata for processing the huge data Worked on debugging performance tuning of Hive Pig Jobs Designed and implemented pig UDFs for evaluation filtering loading and storing of data Worked on Performance Tuning of Hadoop jobs by applying techniques such as MapSide Joins Partitioning Bucketing and using different file formats such as SequenceFile RCFile ORCFile Defined job work flows as per their dependencies in OOZIE Used JAVA J2EE application development skills with Object Oriented Analysis and extensively involved throughout Software Development Life Cycle SDLC Proactively monitored systems and services architecture design and implementation of Hadoop deployment configuration management backup and disaster recovery systems and procedures Used Flume to collect aggregate and store the web log data from different sources like web servers mobile and network devices and pushed to HDFS Load and transform large sets of structured semi structured and unstructured data Supported Map Reduce Programs those are running on the cluster Involved in loading data from UNIX file system to HDFS configuring Hive and writing Hive UDFs Processing the streamed data using real time messaging systems Kafka and Strom Utilized Java and ORACLE from day to day to debug and fix issues with client processes Managed and reviewed log files Having good amount of knowledge on Cassandra HBase Implemented partitioning dynamic partitions and buckets in HIVE Environment Hadoop JDK16 Map Reduce HDFS Hive Strom Kafka Cassandra Pig Spark core Spark SQL Sqoop Flume HTML XML SQL J2EE Eclipse RC ORC Flume Thrift Oozie HBase Hadoop Developer Target Web Intelligence MN February 2017 to September 2018 Description This Project is all about the rehousing of their Target current existing project into Hadoop platform Previously Target was using mysql DB for storing their competitors retailers informationThe Crawled web data Early Target use to have only 4 competitor retailers namely Amazoncom walmartcom etc But as and when the competitor retailers are increasing the data generated out of their web crawling is also increased massively and which cannot be accommodable in a mysql kind of data box with the same reason Target wants to move it Hadoop where exactly we can handle massive amount of data by means of its cluster nodes and also to satisfy the scaling needs of the Target business operation Responsibilities Moving data from Oracle to HDFS and viceversa using SQOOP Collecting and aggregating large amounts of log data using Apache Flume and staging data in HDFS for further analysis Implemented Hive tables and HQL Queries for the reports Worked with different file formats and compression techniques to determine standards importing and exporting data into HDFS and Hive using Sqoop Developed Hive queries to analyzetransform the data in HDFS Designed and Implemented Partitioning Multilevel Buckets in HIVE Designed and implemented pig UDFs for evaluation filtering loading and storing of data AnalyzingTransforming data with Hive and Pig Creating Views on Hive tables Effective coordination with offshore team and managed project deliverable on time Worked on QA support activities test data creation and Unit testing activities Responsible for creating Hive tables loading the structured data resulted from Map Reduce jobs into the tables and writing hive queries to further analyze the logs to identify issues and behavioral patterns Used Hive to analyses data ingested into Hbase by using HiveHbase integration and compute various metrics for reporting on the dashboard Developed job flows to automate the workflow for pig and hive jobs Extensively involved in performance tuning of Oracle queries Loaded the aggregated data onto Oracle from Hadoop environment using Sqoop for reporting on the dashboard Written UNIX scripts to automate batch functions Used Tableau for visualization on processed data Environment Hadoop Ecosystem HortonWorks 2x Apache Pig Hive SQOOP Oozie Platfora HCat Java UNIX Scripts Oracle Cent OS Map Reduce Hbase Cassandra Tableau Lead Java Developer American Express AZ June 2016 to January 2017 Description Infor EAM is a complete product for a company which can manage their Equipment Material Parts and their components This helps in efficient management of their parts reducing the cost of maintenance Infor EAM provides the tools to monitor and manage the deployment performance and maintenance of company assets including alerts that help to eliminate operational downtimes and reveal hidden profits Infor EAM will be used for asset management work management inventory management purchasing management inspectioncondition monitoring crew scheduling and preventive maintenance scheduling It will Work based on CD Key for each module for licensing the Product Responsibilities Involved in Use Case meeting to understand and analyze the requirements Coded as per Prototype Developed various UI User Interface components using Struts MVC JSP and HTML Developed Controllers created JSPs and configured in Strutsconfigxml Webxml files Developed MVC architecture Business Delegate Service Locator Session facade and Data Access Object and Singleton patterns Involved in writing all client side validations using Java Script JSON Involved in the complete development testing and maintenance process of the application Used Hibernate 20 as the ORM tool to communicate with the database Designed and created a webbased test client using Struts up on clients request which is used to test the different parts of the application Involved in writing the test cases for the application using JUnit Used extensive JSP HTML and CSS to develop presentation layer to make it more user friendly Involved in different Testing phases like Unit Test Integration Test and Regression Test Involved in Development process and have knowledge in usage of Tracker Tools like JIRA Developed backend stored procedures and triggers using Oracle PLSQL involved in database objects creation performance tuning of stored procedures and query plan Responsible for developing and maintaining all the session beans Supported the application through debugging fixing production support and maintenance releases Worked closely with the client and the offsite team coordinated activities between them for effective implementation of the project Involved in Restful Web services with JSON using Jackson API Involved in Web servicesSOAPRESTfull Testing using Infor EAM WebService tool kit Environment J2SE JSP Servlets Struts EJB20 Ext JS XML Oracle 11g Postgresql Web Service SQL Server 2008R2 Eclipse TOAD JIRA SVN Tortoise Log4j Java Developer Hyderabad Telangana March 2014 to May 2016 Description Customer relationship management CRM is a system for managing a companys interactions with current and future customers It often involves using technology to organize automate and synchronize sales marketing customer service and technical support Customer relationship management is often thought of as a business strategy that enables businesses to improve in a number of areas Responsibilities Involved in Use Case meeting to understand and analyze the requirements Coded as per Prototype Involved in product development and customizations using Infor CRB studio Designing and Development of BIOs record sets data sources forms shells navigations Involved in Coding and bug fixing Used Validate plugin of struts framework to handle server side validations Involved in usage of SVN for version control process Logging of errors in application is achieved by using Log4J API Tables are created by running Scripts on SQL Developer IDE Involved in Build Deployment Activities and OracleSQL Database Schema Restore and backup Involved in Unit Test Integration Test and Regression Test plans Involved in Development process and have knowledge in usage of Tracker Tools like JIRA Having good Knowledge in Epiphany Platform Open Architecture Environment J2SE JSP Servlets Struts Spring Hibernate Java Beans XML Java Developer Infor Grenville Hyderabad Telangana March 2012 to February 2014 Description The workstation Project automates assignment of Workstation and Keys to each employee capable of identifies the unassigned workstation and Keys based on that we can easily assign that workstation and keys pedestalstoragecabin to new employee Gathered the Employee Details Key Details Workstation Details based on Floor and Zone and all these are entered by using Bulk Import Concept using this project employees can be viewed categorized based on their position Here we can identify the employee location based on FloorZone Responsibilities Functional and UI design has been prepared Implementation at BIO level Creation of Record sets and BIOs for the database schema Created Relationships for data Integrity Created Lookups and attribute domains Implementation at UI level Menus for Navigation Forms for various Perspectives Implemented shells like List Shell Detail Shell Tab Group Shell Toggle Shell to Provide better look and feel Toolbars to allow UI Actions for Buttons Used Form Slots by considering the BIO schema Attachments of documents has been provided for work ordersinvoices Authentication and authorization has been achieved by creating users and profiles in platadmin Implemented objectpermissions at widget menu and form levels Developed Form level extensions to achieve UI level validations and BIO level extensions to fulfil Functional requirements and validations All required data is entered by using Bulk Import Involved in Development process and have knowledge in usage of Tracker Tools like JIRA Having good Knowledge in Epiphany Platform Open Architecture Having Extensive Hands on Experience on Complex PLSQL Programming Environments CRB Studio Web logic server 81 LDAP Core Java SQL Server Associate Java Developer Hyderabad Telangana April 2011 to February 2012 Description Indian Institute of Science Outsourcing Company providing solutions to varied Enterprise Worldwide It is part of Research and Technology Indian Government Responsibilities Involved in translating Business Requirement into Technical Requirement Involved in all the phases of SDLC including Requirements Collection Design and Analysis of the Customer specification from the Business Analyst Designed UML class diagrams and Use Case diagrams to understand the code easily Oracle9i database was used as backend layer Used JDBC API to communicate with the database Developed various test cases such as unit test mock test and integration test using JUnit Developed the presentation layer and GUI framework that are written using JSP Clientside validation was done using JavaScript Extensively involved in the development of Java Server Pages JSP Involved in performance and SQL Query optimization Used CVS as version control tool Environment JDK 15 JDBC SQL Eclipse Oracle XML Apache Tomcat CVS JSP JUnit UML UNIX Education Bachelors Skills Apache 2 years APACHE CASSANDRA 2 years APACHE HADOOP HDFS 2 years APACHE HADOOP OOZIE 2 years APACHE HADOOP SQOOP 2 years APACHE HBASE 2 years ASTERADATA 2 years Cassandra 2 years Java 8 years JIRA 4 years JSP 3 years MS SQL SERVER 2 years ORACLE 6 years Servlets 2 years SQL 6 years SQL Server 2 years Struts 2 years Subversion 2 years SVN 2 years version control 3 years",
    "entities": [
        "Strutsconfigxml Webxml",
        "SQL Server Highly Capable",
        "BIO",
        "Description Indian Institute of Science Outsourcing Company",
        "Description Infor EAM",
        "GUI",
        "ORM",
        "FloorZone Responsibilities Functional",
        "Bulk Import Involved",
        "Technical Requirement Involved",
        "Customer",
        "HDFS",
        "UNIX",
        "Consulting Analysis Implementation of Java",
        "Hadoop Developer Hadoop",
        "Struts MVC JSP",
        "Creation of Record",
        "Oracle XML Apache",
        "AnalyzingTransforming",
        "Hadoop Ecosystem",
        "CVS",
        "Hadoop",
        "Spark SQL Created Hive",
        "Program",
        "Hive UDFs Processing",
        "Big Data Technologies Keen",
        "Software Development Life Cycle SDLC",
        "SQOOP Collecting",
        "Data Access Object",
        "JUnit",
        "BI technologies",
        "JavaJ2EE",
        "Target",
        "Credit",
        "HDFS Load",
        "Implemented Hive",
        "Unit Test Integration Test and Regression Test Involved",
        "Struts Spring",
        "ORACLE",
        "UNIX Scripts Oracle Cent",
        "Created Relationships",
        "JSP",
        "Oracle PLSQL",
        "Toolbars",
        "the Analytics Information Management Big data Data",
        "HiveHbase",
        "Description Customer",
        "the Business Analyst Designed UML",
        "Interface",
        "Spark",
        "Oracle from Hadoop",
        "Java Server Pages JSP Involved",
        "Validate",
        "Agile Development",
        "Java Developer American Express",
        "Oozie Extensive",
        "Setting Hadoop Cluster Good",
        "US",
        "UI Actions",
        "Sqoop",
        "QA",
        "Infor EAM",
        "Jackson API Involved",
        "Responsibilities Installed",
        "Coded",
        "Singleton",
        "Present Description",
        "Coding",
        "Floor",
        "Big Data Centre",
        "log data",
        "Integrity Created Lookups",
        "Java Big Data Technologies Evaluation",
        "Amazoncom",
        "SQL",
        "Implementation at BIO level",
        "Oracle queries Loaded",
        "Oracle SQL Server EDB",
        "Bulk Import Concept",
        "Pig Hive Sqoop Consulting",
        "Big Data Texts",
        "Business Delegate Service Locator Session",
        "Developing J2EE Application",
        "Hive",
        "Business Requirement",
        "Object Oriented Analysis",
        "Perspectives Implemented",
        "Maven",
        "Work Experience Hadoop Developer Infor Grenville",
        "ANT",
        "UI",
        "HTML Developed Controllers",
        "JSP Servlets",
        "Research and Technology Indian Government Responsibilities Involved",
        "Java Script JSON Involved",
        "SVN",
        "Workstation",
        "The American Express",
        "Oozie HBase Hadoop Developer",
        "CSS",
        "Tracker Tools",
        "Tableau",
        "SQL Query",
        "Focus",
        "Requirements Collection Design and Analysis of the"
    ],
    "experience": "Experience in Setting Hadoop Cluster Good working knowledge with Map Reduce and Apache Pig Involved in writing the Pig scripts to reduce the job execution time Having experience with processing real time streamed data using Strom and Spark streaming Experience with configuration of Hadoop Ecosystem components Map Reduce Hive Hbase Pig Sqoop Oozie Flume Storm Spark Yarn and Tez Have executed projects using JavaJ2EE technologies such as Core Java Servlets Jsp JDBC Ext JS Struts Experience in application development frameworks like spring Hibernate and also on validation plugins like Validator frameworks Strong experience with version control tools such as Subversion Clear Case and CVS Experienced in Developing J2EE Application on IDE tools like Eclipse and Net Beans Expertise in build scripts like ANT and Maven and build automation Strong Experience in working with Databases like Oracle SQL Server EDB and proficiency in writing complex SQL PLSQL for creating tables views indexes stored procedures and functions Experience with all stages of the SDLC and Agile Development model right from the requirement gathering to Deployment and production support Also have experience in understanding of existing systems maintenance and production support on technologies such as Java J2EE and various databases oracle SQL Server Highly Capable in learning things quickly and good at good time management Excellent communication skills interpersonal hardworking and ability to proficiently communicate with all levels of the organization and work as a part of the team as well as independently Knowledge on FLUME NOSQL Spark EcosystemSpark Core SQL Streaming ML Lib Graph XData warehouse and BI technologies Authorized to work in the US for any employer Work Experience Hadoop Developer Infor Grenville October 2018 to Present Description The American Express was initiated with the vision of delivering additional value to the customers seeking Credit line enhancement in corporate category This Program has acquired extensive knowledge of customers account history The key intent of the program is to calculate the new credit limit for the customer based on various calculations involving his past purchase behaviors from big data platform and provide the results nearreal time Responsibilities Installed and configured Hadoop Map Reduce HDFS and developed multiple Map Reduce jobs in Java for data cleaning and preprocessing Collaborate with subject matter experts various stakeholders and fellow developers to design develop implement and support data analytics Importing and exporting data into HDFS and Hive using Sqoop Spark Core and Spark SQL Created Hive tables and loading and analyzing data using hive queries Having exposure to Teradata for processing the huge data Worked on debugging performance tuning of Hive Pig Jobs Designed and implemented pig UDFs for evaluation filtering loading and storing of data Worked on Performance Tuning of Hadoop jobs by applying techniques such as MapSide Joins Partitioning Bucketing and using different file formats such as SequenceFile RCFile ORCFile Defined job work flows as per their dependencies in OOZIE Used JAVA J2EE application development skills with Object Oriented Analysis and extensively involved throughout Software Development Life Cycle SDLC Proactively monitored systems and services architecture design and implementation of Hadoop deployment configuration management backup and disaster recovery systems and procedures Used Flume to collect aggregate and store the web log data from different sources like web servers mobile and network devices and pushed to HDFS Load and transform large sets of structured semi structured and unstructured data Supported Map Reduce Programs those are running on the cluster Involved in loading data from UNIX file system to HDFS configuring Hive and writing Hive UDFs Processing the streamed data using real time messaging systems Kafka and Strom Utilized Java and ORACLE from day to day to debug and fix issues with client processes Managed and reviewed log files Having good amount of knowledge on Cassandra HBase Implemented partitioning dynamic partitions and buckets in HIVE Environment Hadoop JDK16 Map Reduce HDFS Hive Strom Kafka Cassandra Pig Spark core Spark SQL Sqoop Flume HTML XML SQL J2EE Eclipse RC ORC Flume Thrift Oozie HBase Hadoop Developer Target Web Intelligence MN February 2017 to September 2018 Description This Project is all about the rehousing of their Target current existing project into Hadoop platform Previously Target was using mysql DB for storing their competitors retailers informationThe Crawled web data Early Target use to have only 4 competitor retailers namely Amazoncom walmartcom etc But as and when the competitor retailers are increasing the data generated out of their web crawling is also increased massively and which can not be accommodable in a mysql kind of data box with the same reason Target wants to move it Hadoop where exactly we can handle massive amount of data by means of its cluster nodes and also to satisfy the scaling needs of the Target business operation Responsibilities Moving data from Oracle to HDFS and viceversa using SQOOP Collecting and aggregating large amounts of log data using Apache Flume and staging data in HDFS for further analysis Implemented Hive tables and HQL Queries for the reports Worked with different file formats and compression techniques to determine standards importing and exporting data into HDFS and Hive using Sqoop Developed Hive queries to analyzetransform the data in HDFS Designed and Implemented Partitioning Multilevel Buckets in HIVE Designed and implemented pig UDFs for evaluation filtering loading and storing of data AnalyzingTransforming data with Hive and Pig Creating Views on Hive tables Effective coordination with offshore team and managed project deliverable on time Worked on QA support activities test data creation and Unit testing activities Responsible for creating Hive tables loading the structured data resulted from Map Reduce jobs into the tables and writing hive queries to further analyze the logs to identify issues and behavioral patterns Used Hive to analyses data ingested into Hbase by using HiveHbase integration and compute various metrics for reporting on the dashboard Developed job flows to automate the workflow for pig and hive jobs Extensively involved in performance tuning of Oracle queries Loaded the aggregated data onto Oracle from Hadoop environment using Sqoop for reporting on the dashboard Written UNIX scripts to automate batch functions Used Tableau for visualization on processed data Environment Hadoop Ecosystem HortonWorks 2x Apache Pig Hive SQOOP Oozie Platfora HCat Java UNIX Scripts Oracle Cent OS Map Reduce Hbase Cassandra Tableau Lead Java Developer American Express AZ June 2016 to January 2017 Description Infor EAM is a complete product for a company which can manage their Equipment Material Parts and their components This helps in efficient management of their parts reducing the cost of maintenance Infor EAM provides the tools to monitor and manage the deployment performance and maintenance of company assets including alerts that help to eliminate operational downtimes and reveal hidden profits Infor EAM will be used for asset management work management inventory management purchasing management inspectioncondition monitoring crew scheduling and preventive maintenance scheduling It will Work based on CD Key for each module for licensing the Product Responsibilities Involved in Use Case meeting to understand and analyze the requirements Coded as per Prototype Developed various UI User Interface components using Struts MVC JSP and HTML Developed Controllers created JSPs and configured in Strutsconfigxml Webxml files Developed MVC architecture Business Delegate Service Locator Session facade and Data Access Object and Singleton patterns Involved in writing all client side validations using Java Script JSON Involved in the complete development testing and maintenance process of the application Used Hibernate 20 as the ORM tool to communicate with the database Designed and created a webbased test client using Struts up on clients request which is used to test the different parts of the application Involved in writing the test cases for the application using JUnit Used extensive JSP HTML and CSS to develop presentation layer to make it more user friendly Involved in different Testing phases like Unit Test Integration Test and Regression Test Involved in Development process and have knowledge in usage of Tracker Tools like JIRA Developed backend stored procedures and triggers using Oracle PLSQL involved in database objects creation performance tuning of stored procedures and query plan Responsible for developing and maintaining all the session beans Supported the application through debugging fixing production support and maintenance releases Worked closely with the client and the offsite team coordinated activities between them for effective implementation of the project Involved in Restful Web services with JSON using Jackson API Involved in Web servicesSOAPRESTfull Testing using Infor EAM WebService tool kit Environment J2SE JSP Servlets Struts EJB20 Ext JS XML Oracle 11 g Postgresql Web Service SQL Server 2008R2 Eclipse TOAD JIRA SVN Tortoise Log4j Java Developer Hyderabad Telangana March 2014 to May 2016 Description Customer relationship management CRM is a system for managing a companys interactions with current and future customers It often involves using technology to organize automate and synchronize sales marketing customer service and technical support Customer relationship management is often thought of as a business strategy that enables businesses to improve in a number of areas Responsibilities Involved in Use Case meeting to understand and analyze the requirements Coded as per Prototype Involved in product development and customizations using Infor CRB studio Designing and Development of BIOs record sets data sources forms shells navigations Involved in Coding and bug fixing Used Validate plugin of struts framework to handle server side validations Involved in usage of SVN for version control process Logging of errors in application is achieved by using Log4J API Tables are created by running Scripts on SQL Developer IDE Involved in Build Deployment Activities and OracleSQL Database Schema Restore and backup Involved in Unit Test Integration Test and Regression Test plans Involved in Development process and have knowledge in usage of Tracker Tools like JIRA Having good Knowledge in Epiphany Platform Open Architecture Environment J2SE JSP Servlets Struts Spring Hibernate Java Beans XML Java Developer Infor Grenville Hyderabad Telangana March 2012 to February 2014 Description The workstation Project automates assignment of Workstation and Keys to each employee capable of identifies the unassigned workstation and Keys based on that we can easily assign that workstation and keys pedestalstoragecabin to new employee Gathered the Employee Details Key Details Workstation Details based on Floor and Zone and all these are entered by using Bulk Import Concept using this project employees can be viewed categorized based on their position Here we can identify the employee location based on FloorZone Responsibilities Functional and UI design has been prepared Implementation at BIO level Creation of Record sets and BIOs for the database schema Created Relationships for data Integrity Created Lookups and attribute domains Implementation at UI level Menus for Navigation Forms for various Perspectives Implemented shells like List Shell Detail Shell Tab Group Shell Toggle Shell to Provide better look and feel Toolbars to allow UI Actions for Buttons Used Form Slots by considering the BIO schema Attachments of documents has been provided for work ordersinvoices Authentication and authorization has been achieved by creating users and profiles in platadmin Implemented objectpermissions at widget menu and form levels Developed Form level extensions to achieve UI level validations and BIO level extensions to fulfil Functional requirements and validations All required data is entered by using Bulk Import Involved in Development process and have knowledge in usage of Tracker Tools like JIRA Having good Knowledge in Epiphany Platform Open Architecture Having Extensive Hands on Experience on Complex PLSQL Programming Environments CRB Studio Web logic server 81 LDAP Core Java SQL Server Associate Java Developer Hyderabad Telangana April 2011 to February 2012 Description Indian Institute of Science Outsourcing Company providing solutions to varied Enterprise Worldwide It is part of Research and Technology Indian Government Responsibilities Involved in translating Business Requirement into Technical Requirement Involved in all the phases of SDLC including Requirements Collection Design and Analysis of the Customer specification from the Business Analyst Designed UML class diagrams and Use Case diagrams to understand the code easily Oracle9i database was used as backend layer Used JDBC API to communicate with the database Developed various test cases such as unit test mock test and integration test using JUnit Developed the presentation layer and GUI framework that are written using JSP Clientside validation was done using JavaScript Extensively involved in the development of Java Server Pages JSP Involved in performance and SQL Query optimization Used CVS as version control tool Environment JDK 15 JDBC SQL Eclipse Oracle XML Apache Tomcat CVS JSP JUnit UML UNIX Education Bachelors Skills Apache 2 years APACHE CASSANDRA 2 years APACHE HADOOP HDFS 2 years APACHE HADOOP OOZIE 2 years APACHE HADOOP SQOOP 2 years APACHE HBASE 2 years ASTERADATA 2 years Cassandra 2 years Java 8 years JIRA 4 years JSP 3 years MS SQL SERVER 2 years ORACLE 6 years Servlets 2 years SQL 6 years SQL Server 2 years Struts 2 years Subversion 2 years SVN 2 years version control 3 years",
    "extracted_keywords": [
        "Hadoop",
        "Developer",
        "Hadoop",
        "span",
        "lDeveloperspan",
        "Hadoop",
        "Developer",
        "Infor",
        "Grenville",
        "Grenville",
        "SD",
        "years",
        "experience",
        "Consulting",
        "Analysis",
        "Implementation",
        "Java",
        "Big",
        "Data",
        "solutions",
        "project",
        "assignments",
        "Lead",
        "contributor",
        "Big",
        "Data",
        "Centre",
        "excellence",
        "technologies",
        "Big",
        "Data",
        "Texts",
        "SQL",
        "databases",
        "areas",
        "Focus",
        "business",
        "solutions",
        "Big",
        "Data",
        "Technologies",
        "Keen",
        "knowledge",
        "technologies",
        "Analytics",
        "Information",
        "Management",
        "Big",
        "data",
        "Data",
        "science",
        "areas",
        "business",
        "solutions",
        "phases",
        "project",
        "lifecycle",
        "emphasis",
        "Planning",
        "Designing",
        "Hadoop",
        "environment",
        "technologiesPlatforms",
        "Pig",
        "Hive",
        "Sqoop",
        "Consulting",
        "Java",
        "Big",
        "Data",
        "Technologies",
        "Evaluation",
        "technologies",
        "data",
        "Analytics",
        "Sql",
        "space",
        "experience",
        "Hadoop",
        "components",
        "Map",
        "Reduce",
        "Apache",
        "Pig",
        "Hive",
        "Sqoop",
        "HBase",
        "Cassandra",
        "Oozie",
        "Experience",
        "Setting",
        "Hadoop",
        "Cluster",
        "Good",
        "knowledge",
        "Map",
        "Reduce",
        "Apache",
        "Pig",
        "Pig",
        "scripts",
        "job",
        "execution",
        "time",
        "experience",
        "processing",
        "time",
        "data",
        "Strom",
        "Spark",
        "Experience",
        "configuration",
        "Hadoop",
        "Ecosystem",
        "components",
        "Map",
        "Reduce",
        "Hive",
        "Hbase",
        "Pig",
        "Sqoop",
        "Oozie",
        "Flume",
        "Storm",
        "Spark",
        "Yarn",
        "Tez",
        "projects",
        "JavaJ2EE",
        "technologies",
        "Core",
        "Java",
        "Servlets",
        "Jsp",
        "JDBC",
        "Ext",
        "JS",
        "Struts",
        "Experience",
        "application",
        "development",
        "frameworks",
        "spring",
        "Hibernate",
        "validation",
        "plugins",
        "Validator",
        "experience",
        "version",
        "control",
        "tools",
        "Subversion",
        "Clear",
        "Case",
        "CVS",
        "J2EE",
        "Application",
        "IDE",
        "tools",
        "Eclipse",
        "Net",
        "Beans",
        "Expertise",
        "build",
        "scripts",
        "ANT",
        "Maven",
        "automation",
        "Strong",
        "Experience",
        "Databases",
        "Oracle",
        "SQL",
        "Server",
        "EDB",
        "proficiency",
        "SQL",
        "PLSQL",
        "tables",
        "views",
        "indexes",
        "procedures",
        "functions",
        "Experience",
        "stages",
        "SDLC",
        "Agile",
        "Development",
        "model",
        "requirement",
        "gathering",
        "Deployment",
        "production",
        "support",
        "experience",
        "understanding",
        "systems",
        "maintenance",
        "production",
        "support",
        "technologies",
        "Java",
        "J2EE",
        "databases",
        "oracle",
        "SQL",
        "Server",
        "things",
        "time",
        "management",
        "Excellent",
        "communication",
        "skills",
        "hardworking",
        "ability",
        "levels",
        "organization",
        "work",
        "part",
        "team",
        "Knowledge",
        "FLUME",
        "NOSQL",
        "Spark",
        "EcosystemSpark",
        "Core",
        "SQL",
        "Streaming",
        "ML",
        "Lib",
        "Graph",
        "XData",
        "warehouse",
        "BI",
        "technologies",
        "US",
        "employer",
        "Work",
        "Experience",
        "Hadoop",
        "Developer",
        "Infor",
        "Grenville",
        "October",
        "Present",
        "Description",
        "American",
        "Express",
        "vision",
        "value",
        "customers",
        "Credit",
        "line",
        "enhancement",
        "category",
        "Program",
        "knowledge",
        "customers",
        "history",
        "intent",
        "program",
        "credit",
        "limit",
        "customer",
        "calculations",
        "purchase",
        "behaviors",
        "data",
        "platform",
        "results",
        "time",
        "Responsibilities",
        "Hadoop",
        "Map",
        "HDFS",
        "Map",
        "Reduce",
        "jobs",
        "Java",
        "data",
        "Collaborate",
        "subject",
        "matter",
        "experts",
        "stakeholders",
        "developers",
        "data",
        "analytics",
        "Importing",
        "data",
        "HDFS",
        "Hive",
        "Sqoop",
        "Spark",
        "Core",
        "Spark",
        "SQL",
        "Created",
        "Hive",
        "tables",
        "loading",
        "data",
        "hive",
        "queries",
        "exposure",
        "Teradata",
        "data",
        "performance",
        "tuning",
        "Hive",
        "Pig",
        "Jobs",
        "pig",
        "UDFs",
        "evaluation",
        "loading",
        "storing",
        "data",
        "Performance",
        "Tuning",
        "Hadoop",
        "jobs",
        "techniques",
        "MapSide",
        "Joins",
        "Partitioning",
        "Bucketing",
        "file",
        "formats",
        "SequenceFile",
        "RCFile",
        "job",
        "work",
        "dependencies",
        "OOZIE",
        "J2EE",
        "application",
        "development",
        "skills",
        "Object",
        "Oriented",
        "Analysis",
        "Software",
        "Development",
        "Life",
        "Cycle",
        "SDLC",
        "systems",
        "services",
        "architecture",
        "design",
        "implementation",
        "Hadoop",
        "deployment",
        "configuration",
        "management",
        "backup",
        "disaster",
        "recovery",
        "systems",
        "procedures",
        "Flume",
        "aggregate",
        "web",
        "log",
        "data",
        "sources",
        "web",
        "servers",
        "mobile",
        "network",
        "devices",
        "HDFS",
        "Load",
        "sets",
        "data",
        "Map",
        "Reduce",
        "Programs",
        "cluster",
        "loading",
        "data",
        "UNIX",
        "file",
        "system",
        "Hive",
        "Hive",
        "UDFs",
        "Processing",
        "data",
        "time",
        "systems",
        "Kafka",
        "Strom",
        "Utilized",
        "Java",
        "ORACLE",
        "day",
        "day",
        "issues",
        "client",
        "processes",
        "log",
        "files",
        "amount",
        "knowledge",
        "Cassandra",
        "HBase",
        "partitions",
        "buckets",
        "HIVE",
        "Environment",
        "Hadoop",
        "JDK16",
        "Map",
        "HDFS",
        "Hive",
        "Strom",
        "Kafka",
        "Cassandra",
        "Pig",
        "Spark",
        "core",
        "Spark",
        "SQL",
        "Sqoop",
        "Flume",
        "HTML",
        "XML",
        "SQL",
        "J2EE",
        "Eclipse",
        "RC",
        "ORC",
        "Flume",
        "Thrift",
        "Oozie",
        "HBase",
        "Hadoop",
        "Developer",
        "Target",
        "Web",
        "Intelligence",
        "MN",
        "February",
        "September",
        "Description",
        "Project",
        "rehousing",
        "Target",
        "project",
        "Hadoop",
        "platform",
        "Target",
        "mysql",
        "DB",
        "competitors",
        "retailers",
        "web",
        "data",
        "Early",
        "Target",
        "competitor",
        "retailers",
        "Amazoncom",
        "walmartcom",
        "competitor",
        "retailers",
        "data",
        "web",
        "crawling",
        "mysql",
        "kind",
        "data",
        "box",
        "reason",
        "Target",
        "Hadoop",
        "amount",
        "data",
        "means",
        "cluster",
        "nodes",
        "scaling",
        "needs",
        "Target",
        "business",
        "operation",
        "Responsibilities",
        "data",
        "Oracle",
        "HDFS",
        "viceversa",
        "SQOOP",
        "Collecting",
        "amounts",
        "log",
        "data",
        "Apache",
        "Flume",
        "data",
        "HDFS",
        "analysis",
        "Hive",
        "tables",
        "HQL",
        "Queries",
        "reports",
        "file",
        "formats",
        "compression",
        "techniques",
        "standards",
        "data",
        "HDFS",
        "Hive",
        "Sqoop",
        "Developed",
        "Hive",
        "data",
        "HDFS",
        "Partitioning",
        "Multilevel",
        "Buckets",
        "HIVE",
        "pig",
        "UDFs",
        "evaluation",
        "loading",
        "storing",
        "data",
        "AnalyzingTransforming",
        "data",
        "Hive",
        "Pig",
        "Views",
        "Hive",
        "tables",
        "coordination",
        "team",
        "project",
        "time",
        "QA",
        "support",
        "activities",
        "data",
        "creation",
        "Unit",
        "testing",
        "activities",
        "Hive",
        "tables",
        "data",
        "Map",
        "Reduce",
        "jobs",
        "tables",
        "hive",
        "queries",
        "logs",
        "issues",
        "patterns",
        "Hive",
        "analyses",
        "data",
        "Hbase",
        "HiveHbase",
        "integration",
        "metrics",
        "dashboard",
        "job",
        "workflow",
        "pig",
        "hive",
        "jobs",
        "performance",
        "tuning",
        "Oracle",
        "data",
        "Oracle",
        "Hadoop",
        "environment",
        "Sqoop",
        "dashboard",
        "UNIX",
        "scripts",
        "batch",
        "functions",
        "Tableau",
        "visualization",
        "data",
        "Environment",
        "Hadoop",
        "Ecosystem",
        "HortonWorks",
        "Apache",
        "Pig",
        "Hive",
        "SQOOP",
        "Oozie",
        "Platfora",
        "HCat",
        "Java",
        "UNIX",
        "Scripts",
        "Oracle",
        "Cent",
        "OS",
        "Map",
        "Hbase",
        "Cassandra",
        "Tableau",
        "Lead",
        "Java",
        "Developer",
        "American",
        "Express",
        "AZ",
        "June",
        "January",
        "Description",
        "Infor",
        "EAM",
        "product",
        "company",
        "Equipment",
        "Material",
        "Parts",
        "components",
        "management",
        "parts",
        "cost",
        "maintenance",
        "Infor",
        "EAM",
        "tools",
        "performance",
        "maintenance",
        "company",
        "assets",
        "alerts",
        "downtimes",
        "profits",
        "Infor",
        "EAM",
        "asset",
        "management",
        "work",
        "management",
        "inventory",
        "management",
        "purchasing",
        "management",
        "inspectioncondition",
        "crew",
        "scheduling",
        "maintenance",
        "scheduling",
        "CD",
        "Key",
        "module",
        "Product",
        "Responsibilities",
        "Use",
        "Case",
        "meeting",
        "requirements",
        "Prototype",
        "UI",
        "User",
        "Interface",
        "components",
        "Struts",
        "MVC",
        "JSP",
        "HTML",
        "Developed",
        "Controllers",
        "JSPs",
        "Strutsconfigxml",
        "Webxml",
        "Developed",
        "MVC",
        "architecture",
        "Business",
        "Delegate",
        "Service",
        "Locator",
        "Session",
        "facade",
        "Data",
        "Access",
        "Object",
        "Singleton",
        "patterns",
        "client",
        "side",
        "validations",
        "Java",
        "Script",
        "JSON",
        "development",
        "testing",
        "maintenance",
        "process",
        "application",
        "Hibernate",
        "ORM",
        "tool",
        "database",
        "test",
        "client",
        "Struts",
        "clients",
        "parts",
        "application",
        "test",
        "cases",
        "application",
        "JUnit",
        "JSP",
        "HTML",
        "CSS",
        "presentation",
        "layer",
        "user",
        "Testing",
        "phases",
        "Unit",
        "Test",
        "Integration",
        "Test",
        "Regression",
        "Test",
        "Development",
        "process",
        "knowledge",
        "usage",
        "Tracker",
        "Tools",
        "JIRA",
        "Developed",
        "backend",
        "procedures",
        "triggers",
        "Oracle",
        "PLSQL",
        "database",
        "creation",
        "performance",
        "procedures",
        "query",
        "session",
        "beans",
        "application",
        "production",
        "support",
        "maintenance",
        "releases",
        "client",
        "team",
        "activities",
        "implementation",
        "project",
        "Restful",
        "Web",
        "services",
        "JSON",
        "Jackson",
        "API",
        "Web",
        "servicesSOAPRESTfull",
        "Testing",
        "Infor",
        "EAM",
        "WebService",
        "tool",
        "kit",
        "Environment",
        "J2SE",
        "JSP",
        "Servlets",
        "Struts",
        "EJB20",
        "Ext",
        "JS",
        "XML",
        "Oracle",
        "g",
        "Postgresql",
        "Web",
        "Service",
        "SQL",
        "Server",
        "Eclipse",
        "TOAD",
        "JIRA",
        "SVN",
        "Tortoise",
        "Log4j",
        "Java",
        "Developer",
        "Hyderabad",
        "Telangana",
        "March",
        "May",
        "Description",
        "Customer",
        "relationship",
        "management",
        "CRM",
        "system",
        "companys",
        "interactions",
        "customers",
        "technology",
        "automate",
        "sales",
        "marketing",
        "customer",
        "service",
        "support",
        "Customer",
        "relationship",
        "management",
        "business",
        "strategy",
        "businesses",
        "number",
        "areas",
        "Responsibilities",
        "Use",
        "Case",
        "meeting",
        "requirements",
        "Prototype",
        "product",
        "development",
        "customizations",
        "Infor",
        "CRB",
        "studio",
        "Designing",
        "Development",
        "BIOs",
        "record",
        "sets",
        "data",
        "sources",
        "shells",
        "navigations",
        "Coding",
        "bug",
        "Validate",
        "plugin",
        "struts",
        "framework",
        "server",
        "side",
        "validations",
        "usage",
        "SVN",
        "version",
        "control",
        "process",
        "Logging",
        "errors",
        "application",
        "API",
        "Tables",
        "Scripts",
        "SQL",
        "Developer",
        "IDE",
        "Build",
        "Deployment",
        "Activities",
        "Database",
        "Schema",
        "Restore",
        "backup",
        "Unit",
        "Test",
        "Integration",
        "Test",
        "Regression",
        "Test",
        "plans",
        "Development",
        "process",
        "knowledge",
        "usage",
        "Tracker",
        "Tools",
        "JIRA",
        "Knowledge",
        "Epiphany",
        "Platform",
        "Open",
        "Architecture",
        "Environment",
        "J2SE",
        "JSP",
        "Servlets",
        "Struts",
        "Spring",
        "Hibernate",
        "Java",
        "Beans",
        "XML",
        "Java",
        "Developer",
        "Infor",
        "Grenville",
        "Hyderabad",
        "Telangana",
        "March",
        "February",
        "Description",
        "workstation",
        "Project",
        "assignment",
        "Workstation",
        "Keys",
        "employee",
        "identifies",
        "workstation",
        "Keys",
        "workstation",
        "keys",
        "pedestalstoragecabin",
        "employee",
        "Employee",
        "Details",
        "Key",
        "Details",
        "Workstation",
        "Details",
        "Floor",
        "Zone",
        "Bulk",
        "Import",
        "Concept",
        "project",
        "employees",
        "position",
        "employee",
        "location",
        "FloorZone",
        "Responsibilities",
        "Functional",
        "UI",
        "design",
        "Implementation",
        "BIO",
        "level",
        "Creation",
        "Record",
        "sets",
        "BIOs",
        "database",
        "schema",
        "Created",
        "Relationships",
        "data",
        "Integrity",
        "Created",
        "Lookups",
        "attribute",
        "domains",
        "Implementation",
        "UI",
        "level",
        "Menus",
        "Navigation",
        "Forms",
        "Perspectives",
        "shells",
        "List",
        "Shell",
        "Detail",
        "Shell",
        "Tab",
        "Group",
        "Shell",
        "Toggle",
        "Shell",
        "Toolbars",
        "UI",
        "Actions",
        "Buttons",
        "Form",
        "Slots",
        "BIO",
        "schema",
        "Attachments",
        "documents",
        "work",
        "ordersinvoices",
        "Authentication",
        "authorization",
        "users",
        "profiles",
        "platadmin",
        "objectpermissions",
        "widget",
        "menu",
        "form",
        "levels",
        "Form",
        "level",
        "extensions",
        "UI",
        "level",
        "validations",
        "BIO",
        "level",
        "extensions",
        "requirements",
        "validations",
        "data",
        "Bulk",
        "Import",
        "Development",
        "process",
        "knowledge",
        "usage",
        "Tracker",
        "Tools",
        "JIRA",
        "Knowledge",
        "Epiphany",
        "Platform",
        "Open",
        "Architecture",
        "Hands",
        "Experience",
        "Complex",
        "PLSQL",
        "Programming",
        "Environments",
        "CRB",
        "Studio",
        "Web",
        "logic",
        "server",
        "LDAP",
        "Core",
        "Java",
        "SQL",
        "Server",
        "Associate",
        "Java",
        "Developer",
        "Hyderabad",
        "Telangana",
        "April",
        "February",
        "Description",
        "Indian",
        "Institute",
        "Science",
        "Outsourcing",
        "Company",
        "solutions",
        "Enterprise",
        "Worldwide",
        "part",
        "Research",
        "Technology",
        "Indian",
        "Government",
        "Responsibilities",
        "Business",
        "Requirement",
        "Technical",
        "Requirement",
        "phases",
        "SDLC",
        "Requirements",
        "Collection",
        "Design",
        "Analysis",
        "Customer",
        "specification",
        "Business",
        "Analyst",
        "UML",
        "class",
        "diagrams",
        "Use",
        "Case",
        "diagrams",
        "code",
        "Oracle9i",
        "database",
        "layer",
        "JDBC",
        "API",
        "database",
        "test",
        "cases",
        "unit",
        "test",
        "test",
        "integration",
        "test",
        "JUnit",
        "presentation",
        "layer",
        "GUI",
        "framework",
        "JSP",
        "Clientside",
        "validation",
        "JavaScript",
        "development",
        "Java",
        "Server",
        "Pages",
        "JSP",
        "performance",
        "SQL",
        "Query",
        "optimization",
        "CVS",
        "version",
        "control",
        "tool",
        "Environment",
        "JDK",
        "JDBC",
        "SQL",
        "Eclipse",
        "Oracle",
        "XML",
        "Apache",
        "Tomcat",
        "CVS",
        "JSP",
        "JUnit",
        "UML",
        "UNIX",
        "Education",
        "Bachelors",
        "Skills",
        "Apache",
        "years",
        "APACHE",
        "CASSANDRA",
        "years",
        "APACHE",
        "HADOOP",
        "HDFS",
        "years",
        "APACHE",
        "HADOOP",
        "OOZIE",
        "years",
        "APACHE",
        "HADOOP",
        "SQOOP",
        "years",
        "APACHE",
        "HBASE",
        "years",
        "ASTERADATA",
        "years",
        "Cassandra",
        "years",
        "Java",
        "years",
        "JIRA",
        "years",
        "JSP",
        "years",
        "MS",
        "SQL",
        "SERVER",
        "years",
        "ORACLE",
        "years",
        "Servlets",
        "years",
        "SQL",
        "years",
        "SQL",
        "Server",
        "years",
        "Struts",
        "years",
        "Subversion",
        "years",
        "SVN",
        "years",
        "version",
        "control",
        "years"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T22:36:58.526394",
    "resume_data": "Hadoop Developer Hadoop span lDeveloperspan Hadoop Developer Infor Grenville Grenville SD Having 8 years of experience in Consulting Analysis Implementation of Java and Big Data solutions for various project assignments Lead contributor for Big Data Centre of excellence in various emerging technologies like Big Data Texts analytics No SQL databases and other related areas Focus on designing and delivering most optimum and critical business solutions for Big Data Technologies Keen in building knowledge on emerging technologies in the Analytics Information Management Big data Data science and related areas and in providing best business solutions Experienced in individual phases of project lifecycle with emphasis on Planning Designing and Coding Experienced in Hadoop environment technologiesPlatforms like Pig Hive Sqoop Consulting for Java Big Data Technologies Evaluation of new technologies in Big data Analytics and NO Sql space Exclusive experience in Hadoop and its components like HDFS Map Reduce Apache Pig Hive Sqoop HBase Cassandra and Oozie Extensive Experience in Setting Hadoop Cluster Good working knowledge with Map Reduce and Apache Pig Involved in writing the Pig scripts to reduce the job execution time Having experience with processing real time streamed data using Strom and Spark streaming Experience with configuration of Hadoop Ecosystem components Map Reduce Hive Hbase Pig Sqoop Oozie Flume Storm Spark Yarn and Tez Have executed projects using JavaJ2EE technologies such as Core Java Servlets Jsp JDBC Ext JS Struts Experience in application development frameworks like spring Hibernate and also on validation plugins like Validator frameworks Strong experience with version control tools such as Subversion Clear Case and CVS Experienced in Developing J2EE Application on IDE tools like Eclipse and Net Beans Expertise in build scripts like ANT and Maven and build automation Strong Experience in working with Databases like Oracle SQL Server EDB and proficiency in writing complex SQL PLSQL for creating tables views indexes stored procedures and functions Experience with all stages of the SDLC and Agile Development model right from the requirement gathering to Deployment and production support Also have experience in understanding of existing systems maintenance and production support on technologies such as Java J2EE and various databases oracle SQL Server Highly Capable in learning things quickly and good at good time management Excellent communication skills interpersonal hardworking and ability to proficiently communicate with all levels of the organization and work as a part of the team as well as independently Knowledge on FLUME NOSQL Spark EcosystemSpark Core SQL Streaming ML Lib Graph XData warehouse and BI technologies Authorized to work in the US for any employer Work Experience Hadoop Developer Infor Grenville October 2018 to Present Description The American Express was initiated with the vision of delivering additional value to the customers seeking Credit line enhancement in corporate category This Program has acquired extensive knowledge of customers account history The key intent of the program is to calculate the new credit limit for the customer based on various calculations involving his past purchase behaviors from big data platform and provide the results nearreal time Responsibilities Installed and configured Hadoop Map Reduce HDFS and developed multiple Map Reduce jobs in Java for data cleaning and preprocessing Collaborate with subject matter experts various stakeholders and fellow developers to design develop implement and support data analytics Importing and exporting data into HDFS and Hive using Sqoop Spark Core and Spark SQL Created Hive tables and loading and analyzing data using hive queries Having exposure to Teradata for processing the huge data Worked on debugging performance tuning of Hive Pig Jobs Designed and implemented pig UDFs for evaluation filtering loading and storing of data Worked on Performance Tuning of Hadoop jobs by applying techniques such as MapSide Joins Partitioning Bucketing and using different file formats such as SequenceFile RCFile ORCFile Defined job work flows as per their dependencies in OOZIE Used JAVA J2EE application development skills with Object Oriented Analysis and extensively involved throughout Software Development Life Cycle SDLC Proactively monitored systems and services architecture design and implementation of Hadoop deployment configuration management backup and disaster recovery systems and procedures Used Flume to collect aggregate and store the web log data from different sources like web servers mobile and network devices and pushed to HDFS Load and transform large sets of structured semi structured and unstructured data Supported Map Reduce Programs those are running on the cluster Involved in loading data from UNIX file system to HDFS configuring Hive and writing Hive UDFs Processing the streamed data using real time messaging systems Kafka and Strom Utilized Java and ORACLE from day to day to debug and fix issues with client processes Managed and reviewed log files Having good amount of knowledge on Cassandra HBase Implemented partitioning dynamic partitions and buckets in HIVE Environment Hadoop JDK16 Map Reduce HDFS Hive Strom Kafka Cassandra Pig Spark core Spark SQL Sqoop Flume HTML XML SQL J2EE Eclipse RC ORC Flume Thrift Oozie HBase Hadoop Developer Target Web Intelligence MN February 2017 to September 2018 Description This Project is all about the rehousing of their Target current existing project into Hadoop platform Previously Target was using mysql DB for storing their competitors retailers informationThe Crawled web data Early Target use to have only 4 competitor retailers namely Amazoncom walmartcom etc But as and when the competitor retailers are increasing the data generated out of their web crawling is also increased massively and which cannot be accommodable in a mysql kind of data box with the same reason Target wants to move it Hadoop where exactly we can handle massive amount of data by means of its cluster nodes and also to satisfy the scaling needs of the Target business operation Responsibilities Moving data from Oracle to HDFS and viceversa using SQOOP Collecting and aggregating large amounts of log data using Apache Flume and staging data in HDFS for further analysis Implemented Hive tables and HQL Queries for the reports Worked with different file formats and compression techniques to determine standards importing and exporting data into HDFS and Hive using Sqoop Developed Hive queries to analyzetransform the data in HDFS Designed and Implemented Partitioning Multilevel Buckets in HIVE Designed and implemented pig UDFs for evaluation filtering loading and storing of data AnalyzingTransforming data with Hive and Pig Creating Views on Hive tables Effective coordination with offshore team and managed project deliverable on time Worked on QA support activities test data creation and Unit testing activities Responsible for creating Hive tables loading the structured data resulted from Map Reduce jobs into the tables and writing hive queries to further analyze the logs to identify issues and behavioral patterns Used Hive to analyses data ingested into Hbase by using HiveHbase integration and compute various metrics for reporting on the dashboard Developed job flows to automate the workflow for pig and hive jobs Extensively involved in performance tuning of Oracle queries Loaded the aggregated data onto Oracle from Hadoop environment using Sqoop for reporting on the dashboard Written UNIX scripts to automate batch functions Used Tableau for visualization on processed data Environment Hadoop Ecosystem HortonWorks 2x Apache Pig Hive SQOOP Oozie Platfora HCat Java UNIX Scripts Oracle Cent OS Map Reduce Hbase Cassandra Tableau Lead Java Developer American Express AZ June 2016 to January 2017 Description Infor EAM is a complete product for a company which can manage their Equipment Material Parts and their components This helps in efficient management of their parts reducing the cost of maintenance Infor EAM provides the tools to monitor and manage the deployment performance and maintenance of company assets including alerts that help to eliminate operational downtimes and reveal hidden profits Infor EAM will be used for asset management work management inventory management purchasing management inspectioncondition monitoring crew scheduling and preventive maintenance scheduling It will Work based on CD Key for each module for licensing the Product Responsibilities Involved in Use Case meeting to understand and analyze the requirements Coded as per Prototype Developed various UI User Interface components using Struts MVC JSP and HTML Developed Controllers created JSPs and configured in Strutsconfigxml Webxml files Developed MVC architecture Business Delegate Service Locator Session facade and Data Access Object and Singleton patterns Involved in writing all client side validations using Java Script JSON Involved in the complete development testing and maintenance process of the application Used Hibernate 20 as the ORM tool to communicate with the database Designed and created a webbased test client using Struts up on clients request which is used to test the different parts of the application Involved in writing the test cases for the application using JUnit Used extensive JSP HTML and CSS to develop presentation layer to make it more user friendly Involved in different Testing phases like Unit Test Integration Test and Regression Test Involved in Development process and have knowledge in usage of Tracker Tools like JIRA Developed backend stored procedures and triggers using Oracle PLSQL involved in database objects creation performance tuning of stored procedures and query plan Responsible for developing and maintaining all the session beans Supported the application through debugging fixing production support and maintenance releases Worked closely with the client and the offsite team coordinated activities between them for effective implementation of the project Involved in Restful Web services with JSON using Jackson API Involved in Web servicesSOAPRESTfull Testing using Infor EAM WebService tool kit Environment J2SE JSP Servlets Struts EJB20 Ext JS XML Oracle 11g Postgresql Web Service SQL Server 2008R2 Eclipse TOAD JIRA SVN Tortoise Log4j Java Developer Hyderabad Telangana March 2014 to May 2016 Description Customer relationship management CRM is a system for managing a companys interactions with current and future customers It often involves using technology to organize automate and synchronize sales marketing customer service and technical support Customer relationship management is often thought of as a business strategy that enables businesses to improve in a number of areas Responsibilities Involved in Use Case meeting to understand and analyze the requirements Coded as per Prototype Involved in product development and customizations using Infor CRB studio Designing and Development of BIOs record sets data sources forms shells navigations Involved in Coding and bug fixing Used Validate plugin of struts framework to handle server side validations Involved in usage of SVN for version control process Logging of errors in application is achieved by using Log4J API Tables are created by running Scripts on SQL Developer IDE Involved in Build Deployment Activities and OracleSQL Database Schema Restore and backup Involved in Unit Test Integration Test and Regression Test plans Involved in Development process and have knowledge in usage of Tracker Tools like JIRA Having good Knowledge in Epiphany Platform Open Architecture Environment J2SE JSP Servlets Struts Spring Hibernate Java Beans XML Java Developer Infor Grenville Hyderabad Telangana March 2012 to February 2014 Description The workstation Project automates assignment of Workstation and Keys to each employee capable of identifies the unassigned workstation and Keys based on that we can easily assign that workstation and keys pedestalstoragecabin to new employee Gathered the Employee Details Key Details Workstation Details based on Floor and Zone and all these are entered by using Bulk Import Concept using this project employees can be viewed categorized based on their position Here we can identify the employee location based on FloorZone Responsibilities Functional and UI design has been prepared Implementation at BIO level Creation of Record sets and BIOs for the database schema Created Relationships for data Integrity Created Lookups and attribute domains Implementation at UI level Menus for Navigation Forms for various Perspectives Implemented shells like List Shell Detail Shell Tab Group Shell Toggle Shell to Provide better look and feel Toolbars to allow UI Actions for Buttons Used Form Slots by considering the BIO schema Attachments of documents has been provided for work ordersinvoices Authentication and authorization has been achieved by creating users and profiles in platadmin Implemented objectpermissions at widget menu and form levels Developed Form level extensions to achieve UI level validations and BIO level extensions to fulfil Functional requirements and validations All required data is entered by using Bulk Import Involved in Development process and have knowledge in usage of Tracker Tools like JIRA Having good Knowledge in Epiphany Platform Open Architecture Having Extensive Hands on Experience on Complex PLSQL Programming Environments CRB Studio Web logic server 81 LDAP Core Java SQL Server Associate Java Developer Hyderabad Telangana April 2011 to February 2012 Description Indian Institute of Science Outsourcing Company providing solutions to varied Enterprise Worldwide It is part of Research and Technology Indian Government Responsibilities Involved in translating Business Requirement into Technical Requirement Involved in all the phases of SDLC including Requirements Collection Design and Analysis of the Customer specification from the Business Analyst Designed UML class diagrams and Use Case diagrams to understand the code easily Oracle9i database was used as backend layer Used JDBC API to communicate with the database Developed various test cases such as unit test mock test and integration test using JUnit Developed the presentation layer and GUI framework that are written using JSP Clientside validation was done using JavaScript Extensively involved in the development of Java Server Pages JSP Involved in performance and SQL Query optimization Used CVS as version control tool Environment JDK 15 JDBC SQL Eclipse Oracle XML Apache Tomcat CVS JSP JUnit UML UNIX Education Bachelors Skills Apache 2 years APACHE CASSANDRA 2 years APACHE HADOOP HDFS 2 years APACHE HADOOP OOZIE 2 years APACHE HADOOP SQOOP 2 years APACHE HBASE 2 years ASTERADATA 2 years Cassandra 2 years Java 8 years JIRA 4 years JSP 3 years MS SQL SERVER 2 years ORACLE 6 years Servlets 2 years SQL 6 years SQL Server 2 years Struts 2 years Subversion 2 years SVN 2 years version control 3 years",
    "unique_id": "70552fee-9cd5-418e-9386-0dcb20ab0ce3"
}