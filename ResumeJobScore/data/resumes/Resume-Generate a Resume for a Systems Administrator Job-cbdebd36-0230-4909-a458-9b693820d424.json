{
    "clean_data": "Hadoop Spark Developer Hadoop amp Spark span lDeveloperspan Hadoop Spark Developer American Express Arizona City AZ Over 8 years of professional IT experience which includes 4 years of experience with Hadoop Map Reduce HDFS and Hadoop Ecosystems like Bigdata HDFS MapReduce Oozie Cassandra Hive Sqoop Pig Flume Hbase and Zookeeper and 5 years in Java and Oracle PLSQL development 7 years of experience in development of applications using Object Oriented Programming Indepth knowledge of Hadoop architecture and its components like HDFS Name Node Data Node Job Tracker Application Master Resource Manager Task Tracker and Map Reduce programming paradigm Experience in cluster planning designing deploying performance tuning administering and monitoring Hadoop ecosystem Commendable knowledge experience in importing and exporting data using Sqoop from HDFS to Relational Database Systems RDBMS and viceversa Experience in developing MapReduce jobs to process large data sets utilizing the MapReduce programming paradigm Good understanding of cloud configuration in Amazon web services AWS Experience in database design Used PLSQL to write Stored Procedures Functions Triggers and strong experience in writing complex queries for Oracle Proficient in writing SQL PLSQL stored procedures functions constraints packages and triggers Good experience in Hive tables design loading the data into hive tables Good understanding of HDFS Designs Daemons federation and HDFS high availability HA Good knowledge on Hadoop Cluster architecture and monitoring the cluster Hadoop Shell commands Writing Map reduce Programs Verifying the Hadoop Log Files Exposure on Query Programming Model of Hadoop Expert on UML for Object Oriented Analysis Design OOAD using MS Visio IBM Rational Expert on Core Java multithreading debugging JVM and optimizing and profiling Java Applications Experience on System Study Analysis of Business requirement preparation of Technical design UTP and UTC Coding Unit testing Integration testing System testing and Implementation Experience in Object Oriented Analysis and Design OOAD and development of software using UML methodology Hands on experience with Core Java with Multithreading Concurrency Exception Handling File handling IO Generics and Java Collections Implemented rich web applications such as HTML XHTML XML XSLT CSS JavaScript AJAXDWR jQuery ExtJS JSON and DOJO Excellent working knowledge of MVC architecture and Struts Spring MVC and JSF Frameworks Developed applications using Core Java Servlets JSP JDBC Struts Spring Hibernate Good understanding of the SOA technologies like SOAP WSDL Web Services Knowledge of Software Development Methodologies like Agile SCRUM Waterfall Proficient in using application servers like JBoss and Tomcat Servers Configured and deployed applications on IBM Web sphere BEA Web logic Tomcat Excellent working knowledge of Service Oriented ArchitectureSOA Messaging and Web Services Experienced on developing building and deploying applications on UNIX Linux Solaris and Windows platforms Experienced in database design and development and JDBC connectivity for Oracle 11g10g9i8i SQL PLSQL Stored procedures MS SQL Server  DB2 9x8x and MySQL Working knowledge of Java external applications like JUnit Log4J Apache Ant Maven Experienced in building and deploying applications on servers using Ant Maven and Perl Worked with query tools like Toad SQL Plus SQL Developer Expert level skills in Designing and Implementing web servers solutions and deploying Java Application Servers like Websphere Web Logic configuring Apache Web Server and configuring various Servlet engines Comprehensive knowledge of physical and logical data modeling performance tuning Resourceful and skilled in analyzing and solving problems Extensive experience in writing and executing JUnit Test cases debugging JavaJ2ee applications Hands on working experience with different version management software such as VSS Win CVS Subversion Star Team and SVN Excellent written verbal communication and customer service skills Strong organizational and interpersonal skills And possess a high level of drive initiative and selfmotivation A collaborative personality who enjoy working in a teamoriented environment Excellent debugging skills Able to debug complex technical issues including multiple system components Highly creative and articulate Can adapt quickly to rapidly changing conditions Work Experience Hadoop Spark Developer American Express Phoenix AZ October 2015 to Present The American Express is an American multinational financial services corporation The company is best known for its credit card charge card and travelers cheque businesses Amex cards account foe approximately 24 of the total dollar volume of credit card transactions in the USA This project is designed to extract the raw weblogs into Hadoop eco system and provide a complete picture of customer behavior across these various work streams About five years ago American Express recognized that traditional databases would not be enough to effectively handle the level of data and analytics needed for their projects and decided that a big data infrastructure would be the solution Data volume is not only increasing but data sources are also changing As part of American Expresss ongoing journey they must keep up with these changes in style of interactions as well as with the increasing volume Part of that involves making a huge number of decisions millions every day If American Express can become just a little bit smarter in these decisions it can have a huge advantage to customers and to the company This project is to expand on how they can use machine learning at large scale With access to big data machine learning models can produce superior discrimination and thus better understand customer behavior The primary focus is Fraud detection new customer acquisition and recommendation for better customer experience Roles Responsibilities Developed Big Data Solutions that enabled the business and technology teams to make datadriven decisions on the best ways to acquire customers and provide them business solutions Exported the businessrequired information to RDBMS using Sqoop to make the data available for BI team to generate reports based on data Migrated the existing data to Hadoop from RDBMS SQL Server and Oracle using Sqoop for processing the data Developed Spark Programs for Batch and Real time Processing Developed Spark Streaming applications for Real Time Processing Implemented Hive custom UDFs to achieve comprehensive data analysis Involved in installing Hadoop and Spark Cluster in Amazon Web Server Involved in installing configuring and managing Hadoop Ecosystem components like Spark Hive Pig Sqoop Kafka and Flume Responsible for loading unstructured and semistructured data into Hadoop cluster coming from different sources using Flume and managing Responsible for Data Ingestion like Flume and Kafka Used Hive data warehouse tool to analyze the data in HDFS and developed Hive queries Created internal and external tables with properly defined static and dynamic partitions for efficiency Developed MapReduce programs to cleanse and parse data in HDFS obtained from various data sources and to perform joins on the Map side using distributed cache Implemented daily workflow for extraction processing and analysis of data with Oozie Used the RegEx JSON and Avro SerDes for serialization and deserialization packaged with Hive to parse the contents of streamed log data Used Pig to develop adhoc queries Responsible for troubleshooting MapReduce jobs by reviewing the log files Environment Hadoop Spark Spark Streaming Spark Mlib Scala Hive Pig Hcatalog MapReduce Oozie Sqoop Flume and Kafka Hadoop DeveloperAdmin Merck Kenilworth NJ June 2014 to October 2015 Merck Co Inc is an American pharmaceutical company and one of the largest pharmaceutical companies in the world The company was established in 1891 It is the worlds seventh largest pharmaceutical company by market capitalization and revenue Its headquarters is located in Kenilworth New Jersey Adverse Drug Event ADE An adverse drug event ADE is an unwanted or unintended reaction that results from the normal use of one or more medications The consequences of ADEs range from mild allergic reactions to death with one study estimating that 97 of adverse drug events lead to permanent disability However the sheer amount of data involved in this analysis is a challenge Even restricting ourselves to analyzing pairs of drugs there are more than 3 trillion potential drugdrugreaction triples in the Adverse Event Reporting System AERS dataset and tens of millions of triples in the data Apache Spark is replacing Mapreduce jobs to clean aggregate and join data parallelize the counting problem across multiple machines to achieve a linear speedup in the overall runtime The faster runtime for each individual analysis allows rapid iteration on smaller models and tackle larger problems involving more drug interactions than anyone has ever looked at before Responsibilities All the datasets are loaded from two different sources such as Oracle MySQL to HDFS and Hive respectively on daily basis Process 8 flat files all are delimitated by Comma Responsible in creating Hive Tables to load the data which comes from MySQL and loading data from Oracle to HDFS using Sqoop Good hands on experience in writing core java level programming in order to perform cleaning preprocessing and data validation Involved in verifying cleaned data using Talend tool with other department Experienced in creating Hive schema external tables and managing views Involved in developing Hive UDFs and reused in some other requirements Worked on performing Join operations Involved in creating partitioning on external tables Good hands on experience in writing HQL statements as per the user requirements Fetching the HQL results into CSV files and handover to reporting team Work with hive complex datatypes and involved in Bucketing Involved in converting HiveSQL queries into Spark transformations using Spark RDDs Scala and have a good experience in using SparkShell and Spark Streaming Develop Spark code using Scala and SparkSQL for faster testing and data processing Import millions of structured data from relational databases using Sqoop import to process using Spark and stored the data into HDFS in CSV format Use Spark SQL to process the huge amount of structured data Implement Spark RDD transformations actions to migrate Map reduce algorithms Expertise in running Hadoop streaming jobs to process terabytes data Experience in importing the real time data to Hadoop using Kafka and implemented the Oozie job Responsible in analysis design testing phases and responsible for documenting technical specifications Along with the Infrastructure team involved in design and developed Kafka and Storm based data pipeline Develop stormmonitoring bolt for validating pump tag values against highlow and Worked on Talend Administrator Console TAC for scheduling jobs and adding users Develop Kafka producer and consumers Hbase clients Spark and Hadoop MapReduce jobs along with components on HDFS Hive Good knowledge in partitions bucketing concepts in Hive and designed both Managed and External tables in Hive to optimize performance Environment Hadoop Hive MapReduce Pig MongoDB Oozie Sqoop Kafka Cloudera Spark HBase HDFS Solr Zookeeper Cassandra DynamoDB Hadoop Developer Home Depot Atlanta GA October 2012 to June 2014 The Home Depot Inc is a home improvement supplies retailing company that sells tools construction products and services It operates many bigbox format stores across the United States including all 50 states the District of Columbia Puerto Rico the Virgin Islands and Guam all 10 provinces of Canada and the country of Mexico The company is headquartered at the Atlanta Store Support Center in Cobb County Georgia in Greater Atlanta Home Depot is the largest home improvement retailer in the United States ahead of rival Lowes Project to predict buying habits of customers and display ads on an online store This was a proof of Concept developed to determine if buying habits can be extracted without adding any additional monitoring scripts and with just based on ad interaction and social interaction of the consumer The data was all gathered from social networks Twitter Facebook to observe how vocal consumers are about their purchases Responsibilities Worked with technology and business groups for Hadoop migration strategy Researched and recommended suitable technology stack for Hadoop migration considering current enterprise architecture Designed docs and specs for the near real time data analytics using Hadoop and HBase Installed Cloudera Manager 37 on the clusters Used a 60 node cluster with Cloudera Hadoop distribution on Amazon EC2 Developed adclicks based data analytics for keyword analysis and insights Crawled public posts from Facebook and tweets Wrote MapReduce jobs with the Data Science team to analyze this data Validated and Recommended on Hadoop Infrastructure and data centre planning considering data growth Transferred data to and from cluster using Sqoop and various storage media such as Informix tables and flat files Developed MapReduce programs and Hive queries to analyse sales pattern and customer satisfaction index over the data present in various relational database tables Worked extensively in performance optimization by adoptingderiving at appropriate design patterns of the MapReduce jobs by analysing the IO latency map time combiner time reduce time etc Developed Pig scripts in the areas where extensive coding needs to be reduced Developed UDFs for Pig as needed Followed agile methodology for the entire project Defined problems to look for right data and analyze results to make room for new project Environment Hadoop 020 HBase HDFS MapReduce Java Cloudera Manager 2 Amazon EC2 classic JavaJ2ee Developer Penn Mutual Horsham PA July 2011 to October 2012 The Penn Mutual Life Insurance Company commonly referred to as Penn Mutual was founded in Philadelphia Pennsylvania in 1847 It was the seventh mutual life insurance company chartered in the United States The project was for an insurance portal which would enable Penn Mutuals customers to get a quote Login to their policy Report a Claim Pay Bills Update their Policy information and make updates to their policies Responsibilities Involved in the analysis Design Coding Modification and implementation of User Requirements in the Electronic Credit File Management system Designed the application using Front Controller Service Controller MVC Session Facade Design Patterns The application is designed using MVC Architecture Implemented the required functionality using Hibernate for persistence Spring Frame work Used Spring Framework for Dependency Injection Designed and implemented the Hibernate Domain Model for the services Developed UI using HTML JavaScript and JSP and developed Business Logic and Interfacing components using Business Objects XML and JDBC Designed userinterface and checking validations using JavaScript Involved in design of JSPs and Servlets for navigation among the modules Developed various EJBs for handling business logic and data manipulations from database Managed connectivity using JDBC for queryinginserting data management including triggers and stored procedures Developed SQL queries and Stored Procedures using PLSQL to retrieve and insert into multiple database schemas Developed the XML Schema and Web services for the data maintenance and structures Wrote test cases in JUnit for unit testing of classes Used DOM and DOM Functions using Firefox and IE Developer Tool bar for IE Debugged the application using Firebug to traverse the documents Involved in developing web pages using HTML and JSP Provided Technical support for production environments resolving the issues analysing the defects providing and implementing the solution defects Built and deployed Java applications into multiple UNIX based environments and produced both unit and functional test results along with release notes Developed the presentation layer using CSS and HTML taken from bootstrap to develop for browsers Environment Java Spring JSP Hibernate XML HTML JavaScript JDBC CSS SOAP Web services JavaJ2ee Developer Axis Bank Hyderabad Telangana May 29 to July 2011 Axis Bank Limited is the third largest private sector bank in India and for Information Security Risk Management The company provides wealth management asset management and investment banking services for private corporate and institutional clients worldwide and is generally considered to be a bulge bracket bank It deals with data related to investment banking It loads IT Credit Risk information into oracle database using Core Java 17 PLSQL and Unix Shell Scripting Responsibilities Developed JavaScript behavior code for user interaction Created database program in SQL server to manipulate data accumulated by internet transactions Wrote Servlets class to generate dynamic HTML pages Developed Servlets and backend Java classes using Web Sphere application server Developed an API to write XML documents from a database Performed usability testing for the application using JUnit Test Maintenance of a Java GUI application using JFCSwing Created complex SQL and used JDBC connectivity to access the database Used Spark API over Cloudera Hadoop YARN to perform analytics on data in Hive Involved in the design and coding of the data capture templates presentation and component templates Developed Scala scripts UDFs using both Data framesSQL and RDDMapReduce in Spark 13 for Data Aggregation queries and writing data back into OLTP system directly or through Sqoop Part of the team that designed customized and implemented metadata search and database synchronization Experience in working with versioning tools like Git CVS Clear Case Used Oracle as Database and used Toad for queries execution and also Involved in writing SQL scripts PL SQL code for procedures and functions Environment Java Web Sphere 35 EJB Servlets Spark JavaScript JDBC SQL Sqoop Git JUnit Eclipse IDE Apache Tomcat 6 UDF Education Bachelors Electronics Engineering DJSanghvi College of Engineering Skills CASSANDRA HDFS MAPREDUCE OOZIE SQOOP HBASE KAFKA SOLR DB2 FLUME JBOSS JMS MONGODB NOSQL APPLICATION SERVER CC C HBase Hive HTML Additional Information SKILLS Big Data Technology HDFS Mapreduce HBase Pig Hive SOLR Sqoop Flume MongoDB Cassandra Puppet Oozie Zookeeper Spark Kafka JavaJ2EE Technology JSP JSF Servlets EJB JDBC Struts Spring Spring MVC Spring Portlet Spring Web Flow Hibernate iBATIS JMS MQ JCA JNDI Java Beans JAXRPC JAXWS RMI RMIIIOP  Axis Castor SOAP WSDL UDDI JiBX JAXB DOM SAX  Facelets JPA Portal Portlet JSR 168286 LifeRay WebLogic Portal LDAP JUnitNET Languages Java 1456 CC Swing SQL HTML CSS i18n l10n DHTML XML XSD XHTML XSL XSLT XPath XQuery SQL PLSQL UML JavaScript AJAXDWR jQuery Dojo ExtJS Shell Scripts Perl Development FrameworkIDE RAD 8x7x60 IBM WebSphere Integration Developer 61 WSAD 5x Eclipse  MyEclipse 3x2x NetBeans 7x6x IntelliJ 7x Workshop 8161 Adobe Photoshop Adobe Dreamweaver Adobe Flash Ant Maven Rational Rose RSA MS Visio OpenMake Meister WebApplication Servers WebSphere Application Server 8x70615150 WebSphere Portal Server 7061 WebSphere Process Server 61 WebLogic Application Server 8161 JBoss 5x3x Apache 2x Tomcat 7x6x5x4x MS IIS IBM HTTP Server Databases NoSQL Oracle 11g10g9i8i DB2 9x8x MS SQL Server  MySQL NoSQL HBase Cassandra MongoDB Accumulo Operating Systems Windows XP 2K MSDOS Linux Red Hat Unix Solaris HP UX IBM AIX Version Control CVS SourceSafe ClearCase Subversion AllFusion Harvest Change Manager 71 Monitoring Tools Embarcadero J Optimizer 29 TPTP IBM Heap Analyzer Wily Introscope JMeter Other JBoss Drools 4x REST IBM Lotus WCM MS ISACA SiteMinder BMC WAM Mingle",
    "entities": [
        "Infrastructure",
        "Cobb County",
        "UTP",
        "Canada",
        "Designing",
        "Greater Atlanta Home Depot",
        "BI",
        "Concept",
        "HDFS",
        "UNIX",
        "Tomcat Excellent",
        "Georgia",
        "Integration testing System",
        "Researched",
        "Hadoop Developer Home Depot",
        "the Hibernate Domain Model",
        "IBM",
        "Merck Co Inc",
        "Hadoop Ecosystem",
        "Environment Hadoop Hive MapReduce Pig",
        "PL SQL",
        "UML for Object Oriented Analysis Design OOAD",
        "jQuery ExtJS JSON",
        "RDD",
        "Hadoop",
        "The Home Depot Inc",
        "XML",
        "CC C",
        "Git CVS Clear Case Used Oracle",
        "Atlanta",
        "Tomcat 7x6x5x4x MS IIS IBM",
        "UX IBM",
        "Puerto Rico",
        "the Virgin Islands",
        "Bucketing Involved",
        "JUnit",
        "Phoenix AZ",
        "IBM WebSphere Integration Developer",
        "Validated",
        "HBase",
        "WCM MS",
        "Developed Spark Programs",
        "Amazon",
        "Pennsylvania",
        "Cloudera Hadoop",
        "Hadoop Ecosystems",
        "Philadelphia",
        "SparkSQL",
        "Real Time Processing Implemented Hive",
        "UTC Coding Unit",
        "MS Visio IBM Rational Expert on",
        "UML",
        "Hadoop MapReduce",
        "the Atlanta Store Support Center",
        "Ant Maven",
        "Penn Mutuals",
        "Axis Castor SOAP WSDL UDDI",
        "Mexico",
        "DOJO Excellent",
        "Node Job Tracker Application Master Resource",
        "Oozie Used the RegEx JSON",
        "Axis Bank Limited",
        "SOA",
        "Oracle MySQL to HDFS and",
        "Firebug",
        "JSP",
        "JUnit Test",
        "JavaJ2EE Technology JSP",
        "REST IBM",
        "Built",
        "UML JavaScript",
        "Oracle Proficient",
        "Hadoop Cluster",
        "Talend",
        "JBoss",
        "Facebook",
        "Front Controller Service",
        "DOM",
        "WebSphere Portal",
        "JBoss Drools",
        "Hadoop Shell",
        "USA",
        "MS",
        "Developed SQL",
        "MVC",
        "Processing Developed Spark Streaming",
        "Spark",
        "Credit Risk",
        "EJB",
        "SparkShell",
        "TPTP",
        "Business Logic and Interfacing",
        "DOM Functions",
        "CSV",
        "Present The American Express",
        "IDE Apache Tomcat 6 UDF Education Bachelors Electronics Engineering DJSanghvi College of Engineering Skills CASSANDRA HDFS MAPREDUCE OOZIE SQOOP",
        "API",
        "American Expresss",
        "Sqoop",
        "Core Java Servlets JSP JDBC",
        "Storm",
        "Created",
        "Tomcat Servers Configured",
        "Hadoop Spark Developer Hadoop",
        "Data Aggregation",
        "Oracle",
        "Object Oriented Programming Indepth",
        "Information Security Risk Management",
        "Amex",
        "Responsible for Data Ingestion like Flume",
        "ADE",
        "HTML",
        "the Data Science",
        "Work Experience Hadoop Spark Developer American Express",
        "Recommended on Hadoop Infrastructure",
        "SQL",
        "American Express",
        "Hive Involved",
        "OLTP",
        "Guam",
        "IO",
        "Query Programming Model of Hadoop Expert",
        "MVC Architecture",
        "Relational Database Systems",
        "DOM SAX  ",
        "JFCSwing Created",
        "the United States",
        "Spark Streaming Develop Spark",
        "Hive",
        "JSP Provided Technical",
        "WebLogic Application",
        "Business Objects XML",
        "Lowes Project",
        "the District of Columbia",
        "Servlet",
        "India",
        "Zookeeper",
        "XSLT",
        "the Oozie job Responsible",
        "IE Developer Tool",
        "Spark Hive Pig",
        "Design Coding Modification",
        "SVN",
        "CSS",
        "Data",
        "MapReduce",
        "NetBeans",
        "Arizona",
        "Kenilworth New Jersey Adverse Drug Event",
        "Implementation Experience in Object Oriented Analysis and Design OOAD",
        "User Requirements",
        "Bigdata HDFS MapReduce Oozie Cassandra Hive Sqoop",
        "Informix",
        "the Electronic Credit File Management",
        "Worked on Talend Administrator Console TAC"
    ],
    "experience": "Experience in cluster planning designing deploying performance tuning administering and monitoring Hadoop ecosystem Commendable knowledge experience in importing and exporting data using Sqoop from HDFS to Relational Database Systems RDBMS and viceversa Experience in developing MapReduce jobs to process large data sets utilizing the MapReduce programming paradigm Good understanding of cloud configuration in Amazon web services AWS Experience in database design Used PLSQL to write Stored Procedures Functions Triggers and strong experience in writing complex queries for Oracle Proficient in writing SQL PLSQL stored procedures functions constraints packages and triggers Good experience in Hive tables design loading the data into hive tables Good understanding of HDFS Designs Daemons federation and HDFS high availability HA Good knowledge on Hadoop Cluster architecture and monitoring the cluster Hadoop Shell commands Writing Map reduce Programs Verifying the Hadoop Log Files Exposure on Query Programming Model of Hadoop Expert on UML for Object Oriented Analysis Design OOAD using MS Visio IBM Rational Expert on Core Java multithreading debugging JVM and optimizing and profiling Java Applications Experience on System Study Analysis of Business requirement preparation of Technical design UTP and UTC Coding Unit testing Integration testing System testing and Implementation Experience in Object Oriented Analysis and Design OOAD and development of software using UML methodology Hands on experience with Core Java with Multithreading Concurrency Exception Handling File handling IO Generics and Java Collections Implemented rich web applications such as HTML XHTML XML XSLT CSS JavaScript AJAXDWR jQuery ExtJS JSON and DOJO Excellent working knowledge of MVC architecture and Struts Spring MVC and JSF Frameworks Developed applications using Core Java Servlets JSP JDBC Struts Spring Hibernate Good understanding of the SOA technologies like SOAP WSDL Web Services Knowledge of Software Development Methodologies like Agile SCRUM Waterfall Proficient in using application servers like JBoss and Tomcat Servers Configured and deployed applications on IBM Web sphere BEA Web logic Tomcat Excellent working knowledge of Service Oriented ArchitectureSOA Messaging and Web Services Experienced on developing building and deploying applications on UNIX Linux Solaris and Windows platforms Experienced in database design and development and JDBC connectivity for Oracle 11g10g9i8i SQL PLSQL Stored procedures MS SQL Server   DB2 9x8x and MySQL Working knowledge of Java external applications like JUnit Log4J Apache Ant Maven Experienced in building and deploying applications on servers using Ant Maven and Perl Worked with query tools like Toad SQL Plus SQL Developer Expert level skills in Designing and Implementing web servers solutions and deploying Java Application Servers like Websphere Web Logic configuring Apache Web Server and configuring various Servlet engines Comprehensive knowledge of physical and logical data modeling performance tuning Resourceful and skilled in analyzing and solving problems Extensive experience in writing and executing JUnit Test cases debugging JavaJ2ee applications Hands on working experience with different version management software such as VSS Win CVS Subversion Star Team and SVN Excellent written verbal communication and customer service skills Strong organizational and interpersonal skills And possess a high level of drive initiative and selfmotivation A collaborative personality who enjoy working in a teamoriented environment Excellent debugging skills Able to debug complex technical issues including multiple system components Highly creative and articulate Can adapt quickly to rapidly changing conditions Work Experience Hadoop Spark Developer American Express Phoenix AZ October 2015 to Present The American Express is an American multinational financial services corporation The company is best known for its credit card charge card and travelers cheque businesses Amex cards account foe approximately 24 of the total dollar volume of credit card transactions in the USA This project is designed to extract the raw weblogs into Hadoop eco system and provide a complete picture of customer behavior across these various work streams About five years ago American Express recognized that traditional databases would not be enough to effectively handle the level of data and analytics needed for their projects and decided that a big data infrastructure would be the solution Data volume is not only increasing but data sources are also changing As part of American Expresss ongoing journey they must keep up with these changes in style of interactions as well as with the increasing volume Part of that involves making a huge number of decisions millions every day If American Express can become just a little bit smarter in these decisions it can have a huge advantage to customers and to the company This project is to expand on how they can use machine learning at large scale With access to big data machine learning models can produce superior discrimination and thus better understand customer behavior The primary focus is Fraud detection new customer acquisition and recommendation for better customer experience Roles Responsibilities Developed Big Data Solutions that enabled the business and technology teams to make datadriven decisions on the best ways to acquire customers and provide them business solutions Exported the businessrequired information to RDBMS using Sqoop to make the data available for BI team to generate reports based on data Migrated the existing data to Hadoop from RDBMS SQL Server and Oracle using Sqoop for processing the data Developed Spark Programs for Batch and Real time Processing Developed Spark Streaming applications for Real Time Processing Implemented Hive custom UDFs to achieve comprehensive data analysis Involved in installing Hadoop and Spark Cluster in Amazon Web Server Involved in installing configuring and managing Hadoop Ecosystem components like Spark Hive Pig Sqoop Kafka and Flume Responsible for loading unstructured and semistructured data into Hadoop cluster coming from different sources using Flume and managing Responsible for Data Ingestion like Flume and Kafka Used Hive data warehouse tool to analyze the data in HDFS and developed Hive queries Created internal and external tables with properly defined static and dynamic partitions for efficiency Developed MapReduce programs to cleanse and parse data in HDFS obtained from various data sources and to perform joins on the Map side using distributed cache Implemented daily workflow for extraction processing and analysis of data with Oozie Used the RegEx JSON and Avro SerDes for serialization and deserialization packaged with Hive to parse the contents of streamed log data Used Pig to develop adhoc queries Responsible for troubleshooting MapReduce jobs by reviewing the log files Environment Hadoop Spark Spark Streaming Spark Mlib Scala Hive Pig Hcatalog MapReduce Oozie Sqoop Flume and Kafka Hadoop DeveloperAdmin Merck Kenilworth NJ June 2014 to October 2015 Merck Co Inc is an American pharmaceutical company and one of the largest pharmaceutical companies in the world The company was established in 1891 It is the worlds seventh largest pharmaceutical company by market capitalization and revenue Its headquarters is located in Kenilworth New Jersey Adverse Drug Event ADE An adverse drug event ADE is an unwanted or unintended reaction that results from the normal use of one or more medications The consequences of ADEs range from mild allergic reactions to death with one study estimating that 97 of adverse drug events lead to permanent disability However the sheer amount of data involved in this analysis is a challenge Even restricting ourselves to analyzing pairs of drugs there are more than 3 trillion potential drugdrugreaction triples in the Adverse Event Reporting System AERS dataset and tens of millions of triples in the data Apache Spark is replacing Mapreduce jobs to clean aggregate and join data parallelize the counting problem across multiple machines to achieve a linear speedup in the overall runtime The faster runtime for each individual analysis allows rapid iteration on smaller models and tackle larger problems involving more drug interactions than anyone has ever looked at before Responsibilities All the datasets are loaded from two different sources such as Oracle MySQL to HDFS and Hive respectively on daily basis Process 8 flat files all are delimitated by Comma Responsible in creating Hive Tables to load the data which comes from MySQL and loading data from Oracle to HDFS using Sqoop Good hands on experience in writing core java level programming in order to perform cleaning preprocessing and data validation Involved in verifying cleaned data using Talend tool with other department Experienced in creating Hive schema external tables and managing views Involved in developing Hive UDFs and reused in some other requirements Worked on performing Join operations Involved in creating partitioning on external tables Good hands on experience in writing HQL statements as per the user requirements Fetching the HQL results into CSV files and handover to reporting team Work with hive complex datatypes and involved in Bucketing Involved in converting HiveSQL queries into Spark transformations using Spark RDDs Scala and have a good experience in using SparkShell and Spark Streaming Develop Spark code using Scala and SparkSQL for faster testing and data processing Import millions of structured data from relational databases using Sqoop import to process using Spark and stored the data into HDFS in CSV format Use Spark SQL to process the huge amount of structured data Implement Spark RDD transformations actions to migrate Map reduce algorithms Expertise in running Hadoop streaming jobs to process terabytes data Experience in importing the real time data to Hadoop using Kafka and implemented the Oozie job Responsible in analysis design testing phases and responsible for documenting technical specifications Along with the Infrastructure team involved in design and developed Kafka and Storm based data pipeline Develop stormmonitoring bolt for validating pump tag values against highlow and Worked on Talend Administrator Console TAC for scheduling jobs and adding users Develop Kafka producer and consumers Hbase clients Spark and Hadoop MapReduce jobs along with components on HDFS Hive Good knowledge in partitions bucketing concepts in Hive and designed both Managed and External tables in Hive to optimize performance Environment Hadoop Hive MapReduce Pig MongoDB Oozie Sqoop Kafka Cloudera Spark HBase HDFS Solr Zookeeper Cassandra DynamoDB Hadoop Developer Home Depot Atlanta GA October 2012 to June 2014 The Home Depot Inc is a home improvement supplies retailing company that sells tools construction products and services It operates many bigbox format stores across the United States including all 50 states the District of Columbia Puerto Rico the Virgin Islands and Guam all 10 provinces of Canada and the country of Mexico The company is headquartered at the Atlanta Store Support Center in Cobb County Georgia in Greater Atlanta Home Depot is the largest home improvement retailer in the United States ahead of rival Lowes Project to predict buying habits of customers and display ads on an online store This was a proof of Concept developed to determine if buying habits can be extracted without adding any additional monitoring scripts and with just based on ad interaction and social interaction of the consumer The data was all gathered from social networks Twitter Facebook to observe how vocal consumers are about their purchases Responsibilities Worked with technology and business groups for Hadoop migration strategy Researched and recommended suitable technology stack for Hadoop migration considering current enterprise architecture Designed docs and specs for the near real time data analytics using Hadoop and HBase Installed Cloudera Manager 37 on the clusters Used a 60 node cluster with Cloudera Hadoop distribution on Amazon EC2 Developed adclicks based data analytics for keyword analysis and insights Crawled public posts from Facebook and tweets Wrote MapReduce jobs with the Data Science team to analyze this data Validated and Recommended on Hadoop Infrastructure and data centre planning considering data growth Transferred data to and from cluster using Sqoop and various storage media such as Informix tables and flat files Developed MapReduce programs and Hive queries to analyse sales pattern and customer satisfaction index over the data present in various relational database tables Worked extensively in performance optimization by adoptingderiving at appropriate design patterns of the MapReduce jobs by analysing the IO latency map time combiner time reduce time etc Developed Pig scripts in the areas where extensive coding needs to be reduced Developed UDFs for Pig as needed Followed agile methodology for the entire project Defined problems to look for right data and analyze results to make room for new project Environment Hadoop 020 HBase HDFS MapReduce Java Cloudera Manager 2 Amazon EC2 classic JavaJ2ee Developer Penn Mutual Horsham PA July 2011 to October 2012 The Penn Mutual Life Insurance Company commonly referred to as Penn Mutual was founded in Philadelphia Pennsylvania in 1847 It was the seventh mutual life insurance company chartered in the United States The project was for an insurance portal which would enable Penn Mutuals customers to get a quote Login to their policy Report a Claim Pay Bills Update their Policy information and make updates to their policies Responsibilities Involved in the analysis Design Coding Modification and implementation of User Requirements in the Electronic Credit File Management system Designed the application using Front Controller Service Controller MVC Session Facade Design Patterns The application is designed using MVC Architecture Implemented the required functionality using Hibernate for persistence Spring Frame work Used Spring Framework for Dependency Injection Designed and implemented the Hibernate Domain Model for the services Developed UI using HTML JavaScript and JSP and developed Business Logic and Interfacing components using Business Objects XML and JDBC Designed userinterface and checking validations using JavaScript Involved in design of JSPs and Servlets for navigation among the modules Developed various EJBs for handling business logic and data manipulations from database Managed connectivity using JDBC for queryinginserting data management including triggers and stored procedures Developed SQL queries and Stored Procedures using PLSQL to retrieve and insert into multiple database schemas Developed the XML Schema and Web services for the data maintenance and structures Wrote test cases in JUnit for unit testing of classes Used DOM and DOM Functions using Firefox and IE Developer Tool bar for IE Debugged the application using Firebug to traverse the documents Involved in developing web pages using HTML and JSP Provided Technical support for production environments resolving the issues analysing the defects providing and implementing the solution defects Built and deployed Java applications into multiple UNIX based environments and produced both unit and functional test results along with release notes Developed the presentation layer using CSS and HTML taken from bootstrap to develop for browsers Environment Java Spring JSP Hibernate XML HTML JavaScript JDBC CSS SOAP Web services JavaJ2ee Developer Axis Bank Hyderabad Telangana May 29 to July 2011 Axis Bank Limited is the third largest private sector bank in India and for Information Security Risk Management The company provides wealth management asset management and investment banking services for private corporate and institutional clients worldwide and is generally considered to be a bulge bracket bank It deals with data related to investment banking It loads IT Credit Risk information into oracle database using Core Java 17 PLSQL and Unix Shell Scripting Responsibilities Developed JavaScript behavior code for user interaction Created database program in SQL server to manipulate data accumulated by internet transactions Wrote Servlets class to generate dynamic HTML pages Developed Servlets and backend Java classes using Web Sphere application server Developed an API to write XML documents from a database Performed usability testing for the application using JUnit Test Maintenance of a Java GUI application using JFCSwing Created complex SQL and used JDBC connectivity to access the database Used Spark API over Cloudera Hadoop YARN to perform analytics on data in Hive Involved in the design and coding of the data capture templates presentation and component templates Developed Scala scripts UDFs using both Data framesSQL and RDDMapReduce in Spark 13 for Data Aggregation queries and writing data back into OLTP system directly or through Sqoop Part of the team that designed customized and implemented metadata search and database synchronization Experience in working with versioning tools like Git CVS Clear Case Used Oracle as Database and used Toad for queries execution and also Involved in writing SQL scripts PL SQL code for procedures and functions Environment Java Web Sphere 35 EJB Servlets Spark JavaScript JDBC SQL Sqoop Git JUnit Eclipse IDE Apache Tomcat 6 UDF Education Bachelors Electronics Engineering DJSanghvi College of Engineering Skills CASSANDRA HDFS MAPREDUCE OOZIE SQOOP HBASE KAFKA SOLR DB2 FLUME JBOSS JMS MONGODB NOSQL APPLICATION SERVER CC C HBase Hive HTML Additional Information SKILLS Big Data Technology HDFS Mapreduce HBase Pig Hive SOLR Sqoop Flume MongoDB Cassandra Puppet Oozie Zookeeper Spark Kafka JavaJ2EE Technology JSP JSF Servlets EJB JDBC Struts Spring Spring MVC Spring Portlet Spring Web Flow Hibernate iBATIS JMS MQ JCA JNDI Java Beans JAXRPC JAXWS RMI RMIIIOP   Axis Castor SOAP WSDL UDDI JiBX JAXB DOM SAX   Facelets JPA Portal Portlet JSR 168286 LifeRay WebLogic Portal LDAP JUnitNET Languages Java 1456 CC Swing SQL HTML CSS i18n l10n DHTML XML XSD XHTML XSL XSLT XPath XQuery SQL PLSQL UML JavaScript AJAXDWR jQuery Dojo ExtJS Shell Scripts Perl Development FrameworkIDE RAD 8x7x60 IBM WebSphere Integration Developer 61 WSAD 5x Eclipse   MyEclipse 3x2x NetBeans 7x6x IntelliJ 7x Workshop 8161 Adobe Photoshop Adobe Dreamweaver Adobe Flash Ant Maven Rational Rose RSA MS Visio OpenMake Meister WebApplication Servers WebSphere Application Server 8x70615150 WebSphere Portal Server 7061 WebSphere Process Server 61 WebLogic Application Server 8161 JBoss 5x3x Apache 2x Tomcat 7x6x5x4x MS IIS IBM HTTP Server Databases NoSQL Oracle 11g10g9i8i DB2 9x8x MS SQL Server   MySQL NoSQL HBase Cassandra MongoDB Accumulo Operating Systems Windows XP 2 K MSDOS Linux Red Hat Unix Solaris HP UX IBM AIX Version Control CVS SourceSafe ClearCase Subversion AllFusion Harvest Change Manager 71 Monitoring Tools Embarcadero J Optimizer 29 TPTP IBM Heap Analyzer Wily Introscope JMeter Other JBoss Drools 4x REST IBM Lotus WCM MS ISACA SiteMinder BMC WAM Mingle",
    "extracted_keywords": [
        "Hadoop",
        "Spark",
        "Developer",
        "Hadoop",
        "amp",
        "Spark",
        "span",
        "lDeveloperspan",
        "Hadoop",
        "Spark",
        "Developer",
        "American",
        "Express",
        "Arizona",
        "City",
        "AZ",
        "years",
        "IT",
        "experience",
        "years",
        "experience",
        "Hadoop",
        "Map",
        "HDFS",
        "Hadoop",
        "Ecosystems",
        "Bigdata",
        "HDFS",
        "MapReduce",
        "Oozie",
        "Cassandra",
        "Hive",
        "Sqoop",
        "Pig",
        "Flume",
        "Hbase",
        "Zookeeper",
        "years",
        "Java",
        "Oracle",
        "PLSQL",
        "development",
        "years",
        "experience",
        "development",
        "applications",
        "Object",
        "Programming",
        "knowledge",
        "Hadoop",
        "architecture",
        "components",
        "HDFS",
        "Name",
        "Node",
        "Data",
        "Node",
        "Job",
        "Tracker",
        "Application",
        "Master",
        "Resource",
        "Manager",
        "Task",
        "Tracker",
        "Map",
        "Reduce",
        "programming",
        "paradigm",
        "Experience",
        "cluster",
        "planning",
        "performance",
        "Hadoop",
        "ecosystem",
        "knowledge",
        "experience",
        "data",
        "Sqoop",
        "HDFS",
        "Relational",
        "Database",
        "Systems",
        "RDBMS",
        "viceversa",
        "Experience",
        "MapReduce",
        "jobs",
        "data",
        "sets",
        "MapReduce",
        "programming",
        "paradigm",
        "understanding",
        "cloud",
        "configuration",
        "Amazon",
        "web",
        "services",
        "AWS",
        "Experience",
        "database",
        "design",
        "PLSQL",
        "Stored",
        "Procedures",
        "Functions",
        "Triggers",
        "experience",
        "queries",
        "Oracle",
        "Proficient",
        "SQL",
        "PLSQL",
        "procedures",
        "functions",
        "constraints",
        "packages",
        "experience",
        "Hive",
        "tables",
        "design",
        "data",
        "tables",
        "understanding",
        "HDFS",
        "Designs",
        "Daemons",
        "federation",
        "HDFS",
        "availability",
        "HA",
        "knowledge",
        "Hadoop",
        "Cluster",
        "architecture",
        "cluster",
        "Hadoop",
        "Shell",
        "Map",
        "Programs",
        "Hadoop",
        "Log",
        "Files",
        "Exposure",
        "Query",
        "Programming",
        "Model",
        "Hadoop",
        "Expert",
        "UML",
        "Object",
        "Oriented",
        "Analysis",
        "Design",
        "OOAD",
        "MS",
        "Visio",
        "IBM",
        "Rational",
        "Expert",
        "Core",
        "Java",
        "JVM",
        "Java",
        "Applications",
        "Experience",
        "System",
        "Study",
        "Analysis",
        "Business",
        "requirement",
        "preparation",
        "design",
        "UTP",
        "UTC",
        "Coding",
        "Unit",
        "testing",
        "Integration",
        "testing",
        "System",
        "testing",
        "Implementation",
        "Experience",
        "Object",
        "Oriented",
        "Analysis",
        "Design",
        "OOAD",
        "development",
        "software",
        "UML",
        "methodology",
        "Hands",
        "experience",
        "Core",
        "Java",
        "Multithreading",
        "Concurrency",
        "Exception",
        "Handling",
        "File",
        "IO",
        "Generics",
        "Java",
        "Collections",
        "web",
        "applications",
        "HTML",
        "XHTML",
        "XML",
        "XSLT",
        "CSS",
        "JavaScript",
        "AJAXDWR",
        "jQuery",
        "JSON",
        "DOJO",
        "Excellent",
        "knowledge",
        "MVC",
        "architecture",
        "Struts",
        "Spring",
        "MVC",
        "JSF",
        "Frameworks",
        "applications",
        "Core",
        "Java",
        "Servlets",
        "JSP",
        "JDBC",
        "Struts",
        "Spring",
        "Hibernate",
        "understanding",
        "SOA",
        "technologies",
        "SOAP",
        "WSDL",
        "Web",
        "Services",
        "Knowledge",
        "Software",
        "Development",
        "Methodologies",
        "SCRUM",
        "Waterfall",
        "Proficient",
        "application",
        "servers",
        "JBoss",
        "Tomcat",
        "Servers",
        "Configured",
        "applications",
        "IBM",
        "Web",
        "sphere",
        "BEA",
        "Web",
        "logic",
        "Tomcat",
        "Excellent",
        "knowledge",
        "Service",
        "ArchitectureSOA",
        "Messaging",
        "Web",
        "Services",
        "building",
        "applications",
        "UNIX",
        "Linux",
        "Solaris",
        "Windows",
        "platforms",
        "database",
        "design",
        "development",
        "JDBC",
        "connectivity",
        "Oracle",
        "11g10g9i8i",
        "SQL",
        "PLSQL",
        "procedures",
        "MS",
        "SQL",
        "Server",
        "DB2",
        "9x8x",
        "MySQL",
        "Working",
        "knowledge",
        "Java",
        "applications",
        "JUnit",
        "Log4J",
        "Apache",
        "Ant",
        "Maven",
        "building",
        "applications",
        "servers",
        "Ant",
        "Maven",
        "Perl",
        "Worked",
        "query",
        "tools",
        "Toad",
        "SQL",
        "SQL",
        "Developer",
        "Expert",
        "level",
        "skills",
        "Designing",
        "web",
        "servers",
        "solutions",
        "Java",
        "Application",
        "Servers",
        "Websphere",
        "Web",
        "Logic",
        "Apache",
        "Web",
        "Server",
        "Servlet",
        "engines",
        "knowledge",
        "data",
        "modeling",
        "performance",
        "problems",
        "experience",
        "writing",
        "JUnit",
        "Test",
        "cases",
        "applications",
        "Hands",
        "working",
        "experience",
        "version",
        "management",
        "software",
        "VSS",
        "Win",
        "CVS",
        "Subversion",
        "Star",
        "Team",
        "SVN",
        "Excellent",
        "communication",
        "customer",
        "service",
        "skills",
        "level",
        "initiative",
        "selfmotivation",
        "personality",
        "environment",
        "Excellent",
        "debugging",
        "skills",
        "issues",
        "system",
        "components",
        "conditions",
        "Work",
        "Experience",
        "Hadoop",
        "Spark",
        "Developer",
        "American",
        "Express",
        "Phoenix",
        "AZ",
        "October",
        "Present",
        "American",
        "Express",
        "services",
        "corporation",
        "company",
        "credit",
        "card",
        "charge",
        "card",
        "travelers",
        "businesses",
        "Amex",
        "cards",
        "account",
        "dollar",
        "volume",
        "credit",
        "card",
        "transactions",
        "USA",
        "project",
        "weblogs",
        "Hadoop",
        "eco",
        "system",
        "picture",
        "customer",
        "behavior",
        "work",
        "years",
        "American",
        "Express",
        "databases",
        "level",
        "data",
        "analytics",
        "projects",
        "data",
        "infrastructure",
        "solution",
        "Data",
        "volume",
        "data",
        "sources",
        "part",
        "American",
        "Expresss",
        "journey",
        "changes",
        "style",
        "interactions",
        "volume",
        "Part",
        "number",
        "decisions",
        "millions",
        "day",
        "American",
        "Express",
        "bit",
        "decisions",
        "advantage",
        "customers",
        "company",
        "project",
        "machine",
        "learning",
        "scale",
        "access",
        "data",
        "machine",
        "learning",
        "models",
        "discrimination",
        "customer",
        "behavior",
        "focus",
        "Fraud",
        "detection",
        "customer",
        "acquisition",
        "recommendation",
        "customer",
        "experience",
        "Roles",
        "Responsibilities",
        "Big",
        "Data",
        "Solutions",
        "business",
        "technology",
        "teams",
        "decisions",
        "ways",
        "customers",
        "business",
        "solutions",
        "information",
        "Sqoop",
        "data",
        "BI",
        "team",
        "reports",
        "data",
        "data",
        "Hadoop",
        "RDBMS",
        "SQL",
        "Server",
        "Oracle",
        "Sqoop",
        "data",
        "Spark",
        "Programs",
        "Batch",
        "time",
        "Spark",
        "Streaming",
        "applications",
        "Real",
        "Time",
        "Processing",
        "Hive",
        "custom",
        "UDFs",
        "data",
        "analysis",
        "Hadoop",
        "Spark",
        "Cluster",
        "Amazon",
        "Web",
        "Server",
        "configuring",
        "Hadoop",
        "Ecosystem",
        "components",
        "Spark",
        "Hive",
        "Pig",
        "Sqoop",
        "Kafka",
        "Flume",
        "Responsible",
        "data",
        "Hadoop",
        "cluster",
        "sources",
        "Flume",
        "Data",
        "Ingestion",
        "Flume",
        "Kafka",
        "Hive",
        "data",
        "warehouse",
        "tool",
        "data",
        "HDFS",
        "Hive",
        "queries",
        "tables",
        "partitions",
        "efficiency",
        "MapReduce",
        "programs",
        "data",
        "HDFS",
        "data",
        "sources",
        "joins",
        "Map",
        "side",
        "cache",
        "workflow",
        "extraction",
        "processing",
        "analysis",
        "data",
        "Oozie",
        "RegEx",
        "JSON",
        "Avro",
        "SerDes",
        "serialization",
        "deserialization",
        "Hive",
        "contents",
        "log",
        "data",
        "Pig",
        "queries",
        "MapReduce",
        "jobs",
        "log",
        "files",
        "Environment",
        "Hadoop",
        "Spark",
        "Spark",
        "Streaming",
        "Spark",
        "Mlib",
        "Scala",
        "Hive",
        "Pig",
        "Hcatalog",
        "MapReduce",
        "Oozie",
        "Sqoop",
        "Flume",
        "Kafka",
        "Hadoop",
        "DeveloperAdmin",
        "Merck",
        "Kenilworth",
        "NJ",
        "June",
        "October",
        "Merck",
        "Co",
        "Inc",
        "company",
        "companies",
        "world",
        "company",
        "worlds",
        "company",
        "market",
        "capitalization",
        "revenue",
        "headquarters",
        "Kenilworth",
        "New",
        "Jersey",
        "Adverse",
        "Drug",
        "Event",
        "ADE",
        "drug",
        "event",
        "ADE",
        "reaction",
        "use",
        "medications",
        "consequences",
        "ADEs",
        "range",
        "reactions",
        "death",
        "study",
        "drug",
        "events",
        "disability",
        "amount",
        "data",
        "analysis",
        "challenge",
        "pairs",
        "drugs",
        "drugdrugreaction",
        "triples",
        "Adverse",
        "Event",
        "System",
        "AERS",
        "dataset",
        "tens",
        "millions",
        "triples",
        "data",
        "Apache",
        "Spark",
        "Mapreduce",
        "jobs",
        "data",
        "counting",
        "problem",
        "machines",
        "speedup",
        "runtime",
        "runtime",
        "analysis",
        "iteration",
        "models",
        "problems",
        "drug",
        "interactions",
        "Responsibilities",
        "datasets",
        "sources",
        "Oracle",
        "MySQL",
        "HDFS",
        "Hive",
        "basis",
        "Process",
        "files",
        "Comma",
        "Responsible",
        "Hive",
        "Tables",
        "data",
        "MySQL",
        "loading",
        "data",
        "Oracle",
        "HDFS",
        "Sqoop",
        "hands",
        "experience",
        "core",
        "level",
        "programming",
        "order",
        "preprocessing",
        "data",
        "validation",
        "data",
        "Talend",
        "tool",
        "department",
        "Hive",
        "schema",
        "tables",
        "managing",
        "views",
        "Hive",
        "UDFs",
        "requirements",
        "Join",
        "operations",
        "tables",
        "hands",
        "experience",
        "HQL",
        "statements",
        "user",
        "requirements",
        "HQL",
        "results",
        "CSV",
        "files",
        "handover",
        "team",
        "datatypes",
        "Bucketing",
        "HiveSQL",
        "queries",
        "Spark",
        "transformations",
        "Spark",
        "RDDs",
        "Scala",
        "experience",
        "SparkShell",
        "Spark",
        "Streaming",
        "Develop",
        "Spark",
        "code",
        "Scala",
        "SparkSQL",
        "testing",
        "data",
        "Import",
        "millions",
        "data",
        "databases",
        "Sqoop",
        "import",
        "process",
        "Spark",
        "data",
        "HDFS",
        "CSV",
        "format",
        "Use",
        "Spark",
        "SQL",
        "amount",
        "data",
        "Implement",
        "Spark",
        "RDD",
        "transformations",
        "actions",
        "Map",
        "Expertise",
        "Hadoop",
        "streaming",
        "jobs",
        "terabytes",
        "data",
        "Experience",
        "time",
        "data",
        "Hadoop",
        "Kafka",
        "Oozie",
        "job",
        "analysis",
        "design",
        "testing",
        "phases",
        "specifications",
        "Infrastructure",
        "team",
        "design",
        "Kafka",
        "Storm",
        "data",
        "pipeline",
        "bolt",
        "pump",
        "tag",
        "values",
        "highlow",
        "Worked",
        "Talend",
        "Administrator",
        "Console",
        "TAC",
        "scheduling",
        "jobs",
        "users",
        "Develop",
        "Kafka",
        "producer",
        "consumers",
        "Hbase",
        "Spark",
        "Hadoop",
        "MapReduce",
        "jobs",
        "components",
        "HDFS",
        "Hive",
        "knowledge",
        "partitions",
        "bucketing",
        "concepts",
        "Hive",
        "Managed",
        "tables",
        "Hive",
        "performance",
        "Environment",
        "Hadoop",
        "Hive",
        "MapReduce",
        "Pig",
        "MongoDB",
        "Oozie",
        "Sqoop",
        "Kafka",
        "Cloudera",
        "Spark",
        "HBase",
        "HDFS",
        "Solr",
        "Zookeeper",
        "Cassandra",
        "DynamoDB",
        "Hadoop",
        "Developer",
        "Home",
        "Depot",
        "Atlanta",
        "GA",
        "October",
        "June",
        "Home",
        "Depot",
        "Inc",
        "home",
        "improvement",
        "supplies",
        "retailing",
        "company",
        "tools",
        "construction",
        "products",
        "services",
        "bigbox",
        "format",
        "stores",
        "United",
        "States",
        "states",
        "District",
        "Columbia",
        "Puerto",
        "Rico",
        "Virgin",
        "Islands",
        "Guam",
        "provinces",
        "Canada",
        "country",
        "Mexico",
        "company",
        "Atlanta",
        "Store",
        "Support",
        "Center",
        "Cobb",
        "County",
        "Georgia",
        "Greater",
        "Atlanta",
        "Home",
        "Depot",
        "home",
        "improvement",
        "retailer",
        "United",
        "States",
        "Lowes",
        "Project",
        "buying",
        "habits",
        "customers",
        "ads",
        "store",
        "proof",
        "Concept",
        "buying",
        "habits",
        "monitoring",
        "scripts",
        "ad",
        "interaction",
        "interaction",
        "consumer",
        "data",
        "networks",
        "Twitter",
        "Facebook",
        "consumers",
        "purchases",
        "Responsibilities",
        "technology",
        "business",
        "groups",
        "Hadoop",
        "migration",
        "strategy",
        "technology",
        "stack",
        "Hadoop",
        "migration",
        "enterprise",
        "architecture",
        "docs",
        "specs",
        "time",
        "data",
        "analytics",
        "Hadoop",
        "HBase",
        "Installed",
        "Cloudera",
        "Manager",
        "clusters",
        "node",
        "cluster",
        "Cloudera",
        "Hadoop",
        "distribution",
        "Amazon",
        "EC2",
        "Developed",
        "adclicks",
        "data",
        "analytics",
        "keyword",
        "analysis",
        "insights",
        "posts",
        "Facebook",
        "Wrote",
        "MapReduce",
        "jobs",
        "Data",
        "Science",
        "team",
        "data",
        "Hadoop",
        "Infrastructure",
        "data",
        "centre",
        "planning",
        "data",
        "growth",
        "data",
        "cluster",
        "Sqoop",
        "storage",
        "media",
        "Informix",
        "tables",
        "files",
        "Developed",
        "MapReduce",
        "programs",
        "Hive",
        "analyse",
        "sales",
        "pattern",
        "customer",
        "satisfaction",
        "index",
        "data",
        "database",
        "tables",
        "performance",
        "optimization",
        "design",
        "patterns",
        "MapReduce",
        "jobs",
        "IO",
        "latency",
        "map",
        "time",
        "combiner",
        "time",
        "time",
        "Developed",
        "Pig",
        "scripts",
        "areas",
        "UDFs",
        "Pig",
        "methodology",
        "project",
        "problems",
        "data",
        "results",
        "room",
        "project",
        "Environment",
        "Hadoop",
        "HBase",
        "HDFS",
        "MapReduce",
        "Java",
        "Cloudera",
        "Manager",
        "Amazon",
        "EC2",
        "JavaJ2ee",
        "Developer",
        "Penn",
        "Mutual",
        "Horsham",
        "PA",
        "July",
        "October",
        "Penn",
        "Mutual",
        "Life",
        "Insurance",
        "Company",
        "Penn",
        "Mutual",
        "Philadelphia",
        "Pennsylvania",
        "life",
        "insurance",
        "company",
        "United",
        "States",
        "project",
        "insurance",
        "portal",
        "Penn",
        "Mutuals",
        "customers",
        "quote",
        "Login",
        "policy",
        "Report",
        "Claim",
        "Pay",
        "Bills",
        "Policy",
        "information",
        "updates",
        "policies",
        "Responsibilities",
        "analysis",
        "Design",
        "Coding",
        "Modification",
        "implementation",
        "User",
        "Requirements",
        "Electronic",
        "Credit",
        "File",
        "Management",
        "system",
        "application",
        "Front",
        "Controller",
        "Service",
        "Controller",
        "MVC",
        "Session",
        "Facade",
        "Design",
        "Patterns",
        "application",
        "MVC",
        "Architecture",
        "functionality",
        "Hibernate",
        "persistence",
        "Spring",
        "Frame",
        "work",
        "Spring",
        "Framework",
        "Dependency",
        "Injection",
        "Hibernate",
        "Domain",
        "Model",
        "services",
        "UI",
        "HTML",
        "JavaScript",
        "JSP",
        "Business",
        "Logic",
        "components",
        "Business",
        "Objects",
        "XML",
        "JDBC",
        "userinterface",
        "validations",
        "JavaScript",
        "design",
        "JSPs",
        "Servlets",
        "navigation",
        "modules",
        "EJBs",
        "business",
        "logic",
        "data",
        "manipulations",
        "database",
        "Managed",
        "connectivity",
        "JDBC",
        "data",
        "management",
        "triggers",
        "procedures",
        "SQL",
        "queries",
        "Procedures",
        "PLSQL",
        "database",
        "schemas",
        "XML",
        "Schema",
        "Web",
        "services",
        "data",
        "maintenance",
        "structures",
        "Wrote",
        "test",
        "cases",
        "JUnit",
        "unit",
        "testing",
        "classes",
        "DOM",
        "DOM",
        "Functions",
        "Firefox",
        "IE",
        "Developer",
        "Tool",
        "bar",
        "IE",
        "application",
        "Firebug",
        "documents",
        "web",
        "pages",
        "HTML",
        "JSP",
        "support",
        "production",
        "environments",
        "issues",
        "defects",
        "solution",
        "defects",
        "Java",
        "applications",
        "UNIX",
        "environments",
        "unit",
        "test",
        "results",
        "release",
        "notes",
        "presentation",
        "layer",
        "CSS",
        "HTML",
        "bootstrap",
        "browsers",
        "Environment",
        "Java",
        "Spring",
        "JSP",
        "Hibernate",
        "XML",
        "HTML",
        "JavaScript",
        "JDBC",
        "CSS",
        "SOAP",
        "Web",
        "services",
        "Developer",
        "Axis",
        "Bank",
        "Hyderabad",
        "Telangana",
        "May",
        "July",
        "Axis",
        "Bank",
        "Limited",
        "sector",
        "bank",
        "India",
        "Information",
        "Security",
        "Risk",
        "Management",
        "company",
        "wealth",
        "management",
        "asset",
        "management",
        "investment",
        "banking",
        "services",
        "clients",
        "bulge",
        "bracket",
        "bank",
        "data",
        "investment",
        "banking",
        "IT",
        "Credit",
        "Risk",
        "information",
        "oracle",
        "database",
        "Core",
        "Java",
        "PLSQL",
        "Unix",
        "Shell",
        "Scripting",
        "Responsibilities",
        "Developed",
        "JavaScript",
        "behavior",
        "code",
        "user",
        "interaction",
        "database",
        "program",
        "SQL",
        "server",
        "data",
        "internet",
        "transactions",
        "Wrote",
        "Servlets",
        "class",
        "HTML",
        "pages",
        "Developed",
        "Servlets",
        "Java",
        "classes",
        "Web",
        "Sphere",
        "application",
        "server",
        "API",
        "XML",
        "documents",
        "database",
        "usability",
        "testing",
        "application",
        "JUnit",
        "Test",
        "Maintenance",
        "Java",
        "GUI",
        "application",
        "JFCSwing",
        "Created",
        "SQL",
        "JDBC",
        "connectivity",
        "database",
        "Spark",
        "API",
        "Cloudera",
        "Hadoop",
        "YARN",
        "analytics",
        "data",
        "Hive",
        "design",
        "coding",
        "data",
        "capture",
        "presentation",
        "component",
        "templates",
        "Scala",
        "UDFs",
        "Data",
        "framesSQL",
        "RDDMapReduce",
        "Spark",
        "Data",
        "Aggregation",
        "data",
        "OLTP",
        "system",
        "Sqoop",
        "Part",
        "team",
        "metadata",
        "search",
        "database",
        "synchronization",
        "Experience",
        "tools",
        "Git",
        "CVS",
        "Clear",
        "Case",
        "Oracle",
        "Database",
        "Toad",
        "queries",
        "execution",
        "SQL",
        "scripts",
        "PL",
        "SQL",
        "code",
        "procedures",
        "functions",
        "Environment",
        "Java",
        "Web",
        "Sphere",
        "EJB",
        "Servlets",
        "Spark",
        "JavaScript",
        "JDBC",
        "SQL",
        "Sqoop",
        "Git",
        "JUnit",
        "Eclipse",
        "IDE",
        "Apache",
        "Tomcat",
        "UDF",
        "Education",
        "Bachelors",
        "Electronics",
        "Engineering",
        "DJSanghvi",
        "College",
        "Engineering",
        "Skills",
        "CASSANDRA",
        "HDFS",
        "MAPREDUCE",
        "OOZIE",
        "SQOOP",
        "HBASE",
        "KAFKA",
        "SOLR",
        "DB2",
        "FLUME",
        "JBOSS",
        "JMS",
        "NOSQL",
        "APPLICATION",
        "SERVER",
        "CC",
        "C",
        "HBase",
        "Hive",
        "HTML",
        "Information",
        "SKILLS",
        "Big",
        "Data",
        "Technology",
        "HDFS",
        "Mapreduce",
        "HBase",
        "Pig",
        "Hive",
        "SOLR",
        "Sqoop",
        "Flume",
        "MongoDB",
        "Cassandra",
        "Puppet",
        "Oozie",
        "Zookeeper",
        "Spark",
        "Kafka",
        "JavaJ2EE",
        "Technology",
        "JSP",
        "JSF",
        "Servlets",
        "EJB",
        "JDBC",
        "Struts",
        "Spring",
        "Spring",
        "MVC",
        "Spring",
        "Portlet",
        "Spring",
        "Web",
        "Flow",
        "Hibernate",
        "JMS",
        "MQ",
        "JCA",
        "JNDI",
        "Java",
        "Beans",
        "JAXRPC",
        "JAXWS",
        "RMI",
        "RMIIIOP",
        "Axis",
        "Castor",
        "SOAP",
        "WSDL",
        "UDDI",
        "JiBX",
        "JAXB",
        "DOM",
        "SAX",
        "Facelets",
        "JPA",
        "Portal",
        "Portlet",
        "JSR",
        "LifeRay",
        "WebLogic",
        "Portal",
        "LDAP",
        "JUnitNET",
        "Languages",
        "Java",
        "CC",
        "Swing",
        "SQL",
        "HTML",
        "CSS",
        "DHTML",
        "XML",
        "XSD",
        "XHTML",
        "XSL",
        "XSLT",
        "XPath",
        "XQuery",
        "SQL",
        "PLSQL",
        "UML",
        "JavaScript",
        "AJAXDWR",
        "jQuery",
        "Dojo",
        "ExtJS",
        "Shell",
        "Scripts",
        "Perl",
        "Development",
        "FrameworkIDE",
        "RAD",
        "IBM",
        "WebSphere",
        "Integration",
        "Developer",
        "WSAD",
        "5x",
        "Eclipse",
        "MyEclipse",
        "NetBeans",
        "7x6x",
        "IntelliJ",
        "7x",
        "Workshop",
        "Adobe",
        "Photoshop",
        "Adobe",
        "Dreamweaver",
        "Adobe",
        "Flash",
        "Ant",
        "Maven",
        "Rational",
        "Rose",
        "RSA",
        "MS",
        "Visio",
        "OpenMake",
        "Meister",
        "WebApplication",
        "Servers",
        "WebSphere",
        "Application",
        "Server",
        "8x70615150",
        "WebSphere",
        "Portal",
        "Server",
        "WebSphere",
        "Process",
        "Server",
        "WebLogic",
        "Application",
        "Server",
        "JBoss",
        "5x3x",
        "Apache",
        "Tomcat",
        "7x6x5x4x",
        "MS",
        "IIS",
        "IBM",
        "HTTP",
        "Server",
        "Databases",
        "NoSQL",
        "Oracle",
        "11g10g9i8i",
        "DB2",
        "MS",
        "SQL",
        "Server",
        "MySQL",
        "NoSQL",
        "HBase",
        "Cassandra",
        "MongoDB",
        "Accumulo",
        "Operating",
        "Systems",
        "Windows",
        "XP",
        "K",
        "MSDOS",
        "Linux",
        "Red",
        "Hat",
        "Unix",
        "Solaris",
        "HP",
        "UX",
        "IBM",
        "AIX",
        "Version",
        "Control",
        "CVS",
        "SourceSafe",
        "ClearCase",
        "Subversion",
        "AllFusion",
        "Harvest",
        "Change",
        "Manager",
        "Monitoring",
        "Tools",
        "Embarcadero",
        "J",
        "Optimizer",
        "TPTP",
        "IBM",
        "Heap",
        "Analyzer",
        "Wily",
        "Introscope",
        "JMeter",
        "JBoss",
        "Drools",
        "4x",
        "REST",
        "IBM",
        "Lotus",
        "WCM",
        "MS",
        "ISACA",
        "SiteMinder",
        "BMC",
        "WAM",
        "Mingle"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T22:43:54.946101",
    "resume_data": "Hadoop Spark Developer Hadoop amp Spark span lDeveloperspan Hadoop Spark Developer American Express Arizona City AZ Over 8 years of professional IT experience which includes 4 years of experience with Hadoop Map Reduce HDFS and Hadoop Ecosystems like Bigdata HDFS MapReduce Oozie Cassandra Hive Sqoop Pig Flume Hbase and Zookeeper and 5 years in Java and Oracle PLSQL development 7 years of experience in development of applications using Object Oriented Programming Indepth knowledge of Hadoop architecture and its components like HDFS Name Node Data Node Job Tracker Application Master Resource Manager Task Tracker and Map Reduce programming paradigm Experience in cluster planning designing deploying performance tuning administering and monitoring Hadoop ecosystem Commendable knowledge experience in importing and exporting data using Sqoop from HDFS to Relational Database Systems RDBMS and viceversa Experience in developing MapReduce jobs to process large data sets utilizing the MapReduce programming paradigm Good understanding of cloud configuration in Amazon web services AWS Experience in database design Used PLSQL to write Stored Procedures Functions Triggers and strong experience in writing complex queries for Oracle Proficient in writing SQL PLSQL stored procedures functions constraints packages and triggers Good experience in Hive tables design loading the data into hive tables Good understanding of HDFS Designs Daemons federation and HDFS high availability HA Good knowledge on Hadoop Cluster architecture and monitoring the cluster Hadoop Shell commands Writing Map reduce Programs Verifying the Hadoop Log Files Exposure on Query Programming Model of Hadoop Expert on UML for Object Oriented Analysis Design OOAD using MS Visio IBM Rational Expert on Core Java multithreading debugging JVM and optimizing and profiling Java Applications Experience on System Study Analysis of Business requirement preparation of Technical design UTP and UTC Coding Unit testing Integration testing System testing and Implementation Experience in Object Oriented Analysis and Design OOAD and development of software using UML methodology Hands on experience with Core Java with Multithreading Concurrency Exception Handling File handling IO Generics and Java Collections Implemented rich web applications such as HTML XHTML XML XSLT CSS JavaScript AJAXDWR jQuery ExtJS JSON and DOJO Excellent working knowledge of MVC architecture and Struts Spring MVC and JSF Frameworks Developed applications using Core Java Servlets JSP JDBC Struts Spring Hibernate Good understanding of the SOA technologies like SOAP WSDL Web Services Knowledge of Software Development Methodologies like Agile SCRUM Waterfall Proficient in using application servers like JBoss and Tomcat Servers Configured and deployed applications on IBM Web sphere BEA Web logic Tomcat Excellent working knowledge of Service Oriented ArchitectureSOA Messaging and Web Services Experienced on developing building and deploying applications on UNIX Linux Solaris and Windows platforms Experienced in database design and development and JDBC connectivity for Oracle 11g10g9i8i SQL PLSQL Stored procedures MS SQL Server 200820052000 DB2 9x8x and MySQL Working knowledge of Java external applications like JUnit Log4J Apache Ant Maven Experienced in building and deploying applications on servers using Ant Maven and Perl Worked with query tools like Toad SQL Plus SQL Developer Expert level skills in Designing and Implementing web servers solutions and deploying Java Application Servers like Websphere Web Logic configuring Apache Web Server and configuring various Servlet engines Comprehensive knowledge of physical and logical data modeling performance tuning Resourceful and skilled in analyzing and solving problems Extensive experience in writing and executing JUnit Test cases debugging JavaJ2ee applications Hands on working experience with different version management software such as VSS Win CVS Subversion Star Team and SVN Excellent written verbal communication and customer service skills Strong organizational and interpersonal skills And possess a high level of drive initiative and selfmotivation A collaborative personality who enjoy working in a teamoriented environment Excellent debugging skills Able to debug complex technical issues including multiple system components Highly creative and articulate Can adapt quickly to rapidly changing conditions Work Experience Hadoop Spark Developer American Express Phoenix AZ October 2015 to Present The American Express is an American multinational financial services corporation The company is best known for its credit card charge card and travelers cheque businesses Amex cards account foe approximately 24 of the total dollar volume of credit card transactions in the USA This project is designed to extract the raw weblogs into Hadoop eco system and provide a complete picture of customer behavior across these various work streams About five years ago American Express recognized that traditional databases would not be enough to effectively handle the level of data and analytics needed for their projects and decided that a big data infrastructure would be the solution Data volume is not only increasing but data sources are also changing As part of American Expresss ongoing journey they must keep up with these changes in style of interactions as well as with the increasing volume Part of that involves making a huge number of decisions millions every day If American Express can become just a little bit smarter in these decisions it can have a huge advantage to customers and to the company This project is to expand on how they can use machine learning at large scale With access to big data machine learning models can produce superior discrimination and thus better understand customer behavior The primary focus is Fraud detection new customer acquisition and recommendation for better customer experience Roles Responsibilities Developed Big Data Solutions that enabled the business and technology teams to make datadriven decisions on the best ways to acquire customers and provide them business solutions Exported the businessrequired information to RDBMS using Sqoop to make the data available for BI team to generate reports based on data Migrated the existing data to Hadoop from RDBMS SQL Server and Oracle using Sqoop for processing the data Developed Spark Programs for Batch and Real time Processing Developed Spark Streaming applications for Real Time Processing Implemented Hive custom UDFs to achieve comprehensive data analysis Involved in installing Hadoop and Spark Cluster in Amazon Web Server Involved in installing configuring and managing Hadoop Ecosystem components like Spark Hive Pig Sqoop Kafka and Flume Responsible for loading unstructured and semistructured data into Hadoop cluster coming from different sources using Flume and managing Responsible for Data Ingestion like Flume and Kafka Used Hive data warehouse tool to analyze the data in HDFS and developed Hive queries Created internal and external tables with properly defined static and dynamic partitions for efficiency Developed MapReduce programs to cleanse and parse data in HDFS obtained from various data sources and to perform joins on the Map side using distributed cache Implemented daily workflow for extraction processing and analysis of data with Oozie Used the RegEx JSON and Avro SerDes for serialization and deserialization packaged with Hive to parse the contents of streamed log data Used Pig to develop adhoc queries Responsible for troubleshooting MapReduce jobs by reviewing the log files Environment Hadoop Spark Spark Streaming Spark Mlib Scala Hive Pig Hcatalog MapReduce Oozie Sqoop Flume and Kafka Hadoop DeveloperAdmin Merck Kenilworth NJ June 2014 to October 2015 Merck Co Inc is an American pharmaceutical company and one of the largest pharmaceutical companies in the world The company was established in 1891 It is the worlds seventh largest pharmaceutical company by market capitalization and revenue Its headquarters is located in Kenilworth New Jersey Adverse Drug Event ADE An adverse drug event ADE is an unwanted or unintended reaction that results from the normal use of one or more medications The consequences of ADEs range from mild allergic reactions to death with one study estimating that 97 of adverse drug events lead to permanent disability However the sheer amount of data involved in this analysis is a challenge Even restricting ourselves to analyzing pairs of drugs there are more than 3 trillion potential drugdrugreaction triples in the Adverse Event Reporting System AERS dataset and tens of millions of triples in the data Apache Spark is replacing Mapreduce jobs to clean aggregate and join data parallelize the counting problem across multiple machines to achieve a linear speedup in the overall runtime The faster runtime for each individual analysis allows rapid iteration on smaller models and tackle larger problems involving more drug interactions than anyone has ever looked at before Responsibilities All the datasets are loaded from two different sources such as Oracle MySQL to HDFS and Hive respectively on daily basis Process 8 flat files all are delimitated by Comma Responsible in creating Hive Tables to load the data which comes from MySQL and loading data from Oracle to HDFS using Sqoop Good hands on experience in writing core java level programming in order to perform cleaning preprocessing and data validation Involved in verifying cleaned data using Talend tool with other department Experienced in creating Hive schema external tables and managing views Involved in developing Hive UDFs and reused in some other requirements Worked on performing Join operations Involved in creating partitioning on external tables Good hands on experience in writing HQL statements as per the user requirements Fetching the HQL results into CSV files and handover to reporting team Work with hive complex datatypes and involved in Bucketing Involved in converting HiveSQL queries into Spark transformations using Spark RDDs Scala and have a good experience in using SparkShell and Spark Streaming Develop Spark code using Scala and SparkSQL for faster testing and data processing Import millions of structured data from relational databases using Sqoop import to process using Spark and stored the data into HDFS in CSV format Use Spark SQL to process the huge amount of structured data Implement Spark RDD transformations actions to migrate Map reduce algorithms Expertise in running Hadoop streaming jobs to process terabytes data Experience in importing the real time data to Hadoop using Kafka and implemented the Oozie job Responsible in analysis design testing phases and responsible for documenting technical specifications Along with the Infrastructure team involved in design and developed Kafka and Storm based data pipeline Develop stormmonitoring bolt for validating pump tag values against highlow and Worked on Talend Administrator Console TAC for scheduling jobs and adding users Develop Kafka producer and consumers Hbase clients Spark and Hadoop MapReduce jobs along with components on HDFS Hive Good knowledge in partitions bucketing concepts in Hive and designed both Managed and External tables in Hive to optimize performance Environment Hadoop Hive MapReduce Pig MongoDB Oozie Sqoop Kafka Cloudera Spark HBase HDFS Solr Zookeeper Cassandra DynamoDB Hadoop Developer Home Depot Atlanta GA October 2012 to June 2014 The Home Depot Inc is a home improvement supplies retailing company that sells tools construction products and services It operates many bigbox format stores across the United States including all 50 states the District of Columbia Puerto Rico the Virgin Islands and Guam all 10 provinces of Canada and the country of Mexico The company is headquartered at the Atlanta Store Support Center in Cobb County Georgia in Greater Atlanta Home Depot is the largest home improvement retailer in the United States ahead of rival Lowes Project to predict buying habits of customers and display ads on an online store This was a proof of Concept developed to determine if buying habits can be extracted without adding any additional monitoring scripts and with just based on ad interaction and social interaction of the consumer The data was all gathered from social networks Twitter Facebook to observe how vocal consumers are about their purchases Responsibilities Worked with technology and business groups for Hadoop migration strategy Researched and recommended suitable technology stack for Hadoop migration considering current enterprise architecture Designed docs and specs for the near real time data analytics using Hadoop and HBase Installed Cloudera Manager 37 on the clusters Used a 60 node cluster with Cloudera Hadoop distribution on Amazon EC2 Developed adclicks based data analytics for keyword analysis and insights Crawled public posts from Facebook and tweets Wrote MapReduce jobs with the Data Science team to analyze this data Validated and Recommended on Hadoop Infrastructure and data centre planning considering data growth Transferred data to and from cluster using Sqoop and various storage media such as Informix tables and flat files Developed MapReduce programs and Hive queries to analyse sales pattern and customer satisfaction index over the data present in various relational database tables Worked extensively in performance optimization by adoptingderiving at appropriate design patterns of the MapReduce jobs by analysing the IO latency map time combiner time reduce time etc Developed Pig scripts in the areas where extensive coding needs to be reduced Developed UDFs for Pig as needed Followed agile methodology for the entire project Defined problems to look for right data and analyze results to make room for new project Environment Hadoop 020 HBase HDFS MapReduce Java Cloudera Manager 2 Amazon EC2 classic JavaJ2ee Developer Penn Mutual Horsham PA July 2011 to October 2012 The Penn Mutual Life Insurance Company commonly referred to as Penn Mutual was founded in Philadelphia Pennsylvania in 1847 It was the seventh mutual life insurance company chartered in the United States The project was for an insurance portal which would enable Penn Mutuals customers to get a quote Login to their policy Report a Claim Pay Bills Update their Policy information and make updates to their policies Responsibilities Involved in the analysis Design Coding Modification and implementation of User Requirements in the Electronic Credit File Management system Designed the application using Front Controller Service Controller MVC Session Facade Design Patterns The application is designed using MVC Architecture Implemented the required functionality using Hibernate for persistence Spring Frame work Used Spring Framework for Dependency Injection Designed and implemented the Hibernate Domain Model for the services Developed UI using HTML JavaScript and JSP and developed Business Logic and Interfacing components using Business Objects XML and JDBC Designed userinterface and checking validations using JavaScript Involved in design of JSPs and Servlets for navigation among the modules Developed various EJBs for handling business logic and data manipulations from database Managed connectivity using JDBC for queryinginserting data management including triggers and stored procedures Developed SQL queries and Stored Procedures using PLSQL to retrieve and insert into multiple database schemas Developed the XML Schema and Web services for the data maintenance and structures Wrote test cases in JUnit for unit testing of classes Used DOM and DOM Functions using Firefox and IE Developer Tool bar for IE Debugged the application using Firebug to traverse the documents Involved in developing web pages using HTML and JSP Provided Technical support for production environments resolving the issues analysing the defects providing and implementing the solution defects Built and deployed Java applications into multiple UNIX based environments and produced both unit and functional test results along with release notes Developed the presentation layer using CSS and HTML taken from bootstrap to develop for browsers Environment Java Spring JSP Hibernate XML HTML JavaScript JDBC CSS SOAP Web services JavaJ2ee Developer Axis Bank Hyderabad Telangana May 2009 to July 2011 Axis Bank Limited is the third largest private sector bank in India and for Information Security Risk Management The company provides wealth management asset management and investment banking services for private corporate and institutional clients worldwide and is generally considered to be a bulge bracket bank It deals with data related to investment banking It loads IT Credit Risk information into oracle database using Core Java 17 PLSQL and Unix Shell Scripting Responsibilities Developed JavaScript behavior code for user interaction Created database program in SQL server to manipulate data accumulated by internet transactions Wrote Servlets class to generate dynamic HTML pages Developed Servlets and backend Java classes using Web Sphere application server Developed an API to write XML documents from a database Performed usability testing for the application using JUnit Test Maintenance of a Java GUI application using JFCSwing Created complex SQL and used JDBC connectivity to access the database Used Spark API over Cloudera Hadoop YARN to perform analytics on data in Hive Involved in the design and coding of the data capture templates presentation and component templates Developed Scala scripts UDFs using both Data framesSQL and RDDMapReduce in Spark 13 for Data Aggregation queries and writing data back into OLTP system directly or through Sqoop Part of the team that designed customized and implemented metadata search and database synchronization Experience in working with versioning tools like Git CVS Clear Case Used Oracle as Database and used Toad for queries execution and also Involved in writing SQL scripts PL SQL code for procedures and functions Environment Java Web Sphere 35 EJB Servlets Spark JavaScript JDBC SQL Sqoop Git JUnit Eclipse IDE Apache Tomcat 6 UDF Education Bachelors Electronics Engineering DJSanghvi College of Engineering Skills CASSANDRA HDFS MAPREDUCE OOZIE SQOOP HBASE KAFKA SOLR DB2 FLUME JBOSS JMS MONGODB NOSQL APPLICATION SERVER CC C HBase Hive HTML Additional Information SKILLS Big Data Technology HDFS Mapreduce HBase Pig Hive SOLR Sqoop Flume MongoDB Cassandra Puppet Oozie Zookeeper Spark Kafka JavaJ2EE Technology JSP JSF Servlets EJB JDBC Struts Spring Spring MVC Spring Portlet Spring Web Flow Hibernate iBATIS JMS MQ JCA JNDI Java Beans JAXRPC JAXWS RMI RMIIIOP EAD4J Axis Castor SOAP WSDL UDDI JiBX JAXB DOM SAX MyFacesTomahawk Facelets JPA Portal Portlet JSR 168286 LifeRay WebLogic Portal LDAP JUnitNET Languages Java 1456 CC Swing SQL HTML CSS i18n l10n DHTML XML XSD XHTML XSL XSLT XPath XQuery SQL PLSQL UML JavaScript AJAXDWR jQuery Dojo ExtJS Shell Scripts Perl Development FrameworkIDE RAD 8x7x60 IBM WebSphere Integration Developer 61 WSAD 5x Eclipse GalileoEuropa3x2x MyEclipse 3x2x NetBeans 7x6x IntelliJ 7x Workshop 8161 Adobe Photoshop Adobe Dreamweaver Adobe Flash Ant Maven Rational Rose RSA MS Visio OpenMake Meister WebApplication Servers WebSphere Application Server 8x70615150 WebSphere Portal Server 7061 WebSphere Process Server 61 WebLogic Application Server 8161 JBoss 5x3x Apache 2x Tomcat 7x6x5x4x MS IIS IBM HTTP Server Databases NoSQL Oracle 11g10g9i8i DB2 9x8x MS SQL Server 200820052000 MySQL NoSQL HBase Cassandra MongoDB Accumulo Operating Systems Windows XP 2K MSDOS Linux Red Hat Unix Solaris HP UX IBM AIX Version Control CVS SourceSafe ClearCase Subversion AllFusion Harvest Change Manager 71 Monitoring Tools Embarcadero J Optimizer 2009 TPTP IBM Heap Analyzer Wily Introscope JMeter Other JBoss Drools 4x REST IBM Lotus WCM MS ISACA SiteMinder BMC WAM Mingle",
    "unique_id": "cbdebd36-0230-4909-a458-9b693820d424"
}