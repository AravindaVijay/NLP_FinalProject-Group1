{
    "clean_data": "SparkHadoop Developer SparkHadoop span lDeveloperspan SparkHadoop Developer DXC Technology Spark developer with 5 years of experience in Big data application development through frameworks Hadoop Spark Hive Sqoop Flume Oozie Kafka Hands on experience with HadoopSpark Distribution Cloudera Hortonworks Experience in implementing Spark with the integration of Hadoop Ecosystem Experience in data cleansing using Spark map and Filter Functions Experience in designing and developing application in Spark using Scala Experience migrating map reduce programs into Spark RDD transformations or actions to improve performance Experience in creating Hive Tables and loading the data from different file formats Implemented Partitioning Dynamic Partition Buckets in HIVE Experience developing and Debugging Hive queries Experience in processing the data using HiveQL and Pig Latin scripts for data Analytics Extending Hive Core functionality by writing UDFs for Data Analysis Experience converting HiveQLSQL queries into Spark transformations through Spark RDD and Data frames API in Scala Used Oozie to Manage and schedule Spark Jobs on a Hadoop Cluster Used HUE GUI to implement Oozie scheduler and workflows Good Experience in Data importing and exporting to Hive and HDFS with Sqoop Experience in using Producer and Consumer APIs of Apache Kafka Skilled in integrating Kafka with Spark streaming for faster data processing Experience in using Spark Streaming programming model for Realtime data processing Experience dealing with the file formats like text files Sequence files JSON Parquet ORC Extensively used Apache Kafka to collect the logs and error messages across the cluster Excellent knowledge and understanding of Distributed Computing and Parallel processing frameworks Experienced at performing read and write operations on HDFS file system Experience working with large data sets and making performance improvements Experience working with EC2 Elastic Compute Cloud cluster instances setup data buckets on S3 Simple Storage Service setting up EMR Elastic MapReduce Extensive programming knowledge in developing Java application using Java J2EE and JDBC Good experience working on Tableau and enabled the JDBCODBC data connectivity from those to Hive tables Experience creating and driving large scale ETL pipelines Good with version control systems like GIT Strong knowledge on UNIXLINUX commands Adequate Knowledge on Python scripting Language Adequate knowledge of Scrum Agile and Waterfall methodologies Highly motivated and committed to the highest levels of professionalism Exhibited strong written and oral communication skills Rapidly learn and adapt quickly to emerging new technologies and paradigms Work Experience SparkHadoop Developer DXC Technology Plano TX January 2019 to Present The main goal of this project Product Recommendation was get data from different databases in realtime into HDFS and redesigning the imported data into useable format by cleansing and performing transformations on the data to provide an aggregated overview of the trials to be accessed by the clients And store the data in hive for further processing by data scientists Responsibilities Worked under the Cloudera distribution CDH 513 version Involved in working with Sqoop for fetching the data from RDBMS Transformed and stored the ingested data into Data frames using spark SQL Created Hive tables to load the transformed Data Performed partitions and bucketing in hive for easy data classification Worked on Performance and Tuning optimization of Hive Involved in exporting Spark SQL Data frames into hive tables stored as Parquet Files Involved in Ingesting realtime log data from various producers using Kafka Used spark streaming to subscribe to desired topics for real time processing Transformed the DStreams into Data frames using spark engine Experienced in performance tuning of Spark Application for setting right Batch Interval time level of Parallelism and memory tuning for optimal Efficiency Responsible for performing sort join aggregations filter and other transformations on the data Appended the Data frames to preexisting data in hive Performed analysis on the hive tables based on the business logic Created a data pipeline using Oozie workflows which performs jobs on a daily basis Involved in Analyzing data by writing queries in HiveQL for faster data processing Involved in Persisting Metadata into HDFS for further data processing Loading data from Linux File systems to HDFS and viceversa using shell commands Used GIT as Version Control System Worked with Jenkins for continuous integration Build hive tables on the transformed data and used different SERDES to store data in HSFS in different formats Used different APIs to perform necessary transformation and actions on the data which gets from kafka in real time Involved in collecting and transferring the data from various web servers to HDFS using Apache Kafka Environment CDH 51 HDFS Hadoop 30 Spark 24 Scala Hive 30 Pig Hue Oozie Sqoop Kafka Linux shell Git Jenkins Agile SparkHadoop Developer Vanguard Charlotte NC February 2018 to December 2018 The main goal of this project Predict Stock Prices was to move market data and News data from RDBMS to Hive and perform data analysis using spark SQL and store output to hive for use by the RD team and setup Kafka for incremental loading for new data from sensors to be appended directly to the respective Hive tables Responsibilities Worked under the Hortonworks Enterprise Worked on large sets of structured and semistructured historical data Involved in working with Sqoop to import the data from RDBMS to Hive Created Hive tables to load the Data and stored as ORC files for processing Implemented Hive Partitioning and bucketing for further classification of data Worked on Performance and Tuning optimization of Hive Involved in cleansing and transforming the data Used spark SQL to perform sort join and filter the data Copied the ORC files to amazon s3 buckets using Sqoop for further processing in amazon EMR Wrote custom UDFs in Spark SQL using Scala Performed data Aggregation operations using Spark SQL queries Copied output data back to Hive from Amazon S3 buckets using Sqoop after getting the output desired by the business Setup Kafka to subscribe to topicssensors and load data directly to Hive table Automated filter and join operations to join new data with the respective Hive tables using Oozie workflows daily Used Oozie and Oozie coordinators to deploy end to end data processing pipelines and scheduling workflows Compared the sensor data to a persisted table on a 24hr period to check if the machine is operating at optimal conditions and Used Kafka as a messaging system to notify the producer of that data and the maintenance department in case a maintenance is required Used Git as Version Control System Worked with Jenkins for continuous integration Environment HDP 25 HDFS Hadoop 27 Spark 21 Kafka Amazon S3 EMR Sqoop Oozie Hive 21 Pig Hue Linux shell Git Jenkins Agile Hadoop Developer MITS Hyderabad Telangana July 2014 to June 2017 The primary objective of this project was focused on creating a Hive metastore and moving data from RDBMS to the Hadoop environment and writes Map Reduce jobs to process the data previously cleaned transformed and loaded into Hive to output results requested as input by the BI teams for further ETL processes Responsibilities Worked under the Cloudera distribution Responsible for building scalable distributed data solutions using Hadoop Developed Simple to complex Map Reduce jobs Created and populated bucketed tables in Hive to allow for faster map side joins and for more efficient jobs and more efficient sampling Also performed partitioning of data to optimize Hive queries Handled importing of data from Oracle 11g to Hive tables using Sqoop on a regular basis later performed join operations on the data in the Hive Develop User defined functions in Hive to work on multiple input rows and provide an aggregated result based on the business requirement Wrote user defined custom counters to add to the Map Reduce job to gain further insight and for debugging purposes Developed a Map Reduce job to perform lookups of all entries based on a given key from a collection of Map files that were created from the data Performed side data distribution using the distributed cache to make read only data available to the job to process the main dataset Used Combine File Input Format to make sure maps had sufficient data to process when there is a large number of small files Also packaged a collection of small files into a Sequence File which was used as input to the Map Reduce job Implemented LZO compression of Map output to reduce IO between mapper and reducer nodes Continuous monitoring and managing the Hadoop cluster using Web console Developed Pig Latin scripts to extract the data from the output files to load into HDFS Installed Oozie workflow engine to run multiple Hive and Pig jobs Environment CDH 50 HDFS Hadoop 27 Map Reduce spark 16 Hive 12 Pig Hue Oozie Sqoop Scala Oracle 12c YARN Linux shell GIT Jenkins Agile Python Developer MITS Hyderabad Telangana June 2013 to June 2014 Responsibilities Experienced with Python frameworks like WPebapp2 and Flask Experienced in WAMP Windows Apache MYSQL and Python PHP and MVC Struts Developed mobile crossbrowser web application Angular JS JavaScript API Successfully migrated the Django database from SQLite to MySQL to PostgreSQL with complete data integrity Used Celery with Rabbit MQ and Flask to create a distributed worker framework Created Automation test framework using Selenium Responsible for design and development of Web Pages using PHP HTML JOOMLA CSS including Ajax controls and XML Developed intranet portal for managing Amazon EC2 servers using Tornado and MongoDB Expertise in developing different web applications implementing the ModelViewController MVC architectures using Full stack frameworks such as Turbo Gears Implemented monitoring and established best practices around using Elastic search Strong experience in building large responsive based REST web application experienced in Cherrypy framework Python Used Test driven approach TDD for developing services required for the application Developed mobile crossbrowser web application Angular JS JavaScript API Environment Python 2730 PLSQL C Redshift XML Agile SCRUM PyUnit MYSQL Apache CSS MySQL DHTML HTML JavaScript Shell Scripts Git Linux Unix and Windows Skills Hdfs Oozie Sqoop Apache kafka Kafka Flume Hadoop Map reduce Apache spark Hadoop Hive Pig Zookeeper Apache Linux Mysql Oracle Scala Tomcat Eclipse Additional Information TECHNICAL SKILLS Big Data Technologies Apache Hadoop Apache Spark Map Reduce Apache Hive Apache Pig Apache Sqoop Apache Kafka Apache Flume Apache Oozie Apache Zookeeper HDFS Databases MySQL Oracle 11g Languages Scala JAVA Operating Systems Mac OS Windows 710 Linux Cent OS Redhat Ubuntu Development Tools Apache Tomcat Eclipse NetBeans IntelliJ",
    "entities": [
        "Installed Oozie",
        "BI",
        "RDBMS Transformed",
        "Tornado",
        "Apache Sqoop",
        "S3 Simple Storage Service",
        "Created Automation",
        "Hadoop Ecosystem",
        "Ajax",
        "HadoopSpark Distribution",
        "Product Recommendation",
        "Hadoop",
        "Rapidly",
        "Spark SQL Data",
        "Automated",
        "TX",
        "Amazon",
        "Flask Experienced",
        "Used Git as Version Control System Worked",
        "Data Performed",
        "Linux File",
        "Developed",
        "NC",
        "Hadoop Spark Hive Sqoop",
        "UNIXLINUX",
        "Oracle 11",
        "Sequence File",
        "Waterfall",
        "Spark Application",
        "RD",
        "Flask",
        "Spark Streaming",
        "ORC",
        "Git Jenkins Agile Hadoop Developer",
        "MVC",
        "Spark",
        "GIT",
        "Scala Performed",
        "Debugging Hive",
        "Filter Functions",
        "Sqoop",
        "HIVE",
        "Created",
        "WAMP Windows Apache",
        "Producer",
        "Copied",
        "Ingesting",
        "Struts Developed",
        "Turbo Gears Implemented",
        "Oozie",
        "Predict Stock Prices",
        "SparkHadoop Developer SparkHadoop",
        "SQL",
        "Hive Involved",
        "Spark RDD",
        "Responsibilities Worked",
        "Data Analysis",
        "News",
        "SQL Created Hive",
        "Hive",
        "HiveQL",
        "Transformed the DStreams",
        "the Hive Develop User",
        "SQLite",
        "ETL",
        "Performed",
        "Spark SQL",
        "Appended the Data",
        "Hive Created Hive",
        "CSS",
        "Data",
        "REST",
        "Version Control System Worked",
        "NetBeans",
        "PHP",
        "TDD",
        "Tableau",
        "lDeveloperspan SparkHadoop Developer DXC Technology Spark",
        "Distributed Computing",
        "JDBCODBC",
        "Copied the ORC"
    ],
    "experience": "Experience in implementing Spark with the integration of Hadoop Ecosystem Experience in data cleansing using Spark map and Filter Functions Experience in designing and developing application in Spark using Scala Experience migrating map reduce programs into Spark RDD transformations or actions to improve performance Experience in creating Hive Tables and loading the data from different file formats Implemented Partitioning Dynamic Partition Buckets in HIVE Experience developing and Debugging Hive queries Experience in processing the data using HiveQL and Pig Latin scripts for data Analytics Extending Hive Core functionality by writing UDFs for Data Analysis Experience converting HiveQLSQL queries into Spark transformations through Spark RDD and Data frames API in Scala Used Oozie to Manage and schedule Spark Jobs on a Hadoop Cluster Used HUE GUI to implement Oozie scheduler and workflows Good Experience in Data importing and exporting to Hive and HDFS with Sqoop Experience in using Producer and Consumer APIs of Apache Kafka Skilled in integrating Kafka with Spark streaming for faster data processing Experience in using Spark Streaming programming model for Realtime data processing Experience dealing with the file formats like text files Sequence files JSON Parquet ORC Extensively used Apache Kafka to collect the logs and error messages across the cluster Excellent knowledge and understanding of Distributed Computing and Parallel processing frameworks Experienced at performing read and write operations on HDFS file system Experience working with large data sets and making performance improvements Experience working with EC2 Elastic Compute Cloud cluster instances setup data buckets on S3 Simple Storage Service setting up EMR Elastic MapReduce Extensive programming knowledge in developing Java application using Java J2EE and JDBC Good experience working on Tableau and enabled the JDBCODBC data connectivity from those to Hive tables Experience creating and driving large scale ETL pipelines Good with version control systems like GIT Strong knowledge on UNIXLINUX commands Adequate Knowledge on Python scripting Language Adequate knowledge of Scrum Agile and Waterfall methodologies Highly motivated and committed to the highest levels of professionalism Exhibited strong written and oral communication skills Rapidly learn and adapt quickly to emerging new technologies and paradigms Work Experience SparkHadoop Developer DXC Technology Plano TX January 2019 to Present The main goal of this project Product Recommendation was get data from different databases in realtime into HDFS and redesigning the imported data into useable format by cleansing and performing transformations on the data to provide an aggregated overview of the trials to be accessed by the clients And store the data in hive for further processing by data scientists Responsibilities Worked under the Cloudera distribution CDH 513 version Involved in working with Sqoop for fetching the data from RDBMS Transformed and stored the ingested data into Data frames using spark SQL Created Hive tables to load the transformed Data Performed partitions and bucketing in hive for easy data classification Worked on Performance and Tuning optimization of Hive Involved in exporting Spark SQL Data frames into hive tables stored as Parquet Files Involved in Ingesting realtime log data from various producers using Kafka Used spark streaming to subscribe to desired topics for real time processing Transformed the DStreams into Data frames using spark engine Experienced in performance tuning of Spark Application for setting right Batch Interval time level of Parallelism and memory tuning for optimal Efficiency Responsible for performing sort join aggregations filter and other transformations on the data Appended the Data frames to preexisting data in hive Performed analysis on the hive tables based on the business logic Created a data pipeline using Oozie workflows which performs jobs on a daily basis Involved in Analyzing data by writing queries in HiveQL for faster data processing Involved in Persisting Metadata into HDFS for further data processing Loading data from Linux File systems to HDFS and viceversa using shell commands Used GIT as Version Control System Worked with Jenkins for continuous integration Build hive tables on the transformed data and used different SERDES to store data in HSFS in different formats Used different APIs to perform necessary transformation and actions on the data which gets from kafka in real time Involved in collecting and transferring the data from various web servers to HDFS using Apache Kafka Environment CDH 51 HDFS Hadoop 30 Spark 24 Scala Hive 30 Pig Hue Oozie Sqoop Kafka Linux shell Git Jenkins Agile SparkHadoop Developer Vanguard Charlotte NC February 2018 to December 2018 The main goal of this project Predict Stock Prices was to move market data and News data from RDBMS to Hive and perform data analysis using spark SQL and store output to hive for use by the RD team and setup Kafka for incremental loading for new data from sensors to be appended directly to the respective Hive tables Responsibilities Worked under the Hortonworks Enterprise Worked on large sets of structured and semistructured historical data Involved in working with Sqoop to import the data from RDBMS to Hive Created Hive tables to load the Data and stored as ORC files for processing Implemented Hive Partitioning and bucketing for further classification of data Worked on Performance and Tuning optimization of Hive Involved in cleansing and transforming the data Used spark SQL to perform sort join and filter the data Copied the ORC files to amazon s3 buckets using Sqoop for further processing in amazon EMR Wrote custom UDFs in Spark SQL using Scala Performed data Aggregation operations using Spark SQL queries Copied output data back to Hive from Amazon S3 buckets using Sqoop after getting the output desired by the business Setup Kafka to subscribe to topicssensors and load data directly to Hive table Automated filter and join operations to join new data with the respective Hive tables using Oozie workflows daily Used Oozie and Oozie coordinators to deploy end to end data processing pipelines and scheduling workflows Compared the sensor data to a persisted table on a 24hr period to check if the machine is operating at optimal conditions and Used Kafka as a messaging system to notify the producer of that data and the maintenance department in case a maintenance is required Used Git as Version Control System Worked with Jenkins for continuous integration Environment HDP 25 HDFS Hadoop 27 Spark 21 Kafka Amazon S3 EMR Sqoop Oozie Hive 21 Pig Hue Linux shell Git Jenkins Agile Hadoop Developer MITS Hyderabad Telangana July 2014 to June 2017 The primary objective of this project was focused on creating a Hive metastore and moving data from RDBMS to the Hadoop environment and writes Map Reduce jobs to process the data previously cleaned transformed and loaded into Hive to output results requested as input by the BI teams for further ETL processes Responsibilities Worked under the Cloudera distribution Responsible for building scalable distributed data solutions using Hadoop Developed Simple to complex Map Reduce jobs Created and populated bucketed tables in Hive to allow for faster map side joins and for more efficient jobs and more efficient sampling Also performed partitioning of data to optimize Hive queries Handled importing of data from Oracle 11 g to Hive tables using Sqoop on a regular basis later performed join operations on the data in the Hive Develop User defined functions in Hive to work on multiple input rows and provide an aggregated result based on the business requirement Wrote user defined custom counters to add to the Map Reduce job to gain further insight and for debugging purposes Developed a Map Reduce job to perform lookups of all entries based on a given key from a collection of Map files that were created from the data Performed side data distribution using the distributed cache to make read only data available to the job to process the main dataset Used Combine File Input Format to make sure maps had sufficient data to process when there is a large number of small files Also packaged a collection of small files into a Sequence File which was used as input to the Map Reduce job Implemented LZO compression of Map output to reduce IO between mapper and reducer nodes Continuous monitoring and managing the Hadoop cluster using Web console Developed Pig Latin scripts to extract the data from the output files to load into HDFS Installed Oozie workflow engine to run multiple Hive and Pig jobs Environment CDH 50 HDFS Hadoop 27 Map Reduce spark 16 Hive 12 Pig Hue Oozie Sqoop Scala Oracle 12c YARN Linux shell GIT Jenkins Agile Python Developer MITS Hyderabad Telangana June 2013 to June 2014 Responsibilities Experienced with Python frameworks like WPebapp2 and Flask Experienced in WAMP Windows Apache MYSQL and Python PHP and MVC Struts Developed mobile crossbrowser web application Angular JS JavaScript API Successfully migrated the Django database from SQLite to MySQL to PostgreSQL with complete data integrity Used Celery with Rabbit MQ and Flask to create a distributed worker framework Created Automation test framework using Selenium Responsible for design and development of Web Pages using PHP HTML JOOMLA CSS including Ajax controls and XML Developed intranet portal for managing Amazon EC2 servers using Tornado and MongoDB Expertise in developing different web applications implementing the ModelViewController MVC architectures using Full stack frameworks such as Turbo Gears Implemented monitoring and established best practices around using Elastic search Strong experience in building large responsive based REST web application experienced in Cherrypy framework Python Used Test driven approach TDD for developing services required for the application Developed mobile crossbrowser web application Angular JS JavaScript API Environment Python 2730 PLSQL C Redshift XML Agile SCRUM PyUnit MYSQL Apache CSS MySQL DHTML HTML JavaScript Shell Scripts Git Linux Unix and Windows Skills Hdfs Oozie Sqoop Apache kafka Kafka Flume Hadoop Map reduce Apache spark Hadoop Hive Pig Zookeeper Apache Linux Mysql Oracle Scala Tomcat Eclipse Additional Information TECHNICAL SKILLS Big Data Technologies Apache Hadoop Apache Spark Map Reduce Apache Hive Apache Pig Apache Sqoop Apache Kafka Apache Flume Apache Oozie Apache Zookeeper HDFS Databases MySQL Oracle 11 g Languages Scala JAVA Operating Systems Mac OS Windows 710 Linux Cent OS Redhat Ubuntu Development Tools Apache Tomcat Eclipse NetBeans IntelliJ",
    "extracted_keywords": [
        "SparkHadoop",
        "Developer",
        "SparkHadoop",
        "span",
        "lDeveloperspan",
        "SparkHadoop",
        "Developer",
        "DXC",
        "Technology",
        "Spark",
        "developer",
        "years",
        "experience",
        "data",
        "application",
        "development",
        "frameworks",
        "Hadoop",
        "Spark",
        "Hive",
        "Sqoop",
        "Flume",
        "Oozie",
        "Kafka",
        "Hands",
        "experience",
        "HadoopSpark",
        "Distribution",
        "Cloudera",
        "Hortonworks",
        "Experience",
        "Spark",
        "integration",
        "Hadoop",
        "Ecosystem",
        "Experience",
        "data",
        "cleansing",
        "Spark",
        "map",
        "Filter",
        "Functions",
        "Experience",
        "application",
        "Spark",
        "Scala",
        "Experience",
        "migrating",
        "map",
        "programs",
        "Spark",
        "RDD",
        "transformations",
        "actions",
        "performance",
        "Experience",
        "Hive",
        "Tables",
        "data",
        "file",
        "formats",
        "Partitioning",
        "Dynamic",
        "Partition",
        "Buckets",
        "HIVE",
        "Experience",
        "Debugging",
        "Hive",
        "Experience",
        "data",
        "HiveQL",
        "Pig",
        "Latin",
        "scripts",
        "data",
        "Analytics",
        "Hive",
        "Core",
        "functionality",
        "UDFs",
        "Data",
        "Analysis",
        "Experience",
        "HiveQLSQL",
        "queries",
        "Spark",
        "transformations",
        "Spark",
        "RDD",
        "Data",
        "API",
        "Scala",
        "Oozie",
        "Spark",
        "Jobs",
        "Hadoop",
        "Cluster",
        "HUE",
        "GUI",
        "Oozie",
        "scheduler",
        "Good",
        "Experience",
        "Data",
        "Hive",
        "HDFS",
        "Sqoop",
        "Experience",
        "Producer",
        "Consumer",
        "APIs",
        "Apache",
        "Kafka",
        "Skilled",
        "Kafka",
        "Spark",
        "streaming",
        "data",
        "Experience",
        "Spark",
        "Streaming",
        "programming",
        "model",
        "Realtime",
        "data",
        "processing",
        "Experience",
        "file",
        "formats",
        "text",
        "files",
        "Sequence",
        "JSON",
        "Parquet",
        "ORC",
        "Apache",
        "Kafka",
        "logs",
        "error",
        "messages",
        "cluster",
        "Excellent",
        "knowledge",
        "understanding",
        "Distributed",
        "Computing",
        "processing",
        "frameworks",
        "read",
        "operations",
        "HDFS",
        "file",
        "system",
        "Experience",
        "data",
        "sets",
        "performance",
        "improvements",
        "Experience",
        "EC2",
        "Elastic",
        "Compute",
        "Cloud",
        "cluster",
        "instances",
        "data",
        "buckets",
        "S3",
        "Simple",
        "Storage",
        "Service",
        "EMR",
        "Elastic",
        "MapReduce",
        "programming",
        "knowledge",
        "Java",
        "application",
        "Java",
        "J2EE",
        "JDBC",
        "Good",
        "experience",
        "Tableau",
        "JDBCODBC",
        "data",
        "connectivity",
        "Hive",
        "tables",
        "Experience",
        "scale",
        "ETL",
        "pipelines",
        "version",
        "control",
        "systems",
        "GIT",
        "Strong",
        "knowledge",
        "UNIXLINUX",
        "Adequate",
        "Knowledge",
        "Python",
        "Language",
        "Adequate",
        "knowledge",
        "Scrum",
        "Agile",
        "Waterfall",
        "methodologies",
        "levels",
        "professionalism",
        "communication",
        "skills",
        "technologies",
        "Work",
        "Experience",
        "SparkHadoop",
        "Developer",
        "DXC",
        "Technology",
        "Plano",
        "TX",
        "January",
        "goal",
        "project",
        "Product",
        "Recommendation",
        "data",
        "databases",
        "realtime",
        "HDFS",
        "data",
        "format",
        "transformations",
        "data",
        "overview",
        "trials",
        "clients",
        "data",
        "hive",
        "processing",
        "data",
        "scientists",
        "Responsibilities",
        "Cloudera",
        "distribution",
        "CDH",
        "version",
        "Sqoop",
        "data",
        "RDBMS",
        "Transformed",
        "data",
        "Data",
        "frames",
        "spark",
        "SQL",
        "Created",
        "Hive",
        "tables",
        "transformed",
        "Data",
        "Performed",
        "partitions",
        "bucketing",
        "hive",
        "data",
        "classification",
        "Performance",
        "optimization",
        "Hive",
        "Spark",
        "SQL",
        "Data",
        "frames",
        "tables",
        "Parquet",
        "Files",
        "log",
        "data",
        "producers",
        "Kafka",
        "spark",
        "streaming",
        "topics",
        "time",
        "Transformed",
        "DStreams",
        "Data",
        "frames",
        "spark",
        "engine",
        "performance",
        "tuning",
        "Spark",
        "Application",
        "Batch",
        "Interval",
        "time",
        "level",
        "Parallelism",
        "memory",
        "tuning",
        "Efficiency",
        "sort",
        "join",
        "aggregations",
        "filter",
        "transformations",
        "data",
        "Data",
        "frames",
        "data",
        "Performed",
        "analysis",
        "tables",
        "business",
        "logic",
        "data",
        "pipeline",
        "Oozie",
        "workflows",
        "jobs",
        "basis",
        "data",
        "queries",
        "HiveQL",
        "data",
        "processing",
        "Persisting",
        "Metadata",
        "HDFS",
        "data",
        "Loading",
        "data",
        "Linux",
        "File",
        "systems",
        "HDFS",
        "viceversa",
        "shell",
        "commands",
        "GIT",
        "Version",
        "Control",
        "System",
        "Jenkins",
        "integration",
        "Build",
        "hive",
        "tables",
        "data",
        "SERDES",
        "data",
        "HSFS",
        "formats",
        "APIs",
        "transformation",
        "actions",
        "data",
        "kafka",
        "time",
        "data",
        "web",
        "servers",
        "HDFS",
        "Apache",
        "Kafka",
        "Environment",
        "CDH",
        "HDFS",
        "Hadoop",
        "Spark",
        "Scala",
        "Hive",
        "Pig",
        "Hue",
        "Oozie",
        "Sqoop",
        "Kafka",
        "Linux",
        "shell",
        "Git",
        "Jenkins",
        "Agile",
        "SparkHadoop",
        "Developer",
        "Vanguard",
        "Charlotte",
        "NC",
        "February",
        "December",
        "goal",
        "project",
        "Predict",
        "Stock",
        "Prices",
        "market",
        "data",
        "News",
        "data",
        "RDBMS",
        "Hive",
        "data",
        "analysis",
        "spark",
        "SQL",
        "store",
        "output",
        "use",
        "RD",
        "team",
        "setup",
        "Kafka",
        "loading",
        "data",
        "sensors",
        "Hive",
        "tables",
        "Responsibilities",
        "Hortonworks",
        "Enterprise",
        "sets",
        "data",
        "Sqoop",
        "data",
        "RDBMS",
        "Hive",
        "Created",
        "Hive",
        "tables",
        "Data",
        "ORC",
        "files",
        "Hive",
        "Partitioning",
        "classification",
        "data",
        "Performance",
        "optimization",
        "Hive",
        "cleansing",
        "data",
        "spark",
        "SQL",
        "data",
        "ORC",
        "files",
        "amazon",
        "s3",
        "buckets",
        "Sqoop",
        "processing",
        "amazon",
        "EMR",
        "custom",
        "UDFs",
        "Spark",
        "SQL",
        "Scala",
        "Performed",
        "data",
        "Aggregation",
        "operations",
        "Spark",
        "SQL",
        "output",
        "data",
        "Hive",
        "Amazon",
        "S3",
        "buckets",
        "Sqoop",
        "output",
        "business",
        "Setup",
        "Kafka",
        "topicssensors",
        "data",
        "Hive",
        "table",
        "Automated",
        "filter",
        "operations",
        "data",
        "Hive",
        "tables",
        "Oozie",
        "workflows",
        "Oozie",
        "Oozie",
        "coordinators",
        "end",
        "data",
        "pipelines",
        "scheduling",
        "workflows",
        "sensor",
        "data",
        "table",
        "period",
        "machine",
        "conditions",
        "Kafka",
        "system",
        "producer",
        "data",
        "maintenance",
        "department",
        "case",
        "maintenance",
        "Used",
        "Git",
        "Version",
        "Control",
        "System",
        "Jenkins",
        "integration",
        "Environment",
        "HDP",
        "HDFS",
        "Hadoop",
        "Spark",
        "Kafka",
        "Amazon",
        "S3",
        "EMR",
        "Sqoop",
        "Oozie",
        "Hive",
        "Pig",
        "Hue",
        "Linux",
        "shell",
        "Git",
        "Jenkins",
        "Agile",
        "Hadoop",
        "Developer",
        "Hyderabad",
        "Telangana",
        "July",
        "June",
        "objective",
        "project",
        "Hive",
        "metastore",
        "data",
        "RDBMS",
        "Hadoop",
        "environment",
        "Map",
        "Reduce",
        "jobs",
        "data",
        "Hive",
        "results",
        "input",
        "BI",
        "teams",
        "ETL",
        "processes",
        "Responsibilities",
        "Cloudera",
        "distribution",
        "data",
        "solutions",
        "Hadoop",
        "Developed",
        "Simple",
        "Map",
        "Reduce",
        "jobs",
        "tables",
        "Hive",
        "map",
        "side",
        "joins",
        "jobs",
        "sampling",
        "partitioning",
        "data",
        "Hive",
        "queries",
        "importing",
        "data",
        "Oracle",
        "g",
        "Hive",
        "tables",
        "Sqoop",
        "basis",
        "join",
        "operations",
        "data",
        "Hive",
        "Develop",
        "User",
        "functions",
        "Hive",
        "input",
        "rows",
        "result",
        "business",
        "requirement",
        "Wrote",
        "user",
        "custom",
        "counters",
        "Map",
        "Reduce",
        "job",
        "insight",
        "purposes",
        "Map",
        "Reduce",
        "job",
        "lookups",
        "entries",
        "key",
        "collection",
        "Map",
        "files",
        "data",
        "side",
        "data",
        "distribution",
        "cache",
        "data",
        "job",
        "dataset",
        "Combine",
        "File",
        "Input",
        "Format",
        "maps",
        "data",
        "number",
        "files",
        "collection",
        "files",
        "Sequence",
        "File",
        "input",
        "Map",
        "Reduce",
        "job",
        "LZO",
        "compression",
        "Map",
        "output",
        "IO",
        "mapper",
        "reducer",
        "nodes",
        "monitoring",
        "Hadoop",
        "cluster",
        "Web",
        "console",
        "Developed",
        "Pig",
        "Latin",
        "scripts",
        "data",
        "output",
        "files",
        "HDFS",
        "Oozie",
        "workflow",
        "engine",
        "Hive",
        "Pig",
        "jobs",
        "Environment",
        "CDH",
        "HDFS",
        "Hadoop",
        "Map",
        "Reduce",
        "spark",
        "Hive",
        "Pig",
        "Hue",
        "Oozie",
        "Sqoop",
        "Scala",
        "Oracle",
        "12c",
        "YARN",
        "Linux",
        "shell",
        "GIT",
        "Jenkins",
        "Agile",
        "Python",
        "Developer",
        "Hyderabad",
        "Telangana",
        "June",
        "June",
        "Responsibilities",
        "Python",
        "frameworks",
        "WPebapp2",
        "Flask",
        "WAMP",
        "Windows",
        "Apache",
        "MYSQL",
        "Python",
        "PHP",
        "MVC",
        "Struts",
        "crossbrowser",
        "web",
        "application",
        "Angular",
        "JS",
        "JavaScript",
        "API",
        "Django",
        "database",
        "SQLite",
        "PostgreSQL",
        "data",
        "integrity",
        "Celery",
        "Rabbit",
        "MQ",
        "Flask",
        "worker",
        "framework",
        "Created",
        "Automation",
        "test",
        "framework",
        "Selenium",
        "Responsible",
        "design",
        "development",
        "Web",
        "Pages",
        "PHP",
        "HTML",
        "JOOMLA",
        "CSS",
        "controls",
        "XML",
        "intranet",
        "portal",
        "Amazon",
        "EC2",
        "servers",
        "Tornado",
        "MongoDB",
        "Expertise",
        "web",
        "applications",
        "ModelViewController",
        "MVC",
        "stack",
        "frameworks",
        "Turbo",
        "Gears",
        "monitoring",
        "practices",
        "search",
        "experience",
        "REST",
        "web",
        "application",
        "Cherrypy",
        "framework",
        "Python",
        "Test",
        "approach",
        "TDD",
        "services",
        "application",
        "crossbrowser",
        "web",
        "application",
        "Angular",
        "JS",
        "JavaScript",
        "API",
        "Environment",
        "Python",
        "PLSQL",
        "C",
        "Redshift",
        "XML",
        "Agile",
        "SCRUM",
        "PyUnit",
        "MYSQL",
        "Apache",
        "CSS",
        "MySQL",
        "DHTML",
        "HTML",
        "JavaScript",
        "Shell",
        "Scripts",
        "Git",
        "Linux",
        "Unix",
        "Windows",
        "Skills",
        "Hdfs",
        "Oozie",
        "Sqoop",
        "Apache",
        "Kafka",
        "Flume",
        "Hadoop",
        "Map",
        "Apache",
        "spark",
        "Hadoop",
        "Hive",
        "Pig",
        "Zookeeper",
        "Apache",
        "Linux",
        "Mysql",
        "Oracle",
        "Scala",
        "Tomcat",
        "Eclipse",
        "Additional",
        "Information",
        "TECHNICAL",
        "SKILLS",
        "Big",
        "Data",
        "Technologies",
        "Apache",
        "Hadoop",
        "Apache",
        "Spark",
        "Map",
        "Reduce",
        "Apache",
        "Hive",
        "Apache",
        "Pig",
        "Apache",
        "Sqoop",
        "Apache",
        "Kafka",
        "Apache",
        "Flume",
        "Apache",
        "Oozie",
        "Apache",
        "Zookeeper",
        "HDFS",
        "MySQL",
        "Oracle",
        "g",
        "Languages",
        "Scala",
        "JAVA",
        "Operating",
        "Systems",
        "Mac",
        "OS",
        "Linux",
        "Cent",
        "OS",
        "Ubuntu",
        "Development",
        "Tools",
        "Apache",
        "Tomcat",
        "Eclipse",
        "NetBeans"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T22:03:10.084711",
    "resume_data": "SparkHadoop Developer SparkHadoop span lDeveloperspan SparkHadoop Developer DXC Technology Spark developer with 5 years of experience in Big data application development through frameworks Hadoop Spark Hive Sqoop Flume Oozie Kafka Hands on experience with HadoopSpark Distribution Cloudera Hortonworks Experience in implementing Spark with the integration of Hadoop Ecosystem Experience in data cleansing using Spark map and Filter Functions Experience in designing and developing application in Spark using Scala Experience migrating map reduce programs into Spark RDD transformations or actions to improve performance Experience in creating Hive Tables and loading the data from different file formats Implemented Partitioning Dynamic Partition Buckets in HIVE Experience developing and Debugging Hive queries Experience in processing the data using HiveQL and Pig Latin scripts for data Analytics Extending Hive Core functionality by writing UDFs for Data Analysis Experience converting HiveQLSQL queries into Spark transformations through Spark RDD and Data frames API in Scala Used Oozie to Manage and schedule Spark Jobs on a Hadoop Cluster Used HUE GUI to implement Oozie scheduler and workflows Good Experience in Data importing and exporting to Hive and HDFS with Sqoop Experience in using Producer and Consumer APIs of Apache Kafka Skilled in integrating Kafka with Spark streaming for faster data processing Experience in using Spark Streaming programming model for Realtime data processing Experience dealing with the file formats like text files Sequence files JSON Parquet ORC Extensively used Apache Kafka to collect the logs and error messages across the cluster Excellent knowledge and understanding of Distributed Computing and Parallel processing frameworks Experienced at performing read and write operations on HDFS file system Experience working with large data sets and making performance improvements Experience working with EC2 Elastic Compute Cloud cluster instances setup data buckets on S3 Simple Storage Service setting up EMR Elastic MapReduce Extensive programming knowledge in developing Java application using Java J2EE and JDBC Good experience working on Tableau and enabled the JDBCODBC data connectivity from those to Hive tables Experience creating and driving large scale ETL pipelines Good with version control systems like GIT Strong knowledge on UNIXLINUX commands Adequate Knowledge on Python scripting Language Adequate knowledge of Scrum Agile and Waterfall methodologies Highly motivated and committed to the highest levels of professionalism Exhibited strong written and oral communication skills Rapidly learn and adapt quickly to emerging new technologies and paradigms Work Experience SparkHadoop Developer DXC Technology Plano TX January 2019 to Present The main goal of this project Product Recommendation was get data from different databases in realtime into HDFS and redesigning the imported data into useable format by cleansing and performing transformations on the data to provide an aggregated overview of the trials to be accessed by the clients And store the data in hive for further processing by data scientists Responsibilities Worked under the Cloudera distribution CDH 513 version Involved in working with Sqoop for fetching the data from RDBMS Transformed and stored the ingested data into Data frames using spark SQL Created Hive tables to load the transformed Data Performed partitions and bucketing in hive for easy data classification Worked on Performance and Tuning optimization of Hive Involved in exporting Spark SQL Data frames into hive tables stored as Parquet Files Involved in Ingesting realtime log data from various producers using Kafka Used spark streaming to subscribe to desired topics for real time processing Transformed the DStreams into Data frames using spark engine Experienced in performance tuning of Spark Application for setting right Batch Interval time level of Parallelism and memory tuning for optimal Efficiency Responsible for performing sort join aggregations filter and other transformations on the data Appended the Data frames to preexisting data in hive Performed analysis on the hive tables based on the business logic Created a data pipeline using Oozie workflows which performs jobs on a daily basis Involved in Analyzing data by writing queries in HiveQL for faster data processing Involved in Persisting Metadata into HDFS for further data processing Loading data from Linux File systems to HDFS and viceversa using shell commands Used GIT as Version Control System Worked with Jenkins for continuous integration Build hive tables on the transformed data and used different SERDES to store data in HSFS in different formats Used different APIs to perform necessary transformation and actions on the data which gets from kafka in real time Involved in collecting and transferring the data from various web servers to HDFS using Apache Kafka Environment CDH 51 HDFS Hadoop 30 Spark 24 Scala Hive 30 Pig Hue Oozie Sqoop Kafka Linux shell Git Jenkins Agile SparkHadoop Developer Vanguard Charlotte NC February 2018 to December 2018 The main goal of this project Predict Stock Prices was to move market data and News data from RDBMS to Hive and perform data analysis using spark SQL and store output to hive for use by the RD team and setup Kafka for incremental loading for new data from sensors to be appended directly to the respective Hive tables Responsibilities Worked under the Hortonworks Enterprise Worked on large sets of structured and semistructured historical data Involved in working with Sqoop to import the data from RDBMS to Hive Created Hive tables to load the Data and stored as ORC files for processing Implemented Hive Partitioning and bucketing for further classification of data Worked on Performance and Tuning optimization of Hive Involved in cleansing and transforming the data Used spark SQL to perform sort join and filter the data Copied the ORC files to amazon s3 buckets using Sqoop for further processing in amazon EMR Wrote custom UDFs in Spark SQL using Scala Performed data Aggregation operations using Spark SQL queries Copied output data back to Hive from Amazon S3 buckets using Sqoop after getting the output desired by the business Setup Kafka to subscribe to topicssensors and load data directly to Hive table Automated filter and join operations to join new data with the respective Hive tables using Oozie workflows daily Used Oozie and Oozie coordinators to deploy end to end data processing pipelines and scheduling workflows Compared the sensor data to a persisted table on a 24hr period to check if the machine is operating at optimal conditions and Used Kafka as a messaging system to notify the producer of that data and the maintenance department in case a maintenance is required Used Git as Version Control System Worked with Jenkins for continuous integration Environment HDP 25 HDFS Hadoop 27 Spark 21 Kafka Amazon S3 EMR Sqoop Oozie Hive 21 Pig Hue Linux shell Git Jenkins Agile Hadoop Developer MITS Hyderabad Telangana July 2014 to June 2017 The primary objective of this project was focused on creating a Hive metastore and moving data from RDBMS to the Hadoop environment and writes Map Reduce jobs to process the data previously cleaned transformed and loaded into Hive to output results requested as input by the BI teams for further ETL processes Responsibilities Worked under the Cloudera distribution Responsible for building scalable distributed data solutions using Hadoop Developed Simple to complex Map Reduce jobs Created and populated bucketed tables in Hive to allow for faster map side joins and for more efficient jobs and more efficient sampling Also performed partitioning of data to optimize Hive queries Handled importing of data from Oracle 11g to Hive tables using Sqoop on a regular basis later performed join operations on the data in the Hive Develop User defined functions in Hive to work on multiple input rows and provide an aggregated result based on the business requirement Wrote user defined custom counters to add to the Map Reduce job to gain further insight and for debugging purposes Developed a Map Reduce job to perform lookups of all entries based on a given key from a collection of Map files that were created from the data Performed side data distribution using the distributed cache to make read only data available to the job to process the main dataset Used Combine File Input Format to make sure maps had sufficient data to process when there is a large number of small files Also packaged a collection of small files into a Sequence File which was used as input to the Map Reduce job Implemented LZO compression of Map output to reduce IO between mapper and reducer nodes Continuous monitoring and managing the Hadoop cluster using Web console Developed Pig Latin scripts to extract the data from the output files to load into HDFS Installed Oozie workflow engine to run multiple Hive and Pig jobs Environment CDH 50 HDFS Hadoop 27 Map Reduce spark 16 Hive 12 Pig Hue Oozie Sqoop Scala Oracle 12c YARN Linux shell GIT Jenkins Agile Python Developer MITS Hyderabad Telangana June 2013 to June 2014 Responsibilities Experienced with Python frameworks like WPebapp2 and Flask Experienced in WAMP Windows Apache MYSQL and Python PHP and MVC Struts Developed mobile crossbrowser web application Angular JS JavaScript API Successfully migrated the Django database from SQLite to MySQL to PostgreSQL with complete data integrity Used Celery with Rabbit MQ and Flask to create a distributed worker framework Created Automation test framework using Selenium Responsible for design and development of Web Pages using PHP HTML JOOMLA CSS including Ajax controls and XML Developed intranet portal for managing Amazon EC2 servers using Tornado and MongoDB Expertise in developing different web applications implementing the ModelViewController MVC architectures using Full stack frameworks such as Turbo Gears Implemented monitoring and established best practices around using Elastic search Strong experience in building large responsive based REST web application experienced in Cherrypy framework Python Used Test driven approach TDD for developing services required for the application Developed mobile crossbrowser web application Angular JS JavaScript API Environment Python 2730 PLSQL C Redshift XML Agile SCRUM PyUnit MYSQL Apache CSS MySQL DHTML HTML JavaScript Shell Scripts Git Linux Unix and Windows Skills Hdfs Oozie Sqoop Apache kafka Kafka Flume Hadoop Map reduce Apache spark Hadoop Hive Pig Zookeeper Apache Linux Mysql Oracle Scala Tomcat Eclipse Additional Information TECHNICAL SKILLS Big Data Technologies Apache Hadoop Apache Spark Map Reduce Apache Hive Apache Pig Apache Sqoop Apache Kafka Apache Flume Apache Oozie Apache Zookeeper HDFS Databases MySQL Oracle 11g Languages Scala JAVA Operating Systems Mac OS Windows 710 Linux Cent OS Redhat Ubuntu Development Tools Apache Tomcat Eclipse NetBeans IntelliJ",
    "unique_id": "f763af0a-367d-4923-9aaa-40d16512109f"
}