{
    "clean_data": "HadoopSpark Developer HadoopSpark span lDeveloperspan HadoopSpark Developer Edward Jones Boston MA Around 8 years of professional experience in Information Technology which includes 5 years in Big Data and Hadoop Ecosystem Experience in working with BI team and transform big data requirements into Hadoop centric technologies Expert in creating indexes views complex stored procedures userdefined functions cursors derived tables common table expressions CTEs and triggers to facilitate efficient data manipulation and data consistency Excellent understandingknowledge of Hadoop architecture and various components of Big Data Hands on experience in installing configuring and using Hadoop ecosystem components like MapReduce HDFS HBase Oozie Hive Sqoop Pig and Zookeeper Experience in analyzing data using HQL Pig Latin and custom MapReduce programs in Java Knowledge on various file formats such as Avro Parquet ORC etc and on various compression codecs such as GZip Snappy LZO etc Strong competency in HIVE schema design partitions and bucketing Experience in ingestion storage querying processing and analysis of Big Data with handson experience in Apache Spark and Spark Streaming Owned the design development and maintenance of ongoing metrics reports analyses dashboards etc to drive key business decisions and communicate key concepts to readers Expertise in designing and developing a distributed processing system running into a Data Warehousing platform for reporting Data cleaning preprocessing and modelling using Spark and Python Experience in performing ETL on top of streaming log data from various web servers into HDFS using Flume Performed data analytics and insights using Impala and Hive Expert in developing and scheduling jobs using Oozie and Crontab Handson Git Agile Scrum JIRA and Confluence Authorized to work in the US for any employer Work Experience HadoopSpark Developer Edward Jones Boston MA March 2019 to Present Responsibilities Developed Spark jobs written in Scala to perform operations like data aggregation data processing and data analysis Load the data into Spark RDD and performed inmemory data computation to generate the output response Developed Hive UDFs for extended use and wrote HiveQL for sorting joining filtering and grouping the structure data Used Spark for series of dependent jobs and for iterative algorithms Developed a data pipeline using Kafka and Spark Streaming to store data into HDFS Developed workflow in Oozie to automate the tasks of loading the data into HDFS Developed data pipeline using Flume Sqoop Pig and Java MapReduce to ingest customer behavioral data and financial histories into HDFS for analysis Developed ETL Applications using Hive Spark and Impala Sqoop for Automation using Oozie Used Pig as ETL tool to do transformations event joins and some preaggregations before storing the data onto HDFS Used Spark for series of dependent jobs and for iterative algorithms Developed a data pipeline using Kafka and Spark Streaming to store data into HDFS Hadoop DeveloperEngineer Navy Federal Credit Union Vienna VA September 2018 to March 2019 Responsibilities Automated the process for extraction of data from warehouses and weblogs by developing workflows and coordinator jobs in Oozie Assisted in upgrading configuration and maintenance of various Hadoop infrastructures like Pig Hive and HBase Developed Spark jobs written in Scala to perform operations like data aggregation data processing and data analysis Load the data into Spark RDD and performed inmemory data computation to generate the output response Implemented various MapReduce Jobs in custom environments and updating them to HBase tables by generating hive queries Responsible for Cluster maintenance adding and removing cluster nodes Cluster Monitoring and Troubleshooting manage and review data backups and log files Developed workflow in Oozie to manage and schedule jobs on Hadoop cluster to trigger daily weekly and monthly batch cycles Used Spark for series of dependent jobs and for iterative algorithms Developed a data pipeline using Kafka and Spark Streaming to store data into HDFS Performance Tuning for Hive and Pig Jobs performance parameters along with native MapReduce parameters to avoid excessive disk spills enabled temp file compression between jobs in the data pipeline to handle production size data in a multitenant cluster environment Developed data pipeline using Flume Sqoop Pig and Java MapReduce to ingest customer behavioral data and financial histories into HDFS for analysis Environment Hadoop HDFS Flume CDH Pig Hive Oozie Zookeeper HBase Spark Storm Spark SQL NoSQL Scala Kafka MongoDB Linux Sqoop Hadoop Developer Bank of America Charlotte NC September 2017 to September 2018 Responsibilities Worked on implementation and data integration in developing largescale system software experiencing with Hadoop ecosystem components like HBase Sqoop Zookeeper Oozie Hive and Pig Developed Hive UDFs for extended use and wrote HiveQL for sorting joining filtering and grouping the structure data Developed ETL Applications using Hive Spark and Impala Sqoop for Automation using Oozie Used Pig as ETL tool to do transformations event joins and some preaggregations before storing the data onto HDFS Automated the process for extraction of data from warehouses and weblogs by developing workflows and coordinator jobs in Oozie Developed workflow in Oozie to automate the tasks of loading the data into HDFS Creating Hive tables dynamic partitions buckets for sampling and working on them using HiveQL Used Sqoop for importing the data into HBase and Hive exporting result set from Hive to MySQL using Sqoop export tool for further processing Enumerated Hive queries to do analysis of the data and to generate the end reports to be used by business users Worked on scalable distributed computing systems software architecture data structures and algorithms using Hadoop Apache Spark and Apache Storm etc and ingested streaming data into Hadoop using Spark Storm Framework and Scala Implemented POCs with Spark SQL to interpret complex JSON records Experience in transferring Streaming data data from different data sources into HDFS and NoSQL databases using Apache Flume Developed Spark jobs written in Scala to perform operations like data aggregation data processing and data analysis Used Kafka Flume for building robust and fault tolerant data Ingestion pipeline between JMS and Spark Streaming Applications for transporting streaming web log data into HDFS Used Spark for series of dependent jobs and for iterative algorithms Developed a data pipeline using Kafka and Spark Streaming to store data into HDFS Environment Hadoop HDFS Flume CDH Pig Hive Oozie Zookeeper HBase Spark Storm Spark SQL NoSQL Scala Kafka MongoDB Big DataHadoop Developer Capital One Dallas TX August 2016 to September 2017 Responsibilities Developed Scala UDFs using both Data framesSQL and RDDMapReduce in Spark for Data Aggregation queries and writing data back into RDBMS through Sqoop Exported analyzed data to relational databases using Sqoop in deploying data from various sources into HDFS and building reports using Tableau Exported analyzed data to relational databases using Sqoop for visualization to generate reports for the BI team Used the JSON and Avro for serialization and deserialization packaged with Hive to parse the contents of streamed log data and implemented Hive custom UDFs Configured Spark streaming to receive real time data from Kafka and store the stream data to HDFS using Scala Involved in developing ETL data pipelines for performing realtime streaming by ingesting data into HDFS and HBase using Kafka and Storm Involved in moving log files generated from varied sources to HDFS further processing through Flume Involved in creating Hive tables by using Impala and working on them using HiveQL and perform data analysis using Hive and Pig Developed workflow in Oozie to manage and schedule jobs on Hadoop cluster to trigger daily weekly and monthly batch cycles Assisted in upgrading configuration and maintenance of various Hadoop infrastructures like Pig Hive and HBase Worked on Apache Flume for collecting and aggregating huge amount of log data and stored it on HDFS for doing further analysis Load the data into Spark RDD and performed inmemory data computation to generate the output response Efficiently put and fetched data tofrom HBase by writing MapReduce job Environment Hadoop Flume Kafka Spark Sqoop Spark SQL SparkStreaming Hive Scala pig NoSQL Impala Oozie HBase Zookeeper Hadoop Consultant ATT Dallas TX June 2015 to August 2016 Responsibilities Worked on loading the customers data and event logs from Kafka into HBase using REST API Responsible for Cluster maintenance adding and removing cluster nodes Cluster Monitoring and Troubleshooting manage and review data backups and log files Implemented various MapReduce Jobs in custom environments and updating them to HBase tables by generating hive queries Developed data pipeline using Flume Sqoop Pig and Java MapReduce to ingest customer behavioral data and financial histories into HDFS for analysis Collecting and aggregating large amounts of log data using Apache Flume and staging data in HDFS for further analysis Created Hive tables from JSON data using data serialization framework using Avro Parquet File formats and Snappy compression Implemented generic export framework for moving data from HDFS to RDBMS and viceversa Worked on analyzing Hadoop cluster using different big data analytic tools including Kafka Pig Hive and Map Reduce MR1 and MR2 Automated all the jobs for pulling data from FTP server to load data into Hive tables using Oozie workflows Involved using HCATALOG to access Hive table metadata from Pig code Implemented SQL PLSQL Stored Procedures Actively involved in code review and bug fixing for improving the performance Worked on tuning the performance Pig queries and involved in loading data from LINUX file system to HDFS Importing and exporting data into HDFS using Sqoop and Kafka Created HBase tables to store various data formats of PII data coming from different portfolios Implemented Mapreduce for loading data from Oracle database Used NoSQL database with HBase and MongoDB Exported the result set from Hive to MySQL using Shell scripts Gained experience in managing and reviewing Hadoop log files Involved in scheduling Oozie workflow engine to run multiple Pig jobs Environment Hadoop HDFS HBase Pig Hive Spark Hortonworks Oozie MapReduce Sqoop MongoDB Kafka LINUX Java APIs Java collection Hadoop Developer Sprint Overland Park KS August 2013 to April 2015 Responsibilities Worked extensively on Hadoop Components such as HDFS Job Tracker Task Tracker Name Node Data Node YARN and MapReduce programming Involved in loading data from UNIX file system to HDFS Imported and exported data into HDFS and Hive using Sqoop Experience in developing batch processing framework to ingest data into HDFS Hive and Cassandra Worked on Hive and Pig extensively to analyze network data in collecting metrics for Hadoop clusters Automation of data pulls from SQL Server to Hadoop for analyzing large amounts of data sets to determine optimal way to aggregate and report on it Provided quick response for client requests and experienced in creating ad hoc reports Performance Tuning for Hive and Pig Jobs performance parameters along with native MapReduce parameters to avoid excessive disk spills enabled temp file compression between jobs in the data pipeline to handle production size data in a multitenant cluster environment Designed workflows and coordinators in Oozie to automate and parallelize Hive and Pig jobs on Apache Hadoop environment by Hortonworks Developed a process for the Batch ingestion of CSV Files Sqoop from different sources and also generating views on the data source using Shell Scripting and Python Delivered Hadoop migration strategy roadmap and technology fitment for importing real time network log data into HDFS POCs on moving existing Hive Pig Latin jobs to Spark for Deploying and configuring agents to stream log events into HDFS for analysis Load the data into Hive tables using HiveQL along with Deduplication and Windowing to generate adhoc reports using Hive to validate customer viewing history and debug issues in production Experienced with multiple Input Formats such as Text File Key Value Sequence File input format load to HDFS Worked on business specific custom UDFs in Hive and Pig for data extraction transformation and aggregation from multiple file formats including XML JSON CSV and other compressed file formats Environment HDFS MapReduce Pig Hive Oozie Sqoop Cassandra Hortonworks Java Developer Blue Cross Blue Shield Detroit MI February 2012 to July 2013 Responsibilities Experience using middleware architecture using Java technologies like J2EE Servlets and application servers like Web Sphere and Web logic Worked on loading data from Linux file system to HDFS Understanding and analyzing the requirements Designed developed and validated User Interface using HTML Java Script and XML Involved with writing SQL queries using Joins and Stored Procedures using Maven to build and deploy the applications in JBoss application Server in Software Development Lifecycle Model Worked on Eclipse IDE for front end development environment for insertions updating and retrieval operations of data from oracle database by writing stored procedures Developed MapReduce jobs to convert data files into Parquet file format and included MRUnit to test the correctness of MapReduce programs Experienced in working with various kinds of datasets for structured semi structured and unstructured data with Teradata and Oracle for successfully loading files to HDFS from Teradata and loaded from HDFS to Hive Installed Oozie workflow engine to run multiple Hive Developed Hive queries to process the data and generate the data cubes for visualizing Concatenated ETL logics from RDBMS to Hive Implemented partitioning bucketing and worked on Hive using file formats and compressions techniques with optimizations Computed various metrics using Map Reduce to calculate metrics that define user experience revenue etc Environment Hadoop HDFS Pig Oozie Hive Python MapReduce Java SQL Scripting and Linux Shell Scripting Cloudera Cloudera Manager Software Developer Amazon Hyderabad Telangana September 2010 to November 2011 Responsibilities Worked on Business logic for web service using spring annotations which enables dependency injection Developed Spring Application Framework for Dependency Injection support for the Data Access Object DAO pattern and integrated with Hibernate ORM Developed user interface for designing and developing the application using Java JEE and spring core using JSP JavaScript Ajax jQuery HTML CSS and JSTL Outlining agile methodology with daily scrums using TDD and continuous integration in the SDLC process and used JIRA for bug tracking and task management Developed Talend jobs to populate the claims data to data warehouse star schema Used Jenkins for continuous integration purpose in using SVN Junit and Mockito as version control and Unit testing by Creating design documents and test cases for development work Environment Java Servlets JSP HTML CSS Talend Ajax JavaScript Hibernate Spring WebLogic JMS REST SVN Education Bachelor of Technology in Computer Science and Engineering in Computer Science and Engineering Gokaraju Rangaraju Institute of Engineering and Technology Hyderabad Telangana Skills Cassandra Hdfs Impala Mapreduce Oozie Military Service Branch United States Navy Rank second Additional Information Technical Skills Languages Java Python Scala HiveQL Hadoop Ecosystem HDFS Hive MapReduce HBase YARN Sqoop Flume Oozie Zookeeper Impala Databases Oracle RDBMS DB2 SQL Server MySQL NoSQL Databases HBase MongoDB Cassandra Scripting Languages JavaScript CSS Python Perl Shell Script",
    "entities": [
        "Zookeeper Experience",
        "HDFS Developed",
        "Tableau Exported",
        "BI",
        "Information Technology",
        "HDFS",
        "UNIX",
        "MapReduce HDFS HBase Oozie Hive Sqoop Pig",
        "Detroit",
        "Oozie Assisted",
        "Flume Sqoop Pig",
        "Flume Performed",
        "Computed",
        "Developed Spring Application Framework for Dependency Injection",
        "Hadoop",
        "second Additional Information Technical Skills",
        "Linux Sqoop Hadoop Developer Bank of America Charlotte",
        "MapReduce programming Involved",
        "Telangana",
        "Shell",
        "HBase",
        "Apache Spark",
        "TX",
        "Avro Parquet File",
        "SQL Server",
        "Assisted",
        "Deduplication and Windowing",
        "Developed",
        "Dallas",
        "Spark Storm Framework",
        "NC",
        "Storm Involved",
        "Mockito",
        "Flume Involved",
        "Hadoop Developer Sprint",
        "Text File Key Value Sequence File",
        "the Data Access Object DAO",
        "Used Spark",
        "Spark for Data Aggregation",
        "Big Data Hands",
        "HDFS Worked",
        "HDFS Used Spark",
        "SVN Junit",
        "Oozie Developed",
        "Linux",
        "JSP",
        "Environment Hadoop HDFS Pig Oozie Hive Python",
        "SDLC",
        "Shell Scripting",
        "JBoss",
        "Parquet",
        "Spark Streaming Owned",
        "MapReduce Jobs",
        "Spark",
        "Created Hive",
        "Amazon Hyderabad",
        "US",
        "Sqoop",
        "Scala Involved",
        "HIVE",
        "LINUX",
        "HDFS Environment Hadoop HDFS Flume CDH Pig Hive Oozie Zookeeper HBase Spark",
        "Server",
        "Oracle",
        "HBase Developed Spark",
        "log data",
        "HDFS Job Tracker Task Tracker",
        "Oozie",
        "SQL",
        "Spark RDD",
        "Spark for Deploying",
        "Hive Installed Oozie",
        "Hadoop Apache Spark",
        "Hortonworks Developed",
        "HDFS Hadoop DeveloperEngineer Navy",
        "Big Data",
        "Hive",
        "GZip Snappy LZO",
        "Work Experience HadoopSpark Developer",
        "FTP",
        "HBase Worked",
        "ETL",
        "Hibernate ORM Developed",
        "Present Responsibilities Developed Spark",
        "PII",
        "Apache Hadoop",
        "Maven",
        "Impala",
        "Spark SQL",
        "Scala HiveQL Hadoop Ecosystem HDFS Hive MapReduce HBase",
        "HadoopSpark Developer HadoopSpark",
        "Input Formats",
        "Hadoop Components",
        "J2EE Servlets",
        "ATT",
        "Implemented Mapreduce",
        "Boston",
        "Data",
        "MapReduce",
        "REST API Responsible",
        "Impala Sqoop for Automation",
        "Data Warehousing",
        "NoSQL",
        "TDD",
        "Teradata",
        "Hive Implemented",
        "Node"
    ],
    "experience": "Experience in working with BI team and transform big data requirements into Hadoop centric technologies Expert in creating indexes views complex stored procedures userdefined functions cursors derived tables common table expressions CTEs and triggers to facilitate efficient data manipulation and data consistency Excellent understandingknowledge of Hadoop architecture and various components of Big Data Hands on experience in installing configuring and using Hadoop ecosystem components like MapReduce HDFS HBase Oozie Hive Sqoop Pig and Zookeeper Experience in analyzing data using HQL Pig Latin and custom MapReduce programs in Java Knowledge on various file formats such as Avro Parquet ORC etc and on various compression codecs such as GZip Snappy LZO etc Strong competency in HIVE schema design partitions and bucketing Experience in ingestion storage querying processing and analysis of Big Data with handson experience in Apache Spark and Spark Streaming Owned the design development and maintenance of ongoing metrics reports analyses dashboards etc to drive key business decisions and communicate key concepts to readers Expertise in designing and developing a distributed processing system running into a Data Warehousing platform for reporting Data cleaning preprocessing and modelling using Spark and Python Experience in performing ETL on top of streaming log data from various web servers into HDFS using Flume Performed data analytics and insights using Impala and Hive Expert in developing and scheduling jobs using Oozie and Crontab Handson Git Agile Scrum JIRA and Confluence Authorized to work in the US for any employer Work Experience HadoopSpark Developer Edward Jones Boston MA March 2019 to Present Responsibilities Developed Spark jobs written in Scala to perform operations like data aggregation data processing and data analysis Load the data into Spark RDD and performed inmemory data computation to generate the output response Developed Hive UDFs for extended use and wrote HiveQL for sorting joining filtering and grouping the structure data Used Spark for series of dependent jobs and for iterative algorithms Developed a data pipeline using Kafka and Spark Streaming to store data into HDFS Developed workflow in Oozie to automate the tasks of loading the data into HDFS Developed data pipeline using Flume Sqoop Pig and Java MapReduce to ingest customer behavioral data and financial histories into HDFS for analysis Developed ETL Applications using Hive Spark and Impala Sqoop for Automation using Oozie Used Pig as ETL tool to do transformations event joins and some preaggregations before storing the data onto HDFS Used Spark for series of dependent jobs and for iterative algorithms Developed a data pipeline using Kafka and Spark Streaming to store data into HDFS Hadoop DeveloperEngineer Navy Federal Credit Union Vienna VA September 2018 to March 2019 Responsibilities Automated the process for extraction of data from warehouses and weblogs by developing workflows and coordinator jobs in Oozie Assisted in upgrading configuration and maintenance of various Hadoop infrastructures like Pig Hive and HBase Developed Spark jobs written in Scala to perform operations like data aggregation data processing and data analysis Load the data into Spark RDD and performed inmemory data computation to generate the output response Implemented various MapReduce Jobs in custom environments and updating them to HBase tables by generating hive queries Responsible for Cluster maintenance adding and removing cluster nodes Cluster Monitoring and Troubleshooting manage and review data backups and log files Developed workflow in Oozie to manage and schedule jobs on Hadoop cluster to trigger daily weekly and monthly batch cycles Used Spark for series of dependent jobs and for iterative algorithms Developed a data pipeline using Kafka and Spark Streaming to store data into HDFS Performance Tuning for Hive and Pig Jobs performance parameters along with native MapReduce parameters to avoid excessive disk spills enabled temp file compression between jobs in the data pipeline to handle production size data in a multitenant cluster environment Developed data pipeline using Flume Sqoop Pig and Java MapReduce to ingest customer behavioral data and financial histories into HDFS for analysis Environment Hadoop HDFS Flume CDH Pig Hive Oozie Zookeeper HBase Spark Storm Spark SQL NoSQL Scala Kafka MongoDB Linux Sqoop Hadoop Developer Bank of America Charlotte NC September 2017 to September 2018 Responsibilities Worked on implementation and data integration in developing largescale system software experiencing with Hadoop ecosystem components like HBase Sqoop Zookeeper Oozie Hive and Pig Developed Hive UDFs for extended use and wrote HiveQL for sorting joining filtering and grouping the structure data Developed ETL Applications using Hive Spark and Impala Sqoop for Automation using Oozie Used Pig as ETL tool to do transformations event joins and some preaggregations before storing the data onto HDFS Automated the process for extraction of data from warehouses and weblogs by developing workflows and coordinator jobs in Oozie Developed workflow in Oozie to automate the tasks of loading the data into HDFS Creating Hive tables dynamic partitions buckets for sampling and working on them using HiveQL Used Sqoop for importing the data into HBase and Hive exporting result set from Hive to MySQL using Sqoop export tool for further processing Enumerated Hive queries to do analysis of the data and to generate the end reports to be used by business users Worked on scalable distributed computing systems software architecture data structures and algorithms using Hadoop Apache Spark and Apache Storm etc and ingested streaming data into Hadoop using Spark Storm Framework and Scala Implemented POCs with Spark SQL to interpret complex JSON records Experience in transferring Streaming data data from different data sources into HDFS and NoSQL databases using Apache Flume Developed Spark jobs written in Scala to perform operations like data aggregation data processing and data analysis Used Kafka Flume for building robust and fault tolerant data Ingestion pipeline between JMS and Spark Streaming Applications for transporting streaming web log data into HDFS Used Spark for series of dependent jobs and for iterative algorithms Developed a data pipeline using Kafka and Spark Streaming to store data into HDFS Environment Hadoop HDFS Flume CDH Pig Hive Oozie Zookeeper HBase Spark Storm Spark SQL NoSQL Scala Kafka MongoDB Big DataHadoop Developer Capital One Dallas TX August 2016 to September 2017 Responsibilities Developed Scala UDFs using both Data framesSQL and RDDMapReduce in Spark for Data Aggregation queries and writing data back into RDBMS through Sqoop Exported analyzed data to relational databases using Sqoop in deploying data from various sources into HDFS and building reports using Tableau Exported analyzed data to relational databases using Sqoop for visualization to generate reports for the BI team Used the JSON and Avro for serialization and deserialization packaged with Hive to parse the contents of streamed log data and implemented Hive custom UDFs Configured Spark streaming to receive real time data from Kafka and store the stream data to HDFS using Scala Involved in developing ETL data pipelines for performing realtime streaming by ingesting data into HDFS and HBase using Kafka and Storm Involved in moving log files generated from varied sources to HDFS further processing through Flume Involved in creating Hive tables by using Impala and working on them using HiveQL and perform data analysis using Hive and Pig Developed workflow in Oozie to manage and schedule jobs on Hadoop cluster to trigger daily weekly and monthly batch cycles Assisted in upgrading configuration and maintenance of various Hadoop infrastructures like Pig Hive and HBase Worked on Apache Flume for collecting and aggregating huge amount of log data and stored it on HDFS for doing further analysis Load the data into Spark RDD and performed inmemory data computation to generate the output response Efficiently put and fetched data tofrom HBase by writing MapReduce job Environment Hadoop Flume Kafka Spark Sqoop Spark SQL SparkStreaming Hive Scala pig NoSQL Impala Oozie HBase Zookeeper Hadoop Consultant ATT Dallas TX June 2015 to August 2016 Responsibilities Worked on loading the customers data and event logs from Kafka into HBase using REST API Responsible for Cluster maintenance adding and removing cluster nodes Cluster Monitoring and Troubleshooting manage and review data backups and log files Implemented various MapReduce Jobs in custom environments and updating them to HBase tables by generating hive queries Developed data pipeline using Flume Sqoop Pig and Java MapReduce to ingest customer behavioral data and financial histories into HDFS for analysis Collecting and aggregating large amounts of log data using Apache Flume and staging data in HDFS for further analysis Created Hive tables from JSON data using data serialization framework using Avro Parquet File formats and Snappy compression Implemented generic export framework for moving data from HDFS to RDBMS and viceversa Worked on analyzing Hadoop cluster using different big data analytic tools including Kafka Pig Hive and Map Reduce MR1 and MR2 Automated all the jobs for pulling data from FTP server to load data into Hive tables using Oozie workflows Involved using HCATALOG to access Hive table metadata from Pig code Implemented SQL PLSQL Stored Procedures Actively involved in code review and bug fixing for improving the performance Worked on tuning the performance Pig queries and involved in loading data from LINUX file system to HDFS Importing and exporting data into HDFS using Sqoop and Kafka Created HBase tables to store various data formats of PII data coming from different portfolios Implemented Mapreduce for loading data from Oracle database Used NoSQL database with HBase and MongoDB Exported the result set from Hive to MySQL using Shell scripts Gained experience in managing and reviewing Hadoop log files Involved in scheduling Oozie workflow engine to run multiple Pig jobs Environment Hadoop HDFS HBase Pig Hive Spark Hortonworks Oozie MapReduce Sqoop MongoDB Kafka LINUX Java APIs Java collection Hadoop Developer Sprint Overland Park KS August 2013 to April 2015 Responsibilities Worked extensively on Hadoop Components such as HDFS Job Tracker Task Tracker Name Node Data Node YARN and MapReduce programming Involved in loading data from UNIX file system to HDFS Imported and exported data into HDFS and Hive using Sqoop Experience in developing batch processing framework to ingest data into HDFS Hive and Cassandra Worked on Hive and Pig extensively to analyze network data in collecting metrics for Hadoop clusters Automation of data pulls from SQL Server to Hadoop for analyzing large amounts of data sets to determine optimal way to aggregate and report on it Provided quick response for client requests and experienced in creating ad hoc reports Performance Tuning for Hive and Pig Jobs performance parameters along with native MapReduce parameters to avoid excessive disk spills enabled temp file compression between jobs in the data pipeline to handle production size data in a multitenant cluster environment Designed workflows and coordinators in Oozie to automate and parallelize Hive and Pig jobs on Apache Hadoop environment by Hortonworks Developed a process for the Batch ingestion of CSV Files Sqoop from different sources and also generating views on the data source using Shell Scripting and Python Delivered Hadoop migration strategy roadmap and technology fitment for importing real time network log data into HDFS POCs on moving existing Hive Pig Latin jobs to Spark for Deploying and configuring agents to stream log events into HDFS for analysis Load the data into Hive tables using HiveQL along with Deduplication and Windowing to generate adhoc reports using Hive to validate customer viewing history and debug issues in production Experienced with multiple Input Formats such as Text File Key Value Sequence File input format load to HDFS Worked on business specific custom UDFs in Hive and Pig for data extraction transformation and aggregation from multiple file formats including XML JSON CSV and other compressed file formats Environment HDFS MapReduce Pig Hive Oozie Sqoop Cassandra Hortonworks Java Developer Blue Cross Blue Shield Detroit MI February 2012 to July 2013 Responsibilities Experience using middleware architecture using Java technologies like J2EE Servlets and application servers like Web Sphere and Web logic Worked on loading data from Linux file system to HDFS Understanding and analyzing the requirements Designed developed and validated User Interface using HTML Java Script and XML Involved with writing SQL queries using Joins and Stored Procedures using Maven to build and deploy the applications in JBoss application Server in Software Development Lifecycle Model Worked on Eclipse IDE for front end development environment for insertions updating and retrieval operations of data from oracle database by writing stored procedures Developed MapReduce jobs to convert data files into Parquet file format and included MRUnit to test the correctness of MapReduce programs Experienced in working with various kinds of datasets for structured semi structured and unstructured data with Teradata and Oracle for successfully loading files to HDFS from Teradata and loaded from HDFS to Hive Installed Oozie workflow engine to run multiple Hive Developed Hive queries to process the data and generate the data cubes for visualizing Concatenated ETL logics from RDBMS to Hive Implemented partitioning bucketing and worked on Hive using file formats and compressions techniques with optimizations Computed various metrics using Map Reduce to calculate metrics that define user experience revenue etc Environment Hadoop HDFS Pig Oozie Hive Python MapReduce Java SQL Scripting and Linux Shell Scripting Cloudera Cloudera Manager Software Developer Amazon Hyderabad Telangana September 2010 to November 2011 Responsibilities Worked on Business logic for web service using spring annotations which enables dependency injection Developed Spring Application Framework for Dependency Injection support for the Data Access Object DAO pattern and integrated with Hibernate ORM Developed user interface for designing and developing the application using Java JEE and spring core using JSP JavaScript Ajax jQuery HTML CSS and JSTL Outlining agile methodology with daily scrums using TDD and continuous integration in the SDLC process and used JIRA for bug tracking and task management Developed Talend jobs to populate the claims data to data warehouse star schema Used Jenkins for continuous integration purpose in using SVN Junit and Mockito as version control and Unit testing by Creating design documents and test cases for development work Environment Java Servlets JSP HTML CSS Talend Ajax JavaScript Hibernate Spring WebLogic JMS REST SVN Education Bachelor of Technology in Computer Science and Engineering in Computer Science and Engineering Gokaraju Rangaraju Institute of Engineering and Technology Hyderabad Telangana Skills Cassandra Hdfs Impala Mapreduce Oozie Military Service Branch United States Navy Rank second Additional Information Technical Skills Languages Java Python Scala HiveQL Hadoop Ecosystem HDFS Hive MapReduce HBase YARN Sqoop Flume Oozie Zookeeper Impala Databases Oracle RDBMS DB2 SQL Server MySQL NoSQL Databases HBase MongoDB Cassandra Scripting Languages JavaScript CSS Python Perl Shell Script",
    "extracted_keywords": [
        "HadoopSpark",
        "Developer",
        "HadoopSpark",
        "span",
        "lDeveloperspan",
        "HadoopSpark",
        "Developer",
        "Edward",
        "Jones",
        "Boston",
        "MA",
        "years",
        "experience",
        "Information",
        "Technology",
        "years",
        "Big",
        "Data",
        "Hadoop",
        "Ecosystem",
        "Experience",
        "BI",
        "team",
        "data",
        "requirements",
        "Hadoop",
        "technologies",
        "Expert",
        "indexes",
        "views",
        "procedures",
        "functions",
        "cursors",
        "tables",
        "table",
        "expressions",
        "CTEs",
        "data",
        "manipulation",
        "data",
        "consistency",
        "understandingknowledge",
        "Hadoop",
        "architecture",
        "components",
        "Big",
        "Data",
        "Hands",
        "experience",
        "configuring",
        "Hadoop",
        "ecosystem",
        "components",
        "MapReduce",
        "HDFS",
        "HBase",
        "Oozie",
        "Hive",
        "Sqoop",
        "Pig",
        "Zookeeper",
        "Experience",
        "data",
        "HQL",
        "Pig",
        "Latin",
        "MapReduce",
        "programs",
        "Java",
        "Knowledge",
        "file",
        "formats",
        "Avro",
        "Parquet",
        "ORC",
        "compression",
        "codecs",
        "GZip",
        "Snappy",
        "LZO",
        "competency",
        "HIVE",
        "schema",
        "design",
        "partitions",
        "Experience",
        "ingestion",
        "storage",
        "processing",
        "analysis",
        "Big",
        "Data",
        "handson",
        "experience",
        "Apache",
        "Spark",
        "Spark",
        "Streaming",
        "design",
        "development",
        "maintenance",
        "metrics",
        "reports",
        "dashboards",
        "business",
        "decisions",
        "concepts",
        "readers",
        "Expertise",
        "processing",
        "system",
        "Data",
        "Warehousing",
        "platform",
        "Data",
        "cleaning",
        "preprocessing",
        "modelling",
        "Spark",
        "Python",
        "Experience",
        "ETL",
        "top",
        "log",
        "data",
        "web",
        "servers",
        "HDFS",
        "Flume",
        "Performed",
        "data",
        "analytics",
        "insights",
        "Impala",
        "Hive",
        "Expert",
        "scheduling",
        "jobs",
        "Oozie",
        "Crontab",
        "Handson",
        "Git",
        "Agile",
        "Scrum",
        "JIRA",
        "Confluence",
        "Authorized",
        "US",
        "employer",
        "Work",
        "Experience",
        "HadoopSpark",
        "Developer",
        "Edward",
        "Jones",
        "Boston",
        "MA",
        "March",
        "Present",
        "Responsibilities",
        "Spark",
        "jobs",
        "Scala",
        "operations",
        "data",
        "aggregation",
        "data",
        "processing",
        "data",
        "analysis",
        "data",
        "Spark",
        "RDD",
        "data",
        "computation",
        "output",
        "response",
        "Developed",
        "Hive",
        "UDFs",
        "use",
        "HiveQL",
        "filtering",
        "structure",
        "data",
        "Spark",
        "series",
        "jobs",
        "algorithms",
        "data",
        "pipeline",
        "Kafka",
        "Spark",
        "Streaming",
        "data",
        "workflow",
        "Oozie",
        "tasks",
        "data",
        "HDFS",
        "data",
        "pipeline",
        "Flume",
        "Sqoop",
        "Pig",
        "Java",
        "MapReduce",
        "customer",
        "data",
        "histories",
        "HDFS",
        "analysis",
        "ETL",
        "Applications",
        "Hive",
        "Spark",
        "Impala",
        "Sqoop",
        "Automation",
        "Oozie",
        "Used",
        "Pig",
        "ETL",
        "tool",
        "transformations",
        "event",
        "preaggregations",
        "data",
        "HDFS",
        "Spark",
        "series",
        "jobs",
        "algorithms",
        "data",
        "pipeline",
        "Kafka",
        "Spark",
        "Streaming",
        "data",
        "HDFS",
        "Hadoop",
        "DeveloperEngineer",
        "Navy",
        "Federal",
        "Credit",
        "Union",
        "Vienna",
        "VA",
        "September",
        "March",
        "Responsibilities",
        "process",
        "extraction",
        "data",
        "warehouses",
        "weblogs",
        "workflows",
        "coordinator",
        "jobs",
        "Oozie",
        "Assisted",
        "configuration",
        "maintenance",
        "Hadoop",
        "infrastructures",
        "Pig",
        "Hive",
        "HBase",
        "Spark",
        "jobs",
        "Scala",
        "operations",
        "data",
        "aggregation",
        "data",
        "processing",
        "data",
        "analysis",
        "data",
        "Spark",
        "RDD",
        "data",
        "computation",
        "output",
        "response",
        "MapReduce",
        "Jobs",
        "custom",
        "environments",
        "HBase",
        "tables",
        "hive",
        "queries",
        "Cluster",
        "maintenance",
        "cluster",
        "nodes",
        "Cluster",
        "Monitoring",
        "Troubleshooting",
        "manage",
        "data",
        "backups",
        "files",
        "workflow",
        "Oozie",
        "schedule",
        "jobs",
        "Hadoop",
        "cluster",
        "batch",
        "cycles",
        "Spark",
        "series",
        "jobs",
        "algorithms",
        "data",
        "pipeline",
        "Kafka",
        "Spark",
        "Streaming",
        "data",
        "HDFS",
        "Performance",
        "Tuning",
        "Hive",
        "Pig",
        "Jobs",
        "performance",
        "parameters",
        "MapReduce",
        "parameters",
        "disk",
        "spills",
        "temp",
        "file",
        "compression",
        "jobs",
        "data",
        "pipeline",
        "production",
        "size",
        "data",
        "cluster",
        "environment",
        "data",
        "pipeline",
        "Flume",
        "Sqoop",
        "Pig",
        "Java",
        "MapReduce",
        "customer",
        "data",
        "histories",
        "HDFS",
        "analysis",
        "Environment",
        "Hadoop",
        "HDFS",
        "Flume",
        "CDH",
        "Pig",
        "Hive",
        "Oozie",
        "Zookeeper",
        "HBase",
        "Spark",
        "Storm",
        "Spark",
        "SQL",
        "NoSQL",
        "Scala",
        "Kafka",
        "MongoDB",
        "Linux",
        "Sqoop",
        "Hadoop",
        "Developer",
        "Bank",
        "America",
        "Charlotte",
        "NC",
        "September",
        "September",
        "Responsibilities",
        "implementation",
        "data",
        "integration",
        "largescale",
        "system",
        "software",
        "Hadoop",
        "ecosystem",
        "components",
        "HBase",
        "Sqoop",
        "Zookeeper",
        "Oozie",
        "Hive",
        "Pig",
        "Developed",
        "Hive",
        "UDFs",
        "use",
        "HiveQL",
        "filtering",
        "structure",
        "data",
        "ETL",
        "Applications",
        "Hive",
        "Spark",
        "Impala",
        "Sqoop",
        "Automation",
        "Oozie",
        "Used",
        "Pig",
        "ETL",
        "tool",
        "transformations",
        "event",
        "preaggregations",
        "data",
        "Automated",
        "process",
        "extraction",
        "data",
        "warehouses",
        "weblogs",
        "workflows",
        "coordinator",
        "jobs",
        "Oozie",
        "Developed",
        "workflow",
        "Oozie",
        "tasks",
        "data",
        "HDFS",
        "Hive",
        "tables",
        "partitions",
        "buckets",
        "HiveQL",
        "Used",
        "Sqoop",
        "data",
        "HBase",
        "Hive",
        "result",
        "Hive",
        "MySQL",
        "Sqoop",
        "export",
        "tool",
        "Hive",
        "analysis",
        "data",
        "end",
        "business",
        "users",
        "computing",
        "systems",
        "software",
        "architecture",
        "data",
        "structures",
        "algorithms",
        "Hadoop",
        "Apache",
        "Spark",
        "Apache",
        "Storm",
        "streaming",
        "data",
        "Hadoop",
        "Spark",
        "Storm",
        "Framework",
        "Scala",
        "POCs",
        "Spark",
        "SQL",
        "records",
        "Experience",
        "Streaming",
        "data",
        "data",
        "data",
        "sources",
        "HDFS",
        "NoSQL",
        "Apache",
        "Flume",
        "Developed",
        "Spark",
        "jobs",
        "Scala",
        "operations",
        "data",
        "aggregation",
        "data",
        "processing",
        "data",
        "analysis",
        "Kafka",
        "Flume",
        "robust",
        "data",
        "Ingestion",
        "pipeline",
        "JMS",
        "Spark",
        "Streaming",
        "Applications",
        "web",
        "log",
        "data",
        "HDFS",
        "Spark",
        "series",
        "jobs",
        "algorithms",
        "data",
        "pipeline",
        "Kafka",
        "Spark",
        "Streaming",
        "data",
        "HDFS",
        "Environment",
        "Hadoop",
        "HDFS",
        "Flume",
        "CDH",
        "Pig",
        "Hive",
        "Oozie",
        "Zookeeper",
        "HBase",
        "Spark",
        "Storm",
        "Spark",
        "SQL",
        "NoSQL",
        "Scala",
        "Kafka",
        "MongoDB",
        "Big",
        "DataHadoop",
        "Developer",
        "Capital",
        "One",
        "Dallas",
        "TX",
        "August",
        "September",
        "Responsibilities",
        "Scala",
        "UDFs",
        "Data",
        "framesSQL",
        "RDDMapReduce",
        "Spark",
        "Data",
        "Aggregation",
        "data",
        "RDBMS",
        "Sqoop",
        "data",
        "databases",
        "Sqoop",
        "data",
        "sources",
        "HDFS",
        "building",
        "reports",
        "Tableau",
        "data",
        "databases",
        "Sqoop",
        "visualization",
        "reports",
        "BI",
        "team",
        "JSON",
        "Avro",
        "serialization",
        "deserialization",
        "Hive",
        "contents",
        "log",
        "data",
        "Hive",
        "custom",
        "UDFs",
        "Configured",
        "Spark",
        "streaming",
        "time",
        "data",
        "Kafka",
        "stream",
        "data",
        "HDFS",
        "Scala",
        "ETL",
        "data",
        "pipelines",
        "streaming",
        "data",
        "HDFS",
        "HBase",
        "Kafka",
        "Storm",
        "log",
        "files",
        "sources",
        "processing",
        "Flume",
        "Hive",
        "tables",
        "Impala",
        "HiveQL",
        "data",
        "analysis",
        "Hive",
        "Pig",
        "workflow",
        "Oozie",
        "schedule",
        "jobs",
        "Hadoop",
        "cluster",
        "batch",
        "cycles",
        "configuration",
        "maintenance",
        "Hadoop",
        "infrastructures",
        "Pig",
        "Hive",
        "HBase",
        "Apache",
        "Flume",
        "amount",
        "log",
        "data",
        "HDFS",
        "analysis",
        "data",
        "Spark",
        "RDD",
        "data",
        "computation",
        "output",
        "response",
        "data",
        "HBase",
        "MapReduce",
        "job",
        "Environment",
        "Hadoop",
        "Flume",
        "Kafka",
        "Spark",
        "Sqoop",
        "Spark",
        "SQL",
        "SparkStreaming",
        "Hive",
        "Scala",
        "pig",
        "NoSQL",
        "Impala",
        "Oozie",
        "HBase",
        "Zookeeper",
        "Hadoop",
        "Consultant",
        "ATT",
        "Dallas",
        "TX",
        "June",
        "August",
        "Responsibilities",
        "customers",
        "data",
        "event",
        "logs",
        "Kafka",
        "HBase",
        "REST",
        "API",
        "Cluster",
        "maintenance",
        "cluster",
        "nodes",
        "Cluster",
        "Monitoring",
        "Troubleshooting",
        "manage",
        "data",
        "backups",
        "files",
        "MapReduce",
        "Jobs",
        "custom",
        "environments",
        "HBase",
        "tables",
        "hive",
        "data",
        "pipeline",
        "Flume",
        "Sqoop",
        "Pig",
        "Java",
        "MapReduce",
        "customer",
        "data",
        "histories",
        "HDFS",
        "analysis",
        "amounts",
        "log",
        "data",
        "Apache",
        "Flume",
        "data",
        "HDFS",
        "analysis",
        "Hive",
        "tables",
        "data",
        "data",
        "serialization",
        "framework",
        "Avro",
        "Parquet",
        "File",
        "formats",
        "compression",
        "export",
        "framework",
        "data",
        "HDFS",
        "viceversa",
        "Hadoop",
        "cluster",
        "data",
        "tools",
        "Kafka",
        "Pig",
        "Hive",
        "Map",
        "Reduce",
        "MR1",
        "MR2",
        "Automated",
        "jobs",
        "data",
        "FTP",
        "server",
        "data",
        "Hive",
        "tables",
        "Oozie",
        "workflows",
        "HCATALOG",
        "Hive",
        "table",
        "metadata",
        "Pig",
        "code",
        "SQL",
        "PLSQL",
        "Stored",
        "Procedures",
        "code",
        "review",
        "bug",
        "performance",
        "performance",
        "Pig",
        "queries",
        "loading",
        "data",
        "LINUX",
        "file",
        "system",
        "HDFS",
        "Importing",
        "data",
        "HDFS",
        "Sqoop",
        "Kafka",
        "Created",
        "HBase",
        "data",
        "formats",
        "PII",
        "data",
        "portfolios",
        "Mapreduce",
        "loading",
        "data",
        "Oracle",
        "database",
        "NoSQL",
        "database",
        "HBase",
        "MongoDB",
        "result",
        "Hive",
        "MySQL",
        "Shell",
        "scripts",
        "experience",
        "Hadoop",
        "log",
        "files",
        "Oozie",
        "workflow",
        "engine",
        "Pig",
        "jobs",
        "Environment",
        "Hadoop",
        "HDFS",
        "HBase",
        "Pig",
        "Hive",
        "Spark",
        "Hortonworks",
        "Oozie",
        "MapReduce",
        "Sqoop",
        "MongoDB",
        "Kafka",
        "LINUX",
        "Java",
        "APIs",
        "Java",
        "collection",
        "Hadoop",
        "Developer",
        "Sprint",
        "Overland",
        "Park",
        "KS",
        "August",
        "April",
        "Responsibilities",
        "Hadoop",
        "Components",
        "HDFS",
        "Job",
        "Tracker",
        "Task",
        "Tracker",
        "Name",
        "Node",
        "Data",
        "Node",
        "YARN",
        "MapReduce",
        "programming",
        "loading",
        "data",
        "UNIX",
        "file",
        "system",
        "data",
        "HDFS",
        "Hive",
        "Sqoop",
        "Experience",
        "batch",
        "processing",
        "framework",
        "data",
        "HDFS",
        "Hive",
        "Cassandra",
        "Worked",
        "Hive",
        "Pig",
        "network",
        "data",
        "metrics",
        "Hadoop",
        "clusters",
        "Automation",
        "data",
        "SQL",
        "Server",
        "Hadoop",
        "amounts",
        "data",
        "sets",
        "way",
        "response",
        "client",
        "requests",
        "ad",
        "hoc",
        "Performance",
        "Hive",
        "Pig",
        "Jobs",
        "performance",
        "parameters",
        "MapReduce",
        "parameters",
        "disk",
        "spills",
        "temp",
        "file",
        "compression",
        "jobs",
        "data",
        "pipeline",
        "production",
        "size",
        "data",
        "cluster",
        "environment",
        "workflows",
        "coordinators",
        "Oozie",
        "Hive",
        "Pig",
        "jobs",
        "Apache",
        "Hadoop",
        "environment",
        "Hortonworks",
        "process",
        "Batch",
        "ingestion",
        "CSV",
        "Files",
        "Sqoop",
        "sources",
        "views",
        "data",
        "source",
        "Shell",
        "Scripting",
        "Python",
        "Delivered",
        "Hadoop",
        "migration",
        "strategy",
        "roadmap",
        "technology",
        "fitment",
        "time",
        "network",
        "log",
        "data",
        "HDFS",
        "POCs",
        "Hive",
        "Pig",
        "Latin",
        "jobs",
        "Spark",
        "Deploying",
        "agents",
        "log",
        "events",
        "HDFS",
        "analysis",
        "data",
        "Hive",
        "tables",
        "HiveQL",
        "Deduplication",
        "Windowing",
        "reports",
        "Hive",
        "customer",
        "history",
        "issues",
        "production",
        "Input",
        "Formats",
        "Text",
        "File",
        "Key",
        "Value",
        "Sequence",
        "File",
        "input",
        "format",
        "load",
        "HDFS",
        "business",
        "custom",
        "UDFs",
        "Hive",
        "Pig",
        "data",
        "extraction",
        "transformation",
        "aggregation",
        "file",
        "formats",
        "XML",
        "CSV",
        "file",
        "formats",
        "Environment",
        "HDFS",
        "MapReduce",
        "Pig",
        "Hive",
        "Oozie",
        "Sqoop",
        "Cassandra",
        "Hortonworks",
        "Java",
        "Developer",
        "Blue",
        "Cross",
        "Blue",
        "Shield",
        "Detroit",
        "MI",
        "February",
        "July",
        "Responsibilities",
        "Experience",
        "middleware",
        "architecture",
        "Java",
        "technologies",
        "J2EE",
        "Servlets",
        "application",
        "servers",
        "Web",
        "Sphere",
        "Web",
        "logic",
        "loading",
        "data",
        "Linux",
        "file",
        "system",
        "HDFS",
        "Understanding",
        "requirements",
        "User",
        "Interface",
        "HTML",
        "Java",
        "Script",
        "XML",
        "SQL",
        "queries",
        "Joins",
        "Stored",
        "Procedures",
        "Maven",
        "applications",
        "JBoss",
        "application",
        "Server",
        "Software",
        "Development",
        "Lifecycle",
        "Model",
        "Worked",
        "Eclipse",
        "IDE",
        "end",
        "development",
        "environment",
        "insertions",
        "retrieval",
        "operations",
        "data",
        "oracle",
        "database",
        "procedures",
        "MapReduce",
        "jobs",
        "data",
        "files",
        "Parquet",
        "file",
        "format",
        "MRUnit",
        "correctness",
        "MapReduce",
        "programs",
        "kinds",
        "datasets",
        "data",
        "Teradata",
        "Oracle",
        "files",
        "HDFS",
        "Teradata",
        "HDFS",
        "Hive",
        "Installed",
        "Oozie",
        "workflow",
        "engine",
        "Hive",
        "Developed",
        "Hive",
        "queries",
        "data",
        "data",
        "cubes",
        "ETL",
        "logics",
        "RDBMS",
        "Hive",
        "bucketing",
        "Hive",
        "file",
        "formats",
        "compressions",
        "techniques",
        "optimizations",
        "metrics",
        "Map",
        "Reduce",
        "metrics",
        "user",
        "experience",
        "revenue",
        "Environment",
        "Hadoop",
        "HDFS",
        "Pig",
        "Oozie",
        "Hive",
        "Python",
        "MapReduce",
        "Java",
        "SQL",
        "Scripting",
        "Linux",
        "Shell",
        "Scripting",
        "Cloudera",
        "Cloudera",
        "Manager",
        "Software",
        "Developer",
        "Amazon",
        "Hyderabad",
        "Telangana",
        "September",
        "November",
        "Responsibilities",
        "Business",
        "logic",
        "web",
        "service",
        "spring",
        "annotations",
        "dependency",
        "injection",
        "Developed",
        "Spring",
        "Application",
        "Framework",
        "Dependency",
        "Injection",
        "support",
        "Data",
        "Access",
        "Object",
        "DAO",
        "pattern",
        "Hibernate",
        "ORM",
        "user",
        "interface",
        "application",
        "Java",
        "JEE",
        "spring",
        "core",
        "JSP",
        "JavaScript",
        "Ajax",
        "jQuery",
        "HTML",
        "CSS",
        "JSTL",
        "methodology",
        "scrums",
        "TDD",
        "integration",
        "SDLC",
        "process",
        "JIRA",
        "bug",
        "tracking",
        "task",
        "management",
        "Talend",
        "jobs",
        "claims",
        "data",
        "data",
        "warehouse",
        "star",
        "schema",
        "Jenkins",
        "integration",
        "purpose",
        "SVN",
        "Junit",
        "Mockito",
        "version",
        "control",
        "Unit",
        "testing",
        "design",
        "documents",
        "test",
        "cases",
        "development",
        "work",
        "Environment",
        "Java",
        "Servlets",
        "JSP",
        "HTML",
        "CSS",
        "Talend",
        "Ajax",
        "JavaScript",
        "Hibernate",
        "Spring",
        "WebLogic",
        "JMS",
        "REST",
        "SVN",
        "Education",
        "Bachelor",
        "Technology",
        "Computer",
        "Science",
        "Engineering",
        "Computer",
        "Science",
        "Engineering",
        "Gokaraju",
        "Rangaraju",
        "Institute",
        "Engineering",
        "Technology",
        "Hyderabad",
        "Telangana",
        "Skills",
        "Cassandra",
        "Hdfs",
        "Impala",
        "Mapreduce",
        "Oozie",
        "Military",
        "Service",
        "Branch",
        "United",
        "States",
        "Navy",
        "Rank",
        "Additional",
        "Information",
        "Technical",
        "Skills",
        "Languages",
        "Java",
        "Python",
        "Scala",
        "HiveQL",
        "Hadoop",
        "Ecosystem",
        "HDFS",
        "Hive",
        "MapReduce",
        "HBase",
        "YARN",
        "Sqoop",
        "Flume",
        "Oozie",
        "Zookeeper",
        "Impala",
        "Oracle",
        "RDBMS",
        "DB2",
        "SQL",
        "Server",
        "MySQL",
        "NoSQL",
        "Databases",
        "HBase",
        "MongoDB",
        "Cassandra",
        "Scripting",
        "Languages",
        "JavaScript",
        "CSS",
        "Python",
        "Perl",
        "Shell",
        "Script"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T19:48:44.364654",
    "resume_data": "HadoopSpark Developer HadoopSpark span lDeveloperspan HadoopSpark Developer Edward Jones Boston MA Around 8 years of professional experience in Information Technology which includes 5 years in Big Data and Hadoop Ecosystem Experience in working with BI team and transform big data requirements into Hadoop centric technologies Expert in creating indexes views complex stored procedures userdefined functions cursors derived tables common table expressions CTEs and triggers to facilitate efficient data manipulation and data consistency Excellent understandingknowledge of Hadoop architecture and various components of Big Data Hands on experience in installing configuring and using Hadoop ecosystem components like MapReduce HDFS HBase Oozie Hive Sqoop Pig and Zookeeper Experience in analyzing data using HQL Pig Latin and custom MapReduce programs in Java Knowledge on various file formats such as Avro Parquet ORC etc and on various compression codecs such as GZip Snappy LZO etc Strong competency in HIVE schema design partitions and bucketing Experience in ingestion storage querying processing and analysis of Big Data with handson experience in Apache Spark and Spark Streaming Owned the design development and maintenance of ongoing metrics reports analyses dashboards etc to drive key business decisions and communicate key concepts to readers Expertise in designing and developing a distributed processing system running into a Data Warehousing platform for reporting Data cleaning preprocessing and modelling using Spark and Python Experience in performing ETL on top of streaming log data from various web servers into HDFS using Flume Performed data analytics and insights using Impala and Hive Expert in developing and scheduling jobs using Oozie and Crontab Handson Git Agile Scrum JIRA and Confluence Authorized to work in the US for any employer Work Experience HadoopSpark Developer Edward Jones Boston MA March 2019 to Present Responsibilities Developed Spark jobs written in Scala to perform operations like data aggregation data processing and data analysis Load the data into Spark RDD and performed inmemory data computation to generate the output response Developed Hive UDFs for extended use and wrote HiveQL for sorting joining filtering and grouping the structure data Used Spark for series of dependent jobs and for iterative algorithms Developed a data pipeline using Kafka and Spark Streaming to store data into HDFS Developed workflow in Oozie to automate the tasks of loading the data into HDFS Developed data pipeline using Flume Sqoop Pig and Java MapReduce to ingest customer behavioral data and financial histories into HDFS for analysis Developed ETL Applications using Hive Spark and Impala Sqoop for Automation using Oozie Used Pig as ETL tool to do transformations event joins and some preaggregations before storing the data onto HDFS Used Spark for series of dependent jobs and for iterative algorithms Developed a data pipeline using Kafka and Spark Streaming to store data into HDFS Hadoop DeveloperEngineer Navy Federal Credit Union Vienna VA September 2018 to March 2019 Responsibilities Automated the process for extraction of data from warehouses and weblogs by developing workflows and coordinator jobs in Oozie Assisted in upgrading configuration and maintenance of various Hadoop infrastructures like Pig Hive and HBase Developed Spark jobs written in Scala to perform operations like data aggregation data processing and data analysis Load the data into Spark RDD and performed inmemory data computation to generate the output response Implemented various MapReduce Jobs in custom environments and updating them to HBase tables by generating hive queries Responsible for Cluster maintenance adding and removing cluster nodes Cluster Monitoring and Troubleshooting manage and review data backups and log files Developed workflow in Oozie to manage and schedule jobs on Hadoop cluster to trigger daily weekly and monthly batch cycles Used Spark for series of dependent jobs and for iterative algorithms Developed a data pipeline using Kafka and Spark Streaming to store data into HDFS Performance Tuning for Hive and Pig Jobs performance parameters along with native MapReduce parameters to avoid excessive disk spills enabled temp file compression between jobs in the data pipeline to handle production size data in a multitenant cluster environment Developed data pipeline using Flume Sqoop Pig and Java MapReduce to ingest customer behavioral data and financial histories into HDFS for analysis Environment Hadoop HDFS Flume CDH Pig Hive Oozie Zookeeper HBase Spark Storm Spark SQL NoSQL Scala Kafka MongoDB Linux Sqoop Hadoop Developer Bank of America Charlotte NC September 2017 to September 2018 Responsibilities Worked on implementation and data integration in developing largescale system software experiencing with Hadoop ecosystem components like HBase Sqoop Zookeeper Oozie Hive and Pig Developed Hive UDFs for extended use and wrote HiveQL for sorting joining filtering and grouping the structure data Developed ETL Applications using Hive Spark and Impala Sqoop for Automation using Oozie Used Pig as ETL tool to do transformations event joins and some preaggregations before storing the data onto HDFS Automated the process for extraction of data from warehouses and weblogs by developing workflows and coordinator jobs in Oozie Developed workflow in Oozie to automate the tasks of loading the data into HDFS Creating Hive tables dynamic partitions buckets for sampling and working on them using HiveQL Used Sqoop for importing the data into HBase and Hive exporting result set from Hive to MySQL using Sqoop export tool for further processing Enumerated Hive queries to do analysis of the data and to generate the end reports to be used by business users Worked on scalable distributed computing systems software architecture data structures and algorithms using Hadoop Apache Spark and Apache Storm etc and ingested streaming data into Hadoop using Spark Storm Framework and Scala Implemented POCs with Spark SQL to interpret complex JSON records Experience in transferring Streaming data data from different data sources into HDFS and NoSQL databases using Apache Flume Developed Spark jobs written in Scala to perform operations like data aggregation data processing and data analysis Used Kafka Flume for building robust and fault tolerant data Ingestion pipeline between JMS and Spark Streaming Applications for transporting streaming web log data into HDFS Used Spark for series of dependent jobs and for iterative algorithms Developed a data pipeline using Kafka and Spark Streaming to store data into HDFS Environment Hadoop HDFS Flume CDH Pig Hive Oozie Zookeeper HBase Spark Storm Spark SQL NoSQL Scala Kafka MongoDB Big DataHadoop Developer Capital One Dallas TX August 2016 to September 2017 Responsibilities Developed Scala UDFs using both Data framesSQL and RDDMapReduce in Spark for Data Aggregation queries and writing data back into RDBMS through Sqoop Exported analyzed data to relational databases using Sqoop in deploying data from various sources into HDFS and building reports using Tableau Exported analyzed data to relational databases using Sqoop for visualization to generate reports for the BI team Used the JSON and Avro for serialization and deserialization packaged with Hive to parse the contents of streamed log data and implemented Hive custom UDFs Configured Spark streaming to receive real time data from Kafka and store the stream data to HDFS using Scala Involved in developing ETL data pipelines for performing realtime streaming by ingesting data into HDFS and HBase using Kafka and Storm Involved in moving log files generated from varied sources to HDFS further processing through Flume Involved in creating Hive tables by using Impala and working on them using HiveQL and perform data analysis using Hive and Pig Developed workflow in Oozie to manage and schedule jobs on Hadoop cluster to trigger daily weekly and monthly batch cycles Assisted in upgrading configuration and maintenance of various Hadoop infrastructures like Pig Hive and HBase Worked on Apache Flume for collecting and aggregating huge amount of log data and stored it on HDFS for doing further analysis Load the data into Spark RDD and performed inmemory data computation to generate the output response Efficiently put and fetched data tofrom HBase by writing MapReduce job Environment Hadoop Flume Kafka Spark Sqoop Spark SQL SparkStreaming Hive Scala pig NoSQL Impala Oozie HBase Zookeeper Hadoop Consultant ATT Dallas TX June 2015 to August 2016 Responsibilities Worked on loading the customers data and event logs from Kafka into HBase using REST API Responsible for Cluster maintenance adding and removing cluster nodes Cluster Monitoring and Troubleshooting manage and review data backups and log files Implemented various MapReduce Jobs in custom environments and updating them to HBase tables by generating hive queries Developed data pipeline using Flume Sqoop Pig and Java MapReduce to ingest customer behavioral data and financial histories into HDFS for analysis Collecting and aggregating large amounts of log data using Apache Flume and staging data in HDFS for further analysis Created Hive tables from JSON data using data serialization framework using Avro Parquet File formats and Snappy compression Implemented generic export framework for moving data from HDFS to RDBMS and viceversa Worked on analyzing Hadoop cluster using different big data analytic tools including Kafka Pig Hive and Map Reduce MR1 and MR2 Automated all the jobs for pulling data from FTP server to load data into Hive tables using Oozie workflows Involved using HCATALOG to access Hive table metadata from Pig code Implemented SQL PLSQL Stored Procedures Actively involved in code review and bug fixing for improving the performance Worked on tuning the performance Pig queries and involved in loading data from LINUX file system to HDFS Importing and exporting data into HDFS using Sqoop and Kafka Created HBase tables to store various data formats of PII data coming from different portfolios Implemented Mapreduce for loading data from Oracle database Used NoSQL database with HBase and MongoDB Exported the result set from Hive to MySQL using Shell scripts Gained experience in managing and reviewing Hadoop log files Involved in scheduling Oozie workflow engine to run multiple Pig jobs Environment Hadoop HDFS HBase Pig Hive Spark Hortonworks Oozie MapReduce Sqoop MongoDB Kafka LINUX Java APIs Java collection Hadoop Developer Sprint Overland Park KS August 2013 to April 2015 Responsibilities Worked extensively on Hadoop Components such as HDFS Job Tracker Task Tracker Name Node Data Node YARN and MapReduce programming Involved in loading data from UNIX file system to HDFS Imported and exported data into HDFS and Hive using Sqoop Experience in developing batch processing framework to ingest data into HDFS Hive and Cassandra Worked on Hive and Pig extensively to analyze network data in collecting metrics for Hadoop clusters Automation of data pulls from SQL Server to Hadoop for analyzing large amounts of data sets to determine optimal way to aggregate and report on it Provided quick response for client requests and experienced in creating ad hoc reports Performance Tuning for Hive and Pig Jobs performance parameters along with native MapReduce parameters to avoid excessive disk spills enabled temp file compression between jobs in the data pipeline to handle production size data in a multitenant cluster environment Designed workflows and coordinators in Oozie to automate and parallelize Hive and Pig jobs on Apache Hadoop environment by Hortonworks Developed a process for the Batch ingestion of CSV Files Sqoop from different sources and also generating views on the data source using Shell Scripting and Python Delivered Hadoop migration strategy roadmap and technology fitment for importing real time network log data into HDFS POCs on moving existing Hive Pig Latin jobs to Spark for Deploying and configuring agents to stream log events into HDFS for analysis Load the data into Hive tables using HiveQL along with Deduplication and Windowing to generate adhoc reports using Hive to validate customer viewing history and debug issues in production Experienced with multiple Input Formats such as Text File Key Value Sequence File input format load to HDFS Worked on business specific custom UDFs in Hive and Pig for data extraction transformation and aggregation from multiple file formats including XML JSON CSV and other compressed file formats Environment HDFS MapReduce Pig Hive Oozie Sqoop Cassandra Hortonworks Java Developer Blue Cross Blue Shield Detroit MI February 2012 to July 2013 Responsibilities Experience using middleware architecture using Java technologies like J2EE Servlets and application servers like Web Sphere and Web logic Worked on loading data from Linux file system to HDFS Understanding and analyzing the requirements Designed developed and validated User Interface using HTML Java Script and XML Involved with writing SQL queries using Joins and Stored Procedures using Maven to build and deploy the applications in JBoss application Server in Software Development Lifecycle Model Worked on Eclipse IDE for front end development environment for insertions updating and retrieval operations of data from oracle database by writing stored procedures Developed MapReduce jobs to convert data files into Parquet file format and included MRUnit to test the correctness of MapReduce programs Experienced in working with various kinds of datasets for structured semi structured and unstructured data with Teradata and Oracle for successfully loading files to HDFS from Teradata and loaded from HDFS to Hive Installed Oozie workflow engine to run multiple Hive Developed Hive queries to process the data and generate the data cubes for visualizing Concatenated ETL logics from RDBMS to Hive Implemented partitioning bucketing and worked on Hive using file formats and compressions techniques with optimizations Computed various metrics using Map Reduce to calculate metrics that define user experience revenue etc Environment Hadoop HDFS Pig Oozie Hive Python MapReduce Java SQL Scripting and Linux Shell Scripting Cloudera Cloudera Manager Software Developer Amazon Hyderabad Telangana September 2010 to November 2011 Responsibilities Worked on Business logic for web service using spring annotations which enables dependency injection Developed Spring Application Framework for Dependency Injection support for the Data Access Object DAO pattern and integrated with Hibernate ORM Developed user interface for designing and developing the application using Java JEE and spring core using JSP JavaScript Ajax jQuery HTML CSS and JSTL Outlining agile methodology with daily scrums using TDD and continuous integration in the SDLC process and used JIRA for bug tracking and task management Developed Talend jobs to populate the claims data to data warehouse star schema Used Jenkins for continuous integration purpose in using SVN Junit and Mockito as version control and Unit testing by Creating design documents and test cases for development work Environment Java Servlets JSP HTML CSS Talend Ajax JavaScript Hibernate Spring WebLogic JMS REST SVN Education Bachelor of Technology in Computer Science and Engineering in Computer Science and Engineering Gokaraju Rangaraju Institute of Engineering and Technology Hyderabad Telangana Skills Cassandra Hdfs Impala Mapreduce Oozie Military Service Branch United States Navy Rank second Additional Information Technical Skills Languages Java Python Scala HiveQL Hadoop Ecosystem HDFS Hive MapReduce HBase YARN Sqoop Flume Oozie Zookeeper Impala Databases Oracle RDBMS DB2 SQL Server MySQL NoSQL Databases HBase MongoDB Cassandra Scripting Languages JavaScript CSS Python Perl Shell Script",
    "unique_id": "cccb94ea-5cfd-43c3-875c-7b98d8ba3182"
}