{
    "clean_data": "ScalaHadoop Developer ScalaHadoop span lDeveloperspan ScalaHadoop Developer Fidelity Investment Smithfield RI Experienced Hadoop Developer with over 10 years of IT experience has a strong background with Bigdata arena File Systems Data Management Analysis and Java Based enterprise application using Java J2EE technologies Expertise with installing configuring testing and using Apache Hadoop framework 27x its ecosystem components like HDFS MapReduce Sqoop 146 Flume 170 Hive 211 Pig 0160 Spark 210 Scala 2120 Kafka 0101 Yarn Oozie 313 and Zookeeper 3410 Experience and strong knowledge on implementation of Spark Core Spark Streaming Spark SQL Developed applications in Spark 210 using Scala 2120 to compare the performance of Spark with Hive Implemented POCs and developed pipeline using Kafka 0101 Spark Streaming and Spark SQL Extensive experience in Spark 210 transformations using Scala 2120 and Spark SQL for faster testing and processing of data files Hands on real time processing with Spark modules Spark RDD Dataset API using Scala 2120 Hands on experience of writing programs in MapReduce to analyze unstructured data Experience in importing and exporting data using Sqoop from HDFS to Relational Database Systems RDBMS and from RDBMS to HDFS Well versed Expert in writing Pig Latin scripts and HiveQL queries to process and analyze data Experience in Spark and indepth knowledge on SparkSQL RDDs Lazy transformation and actions Worked on Apache Kafka 0101 and Flume for handling megabytes of streaming data Transformed various formats of data like sequence File RC ORC Parquet JSON AVRO and experience in dealing with Compression techniques such as Gzip Snappy Deep knowledge in batch job scheduling workflow using Oozie 420 and zookeeper 3410 Understanding of Amazon Web Services stack and handson experience in using S3 EC2 and EMR Worked with different NOSQL databases such as Cassandra 310 HBbase 130 and MongoDB Experience with Relational databases including Oracle SQL Server and MySQL and experience in writing complex SQL queries PLSQL Stored Procedures Triggers sequences Well versed with the working of Data Visualization tools such as Tableau 93 D3js Developed Web applications using HTML5 CSS3 Bootstrap JavaScript JQuery AJAX Expertise in Core Java Data Structures Multithreading JDBC J2EE Algorithms Object Oriented Design OOD and Exception Handling and frameworks like Spring MVC and Hibernate Produced and consumed SOAP and RESTful Web Services and experience in developing Hadoop applications on Spark using Scala as a functional and objectoriented programming Experienced in writing ANT and Maven scripts to build and deploy Java applications Experienced in TDD TestDriven Development and SDLC methodologies such as AgileScrum Through knowledge of development environments such as Maven Git JIRA 64 Jenkins and Confluence Excellent understanding and knowledge of Hadoop architecture and various daemons such as Name Node Data Node Job Tracker Task Tracker Resource Manager and MapReduce programming paradigm Knowledge of Cyber Security concepts like cryptography Access Control Data Security in Linux Unix Expertise in Creating Hive InternalExternal TablesViews using shared Meta store writing scripts in HiveQL also data transformation file processing using Pig Latin Scripts Experience in Object Oriented Analysis Design OOAD and development of software using UML Methodology good knowledge of J2EE design patterns and Core Java design patterns Possess excellent presentation documentation communication skills detail oriented zeal to learn new technologies with a cooperative team focused attitude strong analytical and problemsolving skills Work Experience ScalaHadoop Developer Fidelity Investment Smithfield RI June 2018 to Present Project Description Fidelity Investment is a financial services firm involved in different sectors including Mutual fund Brokerage Retirement IRA and Wealth Management and in one of the leading companies in 410k plan and Stock Market activity The Distributed reporting project involves transferring from legacy system to the Hadoop environment for the power query using HDFS for the optimize query performance for the end customer using hive data warehouse They are using different tools for this project like Apache Hue Apache Hadoop Apache Spark HDFS Impala and few internal frameworks Responsibilities Experience in creating Hive tables to store the processed results in a tabular format optimizing Hive tables using optimization techniques like partitions and bucketing to provide better performance with HiveQL queries and creating custom user defined functions Developed reviewed and updated architecture and process documentation server diagrams requisition documents and other technical documents Involved in Agile methodology attended daily scrum meetings and sprint planning meetings Integrated visualizations into a Spark application using Databricks and visualization libraries ggplot Matplotlib Implemented different analytical algorithms using MapReduce programs to apply on top of HDFS data Installed managed and maintained a cluster of 60 nodes from POC to PROD Installed Hortonworks clusters and Hadoop ecosystem components through Ambari and command line interface Responsible for cluster maintenance commissioning and decommissioning data nodes cluster monitoring troubleshooting management and review data backups and Hadoop log files Skilled in Tableau Desktop for various types of data visualization reporting and analysis including Cross Map Scatter Plots Geographic Map Pie Charts and Bar Charts Page Trails and Density Chart Created HBase tables to store various data formats of data coming from MySQL Oracle Teradata Installed and configured Hive Pig Sqoop Flume and Oozie on the Hadoop cluster Configured Spark streaming to get ongoing information from the Kafka and stored the stream information to HDFS Developed data pipeline using Flume Sqoop to ingest business data and purchase histories into HDFS for analysis Utilized SparkSQL to extract and process data by parsing using Datasets or RDDs in Hive Context with transformations and actions map flat Map filter reduce reduceByKey Extended the capabilities of Data Frames using User Defined Functions in Python and Scala Designed and Implemented Sqoop incremental imports delta imports on tables without primary keys and dates from Oracle and appends directly into Hive Warehouse Import the data from different sources like HDFSHBase into Spark RDD and perform computations using Scala to generate the output response Writing a spark shell code to audit report to bring all the unique field under metadata Implemented HQL scripts for daily based data loading further aggregations Scheduling Oozie workflow and job to automate the run of Unix script daily as per the team requirement Working on clean up the old data and purge the database for having accurate data in the system Involved in building the ETL architecture and Target mapping to load data into Data Lake Developed Workflows with the help of Oozie to manage the flow of jobs and wrote Custom Expression Language EL functions for complex workflows Working on Install process monthly or by weekly new sets of report for new table or existing table and validating all the data coming from working directory to the HDFS Involved in the complete Software Development Life Cycle SDLC phases of the project as a part of Agile scrum methodology Loaded datasets from MySQL to HDFS and Hive respectively on daily basis Worked on Apache Hue as a central web admin User to Query on Hive or Impala and to Schedule job workflow Assisted in Installation and Configuration of Apache Hadoop clusters CHD and Hadoop tools for application development includes HDFS HUE YARN Sqoop Impala and Hive Used Cloudera Manager for continuous managing and monitoring the Hadoop cluster Environment Spark 230 Scala 2130 Sqoop 146 Impala 220 Oracle Hive 233 HDFS Spark SQL CHD 5161 HiveQL Hue 420 YARN 1123 Oozie 430 Agile Big Data Analytics SolutionsScala Developer Bloomberg LP New York NY May 2015 to May 2018 Project Description My Latest project was with Bloomberg which is a global firm which specializes in financial software media and data Company mainly involve in the financial software tools which provides the analytics and equality trading platform for data services and news Where in Bloomberg briefs its deliver news market data and commentary directly to inbox which give competitive advantage thats changing faster than ever Bloomberg Briefs is unique and provide downloadable reports to the subscriber and financial professional to get the actionable ideas and insights into a diverse range of markets and industries Specifically I was working with the Bloomberg Media division for the Bloomberg Terminal platform which caters to near half millions of corporate customers in more than 160 countries Instead of using third party application we were using the Hadoop ecosystem to manage the large scale of data with lowlatency which is processed daily on this platform Responsibilities Experience in Transform Stage and Store data using Spark which includes writing Spark applications in Scala Developed Spark core and Spark SQL scripts using Scala for faster data processing Developed a data pipeline using Kafka and Storm to store data into HDFS Developed Spark code using Scala and Spark SQL for faster testing and data processing Setting up and managing Kafka and Zookeeper for Stream processing Developed Kafka consumers API in Scala for consuming data from Kafka topics Configured deployed and maintained multinode Dev and Test Kafka Clusters Developed Spark scripts using Scala Spark SQL to access Hive tables in Spark for faster data processing Stored realtime data from Spark on HBase for future analysis Involved in implementing High Availability and automatic failover infrastructure to overcome single point of failure for Name node utilizing Zookeeper services Involved in analyzing data coming from various sources and creating Metafiles and control files to ingest the data in to the Data Lake Generate reports for analytics development through Zeppelin Developed Scala code with Spark Streaming for faster testing and processing of data Worked with Spark RDD and Dataframes for sessionization and other transformations Used Apache Avro to transform data between different format Streamed log files using Flume into HDFS and load into Hive tables to query data Created multiple Hive tables implemented partitioning dynamic partitioning and bucketing in Hive for efficient data access Performed daily adhoc data analysis and pulled data from Hadoop using Hive HiveQL Worked closely with team members managers and other teams in Agile Scrum environment Environment Kafka 0101 Spark 210 Scala 2120 Sqoop 146 Avro MySql5x HBase 130 Zeppelin Hive 211 HDFS Spark SQL Flume 170 HiveQL Zookeeper Agile Hadoop Developer IBM New York NY October 2012 to April 2015 Project Explore Solutions Project Description IBM Watson is a distinctively creating a business unit around the World Company headquartered in the United States where it specializes in the sector of Discovery Advisor Engagement Advisor and Explorer and building the application of business smart cities consumer applications and life in general Using Hadoop Ecosystem we have been developing predictive models to identify customer Insight for Banking by making use of realtime and historical data of bank user like mortgage credit history income source spending and reviews The project is about analysis of data by proper cleaning and normalization process that combine predictive and cognitive capabilities We need to collect data from various resources and enables dynamic solution behavioral segmentation to uncover actionable customer insights allowing banks to create personalized sales offerings and marketing campaigns Capability to store large unstructured data sets in NoSQL databases and using spark to analyze this data Responsibilities Worked on performance and optimization of existing algorithms in Hadoop using Spark context SparkSQL and Spark YARN using Scala Experience in installation upgrading configuration monitoring supporting and managing in Hadoop clusters using Cloudera CDH 4 CDH 5 Hortonworks HDP 2x and 3x on Ubuntu RedHat CentOS systems Worked on components of CDH and HDP including HDFS MapReduce Job tracker Task tracker Sqoop Zookeeper YARN Oozie Hive Hue Flume HBase Fair Scheduler Spark and Kafka Deployed Hadoop clusters on public and private cloud environments like AWS and OpenStack Administered the Linux systems to deploy Hadoop cluster and monitoring the cluster using Nagios and Ganglia Experienced in performing backup recovery failover and DR practices on multiple platforms Implemented Kerberos and LDAP authentication of all the services across Hadoop clusters using Apache Sentry Worked with highly transactional merchandise and investment in SQL databases with PCI HIPAA compliance involving data encryption with certificates and security keys at various levels Experienced in automating the provisioning processes and system resources using Puppet Implemented Hadoopbased solutions to store archives and backups from multiple sources Involved in importing and exporting data with Sqoop from RDBMS MySQL Oracle Teradata and used fast loaders and connectors and processed data for commercial analytics Built ingestion framework using flume for streaming logs and aggregating the data into HDFS Experienced in upgrading SQL Server software patches and service packs Practiced Agile scrum to provide operational support installation updates patches and version upgrades Experienced in collaborative platforms including Jira Rally SharePoint and Discovery Installed monitored and performance tuned standalone multinode clusters of Kafka Successfully loaded files to Hive and HDFS from MongoDB Cassandra and HBase Experienced in understanding and managing Hadoop log files with Flume Experienced on SQL DBA in HA and DDR like replication log shipping mirroring and clustering and database security and permissions Responsible for Importing and exporting data into HDFS and Hive using Sqoop from Oracle 11g and MySQL Develop and maintain several batch jobs to run automatically depending on business requirements Responsible for building scalable distributed data solutions using Hadoop Involved in loading data from edge node to HDFS using shell scripting Implemented Partitioning Dynamic Partitioning Buckets in Hive Developed PIG scripts using Pig Latin and worked on tuning the performance of Pig queries Involved in managing and reviewing Hadoop log files Created HBase tables to store variable data formats of PII data coming from different portfolios Implemented test scripts to support test driven development and continuous integration Exported the analyzed data to the relational databases like MySQL using Sqoop for visualization and to generate reports for the BI team Created MapReduce programs using Java API that filter unnecessary records and find out unique records based on different criteria Experienced in implementing POCs to migrate iterative map reduce programs into Spark transformations actions using Scala Supported in setting up QA environment and updating configurations for implementing scripts with Pig and Sqoop Worked in Test Driven Deployment environment and used Confluence for documentation Performed unit testing using MRunit Installed Oozie workflow engine to run multiple Hive and pig jobs Used Jira for project tracking Bug tracking and Project Management Environment Spark 210 Apache Hadoop MapReduce HDFS Hive 211 Java Pig 0160 Sqoop 146 MRunit Oozie 313 HBase130 TDD MySQL5x Oracle 11g Java Developer Axis Bank Gujarat IN October 2010 to September 2012 Project Customer Assistance Axis is one of the leading global banks Customer Assistance is Banking Application for Axis Bank The Customer Assistance Application plays a crucial role in dailybanking operations This Application makes its users to create bank accounts transfer money update contact history and update checking and saving accounts credit card approvals of the customers Responsibilities Gathered user requirements analyzed and wrote functional and technical specifications we use SharePoint to maintain all of our design docs Followed Agile software methodology for software development 3 week Sprint Worked on one of the UI based application and client focus more on look and feel of the UI We use lots of customs components to design the UI Chase standards and HTML CSS JavaScript AJAX EXTJS is being used intensively Used Spring MVC framework on the server side for creating RESTFul web services by giving JSON out and modifying the DOM object on UI by making HTTP calls and used GET and PUT Developed multiple Controller Service DAO classes to interact with data layer and developed Entity classes based on the table structure Involved in core integration to pick the file from FTP location and bring them into our staging tables and did all the validation on the java side Created multiple midtier services to interact with multiple validations and worked on entitlements services to do user validations Interaction and also worked on applying security systems Worked with Oracle database and used Hibernate ORM Created POJOData Objects in midtier service Hands on experience on implementing lazy loading first and second level of caching Leading onshore offshore model Coordinating with Offshore team in India and being flexible on gathering updated from team Work closely with Database team and testing team Worked on Jasper reports using iReport tool and integrated that JRXML into spring frame work Wrote SQL commands and Stored Procedures to retrieve data from Oracle database Worked to plug this procedure in Java classes Used Spring webflow for MVC pattern Used Apachetiles for JSP page fragments for various flows Used SOAP WebServices for sending data to Published New Services for given WSDL file GUI developing using custom JSTL tag library and used AJAX calls for client side HttpRequests Followed Agile Methodology and regular SCRUM meetings Involved in creating the Hibernate 30 POJO Objects and mapped using Hibernate Annotations Used Hibernate objectrelationalmapping ORM solution technique of mapping data representation from MVC model to Oracle Relational data model with a SQLbased schema Provide support to the users for all the service components and help them in production issues Involved in designing test plans test cases and overall Unit testing of the system using JUnit Environment Java 17 JSP Eclipse JUnit Hibernate 30 Oracle Maven Restful Git Scrum SpringWeb Flow SQL Java Developer eInfochip Gujarat IN July 2009 to September 2010 Project Online Invoice System OVS eInfochips is a Product Engineering and Software RD Services firm based that offers solutions in software hardware and VLSI services The company provides a range of products and services spanning multiple industries including silicon engineering embedded systems software engineering extended services Where I was involved in software engineering which offers custom software application development and maintenance services with focus on shorter development cycles enhanced features better user experience We also help our clients to migrate applications to open source technologies while providing a benefit analysis Responsibilities Software Development LifeCycle SDLC phases of design development implementation deployment testing and maintenance as per quality standards using Agile Scrum and waterfall methodologies Good Experience in Application Software Development and Design Object Oriented Technical Documentation Software Testing and Debugging Excellent implementation knowledge of JDK 1617 and 18 Spring Hibernate RESTFUL Web Services AOP Struts JDBC EJB Consumed and Exposed both REST and SOAP based webservices very good experience with SOA model Experience in RDBMS using MySQL Oracle SQL Server PostgreSQL Involved in configuring deploying applications on IBM WebSphere Application Server BEA WebLogic Application Server Apache Tomcat on UNIX Linux and Windows platforms Extensive experience in Design Development and implementation of ModelViewController MVC using Spring Good experience in Database Design writing stored procedure functions triggers SQL queries Developed the code for front end using EXTJS AngularJS JQuery JavaScript AJAX HTML CSS and JSON Good knowledge on TCPIP tunneling and port management on cloud environments installation of servers on cloud unix Good experience on production support and client interaction Experience in creating build scripts using Ant and Maven also have experience in Jenkins Strong TDD test driven development and continuous integration experience using JUnit Mock Framework Worked on Cucumber framework Implemented this application based on MVC Architecture using open source Struts Used UML to create class diagrams Sequence diagrams and State diagrams and implemented these diagrams in Microsoft Visio Developed front end web pages using Servlets 30 HTML JSP JavaScript Swing Responsible for presentation layers using JSP custom tags and JavaScript Developed ServerSide Validations using Struts Validation Framework Created a Transaction History Web Service using SOAP that is used for internal communication in the workflow process Used Core java concepts in application such as multithreaded programming Synchronization of threads used thread wait notify join methods Developed Data Access Objects for accessing Relational Database Designed test cases for unit testing with the help of JUnit Created database connection using JDBC classes for interacting with Oracle 9i database Environment Java 67 JSP Servlets 30 JDBC Swing Java Beans 30 JavaScript HTML Struts Oracle 9i Multithreading Data Access Objects SOAP UML Junit Education Bachelors Skills DYNAMODB CASSANDRA AMBARI MAPREDUCE OOZIE SQOOP HBASE KAFKA FLUME HADOOP MONGODB DATA ANALYSIS DATABASE MYSQL ORACLE PLSQL SQL C Git Hadoop Additional Information TECHNICAL SKILLS Hadoop Ecosystem Database Hadoop 27X Spark 210 Mapreduce Hive Spark SQL Cassandra 310 HBase 130 211 Sqoop 1997 Kafka 210 Oozie 313 MongoDB Oracle 12c11g8i MySQL 5x Yarn 0213 Pig 0160 Flume 170 Ambari PLSQL Programming Language AWS Java SQL JavaScript Scala R C EC2 S3 EMR Lambda Elastic Beanstalk ELB EBS DynamoDB ElastiCache SQS SNS SWF Testing CloudWatch CloudFormation IAM CloudTrail JUnit MRUnit Pytest ScalaTest Glacier Storage Gateway Scripting Language Web Framework Scala 2120 Unix Shell Html5 Xml CSS3 JavaScript jQuery HTML5 CSS3 J2EE Spring JSP SQL MVC Hibernate 30 Data Analysis Visualization Web Services Tableau 93 D3js SOAP Restful Environment CodeBuildDeployment WinSCP Putty SQL Developer Agile Git Svn Maven Jenkins Jira 64 Confluence Operating Systems Tools Mac OS Ubuntu Centos Windows Hive MongoDB Spark Tableau Cloudera Hue",
    "entities": [
        "Data Lake Developed Workflows",
        "TDD TestDriven Development",
        "Java Developer Axis Bank",
        "Oracle SQL Server",
        "AJAX",
        "Access Control Data Security",
        "HDFS Developed",
        "GUI",
        "Project Online Invoice System",
        "New York",
        "Relational",
        "RedHat",
        "Git Hadoop Additional Information TECHNICAL SKILLS Hadoop Ecosystem Database Hadoop",
        "BI",
        "HDFS",
        "Bloomberg Media",
        "HA",
        "JavaScript Developed ServerSide Validations",
        "Zookeeper 3410",
        "Developed Spark",
        "Jasper",
        "HTTP",
        "Spark SQL Extensive",
        "Oracle Maven Restful",
        "HDFS Experienced",
        "Maven Git",
        "EXTJS",
        "Ambari",
        "Nagios",
        "Hadoop",
        "Custom Expression Language EL",
        "SOAP",
        "NOSQL",
        "Zookeeper services Involved",
        "the World Company",
        "CHD",
        "JUnit",
        "Hive Developed",
        "User Defined Functions",
        "Developed Data Access Objects",
        "State",
        "HBase",
        "Avro",
        "JRXML",
        "Target",
        "ElastiCache",
        "HTML CSS JavaScript AJAX EXTJS",
        "Python",
        "SQL Server",
        "Puppet Implemented Hadoopbased",
        "SparkSQL",
        "Developed",
        "OpenStack Administered",
        "LDAP",
        "Oracle Relational",
        "Transform Stage and Store",
        "DR",
        "Utilized",
        "Spring MVC",
        "Oracle 11",
        "Scala Supported",
        "Confluence",
        "Present Project Description Fidelity Investment",
        "UML",
        "Meta",
        "Sequence",
        "Hive HiveQL Worked",
        "Zookeeper Agile Hadoop Developer",
        "Oracle 9i",
        "SOA",
        "iReport",
        "Data Analysis Visualization Web Services",
        "the Bloomberg Terminal",
        "Hadoop Involved",
        "JSP",
        "Using Hadoop Ecosystem",
        "Customer Assistance is Banking Application for Axis Bank",
        "Tableau Desktop",
        "Hive Warehouse Import",
        "Spark Streaming",
        "MRunit Installed Oozie",
        "Density Chart Created HBase",
        "DOM",
        "Flume Sqoop",
        "Project Explore Solutions Project Description IBM Watson",
        "HDP",
        "Created MapReduce",
        "SQLbased",
        "Design Development",
        "Jenkins Strong",
        "Database Design",
        "MVC",
        "Spark",
        "Struts Validation Framework Created",
        "Agile",
        "Data Visualization",
        "Zookeeper for Stream",
        "CDH",
        "API",
        "Flume Experienced",
        "Sqoop",
        "QA",
        "Project Management Environment Spark",
        "Storm",
        "Created",
        "Spark Core Spark",
        "AWS",
        "Oracle",
        "multinode",
        "JUnit Created",
        "Kafka 0101 Spark Streaming",
        "ELB",
        "CloudWatch CloudFormation IAM CloudTrail JUnit",
        "Databricks",
        "Hibernate ORM Created POJOData Objects",
        "java",
        "PUT Developed",
        "SQL",
        "Bigdata arena File Systems Data Management Analysis",
        "Oozie 420",
        "Spark RDD",
        "Amazon Web Services",
        "Used SOAP WebServices",
        "Chase",
        "the HDFS Involved",
        "Published New Services",
        "MVC Architecture",
        "Relational Database Systems",
        "Ant",
        "Object Oriented Analysis Design OOAD",
        "the United States",
        "Hive",
        "Dataframes",
        "HiveQL",
        "Used Spring",
        "Apache Sentry",
        "FTP",
        "Responsibilities Software Development LifeCycle",
        "Scheduling Oozie",
        "Wealth Management",
        "ETL",
        "Responsibilities Gathered",
        "Microsoft Visio Developed",
        "Working on Install",
        "PII",
        "India",
        "Apache Hadoop",
        "Maven",
        "Data Frames",
        "Performed",
        "Bloomberg",
        "Impala",
        "Spark SQL",
        "GET",
        "ANT",
        "Hibernate Annotations Used Hibernate",
        "UI",
        "Insight for Banking",
        "The Customer Assistance Application",
        "VLSI",
        "JUnit Mock Framework Worked",
        "HBase Experienced",
        "Jira Rally SharePoint",
        "Oozie 430",
        "Ganglia Experienced",
        "HDFS MapReduce Sqoop",
        "Created HBase",
        "Apache Hue Apache Hadoop",
        "J2EE Spring JSP SQL MVC",
        "Spark RDD Dataset API",
        "REST",
        "SharePoint",
        "PROD Installed Hortonworks",
        "Used Apachetiles",
        "MapReduce",
        "Application Software Development and Design Object Oriented Technical Documentation Software Testing",
        "IBM WebSphere Application Server",
        "Relational Database Designed",
        "UML Methodology",
        "HttpRequests Followed Agile Methodology",
        "NoSQL",
        "Tableau",
        "Controller Service DAO",
        "SpringWeb Flow",
        "TDD",
        "Application",
        "Software Development Life Cycle",
        "Spring Good",
        "Software RD Services",
        "POJO Objects",
        "Oracle SQL Server PostgreSQL Involved",
        "Followed Agile"
    ],
    "experience": "Experience and strong knowledge on implementation of Spark Core Spark Streaming Spark SQL Developed applications in Spark 210 using Scala 2120 to compare the performance of Spark with Hive Implemented POCs and developed pipeline using Kafka 0101 Spark Streaming and Spark SQL Extensive experience in Spark 210 transformations using Scala 2120 and Spark SQL for faster testing and processing of data files Hands on real time processing with Spark modules Spark RDD Dataset API using Scala 2120 Hands on experience of writing programs in MapReduce to analyze unstructured data Experience in importing and exporting data using Sqoop from HDFS to Relational Database Systems RDBMS and from RDBMS to HDFS Well versed Expert in writing Pig Latin scripts and HiveQL queries to process and analyze data Experience in Spark and indepth knowledge on SparkSQL RDDs Lazy transformation and actions Worked on Apache Kafka 0101 and Flume for handling megabytes of streaming data Transformed various formats of data like sequence File RC ORC Parquet JSON AVRO and experience in dealing with Compression techniques such as Gzip Snappy Deep knowledge in batch job scheduling workflow using Oozie 420 and zookeeper 3410 Understanding of Amazon Web Services stack and handson experience in using S3 EC2 and EMR Worked with different NOSQL databases such as Cassandra 310 HBbase 130 and MongoDB Experience with Relational databases including Oracle SQL Server and MySQL and experience in writing complex SQL queries PLSQL Stored Procedures Triggers sequences Well versed with the working of Data Visualization tools such as Tableau 93 D3js Developed Web applications using HTML5 CSS3 Bootstrap JavaScript JQuery AJAX Expertise in Core Java Data Structures Multithreading JDBC J2EE Algorithms Object Oriented Design OOD and Exception Handling and frameworks like Spring MVC and Hibernate Produced and consumed SOAP and RESTful Web Services and experience in developing Hadoop applications on Spark using Scala as a functional and objectoriented programming Experienced in writing ANT and Maven scripts to build and deploy Java applications Experienced in TDD TestDriven Development and SDLC methodologies such as AgileScrum Through knowledge of development environments such as Maven Git JIRA 64 Jenkins and Confluence Excellent understanding and knowledge of Hadoop architecture and various daemons such as Name Node Data Node Job Tracker Task Tracker Resource Manager and MapReduce programming paradigm Knowledge of Cyber Security concepts like cryptography Access Control Data Security in Linux Unix Expertise in Creating Hive InternalExternal TablesViews using shared Meta store writing scripts in HiveQL also data transformation file processing using Pig Latin Scripts Experience in Object Oriented Analysis Design OOAD and development of software using UML Methodology good knowledge of J2EE design patterns and Core Java design patterns Possess excellent presentation documentation communication skills detail oriented zeal to learn new technologies with a cooperative team focused attitude strong analytical and problemsolving skills Work Experience ScalaHadoop Developer Fidelity Investment Smithfield RI June 2018 to Present Project Description Fidelity Investment is a financial services firm involved in different sectors including Mutual fund Brokerage Retirement IRA and Wealth Management and in one of the leading companies in 410k plan and Stock Market activity The Distributed reporting project involves transferring from legacy system to the Hadoop environment for the power query using HDFS for the optimize query performance for the end customer using hive data warehouse They are using different tools for this project like Apache Hue Apache Hadoop Apache Spark HDFS Impala and few internal frameworks Responsibilities Experience in creating Hive tables to store the processed results in a tabular format optimizing Hive tables using optimization techniques like partitions and bucketing to provide better performance with HiveQL queries and creating custom user defined functions Developed reviewed and updated architecture and process documentation server diagrams requisition documents and other technical documents Involved in Agile methodology attended daily scrum meetings and sprint planning meetings Integrated visualizations into a Spark application using Databricks and visualization libraries ggplot Matplotlib Implemented different analytical algorithms using MapReduce programs to apply on top of HDFS data Installed managed and maintained a cluster of 60 nodes from POC to PROD Installed Hortonworks clusters and Hadoop ecosystem components through Ambari and command line interface Responsible for cluster maintenance commissioning and decommissioning data nodes cluster monitoring troubleshooting management and review data backups and Hadoop log files Skilled in Tableau Desktop for various types of data visualization reporting and analysis including Cross Map Scatter Plots Geographic Map Pie Charts and Bar Charts Page Trails and Density Chart Created HBase tables to store various data formats of data coming from MySQL Oracle Teradata Installed and configured Hive Pig Sqoop Flume and Oozie on the Hadoop cluster Configured Spark streaming to get ongoing information from the Kafka and stored the stream information to HDFS Developed data pipeline using Flume Sqoop to ingest business data and purchase histories into HDFS for analysis Utilized SparkSQL to extract and process data by parsing using Datasets or RDDs in Hive Context with transformations and actions map flat Map filter reduce reduceByKey Extended the capabilities of Data Frames using User Defined Functions in Python and Scala Designed and Implemented Sqoop incremental imports delta imports on tables without primary keys and dates from Oracle and appends directly into Hive Warehouse Import the data from different sources like HDFSHBase into Spark RDD and perform computations using Scala to generate the output response Writing a spark shell code to audit report to bring all the unique field under metadata Implemented HQL scripts for daily based data loading further aggregations Scheduling Oozie workflow and job to automate the run of Unix script daily as per the team requirement Working on clean up the old data and purge the database for having accurate data in the system Involved in building the ETL architecture and Target mapping to load data into Data Lake Developed Workflows with the help of Oozie to manage the flow of jobs and wrote Custom Expression Language EL functions for complex workflows Working on Install process monthly or by weekly new sets of report for new table or existing table and validating all the data coming from working directory to the HDFS Involved in the complete Software Development Life Cycle SDLC phases of the project as a part of Agile scrum methodology Loaded datasets from MySQL to HDFS and Hive respectively on daily basis Worked on Apache Hue as a central web admin User to Query on Hive or Impala and to Schedule job workflow Assisted in Installation and Configuration of Apache Hadoop clusters CHD and Hadoop tools for application development includes HDFS HUE YARN Sqoop Impala and Hive Used Cloudera Manager for continuous managing and monitoring the Hadoop cluster Environment Spark 230 Scala 2130 Sqoop 146 Impala 220 Oracle Hive 233 HDFS Spark SQL CHD 5161 HiveQL Hue 420 YARN 1123 Oozie 430 Agile Big Data Analytics SolutionsScala Developer Bloomberg LP New York NY May 2015 to May 2018 Project Description My Latest project was with Bloomberg which is a global firm which specializes in financial software media and data Company mainly involve in the financial software tools which provides the analytics and equality trading platform for data services and news Where in Bloomberg briefs its deliver news market data and commentary directly to inbox which give competitive advantage that s changing faster than ever Bloomberg Briefs is unique and provide downloadable reports to the subscriber and financial professional to get the actionable ideas and insights into a diverse range of markets and industries Specifically I was working with the Bloomberg Media division for the Bloomberg Terminal platform which caters to near half millions of corporate customers in more than 160 countries Instead of using third party application we were using the Hadoop ecosystem to manage the large scale of data with lowlatency which is processed daily on this platform Responsibilities Experience in Transform Stage and Store data using Spark which includes writing Spark applications in Scala Developed Spark core and Spark SQL scripts using Scala for faster data processing Developed a data pipeline using Kafka and Storm to store data into HDFS Developed Spark code using Scala and Spark SQL for faster testing and data processing Setting up and managing Kafka and Zookeeper for Stream processing Developed Kafka consumers API in Scala for consuming data from Kafka topics Configured deployed and maintained multinode Dev and Test Kafka Clusters Developed Spark scripts using Scala Spark SQL to access Hive tables in Spark for faster data processing Stored realtime data from Spark on HBase for future analysis Involved in implementing High Availability and automatic failover infrastructure to overcome single point of failure for Name node utilizing Zookeeper services Involved in analyzing data coming from various sources and creating Metafiles and control files to ingest the data in to the Data Lake Generate reports for analytics development through Zeppelin Developed Scala code with Spark Streaming for faster testing and processing of data Worked with Spark RDD and Dataframes for sessionization and other transformations Used Apache Avro to transform data between different format Streamed log files using Flume into HDFS and load into Hive tables to query data Created multiple Hive tables implemented partitioning dynamic partitioning and bucketing in Hive for efficient data access Performed daily adhoc data analysis and pulled data from Hadoop using Hive HiveQL Worked closely with team members managers and other teams in Agile Scrum environment Environment Kafka 0101 Spark 210 Scala 2120 Sqoop 146 Avro MySql5x HBase 130 Zeppelin Hive 211 HDFS Spark SQL Flume 170 HiveQL Zookeeper Agile Hadoop Developer IBM New York NY October 2012 to April 2015 Project Explore Solutions Project Description IBM Watson is a distinctively creating a business unit around the World Company headquartered in the United States where it specializes in the sector of Discovery Advisor Engagement Advisor and Explorer and building the application of business smart cities consumer applications and life in general Using Hadoop Ecosystem we have been developing predictive models to identify customer Insight for Banking by making use of realtime and historical data of bank user like mortgage credit history income source spending and reviews The project is about analysis of data by proper cleaning and normalization process that combine predictive and cognitive capabilities We need to collect data from various resources and enables dynamic solution behavioral segmentation to uncover actionable customer insights allowing banks to create personalized sales offerings and marketing campaigns Capability to store large unstructured data sets in NoSQL databases and using spark to analyze this data Responsibilities Worked on performance and optimization of existing algorithms in Hadoop using Spark context SparkSQL and Spark YARN using Scala Experience in installation upgrading configuration monitoring supporting and managing in Hadoop clusters using Cloudera CDH 4 CDH 5 Hortonworks HDP 2x and 3x on Ubuntu RedHat CentOS systems Worked on components of CDH and HDP including HDFS MapReduce Job tracker Task tracker Sqoop Zookeeper YARN Oozie Hive Hue Flume HBase Fair Scheduler Spark and Kafka Deployed Hadoop clusters on public and private cloud environments like AWS and OpenStack Administered the Linux systems to deploy Hadoop cluster and monitoring the cluster using Nagios and Ganglia Experienced in performing backup recovery failover and DR practices on multiple platforms Implemented Kerberos and LDAP authentication of all the services across Hadoop clusters using Apache Sentry Worked with highly transactional merchandise and investment in SQL databases with PCI HIPAA compliance involving data encryption with certificates and security keys at various levels Experienced in automating the provisioning processes and system resources using Puppet Implemented Hadoopbased solutions to store archives and backups from multiple sources Involved in importing and exporting data with Sqoop from RDBMS MySQL Oracle Teradata and used fast loaders and connectors and processed data for commercial analytics Built ingestion framework using flume for streaming logs and aggregating the data into HDFS Experienced in upgrading SQL Server software patches and service packs Practiced Agile scrum to provide operational support installation updates patches and version upgrades Experienced in collaborative platforms including Jira Rally SharePoint and Discovery Installed monitored and performance tuned standalone multinode clusters of Kafka Successfully loaded files to Hive and HDFS from MongoDB Cassandra and HBase Experienced in understanding and managing Hadoop log files with Flume Experienced on SQL DBA in HA and DDR like replication log shipping mirroring and clustering and database security and permissions Responsible for Importing and exporting data into HDFS and Hive using Sqoop from Oracle 11 g and MySQL Develop and maintain several batch jobs to run automatically depending on business requirements Responsible for building scalable distributed data solutions using Hadoop Involved in loading data from edge node to HDFS using shell scripting Implemented Partitioning Dynamic Partitioning Buckets in Hive Developed PIG scripts using Pig Latin and worked on tuning the performance of Pig queries Involved in managing and reviewing Hadoop log files Created HBase tables to store variable data formats of PII data coming from different portfolios Implemented test scripts to support test driven development and continuous integration Exported the analyzed data to the relational databases like MySQL using Sqoop for visualization and to generate reports for the BI team Created MapReduce programs using Java API that filter unnecessary records and find out unique records based on different criteria Experienced in implementing POCs to migrate iterative map reduce programs into Spark transformations actions using Scala Supported in setting up QA environment and updating configurations for implementing scripts with Pig and Sqoop Worked in Test Driven Deployment environment and used Confluence for documentation Performed unit testing using MRunit Installed Oozie workflow engine to run multiple Hive and pig jobs Used Jira for project tracking Bug tracking and Project Management Environment Spark 210 Apache Hadoop MapReduce HDFS Hive 211 Java Pig 0160 Sqoop 146 MRunit Oozie 313 HBase130 TDD MySQL5x Oracle 11 g Java Developer Axis Bank Gujarat IN October 2010 to September 2012 Project Customer Assistance Axis is one of the leading global banks Customer Assistance is Banking Application for Axis Bank The Customer Assistance Application plays a crucial role in dailybanking operations This Application makes its users to create bank accounts transfer money update contact history and update checking and saving accounts credit card approvals of the customers Responsibilities Gathered user requirements analyzed and wrote functional and technical specifications we use SharePoint to maintain all of our design docs Followed Agile software methodology for software development 3 week Sprint Worked on one of the UI based application and client focus more on look and feel of the UI We use lots of customs components to design the UI Chase standards and HTML CSS JavaScript AJAX EXTJS is being used intensively Used Spring MVC framework on the server side for creating RESTFul web services by giving JSON out and modifying the DOM object on UI by making HTTP calls and used GET and PUT Developed multiple Controller Service DAO classes to interact with data layer and developed Entity classes based on the table structure Involved in core integration to pick the file from FTP location and bring them into our staging tables and did all the validation on the java side Created multiple midtier services to interact with multiple validations and worked on entitlements services to do user validations Interaction and also worked on applying security systems Worked with Oracle database and used Hibernate ORM Created POJOData Objects in midtier service Hands on experience on implementing lazy loading first and second level of caching Leading onshore offshore model Coordinating with Offshore team in India and being flexible on gathering updated from team Work closely with Database team and testing team Worked on Jasper reports using iReport tool and integrated that JRXML into spring frame work Wrote SQL commands and Stored Procedures to retrieve data from Oracle database Worked to plug this procedure in Java classes Used Spring webflow for MVC pattern Used Apachetiles for JSP page fragments for various flows Used SOAP WebServices for sending data to Published New Services for given WSDL file GUI developing using custom JSTL tag library and used AJAX calls for client side HttpRequests Followed Agile Methodology and regular SCRUM meetings Involved in creating the Hibernate 30 POJO Objects and mapped using Hibernate Annotations Used Hibernate objectrelationalmapping ORM solution technique of mapping data representation from MVC model to Oracle Relational data model with a SQLbased schema Provide support to the users for all the service components and help them in production issues Involved in designing test plans test cases and overall Unit testing of the system using JUnit Environment Java 17 JSP Eclipse JUnit Hibernate 30 Oracle Maven Restful Git Scrum SpringWeb Flow SQL Java Developer eInfochip Gujarat IN July 2009 to September 2010 Project Online Invoice System OVS eInfochips is a Product Engineering and Software RD Services firm based that offers solutions in software hardware and VLSI services The company provides a range of products and services spanning multiple industries including silicon engineering embedded systems software engineering extended services Where I was involved in software engineering which offers custom software application development and maintenance services with focus on shorter development cycles enhanced features better user experience We also help our clients to migrate applications to open source technologies while providing a benefit analysis Responsibilities Software Development LifeCycle SDLC phases of design development implementation deployment testing and maintenance as per quality standards using Agile Scrum and waterfall methodologies Good Experience in Application Software Development and Design Object Oriented Technical Documentation Software Testing and Debugging Excellent implementation knowledge of JDK 1617 and 18 Spring Hibernate RESTFUL Web Services AOP Struts JDBC EJB Consumed and Exposed both REST and SOAP based webservices very good experience with SOA model Experience in RDBMS using MySQL Oracle SQL Server PostgreSQL Involved in configuring deploying applications on IBM WebSphere Application Server BEA WebLogic Application Server Apache Tomcat on UNIX Linux and Windows platforms Extensive experience in Design Development and implementation of ModelViewController MVC using Spring Good experience in Database Design writing stored procedure functions triggers SQL queries Developed the code for front end using EXTJS AngularJS JQuery JavaScript AJAX HTML CSS and JSON Good knowledge on TCPIP tunneling and port management on cloud environments installation of servers on cloud unix Good experience on production support and client interaction Experience in creating build scripts using Ant and Maven also have experience in Jenkins Strong TDD test driven development and continuous integration experience using JUnit Mock Framework Worked on Cucumber framework Implemented this application based on MVC Architecture using open source Struts Used UML to create class diagrams Sequence diagrams and State diagrams and implemented these diagrams in Microsoft Visio Developed front end web pages using Servlets 30 HTML JSP JavaScript Swing Responsible for presentation layers using JSP custom tags and JavaScript Developed ServerSide Validations using Struts Validation Framework Created a Transaction History Web Service using SOAP that is used for internal communication in the workflow process Used Core java concepts in application such as multithreaded programming Synchronization of threads used thread wait notify join methods Developed Data Access Objects for accessing Relational Database Designed test cases for unit testing with the help of JUnit Created database connection using JDBC classes for interacting with Oracle 9i database Environment Java 67 JSP Servlets 30 JDBC Swing Java Beans 30 JavaScript HTML Struts Oracle 9i Multithreading Data Access Objects SOAP UML Junit Education Bachelors Skills DYNAMODB CASSANDRA AMBARI MAPREDUCE OOZIE SQOOP HBASE KAFKA FLUME HADOOP MONGODB DATA ANALYSIS DATABASE MYSQL ORACLE PLSQL SQL C Git Hadoop Additional Information TECHNICAL SKILLS Hadoop Ecosystem Database Hadoop 27X Spark 210 Mapreduce Hive Spark SQL Cassandra 310 HBase 130 211 Sqoop 1997 Kafka 210 Oozie 313 MongoDB Oracle 12c11g8i MySQL 5x Yarn 0213 Pig 0160 Flume 170 Ambari PLSQL Programming Language AWS Java SQL JavaScript Scala R C EC2 S3 EMR Lambda Elastic Beanstalk ELB EBS DynamoDB ElastiCache SQS SNS SWF Testing CloudWatch CloudFormation IAM CloudTrail JUnit MRUnit Pytest ScalaTest Glacier Storage Gateway Scripting Language Web Framework Scala 2120 Unix Shell Html5 Xml CSS3 JavaScript jQuery HTML5 CSS3 J2EE Spring JSP SQL MVC Hibernate 30 Data Analysis Visualization Web Services Tableau 93 D3js SOAP Restful Environment CodeBuildDeployment WinSCP Putty SQL Developer Agile Git Svn Maven Jenkins Jira 64 Confluence Operating Systems Tools Mac OS Ubuntu Centos Windows Hive MongoDB Spark Tableau Cloudera Hue",
    "extracted_keywords": [
        "ScalaHadoop",
        "Developer",
        "ScalaHadoop",
        "lDeveloperspan",
        "ScalaHadoop",
        "Developer",
        "Fidelity",
        "Investment",
        "Smithfield",
        "RI",
        "Experienced",
        "Hadoop",
        "Developer",
        "years",
        "IT",
        "experience",
        "background",
        "Bigdata",
        "arena",
        "File",
        "Systems",
        "Data",
        "Management",
        "Analysis",
        "Java",
        "enterprise",
        "application",
        "Java",
        "J2EE",
        "technologies",
        "Expertise",
        "configuring",
        "testing",
        "Apache",
        "Hadoop",
        "framework",
        "27x",
        "ecosystem",
        "components",
        "HDFS",
        "MapReduce",
        "Sqoop",
        "Flume",
        "Hive",
        "Pig",
        "Spark",
        "Scala",
        "Kafka",
        "Yarn",
        "Oozie",
        "Zookeeper",
        "Experience",
        "knowledge",
        "implementation",
        "Spark",
        "Core",
        "Spark",
        "Streaming",
        "Spark",
        "SQL",
        "applications",
        "Spark",
        "Scala",
        "performance",
        "Spark",
        "Hive",
        "POCs",
        "pipeline",
        "Kafka",
        "Spark",
        "Streaming",
        "Spark",
        "SQL",
        "experience",
        "Spark",
        "transformations",
        "Scala",
        "Spark",
        "SQL",
        "testing",
        "processing",
        "data",
        "Hands",
        "time",
        "processing",
        "Spark",
        "modules",
        "Spark",
        "Dataset",
        "API",
        "Scala",
        "Hands",
        "experience",
        "programs",
        "MapReduce",
        "data",
        "Experience",
        "data",
        "Sqoop",
        "HDFS",
        "Relational",
        "Database",
        "Systems",
        "RDBMS",
        "RDBMS",
        "HDFS",
        "Expert",
        "Pig",
        "Latin",
        "scripts",
        "queries",
        "data",
        "Experience",
        "Spark",
        "knowledge",
        "SparkSQL",
        "RDDs",
        "Lazy",
        "transformation",
        "actions",
        "Apache",
        "Kafka",
        "Flume",
        "megabytes",
        "data",
        "formats",
        "data",
        "sequence",
        "File",
        "RC",
        "ORC",
        "Parquet",
        "JSON",
        "AVRO",
        "experience",
        "Compression",
        "techniques",
        "Gzip",
        "Snappy",
        "knowledge",
        "batch",
        "job",
        "scheduling",
        "workflow",
        "Oozie",
        "zookeeper",
        "Understanding",
        "Amazon",
        "Web",
        "Services",
        "stack",
        "handson",
        "experience",
        "S3",
        "EC2",
        "EMR",
        "NOSQL",
        "databases",
        "Cassandra",
        "HBbase",
        "Experience",
        "databases",
        "Oracle",
        "SQL",
        "Server",
        "MySQL",
        "experience",
        "SQL",
        "PLSQL",
        "Stored",
        "Procedures",
        "Triggers",
        "sequences",
        "working",
        "Data",
        "Visualization",
        "tools",
        "Tableau",
        "Web",
        "applications",
        "HTML5",
        "CSS3",
        "Bootstrap",
        "JavaScript",
        "JQuery",
        "AJAX",
        "Expertise",
        "Core",
        "Java",
        "Data",
        "Structures",
        "Multithreading",
        "JDBC",
        "J2EE",
        "Algorithms",
        "Object",
        "Oriented",
        "Design",
        "OOD",
        "Exception",
        "Handling",
        "frameworks",
        "Spring",
        "MVC",
        "Hibernate",
        "Produced",
        "SOAP",
        "Web",
        "Services",
        "experience",
        "Hadoop",
        "applications",
        "Spark",
        "Scala",
        "programming",
        "ANT",
        "Maven",
        "scripts",
        "Java",
        "applications",
        "TDD",
        "TestDriven",
        "Development",
        "SDLC",
        "methodologies",
        "AgileScrum",
        "knowledge",
        "development",
        "environments",
        "Maven",
        "Git",
        "JIRA",
        "Jenkins",
        "Confluence",
        "Excellent",
        "understanding",
        "knowledge",
        "Hadoop",
        "architecture",
        "daemons",
        "Name",
        "Node",
        "Data",
        "Node",
        "Job",
        "Tracker",
        "Task",
        "Tracker",
        "Resource",
        "Manager",
        "MapReduce",
        "programming",
        "paradigm",
        "Knowledge",
        "Cyber",
        "Security",
        "concepts",
        "cryptography",
        "Access",
        "Control",
        "Data",
        "Security",
        "Linux",
        "Unix",
        "Expertise",
        "Creating",
        "Hive",
        "InternalExternal",
        "TablesViews",
        "Meta",
        "store",
        "scripts",
        "HiveQL",
        "transformation",
        "file",
        "processing",
        "Pig",
        "Latin",
        "Scripts",
        "Experience",
        "Object",
        "Oriented",
        "Analysis",
        "Design",
        "OOAD",
        "development",
        "software",
        "UML",
        "Methodology",
        "knowledge",
        "J2EE",
        "design",
        "patterns",
        "Core",
        "Java",
        "design",
        "patterns",
        "presentation",
        "documentation",
        "communication",
        "skills",
        "detail",
        "zeal",
        "technologies",
        "team",
        "attitude",
        "skills",
        "Work",
        "Experience",
        "ScalaHadoop",
        "Developer",
        "Fidelity",
        "Investment",
        "Smithfield",
        "RI",
        "June",
        "Present",
        "Project",
        "Description",
        "Fidelity",
        "Investment",
        "services",
        "firm",
        "sectors",
        "fund",
        "Brokerage",
        "Retirement",
        "IRA",
        "Wealth",
        "Management",
        "companies",
        "410k",
        "plan",
        "Stock",
        "Market",
        "activity",
        "reporting",
        "project",
        "legacy",
        "system",
        "Hadoop",
        "environment",
        "power",
        "query",
        "HDFS",
        "optimize",
        "query",
        "performance",
        "end",
        "customer",
        "hive",
        "data",
        "warehouse",
        "tools",
        "project",
        "Apache",
        "Hue",
        "Apache",
        "Hadoop",
        "Apache",
        "Spark",
        "HDFS",
        "Impala",
        "frameworks",
        "Responsibilities",
        "Experience",
        "Hive",
        "tables",
        "results",
        "format",
        "Hive",
        "tables",
        "optimization",
        "techniques",
        "partitions",
        "bucketing",
        "performance",
        "queries",
        "custom",
        "user",
        "functions",
        "architecture",
        "process",
        "documentation",
        "server",
        "diagrams",
        "requisition",
        "documents",
        "documents",
        "methodology",
        "scrum",
        "meetings",
        "sprint",
        "planning",
        "meetings",
        "visualizations",
        "Spark",
        "application",
        "Databricks",
        "visualization",
        "libraries",
        "ggplot",
        "Matplotlib",
        "algorithms",
        "MapReduce",
        "programs",
        "top",
        "HDFS",
        "data",
        "cluster",
        "nodes",
        "POC",
        "PROD",
        "Installed",
        "Hortonworks",
        "clusters",
        "Hadoop",
        "ecosystem",
        "components",
        "Ambari",
        "command",
        "line",
        "interface",
        "cluster",
        "maintenance",
        "data",
        "nodes",
        "cluster",
        "monitoring",
        "troubleshooting",
        "management",
        "data",
        "backups",
        "Hadoop",
        "log",
        "Tableau",
        "Desktop",
        "types",
        "data",
        "visualization",
        "reporting",
        "analysis",
        "Cross",
        "Map",
        "Scatter",
        "Plots",
        "Geographic",
        "Map",
        "Pie",
        "Charts",
        "Bar",
        "Charts",
        "Page",
        "Trails",
        "Density",
        "Chart",
        "Created",
        "HBase",
        "data",
        "formats",
        "data",
        "MySQL",
        "Oracle",
        "Teradata",
        "Installed",
        "Hive",
        "Pig",
        "Sqoop",
        "Flume",
        "Oozie",
        "Hadoop",
        "cluster",
        "Configured",
        "Spark",
        "streaming",
        "information",
        "Kafka",
        "stream",
        "information",
        "HDFS",
        "data",
        "pipeline",
        "Flume",
        "Sqoop",
        "business",
        "data",
        "purchase",
        "histories",
        "HDFS",
        "analysis",
        "SparkSQL",
        "process",
        "data",
        "Datasets",
        "RDDs",
        "Hive",
        "Context",
        "transformations",
        "actions",
        "Map",
        "filter",
        "reduceByKey",
        "capabilities",
        "Data",
        "Frames",
        "User",
        "Defined",
        "Functions",
        "Python",
        "Scala",
        "Sqoop",
        "imports",
        "delta",
        "imports",
        "tables",
        "keys",
        "dates",
        "Oracle",
        "appends",
        "Hive",
        "Warehouse",
        "Import",
        "data",
        "sources",
        "HDFSHBase",
        "Spark",
        "RDD",
        "computations",
        "Scala",
        "output",
        "response",
        "spark",
        "shell",
        "code",
        "report",
        "field",
        "metadata",
        "HQL",
        "scripts",
        "data",
        "aggregations",
        "Oozie",
        "workflow",
        "job",
        "run",
        "Unix",
        "script",
        "team",
        "requirement",
        "data",
        "database",
        "data",
        "system",
        "ETL",
        "architecture",
        "Target",
        "mapping",
        "data",
        "Data",
        "Lake",
        "Developed",
        "Workflows",
        "help",
        "Oozie",
        "flow",
        "jobs",
        "Custom",
        "Expression",
        "Language",
        "EL",
        "functions",
        "workflows",
        "Install",
        "process",
        "sets",
        "report",
        "table",
        "table",
        "data",
        "directory",
        "HDFS",
        "Software",
        "Development",
        "Life",
        "Cycle",
        "SDLC",
        "phases",
        "project",
        "part",
        "scrum",
        "methodology",
        "datasets",
        "MySQL",
        "HDFS",
        "Hive",
        "basis",
        "Apache",
        "Hue",
        "web",
        "admin",
        "User",
        "Query",
        "Hive",
        "Impala",
        "Schedule",
        "job",
        "workflow",
        "Assisted",
        "Installation",
        "Configuration",
        "Apache",
        "Hadoop",
        "CHD",
        "Hadoop",
        "tools",
        "application",
        "development",
        "HDFS",
        "HUE",
        "YARN",
        "Sqoop",
        "Impala",
        "Hive",
        "Used",
        "Cloudera",
        "Manager",
        "managing",
        "Hadoop",
        "cluster",
        "Environment",
        "Spark",
        "Scala",
        "Sqoop",
        "Impala",
        "Oracle",
        "Hive",
        "HDFS",
        "Spark",
        "SQL",
        "CHD",
        "HiveQL",
        "Hue",
        "YARN",
        "Oozie",
        "Agile",
        "Big",
        "Data",
        "Analytics",
        "SolutionsScala",
        "Developer",
        "Bloomberg",
        "LP",
        "New",
        "York",
        "NY",
        "May",
        "May",
        "Project",
        "Description",
        "project",
        "Bloomberg",
        "firm",
        "software",
        "media",
        "data",
        "Company",
        "software",
        "tools",
        "analytics",
        "equality",
        "trading",
        "platform",
        "data",
        "services",
        "news",
        "Bloomberg",
        "news",
        "market",
        "data",
        "commentary",
        "inbox",
        "advantage",
        "Bloomberg",
        "Briefs",
        "reports",
        "subscriber",
        "professional",
        "ideas",
        "insights",
        "range",
        "markets",
        "industries",
        "Bloomberg",
        "Media",
        "division",
        "Bloomberg",
        "Terminal",
        "platform",
        "half",
        "millions",
        "customers",
        "countries",
        "party",
        "application",
        "Hadoop",
        "ecosystem",
        "scale",
        "data",
        "lowlatency",
        "platform",
        "Responsibilities",
        "Experience",
        "Transform",
        "Stage",
        "Store",
        "data",
        "Spark",
        "Spark",
        "applications",
        "Scala",
        "Developed",
        "Spark",
        "core",
        "Spark",
        "SQL",
        "scripts",
        "Scala",
        "data",
        "processing",
        "data",
        "pipeline",
        "Kafka",
        "Storm",
        "data",
        "HDFS",
        "Developed",
        "Spark",
        "code",
        "Scala",
        "Spark",
        "SQL",
        "testing",
        "data",
        "processing",
        "Kafka",
        "Zookeeper",
        "Stream",
        "processing",
        "Kafka",
        "consumers",
        "API",
        "Scala",
        "data",
        "Kafka",
        "topics",
        "multinode",
        "Dev",
        "Test",
        "Kafka",
        "Clusters",
        "Spark",
        "scripts",
        "Scala",
        "Spark",
        "SQL",
        "Hive",
        "tables",
        "Spark",
        "data",
        "data",
        "Spark",
        "HBase",
        "analysis",
        "Availability",
        "infrastructure",
        "point",
        "failure",
        "Name",
        "node",
        "Zookeeper",
        "services",
        "data",
        "sources",
        "Metafiles",
        "files",
        "data",
        "Data",
        "Lake",
        "Generate",
        "analytics",
        "development",
        "Zeppelin",
        "Developed",
        "Scala",
        "code",
        "Spark",
        "Streaming",
        "testing",
        "processing",
        "data",
        "Spark",
        "RDD",
        "Dataframes",
        "sessionization",
        "transformations",
        "Apache",
        "Avro",
        "data",
        "format",
        "Streamed",
        "files",
        "Flume",
        "HDFS",
        "load",
        "Hive",
        "tables",
        "data",
        "Hive",
        "tables",
        "partitioning",
        "Hive",
        "data",
        "access",
        "data",
        "analysis",
        "data",
        "Hadoop",
        "Hive",
        "HiveQL",
        "team",
        "members",
        "managers",
        "teams",
        "Agile",
        "Scrum",
        "environment",
        "Environment",
        "Kafka",
        "Spark",
        "Scala",
        "Sqoop",
        "Avro",
        "MySql5x",
        "HBase",
        "Zeppelin",
        "Hive",
        "HDFS",
        "Spark",
        "SQL",
        "Flume",
        "Zookeeper",
        "Agile",
        "Hadoop",
        "Developer",
        "IBM",
        "New",
        "York",
        "NY",
        "October",
        "April",
        "Project",
        "Explore",
        "Solutions",
        "Project",
        "Description",
        "IBM",
        "Watson",
        "business",
        "unit",
        "World",
        "Company",
        "United",
        "States",
        "sector",
        "Discovery",
        "Advisor",
        "Engagement",
        "Advisor",
        "Explorer",
        "application",
        "business",
        "cities",
        "consumer",
        "applications",
        "life",
        "Using",
        "Hadoop",
        "Ecosystem",
        "models",
        "customer",
        "Insight",
        "Banking",
        "use",
        "data",
        "bank",
        "user",
        "mortgage",
        "credit",
        "history",
        "income",
        "source",
        "spending",
        "project",
        "analysis",
        "data",
        "cleaning",
        "normalization",
        "process",
        "capabilities",
        "data",
        "resources",
        "solution",
        "segmentation",
        "customer",
        "insights",
        "banks",
        "sales",
        "offerings",
        "marketing",
        "campaigns",
        "Capability",
        "data",
        "sets",
        "NoSQL",
        "spark",
        "data",
        "Responsibilities",
        "performance",
        "optimization",
        "algorithms",
        "Hadoop",
        "Spark",
        "context",
        "SparkSQL",
        "Spark",
        "YARN",
        "Scala",
        "Experience",
        "installation",
        "configuration",
        "monitoring",
        "Hadoop",
        "clusters",
        "Cloudera",
        "CDH",
        "CDH",
        "Hortonworks",
        "HDP",
        "Ubuntu",
        "RedHat",
        "CentOS",
        "systems",
        "components",
        "CDH",
        "HDP",
        "HDFS",
        "MapReduce",
        "Job",
        "tracker",
        "Task",
        "tracker",
        "Sqoop",
        "Zookeeper",
        "YARN",
        "Oozie",
        "Hive",
        "Hue",
        "Flume",
        "HBase",
        "Fair",
        "Scheduler",
        "Spark",
        "Kafka",
        "Deployed",
        "Hadoop",
        "clusters",
        "cloud",
        "environments",
        "AWS",
        "OpenStack",
        "Linux",
        "systems",
        "Hadoop",
        "cluster",
        "cluster",
        "Nagios",
        "Ganglia",
        "recovery",
        "failover",
        "DR",
        "practices",
        "platforms",
        "Kerberos",
        "LDAP",
        "authentication",
        "services",
        "Hadoop",
        "clusters",
        "Apache",
        "Sentry",
        "merchandise",
        "investment",
        "SQL",
        "PCI",
        "HIPAA",
        "compliance",
        "data",
        "encryption",
        "certificates",
        "security",
        "keys",
        "levels",
        "provisioning",
        "processes",
        "system",
        "resources",
        "Puppet",
        "solutions",
        "archives",
        "backups",
        "sources",
        "data",
        "Sqoop",
        "RDBMS",
        "MySQL",
        "Oracle",
        "Teradata",
        "loaders",
        "connectors",
        "data",
        "analytics",
        "ingestion",
        "framework",
        "flume",
        "logs",
        "data",
        "HDFS",
        "SQL",
        "Server",
        "software",
        "patches",
        "service",
        "packs",
        "Agile",
        "scrum",
        "support",
        "installation",
        "updates",
        "version",
        "upgrades",
        "platforms",
        "Jira",
        "Rally",
        "SharePoint",
        "Discovery",
        "Installed",
        "performance",
        "multinode",
        "clusters",
        "Kafka",
        "files",
        "Hive",
        "HDFS",
        "Cassandra",
        "HBase",
        "understanding",
        "Hadoop",
        "log",
        "files",
        "Flume",
        "SQL",
        "DBA",
        "HA",
        "DDR",
        "replication",
        "log",
        "shipping",
        "mirroring",
        "clustering",
        "database",
        "security",
        "permissions",
        "data",
        "HDFS",
        "Hive",
        "Sqoop",
        "Oracle",
        "g",
        "MySQL",
        "Develop",
        "batch",
        "jobs",
        "business",
        "requirements",
        "data",
        "solutions",
        "Hadoop",
        "loading",
        "data",
        "edge",
        "node",
        "HDFS",
        "shell",
        "scripting",
        "Partitioning",
        "Partitioning",
        "Buckets",
        "Hive",
        "PIG",
        "scripts",
        "Pig",
        "Latin",
        "performance",
        "Pig",
        "queries",
        "Hadoop",
        "log",
        "files",
        "Created",
        "HBase",
        "data",
        "formats",
        "PII",
        "data",
        "portfolios",
        "test",
        "scripts",
        "test",
        "development",
        "integration",
        "data",
        "databases",
        "MySQL",
        "Sqoop",
        "visualization",
        "reports",
        "BI",
        "team",
        "MapReduce",
        "programs",
        "Java",
        "API",
        "records",
        "records",
        "criteria",
        "POCs",
        "map",
        "programs",
        "Spark",
        "transformations",
        "actions",
        "Scala",
        "QA",
        "environment",
        "configurations",
        "scripts",
        "Pig",
        "Sqoop",
        "Worked",
        "Test",
        "Driven",
        "Deployment",
        "environment",
        "Confluence",
        "documentation",
        "Performed",
        "unit",
        "testing",
        "MRunit",
        "Installed",
        "Oozie",
        "workflow",
        "engine",
        "Hive",
        "pig",
        "jobs",
        "Jira",
        "project",
        "Bug",
        "tracking",
        "Project",
        "Management",
        "Environment",
        "Spark",
        "Apache",
        "Hadoop",
        "MapReduce",
        "HDFS",
        "Hive",
        "Java",
        "Pig",
        "Sqoop",
        "MRunit",
        "Oozie",
        "HBase130",
        "TDD",
        "MySQL5x",
        "Oracle",
        "g",
        "Java",
        "Developer",
        "Axis",
        "Bank",
        "Gujarat",
        "October",
        "September",
        "Project",
        "Customer",
        "Assistance",
        "Axis",
        "banks",
        "Customer",
        "Assistance",
        "Banking",
        "Application",
        "Axis",
        "Bank",
        "Customer",
        "Assistance",
        "Application",
        "role",
        "operations",
        "Application",
        "users",
        "bank",
        "accounts",
        "money",
        "update",
        "contact",
        "history",
        "checking",
        "accounts",
        "credit",
        "card",
        "approvals",
        "customers",
        "Responsibilities",
        "user",
        "requirements",
        "specifications",
        "SharePoint",
        "design",
        "docs",
        "software",
        "methodology",
        "software",
        "development",
        "week",
        "Sprint",
        "UI",
        "application",
        "client",
        "UI",
        "lots",
        "customs",
        "components",
        "UI",
        "Chase",
        "standards",
        "HTML",
        "CSS",
        "JavaScript",
        "AJAX",
        "EXTJS",
        "Spring",
        "MVC",
        "framework",
        "server",
        "side",
        "RESTFul",
        "web",
        "services",
        "JSON",
        "DOM",
        "object",
        "UI",
        "HTTP",
        "calls",
        "GET",
        "Controller",
        "Service",
        "DAO",
        "classes",
        "data",
        "layer",
        "Entity",
        "classes",
        "table",
        "structure",
        "integration",
        "file",
        "FTP",
        "location",
        "staging",
        "tables",
        "validation",
        "side",
        "services",
        "validations",
        "entitlements",
        "services",
        "user",
        "validations",
        "Interaction",
        "security",
        "systems",
        "Oracle",
        "database",
        "Hibernate",
        "ORM",
        "POJOData",
        "Objects",
        "service",
        "Hands",
        "experience",
        "loading",
        "level",
        "model",
        "Offshore",
        "team",
        "India",
        "team",
        "Database",
        "team",
        "testing",
        "team",
        "Jasper",
        "reports",
        "iReport",
        "tool",
        "JRXML",
        "spring",
        "frame",
        "work",
        "Wrote",
        "SQL",
        "commands",
        "Procedures",
        "data",
        "Oracle",
        "database",
        "procedure",
        "Java",
        "classes",
        "Spring",
        "webflow",
        "MVC",
        "pattern",
        "Apachetiles",
        "JSP",
        "page",
        "fragments",
        "flows",
        "SOAP",
        "WebServices",
        "data",
        "New",
        "Services",
        "WSDL",
        "file",
        "GUI",
        "custom",
        "JSTL",
        "tag",
        "library",
        "AJAX",
        "calls",
        "client",
        "side",
        "HttpRequests",
        "Agile",
        "Methodology",
        "SCRUM",
        "meetings",
        "Hibernate",
        "POJO",
        "Objects",
        "Hibernate",
        "Annotations",
        "Hibernate",
        "ORM",
        "solution",
        "technique",
        "mapping",
        "data",
        "representation",
        "MVC",
        "model",
        "Oracle",
        "Relational",
        "data",
        "model",
        "schema",
        "support",
        "users",
        "service",
        "components",
        "production",
        "issues",
        "test",
        "plans",
        "test",
        "cases",
        "Unit",
        "testing",
        "system",
        "JUnit",
        "Environment",
        "Java",
        "JSP",
        "Eclipse",
        "JUnit",
        "Hibernate",
        "Oracle",
        "Maven",
        "Restful",
        "Git",
        "Scrum",
        "SpringWeb",
        "Flow",
        "SQL",
        "Java",
        "Developer",
        "eInfochip",
        "Gujarat",
        "July",
        "September",
        "Project",
        "Online",
        "Invoice",
        "System",
        "OVS",
        "eInfochips",
        "Product",
        "Engineering",
        "Software",
        "RD",
        "Services",
        "firm",
        "solutions",
        "software",
        "hardware",
        "VLSI",
        "services",
        "company",
        "range",
        "products",
        "services",
        "industries",
        "silicon",
        "engineering",
        "systems",
        "software",
        "engineering",
        "services",
        "software",
        "engineering",
        "custom",
        "software",
        "application",
        "development",
        "maintenance",
        "services",
        "focus",
        "development",
        "cycles",
        "features",
        "user",
        "experience",
        "clients",
        "applications",
        "source",
        "technologies",
        "benefit",
        "analysis",
        "Responsibilities",
        "Software",
        "Development",
        "LifeCycle",
        "SDLC",
        "phases",
        "design",
        "development",
        "implementation",
        "deployment",
        "testing",
        "maintenance",
        "quality",
        "standards",
        "Agile",
        "Scrum",
        "waterfall",
        "methodologies",
        "Good",
        "Experience",
        "Application",
        "Software",
        "Development",
        "Design",
        "Object",
        "Oriented",
        "Technical",
        "Documentation",
        "Software",
        "Testing",
        "Excellent",
        "implementation",
        "knowledge",
        "JDK",
        "Spring",
        "Hibernate",
        "RESTFUL",
        "Web",
        "Services",
        "AOP",
        "Struts",
        "JDBC",
        "EJB",
        "Consumed",
        "REST",
        "SOAP",
        "webservices",
        "experience",
        "SOA",
        "model",
        "Experience",
        "RDBMS",
        "MySQL",
        "Oracle",
        "SQL",
        "Server",
        "PostgreSQL",
        "applications",
        "IBM",
        "WebSphere",
        "Application",
        "Server",
        "BEA",
        "WebLogic",
        "Application",
        "Server",
        "Apache",
        "Tomcat",
        "UNIX",
        "Linux",
        "Windows",
        "experience",
        "Design",
        "Development",
        "implementation",
        "ModelViewController",
        "MVC",
        "Spring",
        "experience",
        "Database",
        "Design",
        "procedure",
        "functions",
        "SQL",
        "code",
        "end",
        "EXTJS",
        "JQuery",
        "JavaScript",
        "AJAX",
        "HTML",
        "CSS",
        "knowledge",
        "TCPIP",
        "tunneling",
        "port",
        "management",
        "cloud",
        "environments",
        "installation",
        "servers",
        "unix",
        "experience",
        "production",
        "support",
        "client",
        "interaction",
        "Experience",
        "build",
        "scripts",
        "Ant",
        "Maven",
        "experience",
        "Jenkins",
        "Strong",
        "TDD",
        "test",
        "development",
        "integration",
        "experience",
        "JUnit",
        "Mock",
        "Framework",
        "Cucumber",
        "framework",
        "application",
        "MVC",
        "Architecture",
        "source",
        "Struts",
        "UML",
        "class",
        "diagrams",
        "Sequence",
        "diagrams",
        "State",
        "diagrams",
        "diagrams",
        "Microsoft",
        "Visio",
        "end",
        "web",
        "pages",
        "Servlets",
        "HTML",
        "JSP",
        "JavaScript",
        "Swing",
        "Responsible",
        "presentation",
        "layers",
        "JSP",
        "custom",
        "tags",
        "JavaScript",
        "Developed",
        "ServerSide",
        "Validations",
        "Struts",
        "Validation",
        "Framework",
        "Transaction",
        "History",
        "Web",
        "Service",
        "SOAP",
        "communication",
        "workflow",
        "process",
        "Core",
        "concepts",
        "application",
        "programming",
        "Synchronization",
        "threads",
        "thread",
        "join",
        "methods",
        "Developed",
        "Data",
        "Access",
        "Relational",
        "Database",
        "test",
        "cases",
        "unit",
        "help",
        "JUnit",
        "database",
        "connection",
        "JDBC",
        "classes",
        "Oracle",
        "9i",
        "database",
        "Environment",
        "Java",
        "JSP",
        "Servlets",
        "JDBC",
        "Swing",
        "Java",
        "Beans",
        "JavaScript",
        "HTML",
        "Struts",
        "Oracle",
        "9i",
        "Multithreading",
        "Data",
        "Access",
        "SOAP",
        "UML",
        "Junit",
        "Education",
        "Bachelors",
        "Skills",
        "CASSANDRA",
        "AMBARI",
        "MAPREDUCE",
        "OOZIE",
        "SQOOP",
        "HBASE",
        "KAFKA",
        "FLUME",
        "HADOOP",
        "MONGODB",
        "DATA",
        "DATABASE",
        "MYSQL",
        "ORACLE",
        "PLSQL",
        "SQL",
        "C",
        "Git",
        "Hadoop",
        "Additional",
        "Information",
        "TECHNICAL",
        "SKILLS",
        "Hadoop",
        "Ecosystem",
        "Database",
        "Hadoop",
        "Spark",
        "Mapreduce",
        "Hive",
        "Spark",
        "SQL",
        "Cassandra",
        "HBase",
        "Sqoop",
        "Kafka",
        "Oozie",
        "Oracle",
        "12c11g8i",
        "MySQL",
        "5x",
        "Yarn",
        "Pig",
        "Flume",
        "Ambari",
        "PLSQL",
        "Programming",
        "Language",
        "AWS",
        "Java",
        "SQL",
        "JavaScript",
        "Scala",
        "R",
        "C",
        "EC2",
        "S3",
        "EMR",
        "Lambda",
        "Elastic",
        "Beanstalk",
        "ELB",
        "EBS",
        "DynamoDB",
        "ElastiCache",
        "SQS",
        "SNS",
        "SWF",
        "Testing",
        "CloudWatch",
        "CloudFormation",
        "IAM",
        "CloudTrail",
        "JUnit",
        "MRUnit",
        "Pytest",
        "ScalaTest",
        "Glacier",
        "Storage",
        "Gateway",
        "Scripting",
        "Language",
        "Web",
        "Framework",
        "Scala",
        "Unix",
        "Shell",
        "Html5",
        "Xml",
        "CSS3",
        "JavaScript",
        "jQuery",
        "HTML5",
        "CSS3",
        "J2EE",
        "Spring",
        "JSP",
        "SQL",
        "MVC",
        "Hibernate",
        "Data",
        "Analysis",
        "Visualization",
        "Web",
        "Services",
        "Tableau",
        "D3js",
        "SOAP",
        "Restful",
        "Environment",
        "CodeBuildDeployment",
        "WinSCP",
        "Putty",
        "SQL",
        "Developer",
        "Agile",
        "Git",
        "Svn",
        "Maven",
        "Jenkins",
        "Jira",
        "Confluence",
        "Operating",
        "Systems",
        "Tools",
        "Mac",
        "OS",
        "Ubuntu",
        "Centos",
        "Windows",
        "Hive",
        "MongoDB",
        "Spark",
        "Tableau",
        "Cloudera",
        "Hue"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T22:29:03.916019",
    "resume_data": "ScalaHadoop Developer ScalaHadoop span lDeveloperspan ScalaHadoop Developer Fidelity Investment Smithfield RI Experienced Hadoop Developer with over 10 years of IT experience has a strong background with Bigdata arena File Systems Data Management Analysis and Java Based enterprise application using Java J2EE technologies Expertise with installing configuring testing and using Apache Hadoop framework 27x its ecosystem components like HDFS MapReduce Sqoop 146 Flume 170 Hive 211 Pig 0160 Spark 210 Scala 2120 Kafka 0101 Yarn Oozie 313 and Zookeeper 3410 Experience and strong knowledge on implementation of Spark Core Spark Streaming Spark SQL Developed applications in Spark 210 using Scala 2120 to compare the performance of Spark with Hive Implemented POCs and developed pipeline using Kafka 0101 Spark Streaming and Spark SQL Extensive experience in Spark 210 transformations using Scala 2120 and Spark SQL for faster testing and processing of data files Hands on real time processing with Spark modules Spark RDD Dataset API using Scala 2120 Hands on experience of writing programs in MapReduce to analyze unstructured data Experience in importing and exporting data using Sqoop from HDFS to Relational Database Systems RDBMS and from RDBMS to HDFS Well versed Expert in writing Pig Latin scripts and HiveQL queries to process and analyze data Experience in Spark and indepth knowledge on SparkSQL RDDs Lazy transformation and actions Worked on Apache Kafka 0101 and Flume for handling megabytes of streaming data Transformed various formats of data like sequence File RC ORC Parquet JSON AVRO and experience in dealing with Compression techniques such as Gzip Snappy Deep knowledge in batch job scheduling workflow using Oozie 420 and zookeeper 3410 Understanding of Amazon Web Services stack and handson experience in using S3 EC2 and EMR Worked with different NOSQL databases such as Cassandra 310 HBbase 130 and MongoDB Experience with Relational databases including Oracle SQL Server and MySQL and experience in writing complex SQL queries PLSQL Stored Procedures Triggers sequences Well versed with the working of Data Visualization tools such as Tableau 93 D3js Developed Web applications using HTML5 CSS3 Bootstrap JavaScript JQuery AJAX Expertise in Core Java Data Structures Multithreading JDBC J2EE Algorithms Object Oriented Design OOD and Exception Handling and frameworks like Spring MVC and Hibernate Produced and consumed SOAP and RESTful Web Services and experience in developing Hadoop applications on Spark using Scala as a functional and objectoriented programming Experienced in writing ANT and Maven scripts to build and deploy Java applications Experienced in TDD TestDriven Development and SDLC methodologies such as AgileScrum Through knowledge of development environments such as Maven Git JIRA 64 Jenkins and Confluence Excellent understanding and knowledge of Hadoop architecture and various daemons such as Name Node Data Node Job Tracker Task Tracker Resource Manager and MapReduce programming paradigm Knowledge of Cyber Security concepts like cryptography Access Control Data Security in Linux Unix Expertise in Creating Hive InternalExternal TablesViews using shared Meta store writing scripts in HiveQL also data transformation file processing using Pig Latin Scripts Experience in Object Oriented Analysis Design OOAD and development of software using UML Methodology good knowledge of J2EE design patterns and Core Java design patterns Possess excellent presentation documentation communication skills detail oriented zeal to learn new technologies with a cooperative team focused attitude strong analytical and problemsolving skills Work Experience ScalaHadoop Developer Fidelity Investment Smithfield RI June 2018 to Present Project Description Fidelity Investment is a financial services firm involved in different sectors including Mutual fund Brokerage Retirement IRA and Wealth Management and in one of the leading companies in 410k plan and Stock Market activity The Distributed reporting project involves transferring from legacy system to the Hadoop environment for the power query using HDFS for the optimize query performance for the end customer using hive data warehouse They are using different tools for this project like Apache Hue Apache Hadoop Apache Spark HDFS Impala and few internal frameworks Responsibilities Experience in creating Hive tables to store the processed results in a tabular format optimizing Hive tables using optimization techniques like partitions and bucketing to provide better performance with HiveQL queries and creating custom user defined functions Developed reviewed and updated architecture and process documentation server diagrams requisition documents and other technical documents Involved in Agile methodology attended daily scrum meetings and sprint planning meetings Integrated visualizations into a Spark application using Databricks and visualization libraries ggplot Matplotlib Implemented different analytical algorithms using MapReduce programs to apply on top of HDFS data Installed managed and maintained a cluster of 60 nodes from POC to PROD Installed Hortonworks clusters and Hadoop ecosystem components through Ambari and command line interface Responsible for cluster maintenance commissioning and decommissioning data nodes cluster monitoring troubleshooting management and review data backups and Hadoop log files Skilled in Tableau Desktop for various types of data visualization reporting and analysis including Cross Map Scatter Plots Geographic Map Pie Charts and Bar Charts Page Trails and Density Chart Created HBase tables to store various data formats of data coming from MySQL Oracle Teradata Installed and configured Hive Pig Sqoop Flume and Oozie on the Hadoop cluster Configured Spark streaming to get ongoing information from the Kafka and stored the stream information to HDFS Developed data pipeline using Flume Sqoop to ingest business data and purchase histories into HDFS for analysis Utilized SparkSQL to extract and process data by parsing using Datasets or RDDs in Hive Context with transformations and actions map flat Map filter reduce reduceByKey Extended the capabilities of Data Frames using User Defined Functions in Python and Scala Designed and Implemented Sqoop incremental imports delta imports on tables without primary keys and dates from Oracle and appends directly into Hive Warehouse Import the data from different sources like HDFSHBase into Spark RDD and perform computations using Scala to generate the output response Writing a spark shell code to audit report to bring all the unique field under metadata Implemented HQL scripts for daily based data loading further aggregations Scheduling Oozie workflow and job to automate the run of Unix script daily as per the team requirement Working on clean up the old data and purge the database for having accurate data in the system Involved in building the ETL architecture and Target mapping to load data into Data Lake Developed Workflows with the help of Oozie to manage the flow of jobs and wrote Custom Expression Language EL functions for complex workflows Working on Install process monthly or by weekly new sets of report for new table or existing table and validating all the data coming from working directory to the HDFS Involved in the complete Software Development Life Cycle SDLC phases of the project as a part of Agile scrum methodology Loaded datasets from MySQL to HDFS and Hive respectively on daily basis Worked on Apache Hue as a central web admin User to Query on Hive or Impala and to Schedule job workflow Assisted in Installation and Configuration of Apache Hadoop clusters CHD and Hadoop tools for application development includes HDFS HUE YARN Sqoop Impala and Hive Used Cloudera Manager for continuous managing and monitoring the Hadoop cluster Environment Spark 230 Scala 2130 Sqoop 146 Impala 220 Oracle Hive 233 HDFS Spark SQL CHD 5161 HiveQL Hue 420 YARN 1123 Oozie 430 Agile Big Data Analytics SolutionsScala Developer Bloomberg LP New York NY May 2015 to May 2018 Project Description My Latest project was with Bloomberg which is a global firm which specializes in financial software media and data Company mainly involve in the financial software tools which provides the analytics and equality trading platform for data services and news Where in Bloomberg briefs its deliver news market data and commentary directly to inbox which give competitive advantage thats changing faster than ever Bloomberg Briefs is unique and provide downloadable reports to the subscriber and financial professional to get the actionable ideas and insights into a diverse range of markets and industries Specifically I was working with the Bloomberg Media division for the Bloomberg Terminal platform which caters to near half millions of corporate customers in more than 160 countries Instead of using third party application we were using the Hadoop ecosystem to manage the large scale of data with lowlatency which is processed daily on this platform Responsibilities Experience in Transform Stage and Store data using Spark which includes writing Spark applications in Scala Developed Spark core and Spark SQL scripts using Scala for faster data processing Developed a data pipeline using Kafka and Storm to store data into HDFS Developed Spark code using Scala and Spark SQL for faster testing and data processing Setting up and managing Kafka and Zookeeper for Stream processing Developed Kafka consumers API in Scala for consuming data from Kafka topics Configured deployed and maintained multinode Dev and Test Kafka Clusters Developed Spark scripts using Scala Spark SQL to access Hive tables in Spark for faster data processing Stored realtime data from Spark on HBase for future analysis Involved in implementing High Availability and automatic failover infrastructure to overcome single point of failure for Name node utilizing Zookeeper services Involved in analyzing data coming from various sources and creating Metafiles and control files to ingest the data in to the Data Lake Generate reports for analytics development through Zeppelin Developed Scala code with Spark Streaming for faster testing and processing of data Worked with Spark RDD and Dataframes for sessionization and other transformations Used Apache Avro to transform data between different format Streamed log files using Flume into HDFS and load into Hive tables to query data Created multiple Hive tables implemented partitioning dynamic partitioning and bucketing in Hive for efficient data access Performed daily adhoc data analysis and pulled data from Hadoop using Hive HiveQL Worked closely with team members managers and other teams in Agile Scrum environment Environment Kafka 0101 Spark 210 Scala 2120 Sqoop 146 Avro MySql5x HBase 130 Zeppelin Hive 211 HDFS Spark SQL Flume 170 HiveQL Zookeeper Agile Hadoop Developer IBM New York NY October 2012 to April 2015 Project Explore Solutions Project Description IBM Watson is a distinctively creating a business unit around the World Company headquartered in the United States where it specializes in the sector of Discovery Advisor Engagement Advisor and Explorer and building the application of business smart cities consumer applications and life in general Using Hadoop Ecosystem we have been developing predictive models to identify customer Insight for Banking by making use of realtime and historical data of bank user like mortgage credit history income source spending and reviews The project is about analysis of data by proper cleaning and normalization process that combine predictive and cognitive capabilities We need to collect data from various resources and enables dynamic solution behavioral segmentation to uncover actionable customer insights allowing banks to create personalized sales offerings and marketing campaigns Capability to store large unstructured data sets in NoSQL databases and using spark to analyze this data Responsibilities Worked on performance and optimization of existing algorithms in Hadoop using Spark context SparkSQL and Spark YARN using Scala Experience in installation upgrading configuration monitoring supporting and managing in Hadoop clusters using Cloudera CDH 4 CDH 5 Hortonworks HDP 2x and 3x on Ubuntu RedHat CentOS systems Worked on components of CDH and HDP including HDFS MapReduce Job tracker Task tracker Sqoop Zookeeper YARN Oozie Hive Hue Flume HBase Fair Scheduler Spark and Kafka Deployed Hadoop clusters on public and private cloud environments like AWS and OpenStack Administered the Linux systems to deploy Hadoop cluster and monitoring the cluster using Nagios and Ganglia Experienced in performing backup recovery failover and DR practices on multiple platforms Implemented Kerberos and LDAP authentication of all the services across Hadoop clusters using Apache Sentry Worked with highly transactional merchandise and investment in SQL databases with PCI HIPAA compliance involving data encryption with certificates and security keys at various levels Experienced in automating the provisioning processes and system resources using Puppet Implemented Hadoopbased solutions to store archives and backups from multiple sources Involved in importing and exporting data with Sqoop from RDBMS MySQL Oracle Teradata and used fast loaders and connectors and processed data for commercial analytics Built ingestion framework using flume for streaming logs and aggregating the data into HDFS Experienced in upgrading SQL Server software patches and service packs Practiced Agile scrum to provide operational support installation updates patches and version upgrades Experienced in collaborative platforms including Jira Rally SharePoint and Discovery Installed monitored and performance tuned standalone multinode clusters of Kafka Successfully loaded files to Hive and HDFS from MongoDB Cassandra and HBase Experienced in understanding and managing Hadoop log files with Flume Experienced on SQL DBA in HA and DDR like replication log shipping mirroring and clustering and database security and permissions Responsible for Importing and exporting data into HDFS and Hive using Sqoop from Oracle 11g and MySQL Develop and maintain several batch jobs to run automatically depending on business requirements Responsible for building scalable distributed data solutions using Hadoop Involved in loading data from edge node to HDFS using shell scripting Implemented Partitioning Dynamic Partitioning Buckets in Hive Developed PIG scripts using Pig Latin and worked on tuning the performance of Pig queries Involved in managing and reviewing Hadoop log files Created HBase tables to store variable data formats of PII data coming from different portfolios Implemented test scripts to support test driven development and continuous integration Exported the analyzed data to the relational databases like MySQL using Sqoop for visualization and to generate reports for the BI team Created MapReduce programs using Java API that filter unnecessary records and find out unique records based on different criteria Experienced in implementing POCs to migrate iterative map reduce programs into Spark transformations actions using Scala Supported in setting up QA environment and updating configurations for implementing scripts with Pig and Sqoop Worked in Test Driven Deployment environment and used Confluence for documentation Performed unit testing using MRunit Installed Oozie workflow engine to run multiple Hive and pig jobs Used Jira for project tracking Bug tracking and Project Management Environment Spark 210 Apache Hadoop MapReduce HDFS Hive 211 Java Pig 0160 Sqoop 146 MRunit Oozie 313 HBase130 TDD MySQL5x Oracle 11g Java Developer Axis Bank Gujarat IN October 2010 to September 2012 Project Customer Assistance Axis is one of the leading global banks Customer Assistance is Banking Application for Axis Bank The Customer Assistance Application plays a crucial role in dailybanking operations This Application makes its users to create bank accounts transfer money update contact history and update checking and saving accounts credit card approvals of the customers Responsibilities Gathered user requirements analyzed and wrote functional and technical specifications we use SharePoint to maintain all of our design docs Followed Agile software methodology for software development 3 week Sprint Worked on one of the UI based application and client focus more on look and feel of the UI We use lots of customs components to design the UI Chase standards and HTML CSS JavaScript AJAX EXTJS is being used intensively Used Spring MVC framework on the server side for creating RESTFul web services by giving JSON out and modifying the DOM object on UI by making HTTP calls and used GET and PUT Developed multiple Controller Service DAO classes to interact with data layer and developed Entity classes based on the table structure Involved in core integration to pick the file from FTP location and bring them into our staging tables and did all the validation on the java side Created multiple midtier services to interact with multiple validations and worked on entitlements services to do user validations Interaction and also worked on applying security systems Worked with Oracle database and used Hibernate ORM Created POJOData Objects in midtier service Hands on experience on implementing lazy loading first and second level of caching Leading onshore offshore model Coordinating with Offshore team in India and being flexible on gathering updated from team Work closely with Database team and testing team Worked on Jasper reports using iReport tool and integrated that JRXML into spring frame work Wrote SQL commands and Stored Procedures to retrieve data from Oracle database Worked to plug this procedure in Java classes Used Spring webflow for MVC pattern Used Apachetiles for JSP page fragments for various flows Used SOAP WebServices for sending data to Published New Services for given WSDL file GUI developing using custom JSTL tag library and used AJAX calls for client side HttpRequests Followed Agile Methodology and regular SCRUM meetings Involved in creating the Hibernate 30 POJO Objects and mapped using Hibernate Annotations Used Hibernate objectrelationalmapping ORM solution technique of mapping data representation from MVC model to Oracle Relational data model with a SQLbased schema Provide support to the users for all the service components and help them in production issues Involved in designing test plans test cases and overall Unit testing of the system using JUnit Environment Java 17 JSP Eclipse JUnit Hibernate 30 Oracle Maven Restful Git Scrum SpringWeb Flow SQL Java Developer eInfochip Gujarat IN July 2009 to September 2010 Project Online Invoice System OVS eInfochips is a Product Engineering and Software RD Services firm based that offers solutions in software hardware and VLSI services The company provides a range of products and services spanning multiple industries including silicon engineering embedded systems software engineering extended services Where I was involved in software engineering which offers custom software application development and maintenance services with focus on shorter development cycles enhanced features better user experience We also help our clients to migrate applications to open source technologies while providing a benefit analysis Responsibilities Software Development LifeCycle SDLC phases of design development implementation deployment testing and maintenance as per quality standards using Agile Scrum and waterfall methodologies Good Experience in Application Software Development and Design Object Oriented Technical Documentation Software Testing and Debugging Excellent implementation knowledge of JDK 1617 and 18 Spring Hibernate RESTFUL Web Services AOP Struts JDBC EJB Consumed and Exposed both REST and SOAP based webservices very good experience with SOA model Experience in RDBMS using MySQL Oracle SQL Server PostgreSQL Involved in configuring deploying applications on IBM WebSphere Application Server BEA WebLogic Application Server Apache Tomcat on UNIX Linux and Windows platforms Extensive experience in Design Development and implementation of ModelViewController MVC using Spring Good experience in Database Design writing stored procedure functions triggers SQL queries Developed the code for front end using EXTJS AngularJS JQuery JavaScript AJAX HTML CSS and JSON Good knowledge on TCPIP tunneling and port management on cloud environments installation of servers on cloud unix Good experience on production support and client interaction Experience in creating build scripts using Ant and Maven also have experience in Jenkins Strong TDD test driven development and continuous integration experience using JUnit Mock Framework Worked on Cucumber framework Implemented this application based on MVC Architecture using open source Struts Used UML to create class diagrams Sequence diagrams and State diagrams and implemented these diagrams in Microsoft Visio Developed front end web pages using Servlets 30 HTML JSP JavaScript Swing Responsible for presentation layers using JSP custom tags and JavaScript Developed ServerSide Validations using Struts Validation Framework Created a Transaction History Web Service using SOAP that is used for internal communication in the workflow process Used Core java concepts in application such as multithreaded programming Synchronization of threads used thread wait notify join methods Developed Data Access Objects for accessing Relational Database Designed test cases for unit testing with the help of JUnit Created database connection using JDBC classes for interacting with Oracle 9i database Environment Java 67 JSP Servlets 30 JDBC Swing Java Beans 30 JavaScript HTML Struts Oracle 9i Multithreading Data Access Objects SOAP UML Junit Education Bachelors Skills DYNAMODB CASSANDRA AMBARI MAPREDUCE OOZIE SQOOP HBASE KAFKA FLUME HADOOP MONGODB DATA ANALYSIS DATABASE MYSQL ORACLE PLSQL SQL C Git Hadoop Additional Information TECHNICAL SKILLS Hadoop Ecosystem Database Hadoop 27X Spark 210 Mapreduce Hive Spark SQL Cassandra 310 HBase 130 211 Sqoop 1997 Kafka 210 Oozie 313 MongoDB Oracle 12c11g8i MySQL 5x Yarn 0213 Pig 0160 Flume 170 Ambari PLSQL Programming Language AWS Java SQL JavaScript Scala R C EC2 S3 EMR Lambda Elastic Beanstalk ELB EBS DynamoDB ElastiCache SQS SNS SWF Testing CloudWatch CloudFormation IAM CloudTrail JUnit MRUnit Pytest ScalaTest Glacier Storage Gateway Scripting Language Web Framework Scala 2120 Unix Shell Html5 Xml CSS3 JavaScript jQuery HTML5 CSS3 J2EE Spring JSP SQL MVC Hibernate 30 Data Analysis Visualization Web Services Tableau 93 D3js SOAP Restful Environment CodeBuildDeployment WinSCP Putty SQL Developer Agile Git Svn Maven Jenkins Jira 64 Confluence Operating Systems Tools Mac OS Ubuntu Centos Windows Hive MongoDB Spark Tableau Cloudera Hue",
    "unique_id": "80d63d70-904c-4640-9bcd-22d5045b0e3b"
}