{
    "clean_data": "HadoopSpark Developer HadoopSpark span lDeveloperspan HadoopSpark Developer ConnectiveRx Over 7 years of diversified IT experience in E2E data analytics platforms ETLBI Java as Big data Hadoop JavaJ2EE Development and System Analysis Worked for over 4 years with Big Data Hadoop Ecosystem in the implementation of Data Lake Hands on experience Hadoop framework and its ecosystem like Distributed file system HDFS MapReduce Pig Hive Sqoop Flume and Spark Experience in layers of Hadoop Framework Storage HDFS Analysis Pig and Hive Engineering Jobs and Workflows extending the functionality by writing custom UDFs Extensive experience in developing Data warehouse applications using Hadoop Informatica Oracle Teradata MS SQL server on UNIX and Windows platforms and experience in creating complex mappings using various transformations and developing strategies for Extraction Transformation and Loading ETL mechanism by using Informatica 9x8x Proficient in Hive Query language and experienced in hive performance optimization using StaticPartitioning DynamicPartitioning Bucketing and Parallel Execution concepts As ETL developer designed and maintained high performance ELTETL processes Experience in analyzing data using Hive QL Pig Latin and custom MapReduce programs in Java custom UDF s Good Understanding of Hadoop Architecture and various components such as HDFS Job Tracker Task Tracker Name Node Data Node and MapReduce concepts Knowledge on Cloud computing infrastructure AWS amazon web services Created modules for spark streaming in data into Data Lake using Strom and Spark Experience in Dimensional Data Modeling Star Schema SnowFlake Schema Fact and Dimensional Tables concepts like Lambda Architecture and Batch processing Oozie Extensively used Informatica client tools Source Analyzer Warehouse designer Mapping designer Mapplet Designer ETL Transformations Informatica Repository Manager and Informatica Server Manager Workflow Manager Workflow Monitor Expertise in using core Java J2EE Multithreading JDBC Shell Scripting and proficient in using Java APIs Collections Servlets JSP for application development Worked closely to review pre and postprocessed data to ensure data accuracy and integrity with Dev and QA teams Experience in Java J2ee JDBC Collections Servlets JSP Struts Spring Hibernate JSON XML REST SOAP Web services Groovy MVC Eclipse Weblogic Websphere and Apache Tomcat severs Extensive knowledge of Data Modeling Data Conversions Data integration and Data Migration with specialization in Informatica Power Center Expertise in extraction transformation and loading data from heterogeneous systems like flat files excel Oracle Teradata MSSQL Server Good work experience with UNIXLinux commands scripting and deploying the applications on the servers Maintained tuning and monitoring Hadoop jobs and clusters in a production environment Strong skills in algorithms data structures Object oriented design Design patterns documentation and QAtesting Excellent domain knowledge in Insurance Telecom and Banking Work Experience HadoopSpark Developer ConnectiveRx Bridgeville PA April 2018 to Present Description American Family Insurance is a private mutual company that focuses on property casualty auto insurance and also offers commercial insurance life health and homeowners coverage The purpose of the project is to build a complex ETL pipeline which will handle huge trip data that is collected from vendor servers Transformations which we implemented can calculate different trip level details and events that are based on latitude longitude details from raw data Storing all trip level calculated summary data in data warehouse on top of Hadoop Different machine learning algorithms are used on this summary data to generate the scores which will be useful to provide better service to the existing customers Responsibilities Created and worked on Sqoop jobs with incremental load to populate Hive External tables Designed and developed Hive tables to store staging and historical data Created Hive tables as per requirement internal and external tables are defined with appropriate static and dynamic partitions intended for efficiency Experience in using ORC file format with Snappy compression for optimized storage of Hive tables Solved performance issues in Hive and Pig scripts with understanding of Joins Group and aggregation and used them using Impala process engine Developed Spark scripts by using Scala shell commands as per the requirement Created Oozie workflows for sqoop to migrate the data from source to HDFS and then to target tables Developed Oozie workflow for scheduling and orchestrating the ETL process Responsible for building scalable distributed data solutions using Hadoop Experience in Job management using Fair scheduler and Developed job processing scripts using Oozie workflow Involved in migrating MapReduce jobs into Spark jobs and used Spark SQL and Data Frames API to load structured and semistructured data into Spark clusters Used Spark Streaming to divide streaming data into batches as an input to Spark engine for batch processing Worked extensively with Sqoop for importing metadata from Oracle Developed Oozie workflow jobs to execute Hive Pig Sqoop and MapReduce actions Configured Flume to transport web server logs into HDFS Experience on Amazon Web Services AWS Amazon Cloud Services like Elastic Compute Cloud EC2 Simple Storage ServiceS3 Elastic Map Reduce EMR Amazon Simple DB and Amazon Cloud Watch Implemented Spark using Scala and SparkSQL for faster testing and processing of data Used Apache Kafka for importing real time network log data into HDFS Worked on numerous POCs to prove if Big Data is the right fit for a business case Experience data processing like collecting aggregating moving from various sources using Apache Flume and Kafka Created webbased User interface for creating monitoring and controlling data flows using Apache Nifi Environment Apache Hadoop CDH 47 HDFS MapReduce Sqoop Flume Pig Hive HBase Oozie Scala Spark Spark Streaming Kafka Linux Hadoop Developer Verizon Bel Air MD August 2016 to March 2018 With over 100 million customers and annual revenue of 30 billion Sears generates a huge amount of data on the transactions made by the customers The scope of the project is to use the customer transaction data in store and online over a period to recommend items to customer that they will find engaging Another concept is a recommendation engine that introduces new products to a customer which they might have not came across before To achieve these goals we perform data exploration to learn about user behaviour We perform future engineering to create new features from existing features that truly reflects the signals in the data and avoid noise Responsibilities Worked with the source team to understand the format delimiters of the data files Responsible for generating actionable insights from complex data to drive significant business results for various application teams Developed and implemented API services using Python in spark Troubleshoot and resolve data quality issues and maintain important level of data accuracy in the data being reported Extensively implemented POCs on migrating to SparkStreaming to process the live data Ingested data from RDBMS and performed data transformations and then export the transformed data to Cassandra as per the business requirement Rewriting existing mapreduce jobs to use new features and improvements for achieving faster results Analyses large amount of data sets to determine optimal way to aggregate and report on it Performance tuned slow running resource intensive jobs Worked on Data serialization formats for converting complex objects into sequence bits by using Avro Parquet JSON CSV formats Hands on experience working on inmemory based Apache Spark application for ETL transformations Involved in converting HiveSQL queries into Spark transformations using Spark RDDs Python Developed multiple POCs using Spark and deployed on the Yarn cluster compared the performance of Spark with Hive and SQLTeradata Developed Flume configuration to extract log data from different resources and transfer data with different file formats JSON XML and Parquet to hive tables Setup Oozie workflow sub workflow jobs for HiveSQOOPHDFS actions Experience in accessing Kafka cluster to consume data into Hadoop Involved in importing the realtime data to Hadoop using Kafka and implemented the Oozie job for daily imports Worked with business and functional requirement gathering team updated user comments in JIRA and documented in confluence Handled tasks like maintaining accurate roadmap for project or certain product Monitoring the sprints burndown charts and completing the monthly reports Environment Hive SQL Pig Flume Kafka Map reduce SQOOP Spark Python Java Shell Scripting Teradata Oracle Oozie Cassandra Hadoop Developer SSI GroupMobile AL June 2015 to July 20116 Description SSI Group is a Clinical Data Services provides healthcare organizations with a proven technology to share clinical information The solution enables authorized users for covered entities to see a complete picture of patientexhibited conditions and the treatments received SSI solutions increase the accuracy and velocity of data exchange among healthcare providers and payers with the highest levels of security Responsibilities Gathered business requirements in meetings for successful implementation and POC ProofofConcept of Hadoop Cluster Importing data in regular basis using Sqoop into the Hive partition and controlled work flow by using apache Oozie Developed Sqoop Jobs to both import data into HDFS from Relational Database Management System like Oracle DB2 and export data from HDFS to Oracle Developing HQL queries to implement the select insert update and operations to the database by creating HQL named queries Involved in data extraction that may include analysing reviewing modelling based on requirements using higher level tools such as Hive and Impala Experience in migrating HiveQL into Impala to minimize query response time Involving in creating Hive tables loading with data and writing hive queries Developed Pig functions to preprocess the data for analysis Created HBase tables to store all data Deployed the Hbase cluster in cloud AWS environment with scalable nodes as per the business requirement Analysed identified defects and its root cause and recommended course of actions Loaded data into Hive Tables from Hadoop Distributed File System HDFS to provide SQLlike access on Hadoop data Worked on streaming the analysed data to the existing relational databases using Sqoop for making it available for visualization and report generation by the BI team Generated reports and did predictions using BI Tool called Tableau Integrated data by using Talend Deployed the Hbase cluster in cloud Amazon AWS environment with scalable nodes as per the business requirement Environment HDFS Hive MapReduce Sqoop Impala Java Pig SQL Server HBase Oracle and Tableau AWS Hadoop Developer Net Cracker Technology SolutionsHyderabad March 2014 to May 2015 Responsibilities Integrated Kafka with Storm for real time data processing and written some storm topologies to store the processed data directly to MongoDB and HDFS Experience in writing Spark SQL scripts Imported data from different sources into Spark RDD for processing Developed custom aggregate functions using Spark SQL and performed interactive querying Involved in loading data from edge node to HDFS using shell scripting Worked on installing cluster commissioning and decommissioning of Datanode Namenode high availability capacity planning and slots configuration Completion of unit testing for the new Hadoop jobs in standalone mode designated for Unit region using MR Unit Developed Spark scripts by using Scala and Python shell commands as per the requirement Experience in managing and reviewing Hadoop log files Experience in Hive partitioning bucketing and perform joins on Hive tables and implementing Hive SerDe like REGEX JSON and Avro Optimized Hive analytics Sql queries created tablesviews written custom UDFs and Hive based exception processing Involved in transforming the Teradata to legacy lables to HDFS and HBase tables using Sqoop and vice versa Configured Fair Scheduler to provide fair resources to all the applications across the cluster Environment Hortonworks Hadoop Ambari Spark Solr Kafka MongoDB Linux HDFS Hive Pig Sqoop Flume Zookeeper RDBMS Java Developer Infor Grenville Hyderabad Telangana March 2012 to February 2014 Description The workstation Project automates assignment of Workstation and Keys to each employee capable of identifies the unassigned workstation and Keys based on that we can easily assign that workstation and keys pedestalstoragecabin to new employee Gathered the Employee Details Key Details Workstation Details based on Floor and Zone and all these are entered by using Bulk Import Concept using this project employees can be viewedcategorized based on their position Here we can identify the employee location based on FloorZone Responsibilities Functional and UI design has been prepared Implementation at BIO level Creation of Record sets and BIOs for the database schema Created Relationships for data Integrity Created Lookups and attribute domains Implementation at UI level ie Menus for Navigation Forms for various Perspectives Implemented shells like List Shell Detail Shell Tab Group Shell Toggle Shell to Provide better look and feel toolbars to allow UI actions for buttons Used Form Slots by considering the BIO schema Attachments of documents has been provided for work ordersinvoices Authentication and authorization have been achieved by creating users and profiles in platadmin Implemented objectpermissions at widget menu and form levels Developed Form level extensions to achieve UI level validations and BIO level extensions to fulfil Functional requirements and validations All required data is entered by using Bulk Import Involved in Development process and have knowledge in usage of Tracker Tools like JIRA Having good Knowledge in Epiphany Platform Open Architecture Having Extensive Hands on Experience on Complex PLSQL Programming Environments CRB Studio Web logic server 81 LDAP Core Java SQL Server Skills Hdfs Impala Mapreduce Oozie Sqoop Hbase Kafka Flume Hadoop Jboss Mongodb Nosql Teradata Visual studio Apache spark Application server Git Hadoop Hbase Hive Additional Information Skills Big Data Technologies HDFS MapReduce Hive Pig Sqoop Flume Oozie Zookeeper Kafka MongoDB Apache Spark Spark Streaming HBase Flume Impala Hadoop Distribution Cloudera Horton Works Apache AWS Languages Java SQL PLSQL Pig Latin HiveQL Scala Regular Expressions Operating Systems Windowsxp7810 UNIX LINUX UBUNTU CENTOS PortalsApplication servers WebLogic WebSphere Application server WebSphere Portal server JBOSS Build Automation tools SBT Ant Maven Version Control GIT IDE Build Tools Design Eclipse Visual Studio Net Beans Rational Application Developer Junit Databases Oracle SQL Server MySQL MS Access NoSQL Database HBase MongoDB Teradata",
    "entities": [
        "Git Hadoop Hbase Hive Additional Information Skills Big Data Technologies HDFS MapReduce Hive Pig",
        "Clinical Data Services",
        "Joins Group",
        "BIO",
        "Avro Optimized Hive",
        "Informatica",
        "Created Oozie",
        "SparkStreaming",
        "FloorZone Responsibilities Functional",
        "Bulk Import Involved",
        "BI",
        "HDFS",
        "UNIX",
        "Description SSI Group",
        "WebLogic WebSphere Application",
        "Developed Spark",
        "Creation of Record",
        "Hadoop Framework Storage HDFS Analysis Pig",
        "Hive External",
        "Talend Deployed",
        "Sears",
        "Java",
        "Hadoop",
        "Tableau Integrated",
        "Hadoop Distributed File System",
        "JIRA",
        "Maintained",
        "Responsibilities Created",
        "Fair",
        "HBase",
        "Developed Oozie",
        "Scala Regular Expressions Operating Systems",
        "Apache Spark",
        "UNIXLinux",
        "Impala Hadoop Distribution",
        "Generated",
        "POC ProofofConcept of Hadoop Cluster Importing",
        "SparkSQL",
        "Developed",
        "Node Data",
        "Created Relationships",
        "Hadoop Involved",
        "Oracle DB2",
        "ORC",
        "BI Tool",
        "WebSphere Portal",
        "Lambda Architecture",
        "PortalsApplication",
        "Dimensional Data Modeling Star",
        "Hadoop Different",
        "Spark",
        "Created Hive",
        "Apache Nifi Environment Apache Hadoop",
        "Datanode Namenode",
        "Hive Tables",
        "API",
        "Sqoop",
        "QA",
        "Maven Version",
        "Created",
        "Scala",
        "ELTETL",
        "Amazon Web Services AWS Amazon Cloud Services like",
        "Tableau AWS Hadoop Developer Net Cracker Technology SolutionsHyderabad",
        "Floor",
        "HDFS Job Tracker Task Tracker",
        "Build Tools Design",
        "Integrity Created Lookups",
        "Oracle Developing HQL",
        "UDF",
        "Spark RDD",
        "Hadoop JavaJ2EE Development and System Analysis Worked",
        "Implementation at BIO level",
        "MD",
        "Big Data Hadoop Ecosystem",
        "Bulk Import Concept",
        "Relational Database Management System",
        "Insurance Telecom and Banking Work Experience HadoopSpark Developer",
        "Data Frames API",
        "Big Data",
        "Hive",
        "Amazon AWS",
        "Oracle Developed Oozie",
        "Perspectives Implemented",
        "ETL",
        "Java APIs Collections Servlets JSP",
        "Oozie Developed Sqoop Jobs",
        "Data Modeling Data Conversions Data",
        "Data Migration",
        "Informatica Power Center Expertise",
        "Impala",
        "Spark SQL",
        "UI",
        "HadoopSpark Developer HadoopSpark",
        "StaticPartitioning DynamicPartitioning Bucketing",
        "Oozie Extensively",
        "Workstation",
        "Extraction Transformation",
        "Tracker Tools",
        "Created HBase",
        "Setup Oozie",
        "Imported",
        "Data",
        "MapReduce",
        "RDBMS",
        "Oozie Cassandra Hadoop Developer",
        "Application",
        "E2E data analytics",
        "HBase Oracle",
        "Present Description American Family Insurance",
        "Troubleshoot",
        "Amazon Cloud Watch Implemented Spark"
    ],
    "experience": "Experience in layers of Hadoop Framework Storage HDFS Analysis Pig and Hive Engineering Jobs and Workflows extending the functionality by writing custom UDFs Extensive experience in developing Data warehouse applications using Hadoop Informatica Oracle Teradata MS SQL server on UNIX and Windows platforms and experience in creating complex mappings using various transformations and developing strategies for Extraction Transformation and Loading ETL mechanism by using Informatica 9x8x Proficient in Hive Query language and experienced in hive performance optimization using StaticPartitioning DynamicPartitioning Bucketing and Parallel Execution concepts As ETL developer designed and maintained high performance ELTETL processes Experience in analyzing data using Hive QL Pig Latin and custom MapReduce programs in Java custom UDF s Good Understanding of Hadoop Architecture and various components such as HDFS Job Tracker Task Tracker Name Node Data Node and MapReduce concepts Knowledge on Cloud computing infrastructure AWS amazon web services Created modules for spark streaming in data into Data Lake using Strom and Spark Experience in Dimensional Data Modeling Star Schema SnowFlake Schema Fact and Dimensional Tables concepts like Lambda Architecture and Batch processing Oozie Extensively used Informatica client tools Source Analyzer Warehouse designer Mapping designer Mapplet Designer ETL Transformations Informatica Repository Manager and Informatica Server Manager Workflow Manager Workflow Monitor Expertise in using core Java J2EE Multithreading JDBC Shell Scripting and proficient in using Java APIs Collections Servlets JSP for application development Worked closely to review pre and postprocessed data to ensure data accuracy and integrity with Dev and QA teams Experience in Java J2ee JDBC Collections Servlets JSP Struts Spring Hibernate JSON XML REST SOAP Web services Groovy MVC Eclipse Weblogic Websphere and Apache Tomcat severs Extensive knowledge of Data Modeling Data Conversions Data integration and Data Migration with specialization in Informatica Power Center Expertise in extraction transformation and loading data from heterogeneous systems like flat files excel Oracle Teradata MSSQL Server Good work experience with UNIXLinux commands scripting and deploying the applications on the servers Maintained tuning and monitoring Hadoop jobs and clusters in a production environment Strong skills in algorithms data structures Object oriented design Design patterns documentation and QAtesting Excellent domain knowledge in Insurance Telecom and Banking Work Experience HadoopSpark Developer ConnectiveRx Bridgeville PA April 2018 to Present Description American Family Insurance is a private mutual company that focuses on property casualty auto insurance and also offers commercial insurance life health and homeowners coverage The purpose of the project is to build a complex ETL pipeline which will handle huge trip data that is collected from vendor servers Transformations which we implemented can calculate different trip level details and events that are based on latitude longitude details from raw data Storing all trip level calculated summary data in data warehouse on top of Hadoop Different machine learning algorithms are used on this summary data to generate the scores which will be useful to provide better service to the existing customers Responsibilities Created and worked on Sqoop jobs with incremental load to populate Hive External tables Designed and developed Hive tables to store staging and historical data Created Hive tables as per requirement internal and external tables are defined with appropriate static and dynamic partitions intended for efficiency Experience in using ORC file format with Snappy compression for optimized storage of Hive tables Solved performance issues in Hive and Pig scripts with understanding of Joins Group and aggregation and used them using Impala process engine Developed Spark scripts by using Scala shell commands as per the requirement Created Oozie workflows for sqoop to migrate the data from source to HDFS and then to target tables Developed Oozie workflow for scheduling and orchestrating the ETL process Responsible for building scalable distributed data solutions using Hadoop Experience in Job management using Fair scheduler and Developed job processing scripts using Oozie workflow Involved in migrating MapReduce jobs into Spark jobs and used Spark SQL and Data Frames API to load structured and semistructured data into Spark clusters Used Spark Streaming to divide streaming data into batches as an input to Spark engine for batch processing Worked extensively with Sqoop for importing metadata from Oracle Developed Oozie workflow jobs to execute Hive Pig Sqoop and MapReduce actions Configured Flume to transport web server logs into HDFS Experience on Amazon Web Services AWS Amazon Cloud Services like Elastic Compute Cloud EC2 Simple Storage ServiceS3 Elastic Map Reduce EMR Amazon Simple DB and Amazon Cloud Watch Implemented Spark using Scala and SparkSQL for faster testing and processing of data Used Apache Kafka for importing real time network log data into HDFS Worked on numerous POCs to prove if Big Data is the right fit for a business case Experience data processing like collecting aggregating moving from various sources using Apache Flume and Kafka Created webbased User interface for creating monitoring and controlling data flows using Apache Nifi Environment Apache Hadoop CDH 47 HDFS MapReduce Sqoop Flume Pig Hive HBase Oozie Scala Spark Spark Streaming Kafka Linux Hadoop Developer Verizon Bel Air MD August 2016 to March 2018 With over 100 million customers and annual revenue of 30 billion Sears generates a huge amount of data on the transactions made by the customers The scope of the project is to use the customer transaction data in store and online over a period to recommend items to customer that they will find engaging Another concept is a recommendation engine that introduces new products to a customer which they might have not came across before To achieve these goals we perform data exploration to learn about user behaviour We perform future engineering to create new features from existing features that truly reflects the signals in the data and avoid noise Responsibilities Worked with the source team to understand the format delimiters of the data files Responsible for generating actionable insights from complex data to drive significant business results for various application teams Developed and implemented API services using Python in spark Troubleshoot and resolve data quality issues and maintain important level of data accuracy in the data being reported Extensively implemented POCs on migrating to SparkStreaming to process the live data Ingested data from RDBMS and performed data transformations and then export the transformed data to Cassandra as per the business requirement Rewriting existing mapreduce jobs to use new features and improvements for achieving faster results Analyses large amount of data sets to determine optimal way to aggregate and report on it Performance tuned slow running resource intensive jobs Worked on Data serialization formats for converting complex objects into sequence bits by using Avro Parquet JSON CSV formats Hands on experience working on inmemory based Apache Spark application for ETL transformations Involved in converting HiveSQL queries into Spark transformations using Spark RDDs Python Developed multiple POCs using Spark and deployed on the Yarn cluster compared the performance of Spark with Hive and SQLTeradata Developed Flume configuration to extract log data from different resources and transfer data with different file formats JSON XML and Parquet to hive tables Setup Oozie workflow sub workflow jobs for HiveSQOOPHDFS actions Experience in accessing Kafka cluster to consume data into Hadoop Involved in importing the realtime data to Hadoop using Kafka and implemented the Oozie job for daily imports Worked with business and functional requirement gathering team updated user comments in JIRA and documented in confluence Handled tasks like maintaining accurate roadmap for project or certain product Monitoring the sprints burndown charts and completing the monthly reports Environment Hive SQL Pig Flume Kafka Map reduce SQOOP Spark Python Java Shell Scripting Teradata Oracle Oozie Cassandra Hadoop Developer SSI GroupMobile AL June 2015 to July 20116 Description SSI Group is a Clinical Data Services provides healthcare organizations with a proven technology to share clinical information The solution enables authorized users for covered entities to see a complete picture of patientexhibited conditions and the treatments received SSI solutions increase the accuracy and velocity of data exchange among healthcare providers and payers with the highest levels of security Responsibilities Gathered business requirements in meetings for successful implementation and POC ProofofConcept of Hadoop Cluster Importing data in regular basis using Sqoop into the Hive partition and controlled work flow by using apache Oozie Developed Sqoop Jobs to both import data into HDFS from Relational Database Management System like Oracle DB2 and export data from HDFS to Oracle Developing HQL queries to implement the select insert update and operations to the database by creating HQL named queries Involved in data extraction that may include analysing reviewing modelling based on requirements using higher level tools such as Hive and Impala Experience in migrating HiveQL into Impala to minimize query response time Involving in creating Hive tables loading with data and writing hive queries Developed Pig functions to preprocess the data for analysis Created HBase tables to store all data Deployed the Hbase cluster in cloud AWS environment with scalable nodes as per the business requirement Analysed identified defects and its root cause and recommended course of actions Loaded data into Hive Tables from Hadoop Distributed File System HDFS to provide SQLlike access on Hadoop data Worked on streaming the analysed data to the existing relational databases using Sqoop for making it available for visualization and report generation by the BI team Generated reports and did predictions using BI Tool called Tableau Integrated data by using Talend Deployed the Hbase cluster in cloud Amazon AWS environment with scalable nodes as per the business requirement Environment HDFS Hive MapReduce Sqoop Impala Java Pig SQL Server HBase Oracle and Tableau AWS Hadoop Developer Net Cracker Technology SolutionsHyderabad March 2014 to May 2015 Responsibilities Integrated Kafka with Storm for real time data processing and written some storm topologies to store the processed data directly to MongoDB and HDFS Experience in writing Spark SQL scripts Imported data from different sources into Spark RDD for processing Developed custom aggregate functions using Spark SQL and performed interactive querying Involved in loading data from edge node to HDFS using shell scripting Worked on installing cluster commissioning and decommissioning of Datanode Namenode high availability capacity planning and slots configuration Completion of unit testing for the new Hadoop jobs in standalone mode designated for Unit region using MR Unit Developed Spark scripts by using Scala and Python shell commands as per the requirement Experience in managing and reviewing Hadoop log files Experience in Hive partitioning bucketing and perform joins on Hive tables and implementing Hive SerDe like REGEX JSON and Avro Optimized Hive analytics Sql queries created tablesviews written custom UDFs and Hive based exception processing Involved in transforming the Teradata to legacy lables to HDFS and HBase tables using Sqoop and vice versa Configured Fair Scheduler to provide fair resources to all the applications across the cluster Environment Hortonworks Hadoop Ambari Spark Solr Kafka MongoDB Linux HDFS Hive Pig Sqoop Flume Zookeeper RDBMS Java Developer Infor Grenville Hyderabad Telangana March 2012 to February 2014 Description The workstation Project automates assignment of Workstation and Keys to each employee capable of identifies the unassigned workstation and Keys based on that we can easily assign that workstation and keys pedestalstoragecabin to new employee Gathered the Employee Details Key Details Workstation Details based on Floor and Zone and all these are entered by using Bulk Import Concept using this project employees can be viewedcategorized based on their position Here we can identify the employee location based on FloorZone Responsibilities Functional and UI design has been prepared Implementation at BIO level Creation of Record sets and BIOs for the database schema Created Relationships for data Integrity Created Lookups and attribute domains Implementation at UI level ie Menus for Navigation Forms for various Perspectives Implemented shells like List Shell Detail Shell Tab Group Shell Toggle Shell to Provide better look and feel toolbars to allow UI actions for buttons Used Form Slots by considering the BIO schema Attachments of documents has been provided for work ordersinvoices Authentication and authorization have been achieved by creating users and profiles in platadmin Implemented objectpermissions at widget menu and form levels Developed Form level extensions to achieve UI level validations and BIO level extensions to fulfil Functional requirements and validations All required data is entered by using Bulk Import Involved in Development process and have knowledge in usage of Tracker Tools like JIRA Having good Knowledge in Epiphany Platform Open Architecture Having Extensive Hands on Experience on Complex PLSQL Programming Environments CRB Studio Web logic server 81 LDAP Core Java SQL Server Skills Hdfs Impala Mapreduce Oozie Sqoop Hbase Kafka Flume Hadoop Jboss Mongodb Nosql Teradata Visual studio Apache spark Application server Git Hadoop Hbase Hive Additional Information Skills Big Data Technologies HDFS MapReduce Hive Pig Sqoop Flume Oozie Zookeeper Kafka MongoDB Apache Spark Spark Streaming HBase Flume Impala Hadoop Distribution Cloudera Horton Works Apache AWS Languages Java SQL PLSQL Pig Latin HiveQL Scala Regular Expressions Operating Systems Windowsxp7810 UNIX LINUX UBUNTU CENTOS PortalsApplication servers WebLogic WebSphere Application server WebSphere Portal server JBOSS Build Automation tools SBT Ant Maven Version Control GIT IDE Build Tools Design Eclipse Visual Studio Net Beans Rational Application Developer Junit Databases Oracle SQL Server MySQL MS Access NoSQL Database HBase MongoDB Teradata",
    "extracted_keywords": [
        "HadoopSpark",
        "Developer",
        "HadoopSpark",
        "span",
        "lDeveloperspan",
        "HadoopSpark",
        "Developer",
        "ConnectiveRx",
        "years",
        "IT",
        "experience",
        "E2E",
        "data",
        "analytics",
        "platforms",
        "Java",
        "data",
        "Hadoop",
        "JavaJ2EE",
        "Development",
        "System",
        "Analysis",
        "years",
        "Big",
        "Data",
        "Hadoop",
        "Ecosystem",
        "implementation",
        "Data",
        "Lake",
        "Hands",
        "experience",
        "Hadoop",
        "framework",
        "ecosystem",
        "file",
        "system",
        "HDFS",
        "MapReduce",
        "Pig",
        "Hive",
        "Sqoop",
        "Flume",
        "Spark",
        "Experience",
        "layers",
        "Hadoop",
        "Framework",
        "Storage",
        "HDFS",
        "Analysis",
        "Pig",
        "Hive",
        "Engineering",
        "Jobs",
        "Workflows",
        "functionality",
        "custom",
        "UDFs",
        "experience",
        "Data",
        "warehouse",
        "applications",
        "Hadoop",
        "Informatica",
        "Oracle",
        "Teradata",
        "MS",
        "SQL",
        "server",
        "UNIX",
        "Windows",
        "platforms",
        "experience",
        "mappings",
        "transformations",
        "strategies",
        "Extraction",
        "Transformation",
        "Loading",
        "ETL",
        "mechanism",
        "Informatica",
        "9x8x",
        "Proficient",
        "Hive",
        "Query",
        "language",
        "performance",
        "optimization",
        "StaticPartitioning",
        "DynamicPartitioning",
        "Bucketing",
        "Parallel",
        "Execution",
        "concepts",
        "ETL",
        "developer",
        "performance",
        "ELTETL",
        "Experience",
        "data",
        "Hive",
        "QL",
        "Pig",
        "Latin",
        "MapReduce",
        "programs",
        "Java",
        "custom",
        "UDF",
        "Understanding",
        "Hadoop",
        "Architecture",
        "components",
        "HDFS",
        "Job",
        "Tracker",
        "Task",
        "Tracker",
        "Name",
        "Node",
        "Data",
        "Node",
        "MapReduce",
        "Knowledge",
        "Cloud",
        "infrastructure",
        "AWS",
        "amazon",
        "web",
        "services",
        "modules",
        "spark",
        "streaming",
        "data",
        "Data",
        "Lake",
        "Strom",
        "Spark",
        "Experience",
        "Dimensional",
        "Data",
        "Modeling",
        "Star",
        "Schema",
        "SnowFlake",
        "Schema",
        "Fact",
        "Dimensional",
        "Tables",
        "concepts",
        "Lambda",
        "Architecture",
        "Batch",
        "processing",
        "Oozie",
        "Informatica",
        "client",
        "tools",
        "Source",
        "Analyzer",
        "Warehouse",
        "designer",
        "Mapping",
        "designer",
        "Mapplet",
        "Designer",
        "ETL",
        "Transformations",
        "Informatica",
        "Repository",
        "Manager",
        "Informatica",
        "Server",
        "Manager",
        "Workflow",
        "Manager",
        "Workflow",
        "Monitor",
        "Expertise",
        "core",
        "Java",
        "J2EE",
        "Multithreading",
        "JDBC",
        "Shell",
        "Scripting",
        "Java",
        "APIs",
        "Collections",
        "Servlets",
        "JSP",
        "application",
        "development",
        "data",
        "data",
        "accuracy",
        "integrity",
        "Dev",
        "QA",
        "teams",
        "Experience",
        "Java",
        "J2ee",
        "JDBC",
        "Collections",
        "Servlets",
        "JSP",
        "Struts",
        "Spring",
        "Hibernate",
        "XML",
        "REST",
        "SOAP",
        "Web",
        "services",
        "Groovy",
        "MVC",
        "Eclipse",
        "Weblogic",
        "Websphere",
        "Apache",
        "Tomcat",
        "knowledge",
        "Data",
        "Modeling",
        "Data",
        "Conversions",
        "Data",
        "integration",
        "Data",
        "Migration",
        "specialization",
        "Informatica",
        "Power",
        "Center",
        "Expertise",
        "extraction",
        "transformation",
        "loading",
        "data",
        "systems",
        "files",
        "Oracle",
        "Teradata",
        "MSSQL",
        "Server",
        "Good",
        "work",
        "experience",
        "UNIXLinux",
        "commands",
        "applications",
        "servers",
        "Hadoop",
        "jobs",
        "clusters",
        "production",
        "environment",
        "skills",
        "algorithms",
        "data",
        "structures",
        "design",
        "Design",
        "patterns",
        "documentation",
        "QAtesting",
        "Excellent",
        "domain",
        "knowledge",
        "Insurance",
        "Telecom",
        "Banking",
        "Work",
        "Experience",
        "HadoopSpark",
        "Developer",
        "ConnectiveRx",
        "Bridgeville",
        "PA",
        "April",
        "Present",
        "Description",
        "American",
        "Family",
        "Insurance",
        "company",
        "property",
        "casualty",
        "auto",
        "insurance",
        "insurance",
        "life",
        "health",
        "homeowners",
        "purpose",
        "project",
        "ETL",
        "pipeline",
        "trip",
        "data",
        "vendor",
        "servers",
        "Transformations",
        "trip",
        "level",
        "details",
        "events",
        "latitude",
        "details",
        "data",
        "trip",
        "level",
        "summary",
        "data",
        "data",
        "warehouse",
        "top",
        "Hadoop",
        "Different",
        "machine",
        "learning",
        "algorithms",
        "summary",
        "data",
        "scores",
        "service",
        "customers",
        "Responsibilities",
        "Sqoop",
        "jobs",
        "load",
        "Hive",
        "External",
        "tables",
        "Hive",
        "tables",
        "staging",
        "data",
        "Hive",
        "tables",
        "requirement",
        "tables",
        "partitions",
        "efficiency",
        "Experience",
        "file",
        "format",
        "compression",
        "storage",
        "Hive",
        "tables",
        "performance",
        "issues",
        "Hive",
        "Pig",
        "scripts",
        "understanding",
        "Joins",
        "Group",
        "aggregation",
        "Impala",
        "process",
        "engine",
        "Spark",
        "scripts",
        "Scala",
        "shell",
        "commands",
        "requirement",
        "Created",
        "Oozie",
        "workflows",
        "sqoop",
        "data",
        "source",
        "HDFS",
        "tables",
        "Developed",
        "Oozie",
        "workflow",
        "scheduling",
        "ETL",
        "process",
        "data",
        "solutions",
        "Hadoop",
        "Experience",
        "Job",
        "management",
        "Fair",
        "scheduler",
        "job",
        "processing",
        "scripts",
        "Oozie",
        "workflow",
        "MapReduce",
        "jobs",
        "Spark",
        "jobs",
        "Spark",
        "SQL",
        "Data",
        "Frames",
        "API",
        "data",
        "Spark",
        "clusters",
        "Spark",
        "Streaming",
        "streaming",
        "data",
        "batches",
        "input",
        "Spark",
        "engine",
        "batch",
        "processing",
        "Sqoop",
        "metadata",
        "Oracle",
        "Developed",
        "Oozie",
        "workflow",
        "jobs",
        "Hive",
        "Pig",
        "Sqoop",
        "MapReduce",
        "actions",
        "Configured",
        "Flume",
        "transport",
        "web",
        "server",
        "logs",
        "HDFS",
        "Experience",
        "Amazon",
        "Web",
        "Services",
        "AWS",
        "Amazon",
        "Cloud",
        "Services",
        "Elastic",
        "Compute",
        "Cloud",
        "EC2",
        "Simple",
        "Storage",
        "ServiceS3",
        "Elastic",
        "Map",
        "EMR",
        "Amazon",
        "Simple",
        "DB",
        "Amazon",
        "Cloud",
        "Watch",
        "Spark",
        "Scala",
        "SparkSQL",
        "testing",
        "processing",
        "data",
        "Apache",
        "Kafka",
        "time",
        "network",
        "log",
        "data",
        "HDFS",
        "POCs",
        "Big",
        "Data",
        "fit",
        "business",
        "case",
        "Experience",
        "data",
        "processing",
        "sources",
        "Apache",
        "Flume",
        "Kafka",
        "Created",
        "User",
        "interface",
        "monitoring",
        "data",
        "flows",
        "Apache",
        "Nifi",
        "Environment",
        "Apache",
        "Hadoop",
        "CDH",
        "HDFS",
        "MapReduce",
        "Sqoop",
        "Flume",
        "Pig",
        "Hive",
        "HBase",
        "Oozie",
        "Scala",
        "Spark",
        "Spark",
        "Streaming",
        "Kafka",
        "Linux",
        "Hadoop",
        "Developer",
        "Verizon",
        "Bel",
        "Air",
        "MD",
        "August",
        "March",
        "customers",
        "revenue",
        "Sears",
        "amount",
        "data",
        "transactions",
        "customers",
        "scope",
        "project",
        "customer",
        "transaction",
        "data",
        "store",
        "period",
        "items",
        "customer",
        "concept",
        "recommendation",
        "engine",
        "products",
        "customer",
        "goals",
        "data",
        "exploration",
        "user",
        "behaviour",
        "engineering",
        "features",
        "features",
        "signals",
        "data",
        "noise",
        "Responsibilities",
        "source",
        "team",
        "format",
        "delimiters",
        "data",
        "files",
        "insights",
        "data",
        "business",
        "results",
        "application",
        "teams",
        "API",
        "services",
        "Python",
        "spark",
        "Troubleshoot",
        "data",
        "quality",
        "issues",
        "level",
        "data",
        "accuracy",
        "data",
        "POCs",
        "SparkStreaming",
        "data",
        "data",
        "RDBMS",
        "data",
        "transformations",
        "data",
        "Cassandra",
        "business",
        "requirement",
        "mapreduce",
        "jobs",
        "features",
        "improvements",
        "results",
        "Analyses",
        "amount",
        "data",
        "sets",
        "way",
        "Performance",
        "resource",
        "jobs",
        "Data",
        "serialization",
        "formats",
        "objects",
        "sequence",
        "bits",
        "Avro",
        "Parquet",
        "CSV",
        "Hands",
        "experience",
        "inmemory",
        "Apache",
        "Spark",
        "application",
        "ETL",
        "transformations",
        "HiveSQL",
        "queries",
        "Spark",
        "transformations",
        "Spark",
        "RDDs",
        "Python",
        "POCs",
        "Spark",
        "Yarn",
        "cluster",
        "performance",
        "Spark",
        "Hive",
        "SQLTeradata",
        "Developed",
        "Flume",
        "configuration",
        "data",
        "resources",
        "transfer",
        "data",
        "file",
        "formats",
        "JSON",
        "XML",
        "Parquet",
        "tables",
        "Setup",
        "Oozie",
        "workflow",
        "sub",
        "jobs",
        "HiveSQOOPHDFS",
        "actions",
        "Experience",
        "Kafka",
        "cluster",
        "data",
        "Hadoop",
        "data",
        "Hadoop",
        "Kafka",
        "Oozie",
        "job",
        "imports",
        "business",
        "requirement",
        "gathering",
        "team",
        "user",
        "comments",
        "JIRA",
        "confluence",
        "tasks",
        "roadmap",
        "project",
        "product",
        "sprints",
        "charts",
        "reports",
        "Environment",
        "Hive",
        "SQL",
        "Pig",
        "Flume",
        "Kafka",
        "Map",
        "SQOOP",
        "Spark",
        "Python",
        "Java",
        "Shell",
        "Scripting",
        "Teradata",
        "Oracle",
        "Oozie",
        "Cassandra",
        "Hadoop",
        "Developer",
        "SSI",
        "GroupMobile",
        "AL",
        "June",
        "July",
        "Description",
        "SSI",
        "Group",
        "Clinical",
        "Data",
        "Services",
        "healthcare",
        "organizations",
        "technology",
        "information",
        "solution",
        "users",
        "entities",
        "picture",
        "conditions",
        "treatments",
        "SSI",
        "solutions",
        "accuracy",
        "velocity",
        "data",
        "exchange",
        "healthcare",
        "providers",
        "payers",
        "levels",
        "security",
        "Responsibilities",
        "business",
        "requirements",
        "meetings",
        "implementation",
        "POC",
        "ProofofConcept",
        "Hadoop",
        "Cluster",
        "Importing",
        "data",
        "basis",
        "Sqoop",
        "Hive",
        "partition",
        "work",
        "flow",
        "apache",
        "Oozie",
        "Developed",
        "Sqoop",
        "Jobs",
        "import",
        "data",
        "HDFS",
        "Relational",
        "Database",
        "Management",
        "System",
        "Oracle",
        "DB2",
        "export",
        "data",
        "HDFS",
        "Oracle",
        "Developing",
        "HQL",
        "update",
        "operations",
        "database",
        "HQL",
        "queries",
        "data",
        "extraction",
        "modelling",
        "requirements",
        "level",
        "tools",
        "Hive",
        "Impala",
        "Experience",
        "HiveQL",
        "Impala",
        "query",
        "response",
        "time",
        "Hive",
        "tables",
        "data",
        "hive",
        "Developed",
        "Pig",
        "functions",
        "data",
        "analysis",
        "HBase",
        "data",
        "Hbase",
        "cluster",
        "AWS",
        "environment",
        "nodes",
        "business",
        "requirement",
        "Analysed",
        "defects",
        "root",
        "cause",
        "course",
        "actions",
        "data",
        "Hive",
        "Tables",
        "Hadoop",
        "Distributed",
        "File",
        "System",
        "HDFS",
        "access",
        "Hadoop",
        "data",
        "data",
        "databases",
        "Sqoop",
        "visualization",
        "report",
        "generation",
        "BI",
        "team",
        "predictions",
        "BI",
        "Tool",
        "Tableau",
        "Integrated",
        "data",
        "Talend",
        "Hbase",
        "cluster",
        "Amazon",
        "AWS",
        "environment",
        "nodes",
        "business",
        "requirement",
        "Environment",
        "HDFS",
        "Hive",
        "MapReduce",
        "Sqoop",
        "Impala",
        "Java",
        "Pig",
        "SQL",
        "Server",
        "HBase",
        "Oracle",
        "Tableau",
        "AWS",
        "Hadoop",
        "Developer",
        "Net",
        "Cracker",
        "Technology",
        "SolutionsHyderabad",
        "March",
        "May",
        "Responsibilities",
        "Integrated",
        "Kafka",
        "Storm",
        "time",
        "data",
        "processing",
        "storm",
        "topologies",
        "data",
        "MongoDB",
        "HDFS",
        "Experience",
        "Spark",
        "SQL",
        "data",
        "sources",
        "Spark",
        "RDD",
        "custom",
        "aggregate",
        "functions",
        "Spark",
        "SQL",
        "querying",
        "loading",
        "data",
        "edge",
        "node",
        "HDFS",
        "shell",
        "scripting",
        "cluster",
        "commissioning",
        "decommissioning",
        "Datanode",
        "Namenode",
        "availability",
        "capacity",
        "planning",
        "slots",
        "configuration",
        "Completion",
        "unit",
        "testing",
        "Hadoop",
        "jobs",
        "mode",
        "Unit",
        "region",
        "MR",
        "Unit",
        "Developed",
        "Spark",
        "scripts",
        "Scala",
        "Python",
        "shell",
        "commands",
        "requirement",
        "Experience",
        "Hadoop",
        "log",
        "Experience",
        "Hive",
        "bucketing",
        "joins",
        "Hive",
        "tables",
        "Hive",
        "SerDe",
        "REGEX",
        "JSON",
        "Avro",
        "Hive",
        "analytics",
        "Sql",
        "queries",
        "tablesviews",
        "custom",
        "UDFs",
        "Hive",
        "exception",
        "processing",
        "Teradata",
        "legacy",
        "lables",
        "HDFS",
        "HBase",
        "tables",
        "Sqoop",
        "vice",
        "versa",
        "Configured",
        "Fair",
        "Scheduler",
        "resources",
        "applications",
        "cluster",
        "Environment",
        "Hortonworks",
        "Hadoop",
        "Ambari",
        "Spark",
        "Solr",
        "Kafka",
        "MongoDB",
        "Linux",
        "HDFS",
        "Hive",
        "Pig",
        "Sqoop",
        "Flume",
        "Zookeeper",
        "RDBMS",
        "Java",
        "Developer",
        "Infor",
        "Grenville",
        "Hyderabad",
        "Telangana",
        "March",
        "February",
        "Description",
        "workstation",
        "Project",
        "assignment",
        "Workstation",
        "Keys",
        "employee",
        "identifies",
        "workstation",
        "Keys",
        "workstation",
        "keys",
        "pedestalstoragecabin",
        "employee",
        "Employee",
        "Details",
        "Key",
        "Details",
        "Workstation",
        "Details",
        "Floor",
        "Zone",
        "Bulk",
        "Import",
        "Concept",
        "project",
        "employees",
        "position",
        "employee",
        "location",
        "FloorZone",
        "Responsibilities",
        "Functional",
        "UI",
        "design",
        "Implementation",
        "BIO",
        "level",
        "Creation",
        "Record",
        "sets",
        "BIOs",
        "database",
        "schema",
        "Created",
        "Relationships",
        "data",
        "Integrity",
        "Created",
        "Lookups",
        "attribute",
        "domains",
        "Implementation",
        "UI",
        "level",
        "Menus",
        "Navigation",
        "Forms",
        "Perspectives",
        "shells",
        "List",
        "Shell",
        "Detail",
        "Shell",
        "Tab",
        "Group",
        "Shell",
        "Toggle",
        "Shell",
        "toolbars",
        "UI",
        "actions",
        "buttons",
        "Form",
        "Slots",
        "BIO",
        "schema",
        "Attachments",
        "documents",
        "work",
        "ordersinvoices",
        "Authentication",
        "authorization",
        "users",
        "profiles",
        "platadmin",
        "objectpermissions",
        "widget",
        "menu",
        "form",
        "levels",
        "Form",
        "level",
        "extensions",
        "UI",
        "level",
        "validations",
        "BIO",
        "level",
        "extensions",
        "requirements",
        "validations",
        "data",
        "Bulk",
        "Import",
        "Development",
        "process",
        "knowledge",
        "usage",
        "Tracker",
        "Tools",
        "JIRA",
        "Knowledge",
        "Epiphany",
        "Platform",
        "Open",
        "Architecture",
        "Hands",
        "Experience",
        "Complex",
        "PLSQL",
        "Programming",
        "Environments",
        "CRB",
        "Studio",
        "Web",
        "logic",
        "server",
        "LDAP",
        "Core",
        "Java",
        "SQL",
        "Server",
        "Skills",
        "Hdfs",
        "Impala",
        "Mapreduce",
        "Oozie",
        "Sqoop",
        "Hbase",
        "Kafka",
        "Flume",
        "Hadoop",
        "Jboss",
        "Mongodb",
        "Nosql",
        "Teradata",
        "Visual",
        "studio",
        "Apache",
        "spark",
        "Application",
        "server",
        "Git",
        "Hadoop",
        "Hbase",
        "Hive",
        "Additional",
        "Information",
        "Skills",
        "Big",
        "Data",
        "Technologies",
        "HDFS",
        "MapReduce",
        "Hive",
        "Pig",
        "Sqoop",
        "Flume",
        "Oozie",
        "Zookeeper",
        "Kafka",
        "MongoDB",
        "Apache",
        "Spark",
        "Spark",
        "Streaming",
        "HBase",
        "Flume",
        "Impala",
        "Hadoop",
        "Distribution",
        "Cloudera",
        "Horton",
        "Apache",
        "AWS",
        "Languages",
        "Java",
        "SQL",
        "PLSQL",
        "Pig",
        "Latin",
        "HiveQL",
        "Scala",
        "Regular",
        "Expressions",
        "Operating",
        "Systems",
        "Windowsxp7810",
        "UNIX",
        "LINUX",
        "UBUNTU",
        "CENTOS",
        "PortalsApplication",
        "WebLogic",
        "WebSphere",
        "Application",
        "server",
        "WebSphere",
        "server",
        "JBOSS",
        "Build",
        "Automation",
        "tools",
        "SBT",
        "Ant",
        "Maven",
        "Version",
        "Control",
        "GIT",
        "IDE",
        "Build",
        "Tools",
        "Design",
        "Eclipse",
        "Visual",
        "Studio",
        "Net",
        "Beans",
        "Rational",
        "Application",
        "Developer",
        "Junit",
        "Oracle",
        "SQL",
        "Server",
        "MySQL",
        "MS",
        "Access",
        "NoSQL",
        "Database",
        "HBase",
        "Teradata"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T22:08:33.645054",
    "resume_data": "HadoopSpark Developer HadoopSpark span lDeveloperspan HadoopSpark Developer ConnectiveRx Over 7 years of diversified IT experience in E2E data analytics platforms ETLBI Java as Big data Hadoop JavaJ2EE Development and System Analysis Worked for over 4 years with Big Data Hadoop Ecosystem in the implementation of Data Lake Hands on experience Hadoop framework and its ecosystem like Distributed file system HDFS MapReduce Pig Hive Sqoop Flume and Spark Experience in layers of Hadoop Framework Storage HDFS Analysis Pig and Hive Engineering Jobs and Workflows extending the functionality by writing custom UDFs Extensive experience in developing Data warehouse applications using Hadoop Informatica Oracle Teradata MS SQL server on UNIX and Windows platforms and experience in creating complex mappings using various transformations and developing strategies for Extraction Transformation and Loading ETL mechanism by using Informatica 9x8x Proficient in Hive Query language and experienced in hive performance optimization using StaticPartitioning DynamicPartitioning Bucketing and Parallel Execution concepts As ETL developer designed and maintained high performance ELTETL processes Experience in analyzing data using Hive QL Pig Latin and custom MapReduce programs in Java custom UDF s Good Understanding of Hadoop Architecture and various components such as HDFS Job Tracker Task Tracker Name Node Data Node and MapReduce concepts Knowledge on Cloud computing infrastructure AWS amazon web services Created modules for spark streaming in data into Data Lake using Strom and Spark Experience in Dimensional Data Modeling Star Schema SnowFlake Schema Fact and Dimensional Tables concepts like Lambda Architecture and Batch processing Oozie Extensively used Informatica client tools Source Analyzer Warehouse designer Mapping designer Mapplet Designer ETL Transformations Informatica Repository Manager and Informatica Server Manager Workflow Manager Workflow Monitor Expertise in using core Java J2EE Multithreading JDBC Shell Scripting and proficient in using Java APIs Collections Servlets JSP for application development Worked closely to review pre and postprocessed data to ensure data accuracy and integrity with Dev and QA teams Experience in Java J2ee JDBC Collections Servlets JSP Struts Spring Hibernate JSON XML REST SOAP Web services Groovy MVC Eclipse Weblogic Websphere and Apache Tomcat severs Extensive knowledge of Data Modeling Data Conversions Data integration and Data Migration with specialization in Informatica Power Center Expertise in extraction transformation and loading data from heterogeneous systems like flat files excel Oracle Teradata MSSQL Server Good work experience with UNIXLinux commands scripting and deploying the applications on the servers Maintained tuning and monitoring Hadoop jobs and clusters in a production environment Strong skills in algorithms data structures Object oriented design Design patterns documentation and QAtesting Excellent domain knowledge in Insurance Telecom and Banking Work Experience HadoopSpark Developer ConnectiveRx Bridgeville PA April 2018 to Present Description American Family Insurance is a private mutual company that focuses on property casualty auto insurance and also offers commercial insurance life health and homeowners coverage The purpose of the project is to build a complex ETL pipeline which will handle huge trip data that is collected from vendor servers Transformations which we implemented can calculate different trip level details and events that are based on latitude longitude details from raw data Storing all trip level calculated summary data in data warehouse on top of Hadoop Different machine learning algorithms are used on this summary data to generate the scores which will be useful to provide better service to the existing customers Responsibilities Created and worked on Sqoop jobs with incremental load to populate Hive External tables Designed and developed Hive tables to store staging and historical data Created Hive tables as per requirement internal and external tables are defined with appropriate static and dynamic partitions intended for efficiency Experience in using ORC file format with Snappy compression for optimized storage of Hive tables Solved performance issues in Hive and Pig scripts with understanding of Joins Group and aggregation and used them using Impala process engine Developed Spark scripts by using Scala shell commands as per the requirement Created Oozie workflows for sqoop to migrate the data from source to HDFS and then to target tables Developed Oozie workflow for scheduling and orchestrating the ETL process Responsible for building scalable distributed data solutions using Hadoop Experience in Job management using Fair scheduler and Developed job processing scripts using Oozie workflow Involved in migrating MapReduce jobs into Spark jobs and used Spark SQL and Data Frames API to load structured and semistructured data into Spark clusters Used Spark Streaming to divide streaming data into batches as an input to Spark engine for batch processing Worked extensively with Sqoop for importing metadata from Oracle Developed Oozie workflow jobs to execute Hive Pig Sqoop and MapReduce actions Configured Flume to transport web server logs into HDFS Experience on Amazon Web Services AWS Amazon Cloud Services like Elastic Compute Cloud EC2 Simple Storage ServiceS3 Elastic Map Reduce EMR Amazon Simple DB and Amazon Cloud Watch Implemented Spark using Scala and SparkSQL for faster testing and processing of data Used Apache Kafka for importing real time network log data into HDFS Worked on numerous POCs to prove if Big Data is the right fit for a business case Experience data processing like collecting aggregating moving from various sources using Apache Flume and Kafka Created webbased User interface for creating monitoring and controlling data flows using Apache Nifi Environment Apache Hadoop CDH 47 HDFS MapReduce Sqoop Flume Pig Hive HBase Oozie Scala Spark Spark Streaming Kafka Linux Hadoop Developer Verizon Bel Air MD August 2016 to March 2018 With over 100 million customers and annual revenue of 30 billion Sears generates a huge amount of data on the transactions made by the customers The scope of the project is to use the customer transaction data in store and online over a period to recommend items to customer that they will find engaging Another concept is a recommendation engine that introduces new products to a customer which they might have not came across before To achieve these goals we perform data exploration to learn about user behaviour We perform future engineering to create new features from existing features that truly reflects the signals in the data and avoid noise Responsibilities Worked with the source team to understand the format delimiters of the data files Responsible for generating actionable insights from complex data to drive significant business results for various application teams Developed and implemented API services using Python in spark Troubleshoot and resolve data quality issues and maintain important level of data accuracy in the data being reported Extensively implemented POCs on migrating to SparkStreaming to process the live data Ingested data from RDBMS and performed data transformations and then export the transformed data to Cassandra as per the business requirement Rewriting existing mapreduce jobs to use new features and improvements for achieving faster results Analyses large amount of data sets to determine optimal way to aggregate and report on it Performance tuned slow running resource intensive jobs Worked on Data serialization formats for converting complex objects into sequence bits by using Avro Parquet JSON CSV formats Hands on experience working on inmemory based Apache Spark application for ETL transformations Involved in converting HiveSQL queries into Spark transformations using Spark RDDs Python Developed multiple POCs using Spark and deployed on the Yarn cluster compared the performance of Spark with Hive and SQLTeradata Developed Flume configuration to extract log data from different resources and transfer data with different file formats JSON XML and Parquet to hive tables Setup Oozie workflow sub workflow jobs for HiveSQOOPHDFS actions Experience in accessing Kafka cluster to consume data into Hadoop Involved in importing the realtime data to Hadoop using Kafka and implemented the Oozie job for daily imports Worked with business and functional requirement gathering team updated user comments in JIRA and documented in confluence Handled tasks like maintaining accurate roadmap for project or certain product Monitoring the sprints burndown charts and completing the monthly reports Environment Hive SQL Pig Flume Kafka Map reduce SQOOP Spark Python Java Shell Scripting Teradata Oracle Oozie Cassandra Hadoop Developer SSI GroupMobile AL June 2015 to July 20116 Description SSI Group is a Clinical Data Services provides healthcare organizations with a proven technology to share clinical information The solution enables authorized users for covered entities to see a complete picture of patientexhibited conditions and the treatments received SSI solutions increase the accuracy and velocity of data exchange among healthcare providers and payers with the highest levels of security Responsibilities Gathered business requirements in meetings for successful implementation and POC ProofofConcept of Hadoop Cluster Importing data in regular basis using Sqoop into the Hive partition and controlled work flow by using apache Oozie Developed Sqoop Jobs to both import data into HDFS from Relational Database Management System like Oracle DB2 and export data from HDFS to Oracle Developing HQL queries to implement the select insert update and operations to the database by creating HQL named queries Involved in data extraction that may include analysing reviewing modelling based on requirements using higher level tools such as Hive and Impala Experience in migrating HiveQL into Impala to minimize query response time Involving in creating Hive tables loading with data and writing hive queries Developed Pig functions to preprocess the data for analysis Created HBase tables to store all data Deployed the Hbase cluster in cloud AWS environment with scalable nodes as per the business requirement Analysed identified defects and its root cause and recommended course of actions Loaded data into Hive Tables from Hadoop Distributed File System HDFS to provide SQLlike access on Hadoop data Worked on streaming the analysed data to the existing relational databases using Sqoop for making it available for visualization and report generation by the BI team Generated reports and did predictions using BI Tool called Tableau Integrated data by using Talend Deployed the Hbase cluster in cloud Amazon AWS environment with scalable nodes as per the business requirement Environment HDFS Hive MapReduce Sqoop Impala Java Pig SQL Server HBase Oracle and Tableau AWS Hadoop Developer Net Cracker Technology SolutionsHyderabad March 2014 to May 2015 Responsibilities Integrated Kafka with Storm for real time data processing and written some storm topologies to store the processed data directly to MongoDB and HDFS Experience in writing Spark SQL scripts Imported data from different sources into Spark RDD for processing Developed custom aggregate functions using Spark SQL and performed interactive querying Involved in loading data from edge node to HDFS using shell scripting Worked on installing cluster commissioning and decommissioning of Datanode Namenode high availability capacity planning and slots configuration Completion of unit testing for the new Hadoop jobs in standalone mode designated for Unit region using MR Unit Developed Spark scripts by using Scala and Python shell commands as per the requirement Experience in managing and reviewing Hadoop log files Experience in Hive partitioning bucketing and perform joins on Hive tables and implementing Hive SerDe like REGEX JSON and Avro Optimized Hive analytics Sql queries created tablesviews written custom UDFs and Hive based exception processing Involved in transforming the Teradata to legacy lables to HDFS and HBase tables using Sqoop and vice versa Configured Fair Scheduler to provide fair resources to all the applications across the cluster Environment Hortonworks Hadoop Ambari Spark Solr Kafka MongoDB Linux HDFS Hive Pig Sqoop Flume Zookeeper RDBMS Java Developer Infor Grenville Hyderabad Telangana March 2012 to February 2014 Description The workstation Project automates assignment of Workstation and Keys to each employee capable of identifies the unassigned workstation and Keys based on that we can easily assign that workstation and keys pedestalstoragecabin to new employee Gathered the Employee Details Key Details Workstation Details based on Floor and Zone and all these are entered by using Bulk Import Concept using this project employees can be viewedcategorized based on their position Here we can identify the employee location based on FloorZone Responsibilities Functional and UI design has been prepared Implementation at BIO level Creation of Record sets and BIOs for the database schema Created Relationships for data Integrity Created Lookups and attribute domains Implementation at UI level ie Menus for Navigation Forms for various Perspectives Implemented shells like List Shell Detail Shell Tab Group Shell Toggle Shell to Provide better look and feel toolbars to allow UI actions for buttons Used Form Slots by considering the BIO schema Attachments of documents has been provided for work ordersinvoices Authentication and authorization have been achieved by creating users and profiles in platadmin Implemented objectpermissions at widget menu and form levels Developed Form level extensions to achieve UI level validations and BIO level extensions to fulfil Functional requirements and validations All required data is entered by using Bulk Import Involved in Development process and have knowledge in usage of Tracker Tools like JIRA Having good Knowledge in Epiphany Platform Open Architecture Having Extensive Hands on Experience on Complex PLSQL Programming Environments CRB Studio Web logic server 81 LDAP Core Java SQL Server Skills Hdfs Impala Mapreduce Oozie Sqoop Hbase Kafka Flume Hadoop Jboss Mongodb Nosql Teradata Visual studio Apache spark Application server Git Hadoop Hbase Hive Additional Information Skills Big Data Technologies HDFS MapReduce Hive Pig Sqoop Flume Oozie Zookeeper Kafka MongoDB Apache Spark Spark Streaming HBase Flume Impala Hadoop Distribution Cloudera Horton Works Apache AWS Languages Java SQL PLSQL Pig Latin HiveQL Scala Regular Expressions Operating Systems Windowsxp7810 UNIX LINUX UBUNTU CENTOS PortalsApplication servers WebLogic WebSphere Application server WebSphere Portal server JBOSS Build Automation tools SBT Ant Maven Version Control GIT IDE Build Tools Design Eclipse Visual Studio Net Beans Rational Application Developer Junit Databases Oracle SQL Server MySQL MS Access NoSQL Database HBase MongoDB Teradata",
    "unique_id": "39e8b64f-4589-4539-acb5-a9719980d2f8"
}