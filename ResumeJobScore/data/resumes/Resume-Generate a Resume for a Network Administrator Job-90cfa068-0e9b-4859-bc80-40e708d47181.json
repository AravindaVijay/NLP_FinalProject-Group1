{
    "clean_data": "Sr Spark Developer Sr Spark span lDeveloperspan Sr Spark Developer CenterPoint Energy 8 years of extensive handson experience in IT industry including 5 years experience in deployment of Hadoop Ecosystems like MapReduce Yarn Sqoop Flume Pig Hive HBase Cassandra Zoo Keeper Oozie and Ambari BigQuery Big Table and 5 years experience on Spark Storm Scala Python Experience in OLTP and OLAP design development testing implementation and support of enterprise Data warehouses Strong Knowledge in Hadoop Cluster Capacity Planning Performance Tuning Cluster Monitoring Extensive experience in business data science project life cycle including Data Acquisition Data Cleaning Data Manipulation Data Validation Data Mining Machine Learning Algorithms and Visualization Good Hands on experience in working with Ecosystems like Hive Pig Sqoop Map Reduce Flume Oozie Strong knowledge in HIVE and PIG core functionality by using custom User Defined Functions UDF User Defined TableGenerating Functions UDTF and User Defined Aggregating Functions UDAF for Hive Experience on Productionizing Apache Nifi for dataflows with significant processing requirements and controlling security of data flow Designed and developed RDD Seeds using Scala and Cascading Streaming data to Sparkstreaming using Kafka Exposure to Spark Spark Streaming Spark MLlib Scala and Creating the Data Frames handled in Spark with Scala Good Exposure on Map Reduce programming using Java PIG Latin Scripting and Distributed Application and HDFS Experienced Good understanding of NoSQL databases and hands on work experience in writing applications No SQL Databases HBase Cassandra and MongoDB Very good implementation experience of Object Oriented concepts Multithreading and JavaScala Experienced with the Scala Spark improving the performance and optimization of the existing algorithms in Hadoop using Spark Context Spark SQL Pair RDDs Spark YARN Experienced in installation configuration supporting and managing Hadoop Clusters using Apache Cloudera distributions Horton works Cloud Storage and Amazon web services AWS and related technologies DynamoDB EMR S3 ML Experience in deploying NiFi Data flow in Production team and Integrating data from multiple sources like Cassandra MongoDB Deploying templates to environments can be done via NiFi RestAPI integrated with other automation tools Complete end to end design and development of Apache NiFi flow which acts as the agent between middleware team and EBI team and executes all the actions mentioned above Experienced in Python programming wrote Web Crawlers using Python Experience in bench marking Hadoop cluster for analysis of queue usage Experienced in working with Mahout for applying machine learning techniques in the Hadoop Ecosystem Good Experience on Amazon Web Services like Redshift Data Pipeline ML Good experienced on moving the data in and out of Hadoop RDBMS NoSQL and UNIX from various systems using SQOOP and other traditional data movement technologies Experience on Integration of Quartz scheduler with Oozie work flows to get data from multiple data sources in parallel using fork Experience in installation configuration support and management of a Hadoop Cluster using Cloudera Distributions Experienced Spark scripts by using Scala shell as per requirements Good knowledge on tuning the Spark jobs by changing the configuration properties and using broadcast variables Developed REST APIs using Java Play framework and Akka Expertise in search technologys like SOLR Informatica Lucene Experience in converting SQL queries into Spark Transformations using Spark RDDs and Scala and Performed mapside joins on RDDs Experienced in writing Hadoop Jobs for analyzing data using Hive Query Language HQL Pig Latin Data flow language and custom MapReduce programs in Java Good understanding of NoSQL databases like MongoDB Cassandra and HBase Strong analytical skills with ability to quickly understand clients business needs Involved in business meetings for requirements gathering form business clients Experienced in Storm builder topologies to perform cleansing operations before moving data into HBase Hands on experience in configuring and working with Flume to load the data from multiple sources directly into Hdfs Experience on configuring fully the Flume agent suitable for all type of logger data and store them in Avro Sink in Parquet file format and developing 2tier architecture connecting channels between Avro sinks and Source Experience creating Visual report Graphical analysis and Dashboard reports using Tableau Informatica of historical data saved in Hdfs and data analysis using Splunk enterprise edition Good experience in utilizing Cloud Storage Services like Git Extensive knowledge in using GitHub and Bit Bucket Experienced in job scheduling and monitoring using Oozie Zookeeper Work Experience Sr Spark Developer CenterPoint Energy Houston TX June 2018 to Present Responsibilities Hands on experience in installation configuration supporting and managing Hadoop Clusters Knowledge of Cassandra security maintenance and tuning both database and server Chipped away at outlining and building up the Real Time Analysis module for Analytic Dashboard utilizing Cassandra Kafka Spark Streaming Installed and configured Confluent Kafka in RD line Validated the installation with HDFS connector and Hive connectors Deployed high availability on the Hadoop cluster quorum journal nodes Experience on implementing SAX Symbolic Aggregate approXimation in Java to use with Apache Spark for normalizing time series data Involved in defining job flows managing and reviewing log file Setup configured and optimized the Cassandra cluster Developed realtime Spark based application to work along with the Cassandra database Responsible to manage data coming from different sources through Kafka Installed Kafka Producer on different severs and Scheduled to produce data for every 10 seconds Integrated Kafka with Spark Streaming to listen onto multiple Kafka Brokers with different Kafka topics for every 5 Seconds Enhanced and optimized product Spark code to aggregate group and run data mining tasks using the Spark framework and handled Json Data Handled Json Data comes from Kafka Direct Stream on each partitions and transformed them into required Data Frame Formats Upgraded Spark 16 to latest Version Spark 22 and configure Kafka Version 010 Managing Kafka Offsets Saving Offsets in external data base like HBase and to its own Kafka Worked on Import Export of data using ETL tool Sqoop from MySQL to HDFS Worked on Lambda Architecture for both Batch processing and Real Streaming purposes Used Oozie to Schedule Spark and Kafka Producer Jobs to run in parallel Appended the Data Frames into Cassandra Key Space Tables using DataStax SparkCassandra Connector Experience with Cassandra YAML Configuration files RACK DC properties file Cassandraenv file for JMX configurations etc Installed and configured Datastax OpsCenter and Nagios for Cassandra cluster maintenance and alert Configured Authentication and security in Apache kafka pubsub system Good experience with Century Link Cloud for provisioning virtual machines creating resource groups configuring key vaults for storing encryption keys Monitoring etc Great Hands on Experience in seat stamping Hadoop bunch for investigation of line utilization Performing OS level setups and Kernel level tuning Implement and test integration of BI Business Intelligence tools with Hadoop stack InstalledConfiguredMaintained Apache Hadoop clusters for application development and Hadoop tools like Hive Pig HBase Zookeeper Sqoop Yarn Spark2 Kafka and Oozie Formulated procedures for installation of Hadoop Spark2 patches updates and version upgrades Environment Cloudera HDFS Spark Hive Pig Map Reduce Hue Sqoop Putt Apache Kafka Apache Drill Century Link Cloud AWS Java Netezza Cassandra Oozie Spark SPARK SQL Maven SBT Java Scala SQL and Linux YARN Agile Methodology Solr PHP Admin XAMPP DataStax Cassandra Sr HadoopSpark Developer CPS Energy San Antonio TX January 2017 to June 2018 Responsibilities Involved in deploying systems on Amazon Web Services AWS Infrastructure services EC2 Experience in configuring deploying the web applications on AWS servers using SBT and Play Migrated Map Reduce jobs into Spark RDD transformations using Scala Used Spark API over Cloudera Hadoop YARN to perform analytics on data in Hive Developed Spark code using Spark RDD and SparkSQLStreaming for faster processing of data Performed configuration deployment and support of cloud services including Amazon Web Services AWS Working knowledge of various AWS technologies like SQS Queuing SNS Notification S3 storage Redshift Data Pipeline EMR Responsible for all Public AWS and Private OpenstackVMWareDCOSMesosMarathon cloud infrastructure Developed Flume ETL job for handling data from HTTP Source and Sink as HDFS and configuring Data Pipelining Used Hive data warehouse tool to analyze the unified historic data in HDFS to identify issues and behavioral patterns Involved in Developing a Restful service using Python Flask framework Expertise in working with Python GUI frameworks PyJamas Jython Experienced in using Apache Drill dataintensive distributed applications for interactive analysis of largescale datasets Developed end to end ETL batch and streaming data integration into HadoopMapR transforming data Used Python modules such as requests urllib urllib2 for web crawling Tools developed extensively include Spark Drill Hive HBase Kafka MapR Streams PostgreSQL Stream Sets Used Hive Queries in SparkSQL for analysis and processing the data Worked as a key role in a team of developing an initial prototype of a NiFi big data pipeline This pipeline demonstrated an end to end scenario of data ingestion processing Used HUE for running Hive queries Created Partitions according to day using Hive to improve performance Wrote Python routines to log into the websites and fetch data for selected options Worked on custom Pig Loaders and storage classes to work with variety of data formats such as JSON and XML file formats Loaded some of the data into Cassandra for fast retrieval of data Worked in provisioning and managing multitenant Hadoop clusters on public cloud environment Amazon Web Services AWS and on private cloud infrastructure Open stack cloud platform and worked on DynamoDB Ml Worked on largescale Hadoop YARN cluster for distributed data processing and analysis using Data Bricks Connectors Spark core Spark SQL Sqoop Pig Hive Impala and NoSQL databases Created HBase tables to load large sets of structured semistructured and unstructured data coming from UNIX NoSQL and a variety of portfolios Worked on a POC to compare processing time of Impala with Apache Hive for batch applications to implement the former in project Worked with various HDFS file formats like Avro Sequence File and various compression formats like Snappy bzip2 Used the RegEx JSON and Avro for serialization and deserialization packaged with Hive to parse the contents of streamed log data Converted all the vap processing from Netezza and implemented by using Spark data frames and RDDs Worked in writing Spark Sql scripts for optimizing the query performance Responsible for handling different data formats like Avro Parquet and ORC formats Implemented Spark Scripts using Scala Spark SQL to access hive tables into Spark for faster processing of data Environment Cloudera Horton Works distribution HDFS Spark Hive Pig Map Reduce Hue Sqoop Putty HaaS Hadoop as a Service Apache Kafka Apache Mesos and the AWS Java Netezza Cassandra Oozie Spark SPARK SQL Maven Java Scala SQL and Linux Toad YARN Agile Methodology Hadoop Developer BANK of America Dallas TX May 2016 to December 2016 Responsibilities Concerned and wellinformed on Hadoop Components such as HDFS Job Tracker Task Tracker Name Node Data Node YARN and Map Reduce programming Developed MapReduce programs to get rid of irregularities and aggregate the data Developed Cluster coordination services through Zookeeper Implemented Hive UDFs and did performance tuning for better results Developed Pig Latin Scripts to extract data from log files and store them to HDFS Created User Defined Functions UDFs to preprocess data for analysis Implemented Optimized Map Joins to get data from different sources to perform cleaning operations before applying the algorithms Created highly optimized SQL queries for MapReduce jobs seamlessly matching the query to the appropriate Hive table configuration to generate efficient report Used other packages such as Beautifulsoup for data parsing in Python Tuned and developed SQL on HiveQL Drill and SparkSQL Experience in using Sqoop to import and export the data from Oracle DB into HDFS and HIVE HBase Implemented CRUD operations on HBase data using thrift API to get real time insights Developed workflow in Oozie to manage and schedule jobs on Hadoop cluster for generating reports on nightly weekly and monthly basis Worked on integration independent microservices for realtime bidding scalaakka firebase cassandra Elasticsearch Used slick to query and storing in database in a Scala fashion using the powerful Scala collection framework Using HIVE processed extensively ETL loadings on a Structured Data Defined job flows and developed simple to complex Map Reduce jobs as per the requirement Optimized MapReduce Jobs to use HDFS efficiently by using various compression mechanisms Developed PIG UDFs for manipulating the data according to Business Requirements and also worked on developing custom PIG Loaders Created various Parser programs to extract data from Autosys Tibco Business Objects XML Informatica Java and database views using Scala PIG UDF was required to extract the information of the area from the huge data which we get from the sensors Responsible for creating Hive tables based on business requirements Implemented Partitioning Dynamic Partitions and Buckets in HIVE for efficient data access Involved in NoSQL database design integration and implementation Loaded data into NoSQL database HBase Worked on debugging performance tuning PIG and HIVE scripts by understanding the joins group and aggregation between them Used Flume to collect aggregate and store the web log data from different sources like web servers and pushed to HDFS Connected the hive tables to Data analyzing tools like Tableau for Graphical representation of the trends Experienced in managing and reviewing Hadoop log files Involved in loading data from UNIX file system to HDFS Responsible for design development of Spark SQL Scripts based on Functional Specifications Used Apache HUE interface to monitor and manage the HDFS storageConcerned and wellinformed on Hadoop Components such as HDFS Job Tracker Task Tracker Name Node Data Node YARN and Map Reduce programming Developed MapReduce programs to get rid of irregularities and aggregate the data Developed Cluster coordination services through Zookeeper Implemented Hive UDFs and did performance tuning for better results Developed Pig Latin Scripts to extract data from log files and store them to HDFS Created User Defined Functions UDFs to preprocess data for analysis Implemented Optimized Map Joins to get data from different sources to perform cleaning operations before applying the algorithms Created highly optimized SQL queries for MapReduce jobs seamlessly matching the query to the appropriate Hive table configuration to generate efficient report Used other packages such as Beautifulsoup for data parsing in Python Tuned and developed SQL on HiveQL Drill and SparkSQL Experience in using Sqoop to import and export the data from Oracle DB into HDFS and HIVE HBase Implemented CRUD operations on HBase data using thrift API to get real time insights Developed workflow in Oozie to manage and schedule jobs on Hadoop cluster for generating reports on nightly weekly and monthly basis Worked on integration independent microservices for realtime bidding scalaakka firebase cassandra Elasticsearch Used slick to query and storing in database in a Scala fashion using the powerful Scala collection framework Using HIVE processed extensively ETL loadings on a Structured Data Defined job flows and developed simple to complex Map Reduce jobs as per the requirement Optimized MapReduce Jobs to use HDFS efficiently by using various compression mechanisms Developed PIG UDFs for manipulating the data according to Business Requirements and also worked on developing custom PIG Loaders Created various Parser programs to extract data from Autosys Tibco Business Objects XML Informatica Java and database views using Scala PIG UDF was required to extract the information of the area from the huge data which we get from the sensors Responsible for creating Hive tables based on business requirements Implemented Partitioning Dynamic Partitions and Buckets in HIVE for efficient data access Involved in NoSQL database design integration and implementation Loaded data into NoSQL database HBase Worked on debugging performance tuning PIG and HIVE scripts by understanding the joins group and aggregation between them Used Flume to collect aggregate and store the web log data from different sources like web servers and pushed to HDFS Connected the hive tables to Data analyzing tools like Tableau for Graphical representation of the trends Experienced in managing and reviewing Hadoop log files Involved in loading data from UNIX file system to HDFS Responsible for design development of Spark SQL Scripts based on Functional Specifications Used Apache HUE interface to monitor and manage the HDFS storage Environment HDFS Map Reduce Pig Mesos AWS Hive Sqoop Scala Flume Mahout HBase Spark SPARK SQL Yarn Java Maven Git Cloudera MongoDB Eclipse and Shell Scripting Hadoop Developer DELL Bengaluru Karnataka June 2013 to August 2015 Responsibilities Designed and developed data movement framework for multiple sources like SQL Server Oracle and MySQL Created Sqoop import and export jobs for multiple sources Developed scripts to automate the creation Sqoop jobs for various workflows Developed Hive scripts to alter the tables and perform required transformations Developed a java MapReduce and PIG cleansers for data cleansing Worked on Hive UDFS to mask confidential information in the data Designed and developed MapReduce programs for data lineage Designed and developed the framework to log information for auditing and failure recovery Closed worked with the web application development team to develop the user interface for data movement framework Designed Oozie workflows for Job Automation Created Map Reduce programs to handle semiunstructured data like xml Json Avro data files and sequence files for log files A RESTful web service built with python and cherrypy retrieves data from an accumulo data warehouse Maintaining the MySQL server and Authentication to required users for Databases Appropriately documented various Administrative technical issues Developed MapReduce programs to extract and transform the data sets and results were exported back to RDBMS using Sqoop Built a RESTful web service for storing and retrieving documents in an apache accumulo data store Involved in collecting and aggregating large amounts of log data using Apache Flume and staging data in HDFS for further analysis Optimized our Hadoop infrastructure at both the Software and Hardware level Experience in troubleshooting in MapReduce jobs by reviewing log files Developed endtoend search solution using web crawler Apache Nutch Search Platform Apache SOLR EnvironmentHadoop Cloudera Manager Linux RedHat CentOs Ubuntu Operating System Scala Map Reduce HBase SQL Sqoop HDFS Kafka UML Apache SOLR Hive Oozie Cassandra maven Pig UNIX Python MR Unit Git Java Developer STATE FARM Bengaluru Karnataka July 2011 to June 2013 Responsibilities Developed the J2EE application based on the Service Oriented Architecture by employing SOAP and other tools for data exchanges and updates Developed the functionalities using Agile Methodology Used Apache Maven for project management and building the application Worked in all the modules of the application which involved frontend presentation logic developed using Spring MVC JSP JSTL and JavaScript Business objects developed using POJOs and data access layer using Hibernate framework Used JAXRS REST for producing web services and involved in writing programs to consume the web services with Apache CXF framework Used Restful API and SOAP web services for internal and external consumption Used Spring ORM module for integration with Hibernate for persistence layer Involved in writing Hibernate Query Language HQL for persistence layer Used Spring MVC Spring AOP Spring IOC Spring Transaction and Oracle to create Club Systems Component Wrote backend jobs based on Core Java Oracle Data Base to be run dailyweekly Coding the core modules of the application compliant with the JavaJ2EE coding standards and Design Patterns Written Java Script HTML CSS Servlets and JSP for designing GUI of the application Worked on Serviceside and Middletier technologies extracting catching strategiessolutions Design data access layer using Data Access Layer J2EE patterns Implementing the MVC architecture Struts Framework for handling databases across multiple locations and display information in presentation layer Used XPath for parsing the XML elements as part of business logic processing Environment Java Struts 12 Hibernate 30 JSP JavaScript HTML XML Oracle Eclipse JBoss Application Server ANT CVS and SQL Education Masters Michigan State University Ann Arbor MI Skills Hdfs Impala Oozie Sqoop Apache kafka Kafka Db2 Flume Jboss Jms Map reduce Mongodb Ms visual studio Visual studio Apache spark Api Hive Html Javascript Nodejs Additional Information TECHNICAL SKILLS Big Data Ecosystems HDFS and Map Reduce Pig Hive Pig Latin Impala YARN Oozie Zookeeper Apache Spark Apache Crunch Apache NiFi Apace STORM Apache Kappa Apache Kafka Sqoop Flume Streaming Technologies Spark Streaming Storm Scripting Languages Python Perl Shell Sheme Tcl Unix Shell Scripts Windows Power Shell Programming Languages Java J2EE JDK JDBC Hibernate XML Parsers JSP 122 Servlets EJB JMS Struts Spring Framework Java Beans AJAX JNDI Databases MongoDB Netezza SQL Server MySQL ORACLE DB2 IDEs Tools Eclipse JUnit Maven Ant MS Visual Studio Net Beans Methodologies Agile Waterfall Virtualization Technologies VMware ESXi Windows HyperV Power VM Virtual box Citrix Xen KVM Web Technologies HTML JavaScript JQuery Ajax Boot Strap Angular JS Nodejs Expressjs Web Servers Web Logic Web Sphere Apache Tomcat JBOSS Web Services SOAP RESTful API WSDL",
    "entities": [
        "Python Flask",
        "Spark Transformations",
        "Hadoop Clusters",
        "Data Acquisition Data Cleaning Data Manipulation Data Validation Data Mining Machine Learning Algorithms",
        "HBase Hands",
        "Spark Spark Streaming Spark",
        "Spark SQL Scripts",
        "Spring MVC JSP JSTL",
        "Horton",
        "GUI",
        "Club Systems Component Wrote",
        "Datastax OpsCenter",
        "SQL Education Masters",
        "Graphical",
        "Shell Scripting Hadoop Developer",
        "Data Frame Formats Upgraded",
        "HDFS",
        "UNIX",
        "Avro Sequence File",
        "Hdfs Experience",
        "Web Crawlers",
        "Maven Ant MS Visual Studio Net Beans Methodologies Agile Waterfall Virtualization Technologies",
        "Api Hive Html Javascript",
        "NiFi",
        "JSON",
        "Cloud Storage",
        "HBase Strong",
        "Java PIG Latin Scripting",
        "Netezza",
        "Amazon Web Services AWS",
        "Sparkstreaming",
        "Hadoop",
        "Oozie Sqoop",
        "XML",
        "SOAP",
        "MySQL Created Sqoop",
        "HBase",
        "Spark Storm Scala Python",
        "Apache Spark",
        "HDFS Created User Defined Functions",
        "TX",
        "Amazon",
        "JavaJ2EE",
        "Cloudera Hadoop",
        "Hadoop Ecosystems",
        "Cassandraenv",
        "Python Tuned",
        "Designed Oozie",
        "Kafka Direct Stream",
        "SparkSQL",
        "Developed",
        "CPS Energy",
        "Dallas",
        "AOP Spring IOC Spring Transaction",
        "Ambari BigQuery Big Table",
        "Hadoop Cluster Capacity Planning Performance",
        "MapR Streams PostgreSQL Stream Sets Used Hive Queries",
        "ORACLE",
        "Akka Expertise",
        "Oozie Zookeeper Work",
        "the Scala Spark",
        "RD",
        "Tools",
        "Job Automation Created",
        "JBoss Application",
        "Distributed Application",
        "San Antonio",
        "Oozie Formulated",
        "Hadoop Clusters Knowledge of Cassandra",
        "HIVE HBase Implemented",
        "JSP",
        "log files",
        "Amazon Web Services AWS Infrastructure services EC2",
        "Visual",
        "RDD Seeds",
        "Sr Spark Developer Sr Spark",
        "the Service Oriented Architecture",
        "Hardware",
        "Spark Streaming",
        "Hadoop Cluster",
        "ORC",
        "Core Java Oracle Data Base",
        "Created Partitions",
        "HDFS Connected",
        "Hdfs",
        "MVC",
        "Hive Pig HBase Zookeeper",
        "Spark",
        "Spark SQL Sqoop",
        "the RegEx JSON",
        "Present Responsibilities Hands",
        "Hadoop Jobs",
        "Responsibilities Concerned",
        "HTTP Source",
        "Spark Context Spark",
        "API",
        "PIG Loaders Created",
        "EBI",
        "Sqoop",
        "HIVE",
        "Spark Drill Hive HBase Kafka",
        "Storm",
        "Created",
        "SAX Symbolic Aggregate",
        "Structured Data",
        "the Real Time Analysis",
        "Oracle",
        "Autosys Tibco Business Objects XML Informatica Java",
        "Oracle DB",
        "PIG",
        "HDFS Responsible",
        "log data",
        "HDFS Job Tracker Task Tracker",
        "Oozie",
        "Cloud Storage Services",
        "MapReduce Yarn Sqoop Flume Pig Hive HBase",
        "SQL",
        "OLTP",
        "the Data Frames",
        "GitHub",
        "Spark RDD",
        "UDF",
        "Data Bricks Connectors Spark",
        "Appended the Data Frames",
        "Maintaining the MySQL",
        "Amazon Web Services AWS Working",
        "Michigan State",
        "Functional Specifications Used",
        "Integration of Quartz",
        "SQS Queuing SNS Notification S3",
        "Hive",
        "SQOOP",
        "Data Access Layer",
        "Used Spring",
        "NiFi Data",
        "BI Business Intelligence",
        "Json Data Handled Json Data",
        "Tableau Informatica",
        "ETL",
        "Cassandra YAML Configuration",
        "Hibernate Query Language",
        "SQL Server Oracle",
        "Schedule Spark",
        "Performed",
        "OLAP",
        "the Hadoop Ecosystem Good",
        "Impala",
        "InstalledConfiguredMaintained Apache Hadoop",
        "Sr Spark Developer CenterPoint Energy",
        "SBT",
        "Redshift Data Pipeline EMR Responsible",
        "Scala PIG",
        "Hive Developed Spark",
        "Business Requirements",
        "Hadoop Components",
        "JavaScript Business",
        "Zookeeper Implemented Hive",
        "Hive Query Language HQL",
        "Nodejs Additional Information TECHNICAL SKILLS Big Data Ecosystems",
        "MapReduce",
        "Amazon Web Services like",
        "HadoopMapR",
        "NoSQL",
        "Tableau",
        "Spark Sql",
        "Kernel",
        "Setup",
        "Redshift Data Pipeline ML Good",
        "Node",
        "JMX"
    ],
    "experience": "Experience in OLTP and OLAP design development testing implementation and support of enterprise Data warehouses Strong Knowledge in Hadoop Cluster Capacity Planning Performance Tuning Cluster Monitoring Extensive experience in business data science project life cycle including Data Acquisition Data Cleaning Data Manipulation Data Validation Data Mining Machine Learning Algorithms and Visualization Good Hands on experience in working with Ecosystems like Hive Pig Sqoop Map Reduce Flume Oozie Strong knowledge in HIVE and PIG core functionality by using custom User Defined Functions UDF User Defined TableGenerating Functions UDTF and User Defined Aggregating Functions UDAF for Hive Experience on Productionizing Apache Nifi for dataflows with significant processing requirements and controlling security of data flow Designed and developed RDD Seeds using Scala and Cascading Streaming data to Sparkstreaming using Kafka Exposure to Spark Spark Streaming Spark MLlib Scala and Creating the Data Frames handled in Spark with Scala Good Exposure on Map Reduce programming using Java PIG Latin Scripting and Distributed Application and HDFS Experienced Good understanding of NoSQL databases and hands on work experience in writing applications No SQL Databases HBase Cassandra and MongoDB Very good implementation experience of Object Oriented concepts Multithreading and JavaScala Experienced with the Scala Spark improving the performance and optimization of the existing algorithms in Hadoop using Spark Context Spark SQL Pair RDDs Spark YARN Experienced in installation configuration supporting and managing Hadoop Clusters using Apache Cloudera distributions Horton works Cloud Storage and Amazon web services AWS and related technologies DynamoDB EMR S3 ML Experience in deploying NiFi Data flow in Production team and Integrating data from multiple sources like Cassandra MongoDB Deploying templates to environments can be done via NiFi RestAPI integrated with other automation tools Complete end to end design and development of Apache NiFi flow which acts as the agent between middleware team and EBI team and executes all the actions mentioned above Experienced in Python programming wrote Web Crawlers using Python Experience in bench marking Hadoop cluster for analysis of queue usage Experienced in working with Mahout for applying machine learning techniques in the Hadoop Ecosystem Good Experience on Amazon Web Services like Redshift Data Pipeline ML Good experienced on moving the data in and out of Hadoop RDBMS NoSQL and UNIX from various systems using SQOOP and other traditional data movement technologies Experience on Integration of Quartz scheduler with Oozie work flows to get data from multiple data sources in parallel using fork Experience in installation configuration support and management of a Hadoop Cluster using Cloudera Distributions Experienced Spark scripts by using Scala shell as per requirements Good knowledge on tuning the Spark jobs by changing the configuration properties and using broadcast variables Developed REST APIs using Java Play framework and Akka Expertise in search technologys like SOLR Informatica Lucene Experience in converting SQL queries into Spark Transformations using Spark RDDs and Scala and Performed mapside joins on RDDs Experienced in writing Hadoop Jobs for analyzing data using Hive Query Language HQL Pig Latin Data flow language and custom MapReduce programs in Java Good understanding of NoSQL databases like MongoDB Cassandra and HBase Strong analytical skills with ability to quickly understand clients business needs Involved in business meetings for requirements gathering form business clients Experienced in Storm builder topologies to perform cleansing operations before moving data into HBase Hands on experience in configuring and working with Flume to load the data from multiple sources directly into Hdfs Experience on configuring fully the Flume agent suitable for all type of logger data and store them in Avro Sink in Parquet file format and developing 2tier architecture connecting channels between Avro sinks and Source Experience creating Visual report Graphical analysis and Dashboard reports using Tableau Informatica of historical data saved in Hdfs and data analysis using Splunk enterprise edition Good experience in utilizing Cloud Storage Services like Git Extensive knowledge in using GitHub and Bit Bucket Experienced in job scheduling and monitoring using Oozie Zookeeper Work Experience Sr Spark Developer CenterPoint Energy Houston TX June 2018 to Present Responsibilities Hands on experience in installation configuration supporting and managing Hadoop Clusters Knowledge of Cassandra security maintenance and tuning both database and server Chipped away at outlining and building up the Real Time Analysis module for Analytic Dashboard utilizing Cassandra Kafka Spark Streaming Installed and configured Confluent Kafka in RD line Validated the installation with HDFS connector and Hive connectors Deployed high availability on the Hadoop cluster quorum journal nodes Experience on implementing SAX Symbolic Aggregate approXimation in Java to use with Apache Spark for normalizing time series data Involved in defining job flows managing and reviewing log file Setup configured and optimized the Cassandra cluster Developed realtime Spark based application to work along with the Cassandra database Responsible to manage data coming from different sources through Kafka Installed Kafka Producer on different severs and Scheduled to produce data for every 10 seconds Integrated Kafka with Spark Streaming to listen onto multiple Kafka Brokers with different Kafka topics for every 5 Seconds Enhanced and optimized product Spark code to aggregate group and run data mining tasks using the Spark framework and handled Json Data Handled Json Data comes from Kafka Direct Stream on each partitions and transformed them into required Data Frame Formats Upgraded Spark 16 to latest Version Spark 22 and configure Kafka Version 010 Managing Kafka Offsets Saving Offsets in external data base like HBase and to its own Kafka Worked on Import Export of data using ETL tool Sqoop from MySQL to HDFS Worked on Lambda Architecture for both Batch processing and Real Streaming purposes Used Oozie to Schedule Spark and Kafka Producer Jobs to run in parallel Appended the Data Frames into Cassandra Key Space Tables using DataStax SparkCassandra Connector Experience with Cassandra YAML Configuration files RACK DC properties file Cassandraenv file for JMX configurations etc Installed and configured Datastax OpsCenter and Nagios for Cassandra cluster maintenance and alert Configured Authentication and security in Apache kafka pubsub system Good experience with Century Link Cloud for provisioning virtual machines creating resource groups configuring key vaults for storing encryption keys Monitoring etc Great Hands on Experience in seat stamping Hadoop bunch for investigation of line utilization Performing OS level setups and Kernel level tuning Implement and test integration of BI Business Intelligence tools with Hadoop stack InstalledConfiguredMaintained Apache Hadoop clusters for application development and Hadoop tools like Hive Pig HBase Zookeeper Sqoop Yarn Spark2 Kafka and Oozie Formulated procedures for installation of Hadoop Spark2 patches updates and version upgrades Environment Cloudera HDFS Spark Hive Pig Map Reduce Hue Sqoop Putt Apache Kafka Apache Drill Century Link Cloud AWS Java Netezza Cassandra Oozie Spark SPARK SQL Maven SBT Java Scala SQL and Linux YARN Agile Methodology Solr PHP Admin XAMPP DataStax Cassandra Sr HadoopSpark Developer CPS Energy San Antonio TX January 2017 to June 2018 Responsibilities Involved in deploying systems on Amazon Web Services AWS Infrastructure services EC2 Experience in configuring deploying the web applications on AWS servers using SBT and Play Migrated Map Reduce jobs into Spark RDD transformations using Scala Used Spark API over Cloudera Hadoop YARN to perform analytics on data in Hive Developed Spark code using Spark RDD and SparkSQLStreaming for faster processing of data Performed configuration deployment and support of cloud services including Amazon Web Services AWS Working knowledge of various AWS technologies like SQS Queuing SNS Notification S3 storage Redshift Data Pipeline EMR Responsible for all Public AWS and Private OpenstackVMWareDCOSMesosMarathon cloud infrastructure Developed Flume ETL job for handling data from HTTP Source and Sink as HDFS and configuring Data Pipelining Used Hive data warehouse tool to analyze the unified historic data in HDFS to identify issues and behavioral patterns Involved in Developing a Restful service using Python Flask framework Expertise in working with Python GUI frameworks PyJamas Jython Experienced in using Apache Drill dataintensive distributed applications for interactive analysis of largescale datasets Developed end to end ETL batch and streaming data integration into HadoopMapR transforming data Used Python modules such as requests urllib urllib2 for web crawling Tools developed extensively include Spark Drill Hive HBase Kafka MapR Streams PostgreSQL Stream Sets Used Hive Queries in SparkSQL for analysis and processing the data Worked as a key role in a team of developing an initial prototype of a NiFi big data pipeline This pipeline demonstrated an end to end scenario of data ingestion processing Used HUE for running Hive queries Created Partitions according to day using Hive to improve performance Wrote Python routines to log into the websites and fetch data for selected options Worked on custom Pig Loaders and storage classes to work with variety of data formats such as JSON and XML file formats Loaded some of the data into Cassandra for fast retrieval of data Worked in provisioning and managing multitenant Hadoop clusters on public cloud environment Amazon Web Services AWS and on private cloud infrastructure Open stack cloud platform and worked on DynamoDB Ml Worked on largescale Hadoop YARN cluster for distributed data processing and analysis using Data Bricks Connectors Spark core Spark SQL Sqoop Pig Hive Impala and NoSQL databases Created HBase tables to load large sets of structured semistructured and unstructured data coming from UNIX NoSQL and a variety of portfolios Worked on a POC to compare processing time of Impala with Apache Hive for batch applications to implement the former in project Worked with various HDFS file formats like Avro Sequence File and various compression formats like Snappy bzip2 Used the RegEx JSON and Avro for serialization and deserialization packaged with Hive to parse the contents of streamed log data Converted all the vap processing from Netezza and implemented by using Spark data frames and RDDs Worked in writing Spark Sql scripts for optimizing the query performance Responsible for handling different data formats like Avro Parquet and ORC formats Implemented Spark Scripts using Scala Spark SQL to access hive tables into Spark for faster processing of data Environment Cloudera Horton Works distribution HDFS Spark Hive Pig Map Reduce Hue Sqoop Putty HaaS Hadoop as a Service Apache Kafka Apache Mesos and the AWS Java Netezza Cassandra Oozie Spark SPARK SQL Maven Java Scala SQL and Linux Toad YARN Agile Methodology Hadoop Developer BANK of America Dallas TX May 2016 to December 2016 Responsibilities Concerned and wellinformed on Hadoop Components such as HDFS Job Tracker Task Tracker Name Node Data Node YARN and Map Reduce programming Developed MapReduce programs to get rid of irregularities and aggregate the data Developed Cluster coordination services through Zookeeper Implemented Hive UDFs and did performance tuning for better results Developed Pig Latin Scripts to extract data from log files and store them to HDFS Created User Defined Functions UDFs to preprocess data for analysis Implemented Optimized Map Joins to get data from different sources to perform cleaning operations before applying the algorithms Created highly optimized SQL queries for MapReduce jobs seamlessly matching the query to the appropriate Hive table configuration to generate efficient report Used other packages such as Beautifulsoup for data parsing in Python Tuned and developed SQL on HiveQL Drill and SparkSQL Experience in using Sqoop to import and export the data from Oracle DB into HDFS and HIVE HBase Implemented CRUD operations on HBase data using thrift API to get real time insights Developed workflow in Oozie to manage and schedule jobs on Hadoop cluster for generating reports on nightly weekly and monthly basis Worked on integration independent microservices for realtime bidding scalaakka firebase cassandra Elasticsearch Used slick to query and storing in database in a Scala fashion using the powerful Scala collection framework Using HIVE processed extensively ETL loadings on a Structured Data Defined job flows and developed simple to complex Map Reduce jobs as per the requirement Optimized MapReduce Jobs to use HDFS efficiently by using various compression mechanisms Developed PIG UDFs for manipulating the data according to Business Requirements and also worked on developing custom PIG Loaders Created various Parser programs to extract data from Autosys Tibco Business Objects XML Informatica Java and database views using Scala PIG UDF was required to extract the information of the area from the huge data which we get from the sensors Responsible for creating Hive tables based on business requirements Implemented Partitioning Dynamic Partitions and Buckets in HIVE for efficient data access Involved in NoSQL database design integration and implementation Loaded data into NoSQL database HBase Worked on debugging performance tuning PIG and HIVE scripts by understanding the joins group and aggregation between them Used Flume to collect aggregate and store the web log data from different sources like web servers and pushed to HDFS Connected the hive tables to Data analyzing tools like Tableau for Graphical representation of the trends Experienced in managing and reviewing Hadoop log files Involved in loading data from UNIX file system to HDFS Responsible for design development of Spark SQL Scripts based on Functional Specifications Used Apache HUE interface to monitor and manage the HDFS storageConcerned and wellinformed on Hadoop Components such as HDFS Job Tracker Task Tracker Name Node Data Node YARN and Map Reduce programming Developed MapReduce programs to get rid of irregularities and aggregate the data Developed Cluster coordination services through Zookeeper Implemented Hive UDFs and did performance tuning for better results Developed Pig Latin Scripts to extract data from log files and store them to HDFS Created User Defined Functions UDFs to preprocess data for analysis Implemented Optimized Map Joins to get data from different sources to perform cleaning operations before applying the algorithms Created highly optimized SQL queries for MapReduce jobs seamlessly matching the query to the appropriate Hive table configuration to generate efficient report Used other packages such as Beautifulsoup for data parsing in Python Tuned and developed SQL on HiveQL Drill and SparkSQL Experience in using Sqoop to import and export the data from Oracle DB into HDFS and HIVE HBase Implemented CRUD operations on HBase data using thrift API to get real time insights Developed workflow in Oozie to manage and schedule jobs on Hadoop cluster for generating reports on nightly weekly and monthly basis Worked on integration independent microservices for realtime bidding scalaakka firebase cassandra Elasticsearch Used slick to query and storing in database in a Scala fashion using the powerful Scala collection framework Using HIVE processed extensively ETL loadings on a Structured Data Defined job flows and developed simple to complex Map Reduce jobs as per the requirement Optimized MapReduce Jobs to use HDFS efficiently by using various compression mechanisms Developed PIG UDFs for manipulating the data according to Business Requirements and also worked on developing custom PIG Loaders Created various Parser programs to extract data from Autosys Tibco Business Objects XML Informatica Java and database views using Scala PIG UDF was required to extract the information of the area from the huge data which we get from the sensors Responsible for creating Hive tables based on business requirements Implemented Partitioning Dynamic Partitions and Buckets in HIVE for efficient data access Involved in NoSQL database design integration and implementation Loaded data into NoSQL database HBase Worked on debugging performance tuning PIG and HIVE scripts by understanding the joins group and aggregation between them Used Flume to collect aggregate and store the web log data from different sources like web servers and pushed to HDFS Connected the hive tables to Data analyzing tools like Tableau for Graphical representation of the trends Experienced in managing and reviewing Hadoop log files Involved in loading data from UNIX file system to HDFS Responsible for design development of Spark SQL Scripts based on Functional Specifications Used Apache HUE interface to monitor and manage the HDFS storage Environment HDFS Map Reduce Pig Mesos AWS Hive Sqoop Scala Flume Mahout HBase Spark SPARK SQL Yarn Java Maven Git Cloudera MongoDB Eclipse and Shell Scripting Hadoop Developer DELL Bengaluru Karnataka June 2013 to August 2015 Responsibilities Designed and developed data movement framework for multiple sources like SQL Server Oracle and MySQL Created Sqoop import and export jobs for multiple sources Developed scripts to automate the creation Sqoop jobs for various workflows Developed Hive scripts to alter the tables and perform required transformations Developed a java MapReduce and PIG cleansers for data cleansing Worked on Hive UDFS to mask confidential information in the data Designed and developed MapReduce programs for data lineage Designed and developed the framework to log information for auditing and failure recovery Closed worked with the web application development team to develop the user interface for data movement framework Designed Oozie workflows for Job Automation Created Map Reduce programs to handle semiunstructured data like xml Json Avro data files and sequence files for log files A RESTful web service built with python and cherrypy retrieves data from an accumulo data warehouse Maintaining the MySQL server and Authentication to required users for Databases Appropriately documented various Administrative technical issues Developed MapReduce programs to extract and transform the data sets and results were exported back to RDBMS using Sqoop Built a RESTful web service for storing and retrieving documents in an apache accumulo data store Involved in collecting and aggregating large amounts of log data using Apache Flume and staging data in HDFS for further analysis Optimized our Hadoop infrastructure at both the Software and Hardware level Experience in troubleshooting in MapReduce jobs by reviewing log files Developed endtoend search solution using web crawler Apache Nutch Search Platform Apache SOLR EnvironmentHadoop Cloudera Manager Linux RedHat CentOs Ubuntu Operating System Scala Map Reduce HBase SQL Sqoop HDFS Kafka UML Apache SOLR Hive Oozie Cassandra maven Pig UNIX Python MR Unit Git Java Developer STATE FARM Bengaluru Karnataka July 2011 to June 2013 Responsibilities Developed the J2EE application based on the Service Oriented Architecture by employing SOAP and other tools for data exchanges and updates Developed the functionalities using Agile Methodology Used Apache Maven for project management and building the application Worked in all the modules of the application which involved frontend presentation logic developed using Spring MVC JSP JSTL and JavaScript Business objects developed using POJOs and data access layer using Hibernate framework Used JAXRS REST for producing web services and involved in writing programs to consume the web services with Apache CXF framework Used Restful API and SOAP web services for internal and external consumption Used Spring ORM module for integration with Hibernate for persistence layer Involved in writing Hibernate Query Language HQL for persistence layer Used Spring MVC Spring AOP Spring IOC Spring Transaction and Oracle to create Club Systems Component Wrote backend jobs based on Core Java Oracle Data Base to be run dailyweekly Coding the core modules of the application compliant with the JavaJ2EE coding standards and Design Patterns Written Java Script HTML CSS Servlets and JSP for designing GUI of the application Worked on Serviceside and Middletier technologies extracting catching strategiessolutions Design data access layer using Data Access Layer J2EE patterns Implementing the MVC architecture Struts Framework for handling databases across multiple locations and display information in presentation layer Used XPath for parsing the XML elements as part of business logic processing Environment Java Struts 12 Hibernate 30 JSP JavaScript HTML XML Oracle Eclipse JBoss Application Server ANT CVS and SQL Education Masters Michigan State University Ann Arbor MI Skills Hdfs Impala Oozie Sqoop Apache kafka Kafka Db2 Flume Jboss Jms Map reduce Mongodb Ms visual studio Visual studio Apache spark Api Hive Html Javascript Nodejs Additional Information TECHNICAL SKILLS Big Data Ecosystems HDFS and Map Reduce Pig Hive Pig Latin Impala YARN Oozie Zookeeper Apache Spark Apache Crunch Apache NiFi Apace STORM Apache Kappa Apache Kafka Sqoop Flume Streaming Technologies Spark Streaming Storm Scripting Languages Python Perl Shell Sheme Tcl Unix Shell Scripts Windows Power Shell Programming Languages Java J2EE JDK JDBC Hibernate XML Parsers JSP 122 Servlets EJB JMS Struts Spring Framework Java Beans AJAX JNDI Databases MongoDB Netezza SQL Server MySQL ORACLE DB2 IDEs Tools Eclipse JUnit Maven Ant MS Visual Studio Net Beans Methodologies Agile Waterfall Virtualization Technologies VMware ESXi Windows HyperV Power VM Virtual box Citrix Xen KVM Web Technologies HTML JavaScript JQuery Ajax Boot Strap Angular JS Nodejs Expressjs Web Servers Web Logic Web Sphere Apache Tomcat JBOSS Web Services SOAP RESTful API WSDL",
    "extracted_keywords": [
        "Sr",
        "Spark",
        "Developer",
        "Sr",
        "Spark",
        "span",
        "lDeveloperspan",
        "Sr",
        "Spark",
        "Developer",
        "CenterPoint",
        "Energy",
        "years",
        "handson",
        "experience",
        "IT",
        "industry",
        "years",
        "experience",
        "deployment",
        "Hadoop",
        "Ecosystems",
        "MapReduce",
        "Yarn",
        "Sqoop",
        "Flume",
        "Pig",
        "Hive",
        "HBase",
        "Cassandra",
        "Zoo",
        "Keeper",
        "Oozie",
        "Ambari",
        "BigQuery",
        "Big",
        "Table",
        "years",
        "experience",
        "Spark",
        "Storm",
        "Scala",
        "Python",
        "Experience",
        "OLTP",
        "OLAP",
        "design",
        "development",
        "testing",
        "implementation",
        "support",
        "enterprise",
        "Data",
        "Strong",
        "Knowledge",
        "Hadoop",
        "Cluster",
        "Capacity",
        "Planning",
        "Performance",
        "Cluster",
        "Monitoring",
        "experience",
        "business",
        "data",
        "science",
        "project",
        "life",
        "cycle",
        "Data",
        "Acquisition",
        "Data",
        "Cleaning",
        "Data",
        "Manipulation",
        "Data",
        "Validation",
        "Data",
        "Mining",
        "Machine",
        "Learning",
        "Algorithms",
        "Visualization",
        "Good",
        "Hands",
        "experience",
        "Ecosystems",
        "Hive",
        "Pig",
        "Sqoop",
        "Map",
        "Reduce",
        "Flume",
        "Oozie",
        "knowledge",
        "HIVE",
        "PIG",
        "core",
        "functionality",
        "custom",
        "User",
        "Defined",
        "Functions",
        "UDF",
        "User",
        "Defined",
        "TableGenerating",
        "Functions",
        "UDTF",
        "User",
        "Defined",
        "Aggregating",
        "Functions",
        "UDAF",
        "Hive",
        "Experience",
        "Apache",
        "Nifi",
        "dataflows",
        "processing",
        "requirements",
        "security",
        "data",
        "flow",
        "RDD",
        "Seeds",
        "Scala",
        "Streaming",
        "data",
        "Sparkstreaming",
        "Kafka",
        "Exposure",
        "Spark",
        "Spark",
        "Streaming",
        "Spark",
        "MLlib",
        "Scala",
        "Data",
        "Frames",
        "Spark",
        "Scala",
        "Good",
        "Exposure",
        "Map",
        "Reduce",
        "programming",
        "Java",
        "PIG",
        "Latin",
        "Scripting",
        "Distributed",
        "Application",
        "HDFS",
        "understanding",
        "NoSQL",
        "databases",
        "hands",
        "work",
        "experience",
        "writing",
        "applications",
        "SQL",
        "HBase",
        "Cassandra",
        "implementation",
        "experience",
        "Object",
        "concepts",
        "Multithreading",
        "JavaScala",
        "Scala",
        "Spark",
        "performance",
        "optimization",
        "algorithms",
        "Hadoop",
        "Spark",
        "Context",
        "Spark",
        "SQL",
        "Pair",
        "Spark",
        "YARN",
        "installation",
        "configuration",
        "Hadoop",
        "Clusters",
        "Apache",
        "Cloudera",
        "distributions",
        "Horton",
        "Cloud",
        "Storage",
        "Amazon",
        "web",
        "services",
        "AWS",
        "technologies",
        "EMR",
        "S3",
        "ML",
        "Experience",
        "NiFi",
        "Data",
        "flow",
        "Production",
        "team",
        "data",
        "sources",
        "Cassandra",
        "Deploying",
        "templates",
        "environments",
        "NiFi",
        "RestAPI",
        "automation",
        "tools",
        "end",
        "design",
        "development",
        "Apache",
        "NiFi",
        "flow",
        "agent",
        "middleware",
        "team",
        "EBI",
        "team",
        "actions",
        "Python",
        "programming",
        "Web",
        "Crawlers",
        "Python",
        "Experience",
        "bench",
        "Hadoop",
        "cluster",
        "analysis",
        "queue",
        "usage",
        "Mahout",
        "machine",
        "techniques",
        "Hadoop",
        "Ecosystem",
        "Good",
        "Experience",
        "Amazon",
        "Web",
        "Services",
        "Redshift",
        "Data",
        "Pipeline",
        "ML",
        "Good",
        "data",
        "Hadoop",
        "RDBMS",
        "NoSQL",
        "UNIX",
        "systems",
        "SQOOP",
        "data",
        "movement",
        "technologies",
        "Experience",
        "Integration",
        "Quartz",
        "scheduler",
        "Oozie",
        "work",
        "data",
        "data",
        "sources",
        "fork",
        "Experience",
        "installation",
        "configuration",
        "support",
        "management",
        "Hadoop",
        "Cluster",
        "Cloudera",
        "Distributions",
        "Spark",
        "scripts",
        "Scala",
        "shell",
        "requirements",
        "knowledge",
        "Spark",
        "jobs",
        "configuration",
        "properties",
        "broadcast",
        "variables",
        "REST",
        "APIs",
        "Java",
        "Play",
        "framework",
        "Akka",
        "Expertise",
        "search",
        "technologys",
        "SOLR",
        "Informatica",
        "Lucene",
        "Experience",
        "SQL",
        "queries",
        "Spark",
        "Transformations",
        "Spark",
        "RDDs",
        "Scala",
        "Performed",
        "mapside",
        "RDDs",
        "Hadoop",
        "Jobs",
        "data",
        "Hive",
        "Query",
        "Language",
        "HQL",
        "Pig",
        "Latin",
        "Data",
        "flow",
        "language",
        "custom",
        "MapReduce",
        "programs",
        "Java",
        "understanding",
        "MongoDB",
        "Cassandra",
        "HBase",
        "skills",
        "ability",
        "clients",
        "business",
        "needs",
        "business",
        "meetings",
        "requirements",
        "form",
        "business",
        "clients",
        "Storm",
        "builder",
        "topologies",
        "cleansing",
        "operations",
        "data",
        "HBase",
        "Hands",
        "experience",
        "configuring",
        "Flume",
        "data",
        "sources",
        "Hdfs",
        "Experience",
        "Flume",
        "agent",
        "type",
        "data",
        "Avro",
        "Sink",
        "Parquet",
        "file",
        "format",
        "architecture",
        "channels",
        "Avro",
        "sinks",
        "Source",
        "Experience",
        "report",
        "analysis",
        "Dashboard",
        "reports",
        "Tableau",
        "Informatica",
        "data",
        "Hdfs",
        "data",
        "analysis",
        "Splunk",
        "enterprise",
        "edition",
        "experience",
        "Cloud",
        "Storage",
        "Services",
        "Git",
        "knowledge",
        "GitHub",
        "Bit",
        "Bucket",
        "job",
        "scheduling",
        "monitoring",
        "Oozie",
        "Zookeeper",
        "Work",
        "Experience",
        "Sr",
        "Spark",
        "Developer",
        "CenterPoint",
        "Energy",
        "Houston",
        "TX",
        "June",
        "Present",
        "Responsibilities",
        "Hands",
        "experience",
        "installation",
        "configuration",
        "Hadoop",
        "Clusters",
        "Knowledge",
        "Cassandra",
        "security",
        "maintenance",
        "database",
        "server",
        "Real",
        "Time",
        "Analysis",
        "module",
        "Analytic",
        "Dashboard",
        "Cassandra",
        "Kafka",
        "Spark",
        "Streaming",
        "Installed",
        "Confluent",
        "Kafka",
        "RD",
        "line",
        "installation",
        "HDFS",
        "connector",
        "Hive",
        "connectors",
        "availability",
        "Hadoop",
        "cluster",
        "quorum",
        "journal",
        "nodes",
        "Experience",
        "SAX",
        "Symbolic",
        "Aggregate",
        "approXimation",
        "Java",
        "Apache",
        "Spark",
        "time",
        "series",
        "data",
        "job",
        "log",
        "file",
        "Setup",
        "Cassandra",
        "cluster",
        "realtime",
        "Spark",
        "application",
        "Cassandra",
        "database",
        "data",
        "sources",
        "Kafka",
        "Installed",
        "Kafka",
        "Producer",
        "severs",
        "data",
        "seconds",
        "Integrated",
        "Kafka",
        "Spark",
        "Streaming",
        "Kafka",
        "Brokers",
        "Kafka",
        "topics",
        "Seconds",
        "product",
        "Spark",
        "code",
        "group",
        "data",
        "mining",
        "tasks",
        "Spark",
        "framework",
        "Json",
        "Data",
        "Json",
        "Data",
        "Kafka",
        "Direct",
        "Stream",
        "partitions",
        "Data",
        "Frame",
        "Formats",
        "Spark",
        "Version",
        "Spark",
        "Kafka",
        "Version",
        "Managing",
        "Kafka",
        "Offsets",
        "Saving",
        "Offsets",
        "data",
        "base",
        "HBase",
        "Kafka",
        "Import",
        "Export",
        "data",
        "ETL",
        "tool",
        "Sqoop",
        "MySQL",
        "HDFS",
        "Worked",
        "Lambda",
        "Architecture",
        "Batch",
        "processing",
        "Real",
        "Streaming",
        "purposes",
        "Oozie",
        "Spark",
        "Kafka",
        "Producer",
        "Jobs",
        "parallel",
        "Appended",
        "Data",
        "Frames",
        "Cassandra",
        "Key",
        "Space",
        "Tables",
        "DataStax",
        "SparkCassandra",
        "Connector",
        "Experience",
        "Cassandra",
        "YAML",
        "Configuration",
        "files",
        "RACK",
        "DC",
        "properties",
        "file",
        "JMX",
        "configurations",
        "Datastax",
        "OpsCenter",
        "Nagios",
        "Cassandra",
        "cluster",
        "maintenance",
        "alert",
        "Configured",
        "Authentication",
        "security",
        "Apache",
        "kafka",
        "system",
        "experience",
        "Century",
        "Link",
        "Cloud",
        "machines",
        "resource",
        "groups",
        "vaults",
        "encryption",
        "Hands",
        "Experience",
        "seat",
        "Hadoop",
        "bunch",
        "investigation",
        "line",
        "utilization",
        "OS",
        "level",
        "setups",
        "Kernel",
        "level",
        "Implement",
        "test",
        "integration",
        "BI",
        "Business",
        "Intelligence",
        "tools",
        "Hadoop",
        "stack",
        "Apache",
        "Hadoop",
        "clusters",
        "application",
        "development",
        "Hadoop",
        "tools",
        "Hive",
        "Pig",
        "HBase",
        "Zookeeper",
        "Sqoop",
        "Yarn",
        "Spark2",
        "Kafka",
        "Oozie",
        "Formulated",
        "procedures",
        "installation",
        "Hadoop",
        "Spark2",
        "updates",
        "version",
        "upgrades",
        "Environment",
        "Cloudera",
        "HDFS",
        "Spark",
        "Hive",
        "Pig",
        "Map",
        "Reduce",
        "Hue",
        "Sqoop",
        "Putt",
        "Apache",
        "Kafka",
        "Apache",
        "Drill",
        "Century",
        "Link",
        "Cloud",
        "Java",
        "Netezza",
        "Cassandra",
        "Oozie",
        "Spark",
        "SPARK",
        "SQL",
        "Maven",
        "SBT",
        "Java",
        "Scala",
        "SQL",
        "Linux",
        "YARN",
        "Agile",
        "Methodology",
        "Solr",
        "PHP",
        "Admin",
        "XAMPP",
        "DataStax",
        "Cassandra",
        "Sr",
        "HadoopSpark",
        "Developer",
        "CPS",
        "Energy",
        "San",
        "Antonio",
        "TX",
        "January",
        "June",
        "Responsibilities",
        "systems",
        "Amazon",
        "Web",
        "Services",
        "AWS",
        "Infrastructure",
        "services",
        "EC2",
        "Experience",
        "web",
        "applications",
        "AWS",
        "servers",
        "SBT",
        "Map",
        "jobs",
        "Spark",
        "RDD",
        "transformations",
        "Scala",
        "Spark",
        "API",
        "Cloudera",
        "Hadoop",
        "YARN",
        "analytics",
        "data",
        "Hive",
        "Developed",
        "Spark",
        "code",
        "Spark",
        "RDD",
        "SparkSQLStreaming",
        "processing",
        "data",
        "configuration",
        "deployment",
        "support",
        "cloud",
        "services",
        "Amazon",
        "Web",
        "Services",
        "AWS",
        "knowledge",
        "AWS",
        "technologies",
        "SQS",
        "Queuing",
        "SNS",
        "Notification",
        "S3",
        "storage",
        "Redshift",
        "Data",
        "Pipeline",
        "EMR",
        "AWS",
        "cloud",
        "infrastructure",
        "Developed",
        "Flume",
        "ETL",
        "job",
        "data",
        "HTTP",
        "Source",
        "Sink",
        "HDFS",
        "Data",
        "Hive",
        "data",
        "warehouse",
        "tool",
        "data",
        "HDFS",
        "issues",
        "patterns",
        "service",
        "Python",
        "Flask",
        "framework",
        "Expertise",
        "Python",
        "GUI",
        "PyJamas",
        "Jython",
        "Apache",
        "Drill",
        "applications",
        "analysis",
        "largescale",
        "datasets",
        "end",
        "ETL",
        "batch",
        "streaming",
        "data",
        "integration",
        "HadoopMapR",
        "data",
        "Python",
        "modules",
        "requests",
        "urllib2",
        "web",
        "Tools",
        "Spark",
        "Drill",
        "Hive",
        "HBase",
        "Kafka",
        "MapR",
        "Streams",
        "PostgreSQL",
        "Stream",
        "Sets",
        "Hive",
        "Queries",
        "SparkSQL",
        "analysis",
        "data",
        "role",
        "team",
        "prototype",
        "NiFi",
        "data",
        "pipeline",
        "pipeline",
        "end",
        "scenario",
        "data",
        "ingestion",
        "processing",
        "HUE",
        "Hive",
        "queries",
        "Partitions",
        "day",
        "Hive",
        "performance",
        "Wrote",
        "Python",
        "websites",
        "data",
        "options",
        "custom",
        "Pig",
        "Loaders",
        "storage",
        "classes",
        "variety",
        "data",
        "formats",
        "JSON",
        "XML",
        "file",
        "formats",
        "data",
        "Cassandra",
        "retrieval",
        "data",
        "provisioning",
        "Hadoop",
        "clusters",
        "cloud",
        "environment",
        "Amazon",
        "Web",
        "Services",
        "AWS",
        "cloud",
        "infrastructure",
        "Open",
        "stack",
        "cloud",
        "platform",
        "Ml",
        "largescale",
        "Hadoop",
        "YARN",
        "cluster",
        "data",
        "processing",
        "analysis",
        "Data",
        "Bricks",
        "Connectors",
        "Spark",
        "core",
        "Spark",
        "SQL",
        "Sqoop",
        "Pig",
        "Hive",
        "Impala",
        "NoSQL",
        "Created",
        "HBase",
        "sets",
        "data",
        "UNIX",
        "NoSQL",
        "variety",
        "portfolios",
        "POC",
        "processing",
        "time",
        "Impala",
        "Apache",
        "Hive",
        "batch",
        "applications",
        "project",
        "HDFS",
        "file",
        "formats",
        "Avro",
        "Sequence",
        "File",
        "compression",
        "formats",
        "bzip2",
        "RegEx",
        "JSON",
        "Avro",
        "serialization",
        "deserialization",
        "Hive",
        "contents",
        "log",
        "data",
        "vap",
        "processing",
        "Netezza",
        "Spark",
        "data",
        "frames",
        "RDDs",
        "Spark",
        "Sql",
        "scripts",
        "query",
        "performance",
        "data",
        "formats",
        "Avro",
        "Parquet",
        "ORC",
        "formats",
        "Spark",
        "Scripts",
        "Scala",
        "Spark",
        "SQL",
        "tables",
        "Spark",
        "processing",
        "data",
        "Environment",
        "Cloudera",
        "Horton",
        "distribution",
        "HDFS",
        "Spark",
        "Hive",
        "Pig",
        "Map",
        "Reduce",
        "Hue",
        "Sqoop",
        "Putty",
        "HaaS",
        "Hadoop",
        "Service",
        "Apache",
        "Kafka",
        "Apache",
        "Mesos",
        "AWS",
        "Java",
        "Netezza",
        "Cassandra",
        "Oozie",
        "Spark",
        "SPARK",
        "SQL",
        "Maven",
        "Java",
        "Scala",
        "SQL",
        "Linux",
        "Toad",
        "YARN",
        "Agile",
        "Methodology",
        "Hadoop",
        "Developer",
        "BANK",
        "America",
        "Dallas",
        "TX",
        "May",
        "December",
        "Responsibilities",
        "Hadoop",
        "Components",
        "HDFS",
        "Job",
        "Tracker",
        "Task",
        "Tracker",
        "Name",
        "Node",
        "Data",
        "Node",
        "YARN",
        "Map",
        "Reduce",
        "programming",
        "MapReduce",
        "programs",
        "irregularities",
        "data",
        "Cluster",
        "coordination",
        "services",
        "Zookeeper",
        "Hive",
        "UDFs",
        "performance",
        "results",
        "Developed",
        "Pig",
        "Latin",
        "Scripts",
        "data",
        "log",
        "files",
        "HDFS",
        "Created",
        "User",
        "Defined",
        "Functions",
        "UDFs",
        "data",
        "analysis",
        "Map",
        "Joins",
        "data",
        "sources",
        "operations",
        "algorithms",
        "SQL",
        "queries",
        "MapReduce",
        "jobs",
        "query",
        "Hive",
        "table",
        "configuration",
        "report",
        "packages",
        "Beautifulsoup",
        "data",
        "Python",
        "SQL",
        "HiveQL",
        "Drill",
        "SparkSQL",
        "Experience",
        "Sqoop",
        "data",
        "Oracle",
        "DB",
        "HDFS",
        "HIVE",
        "HBase",
        "CRUD",
        "operations",
        "HBase",
        "data",
        "thrift",
        "API",
        "time",
        "insights",
        "workflow",
        "Oozie",
        "schedule",
        "jobs",
        "Hadoop",
        "cluster",
        "reports",
        "basis",
        "integration",
        "microservices",
        "bidding",
        "scalaakka",
        "firebase",
        "cassandra",
        "Elasticsearch",
        "slick",
        "query",
        "database",
        "Scala",
        "fashion",
        "Scala",
        "collection",
        "framework",
        "HIVE",
        "ETL",
        "loadings",
        "Structured",
        "Data",
        "job",
        "flows",
        "Map",
        "Reduce",
        "jobs",
        "requirement",
        "MapReduce",
        "Jobs",
        "HDFS",
        "compression",
        "mechanisms",
        "PIG",
        "UDFs",
        "data",
        "Business",
        "Requirements",
        "custom",
        "PIG",
        "Loaders",
        "Parser",
        "programs",
        "data",
        "Autosys",
        "Tibco",
        "Business",
        "XML",
        "Informatica",
        "Java",
        "database",
        "views",
        "Scala",
        "PIG",
        "UDF",
        "information",
        "area",
        "data",
        "sensors",
        "Hive",
        "tables",
        "business",
        "requirements",
        "Partitioning",
        "Dynamic",
        "Partitions",
        "Buckets",
        "HIVE",
        "data",
        "access",
        "NoSQL",
        "database",
        "design",
        "integration",
        "implementation",
        "data",
        "NoSQL",
        "database",
        "HBase",
        "performance",
        "PIG",
        "HIVE",
        "scripts",
        "joins",
        "group",
        "aggregation",
        "Flume",
        "aggregate",
        "web",
        "log",
        "data",
        "sources",
        "web",
        "servers",
        "Connected",
        "tables",
        "Data",
        "tools",
        "Tableau",
        "representation",
        "trends",
        "Hadoop",
        "log",
        "files",
        "loading",
        "data",
        "UNIX",
        "file",
        "system",
        "design",
        "development",
        "Spark",
        "SQL",
        "Scripts",
        "Functional",
        "Specifications",
        "Apache",
        "HUE",
        "interface",
        "HDFS",
        "Hadoop",
        "Components",
        "HDFS",
        "Job",
        "Tracker",
        "Task",
        "Tracker",
        "Name",
        "Node",
        "Data",
        "Node",
        "YARN",
        "Map",
        "Reduce",
        "programming",
        "MapReduce",
        "programs",
        "irregularities",
        "data",
        "Cluster",
        "coordination",
        "services",
        "Zookeeper",
        "Hive",
        "UDFs",
        "performance",
        "results",
        "Developed",
        "Pig",
        "Latin",
        "Scripts",
        "data",
        "log",
        "files",
        "HDFS",
        "Created",
        "User",
        "Defined",
        "Functions",
        "UDFs",
        "data",
        "analysis",
        "Map",
        "Joins",
        "data",
        "sources",
        "operations",
        "algorithms",
        "SQL",
        "queries",
        "MapReduce",
        "jobs",
        "query",
        "Hive",
        "table",
        "configuration",
        "report",
        "packages",
        "Beautifulsoup",
        "data",
        "Python",
        "SQL",
        "HiveQL",
        "Drill",
        "SparkSQL",
        "Experience",
        "Sqoop",
        "data",
        "Oracle",
        "DB",
        "HDFS",
        "HIVE",
        "HBase",
        "CRUD",
        "operations",
        "HBase",
        "data",
        "thrift",
        "API",
        "time",
        "insights",
        "workflow",
        "Oozie",
        "schedule",
        "jobs",
        "Hadoop",
        "cluster",
        "reports",
        "basis",
        "integration",
        "microservices",
        "bidding",
        "scalaakka",
        "firebase",
        "cassandra",
        "Elasticsearch",
        "slick",
        "query",
        "database",
        "Scala",
        "fashion",
        "Scala",
        "collection",
        "framework",
        "HIVE",
        "ETL",
        "loadings",
        "Structured",
        "Data",
        "job",
        "flows",
        "Map",
        "Reduce",
        "jobs",
        "requirement",
        "MapReduce",
        "Jobs",
        "HDFS",
        "compression",
        "mechanisms",
        "PIG",
        "UDFs",
        "data",
        "Business",
        "Requirements",
        "custom",
        "PIG",
        "Loaders",
        "Parser",
        "programs",
        "data",
        "Autosys",
        "Tibco",
        "Business",
        "XML",
        "Informatica",
        "Java",
        "database",
        "views",
        "Scala",
        "PIG",
        "UDF",
        "information",
        "area",
        "data",
        "sensors",
        "Hive",
        "tables",
        "business",
        "requirements",
        "Partitioning",
        "Dynamic",
        "Partitions",
        "Buckets",
        "HIVE",
        "data",
        "access",
        "NoSQL",
        "database",
        "design",
        "integration",
        "implementation",
        "data",
        "NoSQL",
        "database",
        "HBase",
        "performance",
        "PIG",
        "HIVE",
        "scripts",
        "joins",
        "group",
        "aggregation",
        "Flume",
        "aggregate",
        "web",
        "log",
        "data",
        "sources",
        "web",
        "servers",
        "Connected",
        "tables",
        "Data",
        "tools",
        "Tableau",
        "representation",
        "trends",
        "Hadoop",
        "log",
        "files",
        "loading",
        "data",
        "UNIX",
        "file",
        "system",
        "design",
        "development",
        "Spark",
        "SQL",
        "Scripts",
        "Functional",
        "Specifications",
        "Apache",
        "HUE",
        "interface",
        "HDFS",
        "storage",
        "Environment",
        "HDFS",
        "Map",
        "Reduce",
        "Pig",
        "Mesos",
        "AWS",
        "Hive",
        "Sqoop",
        "Scala",
        "Flume",
        "Mahout",
        "HBase",
        "Spark",
        "SPARK",
        "SQL",
        "Yarn",
        "Java",
        "Maven",
        "Git",
        "Cloudera",
        "MongoDB",
        "Eclipse",
        "Shell",
        "Scripting",
        "Hadoop",
        "Developer",
        "DELL",
        "Bengaluru",
        "Karnataka",
        "June",
        "August",
        "Responsibilities",
        "data",
        "movement",
        "framework",
        "sources",
        "SQL",
        "Server",
        "Oracle",
        "MySQL",
        "Created",
        "Sqoop",
        "import",
        "export",
        "jobs",
        "sources",
        "scripts",
        "creation",
        "Sqoop",
        "jobs",
        "workflows",
        "Hive",
        "scripts",
        "tables",
        "transformations",
        "MapReduce",
        "PIG",
        "cleansers",
        "data",
        "cleansing",
        "Hive",
        "UDFS",
        "information",
        "data",
        "MapReduce",
        "programs",
        "data",
        "lineage",
        "framework",
        "information",
        "auditing",
        "failure",
        "recovery",
        "Closed",
        "web",
        "application",
        "development",
        "team",
        "user",
        "interface",
        "data",
        "movement",
        "framework",
        "Oozie",
        "workflows",
        "Job",
        "Automation",
        "Created",
        "Map",
        "programs",
        "data",
        "xml",
        "Json",
        "Avro",
        "data",
        "files",
        "sequence",
        "files",
        "log",
        "files",
        "web",
        "service",
        "python",
        "cherrypy",
        "data",
        "accumulo",
        "data",
        "warehouse",
        "MySQL",
        "server",
        "Authentication",
        "users",
        "Databases",
        "issues",
        "MapReduce",
        "programs",
        "data",
        "sets",
        "results",
        "Sqoop",
        "web",
        "service",
        "documents",
        "apache",
        "accumulo",
        "data",
        "store",
        "amounts",
        "log",
        "data",
        "Apache",
        "Flume",
        "data",
        "HDFS",
        "analysis",
        "Hadoop",
        "infrastructure",
        "Software",
        "Hardware",
        "level",
        "Experience",
        "troubleshooting",
        "MapReduce",
        "jobs",
        "log",
        "files",
        "search",
        "solution",
        "web",
        "crawler",
        "Apache",
        "Nutch",
        "Search",
        "Platform",
        "Apache",
        "SOLR",
        "EnvironmentHadoop",
        "Cloudera",
        "Manager",
        "Linux",
        "RedHat",
        "CentOs",
        "Ubuntu",
        "Operating",
        "System",
        "Scala",
        "Map",
        "Reduce",
        "HBase",
        "SQL",
        "Sqoop",
        "HDFS",
        "Kafka",
        "UML",
        "Apache",
        "SOLR",
        "Hive",
        "Oozie",
        "Cassandra",
        "maven",
        "Pig",
        "UNIX",
        "Python",
        "MR",
        "Unit",
        "Git",
        "Java",
        "Developer",
        "STATE",
        "FARM",
        "Bengaluru",
        "Karnataka",
        "July",
        "June",
        "Responsibilities",
        "J2EE",
        "application",
        "Service",
        "Oriented",
        "Architecture",
        "SOAP",
        "tools",
        "data",
        "exchanges",
        "updates",
        "functionalities",
        "Agile",
        "Methodology",
        "Apache",
        "Maven",
        "project",
        "management",
        "application",
        "modules",
        "application",
        "frontend",
        "presentation",
        "logic",
        "Spring",
        "MVC",
        "JSP",
        "JSTL",
        "JavaScript",
        "Business",
        "POJOs",
        "data",
        "access",
        "layer",
        "Hibernate",
        "framework",
        "JAXRS",
        "REST",
        "web",
        "services",
        "programs",
        "web",
        "services",
        "Apache",
        "CXF",
        "framework",
        "API",
        "SOAP",
        "web",
        "services",
        "consumption",
        "Spring",
        "ORM",
        "module",
        "integration",
        "Hibernate",
        "persistence",
        "layer",
        "Hibernate",
        "Query",
        "Language",
        "HQL",
        "persistence",
        "layer",
        "Spring",
        "MVC",
        "Spring",
        "AOP",
        "Spring",
        "IOC",
        "Spring",
        "Transaction",
        "Oracle",
        "Club",
        "Systems",
        "Component",
        "jobs",
        "Core",
        "Java",
        "Oracle",
        "Data",
        "Base",
        "core",
        "modules",
        "application",
        "compliant",
        "JavaJ2EE",
        "standards",
        "Design",
        "Patterns",
        "Written",
        "Java",
        "Script",
        "HTML",
        "CSS",
        "Servlets",
        "JSP",
        "GUI",
        "application",
        "Serviceside",
        "Middletier",
        "technologies",
        "strategiessolutions",
        "Design",
        "data",
        "access",
        "layer",
        "Data",
        "Access",
        "Layer",
        "J2EE",
        "patterns",
        "MVC",
        "architecture",
        "Struts",
        "Framework",
        "databases",
        "locations",
        "information",
        "presentation",
        "layer",
        "XPath",
        "XML",
        "elements",
        "part",
        "business",
        "logic",
        "Environment",
        "Java",
        "Struts",
        "Hibernate",
        "JSP",
        "JavaScript",
        "HTML",
        "XML",
        "Oracle",
        "Eclipse",
        "JBoss",
        "Application",
        "Server",
        "ANT",
        "CVS",
        "SQL",
        "Education",
        "Masters",
        "Michigan",
        "State",
        "University",
        "Ann",
        "Arbor",
        "MI",
        "Skills",
        "Hdfs",
        "Impala",
        "Oozie",
        "Sqoop",
        "Apache",
        "Kafka",
        "Flume",
        "Jboss",
        "Jms",
        "Map",
        "Mongodb",
        "Ms",
        "studio",
        "Visual",
        "studio",
        "Apache",
        "spark",
        "Api",
        "Hive",
        "Html",
        "Javascript",
        "Nodejs",
        "Additional",
        "Information",
        "TECHNICAL",
        "SKILLS",
        "Big",
        "Data",
        "Ecosystems",
        "HDFS",
        "Map",
        "Reduce",
        "Pig",
        "Hive",
        "Pig",
        "Latin",
        "Impala",
        "YARN",
        "Oozie",
        "Zookeeper",
        "Apache",
        "Spark",
        "Apache",
        "Crunch",
        "Apache",
        "NiFi",
        "Apace",
        "STORM",
        "Apache",
        "Kappa",
        "Apache",
        "Kafka",
        "Sqoop",
        "Flume",
        "Streaming",
        "Technologies",
        "Spark",
        "Streaming",
        "Storm",
        "Scripting",
        "Languages",
        "Python",
        "Perl",
        "Shell",
        "Sheme",
        "Tcl",
        "Unix",
        "Shell",
        "Scripts",
        "Windows",
        "Power",
        "Shell",
        "Programming",
        "Languages",
        "Java",
        "J2EE",
        "JDK",
        "JDBC",
        "Hibernate",
        "XML",
        "Parsers",
        "JSP",
        "Servlets",
        "EJB",
        "JMS",
        "Struts",
        "Spring",
        "Framework",
        "Java",
        "Beans",
        "JNDI",
        "Databases",
        "MongoDB",
        "Netezza",
        "SQL",
        "Server",
        "MySQL",
        "ORACLE",
        "DB2",
        "IDEs",
        "Tools",
        "Eclipse",
        "JUnit",
        "Maven",
        "Ant",
        "MS",
        "Visual",
        "Studio",
        "Net",
        "Beans",
        "Methodologies",
        "Agile",
        "Waterfall",
        "Virtualization",
        "Technologies",
        "VMware",
        "ESXi",
        "Windows",
        "Power",
        "VM",
        "Virtual",
        "box",
        "Citrix",
        "Xen",
        "KVM",
        "Web",
        "Technologies",
        "HTML",
        "JavaScript",
        "JQuery",
        "Ajax",
        "Boot",
        "Strap",
        "Angular",
        "JS",
        "Nodejs",
        "Expressjs",
        "Web",
        "Servers",
        "Web",
        "Logic",
        "Web",
        "Sphere",
        "Apache",
        "Tomcat",
        "JBOSS",
        "Web",
        "Services",
        "SOAP",
        "API",
        "WSDL"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T20:26:15.801985",
    "resume_data": "Sr Spark Developer Sr Spark span lDeveloperspan Sr Spark Developer CenterPoint Energy 8 years of extensive handson experience in IT industry including 5 years experience in deployment of Hadoop Ecosystems like MapReduce Yarn Sqoop Flume Pig Hive HBase Cassandra Zoo Keeper Oozie and Ambari BigQuery Big Table and 5 years experience on Spark Storm Scala Python Experience in OLTP and OLAP design development testing implementation and support of enterprise Data warehouses Strong Knowledge in Hadoop Cluster Capacity Planning Performance Tuning Cluster Monitoring Extensive experience in business data science project life cycle including Data Acquisition Data Cleaning Data Manipulation Data Validation Data Mining Machine Learning Algorithms and Visualization Good Hands on experience in working with Ecosystems like Hive Pig Sqoop Map Reduce Flume Oozie Strong knowledge in HIVE and PIG core functionality by using custom User Defined Functions UDF User Defined TableGenerating Functions UDTF and User Defined Aggregating Functions UDAF for Hive Experience on Productionizing Apache Nifi for dataflows with significant processing requirements and controlling security of data flow Designed and developed RDD Seeds using Scala and Cascading Streaming data to Sparkstreaming using Kafka Exposure to Spark Spark Streaming Spark MLlib Scala and Creating the Data Frames handled in Spark with Scala Good Exposure on Map Reduce programming using Java PIG Latin Scripting and Distributed Application and HDFS Experienced Good understanding of NoSQL databases and hands on work experience in writing applications No SQL Databases HBase Cassandra and MongoDB Very good implementation experience of Object Oriented concepts Multithreading and JavaScala Experienced with the Scala Spark improving the performance and optimization of the existing algorithms in Hadoop using Spark Context Spark SQL Pair RDDs Spark YARN Experienced in installation configuration supporting and managing Hadoop Clusters using Apache Cloudera distributions Horton works Cloud Storage and Amazon web services AWS and related technologies DynamoDB EMR S3 ML Experience in deploying NiFi Data flow in Production team and Integrating data from multiple sources like Cassandra MongoDB Deploying templates to environments can be done via NiFi RestAPI integrated with other automation tools Complete end to end design and development of Apache NiFi flow which acts as the agent between middleware team and EBI team and executes all the actions mentioned above Experienced in Python programming wrote Web Crawlers using Python Experience in bench marking Hadoop cluster for analysis of queue usage Experienced in working with Mahout for applying machine learning techniques in the Hadoop Ecosystem Good Experience on Amazon Web Services like Redshift Data Pipeline ML Good experienced on moving the data in and out of Hadoop RDBMS NoSQL and UNIX from various systems using SQOOP and other traditional data movement technologies Experience on Integration of Quartz scheduler with Oozie work flows to get data from multiple data sources in parallel using fork Experience in installation configuration support and management of a Hadoop Cluster using Cloudera Distributions Experienced Spark scripts by using Scala shell as per requirements Good knowledge on tuning the Spark jobs by changing the configuration properties and using broadcast variables Developed REST APIs using Java Play framework and Akka Expertise in search technologys like SOLR Informatica Lucene Experience in converting SQL queries into Spark Transformations using Spark RDDs and Scala and Performed mapside joins on RDDs Experienced in writing Hadoop Jobs for analyzing data using Hive Query Language HQL Pig Latin Data flow language and custom MapReduce programs in Java Good understanding of NoSQL databases like MongoDB Cassandra and HBase Strong analytical skills with ability to quickly understand clients business needs Involved in business meetings for requirements gathering form business clients Experienced in Storm builder topologies to perform cleansing operations before moving data into HBase Hands on experience in configuring and working with Flume to load the data from multiple sources directly into Hdfs Experience on configuring fully the Flume agent suitable for all type of logger data and store them in Avro Sink in Parquet file format and developing 2tier architecture connecting channels between Avro sinks and Source Experience creating Visual report Graphical analysis and Dashboard reports using Tableau Informatica of historical data saved in Hdfs and data analysis using Splunk enterprise edition Good experience in utilizing Cloud Storage Services like Git Extensive knowledge in using GitHub and Bit Bucket Experienced in job scheduling and monitoring using Oozie Zookeeper Work Experience Sr Spark Developer CenterPoint Energy Houston TX June 2018 to Present Responsibilities Hands on experience in installation configuration supporting and managing Hadoop Clusters Knowledge of Cassandra security maintenance and tuning both database and server Chipped away at outlining and building up the Real Time Analysis module for Analytic Dashboard utilizing Cassandra Kafka Spark Streaming Installed and configured Confluent Kafka in RD line Validated the installation with HDFS connector and Hive connectors Deployed high availability on the Hadoop cluster quorum journal nodes Experience on implementing SAX Symbolic Aggregate approXimation in Java to use with Apache Spark for normalizing time series data Involved in defining job flows managing and reviewing log file Setup configured and optimized the Cassandra cluster Developed realtime Spark based application to work along with the Cassandra database Responsible to manage data coming from different sources through Kafka Installed Kafka Producer on different severs and Scheduled to produce data for every 10 seconds Integrated Kafka with Spark Streaming to listen onto multiple Kafka Brokers with different Kafka topics for every 5 Seconds Enhanced and optimized product Spark code to aggregate group and run data mining tasks using the Spark framework and handled Json Data Handled Json Data comes from Kafka Direct Stream on each partitions and transformed them into required Data Frame Formats Upgraded Spark 16 to latest Version Spark 22 and configure Kafka Version 010 Managing Kafka Offsets Saving Offsets in external data base like HBase and to its own Kafka Worked on Import Export of data using ETL tool Sqoop from MySQL to HDFS Worked on Lambda Architecture for both Batch processing and Real Streaming purposes Used Oozie to Schedule Spark and Kafka Producer Jobs to run in parallel Appended the Data Frames into Cassandra Key Space Tables using DataStax SparkCassandra Connector Experience with Cassandra YAML Configuration files RACK DC properties file Cassandraenv file for JMX configurations etc Installed and configured Datastax OpsCenter and Nagios for Cassandra cluster maintenance and alert Configured Authentication and security in Apache kafka pubsub system Good experience with Century Link Cloud for provisioning virtual machines creating resource groups configuring key vaults for storing encryption keys Monitoring etc Great Hands on Experience in seat stamping Hadoop bunch for investigation of line utilization Performing OS level setups and Kernel level tuning Implement and test integration of BI Business Intelligence tools with Hadoop stack InstalledConfiguredMaintained Apache Hadoop clusters for application development and Hadoop tools like Hive Pig HBase Zookeeper Sqoop Yarn Spark2 Kafka and Oozie Formulated procedures for installation of Hadoop Spark2 patches updates and version upgrades Environment Cloudera HDFS Spark Hive Pig Map Reduce Hue Sqoop Putt Apache Kafka Apache Drill Century Link Cloud AWS Java Netezza Cassandra Oozie Spark SPARK SQL Maven SBT Java Scala SQL and Linux YARN Agile Methodology Solr PHP Admin XAMPP DataStax Cassandra Sr HadoopSpark Developer CPS Energy San Antonio TX January 2017 to June 2018 Responsibilities Involved in deploying systems on Amazon Web Services AWS Infrastructure services EC2 Experience in configuring deploying the web applications on AWS servers using SBT and Play Migrated Map Reduce jobs into Spark RDD transformations using Scala Used Spark API over Cloudera Hadoop YARN to perform analytics on data in Hive Developed Spark code using Spark RDD and SparkSQLStreaming for faster processing of data Performed configuration deployment and support of cloud services including Amazon Web Services AWS Working knowledge of various AWS technologies like SQS Queuing SNS Notification S3 storage Redshift Data Pipeline EMR Responsible for all Public AWS and Private OpenstackVMWareDCOSMesosMarathon cloud infrastructure Developed Flume ETL job for handling data from HTTP Source and Sink as HDFS and configuring Data Pipelining Used Hive data warehouse tool to analyze the unified historic data in HDFS to identify issues and behavioral patterns Involved in Developing a Restful service using Python Flask framework Expertise in working with Python GUI frameworks PyJamas Jython Experienced in using Apache Drill dataintensive distributed applications for interactive analysis of largescale datasets Developed end to end ETL batch and streaming data integration into HadoopMapR transforming data Used Python modules such as requests urllib urllib2 for web crawling Tools developed extensively include Spark Drill Hive HBase Kafka MapR Streams PostgreSQL Stream Sets Used Hive Queries in SparkSQL for analysis and processing the data Worked as a key role in a team of developing an initial prototype of a NiFi big data pipeline This pipeline demonstrated an end to end scenario of data ingestion processing Used HUE for running Hive queries Created Partitions according to day using Hive to improve performance Wrote Python routines to log into the websites and fetch data for selected options Worked on custom Pig Loaders and storage classes to work with variety of data formats such as JSON and XML file formats Loaded some of the data into Cassandra for fast retrieval of data Worked in provisioning and managing multitenant Hadoop clusters on public cloud environment Amazon Web Services AWS and on private cloud infrastructure Open stack cloud platform and worked on DynamoDB Ml Worked on largescale Hadoop YARN cluster for distributed data processing and analysis using Data Bricks Connectors Spark core Spark SQL Sqoop Pig Hive Impala and NoSQL databases Created HBase tables to load large sets of structured semistructured and unstructured data coming from UNIX NoSQL and a variety of portfolios Worked on a POC to compare processing time of Impala with Apache Hive for batch applications to implement the former in project Worked with various HDFS file formats like Avro Sequence File and various compression formats like Snappy bzip2 Used the RegEx JSON and Avro for serialization and deserialization packaged with Hive to parse the contents of streamed log data Converted all the vap processing from Netezza and implemented by using Spark data frames and RDDs Worked in writing Spark Sql scripts for optimizing the query performance Responsible for handling different data formats like Avro Parquet and ORC formats Implemented Spark Scripts using Scala Spark SQL to access hive tables into Spark for faster processing of data Environment Cloudera Horton Works distribution HDFS Spark Hive Pig Map Reduce Hue Sqoop Putty HaaS Hadoop as a Service Apache Kafka Apache Mesos and the AWS Java Netezza Cassandra Oozie Spark SPARK SQL Maven Java Scala SQL and Linux Toad YARN Agile Methodology Hadoop Developer BANK of America Dallas TX May 2016 to December 2016 Responsibilities Concerned and wellinformed on Hadoop Components such as HDFS Job Tracker Task Tracker Name Node Data Node YARN and Map Reduce programming Developed MapReduce programs to get rid of irregularities and aggregate the data Developed Cluster coordination services through Zookeeper Implemented Hive UDFs and did performance tuning for better results Developed Pig Latin Scripts to extract data from log files and store them to HDFS Created User Defined Functions UDFs to preprocess data for analysis Implemented Optimized Map Joins to get data from different sources to perform cleaning operations before applying the algorithms Created highly optimized SQL queries for MapReduce jobs seamlessly matching the query to the appropriate Hive table configuration to generate efficient report Used other packages such as Beautifulsoup for data parsing in Python Tuned and developed SQL on HiveQL Drill and SparkSQL Experience in using Sqoop to import and export the data from Oracle DB into HDFS and HIVE HBase Implemented CRUD operations on HBase data using thrift API to get real time insights Developed workflow in Oozie to manage and schedule jobs on Hadoop cluster for generating reports on nightly weekly and monthly basis Worked on integration independent microservices for realtime bidding scalaakka firebase cassandra Elasticsearch Used slick to query and storing in database in a Scala fashion using the powerful Scala collection framework Using HIVE processed extensively ETL loadings on a Structured Data Defined job flows and developed simple to complex Map Reduce jobs as per the requirement Optimized MapReduce Jobs to use HDFS efficiently by using various compression mechanisms Developed PIG UDFs for manipulating the data according to Business Requirements and also worked on developing custom PIG Loaders Created various Parser programs to extract data from Autosys Tibco Business Objects XML Informatica Java and database views using Scala PIG UDF was required to extract the information of the area from the huge data which we get from the sensors Responsible for creating Hive tables based on business requirements Implemented Partitioning Dynamic Partitions and Buckets in HIVE for efficient data access Involved in NoSQL database design integration and implementation Loaded data into NoSQL database HBase Worked on debugging performance tuning PIG and HIVE scripts by understanding the joins group and aggregation between them Used Flume to collect aggregate and store the web log data from different sources like web servers and pushed to HDFS Connected the hive tables to Data analyzing tools like Tableau for Graphical representation of the trends Experienced in managing and reviewing Hadoop log files Involved in loading data from UNIX file system to HDFS Responsible for design development of Spark SQL Scripts based on Functional Specifications Used Apache HUE interface to monitor and manage the HDFS storageConcerned and wellinformed on Hadoop Components such as HDFS Job Tracker Task Tracker Name Node Data Node YARN and Map Reduce programming Developed MapReduce programs to get rid of irregularities and aggregate the data Developed Cluster coordination services through Zookeeper Implemented Hive UDFs and did performance tuning for better results Developed Pig Latin Scripts to extract data from log files and store them to HDFS Created User Defined Functions UDFs to preprocess data for analysis Implemented Optimized Map Joins to get data from different sources to perform cleaning operations before applying the algorithms Created highly optimized SQL queries for MapReduce jobs seamlessly matching the query to the appropriate Hive table configuration to generate efficient report Used other packages such as Beautifulsoup for data parsing in Python Tuned and developed SQL on HiveQL Drill and SparkSQL Experience in using Sqoop to import and export the data from Oracle DB into HDFS and HIVE HBase Implemented CRUD operations on HBase data using thrift API to get real time insights Developed workflow in Oozie to manage and schedule jobs on Hadoop cluster for generating reports on nightly weekly and monthly basis Worked on integration independent microservices for realtime bidding scalaakka firebase cassandra Elasticsearch Used slick to query and storing in database in a Scala fashion using the powerful Scala collection framework Using HIVE processed extensively ETL loadings on a Structured Data Defined job flows and developed simple to complex Map Reduce jobs as per the requirement Optimized MapReduce Jobs to use HDFS efficiently by using various compression mechanisms Developed PIG UDFs for manipulating the data according to Business Requirements and also worked on developing custom PIG Loaders Created various Parser programs to extract data from Autosys Tibco Business Objects XML Informatica Java and database views using Scala PIG UDF was required to extract the information of the area from the huge data which we get from the sensors Responsible for creating Hive tables based on business requirements Implemented Partitioning Dynamic Partitions and Buckets in HIVE for efficient data access Involved in NoSQL database design integration and implementation Loaded data into NoSQL database HBase Worked on debugging performance tuning PIG and HIVE scripts by understanding the joins group and aggregation between them Used Flume to collect aggregate and store the web log data from different sources like web servers and pushed to HDFS Connected the hive tables to Data analyzing tools like Tableau for Graphical representation of the trends Experienced in managing and reviewing Hadoop log files Involved in loading data from UNIX file system to HDFS Responsible for design development of Spark SQL Scripts based on Functional Specifications Used Apache HUE interface to monitor and manage the HDFS storage Environment HDFS Map Reduce Pig Mesos AWS Hive Sqoop Scala Flume Mahout HBase Spark SPARK SQL Yarn Java Maven Git Cloudera MongoDB Eclipse and Shell Scripting Hadoop Developer DELL Bengaluru Karnataka June 2013 to August 2015 Responsibilities Designed and developed data movement framework for multiple sources like SQL Server Oracle and MySQL Created Sqoop import and export jobs for multiple sources Developed scripts to automate the creation Sqoop jobs for various workflows Developed Hive scripts to alter the tables and perform required transformations Developed a java MapReduce and PIG cleansers for data cleansing Worked on Hive UDFS to mask confidential information in the data Designed and developed MapReduce programs for data lineage Designed and developed the framework to log information for auditing and failure recovery Closed worked with the web application development team to develop the user interface for data movement framework Designed Oozie workflows for Job Automation Created Map Reduce programs to handle semiunstructured data like xml Json Avro data files and sequence files for log files A RESTful web service built with python and cherrypy retrieves data from an accumulo data warehouse Maintaining the MySQL server and Authentication to required users for Databases Appropriately documented various Administrative technical issues Developed MapReduce programs to extract and transform the data sets and results were exported back to RDBMS using Sqoop Built a RESTful web service for storing and retrieving documents in an apache accumulo data store Involved in collecting and aggregating large amounts of log data using Apache Flume and staging data in HDFS for further analysis Optimized our Hadoop infrastructure at both the Software and Hardware level Experience in troubleshooting in MapReduce jobs by reviewing log files Developed endtoend search solution using web crawler Apache Nutch Search Platform Apache SOLR EnvironmentHadoop Cloudera Manager Linux RedHat CentOs Ubuntu Operating System Scala Map Reduce HBase SQL Sqoop HDFS Kafka UML Apache SOLR Hive Oozie Cassandra maven Pig UNIX Python MR Unit Git Java Developer STATE FARM Bengaluru Karnataka July 2011 to June 2013 Responsibilities Developed the J2EE application based on the Service Oriented Architecture by employing SOAP and other tools for data exchanges and updates Developed the functionalities using Agile Methodology Used Apache Maven for project management and building the application Worked in all the modules of the application which involved frontend presentation logic developed using Spring MVC JSP JSTL and JavaScript Business objects developed using POJOs and data access layer using Hibernate framework Used JAXRS REST for producing web services and involved in writing programs to consume the web services with Apache CXF framework Used Restful API and SOAP web services for internal and external consumption Used Spring ORM module for integration with Hibernate for persistence layer Involved in writing Hibernate Query Language HQL for persistence layer Used Spring MVC Spring AOP Spring IOC Spring Transaction and Oracle to create Club Systems Component Wrote backend jobs based on Core Java Oracle Data Base to be run dailyweekly Coding the core modules of the application compliant with the JavaJ2EE coding standards and Design Patterns Written Java Script HTML CSS Servlets and JSP for designing GUI of the application Worked on Serviceside and Middletier technologies extracting catching strategiessolutions Design data access layer using Data Access Layer J2EE patterns Implementing the MVC architecture Struts Framework for handling databases across multiple locations and display information in presentation layer Used XPath for parsing the XML elements as part of business logic processing Environment Java Struts 12 Hibernate 30 JSP JavaScript HTML XML Oracle Eclipse JBoss Application Server ANT CVS and SQL Education Masters Michigan State University Ann Arbor MI Skills Hdfs Impala Oozie Sqoop Apache kafka Kafka Db2 Flume Jboss Jms Map reduce Mongodb Ms visual studio Visual studio Apache spark Api Hive Html Javascript Nodejs Additional Information TECHNICAL SKILLS Big Data Ecosystems HDFS and Map Reduce Pig Hive Pig Latin Impala YARN Oozie Zookeeper Apache Spark Apache Crunch Apache NiFi Apace STORM Apache Kappa Apache Kafka Sqoop Flume Streaming Technologies Spark Streaming Storm Scripting Languages Python Perl Shell Sheme Tcl Unix Shell Scripts Windows Power Shell Programming Languages Java J2EE JDK1415161718 JDBC Hibernate XML Parsers JSP 122 Servlets EJB JMS Struts Spring Framework Java Beans AJAX JNDI Databases MongoDB Netezza SQL Server MySQL ORACLE DB2 IDEs Tools Eclipse JUnit Maven Ant MS Visual Studio Net Beans Methodologies Agile Waterfall Virtualization Technologies VMware ESXi Windows HyperV Power VM Virtual box Citrix Xen KVM Web Technologies HTML JavaScript JQuery Ajax Boot Strap Angular JS Nodejs Expressjs Web Servers Web Logic Web Sphere Apache Tomcat JBOSS Web Services SOAP RESTful API WSDL",
    "unique_id": "90cfa068-0e9b-4859-bc80-40e708d47181"
}