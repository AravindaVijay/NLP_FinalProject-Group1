{
    "clean_data": "Big Data Engineer Big Data Engineer Big Data Engineer Canton OH Around 8 years of professional IT experience in the fields of Big Data Business Intelligenceand Java in Financial Insurance and Digital Services Industries Handson experience with major components in Hadoop Ecosystem like Map Reduce HDFS YARN Hive Pig HBase Sqoop Oozie Cassandra Impala and Flume Experience with new Hadoop 20 architecture YARN and developing YARN Applications on it Experience with Apache Sparks Core Spark SQL Streaming and MlLib components Experience with distributed systems largescale nonrelational data stores and multiterabyte data warehouses Firm grip on data modeling database performance tuning and NoSQL mapreduce systems Experience in processing semistructured and unstructured datasets Responsible for setting up processes for Hadoop based application design and implementation Experience in managing HBase database and using it to updatemodify the data Experience in running MapReduce and Spark jobs over YARN Experience with Cloudera Hortonworks and MapR distributions Handling data in various file formats such as Sequential AVRO RC Parquet and ORC Strong knowledge on the scalability and applications of Spark and its components Core SQL and Dataframes Handson experience in complete project life cycle design development testing and implementation of Client Server and Web applications Involved in developing complex ETL transformation performance tuning Extensively worked with Teradata utilities like BTEQ Fast Export Fast Load Multi Load to export and load data tofrom different source systems including flat files Hands on experience using query tools like Teradata SQL Assistant TOAD PLSQL developer and Query man Experience in Object Oriented Analysis and Design OOAD and development of software using UML Methodology good knowledge of J2EE design patterns and Core Java design patterns Experience using middleware architecture using Sun Java technologies like J2EE JSP Servlets and application servers like Web Sphere and Web logic Authorized to work in the US for any employer Work Experience Big Data Engineer SunGard Financial Systems New York NY August 2014 to December 2015 Description Kiodex Risk work bench is a SaaS solution that helps trading firms  and corporations manage commodities exposures through front middle and back office functions including quantifying portfolio risks via different exposure reports Responsibilities Extracted data from relational databases such as SQL Server and MySql by developing Scala and SQL code Uploaded it to Hive and combined new tables with existing databases Developed code to preprocess large sets of various types of file formats such as Text Avro Sequence files XML JSON and Parquet Configured big data workflows to run on the top of Hadoop which comprises of heterogeneous jobs like Pig Hive Sqoop and MapReduce Loaded various formats of structured and unstructured data from Linux file system to HDFS Used Combiners and Partitioners in MapReduce programming Written Pig Scripts to ETL the data into NOSQL database for faster analysis Read from Flume and involved in pushing batches of data to HDFS and HBase for real time processing of the files Parsing XML data into structured format and loading into HDFS Scheduled various ETL process and Hive scripts by developing Oozie workflow Utilized Tableau to visualize the analyzed data and performed report design and delivery Created POC for Flume implementation Involved in reviewing both functional and nonfunctional aspects of the business model Championed to communicate and present the models to business customers and executives using the same Environment Hadoop HDFS Map Reduce Sqoop HBase Shell Scripting PIG HIVE Oozie Core Java Hortonworks Distribution LINUX Business IntelligenceETL Developer Synacor Buffalo NY January 2013 to July 2014 Description Synacor is a provider of private label digital Internet services to North Americas cable and telecom companies Its a global technology company providing Internet solutions to enable ISPs media companies and advertisers to build close relationships with consumers on the Internet Theintranet application represents a near term opportunity in which thecompound would move quickly into Development The purpose of this application is to create a simplified process where ideas are generated tracked and reviewed using existing governance Responsibilities Involved in design development of operational data source and data marts in Oracle Reviewed source data and recommend data acquisition and transformation strategy Involved in conceptual logical and physical data modeling and used star schema in designing the data warehouse Designed ETL process using Teradata to load the data from various source databases and flat files to target data warehouse in Oracle Used Power mart Workflow Manager to design sessions event waitraise and assignment email and command to execute mappings Created parameter based mappings Router and lookup transformations Involved in migration projects to migrate data from data warehouses on OracleDB2 and migrated those to Teradata Optimized mappings using transformation features like Aggregator Filter Joiner Expression and Lookups Created daily and weekly workflows and scheduled to run based on business needs Environment Data modeling SQL Server SSIS SSRS Oracle 10g Teradata 6 XML TOAD SQL PLSQL IBM AIX UNIX Shell Scripts Web Intelligence DSBASIC Cognos Erwin STAR team Remedy Maestro job scheduler Mercury Quality Center ControlM Java Developer Emorphosys August 2011 to December 2012 Description Emorphosys Software Solutions is a global technology based product development company which is a pioneer in creating simple scalable products that focus on business transformation through innovation It provides technology based solutions in the ELearning Digital Media and Enterprise Content Management domains Responsibilities Involved in the core product development using J2EE JSF and Hibernate Actively involved in the full life cycle Object Oriented application development ObjectModeling Database Mapping GUI Design Used JavaScript to perform client side validations and StrutsValidator framework for serverside validation Worked on requirement gathering high level design and Waterfall model to get best result Created data access using SQL and PLSQL stored procedures Used Hibernate annotations with Java for various stages in the application Built web services upon SOAP to export and import attachments from file to associated applications Developed DAO data access objects using Spring Framework Deployed the components in to WebSphere Application server Used HTMLCSS and JavaScript for UI development Written sql queries including Joins Triggers Stored procedures Views using MySql Implemented the JSPs and EJBs in the JSF Framework to handle the workflow of the application Developed Unit Test Cases used JUnit for unit testing of the application Environment Java J2EE Struts SQL JAX RPC XML RAD Websphere MQ Agile JSPS SOAP Education Bachelors in Computer Science VIGNAN University El Segundo CA September 2017 to Present Skills DATABASES 4 years JAVA 4 years PLSQL 4 years SQL 4 years XML 4 years Additional Information TECHNICAL SKILLSET Big Data HDFS MapReduce Hive Pig ZooKeeper Apache Spark Core MlLib Spark SQL and Dataframes Utilities Sqoop Flume Kafka Oozie and AutoSys No SQL Databases Hbase Cassandra Languages C C Java J2EE PLSQL MR Pig Latin HiveQL Unix shell scripting and Scala Operating Systems Sun Solaris RedHat Linux Ubuntu Linux and Windows XPVista78 Web Technologies HTML DHTML XML AJAX WSDL SOAP Databases and Datawarehousing Teradata DB2 Oracle 9i10g11g SQL Server MySQL Tools and IDE Maven Toad Eclipse NetBeans ANT Hudson Sonar JDeveloper Assent PMD DB Visualizer",
    "entities": [
        "Digital Services Industries Handson",
        "ORC Strong",
        "SQL Server",
        "ObjectModeling Database Mapping GUI Design",
        "Used Hibernate",
        "MapR",
        "ETL",
        "US",
        "New York",
        "Utilized Tableau",
        "J2EE JSP Servlets",
        "multiterabyte",
        "WebSphere Application",
        "Oracle Used Power mart",
        "HDFS",
        "Created",
        "MySql Implemented",
        "Additional Information TECHNICAL SKILLSET Big Data",
        "MlLib",
        "Responsibilities Involved",
        "Aggregator Filter Joiner Expression",
        "JavaScript",
        "Scala Operating Systems",
        "Waterfall",
        "UI",
        "Oracle",
        "Oracle Reviewed",
        "IDE Maven Toad",
        "Big Data Engineer Big Data Engineer Big Data",
        "Designed ETL",
        "Router",
        "StrutsValidator",
        "Sequential",
        "MapReduce Loaded",
        "Big Data Business Intelligenceand Java",
        "Development The",
        "Hibernate Actively",
        "Query",
        "Computer Science VIGNAN University",
        "IBM",
        "Hadoop Ecosystem",
        "Dataframes Handson",
        "AutoSys No SQL Databases",
        "Linux",
        "Mercury Quality Center ControlM",
        "BTEQ Fast Export Fast Load Multi Load",
        "SQL",
        "Dataframes Utilities",
        "Lookups Created",
        "Hadoop",
        "Created POC",
        "Joins",
        "Work Experience Big Data",
        "Pig Hive Sqoop",
        "MapReduce",
        "Text Avro Sequence",
        "NOSQL",
        "Developed Unit Test Cases",
        "Present Skills",
        "UML Methodology",
        "Financial Insurance",
        "North Americas",
        "JUnit",
        "Parquet Configured",
        "Core SQL",
        "Object Oriented Analysis and Design OOAD",
        "Views",
        "HBase",
        "Responsibilities Extracted",
        "Hive",
        "Sun Java",
        "NetBeans ANT Hudson Sonar JDeveloper Assent",
        "Spark",
        "RPC",
        "MySql"
    ],
    "experience": "Experience with new Hadoop 20 architecture YARN and developing YARN Applications on it Experience with Apache Sparks Core Spark SQL Streaming and MlLib components Experience with distributed systems largescale nonrelational data stores and multiterabyte data warehouses Firm grip on data modeling database performance tuning and NoSQL mapreduce systems Experience in processing semistructured and unstructured datasets Responsible for setting up processes for Hadoop based application design and implementation Experience in managing HBase database and using it to updatemodify the data Experience in running MapReduce and Spark jobs over YARN Experience with Cloudera Hortonworks and MapR distributions Handling data in various file formats such as Sequential AVRO RC Parquet and ORC Strong knowledge on the scalability and applications of Spark and its components Core SQL and Dataframes Handson experience in complete project life cycle design development testing and implementation of Client Server and Web applications Involved in developing complex ETL transformation performance tuning Extensively worked with Teradata utilities like BTEQ Fast Export Fast Load Multi Load to export and load data tofrom different source systems including flat files Hands on experience using query tools like Teradata SQL Assistant TOAD PLSQL developer and Query man Experience in Object Oriented Analysis and Design OOAD and development of software using UML Methodology good knowledge of J2EE design patterns and Core Java design patterns Experience using middleware architecture using Sun Java technologies like J2EE JSP Servlets and application servers like Web Sphere and Web logic Authorized to work in the US for any employer Work Experience Big Data Engineer SunGard Financial Systems New York NY August 2014 to December 2015 Description Kiodex Risk work bench is a SaaS solution that helps trading firms   and corporations manage commodities exposures through front middle and back office functions including quantifying portfolio risks via different exposure reports Responsibilities Extracted data from relational databases such as SQL Server and MySql by developing Scala and SQL code Uploaded it to Hive and combined new tables with existing databases Developed code to preprocess large sets of various types of file formats such as Text Avro Sequence files XML JSON and Parquet Configured big data workflows to run on the top of Hadoop which comprises of heterogeneous jobs like Pig Hive Sqoop and MapReduce Loaded various formats of structured and unstructured data from Linux file system to HDFS Used Combiners and Partitioners in MapReduce programming Written Pig Scripts to ETL the data into NOSQL database for faster analysis Read from Flume and involved in pushing batches of data to HDFS and HBase for real time processing of the files Parsing XML data into structured format and loading into HDFS Scheduled various ETL process and Hive scripts by developing Oozie workflow Utilized Tableau to visualize the analyzed data and performed report design and delivery Created POC for Flume implementation Involved in reviewing both functional and nonfunctional aspects of the business model Championed to communicate and present the models to business customers and executives using the same Environment Hadoop HDFS Map Reduce Sqoop HBase Shell Scripting PIG HIVE Oozie Core Java Hortonworks Distribution LINUX Business IntelligenceETL Developer Synacor Buffalo NY January 2013 to July 2014 Description Synacor is a provider of private label digital Internet services to North Americas cable and telecom companies Its a global technology company providing Internet solutions to enable ISPs media companies and advertisers to build close relationships with consumers on the Internet Theintranet application represents a near term opportunity in which thecompound would move quickly into Development The purpose of this application is to create a simplified process where ideas are generated tracked and reviewed using existing governance Responsibilities Involved in design development of operational data source and data marts in Oracle Reviewed source data and recommend data acquisition and transformation strategy Involved in conceptual logical and physical data modeling and used star schema in designing the data warehouse Designed ETL process using Teradata to load the data from various source databases and flat files to target data warehouse in Oracle Used Power mart Workflow Manager to design sessions event waitraise and assignment email and command to execute mappings Created parameter based mappings Router and lookup transformations Involved in migration projects to migrate data from data warehouses on OracleDB2 and migrated those to Teradata Optimized mappings using transformation features like Aggregator Filter Joiner Expression and Lookups Created daily and weekly workflows and scheduled to run based on business needs Environment Data modeling SQL Server SSIS SSRS Oracle 10 g Teradata 6 XML TOAD SQL PLSQL IBM AIX UNIX Shell Scripts Web Intelligence DSBASIC Cognos Erwin STAR team Remedy Maestro job scheduler Mercury Quality Center ControlM Java Developer Emorphosys August 2011 to December 2012 Description Emorphosys Software Solutions is a global technology based product development company which is a pioneer in creating simple scalable products that focus on business transformation through innovation It provides technology based solutions in the ELearning Digital Media and Enterprise Content Management domains Responsibilities Involved in the core product development using J2EE JSF and Hibernate Actively involved in the full life cycle Object Oriented application development ObjectModeling Database Mapping GUI Design Used JavaScript to perform client side validations and StrutsValidator framework for serverside validation Worked on requirement gathering high level design and Waterfall model to get best result Created data access using SQL and PLSQL stored procedures Used Hibernate annotations with Java for various stages in the application Built web services upon SOAP to export and import attachments from file to associated applications Developed DAO data access objects using Spring Framework Deployed the components in to WebSphere Application server Used HTMLCSS and JavaScript for UI development Written sql queries including Joins Triggers Stored procedures Views using MySql Implemented the JSPs and EJBs in the JSF Framework to handle the workflow of the application Developed Unit Test Cases used JUnit for unit testing of the application Environment Java J2EE Struts SQL JAX RPC XML RAD Websphere MQ Agile JSPS SOAP Education Bachelors in Computer Science VIGNAN University El Segundo CA September 2017 to Present Skills DATABASES 4 years JAVA 4 years PLSQL 4 years SQL 4 years XML 4 years Additional Information TECHNICAL SKILLSET Big Data HDFS MapReduce Hive Pig ZooKeeper Apache Spark Core MlLib Spark SQL and Dataframes Utilities Sqoop Flume Kafka Oozie and AutoSys No SQL Databases Hbase Cassandra Languages C C Java J2EE PLSQL MR Pig Latin HiveQL Unix shell scripting and Scala Operating Systems Sun Solaris RedHat Linux Ubuntu Linux and Windows XPVista78 Web Technologies HTML DHTML XML AJAX WSDL SOAP Databases and Datawarehousing Teradata DB2 Oracle 9i10g11 g SQL Server MySQL Tools and IDE Maven Toad Eclipse NetBeans ANT Hudson Sonar JDeveloper Assent PMD DB Visualizer",
    "extracted_keywords": [
        "Data",
        "Engineer",
        "Big",
        "Data",
        "Engineer",
        "Big",
        "Data",
        "Engineer",
        "Canton",
        "OH",
        "years",
        "IT",
        "experience",
        "fields",
        "Big",
        "Data",
        "Business",
        "Intelligenceand",
        "Java",
        "Financial",
        "Insurance",
        "Digital",
        "Services",
        "Industries",
        "Handson",
        "components",
        "Hadoop",
        "Ecosystem",
        "Map",
        "Reduce",
        "HDFS",
        "YARN",
        "Hive",
        "Pig",
        "HBase",
        "Sqoop",
        "Oozie",
        "Cassandra",
        "Impala",
        "Flume",
        "Experience",
        "Hadoop",
        "architecture",
        "YARN",
        "YARN",
        "Applications",
        "Experience",
        "Apache",
        "Sparks",
        "Core",
        "Spark",
        "SQL",
        "Streaming",
        "MlLib",
        "components",
        "Experience",
        "systems",
        "largescale",
        "data",
        "stores",
        "multiterabyte",
        "data",
        "grip",
        "data",
        "modeling",
        "database",
        "performance",
        "tuning",
        "NoSQL",
        "systems",
        "Experience",
        "processing",
        "datasets",
        "processes",
        "Hadoop",
        "application",
        "design",
        "implementation",
        "Experience",
        "HBase",
        "database",
        "data",
        "Experience",
        "MapReduce",
        "Spark",
        "jobs",
        "YARN",
        "Experience",
        "Cloudera",
        "Hortonworks",
        "MapR",
        "distributions",
        "data",
        "file",
        "formats",
        "Sequential",
        "AVRO",
        "RC",
        "Parquet",
        "ORC",
        "knowledge",
        "scalability",
        "applications",
        "Spark",
        "components",
        "Core",
        "SQL",
        "Dataframes",
        "Handson",
        "experience",
        "project",
        "life",
        "cycle",
        "design",
        "development",
        "testing",
        "implementation",
        "Client",
        "Server",
        "Web",
        "applications",
        "ETL",
        "transformation",
        "performance",
        "Teradata",
        "utilities",
        "BTEQ",
        "Fast",
        "Export",
        "Fast",
        "Load",
        "Multi",
        "Load",
        "export",
        "data",
        "source",
        "systems",
        "files",
        "Hands",
        "experience",
        "query",
        "tools",
        "Teradata",
        "SQL",
        "Assistant",
        "TOAD",
        "PLSQL",
        "developer",
        "Query",
        "man",
        "Experience",
        "Object",
        "Oriented",
        "Analysis",
        "Design",
        "OOAD",
        "development",
        "software",
        "UML",
        "Methodology",
        "knowledge",
        "J2EE",
        "design",
        "patterns",
        "Core",
        "Java",
        "design",
        "patterns",
        "Experience",
        "middleware",
        "architecture",
        "Sun",
        "Java",
        "technologies",
        "J2EE",
        "JSP",
        "Servlets",
        "application",
        "servers",
        "Web",
        "Sphere",
        "Web",
        "logic",
        "US",
        "employer",
        "Work",
        "Experience",
        "Big",
        "Data",
        "Engineer",
        "SunGard",
        "Financial",
        "Systems",
        "New",
        "York",
        "NY",
        "August",
        "December",
        "Description",
        "Kiodex",
        "Risk",
        "work",
        "bench",
        "SaaS",
        "solution",
        "trading",
        "firms",
        "corporations",
        "commodities",
        "exposures",
        "office",
        "functions",
        "portfolio",
        "risks",
        "exposure",
        "reports",
        "Responsibilities",
        "data",
        "databases",
        "SQL",
        "Server",
        "MySql",
        "Scala",
        "SQL",
        "code",
        "Hive",
        "tables",
        "databases",
        "code",
        "sets",
        "types",
        "file",
        "formats",
        "Text",
        "Avro",
        "Sequence",
        "JSON",
        "Parquet",
        "data",
        "workflows",
        "top",
        "Hadoop",
        "jobs",
        "Pig",
        "Hive",
        "Sqoop",
        "MapReduce",
        "formats",
        "data",
        "Linux",
        "file",
        "system",
        "HDFS",
        "Combiners",
        "Partitioners",
        "MapReduce",
        "programming",
        "Written",
        "Pig",
        "Scripts",
        "data",
        "NOSQL",
        "database",
        "analysis",
        "Flume",
        "batches",
        "data",
        "HDFS",
        "HBase",
        "time",
        "processing",
        "files",
        "XML",
        "data",
        "format",
        "loading",
        "HDFS",
        "ETL",
        "process",
        "Hive",
        "scripts",
        "Oozie",
        "workflow",
        "Tableau",
        "data",
        "report",
        "design",
        "delivery",
        "Created",
        "POC",
        "Flume",
        "implementation",
        "aspects",
        "business",
        "model",
        "models",
        "business",
        "customers",
        "executives",
        "Environment",
        "Hadoop",
        "HDFS",
        "Map",
        "Reduce",
        "Sqoop",
        "HBase",
        "Shell",
        "Scripting",
        "PIG",
        "HIVE",
        "Oozie",
        "Core",
        "Java",
        "Hortonworks",
        "Distribution",
        "LINUX",
        "Business",
        "IntelligenceETL",
        "Developer",
        "Synacor",
        "Buffalo",
        "NY",
        "January",
        "July",
        "Description",
        "Synacor",
        "provider",
        "label",
        "Internet",
        "services",
        "North",
        "Americas",
        "cable",
        "telecom",
        "companies",
        "technology",
        "company",
        "Internet",
        "solutions",
        "ISPs",
        "media",
        "companies",
        "advertisers",
        "relationships",
        "consumers",
        "Internet",
        "Theintranet",
        "application",
        "term",
        "opportunity",
        "thecompound",
        "Development",
        "purpose",
        "application",
        "process",
        "ideas",
        "governance",
        "Responsibilities",
        "design",
        "development",
        "data",
        "source",
        "data",
        "marts",
        "Oracle",
        "source",
        "data",
        "data",
        "acquisition",
        "transformation",
        "strategy",
        "data",
        "modeling",
        "star",
        "schema",
        "data",
        "warehouse",
        "ETL",
        "process",
        "Teradata",
        "data",
        "source",
        "databases",
        "files",
        "data",
        "warehouse",
        "Oracle",
        "Used",
        "Power",
        "mart",
        "Workflow",
        "Manager",
        "sessions",
        "event",
        "waitraise",
        "assignment",
        "email",
        "command",
        "mappings",
        "parameter",
        "mappings",
        "Router",
        "transformations",
        "migration",
        "projects",
        "data",
        "data",
        "warehouses",
        "OracleDB2",
        "Teradata",
        "mappings",
        "transformation",
        "features",
        "Aggregator",
        "Filter",
        "Joiner",
        "Expression",
        "Lookups",
        "workflows",
        "business",
        "needs",
        "Environment",
        "Data",
        "SQL",
        "Server",
        "SSIS",
        "SSRS",
        "Oracle",
        "g",
        "Teradata",
        "XML",
        "TOAD",
        "SQL",
        "PLSQL",
        "IBM",
        "AIX",
        "UNIX",
        "Shell",
        "Scripts",
        "Web",
        "Intelligence",
        "DSBASIC",
        "Cognos",
        "Erwin",
        "STAR",
        "team",
        "Remedy",
        "Maestro",
        "job",
        "scheduler",
        "Mercury",
        "Quality",
        "Center",
        "Java",
        "Developer",
        "Emorphosys",
        "August",
        "December",
        "Description",
        "Emorphosys",
        "Software",
        "Solutions",
        "technology",
        "product",
        "development",
        "company",
        "pioneer",
        "products",
        "business",
        "transformation",
        "innovation",
        "technology",
        "solutions",
        "ELearning",
        "Digital",
        "Media",
        "Enterprise",
        "Content",
        "Management",
        "Responsibilities",
        "core",
        "product",
        "development",
        "J2EE",
        "JSF",
        "Hibernate",
        "life",
        "cycle",
        "Object",
        "application",
        "development",
        "ObjectModeling",
        "Database",
        "Mapping",
        "GUI",
        "Design",
        "JavaScript",
        "client",
        "side",
        "validations",
        "StrutsValidator",
        "framework",
        "serverside",
        "validation",
        "requirement",
        "level",
        "design",
        "Waterfall",
        "model",
        "result",
        "data",
        "access",
        "SQL",
        "procedures",
        "Hibernate",
        "annotations",
        "Java",
        "stages",
        "application",
        "web",
        "services",
        "SOAP",
        "import",
        "attachments",
        "file",
        "applications",
        "Developed",
        "DAO",
        "data",
        "access",
        "objects",
        "Spring",
        "Framework",
        "components",
        "WebSphere",
        "Application",
        "server",
        "HTMLCSS",
        "JavaScript",
        "UI",
        "development",
        "Written",
        "queries",
        "Joins",
        "Triggers",
        "procedures",
        "Views",
        "MySql",
        "JSPs",
        "EJBs",
        "JSF",
        "Framework",
        "workflow",
        "application",
        "Developed",
        "Unit",
        "Test",
        "Cases",
        "JUnit",
        "unit",
        "testing",
        "application",
        "Environment",
        "Java",
        "J2EE",
        "Struts",
        "SQL",
        "JAX",
        "RPC",
        "XML",
        "RAD",
        "Websphere",
        "MQ",
        "Agile",
        "JSPS",
        "SOAP",
        "Education",
        "Bachelors",
        "Computer",
        "Science",
        "VIGNAN",
        "University",
        "El",
        "Segundo",
        "CA",
        "September",
        "Present",
        "Skills",
        "DATABASES",
        "years",
        "years",
        "years",
        "SQL",
        "years",
        "XML",
        "years",
        "Additional",
        "Information",
        "TECHNICAL",
        "SKILLSET",
        "Big",
        "Data",
        "HDFS",
        "MapReduce",
        "Hive",
        "Pig",
        "ZooKeeper",
        "Apache",
        "Spark",
        "Core",
        "MlLib",
        "Spark",
        "SQL",
        "Dataframes",
        "Utilities",
        "Sqoop",
        "Flume",
        "Kafka",
        "Oozie",
        "AutoSys",
        "SQL",
        "Databases",
        "Hbase",
        "Cassandra",
        "Languages",
        "C",
        "C",
        "Java",
        "J2EE",
        "PLSQL",
        "MR",
        "Pig",
        "Latin",
        "HiveQL",
        "Unix",
        "shell",
        "scripting",
        "Scala",
        "Operating",
        "Systems",
        "Sun",
        "Solaris",
        "RedHat",
        "Linux",
        "Ubuntu",
        "Linux",
        "Windows",
        "XPVista78",
        "Web",
        "Technologies",
        "HTML",
        "DHTML",
        "XML",
        "AJAX",
        "WSDL",
        "SOAP",
        "Databases",
        "Teradata",
        "DB2",
        "Oracle",
        "g",
        "SQL",
        "Server",
        "MySQL",
        "Tools",
        "IDE",
        "Maven",
        "Toad",
        "Eclipse",
        "NetBeans",
        "Hudson",
        "Sonar",
        "JDeveloper",
        "Assent",
        "PMD",
        "DB",
        "Visualizer"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T21:59:43.995770",
    "resume_data": "Big Data Engineer Big Data Engineer Big Data Engineer Canton OH Around 8 years of professional IT experience in the fields of Big Data Business Intelligenceand Java in Financial Insurance and Digital Services Industries Handson experience with major components in Hadoop Ecosystem like Map Reduce HDFS YARN Hive Pig HBase Sqoop Oozie Cassandra Impala and Flume Experience with new Hadoop 20 architecture YARN and developing YARN Applications on it Experience with Apache Sparks Core Spark SQL Streaming and MlLib components Experience with distributed systems largescale nonrelational data stores and multiterabyte data warehouses Firm grip on data modeling database performance tuning and NoSQL mapreduce systems Experience in processing semistructured and unstructured datasets Responsible for setting up processes for Hadoop based application design and implementation Experience in managing HBase database and using it to updatemodify the data Experience in running MapReduce and Spark jobs over YARN Experience with Cloudera Hortonworks and MapR distributions Handling data in various file formats such as Sequential AVRO RC Parquet and ORC Strong knowledge on the scalability and applications of Spark and its components Core SQL and Dataframes Handson experience in complete project life cycle design development testing and implementation of Client Server and Web applications Involved in developing complex ETL transformation performance tuning Extensively worked with Teradata utilities like BTEQ Fast Export Fast Load Multi Load to export and load data tofrom different source systems including flat files Hands on experience using query tools like Teradata SQL Assistant TOAD PLSQL developer and Query man Experience in Object Oriented Analysis and Design OOAD and development of software using UML Methodology good knowledge of J2EE design patterns and Core Java design patterns Experience using middleware architecture using Sun Java technologies like J2EE JSP Servlets and application servers like Web Sphere and Web logic Authorized to work in the US for any employer Work Experience Big Data Engineer SunGard Financial Systems New York NY August 2014 to December 2015 Description Kiodex Risk work bench is a SaaS solution that helps trading firms banksFCMs and corporations manage commodities exposures through front middle and back office functions including quantifying portfolio risks via different exposure reports Responsibilities Extracted data from relational databases such as SQL Server and MySql by developing Scala and SQL code Uploaded it to Hive and combined new tables with existing databases Developed code to preprocess large sets of various types of file formats such as Text Avro Sequence files XML JSON and Parquet Configured big data workflows to run on the top of Hadoop which comprises of heterogeneous jobs like Pig Hive Sqoop and MapReduce Loaded various formats of structured and unstructured data from Linux file system to HDFS Used Combiners and Partitioners in MapReduce programming Written Pig Scripts to ETL the data into NOSQL database for faster analysis Read from Flume and involved in pushing batches of data to HDFS and HBase for real time processing of the files Parsing XML data into structured format and loading into HDFS Scheduled various ETL process and Hive scripts by developing Oozie workflow Utilized Tableau to visualize the analyzed data and performed report design and delivery Created POC for Flume implementation Involved in reviewing both functional and nonfunctional aspects of the business model Championed to communicate and present the models to business customers and executives using the same Environment Hadoop HDFS Map Reduce Sqoop HBase Shell Scripting PIG HIVE Oozie Core Java Hortonworks Distribution LINUX Business IntelligenceETL Developer Synacor Buffalo NY January 2013 to July 2014 Description Synacor is a provider of private label digital Internet services to North Americas cable and telecom companies Its a global technology company providing Internet solutions to enable ISPs media companies and advertisers to build close relationships with consumers on the Internet Theintranet application represents a near term opportunity in which thecompound would move quickly into Development The purpose of this application is to create a simplified process where ideas are generated tracked and reviewed using existing governance Responsibilities Involved in design development of operational data source and data marts in Oracle Reviewed source data and recommend data acquisition and transformation strategy Involved in conceptual logical and physical data modeling and used star schema in designing the data warehouse Designed ETL process using Teradata to load the data from various source databases and flat files to target data warehouse in Oracle Used Power mart Workflow Manager to design sessions event waitraise and assignment email and command to execute mappings Created parameter based mappings Router and lookup transformations Involved in migration projects to migrate data from data warehouses on OracleDB2 and migrated those to Teradata Optimized mappings using transformation features like Aggregator Filter Joiner Expression and Lookups Created daily and weekly workflows and scheduled to run based on business needs Environment Data modeling SQL Server SSIS SSRS Oracle 10g Teradata 6 XML TOAD SQL PLSQL IBM AIX UNIX Shell Scripts Web Intelligence DSBASIC Cognos Erwin STAR team Remedy Maestro job scheduler Mercury Quality Center ControlM Java Developer Emorphosys August 2011 to December 2012 Description Emorphosys Software Solutions is a global technology based product development company which is a pioneer in creating simple scalable products that focus on business transformation through innovation It provides technology based solutions in the ELearning Digital Media and Enterprise Content Management domains Responsibilities Involved in the core product development using J2EE JSF and Hibernate Actively involved in the full life cycle Object Oriented application development ObjectModeling Database Mapping GUI Design Used JavaScript to perform client side validations and StrutsValidator framework for serverside validation Worked on requirement gathering high level design and Waterfall model to get best result Created data access using SQL and PLSQL stored procedures Used Hibernate annotations with Java for various stages in the application Built web services upon SOAP to export and import attachments from file to associated applications Developed DAO data access objects using Spring Framework Deployed the components in to WebSphere Application server Used HTMLCSS and JavaScript for UI development Written sql queries including Joins Triggers Stored procedures Views using MySql Implemented the JSPs and EJBs in the JSF Framework to handle the workflow of the application Developed Unit Test Cases used JUnit for unit testing of the application Environment Java J2EE Struts SQL JAX RPC XML RAD Websphere MQ Agile JSPS SOAP Education Bachelors in Computer Science VIGNAN University El Segundo CA September 2017 to Present Skills DATABASES 4 years JAVA 4 years PLSQL 4 years SQL 4 years XML 4 years Additional Information TECHNICAL SKILLSET Big Data HDFS MapReduce Hive Pig ZooKeeper Apache Spark Core MlLib Spark SQL and Dataframes Utilities Sqoop Flume Kafka Oozie and AutoSys No SQL Databases Hbase Cassandra Languages C C Java J2EE PLSQL MR Pig Latin HiveQL Unix shell scripting and Scala Operating Systems Sun Solaris RedHat Linux Ubuntu Linux and Windows XPVista78 Web Technologies HTML DHTML XML AJAX WSDL SOAP Databases and Datawarehousing Teradata DB2 Oracle 9i10g11g SQL Server MySQL Tools and IDE Maven Toad Eclipse NetBeans ANT Hudson Sonar JDeveloper Assent PMD DB Visualizer",
    "unique_id": "76574796-9e99-4bee-b23a-d0af03511b59"
}