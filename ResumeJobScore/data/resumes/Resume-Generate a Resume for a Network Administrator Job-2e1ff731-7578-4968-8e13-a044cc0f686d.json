{
    "clean_data": "Sr Data Engineer Sr Data Engineer Sr Data Engineer ATT Plano TX Proactive IT developer with 9 years of working experience in JavaJ2EE Technology and development design of various scalable systems using Hadoop Technologies on various environments Experience in installation configuration supporting and managing Hadoop Clusters using Horton works and Cloudera CDH3 CDH4 distributions on Amazon web services AWS Extraordinary Understanding of Hadoop building and Hands on involvement with Hadoop segments such as Job Tracker Task Tracker Name Node Data Node and HDFS Framework Extensive experience in analyzing data using Hadoop Ecosystems including HDFS Hive PIG Sqoop Flume MapReduce Spark Kafka HBase Oozie Solr and Zookeeper Extensive knowledge on NoSQL databases like HBase Cassandra and Mongo DB Configured Zookeeper Cassandra and Flume to the existing Hadoop cluster Expertise in writing Hadoop Jobs for analyzing data using Hive QL Queries Pig Latin Data flow language and custom MapReduce programs in Java Experience in converting Hive queries into Spark transformations using Spark RDDs and Scala Hands on Experience in troubleshooting errors in HBase Shell Pig Hive and MapReduce Handson experience in provisioning and managing multitenant Cassandra cluster on public cloud environment Amazon Web Services AWS EC2 Open Stack Experience in NoSQL ColumnOriented Databases like HBase Cassandra and its Integration with Hadoop cluster Experience in maintaining the big data platform using open source technologies such as Spark and Elastic Search Planned and created answer for constant information ingestion utilizing Kafka Storm Spark spilling and different NoSQL databases Developed Scala scripts UDFs using both Data framesSQL and RDDMapReduce in Spark for Data Aggregation queries and writing data back into RDBMS through Sqoop Experience in understanding the security requirements for Hadoop and integrate with Kerberos authentication and authorization infrastructure Good hands on experience in creating the RDDs DFs for the required input data and performed the data transformations using Spark Scala Knowledge in developing a Nifi flow prototype for data ingestion in HDFS Extensive experience working in Oracle DB2 SQL Server PLSQL and My SQL database and Java Core concepts like OOPS Multithreading Collections and IO Experience in Service Oriented Architecture using Web Services like SOAP Restful Work Experience Sr Data Engineer ATT Plano TX January 2018 to Present Sr Bigdata Developer T Mobile Bellevue WA May 2018 to January 2019 Responsibilities Developed various spark applications using Scala to perform various enrichment of these click stream data merged with user profile data Utilized SparkSQL to event enrichment and used SparkSQL to prepare various levels of user behavior summaries Worked on SQS Queue receiver using Spark Streaming context to consume the data from extended queue and integrated with ETL Functions Real time streaming the data using Spark with SQS Responsible for handling Streaming data from web server console logs Optimize the Hive tables using optimization techniques such as partitions and bucketing to provide better performance with HiveQL queries Worked on migrating data from traditional RDBMS to HDFS Used Scala to convert HiveSQL queries into RDD transformations in Apache Spark Written Programs in Spark using Scala for Data quality check Used Spark Streaming to divide streaming data into batches as an input to Spark engine for batch processing Implemented the use of Amazon EMR for Big Data processing among a Hadoop Cluster of virtual servers on Amazon related EC2 and S3 Optimized Hive QL pig scripts by using execution engine like Tez Spark Environment Hadoop HDFS Hive Spark AWS EC2 S3 Kafka Yarn Shell Scripting Scala Pig Oozie Java Agile methods Linux MySQL Elastic Search Kibana Teradata Hadoop Developer Visa Austin TX August 2017 to May 2018 Developed Spark Applications by using Spark Java and Implemented Apache Spark data processing project to handle data from various RDBMS and Streaming sources Handled importing of data from various data sources performed data control checks using Spark and loaded data into HDFS Involved in converting HiveSQL queries into Spark transformations using Spark RDD Scala Used Spark SQL to Load JSON data and create SchemaRDD and loaded it into Hive Tables and handled structured data using Spark SQ Imported data from AWS S3 into Spark RDD Performed transformations and actions on RDDs Used Spark and Spark SQL to read the parquet data and create the tables in hive using the Scala API Implemented Spark using Scala and utilizing Data frames and Spark SQL API for faster processing of data Developed Scala scripts UDFs using both Data framesSQLData sets and RDDMap Reduce in Spark 16 for Data Aggregation queries and writing data back into OLTP system through Sqoop Processing the schema oriented and nonschemaoriented data using Scala and Spark Used Spark API over Cloudera Hadoop YARN to perform analytics on data in HDFS Worked on streaming pipeline that uses Spark to read data from Kafka transform it and write it to HDFS Analyzed the weblog data using the HiveQL integrated Oozie with the rest of the Hadoop stack Utilized cluster coordination services through Zookeeper Environment Scala Spark Spark SQL Spark Streaming Azkaban Presto Hive Apache Crunch Elastic Search GIT Repository Amazon S3 Amazon AWS Ec2EMR Spark cluster Hadoop Framework Sqoop DB2 Data Engineering Walt Disney Glendale CA December 2016 to August 2017 Responsibilities Developed optimal strategies for distributing the web log data over the cluster importing and exporting the stored web log data into HDFS and Hive using Sqoop Design and develop ELT data pipeline using Spark App to fetch data from Legacy system and thirdparty API social media sites Developed custom mappers in python script and Hive UDFs and UDAFs based on the given requirement Design and develop DMA Disney Movies anywhere dashboard for BI analyst team Perform data analytics and load data to Amazon s3Data Lake Spark cluster Involved in querying data using Spark SQL on top of Spark engine Developed Spark scripts by using Python shell commands as per the requirement Writing Pig and Hive scripts with UDF in MR and Python to perform ETL on AWS Cloud Services Worked with file formats text avro parquet and sequence files Involved in migrating HiveQL into Impala to minimize query response time Created Hive tables dynamic partitions buckets for sampling and working on them using HQL Defined job flow using Azkaban scheduler to automate the Hadoop jobs and installed Zookeepers for automatic node failovers Performed Tableau type conversion functions when connected to relational data sources Environment LanguagesTechnologies Java JDK16 and higher Azkaban Spark SQL Presto Hive Apache Crunch Elastic Search Spring boot Eclipse GIT Repository Amazon S3 Amazon AWS Ec2EMR Spark cluster Hadoop Framework Sqoop Hadoop Developer WilliamsSonoma Inc San Francisco CA September 2015 to October 2016 Responsibilities Involved in managing nodes on Hadoop cluster and monitor Hadoop cluster job performance using Cloudera manager Involved in loading data from edge node to HDFS using shell scripting Created Map Reduce programs to handle semiunstructured data like xml json Avro data files and sequence files for log files Developed Spark scripts by using Python shell commands as per the requirement Integrated Elastic Search and implemented dynamic facetedsearch Involved in collecting and aggregating large amounts of log data using Apache Flume and staging data in HDFS for further analysis Design and Develop Pig Latin scripts and Pig command line transformations for data joins and custom processing of Map reduce outputs Implemented Spark using Scala and Spark SQL for faster testing and processing of data Implementing advanced procedures like text analytics and processing using the inmemory computing capabilities like Apache Spark written in Scala Implemented Spark RDD transformations to map business analysis and apply actions on top of transformations Used maven to build and deploy the Jars for MapReduce Pig and Hive UDFs Reviewed basic SQL queries and edited inner left and right joins in Tableau Desktop by connecting livedynamic and static datasets Environment Hadoop Scala Map Reduce HDFS Spark Scala Kafka AWS Apache SOLR Hive Cassandra maven Jenkins Pig UNIX Python MRUnit Git Hadoop Developer Veritas Technologies LLC Mountain View CA November 2014 to August 2015 Responsibilities Responsible for building scalable distributed data solutions using Hadoop Worked in joining raw data with the reference data using Pig scripting Implemented DataStax Enterprise Search with Apache Solr Created java operators to process data using DAG streams and load data to HDFS Configured Designed implemented and monitored Kafka cluster and connectors Developed ETL jobs using SparkScala to migrate data from Oracle to new hive tables Developed and Deployed applications using Apache Spark Scala Developed Oozie workflow for scheduling and orchestrating the ETL process Implemented Spark using Scala and Spark SQL for faster testing and processing of data Helped in troubleshooting Scala problems while working with Micro Strategy to produce illustrative reports and dashboards along with adhoc analysis Developed Hive queries for the analysts and I have written scripts using Scala Created and worked Sqoop jobs with incremental load to populate Hive External tables Installed and configured Flume Hive Pig Sqoop and Oozie on the Hadoop cluster Continuous Integration environments in SCRUM and Agile methodologies Extracted the data from Teradata into HDFS using the Sqoop Managed real time data processing and real time Data Ingestion in HBase and Hive using Storm Environment Hadoop HDFS Pig Hive Oozie HBase Kafka Apache SOLR MapReduce Apache SOLR Sqoop Storm Spark Scala LINUX Cloudera Maven Jenkins Java SQL Client Well Care Tampa Florida Industry HealthCare Mar13 Oct14 Role JavaHadoop Developer Responsibilities Exported data from DB2 to HDFS using Sqoop and Developed MapReduce jobs using Java API Used Spring AOP to implement Distributed declarative transaction throughout the application Designed and developed Java batch programs in Spring Batch Installed and configured Pig and wrote Pig Latin scripts Created and maintained Technical documentation for launching Cloudera Hadoop Clusters and for executing Hive queries and Pig Scripts Developed workflowusing Oozie for running MapReduce jobs and Hive Queries Involved in loading data from UNIX file system to HDFS Created java operators to process data using DAG streams and load data to HDFS Assisted in exporting analyzed data to relational databases using Sqoop Involved in Develop monitoring and performance metrics for Hadoop clusters Continuous monitoring and managing the Hadoop cluster through Cloudera Manager Environment Hadoop HDFS Hive Flume Sqoop HBase PIG Eclipse Spark My SQL and Ubuntu Zookeeper Maven Jenkins Java JDK 16 Oracle10g Java Developer Hexacorp Technical services NJ October 2011 to February 2013 Responsibilities Effectively interacted with team members and business users for requirements gathering Coded front end components using HTML JavaScript and jQuery Back End components using Java spring Hibernate Services Oriented components using Restful and SOAP based web services and Rules based components using JBoss Drools Involved in analysis design and implementation phases of the software development lifecycle SDLC Implementation of spring core J2EE patterns like MVC Dependency Injection DI and Inversion of Control IOC Implemented REST Web Services with Jersey API to deal with customer requests Developed test cases using J Unit and used Log4j as the logging framework Worked with HQL and Criteria API from retrieving the data elements from database Developed user interface using HTML Spring Tags JavaScript J Query and CSS Developed the application using Eclipse IDE and worked under Agile Environment Utilized Eclipse IDE as improvement environment to plan create and convey Spring segments on Web Logic Environment Java J2EE JDBC EJB UML Swing HTML JavaScript CSS J Query Spring 30 JNDI Hibernate 30 Java Mail Web Services REST Oracle 10g J Unit Log4j Eclipse Web logic 103 Java Developer Choice Software Limited Hyderabad Telangana August 2008 to September 2011 Responsibilities Involved in various stages of Enhancements in the Application by doing the required analysis development and testing For analysis and design of application created Use Cases Class and Sequence Diagrams Developed webbased user interfaces using struts framework Developed and maintained JavaJ2EE code required for the web application Handled Client Side Validations used JavaScript and Involved in integration of various Struts actions in the framework Involved in the development of the User Interfaces using HTML JSP CSS and JavaScript Developed Tested and Debugged the Java JSP and EJB components using Eclipse Environments Java JDK 15 J2EE Servlets Struts JSP HTML CSS JavaScript EJB Eclipse WebLogic 81 Windows SOAP Restful Skills CASSANDRA HDFS IMPALA MAHOUT OOZIE Additional Information Technical Skills Big Data Eco systems HDFS Map Reduce Hive YARN Pig Sqoop Kafka Storm Flume Oozie and ZooKeeper Apache Spark Apache Tez Impala Nifi Apache Solr Rabbit MQ Scala No SQL Databases Hbase Cassandra mongoDB Programming Languages C C Java J2EE PLSQL Pig Latin Scala Python JavaJ2EE Technologies Applets Swing JDBC JNDI JSON JSTL RMI JMS Java Script JSP Servlets EJB JSF JQuery AngularJS Frameworks MVC Struts Spring Hibernate Version control SVN CVS Business Intelligence Tools Tableau QlikView Pentaho IBM Cognos intelligence Databases Oracle 9i10g11g DB2 SQL Server MySQL Teradata Tools and IDE Eclipse Net Beans Toad Maven ANT Hudson Sonar JDeveloper Assent PMD DB Visualizer IntelliJ Cloud Technologies Amazon Web Services AWS CDH3 CDH4 CDH5 HortonWorks Mahout Microsoft Azure Insight Amazon Redshift",
    "entities": [
        "Implemented Spark",
        "Writing Pig",
        "CSS Developed",
        "Hadoop Clusters",
        "MapReduce Handson",
        "Implemented DataStax Enterprise Search",
        "Spark SQ Imported",
        "Sr Data Engineer Sr Data",
        "BI",
        "HDFS",
        "UNIX",
        "Sqoop Processing",
        "Developed Spark",
        "JavaJ2EE Technology",
        "HDFS Configured Designed",
        "jQuery Back End",
        "Hive External",
        "Amazon Web Services AWS",
        "Hadoop Framework Sqoop Hadoop Developer",
        "Spark Scala Knowledge",
        "UDAFs",
        "Cloudera Hadoop Clusters",
        "RDD",
        "Hadoop",
        "Sqoop Involved",
        "HDFS Involved",
        "Telangana",
        "Integrated Elastic Search",
        "Scala Created",
        "HDFS Analyzed",
        "Java spring Hibernate Services Oriented",
        "HBase Cassandra",
        "ELT",
        "Avro",
        "Implemented Apache Spark",
        "Apache Spark",
        "TX",
        "Amazon",
        "SparkScala",
        "Hadoop Technologies",
        "Load JSON",
        "CDH3",
        "Spark App",
        "Cloudera Hadoop",
        "Hadoop Ecosystems",
        "Apache Spark Written Programs",
        "HDFS Hive PIG",
        "Inversion of Control IOC Implemented REST Web Services",
        "SVN CVS Business Intelligence Tools Tableau",
        "Hadoop Framework Sqoop DB2",
        "SparkSQL",
        "Developed",
        "Kerberos",
        "AWS S3",
        "Hive Queries Involved",
        "Node Data",
        "Utilized",
        "San Francisco",
        "WebLogic 81 Windows SOAP Restful Skills CASSANDRA HDFS IMPALA MAHOUT OOZIE Additional Information Technical Skills Big Data Eco",
        "Spring Hibernate Version",
        "SOLR Sqoop",
        "ZooKeeper Apache Spark",
        "Hadoop Worked",
        "Spark for Data Aggregation",
        "Oracle DB2 SQL Server",
        "Use Cases Class",
        "Develop",
        "JSP",
        "HDFS Assisted",
        "Tableau Desktop",
        "Restful and SOAP",
        "Spark Streaming",
        "SQS Responsible",
        "Present Sr Bigdata Developer T Mobile Bellevue",
        "Java Developer Hexacorp Technical",
        "J Unit",
        "Spark SQL API",
        "MVC",
        "Spark",
        "EJB",
        "Created Hive",
        "Amazon EMR",
        "Hadoop Jobs",
        "Hive Tables",
        "API",
        "Perform",
        "Sqoop",
        "Created",
        "Zookeepers",
        "Data Aggregation",
        "HTML Spring Tags",
        "MR",
        "Oracle",
        "Coded",
        "HortonWorks Mahout",
        "Python JavaJ2EE Technologies Applets",
        "log data",
        "OOPS Multithreading Collections",
        "Scala Hands",
        "SQL",
        "OLTP",
        "UDF",
        "Java Developer Choice Software Limited Hyderabad",
        "Azkaban",
        "J2EE Servlets Struts",
        "the User Interfaces",
        "JBoss Drools Involved",
        "IDE Eclipse Net Beans Toad Maven ANT Hudson Sonar JDeveloper Assent",
        "Big Data",
        "Hive",
        "HDFS Created",
        "Azkaban Spark",
        "DMA Disney Movies",
        "DAG",
        "AWS Cloud Services Worked",
        "ETL",
        "Optimize the Hive",
        "Impala",
        "Spark SQL",
        "HDFS Framework Extensive",
        "Walt Disney Glendale",
        "SchemaRDD",
        "HBase Shell Pig Hive",
        "HDFS Extensive",
        "Continuous Integration",
        "Data Ingestion",
        "Java Mail Web Services",
        "JNDI Hibernate",
        "Sequence Diagrams Developed",
        "Job Tracker Task Tracker",
        "CSS",
        "Integration with Hadoop",
        "SQS Queue",
        "Storm Environment Hadoop HDFS Pig Hive Oozie HBase",
        "Developed MapReduce",
        "Nifi",
        "Data",
        "MapReduce",
        "NoSQL",
        "Application",
        "Zookeeper Environment Scala Spark Spark",
        "Teradata",
        "WilliamsSonoma Inc",
        "Cloud Technologies Amazon Web Services AWS",
        "Cloudera"
    ],
    "experience": "Experience in installation configuration supporting and managing Hadoop Clusters using Horton works and Cloudera CDH3 CDH4 distributions on Amazon web services AWS Extraordinary Understanding of Hadoop building and Hands on involvement with Hadoop segments such as Job Tracker Task Tracker Name Node Data Node and HDFS Framework Extensive experience in analyzing data using Hadoop Ecosystems including HDFS Hive PIG Sqoop Flume MapReduce Spark Kafka HBase Oozie Solr and Zookeeper Extensive knowledge on NoSQL databases like HBase Cassandra and Mongo DB Configured Zookeeper Cassandra and Flume to the existing Hadoop cluster Expertise in writing Hadoop Jobs for analyzing data using Hive QL Queries Pig Latin Data flow language and custom MapReduce programs in Java Experience in converting Hive queries into Spark transformations using Spark RDDs and Scala Hands on Experience in troubleshooting errors in HBase Shell Pig Hive and MapReduce Handson experience in provisioning and managing multitenant Cassandra cluster on public cloud environment Amazon Web Services AWS EC2 Open Stack Experience in NoSQL ColumnOriented Databases like HBase Cassandra and its Integration with Hadoop cluster Experience in maintaining the big data platform using open source technologies such as Spark and Elastic Search Planned and created answer for constant information ingestion utilizing Kafka Storm Spark spilling and different NoSQL databases Developed Scala scripts UDFs using both Data framesSQL and RDDMapReduce in Spark for Data Aggregation queries and writing data back into RDBMS through Sqoop Experience in understanding the security requirements for Hadoop and integrate with Kerberos authentication and authorization infrastructure Good hands on experience in creating the RDDs DFs for the required input data and performed the data transformations using Spark Scala Knowledge in developing a Nifi flow prototype for data ingestion in HDFS Extensive experience working in Oracle DB2 SQL Server PLSQL and My SQL database and Java Core concepts like OOPS Multithreading Collections and IO Experience in Service Oriented Architecture using Web Services like SOAP Restful Work Experience Sr Data Engineer ATT Plano TX January 2018 to Present Sr Bigdata Developer T Mobile Bellevue WA May 2018 to January 2019 Responsibilities Developed various spark applications using Scala to perform various enrichment of these click stream data merged with user profile data Utilized SparkSQL to event enrichment and used SparkSQL to prepare various levels of user behavior summaries Worked on SQS Queue receiver using Spark Streaming context to consume the data from extended queue and integrated with ETL Functions Real time streaming the data using Spark with SQS Responsible for handling Streaming data from web server console logs Optimize the Hive tables using optimization techniques such as partitions and bucketing to provide better performance with HiveQL queries Worked on migrating data from traditional RDBMS to HDFS Used Scala to convert HiveSQL queries into RDD transformations in Apache Spark Written Programs in Spark using Scala for Data quality check Used Spark Streaming to divide streaming data into batches as an input to Spark engine for batch processing Implemented the use of Amazon EMR for Big Data processing among a Hadoop Cluster of virtual servers on Amazon related EC2 and S3 Optimized Hive QL pig scripts by using execution engine like Tez Spark Environment Hadoop HDFS Hive Spark AWS EC2 S3 Kafka Yarn Shell Scripting Scala Pig Oozie Java Agile methods Linux MySQL Elastic Search Kibana Teradata Hadoop Developer Visa Austin TX August 2017 to May 2018 Developed Spark Applications by using Spark Java and Implemented Apache Spark data processing project to handle data from various RDBMS and Streaming sources Handled importing of data from various data sources performed data control checks using Spark and loaded data into HDFS Involved in converting HiveSQL queries into Spark transformations using Spark RDD Scala Used Spark SQL to Load JSON data and create SchemaRDD and loaded it into Hive Tables and handled structured data using Spark SQ Imported data from AWS S3 into Spark RDD Performed transformations and actions on RDDs Used Spark and Spark SQL to read the parquet data and create the tables in hive using the Scala API Implemented Spark using Scala and utilizing Data frames and Spark SQL API for faster processing of data Developed Scala scripts UDFs using both Data framesSQLData sets and RDDMap Reduce in Spark 16 for Data Aggregation queries and writing data back into OLTP system through Sqoop Processing the schema oriented and nonschemaoriented data using Scala and Spark Used Spark API over Cloudera Hadoop YARN to perform analytics on data in HDFS Worked on streaming pipeline that uses Spark to read data from Kafka transform it and write it to HDFS Analyzed the weblog data using the HiveQL integrated Oozie with the rest of the Hadoop stack Utilized cluster coordination services through Zookeeper Environment Scala Spark Spark SQL Spark Streaming Azkaban Presto Hive Apache Crunch Elastic Search GIT Repository Amazon S3 Amazon AWS Ec2EMR Spark cluster Hadoop Framework Sqoop DB2 Data Engineering Walt Disney Glendale CA December 2016 to August 2017 Responsibilities Developed optimal strategies for distributing the web log data over the cluster importing and exporting the stored web log data into HDFS and Hive using Sqoop Design and develop ELT data pipeline using Spark App to fetch data from Legacy system and thirdparty API social media sites Developed custom mappers in python script and Hive UDFs and UDAFs based on the given requirement Design and develop DMA Disney Movies anywhere dashboard for BI analyst team Perform data analytics and load data to Amazon s3Data Lake Spark cluster Involved in querying data using Spark SQL on top of Spark engine Developed Spark scripts by using Python shell commands as per the requirement Writing Pig and Hive scripts with UDF in MR and Python to perform ETL on AWS Cloud Services Worked with file formats text avro parquet and sequence files Involved in migrating HiveQL into Impala to minimize query response time Created Hive tables dynamic partitions buckets for sampling and working on them using HQL Defined job flow using Azkaban scheduler to automate the Hadoop jobs and installed Zookeepers for automatic node failovers Performed Tableau type conversion functions when connected to relational data sources Environment LanguagesTechnologies Java JDK16 and higher Azkaban Spark SQL Presto Hive Apache Crunch Elastic Search Spring boot Eclipse GIT Repository Amazon S3 Amazon AWS Ec2EMR Spark cluster Hadoop Framework Sqoop Hadoop Developer WilliamsSonoma Inc San Francisco CA September 2015 to October 2016 Responsibilities Involved in managing nodes on Hadoop cluster and monitor Hadoop cluster job performance using Cloudera manager Involved in loading data from edge node to HDFS using shell scripting Created Map Reduce programs to handle semiunstructured data like xml json Avro data files and sequence files for log files Developed Spark scripts by using Python shell commands as per the requirement Integrated Elastic Search and implemented dynamic facetedsearch Involved in collecting and aggregating large amounts of log data using Apache Flume and staging data in HDFS for further analysis Design and Develop Pig Latin scripts and Pig command line transformations for data joins and custom processing of Map reduce outputs Implemented Spark using Scala and Spark SQL for faster testing and processing of data Implementing advanced procedures like text analytics and processing using the inmemory computing capabilities like Apache Spark written in Scala Implemented Spark RDD transformations to map business analysis and apply actions on top of transformations Used maven to build and deploy the Jars for MapReduce Pig and Hive UDFs Reviewed basic SQL queries and edited inner left and right joins in Tableau Desktop by connecting livedynamic and static datasets Environment Hadoop Scala Map Reduce HDFS Spark Scala Kafka AWS Apache SOLR Hive Cassandra maven Jenkins Pig UNIX Python MRUnit Git Hadoop Developer Veritas Technologies LLC Mountain View CA November 2014 to August 2015 Responsibilities Responsible for building scalable distributed data solutions using Hadoop Worked in joining raw data with the reference data using Pig scripting Implemented DataStax Enterprise Search with Apache Solr Created java operators to process data using DAG streams and load data to HDFS Configured Designed implemented and monitored Kafka cluster and connectors Developed ETL jobs using SparkScala to migrate data from Oracle to new hive tables Developed and Deployed applications using Apache Spark Scala Developed Oozie workflow for scheduling and orchestrating the ETL process Implemented Spark using Scala and Spark SQL for faster testing and processing of data Helped in troubleshooting Scala problems while working with Micro Strategy to produce illustrative reports and dashboards along with adhoc analysis Developed Hive queries for the analysts and I have written scripts using Scala Created and worked Sqoop jobs with incremental load to populate Hive External tables Installed and configured Flume Hive Pig Sqoop and Oozie on the Hadoop cluster Continuous Integration environments in SCRUM and Agile methodologies Extracted the data from Teradata into HDFS using the Sqoop Managed real time data processing and real time Data Ingestion in HBase and Hive using Storm Environment Hadoop HDFS Pig Hive Oozie HBase Kafka Apache SOLR MapReduce Apache SOLR Sqoop Storm Spark Scala LINUX Cloudera Maven Jenkins Java SQL Client Well Care Tampa Florida Industry HealthCare Mar13 Oct14 Role JavaHadoop Developer Responsibilities Exported data from DB2 to HDFS using Sqoop and Developed MapReduce jobs using Java API Used Spring AOP to implement Distributed declarative transaction throughout the application Designed and developed Java batch programs in Spring Batch Installed and configured Pig and wrote Pig Latin scripts Created and maintained Technical documentation for launching Cloudera Hadoop Clusters and for executing Hive queries and Pig Scripts Developed workflowusing Oozie for running MapReduce jobs and Hive Queries Involved in loading data from UNIX file system to HDFS Created java operators to process data using DAG streams and load data to HDFS Assisted in exporting analyzed data to relational databases using Sqoop Involved in Develop monitoring and performance metrics for Hadoop clusters Continuous monitoring and managing the Hadoop cluster through Cloudera Manager Environment Hadoop HDFS Hive Flume Sqoop HBase PIG Eclipse Spark My SQL and Ubuntu Zookeeper Maven Jenkins Java JDK 16 Oracle10 g Java Developer Hexacorp Technical services NJ October 2011 to February 2013 Responsibilities Effectively interacted with team members and business users for requirements gathering Coded front end components using HTML JavaScript and jQuery Back End components using Java spring Hibernate Services Oriented components using Restful and SOAP based web services and Rules based components using JBoss Drools Involved in analysis design and implementation phases of the software development lifecycle SDLC Implementation of spring core J2EE patterns like MVC Dependency Injection DI and Inversion of Control IOC Implemented REST Web Services with Jersey API to deal with customer requests Developed test cases using J Unit and used Log4j as the logging framework Worked with HQL and Criteria API from retrieving the data elements from database Developed user interface using HTML Spring Tags JavaScript J Query and CSS Developed the application using Eclipse IDE and worked under Agile Environment Utilized Eclipse IDE as improvement environment to plan create and convey Spring segments on Web Logic Environment Java J2EE JDBC EJB UML Swing HTML JavaScript CSS J Query Spring 30 JNDI Hibernate 30 Java Mail Web Services REST Oracle 10 g J Unit Log4j Eclipse Web logic 103 Java Developer Choice Software Limited Hyderabad Telangana August 2008 to September 2011 Responsibilities Involved in various stages of Enhancements in the Application by doing the required analysis development and testing For analysis and design of application created Use Cases Class and Sequence Diagrams Developed webbased user interfaces using struts framework Developed and maintained JavaJ2EE code required for the web application Handled Client Side Validations used JavaScript and Involved in integration of various Struts actions in the framework Involved in the development of the User Interfaces using HTML JSP CSS and JavaScript Developed Tested and Debugged the Java JSP and EJB components using Eclipse Environments Java JDK 15 J2EE Servlets Struts JSP HTML CSS JavaScript EJB Eclipse WebLogic 81 Windows SOAP Restful Skills CASSANDRA HDFS IMPALA MAHOUT OOZIE Additional Information Technical Skills Big Data Eco systems HDFS Map Reduce Hive YARN Pig Sqoop Kafka Storm Flume Oozie and ZooKeeper Apache Spark Apache Tez Impala Nifi Apache Solr Rabbit MQ Scala No SQL Databases Hbase Cassandra mongoDB Programming Languages C C Java J2EE PLSQL Pig Latin Scala Python JavaJ2EE Technologies Applets Swing JDBC JNDI JSON JSTL RMI JMS Java Script JSP Servlets EJB JSF JQuery AngularJS Frameworks MVC Struts Spring Hibernate Version control SVN CVS Business Intelligence Tools Tableau QlikView Pentaho IBM Cognos intelligence Databases Oracle 9i10g11 g DB2 SQL Server MySQL Teradata Tools and IDE Eclipse Net Beans Toad Maven ANT Hudson Sonar JDeveloper Assent PMD DB Visualizer IntelliJ Cloud Technologies Amazon Web Services AWS CDH3 CDH4 CDH5 HortonWorks Mahout Microsoft Azure Insight Amazon Redshift",
    "extracted_keywords": [
        "Sr",
        "Data",
        "Engineer",
        "Sr",
        "Data",
        "Engineer",
        "Sr",
        "Data",
        "Engineer",
        "ATT",
        "Plano",
        "TX",
        "Proactive",
        "IT",
        "developer",
        "years",
        "experience",
        "JavaJ2EE",
        "Technology",
        "development",
        "design",
        "systems",
        "Hadoop",
        "Technologies",
        "environments",
        "Experience",
        "installation",
        "configuration",
        "Hadoop",
        "Clusters",
        "Horton",
        "works",
        "Cloudera",
        "CDH3",
        "CDH4",
        "distributions",
        "Amazon",
        "web",
        "services",
        "Understanding",
        "Hadoop",
        "building",
        "Hands",
        "involvement",
        "Hadoop",
        "segments",
        "Job",
        "Tracker",
        "Task",
        "Tracker",
        "Name",
        "Node",
        "Data",
        "Node",
        "HDFS",
        "Framework",
        "experience",
        "data",
        "Hadoop",
        "Ecosystems",
        "HDFS",
        "Hive",
        "PIG",
        "Sqoop",
        "Flume",
        "MapReduce",
        "Spark",
        "Kafka",
        "HBase",
        "Oozie",
        "Solr",
        "Zookeeper",
        "knowledge",
        "NoSQL",
        "databases",
        "HBase",
        "Cassandra",
        "Mongo",
        "DB",
        "Configured",
        "Zookeeper",
        "Cassandra",
        "Flume",
        "Hadoop",
        "cluster",
        "Expertise",
        "Hadoop",
        "Jobs",
        "data",
        "Hive",
        "QL",
        "Queries",
        "Pig",
        "Latin",
        "Data",
        "flow",
        "language",
        "custom",
        "MapReduce",
        "programs",
        "Java",
        "Experience",
        "Hive",
        "queries",
        "Spark",
        "transformations",
        "Spark",
        "RDDs",
        "Scala",
        "Hands",
        "Experience",
        "troubleshooting",
        "errors",
        "HBase",
        "Shell",
        "Pig",
        "Hive",
        "MapReduce",
        "Handson",
        "experience",
        "provisioning",
        "Cassandra",
        "cluster",
        "cloud",
        "environment",
        "Amazon",
        "Web",
        "Services",
        "AWS",
        "EC2",
        "Open",
        "Stack",
        "Experience",
        "NoSQL",
        "ColumnOriented",
        "Databases",
        "HBase",
        "Cassandra",
        "Integration",
        "Hadoop",
        "cluster",
        "Experience",
        "data",
        "platform",
        "source",
        "technologies",
        "Spark",
        "Elastic",
        "Search",
        "Planned",
        "answer",
        "information",
        "ingestion",
        "Kafka",
        "Storm",
        "Spark",
        "spilling",
        "NoSQL",
        "Developed",
        "Scala",
        "scripts",
        "UDFs",
        "Data",
        "framesSQL",
        "RDDMapReduce",
        "Spark",
        "Data",
        "Aggregation",
        "data",
        "RDBMS",
        "Sqoop",
        "Experience",
        "security",
        "requirements",
        "Hadoop",
        "authentication",
        "authorization",
        "infrastructure",
        "hands",
        "experience",
        "RDDs",
        "DFs",
        "input",
        "data",
        "data",
        "transformations",
        "Spark",
        "Scala",
        "Knowledge",
        "Nifi",
        "flow",
        "prototype",
        "data",
        "ingestion",
        "HDFS",
        "experience",
        "Oracle",
        "DB2",
        "SQL",
        "Server",
        "PLSQL",
        "SQL",
        "database",
        "Java",
        "Core",
        "concepts",
        "Multithreading",
        "Collections",
        "IO",
        "Experience",
        "Service",
        "Oriented",
        "Architecture",
        "Web",
        "Services",
        "SOAP",
        "Restful",
        "Work",
        "Experience",
        "Sr",
        "Data",
        "Engineer",
        "ATT",
        "Plano",
        "TX",
        "January",
        "Present",
        "Sr",
        "Bigdata",
        "Developer",
        "T",
        "Mobile",
        "Bellevue",
        "WA",
        "May",
        "January",
        "Responsibilities",
        "spark",
        "applications",
        "Scala",
        "enrichment",
        "stream",
        "data",
        "user",
        "profile",
        "data",
        "SparkSQL",
        "event",
        "enrichment",
        "SparkSQL",
        "levels",
        "user",
        "behavior",
        "summaries",
        "SQS",
        "Queue",
        "receiver",
        "Spark",
        "Streaming",
        "context",
        "data",
        "queue",
        "ETL",
        "Functions",
        "time",
        "data",
        "Spark",
        "SQS",
        "Responsible",
        "Streaming",
        "data",
        "web",
        "server",
        "console",
        "Optimize",
        "Hive",
        "tables",
        "optimization",
        "techniques",
        "partitions",
        "bucketing",
        "performance",
        "queries",
        "data",
        "RDBMS",
        "HDFS",
        "Scala",
        "HiveSQL",
        "queries",
        "RDD",
        "transformations",
        "Apache",
        "Spark",
        "Written",
        "Programs",
        "Spark",
        "Scala",
        "Data",
        "quality",
        "check",
        "Spark",
        "Streaming",
        "streaming",
        "data",
        "batches",
        "input",
        "Spark",
        "engine",
        "batch",
        "processing",
        "use",
        "Amazon",
        "EMR",
        "Big",
        "Data",
        "processing",
        "Hadoop",
        "Cluster",
        "servers",
        "Amazon",
        "EC2",
        "S3",
        "Optimized",
        "Hive",
        "QL",
        "pig",
        "scripts",
        "execution",
        "engine",
        "Tez",
        "Spark",
        "Environment",
        "Hadoop",
        "HDFS",
        "Hive",
        "Spark",
        "AWS",
        "EC2",
        "S3",
        "Kafka",
        "Yarn",
        "Shell",
        "Scripting",
        "Scala",
        "Pig",
        "Oozie",
        "Java",
        "Agile",
        "methods",
        "Linux",
        "MySQL",
        "Elastic",
        "Search",
        "Kibana",
        "Teradata",
        "Hadoop",
        "Developer",
        "Visa",
        "Austin",
        "TX",
        "August",
        "May",
        "Developed",
        "Spark",
        "Applications",
        "Spark",
        "Java",
        "Apache",
        "Spark",
        "data",
        "processing",
        "project",
        "data",
        "RDBMS",
        "Streaming",
        "sources",
        "importing",
        "data",
        "data",
        "sources",
        "data",
        "control",
        "checks",
        "Spark",
        "data",
        "HDFS",
        "HiveSQL",
        "queries",
        "Spark",
        "transformations",
        "Spark",
        "RDD",
        "Scala",
        "Spark",
        "SQL",
        "data",
        "SchemaRDD",
        "Hive",
        "Tables",
        "data",
        "Spark",
        "SQ",
        "data",
        "AWS",
        "S3",
        "Spark",
        "transformations",
        "actions",
        "RDDs",
        "Spark",
        "Spark",
        "SQL",
        "parquet",
        "data",
        "tables",
        "hive",
        "Scala",
        "API",
        "Spark",
        "Scala",
        "Data",
        "frames",
        "Spark",
        "SQL",
        "API",
        "processing",
        "data",
        "Scala",
        "UDFs",
        "Data",
        "framesSQLData",
        "sets",
        "RDDMap",
        "Reduce",
        "Spark",
        "Data",
        "Aggregation",
        "data",
        "OLTP",
        "system",
        "Sqoop",
        "Processing",
        "schema",
        "data",
        "Scala",
        "Spark",
        "Spark",
        "API",
        "Cloudera",
        "Hadoop",
        "YARN",
        "analytics",
        "data",
        "HDFS",
        "streaming",
        "pipeline",
        "Spark",
        "data",
        "Kafka",
        "HDFS",
        "weblog",
        "data",
        "HiveQL",
        "Oozie",
        "rest",
        "Hadoop",
        "stack",
        "cluster",
        "coordination",
        "services",
        "Zookeeper",
        "Environment",
        "Scala",
        "Spark",
        "Spark",
        "SQL",
        "Spark",
        "Streaming",
        "Azkaban",
        "Presto",
        "Hive",
        "Apache",
        "Crunch",
        "Elastic",
        "Search",
        "GIT",
        "Repository",
        "Amazon",
        "S3",
        "Amazon",
        "AWS",
        "Ec2EMR",
        "Spark",
        "cluster",
        "Hadoop",
        "Framework",
        "Sqoop",
        "DB2",
        "Data",
        "Engineering",
        "Walt",
        "Disney",
        "Glendale",
        "CA",
        "December",
        "August",
        "Responsibilities",
        "strategies",
        "web",
        "log",
        "data",
        "cluster",
        "importing",
        "web",
        "log",
        "data",
        "HDFS",
        "Hive",
        "Sqoop",
        "Design",
        "ELT",
        "data",
        "pipeline",
        "Spark",
        "App",
        "data",
        "Legacy",
        "system",
        "thirdparty",
        "API",
        "media",
        "sites",
        "custom",
        "mappers",
        "python",
        "script",
        "Hive",
        "UDFs",
        "UDAFs",
        "requirement",
        "Design",
        "DMA",
        "Disney",
        "Movies",
        "dashboard",
        "BI",
        "analyst",
        "team",
        "data",
        "analytics",
        "data",
        "Amazon",
        "s3Data",
        "Lake",
        "Spark",
        "cluster",
        "data",
        "Spark",
        "SQL",
        "top",
        "Spark",
        "engine",
        "Spark",
        "scripts",
        "Python",
        "shell",
        "commands",
        "requirement",
        "Writing",
        "Pig",
        "Hive",
        "scripts",
        "UDF",
        "MR",
        "Python",
        "ETL",
        "AWS",
        "Cloud",
        "Services",
        "file",
        "formats",
        "text",
        "avro",
        "parquet",
        "sequence",
        "files",
        "HiveQL",
        "Impala",
        "query",
        "response",
        "time",
        "Hive",
        "partitions",
        "buckets",
        "HQL",
        "job",
        "flow",
        "scheduler",
        "Hadoop",
        "jobs",
        "Zookeepers",
        "node",
        "failovers",
        "Performed",
        "Tableau",
        "type",
        "conversion",
        "functions",
        "data",
        "sources",
        "Environment",
        "LanguagesTechnologies",
        "Java",
        "JDK16",
        "Spark",
        "SQL",
        "Presto",
        "Hive",
        "Apache",
        "Crunch",
        "Elastic",
        "Search",
        "Spring",
        "boot",
        "Eclipse",
        "GIT",
        "Repository",
        "Amazon",
        "S3",
        "Amazon",
        "AWS",
        "Ec2EMR",
        "Spark",
        "cluster",
        "Hadoop",
        "Framework",
        "Sqoop",
        "Hadoop",
        "Developer",
        "WilliamsSonoma",
        "Inc",
        "San",
        "Francisco",
        "CA",
        "September",
        "October",
        "Responsibilities",
        "nodes",
        "Hadoop",
        "cluster",
        "Hadoop",
        "cluster",
        "job",
        "performance",
        "Cloudera",
        "manager",
        "loading",
        "data",
        "edge",
        "node",
        "HDFS",
        "shell",
        "scripting",
        "Created",
        "Map",
        "programs",
        "data",
        "xml",
        "json",
        "Avro",
        "data",
        "files",
        "sequence",
        "files",
        "log",
        "files",
        "Developed",
        "Spark",
        "scripts",
        "Python",
        "shell",
        "commands",
        "requirement",
        "Integrated",
        "Elastic",
        "Search",
        "facetedsearch",
        "amounts",
        "log",
        "data",
        "Apache",
        "Flume",
        "data",
        "HDFS",
        "analysis",
        "Design",
        "Develop",
        "Pig",
        "Latin",
        "scripts",
        "Pig",
        "command",
        "line",
        "transformations",
        "data",
        "joins",
        "custom",
        "processing",
        "Map",
        "outputs",
        "Spark",
        "Scala",
        "Spark",
        "SQL",
        "testing",
        "processing",
        "data",
        "procedures",
        "text",
        "analytics",
        "processing",
        "inmemory",
        "computing",
        "capabilities",
        "Apache",
        "Spark",
        "Scala",
        "Spark",
        "RDD",
        "transformations",
        "business",
        "analysis",
        "actions",
        "top",
        "transformations",
        "Jars",
        "MapReduce",
        "Pig",
        "Hive",
        "UDFs",
        "SQL",
        "queries",
        "left",
        "Tableau",
        "Desktop",
        "datasets",
        "Environment",
        "Hadoop",
        "Scala",
        "Map",
        "Reduce",
        "HDFS",
        "Spark",
        "Scala",
        "Kafka",
        "Apache",
        "SOLR",
        "Hive",
        "Cassandra",
        "Jenkins",
        "Pig",
        "UNIX",
        "Python",
        "MRUnit",
        "Git",
        "Hadoop",
        "Developer",
        "Veritas",
        "Technologies",
        "LLC",
        "Mountain",
        "View",
        "CA",
        "November",
        "August",
        "Responsibilities",
        "data",
        "solutions",
        "Hadoop",
        "Worked",
        "data",
        "reference",
        "data",
        "Pig",
        "scripting",
        "DataStax",
        "Enterprise",
        "Search",
        "Apache",
        "Solr",
        "operators",
        "data",
        "DAG",
        "streams",
        "data",
        "HDFS",
        "Configured",
        "Kafka",
        "cluster",
        "connectors",
        "ETL",
        "jobs",
        "SparkScala",
        "data",
        "Oracle",
        "hive",
        "tables",
        "applications",
        "Apache",
        "Spark",
        "Scala",
        "Developed",
        "Oozie",
        "workflow",
        "scheduling",
        "ETL",
        "process",
        "Spark",
        "Scala",
        "Spark",
        "SQL",
        "testing",
        "processing",
        "data",
        "Scala",
        "problems",
        "Micro",
        "Strategy",
        "reports",
        "dashboards",
        "analysis",
        "Developed",
        "Hive",
        "analysts",
        "scripts",
        "Scala",
        "Created",
        "Sqoop",
        "jobs",
        "load",
        "Hive",
        "External",
        "tables",
        "Flume",
        "Hive",
        "Pig",
        "Sqoop",
        "Oozie",
        "Hadoop",
        "cluster",
        "Continuous",
        "Integration",
        "environments",
        "SCRUM",
        "methodologies",
        "data",
        "Teradata",
        "HDFS",
        "Sqoop",
        "time",
        "data",
        "processing",
        "time",
        "Data",
        "Ingestion",
        "HBase",
        "Hive",
        "Storm",
        "Environment",
        "Hadoop",
        "HDFS",
        "Pig",
        "Hive",
        "Oozie",
        "HBase",
        "Kafka",
        "Apache",
        "SOLR",
        "MapReduce",
        "Apache",
        "SOLR",
        "Sqoop",
        "Storm",
        "Spark",
        "Scala",
        "LINUX",
        "Cloudera",
        "Maven",
        "Jenkins",
        "Java",
        "SQL",
        "Client",
        "Care",
        "Tampa",
        "Florida",
        "Industry",
        "HealthCare",
        "Mar13",
        "Oct14",
        "Role",
        "JavaHadoop",
        "Developer",
        "Responsibilities",
        "data",
        "DB2",
        "HDFS",
        "Sqoop",
        "Developed",
        "MapReduce",
        "jobs",
        "Java",
        "API",
        "Spring",
        "AOP",
        "transaction",
        "application",
        "Java",
        "batch",
        "programs",
        "Spring",
        "Batch",
        "Pig",
        "Pig",
        "Latin",
        "scripts",
        "documentation",
        "Cloudera",
        "Hadoop",
        "Clusters",
        "Hive",
        "queries",
        "Pig",
        "Scripts",
        "Developed",
        "Oozie",
        "MapReduce",
        "jobs",
        "Hive",
        "Queries",
        "loading",
        "data",
        "UNIX",
        "file",
        "system",
        "HDFS",
        "operators",
        "data",
        "DAG",
        "streams",
        "data",
        "HDFS",
        "Assisted",
        "data",
        "databases",
        "Sqoop",
        "Develop",
        "monitoring",
        "performance",
        "metrics",
        "Hadoop",
        "monitoring",
        "Hadoop",
        "cluster",
        "Cloudera",
        "Manager",
        "Environment",
        "Hadoop",
        "HDFS",
        "Hive",
        "Flume",
        "Sqoop",
        "HBase",
        "PIG",
        "Eclipse",
        "Spark",
        "SQL",
        "Ubuntu",
        "Zookeeper",
        "Maven",
        "Jenkins",
        "Java",
        "JDK",
        "Oracle10",
        "g",
        "Java",
        "Developer",
        "Hexacorp",
        "Technical",
        "services",
        "NJ",
        "October",
        "February",
        "Responsibilities",
        "team",
        "members",
        "business",
        "users",
        "requirements",
        "end",
        "components",
        "HTML",
        "JavaScript",
        "jQuery",
        "Back",
        "End",
        "components",
        "Java",
        "spring",
        "Hibernate",
        "Services",
        "components",
        "Restful",
        "SOAP",
        "web",
        "services",
        "Rules",
        "components",
        "JBoss",
        "Drools",
        "analysis",
        "design",
        "implementation",
        "phases",
        "software",
        "development",
        "lifecycle",
        "SDLC",
        "Implementation",
        "spring",
        "core",
        "J2EE",
        "patterns",
        "MVC",
        "Dependency",
        "Injection",
        "DI",
        "Inversion",
        "Control",
        "IOC",
        "REST",
        "Web",
        "Services",
        "Jersey",
        "API",
        "customer",
        "requests",
        "test",
        "cases",
        "J",
        "Unit",
        "Log4j",
        "framework",
        "HQL",
        "Criteria",
        "API",
        "data",
        "elements",
        "database",
        "user",
        "interface",
        "HTML",
        "Spring",
        "Tags",
        "JavaScript",
        "J",
        "Query",
        "CSS",
        "application",
        "Eclipse",
        "IDE",
        "Agile",
        "Environment",
        "Eclipse",
        "IDE",
        "improvement",
        "environment",
        "Spring",
        "segments",
        "Web",
        "Logic",
        "Environment",
        "Java",
        "J2EE",
        "JDBC",
        "EJB",
        "UML",
        "Swing",
        "HTML",
        "JavaScript",
        "CSS",
        "J",
        "Query",
        "Spring",
        "JNDI",
        "Hibernate",
        "Java",
        "Mail",
        "Web",
        "Services",
        "REST",
        "Oracle",
        "g",
        "J",
        "Unit",
        "Log4j",
        "Eclipse",
        "Web",
        "logic",
        "Java",
        "Developer",
        "Choice",
        "Software",
        "Limited",
        "Hyderabad",
        "Telangana",
        "August",
        "September",
        "Responsibilities",
        "stages",
        "Enhancements",
        "Application",
        "analysis",
        "development",
        "testing",
        "analysis",
        "design",
        "application",
        "Use",
        "Cases",
        "Class",
        "Sequence",
        "Diagrams",
        "Developed",
        "user",
        "interfaces",
        "struts",
        "framework",
        "Developed",
        "JavaJ2EE",
        "code",
        "web",
        "application",
        "Client",
        "Side",
        "Validations",
        "JavaScript",
        "integration",
        "Struts",
        "actions",
        "framework",
        "development",
        "User",
        "Interfaces",
        "HTML",
        "JSP",
        "CSS",
        "JavaScript",
        "Tested",
        "Java",
        "JSP",
        "EJB",
        "components",
        "Eclipse",
        "Environments",
        "Java",
        "JDK",
        "J2EE",
        "Servlets",
        "Struts",
        "JSP",
        "HTML",
        "CSS",
        "JavaScript",
        "EJB",
        "Eclipse",
        "WebLogic",
        "Windows",
        "SOAP",
        "Restful",
        "Skills",
        "CASSANDRA",
        "HDFS",
        "IMPALA",
        "MAHOUT",
        "OOZIE",
        "Additional",
        "Information",
        "Technical",
        "Skills",
        "Big",
        "Data",
        "Eco",
        "systems",
        "HDFS",
        "Map",
        "Reduce",
        "Hive",
        "YARN",
        "Pig",
        "Sqoop",
        "Kafka",
        "Storm",
        "Flume",
        "Oozie",
        "ZooKeeper",
        "Apache",
        "Spark",
        "Apache",
        "Tez",
        "Impala",
        "Nifi",
        "Apache",
        "Solr",
        "Rabbit",
        "MQ",
        "Scala",
        "SQL",
        "Databases",
        "Hbase",
        "Cassandra",
        "mongoDB",
        "Programming",
        "Languages",
        "C",
        "C",
        "Java",
        "J2EE",
        "PLSQL",
        "Pig",
        "Latin",
        "Scala",
        "Python",
        "JavaJ2EE",
        "Technologies",
        "Applets",
        "Swing",
        "JDBC",
        "JNDI",
        "JSON",
        "JSTL",
        "RMI",
        "JMS",
        "Java",
        "Script",
        "JSP",
        "Servlets",
        "EJB",
        "JSF",
        "JQuery",
        "Frameworks",
        "MVC",
        "Struts",
        "Spring",
        "Hibernate",
        "Version",
        "control",
        "SVN",
        "CVS",
        "Business",
        "Intelligence",
        "Tools",
        "Tableau",
        "QlikView",
        "Pentaho",
        "IBM",
        "Cognos",
        "intelligence",
        "Oracle",
        "9i10g11",
        "g",
        "DB2",
        "SQL",
        "Server",
        "MySQL",
        "Teradata",
        "Tools",
        "IDE",
        "Eclipse",
        "Net",
        "Beans",
        "Toad",
        "Maven",
        "ANT",
        "Hudson",
        "Sonar",
        "JDeveloper",
        "Assent",
        "PMD",
        "DB",
        "Visualizer",
        "IntelliJ",
        "Cloud",
        "Technologies",
        "Amazon",
        "Web",
        "Services",
        "AWS",
        "CDH3",
        "CDH4",
        "CDH5",
        "HortonWorks",
        "Mahout",
        "Microsoft",
        "Azure",
        "Insight",
        "Amazon",
        "Redshift"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T20:35:45.708819",
    "resume_data": "Sr Data Engineer Sr Data Engineer Sr Data Engineer ATT Plano TX Proactive IT developer with 9 years of working experience in JavaJ2EE Technology and development design of various scalable systems using Hadoop Technologies on various environments Experience in installation configuration supporting and managing Hadoop Clusters using Horton works and Cloudera CDH3 CDH4 distributions on Amazon web services AWS Extraordinary Understanding of Hadoop building and Hands on involvement with Hadoop segments such as Job Tracker Task Tracker Name Node Data Node and HDFS Framework Extensive experience in analyzing data using Hadoop Ecosystems including HDFS Hive PIG Sqoop Flume MapReduce Spark Kafka HBase Oozie Solr and Zookeeper Extensive knowledge on NoSQL databases like HBase Cassandra and Mongo DB Configured Zookeeper Cassandra and Flume to the existing Hadoop cluster Expertise in writing Hadoop Jobs for analyzing data using Hive QL Queries Pig Latin Data flow language and custom MapReduce programs in Java Experience in converting Hive queries into Spark transformations using Spark RDDs and Scala Hands on Experience in troubleshooting errors in HBase Shell Pig Hive and MapReduce Handson experience in provisioning and managing multitenant Cassandra cluster on public cloud environment Amazon Web Services AWS EC2 Open Stack Experience in NoSQL ColumnOriented Databases like HBase Cassandra and its Integration with Hadoop cluster Experience in maintaining the big data platform using open source technologies such as Spark and Elastic Search Planned and created answer for constant information ingestion utilizing Kafka Storm Spark spilling and different NoSQL databases Developed Scala scripts UDFs using both Data framesSQL and RDDMapReduce in Spark for Data Aggregation queries and writing data back into RDBMS through Sqoop Experience in understanding the security requirements for Hadoop and integrate with Kerberos authentication and authorization infrastructure Good hands on experience in creating the RDDs DFs for the required input data and performed the data transformations using Spark Scala Knowledge in developing a Nifi flow prototype for data ingestion in HDFS Extensive experience working in Oracle DB2 SQL Server PLSQL and My SQL database and Java Core concepts like OOPS Multithreading Collections and IO Experience in Service Oriented Architecture using Web Services like SOAP Restful Work Experience Sr Data Engineer ATT Plano TX January 2018 to Present Sr Bigdata Developer T Mobile Bellevue WA May 2018 to January 2019 Responsibilities Developed various spark applications using Scala to perform various enrichment of these click stream data merged with user profile data Utilized SparkSQL to event enrichment and used SparkSQL to prepare various levels of user behavior summaries Worked on SQS Queue receiver using Spark Streaming context to consume the data from extended queue and integrated with ETL Functions Real time streaming the data using Spark with SQS Responsible for handling Streaming data from web server console logs Optimize the Hive tables using optimization techniques such as partitions and bucketing to provide better performance with HiveQL queries Worked on migrating data from traditional RDBMS to HDFS Used Scala to convert HiveSQL queries into RDD transformations in Apache Spark Written Programs in Spark using Scala for Data quality check Used Spark Streaming to divide streaming data into batches as an input to Spark engine for batch processing Implemented the use of Amazon EMR for Big Data processing among a Hadoop Cluster of virtual servers on Amazon related EC2 and S3 Optimized Hive QL pig scripts by using execution engine like Tez Spark Environment Hadoop HDFS Hive Spark AWS EC2 S3 Kafka Yarn Shell Scripting Scala Pig Oozie Java Agile methods Linux MySQL Elastic Search Kibana Teradata Hadoop Developer Visa Austin TX August 2017 to May 2018 Developed Spark Applications by using Spark Java and Implemented Apache Spark data processing project to handle data from various RDBMS and Streaming sources Handled importing of data from various data sources performed data control checks using Spark and loaded data into HDFS Involved in converting HiveSQL queries into Spark transformations using Spark RDD Scala Used Spark SQL to Load JSON data and create SchemaRDD and loaded it into Hive Tables and handled structured data using Spark SQ Imported data from AWS S3 into Spark RDD Performed transformations and actions on RDDs Used Spark and Spark SQL to read the parquet data and create the tables in hive using the Scala API Implemented Spark using Scala and utilizing Data frames and Spark SQL API for faster processing of data Developed Scala scripts UDFs using both Data framesSQLData sets and RDDMap Reduce in Spark 16 for Data Aggregation queries and writing data back into OLTP system through Sqoop Processing the schema oriented and nonschemaoriented data using Scala and Spark Used Spark API over Cloudera Hadoop YARN to perform analytics on data in HDFS Worked on streaming pipeline that uses Spark to read data from Kafka transform it and write it to HDFS Analyzed the weblog data using the HiveQL integrated Oozie with the rest of the Hadoop stack Utilized cluster coordination services through Zookeeper Environment Scala Spark Spark SQL Spark Streaming Azkaban Presto Hive Apache Crunch Elastic Search GIT Repository Amazon S3 Amazon AWS Ec2EMR Spark cluster Hadoop Framework Sqoop DB2 Data Engineering Walt Disney Glendale CA December 2016 to August 2017 Responsibilities Developed optimal strategies for distributing the web log data over the cluster importing and exporting the stored web log data into HDFS and Hive using Sqoop Design and develop ELT data pipeline using Spark App to fetch data from Legacy system and thirdparty API social media sites Developed custom mappers in python script and Hive UDFs and UDAFs based on the given requirement Design and develop DMA Disney Movies anywhere dashboard for BI analyst team Perform data analytics and load data to Amazon s3Data Lake Spark cluster Involved in querying data using Spark SQL on top of Spark engine Developed Spark scripts by using Python shell commands as per the requirement Writing Pig and Hive scripts with UDF in MR and Python to perform ETL on AWS Cloud Services Worked with file formats text avro parquet and sequence files Involved in migrating HiveQL into Impala to minimize query response time Created Hive tables dynamic partitions buckets for sampling and working on them using HQL Defined job flow using Azkaban scheduler to automate the Hadoop jobs and installed Zookeepers for automatic node failovers Performed Tableau type conversion functions when connected to relational data sources Environment LanguagesTechnologies Java JDK16 and higher Azkaban Spark SQL Presto Hive Apache Crunch Elastic Search Spring boot Eclipse GIT Repository Amazon S3 Amazon AWS Ec2EMR Spark cluster Hadoop Framework Sqoop Hadoop Developer WilliamsSonoma Inc San Francisco CA September 2015 to October 2016 Responsibilities Involved in managing nodes on Hadoop cluster and monitor Hadoop cluster job performance using Cloudera manager Involved in loading data from edge node to HDFS using shell scripting Created Map Reduce programs to handle semiunstructured data like xml json Avro data files and sequence files for log files Developed Spark scripts by using Python shell commands as per the requirement Integrated Elastic Search and implemented dynamic facetedsearch Involved in collecting and aggregating large amounts of log data using Apache Flume and staging data in HDFS for further analysis Design and Develop Pig Latin scripts and Pig command line transformations for data joins and custom processing of Map reduce outputs Implemented Spark using Scala and Spark SQL for faster testing and processing of data Implementing advanced procedures like text analytics and processing using the inmemory computing capabilities like Apache Spark written in Scala Implemented Spark RDD transformations to map business analysis and apply actions on top of transformations Used maven to build and deploy the Jars for MapReduce Pig and Hive UDFs Reviewed basic SQL queries and edited inner left and right joins in Tableau Desktop by connecting livedynamic and static datasets Environment Hadoop Scala Map Reduce HDFS Spark Scala Kafka AWS Apache SOLR Hive Cassandra maven Jenkins Pig UNIX Python MRUnit Git Hadoop Developer Veritas Technologies LLC Mountain View CA November 2014 to August 2015 Responsibilities Responsible for building scalable distributed data solutions using Hadoop Worked in joining raw data with the reference data using Pig scripting Implemented DataStax Enterprise Search with Apache Solr Created java operators to process data using DAG streams and load data to HDFS Configured Designed implemented and monitored Kafka cluster and connectors Developed ETL jobs using SparkScala to migrate data from Oracle to new hive tables Developed and Deployed applications using Apache Spark Scala Developed Oozie workflow for scheduling and orchestrating the ETL process Implemented Spark using Scala and Spark SQL for faster testing and processing of data Helped in troubleshooting Scala problems while working with Micro Strategy to produce illustrative reports and dashboards along with adhoc analysis Developed Hive queries for the analysts and I have written scripts using Scala Created and worked Sqoop jobs with incremental load to populate Hive External tables Installed and configured Flume Hive Pig Sqoop and Oozie on the Hadoop cluster Continuous Integration environments in SCRUM and Agile methodologies Extracted the data from Teradata into HDFS using the Sqoop Managed real time data processing and real time Data Ingestion in HBase and Hive using Storm Environment Hadoop HDFS Pig Hive Oozie HBase Kafka Apache SOLR MapReduce Apache SOLR Sqoop Storm Spark Scala LINUX Cloudera Maven Jenkins Java SQL Client Well Care Tampa Florida Industry HealthCare Mar13 Oct14 Role JavaHadoop Developer Responsibilities Exported data from DB2 to HDFS using Sqoop and Developed MapReduce jobs using Java API Used Spring AOP to implement Distributed declarative transaction throughout the application Designed and developed Java batch programs in Spring Batch Installed and configured Pig and wrote Pig Latin scripts Created and maintained Technical documentation for launching Cloudera Hadoop Clusters and for executing Hive queries and Pig Scripts Developed workflowusing Oozie for running MapReduce jobs and Hive Queries Involved in loading data from UNIX file system to HDFS Created java operators to process data using DAG streams and load data to HDFS Assisted in exporting analyzed data to relational databases using Sqoop Involved in Develop monitoring and performance metrics for Hadoop clusters Continuous monitoring and managing the Hadoop cluster through Cloudera Manager Environment Hadoop HDFS Hive Flume Sqoop HBase PIG Eclipse Spark My SQL and Ubuntu Zookeeper Maven Jenkins Java JDK 16 Oracle10g Java Developer Hexacorp Technical services NJ October 2011 to February 2013 Responsibilities Effectively interacted with team members and business users for requirements gathering Coded front end components using HTML JavaScript and jQuery Back End components using Java spring Hibernate Services Oriented components using Restful and SOAP based web services and Rules based components using JBoss Drools Involved in analysis design and implementation phases of the software development lifecycle SDLC Implementation of spring core J2EE patterns like MVC Dependency Injection DI and Inversion of Control IOC Implemented REST Web Services with Jersey API to deal with customer requests Developed test cases using J Unit and used Log4j as the logging framework Worked with HQL and Criteria API from retrieving the data elements from database Developed user interface using HTML Spring Tags JavaScript J Query and CSS Developed the application using Eclipse IDE and worked under Agile Environment Utilized Eclipse IDE as improvement environment to plan create and convey Spring segments on Web Logic Environment Java J2EE JDBC EJB UML Swing HTML JavaScript CSS J Query Spring 30 JNDI Hibernate 30 Java Mail Web Services REST Oracle 10g J Unit Log4j Eclipse Web logic 103 Java Developer Choice Software Limited Hyderabad Telangana August 2008 to September 2011 Responsibilities Involved in various stages of Enhancements in the Application by doing the required analysis development and testing For analysis and design of application created Use Cases Class and Sequence Diagrams Developed webbased user interfaces using struts framework Developed and maintained JavaJ2EE code required for the web application Handled Client Side Validations used JavaScript and Involved in integration of various Struts actions in the framework Involved in the development of the User Interfaces using HTML JSP CSS and JavaScript Developed Tested and Debugged the Java JSP and EJB components using Eclipse Environments Java JDK 15 J2EE Servlets Struts JSP HTML CSS JavaScript EJB Eclipse WebLogic 81 Windows SOAP Restful Skills CASSANDRA HDFS IMPALA MAHOUT OOZIE Additional Information Technical Skills Big Data Eco systems HDFS Map Reduce Hive YARN Pig Sqoop Kafka Storm Flume Oozie and ZooKeeper Apache Spark Apache Tez Impala Nifi Apache Solr Rabbit MQ Scala No SQL Databases Hbase Cassandra mongoDB Programming Languages C C Java J2EE PLSQL Pig Latin Scala Python JavaJ2EE Technologies Applets Swing JDBC JNDI JSON JSTL RMI JMS Java Script JSP Servlets EJB JSF JQuery AngularJS Frameworks MVC Struts Spring Hibernate Version control SVN CVS Business Intelligence Tools Tableau QlikView Pentaho IBM Cognos intelligence Databases Oracle 9i10g11g DB2 SQL Server MySQL Teradata Tools and IDE Eclipse Net Beans Toad Maven ANT Hudson Sonar JDeveloper Assent PMD DB Visualizer IntelliJ Cloud Technologies Amazon Web Services AWS CDH3 CDH4 CDH5 HortonWorks Mahout Microsoft Azure Insight Amazon Redshift",
    "unique_id": "2e1ff731-7578-4968-8e13-a044cc0f686d"
}