{
    "clean_data": "Hadoop Developer Hadoop span lDeveloperspan Hadoop Developer Sutter Health Sacramento CA 7 years of experience in all phases of Software Application requirement analysis design development and maintenance of HadoopBig Data application like SPARK KAFKA EMR Hive Sqoop and applications using java and scala to tailored with industry needs Hands on experience with Spark Core Spark SQL Spark Streaming Used SparkSQL to perform transformations and actions on data residing in Hive Used Kafka Spark Streaming for realtime processing Experience with migrating data to and from RDBMS and unstructured sources into HDFS using Sqoop Good Knowledge in Apache Spark data processing to handle data from RDBMS and streaming sources with Spark streaming Experience in Data Warehousing and ETL processes and Strong database SQL ETL and data analysis skills Good understandingknowledge of Hadoop architecture and various components such as HDFS Job Tracker Task Tracker Name Node Data Node and Map Reduce programming paradigm Have good skills in writing SPARK Jobs in Scala for processing large sets of structured semistructured and store them in HDFS Good Knowledge in Spark SQL queries to load tables into HDFS to run select queries on top Experience in writing Hive Queries for processing and analyzing large volumes of data Experience in importing and exporting data using Sqoop from Relational Database Systems to HDFS and viceversa Implemented several optimization mechanisms like Combiners Distributed Cache Data Compression and Custom Practitioner to speed up the jobs Authorized to work in the US for any employer Work Experience Hadoop Developer Sutter Health Sacramento CA September 2018 to Present Sutter Health is a notforprofit health system in Northern California headquartered in Sacramento It includes doctors hospitals and other health care services in more than 100 Northern California cities and towns Major service lines of Sutter Healthaffiliated hospitals include cardiac care womens and childrens services cancer care orthopedics and advanced patient safety technology This project seek to harness these massive complex buckets of data to obtain more focused knowledge and insights into the world of healthcare Big Data attempts to make more sense of this information overload and provide better insights from the expanding volumes and sources of data Responsibilities Interacting with multiple teams understanding their business requirements for designing flexible and common component Validating the source file for Data Integrity and Data Quality by reading header and trailer information and column validations Implemented Spark SQL to access hive tables into spark for faster processing of data Used Hive to do transformations joins filter and some preaggregations before storing the data Validating and visualizing the data in Tableau Using hive extensively to create a views for the feature data Working with platform and Hadoop teams closely for the needs of the team Using Kafka for Data ingestion for different data sets Experienced in importing and exporting data into HDFS and assisted in exporting analyzed data to RDBMS using SQOOP Developed sqoop jobs to import the data from RDBMS and file servers into Hadoop Environment Hadoop Cloudera Amazon AWS HDFS Hive Impala Spark Kafka s3 Sqoop SparkHadoop Developer Nike Portland OR November 2017 to August 2018 Nike Inc is an American multinational corporation that is engaged in the design development manufacturing and worldwide marketing and sales of footwear apparel equipment accessories and services The company is headquartered near Beaverton Oregon in the Portland metropolitan area Nike Inc project provides upsell and crosssell recommendations for customers enabling the increase in online purchases by recommending relevant products and promotions in real time Products recommend based on what other similar customers have bought providing upsell crosssell or next best offer opportunities Responsibilities Interacting with multiple teams understanding their business requirements for designing flexible and common component Validating the source file for Data Integrity and Data Quality by reading header and trailer information and column validations Used Spark SQL for creating data frames and performed transformations on data frames like adding schema manually casting joining data frames before storing them Implemented Spark SQL to access hive tables into spark for faster processing of data Worked on Spark streaming using Apache Kafka for real time data processing Experience in creating Kafka producer and Kafka consumer for Spark streaming Used Hive to do transformations joins filter and some preaggregations before storing the data into HDFS Worked on three layers for storing data such as raw layer intermediate layer and publish layer Creating external hive tables to store and queries the data which is loaded Optimizations techniques include partitioning bucketing Using Avro file format compressed with Snappy in intermediate tables for faster processing of data Used parquet file format for published tables and created views on the tables Created sentry policy files to provide access to the required databases and tables to view from impala to the business users in the dev uat and prod environment Automated the jobs with Oozie and scheduled them with Autosys Experience in AWS to spin up the EMR cluster to process the huge data which is stored in S3 and push it to HDFS Participated in evaluation and selection of new technologies to support system efficiency Participated in development and execution of system and disaster recovery processes Environment Hadoop Cloudera Amazon AWS HDFS Hive Impala Spark Kafka s3 Sqoop Java Scala Eclipse Tableau and Maven SBT SparkHadoop Developer Capital one Richmond VA September 2016 to October 2017 Retail Enterprise Credit Risk application calculates Banks retail data such as credit cards auto student and home loans for risk domains including Enterprise Capital Management Data comes from different System of records mainly from Teradata This data will undergo several cleansing and value added processing and then finally views will be created on this data as Hadoop warehouse This data will be consumed by downstream like ECM for analyzing and generating reports Responsibilities Interacting with multiple teams understanding their business requirements for designing flexible and common component Validating the source file for Data Integrity and Data Quality by reading header and trailer information and column validations Used Spark SQL for creating data frames and performed transformations on data frames like adding schema manually casting joining data frames before storing them Implemented Spark SQL to access hive tables into spark for faster processing of data Worked on Spark streaming using Apache Kafka for real time data processing Experience in creating Kafka producer and Kafka consumer for Spark streaming Used Hive to do transformations joins filter and some preaggregations before storing the data into HDFS Used Sqoop for importing and exporting data from Netezza Teradata into HDFS and Hive Worked on three layers for storing data such as raw layer intermediate layer and publish layer Creating external hive tables to store and queries the data which is loaded Optimizations techniques include partitioning bucketing Using Avro file format compressed with Snappy in intermediate tables for faster processing of data Used parquet file format for published tables and created views on the tables Created sentry policy files to provide access to the required databases and tables to view from impala to the business users in the dev uat and prod environment Automated the jobs with Oozie and scheduled them with Autosys Experience in AWS to spin up the EMR cluster to process the huge data which is stored in S3 and push it to HDFS Participated in evaluation and selection of new technologies to support system efficiency Participated in development and execution of system and disaster recovery processes Environment Hadoop Cloudera Amazon AWS HDFS Hive Impala Spark Kafka s3 Sqoop Java Scala Eclipse Tableau and Maven SBT Java Developer Keane India LTD Bengaluru Karnataka April 2014 to May 2016 I worked as an associate java developer on ATS Asset Tracking Software which manages and tracks the assets of a company It manages separate login functionality for asset registry and transactions including orders invoices and returns It also allows the vendor to closely monitor the inventory and sales Responsibilities Involved in the complete SDLC software development life cycle of the application from requirement gathering and analysis to testing and maintenance Developed the modules based on MVC Architecture Developed UI using JavaScript JSP HTML and CSS for interactive cross browser functionality and complex user interface Created business logic using servlets and session beans and deployed them on Apache Tomcat server Created complex SQL Queries PLSQL Stored procedures and functions for back end Prepared the functional design and test case specifications Performed unit testing system testing and integration testing Developed unit test cases Used JUnit for unit testing of the application Provided Technical support for production environments resolving the issues analyzing the defects providing and implementing the solution defects Resolved more priority defects as per the schedule Environment Java JSP Servlets Apache Tomcat Oracle SQL Java Developer Senmex Hyderabad Telangana February 2011 to March 2014 Developed with a flexible design which provides the platform to offer online integrated financial services to the bank customers Using this web based application customers can conduct activities like online retail banking Secure messaging Credit Card Payments Shopping Corporate banking and even payment to third party individual sand institutes Responsibilities Involved in design development and analysis documents in sharing with Clients Developed web pages using Struts framework JSP XML JavaScript Hibernate springs Html DHTML and CSS configure struts application use tag library Developed Application using Spring and Hibernate Spring batch Web Services like Soap and restful Web services Used Spring Framework at Business Tier and also springs Bean Factory for initializing services Used AJAX JavaScript to create interactive user interface Implemented client side validations using JavaScript server side validations Developed Single Page application using angular JS backbone JS Implemented Hibernate to persist the data into Database and wrote HQL based queries to implement CRUD operations on the data Developed an API to write XML documents from a database Utilized XML and XSL Transformation for dynamic webcontent and database connectivity Database modeling administration and development using SQL and PLSQL in Oracle 11g Coded different deployment descriptors using XML Generated Jar files are deployed on Apache Tomcat Server Involved in the development of presentation layer and GUI framework in JSP ClientSide validations were done using JavaScript Involved in configuring and deploying the application using WebSphere Involved in code reviews and mentored the team in resolving issues Undertook the Integration and testing of the various parts of the application Developed automated Build files using ANT Used Subversion for version control and log4j for logging errors Code Walkthrough Test cases and Test Plans Environment HTML5 JSP Servlets JDBC JavaScript Json Spring SQL Oracle 11g Tomcat Eclipse IDE XML XSL ANT Tomcat 5 Education Bachelor of Technology in Electronics and Communication Engineering in Electronics and Communication Engineering Anna University Chennai Tamil Nadu May 2009 Skills Hdfs Impala Sqoop Kafka Hadoop Additional Information Technical Skills Big DataHadoop HDFS Hive Sqoop Impala Kafka Map Reduce Cloudera Amazon EMR Spark Components Spark Core Spark SQL Spark Streaming Programming Languages SQL Scala and Java Databases MySQL HiveQL RDBMS Cloud Amazon EMR EC2 S3 Operating Systems Windows Unix Red Hat Linux",
    "entities": [
        "Implemented Spark",
        "Hadoop Environment Hadoop",
        "GUI",
        "SPARK",
        "ATS Asset Tracking Software",
        "ECM",
        "HDFS",
        "Beaverton",
        "Hadoop Developer Hadoop",
        "Oregon",
        "Bean Factory",
        "SQL ETL",
        "Amazon AWS HDFS Hive",
        "Developed Single Page",
        "Sacramento",
        "Hadoop",
        "SQOOP Developed",
        "XML",
        "Telangana",
        "Retail Enterprise Credit Risk",
        "HDFS Used Sqoop",
        "Hadoop Additional Information Technical Skills Big DataHadoop",
        "JUnit",
        "Apache Spark",
        "IDE XML",
        "SparkSQL",
        "Developed",
        "SPARK Jobs",
        "Node Data",
        "Provided Technical",
        "Responsibilities Involved",
        "Clients Developed",
        "JS Implemented Hibernate",
        "S3 Operating Systems Windows Unix Red Hat",
        "Soap",
        "JavaScript Involved",
        "Enterprise Capital Management Data",
        "HadoopBig Data",
        "JSP",
        "Hive Queries",
        "Autosys",
        "Spark",
        "Present Sutter Health",
        "API",
        "US",
        "Database",
        "Sqoop",
        "ANT Used Subversion",
        "Sutter Healthaffiliated",
        "Created",
        "Spark Core Spark",
        "AWS",
        "Coded",
        "Software Application",
        "HDFS Job Tracker Task Tracker",
        "java",
        "Oozie",
        "SQL Queries PLSQL Stored",
        "SQL",
        "Keane India LTD",
        "Work Experience Hadoop Developer Sutter Health",
        "Portland",
        "Relational Database Systems",
        "Nike Inc",
        "Big Data",
        "Maven SBT SparkHadoop Developer Capital",
        "Developer Senmex Hyderabad",
        "Skills Hdfs",
        "ETL",
        "CRUD",
        "WebSphere Involved",
        "Maven",
        "Performed",
        "Data Integrity and Data Quality",
        "Undertook the Integration",
        "Spark SQL",
        "Developed Application",
        "Impala",
        "JavaScript",
        "Communication Engineering in Electronics and",
        "Combiners Distributed Cache Data Compression",
        "MVC Architecture Developed UI",
        "CSS",
        "Tomcat",
        "Responsibilities Interacting",
        "RDBMS",
        "Data Warehousing",
        "Tableau"
    ],
    "experience": "Experience with migrating data to and from RDBMS and unstructured sources into HDFS using Sqoop Good Knowledge in Apache Spark data processing to handle data from RDBMS and streaming sources with Spark streaming Experience in Data Warehousing and ETL processes and Strong database SQL ETL and data analysis skills Good understandingknowledge of Hadoop architecture and various components such as HDFS Job Tracker Task Tracker Name Node Data Node and Map Reduce programming paradigm Have good skills in writing SPARK Jobs in Scala for processing large sets of structured semistructured and store them in HDFS Good Knowledge in Spark SQL queries to load tables into HDFS to run select queries on top Experience in writing Hive Queries for processing and analyzing large volumes of data Experience in importing and exporting data using Sqoop from Relational Database Systems to HDFS and viceversa Implemented several optimization mechanisms like Combiners Distributed Cache Data Compression and Custom Practitioner to speed up the jobs Authorized to work in the US for any employer Work Experience Hadoop Developer Sutter Health Sacramento CA September 2018 to Present Sutter Health is a notforprofit health system in Northern California headquartered in Sacramento It includes doctors hospitals and other health care services in more than 100 Northern California cities and towns Major service lines of Sutter Healthaffiliated hospitals include cardiac care womens and childrens services cancer care orthopedics and advanced patient safety technology This project seek to harness these massive complex buckets of data to obtain more focused knowledge and insights into the world of healthcare Big Data attempts to make more sense of this information overload and provide better insights from the expanding volumes and sources of data Responsibilities Interacting with multiple teams understanding their business requirements for designing flexible and common component Validating the source file for Data Integrity and Data Quality by reading header and trailer information and column validations Implemented Spark SQL to access hive tables into spark for faster processing of data Used Hive to do transformations joins filter and some preaggregations before storing the data Validating and visualizing the data in Tableau Using hive extensively to create a views for the feature data Working with platform and Hadoop teams closely for the needs of the team Using Kafka for Data ingestion for different data sets Experienced in importing and exporting data into HDFS and assisted in exporting analyzed data to RDBMS using SQOOP Developed sqoop jobs to import the data from RDBMS and file servers into Hadoop Environment Hadoop Cloudera Amazon AWS HDFS Hive Impala Spark Kafka s3 Sqoop SparkHadoop Developer Nike Portland OR November 2017 to August 2018 Nike Inc is an American multinational corporation that is engaged in the design development manufacturing and worldwide marketing and sales of footwear apparel equipment accessories and services The company is headquartered near Beaverton Oregon in the Portland metropolitan area Nike Inc project provides upsell and crosssell recommendations for customers enabling the increase in online purchases by recommending relevant products and promotions in real time Products recommend based on what other similar customers have bought providing upsell crosssell or next best offer opportunities Responsibilities Interacting with multiple teams understanding their business requirements for designing flexible and common component Validating the source file for Data Integrity and Data Quality by reading header and trailer information and column validations Used Spark SQL for creating data frames and performed transformations on data frames like adding schema manually casting joining data frames before storing them Implemented Spark SQL to access hive tables into spark for faster processing of data Worked on Spark streaming using Apache Kafka for real time data processing Experience in creating Kafka producer and Kafka consumer for Spark streaming Used Hive to do transformations joins filter and some preaggregations before storing the data into HDFS Worked on three layers for storing data such as raw layer intermediate layer and publish layer Creating external hive tables to store and queries the data which is loaded Optimizations techniques include partitioning bucketing Using Avro file format compressed with Snappy in intermediate tables for faster processing of data Used parquet file format for published tables and created views on the tables Created sentry policy files to provide access to the required databases and tables to view from impala to the business users in the dev uat and prod environment Automated the jobs with Oozie and scheduled them with Autosys Experience in AWS to spin up the EMR cluster to process the huge data which is stored in S3 and push it to HDFS Participated in evaluation and selection of new technologies to support system efficiency Participated in development and execution of system and disaster recovery processes Environment Hadoop Cloudera Amazon AWS HDFS Hive Impala Spark Kafka s3 Sqoop Java Scala Eclipse Tableau and Maven SBT SparkHadoop Developer Capital one Richmond VA September 2016 to October 2017 Retail Enterprise Credit Risk application calculates Banks retail data such as credit cards auto student and home loans for risk domains including Enterprise Capital Management Data comes from different System of records mainly from Teradata This data will undergo several cleansing and value added processing and then finally views will be created on this data as Hadoop warehouse This data will be consumed by downstream like ECM for analyzing and generating reports Responsibilities Interacting with multiple teams understanding their business requirements for designing flexible and common component Validating the source file for Data Integrity and Data Quality by reading header and trailer information and column validations Used Spark SQL for creating data frames and performed transformations on data frames like adding schema manually casting joining data frames before storing them Implemented Spark SQL to access hive tables into spark for faster processing of data Worked on Spark streaming using Apache Kafka for real time data processing Experience in creating Kafka producer and Kafka consumer for Spark streaming Used Hive to do transformations joins filter and some preaggregations before storing the data into HDFS Used Sqoop for importing and exporting data from Netezza Teradata into HDFS and Hive Worked on three layers for storing data such as raw layer intermediate layer and publish layer Creating external hive tables to store and queries the data which is loaded Optimizations techniques include partitioning bucketing Using Avro file format compressed with Snappy in intermediate tables for faster processing of data Used parquet file format for published tables and created views on the tables Created sentry policy files to provide access to the required databases and tables to view from impala to the business users in the dev uat and prod environment Automated the jobs with Oozie and scheduled them with Autosys Experience in AWS to spin up the EMR cluster to process the huge data which is stored in S3 and push it to HDFS Participated in evaluation and selection of new technologies to support system efficiency Participated in development and execution of system and disaster recovery processes Environment Hadoop Cloudera Amazon AWS HDFS Hive Impala Spark Kafka s3 Sqoop Java Scala Eclipse Tableau and Maven SBT Java Developer Keane India LTD Bengaluru Karnataka April 2014 to May 2016 I worked as an associate java developer on ATS Asset Tracking Software which manages and tracks the assets of a company It manages separate login functionality for asset registry and transactions including orders invoices and returns It also allows the vendor to closely monitor the inventory and sales Responsibilities Involved in the complete SDLC software development life cycle of the application from requirement gathering and analysis to testing and maintenance Developed the modules based on MVC Architecture Developed UI using JavaScript JSP HTML and CSS for interactive cross browser functionality and complex user interface Created business logic using servlets and session beans and deployed them on Apache Tomcat server Created complex SQL Queries PLSQL Stored procedures and functions for back end Prepared the functional design and test case specifications Performed unit testing system testing and integration testing Developed unit test cases Used JUnit for unit testing of the application Provided Technical support for production environments resolving the issues analyzing the defects providing and implementing the solution defects Resolved more priority defects as per the schedule Environment Java JSP Servlets Apache Tomcat Oracle SQL Java Developer Senmex Hyderabad Telangana February 2011 to March 2014 Developed with a flexible design which provides the platform to offer online integrated financial services to the bank customers Using this web based application customers can conduct activities like online retail banking Secure messaging Credit Card Payments Shopping Corporate banking and even payment to third party individual sand institutes Responsibilities Involved in design development and analysis documents in sharing with Clients Developed web pages using Struts framework JSP XML JavaScript Hibernate springs Html DHTML and CSS configure struts application use tag library Developed Application using Spring and Hibernate Spring batch Web Services like Soap and restful Web services Used Spring Framework at Business Tier and also springs Bean Factory for initializing services Used AJAX JavaScript to create interactive user interface Implemented client side validations using JavaScript server side validations Developed Single Page application using angular JS backbone JS Implemented Hibernate to persist the data into Database and wrote HQL based queries to implement CRUD operations on the data Developed an API to write XML documents from a database Utilized XML and XSL Transformation for dynamic webcontent and database connectivity Database modeling administration and development using SQL and PLSQL in Oracle 11 g Coded different deployment descriptors using XML Generated Jar files are deployed on Apache Tomcat Server Involved in the development of presentation layer and GUI framework in JSP ClientSide validations were done using JavaScript Involved in configuring and deploying the application using WebSphere Involved in code reviews and mentored the team in resolving issues Undertook the Integration and testing of the various parts of the application Developed automated Build files using ANT Used Subversion for version control and log4j for logging errors Code Walkthrough Test cases and Test Plans Environment HTML5 JSP Servlets JDBC JavaScript Json Spring SQL Oracle 11 g Tomcat Eclipse IDE XML XSL ANT Tomcat 5 Education Bachelor of Technology in Electronics and Communication Engineering in Electronics and Communication Engineering Anna University Chennai Tamil Nadu May 2009 Skills Hdfs Impala Sqoop Kafka Hadoop Additional Information Technical Skills Big DataHadoop HDFS Hive Sqoop Impala Kafka Map Reduce Cloudera Amazon EMR Spark Components Spark Core Spark SQL Spark Streaming Programming Languages SQL Scala and Java Databases MySQL HiveQL RDBMS Cloud Amazon EMR EC2 S3 Operating Systems Windows Unix Red Hat Linux",
    "extracted_keywords": [
        "Hadoop",
        "Developer",
        "Hadoop",
        "span",
        "lDeveloperspan",
        "Hadoop",
        "Developer",
        "Sutter",
        "Health",
        "Sacramento",
        "CA",
        "years",
        "experience",
        "phases",
        "Software",
        "Application",
        "requirement",
        "analysis",
        "design",
        "development",
        "maintenance",
        "HadoopBig",
        "Data",
        "application",
        "SPARK",
        "KAFKA",
        "EMR",
        "Hive",
        "Sqoop",
        "applications",
        "java",
        "industry",
        "Hands",
        "experience",
        "Spark",
        "Core",
        "Spark",
        "SQL",
        "Spark",
        "Streaming",
        "SparkSQL",
        "transformations",
        "actions",
        "data",
        "Hive",
        "Used",
        "Kafka",
        "Spark",
        "Streaming",
        "processing",
        "Experience",
        "data",
        "RDBMS",
        "sources",
        "HDFS",
        "Sqoop",
        "Good",
        "Knowledge",
        "Apache",
        "Spark",
        "data",
        "processing",
        "data",
        "RDBMS",
        "streaming",
        "sources",
        "Spark",
        "Experience",
        "Data",
        "Warehousing",
        "ETL",
        "processes",
        "database",
        "SQL",
        "ETL",
        "data",
        "analysis",
        "understandingknowledge",
        "Hadoop",
        "architecture",
        "components",
        "HDFS",
        "Job",
        "Tracker",
        "Task",
        "Tracker",
        "Name",
        "Node",
        "Data",
        "Node",
        "Map",
        "Reduce",
        "programming",
        "paradigm",
        "skills",
        "SPARK",
        "Jobs",
        "Scala",
        "sets",
        "semistructured",
        "HDFS",
        "Good",
        "Knowledge",
        "Spark",
        "SQL",
        "tables",
        "HDFS",
        "queries",
        "Experience",
        "Hive",
        "Queries",
        "processing",
        "volumes",
        "data",
        "Experience",
        "data",
        "Sqoop",
        "Relational",
        "Database",
        "Systems",
        "HDFS",
        "viceversa",
        "optimization",
        "mechanisms",
        "Combiners",
        "Cache",
        "Data",
        "Compression",
        "Custom",
        "Practitioner",
        "jobs",
        "US",
        "employer",
        "Work",
        "Experience",
        "Hadoop",
        "Developer",
        "Sutter",
        "Health",
        "Sacramento",
        "CA",
        "September",
        "Present",
        "Sutter",
        "Health",
        "health",
        "system",
        "Northern",
        "California",
        "Sacramento",
        "doctors",
        "hospitals",
        "health",
        "care",
        "services",
        "Northern",
        "California",
        "cities",
        "towns",
        "service",
        "lines",
        "Sutter",
        "Healthaffiliated",
        "hospitals",
        "care",
        "womens",
        "services",
        "cancer",
        "care",
        "orthopedics",
        "safety",
        "technology",
        "project",
        "buckets",
        "data",
        "knowledge",
        "insights",
        "world",
        "healthcare",
        "Big",
        "Data",
        "sense",
        "information",
        "overload",
        "insights",
        "volumes",
        "sources",
        "data",
        "Responsibilities",
        "teams",
        "business",
        "requirements",
        "component",
        "source",
        "file",
        "Data",
        "Integrity",
        "Data",
        "Quality",
        "header",
        "trailer",
        "information",
        "column",
        "validations",
        "Spark",
        "SQL",
        "tables",
        "spark",
        "processing",
        "data",
        "Hive",
        "transformations",
        "filter",
        "preaggregations",
        "data",
        "data",
        "Tableau",
        "hive",
        "views",
        "feature",
        "data",
        "platform",
        "Hadoop",
        "teams",
        "needs",
        "team",
        "Kafka",
        "Data",
        "ingestion",
        "data",
        "sets",
        "data",
        "HDFS",
        "data",
        "SQOOP",
        "sqoop",
        "jobs",
        "data",
        "RDBMS",
        "servers",
        "Hadoop",
        "Environment",
        "Hadoop",
        "Cloudera",
        "Amazon",
        "AWS",
        "HDFS",
        "Hive",
        "Impala",
        "Spark",
        "Kafka",
        "s3",
        "Sqoop",
        "SparkHadoop",
        "Developer",
        "Nike",
        "Portland",
        "November",
        "August",
        "Nike",
        "Inc",
        "corporation",
        "design",
        "development",
        "manufacturing",
        "marketing",
        "sales",
        "footwear",
        "apparel",
        "equipment",
        "accessories",
        "services",
        "company",
        "Beaverton",
        "Oregon",
        "Portland",
        "area",
        "Nike",
        "Inc",
        "project",
        "recommendations",
        "customers",
        "increase",
        "purchases",
        "products",
        "promotions",
        "time",
        "Products",
        "customers",
        "crosssell",
        "offer",
        "opportunities",
        "Responsibilities",
        "teams",
        "business",
        "requirements",
        "component",
        "source",
        "file",
        "Data",
        "Integrity",
        "Data",
        "Quality",
        "header",
        "trailer",
        "information",
        "column",
        "validations",
        "Spark",
        "SQL",
        "data",
        "frames",
        "transformations",
        "data",
        "frames",
        "schema",
        "data",
        "frames",
        "Spark",
        "SQL",
        "tables",
        "spark",
        "processing",
        "data",
        "Spark",
        "streaming",
        "Apache",
        "Kafka",
        "time",
        "data",
        "Experience",
        "Kafka",
        "producer",
        "Kafka",
        "consumer",
        "Spark",
        "Hive",
        "transformations",
        "filter",
        "preaggregations",
        "data",
        "HDFS",
        "Worked",
        "layers",
        "data",
        "layer",
        "layer",
        "layer",
        "hive",
        "tables",
        "data",
        "Optimizations",
        "techniques",
        "bucketing",
        "Avro",
        "file",
        "format",
        "Snappy",
        "tables",
        "processing",
        "data",
        "file",
        "format",
        "tables",
        "views",
        "tables",
        "sentry",
        "policy",
        "files",
        "access",
        "databases",
        "tables",
        "impala",
        "business",
        "users",
        "dev",
        "uat",
        "prod",
        "environment",
        "jobs",
        "Oozie",
        "Autosys",
        "Experience",
        "AWS",
        "EMR",
        "cluster",
        "data",
        "S3",
        "HDFS",
        "evaluation",
        "selection",
        "technologies",
        "system",
        "efficiency",
        "development",
        "execution",
        "system",
        "disaster",
        "recovery",
        "processes",
        "Environment",
        "Hadoop",
        "Cloudera",
        "Amazon",
        "AWS",
        "HDFS",
        "Hive",
        "Impala",
        "Spark",
        "Kafka",
        "s3",
        "Sqoop",
        "Java",
        "Scala",
        "Eclipse",
        "Tableau",
        "Maven",
        "SBT",
        "SparkHadoop",
        "Developer",
        "Capital",
        "Richmond",
        "VA",
        "September",
        "October",
        "Retail",
        "Enterprise",
        "Credit",
        "Risk",
        "application",
        "Banks",
        "data",
        "credit",
        "cards",
        "auto",
        "student",
        "home",
        "loans",
        "risk",
        "domains",
        "Enterprise",
        "Capital",
        "Management",
        "Data",
        "System",
        "records",
        "Teradata",
        "data",
        "cleansing",
        "value",
        "processing",
        "views",
        "data",
        "Hadoop",
        "warehouse",
        "data",
        "ECM",
        "generating",
        "reports",
        "Responsibilities",
        "teams",
        "business",
        "requirements",
        "component",
        "source",
        "file",
        "Data",
        "Integrity",
        "Data",
        "Quality",
        "header",
        "trailer",
        "information",
        "column",
        "validations",
        "Spark",
        "SQL",
        "data",
        "frames",
        "transformations",
        "data",
        "frames",
        "schema",
        "data",
        "frames",
        "Spark",
        "SQL",
        "tables",
        "spark",
        "processing",
        "data",
        "Spark",
        "streaming",
        "Apache",
        "Kafka",
        "time",
        "data",
        "Experience",
        "Kafka",
        "producer",
        "Kafka",
        "consumer",
        "Spark",
        "Hive",
        "transformations",
        "filter",
        "preaggregations",
        "data",
        "HDFS",
        "Sqoop",
        "data",
        "Netezza",
        "Teradata",
        "HDFS",
        "Hive",
        "Worked",
        "layers",
        "data",
        "layer",
        "layer",
        "layer",
        "hive",
        "tables",
        "data",
        "Optimizations",
        "techniques",
        "bucketing",
        "Avro",
        "file",
        "format",
        "Snappy",
        "tables",
        "processing",
        "data",
        "file",
        "format",
        "tables",
        "views",
        "tables",
        "sentry",
        "policy",
        "files",
        "access",
        "databases",
        "tables",
        "impala",
        "business",
        "users",
        "dev",
        "uat",
        "prod",
        "environment",
        "jobs",
        "Oozie",
        "Autosys",
        "Experience",
        "AWS",
        "EMR",
        "cluster",
        "data",
        "S3",
        "HDFS",
        "evaluation",
        "selection",
        "technologies",
        "system",
        "efficiency",
        "development",
        "execution",
        "system",
        "disaster",
        "recovery",
        "processes",
        "Environment",
        "Hadoop",
        "Cloudera",
        "Amazon",
        "AWS",
        "HDFS",
        "Hive",
        "Impala",
        "Spark",
        "Kafka",
        "s3",
        "Sqoop",
        "Java",
        "Scala",
        "Eclipse",
        "Tableau",
        "Maven",
        "SBT",
        "Java",
        "Developer",
        "Keane",
        "India",
        "LTD",
        "Bengaluru",
        "Karnataka",
        "April",
        "May",
        "associate",
        "developer",
        "ATS",
        "Asset",
        "Tracking",
        "Software",
        "assets",
        "company",
        "functionality",
        "asset",
        "registry",
        "transactions",
        "orders",
        "invoices",
        "vendor",
        "inventory",
        "sales",
        "Responsibilities",
        "SDLC",
        "software",
        "development",
        "life",
        "cycle",
        "application",
        "requirement",
        "gathering",
        "analysis",
        "testing",
        "maintenance",
        "modules",
        "MVC",
        "Architecture",
        "UI",
        "JavaScript",
        "JSP",
        "HTML",
        "CSS",
        "cross",
        "browser",
        "functionality",
        "user",
        "interface",
        "business",
        "logic",
        "servlets",
        "session",
        "beans",
        "Apache",
        "Tomcat",
        "server",
        "SQL",
        "Queries",
        "PLSQL",
        "procedures",
        "functions",
        "end",
        "design",
        "test",
        "case",
        "specifications",
        "Performed",
        "unit",
        "testing",
        "system",
        "testing",
        "integration",
        "testing",
        "unit",
        "test",
        "cases",
        "JUnit",
        "unit",
        "testing",
        "application",
        "support",
        "production",
        "environments",
        "issues",
        "defects",
        "solution",
        "defects",
        "priority",
        "defects",
        "schedule",
        "Environment",
        "Java",
        "JSP",
        "Servlets",
        "Apache",
        "Tomcat",
        "Oracle",
        "SQL",
        "Java",
        "Developer",
        "Senmex",
        "Hyderabad",
        "Telangana",
        "February",
        "March",
        "design",
        "platform",
        "services",
        "bank",
        "customers",
        "web",
        "application",
        "customers",
        "activities",
        "banking",
        "Secure",
        "Credit",
        "Card",
        "Payments",
        "Shopping",
        "banking",
        "payment",
        "party",
        "sand",
        "institutes",
        "Responsibilities",
        "design",
        "development",
        "analysis",
        "documents",
        "Clients",
        "web",
        "pages",
        "Struts",
        "framework",
        "JSP",
        "XML",
        "JavaScript",
        "Hibernate",
        "springs",
        "Html",
        "DHTML",
        "CSS",
        "configure",
        "struts",
        "application",
        "use",
        "tag",
        "library",
        "Developed",
        "Application",
        "Spring",
        "Hibernate",
        "Spring",
        "batch",
        "Web",
        "Services",
        "Soap",
        "Web",
        "services",
        "Spring",
        "Framework",
        "Business",
        "Tier",
        "Bean",
        "Factory",
        "services",
        "AJAX",
        "JavaScript",
        "user",
        "interface",
        "client",
        "side",
        "validations",
        "JavaScript",
        "server",
        "side",
        "validations",
        "Single",
        "Page",
        "application",
        "JS",
        "backbone",
        "JS",
        "Hibernate",
        "data",
        "Database",
        "HQL",
        "queries",
        "CRUD",
        "operations",
        "data",
        "API",
        "XML",
        "documents",
        "database",
        "XML",
        "XSL",
        "Transformation",
        "webcontent",
        "database",
        "connectivity",
        "Database",
        "administration",
        "development",
        "SQL",
        "PLSQL",
        "Oracle",
        "g",
        "deployment",
        "descriptors",
        "XML",
        "Jar",
        "files",
        "Apache",
        "Tomcat",
        "Server",
        "development",
        "presentation",
        "layer",
        "GUI",
        "framework",
        "JSP",
        "ClientSide",
        "validations",
        "JavaScript",
        "application",
        "WebSphere",
        "code",
        "reviews",
        "team",
        "issues",
        "Undertook",
        "Integration",
        "testing",
        "parts",
        "application",
        "Build",
        "files",
        "ANT",
        "Used",
        "Subversion",
        "version",
        "control",
        "log4j",
        "errors",
        "Code",
        "Walkthrough",
        "Test",
        "cases",
        "Test",
        "Plans",
        "Environment",
        "HTML5",
        "JSP",
        "Servlets",
        "JDBC",
        "JavaScript",
        "Json",
        "Spring",
        "SQL",
        "Oracle",
        "g",
        "Tomcat",
        "Eclipse",
        "IDE",
        "XML",
        "XSL",
        "ANT",
        "Tomcat",
        "Education",
        "Bachelor",
        "Technology",
        "Electronics",
        "Communication",
        "Engineering",
        "Electronics",
        "Communication",
        "Engineering",
        "Anna",
        "University",
        "Chennai",
        "Tamil",
        "Nadu",
        "May",
        "Skills",
        "Hdfs",
        "Impala",
        "Sqoop",
        "Kafka",
        "Hadoop",
        "Additional",
        "Information",
        "Technical",
        "Skills",
        "Big",
        "DataHadoop",
        "HDFS",
        "Hive",
        "Sqoop",
        "Impala",
        "Kafka",
        "Map",
        "Reduce",
        "Cloudera",
        "Amazon",
        "EMR",
        "Spark",
        "Components",
        "Spark",
        "Core",
        "Spark",
        "SQL",
        "Spark",
        "Streaming",
        "Programming",
        "Languages",
        "SQL",
        "Scala",
        "Java",
        "MySQL",
        "HiveQL",
        "Cloud",
        "Amazon",
        "EMR",
        "EC2",
        "S3",
        "Operating",
        "Systems",
        "Windows",
        "Unix",
        "Red",
        "Hat",
        "Linux"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T21:35:18.862956",
    "resume_data": "Hadoop Developer Hadoop span lDeveloperspan Hadoop Developer Sutter Health Sacramento CA 7 years of experience in all phases of Software Application requirement analysis design development and maintenance of HadoopBig Data application like SPARK KAFKA EMR Hive Sqoop and applications using java and scala to tailored with industry needs Hands on experience with Spark Core Spark SQL Spark Streaming Used SparkSQL to perform transformations and actions on data residing in Hive Used Kafka Spark Streaming for realtime processing Experience with migrating data to and from RDBMS and unstructured sources into HDFS using Sqoop Good Knowledge in Apache Spark data processing to handle data from RDBMS and streaming sources with Spark streaming Experience in Data Warehousing and ETL processes and Strong database SQL ETL and data analysis skills Good understandingknowledge of Hadoop architecture and various components such as HDFS Job Tracker Task Tracker Name Node Data Node and Map Reduce programming paradigm Have good skills in writing SPARK Jobs in Scala for processing large sets of structured semistructured and store them in HDFS Good Knowledge in Spark SQL queries to load tables into HDFS to run select queries on top Experience in writing Hive Queries for processing and analyzing large volumes of data Experience in importing and exporting data using Sqoop from Relational Database Systems to HDFS and viceversa Implemented several optimization mechanisms like Combiners Distributed Cache Data Compression and Custom Practitioner to speed up the jobs Authorized to work in the US for any employer Work Experience Hadoop Developer Sutter Health Sacramento CA September 2018 to Present Sutter Health is a notforprofit health system in Northern California headquartered in Sacramento It includes doctors hospitals and other health care services in more than 100 Northern California cities and towns Major service lines of Sutter Healthaffiliated hospitals include cardiac care womens and childrens services cancer care orthopedics and advanced patient safety technology This project seek to harness these massive complex buckets of data to obtain more focused knowledge and insights into the world of healthcare Big Data attempts to make more sense of this information overload and provide better insights from the expanding volumes and sources of data Responsibilities Interacting with multiple teams understanding their business requirements for designing flexible and common component Validating the source file for Data Integrity and Data Quality by reading header and trailer information and column validations Implemented Spark SQL to access hive tables into spark for faster processing of data Used Hive to do transformations joins filter and some preaggregations before storing the data Validating and visualizing the data in Tableau Using hive extensively to create a views for the feature data Working with platform and Hadoop teams closely for the needs of the team Using Kafka for Data ingestion for different data sets Experienced in importing and exporting data into HDFS and assisted in exporting analyzed data to RDBMS using SQOOP Developed sqoop jobs to import the data from RDBMS and file servers into Hadoop Environment Hadoop Cloudera Amazon AWS HDFS Hive Impala Spark Kafka s3 Sqoop SparkHadoop Developer Nike Portland OR November 2017 to August 2018 Nike Inc is an American multinational corporation that is engaged in the design development manufacturing and worldwide marketing and sales of footwear apparel equipment accessories and services The company is headquartered near Beaverton Oregon in the Portland metropolitan area Nike Inc project provides upsell and crosssell recommendations for customers enabling the increase in online purchases by recommending relevant products and promotions in real time Products recommend based on what other similar customers have bought providing upsell crosssell or next best offer opportunities Responsibilities Interacting with multiple teams understanding their business requirements for designing flexible and common component Validating the source file for Data Integrity and Data Quality by reading header and trailer information and column validations Used Spark SQL for creating data frames and performed transformations on data frames like adding schema manually casting joining data frames before storing them Implemented Spark SQL to access hive tables into spark for faster processing of data Worked on Spark streaming using Apache Kafka for real time data processing Experience in creating Kafka producer and Kafka consumer for Spark streaming Used Hive to do transformations joins filter and some preaggregations before storing the data into HDFS Worked on three layers for storing data such as raw layer intermediate layer and publish layer Creating external hive tables to store and queries the data which is loaded Optimizations techniques include partitioning bucketing Using Avro file format compressed with Snappy in intermediate tables for faster processing of data Used parquet file format for published tables and created views on the tables Created sentry policy files to provide access to the required databases and tables to view from impala to the business users in the dev uat and prod environment Automated the jobs with Oozie and scheduled them with Autosys Experience in AWS to spin up the EMR cluster to process the huge data which is stored in S3 and push it to HDFS Participated in evaluation and selection of new technologies to support system efficiency Participated in development and execution of system and disaster recovery processes Environment Hadoop Cloudera Amazon AWS HDFS Hive Impala Spark Kafka s3 Sqoop Java Scala Eclipse Tableau and Maven SBT SparkHadoop Developer Capital one Richmond VA September 2016 to October 2017 Retail Enterprise Credit Risk application calculates Banks retail data such as credit cards auto student and home loans for risk domains including Enterprise Capital Management Data comes from different System of records mainly from Teradata This data will undergo several cleansing and value added processing and then finally views will be created on this data as Hadoop warehouse This data will be consumed by downstream like ECM for analyzing and generating reports Responsibilities Interacting with multiple teams understanding their business requirements for designing flexible and common component Validating the source file for Data Integrity and Data Quality by reading header and trailer information and column validations Used Spark SQL for creating data frames and performed transformations on data frames like adding schema manually casting joining data frames before storing them Implemented Spark SQL to access hive tables into spark for faster processing of data Worked on Spark streaming using Apache Kafka for real time data processing Experience in creating Kafka producer and Kafka consumer for Spark streaming Used Hive to do transformations joins filter and some preaggregations before storing the data into HDFS Used Sqoop for importing and exporting data from Netezza Teradata into HDFS and Hive Worked on three layers for storing data such as raw layer intermediate layer and publish layer Creating external hive tables to store and queries the data which is loaded Optimizations techniques include partitioning bucketing Using Avro file format compressed with Snappy in intermediate tables for faster processing of data Used parquet file format for published tables and created views on the tables Created sentry policy files to provide access to the required databases and tables to view from impala to the business users in the dev uat and prod environment Automated the jobs with Oozie and scheduled them with Autosys Experience in AWS to spin up the EMR cluster to process the huge data which is stored in S3 and push it to HDFS Participated in evaluation and selection of new technologies to support system efficiency Participated in development and execution of system and disaster recovery processes Environment Hadoop Cloudera Amazon AWS HDFS Hive Impala Spark Kafka s3 Sqoop Java Scala Eclipse Tableau and Maven SBT Java Developer Keane India LTD Bengaluru Karnataka April 2014 to May 2016 I worked as an associate java developer on ATS Asset Tracking Software which manages and tracks the assets of a company It manages separate login functionality for asset registry and transactions including orders invoices and returns It also allows the vendor to closely monitor the inventory and sales Responsibilities Involved in the complete SDLC software development life cycle of the application from requirement gathering and analysis to testing and maintenance Developed the modules based on MVC Architecture Developed UI using JavaScript JSP HTML and CSS for interactive cross browser functionality and complex user interface Created business logic using servlets and session beans and deployed them on Apache Tomcat server Created complex SQL Queries PLSQL Stored procedures and functions for back end Prepared the functional design and test case specifications Performed unit testing system testing and integration testing Developed unit test cases Used JUnit for unit testing of the application Provided Technical support for production environments resolving the issues analyzing the defects providing and implementing the solution defects Resolved more priority defects as per the schedule Environment Java JSP Servlets Apache Tomcat Oracle SQL Java Developer Senmex Hyderabad Telangana February 2011 to March 2014 Developed with a flexible design which provides the platform to offer online integrated financial services to the bank customers Using this web based application customers can conduct activities like online retail banking Secure messaging Credit Card Payments Shopping Corporate banking and even payment to third party individual sand institutes Responsibilities Involved in design development and analysis documents in sharing with Clients Developed web pages using Struts framework JSP XML JavaScript Hibernate springs Html DHTML and CSS configure struts application use tag library Developed Application using Spring and Hibernate Spring batch Web Services like Soap and restful Web services Used Spring Framework at Business Tier and also springs Bean Factory for initializing services Used AJAX JavaScript to create interactive user interface Implemented client side validations using JavaScript server side validations Developed Single Page application using angular JS backbone JS Implemented Hibernate to persist the data into Database and wrote HQL based queries to implement CRUD operations on the data Developed an API to write XML documents from a database Utilized XML and XSL Transformation for dynamic webcontent and database connectivity Database modeling administration and development using SQL and PLSQL in Oracle 11g Coded different deployment descriptors using XML Generated Jar files are deployed on Apache Tomcat Server Involved in the development of presentation layer and GUI framework in JSP ClientSide validations were done using JavaScript Involved in configuring and deploying the application using WebSphere Involved in code reviews and mentored the team in resolving issues Undertook the Integration and testing of the various parts of the application Developed automated Build files using ANT Used Subversion for version control and log4j for logging errors Code Walkthrough Test cases and Test Plans Environment HTML5 JSP Servlets JDBC JavaScript Json Spring SQL Oracle 11g Tomcat Eclipse IDE XML XSL ANT Tomcat 5 Education Bachelor of Technology in Electronics and Communication Engineering in Electronics and Communication Engineering Anna University Chennai Tamil Nadu May 2009 Skills Hdfs Impala Sqoop Kafka Hadoop Additional Information Technical Skills Big DataHadoop HDFS Hive Sqoop Impala Kafka Map Reduce Cloudera Amazon EMR Spark Components Spark Core Spark SQL Spark Streaming Programming Languages SQL Scala and Java Databases MySQL HiveQL RDBMS Cloud Amazon EMR EC2 S3 Operating Systems Windows Unix Red Hat Linux",
    "unique_id": "a445d9ff-e26d-41cb-b26f-1a91a6b1f2a8"
}