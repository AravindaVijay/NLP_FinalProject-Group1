{
    "clean_data": "Hadoop Developer Hadoop span lDeveloperspan Hadoop Developer Dell Raleigh NC 7 years of professional experience involving project development implementation deployment and maintenance using JavaJ2EE and Big Data related technologies Hadoop Developer with 5 years of working experience in designing and implementing complete endtoendHadoop based data analytical solutions using HDFS MapReduce Spark Yarn Kafka PIG HIVE Sqoop Storm Flume Oozie Impala HBase etc Good experience in creating data ingestion pipelines data transformations data management data governance and real time streaming at an enterprise level Experience in importing and exporting different formats of data into HDFS HBASE from different RDBMS databases and vice versa using Sqoop Exposure to Cloudera development environment and management using Cloudera Manager Experience in analyzing data using HiveQL Pig Latin Hbase Mongo and custom MapReduce programs in Java Experience in extending Hive and Pig core functionality by writing custom UDFs using Java Developed analytical components using Spark and Spark Stream Background with traditional databases such as Oracle SQL Server MySQL Good knowledge and Handson experience in storing processing unstructured data using NOSQL databases like HBase and MongoDB Good knowledge in distributed coordination system ZooKeeper and experience with Data Warehousing and ETL Hands on experience in setting up workflow using Apache Oozie workflow engine for managing and scheduling Hadoop jobs Experience in creating complex SQL Queries and SQL tuning writing PLSQL blocks like stored procedures Functions Cursors Index triggers and packages Good knowledge of database connectivity JDBC for databases like Oracle DB2 SQL Server MySQL NoSQL MS Access Profound experience in creating real time data streaming solutions using Apache SparkSpark Streaming Kafka Developed analytical components using Spark and Spark Stream Worked on a prototype Apache Spark Streaming project and converted our existing Java Strom Topology Proficient in visualizing data using Tableau QlikView Microstratergy and MS Excel Experience in developing ETL scripts for data acquisition and transformation using Informatica and Talend Used Maven extensively for building MapReduce jar files and deployed it to Amazon Web Services AWS using EC2 virtual Servers in the cloud and Experience in build scripts to do continuous integrations systems like Jenkins Experienced in Java Application Development ClientServer Applications InternetIntranet based applications using Core Java J2EE patterns Spring Hibernate Struts JMS Web Services SOAPREST Oracle SQL Server and other relational databases Experience writing Shell scripts in Linux OS and integrating them with other solutions Experienced in using agile methodologies including extreme programming SCRUM and Test Driven Development TDD Experienced in creating and analyzing Software Requirement Specifications SRS and Functional Specification Document FSD Strong knowledge of Software Development Life Cycle SDLC Devoted to professionalism highly organized ability to work under strict deadline schedules with attention to details possess excellent written and communication skillsAuthorized to work in United States for any employer Work Experience Hadoop Developer Dell Austin TX October 2018 to Present Responsibilities Involved in integrating the GIT into the Puppet to ensure the integrity of applications by creating Production Development Test and Release Branches Responsible for loading the customers data and event logs from Oracle database Teradata into HDFS using Sqoop Endtoend performance tuning of Hadoop clusters and Hadoop MapReduce routines against very large data sets Performance tuning of Hadoop clusters and Hadoop MapReduce routines Developed Python code to gather the data from HBase and designs the solution to implement using PySpark Developed PySpark code to mimic the transformations performed in the onpremise environment Analyzed the Sql scripts and designed solutions to implement using pyspark created custom new columns depending up on the use case while ingesting the data into Hadoop lake using pyspark Analyze Cassandra database and compare it with other opensource NoSQL databases to find which one of them better suites the current requirement Integrated Cassandra as a distributed persistent metadata store to provide metadata resolution for network entities on the network Implemented Spark using Scala and also used Pyspark using Python for faster testing and processing of data Build servers using AWS Importing volumes launching EC2 RDS creating security groups autoscaling load balancers ELBs in the defined virtual private connection Installed and configured Apache Hadoop Hive and Pig environment on AWSDiligently teaming with the infrastructure network database application and business intelligence teams to guarantee high data quality and availability Installed and configured various components of Hadoop ecosystem and maintained their integrity Designed configured and managed the backup and disaster recovery for HDFS data Migrating physical LinuxWindows servers to cloud AWS and testing Installing Upgrading and Managing Hadoop Cluster on Cloudera distribution Commissioned Data Nodes when data grew and decommissioned when the hardware degraded Development Acceptance Integration and Production AWS Endpoints Configured various property files like coresitexml hdfssitexml mapredsitexml based upon the job requirement Monitored multiple Hadoop clusters environments using Ganglia and Nagios Monitored workload job performance and capacity planning Develop Hive scripts for end user analyst requirements to perform ad hoc analysis Plan deploy monitor and maintain Amazon AWS cloud infrastructure consisting of multiple EC2 nodes and VMware Vms as required in the environment Expertise in AWS data migration between different database platforms like SQL Server to Amazon Aurora using RDS tool Installing Upgrading and Managing Hadoop Cluster on Cloudera distribution Installed and configured Hadoop HDFS MapReduce Pig Hive and Sqoop Wrote Pig Scripts to generate MapReduce jobs and performed ETL procedures on the data in HDFS Exported analyzed data to HDFS using Sqoop for generating reports Scheduling all hadoophivesqoopHbase jobs using Oozie Environment Hadoop Map Reduce Shell Scripting spark Pig Hive Cloudera Manager AWSCDH 543Pyspark HDFS Yarn Hue Sentry Oozie Zoo keeper Impala Solr Kerberos cluster health Puppet Ganglia Nagios Flume Sqoop storm Kafka KMS PythonJava Consultant Walmart Bentonville AR September 2016 to October 2018 Responsibilities Implemented user interface guidelines and standards throughout the development and maintenance of the website using the HTML CSS JavaScript and jQuery Developed GUI using Django and Python for dynamically displaying the test block documentation and other features with Python code for a web browser Implemented AJAX for dynamic functionality of a webpages for front end applications Developed and tested many features for dashboard created using Bootstrap CSS and JavaScript Developed front end using Angularjs ReactJS Nodejs bootstrapjs backbonejs JavaScript where back end is java with REST webservice Involved in the application development using Spring Core MVC modules and Java web based technologies such as Servlets JSP Java Web Service RESTSOAP based WSDL Utilized standard Python modules such as csv robot parser iterators and pickle for development Developed views and templates with Python and Djangos view controller and templating language to create userfriendly website interface Worked on Python OpenStack APIs and used NumPy for Numerical analysis Developed Wrapper in Python for instantiating multithreaded application Managed datasets using Panda data frames and MySQL queried MYSQL database queries from python using PythonMySQL connector and MySQL dB package to retrieve information Used Ajax and jQuery for transmitting JSON data objects between frontend and controllers Used Angular MVC and twoway data binding Worked on automation scripts using Selenium in JAVA Developed entire frontend and backend modules using on Django including Tastypie Web Framework using Git Designed coded and tested key modules of the project using java oops concepts Install KAFKA on Hadoop cluster and configure producer and consumer coding part in java to establish connection from twitter source to HDFS with popular hash tags Developed Splunk infrastructure and related solution for application toolsets Helped team to onboard data create various knowledge objects Install and maintain the Splunk Apps Creating Application on Splunk to analyse the data Manage Splunk configurations files like input props transforms and lookups Configured Maven for Java automation projects and developed Maven Project Object Model POM Setup automated cron jobs to upload data into database generate graphs bar charts upload these chartsto wiki and backup the database Software Development in Linux Environment utilized XShell to build deploy Java applications Developed Founctional Package with Java Erlang and Python Added several options to the application to choose particular algorithm for data and address generation Maintained the versions using GIT and sending the release notes for each release Supported the issues seen in the tool across all the teams for several projects Environment Python 32 Django HTML5CSS MS SQL Server 2013 MySQL JavaScript Eclipse Linux Shell Scripting PyCharm Urllib jQuery GitHub AngularJS Jira Sr Hadoop Developer CoxAuto New York NY August 2014 to September 2016 Responsibilities Loading Defined designed and developed Java applications specially using Hadoop MapReduce by leveraging frameworks such as Cascading and Hive Developed workflow using Oozie for running Map Reduce jobs and Hive Queries Worked on loading log data directly into HDFS using Flume Worked on Cloudera to analyze data present on top of HDFS Responsible for managing data from multiple sources Load data from various data sources into HDFS using Flume Involved in requirement and design phase to implement Streaming Lambda Architecture to use real time streaming using Spark and Kafka Created and maintained Technical documentation for launching Cloudera Hadoop Clusters and for executing Hive queries and Pig Scripts Worked on SPARK engine creating batch jobs with incremental load through STORM KAFKA SPLUNK FLUME HDFSS3 KINESIS Sockets AWS etc Writing scala classes to interact with the database This plugin allows Hadoop MapReduce programs HBase Pig and Hive to work unmodified and access files directly Involved in analyzing existing architecture on premise datacenters and designed to migrate applications from onprem to AWS Public Cloud Designed and implemented MapReducebased largescale parallel relationlearning system Successfully loaded files to Hive and HDFS from Mongo DB Solar Familiarity with a NoSQL database such as MongoDb Solar Successfully loaded files to Hive and HDFS from Mongo DB Solar Extracted files from MySQL through Sqoop and placed in HDFS and processed Collecting and aggregating large amounts of log data using Apache Flume and staging data in HDFS for further analysis Developed Pig Latin scripts to extract data from the web server output files to load into HDFS Built reusable Hive UDF libraries for business requirements which enabled users to use these UDFs in Hive Querying Worked on debugging performance tuning of Hive Pig Jobs Created Hbase tables to store various data formats of PII data coming from different portfolios Developed Pig Scripts Pig UDFs and Hive Scripts Hive UDFs to load data files into Hadoop Worked closely on parallel computing with Spark team to explore RDD in Datastax Cassandra Generating scala and java classes from the respective APIs so that they can be incorporated in the overall application Writing entities in Scala and Java along with named queries to interact with database Writing user console page in lift along with the snippets in Scala The product is responsible to give access to the user to all their credentials and privileges within the system Writing scala test cases to test scala written code Environment Python 3 Django Hadoop HDFS Map Reduce Shell Scripting spark solr Pig Hive HBase Sqoop Flume Oozie Zoo keeper cluster health monitoring security Redhat Linux impala Cloudera ManagerGolang bitbucket pdb AWS Jira Jenkins dockers Pyspark Rest Virtual Machine Ajax jQuery JavaScript LINUX Software Developer Cognizant Greenville SC July 2012 to August 2014 Responsibilities Design develop test deploy and maintain the website Interaction with client to understand expectations and requirements Designed and developed the UI of the website using HTML AJAX CSS and JavaScript Developed entire frontend and backend modules using Python on Django Web Framework Performed testing using Djangos Test Module Assisting junior Perl developers in the development of scripts Developed a fully automated continuous integration system using GIT Gerrit Jenkins MySQL and custom tools developed in Python and Bash Managed developed and designed a dashboard control panel for customers and Administrators using Django Oracle DB PostgreSQL Extensively used python modules such as requests urllib urllib2 for web crawling Implemented configuration changes for data models Used Pandas datamining library for statistics Analysis NumPY for Numerical analysis Managed large datasets using Panda data frames and MySQL ExportedImported data between different data sources using SQL Server Management Studio Maintained program libraries users manuals and technical documentation Responsible for debugging and troubleshooting the web application Successfully migrated all the data to the database while the site was in production Developed GUI using webapp2 for dynamically displaying the test block documentation and other features of python code using a web browser Environment Python 27 Django 18 CSS HTML JavaScript JQuery webbapp2 AJAX MYSQL Linux Heroku GIT urllib urllib2 Oracle DB PostgreSQL and VMWare API",
    "entities": [
        "Implemented Spark",
        "STORM KAFKA",
        "Pyspark using Python",
        "Oracle SQL Server",
        "Maven Project Object Model POM Setup",
        "Informatica",
        "onprem",
        "Commissioned Data Nodes",
        "SPARK",
        "Nodejs",
        "HTML AJAX CSS",
        "jQuery Developed GUI",
        "SQL Server Management Studio Maintained",
        "Hadoop Developer Hadoop",
        "webapp2",
        "Puppet Ganglia Nagios",
        "Present Responsibilities Involved",
        "Software Development Life Cycle SDLC Devoted",
        "Amazon Web Services AWS",
        "Panda",
        "Cloudera Hadoop Clusters",
        "RDD",
        "Hadoop",
        "Installing Upgrading and Managing Hadoop Cluster",
        "Spring Core",
        "NOSQL",
        "Development Acceptance Integration and Production AWS Endpoints Configured",
        "Bootstrap CSS",
        "WSDL Utilized",
        "Hive UDF",
        "Shell",
        "HBase",
        "Hive Pig Jobs Created Hbase",
        "JavaJ2EE",
        "Amazon",
        "Oozie Environment Hadoop",
        "Functional Specification Document",
        "TX",
        "Djangos Test Module",
        "HBase Pig",
        "MapReducebased",
        "Python",
        "SQL Server",
        "Developed",
        "Data Warehousing and ETL Hands",
        "Flume Involved",
        "Spark Stream Background",
        "Puppet",
        "Hadoop MapReduce",
        "AWSDiligently",
        "Pig Hive HBase",
        "Hadoop Worked",
        "Oracle DB2 SQL Server",
        "Developed Pig Scripts Pig",
        "Hadoop Developer",
        "Linux",
        "Numerical",
        "JSP",
        "Ganglia",
        "SQL Queries",
        "VMWare API",
        "AWS Public Cloud Designed",
        "RDS",
        "MVC",
        "Spark",
        "Java Application Development ClientServer Applications InternetIntranet",
        "PySpark Developed PySpark",
        "Datastax",
        "GIT",
        "XShell",
        "Mongo DB Solar Familiarity",
        "HTML CSS JavaScript",
        "Sqoop",
        "LinuxWindows",
        "Tableau QlikView Microstratergy",
        "AWS",
        "Scala",
        "PIG",
        "HDFS Responsible",
        "log data",
        "java",
        "Spark Stream Worked",
        "SQL",
        "Django Oracle DB PostgreSQL",
        "Python Added",
        "Administrators",
        "Software Development",
        "Oracle DB PostgreSQL",
        "Monitored multiple Hadoop",
        "Mongo DB Solar",
        "Big Data",
        "Hive",
        "MongoDb Solar Successfully",
        "Oozie Zoo",
        "Amazon AWS",
        "Handson",
        "ZooKeeper",
        "Bash Managed",
        "Pandas",
        "United States",
        "ETL",
        "PII",
        "Maven",
        "Djangos",
        "JavaScript",
        "Selenium",
        "HDFS MapReduce Spark",
        "Work Experience Hadoop Developer",
        "Nagios Monitored",
        "REST",
        "Release Branches Responsible",
        "MapReduce",
        "Software Requirement Specifications SRS",
        "Hive Queries Worked",
        "NoSQL",
        "JAVA Developed",
        "Responsibilities Design",
        "Developed GUI"
    ],
    "experience": "Experience in importing and exporting different formats of data into HDFS HBASE from different RDBMS databases and vice versa using Sqoop Exposure to Cloudera development environment and management using Cloudera Manager Experience in analyzing data using HiveQL Pig Latin Hbase Mongo and custom MapReduce programs in Java Experience in extending Hive and Pig core functionality by writing custom UDFs using Java Developed analytical components using Spark and Spark Stream Background with traditional databases such as Oracle SQL Server MySQL Good knowledge and Handson experience in storing processing unstructured data using NOSQL databases like HBase and MongoDB Good knowledge in distributed coordination system ZooKeeper and experience with Data Warehousing and ETL Hands on experience in setting up workflow using Apache Oozie workflow engine for managing and scheduling Hadoop jobs Experience in creating complex SQL Queries and SQL tuning writing PLSQL blocks like stored procedures Functions Cursors Index triggers and packages Good knowledge of database connectivity JDBC for databases like Oracle DB2 SQL Server MySQL NoSQL MS Access Profound experience in creating real time data streaming solutions using Apache SparkSpark Streaming Kafka Developed analytical components using Spark and Spark Stream Worked on a prototype Apache Spark Streaming project and converted our existing Java Strom Topology Proficient in visualizing data using Tableau QlikView Microstratergy and MS Excel Experience in developing ETL scripts for data acquisition and transformation using Informatica and Talend Used Maven extensively for building MapReduce jar files and deployed it to Amazon Web Services AWS using EC2 virtual Servers in the cloud and Experience in build scripts to do continuous integrations systems like Jenkins Experienced in Java Application Development ClientServer Applications InternetIntranet based applications using Core Java J2EE patterns Spring Hibernate Struts JMS Web Services SOAPREST Oracle SQL Server and other relational databases Experience writing Shell scripts in Linux OS and integrating them with other solutions Experienced in using agile methodologies including extreme programming SCRUM and Test Driven Development TDD Experienced in creating and analyzing Software Requirement Specifications SRS and Functional Specification Document FSD Strong knowledge of Software Development Life Cycle SDLC Devoted to professionalism highly organized ability to work under strict deadline schedules with attention to details possess excellent written and communication skillsAuthorized to work in United States for any employer Work Experience Hadoop Developer Dell Austin TX October 2018 to Present Responsibilities Involved in integrating the GIT into the Puppet to ensure the integrity of applications by creating Production Development Test and Release Branches Responsible for loading the customers data and event logs from Oracle database Teradata into HDFS using Sqoop Endtoend performance tuning of Hadoop clusters and Hadoop MapReduce routines against very large data sets Performance tuning of Hadoop clusters and Hadoop MapReduce routines Developed Python code to gather the data from HBase and designs the solution to implement using PySpark Developed PySpark code to mimic the transformations performed in the onpremise environment Analyzed the Sql scripts and designed solutions to implement using pyspark created custom new columns depending up on the use case while ingesting the data into Hadoop lake using pyspark Analyze Cassandra database and compare it with other opensource NoSQL databases to find which one of them better suites the current requirement Integrated Cassandra as a distributed persistent metadata store to provide metadata resolution for network entities on the network Implemented Spark using Scala and also used Pyspark using Python for faster testing and processing of data Build servers using AWS Importing volumes launching EC2 RDS creating security groups autoscaling load balancers ELBs in the defined virtual private connection Installed and configured Apache Hadoop Hive and Pig environment on AWSDiligently teaming with the infrastructure network database application and business intelligence teams to guarantee high data quality and availability Installed and configured various components of Hadoop ecosystem and maintained their integrity Designed configured and managed the backup and disaster recovery for HDFS data Migrating physical LinuxWindows servers to cloud AWS and testing Installing Upgrading and Managing Hadoop Cluster on Cloudera distribution Commissioned Data Nodes when data grew and decommissioned when the hardware degraded Development Acceptance Integration and Production AWS Endpoints Configured various property files like coresitexml hdfssitexml mapredsitexml based upon the job requirement Monitored multiple Hadoop clusters environments using Ganglia and Nagios Monitored workload job performance and capacity planning Develop Hive scripts for end user analyst requirements to perform ad hoc analysis Plan deploy monitor and maintain Amazon AWS cloud infrastructure consisting of multiple EC2 nodes and VMware Vms as required in the environment Expertise in AWS data migration between different database platforms like SQL Server to Amazon Aurora using RDS tool Installing Upgrading and Managing Hadoop Cluster on Cloudera distribution Installed and configured Hadoop HDFS MapReduce Pig Hive and Sqoop Wrote Pig Scripts to generate MapReduce jobs and performed ETL procedures on the data in HDFS Exported analyzed data to HDFS using Sqoop for generating reports Scheduling all hadoophivesqoopHbase jobs using Oozie Environment Hadoop Map Reduce Shell Scripting spark Pig Hive Cloudera Manager AWSCDH 543Pyspark HDFS Yarn Hue Sentry Oozie Zoo keeper Impala Solr Kerberos cluster health Puppet Ganglia Nagios Flume Sqoop storm Kafka KMS PythonJava Consultant Walmart Bentonville AR September 2016 to October 2018 Responsibilities Implemented user interface guidelines and standards throughout the development and maintenance of the website using the HTML CSS JavaScript and jQuery Developed GUI using Django and Python for dynamically displaying the test block documentation and other features with Python code for a web browser Implemented AJAX for dynamic functionality of a webpages for front end applications Developed and tested many features for dashboard created using Bootstrap CSS and JavaScript Developed front end using Angularjs ReactJS Nodejs bootstrapjs backbonejs JavaScript where back end is java with REST webservice Involved in the application development using Spring Core MVC modules and Java web based technologies such as Servlets JSP Java Web Service RESTSOAP based WSDL Utilized standard Python modules such as csv robot parser iterators and pickle for development Developed views and templates with Python and Djangos view controller and templating language to create userfriendly website interface Worked on Python OpenStack APIs and used NumPy for Numerical analysis Developed Wrapper in Python for instantiating multithreaded application Managed datasets using Panda data frames and MySQL queried MYSQL database queries from python using PythonMySQL connector and MySQL dB package to retrieve information Used Ajax and jQuery for transmitting JSON data objects between frontend and controllers Used Angular MVC and twoway data binding Worked on automation scripts using Selenium in JAVA Developed entire frontend and backend modules using on Django including Tastypie Web Framework using Git Designed coded and tested key modules of the project using java oops concepts Install KAFKA on Hadoop cluster and configure producer and consumer coding part in java to establish connection from twitter source to HDFS with popular hash tags Developed Splunk infrastructure and related solution for application toolsets Helped team to onboard data create various knowledge objects Install and maintain the Splunk Apps Creating Application on Splunk to analyse the data Manage Splunk configurations files like input props transforms and lookups Configured Maven for Java automation projects and developed Maven Project Object Model POM Setup automated cron jobs to upload data into database generate graphs bar charts upload these chartsto wiki and backup the database Software Development in Linux Environment utilized XShell to build deploy Java applications Developed Founctional Package with Java Erlang and Python Added several options to the application to choose particular algorithm for data and address generation Maintained the versions using GIT and sending the release notes for each release Supported the issues seen in the tool across all the teams for several projects Environment Python 32 Django HTML5CSS MS SQL Server 2013 MySQL JavaScript Eclipse Linux Shell Scripting PyCharm Urllib jQuery GitHub AngularJS Jira Sr Hadoop Developer CoxAuto New York NY August 2014 to September 2016 Responsibilities Loading Defined designed and developed Java applications specially using Hadoop MapReduce by leveraging frameworks such as Cascading and Hive Developed workflow using Oozie for running Map Reduce jobs and Hive Queries Worked on loading log data directly into HDFS using Flume Worked on Cloudera to analyze data present on top of HDFS Responsible for managing data from multiple sources Load data from various data sources into HDFS using Flume Involved in requirement and design phase to implement Streaming Lambda Architecture to use real time streaming using Spark and Kafka Created and maintained Technical documentation for launching Cloudera Hadoop Clusters and for executing Hive queries and Pig Scripts Worked on SPARK engine creating batch jobs with incremental load through STORM KAFKA SPLUNK FLUME HDFSS3 KINESIS Sockets AWS etc Writing scala classes to interact with the database This plugin allows Hadoop MapReduce programs HBase Pig and Hive to work unmodified and access files directly Involved in analyzing existing architecture on premise datacenters and designed to migrate applications from onprem to AWS Public Cloud Designed and implemented MapReducebased largescale parallel relationlearning system Successfully loaded files to Hive and HDFS from Mongo DB Solar Familiarity with a NoSQL database such as MongoDb Solar Successfully loaded files to Hive and HDFS from Mongo DB Solar Extracted files from MySQL through Sqoop and placed in HDFS and processed Collecting and aggregating large amounts of log data using Apache Flume and staging data in HDFS for further analysis Developed Pig Latin scripts to extract data from the web server output files to load into HDFS Built reusable Hive UDF libraries for business requirements which enabled users to use these UDFs in Hive Querying Worked on debugging performance tuning of Hive Pig Jobs Created Hbase tables to store various data formats of PII data coming from different portfolios Developed Pig Scripts Pig UDFs and Hive Scripts Hive UDFs to load data files into Hadoop Worked closely on parallel computing with Spark team to explore RDD in Datastax Cassandra Generating scala and java classes from the respective APIs so that they can be incorporated in the overall application Writing entities in Scala and Java along with named queries to interact with database Writing user console page in lift along with the snippets in Scala The product is responsible to give access to the user to all their credentials and privileges within the system Writing scala test cases to test scala written code Environment Python 3 Django Hadoop HDFS Map Reduce Shell Scripting spark solr Pig Hive HBase Sqoop Flume Oozie Zoo keeper cluster health monitoring security Redhat Linux impala Cloudera ManagerGolang bitbucket pdb AWS Jira Jenkins dockers Pyspark Rest Virtual Machine Ajax jQuery JavaScript LINUX Software Developer Cognizant Greenville SC July 2012 to August 2014 Responsibilities Design develop test deploy and maintain the website Interaction with client to understand expectations and requirements Designed and developed the UI of the website using HTML AJAX CSS and JavaScript Developed entire frontend and backend modules using Python on Django Web Framework Performed testing using Djangos Test Module Assisting junior Perl developers in the development of scripts Developed a fully automated continuous integration system using GIT Gerrit Jenkins MySQL and custom tools developed in Python and Bash Managed developed and designed a dashboard control panel for customers and Administrators using Django Oracle DB PostgreSQL Extensively used python modules such as requests urllib urllib2 for web crawling Implemented configuration changes for data models Used Pandas datamining library for statistics Analysis NumPY for Numerical analysis Managed large datasets using Panda data frames and MySQL ExportedImported data between different data sources using SQL Server Management Studio Maintained program libraries users manuals and technical documentation Responsible for debugging and troubleshooting the web application Successfully migrated all the data to the database while the site was in production Developed GUI using webapp2 for dynamically displaying the test block documentation and other features of python code using a web browser Environment Python 27 Django 18 CSS HTML JavaScript JQuery webbapp2 AJAX MYSQL Linux Heroku GIT urllib urllib2 Oracle DB PostgreSQL and VMWare API",
    "extracted_keywords": [
        "Hadoop",
        "Developer",
        "Hadoop",
        "span",
        "lDeveloperspan",
        "Hadoop",
        "Developer",
        "Dell",
        "Raleigh",
        "NC",
        "years",
        "experience",
        "project",
        "development",
        "implementation",
        "deployment",
        "maintenance",
        "JavaJ2EE",
        "Big",
        "Data",
        "technologies",
        "Hadoop",
        "Developer",
        "years",
        "working",
        "experience",
        "endtoendHadoop",
        "data",
        "solutions",
        "HDFS",
        "MapReduce",
        "Spark",
        "Yarn",
        "Kafka",
        "PIG",
        "HIVE",
        "Sqoop",
        "Storm",
        "Flume",
        "Oozie",
        "Impala",
        "HBase",
        "experience",
        "data",
        "ingestion",
        "pipelines",
        "data",
        "transformations",
        "data",
        "management",
        "data",
        "governance",
        "time",
        "streaming",
        "enterprise",
        "level",
        "Experience",
        "formats",
        "data",
        "HDFS",
        "HBASE",
        "RDBMS",
        "databases",
        "vice",
        "Sqoop",
        "Exposure",
        "Cloudera",
        "development",
        "environment",
        "management",
        "Cloudera",
        "Manager",
        "Experience",
        "data",
        "HiveQL",
        "Pig",
        "Latin",
        "Hbase",
        "Mongo",
        "MapReduce",
        "programs",
        "Java",
        "Experience",
        "Hive",
        "Pig",
        "core",
        "functionality",
        "custom",
        "UDFs",
        "Java",
        "Developed",
        "components",
        "Spark",
        "Spark",
        "Stream",
        "Background",
        "databases",
        "Oracle",
        "SQL",
        "Server",
        "MySQL",
        "knowledge",
        "Handson",
        "experience",
        "processing",
        "data",
        "NOSQL",
        "databases",
        "HBase",
        "knowledge",
        "coordination",
        "system",
        "ZooKeeper",
        "experience",
        "Data",
        "Warehousing",
        "ETL",
        "Hands",
        "experience",
        "workflow",
        "Apache",
        "Oozie",
        "workflow",
        "engine",
        "scheduling",
        "Hadoop",
        "jobs",
        "Experience",
        "SQL",
        "Queries",
        "SQL",
        "PLSQL",
        "blocks",
        "procedures",
        "Functions",
        "Cursors",
        "Index",
        "triggers",
        "knowledge",
        "database",
        "connectivity",
        "JDBC",
        "databases",
        "Oracle",
        "DB2",
        "SQL",
        "Server",
        "MySQL",
        "NoSQL",
        "MS",
        "Access",
        "Profound",
        "experience",
        "time",
        "data",
        "streaming",
        "solutions",
        "Apache",
        "SparkSpark",
        "Streaming",
        "Kafka",
        "components",
        "Spark",
        "Spark",
        "Stream",
        "prototype",
        "Apache",
        "Spark",
        "Streaming",
        "project",
        "Java",
        "Strom",
        "Topology",
        "Proficient",
        "data",
        "Tableau",
        "QlikView",
        "Microstratergy",
        "MS",
        "Excel",
        "Experience",
        "ETL",
        "scripts",
        "data",
        "acquisition",
        "transformation",
        "Informatica",
        "Talend",
        "Maven",
        "MapReduce",
        "jar",
        "files",
        "Amazon",
        "Web",
        "Services",
        "AWS",
        "EC2",
        "Servers",
        "cloud",
        "Experience",
        "build",
        "scripts",
        "integrations",
        "systems",
        "Jenkins",
        "Java",
        "Application",
        "Development",
        "ClientServer",
        "Applications",
        "InternetIntranet",
        "applications",
        "Core",
        "Java",
        "J2EE",
        "Spring",
        "Hibernate",
        "Struts",
        "JMS",
        "Web",
        "Services",
        "SOAPREST",
        "Oracle",
        "SQL",
        "Server",
        "databases",
        "Experience",
        "Shell",
        "scripts",
        "Linux",
        "OS",
        "solutions",
        "methodologies",
        "programming",
        "SCRUM",
        "Test",
        "Driven",
        "Development",
        "TDD",
        "Software",
        "Requirement",
        "Specifications",
        "SRS",
        "Functional",
        "Specification",
        "Document",
        "FSD",
        "knowledge",
        "Software",
        "Development",
        "Life",
        "Cycle",
        "SDLC",
        "professionalism",
        "ability",
        "deadline",
        "schedules",
        "attention",
        "details",
        "communication",
        "United",
        "States",
        "employer",
        "Work",
        "Experience",
        "Hadoop",
        "Developer",
        "Dell",
        "Austin",
        "TX",
        "October",
        "Present",
        "Responsibilities",
        "GIT",
        "Puppet",
        "integrity",
        "applications",
        "Production",
        "Development",
        "Test",
        "Release",
        "Branches",
        "customers",
        "data",
        "event",
        "Oracle",
        "database",
        "Teradata",
        "HDFS",
        "Sqoop",
        "Endtoend",
        "performance",
        "tuning",
        "Hadoop",
        "clusters",
        "Hadoop",
        "MapReduce",
        "routines",
        "data",
        "sets",
        "Performance",
        "tuning",
        "Hadoop",
        "clusters",
        "Hadoop",
        "MapReduce",
        "routines",
        "Python",
        "code",
        "data",
        "HBase",
        "solution",
        "PySpark",
        "Developed",
        "PySpark",
        "code",
        "transformations",
        "onpremise",
        "environment",
        "Sql",
        "scripts",
        "solutions",
        "pyspark",
        "custom",
        "columns",
        "use",
        "case",
        "data",
        "Hadoop",
        "lake",
        "pyspark",
        "Analyze",
        "Cassandra",
        "database",
        "opensource",
        "NoSQL",
        "requirement",
        "Integrated",
        "Cassandra",
        "metadata",
        "store",
        "metadata",
        "resolution",
        "network",
        "entities",
        "network",
        "Spark",
        "Scala",
        "Pyspark",
        "Python",
        "testing",
        "processing",
        "data",
        "Build",
        "servers",
        "AWS",
        "volumes",
        "EC2",
        "RDS",
        "security",
        "groups",
        "load",
        "balancers",
        "ELBs",
        "connection",
        "Apache",
        "Hadoop",
        "Hive",
        "Pig",
        "environment",
        "infrastructure",
        "network",
        "database",
        "application",
        "business",
        "intelligence",
        "teams",
        "data",
        "quality",
        "availability",
        "components",
        "Hadoop",
        "ecosystem",
        "integrity",
        "backup",
        "disaster",
        "recovery",
        "HDFS",
        "data",
        "LinuxWindows",
        "servers",
        "AWS",
        "Installing",
        "Upgrading",
        "Managing",
        "Hadoop",
        "Cluster",
        "Cloudera",
        "distribution",
        "Commissioned",
        "Data",
        "Nodes",
        "data",
        "hardware",
        "Development",
        "Acceptance",
        "Integration",
        "Production",
        "AWS",
        "Endpoints",
        "property",
        "files",
        "coresitexml",
        "hdfssitexml",
        "mapredsitexml",
        "job",
        "requirement",
        "Monitored",
        "multiple",
        "Hadoop",
        "clusters",
        "environments",
        "Ganglia",
        "Nagios",
        "Monitored",
        "workload",
        "job",
        "performance",
        "capacity",
        "Develop",
        "Hive",
        "scripts",
        "end",
        "user",
        "analyst",
        "requirements",
        "ad",
        "analysis",
        "Plan",
        "monitor",
        "Amazon",
        "AWS",
        "cloud",
        "infrastructure",
        "EC2",
        "nodes",
        "VMware",
        "Vms",
        "environment",
        "Expertise",
        "AWS",
        "data",
        "migration",
        "database",
        "platforms",
        "SQL",
        "Server",
        "Amazon",
        "Aurora",
        "RDS",
        "tool",
        "Upgrading",
        "Managing",
        "Hadoop",
        "Cluster",
        "Cloudera",
        "distribution",
        "Installed",
        "Hadoop",
        "HDFS",
        "MapReduce",
        "Pig",
        "Hive",
        "Sqoop",
        "Wrote",
        "Pig",
        "Scripts",
        "MapReduce",
        "jobs",
        "ETL",
        "procedures",
        "data",
        "HDFS",
        "data",
        "HDFS",
        "Sqoop",
        "reports",
        "hadoophivesqoopHbase",
        "jobs",
        "Oozie",
        "Environment",
        "Hadoop",
        "Map",
        "Reduce",
        "Shell",
        "Scripting",
        "spark",
        "Pig",
        "Hive",
        "Cloudera",
        "Manager",
        "AWSCDH",
        "543Pyspark",
        "HDFS",
        "Yarn",
        "Hue",
        "Sentry",
        "Oozie",
        "Zoo",
        "keeper",
        "Impala",
        "Solr",
        "Kerberos",
        "cluster",
        "health",
        "Puppet",
        "Ganglia",
        "Nagios",
        "Flume",
        "Sqoop",
        "storm",
        "Kafka",
        "KMS",
        "PythonJava",
        "Consultant",
        "Walmart",
        "Bentonville",
        "AR",
        "September",
        "October",
        "Responsibilities",
        "user",
        "interface",
        "guidelines",
        "standards",
        "development",
        "maintenance",
        "website",
        "HTML",
        "CSS",
        "JavaScript",
        "jQuery",
        "Developed",
        "GUI",
        "Django",
        "Python",
        "test",
        "block",
        "documentation",
        "features",
        "Python",
        "code",
        "web",
        "browser",
        "AJAX",
        "functionality",
        "webpages",
        "end",
        "applications",
        "features",
        "dashboard",
        "Bootstrap",
        "CSS",
        "JavaScript",
        "end",
        "Angularjs",
        "ReactJS",
        "Nodejs",
        "bootstrapjs",
        "backbonejs",
        "JavaScript",
        "end",
        "REST",
        "webservice",
        "application",
        "development",
        "Spring",
        "Core",
        "MVC",
        "modules",
        "Java",
        "web",
        "technologies",
        "Servlets",
        "JSP",
        "Java",
        "Web",
        "Service",
        "RESTSOAP",
        "WSDL",
        "Python",
        "modules",
        "csv",
        "robot",
        "parser",
        "iterators",
        "pickle",
        "development",
        "views",
        "templates",
        "Python",
        "Djangos",
        "controller",
        "templating",
        "language",
        "website",
        "interface",
        "Python",
        "OpenStack",
        "APIs",
        "NumPy",
        "analysis",
        "Developed",
        "Wrapper",
        "Python",
        "application",
        "datasets",
        "Panda",
        "data",
        "frames",
        "MySQL",
        "MYSQL",
        "database",
        "python",
        "PythonMySQL",
        "connector",
        "MySQL",
        "package",
        "information",
        "Ajax",
        "jQuery",
        "JSON",
        "data",
        "objects",
        "frontend",
        "controllers",
        "Angular",
        "MVC",
        "twoway",
        "data",
        "automation",
        "scripts",
        "Selenium",
        "JAVA",
        "frontend",
        "modules",
        "Django",
        "Tastypie",
        "Web",
        "Framework",
        "Git",
        "Designed",
        "modules",
        "project",
        "oops",
        "Install",
        "KAFKA",
        "Hadoop",
        "cluster",
        "configure",
        "producer",
        "consumer",
        "part",
        "connection",
        "twitter",
        "source",
        "HDFS",
        "hash",
        "tags",
        "Splunk",
        "infrastructure",
        "solution",
        "application",
        "toolsets",
        "data",
        "knowledge",
        "Splunk",
        "Apps",
        "Application",
        "Splunk",
        "data",
        "Manage",
        "Splunk",
        "configurations",
        "files",
        "input",
        "props",
        "lookups",
        "Configured",
        "Maven",
        "Java",
        "automation",
        "projects",
        "Maven",
        "Project",
        "Object",
        "Model",
        "POM",
        "Setup",
        "cron",
        "jobs",
        "data",
        "database",
        "graphs",
        "bar",
        "charts",
        "wiki",
        "database",
        "Software",
        "Development",
        "Linux",
        "Environment",
        "XShell",
        "Java",
        "applications",
        "Founctional",
        "Package",
        "Java",
        "Erlang",
        "Python",
        "options",
        "application",
        "algorithm",
        "data",
        "address",
        "generation",
        "versions",
        "GIT",
        "release",
        "notes",
        "release",
        "issues",
        "tool",
        "teams",
        "projects",
        "Environment",
        "Python",
        "Django",
        "HTML5CSS",
        "MS",
        "SQL",
        "Server",
        "MySQL",
        "JavaScript",
        "Eclipse",
        "Linux",
        "Shell",
        "Scripting",
        "PyCharm",
        "Urllib",
        "jQuery",
        "GitHub",
        "AngularJS",
        "Jira",
        "Sr",
        "Hadoop",
        "Developer",
        "CoxAuto",
        "New",
        "York",
        "NY",
        "August",
        "September",
        "Responsibilities",
        "Loading",
        "Java",
        "applications",
        "Hadoop",
        "MapReduce",
        "frameworks",
        "Cascading",
        "Hive",
        "Developed",
        "workflow",
        "Oozie",
        "Map",
        "Reduce",
        "jobs",
        "Hive",
        "Queries",
        "loading",
        "log",
        "data",
        "HDFS",
        "Flume",
        "Worked",
        "Cloudera",
        "data",
        "top",
        "data",
        "sources",
        "Load",
        "data",
        "data",
        "sources",
        "HDFS",
        "Flume",
        "requirement",
        "design",
        "phase",
        "Streaming",
        "Lambda",
        "Architecture",
        "time",
        "streaming",
        "Spark",
        "Kafka",
        "Created",
        "documentation",
        "Cloudera",
        "Hadoop",
        "Clusters",
        "Hive",
        "queries",
        "Pig",
        "Scripts",
        "SPARK",
        "engine",
        "batch",
        "jobs",
        "load",
        "STORM",
        "KAFKA",
        "FLUME",
        "HDFSS3",
        "KINESIS",
        "Sockets",
        "AWS",
        "scala",
        "classes",
        "database",
        "plugin",
        "Hadoop",
        "MapReduce",
        "programs",
        "HBase",
        "Pig",
        "Hive",
        "access",
        "files",
        "architecture",
        "premise",
        "datacenters",
        "applications",
        "onprem",
        "AWS",
        "Public",
        "Cloud",
        "MapReducebased",
        "largescale",
        "system",
        "files",
        "Hive",
        "HDFS",
        "Mongo",
        "DB",
        "Solar",
        "Familiarity",
        "NoSQL",
        "database",
        "MongoDb",
        "Solar",
        "files",
        "Hive",
        "HDFS",
        "Mongo",
        "DB",
        "Solar",
        "files",
        "MySQL",
        "Sqoop",
        "HDFS",
        "Collecting",
        "amounts",
        "log",
        "data",
        "Apache",
        "Flume",
        "data",
        "HDFS",
        "analysis",
        "Developed",
        "Pig",
        "Latin",
        "scripts",
        "data",
        "web",
        "server",
        "output",
        "files",
        "Hive",
        "UDF",
        "business",
        "requirements",
        "users",
        "UDFs",
        "Hive",
        "Querying",
        "performance",
        "tuning",
        "Hive",
        "Pig",
        "Jobs",
        "Created",
        "Hbase",
        "data",
        "formats",
        "PII",
        "data",
        "portfolios",
        "Developed",
        "Pig",
        "Scripts",
        "Pig",
        "UDFs",
        "Hive",
        "Scripts",
        "Hive",
        "UDFs",
        "data",
        "files",
        "Hadoop",
        "Worked",
        "computing",
        "Spark",
        "team",
        "RDD",
        "Datastax",
        "Cassandra",
        "Generating",
        "scala",
        "classes",
        "APIs",
        "application",
        "entities",
        "Scala",
        "Java",
        "queries",
        "database",
        "user",
        "console",
        "page",
        "lift",
        "snippets",
        "Scala",
        "product",
        "access",
        "user",
        "credentials",
        "privileges",
        "system",
        "scala",
        "test",
        "cases",
        "scala",
        "code",
        "Environment",
        "Python",
        "Django",
        "Hadoop",
        "HDFS",
        "Map",
        "Reduce",
        "Shell",
        "Scripting",
        "spark",
        "solr",
        "Pig",
        "Hive",
        "HBase",
        "Sqoop",
        "Flume",
        "Oozie",
        "Zoo",
        "keeper",
        "cluster",
        "health",
        "monitoring",
        "security",
        "Redhat",
        "Linux",
        "impala",
        "Cloudera",
        "ManagerGolang",
        "bitbucket",
        "pdb",
        "AWS",
        "Jira",
        "Jenkins",
        "Pyspark",
        "Rest",
        "Virtual",
        "Machine",
        "Ajax",
        "jQuery",
        "JavaScript",
        "LINUX",
        "Software",
        "Developer",
        "Cognizant",
        "Greenville",
        "SC",
        "July",
        "August",
        "Responsibilities",
        "Design",
        "test",
        "deploy",
        "website",
        "Interaction",
        "client",
        "expectations",
        "requirements",
        "UI",
        "website",
        "HTML",
        "AJAX",
        "CSS",
        "JavaScript",
        "frontend",
        "modules",
        "Python",
        "Django",
        "Web",
        "Framework",
        "Performed",
        "testing",
        "Djangos",
        "Test",
        "Module",
        "Perl",
        "developers",
        "development",
        "scripts",
        "integration",
        "system",
        "GIT",
        "Gerrit",
        "Jenkins",
        "MySQL",
        "custom",
        "tools",
        "Python",
        "Bash",
        "Managed",
        "dashboard",
        "control",
        "panel",
        "customers",
        "Administrators",
        "Django",
        "Oracle",
        "DB",
        "PostgreSQL",
        "modules",
        "requests",
        "urllib2",
        "web",
        "configuration",
        "changes",
        "data",
        "models",
        "Pandas",
        "library",
        "statistics",
        "Analysis",
        "NumPY",
        "Numerical",
        "analysis",
        "datasets",
        "Panda",
        "data",
        "frames",
        "MySQL",
        "ExportedImported",
        "data",
        "data",
        "sources",
        "SQL",
        "Server",
        "Management",
        "Studio",
        "program",
        "users",
        "manuals",
        "documentation",
        "web",
        "application",
        "data",
        "database",
        "site",
        "production",
        "GUI",
        "webapp2",
        "test",
        "block",
        "documentation",
        "features",
        "python",
        "code",
        "web",
        "browser",
        "Environment",
        "Python",
        "Django",
        "CSS",
        "HTML",
        "JavaScript",
        "JQuery",
        "webbapp2",
        "AJAX",
        "MYSQL",
        "Linux",
        "Heroku",
        "GIT",
        "Oracle",
        "DB",
        "PostgreSQL",
        "VMWare",
        "API"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T21:17:00.854596",
    "resume_data": "Hadoop Developer Hadoop span lDeveloperspan Hadoop Developer Dell Raleigh NC 7 years of professional experience involving project development implementation deployment and maintenance using JavaJ2EE and Big Data related technologies Hadoop Developer with 5 years of working experience in designing and implementing complete endtoendHadoop based data analytical solutions using HDFS MapReduce Spark Yarn Kafka PIG HIVE Sqoop Storm Flume Oozie Impala HBase etc Good experience in creating data ingestion pipelines data transformations data management data governance and real time streaming at an enterprise level Experience in importing and exporting different formats of data into HDFS HBASE from different RDBMS databases and vice versa using Sqoop Exposure to Cloudera development environment and management using Cloudera Manager Experience in analyzing data using HiveQL Pig Latin Hbase Mongo and custom MapReduce programs in Java Experience in extending Hive and Pig core functionality by writing custom UDFs using Java Developed analytical components using Spark and Spark Stream Background with traditional databases such as Oracle SQL Server MySQL Good knowledge and Handson experience in storing processing unstructured data using NOSQL databases like HBase and MongoDB Good knowledge in distributed coordination system ZooKeeper and experience with Data Warehousing and ETL Hands on experience in setting up workflow using Apache Oozie workflow engine for managing and scheduling Hadoop jobs Experience in creating complex SQL Queries and SQL tuning writing PLSQL blocks like stored procedures Functions Cursors Index triggers and packages Good knowledge of database connectivity JDBC for databases like Oracle DB2 SQL Server MySQL NoSQL MS Access Profound experience in creating real time data streaming solutions using Apache SparkSpark Streaming Kafka Developed analytical components using Spark and Spark Stream Worked on a prototype Apache Spark Streaming project and converted our existing Java Strom Topology Proficient in visualizing data using Tableau QlikView Microstratergy and MS Excel Experience in developing ETL scripts for data acquisition and transformation using Informatica and Talend Used Maven extensively for building MapReduce jar files and deployed it to Amazon Web Services AWS using EC2 virtual Servers in the cloud and Experience in build scripts to do continuous integrations systems like Jenkins Experienced in Java Application Development ClientServer Applications InternetIntranet based applications using Core Java J2EE patterns Spring Hibernate Struts JMS Web Services SOAPREST Oracle SQL Server and other relational databases Experience writing Shell scripts in Linux OS and integrating them with other solutions Experienced in using agile methodologies including extreme programming SCRUM and Test Driven Development TDD Experienced in creating and analyzing Software Requirement Specifications SRS and Functional Specification Document FSD Strong knowledge of Software Development Life Cycle SDLC Devoted to professionalism highly organized ability to work under strict deadline schedules with attention to details possess excellent written and communication skillsAuthorized to work in United States for any employer Work Experience Hadoop Developer Dell Austin TX October 2018 to Present Responsibilities Involved in integrating the GIT into the Puppet to ensure the integrity of applications by creating Production Development Test and Release Branches Responsible for loading the customers data and event logs from Oracle database Teradata into HDFS using Sqoop Endtoend performance tuning of Hadoop clusters and Hadoop MapReduce routines against very large data sets Performance tuning of Hadoop clusters and Hadoop MapReduce routines Developed Python code to gather the data from HBase and designs the solution to implement using PySpark Developed PySpark code to mimic the transformations performed in the onpremise environment Analyzed the Sql scripts and designed solutions to implement using pyspark created custom new columns depending up on the use case while ingesting the data into Hadoop lake using pyspark Analyze Cassandra database and compare it with other opensource NoSQL databases to find which one of them better suites the current requirement Integrated Cassandra as a distributed persistent metadata store to provide metadata resolution for network entities on the network Implemented Spark using Scala and also used Pyspark using Python for faster testing and processing of data Build servers using AWS Importing volumes launching EC2 RDS creating security groups autoscaling load balancers ELBs in the defined virtual private connection Installed and configured Apache Hadoop Hive and Pig environment on AWSDiligently teaming with the infrastructure network database application and business intelligence teams to guarantee high data quality and availability Installed and configured various components of Hadoop ecosystem and maintained their integrity Designed configured and managed the backup and disaster recovery for HDFS data Migrating physical LinuxWindows servers to cloud AWS and testing Installing Upgrading and Managing Hadoop Cluster on Cloudera distribution Commissioned Data Nodes when data grew and decommissioned when the hardware degraded Development Acceptance Integration and Production AWS Endpoints Configured various property files like coresitexml hdfssitexml mapredsitexml based upon the job requirement Monitored multiple Hadoop clusters environments using Ganglia and Nagios Monitored workload job performance and capacity planning Develop Hive scripts for end user analyst requirements to perform ad hoc analysis Plan deploy monitor and maintain Amazon AWS cloud infrastructure consisting of multiple EC2 nodes and VMware Vms as required in the environment Expertise in AWS data migration between different database platforms like SQL Server to Amazon Aurora using RDS tool Installing Upgrading and Managing Hadoop Cluster on Cloudera distribution Installed and configured Hadoop HDFS MapReduce Pig Hive and Sqoop Wrote Pig Scripts to generate MapReduce jobs and performed ETL procedures on the data in HDFS Exported analyzed data to HDFS using Sqoop for generating reports Scheduling all hadoophivesqoopHbase jobs using Oozie Environment Hadoop Map Reduce Shell Scripting spark Pig Hive Cloudera Manager AWSCDH 543Pyspark HDFS Yarn Hue Sentry Oozie Zoo keeper Impala Solr Kerberos cluster health Puppet Ganglia Nagios Flume Sqoop storm Kafka KMS PythonJava Consultant Walmart Bentonville AR September 2016 to October 2018 Responsibilities Implemented user interface guidelines and standards throughout the development and maintenance of the website using the HTML CSS JavaScript and jQuery Developed GUI using Django and Python for dynamically displaying the test block documentation and other features with Python code for a web browser Implemented AJAX for dynamic functionality of a webpages for front end applications Developed and tested many features for dashboard created using Bootstrap CSS and JavaScript Developed front end using Angularjs ReactJS Nodejs bootstrapjs backbonejs JavaScript where back end is java with REST webservice Involved in the application development using Spring Core MVC modules and Java web based technologies such as Servlets JSP Java Web Service RESTSOAP based WSDL Utilized standard Python modules such as csv robot parser iterators and pickle for development Developed views and templates with Python and Djangos view controller and templating language to create userfriendly website interface Worked on Python OpenStack APIs and used NumPy for Numerical analysis Developed Wrapper in Python for instantiating multithreaded application Managed datasets using Panda data frames and MySQL queried MYSQL database queries from python using PythonMySQL connector and MySQL dB package to retrieve information Used Ajax and jQuery for transmitting JSON data objects between frontend and controllers Used Angular MVC and twoway data binding Worked on automation scripts using Selenium in JAVA Developed entire frontend and backend modules using on Django including Tastypie Web Framework using Git Designed coded and tested key modules of the project using java oops concepts Install KAFKA on Hadoop cluster and configure producer and consumer coding part in java to establish connection from twitter source to HDFS with popular hash tags Developed Splunk infrastructure and related solution for application toolsets Helped team to onboard data create various knowledge objects Install and maintain the Splunk Apps Creating Application on Splunk to analyse the data Manage Splunk configurations files like input props transforms and lookups Configured Maven for Java automation projects and developed Maven Project Object Model POM Setup automated cron jobs to upload data into database generate graphs bar charts upload these chartsto wiki and backup the database Software Development in Linux Environment utilized XShell to build deploy Java applications Developed Founctional Package with Java Erlang and Python Added several options to the application to choose particular algorithm for data and address generation Maintained the versions using GIT and sending the release notes for each release Supported the issues seen in the tool across all the teams for several projects Environment Python 32 Django HTML5CSS MS SQL Server 2013 MySQL JavaScript Eclipse Linux Shell Scripting PyCharm Urllib jQuery GitHub AngularJS Jira Sr Hadoop Developer CoxAuto New York NY August 2014 to September 2016 Responsibilities Loading Defined designed and developed Java applications specially using Hadoop MapReduce by leveraging frameworks such as Cascading and Hive Developed workflow using Oozie for running Map Reduce jobs and Hive Queries Worked on loading log data directly into HDFS using Flume Worked on Cloudera to analyze data present on top of HDFS Responsible for managing data from multiple sources Load data from various data sources into HDFS using Flume Involved in requirement and design phase to implement Streaming Lambda Architecture to use real time streaming using Spark and Kafka Created and maintained Technical documentation for launching Cloudera Hadoop Clusters and for executing Hive queries and Pig Scripts Worked on SPARK engine creating batch jobs with incremental load through STORM KAFKA SPLUNK FLUME HDFSS3 KINESIS Sockets AWS etc Writing scala classes to interact with the database This plugin allows Hadoop MapReduce programs HBase Pig and Hive to work unmodified and access files directly Involved in analyzing existing architecture on premise datacenters and designed to migrate applications from onprem to AWS Public Cloud Designed and implemented MapReducebased largescale parallel relationlearning system Successfully loaded files to Hive and HDFS from Mongo DB Solar Familiarity with a NoSQL database such as MongoDb Solar Successfully loaded files to Hive and HDFS from Mongo DB Solar Extracted files from MySQL through Sqoop and placed in HDFS and processed Collecting and aggregating large amounts of log data using Apache Flume and staging data in HDFS for further analysis Developed Pig Latin scripts to extract data from the web server output files to load into HDFS Built reusable Hive UDF libraries for business requirements which enabled users to use these UDFs in Hive Querying Worked on debugging performance tuning of Hive Pig Jobs Created Hbase tables to store various data formats of PII data coming from different portfolios Developed Pig Scripts Pig UDFs and Hive Scripts Hive UDFs to load data files into Hadoop Worked closely on parallel computing with Spark team to explore RDD in Datastax Cassandra Generating scala and java classes from the respective APIs so that they can be incorporated in the overall application Writing entities in Scala and Java along with named queries to interact with database Writing user console page in lift along with the snippets in Scala The product is responsible to give access to the user to all their credentials and privileges within the system Writing scala test cases to test scala written code Environment Python 3 Django Hadoop HDFS Map Reduce Shell Scripting spark solr Pig Hive HBase Sqoop Flume Oozie Zoo keeper cluster health monitoring security Redhat Linux impala Cloudera ManagerGolang bitbucket pdb AWS Jira Jenkins dockers Pyspark Rest Virtual Machine Ajax jQuery JavaScript LINUX Software Developer Cognizant Greenville SC July 2012 to August 2014 Responsibilities Design develop test deploy and maintain the website Interaction with client to understand expectations and requirements Designed and developed the UI of the website using HTML AJAX CSS and JavaScript Developed entire frontend and backend modules using Python on Django Web Framework Performed testing using Djangos Test Module Assisting junior Perl developers in the development of scripts Developed a fully automated continuous integration system using GIT Gerrit Jenkins MySQL and custom tools developed in Python and Bash Managed developed and designed a dashboard control panel for customers and Administrators using Django Oracle DB PostgreSQL Extensively used python modules such as requests urllib urllib2 for web crawling Implemented configuration changes for data models Used Pandas datamining library for statistics Analysis NumPY for Numerical analysis Managed large datasets using Panda data frames and MySQL ExportedImported data between different data sources using SQL Server Management Studio Maintained program libraries users manuals and technical documentation Responsible for debugging and troubleshooting the web application Successfully migrated all the data to the database while the site was in production Developed GUI using webapp2 for dynamically displaying the test block documentation and other features of python code using a web browser Environment Python 27 Django 18 CSS HTML JavaScript JQuery webbapp2 AJAX MYSQL Linux Heroku GIT urllib urllib2 Oracle DB PostgreSQL and VMWare API",
    "unique_id": "d095e6a0-8341-435b-b7c2-e29e3eb6d4b6"
}