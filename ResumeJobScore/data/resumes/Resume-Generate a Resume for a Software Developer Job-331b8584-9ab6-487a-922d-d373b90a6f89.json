{
    "clean_data": "Data Scientist Data Scientist Data Scientist BBVA Compass Birmingham AL Over 8 years of Experience in Data Architecture Design Developmentand Testing of business application systems Data Analysis and developing Conceptual logical models and physical database design for Online Transactional processing OLTP and Online Analytical Processing OLAP systems Experienced in designing star schema Snowflake schema for Data Warehouse and ODS architecture Experienced in Data Modeling Data Analysis experience using Dimensional Data Modeling and Relational Data Modeling Star SchemaSnowflake Modeling FACT Dimensions tables Physical Logical Data Modeling Experienced in big data analysis and developing data models using Hive PIG and Map reduce SQL with strong data architecting skills designing datacentric solutions Hands on experience with big data tools like Hadoop Spark Hive Pig Impala PySpark SparkSQL Very good knowledge and experience on AWS Redshift  Proficient in Data Analysis mapping source and target systems for datamigration efforts and resolving issues relating to data migration Excellent development experience SQL Procedural LanguagePL of databases like Oracle TeradataNetezzaandDB2 Very good knowledge and working experience on big data tools like Hadoop Azure Data Lake AWS Redshift Experienced in Data ScrubbingCleansing Data Quality Data Mapping Data Profiling Data Validation in ETL Experienced in creating and documenting Metadata for OLTP and OLAP when designing systems Expertise in synthesizing Machine learning Predictive Analytics and Big data technologies into integrated solutions Extensive experience in development of TSQL DTS OLAP PLSQL Stored Procedures Triggers Functions Packages performance tuning and optimization for business logic implementation Experience in using various packages in Rand python like ggplot2 caret dplyr Rweka gmodels RCurl tm C50 twitter NLP Reshape2 rjson dplyr pandas NumPy Seaborn SciPy matplotlib Scikitlearn Beautiful Soup Rpy2 Experienced using query tools like SQL Developer PLSQL Developer and Teradata SQL Assistant Excellent knowledge of Machine Learning Mathematical Modeling and Operations Research Comfortable with R Python SAS and Weka MATLAB Relational databases Deep understanding exposure of Big Data Ecosystem Expertise in designing complex Mappings and have expertise in performance tuning and slowlychanging Dimension Tables and Fact tables Extensively worked with Teradata utilities BTEQ Fast export and MultiLoad to export and load data tofrom different source systems including flat files Hands on experience in implementing LDA Naive Bayes and skilled in Random Forests Decision Trees Linear and Logistic Regression SVM Clustering neural networks Principle Component Analysis Expertise in extracting transforming and loading data between homogeneous and heterogeneous systems like SQL Server Oracle DB2 MS Access Excel Flat File and etc using SSIS packages Proficient in System Analysis ERDimensional Data Modeling Database design and implementing RDBMS specific features Experience in UNIX shell scripting Perl scriptingand automation of ETL Processes Extensively used ETL to load data using Power Center Power Exchange from source systems like Flat Files and Excel Files into staging tables and load the data into the target database Oracle Analyzed the existing systems and made a Feasibility Study Excellent understanding and working experience of industry standard methodologies like System Development Life Cycle SDLC as per Rational Unified Process RUP Agile Methodologies Proficiency in SQL across a number of dialects we commonly write MySQL PostgreSQL Redshift SQL Server and Oracle Experienced in developing EntityRelationship diagrams and modeling Transactional Databases and Data Warehouse using tools like ERWIN ERStudioandPower Designer and experienced with modeling using ERWIN in both forward and reverse engineering cases Authorized to work in the US for any employer Work Experience Data Scientist BBVA Compass Birmingham AL July 2017 to Present Architect framework BBVA Compass Bancshares Inc Birmingham AL 2007 to Present is a bank holding company headquartered in Birmingham Alabama It has been a subsidiary of the Spanish multinational Banco Bilbao Vizcaya Argentaria since 2007 and operates chiefly in Alabama Arizona California Colorado Florida New Mexico and Texas Responsibilities Design Develop and implement Comprehensive Data Warehouse Solution to extract clean transfer load and manage qualityaccuracy of data from various sources to EDW Enterprise Data Warehouse Architect framework for data warehouse solutions to bring data from source system to EDW and provide data mart solutions for OrderSales operation Salesforce activity Inventory tracking indepth data mining and analysis for market projection etc Utilized Apache Spark with Python to develop and execute Big Data Analytics and Machine learning applications executed machine learning use cases under Spark ML and  Handled importing data from various data sources performed transformations using Hive Map Reduce and loaded data into HDFS Performed data cleaning and feature selection using  package in PySpark and working with deep learning frameworks such as Caffe Neon Tested Complex ETL Mappings and Sessions based on business user requirements and business rules to load data from source flat files and RDBMS tables to target tables Utilized Spark Scala Hadoop HBase Cassandra MongoDB Kafka Spark Streaming  Python a broad variety of machine learning methods including classifications regressions dimensionally reduction etc and Utilized the engine to increase user lifetime by 45 and triple user conversations for target categories Develop a high performance scalable data architecture solution that incorporates a matrix of technology to relate architectural decision to business needs Conducting strategy and architecture sessions and deliver artifacts such as MDM strategy Current state Interim Stateand Target state and MDM Architecture Conceptual Logical and Physical at the detail level Design and development of dimensional data model on Redshift to provide advanced selection analytics platform and developed Simple to complex Map Reduce Jobs using Hive and Pig Designed and developed NLP models for sentiment analysis Designed and provisioned the platform architecture to execute Hadoop and machine learning use cases under Cloud infrastructure AWS EMR and S3 Developed and configured on Informatica MDM hub supports the Master Data Management MDM Business Intelligence BI and Data Warehousing platforms to meet business needs Transforming staging area data into a STAR schema hosted on Amazon Redshift which was then used for developing embedded Tableau dashboards Worked on machine learning on large size data using Spark and MapReduce Let the implementation of new statistical algorithms and operators on Hadoop and SQL platforms and utilized optimizations techniques linear regressions Kmeans clustering NativeBayes and other approaches Developed SparkScala Python for regular expression regex project in the HadoopHive environment with LinuxWindows for big data resources Proficiency in SQL across a number of dialects we commonly write MySQL PostgreSQL Redshift Teradata and Oracle Responsible for full data loads from production to AWSRedshift staging environment and Worked on migrating of EDW to AWS using EMR and various other technologies Worked on TeradataSQLqueries Teradata Indexes Utilities such as Mload TPump Fast load and Fast Export Application of various machine learning algorithms and statistical modeling like decision trees regression models neural networks SVM clustering to identify Volume using Scikitlearn package in python MATLAB Worked on data preprocessing and cleaning the data to perform feature engineering and performed data imputation techniques for the missing values in the dataset using Python Created Data Quality Scripts using SQL and Hive to validate successful data load and quality of the data Created various types of data visualizations using Python and Tableau Worked with BTEQ to submit SQL statements import and export data and generate reports in Teradata Build and maintain scalable data pipelines using the Hadoop ecosystem and other open sources components like Hive and HBase Created Hive architecture used for realtime monitoring and HBase used for reporting and worked for map reduce and query optimization for Hadoop Hive and HBase architecture Involved in Teradata utilities BTEQ Fast Load Fast Export Multiload and TPump in both Windows and Mainframe platforms Built analytical data pipelines to port data in and out of HadoopHDFS from structured and unstructured sources and designed and implemented system architecture for Amazon EC2 based cloudhosted solution for the client Environment Erwin964 Oracle 12c Python PySpark Spark Spark  Tableau ODS PLSQL OLAP OLTP AWS Hadoop Map Reduce HDFS Python MDM Teradata 15 Hadoop Spark Cassandra SAP MS Excel Flat files Tableau Informatica SSIS SSRS AWS EC2 AWS EMR Elastic Search Data Scientist BBVA Compass Bancshares Inc Chicago IL August 2016 to June 2017 DescriptionAt AIM Specialty Health AIM it is our mission to promote appropriate safe and affordable health care As the specialty benefits management partner of choice for todays leading healthcare organizations we help improve the quality of care and reduce costs for todays most complex tests and treatments Responsibilities Developed applications of MachineLearning Statistical Analysisand Data Visualizations with challenging data Processing problems in sustainability and biomedical domain Worked on Natural Language Processing with NLTK module of python for application development for automated customer response Used predictive modeling with tools in SAS SPSS R Python Responsible for design and development of advanced R Python programs to prepare to transform and harmonize data sets in preparation for modeling Identifying and executing process improvements handson in various technologies such as Oracle Informatica and BusinessObjects Handled importing data from various data sources performed transformations using Hive Map Reduce and loaded data into HDFS Interaction with Business Analyst SMEsand other Data Architects to understand Business needs and functionality for various project solutions Created SQL tables with referential integrity and developed queries using SQL SQLPLUSand PLSQL Involved with Data Analysis primarily Identifying Data Sets Source Data Source Meta Data Data Definitions and Data Formats Wrote simple and advanced SQL queries and scripts to create standard and Adhoc reports for senior managers Created PLSQL packages and Database Triggers and developed user procedures and prepared user manuals for the new programs Prepare ETLarchitect design document which covers ETLarchitect SSISdesign Extraction transformationand loading of Duck Creek data into the dimensional model Design Logical Physical Data Model using MSVisio 2003 data modeler tool Participated in Architect solution meetings guidance in Dimensional Data Modeling design Applied linear regression multiple regression ordinary least square method meanvariance the theory of large numbers logistic regression dummy variable residuals Poisson distribution Bayes Naive Bayes fitting function etc to data with help of Scikit SciPy NumPy and Pandas module of Python Applied clustering algorithms ie Hierarchical Kmeans with help of Scikit and SciPy Developed visualizations and dashboards using ggplot Tableau Worked on development of data warehouse Data Lake and ETL systems using relational and nonrelational tools like SQL No SQL Built and analyzed datasets using R SAS MATLABand Python in decreasing order of usage Applied linear regression in Python and SAS to understand the relationship between different attributes of the dataset and causal relationship between them Pipelined ingestcleanmungetransform data for feature extraction toward downstream classification Used ClouderaHadoopYARN to perform analytics on data in Hive Wrote Hive queries for data analysis to meet the business requirements Expertise in BusinessIntelligence and data visualization using R and Tableau Validated the MacroEconomic data eg BlackRock Moodys etc and predictive analysis of world markets using key indicators in Python and machine learning concepts like regression Bootstrap Aggregation and Random Forest Worked in largescale database environments like Hadoop and MapReduce with working mechanism of Hadoop clusters nodes and Hadoop Distributed File System HDFS Interfaced with largescaledatabase system through an ETL server for data extraction and preparation Identified patterns data quality issues and opportunities and leveraged insights by communicating opportunities with business partners EnvironmentMachine learning AWS MS Azure Cassandra Spark HDFS Hive Pig Linux Python ScikitLearnSciPyNumPyPandas R SAS SPSS MySQL Eclipse PLSQL SQL connector Tableau Python Developer Walgreens Chicago IL March 2015 to July 2016 DescriptionThe Walgreen Company is an American company that operates as the secondlargest pharmacy store chain in the United States behind CVS Health It specializes in filling prescriptions health and wellness products health information and photo services Responsibilities Designed and Developed UI for creating Dashboard application using AngularJS D3 C3 HTML CSS Bootstrap JavaScript and jQuery Developed and implemented Python scripts to automate retrieval parsing and reporting of configuration parameters from Network Devices connected to customer networks Involved in user interface design and development using JSP Servlet HTML5  Wrote and tested Python scripts to create new data files for Linux server configuration using a Python template tool Modified controlling databases using SQL generated via Python and Perl code collected and analyzed data with Python programs using SQL queries from the database of data collected from the systems under tests Developed new user interface components for different modules using Kendo UI with various controls including Grid controls and chart controls etc Involved in write application level code to interact with APIs Web Services using AJAX JSON and hence building typeahead feature for zip code city and county lookup using jQuery Ajax and jQuery UI Worked on updating the existing clipboard to have the new features as per the client requirements Skilled in using collections in Python for manipulating and looping through different userdefined objects Taken part in the entire lifecycle of the projects including Design Development and Deployment Testing and Implementation and support Developed views and templates with Python and Djangos view controller and templating language to create a userfriendly website interface Automated different workflows which are initiated manually with Python scripts and Unix shell scripting Used Python unit and functional testing modules such as unit test unittest2 mock and custom frameworks inline with Agile Software Development methodologies Wrote and executed various MYSQL database queries from Python using PythonMySQL connector and MySQL dB package Created and maintained Technical documentation for launching Hadoop Clusters and for executing Hive queries and Pig Scripts Developed Sqoop scripts to handle change data capture for processing incremental records between new arrived and existing data in RDBMS tables Installed Hadoop Map Reduce HDFS AWS and developed multiple MapReduce jobs in PIG and Hive for data cleaning and preprocessing Managed datasets using Panda data frames and MySQL queried MYSQL database queries from python using PythonMySQL connector and MySQL dB package to retrieve information Involved in the WebApplication development using Python HTML5 CSS3 AJAX JSONandjQuery Developed and tested many features for a dashboard using Python Java Bootstrap CSS JavaScript and jQuery Generated Python Django forms to record data of online users and used PyTest for writing test cases Implemented and modified various SQL queries and Functions Cursors and Triggers as per the client requirements Prototype proposal for Issue Tracker website using PythonDjango connecting MySQL as Database The developed overall layout that meetscrossplatform compatibility using Bootstrap media queries and Angular UI Bootstrap Environment Python HTML5 CSS3 AJAX JSON jQuery MySQL NumPy SQL Alchemy Matplotlib Hadoop Pig Scripts Python Developer Citi Bank Irving TX July 2013 to February 2015 DescriptionThe project was to build an algorithm that accurately classifies credit card holders among multiple classes based on the historical data available on multiple variables Further the aim was to improve banks efficiency by reducing default rate while offering new products Moreover I was Involved in a project to identify the employees access level based on hisher current historical tasks and duties Responsibilities Involved in the software development lifecycle SDLC of tracking the requirements gathering analysis detailed design development system testing and user acceptance testing Developed entire frontend and backend modules using Python on Django Web Framework Involved in designing user interactive web pages as the frontend part of the web application using various web technologies like HTML JavaScript Angular JS jQuery AJAX and implemented CSS for better appearance and feel Knowledge of crossbrowser and crossplatform development of HTML and CSS based websites in Windows like IE 6 IE 7 IE 8 and FF Interactive in providing change requests trouble reports and requirements collection with the client Actively involved in developing the methods for Create Read Update and Delete CRUD in Active Record Working knowledge of various AWS technologies like SQS Queuing SNS Notification S3 storage Redshift Data Pipeline EMR Developed a fully automated continuous integration system using Git Jenkins MySQL and custom tools developed in Python and Bash Implemented Multithreading module and complex networking operations like race route SMTP mail server and web server Using Python Used NumPy for Numerical analysis for the Insurance premium Implemented and modified various SQL queries and Functions Cursors and Triggers as per the client requirements Managed code versioning with GitHub BitBucketand deployment to staging and production servers Implemented MVC architecture in developing the web application with the help of Django framework Used Celery as task queue and Rabbit MQ Redis as messaging broker to execute asynchronous tasks Designed and managed API system deployment using a fastHTTP server and Amazon AWS architecture Involved in code reviews using GitHub pull requests reducing bugs improving code quality and increasing knowledge sharing Install and configuring monitoring scripts for AWS EC2 instances Working under UNIX environment in the development of application using Python and familiar with all its commands Developed remote integration with thirdparty platforms by using RESTful web services Updated and maintained Jenkins for automatic building jobs and deployment Improved code reuse and performance by making effective use of various design patterns and refactoring code base Updated and maintained Puppet RSpec unitsystem test Worked on debugging and troubleshooting programming related issues Worked in the MySQL database on simple queries and writing Stored Procedures for normalization Deployment of the web application using the Linux server Environment Python 27 Django 14 HTML5 CSS XML MySQL JavaScript Backbone JS JQuery Mongo DB MS SQL Server JavaScript Git GitHub AWS Linux Shell Scripting AJAX JAVA Hadoop Developer SITEL India Pvt LTD Hyderabad Telangana August 2012 to June 2013 Description Sitel Group combines comprehensive customer care capabilities with unparalleled digital training and technology expertise to help build brand loyalty and improve customer satisfaction We partner with our clients to effectively harness our industrys explosive digital transformation to ensure an innovative endtoend solution to managing and enhancing the customer experience Responsibilities Designed and developed multiple MapReduce jobs in Java for complex analysis Importing and exporting the data using Sqoop from HDFS to Relational Database systems and viceversa Integrated Apache Storm with Kafka to perform web analytics Uploaded clickstream data from Kafka to HDFS HBase and Hive by integrating with Storm Configured Flume to transport web server logs into HDFS Also used Kite logging module to upload web server logs into HDFS Developed UDF functions for Hive and wrote complex queries in Hive for data analysis Performed Installation of Hadoop in fully and Pseudo Distributed Mode for POC in early stages of the project Analyze develop integrate and then direct the operationalization of new data sources Generating Scala and Java classes from the respective APIs so that they can be incorporated into the overall application Responsible for working with different teams in building Hadoop Infrastructure Gathered business requirements in meetings for successful implementation and POC and moving it to Production and implemented POC to migrate map reduce jobs into Spark RDD transformations using Scala Implemented different machine learning techniques in Scala using Scala machine learning library Developed Spark applications using Scala for easy Hadoop transitions Successfully loaded files to Hive and HDFS from Oracle Netezza and SQL Server using SQOOP Uses Talend Open Studio to load files into Hadoop HIVE tables and performed ETL aggregations in Hadoop Hive Developed Simple to Quebec and Python MapReduce streaming jobs using Python language that is implemented using Hive and Pig Designing Creating ETL Jobs through Talend to load huge volumes of data into Cassandra Hadoop Ecosystem and relational databases Worked on analyzing writing Hadoop MapReduce jobs using Java API Pig and Hive Developed some machine learning algorithms using Mahout for data mining for the data stored in HDFS Used Flume extensively in gathering and moving log data files from Application Servers to a central location in Hadoop Distributed File System HDFS Worked with Oozie Workflow manager to schedule Hadoop jobs and high intensive jobs Responsible for cluster maintenance adding and removing cluster nodes cluster monitoring and troubleshooting manage and review data backups manage and review Hadoop log files Extensively used HiveHQL or Hive queries to query data in Hive Tables and loaded data into HIVE tables Creating UDF functions in Pig Hive and applying partitioning and bucketing techniques in Hive for performance improvement Creating indexes and tuning the SQL queries in Hive and Involved in database connection by using Sqoop Involved in Hadoop Name node metadata backups and load balancing as a part of Cluster Maintenance and Monitoring Used File System Check FSCK to check the health of files in HDFS and used Sqoop to import data from SQL server to Cassandra Monitored Nightly jobs to export data out of HDFS to be stored offsite as part of HDFS backup Used Pig for analysis of large datasets and brought data back to HBaseby Pig Developed Python Mapper and Reducer scripts and implemented them using Hadoop streaming Created schema and database objects in HIVE and developed Unix Scripts for data loading and automation Involved in the training of big data ecosystem to endusers Environment Java15 J2EE Hibernate Spring JUnit WebLogic HTML CSS JavaScript Jenkins Nodejs jQuery Linux CICD Spring Boot Maven Log4J and Junit Eclipse REST SQL Navigator Java Developer Napier Healthcare Pvt Ltd Hyderabad Telangana October 2009 to July 2011 Description Napier Healthcare was established in 1996 as a Healthcare IT Products and Services Company With deep domain knowledge and singular focus we have built a robust featurerich suite of solutions to deliver value to the stakeholders of the healthcare industry worldwide Responsibilities Development Testing Maintenance and product delivery Developed a scalable and maintainable application using J2EE Framework Hibernate MVC Model Struts and J2EE Design Patterns Prepared SOW Statement of Work by communicating with agencies and organized meetings about requirements Followed Java J2EE design patterns and the coding guidelines to design and develop the application Extensively used JSTL tags and Struts tag libraries Used Struts tiles as well in the presentation tier Developing the application using Struts and Spring based frameworks Actively involved in designing and implementing the application using various design patterns Coordinating with clients and closing production issues relating to software development Identifying and Evaluate Technology Solutions Problem Solving and Troubleshooting Done with Serverside validations using Struts Validation framework Processed JSON response data by consuming RESTful web services and used an Angular filter for implementing search results Used Strutsconfigxml file for defining Mapping Definitions and Action Forward Definitions Developed the Action Classes which is in between view and model layers Action Form Classes which is used to maintain session state of a web application created JSPs Java Server pages using Struts tag libraries and configured in strutsconfigXML webxml files This application is designed using MVC architecture to maintain easily Hibernate is used for database connectivity and designed HQL Hibernate Query language to create modify and update the tables Created new Soap services using JAXWS specifications Wrote JUnit test cases for testing EnvironmentJava Struts Hibernate JSP Servlets SOAP UI HTML CSS Java Script JUnit Apache Tomcat Server EJB NetBeans Education Bachelors Skills PYTHON 10 years MYSQL 10 years ORACLE 10 years PLSQL 10 years SQL 10 years Additional Information TECHNICAL SKILLS Python Libraries Beautiful Soup NumPy SciPy Matplotlib pythontwitter Pandas data frame urllib2 Data Analytics ToolsProgramming Python NumPy SciPy pandas Gensim Keras R Caret Weka ggplot MATLAB Microsoft SQL Server Oracle PLSQL Python Database Oracle11g MySQL 5x and SQLServer Version Control SVN Clear case CVS Reporting Tools MS Office  VisioOutlook Crystal Reports XI SSRS Cognos 7060 BI Tools Microsoft Power BI Tableau SSIS SSRS SSAS Business Intelligence Development Studio BIDS Visual Studio Crystal Reports Informatica 61 Database Design Tools and Data Modeling MS Visio ERWIN 4540 Star SchemaSnowflake Schema modeling Fact Dimensions tables physical logical data modeling Normalization and Denormalization techniques Kimball Inmon Methodologies IDEs PyCharm Emacs Eclipse NetBeans Sublime SOAP UI WebApp Servers WebSphere Application Server 80 Apache Tomcat Web Logic 11g 12c JBoss 4x5x CloudTechnologies AWS S3",
    "entities": [
        "Hadoop Clusters",
        "Present Architect",
        "Bash Implemented Multithreading",
        "Network Devices",
        "Fact Dimensions",
        "Machine Learning Mathematical Modeling and Operations Research Comfortable",
        "C50",
        "Informatica",
        "BI Tools",
        "Strutsconfigxml",
        "Transactional Databases",
        "Oozie Workflow",
        "HDFS",
        "UNIX",
        "BBVA Compass Bancshares Inc Birmingham AL",
        "AJAX JSON",
        "Prototype",
        "Developed Spark",
        "HQL Hibernate Query",
        "Working",
        "Flat Files",
        "ETL Processes",
        "Data Lake",
        "Fast Export Application",
        "SQOOP Uses Talend Open Studio",
        "CloudTechnologies AWS S3",
        "Python Applied",
        "Installed Hadoop Map Reduce HDFS AWS",
        "Panda",
        "Design Development and Deployment Testing and Implementation",
        "Modeling Database",
        "NetBeans Sublime SOAP UI",
        "Additional Information TECHNICAL SKILLS Python Libraries Beautiful Soup NumPy",
        "EnvironmentJava Struts Hibernate JSP Servlets",
        "Hadoop",
        "Sqoop Involved",
        "Telangana",
        "Integrated Apache Storm",
        "Hadoop Distributed File System",
        "Hadoop Azure Data Lake AWS Redshift Experienced",
        "JAXWS",
        "JUnit",
        "J2EE Framework Hibernate",
        "Oracle Informatica",
        "HBase",
        "Rand",
        "Description Sitel Group",
        "MySQL PostgreSQL Redshift",
        "Amazon",
        "RCurl",
        "Hive PIG",
        "Spark ML",
        "SSIS",
        "OrderSales operation Salesforce",
        "ETL Experienced",
        "Hadoop Distributed File System HDFS Interfaced",
        "Python",
        "ETLarchitect SSISdesign Extraction",
        "Python HTML5 CSS3",
        "SQL Server",
        "Dimension Tables and Fact",
        "DescriptionThe Walgreen Company",
        "Developed",
        "FF Interactive",
        "Stored Procedures",
        "Mload TPump Fast",
        "Data Warehouse",
        "SMEsand",
        "Created new Soap",
        "Utilized",
        "Prepare ETLarchitect",
        "Amazon Redshift",
        "Healthcare",
        "Hadoop MapReduce",
        "Windows",
        "HDFS Performed",
        "HBase Created Hive",
        "Hive Wrote Hive",
        "AWS Redshift  Proficient",
        "SQLServer Version",
        "SQL Developer PLSQL Developer and Teradata",
        "LDA Naive Bayes",
        "jQuery Developed",
        "JSP",
        "Application Servers",
        "Predictive Analytics",
        "Colorado",
        "Utilized Apache Spark",
        "Alabama",
        "Struts Validation",
        "New Mexico",
        "HDFS Interaction with Business Analyst",
        "Adhoc",
        "Banco Bilbao Vizcaya Argentaria",
        "Utilized Spark",
        "ERWIN",
        "Online Transactional",
        "NativeBayes",
        "Identifying and Evaluate Technology Solutions Problem Solving",
        "BusinessObjects",
        "Mapping Definitions and Action Forward Definitions Developed the Action Classes",
        "MVC",
        "Spark",
        "PythonDjango",
        "Cassandra Hadoop Ecosystem",
        "Mahout",
        "Design Logical Physical Data Model",
        "Scikit",
        "Storm Configured Flume",
        "Data ScrubbingCleansing Data Quality Data Mapping Data Profiling Data Validation",
        "AWSRedshift",
        "RSpec",
        "Hive Tables",
        "API",
        "Data Architecture Design Developmentand",
        "Creating UDF",
        "US",
        "Hadoop Hive",
        "Sqoop",
        "jQuery Ajax",
        "LinuxWindows",
        "MachineLearning Statistical Analysisand Data Visualizations",
        "HIVE",
        "Texas Responsibilities Design Develop",
        "Identifying Data Sets Source Data Source Meta Data Data Definitions",
        "SQL Procedural",
        "GitHub BitBucketand",
        "SQL Server Oracle DB2 MS Access Excel Flat File",
        "Data Architects",
        "Created",
        "CVS Health",
        "AWS",
        "Created SQL",
        "Power Center Power Exchange",
        "Florida",
        "Scala",
        "PySpark",
        "Online Analytical Processing",
        "BlackRock Moodys",
        "Big Data Analytics",
        "PIG",
        "MultiLoad",
        "HTML",
        "SAS",
        "California",
        "Action Form Classes",
        "SQL",
        "OLTP",
        "BusinessIntelligence",
        "GitHub",
        "Spark RDD",
        "Redshift",
        "Bootstrap",
        "NLP",
        "MacroEconomic",
        "Data Analysis",
        "Data Modeling Data Analysis",
        "Angular UI Bootstrap Environment Python HTML5 CSS3 AJAX JSON jQuery MySQL NumPy SQL Alchemy Matplotlib Hadoop",
        "Redshift Data Pipeline EMR Developed",
        "Principle Component Analysis Expertise",
        "Teradata Build",
        "the United States",
        "DescriptionThe",
        "Relational Data Modeling Star SchemaSnowflake Modeling FACT Dimensions",
        "SQS Queuing SNS Notification S3",
        "Hive",
        "Hadoop Infrastructure Gathered",
        "Amazon AWS",
        "Serverside",
        "Mappings",
        "MDM Architecture Conceptual Logical and Physical",
        "Pandas",
        "Normalization and Denormalization",
        "MDM",
        "EnvironmentMachine",
        "ETL",
        "Agile Software Development",
        "Kendo UI",
        "Python Created Data Quality Scripts",
        "Dimensional Data Modeling",
        "Big Data Ecosystem Expertise",
        "Oracle Netezza",
        "Transforming",
        "OLAP",
        "unittest2",
        "Impala",
        "Random Forests Decision Trees Linear and Logistic Regression SVM Clustering",
        "Djangos",
        "MSVisio 2003",
        "TPump",
        "urllib2 Data Analytics ToolsProgramming Python NumPy SciPy",
        "Bootstrap Aggregation",
        "Oracle Experienced",
        "Microsoft",
        "Hive and Pig Designing Creating ETL Jobs",
        "WebApplication",
        "SMTP",
        "Oracle Responsible",
        "Data Scientist Data Scientist Data",
        "Oracle Analyzed",
        "CSS",
        "Random Forest Worked",
        "Hadoop Spark",
        "System Development Life Cycle",
        "Performed Installation of Hadoop",
        "EDW",
        "Analyze",
        "Relational Database",
        "MapReduce",
        "Responsibilities Development Testing Maintenance",
        "Microsoft Power",
        "Data Warehousing",
        "Arizona",
        "Tableau",
        "CVS Reporting Tools MS Office",
        "WebSphere Application Server",
        "Oracle TeradataNetezzaandDB2 Very",
        "PyTest",
        "SVM",
        "the Master Data Management MDM Business Intelligence BI",
        "Feasibility Study Excellent",
        "Designer"
    ],
    "experience": "Experience in Data Architecture Design Developmentand Testing of business application systems Data Analysis and developing Conceptual logical models and physical database design for Online Transactional processing OLTP and Online Analytical Processing OLAP systems Experienced in designing star schema Snowflake schema for Data Warehouse and ODS architecture Experienced in Data Modeling Data Analysis experience using Dimensional Data Modeling and Relational Data Modeling Star SchemaSnowflake Modeling FACT Dimensions tables Physical Logical Data Modeling Experienced in big data analysis and developing data models using Hive PIG and Map reduce SQL with strong data architecting skills designing datacentric solutions Hands on experience with big data tools like Hadoop Spark Hive Pig Impala PySpark SparkSQL Very good knowledge and experience on AWS Redshift   Proficient in Data Analysis mapping source and target systems for datamigration efforts and resolving issues relating to data migration Excellent development experience SQL Procedural LanguagePL of databases like Oracle TeradataNetezzaandDB2 Very good knowledge and working experience on big data tools like Hadoop Azure Data Lake AWS Redshift Experienced in Data ScrubbingCleansing Data Quality Data Mapping Data Profiling Data Validation in ETL Experienced in creating and documenting Metadata for OLTP and OLAP when designing systems Expertise in synthesizing Machine learning Predictive Analytics and Big data technologies into integrated solutions Extensive experience in development of TSQL DTS OLAP PLSQL Stored Procedures Triggers Functions Packages performance tuning and optimization for business logic implementation Experience in using various packages in Rand python like ggplot2 caret dplyr Rweka gmodels RCurl tm C50 twitter NLP Reshape2 rjson dplyr pandas NumPy Seaborn SciPy matplotlib Scikitlearn Beautiful Soup Rpy2 Experienced using query tools like SQL Developer PLSQL Developer and Teradata SQL Assistant Excellent knowledge of Machine Learning Mathematical Modeling and Operations Research Comfortable with R Python SAS and Weka MATLAB Relational databases Deep understanding exposure of Big Data Ecosystem Expertise in designing complex Mappings and have expertise in performance tuning and slowlychanging Dimension Tables and Fact tables Extensively worked with Teradata utilities BTEQ Fast export and MultiLoad to export and load data tofrom different source systems including flat files Hands on experience in implementing LDA Naive Bayes and skilled in Random Forests Decision Trees Linear and Logistic Regression SVM Clustering neural networks Principle Component Analysis Expertise in extracting transforming and loading data between homogeneous and heterogeneous systems like SQL Server Oracle DB2 MS Access Excel Flat File and etc using SSIS packages Proficient in System Analysis ERDimensional Data Modeling Database design and implementing RDBMS specific features Experience in UNIX shell scripting Perl scriptingand automation of ETL Processes Extensively used ETL to load data using Power Center Power Exchange from source systems like Flat Files and Excel Files into staging tables and load the data into the target database Oracle Analyzed the existing systems and made a Feasibility Study Excellent understanding and working experience of industry standard methodologies like System Development Life Cycle SDLC as per Rational Unified Process RUP Agile Methodologies Proficiency in SQL across a number of dialects we commonly write MySQL PostgreSQL Redshift SQL Server and Oracle Experienced in developing EntityRelationship diagrams and modeling Transactional Databases and Data Warehouse using tools like ERWIN ERStudioandPower Designer and experienced with modeling using ERWIN in both forward and reverse engineering cases Authorized to work in the US for any employer Work Experience Data Scientist BBVA Compass Birmingham AL July 2017 to Present Architect framework BBVA Compass Bancshares Inc Birmingham AL 2007 to Present is a bank holding company headquartered in Birmingham Alabama It has been a subsidiary of the Spanish multinational Banco Bilbao Vizcaya Argentaria since 2007 and operates chiefly in Alabama Arizona California Colorado Florida New Mexico and Texas Responsibilities Design Develop and implement Comprehensive Data Warehouse Solution to extract clean transfer load and manage qualityaccuracy of data from various sources to EDW Enterprise Data Warehouse Architect framework for data warehouse solutions to bring data from source system to EDW and provide data mart solutions for OrderSales operation Salesforce activity Inventory tracking indepth data mining and analysis for market projection etc Utilized Apache Spark with Python to develop and execute Big Data Analytics and Machine learning applications executed machine learning use cases under Spark ML and   Handled importing data from various data sources performed transformations using Hive Map Reduce and loaded data into HDFS Performed data cleaning and feature selection using   package in PySpark and working with deep learning frameworks such as Caffe Neon Tested Complex ETL Mappings and Sessions based on business user requirements and business rules to load data from source flat files and RDBMS tables to target tables Utilized Spark Scala Hadoop HBase Cassandra MongoDB Kafka Spark Streaming   Python a broad variety of machine learning methods including classifications regressions dimensionally reduction etc and Utilized the engine to increase user lifetime by 45 and triple user conversations for target categories Develop a high performance scalable data architecture solution that incorporates a matrix of technology to relate architectural decision to business needs Conducting strategy and architecture sessions and deliver artifacts such as MDM strategy Current state Interim Stateand Target state and MDM Architecture Conceptual Logical and Physical at the detail level Design and development of dimensional data model on Redshift to provide advanced selection analytics platform and developed Simple to complex Map Reduce Jobs using Hive and Pig Designed and developed NLP models for sentiment analysis Designed and provisioned the platform architecture to execute Hadoop and machine learning use cases under Cloud infrastructure AWS EMR and S3 Developed and configured on Informatica MDM hub supports the Master Data Management MDM Business Intelligence BI and Data Warehousing platforms to meet business needs Transforming staging area data into a STAR schema hosted on Amazon Redshift which was then used for developing embedded Tableau dashboards Worked on machine learning on large size data using Spark and MapReduce Let the implementation of new statistical algorithms and operators on Hadoop and SQL platforms and utilized optimizations techniques linear regressions Kmeans clustering NativeBayes and other approaches Developed SparkScala Python for regular expression regex project in the HadoopHive environment with LinuxWindows for big data resources Proficiency in SQL across a number of dialects we commonly write MySQL PostgreSQL Redshift Teradata and Oracle Responsible for full data loads from production to AWSRedshift staging environment and Worked on migrating of EDW to AWS using EMR and various other technologies Worked on TeradataSQLqueries Teradata Indexes Utilities such as Mload TPump Fast load and Fast Export Application of various machine learning algorithms and statistical modeling like decision trees regression models neural networks SVM clustering to identify Volume using Scikitlearn package in python MATLAB Worked on data preprocessing and cleaning the data to perform feature engineering and performed data imputation techniques for the missing values in the dataset using Python Created Data Quality Scripts using SQL and Hive to validate successful data load and quality of the data Created various types of data visualizations using Python and Tableau Worked with BTEQ to submit SQL statements import and export data and generate reports in Teradata Build and maintain scalable data pipelines using the Hadoop ecosystem and other open sources components like Hive and HBase Created Hive architecture used for realtime monitoring and HBase used for reporting and worked for map reduce and query optimization for Hadoop Hive and HBase architecture Involved in Teradata utilities BTEQ Fast Load Fast Export Multiload and TPump in both Windows and Mainframe platforms Built analytical data pipelines to port data in and out of HadoopHDFS from structured and unstructured sources and designed and implemented system architecture for Amazon EC2 based cloudhosted solution for the client Environment Erwin964 Oracle 12c Python PySpark Spark Spark   Tableau ODS PLSQL OLAP OLTP AWS Hadoop Map Reduce HDFS Python MDM Teradata 15 Hadoop Spark Cassandra SAP MS Excel Flat files Tableau Informatica SSIS SSRS AWS EC2 AWS EMR Elastic Search Data Scientist BBVA Compass Bancshares Inc Chicago IL August 2016 to June 2017 DescriptionAt AIM Specialty Health AIM it is our mission to promote appropriate safe and affordable health care As the specialty benefits management partner of choice for todays leading healthcare organizations we help improve the quality of care and reduce costs for todays most complex tests and treatments Responsibilities Developed applications of MachineLearning Statistical Analysisand Data Visualizations with challenging data Processing problems in sustainability and biomedical domain Worked on Natural Language Processing with NLTK module of python for application development for automated customer response Used predictive modeling with tools in SAS SPSS R Python Responsible for design and development of advanced R Python programs to prepare to transform and harmonize data sets in preparation for modeling Identifying and executing process improvements handson in various technologies such as Oracle Informatica and BusinessObjects Handled importing data from various data sources performed transformations using Hive Map Reduce and loaded data into HDFS Interaction with Business Analyst SMEsand other Data Architects to understand Business needs and functionality for various project solutions Created SQL tables with referential integrity and developed queries using SQL SQLPLUSand PLSQL Involved with Data Analysis primarily Identifying Data Sets Source Data Source Meta Data Data Definitions and Data Formats Wrote simple and advanced SQL queries and scripts to create standard and Adhoc reports for senior managers Created PLSQL packages and Database Triggers and developed user procedures and prepared user manuals for the new programs Prepare ETLarchitect design document which covers ETLarchitect SSISdesign Extraction transformationand loading of Duck Creek data into the dimensional model Design Logical Physical Data Model using MSVisio 2003 data modeler tool Participated in Architect solution meetings guidance in Dimensional Data Modeling design Applied linear regression multiple regression ordinary least square method meanvariance the theory of large numbers logistic regression dummy variable residuals Poisson distribution Bayes Naive Bayes fitting function etc to data with help of Scikit SciPy NumPy and Pandas module of Python Applied clustering algorithms ie Hierarchical Kmeans with help of Scikit and SciPy Developed visualizations and dashboards using ggplot Tableau Worked on development of data warehouse Data Lake and ETL systems using relational and nonrelational tools like SQL No SQL Built and analyzed datasets using R SAS MATLABand Python in decreasing order of usage Applied linear regression in Python and SAS to understand the relationship between different attributes of the dataset and causal relationship between them Pipelined ingestcleanmungetransform data for feature extraction toward downstream classification Used ClouderaHadoopYARN to perform analytics on data in Hive Wrote Hive queries for data analysis to meet the business requirements Expertise in BusinessIntelligence and data visualization using R and Tableau Validated the MacroEconomic data eg BlackRock Moodys etc and predictive analysis of world markets using key indicators in Python and machine learning concepts like regression Bootstrap Aggregation and Random Forest Worked in largescale database environments like Hadoop and MapReduce with working mechanism of Hadoop clusters nodes and Hadoop Distributed File System HDFS Interfaced with largescaledatabase system through an ETL server for data extraction and preparation Identified patterns data quality issues and opportunities and leveraged insights by communicating opportunities with business partners EnvironmentMachine learning AWS MS Azure Cassandra Spark HDFS Hive Pig Linux Python ScikitLearnSciPyNumPyPandas R SAS SPSS MySQL Eclipse PLSQL SQL connector Tableau Python Developer Walgreens Chicago IL March 2015 to July 2016 DescriptionThe Walgreen Company is an American company that operates as the secondlargest pharmacy store chain in the United States behind CVS Health It specializes in filling prescriptions health and wellness products health information and photo services Responsibilities Designed and Developed UI for creating Dashboard application using AngularJS D3 C3 HTML CSS Bootstrap JavaScript and jQuery Developed and implemented Python scripts to automate retrieval parsing and reporting of configuration parameters from Network Devices connected to customer networks Involved in user interface design and development using JSP Servlet HTML5   Wrote and tested Python scripts to create new data files for Linux server configuration using a Python template tool Modified controlling databases using SQL generated via Python and Perl code collected and analyzed data with Python programs using SQL queries from the database of data collected from the systems under tests Developed new user interface components for different modules using Kendo UI with various controls including Grid controls and chart controls etc Involved in write application level code to interact with APIs Web Services using AJAX JSON and hence building typeahead feature for zip code city and county lookup using jQuery Ajax and jQuery UI Worked on updating the existing clipboard to have the new features as per the client requirements Skilled in using collections in Python for manipulating and looping through different userdefined objects Taken part in the entire lifecycle of the projects including Design Development and Deployment Testing and Implementation and support Developed views and templates with Python and Djangos view controller and templating language to create a userfriendly website interface Automated different workflows which are initiated manually with Python scripts and Unix shell scripting Used Python unit and functional testing modules such as unit test unittest2 mock and custom frameworks inline with Agile Software Development methodologies Wrote and executed various MYSQL database queries from Python using PythonMySQL connector and MySQL dB package Created and maintained Technical documentation for launching Hadoop Clusters and for executing Hive queries and Pig Scripts Developed Sqoop scripts to handle change data capture for processing incremental records between new arrived and existing data in RDBMS tables Installed Hadoop Map Reduce HDFS AWS and developed multiple MapReduce jobs in PIG and Hive for data cleaning and preprocessing Managed datasets using Panda data frames and MySQL queried MYSQL database queries from python using PythonMySQL connector and MySQL dB package to retrieve information Involved in the WebApplication development using Python HTML5 CSS3 AJAX JSONandjQuery Developed and tested many features for a dashboard using Python Java Bootstrap CSS JavaScript and jQuery Generated Python Django forms to record data of online users and used PyTest for writing test cases Implemented and modified various SQL queries and Functions Cursors and Triggers as per the client requirements Prototype proposal for Issue Tracker website using PythonDjango connecting MySQL as Database The developed overall layout that meetscrossplatform compatibility using Bootstrap media queries and Angular UI Bootstrap Environment Python HTML5 CSS3 AJAX JSON jQuery MySQL NumPy SQL Alchemy Matplotlib Hadoop Pig Scripts Python Developer Citi Bank Irving TX July 2013 to February 2015 DescriptionThe project was to build an algorithm that accurately classifies credit card holders among multiple classes based on the historical data available on multiple variables Further the aim was to improve banks efficiency by reducing default rate while offering new products Moreover I was Involved in a project to identify the employees access level based on hisher current historical tasks and duties Responsibilities Involved in the software development lifecycle SDLC of tracking the requirements gathering analysis detailed design development system testing and user acceptance testing Developed entire frontend and backend modules using Python on Django Web Framework Involved in designing user interactive web pages as the frontend part of the web application using various web technologies like HTML JavaScript Angular JS jQuery AJAX and implemented CSS for better appearance and feel Knowledge of crossbrowser and crossplatform development of HTML and CSS based websites in Windows like IE 6 IE 7 IE 8 and FF Interactive in providing change requests trouble reports and requirements collection with the client Actively involved in developing the methods for Create Read Update and Delete CRUD in Active Record Working knowledge of various AWS technologies like SQS Queuing SNS Notification S3 storage Redshift Data Pipeline EMR Developed a fully automated continuous integration system using Git Jenkins MySQL and custom tools developed in Python and Bash Implemented Multithreading module and complex networking operations like race route SMTP mail server and web server Using Python Used NumPy for Numerical analysis for the Insurance premium Implemented and modified various SQL queries and Functions Cursors and Triggers as per the client requirements Managed code versioning with GitHub BitBucketand deployment to staging and production servers Implemented MVC architecture in developing the web application with the help of Django framework Used Celery as task queue and Rabbit MQ Redis as messaging broker to execute asynchronous tasks Designed and managed API system deployment using a fastHTTP server and Amazon AWS architecture Involved in code reviews using GitHub pull requests reducing bugs improving code quality and increasing knowledge sharing Install and configuring monitoring scripts for AWS EC2 instances Working under UNIX environment in the development of application using Python and familiar with all its commands Developed remote integration with thirdparty platforms by using RESTful web services Updated and maintained Jenkins for automatic building jobs and deployment Improved code reuse and performance by making effective use of various design patterns and refactoring code base Updated and maintained Puppet RSpec unitsystem test Worked on debugging and troubleshooting programming related issues Worked in the MySQL database on simple queries and writing Stored Procedures for normalization Deployment of the web application using the Linux server Environment Python 27 Django 14 HTML5 CSS XML MySQL JavaScript Backbone JS JQuery Mongo DB MS SQL Server JavaScript Git GitHub AWS Linux Shell Scripting AJAX JAVA Hadoop Developer SITEL India Pvt LTD Hyderabad Telangana August 2012 to June 2013 Description Sitel Group combines comprehensive customer care capabilities with unparalleled digital training and technology expertise to help build brand loyalty and improve customer satisfaction We partner with our clients to effectively harness our industrys explosive digital transformation to ensure an innovative endtoend solution to managing and enhancing the customer experience Responsibilities Designed and developed multiple MapReduce jobs in Java for complex analysis Importing and exporting the data using Sqoop from HDFS to Relational Database systems and viceversa Integrated Apache Storm with Kafka to perform web analytics Uploaded clickstream data from Kafka to HDFS HBase and Hive by integrating with Storm Configured Flume to transport web server logs into HDFS Also used Kite logging module to upload web server logs into HDFS Developed UDF functions for Hive and wrote complex queries in Hive for data analysis Performed Installation of Hadoop in fully and Pseudo Distributed Mode for POC in early stages of the project Analyze develop integrate and then direct the operationalization of new data sources Generating Scala and Java classes from the respective APIs so that they can be incorporated into the overall application Responsible for working with different teams in building Hadoop Infrastructure Gathered business requirements in meetings for successful implementation and POC and moving it to Production and implemented POC to migrate map reduce jobs into Spark RDD transformations using Scala Implemented different machine learning techniques in Scala using Scala machine learning library Developed Spark applications using Scala for easy Hadoop transitions Successfully loaded files to Hive and HDFS from Oracle Netezza and SQL Server using SQOOP Uses Talend Open Studio to load files into Hadoop HIVE tables and performed ETL aggregations in Hadoop Hive Developed Simple to Quebec and Python MapReduce streaming jobs using Python language that is implemented using Hive and Pig Designing Creating ETL Jobs through Talend to load huge volumes of data into Cassandra Hadoop Ecosystem and relational databases Worked on analyzing writing Hadoop MapReduce jobs using Java API Pig and Hive Developed some machine learning algorithms using Mahout for data mining for the data stored in HDFS Used Flume extensively in gathering and moving log data files from Application Servers to a central location in Hadoop Distributed File System HDFS Worked with Oozie Workflow manager to schedule Hadoop jobs and high intensive jobs Responsible for cluster maintenance adding and removing cluster nodes cluster monitoring and troubleshooting manage and review data backups manage and review Hadoop log files Extensively used HiveHQL or Hive queries to query data in Hive Tables and loaded data into HIVE tables Creating UDF functions in Pig Hive and applying partitioning and bucketing techniques in Hive for performance improvement Creating indexes and tuning the SQL queries in Hive and Involved in database connection by using Sqoop Involved in Hadoop Name node metadata backups and load balancing as a part of Cluster Maintenance and Monitoring Used File System Check FSCK to check the health of files in HDFS and used Sqoop to import data from SQL server to Cassandra Monitored Nightly jobs to export data out of HDFS to be stored offsite as part of HDFS backup Used Pig for analysis of large datasets and brought data back to HBaseby Pig Developed Python Mapper and Reducer scripts and implemented them using Hadoop streaming Created schema and database objects in HIVE and developed Unix Scripts for data loading and automation Involved in the training of big data ecosystem to endusers Environment Java15 J2EE Hibernate Spring JUnit WebLogic HTML CSS JavaScript Jenkins Nodejs jQuery Linux CICD Spring Boot Maven Log4J and Junit Eclipse REST SQL Navigator Java Developer Napier Healthcare Pvt Ltd Hyderabad Telangana October 2009 to July 2011 Description Napier Healthcare was established in 1996 as a Healthcare IT Products and Services Company With deep domain knowledge and singular focus we have built a robust featurerich suite of solutions to deliver value to the stakeholders of the healthcare industry worldwide Responsibilities Development Testing Maintenance and product delivery Developed a scalable and maintainable application using J2EE Framework Hibernate MVC Model Struts and J2EE Design Patterns Prepared SOW Statement of Work by communicating with agencies and organized meetings about requirements Followed Java J2EE design patterns and the coding guidelines to design and develop the application Extensively used JSTL tags and Struts tag libraries Used Struts tiles as well in the presentation tier Developing the application using Struts and Spring based frameworks Actively involved in designing and implementing the application using various design patterns Coordinating with clients and closing production issues relating to software development Identifying and Evaluate Technology Solutions Problem Solving and Troubleshooting Done with Serverside validations using Struts Validation framework Processed JSON response data by consuming RESTful web services and used an Angular filter for implementing search results Used Strutsconfigxml file for defining Mapping Definitions and Action Forward Definitions Developed the Action Classes which is in between view and model layers Action Form Classes which is used to maintain session state of a web application created JSPs Java Server pages using Struts tag libraries and configured in strutsconfigXML webxml files This application is designed using MVC architecture to maintain easily Hibernate is used for database connectivity and designed HQL Hibernate Query language to create modify and update the tables Created new Soap services using JAXWS specifications Wrote JUnit test cases for testing EnvironmentJava Struts Hibernate JSP Servlets SOAP UI HTML CSS Java Script JUnit Apache Tomcat Server EJB NetBeans Education Bachelors Skills PYTHON 10 years MYSQL 10 years ORACLE 10 years PLSQL 10 years SQL 10 years Additional Information TECHNICAL SKILLS Python Libraries Beautiful Soup NumPy SciPy Matplotlib pythontwitter Pandas data frame urllib2 Data Analytics ToolsProgramming Python NumPy SciPy pandas Gensim Keras R Caret Weka ggplot MATLAB Microsoft SQL Server Oracle PLSQL Python Database Oracle11 g MySQL 5x and SQLServer Version Control SVN Clear case CVS Reporting Tools MS Office   VisioOutlook Crystal Reports XI SSRS Cognos 7060 BI Tools Microsoft Power BI Tableau SSIS SSRS SSAS Business Intelligence Development Studio BIDS Visual Studio Crystal Reports Informatica 61 Database Design Tools and Data Modeling MS Visio ERWIN 4540 Star SchemaSnowflake Schema modeling Fact Dimensions tables physical logical data modeling Normalization and Denormalization techniques Kimball Inmon Methodologies IDEs PyCharm Emacs Eclipse NetBeans Sublime SOAP UI WebApp Servers WebSphere Application Server 80 Apache Tomcat Web Logic 11 g 12c JBoss 4x5x CloudTechnologies AWS S3",
    "extracted_keywords": [
        "Data",
        "Scientist",
        "Data",
        "Scientist",
        "Data",
        "Scientist",
        "BBVA",
        "Compass",
        "Birmingham",
        "AL",
        "years",
        "Experience",
        "Data",
        "Architecture",
        "Design",
        "Developmentand",
        "Testing",
        "business",
        "application",
        "systems",
        "Data",
        "Analysis",
        "models",
        "database",
        "design",
        "Online",
        "Transactional",
        "processing",
        "OLTP",
        "Online",
        "Analytical",
        "Processing",
        "OLAP",
        "systems",
        "star",
        "schema",
        "Snowflake",
        "schema",
        "Data",
        "Warehouse",
        "ODS",
        "architecture",
        "Data",
        "Modeling",
        "Data",
        "Analysis",
        "experience",
        "Dimensional",
        "Data",
        "Modeling",
        "Relational",
        "Data",
        "Modeling",
        "Star",
        "SchemaSnowflake",
        "FACT",
        "Dimensions",
        "Physical",
        "Logical",
        "Data",
        "Modeling",
        "data",
        "analysis",
        "data",
        "models",
        "Hive",
        "PIG",
        "Map",
        "SQL",
        "data",
        "skills",
        "datacentric",
        "solutions",
        "Hands",
        "experience",
        "data",
        "tools",
        "Hadoop",
        "Spark",
        "Hive",
        "Pig",
        "Impala",
        "PySpark",
        "SparkSQL",
        "knowledge",
        "experience",
        "AWS",
        "Proficient",
        "Data",
        "Analysis",
        "mapping",
        "source",
        "target",
        "systems",
        "datamigration",
        "efforts",
        "issues",
        "data",
        "migration",
        "Excellent",
        "development",
        "experience",
        "SQL",
        "Procedural",
        "LanguagePL",
        "databases",
        "Oracle",
        "knowledge",
        "experience",
        "data",
        "tools",
        "Hadoop",
        "Azure",
        "Data",
        "Lake",
        "AWS",
        "Redshift",
        "Data",
        "ScrubbingCleansing",
        "Data",
        "Quality",
        "Data",
        "Mapping",
        "Data",
        "Profiling",
        "Data",
        "Validation",
        "ETL",
        "Metadata",
        "OLTP",
        "OLAP",
        "systems",
        "Expertise",
        "Machine",
        "Predictive",
        "Analytics",
        "data",
        "technologies",
        "solutions",
        "experience",
        "development",
        "TSQL",
        "DTS",
        "OLAP",
        "PLSQL",
        "Stored",
        "Procedures",
        "Triggers",
        "Functions",
        "Packages",
        "performance",
        "tuning",
        "optimization",
        "business",
        "logic",
        "implementation",
        "Experience",
        "packages",
        "Rand",
        "python",
        "ggplot2",
        "dplyr",
        "Rweka",
        "RCurl",
        "tm",
        "C50",
        "twitter",
        "NLP",
        "Reshape2",
        "rjson",
        "NumPy",
        "Seaborn",
        "SciPy",
        "matplotlib",
        "Scikitlearn",
        "Beautiful",
        "Soup",
        "Rpy2",
        "query",
        "tools",
        "SQL",
        "Developer",
        "PLSQL",
        "Developer",
        "Teradata",
        "SQL",
        "Assistant",
        "Excellent",
        "knowledge",
        "Machine",
        "Learning",
        "Mathematical",
        "Modeling",
        "Operations",
        "Research",
        "Comfortable",
        "R",
        "Python",
        "SAS",
        "Weka",
        "MATLAB",
        "Relational",
        "understanding",
        "exposure",
        "Big",
        "Data",
        "Ecosystem",
        "Expertise",
        "Mappings",
        "expertise",
        "performance",
        "Dimension",
        "Tables",
        "Fact",
        "tables",
        "utilities",
        "BTEQ",
        "export",
        "MultiLoad",
        "data",
        "source",
        "systems",
        "files",
        "Hands",
        "experience",
        "LDA",
        "Naive",
        "Bayes",
        "Random",
        "Forests",
        "Decision",
        "Trees",
        "Linear",
        "Logistic",
        "Regression",
        "SVM",
        "networks",
        "Principle",
        "Component",
        "Analysis",
        "Expertise",
        "transforming",
        "loading",
        "data",
        "systems",
        "SQL",
        "Server",
        "Oracle",
        "DB2",
        "MS",
        "Access",
        "Excel",
        "Flat",
        "File",
        "packages",
        "Proficient",
        "System",
        "Analysis",
        "ERDimensional",
        "Data",
        "Modeling",
        "Database",
        "design",
        "features",
        "Experience",
        "UNIX",
        "shell",
        "scripting",
        "Perl",
        "scriptingand",
        "automation",
        "Processes",
        "ETL",
        "data",
        "Power",
        "Center",
        "Power",
        "Exchange",
        "source",
        "systems",
        "Files",
        "Excel",
        "Files",
        "tables",
        "data",
        "target",
        "database",
        "Oracle",
        "systems",
        "Feasibility",
        "Study",
        "Excellent",
        "understanding",
        "working",
        "experience",
        "industry",
        "methodologies",
        "System",
        "Development",
        "Life",
        "Cycle",
        "SDLC",
        "Rational",
        "Unified",
        "Process",
        "RUP",
        "Agile",
        "Methodologies",
        "Proficiency",
        "SQL",
        "number",
        "dialects",
        "MySQL",
        "PostgreSQL",
        "Redshift",
        "SQL",
        "Server",
        "Oracle",
        "EntityRelationship",
        "diagrams",
        "Transactional",
        "Databases",
        "Data",
        "Warehouse",
        "tools",
        "ERWIN",
        "ERStudioandPower",
        "Designer",
        "modeling",
        "ERWIN",
        "engineering",
        "cases",
        "US",
        "employer",
        "Work",
        "Experience",
        "Data",
        "Scientist",
        "BBVA",
        "Compass",
        "Birmingham",
        "AL",
        "July",
        "Present",
        "Architect",
        "framework",
        "BBVA",
        "Compass",
        "Bancshares",
        "Inc",
        "Birmingham",
        "AL",
        "Present",
        "bank",
        "company",
        "Birmingham",
        "Alabama",
        "subsidiary",
        "Banco",
        "Bilbao",
        "Vizcaya",
        "Argentaria",
        "Alabama",
        "Arizona",
        "California",
        "Colorado",
        "Florida",
        "New",
        "Mexico",
        "Texas",
        "Responsibilities",
        "Design",
        "Develop",
        "Comprehensive",
        "Data",
        "Warehouse",
        "Solution",
        "transfer",
        "load",
        "qualityaccuracy",
        "data",
        "sources",
        "EDW",
        "Enterprise",
        "Data",
        "Warehouse",
        "Architect",
        "framework",
        "data",
        "warehouse",
        "solutions",
        "data",
        "source",
        "system",
        "EDW",
        "data",
        "mart",
        "solutions",
        "OrderSales",
        "operation",
        "Salesforce",
        "activity",
        "Inventory",
        "data",
        "mining",
        "analysis",
        "market",
        "projection",
        "Apache",
        "Spark",
        "Python",
        "Big",
        "Data",
        "Analytics",
        "Machine",
        "learning",
        "applications",
        "machine",
        "learning",
        "use",
        "cases",
        "Spark",
        "ML",
        "data",
        "data",
        "sources",
        "transformations",
        "Hive",
        "Map",
        "Reduce",
        "data",
        "HDFS",
        "Performed",
        "data",
        "cleaning",
        "feature",
        "selection",
        "package",
        "PySpark",
        "frameworks",
        "Caffe",
        "Neon",
        "Complex",
        "ETL",
        "Mappings",
        "Sessions",
        "business",
        "user",
        "requirements",
        "business",
        "rules",
        "data",
        "source",
        "files",
        "RDBMS",
        "tables",
        "tables",
        "Spark",
        "Scala",
        "Hadoop",
        "HBase",
        "Cassandra",
        "MongoDB",
        "Kafka",
        "Spark",
        "Streaming",
        "Python",
        "variety",
        "machine",
        "learning",
        "methods",
        "classifications",
        "regressions",
        "engine",
        "user",
        "lifetime",
        "user",
        "conversations",
        "target",
        "categories",
        "performance",
        "data",
        "architecture",
        "solution",
        "matrix",
        "technology",
        "decision",
        "business",
        "Conducting",
        "strategy",
        "architecture",
        "sessions",
        "artifacts",
        "MDM",
        "strategy",
        "state",
        "Interim",
        "Stateand",
        "Target",
        "state",
        "MDM",
        "Architecture",
        "Conceptual",
        "Logical",
        "Physical",
        "detail",
        "level",
        "Design",
        "development",
        "data",
        "model",
        "Redshift",
        "selection",
        "analytics",
        "platform",
        "Simple",
        "Map",
        "Reduce",
        "Jobs",
        "Hive",
        "Pig",
        "NLP",
        "models",
        "sentiment",
        "analysis",
        "platform",
        "architecture",
        "Hadoop",
        "machine",
        "learning",
        "use",
        "cases",
        "Cloud",
        "infrastructure",
        "AWS",
        "EMR",
        "S3",
        "Developed",
        "Informatica",
        "MDM",
        "hub",
        "Master",
        "Data",
        "Management",
        "MDM",
        "Business",
        "Intelligence",
        "BI",
        "Data",
        "Warehousing",
        "platforms",
        "business",
        "area",
        "data",
        "STAR",
        "schema",
        "Amazon",
        "Redshift",
        "Tableau",
        "dashboards",
        "machine",
        "learning",
        "size",
        "data",
        "Spark",
        "MapReduce",
        "implementation",
        "algorithms",
        "operators",
        "Hadoop",
        "SQL",
        "platforms",
        "optimizations",
        "regressions",
        "Kmeans",
        "NativeBayes",
        "approaches",
        "SparkScala",
        "Python",
        "expression",
        "regex",
        "project",
        "environment",
        "LinuxWindows",
        "data",
        "resources",
        "Proficiency",
        "SQL",
        "number",
        "dialects",
        "MySQL",
        "PostgreSQL",
        "Redshift",
        "Teradata",
        "Oracle",
        "Responsible",
        "data",
        "loads",
        "production",
        "AWSRedshift",
        "staging",
        "environment",
        "migrating",
        "EDW",
        "AWS",
        "EMR",
        "technologies",
        "TeradataSQLqueries",
        "Teradata",
        "Indexes",
        "Utilities",
        "Mload",
        "TPump",
        "load",
        "Fast",
        "Export",
        "Application",
        "machine",
        "algorithms",
        "modeling",
        "decision",
        "trees",
        "regression",
        "models",
        "networks",
        "SVM",
        "Volume",
        "Scikitlearn",
        "package",
        "python",
        "MATLAB",
        "Worked",
        "data",
        "data",
        "feature",
        "engineering",
        "data",
        "imputation",
        "techniques",
        "values",
        "dataset",
        "Python",
        "Created",
        "Data",
        "Quality",
        "Scripts",
        "SQL",
        "Hive",
        "data",
        "load",
        "quality",
        "data",
        "types",
        "data",
        "visualizations",
        "Python",
        "Tableau",
        "BTEQ",
        "SQL",
        "statements",
        "import",
        "export",
        "data",
        "reports",
        "Teradata",
        "Build",
        "data",
        "pipelines",
        "Hadoop",
        "ecosystem",
        "sources",
        "components",
        "Hive",
        "HBase",
        "Created",
        "Hive",
        "architecture",
        "monitoring",
        "HBase",
        "reporting",
        "map",
        "query",
        "optimization",
        "Hadoop",
        "Hive",
        "HBase",
        "architecture",
        "Teradata",
        "utilities",
        "BTEQ",
        "Fast",
        "Load",
        "Fast",
        "Export",
        "Multiload",
        "TPump",
        "Windows",
        "Mainframe",
        "platforms",
        "data",
        "pipelines",
        "port",
        "data",
        "sources",
        "system",
        "architecture",
        "Amazon",
        "EC2",
        "solution",
        "client",
        "Environment",
        "Erwin964",
        "Oracle",
        "Python",
        "PySpark",
        "Spark",
        "Spark",
        "Tableau",
        "ODS",
        "PLSQL",
        "OLTP",
        "AWS",
        "Hadoop",
        "Map",
        "Reduce",
        "HDFS",
        "Python",
        "MDM",
        "Teradata",
        "Hadoop",
        "Spark",
        "Cassandra",
        "SAP",
        "MS",
        "Excel",
        "Flat",
        "files",
        "Tableau",
        "Informatica",
        "SSIS",
        "SSRS",
        "EC2",
        "AWS",
        "EMR",
        "Elastic",
        "Search",
        "Data",
        "Scientist",
        "BBVA",
        "Compass",
        "Bancshares",
        "Inc",
        "Chicago",
        "IL",
        "August",
        "June",
        "DescriptionAt",
        "AIM",
        "Specialty",
        "Health",
        "AIM",
        "mission",
        "health",
        "care",
        "specialty",
        "benefits",
        "management",
        "partner",
        "choice",
        "todays",
        "healthcare",
        "organizations",
        "quality",
        "care",
        "costs",
        "todays",
        "tests",
        "treatments",
        "Responsibilities",
        "applications",
        "MachineLearning",
        "Statistical",
        "Analysisand",
        "Data",
        "Visualizations",
        "data",
        "Processing",
        "problems",
        "sustainability",
        "domain",
        "Natural",
        "Language",
        "Processing",
        "NLTK",
        "module",
        "python",
        "application",
        "development",
        "customer",
        "response",
        "modeling",
        "tools",
        "SAS",
        "SPSS",
        "R",
        "Python",
        "design",
        "development",
        "R",
        "Python",
        "programs",
        "data",
        "sets",
        "preparation",
        "process",
        "improvements",
        "handson",
        "technologies",
        "Oracle",
        "Informatica",
        "BusinessObjects",
        "data",
        "data",
        "sources",
        "transformations",
        "Hive",
        "Map",
        "Reduce",
        "data",
        "HDFS",
        "Interaction",
        "Business",
        "Analyst",
        "SMEsand",
        "Data",
        "Architects",
        "Business",
        "needs",
        "functionality",
        "project",
        "solutions",
        "SQL",
        "tables",
        "integrity",
        "queries",
        "SQL",
        "SQLPLUSand",
        "PLSQL",
        "Data",
        "Analysis",
        "Data",
        "Sets",
        "Source",
        "Data",
        "Source",
        "Meta",
        "Data",
        "Data",
        "Definitions",
        "Data",
        "Formats",
        "SQL",
        "queries",
        "scripts",
        "Adhoc",
        "reports",
        "managers",
        "PLSQL",
        "packages",
        "Database",
        "Triggers",
        "user",
        "procedures",
        "user",
        "manuals",
        "programs",
        "design",
        "document",
        "ETLarchitect",
        "SSISdesign",
        "Extraction",
        "transformationand",
        "loading",
        "Duck",
        "Creek",
        "data",
        "model",
        "Design",
        "Logical",
        "Physical",
        "Data",
        "Model",
        "MSVisio",
        "data",
        "modeler",
        "tool",
        "Architect",
        "solution",
        "meetings",
        "guidance",
        "Dimensional",
        "Data",
        "Modeling",
        "design",
        "linear",
        "regression",
        "regression",
        "method",
        "meanvariance",
        "theory",
        "numbers",
        "regression",
        "residuals",
        "Poisson",
        "distribution",
        "Bayes",
        "Naive",
        "Bayes",
        "function",
        "data",
        "help",
        "Scikit",
        "SciPy",
        "NumPy",
        "Pandas",
        "module",
        "Python",
        "algorithms",
        "Kmeans",
        "help",
        "Scikit",
        "SciPy",
        "visualizations",
        "dashboards",
        "ggplot",
        "Tableau",
        "development",
        "data",
        "warehouse",
        "Data",
        "Lake",
        "ETL",
        "systems",
        "tools",
        "SQL",
        "SQL",
        "datasets",
        "R",
        "SAS",
        "MATLABand",
        "Python",
        "order",
        "usage",
        "regression",
        "Python",
        "SAS",
        "relationship",
        "attributes",
        "dataset",
        "relationship",
        "ingestcleanmungetransform",
        "data",
        "feature",
        "extraction",
        "classification",
        "ClouderaHadoopYARN",
        "analytics",
        "data",
        "Hive",
        "Wrote",
        "Hive",
        "data",
        "analysis",
        "business",
        "requirements",
        "Expertise",
        "BusinessIntelligence",
        "data",
        "visualization",
        "R",
        "Tableau",
        "MacroEconomic",
        "data",
        "eg",
        "BlackRock",
        "Moodys",
        "analysis",
        "world",
        "markets",
        "indicators",
        "Python",
        "machine",
        "learning",
        "concepts",
        "regression",
        "Bootstrap",
        "Aggregation",
        "Random",
        "Forest",
        "largescale",
        "database",
        "environments",
        "Hadoop",
        "MapReduce",
        "mechanism",
        "Hadoop",
        "clusters",
        "nodes",
        "Hadoop",
        "Distributed",
        "File",
        "System",
        "HDFS",
        "largescaledatabase",
        "system",
        "ETL",
        "server",
        "data",
        "extraction",
        "preparation",
        "patterns",
        "data",
        "quality",
        "issues",
        "opportunities",
        "insights",
        "opportunities",
        "business",
        "partners",
        "EnvironmentMachine",
        "AWS",
        "MS",
        "Azure",
        "Cassandra",
        "Spark",
        "HDFS",
        "Hive",
        "Pig",
        "Linux",
        "Python",
        "ScikitLearnSciPyNumPyPandas",
        "R",
        "SAS",
        "SPSS",
        "MySQL",
        "Eclipse",
        "PLSQL",
        "SQL",
        "connector",
        "Tableau",
        "Python",
        "Developer",
        "Chicago",
        "IL",
        "March",
        "July",
        "DescriptionThe",
        "Walgreen",
        "Company",
        "company",
        "pharmacy",
        "store",
        "chain",
        "United",
        "States",
        "CVS",
        "Health",
        "prescriptions",
        "health",
        "wellness",
        "products",
        "health",
        "information",
        "photo",
        "services",
        "Responsibilities",
        "UI",
        "Dashboard",
        "application",
        "D3",
        "C3",
        "HTML",
        "CSS",
        "Bootstrap",
        "JavaScript",
        "jQuery",
        "Developed",
        "Python",
        "scripts",
        "retrieval",
        "parsing",
        "reporting",
        "configuration",
        "parameters",
        "Network",
        "Devices",
        "customer",
        "networks",
        "user",
        "interface",
        "design",
        "development",
        "JSP",
        "Servlet",
        "HTML5",
        "Wrote",
        "Python",
        "scripts",
        "data",
        "files",
        "Linux",
        "server",
        "configuration",
        "Python",
        "template",
        "tool",
        "Modified",
        "databases",
        "SQL",
        "Python",
        "Perl",
        "code",
        "data",
        "Python",
        "programs",
        "SQL",
        "queries",
        "database",
        "data",
        "systems",
        "tests",
        "user",
        "interface",
        "components",
        "modules",
        "Kendo",
        "UI",
        "controls",
        "Grid",
        "controls",
        "chart",
        "controls",
        "application",
        "level",
        "code",
        "APIs",
        "Web",
        "Services",
        "AJAX",
        "JSON",
        "typeahead",
        "feature",
        "zip",
        "code",
        "city",
        "county",
        "lookup",
        "jQuery",
        "Ajax",
        "jQuery",
        "UI",
        "clipboard",
        "features",
        "client",
        "requirements",
        "collections",
        "Python",
        "objects",
        "part",
        "lifecycle",
        "projects",
        "Design",
        "Development",
        "Deployment",
        "Testing",
        "Implementation",
        "views",
        "templates",
        "Python",
        "Djangos",
        "controller",
        "templating",
        "language",
        "website",
        "interface",
        "workflows",
        "Python",
        "scripts",
        "Unix",
        "shell",
        "scripting",
        "Python",
        "unit",
        "testing",
        "modules",
        "unit",
        "test",
        "custom",
        "frameworks",
        "Agile",
        "Software",
        "Development",
        "methodologies",
        "Wrote",
        "MYSQL",
        "database",
        "Python",
        "PythonMySQL",
        "connector",
        "MySQL",
        "package",
        "documentation",
        "Hadoop",
        "Clusters",
        "Hive",
        "queries",
        "Pig",
        "Scripts",
        "Developed",
        "Sqoop",
        "scripts",
        "change",
        "data",
        "capture",
        "records",
        "data",
        "tables",
        "Installed",
        "Hadoop",
        "Map",
        "HDFS",
        "AWS",
        "MapReduce",
        "jobs",
        "PIG",
        "Hive",
        "data",
        "datasets",
        "Panda",
        "data",
        "frames",
        "MySQL",
        "MYSQL",
        "database",
        "python",
        "PythonMySQL",
        "connector",
        "MySQL",
        "package",
        "information",
        "WebApplication",
        "development",
        "Python",
        "HTML5",
        "CSS3",
        "AJAX",
        "JSONandjQuery",
        "Developed",
        "features",
        "dashboard",
        "Python",
        "Java",
        "Bootstrap",
        "CSS",
        "JavaScript",
        "jQuery",
        "Python",
        "Django",
        "data",
        "users",
        "PyTest",
        "test",
        "cases",
        "SQL",
        "queries",
        "Functions",
        "Cursors",
        "Triggers",
        "client",
        "requirements",
        "Prototype",
        "proposal",
        "Issue",
        "Tracker",
        "website",
        "PythonDjango",
        "MySQL",
        "Database",
        "layout",
        "meetscrossplatform",
        "compatibility",
        "Bootstrap",
        "media",
        "queries",
        "Angular",
        "UI",
        "Bootstrap",
        "Environment",
        "Python",
        "HTML5",
        "CSS3",
        "AJAX",
        "JSON",
        "jQuery",
        "MySQL",
        "NumPy",
        "SQL",
        "Alchemy",
        "Matplotlib",
        "Hadoop",
        "Pig",
        "Scripts",
        "Python",
        "Developer",
        "Citi",
        "Bank",
        "Irving",
        "TX",
        "July",
        "February",
        "DescriptionThe",
        "project",
        "algorithm",
        "credit",
        "card",
        "holders",
        "classes",
        "data",
        "variables",
        "aim",
        "banks",
        "efficiency",
        "default",
        "rate",
        "products",
        "project",
        "employees",
        "access",
        "level",
        "hisher",
        "tasks",
        "duties",
        "Responsibilities",
        "software",
        "development",
        "lifecycle",
        "SDLC",
        "requirements",
        "analysis",
        "design",
        "development",
        "system",
        "testing",
        "user",
        "acceptance",
        "testing",
        "frontend",
        "modules",
        "Python",
        "Django",
        "Web",
        "Framework",
        "user",
        "web",
        "pages",
        "part",
        "web",
        "application",
        "web",
        "technologies",
        "HTML",
        "JavaScript",
        "Angular",
        "JS",
        "jQuery",
        "AJAX",
        "CSS",
        "appearance",
        "Knowledge",
        "crossbrowser",
        "development",
        "HTML",
        "CSS",
        "websites",
        "Windows",
        "IE",
        "IE",
        "IE",
        "FF",
        "Interactive",
        "change",
        "requests",
        "trouble",
        "reports",
        "requirements",
        "collection",
        "client",
        "methods",
        "Create",
        "Read",
        "Update",
        "Delete",
        "CRUD",
        "Active",
        "Record",
        "Working",
        "knowledge",
        "AWS",
        "technologies",
        "SQS",
        "Queuing",
        "SNS",
        "Notification",
        "S3",
        "storage",
        "Redshift",
        "Data",
        "Pipeline",
        "EMR",
        "integration",
        "system",
        "Git",
        "Jenkins",
        "MySQL",
        "custom",
        "tools",
        "Python",
        "Bash",
        "Multithreading",
        "module",
        "networking",
        "operations",
        "race",
        "route",
        "SMTP",
        "mail",
        "server",
        "web",
        "server",
        "Python",
        "NumPy",
        "analysis",
        "Insurance",
        "premium",
        "SQL",
        "queries",
        "Functions",
        "Cursors",
        "Triggers",
        "client",
        "requirements",
        "Managed",
        "code",
        "GitHub",
        "BitBucketand",
        "deployment",
        "staging",
        "production",
        "servers",
        "MVC",
        "architecture",
        "web",
        "application",
        "help",
        "Django",
        "framework",
        "Celery",
        "task",
        "queue",
        "Rabbit",
        "MQ",
        "Redis",
        "broker",
        "tasks",
        "API",
        "system",
        "deployment",
        "fastHTTP",
        "server",
        "Amazon",
        "AWS",
        "architecture",
        "code",
        "reviews",
        "GitHub",
        "pull",
        "requests",
        "bugs",
        "code",
        "quality",
        "knowledge",
        "Install",
        "monitoring",
        "scripts",
        "AWS",
        "EC2",
        "instances",
        "UNIX",
        "environment",
        "development",
        "application",
        "Python",
        "commands",
        "integration",
        "thirdparty",
        "platforms",
        "web",
        "services",
        "Jenkins",
        "building",
        "jobs",
        "deployment",
        "code",
        "reuse",
        "performance",
        "use",
        "design",
        "patterns",
        "refactoring",
        "code",
        "base",
        "Puppet",
        "RSpec",
        "unitsystem",
        "test",
        "troubleshooting",
        "programming",
        "issues",
        "MySQL",
        "database",
        "queries",
        "Procedures",
        "normalization",
        "Deployment",
        "web",
        "application",
        "Linux",
        "server",
        "Environment",
        "Python",
        "Django",
        "HTML5",
        "CSS",
        "XML",
        "MySQL",
        "JavaScript",
        "Backbone",
        "JS",
        "JQuery",
        "Mongo",
        "DB",
        "MS",
        "SQL",
        "Server",
        "JavaScript",
        "Git",
        "GitHub",
        "Linux",
        "Shell",
        "Scripting",
        "AJAX",
        "JAVA",
        "Hadoop",
        "Developer",
        "SITEL",
        "India",
        "Pvt",
        "LTD",
        "Hyderabad",
        "Telangana",
        "August",
        "June",
        "Description",
        "Sitel",
        "Group",
        "customer",
        "care",
        "capabilities",
        "training",
        "technology",
        "expertise",
        "brand",
        "loyalty",
        "customer",
        "satisfaction",
        "partner",
        "clients",
        "industrys",
        "transformation",
        "endtoend",
        "solution",
        "customer",
        "experience",
        "Responsibilities",
        "MapReduce",
        "jobs",
        "Java",
        "analysis",
        "data",
        "Sqoop",
        "HDFS",
        "Relational",
        "Database",
        "systems",
        "viceversa",
        "Integrated",
        "Apache",
        "Storm",
        "Kafka",
        "web",
        "analytics",
        "clickstream",
        "data",
        "Kafka",
        "HDFS",
        "HBase",
        "Hive",
        "Storm",
        "Configured",
        "Flume",
        "transport",
        "web",
        "server",
        "logs",
        "HDFS",
        "Kite",
        "module",
        "web",
        "server",
        "logs",
        "UDF",
        "functions",
        "Hive",
        "queries",
        "Hive",
        "data",
        "analysis",
        "Performed",
        "Installation",
        "Hadoop",
        "Pseudo",
        "Mode",
        "POC",
        "stages",
        "project",
        "Analyze",
        "operationalization",
        "data",
        "sources",
        "Generating",
        "Scala",
        "Java",
        "classes",
        "APIs",
        "application",
        "teams",
        "Hadoop",
        "Infrastructure",
        "Gathered",
        "business",
        "requirements",
        "meetings",
        "implementation",
        "POC",
        "Production",
        "POC",
        "map",
        "jobs",
        "Spark",
        "RDD",
        "transformations",
        "Scala",
        "machine",
        "techniques",
        "Scala",
        "Scala",
        "machine",
        "learning",
        "library",
        "Developed",
        "Spark",
        "applications",
        "Scala",
        "Hadoop",
        "transitions",
        "files",
        "Hive",
        "HDFS",
        "Oracle",
        "Netezza",
        "SQL",
        "Server",
        "SQOOP",
        "Uses",
        "Talend",
        "Open",
        "Studio",
        "files",
        "Hadoop",
        "HIVE",
        "tables",
        "ETL",
        "aggregations",
        "Hadoop",
        "Hive",
        "Simple",
        "Quebec",
        "Python",
        "MapReduce",
        "streaming",
        "jobs",
        "Python",
        "language",
        "Hive",
        "Pig",
        "Designing",
        "Creating",
        "ETL",
        "Jobs",
        "Talend",
        "volumes",
        "data",
        "Cassandra",
        "Hadoop",
        "Ecosystem",
        "databases",
        "Hadoop",
        "MapReduce",
        "jobs",
        "Java",
        "API",
        "Pig",
        "Hive",
        "machine",
        "algorithms",
        "Mahout",
        "data",
        "mining",
        "data",
        "HDFS",
        "Flume",
        "log",
        "data",
        "files",
        "Application",
        "Servers",
        "location",
        "Hadoop",
        "Distributed",
        "File",
        "System",
        "HDFS",
        "Oozie",
        "Workflow",
        "manager",
        "Hadoop",
        "jobs",
        "jobs",
        "cluster",
        "maintenance",
        "cluster",
        "nodes",
        "cluster",
        "monitoring",
        "troubleshooting",
        "manage",
        "data",
        "backups",
        "Hadoop",
        "log",
        "files",
        "HiveHQL",
        "Hive",
        "queries",
        "data",
        "Hive",
        "Tables",
        "data",
        "HIVE",
        "tables",
        "UDF",
        "functions",
        "Pig",
        "Hive",
        "techniques",
        "Hive",
        "performance",
        "improvement",
        "indexes",
        "SQL",
        "queries",
        "Hive",
        "database",
        "connection",
        "Sqoop",
        "Hadoop",
        "Name",
        "node",
        "metadata",
        "backups",
        "load",
        "part",
        "Cluster",
        "Maintenance",
        "Monitoring",
        "File",
        "System",
        "Check",
        "FSCK",
        "health",
        "files",
        "HDFS",
        "Sqoop",
        "data",
        "SQL",
        "server",
        "Cassandra",
        "Monitored",
        "Nightly",
        "jobs",
        "data",
        "HDFS",
        "part",
        "HDFS",
        "backup",
        "Used",
        "Pig",
        "analysis",
        "datasets",
        "data",
        "HBaseby",
        "Pig",
        "Python",
        "Mapper",
        "Reducer",
        "scripts",
        "Hadoop",
        "streaming",
        "Created",
        "schema",
        "database",
        "objects",
        "HIVE",
        "Unix",
        "Scripts",
        "data",
        "loading",
        "automation",
        "training",
        "data",
        "ecosystem",
        "endusers",
        "Environment",
        "Java15",
        "J2EE",
        "Hibernate",
        "Spring",
        "JUnit",
        "WebLogic",
        "HTML",
        "CSS",
        "JavaScript",
        "Jenkins",
        "Nodejs",
        "jQuery",
        "Linux",
        "CICD",
        "Spring",
        "Boot",
        "Maven",
        "Log4J",
        "Junit",
        "Eclipse",
        "REST",
        "SQL",
        "Navigator",
        "Java",
        "Developer",
        "Napier",
        "Healthcare",
        "Pvt",
        "Ltd",
        "Hyderabad",
        "Telangana",
        "October",
        "July",
        "Description",
        "Napier",
        "Healthcare",
        "Healthcare",
        "IT",
        "Products",
        "Services",
        "Company",
        "domain",
        "knowledge",
        "focus",
        "featurerich",
        "suite",
        "solutions",
        "value",
        "stakeholders",
        "healthcare",
        "industry",
        "Responsibilities",
        "Development",
        "Testing",
        "Maintenance",
        "product",
        "delivery",
        "application",
        "J2EE",
        "Framework",
        "Hibernate",
        "MVC",
        "Model",
        "Struts",
        "J2EE",
        "Design",
        "Patterns",
        "Prepared",
        "SOW",
        "Statement",
        "Work",
        "agencies",
        "meetings",
        "requirements",
        "Java",
        "J2EE",
        "design",
        "patterns",
        "guidelines",
        "application",
        "JSTL",
        "tags",
        "Struts",
        "tag",
        "Struts",
        "tiles",
        "presentation",
        "tier",
        "application",
        "Struts",
        "Spring",
        "frameworks",
        "application",
        "design",
        "patterns",
        "clients",
        "closing",
        "production",
        "issues",
        "software",
        "development",
        "Identifying",
        "Evaluate",
        "Technology",
        "Solutions",
        "Problem",
        "Solving",
        "Troubleshooting",
        "Serverside",
        "validations",
        "Struts",
        "Validation",
        "framework",
        "JSON",
        "response",
        "data",
        "web",
        "services",
        "filter",
        "search",
        "results",
        "Strutsconfigxml",
        "file",
        "Mapping",
        "Definitions",
        "Action",
        "Forward",
        "Definitions",
        "Action",
        "Classes",
        "view",
        "model",
        "layers",
        "Action",
        "Form",
        "Classes",
        "session",
        "state",
        "web",
        "application",
        "JSPs",
        "Java",
        "Server",
        "pages",
        "Struts",
        "tag",
        "libraries",
        "strutsconfigXML",
        "webxml",
        "application",
        "MVC",
        "architecture",
        "Hibernate",
        "database",
        "connectivity",
        "HQL",
        "Hibernate",
        "Query",
        "language",
        "tables",
        "Soap",
        "services",
        "JAXWS",
        "specifications",
        "Wrote",
        "JUnit",
        "test",
        "cases",
        "EnvironmentJava",
        "Struts",
        "Hibernate",
        "JSP",
        "Servlets",
        "SOAP",
        "UI",
        "HTML",
        "CSS",
        "Java",
        "Script",
        "JUnit",
        "Apache",
        "Tomcat",
        "Server",
        "EJB",
        "NetBeans",
        "Education",
        "Bachelors",
        "Skills",
        "PYTHON",
        "years",
        "MYSQL",
        "years",
        "ORACLE",
        "years",
        "years",
        "SQL",
        "years",
        "Additional",
        "Information",
        "TECHNICAL",
        "SKILLS",
        "Python",
        "Beautiful",
        "Soup",
        "NumPy",
        "SciPy",
        "Matplotlib",
        "pythontwitter",
        "Pandas",
        "data",
        "frame",
        "urllib2",
        "Data",
        "Analytics",
        "ToolsProgramming",
        "Python",
        "NumPy",
        "SciPy",
        "Gensim",
        "Keras",
        "R",
        "Caret",
        "Weka",
        "ggplot",
        "MATLAB",
        "Microsoft",
        "SQL",
        "Server",
        "Oracle",
        "PLSQL",
        "Python",
        "Database",
        "Oracle11",
        "g",
        "MySQL",
        "5x",
        "SQLServer",
        "Version",
        "Control",
        "SVN",
        "Clear",
        "case",
        "CVS",
        "Reporting",
        "Tools",
        "MS",
        "Office",
        "VisioOutlook",
        "Crystal",
        "Reports",
        "XI",
        "SSRS",
        "Cognos",
        "BI",
        "Tools",
        "Microsoft",
        "Power",
        "BI",
        "Tableau",
        "SSIS",
        "SSRS",
        "SSAS",
        "Business",
        "Intelligence",
        "Development",
        "Studio",
        "BIDS",
        "Visual",
        "Studio",
        "Crystal",
        "Reports",
        "Informatica",
        "Database",
        "Design",
        "Tools",
        "Data",
        "Modeling",
        "MS",
        "Visio",
        "ERWIN",
        "Star",
        "SchemaSnowflake",
        "Schema",
        "Fact",
        "Dimensions",
        "data",
        "Normalization",
        "Denormalization",
        "Kimball",
        "Inmon",
        "Methodologies",
        "IDEs",
        "PyCharm",
        "Emacs",
        "Eclipse",
        "NetBeans",
        "Sublime",
        "SOAP",
        "UI",
        "WebApp",
        "Servers",
        "WebSphere",
        "Application",
        "Server",
        "Apache",
        "Tomcat",
        "Web",
        "Logic",
        "g",
        "JBoss",
        "CloudTechnologies",
        "S3"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T22:09:01.209157",
    "resume_data": "Data Scientist Data Scientist Data Scientist BBVA Compass Birmingham AL Over 8 years of Experience in Data Architecture Design Developmentand Testing of business application systems Data Analysis and developing Conceptual logical models and physical database design for Online Transactional processing OLTP and Online Analytical Processing OLAP systems Experienced in designing star schema Snowflake schema for Data Warehouse and ODS architecture Experienced in Data Modeling Data Analysis experience using Dimensional Data Modeling and Relational Data Modeling Star SchemaSnowflake Modeling FACT Dimensions tables Physical Logical Data Modeling Experienced in big data analysis and developing data models using Hive PIG and Map reduce SQL with strong data architecting skills designing datacentric solutions Hands on experience with big data tools like Hadoop Spark Hive Pig Impala PySpark SparkSQL Very good knowledge and experience on AWS Redshift S3andEMR Proficient in Data Analysis mapping source and target systems for datamigration efforts and resolving issues relating to data migration Excellent development experience SQL Procedural LanguagePL of databases like Oracle TeradataNetezzaandDB2 Very good knowledge and working experience on big data tools like Hadoop Azure Data Lake AWS Redshift Experienced in Data ScrubbingCleansing Data Quality Data Mapping Data Profiling Data Validation in ETL Experienced in creating and documenting Metadata for OLTP and OLAP when designing systems Expertise in synthesizing Machine learning Predictive Analytics and Big data technologies into integrated solutions Extensive experience in development of TSQL DTS OLAP PLSQL Stored Procedures Triggers Functions Packages performance tuning and optimization for business logic implementation Experience in using various packages in Rand python like ggplot2 caret dplyr Rweka gmodels RCurl tm C50 twitter NLP Reshape2 rjson dplyr pandas NumPy Seaborn SciPy matplotlib Scikitlearn Beautiful Soup Rpy2 Experienced using query tools like SQL Developer PLSQL Developer and Teradata SQL Assistant Excellent knowledge of Machine Learning Mathematical Modeling and Operations Research Comfortable with R Python SAS and Weka MATLAB Relational databases Deep understanding exposure of Big Data Ecosystem Expertise in designing complex Mappings and have expertise in performance tuning and slowlychanging Dimension Tables and Fact tables Extensively worked with Teradata utilities BTEQ Fast export and MultiLoad to export and load data tofrom different source systems including flat files Hands on experience in implementing LDA Naive Bayes and skilled in Random Forests Decision Trees Linear and Logistic Regression SVM Clustering neural networks Principle Component Analysis Expertise in extracting transforming and loading data between homogeneous and heterogeneous systems like SQL Server Oracle DB2 MS Access Excel Flat File and etc using SSIS packages Proficient in System Analysis ERDimensional Data Modeling Database design and implementing RDBMS specific features Experience in UNIX shell scripting Perl scriptingand automation of ETL Processes Extensively used ETL to load data using Power Center Power Exchange from source systems like Flat Files and Excel Files into staging tables and load the data into the target database Oracle Analyzed the existing systems and made a Feasibility Study Excellent understanding and working experience of industry standard methodologies like System Development Life Cycle SDLC as per Rational Unified Process RUP Agile Methodologies Proficiency in SQL across a number of dialects we commonly write MySQL PostgreSQL Redshift SQL Server and Oracle Experienced in developing EntityRelationship diagrams and modeling Transactional Databases and Data Warehouse using tools like ERWIN ERStudioandPower Designer and experienced with modeling using ERWIN in both forward and reverse engineering cases Authorized to work in the US for any employer Work Experience Data Scientist BBVA Compass Birmingham AL July 2017 to Present Architect framework BBVA Compass Bancshares Inc Birmingham AL 2007 to Present is a bank holding company headquartered in Birmingham Alabama It has been a subsidiary of the Spanish multinational Banco Bilbao Vizcaya Argentaria since 2007 and operates chiefly in Alabama Arizona California Colorado Florida New Mexico and Texas Responsibilities Design Develop and implement Comprehensive Data Warehouse Solution to extract clean transfer load and manage qualityaccuracy of data from various sources to EDW Enterprise Data Warehouse Architect framework for data warehouse solutions to bring data from source system to EDW and provide data mart solutions for OrderSales operation Salesforce activity Inventory tracking indepth data mining and analysis for market projection etc Utilized Apache Spark with Python to develop and execute Big Data Analytics and Machine learning applications executed machine learning use cases under Spark ML and MLLib Handled importing data from various data sources performed transformations using Hive Map Reduce and loaded data into HDFS Performed data cleaning and feature selection using MLLib package in PySpark and working with deep learning frameworks such as Caffe Neon Tested Complex ETL Mappings and Sessions based on business user requirements and business rules to load data from source flat files and RDBMS tables to target tables Utilized Spark Scala Hadoop HBase Cassandra MongoDB Kafka Spark Streaming MLLib Python a broad variety of machine learning methods including classifications regressions dimensionally reduction etc and Utilized the engine to increase user lifetime by 45 and triple user conversations for target categories Develop a high performance scalable data architecture solution that incorporates a matrix of technology to relate architectural decision to business needs Conducting strategy and architecture sessions and deliver artifacts such as MDM strategy Current state Interim Stateand Target state and MDM Architecture Conceptual Logical and Physical at the detail level Design and development of dimensional data model on Redshift to provide advanced selection analytics platform and developed Simple to complex Map Reduce Jobs using Hive and Pig Designed and developed NLP models for sentiment analysis Designed and provisioned the platform architecture to execute Hadoop and machine learning use cases under Cloud infrastructure AWS EMR and S3 Developed and configured on Informatica MDM hub supports the Master Data Management MDM Business Intelligence BI and Data Warehousing platforms to meet business needs Transforming staging area data into a STAR schema hosted on Amazon Redshift which was then used for developing embedded Tableau dashboards Worked on machine learning on large size data using Spark and MapReduce Let the implementation of new statistical algorithms and operators on Hadoop and SQL platforms and utilized optimizations techniques linear regressions Kmeans clustering NativeBayes and other approaches Developed SparkScala Python for regular expression regex project in the HadoopHive environment with LinuxWindows for big data resources Proficiency in SQL across a number of dialects we commonly write MySQL PostgreSQL Redshift Teradata and Oracle Responsible for full data loads from production to AWSRedshift staging environment and Worked on migrating of EDW to AWS using EMR and various other technologies Worked on TeradataSQLqueries Teradata Indexes Utilities such as Mload TPump Fast load and Fast Export Application of various machine learning algorithms and statistical modeling like decision trees regression models neural networks SVM clustering to identify Volume using Scikitlearn package in python MATLAB Worked on data preprocessing and cleaning the data to perform feature engineering and performed data imputation techniques for the missing values in the dataset using Python Created Data Quality Scripts using SQL and Hive to validate successful data load and quality of the data Created various types of data visualizations using Python and Tableau Worked with BTEQ to submit SQL statements import and export data and generate reports in Teradata Build and maintain scalable data pipelines using the Hadoop ecosystem and other open sources components like Hive and HBase Created Hive architecture used for realtime monitoring and HBase used for reporting and worked for map reduce and query optimization for Hadoop Hive and HBase architecture Involved in Teradata utilities BTEQ Fast Load Fast Export Multiload and TPump in both Windows and Mainframe platforms Built analytical data pipelines to port data in and out of HadoopHDFS from structured and unstructured sources and designed and implemented system architecture for Amazon EC2 based cloudhosted solution for the client Environment Erwin964 Oracle 12c Python PySpark Spark Spark MLLib Tableau ODS PLSQL OLAP OLTP AWS Hadoop Map Reduce HDFS Python MDM Teradata 15 Hadoop Spark Cassandra SAP MS Excel Flat files Tableau Informatica SSIS SSRS AWS EC2 AWS EMR Elastic Search Data Scientist BBVA Compass Bancshares Inc Chicago IL August 2016 to June 2017 DescriptionAt AIM Specialty Health AIM it is our mission to promote appropriate safe and affordable health care As the specialty benefits management partner of choice for todays leading healthcare organizations we help improve the quality of care and reduce costs for todays most complex tests and treatments Responsibilities Developed applications of MachineLearning Statistical Analysisand Data Visualizations with challenging data Processing problems in sustainability and biomedical domain Worked on Natural Language Processing with NLTK module of python for application development for automated customer response Used predictive modeling with tools in SAS SPSS R Python Responsible for design and development of advanced R Python programs to prepare to transform and harmonize data sets in preparation for modeling Identifying and executing process improvements handson in various technologies such as Oracle Informatica and BusinessObjects Handled importing data from various data sources performed transformations using Hive Map Reduce and loaded data into HDFS Interaction with Business Analyst SMEsand other Data Architects to understand Business needs and functionality for various project solutions Created SQL tables with referential integrity and developed queries using SQL SQLPLUSand PLSQL Involved with Data Analysis primarily Identifying Data Sets Source Data Source Meta Data Data Definitions and Data Formats Wrote simple and advanced SQL queries and scripts to create standard and Adhoc reports for senior managers Created PLSQL packages and Database Triggers and developed user procedures and prepared user manuals for the new programs Prepare ETLarchitect design document which covers ETLarchitect SSISdesign Extraction transformationand loading of Duck Creek data into the dimensional model Design Logical Physical Data Model using MSVisio 2003 data modeler tool Participated in Architect solution meetings guidance in Dimensional Data Modeling design Applied linear regression multiple regression ordinary least square method meanvariance the theory of large numbers logistic regression dummy variable residuals Poisson distribution Bayes Naive Bayes fitting function etc to data with help of Scikit SciPy NumPy and Pandas module of Python Applied clustering algorithms ie Hierarchical Kmeans with help of Scikit and SciPy Developed visualizations and dashboards using ggplot Tableau Worked on development of data warehouse Data Lake and ETL systems using relational and nonrelational tools like SQL No SQL Built and analyzed datasets using R SAS MATLABand Python in decreasing order of usage Applied linear regression in Python and SAS to understand the relationship between different attributes of the dataset and causal relationship between them Pipelined ingestcleanmungetransform data for feature extraction toward downstream classification Used ClouderaHadoopYARN to perform analytics on data in Hive Wrote Hive queries for data analysis to meet the business requirements Expertise in BusinessIntelligence and data visualization using R and Tableau Validated the MacroEconomic data eg BlackRock Moodys etc and predictive analysis of world markets using key indicators in Python and machine learning concepts like regression Bootstrap Aggregation and Random Forest Worked in largescale database environments like Hadoop and MapReduce with working mechanism of Hadoop clusters nodes and Hadoop Distributed File System HDFS Interfaced with largescaledatabase system through an ETL server for data extraction and preparation Identified patterns data quality issues and opportunities and leveraged insights by communicating opportunities with business partners EnvironmentMachine learning AWS MS Azure Cassandra Spark HDFS Hive Pig Linux Python ScikitLearnSciPyNumPyPandas R SAS SPSS MySQL Eclipse PLSQL SQL connector Tableau Python Developer Walgreens Chicago IL March 2015 to July 2016 DescriptionThe Walgreen Company is an American company that operates as the secondlargest pharmacy store chain in the United States behind CVS Health It specializes in filling prescriptions health and wellness products health information and photo services Responsibilities Designed and Developed UI for creating Dashboard application using AngularJS D3 C3 HTML CSS Bootstrap JavaScript and jQuery Developed and implemented Python scripts to automate retrieval parsing and reporting of configuration parameters from Network Devices connected to customer networks Involved in user interface design and development using JSP Servlet HTML5 CSS3andJavaScript Wrote and tested Python scripts to create new data files for Linux server configuration using a Python template tool Modified controlling databases using SQL generated via Python and Perl code collected and analyzed data with Python programs using SQL queries from the database of data collected from the systems under tests Developed new user interface components for different modules using Kendo UI with various controls including Grid controls and chart controls etc Involved in write application level code to interact with APIs Web Services using AJAX JSON and hence building typeahead feature for zip code city and county lookup using jQuery Ajax and jQuery UI Worked on updating the existing clipboard to have the new features as per the client requirements Skilled in using collections in Python for manipulating and looping through different userdefined objects Taken part in the entire lifecycle of the projects including Design Development and Deployment Testing and Implementation and support Developed views and templates with Python and Djangos view controller and templating language to create a userfriendly website interface Automated different workflows which are initiated manually with Python scripts and Unix shell scripting Used Python unit and functional testing modules such as unit test unittest2 mock and custom frameworks inline with Agile Software Development methodologies Wrote and executed various MYSQL database queries from Python using PythonMySQL connector and MySQL dB package Created and maintained Technical documentation for launching Hadoop Clusters and for executing Hive queries and Pig Scripts Developed Sqoop scripts to handle change data capture for processing incremental records between new arrived and existing data in RDBMS tables Installed Hadoop Map Reduce HDFS AWS and developed multiple MapReduce jobs in PIG and Hive for data cleaning and preprocessing Managed datasets using Panda data frames and MySQL queried MYSQL database queries from python using PythonMySQL connector and MySQL dB package to retrieve information Involved in the WebApplication development using Python HTML5 CSS3 AJAX JSONandjQuery Developed and tested many features for a dashboard using Python Java Bootstrap CSS JavaScript and jQuery Generated Python Django forms to record data of online users and used PyTest for writing test cases Implemented and modified various SQL queries and Functions Cursors and Triggers as per the client requirements Prototype proposal for Issue Tracker website using PythonDjango connecting MySQL as Database The developed overall layout that meetscrossplatform compatibility using Bootstrap media queries and Angular UI Bootstrap Environment Python HTML5 CSS3 AJAX JSON jQuery MySQL NumPy SQL Alchemy Matplotlib Hadoop Pig Scripts Python Developer Citi Bank Irving TX July 2013 to February 2015 DescriptionThe project was to build an algorithm that accurately classifies credit card holders among multiple classes based on the historical data available on multiple variables Further the aim was to improve banks efficiency by reducing default rate while offering new products Moreover I was Involved in a project to identify the employees access level based on hisher current historical tasks and duties Responsibilities Involved in the software development lifecycle SDLC of tracking the requirements gathering analysis detailed design development system testing and user acceptance testing Developed entire frontend and backend modules using Python on Django Web Framework Involved in designing user interactive web pages as the frontend part of the web application using various web technologies like HTML JavaScript Angular JS jQuery AJAX and implemented CSS for better appearance and feel Knowledge of crossbrowser and crossplatform development of HTML and CSS based websites in Windows like IE 6 IE 7 IE 8 and FF Interactive in providing change requests trouble reports and requirements collection with the client Actively involved in developing the methods for Create Read Update and Delete CRUD in Active Record Working knowledge of various AWS technologies like SQS Queuing SNS Notification S3 storage Redshift Data Pipeline EMR Developed a fully automated continuous integration system using Git Jenkins MySQL and custom tools developed in Python and Bash Implemented Multithreading module and complex networking operations like race route SMTP mail server and web server Using Python Used NumPy for Numerical analysis for the Insurance premium Implemented and modified various SQL queries and Functions Cursors and Triggers as per the client requirements Managed code versioning with GitHub BitBucketand deployment to staging and production servers Implemented MVC architecture in developing the web application with the help of Django framework Used Celery as task queue and Rabbit MQ Redis as messaging broker to execute asynchronous tasks Designed and managed API system deployment using a fastHTTP server and Amazon AWS architecture Involved in code reviews using GitHub pull requests reducing bugs improving code quality and increasing knowledge sharing Install and configuring monitoring scripts for AWS EC2 instances Working under UNIX environment in the development of application using Python and familiar with all its commands Developed remote integration with thirdparty platforms by using RESTful web services Updated and maintained Jenkins for automatic building jobs and deployment Improved code reuse and performance by making effective use of various design patterns and refactoring code base Updated and maintained Puppet RSpec unitsystem test Worked on debugging and troubleshooting programming related issues Worked in the MySQL database on simple queries and writing Stored Procedures for normalization Deployment of the web application using the Linux server Environment Python 27 Django 14 HTML5 CSS XML MySQL JavaScript Backbone JS JQuery Mongo DB MS SQL Server JavaScript Git GitHub AWS Linux Shell Scripting AJAX JAVA Hadoop Developer SITEL India Pvt LTD Hyderabad Telangana August 2012 to June 2013 Description Sitel Group combines comprehensive customer care capabilities with unparalleled digital training and technology expertise to help build brand loyalty and improve customer satisfaction We partner with our clients to effectively harness our industrys explosive digital transformation to ensure an innovative endtoend solution to managing and enhancing the customer experience Responsibilities Designed and developed multiple MapReduce jobs in Java for complex analysis Importing and exporting the data using Sqoop from HDFS to Relational Database systems and viceversa Integrated Apache Storm with Kafka to perform web analytics Uploaded clickstream data from Kafka to HDFS HBase and Hive by integrating with Storm Configured Flume to transport web server logs into HDFS Also used Kite logging module to upload web server logs into HDFS Developed UDF functions for Hive and wrote complex queries in Hive for data analysis Performed Installation of Hadoop in fully and Pseudo Distributed Mode for POC in early stages of the project Analyze develop integrate and then direct the operationalization of new data sources Generating Scala and Java classes from the respective APIs so that they can be incorporated into the overall application Responsible for working with different teams in building Hadoop Infrastructure Gathered business requirements in meetings for successful implementation and POC and moving it to Production and implemented POC to migrate map reduce jobs into Spark RDD transformations using Scala Implemented different machine learning techniques in Scala using Scala machine learning library Developed Spark applications using Scala for easy Hadoop transitions Successfully loaded files to Hive and HDFS from Oracle Netezza and SQL Server using SQOOP Uses Talend Open Studio to load files into Hadoop HIVE tables and performed ETL aggregations in Hadoop Hive Developed Simple to Quebec and Python MapReduce streaming jobs using Python language that is implemented using Hive and Pig Designing Creating ETL Jobs through Talend to load huge volumes of data into Cassandra Hadoop Ecosystem and relational databases Worked on analyzing writing Hadoop MapReduce jobs using Java API Pig and Hive Developed some machine learning algorithms using Mahout for data mining for the data stored in HDFS Used Flume extensively in gathering and moving log data files from Application Servers to a central location in Hadoop Distributed File System HDFS Worked with Oozie Workflow manager to schedule Hadoop jobs and high intensive jobs Responsible for cluster maintenance adding and removing cluster nodes cluster monitoring and troubleshooting manage and review data backups manage and review Hadoop log files Extensively used HiveHQL or Hive queries to query data in Hive Tables and loaded data into HIVE tables Creating UDF functions in Pig Hive and applying partitioning and bucketing techniques in Hive for performance improvement Creating indexes and tuning the SQL queries in Hive and Involved in database connection by using Sqoop Involved in Hadoop Name node metadata backups and load balancing as a part of Cluster Maintenance and Monitoring Used File System Check FSCK to check the health of files in HDFS and used Sqoop to import data from SQL server to Cassandra Monitored Nightly jobs to export data out of HDFS to be stored offsite as part of HDFS backup Used Pig for analysis of large datasets and brought data back to HBaseby Pig Developed Python Mapper and Reducer scripts and implemented them using Hadoop streaming Created schema and database objects in HIVE and developed Unix Scripts for data loading and automation Involved in the training of big data ecosystem to endusers Environment Java15 J2EE Hibernate Spring JUnit WebLogic HTML CSS JavaScript Jenkins Nodejs jQuery Linux CICD Spring Boot Maven Log4J and Junit Eclipse REST SQL Navigator Java Developer Napier Healthcare Pvt Ltd Hyderabad Telangana October 2009 to July 2011 Description Napier Healthcare was established in 1996 as a Healthcare IT Products and Services Company With deep domain knowledge and singular focus we have built a robust featurerich suite of solutions to deliver value to the stakeholders of the healthcare industry worldwide Responsibilities Development Testing Maintenance and product delivery Developed a scalable and maintainable application using J2EE Framework Hibernate MVC Model Struts and J2EE Design Patterns Prepared SOW Statement of Work by communicating with agencies and organized meetings about requirements Followed Java J2EE design patterns and the coding guidelines to design and develop the application Extensively used JSTL tags and Struts tag libraries Used Struts tiles as well in the presentation tier Developing the application using Struts and Spring based frameworks Actively involved in designing and implementing the application using various design patterns Coordinating with clients and closing production issues relating to software development Identifying and Evaluate Technology Solutions Problem Solving and Troubleshooting Done with Serverside validations using Struts Validation framework Processed JSON response data by consuming RESTful web services and used an Angular filter for implementing search results Used Strutsconfigxml file for defining Mapping Definitions and Action Forward Definitions Developed the Action Classes which is in between view and model layers Action Form Classes which is used to maintain session state of a web application created JSPs Java Server pages using Struts tag libraries and configured in strutsconfigXML webxml files This application is designed using MVC architecture to maintain easily Hibernate is used for database connectivity and designed HQL Hibernate Query language to create modify and update the tables Created new Soap services using JAXWS specifications Wrote JUnit test cases for testing EnvironmentJava Struts Hibernate JSP Servlets SOAP UI HTML CSS Java Script JUnit Apache Tomcat Server EJB NetBeans Education Bachelors Skills PYTHON 10 years MYSQL 10 years ORACLE 10 years PLSQL 10 years SQL 10 years Additional Information TECHNICAL SKILLS Python Libraries Beautiful Soup NumPy SciPy Matplotlib pythontwitter Pandas data frame urllib2 Data Analytics ToolsProgramming Python NumPy SciPy pandas Gensim Keras R Caret Weka ggplot MATLAB Microsoft SQL Server Oracle PLSQL Python Database Oracle11g MySQL 5x and SQLServer Version Control SVN Clear case CVS Reporting Tools MS Office WordExcelPowerPoint VisioOutlook Crystal Reports XI SSRS Cognos 7060 BI Tools Microsoft Power BI Tableau SSIS SSRS SSAS Business Intelligence Development Studio BIDS Visual Studio Crystal Reports Informatica 61 Database Design Tools and Data Modeling MS Visio ERWIN 4540 Star SchemaSnowflake Schema modeling Fact Dimensions tables physical logical data modeling Normalization and Denormalization techniques Kimball Inmon Methodologies IDEs PyCharm Emacs Eclipse NetBeans Sublime SOAP UI WebApp Servers WebSphere Application Server 80 Apache Tomcat Web Logic 11g 12c JBoss 4x5x CloudTechnologies AWS S3",
    "unique_id": "331b8584-9ab6-487a-922d-d373b90a6f89"
}