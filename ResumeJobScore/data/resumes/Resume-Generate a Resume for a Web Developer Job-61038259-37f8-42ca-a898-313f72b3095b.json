{
    "clean_data": "Software Engineer ETL Developer span lSoftwarespan Engineer ETL span lDeveloperspan Software Engineer ETL Developer Zions Bancorporation Salt Lake City UT Authorized to work in the US for any employer Work Experience Software Engineer ETL Developer Zions Bancorporation March 2019 to Present Location SaltLakeCityUtah Project Description ZionsBancorporation is one of the largest Financial services bank based out in Salt Lake City Utah It has six different subsidiaries which operates under the control of Zions Bank Now Zionsbancorporation want to merge all its subsidaries banks into one centralized system For this they are using BANCS application which migrates data from each subsidiary to BANCS They want to decommission their application on TRISON and want to migrate all subsidaries data to BANCS applications As part of Migration we are using ETL DataStage which extracts data from Mainframe applicationEBCDIC to ASCII format conversion Also we are applying different Transformation logics to fields that are needed as part of Migration Here we are migrating data on each subject area which involves RCIFTIME DEPOSITS DDACRL Accounts Data Responsibilities Designs develops and tests ETL applications using Datastage 113 with application architecture guidelines Access disparate data sources databases hardware devices VSAM and flat files and apply internal and external business rules policies and data quality measures Use 253 files and apply all Transformation logics accordingly to generate middle file as expected that helps in loading to BANCS applicationFor this worked on SequentialfilestageTransformerJoinlookupCopyFunnel Aggregator stages Worked on Master sequence for generating summary reportSnapshot report accordingly Gathers mapping and reporting requirements and coordinate all data movement activities Insures the data is safely and securely transferred to target destinations Analyzes business requirements as they relate to the ETL process research evaluate and recommend alternative solutions Transforms business requirements into technical requirement documents Analyzes develops installs tests upgrades maintains and supports complex dataprocess models and processes in an ETL environment Works with management and staff to establish best practice standards for ETL functions and development of new methodologies for supporting data transformations and processing in a corporatewide solutions Mercedes Benz BI Data Migration Senior Datastage Developer and Tableau Developer Mercedes Benz Atlanta GA September 2017 to October 2018 Project Description MBUSAis a distribution company for cars in USA The main objective of the project is to migrate different applications running on existing Mainframes to Data stage This include Warranty as the major application Along with this there involves other applications like ASBDCRM 1WarrantyCRM These are main application in which we need to extract the data from DB2 and load data to flat files and xml and also to target DB2in this application we need to design parallel DS jobs based on mappings prepared from mainframe code in Prod environment Once parallel job is designed and tested then a subsequence followed with master sequence is developed This entire mechanism is running on mainframes Now as part of Datastage we need to implement the migration of code running in mainframes and implement the same ETL mechanism in Datastage Responsibilities involved in understanding the existing Mainframe COBOL code and prepare the mapping based on the code involved in Developing Datastage Job based on mapping and Cobol code such that both functionalities remain same For this we validate the data in DB2 worked on different transformation logicsnull  Developed complete sequencer with all logics handled for each subject area which includes Execution commanduser variable activities mail notification Involved in reducing the complexity of code by writing the DB2 queries and also done portioning at Datastage job level Tested the job in all environments till the code moves from SIT to UAT After Loading data to warehouse tables prepare reports and dashboard usingActionsfiltersCalculated fields and deploy to tableau server Build customize and publish interactive reports visualizations and dashboards using Tableau Worked in Drill down and drill up hirachiescalculated fields and parametrization in Tableau Worked on different charts like Heat Maps Scatterd ChartsDo nut chart and also Created complex reports utilizing the features like hierarchy New Calculated Columns Drill functionality Tables graphs line Charts and Bar Charts Worked mainly on Interactive Dashbaords with best performance tuning mecahnisms Deployed the code to production as per the Sprint timelines with all the CRQ approved and documents needed Monitored the Jobs in production using Control M in doing support activities and complete the batch as per the SLA Create basic calculations including string manipulation basic arithmetic calculations custom aggregations and ratios date math logic statements and quick table calculations Developed Adhoc reports using Tableau Desktop Excel Developed visualization Dashboards using sets Parameters Calculated Fields Dynamic sorting Filtering Parameter driven analysis Use Measure name and Measure Value fields to create visualizations with multiple measures and dimensions Combine the visualizations into Interactive Dashboards and publish them to the Server Created the SharePoint folder and maintained all the documents of the project and code Invloved in Status reporting on a daily weekly basis of the project to ensure that all code is running with zero defects worked in Reusability of the code and Performance Tuning activities of Datastage Tools used Datastage 117 Putty IBM Datastudio 41DB2MainframesTableau desktop 20191 Datastage developerTableau Developer Michaels Ecommerce Business Intelligence Irving TX September 2015 to August 2017 Project Description The Michaels Companies Inc is North Americas largest provider of arts crafts framing floral wall dcor and merchandise for makers and doityourself home decorators The company owns and operates more than 1250 Michaels stores Aaron Brothers Custom Framing storewithinastore Artistree a manufacturer of high quality custom and specialty framing merchandise and Darice a wholesale distributor to the craft gift and dcor industry The companys flagship is Michaels Stores As of August 2018 the company reported that in addition to its Michaels Stores brand it produces over a dozen private brands including Recollections Studio Decor As part of Ecommerce Micheals used to sell their products through alternate channel EbayAs part of existing architecture they are going to decommission channel through e bay and want to set up their own online store such that every transactions willhapen with in Michaels store Responsibilities involved in understanding the existing Architecture and prepare the mapping based on the code The work involved preparing the LLD mappings based on the Business from HLD on each subject area like Item MasterInventoryPOSPoint of Sales The work involved Design and develop the jobs as per mappings with zero defects and good performance Involved gathering requirement from client ensuring quality deliverables and giving technical solutions to the team Worked on different files like XML filessequentials files datasets as part of datastage jobs design Data validation was done in various systems like WMSTIPS and also Data stage Jobs Once data is validated data is send to downstream applications which is WMS 2 Proficient in design and development of various dashboards reports utilizing Tableau Visualizations like Bar chart scatter plot Dual axis charts Water fall charts Donut charts Bubble charts Heat maps Line charts Cross tab Geographic VisualizationMaps and making use of actions for interactivity and global filters according to the businessclient requirement Worked extensively with Advance analysis Actions Calculations LODs Parameters Background images and Maps in Tableau Set parameters for viewing data as daily weekly monthly quarterly and yearly format Developed Trend Lines Reference Lines and statistical techniques to describe the data Involved job execution and checking the logs and rectify them Through Datastage Director and ensure code runs with zero warnings and errors Involved in Production support activities by monitoring the jobs From Control M if any jobs fails then check in Unix script followed with Datstage Job name and fix accordingly Created folder structure and uploaded all the documents in share point to keep track Status reporting on a daily weekly basis defect capture retesting and closure of defects Tools used Datastage 113 Putty Oracle SQL developerControl MSql AssistantTeradatatableau Desktop 20181 Datastage version code Migration 87 to 91 Software developerTableau Developer DNB Bank Bergen NO September 2014 to July 2015 Norway Project Description DNB ASA formerly DnB NOR ASA is Norways largest financial services group with total combined assets of more than NOK 19 trillion and a market capitalisation NOK164 billion as per 20 May 2016 DNBs head office is located in Oslo The two largest owners of DNB are the Norwegian Ministry of Trade and Industry and Sparebankstiftelsen DnB NOR The latter was created as a foundation with the sole purpose of owning part of the company The main objective is to migrate the existing code running on Datastage 87 to 91 version such that all other systems run on same Unix Also once the code migrated we need to validate the data from databases and other file systems Responsibilities Was involved in the project from requirements analysis phase interacted with customer as needed to get better understanding of requirements and responsible for Planning of test execution based on priority of the requirements for business Involved in taking the back up of dsx files from production and created all the folders and file systems in development of 91 version created the project folder and environmental variables and user defined variables in development environment of 91 version then imported the dsx file After import compiled all the jobs using Multi jobs compile and ran the jobs by pointing the server to new version 91 performed SITUAT to ensure data is fine in both environments 87 and 91 and capture all the test results and documented accordingly Implemented the same export and import of data from development environment of 91 to test environment and tested again Finally deployed the code to production environment 91 and monitored the jobs using ControlM such that all jobs running fine without any errors and warnings Fix the code accordingly if there are any issues come up while executing the jobs by monitoring the logs in Datastage Director Tools used Datastage 8791 Putty DB2CONTROLM Software developer StarBucks DataFiltering Seattle WA October 2013 to 2014 Project Description starbucks Corporation is an American coffee company and coffeehouse chain Starbucks was founded in Seattle Washington in 1971 As of 2018 the company operates 28218 locations worldwideStarbucks is considered the main representative of second wave coffee initially distinguishing itself from other coffeeserving venues in the US by taste quality and customer experience while popularizing darkly roasted coffee Since the 2000s third wave coffee makers have targeted qualityminded coffee drinkers with handmade coffee based on lighter roasts while Starbucks nowadays uses automated espresso machines for efficiency and safety reasons The main objective is to refine datasets that are having junk and older data which is loading to database so as part of existing jobs we are adding new extra logic in sql code such that junk data is removed and proper data is loaded to datasets Responsibilities Work involved Performance Tuning Techniques at Query level and Data stage job Level to Improve the Execution Time The work involves formulate testing strategy test planning identifying test scenarios test data set up ensuring quality deliverables and Design Remodified the existing queries by adding some extra logic to remove unwanted data by filtering at date level so that refined data is loaded to datasets Status reporting on a daily weekly basis defect capture retesting and closure of defects Created folder structure and uploaded all the documents in share point to keep track Used Data Stage Director to identify the logs and rectify them with Zero errors Extensively used data stage to check for logs and Performed Partioninghash same as per required and used Oracle to query database Tools used Datastage 87Putty TeradataSql plusCRONTAB Software developer TargetEnterprise DatawarehouseEDW February 2011 to 2013 Location Minneapolis Project Description Target Corporation is the eighthlargest retailer in the United States and is a component of the SP 500 IndexFounded by George Dayton and headquartered in Minneapolis the company was originally named Goodfellow Dry Goods in June 1902 before being renamed the Daytons Dry Goods Company in 1903 and later the Dayton Company in 1910 The first Target store opened in Roseville Minnesota in 1962 while the parent company was renamed the Dayton Corporation The main objective is to load data from multiple systems to enterprise warehouse this involves design of parallel jobs and followd with sequnecrs and Unix scripts accordingly Responsibilities Understanding the business functionality Analysis of business requirements Our responsibility starts from the point where a file from our source system lands in our server and ends when it is loaded to Target tables Monitoring jobs and viewing logs for the jobs Running and Monitoring the Jobs in Director and performing Cleanup resources if needed Used Plugin Meta Data to import the source and target table definitions Used most of the Stages in Data stage such as the Transformer Stage SCD Stage Change Capture Aggregator Stage filter stage job parameters Sequential file Stage Used Lookup Stage efficiently for storing the primary rejected data for reference Involved on Deployments ie deploying code from DEV to UAT and UAT to PROD The work involves gathering requirement from client Preparing Mapping Specifications formulate testing strategy test planning identifying test scenarios test data set up ensuring quality deliverables and Design Effectively interacts with clients Status reporting on a daily weekly basis defect capture retesting and closure of defects Created folder structure and uploaded all the documents in share point to keep track Used Data Stage Director to identify the logs and rectify them with Zero errors Coordinating with onsite team and involved in Oncalls Project progress reviews Tools used Datastage 87unixPutty Sql plusTeradataControl M Datastage developer AetnaPredictive April 2009 to 2011 Location Hartford Project Description Aetna Incis an American managed health care company that sells traditional and consumer directed health care insurance plans and related services such as medical pharmaceutical dental behavioral health longterm care and disability plans primarily through employerpaid fully or partly insurance and benefit programs and through Medicare Since November 28 2018 the company has been a subsidiary of CVS HealthThe companys network includes million medical members 127 million dental members 131 million pharmacy benefit management services members 1200000 healthcare professionals over 690000 primary care doctors and specialists and over 5700 hospitals The goal of the Claims Fraud Analytics project is to identify the fraudulent claims from the information available from internal and external sources In order to identify fraud Selective Insurance has developed models based on a certain set of variables to score the claims The Fraud Analytics Variable Development application was developed to pull information from various internal and external sources and present the information to the User to do mining on the data This process also includes the scoring of all the current open claims on a daily basis Responsibilities Understanding the business functionality Analysis of business requirements Designed and developed DataStage ETL Parallel Jobs between Source and Target using sequential file stagedatasetoracle connectorsortremove duplicatesaggregatorchange capturetransformations accordingly Import and export the DSX from one environment to test for testing purpose created the jobs using change capture to handle SCD Type 2 dimensions accordingly Involved in promoting the code from DEV to UAT and to support QA team for each and every querys Resolving the defects assigned by the QA and Business team Involved in deploying code to UAT and PROD Involved in UAT Production support for the earlier releases for Daily loads pre verified and Monthly Coordinating with onsite team and involved in Oncalls Project progress reviews Tools used Datastage 85puttysql plusDB2NotepadGIT HubControl M Education Bachelor of Technology in Electronics and Communication Engineering Jawaharlal Nehru technology University College of Engineering Skills Hdfs Datastage Db2 Replication Teradata Database Ms access Oracle Sql Tableau Data replication Unix Unix shell Git Hive Real time Xml Itil Retail Excel Additional Information Technical Skills Languages SQLUNIX Shell Script XML Database Oracle 9i10g11g DB2WINSQLNetezzaTeradataSQL Assistant BI Tools IBM Datastage 11x9x8x75Tableau 20191 Big data HDFSHive Real Time App IBM CDCIBM data Replication ITIL Tools Service now ITSM Scheduling tools Control M Zeke Utilities Git hubZira Operating Systems Windows XP2007Vista Software Applications MS Word MS Excel MS Access Outlook PowerPoint Business areas Automobile RetailBankingInsurance",
    "entities": [
        "IndexFounded",
        "Developed Trend Lines Reference Lines",
        "Xml Itil Retail Excel Additional Information Technical Skills Languages SQLUNIX Shell Script XML Database Oracle",
        "Tableau Set",
        "Starbucks",
        "Import",
        "Monthly Coordinating",
        "Tableau Desktop Excel Developed",
        "Interactive Dashboards",
        "ITSM Scheduling",
        "Monitored the Jobs",
        "Mainframes to Data",
        "TRISON",
        "the Dayton Corporation",
        "XML",
        "Tableau Visualizations",
        "sequnecrs",
        "Project Description",
        "Parameters Background",
        "DataStage",
        "Advance analysis Actions Calculations LODs",
        "UAT",
        "Target",
        "Datastage",
        "Ecommerce Micheals",
        "Measure Value",
        "Used Plugin Meta Data",
        "New Calculated",
        "Insures",
        "BANCS",
        "Salt Lake City",
        "PROD Involved",
        "University College of Engineering Skills Hdfs Datastage Db2",
        "Tableau Worked",
        "Oracle Sql Tableau Data",
        "Sequential",
        "Use Measure",
        "Datastage Tools",
        "Selective Insurance",
        "Works",
        "Goodfellow Dry Goods",
        "The Fraud Analytics Variable Development",
        "Project Description starbucks Corporation",
        "Minneapolis",
        "Mercedes Benz BI Data Migration Senior Datastage Developer",
        "the Norwegian Ministry of Trade and Industry",
        "USA",
        "Architecture",
        "Aaron Brothers Custom",
        "Responsibilities Understanding",
        "Software Engineer ETL Developer",
        "DSX",
        "Migration",
        "Location Minneapolis Project Description Target Corporation",
        "HubControl M Education Bachelor of Technology in Electronics and",
        "SIT",
        "Replication ITIL Tools Service",
        "US",
        "WMSTIPS",
        "QA",
        "CVS HealthThe",
        "Created",
        "Warranty",
        "Oracle",
        "Work Experience Software Engineer ETL",
        "DnB NOR ASA",
        "Zions Bank Now Zionsbancorporation",
        "Seattle",
        "Drill",
        "Oracle SQL",
        "Artistree",
        "Transformation",
        "Financial services",
        "WMS 2 Proficient",
        "The Michaels Companies Inc",
        "TargetEnterprise",
        "Cleanup",
        "the United States",
        "Control M",
        "HLD",
        "Oslo",
        "Filtering Parameter",
        "Interactive Dashbaords",
        "Present Location SaltLakeCityUtah Project Description ZionsBancorporation",
        "CRQ",
        "the Dayton Company",
        "ETL",
        "Developing Datastage Job",
        "Medicare",
        "Oncalls Project",
        "mecahnisms",
        "Utah",
        "Status",
        "DEV",
        "DNB Bank",
        "the Claims Fraud Analytics",
        "BI Tools IBM Datastage",
        "Control M Zeke Utilities Git",
        "Multi",
        "DNB",
        "SharePoint",
        "the Daytons Dry Goods Company",
        "Tableau",
        "Michaels Stores",
        "Washington",
        "ETL DataStage",
        "Datastage 87Putty",
        "the Server Created",
        "Sprint"
    ],
    "experience": "Experience Software Engineer ETL Developer Zions Bancorporation March 2019 to Present Location SaltLakeCityUtah Project Description ZionsBancorporation is one of the largest Financial services bank based out in Salt Lake City Utah It has six different subsidiaries which operates under the control of Zions Bank Now Zionsbancorporation want to merge all its subsidaries banks into one centralized system For this they are using BANCS application which migrates data from each subsidiary to BANCS They want to decommission their application on TRISON and want to migrate all subsidaries data to BANCS applications As part of Migration we are using ETL DataStage which extracts data from Mainframe applicationEBCDIC to ASCII format conversion Also we are applying different Transformation logics to fields that are needed as part of Migration Here we are migrating data on each subject area which involves RCIFTIME DEPOSITS DDACRL Accounts Data Responsibilities Designs develops and tests ETL applications using Datastage 113 with application architecture guidelines Access disparate data sources databases hardware devices VSAM and flat files and apply internal and external business rules policies and data quality measures Use 253 files and apply all Transformation logics accordingly to generate middle file as expected that helps in loading to BANCS applicationFor this worked on SequentialfilestageTransformerJoinlookupCopyFunnel Aggregator stages Worked on Master sequence for generating summary reportSnapshot report accordingly Gathers mapping and reporting requirements and coordinate all data movement activities Insures the data is safely and securely transferred to target destinations Analyzes business requirements as they relate to the ETL process research evaluate and recommend alternative solutions Transforms business requirements into technical requirement documents Analyzes develops installs tests upgrades maintains and supports complex dataprocess models and processes in an ETL environment Works with management and staff to establish best practice standards for ETL functions and development of new methodologies for supporting data transformations and processing in a corporatewide solutions Mercedes Benz BI Data Migration Senior Datastage Developer and Tableau Developer Mercedes Benz Atlanta GA September 2017 to October 2018 Project Description MBUSAis a distribution company for cars in USA The main objective of the project is to migrate different applications running on existing Mainframes to Data stage This include Warranty as the major application Along with this there involves other applications like ASBDCRM 1WarrantyCRM These are main application in which we need to extract the data from DB2 and load data to flat files and xml and also to target DB2 in this application we need to design parallel DS jobs based on mappings prepared from mainframe code in Prod environment Once parallel job is designed and tested then a subsequence followed with master sequence is developed This entire mechanism is running on mainframes Now as part of Datastage we need to implement the migration of code running in mainframes and implement the same ETL mechanism in Datastage Responsibilities involved in understanding the existing Mainframe COBOL code and prepare the mapping based on the code involved in Developing Datastage Job based on mapping and Cobol code such that both functionalities remain same For this we validate the data in DB2 worked on different transformation logicsnull   Developed complete sequencer with all logics handled for each subject area which includes Execution commanduser variable activities mail notification Involved in reducing the complexity of code by writing the DB2 queries and also done portioning at Datastage job level Tested the job in all environments till the code moves from SIT to UAT After Loading data to warehouse tables prepare reports and dashboard usingActionsfiltersCalculated fields and deploy to tableau server Build customize and publish interactive reports visualizations and dashboards using Tableau Worked in Drill down and drill up hirachiescalculated fields and parametrization in Tableau Worked on different charts like Heat Maps Scatterd ChartsDo nut chart and also Created complex reports utilizing the features like hierarchy New Calculated Columns Drill functionality Tables graphs line Charts and Bar Charts Worked mainly on Interactive Dashbaords with best performance tuning mecahnisms Deployed the code to production as per the Sprint timelines with all the CRQ approved and documents needed Monitored the Jobs in production using Control M in doing support activities and complete the batch as per the SLA Create basic calculations including string manipulation basic arithmetic calculations custom aggregations and ratios date math logic statements and quick table calculations Developed Adhoc reports using Tableau Desktop Excel Developed visualization Dashboards using sets Parameters Calculated Fields Dynamic sorting Filtering Parameter driven analysis Use Measure name and Measure Value fields to create visualizations with multiple measures and dimensions Combine the visualizations into Interactive Dashboards and publish them to the Server Created the SharePoint folder and maintained all the documents of the project and code Invloved in Status reporting on a daily weekly basis of the project to ensure that all code is running with zero defects worked in Reusability of the code and Performance Tuning activities of Datastage Tools used Datastage 117 Putty IBM Datastudio 41DB2MainframesTableau desktop 20191 Datastage developerTableau Developer Michaels Ecommerce Business Intelligence Irving TX September 2015 to August 2017 Project Description The Michaels Companies Inc is North Americas largest provider of arts crafts framing floral wall dcor and merchandise for makers and doityourself home decorators The company owns and operates more than 1250 Michaels stores Aaron Brothers Custom Framing storewithinastore Artistree a manufacturer of high quality custom and specialty framing merchandise and Darice a wholesale distributor to the craft gift and dcor industry The companys flagship is Michaels Stores As of August 2018 the company reported that in addition to its Michaels Stores brand it produces over a dozen private brands including Recollections Studio Decor As part of Ecommerce Micheals used to sell their products through alternate channel EbayAs part of existing architecture they are going to decommission channel through e bay and want to set up their own online store such that every transactions willhapen with in Michaels store Responsibilities involved in understanding the existing Architecture and prepare the mapping based on the code The work involved preparing the LLD mappings based on the Business from HLD on each subject area like Item MasterInventoryPOSPoint of Sales The work involved Design and develop the jobs as per mappings with zero defects and good performance Involved gathering requirement from client ensuring quality deliverables and giving technical solutions to the team Worked on different files like XML filessequentials files datasets as part of datastage jobs design Data validation was done in various systems like WMSTIPS and also Data stage Jobs Once data is validated data is send to downstream applications which is WMS 2 Proficient in design and development of various dashboards reports utilizing Tableau Visualizations like Bar chart scatter plot Dual axis charts Water fall charts Donut charts Bubble charts Heat maps Line charts Cross tab Geographic VisualizationMaps and making use of actions for interactivity and global filters according to the businessclient requirement Worked extensively with Advance analysis Actions Calculations LODs Parameters Background images and Maps in Tableau Set parameters for viewing data as daily weekly monthly quarterly and yearly format Developed Trend Lines Reference Lines and statistical techniques to describe the data Involved job execution and checking the logs and rectify them Through Datastage Director and ensure code runs with zero warnings and errors Involved in Production support activities by monitoring the jobs From Control M if any jobs fails then check in Unix script followed with Datstage Job name and fix accordingly Created folder structure and uploaded all the documents in share point to keep track Status reporting on a daily weekly basis defect capture retesting and closure of defects Tools used Datastage 113 Putty Oracle SQL developerControl MSql AssistantTeradatatableau Desktop 20181 Datastage version code Migration 87 to 91 Software developerTableau Developer DNB Bank Bergen NO September 2014 to July 2015 Norway Project Description DNB ASA formerly DnB NOR ASA is Norways largest financial services group with total combined assets of more than NOK 19 trillion and a market capitalisation NOK164 billion as per 20 May 2016 DNBs head office is located in Oslo The two largest owners of DNB are the Norwegian Ministry of Trade and Industry and Sparebankstiftelsen DnB NOR The latter was created as a foundation with the sole purpose of owning part of the company The main objective is to migrate the existing code running on Datastage 87 to 91 version such that all other systems run on same Unix Also once the code migrated we need to validate the data from databases and other file systems Responsibilities Was involved in the project from requirements analysis phase interacted with customer as needed to get better understanding of requirements and responsible for Planning of test execution based on priority of the requirements for business Involved in taking the back up of dsx files from production and created all the folders and file systems in development of 91 version created the project folder and environmental variables and user defined variables in development environment of 91 version then imported the dsx file After import compiled all the jobs using Multi jobs compile and ran the jobs by pointing the server to new version 91 performed SITUAT to ensure data is fine in both environments 87 and 91 and capture all the test results and documented accordingly Implemented the same export and import of data from development environment of 91 to test environment and tested again Finally deployed the code to production environment 91 and monitored the jobs using ControlM such that all jobs running fine without any errors and warnings Fix the code accordingly if there are any issues come up while executing the jobs by monitoring the logs in Datastage Director Tools used Datastage 8791 Putty DB2CONTROLM Software developer StarBucks DataFiltering Seattle WA October 2013 to 2014 Project Description starbucks Corporation is an American coffee company and coffeehouse chain Starbucks was founded in Seattle Washington in 1971 As of 2018 the company operates 28218 locations worldwideStarbucks is considered the main representative of second wave coffee initially distinguishing itself from other coffeeserving venues in the US by taste quality and customer experience while popularizing darkly roasted coffee Since the 2000s third wave coffee makers have targeted qualityminded coffee drinkers with handmade coffee based on lighter roasts while Starbucks nowadays uses automated espresso machines for efficiency and safety reasons The main objective is to refine datasets that are having junk and older data which is loading to database so as part of existing jobs we are adding new extra logic in sql code such that junk data is removed and proper data is loaded to datasets Responsibilities Work involved Performance Tuning Techniques at Query level and Data stage job Level to Improve the Execution Time The work involves formulate testing strategy test planning identifying test scenarios test data set up ensuring quality deliverables and Design Remodified the existing queries by adding some extra logic to remove unwanted data by filtering at date level so that refined data is loaded to datasets Status reporting on a daily weekly basis defect capture retesting and closure of defects Created folder structure and uploaded all the documents in share point to keep track Used Data Stage Director to identify the logs and rectify them with Zero errors Extensively used data stage to check for logs and Performed Partioninghash same as per required and used Oracle to query database Tools used Datastage 87Putty TeradataSql plusCRONTAB Software developer TargetEnterprise DatawarehouseEDW February 2011 to 2013 Location Minneapolis Project Description Target Corporation is the eighthlargest retailer in the United States and is a component of the SP 500 IndexFounded by George Dayton and headquartered in Minneapolis the company was originally named Goodfellow Dry Goods in June 1902 before being renamed the Daytons Dry Goods Company in 1903 and later the Dayton Company in 1910 The first Target store opened in Roseville Minnesota in 1962 while the parent company was renamed the Dayton Corporation The main objective is to load data from multiple systems to enterprise warehouse this involves design of parallel jobs and followd with sequnecrs and Unix scripts accordingly Responsibilities Understanding the business functionality Analysis of business requirements Our responsibility starts from the point where a file from our source system lands in our server and ends when it is loaded to Target tables Monitoring jobs and viewing logs for the jobs Running and Monitoring the Jobs in Director and performing Cleanup resources if needed Used Plugin Meta Data to import the source and target table definitions Used most of the Stages in Data stage such as the Transformer Stage SCD Stage Change Capture Aggregator Stage filter stage job parameters Sequential file Stage Used Lookup Stage efficiently for storing the primary rejected data for reference Involved on Deployments ie deploying code from DEV to UAT and UAT to PROD The work involves gathering requirement from client Preparing Mapping Specifications formulate testing strategy test planning identifying test scenarios test data set up ensuring quality deliverables and Design Effectively interacts with clients Status reporting on a daily weekly basis defect capture retesting and closure of defects Created folder structure and uploaded all the documents in share point to keep track Used Data Stage Director to identify the logs and rectify them with Zero errors Coordinating with onsite team and involved in Oncalls Project progress reviews Tools used Datastage 87unixPutty Sql plusTeradataControl M Datastage developer AetnaPredictive April 2009 to 2011 Location Hartford Project Description Aetna Incis an American managed health care company that sells traditional and consumer directed health care insurance plans and related services such as medical pharmaceutical dental behavioral health longterm care and disability plans primarily through employerpaid fully or partly insurance and benefit programs and through Medicare Since November 28 2018 the company has been a subsidiary of CVS HealthThe companys network includes million medical members 127 million dental members 131 million pharmacy benefit management services members 1200000 healthcare professionals over 690000 primary care doctors and specialists and over 5700 hospitals The goal of the Claims Fraud Analytics project is to identify the fraudulent claims from the information available from internal and external sources In order to identify fraud Selective Insurance has developed models based on a certain set of variables to score the claims The Fraud Analytics Variable Development application was developed to pull information from various internal and external sources and present the information to the User to do mining on the data This process also includes the scoring of all the current open claims on a daily basis Responsibilities Understanding the business functionality Analysis of business requirements Designed and developed DataStage ETL Parallel Jobs between Source and Target using sequential file stagedatasetoracle connectorsortremove duplicatesaggregatorchange capturetransformations accordingly Import and export the DSX from one environment to test for testing purpose created the jobs using change capture to handle SCD Type 2 dimensions accordingly Involved in promoting the code from DEV to UAT and to support QA team for each and every querys Resolving the defects assigned by the QA and Business team Involved in deploying code to UAT and PROD Involved in UAT Production support for the earlier releases for Daily loads pre verified and Monthly Coordinating with onsite team and involved in Oncalls Project progress reviews Tools used Datastage 85puttysql plusDB2NotepadGIT HubControl M Education Bachelor of Technology in Electronics and Communication Engineering Jawaharlal Nehru technology University College of Engineering Skills Hdfs Datastage Db2 Replication Teradata Database Ms access Oracle Sql Tableau Data replication Unix Unix shell Git Hive Real time Xml Itil Retail Excel Additional Information Technical Skills Languages SQLUNIX Shell Script XML Database Oracle 9i10g11 g DB2WINSQLNetezzaTeradataSQL Assistant BI Tools IBM Datastage 11x9x8x75Tableau 20191 Big data HDFSHive Real Time App IBM CDCIBM data Replication ITIL Tools Service now ITSM Scheduling tools Control M Zeke Utilities Git hubZira Operating Systems Windows XP2007Vista Software Applications MS Word MS Excel MS Access Outlook PowerPoint Business areas Automobile RetailBankingInsurance",
    "extracted_keywords": [
        "Software",
        "Engineer",
        "ETL",
        "Developer",
        "span",
        "lSoftwarespan",
        "Engineer",
        "ETL",
        "span",
        "lDeveloperspan",
        "Software",
        "Engineer",
        "ETL",
        "Developer",
        "Zions",
        "Bancorporation",
        "Salt",
        "Lake",
        "City",
        "UT",
        "Authorized",
        "US",
        "employer",
        "Work",
        "Experience",
        "Software",
        "Engineer",
        "ETL",
        "Developer",
        "Zions",
        "Bancorporation",
        "March",
        "Present",
        "Location",
        "SaltLakeCityUtah",
        "Project",
        "Description",
        "ZionsBancorporation",
        "Financial",
        "services",
        "bank",
        "Salt",
        "Lake",
        "City",
        "Utah",
        "subsidiaries",
        "control",
        "Zions",
        "Bank",
        "Now",
        "Zionsbancorporation",
        "subsidaries",
        "banks",
        "system",
        "BANCS",
        "application",
        "data",
        "subsidiary",
        "BANCS",
        "application",
        "TRISON",
        "subsidaries",
        "data",
        "BANCS",
        "applications",
        "part",
        "Migration",
        "ETL",
        "DataStage",
        "data",
        "Mainframe",
        "format",
        "conversion",
        "Transformation",
        "logics",
        "fields",
        "part",
        "Migration",
        "data",
        "area",
        "DEPOSITS",
        "DDACRL",
        "Accounts",
        "Data",
        "Responsibilities",
        "Designs",
        "ETL",
        "applications",
        "Datastage",
        "application",
        "architecture",
        "guidelines",
        "Access",
        "data",
        "sources",
        "hardware",
        "devices",
        "VSAM",
        "files",
        "business",
        "policies",
        "data",
        "quality",
        "measures",
        "files",
        "Transformation",
        "logics",
        "file",
        "loading",
        "BANCS",
        "SequentialfilestageTransformerJoinlookupCopyFunnel",
        "Aggregator",
        "stages",
        "Master",
        "sequence",
        "summary",
        "reportSnapshot",
        "report",
        "mapping",
        "reporting",
        "requirements",
        "data",
        "movement",
        "activities",
        "data",
        "destinations",
        "Analyzes",
        "business",
        "requirements",
        "ETL",
        "process",
        "research",
        "evaluate",
        "solutions",
        "business",
        "requirements",
        "requirement",
        "documents",
        "Analyzes",
        "installs",
        "tests",
        "upgrades",
        "dataprocess",
        "models",
        "processes",
        "ETL",
        "environment",
        "management",
        "staff",
        "practice",
        "standards",
        "ETL",
        "functions",
        "development",
        "methodologies",
        "data",
        "transformations",
        "processing",
        "solutions",
        "Mercedes",
        "Benz",
        "BI",
        "Data",
        "Migration",
        "Senior",
        "Datastage",
        "Developer",
        "Tableau",
        "Developer",
        "Mercedes",
        "Benz",
        "Atlanta",
        "GA",
        "September",
        "October",
        "Project",
        "Description",
        "MBUSAis",
        "distribution",
        "company",
        "cars",
        "USA",
        "objective",
        "project",
        "applications",
        "Mainframes",
        "Data",
        "stage",
        "Warranty",
        "application",
        "applications",
        "ASBDCRM",
        "1WarrantyCRM",
        "application",
        "data",
        "DB2",
        "data",
        "files",
        "xml",
        "DB2",
        "application",
        "DS",
        "jobs",
        "mappings",
        "mainframe",
        "code",
        "Prod",
        "environment",
        "job",
        "subsequence",
        "master",
        "sequence",
        "mechanism",
        "mainframes",
        "part",
        "Datastage",
        "migration",
        "code",
        "mainframes",
        "ETL",
        "mechanism",
        "Datastage",
        "Responsibilities",
        "Mainframe",
        "COBOL",
        "code",
        "mapping",
        "code",
        "Datastage",
        "Job",
        "mapping",
        "Cobol",
        "code",
        "functionalities",
        "data",
        "DB2",
        "transformation",
        "logicsnull",
        "sequencer",
        "logics",
        "area",
        "Execution",
        "commanduser",
        "variable",
        "activities",
        "mail",
        "notification",
        "complexity",
        "code",
        "DB2",
        "queries",
        "Datastage",
        "job",
        "level",
        "job",
        "environments",
        "code",
        "SIT",
        "UAT",
        "Loading",
        "data",
        "tables",
        "reports",
        "dashboard",
        "fields",
        "tableau",
        "server",
        "Build",
        "customize",
        "reports",
        "visualizations",
        "dashboards",
        "Tableau",
        "Worked",
        "Drill",
        "fields",
        "parametrization",
        "Tableau",
        "charts",
        "Heat",
        "Maps",
        "Scatterd",
        "ChartsDo",
        "nut",
        "chart",
        "reports",
        "features",
        "hierarchy",
        "New",
        "Calculated",
        "Columns",
        "Drill",
        "functionality",
        "Tables",
        "graphs",
        "Charts",
        "Bar",
        "Charts",
        "Worked",
        "Interactive",
        "Dashbaords",
        "performance",
        "mecahnisms",
        "code",
        "production",
        "Sprint",
        "timelines",
        "CRQ",
        "documents",
        "Monitored",
        "Jobs",
        "production",
        "Control",
        "M",
        "support",
        "activities",
        "batch",
        "SLA",
        "calculations",
        "string",
        "manipulation",
        "calculations",
        "custom",
        "aggregations",
        "ratios",
        "math",
        "logic",
        "statements",
        "table",
        "calculations",
        "Adhoc",
        "reports",
        "Tableau",
        "Desktop",
        "Excel",
        "visualization",
        "Dashboards",
        "sets",
        "Parameters",
        "Calculated",
        "Fields",
        "Filtering",
        "Parameter",
        "analysis",
        "Use",
        "Measure",
        "name",
        "Measure",
        "Value",
        "fields",
        "visualizations",
        "measures",
        "dimensions",
        "visualizations",
        "Interactive",
        "Dashboards",
        "Server",
        "SharePoint",
        "folder",
        "documents",
        "project",
        "code",
        "Status",
        "basis",
        "project",
        "code",
        "defects",
        "Reusability",
        "code",
        "Performance",
        "activities",
        "Datastage",
        "Tools",
        "Datastage",
        "Putty",
        "IBM",
        "Datastudio",
        "41DB2MainframesTableau",
        "desktop",
        "Datastage",
        "developerTableau",
        "Developer",
        "Michaels",
        "Ecommerce",
        "Business",
        "Intelligence",
        "Irving",
        "TX",
        "September",
        "August",
        "Project",
        "Description",
        "Michaels",
        "Companies",
        "Inc",
        "North",
        "Americas",
        "provider",
        "arts",
        "crafts",
        "wall",
        "dcor",
        "merchandise",
        "makers",
        "home",
        "decorators",
        "company",
        "Michaels",
        "stores",
        "Aaron",
        "Brothers",
        "Custom",
        "Framing",
        "storewithinastore",
        "Artistree",
        "manufacturer",
        "quality",
        "custom",
        "specialty",
        "merchandise",
        "Darice",
        "distributor",
        "craft",
        "gift",
        "dcor",
        "industry",
        "companys",
        "flagship",
        "Michaels",
        "Stores",
        "August",
        "company",
        "addition",
        "Michaels",
        "Stores",
        "brand",
        "dozen",
        "brands",
        "Recollections",
        "Studio",
        "Decor",
        "part",
        "Ecommerce",
        "Micheals",
        "products",
        "channel",
        "part",
        "architecture",
        "decommission",
        "channel",
        "e",
        "bay",
        "store",
        "transactions",
        "Michaels",
        "store",
        "Responsibilities",
        "Architecture",
        "mapping",
        "code",
        "work",
        "LLD",
        "mappings",
        "Business",
        "HLD",
        "area",
        "Item",
        "MasterInventoryPOSPoint",
        "Sales",
        "work",
        "Design",
        "jobs",
        "mappings",
        "defects",
        "performance",
        "gathering",
        "requirement",
        "client",
        "quality",
        "deliverables",
        "solutions",
        "team",
        "files",
        "XML",
        "filessequentials",
        "datasets",
        "part",
        "datastage",
        "jobs",
        "design",
        "Data",
        "validation",
        "systems",
        "WMSTIPS",
        "Data",
        "stage",
        "Jobs",
        "data",
        "data",
        "applications",
        "WMS",
        "Proficient",
        "design",
        "development",
        "dashboards",
        "reports",
        "Tableau",
        "Visualizations",
        "Bar",
        "chart",
        "scatter",
        "plot",
        "axis",
        "charts",
        "Water",
        "fall",
        "charts",
        "Donut",
        "charts",
        "Bubble",
        "charts",
        "Heat",
        "maps",
        "Line",
        "charts",
        "Cross",
        "tab",
        "Geographic",
        "VisualizationMaps",
        "use",
        "actions",
        "interactivity",
        "filters",
        "requirement",
        "Advance",
        "analysis",
        "Actions",
        "Calculations",
        "Parameters",
        "Background",
        "images",
        "Maps",
        "Tableau",
        "Set",
        "parameters",
        "data",
        "format",
        "Developed",
        "Trend",
        "Lines",
        "Reference",
        "Lines",
        "techniques",
        "data",
        "job",
        "execution",
        "logs",
        "Datastage",
        "Director",
        "code",
        "runs",
        "warnings",
        "errors",
        "Production",
        "support",
        "activities",
        "jobs",
        "Control",
        "M",
        "jobs",
        "Unix",
        "script",
        "Datstage",
        "Job",
        "name",
        "folder",
        "structure",
        "documents",
        "share",
        "point",
        "track",
        "Status",
        "basis",
        "defect",
        "capture",
        "closure",
        "defects",
        "Tools",
        "Datastage",
        "Putty",
        "Oracle",
        "SQL",
        "developerControl",
        "MSql",
        "AssistantTeradatatableau",
        "Desktop",
        "Datastage",
        "version",
        "code",
        "Migration",
        "Software",
        "developerTableau",
        "Developer",
        "DNB",
        "Bank",
        "Bergen",
        "NO",
        "September",
        "July",
        "Norway",
        "Project",
        "Description",
        "DNB",
        "ASA",
        "DnB",
        "ASA",
        "Norways",
        "services",
        "group",
        "assets",
        "NOK",
        "market",
        "capitalisation",
        "NOK164",
        "May",
        "DNBs",
        "head",
        "office",
        "Oslo",
        "owners",
        "DNB",
        "Ministry",
        "Trade",
        "Industry",
        "Sparebankstiftelsen",
        "DnB",
        "foundation",
        "purpose",
        "part",
        "company",
        "objective",
        "code",
        "Datastage",
        "version",
        "systems",
        "Unix",
        "code",
        "data",
        "databases",
        "file",
        "systems",
        "Responsibilities",
        "project",
        "requirements",
        "analysis",
        "phase",
        "customer",
        "understanding",
        "requirements",
        "Planning",
        "test",
        "execution",
        "priority",
        "requirements",
        "business",
        "back",
        "dsx",
        "files",
        "production",
        "folders",
        "file",
        "systems",
        "development",
        "version",
        "project",
        "folder",
        "variables",
        "user",
        "variables",
        "development",
        "environment",
        "version",
        "dsx",
        "file",
        "import",
        "jobs",
        "Multi",
        "jobs",
        "compile",
        "jobs",
        "server",
        "version",
        "SITUAT",
        "data",
        "environments",
        "test",
        "results",
        "export",
        "import",
        "data",
        "development",
        "environment",
        "environment",
        "code",
        "production",
        "environment",
        "jobs",
        "jobs",
        "errors",
        "warnings",
        "code",
        "issues",
        "jobs",
        "logs",
        "Datastage",
        "Director",
        "Tools",
        "Datastage",
        "Putty",
        "DB2CONTROLM",
        "Software",
        "developer",
        "StarBucks",
        "DataFiltering",
        "Seattle",
        "WA",
        "October",
        "Project",
        "Description",
        "Corporation",
        "coffee",
        "company",
        "coffeehouse",
        "chain",
        "Starbucks",
        "Seattle",
        "Washington",
        "company",
        "locations",
        "worldwideStarbucks",
        "representative",
        "wave",
        "coffee",
        "venues",
        "US",
        "taste",
        "quality",
        "customer",
        "experience",
        "coffee",
        "wave",
        "coffee",
        "makers",
        "coffee",
        "drinkers",
        "coffee",
        "roasts",
        "Starbucks",
        "machines",
        "efficiency",
        "safety",
        "reasons",
        "objective",
        "datasets",
        "junk",
        "data",
        "database",
        "part",
        "jobs",
        "logic",
        "sql",
        "code",
        "junk",
        "data",
        "data",
        "datasets",
        "Responsibilities",
        "Work",
        "Performance",
        "Tuning",
        "Techniques",
        "Query",
        "level",
        "Data",
        "stage",
        "job",
        "Level",
        "Execution",
        "Time",
        "work",
        "testing",
        "strategy",
        "test",
        "planning",
        "test",
        "scenarios",
        "data",
        "quality",
        "deliverables",
        "Design",
        "queries",
        "logic",
        "data",
        "date",
        "level",
        "data",
        "datasets",
        "Status",
        "basis",
        "defect",
        "capture",
        "closure",
        "defects",
        "folder",
        "structure",
        "documents",
        "share",
        "point",
        "track",
        "Data",
        "Stage",
        "Director",
        "logs",
        "Zero",
        "errors",
        "data",
        "stage",
        "logs",
        "Performed",
        "Partioninghash",
        "Oracle",
        "query",
        "database",
        "Tools",
        "Datastage",
        "87Putty",
        "TeradataSql",
        "plusCRONTAB",
        "Software",
        "developer",
        "TargetEnterprise",
        "DatawarehouseEDW",
        "February",
        "Location",
        "Minneapolis",
        "Project",
        "Description",
        "Target",
        "Corporation",
        "retailer",
        "United",
        "States",
        "component",
        "SP",
        "IndexFounded",
        "George",
        "Dayton",
        "Minneapolis",
        "company",
        "Goodfellow",
        "Dry",
        "Goods",
        "June",
        "Daytons",
        "Dry",
        "Goods",
        "Company",
        "Dayton",
        "Company",
        "Target",
        "store",
        "Roseville",
        "Minnesota",
        "parent",
        "company",
        "Dayton",
        "Corporation",
        "objective",
        "data",
        "systems",
        "enterprise",
        "warehouse",
        "design",
        "jobs",
        "followd",
        "sequnecrs",
        "Unix",
        "scripts",
        "Responsibilities",
        "business",
        "functionality",
        "Analysis",
        "business",
        "requirements",
        "responsibility",
        "point",
        "file",
        "source",
        "system",
        "lands",
        "server",
        "Target",
        "tables",
        "Monitoring",
        "jobs",
        "logs",
        "jobs",
        "Jobs",
        "Director",
        "Cleanup",
        "resources",
        "Used",
        "Plugin",
        "Meta",
        "Data",
        "source",
        "table",
        "definitions",
        "Stages",
        "Data",
        "stage",
        "Transformer",
        "Stage",
        "SCD",
        "Stage",
        "Change",
        "Capture",
        "Aggregator",
        "Stage",
        "filter",
        "stage",
        "job",
        "parameters",
        "file",
        "Stage",
        "Lookup",
        "Stage",
        "primary",
        "data",
        "reference",
        "Deployments",
        "code",
        "DEV",
        "UAT",
        "UAT",
        "work",
        "requirement",
        "client",
        "Preparing",
        "Mapping",
        "Specifications",
        "testing",
        "strategy",
        "test",
        "planning",
        "test",
        "scenarios",
        "data",
        "quality",
        "deliverables",
        "Design",
        "clients",
        "Status",
        "basis",
        "defect",
        "capture",
        "closure",
        "defects",
        "folder",
        "structure",
        "documents",
        "share",
        "point",
        "track",
        "Data",
        "Stage",
        "Director",
        "logs",
        "Zero",
        "errors",
        "team",
        "Oncalls",
        "Project",
        "progress",
        "Tools",
        "Datastage",
        "87unixPutty",
        "Sql",
        "plusTeradataControl",
        "M",
        "Datastage",
        "developer",
        "AetnaPredictive",
        "April",
        "Location",
        "Hartford",
        "Project",
        "Description",
        "Aetna",
        "Incis",
        "American",
        "health",
        "care",
        "company",
        "consumer",
        "health",
        "care",
        "insurance",
        "plans",
        "services",
        "health",
        "longterm",
        "care",
        "disability",
        "plans",
        "employerpaid",
        "insurance",
        "benefit",
        "programs",
        "Medicare",
        "November",
        "company",
        "subsidiary",
        "CVS",
        "HealthThe",
        "companys",
        "network",
        "members",
        "members",
        "pharmacy",
        "benefit",
        "management",
        "services",
        "members",
        "healthcare",
        "professionals",
        "care",
        "doctors",
        "specialists",
        "hospitals",
        "goal",
        "Claims",
        "Fraud",
        "Analytics",
        "project",
        "claims",
        "information",
        "sources",
        "order",
        "fraud",
        "Selective",
        "Insurance",
        "models",
        "set",
        "variables",
        "claims",
        "Fraud",
        "Analytics",
        "Variable",
        "Development",
        "application",
        "information",
        "sources",
        "information",
        "User",
        "mining",
        "data",
        "process",
        "scoring",
        "claims",
        "basis",
        "Responsibilities",
        "business",
        "functionality",
        "Analysis",
        "business",
        "requirements",
        "DataStage",
        "ETL",
        "Parallel",
        "Jobs",
        "Source",
        "Target",
        "file",
        "stagedatasetoracle",
        "connectorsortremove",
        "duplicatesaggregatorchange",
        "capturetransformations",
        "DSX",
        "environment",
        "testing",
        "purpose",
        "jobs",
        "change",
        "capture",
        "SCD",
        "Type",
        "dimensions",
        "code",
        "DEV",
        "UAT",
        "QA",
        "team",
        "querys",
        "defects",
        "QA",
        "Business",
        "team",
        "code",
        "UAT",
        "PROD",
        "Production",
        "support",
        "releases",
        "Daily",
        "loads",
        "pre",
        "Monthly",
        "Coordinating",
        "team",
        "Oncalls",
        "Project",
        "progress",
        "Tools",
        "Datastage",
        "plusDB2NotepadGIT",
        "HubControl",
        "M",
        "Education",
        "Bachelor",
        "Technology",
        "Electronics",
        "Communication",
        "Engineering",
        "Jawaharlal",
        "Nehru",
        "technology",
        "University",
        "College",
        "Engineering",
        "Skills",
        "Hdfs",
        "Datastage",
        "Replication",
        "Teradata",
        "Database",
        "Ms",
        "access",
        "Oracle",
        "Sql",
        "Tableau",
        "Data",
        "replication",
        "Unix",
        "Unix",
        "shell",
        "Git",
        "Hive",
        "Real",
        "time",
        "Xml",
        "Itil",
        "Retail",
        "Excel",
        "Additional",
        "Information",
        "Technical",
        "Skills",
        "Languages",
        "SQLUNIX",
        "Shell",
        "Script",
        "XML",
        "Database",
        "Oracle",
        "9i10g11",
        "g",
        "DB2WINSQLNetezzaTeradataSQL",
        "Assistant",
        "BI",
        "Tools",
        "IBM",
        "Datastage",
        "data",
        "HDFSHive",
        "Real",
        "Time",
        "App",
        "IBM",
        "CDCIBM",
        "data",
        "Replication",
        "ITIL",
        "Tools",
        "Service",
        "ITSM",
        "Scheduling",
        "tools",
        "Control",
        "M",
        "Zeke",
        "Utilities",
        "Git",
        "hubZira",
        "Operating",
        "Systems",
        "Windows",
        "XP2007Vista",
        "Software",
        "Applications",
        "MS",
        "Word",
        "MS",
        "Excel",
        "MS",
        "Access",
        "Outlook",
        "PowerPoint",
        "Business",
        "Automobile",
        "RetailBankingInsurance"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T23:05:13.621846",
    "resume_data": "Software Engineer ETL Developer span lSoftwarespan Engineer ETL span lDeveloperspan Software Engineer ETL Developer Zions Bancorporation Salt Lake City UT Authorized to work in the US for any employer Work Experience Software Engineer ETL Developer Zions Bancorporation March 2019 to Present Location SaltLakeCityUtah Project Description ZionsBancorporation is one of the largest Financial services bank based out in Salt Lake City Utah It has six different subsidiaries which operates under the control of Zions Bank Now Zionsbancorporation want to merge all its subsidaries banks into one centralized system For this they are using BANCS application which migrates data from each subsidiary to BANCS They want to decommission their application on TRISON and want to migrate all subsidaries data to BANCS applications As part of Migration we are using ETL DataStage which extracts data from Mainframe applicationEBCDIC to ASCII format conversion Also we are applying different Transformation logics to fields that are needed as part of Migration Here we are migrating data on each subject area which involves RCIFTIME DEPOSITS DDACRL Accounts Data Responsibilities Designs develops and tests ETL applications using Datastage 113 with application architecture guidelines Access disparate data sources databases hardware devices VSAM and flat files and apply internal and external business rules policies and data quality measures Use 253 files and apply all Transformation logics accordingly to generate middle file as expected that helps in loading to BANCS applicationFor this worked on SequentialfilestageTransformerJoinlookupCopyFunnel Aggregator stages Worked on Master sequence for generating summary reportSnapshot report accordingly Gathers mapping and reporting requirements and coordinate all data movement activities Insures the data is safely and securely transferred to target destinations Analyzes business requirements as they relate to the ETL process research evaluate and recommend alternative solutions Transforms business requirements into technical requirement documents Analyzes develops installs tests upgrades maintains and supports complex dataprocess models and processes in an ETL environment Works with management and staff to establish best practice standards for ETL functions and development of new methodologies for supporting data transformations and processing in a corporatewide solutions Mercedes Benz BI Data Migration Senior Datastage Developer and Tableau Developer Mercedes Benz Atlanta GA September 2017 to October 2018 Project Description MBUSAis a distribution company for cars in USA The main objective of the project is to migrate different applications running on existing Mainframes to Data stage This include Warranty as the major application Along with this there involves other applications like ASBDCRM 1WarrantyCRM These are main application in which we need to extract the data from DB2 and load data to flat files and xml and also to target DB2in this application we need to design parallel DS jobs based on mappings prepared from mainframe code in Prod environment Once parallel job is designed and tested then a subsequence followed with master sequence is developed This entire mechanism is running on mainframes Now as part of Datastage we need to implement the migration of code running in mainframes and implement the same ETL mechanism in Datastage Responsibilities involved in understanding the existing Mainframe COBOL code and prepare the mapping based on the code involved in Developing Datastage Job based on mapping and Cobol code such that both functionalities remain same For this we validate the data in DB2 worked on different transformation logicsnull handlingtypeconversionsjoinLookupAggregtorSortXMLDB2 Developed complete sequencer with all logics handled for each subject area which includes Execution commanduser variable activities mail notification Involved in reducing the complexity of code by writing the DB2 queries and also done portioning at Datastage job level Tested the job in all environments till the code moves from SIT to UAT After Loading data to warehouse tables prepare reports and dashboard usingActionsfiltersCalculated fields and deploy to tableau server Build customize and publish interactive reports visualizations and dashboards using Tableau Worked in Drill down and drill up hirachiescalculated fields and parametrization in Tableau Worked on different charts like Heat Maps Scatterd ChartsDo nut chart and also Created complex reports utilizing the features like hierarchy New Calculated Columns Drill functionality Tables graphs line Charts and Bar Charts Worked mainly on Interactive Dashbaords with best performance tuning mecahnisms Deployed the code to production as per the Sprint timelines with all the CRQ approved and documents needed Monitored the Jobs in production using Control M in doing support activities and complete the batch as per the SLA Create basic calculations including string manipulation basic arithmetic calculations custom aggregations and ratios date math logic statements and quick table calculations Developed Adhoc reports using Tableau Desktop Excel Developed visualization Dashboards using sets Parameters Calculated Fields Dynamic sorting Filtering Parameter driven analysis Use Measure name and Measure Value fields to create visualizations with multiple measures and dimensions Combine the visualizations into Interactive Dashboards and publish them to the Server Created the SharePoint folder and maintained all the documents of the project and code Invloved in Status reporting on a daily weekly basis of the project to ensure that all code is running with zero defects worked in Reusability of the code and Performance Tuning activities of Datastage Tools used Datastage 117 Putty IBM Datastudio 41DB2MainframesTableau desktop 20191 Datastage developerTableau Developer Michaels Ecommerce Business Intelligence Irving TX September 2015 to August 2017 Project Description The Michaels Companies Inc is North Americas largest provider of arts crafts framing floral wall dcor and merchandise for makers and doityourself home decorators The company owns and operates more than 1250 Michaels stores Aaron Brothers Custom Framing storewithinastore Artistree a manufacturer of high quality custom and specialty framing merchandise and Darice a wholesale distributor to the craft gift and dcor industry The companys flagship is Michaels Stores As of August 2018 the company reported that in addition to its Michaels Stores brand it produces over a dozen private brands including Recollections Studio Decor As part of Ecommerce Micheals used to sell their products through alternate channel EbayAs part of existing architecture they are going to decommission channel through e bay and want to set up their own online store such that every transactions willhapen with in Michaels store Responsibilities involved in understanding the existing Architecture and prepare the mapping based on the code The work involved preparing the LLD mappings based on the Business from HLD on each subject area like Item MasterInventoryPOSPoint of Sales The work involved Design and develop the jobs as per mappings with zero defects and good performance Involved gathering requirement from client ensuring quality deliverables and giving technical solutions to the team Worked on different files like XML filessequentials files datasets as part of datastage jobs design Data validation was done in various systems like WMSTIPS and also Data stage Jobs Once data is validated data is send to downstream applications which is WMS 2 Proficient in design and development of various dashboards reports utilizing Tableau Visualizations like Bar chart scatter plot Dual axis charts Water fall charts Donut charts Bubble charts Heat maps Line charts Cross tab Geographic VisualizationMaps and making use of actions for interactivity and global filters according to the businessclient requirement Worked extensively with Advance analysis Actions Calculations LODs Parameters Background images and Maps in Tableau Set parameters for viewing data as daily weekly monthly quarterly and yearly format Developed Trend Lines Reference Lines and statistical techniques to describe the data Involved job execution and checking the logs and rectify them Through Datastage Director and ensure code runs with zero warnings and errors Involved in Production support activities by monitoring the jobs From Control M if any jobs fails then check in Unix script followed with Datstage Job name and fix accordingly Created folder structure and uploaded all the documents in share point to keep track Status reporting on a daily weekly basis defect capture retesting and closure of defects Tools used Datastage 113 Putty Oracle SQL developerControl MSql AssistantTeradatatableau Desktop 20181 Datastage version code Migration 87 to 91 Software developerTableau Developer DNB Bank Bergen NO September 2014 to July 2015 Norway Project Description DNB ASA formerly DnB NOR ASA is Norways largest financial services group with total combined assets of more than NOK 19 trillion and a market capitalisation NOK164 billion as per 20 May 2016 DNBs head office is located in Oslo The two largest owners of DNB are the Norwegian Ministry of Trade and Industry and Sparebankstiftelsen DnB NOR The latter was created as a foundation with the sole purpose of owning part of the company The main objective is to migrate the existing code running on Datastage 87 to 91 version such that all other systems run on same Unix Also once the code migrated we need to validate the data from databases and other file systems Responsibilities Was involved in the project from requirements analysis phase interacted with customer as needed to get better understanding of requirements and responsible for Planning of test execution based on priority of the requirements for business Involved in taking the back up of dsx files from production and created all the folders and file systems in development of 91 version created the project folder and environmental variables and user defined variables in development environment of 91 version then imported the dsx file After import compiled all the jobs using Multi jobs compile and ran the jobs by pointing the server to new version 91 performed SITUAT to ensure data is fine in both environments 87 and 91 and capture all the test results and documented accordingly Implemented the same export and import of data from development environment of 91 to test environment and tested again Finally deployed the code to production environment 91 and monitored the jobs using ControlM such that all jobs running fine without any errors and warnings Fix the code accordingly if there are any issues come up while executing the jobs by monitoring the logs in Datastage Director Tools used Datastage 8791 Putty DB2CONTROLM Software developer StarBucks DataFiltering Seattle WA October 2013 to 2014 Project Description starbucks Corporation is an American coffee company and coffeehouse chain Starbucks was founded in Seattle Washington in 1971 As of 2018 the company operates 28218 locations worldwideStarbucks is considered the main representative of second wave coffee initially distinguishing itself from other coffeeserving venues in the US by taste quality and customer experience while popularizing darkly roasted coffee Since the 2000s third wave coffee makers have targeted qualityminded coffee drinkers with handmade coffee based on lighter roasts while Starbucks nowadays uses automated espresso machines for efficiency and safety reasons The main objective is to refine datasets that are having junk and older data which is loading to database so as part of existing jobs we are adding new extra logic in sql code such that junk data is removed and proper data is loaded to datasets Responsibilities Work involved Performance Tuning Techniques at Query level and Data stage job Level to Improve the Execution Time The work involves formulate testing strategy test planning identifying test scenarios test data set up ensuring quality deliverables and Design Remodified the existing queries by adding some extra logic to remove unwanted data by filtering at date level so that refined data is loaded to datasets Status reporting on a daily weekly basis defect capture retesting and closure of defects Created folder structure and uploaded all the documents in share point to keep track Used Data Stage Director to identify the logs and rectify them with Zero errors Extensively used data stage to check for logs and Performed Partioninghash same as per required and used Oracle to query database Tools used Datastage 87Putty TeradataSql plusCRONTAB Software developer TargetEnterprise DatawarehouseEDW February 2011 to 2013 Location Minneapolis Project Description Target Corporation is the eighthlargest retailer in the United States and is a component of the SP 500 IndexFounded by George Dayton and headquartered in Minneapolis the company was originally named Goodfellow Dry Goods in June 1902 before being renamed the Daytons Dry Goods Company in 1903 and later the Dayton Company in 1910 The first Target store opened in Roseville Minnesota in 1962 while the parent company was renamed the Dayton Corporation The main objective is to load data from multiple systems to enterprise warehouse this involves design of parallel jobs and followd with sequnecrs and Unix scripts accordingly Responsibilities Understanding the business functionality Analysis of business requirements Our responsibility starts from the point where a file from our source system lands in our server and ends when it is loaded to Target tables Monitoring jobs and viewing logs for the jobs Running and Monitoring the Jobs in Director and performing Cleanup resources if needed Used Plugin Meta Data to import the source and target table definitions Used most of the Stages in Data stage such as the Transformer Stage SCD Stage Change Capture Aggregator Stage filter stage job parameters Sequential file Stage Used Lookup Stage efficiently for storing the primary rejected data for reference Involved on Deployments ie deploying code from DEV to UAT and UAT to PROD The work involves gathering requirement from client Preparing Mapping Specifications formulate testing strategy test planning identifying test scenarios test data set up ensuring quality deliverables and Design Effectively interacts with clients Status reporting on a daily weekly basis defect capture retesting and closure of defects Created folder structure and uploaded all the documents in share point to keep track Used Data Stage Director to identify the logs and rectify them with Zero errors Coordinating with onsite team and involved in Oncalls Project progress reviews Tools used Datastage 87unixPutty Sql plusTeradataControl M Datastage developer AetnaPredictive April 2009 to 2011 Location Hartford Project Description Aetna Incis an American managed health care company that sells traditional and consumer directed health care insurance plans and related services such as medical pharmaceutical dental behavioral health longterm care and disability plans primarily through employerpaid fully or partly insurance and benefit programs and through Medicare Since November 28 2018 the company has been a subsidiary of CVS HealthThe companys network includes million medical members 127 million dental members 131 million pharmacy benefit management services members 1200000 healthcare professionals over 690000 primary care doctors and specialists and over 5700 hospitals The goal of the Claims Fraud Analytics project is to identify the fraudulent claims from the information available from internal and external sources In order to identify fraud Selective Insurance has developed models based on a certain set of variables to score the claims The Fraud Analytics Variable Development application was developed to pull information from various internal and external sources and present the information to the User to do mining on the data This process also includes the scoring of all the current open claims on a daily basis Responsibilities Understanding the business functionality Analysis of business requirements Designed and developed DataStage ETL Parallel Jobs between Source and Target using sequential file stagedatasetoracle connectorsortremove duplicatesaggregatorchange capturetransformations accordingly Import and export the DSX from one environment to test for testing purpose created the jobs using change capture to handle SCD Type 2 dimensions accordingly Involved in promoting the code from DEV to UAT and to support QA team for each and every querys Resolving the defects assigned by the QA and Business team Involved in deploying code to UAT and PROD Involved in UAT Production support for the earlier releases for Daily loads pre verified and Monthly Coordinating with onsite team and involved in Oncalls Project progress reviews Tools used Datastage 85puttysql plusDB2NotepadGIT HubControl M Education Bachelor of Technology in Electronics and Communication Engineering Jawaharlal Nehru technology University College of Engineering Skills Hdfs Datastage Db2 Replication Teradata Database Ms access Oracle Sql Tableau Data replication Unix Unix shell Git Hive Real time Xml Itil Retail Excel Additional Information Technical Skills Languages SQLUNIX Shell Script XML Database Oracle 9i10g11g DB2WINSQLNetezzaTeradataSQL Assistant BI Tools IBM Datastage 11x9x8x75Tableau 20191 Big data HDFSHive Real Time App IBM CDCIBM data Replication ITIL Tools Service now ITSM Scheduling tools Control M Zeke Utilities Git hubZira Operating Systems Windows XP2007Vista Software Applications MS Word MS Excel MS Access Outlook PowerPoint Business areas Automobile RetailBankingInsurance",
    "unique_id": "61038259-37f8-42ca-a898-313f72b3095b"
}