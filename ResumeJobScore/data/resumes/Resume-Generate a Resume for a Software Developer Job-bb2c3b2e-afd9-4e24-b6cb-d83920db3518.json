{
    "clean_data": "Data Scientist Data Scientist Data Scientist VF Corporation Reading PA Data Scientist with 7 years of professional experience in the Ecommerce Retail and Music Streaming domain performing Statistical Modelling Data Extraction Data screening Data cleaning Data Exploration and Data Visualization of structured and unstructured datasets as well as implementing large scale Machine Learning and Deep Learning algorithms to deliver resourceful insights inferences and significantly impacted business revenues and user experience Experienced in facilitating the entire lifecycle of a data science project Data Extraction Data PreProcessing FeatureEngineering Dimensionality Reduction Algorithm implementation and Validation Proficient in Data transformations using log squareroot reciprocal differencing and complete boxcox transformation depending upon the dataset Knowledge of normality tests like ShapiroWilk AndersonDarling Adept at analysis of Missing data by exploring correlations and similarities introducing dummy variables for missingness and choosing from imputation methods such iterative imputer on Python Experienced in Machine Learning techniques such as regression and classification models like Linear Polynomial Support Vector Machines Decision Trees Logistic Regression Experienced in Ensemble learning using Bagging Boosting Random Forests clustering like Kmeans Indepth Knowledge of Dimensionality Reduction PCA LDA Hyperparameter tuning Model Regularization Ridge Lasso Elastic Net and Grid Search techniques to optimize model performance Adept with Python and OOP concepts such as Inheritance Polymorphism Abstraction Association etc Hands on experience in Artificial Neural Networks Deep Learning Convolution Neural Networks Expertise in creating executive Tableau Dashboards for Data visualization and deploying it to the servers Skilled in using Tidyverse framework in R and Pandas in Python for performing exploratory data analysis Proficient in Data Visualization tools such as Tableau and PowerBI Big Data tools such as Hadoop HDFS Spark and MapReduce and Microsoft Excel VLOOKUP Pivot tables Experience in Web Data Mining with Pythons ScraPy and BeautifulSoup packages along with working knowledge of Natural Language Processing NLP to analyze text patterns Experience with Python libraries including NumPy Pandas SciPy ScikitLearnstatsmodels MatplotLib Seaborn NLTK and R libraries like ggplot2 dplyr Work Experience Data Scientist VF Corporation Greensboro NC June 2018 to Present Description VF Corporation is an American worldwide apparel and footwear company Worked on the recommender system by implementing Sentiment Analysis on other people reviews and extract the best product for the user to buy Responsibilities Reviewed business requirements to analyze the data sources and worked closely with the business analysts to understand business objectives Extracted data by webscraping through the reviews using Beautiful Soup Involved in various preprocessing phases of textdata like Tokenization Stemming Lemmatization and converting the raw text data to structured data Performed data collection data cleaning feature scaling feature engineering validation visualization report findings develop strategic uses of data by Python libraries like NumPy Pandas Scipy MatplotLib ScikitLearn Used Tableau for visualizing and analyzing the data to facilitate the understanding of the team about the data Implemented various statistical techniques to manipulate the data like missing data imputation Principal Component Analysis for dimensionreduction Constructed new vocabulary to convert the data into numbers to be processed by the machine by using the approaches like Bag of Words model tfidf Word2Vec Employed statistical methodologies such as AB test experiment design and hypothesis testing and deployed models on Docker Performed Nave Bayes KNN Logistic Regression Random Forest SVM and KMeans to categorize customers into certain groups Employed various metrics such as CrossValidation LogLoss function Confusion Matrix ROCAUC to evaluate the performance of each model Using NLP developed deep learning algorithms for analyzing text over the existing dictionarybased approaches Environment PythonNumPy Pandas Matplotlib TensorFlow NLP Data Scientist SiriusXM Washington DC April 2017 to May 2018 Description SiriusXM is a broadcasting company that provides satellite radio and online radio The project required to build a recommender system by extracting and analyzing different features from raw audio files which took new songs into account Responsibilities Performed Data Collection Data Cleaning Data Visualization using Python Deep Feature Synthesis and extracted key statistical findings to develop business strategies Since sound is represented in the form of audio signals parameters like frequency decibel timbre pitch were usedfor analysis Used Librosa Python library to analyze the audio signals and plot wave plot and create a spectrogram to analyze the behavior for the sound Cleaned the audio files to remove the audio with no noise by setting a threshold and retrieve the audio above the set threshold Created 2D Convolution Neural Networks using Keras on GPUs by extracting different number of MFCC features using Librosa Used globaltemporal pooling layer to effectively compute statistics of learned features across time Implemented regularization methods like Dropout Lasso Regression and Ridge Regression to prevent the model from overfitting Final model was selected by evaluating them using various metrics like Accuracy ConfusionMatrix Precision Recall Environment Python Pandas Scikit Numpy TensorFlow Keras Librosa Data Scientist Swiggy IN November 2014 to January 2017 Description Swiggy is online food delivery company in India The project was predicting deals and coupons for frequent customers of the company Responsibilities Participated in all phases of project life cycle including data collection data mining data cleaning developing models validation and creating reports Performed data cleaning on a huge dataset which had missing data and extreme outliers from Hadoop workbooks and explored data to draw relationships and correlations between variables Performed datapreprocessing on messy data including imputation normalization scaling and feature engineering using ScikitLearn Conducted exploratory data analysis using Python Matplotlib and Seaborn to identify underlying patterns and correlations between features Build classification models based on Logistic Regression Decision Trees Support Vector Machine to predict the probability of a customer using the application Employed Ensemble Learning techniques such as Random Forests and Ada Gradient Boosting to improve the model performance by 10 Used various metrics such as FScore ROC and AUC to evaluate the performance of each model and 5Fold Cross Validation to test the models with different batches of data to optimize the models Implemented and tested the model on AWS EC2 and collaborated with development team to get the best algorithms and parameters Prepared datavisualization designed dashboards with Tableau and generated complex reports including summaries and graphs to interpret the findings to the team Environment PythonNumPy Pandas Matplotlib Amazon Web Services Jupyter Notebook Tableau Data Analyst Python Developer BigBasket IN July 2012 to October 2014 Description Bigbasket is the Indian online grocery delivery service My responsibilities included working on RestFul Web Services on Python Flask and working on a team building a predictive model to enhance the online shopping for the users Responsibilities Worked on both legacy data and new data mostly built around the user experience and grocery inventory available Performed Data Analysis on target data after transfer to Data Warehouse Created ETL solution using MS SQL Server and worked with Agile and TestDriven development within SDLC Worked on RESTful Web Services on Python Flask and built primary functions for classification Conducted data preparation and outlier detection using Pythonand implemented Logistic Regression Random Forest Nave Bayes Classifier for classification for recommendation Employed KFold Crossvalidation to test and verify the model accuracy Worked with the team to host data and certain web interfaces on Amazon Web Services EC2 and store data on S3 bucket Worked with Team manager to develop a lucrative system of classifying auditions and vendors best fitting for the company in the long run Presented executive dashboards and scorecards to visualize and present trends in the data using Excel and Python Matplotlib Environment PythonNumPy Pandas Matplotlib Amazon Web Services Python Flask REST APIs Linux Education Bachelors Skills Amazon web services Hadoop Hdfs Mapreduce Python Ggplot2 Matplotlib Anova Mapreduce Kafka Data visualization Hadoop Mongodb Database Microsoft sql server Sql server Mysql Oracle Postgresql Sql Additional Information SKILLS Languages Python R Matlab SQL Database MySQL PostgreSQL Oracle MongoDB Microsoft SQL Server Statistical Tests Hypothesis Testing ANOVA tests ttests ChiSquare Fit test Validation Techniques kfold cross validation Out of the Box Estimates AB Tests Optimization Techniques Gradient Descent Stochastic Gradient Descent MiniBatch Gradient Descent Gradient Optimization Adam Momentum RMSProp Data Visualization Tableau Microsoft PowerBI ggplot2 MatplotLib Seaborn Big Data Apache Hadoop HDFS Kafka MapReduce Spark Cloud Technologies Amazon Web Services Tools and Software PyCharm XCode Jupyter Notebook Microsoft SQL Linux Unix Microsoft Office",
    "entities": [
        "TestDriven",
        "Data Visualization",
        "MFCC",
        "MS SQL Server",
        "Natural Language Processing NLP",
        "ChiSquare",
        "Created 2D Convolution Neural Networks",
        "Amazon Web Services EC2",
        "ScikitLearn Conducted",
        "Validation Proficient",
        "Linear Polynomial Support Vector Machines",
        "PA Data Scientist",
        "Bag of Words",
        "Data Extraction Data PreProcessing FeatureEngineering Dimensionality Reduction Algorithm",
        "Present Description VF Corporation",
        "Python Experienced",
        "NC",
        "Performed",
        "India",
        "AUC",
        "Data Warehouse Created",
        "MatplotLib ScikitLearn",
        "Presented",
        "Validation Techniques",
        "the Box Estimates AB Tests Optimization Techniques Gradient Descent Stochastic Gradient Descent MiniBatch Gradient Descent Gradient Optimization",
        "Data Scientist Data Scientist Data Scientist VF Corporation Reading",
        "AWS",
        "Ensemble",
        "Pythonand",
        "Accuracy ConfusionMatrix Precision Recall Environment Python",
        "Keras",
        "Microsoft",
        "Employed Ensemble Learning",
        "Cross Validation",
        "Responsibilities Performed Data Collection Data Cleaning Data Visualization",
        "Deep Learning",
        "RestFul Web Services on Python Flask",
        "Kmeans Indepth Knowledge of Dimensionality Reduction PCA LDA Hyperparameter",
        "CrossValidation LogLoss",
        "Sql",
        "ScikitLearnstatsmodels",
        "Web Data Mining with Pythons ScraPy",
        "Hadoop HDFS Spark",
        "Bagging Boosting Random Forests",
        "Hadoop",
        "Data",
        "Artificial Neural Networks Deep Learning Convolution Neural Networks Expertise",
        "Responsibilities Reviewed",
        "MapReduce",
        "ShapiroWilk AndersonDarling Adept",
        "NLP",
        "Washington DC",
        "Team",
        "Inheritance Polymorphism Abstraction Association etc Hands",
        "Performed Data Analysis",
        "Statistical Modelling Data Extraction Data",
        "Tableau",
        "Logistic Regression Random Forest Nave Bayes Classifier",
        "Machine Learning",
        "Logistic Regression Decision Trees Support Vector Machine",
        "NLP Data Scientist",
        "ROC",
        "Tokenization Stemming Lemmatization",
        "Mysql Oracle Postgresql Sql Additional Information SKILLS Languages Python R Matlab SQL Database",
        "Principal Component Analysis",
        "SDLC Worked on RESTful Web Services on Python Flask",
        "Beautiful Soup Involved",
        "Data Exploration and Data Visualization",
        "Agile"
    ],
    "experience": "Experience in Web Data Mining with Pythons ScraPy and BeautifulSoup packages along with working knowledge of Natural Language Processing NLP to analyze text patterns Experience with Python libraries including NumPy Pandas SciPy ScikitLearnstatsmodels MatplotLib Seaborn NLTK and R libraries like ggplot2 dplyr Work Experience Data Scientist VF Corporation Greensboro NC June 2018 to Present Description VF Corporation is an American worldwide apparel and footwear company Worked on the recommender system by implementing Sentiment Analysis on other people reviews and extract the best product for the user to buy Responsibilities Reviewed business requirements to analyze the data sources and worked closely with the business analysts to understand business objectives Extracted data by webscraping through the reviews using Beautiful Soup Involved in various preprocessing phases of textdata like Tokenization Stemming Lemmatization and converting the raw text data to structured data Performed data collection data cleaning feature scaling feature engineering validation visualization report findings develop strategic uses of data by Python libraries like NumPy Pandas Scipy MatplotLib ScikitLearn Used Tableau for visualizing and analyzing the data to facilitate the understanding of the team about the data Implemented various statistical techniques to manipulate the data like missing data imputation Principal Component Analysis for dimensionreduction Constructed new vocabulary to convert the data into numbers to be processed by the machine by using the approaches like Bag of Words model tfidf Word2Vec Employed statistical methodologies such as AB test experiment design and hypothesis testing and deployed models on Docker Performed Nave Bayes KNN Logistic Regression Random Forest SVM and KMeans to categorize customers into certain groups Employed various metrics such as CrossValidation LogLoss function Confusion Matrix ROCAUC to evaluate the performance of each model Using NLP developed deep learning algorithms for analyzing text over the existing dictionarybased approaches Environment PythonNumPy Pandas Matplotlib TensorFlow NLP Data Scientist SiriusXM Washington DC April 2017 to May 2018 Description SiriusXM is a broadcasting company that provides satellite radio and online radio The project required to build a recommender system by extracting and analyzing different features from raw audio files which took new songs into account Responsibilities Performed Data Collection Data Cleaning Data Visualization using Python Deep Feature Synthesis and extracted key statistical findings to develop business strategies Since sound is represented in the form of audio signals parameters like frequency decibel timbre pitch were usedfor analysis Used Librosa Python library to analyze the audio signals and plot wave plot and create a spectrogram to analyze the behavior for the sound Cleaned the audio files to remove the audio with no noise by setting a threshold and retrieve the audio above the set threshold Created 2D Convolution Neural Networks using Keras on GPUs by extracting different number of MFCC features using Librosa Used globaltemporal pooling layer to effectively compute statistics of learned features across time Implemented regularization methods like Dropout Lasso Regression and Ridge Regression to prevent the model from overfitting Final model was selected by evaluating them using various metrics like Accuracy ConfusionMatrix Precision Recall Environment Python Pandas Scikit Numpy TensorFlow Keras Librosa Data Scientist Swiggy IN November 2014 to January 2017 Description Swiggy is online food delivery company in India The project was predicting deals and coupons for frequent customers of the company Responsibilities Participated in all phases of project life cycle including data collection data mining data cleaning developing models validation and creating reports Performed data cleaning on a huge dataset which had missing data and extreme outliers from Hadoop workbooks and explored data to draw relationships and correlations between variables Performed datapreprocessing on messy data including imputation normalization scaling and feature engineering using ScikitLearn Conducted exploratory data analysis using Python Matplotlib and Seaborn to identify underlying patterns and correlations between features Build classification models based on Logistic Regression Decision Trees Support Vector Machine to predict the probability of a customer using the application Employed Ensemble Learning techniques such as Random Forests and Ada Gradient Boosting to improve the model performance by 10 Used various metrics such as FScore ROC and AUC to evaluate the performance of each model and 5Fold Cross Validation to test the models with different batches of data to optimize the models Implemented and tested the model on AWS EC2 and collaborated with development team to get the best algorithms and parameters Prepared datavisualization designed dashboards with Tableau and generated complex reports including summaries and graphs to interpret the findings to the team Environment PythonNumPy Pandas Matplotlib Amazon Web Services Jupyter Notebook Tableau Data Analyst Python Developer BigBasket IN July 2012 to October 2014 Description Bigbasket is the Indian online grocery delivery service My responsibilities included working on RestFul Web Services on Python Flask and working on a team building a predictive model to enhance the online shopping for the users Responsibilities Worked on both legacy data and new data mostly built around the user experience and grocery inventory available Performed Data Analysis on target data after transfer to Data Warehouse Created ETL solution using MS SQL Server and worked with Agile and TestDriven development within SDLC Worked on RESTful Web Services on Python Flask and built primary functions for classification Conducted data preparation and outlier detection using Pythonand implemented Logistic Regression Random Forest Nave Bayes Classifier for classification for recommendation Employed KFold Crossvalidation to test and verify the model accuracy Worked with the team to host data and certain web interfaces on Amazon Web Services EC2 and store data on S3 bucket Worked with Team manager to develop a lucrative system of classifying auditions and vendors best fitting for the company in the long run Presented executive dashboards and scorecards to visualize and present trends in the data using Excel and Python Matplotlib Environment PythonNumPy Pandas Matplotlib Amazon Web Services Python Flask REST APIs Linux Education Bachelors Skills Amazon web services Hadoop Hdfs Mapreduce Python Ggplot2 Matplotlib Anova Mapreduce Kafka Data visualization Hadoop Mongodb Database Microsoft sql server Sql server Mysql Oracle Postgresql Sql Additional Information SKILLS Languages Python R Matlab SQL Database MySQL PostgreSQL Oracle MongoDB Microsoft SQL Server Statistical Tests Hypothesis Testing ANOVA tests ttests ChiSquare Fit test Validation Techniques kfold cross validation Out of the Box Estimates AB Tests Optimization Techniques Gradient Descent Stochastic Gradient Descent MiniBatch Gradient Descent Gradient Optimization Adam Momentum RMSProp Data Visualization Tableau Microsoft PowerBI ggplot2 MatplotLib Seaborn Big Data Apache Hadoop HDFS Kafka MapReduce Spark Cloud Technologies Amazon Web Services Tools and Software PyCharm XCode Jupyter Notebook Microsoft SQL Linux Unix Microsoft Office",
    "extracted_keywords": [
        "Data",
        "Scientist",
        "Data",
        "Scientist",
        "Data",
        "Scientist",
        "VF",
        "Corporation",
        "PA",
        "Data",
        "Scientist",
        "years",
        "experience",
        "Ecommerce",
        "Retail",
        "Music",
        "Streaming",
        "domain",
        "Statistical",
        "Modelling",
        "Data",
        "Extraction",
        "Data",
        "Data",
        "Data",
        "Exploration",
        "Data",
        "Visualization",
        "datasets",
        "scale",
        "Machine",
        "Learning",
        "Deep",
        "Learning",
        "insights",
        "inferences",
        "business",
        "revenues",
        "user",
        "experience",
        "lifecycle",
        "data",
        "science",
        "project",
        "Data",
        "Extraction",
        "Data",
        "PreProcessing",
        "FeatureEngineering",
        "Dimensionality",
        "Reduction",
        "Algorithm",
        "implementation",
        "Validation",
        "Proficient",
        "Data",
        "transformations",
        "log",
        "squareroot",
        "differencing",
        "boxcox",
        "transformation",
        "Knowledge",
        "normality",
        "tests",
        "ShapiroWilk",
        "AndersonDarling",
        "Adept",
        "analysis",
        "data",
        "correlations",
        "similarities",
        "variables",
        "missingness",
        "imputation",
        "methods",
        "imputer",
        "Python",
        "Machine",
        "Learning",
        "techniques",
        "regression",
        "classification",
        "models",
        "Linear",
        "Polynomial",
        "Support",
        "Vector",
        "Machines",
        "Decision",
        "Trees",
        "Logistic",
        "Regression",
        "Ensemble",
        "learning",
        "Bagging",
        "Random",
        "Forests",
        "Kmeans",
        "Knowledge",
        "Dimensionality",
        "Reduction",
        "PCA",
        "LDA",
        "Hyperparameter",
        "Model",
        "Regularization",
        "Ridge",
        "Lasso",
        "Elastic",
        "Net",
        "Grid",
        "Search",
        "techniques",
        "model",
        "performance",
        "Adept",
        "Python",
        "OOP",
        "concepts",
        "Inheritance",
        "Polymorphism",
        "Abstraction",
        "Association",
        "Hands",
        "experience",
        "Artificial",
        "Neural",
        "Networks",
        "Deep",
        "Learning",
        "Convolution",
        "Neural",
        "Networks",
        "Expertise",
        "executive",
        "Tableau",
        "Dashboards",
        "Data",
        "visualization",
        "servers",
        "framework",
        "R",
        "Pandas",
        "Python",
        "data",
        "analysis",
        "Proficient",
        "Data",
        "Visualization",
        "tools",
        "Tableau",
        "PowerBI",
        "Big",
        "Data",
        "tools",
        "Hadoop",
        "HDFS",
        "Spark",
        "MapReduce",
        "Microsoft",
        "Excel",
        "VLOOKUP",
        "Pivot",
        "Experience",
        "Web",
        "Data",
        "Mining",
        "Pythons",
        "BeautifulSoup",
        "packages",
        "knowledge",
        "Natural",
        "Language",
        "Processing",
        "NLP",
        "text",
        "patterns",
        "Experience",
        "Python",
        "libraries",
        "NumPy",
        "Pandas",
        "SciPy",
        "ScikitLearnstatsmodels",
        "MatplotLib",
        "Seaborn",
        "NLTK",
        "R",
        "libraries",
        "ggplot2",
        "dplyr",
        "Work",
        "Experience",
        "Data",
        "Scientist",
        "VF",
        "Corporation",
        "Greensboro",
        "NC",
        "June",
        "Present",
        "Description",
        "VF",
        "Corporation",
        "apparel",
        "footwear",
        "company",
        "recommender",
        "system",
        "Sentiment",
        "Analysis",
        "people",
        "reviews",
        "product",
        "user",
        "Responsibilities",
        "business",
        "requirements",
        "data",
        "sources",
        "business",
        "analysts",
        "business",
        "objectives",
        "data",
        "reviews",
        "Beautiful",
        "Soup",
        "phases",
        "textdata",
        "Tokenization",
        "Stemming",
        "Lemmatization",
        "text",
        "data",
        "data",
        "data",
        "collection",
        "data",
        "feature",
        "feature",
        "engineering",
        "validation",
        "visualization",
        "report",
        "findings",
        "uses",
        "data",
        "Python",
        "libraries",
        "NumPy",
        "Pandas",
        "Scipy",
        "MatplotLib",
        "ScikitLearn",
        "Tableau",
        "data",
        "understanding",
        "team",
        "data",
        "techniques",
        "data",
        "data",
        "imputation",
        "Principal",
        "Component",
        "Analysis",
        "dimensionreduction",
        "vocabulary",
        "data",
        "numbers",
        "machine",
        "approaches",
        "Bag",
        "Words",
        "model",
        "methodologies",
        "AB",
        "test",
        "experiment",
        "design",
        "hypothesis",
        "testing",
        "models",
        "Docker",
        "Performed",
        "Nave",
        "Bayes",
        "KNN",
        "Logistic",
        "Regression",
        "Random",
        "Forest",
        "SVM",
        "KMeans",
        "customers",
        "groups",
        "metrics",
        "CrossValidation",
        "LogLoss",
        "function",
        "Confusion",
        "Matrix",
        "ROCAUC",
        "performance",
        "model",
        "NLP",
        "learning",
        "algorithms",
        "text",
        "approaches",
        "Environment",
        "Pandas",
        "Matplotlib",
        "TensorFlow",
        "NLP",
        "Data",
        "Scientist",
        "SiriusXM",
        "Washington",
        "DC",
        "April",
        "May",
        "Description",
        "SiriusXM",
        "broadcasting",
        "company",
        "satellite",
        "radio",
        "radio",
        "project",
        "recommender",
        "system",
        "features",
        "files",
        "songs",
        "account",
        "Responsibilities",
        "Performed",
        "Data",
        "Collection",
        "Data",
        "Cleaning",
        "Data",
        "Visualization",
        "Python",
        "Deep",
        "Feature",
        "Synthesis",
        "findings",
        "business",
        "strategies",
        "sound",
        "form",
        "signals",
        "parameters",
        "frequency",
        "decibel",
        "timbre",
        "pitch",
        "analysis",
        "Librosa",
        "Python",
        "library",
        "signals",
        "plot",
        "wave",
        "plot",
        "spectrogram",
        "behavior",
        "sound",
        "files",
        "audio",
        "noise",
        "threshold",
        "audio",
        "threshold",
        "2D",
        "Convolution",
        "Neural",
        "Networks",
        "Keras",
        "GPUs",
        "number",
        "MFCC",
        "features",
        "Librosa",
        "pooling",
        "layer",
        "statistics",
        "features",
        "time",
        "regularization",
        "methods",
        "Dropout",
        "Lasso",
        "Regression",
        "Ridge",
        "Regression",
        "model",
        "model",
        "metrics",
        "Accuracy",
        "ConfusionMatrix",
        "Precision",
        "Recall",
        "Environment",
        "Python",
        "Scikit",
        "Numpy",
        "TensorFlow",
        "Keras",
        "Librosa",
        "Data",
        "Scientist",
        "Swiggy",
        "November",
        "January",
        "Description",
        "Swiggy",
        "food",
        "delivery",
        "company",
        "India",
        "project",
        "deals",
        "coupons",
        "customers",
        "company",
        "Responsibilities",
        "phases",
        "project",
        "life",
        "cycle",
        "data",
        "collection",
        "data",
        "mining",
        "data",
        "models",
        "validation",
        "reports",
        "Performed",
        "data",
        "dataset",
        "data",
        "outliers",
        "Hadoop",
        "workbooks",
        "data",
        "relationships",
        "correlations",
        "variables",
        "data",
        "imputation",
        "normalization",
        "scaling",
        "feature",
        "engineering",
        "ScikitLearn",
        "data",
        "analysis",
        "Python",
        "Matplotlib",
        "Seaborn",
        "patterns",
        "correlations",
        "features",
        "classification",
        "models",
        "Logistic",
        "Regression",
        "Decision",
        "Trees",
        "Support",
        "Vector",
        "Machine",
        "probability",
        "customer",
        "application",
        "Employed",
        "Ensemble",
        "Learning",
        "techniques",
        "Random",
        "Forests",
        "Ada",
        "Gradient",
        "Boosting",
        "model",
        "performance",
        "metrics",
        "FScore",
        "ROC",
        "AUC",
        "performance",
        "model",
        "Cross",
        "Validation",
        "models",
        "batches",
        "data",
        "models",
        "model",
        "AWS",
        "EC2",
        "development",
        "team",
        "algorithms",
        "parameters",
        "Prepared",
        "datavisualization",
        "dashboards",
        "Tableau",
        "reports",
        "summaries",
        "graphs",
        "findings",
        "team",
        "Environment",
        "Matplotlib",
        "Amazon",
        "Web",
        "Services",
        "Jupyter",
        "Notebook",
        "Tableau",
        "Data",
        "Analyst",
        "Python",
        "Developer",
        "BigBasket",
        "July",
        "October",
        "Description",
        "Bigbasket",
        "grocery",
        "delivery",
        "service",
        "responsibilities",
        "RestFul",
        "Web",
        "Services",
        "Python",
        "Flask",
        "team",
        "model",
        "online",
        "shopping",
        "users",
        "Responsibilities",
        "legacy",
        "data",
        "data",
        "user",
        "experience",
        "grocery",
        "inventory",
        "Performed",
        "Data",
        "Analysis",
        "target",
        "data",
        "transfer",
        "Data",
        "Warehouse",
        "ETL",
        "solution",
        "MS",
        "SQL",
        "Server",
        "Agile",
        "TestDriven",
        "development",
        "SDLC",
        "Web",
        "Services",
        "Python",
        "Flask",
        "functions",
        "classification",
        "data",
        "preparation",
        "outlier",
        "detection",
        "Pythonand",
        "Logistic",
        "Regression",
        "Random",
        "Forest",
        "Nave",
        "Bayes",
        "Classifier",
        "classification",
        "recommendation",
        "Employed",
        "KFold",
        "Crossvalidation",
        "model",
        "accuracy",
        "team",
        "data",
        "web",
        "interfaces",
        "Amazon",
        "Web",
        "Services",
        "EC2",
        "store",
        "data",
        "S3",
        "bucket",
        "Team",
        "manager",
        "system",
        "auditions",
        "vendors",
        "company",
        "run",
        "executive",
        "dashboards",
        "scorecards",
        "trends",
        "data",
        "Excel",
        "Python",
        "Matplotlib",
        "Environment",
        "Pandas",
        "Matplotlib",
        "Amazon",
        "Web",
        "Services",
        "Python",
        "Flask",
        "REST",
        "APIs",
        "Linux",
        "Education",
        "Bachelors",
        "Skills",
        "Amazon",
        "web",
        "services",
        "Hadoop",
        "Hdfs",
        "Mapreduce",
        "Python",
        "Ggplot2",
        "Matplotlib",
        "Anova",
        "Mapreduce",
        "Kafka",
        "Data",
        "visualization",
        "Hadoop",
        "Mongodb",
        "Database",
        "Microsoft",
        "server",
        "Sql",
        "server",
        "Mysql",
        "Oracle",
        "Postgresql",
        "Sql",
        "Additional",
        "Information",
        "SKILLS",
        "Languages",
        "Python",
        "R",
        "Matlab",
        "SQL",
        "Database",
        "MySQL",
        "PostgreSQL",
        "Oracle",
        "MongoDB",
        "Microsoft",
        "SQL",
        "Server",
        "Statistical",
        "Tests",
        "Hypothesis",
        "Testing",
        "ANOVA",
        "ttests",
        "ChiSquare",
        "Fit",
        "test",
        "Validation",
        "Techniques",
        "kfold",
        "validation",
        "Box",
        "Estimates",
        "AB",
        "Tests",
        "Optimization",
        "Techniques",
        "Gradient",
        "Descent",
        "Stochastic",
        "Gradient",
        "Descent",
        "MiniBatch",
        "Gradient",
        "Descent",
        "Gradient",
        "Optimization",
        "Adam",
        "Momentum",
        "RMSProp",
        "Data",
        "Visualization",
        "Tableau",
        "Microsoft",
        "PowerBI",
        "MatplotLib",
        "Seaborn",
        "Big",
        "Data",
        "Apache",
        "Hadoop",
        "HDFS",
        "Kafka",
        "MapReduce",
        "Spark",
        "Cloud",
        "Technologies",
        "Amazon",
        "Web",
        "Services",
        "Tools",
        "Software",
        "PyCharm",
        "XCode",
        "Jupyter",
        "Notebook",
        "Microsoft",
        "SQL",
        "Linux",
        "Unix",
        "Microsoft",
        "Office"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T22:05:22.399493",
    "resume_data": "Data Scientist Data Scientist Data Scientist VF Corporation Reading PA Data Scientist with 7 years of professional experience in the Ecommerce Retail and Music Streaming domain performing Statistical Modelling Data Extraction Data screening Data cleaning Data Exploration and Data Visualization of structured and unstructured datasets as well as implementing large scale Machine Learning and Deep Learning algorithms to deliver resourceful insights inferences and significantly impacted business revenues and user experience Experienced in facilitating the entire lifecycle of a data science project Data Extraction Data PreProcessing FeatureEngineering Dimensionality Reduction Algorithm implementation and Validation Proficient in Data transformations using log squareroot reciprocal differencing and complete boxcox transformation depending upon the dataset Knowledge of normality tests like ShapiroWilk AndersonDarling Adept at analysis of Missing data by exploring correlations and similarities introducing dummy variables for missingness and choosing from imputation methods such iterative imputer on Python Experienced in Machine Learning techniques such as regression and classification models like Linear Polynomial Support Vector Machines Decision Trees Logistic Regression Experienced in Ensemble learning using Bagging Boosting Random Forests clustering like Kmeans Indepth Knowledge of Dimensionality Reduction PCA LDA Hyperparameter tuning Model Regularization Ridge Lasso Elastic Net and Grid Search techniques to optimize model performance Adept with Python and OOP concepts such as Inheritance Polymorphism Abstraction Association etc Hands on experience in Artificial Neural Networks Deep Learning Convolution Neural Networks Expertise in creating executive Tableau Dashboards for Data visualization and deploying it to the servers Skilled in using Tidyverse framework in R and Pandas in Python for performing exploratory data analysis Proficient in Data Visualization tools such as Tableau and PowerBI Big Data tools such as Hadoop HDFS Spark and MapReduce and Microsoft Excel VLOOKUP Pivot tables Experience in Web Data Mining with Pythons ScraPy and BeautifulSoup packages along with working knowledge of Natural Language Processing NLP to analyze text patterns Experience with Python libraries including NumPy Pandas SciPy ScikitLearnstatsmodels MatplotLib Seaborn NLTK and R libraries like ggplot2 dplyr Work Experience Data Scientist VF Corporation Greensboro NC June 2018 to Present Description VF Corporation is an American worldwide apparel and footwear company Worked on the recommender system by implementing Sentiment Analysis on other people reviews and extract the best product for the user to buy Responsibilities Reviewed business requirements to analyze the data sources and worked closely with the business analysts to understand business objectives Extracted data by webscraping through the reviews using Beautiful Soup Involved in various preprocessing phases of textdata like Tokenization Stemming Lemmatization and converting the raw text data to structured data Performed data collection data cleaning feature scaling feature engineering validation visualization report findings develop strategic uses of data by Python libraries like NumPy Pandas Scipy MatplotLib ScikitLearn Used Tableau for visualizing and analyzing the data to facilitate the understanding of the team about the data Implemented various statistical techniques to manipulate the data like missing data imputation Principal Component Analysis for dimensionreduction Constructed new vocabulary to convert the data into numbers to be processed by the machine by using the approaches like Bag of Words model tfidf Word2Vec Employed statistical methodologies such as AB test experiment design and hypothesis testing and deployed models on Docker Performed Nave Bayes KNN Logistic Regression Random Forest SVM and KMeans to categorize customers into certain groups Employed various metrics such as CrossValidation LogLoss function Confusion Matrix ROCAUC to evaluate the performance of each model Using NLP developed deep learning algorithms for analyzing text over the existing dictionarybased approaches Environment PythonNumPy Pandas Matplotlib TensorFlow NLP Data Scientist SiriusXM Washington DC April 2017 to May 2018 Description SiriusXM is a broadcasting company that provides satellite radio and online radio The project required to build a recommender system by extracting and analyzing different features from raw audio files which took new songs into account Responsibilities Performed Data Collection Data Cleaning Data Visualization using Python Deep Feature Synthesis and extracted key statistical findings to develop business strategies Since sound is represented in the form of audio signals parameters like frequency decibel timbre pitch were usedfor analysis Used Librosa Python library to analyze the audio signals and plot wave plot and create a spectrogram to analyze the behavior for the sound Cleaned the audio files to remove the audio with no noise by setting a threshold and retrieve the audio above the set threshold Created 2D Convolution Neural Networks using Keras on GPUs by extracting different number of MFCC features using Librosa Used globaltemporal pooling layer to effectively compute statistics of learned features across time Implemented regularization methods like Dropout Lasso Regression and Ridge Regression to prevent the model from overfitting Final model was selected by evaluating them using various metrics like Accuracy ConfusionMatrix Precision Recall Environment Python Pandas Scikit Numpy TensorFlow Keras Librosa Data Scientist Swiggy IN November 2014 to January 2017 Description Swiggy is online food delivery company in India The project was predicting deals and coupons for frequent customers of the company Responsibilities Participated in all phases of project life cycle including data collection data mining data cleaning developing models validation and creating reports Performed data cleaning on a huge dataset which had missing data and extreme outliers from Hadoop workbooks and explored data to draw relationships and correlations between variables Performed datapreprocessing on messy data including imputation normalization scaling and feature engineering using ScikitLearn Conducted exploratory data analysis using Python Matplotlib and Seaborn to identify underlying patterns and correlations between features Build classification models based on Logistic Regression Decision Trees Support Vector Machine to predict the probability of a customer using the application Employed Ensemble Learning techniques such as Random Forests and Ada Gradient Boosting to improve the model performance by 10 Used various metrics such as FScore ROC and AUC to evaluate the performance of each model and 5Fold Cross Validation to test the models with different batches of data to optimize the models Implemented and tested the model on AWS EC2 and collaborated with development team to get the best algorithms and parameters Prepared datavisualization designed dashboards with Tableau and generated complex reports including summaries and graphs to interpret the findings to the team Environment PythonNumPy Pandas Matplotlib Amazon Web Services Jupyter Notebook Tableau Data Analyst Python Developer BigBasket IN July 2012 to October 2014 Description Bigbasket is the Indian online grocery delivery service My responsibilities included working on RestFul Web Services on Python Flask and working on a team building a predictive model to enhance the online shopping for the users Responsibilities Worked on both legacy data and new data mostly built around the user experience and grocery inventory available Performed Data Analysis on target data after transfer to Data Warehouse Created ETL solution using MS SQL Server and worked with Agile and TestDriven development within SDLC Worked on RESTful Web Services on Python Flask and built primary functions for classification Conducted data preparation and outlier detection using Pythonand implemented Logistic Regression Random Forest Nave Bayes Classifier for classification for recommendation Employed KFold Crossvalidation to test and verify the model accuracy Worked with the team to host data and certain web interfaces on Amazon Web Services EC2 and store data on S3 bucket Worked with Team manager to develop a lucrative system of classifying auditions and vendors best fitting for the company in the long run Presented executive dashboards and scorecards to visualize and present trends in the data using Excel and Python Matplotlib Environment PythonNumPy Pandas Matplotlib Amazon Web Services Python Flask REST APIs Linux Education Bachelors Skills Amazon web services Hadoop Hdfs Mapreduce Python Ggplot2 Matplotlib Anova Mapreduce Kafka Data visualization Hadoop Mongodb Database Microsoft sql server Sql server Mysql Oracle Postgresql Sql Additional Information SKILLS Languages Python R Matlab SQL Database MySQL PostgreSQL Oracle MongoDB Microsoft SQL Server Statistical Tests Hypothesis Testing ANOVA tests ttests ChiSquare Fit test Validation Techniques kfold cross validation Out of the Box Estimates AB Tests Optimization Techniques Gradient Descent Stochastic Gradient Descent MiniBatch Gradient Descent Gradient Optimization Adam Momentum RMSProp Data Visualization Tableau Microsoft PowerBI ggplot2 MatplotLib Seaborn Big Data Apache Hadoop HDFS Kafka MapReduce Spark Cloud Technologies Amazon Web Services Tools and Software PyCharm XCode Jupyter Notebook Microsoft SQL Linux Unix Microsoft Office",
    "unique_id": "bb2c3b2e-afd9-4e24-b6cb-d83920db3518"
}