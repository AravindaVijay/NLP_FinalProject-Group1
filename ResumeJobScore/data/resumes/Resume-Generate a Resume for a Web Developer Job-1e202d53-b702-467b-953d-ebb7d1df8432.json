{
    "clean_data": "Technology Lead Hadoop Technology Lead Hadoop Technology Lead Hadoop Neiman Marcus Dallas TX Work Experience Technology Lead Hadoop Neiman Marcus Dallas TX May 2018 to Present AA to store and join customer centric data like click stream sales email campaigns in generating UCIDs and personalization which are consumed by CXP through APIs Developed data models for personalization and product recommendations using Storm Kafka Hive Pig Processing PLLC Private Label Credit Card Capitol One data to provide insights into percentage of penetration of sales by associates and stores Developed scripts for data migration of enterprise data from inhouse infra to AWS cloud Environment Hadoop26chd51340 node AWS cloud Hive121 Storm Cassandra Solr CouchDB EC2 S3 Airflow ETLHadoop Developer Lutron Inc Philadelphia PA April 2016 to April 2018 Hadoop and Informatica based ETL and analytical system to have insights about customers usage of Lutron products across different product line leading to future enhancements improvement in business and services Developed data pipeline using Spark Kafka Hive Pig and HBase to ingest customer system usage data and financial histories into Hadoop cluster for analysis Developed Scala scripts UDFs using both Data framesSQL and RDDMapReduce in Spark for data aggregation and writing data back into S3 through Sqoop Extensively used Informatica to create data ingestion jobs into HDFS using complex data file objects such as AVRO and Parquet and to evaluate dynamic mapping capabilities Implement Data Quality Rules using Informatica Data Quality IDQ to check correctness of the source files and perform the data cleansingenrichment Analyze log records data a day and its aggregated hourly daily reporting using Tableau Environment Hadoop27 Informatica9x Hive121 Spark16 Teradata Oracle EC2 S3 Hadoop Developer Cisco Systems San Jose CA September 2014 to March 2016 Worked with highly unstructured and semi structured data of 100TB in size Developed Pig and Hive scripts to be used by end user analyst product managers requirements for adhoc analysis Used Informatica to validate and test the business logic implemented in the mappings and fix the bugs Developed reusable Mapplets and Transformations Managed External tables in Hive for optimized performance using Sqoop jobs Solved performance issues in Hive and Pig scripts with understanding of joins group and aggregation and how it translates to MapReduce jobs Exploring with Spark improving the performance and optimization of the existing algorithms in Hadoop using Spark context SparkSQL Data Frame pair RDDs Spark on YARN Worked with HadoopKerberos security environment which is supported by the Cloudera team Environment 32 Node Hadoop 26 cluster Informatica9x HDFS Flume 15 Sqoop 143 Hive 101 Spark 14 HBase XML JSON Teradata Oracle MongoDB Cassandra Hadoop Developer Bayer Healthcare Leverkusen DE November 2013 to August 2014 Migration of 100 TBs of data from different databases ie Oracle SQL Server to Hadoop Wiring code in different applications of Hadoop and Informatica Ecosystem Extensively involved in performance tuning of the Informatica ETL mappings by using the caches and overriding the SQL queries and also by using Parameter files Worked on various file formats Avro SerDe Parquet and Text by using snappy compression Used Pig Custom Loaders to load different forms of data files such as XML JSON and CSV Designed dynamic partition mechanism for optimal query performance of system using HIVE to reduce report time generation under SLA requirements Environment Hadoop 22 Informatica Power Center 9x HDFS HBase Flume 14 Sqoop 143 Hive 0131 Avro 174 Parquet 14 XML JSON Oracle 11g Amazon EC2 S3 ETL Developer Grattan Plc Bradford February 2010 to October 2013 Developed mappingssessions to import transform and load data into respective target tables and flat files using Informatica Power Center for data loading Automation of the Informatica ETL jobs for different ETL design pattern Extensively used Transformations like Router Aggregator Source Qualifier Joiner Expression Aggregator and Sequence generator by using Source Analyzer Warehouse Designer Mapping Designer Mapplet and Transformation Developer Environment Informatica Power Center 9x Repository Manager Designer Workflow Manager and Workflow Monitor Oracle 11g SeaQuest HPDM SQL Server Teradata Toad ControlM ETL Developer Star Health and Allied Insurance Company Ltd Bengaluru Karnataka October 2007 to February 2010 Extensively used Slowly Changing Dimensions technique for updating dimensional schema Processed data using various transformations like Aggregator Router Expression Source Qualifier Filter Lookup Joiner Sorter XML Source qualifier and webconsumer for WSDL Used Informatica user defined functions to reduce the code dependency Environment Informatica Power Center  Informatica Power Connect Power Exchange Power Analyzer Toad Erwin Oracle 11g10g Teradata V2R5 PLSQL ODI Trillium 11 ETL Developer United Overseas bank Singapore October 2005 to September 2007 Used SSIS as an Extract Transform Loading ETL tool of SQL Server to populate data from various data sources creating packages for different data loading operations for application Extensive use of TransactSQL stored procedures trigger scripts for creating database objects Generated various reports using features such as group by drilldowns drill through subreports Parameterized Reports Deploying new strategies for checksum calculations and exception population using mapplets and normalizer transformations Environment SQL Server 2005 TSQL SSISDTS Designer and Reporting tools ControlM Java Developer ExpertNet CAD Pune Maharashtra January 2004 to August 2005 Developed the web applications using Spring MVC Framework including writing actions classes forms custom tag libraries and JSP pages Worked on Integration of Spring and Hibernate Frameworks using Spring ORM Module Implemented caching techniques wrote POJO classes for storing data and DAOs to retrieve the data and did database configurations Java Developer Honeywell Bengaluru Karnataka January 2002 to January 2004 Implementation of routing and shortest path algorithms along with parsing logic for device discovery using HeartBeat Implementation of Java Native InterfaceJNI APIs for Indus Mote to access devices dynamically through C code Education Masters in Data Analytics in Data Analytics Boston University Boston MA",
    "entities": [
        "Transformation Developer Environment Informatica Power Center",
        "Tableau Environment",
        "SQL Server",
        "C code Education Masters",
        "Oracle SQL Server",
        "TX Work Experience Technology Lead Hadoop",
        "CSV",
        "SparkSQL Data Frame",
        "Developed",
        "Informatica",
        "ETL",
        "Implement Data Quality Rules",
        "Informatica Power Center",
        "Bradford",
        "Informatica Power Center  Informatica Power Connect Power Exchange Power Analyzer",
        "Sqoop",
        "Informatica Data Quality IDQ",
        "Node Hadoop",
        "HIVE",
        "POJO",
        "Karnataka",
        "AVRO",
        "Boston MA",
        "Informatica Ecosystem",
        "Worked on Integration of Spring and",
        "CXP",
        "San Jose",
        "Data Analytics Boston University",
        "Amazon",
        "Workflow Monitor Oracle",
        "Processed",
        "Extract Transform Loading ETL",
        "JSP",
        "Allied Insurance Company Ltd",
        "TB",
        "SQL",
        "Hadoop",
        "Data",
        "Spring MVC Framework",
        "MapReduce",
        "HadoopKerberos",
        "Data Analytics",
        "Lutron Inc Philadelphia",
        "SeaQuest",
        "Singapore",
        "Slowly Changing Dimensions",
        "Hadoop Wiring",
        "HBase",
        "Storm Kafka Hive Pig Processing PLLC Private Label Credit Card Capitol One",
        "Spark Kafka Hive Pig and",
        "TX",
        "Hive",
        "United Overseas bank",
        "Spark",
        "SSIS"
    ],
    "experience": "Experience Technology Lead Hadoop Neiman Marcus Dallas TX May 2018 to Present AA to store and join customer centric data like click stream sales email campaigns in generating UCIDs and personalization which are consumed by CXP through APIs Developed data models for personalization and product recommendations using Storm Kafka Hive Pig Processing PLLC Private Label Credit Card Capitol One data to provide insights into percentage of penetration of sales by associates and stores Developed scripts for data migration of enterprise data from inhouse infra to AWS cloud Environment Hadoop26chd51340 node AWS cloud Hive121 Storm Cassandra Solr CouchDB EC2 S3 Airflow ETLHadoop Developer Lutron Inc Philadelphia PA April 2016 to April 2018 Hadoop and Informatica based ETL and analytical system to have insights about customers usage of Lutron products across different product line leading to future enhancements improvement in business and services Developed data pipeline using Spark Kafka Hive Pig and HBase to ingest customer system usage data and financial histories into Hadoop cluster for analysis Developed Scala scripts UDFs using both Data framesSQL and RDDMapReduce in Spark for data aggregation and writing data back into S3 through Sqoop Extensively used Informatica to create data ingestion jobs into HDFS using complex data file objects such as AVRO and Parquet and to evaluate dynamic mapping capabilities Implement Data Quality Rules using Informatica Data Quality IDQ to check correctness of the source files and perform the data cleansingenrichment Analyze log records data a day and its aggregated hourly daily reporting using Tableau Environment Hadoop27 Informatica9x Hive121 Spark16 Teradata Oracle EC2 S3 Hadoop Developer Cisco Systems San Jose CA September 2014 to March 2016 Worked with highly unstructured and semi structured data of 100 TB in size Developed Pig and Hive scripts to be used by end user analyst product managers requirements for adhoc analysis Used Informatica to validate and test the business logic implemented in the mappings and fix the bugs Developed reusable Mapplets and Transformations Managed External tables in Hive for optimized performance using Sqoop jobs Solved performance issues in Hive and Pig scripts with understanding of joins group and aggregation and how it translates to MapReduce jobs Exploring with Spark improving the performance and optimization of the existing algorithms in Hadoop using Spark context SparkSQL Data Frame pair RDDs Spark on YARN Worked with HadoopKerberos security environment which is supported by the Cloudera team Environment 32 Node Hadoop 26 cluster Informatica9x HDFS Flume 15 Sqoop 143 Hive 101 Spark 14 HBase XML JSON Teradata Oracle MongoDB Cassandra Hadoop Developer Bayer Healthcare Leverkusen DE November 2013 to August 2014 Migration of 100 TBs of data from different databases ie Oracle SQL Server to Hadoop Wiring code in different applications of Hadoop and Informatica Ecosystem Extensively involved in performance tuning of the Informatica ETL mappings by using the caches and overriding the SQL queries and also by using Parameter files Worked on various file formats Avro SerDe Parquet and Text by using snappy compression Used Pig Custom Loaders to load different forms of data files such as XML JSON and CSV Designed dynamic partition mechanism for optimal query performance of system using HIVE to reduce report time generation under SLA requirements Environment Hadoop 22 Informatica Power Center 9x HDFS HBase Flume 14 Sqoop 143 Hive 0131 Avro 174 Parquet 14 XML JSON Oracle 11 g Amazon EC2 S3 ETL Developer Grattan Plc Bradford February 2010 to October 2013 Developed mappingssessions to import transform and load data into respective target tables and flat files using Informatica Power Center for data loading Automation of the Informatica ETL jobs for different ETL design pattern Extensively used Transformations like Router Aggregator Source Qualifier Joiner Expression Aggregator and Sequence generator by using Source Analyzer Warehouse Designer Mapping Designer Mapplet and Transformation Developer Environment Informatica Power Center 9x Repository Manager Designer Workflow Manager and Workflow Monitor Oracle 11 g SeaQuest HPDM SQL Server Teradata Toad ControlM ETL Developer Star Health and Allied Insurance Company Ltd Bengaluru Karnataka October 2007 to February 2010 Extensively used Slowly Changing Dimensions technique for updating dimensional schema Processed data using various transformations like Aggregator Router Expression Source Qualifier Filter Lookup Joiner Sorter XML Source qualifier and webconsumer for WSDL Used Informatica user defined functions to reduce the code dependency Environment Informatica Power Center   Informatica Power Connect Power Exchange Power Analyzer Toad Erwin Oracle 11g10 g Teradata V2R5 PLSQL ODI Trillium 11 ETL Developer United Overseas bank Singapore October 2005 to September 2007 Used SSIS as an Extract Transform Loading ETL tool of SQL Server to populate data from various data sources creating packages for different data loading operations for application Extensive use of TransactSQL stored procedures trigger scripts for creating database objects Generated various reports using features such as group by drilldowns drill through subreports Parameterized Reports Deploying new strategies for checksum calculations and exception population using mapplets and normalizer transformations Environment SQL Server 2005 TSQL SSISDTS Designer and Reporting tools ControlM Java Developer ExpertNet CAD Pune Maharashtra January 2004 to August 2005 Developed the web applications using Spring MVC Framework including writing actions classes forms custom tag libraries and JSP pages Worked on Integration of Spring and Hibernate Frameworks using Spring ORM Module Implemented caching techniques wrote POJO classes for storing data and DAOs to retrieve the data and did database configurations Java Developer Honeywell Bengaluru Karnataka January 2002 to January 2004 Implementation of routing and shortest path algorithms along with parsing logic for device discovery using HeartBeat Implementation of Java Native InterfaceJNI APIs for Indus Mote to access devices dynamically through C code Education Masters in Data Analytics in Data Analytics Boston University Boston MA",
    "extracted_keywords": [
        "Technology",
        "Lead",
        "Hadoop",
        "Technology",
        "Lead",
        "Hadoop",
        "Technology",
        "Lead",
        "Hadoop",
        "Neiman",
        "Marcus",
        "Dallas",
        "TX",
        "Work",
        "Experience",
        "Technology",
        "Lead",
        "Hadoop",
        "Neiman",
        "Marcus",
        "Dallas",
        "TX",
        "May",
        "Present",
        "AA",
        "customer",
        "data",
        "stream",
        "sales",
        "email",
        "campaigns",
        "UCIDs",
        "personalization",
        "CXP",
        "APIs",
        "data",
        "models",
        "personalization",
        "product",
        "recommendations",
        "Storm",
        "Kafka",
        "Hive",
        "Pig",
        "Processing",
        "PLLC",
        "Private",
        "Label",
        "Credit",
        "Card",
        "Capitol",
        "data",
        "insights",
        "percentage",
        "penetration",
        "sales",
        "associates",
        "stores",
        "scripts",
        "data",
        "migration",
        "enterprise",
        "data",
        "inhouse",
        "infra",
        "AWS",
        "cloud",
        "Environment",
        "Hadoop26chd51340",
        "node",
        "AWS",
        "cloud",
        "Hive121",
        "Storm",
        "Cassandra",
        "Solr",
        "CouchDB",
        "EC2",
        "S3",
        "Airflow",
        "ETLHadoop",
        "Developer",
        "Lutron",
        "Inc",
        "Philadelphia",
        "PA",
        "April",
        "April",
        "Hadoop",
        "Informatica",
        "ETL",
        "system",
        "insights",
        "customers",
        "usage",
        "Lutron",
        "products",
        "product",
        "line",
        "enhancements",
        "improvement",
        "business",
        "services",
        "data",
        "pipeline",
        "Spark",
        "Kafka",
        "Hive",
        "Pig",
        "HBase",
        "customer",
        "system",
        "usage",
        "data",
        "histories",
        "Hadoop",
        "cluster",
        "analysis",
        "Scala",
        "scripts",
        "UDFs",
        "Data",
        "framesSQL",
        "RDDMapReduce",
        "Spark",
        "data",
        "aggregation",
        "data",
        "S3",
        "Sqoop",
        "Informatica",
        "data",
        "ingestion",
        "jobs",
        "HDFS",
        "data",
        "file",
        "objects",
        "AVRO",
        "Parquet",
        "mapping",
        "capabilities",
        "Implement",
        "Data",
        "Quality",
        "Rules",
        "Informatica",
        "Data",
        "Quality",
        "IDQ",
        "correctness",
        "source",
        "files",
        "data",
        "cleansingenrichment",
        "Analyze",
        "log",
        "records",
        "data",
        "day",
        "reporting",
        "Tableau",
        "Environment",
        "Hadoop27",
        "Informatica9x",
        "Hive121",
        "Spark16",
        "Teradata",
        "Oracle",
        "EC2",
        "S3",
        "Hadoop",
        "Developer",
        "Cisco",
        "Systems",
        "San",
        "Jose",
        "CA",
        "September",
        "March",
        "data",
        "TB",
        "size",
        "Developed",
        "Pig",
        "Hive",
        "scripts",
        "end",
        "user",
        "analyst",
        "product",
        "managers",
        "requirements",
        "analysis",
        "Informatica",
        "business",
        "logic",
        "mappings",
        "bugs",
        "Mapplets",
        "Transformations",
        "tables",
        "Hive",
        "performance",
        "Sqoop",
        "jobs",
        "performance",
        "issues",
        "Hive",
        "Pig",
        "scripts",
        "understanding",
        "joins",
        "group",
        "aggregation",
        "MapReduce",
        "jobs",
        "Spark",
        "performance",
        "optimization",
        "algorithms",
        "Hadoop",
        "Spark",
        "context",
        "SparkSQL",
        "Data",
        "Frame",
        "pair",
        "Spark",
        "YARN",
        "HadoopKerberos",
        "security",
        "environment",
        "Cloudera",
        "team",
        "Environment",
        "Node",
        "Hadoop",
        "cluster",
        "Informatica9x",
        "HDFS",
        "Flume",
        "Sqoop",
        "143",
        "Hive",
        "Spark",
        "HBase",
        "XML",
        "JSON",
        "Teradata",
        "Oracle",
        "MongoDB",
        "Cassandra",
        "Hadoop",
        "Developer",
        "Bayer",
        "Healthcare",
        "Leverkusen",
        "DE",
        "November",
        "August",
        "Migration",
        "TBs",
        "data",
        "databases",
        "Oracle",
        "SQL",
        "Server",
        "Hadoop",
        "Wiring",
        "code",
        "applications",
        "Hadoop",
        "Informatica",
        "Ecosystem",
        "performance",
        "tuning",
        "Informatica",
        "ETL",
        "mappings",
        "caches",
        "SQL",
        "queries",
        "Parameter",
        "files",
        "file",
        "formats",
        "Avro",
        "SerDe",
        "Parquet",
        "Text",
        "compression",
        "Pig",
        "Custom",
        "Loaders",
        "forms",
        "data",
        "files",
        "XML",
        "JSON",
        "CSV",
        "partition",
        "mechanism",
        "query",
        "performance",
        "system",
        "HIVE",
        "report",
        "time",
        "generation",
        "SLA",
        "requirements",
        "Environment",
        "Hadoop",
        "Informatica",
        "Power",
        "Center",
        "9x",
        "HDFS",
        "HBase",
        "Flume",
        "Sqoop",
        "143",
        "Hive",
        "Avro",
        "Parquet",
        "XML",
        "JSON",
        "Oracle",
        "g",
        "Amazon",
        "EC2",
        "S3",
        "ETL",
        "Developer",
        "Grattan",
        "Plc",
        "Bradford",
        "February",
        "October",
        "mappingssessions",
        "transform",
        "load",
        "data",
        "target",
        "tables",
        "files",
        "Informatica",
        "Power",
        "Center",
        "data",
        "Automation",
        "Informatica",
        "ETL",
        "jobs",
        "ETL",
        "design",
        "pattern",
        "Transformations",
        "Router",
        "Aggregator",
        "Source",
        "Qualifier",
        "Joiner",
        "Expression",
        "Aggregator",
        "Sequence",
        "generator",
        "Source",
        "Analyzer",
        "Warehouse",
        "Designer",
        "Mapping",
        "Designer",
        "Mapplet",
        "Transformation",
        "Developer",
        "Environment",
        "Informatica",
        "Power",
        "Center",
        "9x",
        "Repository",
        "Manager",
        "Designer",
        "Workflow",
        "Manager",
        "Workflow",
        "Monitor",
        "Oracle",
        "g",
        "SeaQuest",
        "HPDM",
        "SQL",
        "Server",
        "Teradata",
        "Toad",
        "ETL",
        "Developer",
        "Star",
        "Health",
        "Allied",
        "Insurance",
        "Company",
        "Ltd",
        "Bengaluru",
        "Karnataka",
        "October",
        "February",
        "Dimensions",
        "technique",
        "schema",
        "data",
        "transformations",
        "Aggregator",
        "Router",
        "Expression",
        "Source",
        "Qualifier",
        "Filter",
        "Lookup",
        "Joiner",
        "Sorter",
        "XML",
        "Source",
        "qualifier",
        "webconsumer",
        "WSDL",
        "Informatica",
        "user",
        "functions",
        "code",
        "dependency",
        "Environment",
        "Informatica",
        "Power",
        "Center",
        "Informatica",
        "Power",
        "Connect",
        "Power",
        "Exchange",
        "Power",
        "Analyzer",
        "Toad",
        "Erwin",
        "Oracle",
        "g",
        "Teradata",
        "V2R5",
        "PLSQL",
        "ODI",
        "Trillium",
        "ETL",
        "Developer",
        "United",
        "Overseas",
        "bank",
        "Singapore",
        "October",
        "September",
        "SSIS",
        "Extract",
        "Transform",
        "Loading",
        "ETL",
        "tool",
        "SQL",
        "Server",
        "data",
        "data",
        "sources",
        "packages",
        "data",
        "loading",
        "operations",
        "application",
        "use",
        "TransactSQL",
        "procedures",
        "trigger",
        "scripts",
        "database",
        "objects",
        "reports",
        "features",
        "group",
        "drilldowns",
        "subreports",
        "Parameterized",
        "Reports",
        "strategies",
        "checksum",
        "calculations",
        "exception",
        "population",
        "mapplets",
        "normalizer",
        "transformations",
        "Environment",
        "SQL",
        "Server",
        "TSQL",
        "SSISDTS",
        "Designer",
        "Reporting",
        "tools",
        "ControlM",
        "Java",
        "Developer",
        "ExpertNet",
        "CAD",
        "Pune",
        "Maharashtra",
        "January",
        "August",
        "web",
        "applications",
        "Spring",
        "MVC",
        "Framework",
        "actions",
        "classes",
        "custom",
        "tag",
        "libraries",
        "JSP",
        "pages",
        "Integration",
        "Spring",
        "Hibernate",
        "Frameworks",
        "Spring",
        "ORM",
        "Module",
        "techniques",
        "POJO",
        "classes",
        "data",
        "DAOs",
        "data",
        "database",
        "configurations",
        "Java",
        "Developer",
        "Honeywell",
        "Bengaluru",
        "Karnataka",
        "January",
        "January",
        "Implementation",
        "routing",
        "path",
        "logic",
        "device",
        "discovery",
        "HeartBeat",
        "Implementation",
        "Java",
        "Native",
        "InterfaceJNI",
        "APIs",
        "Indus",
        "Mote",
        "devices",
        "C",
        "code",
        "Education",
        "Masters",
        "Data",
        "Analytics",
        "Data",
        "Analytics",
        "Boston",
        "University",
        "Boston",
        "MA"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T22:58:30.195489",
    "resume_data": "Technology Lead Hadoop Technology Lead Hadoop Technology Lead Hadoop Neiman Marcus Dallas TX Work Experience Technology Lead Hadoop Neiman Marcus Dallas TX May 2018 to Present AA to store and join customer centric data like click stream sales email campaigns in generating UCIDs and personalization which are consumed by CXP through APIs Developed data models for personalization and product recommendations using Storm Kafka Hive Pig Processing PLLC Private Label Credit Card Capitol One data to provide insights into percentage of penetration of sales by associates and stores Developed scripts for data migration of enterprise data from inhouse infra to AWS cloud Environment Hadoop26chd51340 node AWS cloud Hive121 Storm Cassandra Solr CouchDB EC2 S3 Airflow ETLHadoop Developer Lutron Inc Philadelphia PA April 2016 to April 2018 Hadoop and Informatica based ETL and analytical system to have insights about customers usage of Lutron products across different product line leading to future enhancements improvement in business and services Developed data pipeline using Spark Kafka Hive Pig and HBase to ingest customer system usage data and financial histories into Hadoop cluster for analysis Developed Scala scripts UDFs using both Data framesSQL and RDDMapReduce in Spark for data aggregation and writing data back into S3 through Sqoop Extensively used Informatica to create data ingestion jobs into HDFS using complex data file objects such as AVRO and Parquet and to evaluate dynamic mapping capabilities Implement Data Quality Rules using Informatica Data Quality IDQ to check correctness of the source files and perform the data cleansingenrichment Analyze log records data a day and its aggregated hourly daily reporting using Tableau Environment Hadoop27 Informatica9x Hive121 Spark16 Teradata Oracle EC2 S3 Hadoop Developer Cisco Systems San Jose CA September 2014 to March 2016 Worked with highly unstructured and semi structured data of 100TB in size Developed Pig and Hive scripts to be used by end user analyst product managers requirements for adhoc analysis Used Informatica to validate and test the business logic implemented in the mappings and fix the bugs Developed reusable Mapplets and Transformations Managed External tables in Hive for optimized performance using Sqoop jobs Solved performance issues in Hive and Pig scripts with understanding of joins group and aggregation and how it translates to MapReduce jobs Exploring with Spark improving the performance and optimization of the existing algorithms in Hadoop using Spark context SparkSQL Data Frame pair RDDs Spark on YARN Worked with HadoopKerberos security environment which is supported by the Cloudera team Environment 32 Node Hadoop 26 cluster Informatica9x HDFS Flume 15 Sqoop 143 Hive 101 Spark 14 HBase XML JSON Teradata Oracle MongoDB Cassandra Hadoop Developer Bayer Healthcare Leverkusen DE November 2013 to August 2014 Migration of 100 TBs of data from different databases ie Oracle SQL Server to Hadoop Wiring code in different applications of Hadoop and Informatica Ecosystem Extensively involved in performance tuning of the Informatica ETL mappings by using the caches and overriding the SQL queries and also by using Parameter files Worked on various file formats Avro SerDe Parquet and Text by using snappy compression Used Pig Custom Loaders to load different forms of data files such as XML JSON and CSV Designed dynamic partition mechanism for optimal query performance of system using HIVE to reduce report time generation under SLA requirements Environment Hadoop 22 Informatica Power Center 9x HDFS HBase Flume 14 Sqoop 143 Hive 0131 Avro 174 Parquet 14 XML JSON Oracle 11g Amazon EC2 S3 ETL Developer Grattan Plc Bradford February 2010 to October 2013 Developed mappingssessions to import transform and load data into respective target tables and flat files using Informatica Power Center for data loading Automation of the Informatica ETL jobs for different ETL design pattern Extensively used Transformations like Router Aggregator Source Qualifier Joiner Expression Aggregator and Sequence generator by using Source Analyzer Warehouse Designer Mapping Designer Mapplet and Transformation Developer Environment Informatica Power Center 9x Repository Manager Designer Workflow Manager and Workflow Monitor Oracle 11g SeaQuest HPDM SQL Server Teradata Toad ControlM ETL Developer Star Health and Allied Insurance Company Ltd Bengaluru Karnataka October 2007 to February 2010 Extensively used Slowly Changing Dimensions technique for updating dimensional schema Processed data using various transformations like Aggregator Router Expression Source Qualifier Filter Lookup Joiner Sorter XML Source qualifier and webconsumer for WSDL Used Informatica user defined functions to reduce the code dependency Environment Informatica Power Center 8x Informatica Power Connect Power Exchange Power Analyzer Toad Erwin Oracle 11g10g Teradata V2R5 PLSQL ODI Trillium 11 ETL Developer United Overseas bank Singapore October 2005 to September 2007 Used SSIS as an Extract Transform Loading ETL tool of SQL Server to populate data from various data sources creating packages for different data loading operations for application Extensive use of TransactSQL stored procedures trigger scripts for creating database objects Generated various reports using features such as group by drilldowns drill through subreports Parameterized Reports Deploying new strategies for checksum calculations and exception population using mapplets and normalizer transformations Environment SQL Server 2005 TSQL SSISDTS Designer and Reporting tools ControlM Java Developer ExpertNet CAD Pune Maharashtra January 2004 to August 2005 Developed the web applications using Spring MVC Framework including writing actions classes forms custom tag libraries and JSP pages Worked on Integration of Spring and Hibernate Frameworks using Spring ORM Module Implemented caching techniques wrote POJO classes for storing data and DAOs to retrieve the data and did database configurations Java Developer Honeywell Bengaluru Karnataka January 2002 to January 2004 Implementation of routing and shortest path algorithms along with parsing logic for device discovery using HeartBeat Implementation of Java Native InterfaceJNI APIs for Indus Mote to access devices dynamically through C code Education Masters in Data Analytics in Data Analytics Boston University Boston MA",
    "unique_id": "1e202d53-b702-467b-953d-ebb7d1df8432"
}