{
    "clean_data": "SrHadoop Developer SrHadoop span lDeveloperspan SrHadoop Developer Rsvp hospitality New York NY Having 8years of overall experience in IT industry includes implementing developing and maintenance in Big Data Technologies and Web based applications using Java J2EE technologies Hands on experience in implementing various solutions and analysing data using Hadoop Ecosystems like HDFS Map Reduce Yarn Spark Sqoop Hive Pig Flume Kafka Impala Oozie Oozie coordinator Zookeeper and Cassandra HBase Hands on experience in various Hadoop distributions like Cloudera CDH3 CDH4 CDH5 Horton works Data Platform HDP and MapR Experience in performing inmemory data processing and real time streaming analytics using Apache Spark with Scala Java and Python Experience in Sqoop to import data from various external sources into Hadoop ecosystem components like HDFS HBase and Hive as well as exports data from Hadoop to other external sources Experience on Spark Streaming API with Kafka to feed live streaming data into HDFS and optimized it using spark concepts like Data frames Partitioning Bucketing Parallel execution and Map Side Joins using broadcast joins Experience in advanced procedures like text analytics processing using in memory computing capabilities like Spark written in Scala Experience in SparkSQL to process large amount of data by implementing SparkRDD transformations Actions and Data frames to required input data Experience on data analysis and developing scripts for pig Latin and Hive QLusing Java Experience in using Ambari for provisioning managing monitoring and securing apache Hadoop cluster Experience in analyzing data from Cassandra for quick processing sorting and grouping through CQL Experience on Managing and scheduling Jobs in Hadoop Cluster using Oozie and used Zookeeper for cluster coordination services Experience in configuring different topologies in Storm to import and process information only from multiple sources collect into central repository Hadoop Experience in developing and designing POCs using Scala and deployed on the Yarn cluster compared the performance of Spark with Hive and SQLTeradata Good Knowledge in Amazon AWS concepts like EMR EC2 EBS S3 and RDS web services which provides fast and efficient processing of Big Data Extensive experience with SQL PLSQL Shell Scripting and database concepts Experience with front end technologies like HTML CSS and JavaScript Experience in working with Windows UNIXLINUX platform with different technologies such as Big Data SQL XML HTML Core Java Shell Scripting etc Good working experience on different file formats like Json Avro Parquet compression techniques like snappy bzip Hands on experience in data warehousing with ETL tools like Informatica Ab initio IBM Data Stage for various loading and transformation processes Good working experience and knowledge in NOSQL databases like HBase Cassandra Mongo DB Couch DB Intensive working experience with Amazon Web ServicesAWSusing S3 for storage EC2 for computing and RDS EBS Experience in NIFI work flow scheduler managing Hadoop jobs by Direct Acyclic graph DAG of actions with control flows Experience in using IDEs and source control repositories like Eclipse IntelliJ and GitHub Maven SBT Experience in preparing Tableau reports for analyzed data by using excel sheets flat files CSV files Experience in different data sources like Oracle Netezza SQL and PostgreSQL Experience in J2EE technologies like Java servlets JSP EJB and JDBC etc Work Experience SrHadoop Developer Rsvp hospitality New York NY March 2018 to Present Description RSVP Hospitality is a UAE consulting firm specialized in Revenue Management and Business Analytics for Hotels SpaSalons and Service Industry projects RSVP Hospitality provide our clients extensive expertise in different sectors in hospitality leisure and wellness industries Responsibilities Contributing as a member of a high performing the agile team focused on nextgeneration data analytics Build Big Data Analytics and Visualization platform for handling highvolume batchoriented and realtime data streams Utilized Agile Scrum Methodology to help manage and organize a team with regular code review sessions Used Pig as ETL tool to do transformations event joins filter both traffic and some preaggregations before storing the data onto HDFS Experienced on loading and transforming of large sets of structured semi structured and optimizing of existing algorithms in Hadoop using Spark Context HiveSQL Data Frames Analysed different big data analytics using Hive import data from RDBMS to HDFS Implemented complex big data with a focus on collecting parsing managing analyzing and visualizing large sets of data to turn information into business insights using multiple platforms in the Hadoopecosystem Developed workflow in Oozie to automate the tasks of loading the data into HDFS and preprocessing with Pig Imported data from the structured data source into HDFS using Sqoop incremental imports Created Hive tables partitions and implemented incremental imports to perform adhoc queries on structured data Build Hive tables using list partitioning and hash partitioning and created Hive Generic UDFs to process business logic with HiveQL Integrated HBase with MapReduce to move the bulk amount of data into HBase Developed SQL scripts using Spark for handling different data sets and verifying the performance over Map Reduce jobs Supported MapReduce Programs those are running on the cluster and Wrote MapReduce jobs using JavaAPI Extensively used Apache Sqoop for efficiently transferring bulk data between Apache Hadoop and relational databases Oracle MySQL for predictive analytics Developed storytelling dashboards in Tableau Desktop and published them on to TableauServer and used GitHub version controlling tools to maintain project versions EnvironmentHadoop Java MapReduce HDFS Hive Linux XML Eclipse Cloudera Spark HBase MongoDB Python GitHub Sqoop Oozie DB2 SQL Server Oracle 12c MySQL Hadoop Developer Neurotrack San Francisco CA June 2016 to February 2018 Description Neurotrack is dedicated to the development of noninvasive cognitive health assessment tools that will enable earlier and more effective evaluation of patients who may be at risk for cognitive decline and help advance research of treatments for cognitive diseases including Alzheimers Neurotracks mission is to revolutionize the early diagnosis and treatment of Alzheimers disease with our online assessment and digital therapy Responsibilities Analysing the requirement to setup a cluster Worked on analyzing Hadoop cluster and different big data analytic tools including Map Reduce Hive and Spark Involved in loading data from LINUX file system servers Java web services using Kafka Producers partitions Implemented Kafka Custom encoders for custom input format to load data into Kafka Partitions Implemented Storm topologies to preprocess data before moving into HDFS system Implemented Kafka High level consumers to get data from Kafka partitions and move into HDFS Implemented POC to migrate Map Reduce programs into Spark transformations using Spark and Scala Migrated complex Map Reduce programs into Spark RDD transformations actions Implemented Spark RDD transformations to map business analysis and apply actions on top of transformations Involved in creating Hive tables loading with data and writing hive queries which runs internally in Map Reduce way Developed the Map Reduce programs to parse the raw data and store the pre Aggregated data in the partitioned tables Loaded and transformed large sets of structured semi structured and unstructured data with Map Reduce Hive and pig Developed Map Reduce programs in Java for parsing the raw data and populating staging Tables Experienced in developing custom input formats and data types to parse and process unstructured and semi structured input data and mapped them into key value pairs to implement business logic in Map Reduce Involved in using HCATALOG to access Hive table metadata for Map Reduce or Pig code Experience in implementing custom sterilizer interceptor source and sink as per the requirement in flume to ingest data from multiple sources Experience in setting up Fanout workflow in flume to design vshaped architecture to take data from many sources and ingest into single sink Developed Shell Perl and Python scripts to automate and provide Control flow to Pig scripts Exporting of result set from HIVE to MySQL using Sqoop export tool for further processing Evaluated usage of Oozie for Workflow Orchestration Converted unstructured data to structured data by writing Spark code Indexed documents using Apache Solr Set up Solr Clouds for distributing indexing and search Automation of all the jobs starting from pulling the Data from different Data Sources like MySQL and pushing the result dataset to Hadoop Distributed File System and running MR PIG and Hive jobs using Kettle and Oozie Work Flow management Worked on NoSQL databases like Cassandra Mongo DB for POC purpose in storing images and URIs Integrating bulk data into Cassandra file system using Map Reduce programs Used Talend ETL tool to develop multiple jobs and in setting workflows Created Talend jobs to copy the files from one server to another and utilized Talend FTP components Worked on Mongo DB for distributed storage and processing Designed and implemented Cassandra and associated Restful web service Implemented Row Level Updates and Real time analytics using CQL on Cassandra Data Used Cassandra CQL with Java APIs to retrieve data from Cassandra tables Worked on analysing and examining customer behavioural data using Cassandra Created partitioned tables in Hive mentored analyst and SQA team for writing Hive Queries Developed Pig Latin scripts to extract the data from the web server output files to load into HDFS Involved in cluster setup monitoring test benchmarks for results Involved in builddeploy applications using Maven and integrated with CICD server Jenkins Involved in agile methodologies daily scrum meetings spring plannings Environment Hadoop Cloudera HDFS pig0 Hive Flume Sqoop Oozie AWS Redshift9 Python Spark Scala Mongo DB Cassandra Solr ZooKeeper MySQL Talend Shell Scripting Linux Red Hat Java Hadoop Developer ATT Dallas TX February 2015 to May 2016 Description ATT Inc is an American multinational telecommunications holding company headquartered at Whitacre Tower in Downtown Dallas Texas It is the worlds largest telecommunications company the second largest provider of mobile telephone services The purpose of this project was to eliminate the need for various regional databases to streamline processes and reduce the address fallouts This provides consistent data for various stakeholders while having access to realtime data Responsibilities Apart from the normal requirement gathering participated in a Business meeting with the client to gather security requirements Assisted with the architect to analyze the existing system and future system Prepared design blue pints and application flow documentation Experienced in managing and reviewing Hadoop log files Load and transform large sets of structured semistructured and unstructured data Responsible to manage data coming from different sources and application Supported Map Reduce Programs those are running on the cluster Created MapReduce jobs to extracts the contents from HBase and configured in OOZIE workflow to generate analytical reports Extracted files from Cassandra through Sqoop and placed in HDFS and processed Implemented Bloom filters in Cassandra using keyspace creation Involved in writing Cassandra CQL statements God handson experience in developing concurrency using spark and Cassandra together Involved in writing spark applications using Scala Hands on experience in creating RDDs transformationsand Actions while implementing spark applications Good knowledge in creating data frames using Spark SQL Involved in loading data into Cassandra NoSQL Database Documented all the challenges issues involved to deal with the security system and Implemented best practices Created Project structures and configurations according to the project architecture and made it available to the junior developer to continue their work Environment Java Python Cassandra HTML5 CSS PIG HIVE Hortonworks distribution of Hadoop 23 YARN Ambari Hadoop developer Standard Poors New York NY March 2013 to January 2015 Description Standard Poors Financial Services LLC SP is an American financial services company It is a division of SP Global that publishes financial research and analysis on stocks bonds and commodities SP is known for its stock market indices such as the USbased SP 500 the Canadian SPTSX and the Australian SPASX 200 Responsibilities Worked on analyzing Hadoop stack and different big data analytic tools including Pig Hive HBase database and Sqoop Experienced to implement Hortonworks distribution system HDP 21 HDP 22 and HDP 23 Developed Map Reduce programs for some refined queries on big data Developed interactive shell scripts for scheduling various data cleansing and data loading process Experienced in managing and reviewing the Hadooplog files and importing log files from various sources into HDFS using Flume Used HIVE queries to import data into MicrosoftAzure cloud and analyzed the data using HIVE scripts Creating Hive tables and working on them for data analysis to cope up with the requirements Developed a framework to handle loading and transform large sets of unstructured data from the UNIX system to HIVE tables Worked with a business team in creating Hive queries for ad hoc access Indepth understanding of Classic MapReduce and YARN architectures Implemented Hive Generic UDFs to implement business logic Used Hive to analyse the partitioned and bucketed data and compute various metrics for reporting Analysed the data by performing Hive queries ran Pig scripts SparkSQL and Spark Streaming Extracted files from Cassandra through Sqoop and placed in HDFS for further processing Involved in creating generic Sqoop import script for loading data into Hive tables from RDBMS Environment Horton works Hadoop Map Reduce HDFS Hive Pig Sqoop AZURE Oozie SQL Spark HBase Cassandra and GitHub Java Developer Comcast Philadelphia PA December 2011 to February 2013 Description Comcast NBCUniversal creates incredible technology and entertainment that connects millions of people to the moments and experiences that matter most Responsibilities Involved in each phase of Software Development Life CycleSDLC models like Requirement gathering and analysis Design Implementation Testing Deployment and Maintenance Developed Login Policy and Claims Screens for customers using HTML 5 CSS3 JavaScript AJAX JSP and jQuery Used Core Java to develop Business Logic Involved in the development of business module applications using J2EE technologies like Servlets JSP Designed and developed the webtier using JSPs Servlets framework Used various Core Java concepts such as MultiThreading Exception Handling Collection APIs to implement various features and enhancements Strong experience in design development of applications using JavaJ2EE components such as Java Server Pages JSP Developed EJB MDBs and message Queues using JMS technology EJB Session Beans were used to process requests from the user interface and CMP entity beans were used to interact with the persistence layer Developed stored procedures triggers and queries using PLSQL in SQL Server Use Spring MVC as framework and JavaScript for clientside view used frameworks for clientside data validation creating dynamic web pagesAjax jQuery Developed model classes based on the forms to be displayed on the UI Implemented various design patterns in the project such as Business Delegate Data Transfer Object Data Access Object Service Locator and Singleton Used SQL statements and procedures to fetch the data from the database Developed test cases and performed unit test using JUnit Framework Used CVS as version control and ANT scripts to fetch build and deploy application to development environment Environment Java HTML CSS JavaScript MySQL Struts EJB Spring MVC Java Developer Unisys Blue Bell PA April 2010 to November 2011 Description Unisys Corporation is an American global information technology company that provides IT services software and technology It is the legacy proprietor of the Burroughs and UNIVAC line of computers formed when the former bought the latter Responsibilities Performed Analysis Design Development Integration and Testing of application modules Implemented application prototype using JSP Servlets JDBC and to give the presentation Developed new web page designs and development of the project presentation layer by using HTML JavaScript JSF Ajax and implemented CSS for User Interface and better appearance Implemented data base queries by using SQL and PLSQL to perform data analysis extraction and various functions Implemented an application by using Struts Framework which leverages classical Model View Layer ArchitectureMVC Involved in fixing bugs and unit testing with test cases using Junit Worked with various software development methodologies like Agile Waterfall to increase the development of the project Environment Java HTML CSS JavaScript JSON JSP JDBC and SQL PLSQL Education Bachelors Skills Cassandra Hdfs Impala Oozie Sqoop CertificationsLicenses Drivers License",
    "entities": [
        "Implemented Spark",
        "Oozie Work Flow",
        "UI Implemented",
        "Burroughs",
        "Spark Context",
        "SQA",
        "New York",
        "Oracle MySQL",
        "Description ATT Inc",
        "Cassandra",
        "SP Global",
        "Implemented Hive Generic",
        "Big Data Extensive",
        "UNIX",
        "Oozie Sqoop CertificationsLicenses",
        "GitHub Maven SBT Experience",
        "JMS",
        "Apache Sqoop",
        "Data Sources",
        "Oozie for Workflow Orchestration Converted",
        "HDFS Experienced",
        "Data Frames Analysed",
        "jQuery Used Core Java",
        "CVS",
        "Indepth",
        "RDD",
        "Hadoop",
        "Spark code Indexed",
        "HDFS Involved",
        "Kafka Partitions Implemented Storm",
        "NOSQL",
        "Responsibilities Contributing",
        "SQL PLSQL Education Bachelors Skills Cassandra Hdfs",
        "Hadoop Distributed File System",
        "HBase",
        "Spark Streaming Extracted",
        "Apache Spark",
        "TX",
        "Amazon",
        "Spark Streaming API",
        "Oozie SQL Spark",
        "JavaJ2EE",
        "CDH3",
        "Kettle",
        "Hadoop Ecosystems",
        "Created Project",
        "SQL Server",
        "SparkSQL",
        "Developed",
        "Cassandra CQL",
        "JSP Servlets JDBC",
        "Dallas",
        "Jenkins Involved",
        "UAE",
        "NIFI",
        "Responsibilities Involved",
        "Informatica Ab",
        "Oracle Netezza SQL",
        "Sqoop Experienced",
        "jQuery Developed",
        "UNIVAC",
        "JSP",
        "Tableau Desktop",
        "Description Comcast NBCUniversal",
        "Worked",
        "Design Implementation Testing Deployment and Maintenance Developed Login Policy",
        "Hive Queries Developed",
        "RDS",
        "HDP",
        "Created MapReduce",
        "HDFS HBase",
        "MVC",
        "Spark",
        "EJB",
        "Created Hive",
        "CSV",
        "HTML CSS",
        "Sqoop",
        "HIVE",
        "LINUX",
        "Storm",
        "Spark SQL Involved",
        "Description Unisys Corporation",
        "SQL PLSQL Shell Scripting",
        "PIG",
        "Spark Involved",
        "Description Standard Poors Financial Services",
        "Business Logic Involved",
        "Responsibilities Performed Analysis Design Development Integration",
        "Oozie",
        "Software Development Life",
        "Created Talend",
        "SQL",
        "GitHub",
        "Spark RDD",
        "Revenue Management and Business Analytics for Hotels SpaSalons and Service Industry",
        "MicrosoftAzure",
        "Big Data",
        "Hive",
        "CICD",
        "Ambari Hadoop",
        "Amazon AWS",
        "HBase Cassandra Mongo DB",
        "DAG",
        "Oozie Oozie",
        "OOZIE",
        "Kafka Producers",
        "Queues",
        "MapR",
        "Supported MapReduce Programs",
        "ETL",
        "Hadooplog",
        "SP",
        "CMP",
        "Cassandra Created",
        "Apache Hadoop",
        "Maven",
        "Standard Poors New York",
        "Worked on Mongo DB",
        "Impala",
        "Windows UNIXLINUX",
        "JavaScript",
        "ANT",
        "CQL",
        "MultiThreading Exception Handling Collection",
        "Texas",
        "IBM Data Stage",
        "Business Delegate Data Transfer Object Data Access Object Service Locator",
        "Big Data Technologies",
        "HiveQL Integrated HBase",
        "ATT",
        "Direct Acyclic",
        "Actions",
        "Data",
        "Implemented Row Level Updates",
        "MapReduce",
        "NoSQL",
        "Tableau",
        "GitHub Sqoop Oozie",
        "Build Big Data Analytics",
        "HBase Developed",
        "Cloudera"
    ],
    "experience": "Experience in performing inmemory data processing and real time streaming analytics using Apache Spark with Scala Java and Python Experience in Sqoop to import data from various external sources into Hadoop ecosystem components like HDFS HBase and Hive as well as exports data from Hadoop to other external sources Experience on Spark Streaming API with Kafka to feed live streaming data into HDFS and optimized it using spark concepts like Data frames Partitioning Bucketing Parallel execution and Map Side Joins using broadcast joins Experience in advanced procedures like text analytics processing using in memory computing capabilities like Spark written in Scala Experience in SparkSQL to process large amount of data by implementing SparkRDD transformations Actions and Data frames to required input data Experience on data analysis and developing scripts for pig Latin and Hive QLusing Java Experience in using Ambari for provisioning managing monitoring and securing apache Hadoop cluster Experience in analyzing data from Cassandra for quick processing sorting and grouping through CQL Experience on Managing and scheduling Jobs in Hadoop Cluster using Oozie and used Zookeeper for cluster coordination services Experience in configuring different topologies in Storm to import and process information only from multiple sources collect into central repository Hadoop Experience in developing and designing POCs using Scala and deployed on the Yarn cluster compared the performance of Spark with Hive and SQLTeradata Good Knowledge in Amazon AWS concepts like EMR EC2 EBS S3 and RDS web services which provides fast and efficient processing of Big Data Extensive experience with SQL PLSQL Shell Scripting and database concepts Experience with front end technologies like HTML CSS and JavaScript Experience in working with Windows UNIXLINUX platform with different technologies such as Big Data SQL XML HTML Core Java Shell Scripting etc Good working experience on different file formats like Json Avro Parquet compression techniques like snappy bzip Hands on experience in data warehousing with ETL tools like Informatica Ab initio IBM Data Stage for various loading and transformation processes Good working experience and knowledge in NOSQL databases like HBase Cassandra Mongo DB Couch DB Intensive working experience with Amazon Web ServicesAWSusing S3 for storage EC2 for computing and RDS EBS Experience in NIFI work flow scheduler managing Hadoop jobs by Direct Acyclic graph DAG of actions with control flows Experience in using IDEs and source control repositories like Eclipse IntelliJ and GitHub Maven SBT Experience in preparing Tableau reports for analyzed data by using excel sheets flat files CSV files Experience in different data sources like Oracle Netezza SQL and PostgreSQL Experience in J2EE technologies like Java servlets JSP EJB and JDBC etc Work Experience SrHadoop Developer Rsvp hospitality New York NY March 2018 to Present Description RSVP Hospitality is a UAE consulting firm specialized in Revenue Management and Business Analytics for Hotels SpaSalons and Service Industry projects RSVP Hospitality provide our clients extensive expertise in different sectors in hospitality leisure and wellness industries Responsibilities Contributing as a member of a high performing the agile team focused on nextgeneration data analytics Build Big Data Analytics and Visualization platform for handling highvolume batchoriented and realtime data streams Utilized Agile Scrum Methodology to help manage and organize a team with regular code review sessions Used Pig as ETL tool to do transformations event joins filter both traffic and some preaggregations before storing the data onto HDFS Experienced on loading and transforming of large sets of structured semi structured and optimizing of existing algorithms in Hadoop using Spark Context HiveSQL Data Frames Analysed different big data analytics using Hive import data from RDBMS to HDFS Implemented complex big data with a focus on collecting parsing managing analyzing and visualizing large sets of data to turn information into business insights using multiple platforms in the Hadoopecosystem Developed workflow in Oozie to automate the tasks of loading the data into HDFS and preprocessing with Pig Imported data from the structured data source into HDFS using Sqoop incremental imports Created Hive tables partitions and implemented incremental imports to perform adhoc queries on structured data Build Hive tables using list partitioning and hash partitioning and created Hive Generic UDFs to process business logic with HiveQL Integrated HBase with MapReduce to move the bulk amount of data into HBase Developed SQL scripts using Spark for handling different data sets and verifying the performance over Map Reduce jobs Supported MapReduce Programs those are running on the cluster and Wrote MapReduce jobs using JavaAPI Extensively used Apache Sqoop for efficiently transferring bulk data between Apache Hadoop and relational databases Oracle MySQL for predictive analytics Developed storytelling dashboards in Tableau Desktop and published them on to TableauServer and used GitHub version controlling tools to maintain project versions EnvironmentHadoop Java MapReduce HDFS Hive Linux XML Eclipse Cloudera Spark HBase MongoDB Python GitHub Sqoop Oozie DB2 SQL Server Oracle 12c MySQL Hadoop Developer Neurotrack San Francisco CA June 2016 to February 2018 Description Neurotrack is dedicated to the development of noninvasive cognitive health assessment tools that will enable earlier and more effective evaluation of patients who may be at risk for cognitive decline and help advance research of treatments for cognitive diseases including Alzheimers Neurotracks mission is to revolutionize the early diagnosis and treatment of Alzheimers disease with our online assessment and digital therapy Responsibilities Analysing the requirement to setup a cluster Worked on analyzing Hadoop cluster and different big data analytic tools including Map Reduce Hive and Spark Involved in loading data from LINUX file system servers Java web services using Kafka Producers partitions Implemented Kafka Custom encoders for custom input format to load data into Kafka Partitions Implemented Storm topologies to preprocess data before moving into HDFS system Implemented Kafka High level consumers to get data from Kafka partitions and move into HDFS Implemented POC to migrate Map Reduce programs into Spark transformations using Spark and Scala Migrated complex Map Reduce programs into Spark RDD transformations actions Implemented Spark RDD transformations to map business analysis and apply actions on top of transformations Involved in creating Hive tables loading with data and writing hive queries which runs internally in Map Reduce way Developed the Map Reduce programs to parse the raw data and store the pre Aggregated data in the partitioned tables Loaded and transformed large sets of structured semi structured and unstructured data with Map Reduce Hive and pig Developed Map Reduce programs in Java for parsing the raw data and populating staging Tables Experienced in developing custom input formats and data types to parse and process unstructured and semi structured input data and mapped them into key value pairs to implement business logic in Map Reduce Involved in using HCATALOG to access Hive table metadata for Map Reduce or Pig code Experience in implementing custom sterilizer interceptor source and sink as per the requirement in flume to ingest data from multiple sources Experience in setting up Fanout workflow in flume to design vshaped architecture to take data from many sources and ingest into single sink Developed Shell Perl and Python scripts to automate and provide Control flow to Pig scripts Exporting of result set from HIVE to MySQL using Sqoop export tool for further processing Evaluated usage of Oozie for Workflow Orchestration Converted unstructured data to structured data by writing Spark code Indexed documents using Apache Solr Set up Solr Clouds for distributing indexing and search Automation of all the jobs starting from pulling the Data from different Data Sources like MySQL and pushing the result dataset to Hadoop Distributed File System and running MR PIG and Hive jobs using Kettle and Oozie Work Flow management Worked on NoSQL databases like Cassandra Mongo DB for POC purpose in storing images and URIs Integrating bulk data into Cassandra file system using Map Reduce programs Used Talend ETL tool to develop multiple jobs and in setting workflows Created Talend jobs to copy the files from one server to another and utilized Talend FTP components Worked on Mongo DB for distributed storage and processing Designed and implemented Cassandra and associated Restful web service Implemented Row Level Updates and Real time analytics using CQL on Cassandra Data Used Cassandra CQL with Java APIs to retrieve data from Cassandra tables Worked on analysing and examining customer behavioural data using Cassandra Created partitioned tables in Hive mentored analyst and SQA team for writing Hive Queries Developed Pig Latin scripts to extract the data from the web server output files to load into HDFS Involved in cluster setup monitoring test benchmarks for results Involved in builddeploy applications using Maven and integrated with CICD server Jenkins Involved in agile methodologies daily scrum meetings spring plannings Environment Hadoop Cloudera HDFS pig0 Hive Flume Sqoop Oozie AWS Redshift9 Python Spark Scala Mongo DB Cassandra Solr ZooKeeper MySQL Talend Shell Scripting Linux Red Hat Java Hadoop Developer ATT Dallas TX February 2015 to May 2016 Description ATT Inc is an American multinational telecommunications holding company headquartered at Whitacre Tower in Downtown Dallas Texas It is the worlds largest telecommunications company the second largest provider of mobile telephone services The purpose of this project was to eliminate the need for various regional databases to streamline processes and reduce the address fallouts This provides consistent data for various stakeholders while having access to realtime data Responsibilities Apart from the normal requirement gathering participated in a Business meeting with the client to gather security requirements Assisted with the architect to analyze the existing system and future system Prepared design blue pints and application flow documentation Experienced in managing and reviewing Hadoop log files Load and transform large sets of structured semistructured and unstructured data Responsible to manage data coming from different sources and application Supported Map Reduce Programs those are running on the cluster Created MapReduce jobs to extracts the contents from HBase and configured in OOZIE workflow to generate analytical reports Extracted files from Cassandra through Sqoop and placed in HDFS and processed Implemented Bloom filters in Cassandra using keyspace creation Involved in writing Cassandra CQL statements God handson experience in developing concurrency using spark and Cassandra together Involved in writing spark applications using Scala Hands on experience in creating RDDs transformationsand Actions while implementing spark applications Good knowledge in creating data frames using Spark SQL Involved in loading data into Cassandra NoSQL Database Documented all the challenges issues involved to deal with the security system and Implemented best practices Created Project structures and configurations according to the project architecture and made it available to the junior developer to continue their work Environment Java Python Cassandra HTML5 CSS PIG HIVE Hortonworks distribution of Hadoop 23 YARN Ambari Hadoop developer Standard Poors New York NY March 2013 to January 2015 Description Standard Poors Financial Services LLC SP is an American financial services company It is a division of SP Global that publishes financial research and analysis on stocks bonds and commodities SP is known for its stock market indices such as the USbased SP 500 the Canadian SPTSX and the Australian SPASX 200 Responsibilities Worked on analyzing Hadoop stack and different big data analytic tools including Pig Hive HBase database and Sqoop Experienced to implement Hortonworks distribution system HDP 21 HDP 22 and HDP 23 Developed Map Reduce programs for some refined queries on big data Developed interactive shell scripts for scheduling various data cleansing and data loading process Experienced in managing and reviewing the Hadooplog files and importing log files from various sources into HDFS using Flume Used HIVE queries to import data into MicrosoftAzure cloud and analyzed the data using HIVE scripts Creating Hive tables and working on them for data analysis to cope up with the requirements Developed a framework to handle loading and transform large sets of unstructured data from the UNIX system to HIVE tables Worked with a business team in creating Hive queries for ad hoc access Indepth understanding of Classic MapReduce and YARN architectures Implemented Hive Generic UDFs to implement business logic Used Hive to analyse the partitioned and bucketed data and compute various metrics for reporting Analysed the data by performing Hive queries ran Pig scripts SparkSQL and Spark Streaming Extracted files from Cassandra through Sqoop and placed in HDFS for further processing Involved in creating generic Sqoop import script for loading data into Hive tables from RDBMS Environment Horton works Hadoop Map Reduce HDFS Hive Pig Sqoop AZURE Oozie SQL Spark HBase Cassandra and GitHub Java Developer Comcast Philadelphia PA December 2011 to February 2013 Description Comcast NBCUniversal creates incredible technology and entertainment that connects millions of people to the moments and experiences that matter most Responsibilities Involved in each phase of Software Development Life CycleSDLC models like Requirement gathering and analysis Design Implementation Testing Deployment and Maintenance Developed Login Policy and Claims Screens for customers using HTML 5 CSS3 JavaScript AJAX JSP and jQuery Used Core Java to develop Business Logic Involved in the development of business module applications using J2EE technologies like Servlets JSP Designed and developed the webtier using JSPs Servlets framework Used various Core Java concepts such as MultiThreading Exception Handling Collection APIs to implement various features and enhancements Strong experience in design development of applications using JavaJ2EE components such as Java Server Pages JSP Developed EJB MDBs and message Queues using JMS technology EJB Session Beans were used to process requests from the user interface and CMP entity beans were used to interact with the persistence layer Developed stored procedures triggers and queries using PLSQL in SQL Server Use Spring MVC as framework and JavaScript for clientside view used frameworks for clientside data validation creating dynamic web pagesAjax jQuery Developed model classes based on the forms to be displayed on the UI Implemented various design patterns in the project such as Business Delegate Data Transfer Object Data Access Object Service Locator and Singleton Used SQL statements and procedures to fetch the data from the database Developed test cases and performed unit test using JUnit Framework Used CVS as version control and ANT scripts to fetch build and deploy application to development environment Environment Java HTML CSS JavaScript MySQL Struts EJB Spring MVC Java Developer Unisys Blue Bell PA April 2010 to November 2011 Description Unisys Corporation is an American global information technology company that provides IT services software and technology It is the legacy proprietor of the Burroughs and UNIVAC line of computers formed when the former bought the latter Responsibilities Performed Analysis Design Development Integration and Testing of application modules Implemented application prototype using JSP Servlets JDBC and to give the presentation Developed new web page designs and development of the project presentation layer by using HTML JavaScript JSF Ajax and implemented CSS for User Interface and better appearance Implemented data base queries by using SQL and PLSQL to perform data analysis extraction and various functions Implemented an application by using Struts Framework which leverages classical Model View Layer ArchitectureMVC Involved in fixing bugs and unit testing with test cases using Junit Worked with various software development methodologies like Agile Waterfall to increase the development of the project Environment Java HTML CSS JavaScript JSON JSP JDBC and SQL PLSQL Education Bachelors Skills Cassandra Hdfs Impala Oozie Sqoop CertificationsLicenses Drivers License",
    "extracted_keywords": [
        "SrHadoop",
        "Developer",
        "SrHadoop",
        "span",
        "lDeveloperspan",
        "SrHadoop",
        "Developer",
        "Rsvp",
        "hospitality",
        "New",
        "York",
        "NY",
        "8years",
        "experience",
        "IT",
        "industry",
        "maintenance",
        "Big",
        "Data",
        "Technologies",
        "Web",
        "applications",
        "Java",
        "J2EE",
        "Hands",
        "experience",
        "solutions",
        "data",
        "Hadoop",
        "Ecosystems",
        "Map",
        "Reduce",
        "Yarn",
        "Spark",
        "Sqoop",
        "Hive",
        "Pig",
        "Flume",
        "Kafka",
        "Impala",
        "Oozie",
        "Oozie",
        "coordinator",
        "Zookeeper",
        "Cassandra",
        "HBase",
        "Hands",
        "experience",
        "Hadoop",
        "distributions",
        "Cloudera",
        "CDH3",
        "CDH4",
        "CDH5",
        "Horton",
        "Data",
        "Platform",
        "HDP",
        "MapR",
        "Experience",
        "data",
        "processing",
        "time",
        "streaming",
        "analytics",
        "Apache",
        "Spark",
        "Scala",
        "Java",
        "Python",
        "Experience",
        "Sqoop",
        "data",
        "sources",
        "Hadoop",
        "ecosystem",
        "components",
        "HDFS",
        "HBase",
        "Hive",
        "exports",
        "data",
        "Hadoop",
        "sources",
        "Experience",
        "Spark",
        "Streaming",
        "API",
        "Kafka",
        "data",
        "HDFS",
        "spark",
        "concepts",
        "Data",
        "frames",
        "Bucketing",
        "execution",
        "Map",
        "Side",
        "Joins",
        "broadcast",
        "Experience",
        "procedures",
        "text",
        "analytics",
        "processing",
        "memory",
        "computing",
        "capabilities",
        "Spark",
        "Scala",
        "Experience",
        "SparkSQL",
        "amount",
        "data",
        "SparkRDD",
        "transformations",
        "Actions",
        "Data",
        "frames",
        "input",
        "data",
        "Experience",
        "data",
        "analysis",
        "scripts",
        "pig",
        "Latin",
        "Hive",
        "QLusing",
        "Java",
        "Experience",
        "Ambari",
        "monitoring",
        "apache",
        "Hadoop",
        "cluster",
        "Experience",
        "data",
        "Cassandra",
        "processing",
        "sorting",
        "CQL",
        "Experience",
        "Managing",
        "scheduling",
        "Jobs",
        "Hadoop",
        "Cluster",
        "Oozie",
        "Zookeeper",
        "cluster",
        "coordination",
        "services",
        "Experience",
        "topologies",
        "Storm",
        "process",
        "information",
        "sources",
        "repository",
        "Hadoop",
        "Experience",
        "designing",
        "POCs",
        "Scala",
        "Yarn",
        "cluster",
        "performance",
        "Spark",
        "Hive",
        "SQLTeradata",
        "Good",
        "Knowledge",
        "Amazon",
        "AWS",
        "concepts",
        "EMR",
        "EC2",
        "EBS",
        "S3",
        "RDS",
        "web",
        "services",
        "processing",
        "Big",
        "Data",
        "experience",
        "SQL",
        "PLSQL",
        "Shell",
        "Scripting",
        "database",
        "concepts",
        "Experience",
        "end",
        "technologies",
        "HTML",
        "CSS",
        "JavaScript",
        "Experience",
        "Windows",
        "UNIXLINUX",
        "platform",
        "technologies",
        "Big",
        "Data",
        "SQL",
        "XML",
        "HTML",
        "Core",
        "Java",
        "Shell",
        "Scripting",
        "working",
        "experience",
        "file",
        "formats",
        "Json",
        "Avro",
        "Parquet",
        "compression",
        "techniques",
        "bzip",
        "Hands",
        "experience",
        "data",
        "ETL",
        "tools",
        "Informatica",
        "Ab",
        "initio",
        "IBM",
        "Data",
        "Stage",
        "loading",
        "transformation",
        "working",
        "experience",
        "knowledge",
        "NOSQL",
        "HBase",
        "Cassandra",
        "Mongo",
        "DB",
        "Couch",
        "DB",
        "working",
        "experience",
        "Amazon",
        "Web",
        "ServicesAWSusing",
        "S3",
        "storage",
        "EC2",
        "computing",
        "RDS",
        "EBS",
        "Experience",
        "NIFI",
        "work",
        "flow",
        "scheduler",
        "Hadoop",
        "jobs",
        "Direct",
        "Acyclic",
        "graph",
        "DAG",
        "actions",
        "control",
        "Experience",
        "IDEs",
        "source",
        "control",
        "repositories",
        "Eclipse",
        "IntelliJ",
        "GitHub",
        "Maven",
        "SBT",
        "Experience",
        "Tableau",
        "reports",
        "data",
        "excel",
        "sheets",
        "files",
        "CSV",
        "files",
        "Experience",
        "data",
        "sources",
        "Oracle",
        "Netezza",
        "SQL",
        "PostgreSQL",
        "Experience",
        "J2EE",
        "technologies",
        "Java",
        "servlets",
        "JSP",
        "EJB",
        "JDBC",
        "Work",
        "Experience",
        "SrHadoop",
        "Developer",
        "Rsvp",
        "hospitality",
        "New",
        "York",
        "NY",
        "March",
        "Present",
        "Description",
        "RSVP",
        "Hospitality",
        "UAE",
        "firm",
        "Revenue",
        "Management",
        "Business",
        "Analytics",
        "Hotels",
        "SpaSalons",
        "Service",
        "Industry",
        "projects",
        "RSVP",
        "Hospitality",
        "clients",
        "expertise",
        "sectors",
        "hospitality",
        "leisure",
        "wellness",
        "industries",
        "Responsibilities",
        "member",
        "team",
        "nextgeneration",
        "data",
        "analytics",
        "Build",
        "Big",
        "Data",
        "Analytics",
        "Visualization",
        "platform",
        "highvolume",
        "data",
        "streams",
        "Agile",
        "Scrum",
        "Methodology",
        "team",
        "code",
        "review",
        "sessions",
        "Pig",
        "ETL",
        "tool",
        "transformations",
        "event",
        "filter",
        "traffic",
        "preaggregations",
        "data",
        "HDFS",
        "loading",
        "transforming",
        "sets",
        "semi",
        "algorithms",
        "Hadoop",
        "Spark",
        "Context",
        "HiveSQL",
        "Data",
        "Frames",
        "data",
        "analytics",
        "Hive",
        "import",
        "data",
        "RDBMS",
        "data",
        "focus",
        "sets",
        "data",
        "information",
        "business",
        "insights",
        "platforms",
        "Hadoopecosystem",
        "workflow",
        "Oozie",
        "tasks",
        "data",
        "HDFS",
        "Pig",
        "data",
        "data",
        "source",
        "HDFS",
        "Sqoop",
        "imports",
        "Hive",
        "tables",
        "partitions",
        "imports",
        "queries",
        "data",
        "Build",
        "Hive",
        "tables",
        "list",
        "hash",
        "partitioning",
        "Hive",
        "Generic",
        "UDFs",
        "business",
        "logic",
        "HiveQL",
        "Integrated",
        "HBase",
        "MapReduce",
        "amount",
        "data",
        "HBase",
        "SQL",
        "scripts",
        "Spark",
        "data",
        "sets",
        "performance",
        "Map",
        "Reduce",
        "jobs",
        "MapReduce",
        "Programs",
        "cluster",
        "Wrote",
        "MapReduce",
        "jobs",
        "JavaAPI",
        "Apache",
        "Sqoop",
        "data",
        "Apache",
        "Hadoop",
        "databases",
        "Oracle",
        "MySQL",
        "analytics",
        "storytelling",
        "dashboards",
        "Tableau",
        "Desktop",
        "TableauServer",
        "GitHub",
        "version",
        "tools",
        "project",
        "versions",
        "EnvironmentHadoop",
        "Java",
        "MapReduce",
        "HDFS",
        "Hive",
        "Linux",
        "XML",
        "Eclipse",
        "Cloudera",
        "Spark",
        "HBase",
        "MongoDB",
        "Python",
        "GitHub",
        "Sqoop",
        "Oozie",
        "DB2",
        "SQL",
        "Server",
        "Oracle",
        "12c",
        "MySQL",
        "Hadoop",
        "Developer",
        "Neurotrack",
        "San",
        "Francisco",
        "CA",
        "June",
        "February",
        "Description",
        "Neurotrack",
        "development",
        "health",
        "assessment",
        "tools",
        "evaluation",
        "patients",
        "risk",
        "decline",
        "research",
        "treatments",
        "diseases",
        "Alzheimers",
        "Neurotracks",
        "mission",
        "diagnosis",
        "treatment",
        "Alzheimers",
        "disease",
        "assessment",
        "therapy",
        "Responsibilities",
        "requirement",
        "cluster",
        "Hadoop",
        "cluster",
        "data",
        "tools",
        "Map",
        "Reduce",
        "Hive",
        "Spark",
        "loading",
        "data",
        "LINUX",
        "file",
        "system",
        "servers",
        "Java",
        "web",
        "services",
        "Kafka",
        "Producers",
        "Kafka",
        "Custom",
        "encoders",
        "custom",
        "input",
        "format",
        "data",
        "Kafka",
        "Partitions",
        "Storm",
        "topologies",
        "data",
        "HDFS",
        "system",
        "Kafka",
        "level",
        "consumers",
        "data",
        "Kafka",
        "partitions",
        "HDFS",
        "Implemented",
        "POC",
        "Map",
        "Reduce",
        "programs",
        "Spark",
        "transformations",
        "Spark",
        "Scala",
        "Map",
        "Reduce",
        "programs",
        "Spark",
        "RDD",
        "transformations",
        "actions",
        "Spark",
        "RDD",
        "transformations",
        "business",
        "analysis",
        "actions",
        "top",
        "transformations",
        "Hive",
        "tables",
        "data",
        "hive",
        "queries",
        "Map",
        "Reduce",
        "way",
        "Map",
        "Reduce",
        "programs",
        "data",
        "data",
        "tables",
        "sets",
        "data",
        "Map",
        "Reduce",
        "Hive",
        "pig",
        "Developed",
        "Map",
        "Reduce",
        "programs",
        "Java",
        "data",
        "staging",
        "Tables",
        "custom",
        "input",
        "formats",
        "data",
        "types",
        "process",
        "input",
        "data",
        "value",
        "pairs",
        "business",
        "logic",
        "Map",
        "Reduce",
        "HCATALOG",
        "Hive",
        "table",
        "metadata",
        "Map",
        "Reduce",
        "Pig",
        "code",
        "Experience",
        "custom",
        "sterilizer",
        "interceptor",
        "source",
        "sink",
        "requirement",
        "flume",
        "data",
        "sources",
        "Experience",
        "Fanout",
        "flume",
        "architecture",
        "data",
        "sources",
        "sink",
        "Developed",
        "Shell",
        "Perl",
        "Python",
        "scripts",
        "Control",
        "flow",
        "Pig",
        "scripts",
        "Exporting",
        "result",
        "HIVE",
        "MySQL",
        "Sqoop",
        "export",
        "tool",
        "usage",
        "Oozie",
        "Workflow",
        "Orchestration",
        "data",
        "data",
        "Spark",
        "code",
        "Indexed",
        "documents",
        "Apache",
        "Solr",
        "Set",
        "Solr",
        "Clouds",
        "indexing",
        "search",
        "Automation",
        "jobs",
        "Data",
        "Data",
        "Sources",
        "result",
        "Hadoop",
        "Distributed",
        "File",
        "System",
        "MR",
        "PIG",
        "Hive",
        "jobs",
        "Kettle",
        "Oozie",
        "Work",
        "Flow",
        "management",
        "NoSQL",
        "databases",
        "Cassandra",
        "Mongo",
        "DB",
        "POC",
        "purpose",
        "images",
        "URIs",
        "data",
        "Cassandra",
        "file",
        "system",
        "Map",
        "Reduce",
        "programs",
        "Talend",
        "ETL",
        "tool",
        "jobs",
        "workflows",
        "Talend",
        "jobs",
        "files",
        "server",
        "Talend",
        "FTP",
        "components",
        "Mongo",
        "DB",
        "storage",
        "processing",
        "Cassandra",
        "Restful",
        "web",
        "service",
        "Row",
        "Level",
        "Updates",
        "time",
        "analytics",
        "CQL",
        "Cassandra",
        "Data",
        "Cassandra",
        "CQL",
        "Java",
        "APIs",
        "data",
        "Cassandra",
        "tables",
        "customer",
        "data",
        "Cassandra",
        "Created",
        "tables",
        "Hive",
        "analyst",
        "SQA",
        "team",
        "Hive",
        "Queries",
        "Developed",
        "Pig",
        "Latin",
        "scripts",
        "data",
        "web",
        "server",
        "output",
        "files",
        "HDFS",
        "cluster",
        "setup",
        "test",
        "benchmarks",
        "results",
        "builddeploy",
        "applications",
        "Maven",
        "CICD",
        "server",
        "Jenkins",
        "methodologies",
        "meetings",
        "spring",
        "plannings",
        "Environment",
        "Hadoop",
        "Cloudera",
        "HDFS",
        "pig0",
        "Hive",
        "Flume",
        "Sqoop",
        "Oozie",
        "AWS",
        "Python",
        "Spark",
        "Scala",
        "Mongo",
        "DB",
        "Cassandra",
        "Solr",
        "ZooKeeper",
        "MySQL",
        "Talend",
        "Shell",
        "Scripting",
        "Linux",
        "Red",
        "Hat",
        "Java",
        "Hadoop",
        "Developer",
        "ATT",
        "Dallas",
        "TX",
        "February",
        "May",
        "Description",
        "ATT",
        "Inc",
        "telecommunications",
        "company",
        "Whitacre",
        "Tower",
        "Downtown",
        "Dallas",
        "Texas",
        "worlds",
        "telecommunications",
        "company",
        "provider",
        "telephone",
        "services",
        "purpose",
        "project",
        "need",
        "databases",
        "processes",
        "address",
        "data",
        "stakeholders",
        "access",
        "data",
        "Responsibilities",
        "requirement",
        "gathering",
        "Business",
        "meeting",
        "client",
        "security",
        "requirements",
        "architect",
        "system",
        "system",
        "Prepared",
        "design",
        "pints",
        "application",
        "flow",
        "documentation",
        "Hadoop",
        "log",
        "Load",
        "sets",
        "data",
        "data",
        "sources",
        "application",
        "Map",
        "Reduce",
        "Programs",
        "cluster",
        "Created",
        "MapReduce",
        "jobs",
        "contents",
        "HBase",
        "OOZIE",
        "reports",
        "files",
        "Cassandra",
        "Sqoop",
        "HDFS",
        "Implemented",
        "Bloom",
        "filters",
        "Cassandra",
        "creation",
        "Cassandra",
        "CQL",
        "God",
        "handson",
        "experience",
        "concurrency",
        "spark",
        "Cassandra",
        "spark",
        "applications",
        "Scala",
        "Hands",
        "experience",
        "RDDs",
        "transformationsand",
        "Actions",
        "spark",
        "applications",
        "knowledge",
        "data",
        "frames",
        "Spark",
        "SQL",
        "loading",
        "data",
        "Cassandra",
        "NoSQL",
        "Database",
        "challenges",
        "issues",
        "security",
        "system",
        "practices",
        "Project",
        "structures",
        "configurations",
        "project",
        "architecture",
        "developer",
        "work",
        "Environment",
        "Java",
        "Python",
        "Cassandra",
        "HTML5",
        "CSS",
        "PIG",
        "HIVE",
        "Hortonworks",
        "distribution",
        "Hadoop",
        "YARN",
        "Ambari",
        "Hadoop",
        "developer",
        "Standard",
        "Poors",
        "New",
        "York",
        "NY",
        "March",
        "January",
        "Description",
        "Standard",
        "Poors",
        "Financial",
        "Services",
        "LLC",
        "SP",
        "services",
        "company",
        "division",
        "SP",
        "Global",
        "research",
        "analysis",
        "stocks",
        "bonds",
        "commodities",
        "SP",
        "stock",
        "market",
        "indices",
        "USbased",
        "SP",
        "SPTSX",
        "SPASX",
        "Responsibilities",
        "Hadoop",
        "stack",
        "data",
        "tools",
        "Pig",
        "Hive",
        "HBase",
        "database",
        "Sqoop",
        "Hortonworks",
        "distribution",
        "system",
        "HDP",
        "HDP",
        "HDP",
        "Developed",
        "Map",
        "programs",
        "queries",
        "data",
        "shell",
        "scripts",
        "data",
        "cleansing",
        "data",
        "loading",
        "process",
        "Hadooplog",
        "files",
        "log",
        "files",
        "sources",
        "HDFS",
        "Flume",
        "Used",
        "HIVE",
        "data",
        "MicrosoftAzure",
        "cloud",
        "data",
        "HIVE",
        "scripts",
        "Hive",
        "tables",
        "data",
        "analysis",
        "requirements",
        "framework",
        "loading",
        "sets",
        "data",
        "UNIX",
        "system",
        "HIVE",
        "tables",
        "business",
        "team",
        "Hive",
        "queries",
        "access",
        "understanding",
        "Classic",
        "MapReduce",
        "YARN",
        "Hive",
        "Generic",
        "UDFs",
        "business",
        "logic",
        "Hive",
        "data",
        "metrics",
        "data",
        "Hive",
        "queries",
        "Pig",
        "scripts",
        "SparkSQL",
        "Spark",
        "Streaming",
        "files",
        "Cassandra",
        "Sqoop",
        "HDFS",
        "processing",
        "Sqoop",
        "import",
        "script",
        "loading",
        "data",
        "Hive",
        "tables",
        "RDBMS",
        "Environment",
        "Horton",
        "Hadoop",
        "Map",
        "Reduce",
        "HDFS",
        "Hive",
        "Pig",
        "Sqoop",
        "AZURE",
        "Oozie",
        "SQL",
        "Spark",
        "HBase",
        "Cassandra",
        "GitHub",
        "Java",
        "Developer",
        "Comcast",
        "Philadelphia",
        "PA",
        "December",
        "February",
        "Description",
        "Comcast",
        "NBCUniversal",
        "technology",
        "entertainment",
        "millions",
        "people",
        "moments",
        "experiences",
        "Responsibilities",
        "phase",
        "Software",
        "Development",
        "Life",
        "models",
        "Requirement",
        "gathering",
        "analysis",
        "Design",
        "Implementation",
        "Testing",
        "Deployment",
        "Maintenance",
        "Developed",
        "Login",
        "Policy",
        "Claims",
        "Screens",
        "customers",
        "HTML",
        "CSS3",
        "JavaScript",
        "AJAX",
        "JSP",
        "jQuery",
        "Core",
        "Java",
        "Business",
        "Logic",
        "development",
        "business",
        "module",
        "applications",
        "J2EE",
        "technologies",
        "Servlets",
        "JSP",
        "webtier",
        "JSPs",
        "Servlets",
        "framework",
        "Core",
        "Java",
        "concepts",
        "MultiThreading",
        "Exception",
        "Handling",
        "Collection",
        "APIs",
        "features",
        "experience",
        "design",
        "development",
        "applications",
        "JavaJ2EE",
        "components",
        "Java",
        "Server",
        "Pages",
        "JSP",
        "Developed",
        "EJB",
        "MDBs",
        "message",
        "Queues",
        "JMS",
        "technology",
        "EJB",
        "Session",
        "Beans",
        "requests",
        "user",
        "interface",
        "CMP",
        "entity",
        "beans",
        "persistence",
        "layer",
        "procedures",
        "triggers",
        "queries",
        "PLSQL",
        "SQL",
        "Server",
        "Use",
        "Spring",
        "MVC",
        "framework",
        "JavaScript",
        "view",
        "frameworks",
        "data",
        "validation",
        "web",
        "jQuery",
        "model",
        "classes",
        "forms",
        "UI",
        "design",
        "patterns",
        "project",
        "Business",
        "Delegate",
        "Data",
        "Transfer",
        "Object",
        "Data",
        "Access",
        "Object",
        "Service",
        "Locator",
        "Singleton",
        "SQL",
        "statements",
        "procedures",
        "data",
        "database",
        "test",
        "cases",
        "unit",
        "test",
        "JUnit",
        "Framework",
        "CVS",
        "version",
        "control",
        "scripts",
        "build",
        "application",
        "development",
        "environment",
        "Environment",
        "Java",
        "HTML",
        "CSS",
        "JavaScript",
        "MySQL",
        "Struts",
        "EJB",
        "Spring",
        "MVC",
        "Java",
        "Developer",
        "Unisys",
        "Blue",
        "Bell",
        "PA",
        "April",
        "November",
        "Description",
        "Unisys",
        "Corporation",
        "information",
        "technology",
        "company",
        "IT",
        "services",
        "software",
        "technology",
        "proprietor",
        "Burroughs",
        "UNIVAC",
        "line",
        "computers",
        "Responsibilities",
        "Performed",
        "Analysis",
        "Design",
        "Development",
        "Integration",
        "Testing",
        "application",
        "modules",
        "application",
        "prototype",
        "JSP",
        "Servlets",
        "JDBC",
        "presentation",
        "web",
        "page",
        "designs",
        "development",
        "project",
        "presentation",
        "layer",
        "HTML",
        "JavaScript",
        "JSF",
        "Ajax",
        "CSS",
        "User",
        "Interface",
        "appearance",
        "data",
        "base",
        "SQL",
        "PLSQL",
        "data",
        "analysis",
        "extraction",
        "functions",
        "application",
        "Struts",
        "Framework",
        "Model",
        "View",
        "Layer",
        "ArchitectureMVC",
        "bugs",
        "unit",
        "testing",
        "test",
        "cases",
        "Junit",
        "Worked",
        "software",
        "development",
        "methodologies",
        "Agile",
        "Waterfall",
        "development",
        "project",
        "Environment",
        "Java",
        "HTML",
        "CSS",
        "JavaScript",
        "JSON",
        "JSP",
        "JDBC",
        "SQL",
        "PLSQL",
        "Education",
        "Bachelors",
        "Skills",
        "Cassandra",
        "Hdfs",
        "Impala",
        "Oozie",
        "Sqoop",
        "CertificationsLicenses",
        "Drivers",
        "License"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T21:54:22.134363",
    "resume_data": "SrHadoop Developer SrHadoop span lDeveloperspan SrHadoop Developer Rsvp hospitality New York NY Having 8years of overall experience in IT industry includes implementing developing and maintenance in Big Data Technologies and Web based applications using Java J2EE technologies Hands on experience in implementing various solutions and analysing data using Hadoop Ecosystems like HDFS Map Reduce Yarn Spark Sqoop Hive Pig Flume Kafka Impala Oozie Oozie coordinator Zookeeper and Cassandra HBase Hands on experience in various Hadoop distributions like Cloudera CDH3 CDH4 CDH5 Horton works Data Platform HDP and MapR Experience in performing inmemory data processing and real time streaming analytics using Apache Spark with Scala Java and Python Experience in Sqoop to import data from various external sources into Hadoop ecosystem components like HDFS HBase and Hive as well as exports data from Hadoop to other external sources Experience on Spark Streaming API with Kafka to feed live streaming data into HDFS and optimized it using spark concepts like Data frames Partitioning Bucketing Parallel execution and Map Side Joins using broadcast joins Experience in advanced procedures like text analytics processing using in memory computing capabilities like Spark written in Scala Experience in SparkSQL to process large amount of data by implementing SparkRDD transformations Actions and Data frames to required input data Experience on data analysis and developing scripts for pig Latin and Hive QLusing Java Experience in using Ambari for provisioning managing monitoring and securing apache Hadoop cluster Experience in analyzing data from Cassandra for quick processing sorting and grouping through CQL Experience on Managing and scheduling Jobs in Hadoop Cluster using Oozie and used Zookeeper for cluster coordination services Experience in configuring different topologies in Storm to import and process information only from multiple sources collect into central repository Hadoop Experience in developing and designing POCs using Scala and deployed on the Yarn cluster compared the performance of Spark with Hive and SQLTeradata Good Knowledge in Amazon AWS concepts like EMR EC2 EBS S3 and RDS web services which provides fast and efficient processing of Big Data Extensive experience with SQL PLSQL Shell Scripting and database concepts Experience with front end technologies like HTML CSS and JavaScript Experience in working with Windows UNIXLINUX platform with different technologies such as Big Data SQL XML HTML Core Java Shell Scripting etc Good working experience on different file formats like Json Avro Parquet compression techniques like snappy bzip Hands on experience in data warehousing with ETL tools like Informatica Ab initio IBM Data Stage for various loading and transformation processes Good working experience and knowledge in NOSQL databases like HBase Cassandra Mongo DB Couch DB Intensive working experience with Amazon Web ServicesAWSusing S3 for storage EC2 for computing and RDS EBS Experience in NIFI work flow scheduler managing Hadoop jobs by Direct Acyclic graph DAG of actions with control flows Experience in using IDEs and source control repositories like Eclipse IntelliJ and GitHub Maven SBT Experience in preparing Tableau reports for analyzed data by using excel sheets flat files CSV files Experience in different data sources like Oracle Netezza SQL and PostgreSQL Experience in J2EE technologies like Java servlets JSP EJB and JDBC etc Work Experience SrHadoop Developer Rsvp hospitality New York NY March 2018 to Present Description RSVP Hospitality is a UAE consulting firm specialized in Revenue Management and Business Analytics for Hotels SpaSalons and Service Industry projects RSVP Hospitality provide our clients extensive expertise in different sectors in hospitality leisure and wellness industries Responsibilities Contributing as a member of a high performing the agile team focused on nextgeneration data analytics Build Big Data Analytics and Visualization platform for handling highvolume batchoriented and realtime data streams Utilized Agile Scrum Methodology to help manage and organize a team with regular code review sessions Used Pig as ETL tool to do transformations event joins filter both traffic and some preaggregations before storing the data onto HDFS Experienced on loading and transforming of large sets of structured semi structured and optimizing of existing algorithms in Hadoop using Spark Context HiveSQL Data Frames Analysed different big data analytics using Hive import data from RDBMS to HDFS Implemented complex big data with a focus on collecting parsing managing analyzing and visualizing large sets of data to turn information into business insights using multiple platforms in the Hadoopecosystem Developed workflow in Oozie to automate the tasks of loading the data into HDFS and preprocessing with Pig Imported data from the structured data source into HDFS using Sqoop incremental imports Created Hive tables partitions and implemented incremental imports to perform adhoc queries on structured data Build Hive tables using list partitioning and hash partitioning and created Hive Generic UDFs to process business logic with HiveQL Integrated HBase with MapReduce to move the bulk amount of data into HBase Developed SQL scripts using Spark for handling different data sets and verifying the performance over Map Reduce jobs Supported MapReduce Programs those are running on the cluster and Wrote MapReduce jobs using JavaAPI Extensively used Apache Sqoop for efficiently transferring bulk data between Apache Hadoop and relational databases Oracle MySQL for predictive analytics Developed storytelling dashboards in Tableau Desktop and published them on to TableauServer and used GitHub version controlling tools to maintain project versions EnvironmentHadoop Java MapReduce HDFS Hive Linux XML Eclipse Cloudera Spark HBase MongoDB Python GitHub Sqoop Oozie DB2 SQL Server Oracle 12c MySQL Hadoop Developer Neurotrack San Francisco CA June 2016 to February 2018 Description Neurotrack is dedicated to the development of noninvasive cognitive health assessment tools that will enable earlier and more effective evaluation of patients who may be at risk for cognitive decline and help advance research of treatments for cognitive diseases including Alzheimers Neurotracks mission is to revolutionize the early diagnosis and treatment of Alzheimers disease with our online assessment and digital therapy Responsibilities Analysing the requirement to setup a cluster Worked on analyzing Hadoop cluster and different big data analytic tools including Map Reduce Hive and Spark Involved in loading data from LINUX file system servers Java web services using Kafka Producers partitions Implemented Kafka Custom encoders for custom input format to load data into Kafka Partitions Implemented Storm topologies to preprocess data before moving into HDFS system Implemented Kafka High level consumers to get data from Kafka partitions and move into HDFS Implemented POC to migrate Map Reduce programs into Spark transformations using Spark and Scala Migrated complex Map Reduce programs into Spark RDD transformations actions Implemented Spark RDD transformations to map business analysis and apply actions on top of transformations Involved in creating Hive tables loading with data and writing hive queries which runs internally in Map Reduce way Developed the Map Reduce programs to parse the raw data and store the pre Aggregated data in the partitioned tables Loaded and transformed large sets of structured semi structured and unstructured data with Map Reduce Hive and pig Developed Map Reduce programs in Java for parsing the raw data and populating staging Tables Experienced in developing custom input formats and data types to parse and process unstructured and semi structured input data and mapped them into key value pairs to implement business logic in Map Reduce Involved in using HCATALOG to access Hive table metadata for Map Reduce or Pig code Experience in implementing custom sterilizer interceptor source and sink as per the requirement in flume to ingest data from multiple sources Experience in setting up Fanout workflow in flume to design vshaped architecture to take data from many sources and ingest into single sink Developed Shell Perl and Python scripts to automate and provide Control flow to Pig scripts Exporting of result set from HIVE to MySQL using Sqoop export tool for further processing Evaluated usage of Oozie for Workflow Orchestration Converted unstructured data to structured data by writing Spark code Indexed documents using Apache Solr Set up Solr Clouds for distributing indexing and search Automation of all the jobs starting from pulling the Data from different Data Sources like MySQL and pushing the result dataset to Hadoop Distributed File System and running MR PIG and Hive jobs using Kettle and Oozie Work Flow management Worked on NoSQL databases like Cassandra Mongo DB for POC purpose in storing images and URIs Integrating bulk data into Cassandra file system using Map Reduce programs Used Talend ETL tool to develop multiple jobs and in setting workflows Created Talend jobs to copy the files from one server to another and utilized Talend FTP components Worked on Mongo DB for distributed storage and processing Designed and implemented Cassandra and associated Restful web service Implemented Row Level Updates and Real time analytics using CQL on Cassandra Data Used Cassandra CQL with Java APIs to retrieve data from Cassandra tables Worked on analysing and examining customer behavioural data using Cassandra Created partitioned tables in Hive mentored analyst and SQA team for writing Hive Queries Developed Pig Latin scripts to extract the data from the web server output files to load into HDFS Involved in cluster setup monitoring test benchmarks for results Involved in builddeploy applications using Maven and integrated with CICD server Jenkins Involved in agile methodologies daily scrum meetings spring plannings Environment Hadoop Cloudera HDFS pig0 Hive Flume Sqoop Oozie AWS Redshift9 Python Spark Scala Mongo DB Cassandra Solr ZooKeeper MySQL Talend Shell Scripting Linux Red Hat Java Hadoop Developer ATT Dallas TX February 2015 to May 2016 Description ATT Inc is an American multinational telecommunications holding company headquartered at Whitacre Tower in Downtown Dallas Texas It is the worlds largest telecommunications company the second largest provider of mobile telephone services The purpose of this project was to eliminate the need for various regional databases to streamline processes and reduce the address fallouts This provides consistent data for various stakeholders while having access to realtime data Responsibilities Apart from the normal requirement gathering participated in a Business meeting with the client to gather security requirements Assisted with the architect to analyze the existing system and future system Prepared design blue pints and application flow documentation Experienced in managing and reviewing Hadoop log files Load and transform large sets of structured semistructured and unstructured data Responsible to manage data coming from different sources and application Supported Map Reduce Programs those are running on the cluster Created MapReduce jobs to extracts the contents from HBase and configured in OOZIE workflow to generate analytical reports Extracted files from Cassandra through Sqoop and placed in HDFS and processed Implemented Bloom filters in Cassandra using keyspace creation Involved in writing Cassandra CQL statements God handson experience in developing concurrency using spark and Cassandra together Involved in writing spark applications using Scala Hands on experience in creating RDDs transformationsand Actions while implementing spark applications Good knowledge in creating data frames using Spark SQL Involved in loading data into Cassandra NoSQL Database Documented all the challenges issues involved to deal with the security system and Implemented best practices Created Project structures and configurations according to the project architecture and made it available to the junior developer to continue their work Environment Java Python Cassandra HTML5 CSS PIG HIVE Hortonworks distribution of Hadoop 23 YARN Ambari Hadoop developer Standard Poors New York NY March 2013 to January 2015 Description Standard Poors Financial Services LLC SP is an American financial services company It is a division of SP Global that publishes financial research and analysis on stocks bonds and commodities SP is known for its stock market indices such as the USbased SP 500 the Canadian SPTSX and the Australian SPASX 200 Responsibilities Worked on analyzing Hadoop stack and different big data analytic tools including Pig Hive HBase database and Sqoop Experienced to implement Hortonworks distribution system HDP 21 HDP 22 and HDP 23 Developed Map Reduce programs for some refined queries on big data Developed interactive shell scripts for scheduling various data cleansing and data loading process Experienced in managing and reviewing the Hadooplog files and importing log files from various sources into HDFS using Flume Used HIVE queries to import data into MicrosoftAzure cloud and analyzed the data using HIVE scripts Creating Hive tables and working on them for data analysis to cope up with the requirements Developed a framework to handle loading and transform large sets of unstructured data from the UNIX system to HIVE tables Worked with a business team in creating Hive queries for ad hoc access Indepth understanding of Classic MapReduce and YARN architectures Implemented Hive Generic UDFs to implement business logic Used Hive to analyse the partitioned and bucketed data and compute various metrics for reporting Analysed the data by performing Hive queries ran Pig scripts SparkSQL and Spark Streaming Extracted files from Cassandra through Sqoop and placed in HDFS for further processing Involved in creating generic Sqoop import script for loading data into Hive tables from RDBMS Environment Horton works Hadoop Map Reduce HDFS Hive Pig Sqoop AZURE Oozie SQL Spark HBase Cassandra and GitHub Java Developer Comcast Philadelphia PA December 2011 to February 2013 Description Comcast NBCUniversal creates incredible technology and entertainment that connects millions of people to the moments and experiences that matter most Responsibilities Involved in each phase of Software Development Life CycleSDLC models like Requirement gathering and analysis Design Implementation Testing Deployment and Maintenance Developed Login Policy and Claims Screens for customers using HTML 5 CSS3 JavaScript AJAX JSP and jQuery Used Core Java to develop Business Logic Involved in the development of business module applications using J2EE technologies like Servlets JSP Designed and developed the webtier using JSPs Servlets framework Used various Core Java concepts such as MultiThreading Exception Handling Collection APIs to implement various features and enhancements Strong experience in design development of applications using JavaJ2EE components such as Java Server Pages JSP Developed EJB MDBs and message Queues using JMS technology EJB Session Beans were used to process requests from the user interface and CMP entity beans were used to interact with the persistence layer Developed stored procedures triggers and queries using PLSQL in SQL Server Use Spring MVC as framework and JavaScript for clientside view used frameworks for clientside data validation creating dynamic web pagesAjax jQuery Developed model classes based on the forms to be displayed on the UI Implemented various design patterns in the project such as Business Delegate Data Transfer Object Data Access Object Service Locator and Singleton Used SQL statements and procedures to fetch the data from the database Developed test cases and performed unit test using JUnit Framework Used CVS as version control and ANT scripts to fetch build and deploy application to development environment Environment Java HTML CSS JavaScript MySQL Struts EJB Spring MVC Java Developer Unisys Blue Bell PA April 2010 to November 2011 Description Unisys Corporation is an American global information technology company that provides IT services software and technology It is the legacy proprietor of the Burroughs and UNIVAC line of computers formed when the former bought the latter Responsibilities Performed Analysis Design Development Integration and Testing of application modules Implemented application prototype using JSP Servlets JDBC and to give the presentation Developed new web page designs and development of the project presentation layer by using HTML JavaScript JSF Ajax and implemented CSS for User Interface and better appearance Implemented data base queries by using SQL and PLSQL to perform data analysis extraction and various functions Implemented an application by using Struts Framework which leverages classical Model View Layer ArchitectureMVC Involved in fixing bugs and unit testing with test cases using Junit Worked with various software development methodologies like Agile Waterfall to increase the development of the project Environment Java HTML CSS JavaScript JSON JSP JDBC and SQL PLSQL Education Bachelors Skills Cassandra Hdfs Impala Oozie Sqoop CertificationsLicenses Drivers License",
    "unique_id": "7c5632bb-7549-400f-bb5b-cda098866713"
}