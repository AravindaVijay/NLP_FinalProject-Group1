{
    "clean_data": "HadoopSpark Developer HadoopSpark span lDeveloperspan HadoopSpark Developer BECU Seattle WA Around 5 years of experience in Big data application development through Hadoop ecosystem components like Hadoop Spark Hive Kafka Sqoop Yarn Oozie Handson experience in working with Cloudera Hortonworks Hadoop Distributions Having knowledge and understanding of Distributed Computing and Parallel processing frameworks Experienced at performing read and write operations on HDFS filesystem Experience in implementing Spark with the integration of Hadoop Ecosystem Experience in data cleansing using Spark Map and Filter Functions Experience in designing and developing applications in Spark using Scala Worked with Spark RDD for parallel processing of datasets in HDFS SQL Server and other data sources Used job scheduling tool Oozie to manage and schedule Spark Jobs on a Hadoop cluster Having good knowledge and experience on Apache Spark Spark Streaming Spark SQL Hands on experience in using Spark Streaming programming model for real time processing of data stored in HDFS Skilled in integrating Kafka with Spark streaming for faster data processing Knowledge of using Producer and Consumer APIs of Apache Kafka Experience in creating Hive Tables and loading the data from different file formats Good Experience in Data importing and Exporting to Hive and HDFS with Sqoop and processing data using Hive QL Implemented Partitioning Dynamic Partition Buckets in HIVE Extending Hive Core functionality by writing UDFs for Data Analysis Experience in processing the data using HiveQL for data Analytics Experience in converting HiveSQL queries into Spark transformations using Spark RDD and Dataframe API in Scala and Python and performing mapside joins on RDDs Good exposure to Python programming Good knowledge on Python Collections and Python Scripting Experience working with large data sets and making performance improvements Experience dealing with file formats like Sequence files Avro JSON Parquet ORC Sufficient knowledge on NOSQL databases HBASE Experience in creating and driving large scale ETL pipelines Knowledge of working with Amazons Elastic Cloud Compute EC2 cluster instances for computational tasks Simple Storage Service S3 as Storage mechanism and setting up EMR Elastic MapReduce Experience in working with Tableau visualization tool Good with version control systems like Git Experience in using different build tools like SBT and Maven Strong Knowledge on UNIXLINUX commands and shell scripting Adequate knowledge of Scrum Agile methodologies Good communication and presentation skills willing to learn and adapt to emerging new technologies Highly motivated with the ability to work independently or as an integral part of a team and committed to highest levels of profession Work Experience HadoopSpark Developer BECU Tukwila WA August 2018 to Present Responsibilities Having good Knowledge on Spark Architecture and core components of Hadoop frameworks Experience in designing and deployment of Hadoop cluster and different Big Data analytic tools including Spark Hive Sqoop Oozie with Cloudera distribution Worked under the Cloudera distribution CDH 56 version Imported and transformed large scale volumes of data from various data sources to HDFS Loading data from Linux Filesystems to HDFS and viceversa Experience in importing and exporting data using Sqoop from Relational Database Systems to HDFS and viceversa Processed different data sets files from HDFS into Spark code using Scala and SparkSQL for faster testing and processing of data Load the data into Spark RDDs Spark Data Frame APIs and performed inmemory data computation to generate the output response Worked on writing various spark transformations using Scala for Data Validation Cleansing and Filtering in Hadoop HDFS Developed Scala scripts using both Data frames and RDDs in Spark for Data aggregation queries Responsible in performing a sort join filter and other transformations on the datasets Involved in creating Hive tables to load the transformed data and stored it in HDFS Did various performance optimizations like using distributed cache for small datasets partitioning bucketing of the tables in hive and Map Side joins Written customized Hive UDFs in Scala where the functionality is too complex to extend Hive functionality Performed analysis on the hive tables based on the business logic by writing queries using HiveQL for faster data processing Appended the Dataframes to preexisting data in hive Performed data cleansing to meet business requirements and stored the output data to Hive and HDFS Implemented the workflows using Apache Oozie framework to automate tasks Used Git extensively as versioning tool Worked with Jenkins for continuous integration Environments Cloudera 56 Hadoop 30 HDFS Spark 24 Hive 30 Spark SQL Scala Sqoop Oozie Linux shell GIT Jenkins Agile SparkHadoop Developer TMobile Seattle WA April 2017 to July 2018 Responsibilities Worked under the Hortonworks HDP Enterprise Worked on large sets of structured and semistructured data Identifying data sources and create appropriate data ingestion procedures Worked on importing and exporting data from Oracle into HDFS and HIVE using Sqoop Involved in creating Hive tables loading data into them Analyzed the data by performing Hive queries Hive QL that will run internally in Map reduce way Developed UDFs to analysetransform the data Implemented Partitioning Dynamic Partitions Buckets in HIVE and designed both Managed and External tables in Hive to optimize performance Written PySpark scripts for data extraction transformation and aggregation Experience converting HiveQLSQL queries into Spark transformations through Spark RDD and Dataframe API in Python Wrote various spark transformations in Python to perform data cleansing validation and summarization activities on the data Implemented the persistence of frequently used transformed data from data frames for faster processing Used Spark SQL to perform sort join and filter the data Performed data aggregation operations using Spark SQL queries Implemented data ingestion and handling clusters in real time processing using Kafka Configured Spark streaming to receive data from Kafka and store the streamed data to HDFS using Scala for real time processing Transformed the Dstreams into Dataframes using spark SQL Build hive tables on the transformed data and used different SERDEs to store the data in HDFS in ORC format Copied the ORC files to Amazon S3 buckets using Sqoop for further processing in amazon EMR Exported the analyzed data to the relational databases using Sqoop to further visualize and generate reports for the BI team Used Oozie and Oozie coordinators to deploy end to end data processing pipelines and scheduling the work flows Used Git as Version Control System Automated the code deployment process using Jenkins Environments HDP 25 Hadoop 26 HDFS Spark 24 Hive 21 Scala Sqoop Kafka Oozie Amazon S3 Linux shell Git Jenkins Agile Software Engineer Dhanush IT Solutions May 2014 to July 2016 Responsibilities Was involved in Big Data project implementation and support Worked on big data platform and ecosystem created complex data processing pipelines for data management functionalities Was involved in design and implementation of highly scalable system that meet the architectural requirements for system scalability availability and performance Implemented CDH4 Hadoop cluster on Linux Assisted with performance tuning and monitoring Worked extensively with Sqoop for importing and exporting the data from HDFS to Relational Database systems and viceversa loading data into HDFS Developed and maintained application that run on custom architecture using diverse technologies including Core Java J2EE XML JMS Used Oozie to automate data loading into the Hadoop Distributed File System Shared responsibility for administration of Hadoop Hive Organized tasks and resources to complete work and meet deadlines according to established departmental procedures Conferred with systems analysts data engineers programmers and others to design system and to obtain information on project performance requirements and interfaces Was involved in building distributed scalable and reliable data pipelines that ingest and process data at large scale Experienced working in a Linux environment Detected data quality issues identified their root causes developed fixes Design build and maintain data pipelines to support data and analytical needs Utilized programming tools to bring together a diverse set of data sources and making them easily accessible and useful for analysis Developed user interfaces using JSP HTML XML and JavaScript for interactive cross browser functionality and complex user interface Provided Technical support for production environments resolving the issues analyzing the defects providing and implementing the solution defects Environments Hadoop HDFS Sqoop Hive Java JSP Cloudera Linux Agile Skills Hdfs Mapreduce Oozie Sqoop Hbase Kafka Hadoop Nosql C Git Hadoop Hbase Hive Mapreduce Python Zookeeper Sql server Oracle Sql Linux Additional Information Technical Skills BigData Technologies Spark Hadoop MapReduce HDFS Hive Yarn Oozie Sqoop Kafka Zookeeper Hadoop Distributions Horton Works Cloudera Cloud platforms Aws Databases Oracle 11g10g SQL Server NOSQL Databases HBase Version Control Tools Git Build Tools Maven sbt Languages Scala Python Java C C SQL HiveQL Unix shell scripts Operating System Linux Various Versions Windows 710 Development Tools IntelliJ NetBeans Scala IDE for Eclipse",
    "entities": [
        "Python Collections and Python Scripting",
        "Work Experience HadoopSpark Developer",
        "Spark Hive Sqoop Oozie",
        "Written PySpark",
        "CDH4 Hadoop",
        "Filter Functions",
        "ETL",
        "SparkSQL",
        "Simple Storage Service S3",
        "Sqoop",
        "SQL Build",
        "HIVE",
        "Spark Data Frame",
        "Maven",
        "Performed",
        "Conferred",
        "Provided Technical",
        "BI",
        "Utilized",
        "UNIXLINUX",
        "HDFS",
        "HDFS SQL Server",
        "JavaScript",
        "Oracle",
        "Sequence",
        "SBT",
        "Linux Filesystems",
        "Used Spark",
        "Copied",
        "the Hadoop Distributed File System Shared",
        "HadoopSpark Developer HadoopSpark",
        "Detected",
        "Appended the Dataframes",
        "Exporting to Hive and HDFS",
        "Present Responsibilities Having good Knowledge on Spark Architecture",
        "Git Jenkins Agile Software Engineer Dhanush IT",
        "Cloudera Hortonworks Hadoop Distributions Having",
        "Hadoop Ecosystem",
        "Control Tools Git Build",
        "Hadoop Hive Organized",
        "Linux",
        "Hadoop Spark",
        "JSP",
        "Git Experience",
        "Maven Strong",
        "Adequate",
        "Spark for Data",
        "SQL",
        "Hadoop",
        "Data",
        "Spark RDD",
        "Sqoop Involved",
        "Amazons Elastic Cloud Compute EC2",
        "Relational Database",
        "XML",
        "Linux Agile Skills Hdfs Mapreduce Oozie Sqoop Hbase",
        "NOSQL",
        "HBase Version",
        "Spark Streaming",
        "Transformed the Dstreams",
        "Data Analysis",
        "Tableau",
        "Relational Database Systems",
        "Distributed Computing",
        "Scala for Data Validation Cleansing and Filtering",
        "ORC",
        "Environments Hadoop HDFS Sqoop",
        "Apache Spark",
        "Amazon S3",
        "Big Data",
        "Hive",
        "Git as Version Control System Automated",
        "Oracle Sql Linux Additional Information Technical Skills BigData Technologies Spark Hadoop MapReduce HDFS Hive Yarn Oozie",
        "Spark"
    ],
    "experience": "Experience in implementing Spark with the integration of Hadoop Ecosystem Experience in data cleansing using Spark Map and Filter Functions Experience in designing and developing applications in Spark using Scala Worked with Spark RDD for parallel processing of datasets in HDFS SQL Server and other data sources Used job scheduling tool Oozie to manage and schedule Spark Jobs on a Hadoop cluster Having good knowledge and experience on Apache Spark Spark Streaming Spark SQL Hands on experience in using Spark Streaming programming model for real time processing of data stored in HDFS Skilled in integrating Kafka with Spark streaming for faster data processing Knowledge of using Producer and Consumer APIs of Apache Kafka Experience in creating Hive Tables and loading the data from different file formats Good Experience in Data importing and Exporting to Hive and HDFS with Sqoop and processing data using Hive QL Implemented Partitioning Dynamic Partition Buckets in HIVE Extending Hive Core functionality by writing UDFs for Data Analysis Experience in processing the data using HiveQL for data Analytics Experience in converting HiveSQL queries into Spark transformations using Spark RDD and Dataframe API in Scala and Python and performing mapside joins on RDDs Good exposure to Python programming Good knowledge on Python Collections and Python Scripting Experience working with large data sets and making performance improvements Experience dealing with file formats like Sequence files Avro JSON Parquet ORC Sufficient knowledge on NOSQL databases HBASE Experience in creating and driving large scale ETL pipelines Knowledge of working with Amazons Elastic Cloud Compute EC2 cluster instances for computational tasks Simple Storage Service S3 as Storage mechanism and setting up EMR Elastic MapReduce Experience in working with Tableau visualization tool Good with version control systems like Git Experience in using different build tools like SBT and Maven Strong Knowledge on UNIXLINUX commands and shell scripting Adequate knowledge of Scrum Agile methodologies Good communication and presentation skills willing to learn and adapt to emerging new technologies Highly motivated with the ability to work independently or as an integral part of a team and committed to highest levels of profession Work Experience HadoopSpark Developer BECU Tukwila WA August 2018 to Present Responsibilities Having good Knowledge on Spark Architecture and core components of Hadoop frameworks Experience in designing and deployment of Hadoop cluster and different Big Data analytic tools including Spark Hive Sqoop Oozie with Cloudera distribution Worked under the Cloudera distribution CDH 56 version Imported and transformed large scale volumes of data from various data sources to HDFS Loading data from Linux Filesystems to HDFS and viceversa Experience in importing and exporting data using Sqoop from Relational Database Systems to HDFS and viceversa Processed different data sets files from HDFS into Spark code using Scala and SparkSQL for faster testing and processing of data Load the data into Spark RDDs Spark Data Frame APIs and performed inmemory data computation to generate the output response Worked on writing various spark transformations using Scala for Data Validation Cleansing and Filtering in Hadoop HDFS Developed Scala scripts using both Data frames and RDDs in Spark for Data aggregation queries Responsible in performing a sort join filter and other transformations on the datasets Involved in creating Hive tables to load the transformed data and stored it in HDFS Did various performance optimizations like using distributed cache for small datasets partitioning bucketing of the tables in hive and Map Side joins Written customized Hive UDFs in Scala where the functionality is too complex to extend Hive functionality Performed analysis on the hive tables based on the business logic by writing queries using HiveQL for faster data processing Appended the Dataframes to preexisting data in hive Performed data cleansing to meet business requirements and stored the output data to Hive and HDFS Implemented the workflows using Apache Oozie framework to automate tasks Used Git extensively as versioning tool Worked with Jenkins for continuous integration Environments Cloudera 56 Hadoop 30 HDFS Spark 24 Hive 30 Spark SQL Scala Sqoop Oozie Linux shell GIT Jenkins Agile SparkHadoop Developer TMobile Seattle WA April 2017 to July 2018 Responsibilities Worked under the Hortonworks HDP Enterprise Worked on large sets of structured and semistructured data Identifying data sources and create appropriate data ingestion procedures Worked on importing and exporting data from Oracle into HDFS and HIVE using Sqoop Involved in creating Hive tables loading data into them Analyzed the data by performing Hive queries Hive QL that will run internally in Map reduce way Developed UDFs to analysetransform the data Implemented Partitioning Dynamic Partitions Buckets in HIVE and designed both Managed and External tables in Hive to optimize performance Written PySpark scripts for data extraction transformation and aggregation Experience converting HiveQLSQL queries into Spark transformations through Spark RDD and Dataframe API in Python Wrote various spark transformations in Python to perform data cleansing validation and summarization activities on the data Implemented the persistence of frequently used transformed data from data frames for faster processing Used Spark SQL to perform sort join and filter the data Performed data aggregation operations using Spark SQL queries Implemented data ingestion and handling clusters in real time processing using Kafka Configured Spark streaming to receive data from Kafka and store the streamed data to HDFS using Scala for real time processing Transformed the Dstreams into Dataframes using spark SQL Build hive tables on the transformed data and used different SERDEs to store the data in HDFS in ORC format Copied the ORC files to Amazon S3 buckets using Sqoop for further processing in amazon EMR Exported the analyzed data to the relational databases using Sqoop to further visualize and generate reports for the BI team Used Oozie and Oozie coordinators to deploy end to end data processing pipelines and scheduling the work flows Used Git as Version Control System Automated the code deployment process using Jenkins Environments HDP 25 Hadoop 26 HDFS Spark 24 Hive 21 Scala Sqoop Kafka Oozie Amazon S3 Linux shell Git Jenkins Agile Software Engineer Dhanush IT Solutions May 2014 to July 2016 Responsibilities Was involved in Big Data project implementation and support Worked on big data platform and ecosystem created complex data processing pipelines for data management functionalities Was involved in design and implementation of highly scalable system that meet the architectural requirements for system scalability availability and performance Implemented CDH4 Hadoop cluster on Linux Assisted with performance tuning and monitoring Worked extensively with Sqoop for importing and exporting the data from HDFS to Relational Database systems and viceversa loading data into HDFS Developed and maintained application that run on custom architecture using diverse technologies including Core Java J2EE XML JMS Used Oozie to automate data loading into the Hadoop Distributed File System Shared responsibility for administration of Hadoop Hive Organized tasks and resources to complete work and meet deadlines according to established departmental procedures Conferred with systems analysts data engineers programmers and others to design system and to obtain information on project performance requirements and interfaces Was involved in building distributed scalable and reliable data pipelines that ingest and process data at large scale Experienced working in a Linux environment Detected data quality issues identified their root causes developed fixes Design build and maintain data pipelines to support data and analytical needs Utilized programming tools to bring together a diverse set of data sources and making them easily accessible and useful for analysis Developed user interfaces using JSP HTML XML and JavaScript for interactive cross browser functionality and complex user interface Provided Technical support for production environments resolving the issues analyzing the defects providing and implementing the solution defects Environments Hadoop HDFS Sqoop Hive Java JSP Cloudera Linux Agile Skills Hdfs Mapreduce Oozie Sqoop Hbase Kafka Hadoop Nosql C Git Hadoop Hbase Hive Mapreduce Python Zookeeper Sql server Oracle Sql Linux Additional Information Technical Skills BigData Technologies Spark Hadoop MapReduce HDFS Hive Yarn Oozie Sqoop Kafka Zookeeper Hadoop Distributions Horton Works Cloudera Cloud platforms Aws Databases Oracle 11g10 g SQL Server NOSQL Databases HBase Version Control Tools Git Build Tools Maven sbt Languages Scala Python Java C C SQL HiveQL Unix shell scripts Operating System Linux Various Versions Windows 710 Development Tools IntelliJ NetBeans Scala IDE for Eclipse",
    "extracted_keywords": [
        "HadoopSpark",
        "Developer",
        "HadoopSpark",
        "span",
        "lDeveloperspan",
        "HadoopSpark",
        "Developer",
        "BECU",
        "Seattle",
        "WA",
        "years",
        "experience",
        "data",
        "application",
        "development",
        "Hadoop",
        "ecosystem",
        "components",
        "Hadoop",
        "Spark",
        "Hive",
        "Kafka",
        "Sqoop",
        "Yarn",
        "Oozie",
        "Handson",
        "experience",
        "Cloudera",
        "Hortonworks",
        "Hadoop",
        "Distributions",
        "knowledge",
        "understanding",
        "Distributed",
        "Computing",
        "processing",
        "frameworks",
        "read",
        "operations",
        "HDFS",
        "filesystem",
        "Experience",
        "Spark",
        "integration",
        "Hadoop",
        "Ecosystem",
        "Experience",
        "data",
        "cleansing",
        "Spark",
        "Map",
        "Filter",
        "Functions",
        "Experience",
        "applications",
        "Spark",
        "Scala",
        "Worked",
        "Spark",
        "RDD",
        "processing",
        "datasets",
        "HDFS",
        "SQL",
        "Server",
        "data",
        "sources",
        "job",
        "scheduling",
        "tool",
        "Oozie",
        "Spark",
        "Jobs",
        "Hadoop",
        "cluster",
        "knowledge",
        "experience",
        "Apache",
        "Spark",
        "Spark",
        "Streaming",
        "Spark",
        "SQL",
        "Hands",
        "experience",
        "Spark",
        "Streaming",
        "programming",
        "model",
        "time",
        "processing",
        "data",
        "HDFS",
        "Kafka",
        "Spark",
        "streaming",
        "data",
        "Knowledge",
        "Producer",
        "Consumer",
        "APIs",
        "Apache",
        "Kafka",
        "Experience",
        "Hive",
        "Tables",
        "data",
        "file",
        "formats",
        "Experience",
        "Data",
        "Exporting",
        "Hive",
        "HDFS",
        "Sqoop",
        "processing",
        "data",
        "Hive",
        "QL",
        "Partitioning",
        "Dynamic",
        "Partition",
        "Buckets",
        "HIVE",
        "Hive",
        "Core",
        "functionality",
        "UDFs",
        "Data",
        "Analysis",
        "Experience",
        "data",
        "HiveQL",
        "data",
        "Analytics",
        "Experience",
        "HiveSQL",
        "queries",
        "Spark",
        "transformations",
        "Spark",
        "RDD",
        "API",
        "Scala",
        "Python",
        "mapside",
        "RDDs",
        "exposure",
        "Python",
        "programming",
        "knowledge",
        "Python",
        "Collections",
        "Python",
        "Scripting",
        "Experience",
        "data",
        "sets",
        "performance",
        "improvements",
        "Experience",
        "file",
        "formats",
        "Sequence",
        "files",
        "Avro",
        "JSON",
        "Parquet",
        "ORC",
        "knowledge",
        "NOSQL",
        "HBASE",
        "Experience",
        "scale",
        "ETL",
        "pipelines",
        "Knowledge",
        "Amazons",
        "Elastic",
        "Cloud",
        "Compute",
        "EC2",
        "cluster",
        "instances",
        "tasks",
        "Simple",
        "Storage",
        "Service",
        "S3",
        "Storage",
        "mechanism",
        "EMR",
        "Elastic",
        "MapReduce",
        "Experience",
        "Tableau",
        "visualization",
        "tool",
        "version",
        "control",
        "systems",
        "Git",
        "Experience",
        "build",
        "tools",
        "SBT",
        "Maven",
        "Strong",
        "Knowledge",
        "UNIXLINUX",
        "commands",
        "shell",
        "knowledge",
        "Scrum",
        "Agile",
        "methodologies",
        "communication",
        "presentation",
        "skills",
        "technologies",
        "ability",
        "part",
        "team",
        "levels",
        "profession",
        "Work",
        "Experience",
        "HadoopSpark",
        "Developer",
        "BECU",
        "Tukwila",
        "WA",
        "August",
        "Present",
        "Responsibilities",
        "Knowledge",
        "Spark",
        "Architecture",
        "core",
        "components",
        "Hadoop",
        "frameworks",
        "Experience",
        "designing",
        "deployment",
        "Hadoop",
        "cluster",
        "Big",
        "Data",
        "tools",
        "Spark",
        "Hive",
        "Sqoop",
        "Oozie",
        "Cloudera",
        "distribution",
        "Cloudera",
        "distribution",
        "CDH",
        "version",
        "scale",
        "volumes",
        "data",
        "data",
        "sources",
        "HDFS",
        "Loading",
        "data",
        "Linux",
        "Filesystems",
        "HDFS",
        "viceversa",
        "Experience",
        "data",
        "Sqoop",
        "Relational",
        "Database",
        "Systems",
        "HDFS",
        "viceversa",
        "data",
        "files",
        "HDFS",
        "Spark",
        "code",
        "Scala",
        "SparkSQL",
        "testing",
        "processing",
        "data",
        "data",
        "Spark",
        "RDDs",
        "Spark",
        "Data",
        "Frame",
        "APIs",
        "data",
        "computation",
        "output",
        "response",
        "spark",
        "transformations",
        "Scala",
        "Data",
        "Validation",
        "Cleansing",
        "Filtering",
        "Hadoop",
        "HDFS",
        "Scala",
        "scripts",
        "Data",
        "frames",
        "RDDs",
        "Spark",
        "Data",
        "aggregation",
        "sort",
        "join",
        "filter",
        "transformations",
        "datasets",
        "Hive",
        "tables",
        "data",
        "HDFS",
        "performance",
        "optimizations",
        "cache",
        "datasets",
        "bucketing",
        "tables",
        "hive",
        "Map",
        "Side",
        "Hive",
        "UDFs",
        "Scala",
        "functionality",
        "functionality",
        "analysis",
        "tables",
        "business",
        "logic",
        "queries",
        "HiveQL",
        "data",
        "processing",
        "Dataframes",
        "data",
        "Performed",
        "data",
        "cleansing",
        "business",
        "requirements",
        "output",
        "data",
        "Hive",
        "HDFS",
        "workflows",
        "Apache",
        "Oozie",
        "framework",
        "tasks",
        "Git",
        "tool",
        "Jenkins",
        "integration",
        "Environments",
        "Cloudera",
        "Hadoop",
        "HDFS",
        "Spark",
        "Hive",
        "Spark",
        "SQL",
        "Scala",
        "Sqoop",
        "Oozie",
        "Linux",
        "shell",
        "GIT",
        "Jenkins",
        "Agile",
        "SparkHadoop",
        "Developer",
        "TMobile",
        "Seattle",
        "WA",
        "April",
        "July",
        "Responsibilities",
        "Hortonworks",
        "HDP",
        "Enterprise",
        "sets",
        "data",
        "data",
        "sources",
        "data",
        "ingestion",
        "procedures",
        "data",
        "Oracle",
        "HDFS",
        "HIVE",
        "Sqoop",
        "Hive",
        "tables",
        "loading",
        "data",
        "data",
        "Hive",
        "queries",
        "Hive",
        "QL",
        "Map",
        "way",
        "UDFs",
        "data",
        "Partitioning",
        "Dynamic",
        "Partitions",
        "Buckets",
        "HIVE",
        "Managed",
        "tables",
        "Hive",
        "performance",
        "Written",
        "PySpark",
        "scripts",
        "data",
        "extraction",
        "transformation",
        "aggregation",
        "Experience",
        "HiveQLSQL",
        "queries",
        "Spark",
        "transformations",
        "Spark",
        "RDD",
        "API",
        "Python",
        "spark",
        "transformations",
        "Python",
        "data",
        "cleansing",
        "validation",
        "summarization",
        "activities",
        "data",
        "persistence",
        "data",
        "data",
        "frames",
        "processing",
        "Spark",
        "SQL",
        "data",
        "data",
        "aggregation",
        "operations",
        "Spark",
        "SQL",
        "data",
        "ingestion",
        "clusters",
        "time",
        "processing",
        "Kafka",
        "Configured",
        "Spark",
        "streaming",
        "data",
        "Kafka",
        "data",
        "HDFS",
        "Scala",
        "time",
        "Transformed",
        "Dstreams",
        "Dataframes",
        "spark",
        "SQL",
        "Build",
        "hive",
        "tables",
        "data",
        "SERDEs",
        "data",
        "HDFS",
        "ORC",
        "format",
        "ORC",
        "files",
        "Amazon",
        "S3",
        "buckets",
        "Sqoop",
        "processing",
        "amazon",
        "EMR",
        "data",
        "databases",
        "Sqoop",
        "reports",
        "BI",
        "team",
        "Oozie",
        "Oozie",
        "coordinators",
        "end",
        "data",
        "pipelines",
        "work",
        "Git",
        "Version",
        "Control",
        "System",
        "code",
        "deployment",
        "process",
        "Jenkins",
        "Environments",
        "HDP",
        "Hadoop",
        "HDFS",
        "Spark",
        "Hive",
        "Scala",
        "Sqoop",
        "Kafka",
        "Oozie",
        "Amazon",
        "S3",
        "Linux",
        "shell",
        "Git",
        "Jenkins",
        "Agile",
        "Software",
        "Engineer",
        "Dhanush",
        "IT",
        "Solutions",
        "May",
        "July",
        "Responsibilities",
        "Big",
        "Data",
        "project",
        "implementation",
        "support",
        "data",
        "platform",
        "ecosystem",
        "data",
        "processing",
        "pipelines",
        "data",
        "management",
        "functionalities",
        "design",
        "implementation",
        "system",
        "requirements",
        "system",
        "scalability",
        "availability",
        "performance",
        "CDH4",
        "Hadoop",
        "cluster",
        "Linux",
        "Assisted",
        "performance",
        "tuning",
        "Sqoop",
        "data",
        "HDFS",
        "Relational",
        "Database",
        "systems",
        "viceversa",
        "loading",
        "data",
        "HDFS",
        "Developed",
        "application",
        "custom",
        "architecture",
        "technologies",
        "Core",
        "Java",
        "J2EE",
        "XML",
        "JMS",
        "Oozie",
        "data",
        "loading",
        "Hadoop",
        "Distributed",
        "File",
        "System",
        "responsibility",
        "administration",
        "Hadoop",
        "Hive",
        "Organized",
        "tasks",
        "resources",
        "work",
        "deadlines",
        "procedures",
        "systems",
        "analysts",
        "data",
        "engineers",
        "programmers",
        "others",
        "system",
        "information",
        "project",
        "performance",
        "requirements",
        "interfaces",
        "building",
        "data",
        "pipelines",
        "process",
        "data",
        "scale",
        "Linux",
        "environment",
        "data",
        "quality",
        "issues",
        "root",
        "causes",
        "fixes",
        "Design",
        "build",
        "data",
        "pipelines",
        "data",
        "programming",
        "tools",
        "set",
        "data",
        "sources",
        "analysis",
        "user",
        "interfaces",
        "JSP",
        "HTML",
        "XML",
        "JavaScript",
        "cross",
        "browser",
        "functionality",
        "user",
        "interface",
        "support",
        "production",
        "environments",
        "issues",
        "defects",
        "solution",
        "Environments",
        "Hadoop",
        "HDFS",
        "Sqoop",
        "Hive",
        "Java",
        "JSP",
        "Cloudera",
        "Linux",
        "Agile",
        "Skills",
        "Hdfs",
        "Mapreduce",
        "Oozie",
        "Sqoop",
        "Hbase",
        "Kafka",
        "Hadoop",
        "Nosql",
        "C",
        "Git",
        "Hadoop",
        "Hbase",
        "Hive",
        "Mapreduce",
        "Python",
        "Zookeeper",
        "Sql",
        "server",
        "Oracle",
        "Sql",
        "Linux",
        "Additional",
        "Information",
        "Technical",
        "Skills",
        "BigData",
        "Technologies",
        "Spark",
        "Hadoop",
        "MapReduce",
        "HDFS",
        "Hive",
        "Yarn",
        "Oozie",
        "Sqoop",
        "Kafka",
        "Zookeeper",
        "Hadoop",
        "Distributions",
        "Horton",
        "Cloudera",
        "Cloud",
        "Aws",
        "Oracle",
        "g",
        "SQL",
        "Server",
        "NOSQL",
        "Databases",
        "HBase",
        "Version",
        "Control",
        "Tools",
        "Git",
        "Build",
        "Tools",
        "Maven",
        "sbt",
        "Languages",
        "Scala",
        "Python",
        "Java",
        "C",
        "C",
        "SQL",
        "HiveQL",
        "Unix",
        "shell",
        "scripts",
        "System",
        "Linux",
        "Various",
        "Versions",
        "Windows",
        "Development",
        "Tools",
        "IntelliJ",
        "NetBeans",
        "Scala",
        "IDE",
        "Eclipse"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T23:08:38.743205",
    "resume_data": "HadoopSpark Developer HadoopSpark span lDeveloperspan HadoopSpark Developer BECU Seattle WA Around 5 years of experience in Big data application development through Hadoop ecosystem components like Hadoop Spark Hive Kafka Sqoop Yarn Oozie Handson experience in working with Cloudera Hortonworks Hadoop Distributions Having knowledge and understanding of Distributed Computing and Parallel processing frameworks Experienced at performing read and write operations on HDFS filesystem Experience in implementing Spark with the integration of Hadoop Ecosystem Experience in data cleansing using Spark Map and Filter Functions Experience in designing and developing applications in Spark using Scala Worked with Spark RDD for parallel processing of datasets in HDFS SQL Server and other data sources Used job scheduling tool Oozie to manage and schedule Spark Jobs on a Hadoop cluster Having good knowledge and experience on Apache Spark Spark Streaming Spark SQL Hands on experience in using Spark Streaming programming model for real time processing of data stored in HDFS Skilled in integrating Kafka with Spark streaming for faster data processing Knowledge of using Producer and Consumer APIs of Apache Kafka Experience in creating Hive Tables and loading the data from different file formats Good Experience in Data importing and Exporting to Hive and HDFS with Sqoop and processing data using Hive QL Implemented Partitioning Dynamic Partition Buckets in HIVE Extending Hive Core functionality by writing UDFs for Data Analysis Experience in processing the data using HiveQL for data Analytics Experience in converting HiveSQL queries into Spark transformations using Spark RDD and Dataframe API in Scala and Python and performing mapside joins on RDDs Good exposure to Python programming Good knowledge on Python Collections and Python Scripting Experience working with large data sets and making performance improvements Experience dealing with file formats like Sequence files Avro JSON Parquet ORC Sufficient knowledge on NOSQL databases HBASE Experience in creating and driving large scale ETL pipelines Knowledge of working with Amazons Elastic Cloud Compute EC2 cluster instances for computational tasks Simple Storage Service S3 as Storage mechanism and setting up EMR Elastic MapReduce Experience in working with Tableau visualization tool Good with version control systems like Git Experience in using different build tools like SBT and Maven Strong Knowledge on UNIXLINUX commands and shell scripting Adequate knowledge of Scrum Agile methodologies Good communication and presentation skills willing to learn and adapt to emerging new technologies Highly motivated with the ability to work independently or as an integral part of a team and committed to highest levels of profession Work Experience HadoopSpark Developer BECU Tukwila WA August 2018 to Present Responsibilities Having good Knowledge on Spark Architecture and core components of Hadoop frameworks Experience in designing and deployment of Hadoop cluster and different Big Data analytic tools including Spark Hive Sqoop Oozie with Cloudera distribution Worked under the Cloudera distribution CDH 56 version Imported and transformed large scale volumes of data from various data sources to HDFS Loading data from Linux Filesystems to HDFS and viceversa Experience in importing and exporting data using Sqoop from Relational Database Systems to HDFS and viceversa Processed different data sets files from HDFS into Spark code using Scala and SparkSQL for faster testing and processing of data Load the data into Spark RDDs Spark Data Frame APIs and performed inmemory data computation to generate the output response Worked on writing various spark transformations using Scala for Data Validation Cleansing and Filtering in Hadoop HDFS Developed Scala scripts using both Data frames and RDDs in Spark for Data aggregation queries Responsible in performing a sort join filter and other transformations on the datasets Involved in creating Hive tables to load the transformed data and stored it in HDFS Did various performance optimizations like using distributed cache for small datasets partitioning bucketing of the tables in hive and Map Side joins Written customized Hive UDFs in Scala where the functionality is too complex to extend Hive functionality Performed analysis on the hive tables based on the business logic by writing queries using HiveQL for faster data processing Appended the Dataframes to preexisting data in hive Performed data cleansing to meet business requirements and stored the output data to Hive and HDFS Implemented the workflows using Apache Oozie framework to automate tasks Used Git extensively as versioning tool Worked with Jenkins for continuous integration Environments Cloudera 56 Hadoop 30 HDFS Spark 24 Hive 30 Spark SQL Scala Sqoop Oozie Linux shell GIT Jenkins Agile SparkHadoop Developer TMobile Seattle WA April 2017 to July 2018 Responsibilities Worked under the Hortonworks HDP Enterprise Worked on large sets of structured and semistructured data Identifying data sources and create appropriate data ingestion procedures Worked on importing and exporting data from Oracle into HDFS and HIVE using Sqoop Involved in creating Hive tables loading data into them Analyzed the data by performing Hive queries Hive QL that will run internally in Map reduce way Developed UDFs to analysetransform the data Implemented Partitioning Dynamic Partitions Buckets in HIVE and designed both Managed and External tables in Hive to optimize performance Written PySpark scripts for data extraction transformation and aggregation Experience converting HiveQLSQL queries into Spark transformations through Spark RDD and Dataframe API in Python Wrote various spark transformations in Python to perform data cleansing validation and summarization activities on the data Implemented the persistence of frequently used transformed data from data frames for faster processing Used Spark SQL to perform sort join and filter the data Performed data aggregation operations using Spark SQL queries Implemented data ingestion and handling clusters in real time processing using Kafka Configured Spark streaming to receive data from Kafka and store the streamed data to HDFS using Scala for real time processing Transformed the Dstreams into Dataframes using spark SQL Build hive tables on the transformed data and used different SERDEs to store the data in HDFS in ORC format Copied the ORC files to Amazon S3 buckets using Sqoop for further processing in amazon EMR Exported the analyzed data to the relational databases using Sqoop to further visualize and generate reports for the BI team Used Oozie and Oozie coordinators to deploy end to end data processing pipelines and scheduling the work flows Used Git as Version Control System Automated the code deployment process using Jenkins Environments HDP 25 Hadoop 26 HDFS Spark 24 Hive 21 Scala Sqoop Kafka Oozie Amazon S3 Linux shell Git Jenkins Agile Software Engineer Dhanush IT Solutions May 2014 to July 2016 Responsibilities Was involved in Big Data project implementation and support Worked on big data platform and ecosystem created complex data processing pipelines for data management functionalities Was involved in design and implementation of highly scalable system that meet the architectural requirements for system scalability availability and performance Implemented CDH4 Hadoop cluster on Linux Assisted with performance tuning and monitoring Worked extensively with Sqoop for importing and exporting the data from HDFS to Relational Database systems and viceversa loading data into HDFS Developed and maintained application that run on custom architecture using diverse technologies including Core Java J2EE XML JMS Used Oozie to automate data loading into the Hadoop Distributed File System Shared responsibility for administration of Hadoop Hive Organized tasks and resources to complete work and meet deadlines according to established departmental procedures Conferred with systems analysts data engineers programmers and others to design system and to obtain information on project performance requirements and interfaces Was involved in building distributed scalable and reliable data pipelines that ingest and process data at large scale Experienced working in a Linux environment Detected data quality issues identified their root causes developed fixes Design build and maintain data pipelines to support data and analytical needs Utilized programming tools to bring together a diverse set of data sources and making them easily accessible and useful for analysis Developed user interfaces using JSP HTML XML and JavaScript for interactive cross browser functionality and complex user interface Provided Technical support for production environments resolving the issues analyzing the defects providing and implementing the solution defects Environments Hadoop HDFS Sqoop Hive Java JSP Cloudera Linux Agile Skills Hdfs Mapreduce Oozie Sqoop Hbase Kafka Hadoop Nosql C Git Hadoop Hbase Hive Mapreduce Python Zookeeper Sql server Oracle Sql Linux Additional Information Technical Skills BigData Technologies Spark Hadoop MapReduce HDFS Hive Yarn Oozie Sqoop Kafka Zookeeper Hadoop Distributions Horton Works Cloudera Cloud platforms Aws Databases Oracle 11g10g SQL Server NOSQL Databases HBase Version Control Tools Git Build Tools Maven sbt Languages Scala Python Java C C SQL HiveQL Unix shell scripts Operating System Linux Various Versions Windows 710 Development Tools IntelliJ NetBeans Scala IDE for Eclipse",
    "unique_id": "dcd03fbe-c466-4270-9502-85162c43a063"
}