{
    "clean_data": "Tech Lead Tech Lead Tech Lead Elevate Credit Inc Work Experience Tech Lead Elevate Credit Inc Addison TX November 2018 to Present Environment Hive StreamSets Cloudera Kafka SQL Server Jython The main objective of this project was to design and develop a rule engine to execute business rules to detect and flag fraudulent application for loan or line of credit The implementation of this project involved developing a near real time data pipeline using StreamSets and rules execution engine in SQL servers I was responsible was data modeling design and also the implementation of the data pipelines This project is still in progress and finer details are still being work on Project Credit Application Underwriting ReDesign Using Cloud Technologies Senior Hadoop Developer Elevate Credit Inc Dallas TX September 2017 to November 2018 Environment Hive StreamSets Cloudera AWS S3 AWS EMR Spark JAVA Kafka R The main objective of this project was to design and develop next generation underwriting process which is scalable and future proof leveraging cloud technologies From big data perspective my role was to lead the development effort for design development of a near real time data pipeline to process a credit application create base data attributes run it through R data models developed by data scientists and return the credit score back to the underwriting services in a very strict SLA The implementation of this project involved developing a Spark application in JAVA Kafka message queues were used for integration between various services The data format used for the files requests were XML JSON Once scored the application data relevant attributes were stored in AWS S3 buckets This project also involved developing various housekeeping support data pipelines to download data from the AWS EMR cluster to HADOOP onpremise cluster in batch mode RESPONSIBILITIES Requirement analysis and documentation Collaborate with design teams in design and architectural meetings discussions Design development Spark application to consume credit data with highlights Custom XML parser REST API calls to get more data and join with the credit data Parse JSON data request and create JSON reply Multiple DStreams to handle Kafka Queues for parallelization R integration with Spark AWS S3 data storage for persistence and auditing Develop data pipeline to download data from AWS S3 to onpremise HADOOP cluster Senior Hadoop Developer Elevate Credit Inc Addison TX March 2017 to August 2017 Environment Hive StreamSets SQOOP Cloudera AWS S3 AWS RedShift The objective for this project was to work in collaboration with the Marketing team and external vendors to build a real time batch data pipeline to ingest marketing data captured from our product websites The main challenge was to come up with an efficient solution to capture and download data from AWS RedShift S3 location to on premise HADOOP clusters RESPONSIBILITIES Data profiling data schema design and document Design develop bash Linux script using psql cli commands HDFS beeline commands to pull data from AWS S3 buckets to on premise HADOOP cluster Schedule Monitor batch scripts using JAMs Develop JAVA application to download data from Google BigTable Senior Hadoop Developer Elevate Credit Inc Addison TX July 2016 to March 2017 Environment Hive Nifi SQOOP Flume Cloudera Spark PIG StreamSets Informatica The objective for this project was to build a data lake to store data from third party sources ie Credit Bureaus internal transactional data including marketing collections sales finance data for different products This project involved profiling and data analysis of various data sources Develop document test debug data schemas ingestion process data streams It involved a lot of cross functional communication which was one of the challenges apart from various technical challenges This data lake hosted data to various downstream process including Marketing Reporting Data Science modeling etc RESPONSIBILITIES POCs including flume Nifi StreamSets Spark Hive AWS S3 PIG Kafka Collaborate with ETL developers to develop pipelines to deliver data to various Data Marts Work closely with Data Warehouse team to develop data delivery platform Work with BI team to identify the reporting data points and expose them in flat tables views for dashboards Extend custom Hive SerDe to parse XML Extend Debug JAVA application to develop Hive DDL from a given XML JSON files Mentor support offshore team members contractors Onboarding training new resources Training business partners in using HADOOP data tools for data exploration Senior Software Engineer STMicroelectronics Inc Dallas TX January 2014 to June 2016 Environment Hive Pig Linux Sqoop Flume Horton Works 232 The objective was to build a digital media services data collection analytical framework which enables the client to analyze the subscribers usage trend and create reports and dashboards The solution collects various eventslogs from the digital media gateway customer premise equipment CPE devices and after processing them it generates load ready files These load ready files are then sent to enterprise data warehouse for data mining billing and reporting Each and every activity on the Set Top Box generates an Event Most of this data is used to capture the TV habits of the Clients Some of this data is also used for the auditing purposes The solution processes more than 200 Million events per day and generates load ready files which are in gigabytes The solution needed to be robust to make sure that data generated by IPTV Platform is accurate and events are processed without any loss of data RESPONSIBILITIES Created SQOOP version 144 incremental job to import operational data from databases CRM system with incremental load to Hive external tables Developed FLUME version 152 with Fan in architecture using AVRO source and sink with multiple collector tiersbased ingestion scheme for ingesting near Real Time STB box tuning data and system logs The RAW data stored in PARTITION by day The FLUME pipeline was implemented using file type channels for reliability and with failover functionality using load balancing sink group processor Interceptors were used to do some filtering of data anomalies Transferred data using SQOOP from Teradata to be used in HDFS and in hive Used Teradata Connector to Hadoop TDCH 13 to increase performance to sqoop large datasets from Teradata to Hadoop Did POC on TDCH 14 to get the data directly in ORC format in hive tables On boarded new feeds for collection by interfacing with respective teams analyze and agree on a log format to be collected and distributed Providing the support for Hadoop Jobs deployed on production Ensure no failures and smooth run of each Job on Daily execution Prepared evaluated and optimized the workflow and efficiency of Hadoop Production Jobs which leads to minimizing the manual intervention when failure occurs Senior Software Engineer STMicroelectronics Inc Dallas TX January 2013 to December 2014 Environment Hive Pig Linux Sqoop Flume Oozie DATAMEER Wireshark Developed an endtoend network analysis solution for efficient stable operations of MSO networks The main objective was to create a framework to monitor the status and performance of a network proactively to diagnose and resolve connectivity or service issues To detect network security threats to block unauthorized access To achieve the objective the network data was captured ingested cleansed and stored for further analysis and run various throughput tests for TCPIP protocols RESPONSIBILITIES Developed flume Sqoop based ingestion scheme for efficiently moving large amounts of network sniffer captures recorded in libpcap format to HDFS Evaluation of various custom SerDe for most efficient query performance and writes Hive schema development for storing the TCPIP packets for optimal query Development of Various PIG scripts Hive queries for per protocol analysis Various UDFs developed for manipulating the IP header data Basic visualization done using Datameer Worked on various POCs using AWS EC2 EMR S3 Software Engineer STMicroelectronics Inc Dallas TX January 2007 to December 2012 Environment Java C Verilog TclTk MIB Browser RF Sniffer Spirent packet Generator Cisco CMTS This project was done for the Cable Division designing Cable Modem Settop box and gateways solutions The main objective was to create in house test measurement and performance metrics collection capability for next generation DOCSIS devices RESPONSIBILITIES Developed automated validation procedures conforming to standards developed by Cable Labs in TCLTk Expanding CLTEPs to cover all ATPs Developed APIs for automating the handling and of RF Packet Sniffer Packet Generator Represented ST in various interoperability runs Developed expect based scripts to remotely login to various equipment for configuration Developed a simulation environment for simulating cable modem termination system in C Involved in implementation of various vendor specific SNMP MIBs counters for performance monitoring and events logging Developed various GUIs for performance metrics visualization for customer demonstration Developed scripts to dump critical performance data for troubleshooting Developed OpenSSL based image signing tool for software upgrade in the field Education Bachelors Skills Java Ecommerce Links httpbcertmesbvladkl",
    "entities": [
        "Parse JSON",
        "Data Marts Work",
        "DOCSIS",
        "Hadoop Jobs",
        "MSO",
        "ETL",
        "IPTV Platform",
        "Developed",
        "Data Warehouse",
        "Nifi StreamSets Spark Hive AWS S3",
        "Real Time STB",
        "AWS S3",
        "Sqoop",
        "HADOOP",
        "Schedule Monitor",
        "Dallas",
        "Cable Modem Settop",
        "Google BigTable Senior Hadoop Developer Elevate Credit Inc Addison",
        "RESPONSIBILITIES Data",
        "BI",
        "AWS",
        "XML JSON Once",
        "StreamSets",
        "CPE",
        "PIG",
        "RF Packet Sniffer Packet Generator Represented ST",
        "Develop",
        "each Job on Daily",
        "IP",
        "the Cable Division",
        "SQL",
        "RAW",
        "Spark AWS S3",
        "Hadoop",
        "Multiple DStreams",
        "Present Environment Hive",
        "Project Credit Application Underwriting ReDesign Using Cloud Technologies Senior Hadoop Developer Elevate Credit Inc",
        "ORC",
        "SNMP",
        "Credit Bureaus",
        "Marketing Reporting Data Science",
        "Teradata",
        "TX",
        "Hive",
        "SQOOP",
        "C Involved",
        "Spark",
        "Software Engineer STMicroelectronics Inc"
    ],
    "experience": "Experience Tech Lead Elevate Credit Inc Addison TX November 2018 to Present Environment Hive StreamSets Cloudera Kafka SQL Server Jython The main objective of this project was to design and develop a rule engine to execute business rules to detect and flag fraudulent application for loan or line of credit The implementation of this project involved developing a near real time data pipeline using StreamSets and rules execution engine in SQL servers I was responsible was data modeling design and also the implementation of the data pipelines This project is still in progress and finer details are still being work on Project Credit Application Underwriting ReDesign Using Cloud Technologies Senior Hadoop Developer Elevate Credit Inc Dallas TX September 2017 to November 2018 Environment Hive StreamSets Cloudera AWS S3 AWS EMR Spark JAVA Kafka R The main objective of this project was to design and develop next generation underwriting process which is scalable and future proof leveraging cloud technologies From big data perspective my role was to lead the development effort for design development of a near real time data pipeline to process a credit application create base data attributes run it through R data models developed by data scientists and return the credit score back to the underwriting services in a very strict SLA The implementation of this project involved developing a Spark application in JAVA Kafka message queues were used for integration between various services The data format used for the files requests were XML JSON Once scored the application data relevant attributes were stored in AWS S3 buckets This project also involved developing various housekeeping support data pipelines to download data from the AWS EMR cluster to HADOOP onpremise cluster in batch mode RESPONSIBILITIES Requirement analysis and documentation Collaborate with design teams in design and architectural meetings discussions Design development Spark application to consume credit data with highlights Custom XML parser REST API calls to get more data and join with the credit data Parse JSON data request and create JSON reply Multiple DStreams to handle Kafka Queues for parallelization R integration with Spark AWS S3 data storage for persistence and auditing Develop data pipeline to download data from AWS S3 to onpremise HADOOP cluster Senior Hadoop Developer Elevate Credit Inc Addison TX March 2017 to August 2017 Environment Hive StreamSets SQOOP Cloudera AWS S3 AWS RedShift The objective for this project was to work in collaboration with the Marketing team and external vendors to build a real time batch data pipeline to ingest marketing data captured from our product websites The main challenge was to come up with an efficient solution to capture and download data from AWS RedShift S3 location to on premise HADOOP clusters RESPONSIBILITIES Data profiling data schema design and document Design develop bash Linux script using psql cli commands HDFS beeline commands to pull data from AWS S3 buckets to on premise HADOOP cluster Schedule Monitor batch scripts using JAMs Develop JAVA application to download data from Google BigTable Senior Hadoop Developer Elevate Credit Inc Addison TX July 2016 to March 2017 Environment Hive Nifi SQOOP Flume Cloudera Spark PIG StreamSets Informatica The objective for this project was to build a data lake to store data from third party sources ie Credit Bureaus internal transactional data including marketing collections sales finance data for different products This project involved profiling and data analysis of various data sources Develop document test debug data schemas ingestion process data streams It involved a lot of cross functional communication which was one of the challenges apart from various technical challenges This data lake hosted data to various downstream process including Marketing Reporting Data Science modeling etc RESPONSIBILITIES POCs including flume Nifi StreamSets Spark Hive AWS S3 PIG Kafka Collaborate with ETL developers to develop pipelines to deliver data to various Data Marts Work closely with Data Warehouse team to develop data delivery platform Work with BI team to identify the reporting data points and expose them in flat tables views for dashboards Extend custom Hive SerDe to parse XML Extend Debug JAVA application to develop Hive DDL from a given XML JSON files Mentor support offshore team members contractors Onboarding training new resources Training business partners in using HADOOP data tools for data exploration Senior Software Engineer STMicroelectronics Inc Dallas TX January 2014 to June 2016 Environment Hive Pig Linux Sqoop Flume Horton Works 232 The objective was to build a digital media services data collection analytical framework which enables the client to analyze the subscribers usage trend and create reports and dashboards The solution collects various eventslogs from the digital media gateway customer premise equipment CPE devices and after processing them it generates load ready files These load ready files are then sent to enterprise data warehouse for data mining billing and reporting Each and every activity on the Set Top Box generates an Event Most of this data is used to capture the TV habits of the Clients Some of this data is also used for the auditing purposes The solution processes more than 200 Million events per day and generates load ready files which are in gigabytes The solution needed to be robust to make sure that data generated by IPTV Platform is accurate and events are processed without any loss of data RESPONSIBILITIES Created SQOOP version 144 incremental job to import operational data from databases CRM system with incremental load to Hive external tables Developed FLUME version 152 with Fan in architecture using AVRO source and sink with multiple collector tiersbased ingestion scheme for ingesting near Real Time STB box tuning data and system logs The RAW data stored in PARTITION by day The FLUME pipeline was implemented using file type channels for reliability and with failover functionality using load balancing sink group processor Interceptors were used to do some filtering of data anomalies Transferred data using SQOOP from Teradata to be used in HDFS and in hive Used Teradata Connector to Hadoop TDCH 13 to increase performance to sqoop large datasets from Teradata to Hadoop Did POC on TDCH 14 to get the data directly in ORC format in hive tables On boarded new feeds for collection by interfacing with respective teams analyze and agree on a log format to be collected and distributed Providing the support for Hadoop Jobs deployed on production Ensure no failures and smooth run of each Job on Daily execution Prepared evaluated and optimized the workflow and efficiency of Hadoop Production Jobs which leads to minimizing the manual intervention when failure occurs Senior Software Engineer STMicroelectronics Inc Dallas TX January 2013 to December 2014 Environment Hive Pig Linux Sqoop Flume Oozie DATAMEER Wireshark Developed an endtoend network analysis solution for efficient stable operations of MSO networks The main objective was to create a framework to monitor the status and performance of a network proactively to diagnose and resolve connectivity or service issues To detect network security threats to block unauthorized access To achieve the objective the network data was captured ingested cleansed and stored for further analysis and run various throughput tests for TCPIP protocols RESPONSIBILITIES Developed flume Sqoop based ingestion scheme for efficiently moving large amounts of network sniffer captures recorded in libpcap format to HDFS Evaluation of various custom SerDe for most efficient query performance and writes Hive schema development for storing the TCPIP packets for optimal query Development of Various PIG scripts Hive queries for per protocol analysis Various UDFs developed for manipulating the IP header data Basic visualization done using Datameer Worked on various POCs using AWS EC2 EMR S3 Software Engineer STMicroelectronics Inc Dallas TX January 2007 to December 2012 Environment Java C Verilog TclTk MIB Browser RF Sniffer Spirent packet Generator Cisco CMTS This project was done for the Cable Division designing Cable Modem Settop box and gateways solutions The main objective was to create in house test measurement and performance metrics collection capability for next generation DOCSIS devices RESPONSIBILITIES Developed automated validation procedures conforming to standards developed by Cable Labs in TCLTk Expanding CLTEPs to cover all ATPs Developed APIs for automating the handling and of RF Packet Sniffer Packet Generator Represented ST in various interoperability runs Developed expect based scripts to remotely login to various equipment for configuration Developed a simulation environment for simulating cable modem termination system in C Involved in implementation of various vendor specific SNMP MIBs counters for performance monitoring and events logging Developed various GUIs for performance metrics visualization for customer demonstration Developed scripts to dump critical performance data for troubleshooting Developed OpenSSL based image signing tool for software upgrade in the field Education Bachelors Skills Java Ecommerce Links httpbcertmesbvladkl",
    "extracted_keywords": [
        "Tech",
        "Lead",
        "Tech",
        "Lead",
        "Tech",
        "Lead",
        "Elevate",
        "Credit",
        "Inc",
        "Work",
        "Experience",
        "Tech",
        "Lead",
        "Elevate",
        "Credit",
        "Inc",
        "Addison",
        "TX",
        "November",
        "Present",
        "Environment",
        "Hive",
        "StreamSets",
        "Cloudera",
        "Kafka",
        "SQL",
        "Server",
        "Jython",
        "objective",
        "project",
        "rule",
        "engine",
        "business",
        "rules",
        "application",
        "loan",
        "line",
        "credit",
        "implementation",
        "project",
        "time",
        "data",
        "pipeline",
        "StreamSets",
        "execution",
        "engine",
        "SQL",
        "servers",
        "data",
        "modeling",
        "design",
        "implementation",
        "data",
        "pipelines",
        "project",
        "progress",
        "details",
        "work",
        "Project",
        "Credit",
        "Application",
        "Underwriting",
        "ReDesign",
        "Cloud",
        "Technologies",
        "Senior",
        "Hadoop",
        "Developer",
        "Elevate",
        "Credit",
        "Inc",
        "Dallas",
        "TX",
        "September",
        "November",
        "Environment",
        "Hive",
        "StreamSets",
        "Cloudera",
        "S3",
        "AWS",
        "EMR",
        "Spark",
        "JAVA",
        "Kafka",
        "objective",
        "project",
        "generation",
        "underwriting",
        "process",
        "proof",
        "cloud",
        "technologies",
        "data",
        "perspective",
        "role",
        "development",
        "effort",
        "design",
        "development",
        "time",
        "data",
        "pipeline",
        "credit",
        "application",
        "base",
        "data",
        "attributes",
        "R",
        "data",
        "models",
        "data",
        "scientists",
        "credit",
        "score",
        "underwriting",
        "services",
        "SLA",
        "implementation",
        "project",
        "Spark",
        "application",
        "JAVA",
        "Kafka",
        "message",
        "queues",
        "integration",
        "services",
        "data",
        "format",
        "files",
        "requests",
        "XML",
        "JSON",
        "application",
        "data",
        "attributes",
        "AWS",
        "S3",
        "buckets",
        "project",
        "housekeeping",
        "support",
        "data",
        "pipelines",
        "data",
        "AWS",
        "EMR",
        "cluster",
        "HADOOP",
        "onpremise",
        "cluster",
        "batch",
        "mode",
        "RESPONSIBILITIES",
        "Requirement",
        "analysis",
        "documentation",
        "Collaborate",
        "design",
        "teams",
        "design",
        "meetings",
        "discussions",
        "Design",
        "development",
        "Spark",
        "application",
        "credit",
        "data",
        "highlights",
        "Custom",
        "XML",
        "parser",
        "REST",
        "API",
        "data",
        "credit",
        "data",
        "Parse",
        "data",
        "request",
        "reply",
        "Multiple",
        "DStreams",
        "Kafka",
        "Queues",
        "parallelization",
        "R",
        "integration",
        "Spark",
        "AWS",
        "S3",
        "data",
        "storage",
        "persistence",
        "Develop",
        "data",
        "pipeline",
        "data",
        "AWS",
        "S3",
        "HADOOP",
        "cluster",
        "Senior",
        "Hadoop",
        "Developer",
        "Elevate",
        "Credit",
        "Inc",
        "Addison",
        "TX",
        "March",
        "August",
        "Environment",
        "Hive",
        "StreamSets",
        "SQOOP",
        "Cloudera",
        "S3",
        "AWS",
        "RedShift",
        "objective",
        "project",
        "collaboration",
        "Marketing",
        "team",
        "vendors",
        "time",
        "batch",
        "data",
        "pipeline",
        "marketing",
        "data",
        "product",
        "challenge",
        "solution",
        "data",
        "AWS",
        "RedShift",
        "S3",
        "location",
        "premise",
        "HADOOP",
        "clusters",
        "Data",
        "profiling",
        "data",
        "schema",
        "design",
        "document",
        "Design",
        "bash",
        "Linux",
        "script",
        "psql",
        "cli",
        "commands",
        "data",
        "AWS",
        "S3",
        "buckets",
        "premise",
        "HADOOP",
        "cluster",
        "Schedule",
        "Monitor",
        "batch",
        "scripts",
        "JAMs",
        "Develop",
        "application",
        "data",
        "Google",
        "BigTable",
        "Senior",
        "Hadoop",
        "Developer",
        "Elevate",
        "Credit",
        "Inc",
        "Addison",
        "TX",
        "July",
        "March",
        "Environment",
        "Hive",
        "Nifi",
        "SQOOP",
        "Flume",
        "Cloudera",
        "Spark",
        "PIG",
        "StreamSets",
        "Informatica",
        "objective",
        "project",
        "data",
        "lake",
        "data",
        "party",
        "sources",
        "Credit",
        "Bureaus",
        "data",
        "marketing",
        "collections",
        "sales",
        "finance",
        "data",
        "products",
        "project",
        "profiling",
        "data",
        "analysis",
        "data",
        "sources",
        "document",
        "test",
        "data",
        "schemas",
        "ingestion",
        "process",
        "data",
        "lot",
        "cross",
        "communication",
        "challenges",
        "challenges",
        "data",
        "lake",
        "data",
        "process",
        "Marketing",
        "Reporting",
        "Data",
        "Science",
        "RESPONSIBILITIES",
        "POCs",
        "flume",
        "Nifi",
        "StreamSets",
        "Spark",
        "Hive",
        "AWS",
        "S3",
        "PIG",
        "Kafka",
        "Collaborate",
        "ETL",
        "developers",
        "pipelines",
        "data",
        "Data",
        "Marts",
        "Work",
        "Data",
        "Warehouse",
        "team",
        "data",
        "delivery",
        "platform",
        "Work",
        "BI",
        "team",
        "reporting",
        "data",
        "points",
        "tables",
        "views",
        "dashboards",
        "custom",
        "Hive",
        "SerDe",
        "XML",
        "Extend",
        "application",
        "Hive",
        "DDL",
        "XML",
        "JSON",
        "files",
        "Mentor",
        "support",
        "team",
        "members",
        "Onboarding",
        "resources",
        "Training",
        "business",
        "partners",
        "HADOOP",
        "data",
        "tools",
        "data",
        "exploration",
        "Senior",
        "Software",
        "Engineer",
        "STMicroelectronics",
        "Inc",
        "Dallas",
        "TX",
        "January",
        "June",
        "Environment",
        "Hive",
        "Pig",
        "Linux",
        "Sqoop",
        "Flume",
        "Horton",
        "objective",
        "media",
        "services",
        "data",
        "collection",
        "framework",
        "client",
        "subscribers",
        "usage",
        "trend",
        "reports",
        "dashboards",
        "solution",
        "eventslogs",
        "media",
        "gateway",
        "customer",
        "premise",
        "equipment",
        "CPE",
        "devices",
        "load",
        "files",
        "load",
        "files",
        "enterprise",
        "data",
        "warehouse",
        "data",
        "mining",
        "billing",
        "activity",
        "Set",
        "Top",
        "Box",
        "Event",
        "data",
        "TV",
        "habits",
        "Clients",
        "data",
        "auditing",
        "purposes",
        "solution",
        "events",
        "day",
        "load",
        "files",
        "gigabytes",
        "solution",
        "data",
        "IPTV",
        "Platform",
        "events",
        "loss",
        "data",
        "RESPONSIBILITIES",
        "SQOOP",
        "version",
        "job",
        "data",
        "databases",
        "CRM",
        "system",
        "load",
        "tables",
        "FLUME",
        "version",
        "Fan",
        "architecture",
        "AVRO",
        "source",
        "sink",
        "collector",
        "ingestion",
        "scheme",
        "Real",
        "Time",
        "STB",
        "box",
        "data",
        "system",
        "RAW",
        "data",
        "PARTITION",
        "day",
        "FLUME",
        "pipeline",
        "file",
        "type",
        "channels",
        "reliability",
        "functionality",
        "load",
        "sink",
        "group",
        "processor",
        "Interceptors",
        "filtering",
        "data",
        "anomalies",
        "data",
        "SQOOP",
        "Teradata",
        "HDFS",
        "hive",
        "Used",
        "Teradata",
        "Connector",
        "Hadoop",
        "TDCH",
        "performance",
        "datasets",
        "Teradata",
        "Hadoop",
        "POC",
        "TDCH",
        "data",
        "format",
        "tables",
        "feeds",
        "collection",
        "teams",
        "log",
        "format",
        "support",
        "Hadoop",
        "Jobs",
        "production",
        "Ensure",
        "failures",
        "run",
        "Job",
        "execution",
        "Prepared",
        "workflow",
        "efficiency",
        "Hadoop",
        "Production",
        "Jobs",
        "intervention",
        "failure",
        "Senior",
        "Software",
        "Engineer",
        "STMicroelectronics",
        "Inc",
        "Dallas",
        "TX",
        "January",
        "December",
        "Environment",
        "Hive",
        "Pig",
        "Linux",
        "Sqoop",
        "Flume",
        "Oozie",
        "DATAMEER",
        "Wireshark",
        "endtoend",
        "network",
        "analysis",
        "solution",
        "operations",
        "MSO",
        "networks",
        "objective",
        "framework",
        "status",
        "performance",
        "network",
        "connectivity",
        "service",
        "issues",
        "network",
        "security",
        "threats",
        "access",
        "objective",
        "network",
        "data",
        "analysis",
        "throughput",
        "tests",
        "TCPIP",
        "protocols",
        "RESPONSIBILITIES",
        "flume",
        "Sqoop",
        "ingestion",
        "scheme",
        "amounts",
        "network",
        "sniffer",
        "captures",
        "format",
        "HDFS",
        "Evaluation",
        "custom",
        "SerDe",
        "query",
        "performance",
        "Hive",
        "schema",
        "development",
        "TCPIP",
        "packets",
        "query",
        "Development",
        "PIG",
        "scripts",
        "Hive",
        "queries",
        "protocol",
        "analysis",
        "UDFs",
        "IP",
        "header",
        "data",
        "Basic",
        "visualization",
        "Datameer",
        "Worked",
        "POCs",
        "AWS",
        "EC2",
        "EMR",
        "S3",
        "Software",
        "Engineer",
        "STMicroelectronics",
        "Inc",
        "Dallas",
        "TX",
        "January",
        "December",
        "Environment",
        "Java",
        "C",
        "Verilog",
        "TclTk",
        "MIB",
        "Browser",
        "RF",
        "Sniffer",
        "Spirent",
        "packet",
        "Generator",
        "Cisco",
        "CMTS",
        "project",
        "Cable",
        "Division",
        "Cable",
        "Modem",
        "Settop",
        "box",
        "gateways",
        "solutions",
        "objective",
        "house",
        "test",
        "measurement",
        "performance",
        "metrics",
        "collection",
        "capability",
        "generation",
        "DOCSIS",
        "devices",
        "RESPONSIBILITIES",
        "validation",
        "procedures",
        "standards",
        "Cable",
        "Labs",
        "TCLTk",
        "CLTEPs",
        "ATPs",
        "APIs",
        "handling",
        "RF",
        "Packet",
        "Sniffer",
        "Packet",
        "Generator",
        "Represented",
        "ST",
        "interoperability",
        "Developed",
        "scripts",
        "equipment",
        "configuration",
        "simulation",
        "environment",
        "cable",
        "modem",
        "termination",
        "system",
        "C",
        "implementation",
        "vendor",
        "SNMP",
        "MIBs",
        "counters",
        "performance",
        "monitoring",
        "events",
        "GUIs",
        "performance",
        "metrics",
        "visualization",
        "customer",
        "demonstration",
        "scripts",
        "performance",
        "data",
        "image",
        "signing",
        "tool",
        "software",
        "upgrade",
        "field",
        "Education",
        "Bachelors",
        "Skills",
        "Java",
        "Ecommerce",
        "Links",
        "httpbcertmesbvladkl"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T23:10:44.696816",
    "resume_data": "Tech Lead Tech Lead Tech Lead Elevate Credit Inc Work Experience Tech Lead Elevate Credit Inc Addison TX November 2018 to Present Environment Hive StreamSets Cloudera Kafka SQL Server Jython The main objective of this project was to design and develop a rule engine to execute business rules to detect and flag fraudulent application for loan or line of credit The implementation of this project involved developing a near real time data pipeline using StreamSets and rules execution engine in SQL servers I was responsible was data modeling design and also the implementation of the data pipelines This project is still in progress and finer details are still being work on Project Credit Application Underwriting ReDesign Using Cloud Technologies Senior Hadoop Developer Elevate Credit Inc Dallas TX September 2017 to November 2018 Environment Hive StreamSets Cloudera AWS S3 AWS EMR Spark JAVA Kafka R The main objective of this project was to design and develop next generation underwriting process which is scalable and future proof leveraging cloud technologies From big data perspective my role was to lead the development effort for design development of a near real time data pipeline to process a credit application create base data attributes run it through R data models developed by data scientists and return the credit score back to the underwriting services in a very strict SLA The implementation of this project involved developing a Spark application in JAVA Kafka message queues were used for integration between various services The data format used for the files requests were XML JSON Once scored the application data relevant attributes were stored in AWS S3 buckets This project also involved developing various housekeeping support data pipelines to download data from the AWS EMR cluster to HADOOP onpremise cluster in batch mode RESPONSIBILITIES Requirement analysis and documentation Collaborate with design teams in design and architectural meetings discussions Design development Spark application to consume credit data with highlights Custom XML parser REST API calls to get more data and join with the credit data Parse JSON data request and create JSON reply Multiple DStreams to handle Kafka Queues for parallelization R integration with Spark AWS S3 data storage for persistence and auditing Develop data pipeline to download data from AWS S3 to onpremise HADOOP cluster Senior Hadoop Developer Elevate Credit Inc Addison TX March 2017 to August 2017 Environment Hive StreamSets SQOOP Cloudera AWS S3 AWS RedShift The objective for this project was to work in collaboration with the Marketing team and external vendors to build a real time batch data pipeline to ingest marketing data captured from our product websites The main challenge was to come up with an efficient solution to capture and download data from AWS RedShift S3 location to on premise HADOOP clusters RESPONSIBILITIES Data profiling data schema design and document Design develop bash Linux script using psql cli commands HDFS beeline commands to pull data from AWS S3 buckets to on premise HADOOP cluster Schedule Monitor batch scripts using JAMs Develop JAVA application to download data from Google BigTable Senior Hadoop Developer Elevate Credit Inc Addison TX July 2016 to March 2017 Environment Hive Nifi SQOOP Flume Cloudera Spark PIG StreamSets Informatica The objective for this project was to build a data lake to store data from third party sources ie Credit Bureaus internal transactional data including marketing collections sales finance data for different products This project involved profiling and data analysis of various data sources Develop document test debug data schemas ingestion process data streams It involved a lot of cross functional communication which was one of the challenges apart from various technical challenges This data lake hosted data to various downstream process including Marketing Reporting Data Science modeling etc RESPONSIBILITIES POCs including flume Nifi StreamSets Spark Hive AWS S3 PIG Kafka Collaborate with ETL developers to develop pipelines to deliver data to various Data Marts Work closely with Data Warehouse team to develop data delivery platform Work with BI team to identify the reporting data points and expose them in flat tables views for dashboards Extend custom Hive SerDe to parse XML Extend Debug JAVA application to develop Hive DDL from a given XML JSON files Mentor support offshore team members contractors Onboarding training new resources Training business partners in using HADOOP data tools for data exploration Senior Software Engineer STMicroelectronics Inc Dallas TX January 2014 to June 2016 Environment Hive Pig Linux Sqoop Flume Horton Works 232 The objective was to build a digital media services data collection analytical framework which enables the client to analyze the subscribers usage trend and create reports and dashboards The solution collects various eventslogs from the digital media gateway customer premise equipment CPE devices and after processing them it generates load ready files These load ready files are then sent to enterprise data warehouse for data mining billing and reporting Each and every activity on the Set Top Box generates an Event Most of this data is used to capture the TV habits of the Clients Some of this data is also used for the auditing purposes The solution processes more than 200 Million events per day and generates load ready files which are in gigabytes The solution needed to be robust to make sure that data generated by IPTV Platform is accurate and events are processed without any loss of data RESPONSIBILITIES Created SQOOP version 144 incremental job to import operational data from databases CRM system with incremental load to Hive external tables Developed FLUME version 152 with Fan in architecture using AVRO source and sink with multiple collector tiersbased ingestion scheme for ingesting near Real Time STB box tuning data and system logs The RAW data stored in PARTITION by day The FLUME pipeline was implemented using file type channels for reliability and with failover functionality using load balancing sink group processor Interceptors were used to do some filtering of data anomalies Transferred data using SQOOP from Teradata to be used in HDFS and in hive Used Teradata Connector to Hadoop TDCH 13 to increase performance to sqoop large datasets from Teradata to Hadoop Did POC on TDCH 14 to get the data directly in ORC format in hive tables On boarded new feeds for collection by interfacing with respective teams analyze and agree on a log format to be collected and distributed Providing the support for Hadoop Jobs deployed on production Ensure no failures and smooth run of each Job on Daily execution Prepared evaluated and optimized the workflow and efficiency of Hadoop Production Jobs which leads to minimizing the manual intervention when failure occurs Senior Software Engineer STMicroelectronics Inc Dallas TX January 2013 to December 2014 Environment Hive Pig Linux Sqoop Flume Oozie DATAMEER Wireshark Developed an endtoend network analysis solution for efficient stable operations of MSO networks The main objective was to create a framework to monitor the status and performance of a network proactively to diagnose and resolve connectivity or service issues To detect network security threats to block unauthorized access To achieve the objective the network data was captured ingested cleansed and stored for further analysis and run various throughput tests for TCPIP protocols RESPONSIBILITIES Developed flume Sqoop based ingestion scheme for efficiently moving large amounts of network sniffer captures recorded in libpcap format to HDFS Evaluation of various custom SerDe for most efficient query performance and writes Hive schema development for storing the TCPIP packets for optimal query Development of Various PIG scripts Hive queries for per protocol analysis Various UDFs developed for manipulating the IP header data Basic visualization done using Datameer Worked on various POCs using AWS EC2 EMR S3 Software Engineer STMicroelectronics Inc Dallas TX January 2007 to December 2012 Environment Java C Verilog TclTk MIB Browser RF Sniffer Spirent packet Generator Cisco CMTS This project was done for the Cable Division designing Cable Modem Settop box and gateways solutions The main objective was to create in house test measurement and performance metrics collection capability for next generation DOCSIS devices RESPONSIBILITIES Developed automated validation procedures conforming to standards developed by Cable Labs in TCLTk Expanding CLTEPs to cover all ATPs Developed APIs for automating the handling and of RF Packet Sniffer Packet Generator Represented ST in various interoperability runs Developed expect based scripts to remotely login to various equipment for configuration Developed a simulation environment for simulating cable modem termination system in C Involved in implementation of various vendor specific SNMP MIBs counters for performance monitoring and events logging Developed various GUIs for performance metrics visualization for customer demonstration Developed scripts to dump critical performance data for troubleshooting Developed OpenSSL based image signing tool for software upgrade in the field Education Bachelors Skills Java Ecommerce Links httpbcertmesbvladkl",
    "unique_id": "da1e7f24-8ba3-4511-b0d2-f606d9868046"
}