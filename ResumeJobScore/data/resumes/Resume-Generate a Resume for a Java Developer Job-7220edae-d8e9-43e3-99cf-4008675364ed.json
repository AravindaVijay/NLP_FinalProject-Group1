{
    "clean_data": "Sr Hadoop Developer Sr Hadoop span lDeveloperspan Sr Hadoop Developer 8451 Over 7 years of professional IT experience with 3 Years of Big Data Hadoop Ecosystems experience in ingestion storage querying processing and analysis of big data Handson experience architecting and implementing Hadoop clusters on Amazon AWS using EMR S2 S3 Redshift Cassandra MangoDB CosmosDB SimpleDB AmazonRDS DynamoDB Postgresql SQL MS SQL Experience in Hadoop Administration activities such as installation configuration and management of clusters in Cloudera CDH4 CDH5 Hortonworks HDP Distributions using Cloudera Manager Ambari Hands on experience in installing configuring and using Hadoop ecosystem components like HDFS MapReduce Hive Impala Sqoop Pig Oozie Zookeeper Spark Solr Hue Flume Storm Kafka and Yarn distributions Very good Knowledge and experience in Amazon AWS concepts like EMR and EC2 web services which provides fast and efficient processing of Big Data Experienced in performance tuning of Yarn Spark and Hiveandexperienced in developing MapReduce Programs using Apache Hadoop for analyzing the big data as per the requirement Extensively worked on Spark using Scala on cluster for computational analytics installed it on top of Hadoop performed advanced analytical application by making use of Spark with Hive and SQLOracle Expert in big data ecosystem using Hadoop Spark Kafka with columnoriented big data systems on cloud platforms such as Amazon CLoud AWS Microsoft Azure and Google Cloud Platform Experienced in importing exporting data between HDFS and Relational Database Management systems using Sqoop and troubleshooting for any issues Extensive experience in working with various distributions of Hadoop like enterprise versions of Cloudera CDH4CDH5 Hortonworks and good knowledge on MapR distribution IBM Big Insights and Amazons EMR Elastic MapReduce Exposure to Data Lake Implementation using Apache Spark and developed Data pipe lines and applied business logics using Sparkandused Scala and Python to convert HiveSQL queries into RDD transformations in Apache Spark Good Understanding and experience on NameNode HA architecture and experience in monitoring the health of cluster using Ambari Nagios Ganglia and Cronjobs Experienced in Cluster maintenance and Commissioning Decommissioning of Data Nodes and good understandingknowledge of Hadoop Architecture and various components such as HDFS Job Tracker and Task Tracker NameNode DataNode and MapReduce concepts Experienced in implementation of security controls using Kerberos principals ACLs Data encryptions using DMCrypt to protect entire Hadoop clusters Wellversed in spark components like Spark SQL MLib Spark streaming and GraphX Expertise in installation administration patches upgrade configuration performance tuning and troubleshooting of Red hat Linux SUSE CentOS AIX Solaris Experienced Schedule Recurring Hadoop Jobs with Apache Oozie and experience in Jumpstart Kickstart Infrastructure setup and Installation Methods for Linux Good knowledge in troubleshooting skills understanding of systems capacity bottlenecks basics of memory CPU OS storage and network Experience in administration activities of RDBMS data bases such as MS SQL Server Experienced in Hadoop Distributed File System and Ecosystem MapReduce Pig Hive Sqoop YARN MongoDB and HBase and knowledge of NoSQL databases such as HBase Cassandra and MongoDB Major strengths are familiarity with multiple software systems ability to learn quickly new technologies adapt to new environments focused adaptive and quick learner with excellent interpersonal technical and communication skills Work Experience Sr Hadoop Developer 8451 Cincinnati OH June 2018 to Present Roles Responsibilities Worked directly with the Big Data Architecture Team which created the foundation of this Enterprise Analytics initiative in a Hadoopbased Data Lake Created multinode Hadoop and Spark clusters in AWS instances to generate terabytes of data and stored it in AWS HDFS InstalledConfiguredMaintained Apache Hadoop clusters for application development and Hadoop tools like Hive Pig HBase Zookeeper and Sqoop Extracted real time feed using Kafka and Spark streaming and convert it to RDD and process data in the form of Data Frame and save the data as Parquet format in HDFS Developed data pipeline using Flume Sqoop Pig and Java map reduce to ingest customer behavioral data and financial histories into HDFS for analysis Involved in collecting and aggregating large amounts of log data using Apache Flume and staging data in HDFS for further analysis Upgraded the Hadoop cluster from CDH47 to CDH52 and worked on installing cluster commissioning decommissioning of Data Nodes NameNode recovery capacity planning and slots configuration Developed Spark scripts to import large files from Amazon S3 buckets and imported the data from different sources like HDFSHBaseintoSparkRDD Involved in migration of ETL processes from Oracle to Hive to test the easy data manipulation and worked on importing and exporting data from Oracle and DB2 into HDFS and HIVE using Sqoop Worked on Installing Cloudera Manager CDH and install the JCE Policy File to Create a Kerberos Principal for the Cloudera Manager Server enabling Kerberos Using the Wizard Developed Spark jobs using Scala and Python on top of  for interactive and Batch Analysis Monitored cluster for performance and networking and data integrity issues and responsible for troubleshooting issues in the execution of MapReduce jobs by inspecting and reviewing log files Created 25 Linux Bash scripts for users groups data distribution capacity planning and system monitoring Install OS and administrated Hadoop stack with CDH5 with YARN Cloudera distribution including configuration management monitoring debugging and performance tuning Supported MapReduce Programs and distributed applications running on the Hadoop clusterand scripting Hadoop package installation and configuration to support fullyautomated deployments Migrated existing onpremises application to AWS and used AWS services like EC2 and S3 for large data sets processing and storage and worked with ELASTIC MAPREDUCE and setup Hadoop environment in AWS EC2 Instances Worked with systems engineering team to plan and deploy new Hadoop environments and expand existing Hadoop clusters Perform maintenance monitoring deployments and upgrades across infrastructure that supports all our Hadoop clustersand worked on Hive for further analysis and for generating transforming files from different analytical formats to text files Created Hive External tables and loaded the data in to tables and query data using HQL and worked with application teams to install operating system Hadoop updates patches version upgrades as required Worked and learned a great deal from AWS Cloud services like EC2 S3 EBS RDS and VPC Monitoring Hadoopcluster using tools like Nagios Ganglia and Cloudera Manager and maintaining the Cluster by adding and removing of nodes using tools like Ganglia Nagios and Cloudera Manager Worked on Hive for exposing data for further analysis and for generating transforming files from different analytical formats to text files Worked on migrating MapReduce programs into Spark transformations using Spark and Scala initially done using python PySpark Environment Hadoop MapReduce Hive PIG Sqoop Python Spark SparkStreaming Spark SQL AWS EMR AWS S3 AWS Redshift Python Scala Pyspark MapR Java Oozie Flume HBase Nagios Ganglia Hue Cloudera Manager Zookeeper Cloudera Oracle Kerberos and RedHat 65 Sr Hadoop Developer AMEX Phoenix AZ May 2018 to May 2018 Roles Responsibilities Collaborate in identifying the current problems constraints and root causes with data sets to identify the descriptive and predictive solution with support of the Hadoop HDFS MapReduce Pig Hive and Hbase and further to develop reports in Tableau Architect the Hadoop cluster in Pseudo distributed Mode working with Zookeeper and Apache andstoring and loading the data from HDFS to AmazonAWSS3 and backing up and Created tables in AWS cluster with S3 storage Evaluated existing infrastructure systems and technologies and provided gap analysis and documented requirements evaluation and recommendations of system upgrades technologies and created proposed architecture and specifications along with recommendations InstalledConfiguredMaintained Apache Hadoop clusters for application development and Hadoop tools like Hive Pig HBase Zookeeper and Sqoop Installed and Configured Sqoop to import and export the data into MapRFS HBase and Hive from Relational databases Administering large MapR Hadoop environments build and support cluster set up performance tuning and monitoring in an enterprise environment Installed and Configured MapRzookeeper MapRcldb MapPjobtracker MapRtasktracker MapRresourcemanager MapRnode manager MapRfileserver and MapRwebserver Installed and configured Knox gateway to secure HIVE through ODBC WebHcat and Oozie services Load data from relational databases into MapRFS filesystem and HBase using Sqoop and setting up MapR metrics with NoSQL database to log metrics data Close monitoring and analysis of the MapReducejob executions on cluster at task level and optimized Hadoop clusters components to achieve high performance Developed data pipeline using Flume Sqoop Pig and Java MapReduce to ingest customer behavioral data into HDFS for analysis Integrated HDP clusters with Active Directory and enabled Kerberos for Authentication Worked on commissioning decommissioning of Data Nodes NameNode recovery capacity planning and installed Oozie workflow engine to run multiple Hive and Pig Jobs Worked on creating the Data Model for HBase from the current Oracle Data model Implemented High Availability and automatic failover infrastructure to overcome single point of failure for Name node utilizing zookeeper services Leveraged Chef to manage and maintain builds in various environments and planned for hardware and software installation on production cluster and communicated with multiple teams to get it done Monitoring the Hadoop cluster functioning through MCS and worked on NoSQL databases including HBase Used Hive and created Hive tables and involved in data loading and writing Hive UDFs and worked with Linux server admin team in administering the server hardware and operating system Worked closely with data analysts to construct creative solutions for their analysis tasks and managed and reviewed Hadoop and HBase log files Exported the analyzed data to the relational databases using Sqoop for visualization and to generate reports and worked on importing and exporting data from Oracle into HDFS and HIVE using Sqoop Collaborating with application teams to install operating system and Hadoop updates patches version upgrades when required Automated workflows using shell scripts pull data from various databases into Hadoop Supported in setting up QA environment and updating configurations for implementing scripts with Pig and Sqoop EnvironmentHadoop HDFS Map Reduce Hive HBase Kafka Zookeeper Oozie Impala Java Cloudera Oracle Teradata SQL Server Python UNIX Shell Scripting ETL Flume Scala Spark Sqoop Python AWS S3 EC2 Kafka Oracle MySQL Hortonworks YARN Python Hadoop Developer Charles Schwab Littleton CO January 2016 to May 2017 Roles Responsibilities Evaluated suitability of Hadoop and its ecosystem to the above project and implementing validating with various proof of concept POC applications to eventually adopt them to benefit from the Big Data Hadoop initiative Architected Hadoop system pulling data from Linux systems and RDBMS database on a regular basis in order to ingest data using Sqoop Installation configuration supporting and managing Hadoop Clusters using Apache Cloudera CDH4 distributions and on Amazon web services AWS Developed MapReduce programs to parse the raw data populate staging tables and store the refined data in partitioned tables in the EDW Worked on Cloudera Hadoop Upgrades and Patches and Installation of Ecosystem Products through Cloudera manager along with Cloudera Manager Upgrade Created Hive queries that helped market analysts spot emerging trends by comparing fresh data with EDW reference tables and historical metrics Setting an Amazon Web Services AWS EC2 instance for the Cloudera Manager server Enabled speedy reviews and first mover advantages by using Oozie to automate data loading into the Hadoop Distributed File System and PIG to preprocess the data Provided design recommendations and thought leadership to sponsorsstakeholders that improved review processes and resolved technical problems Identify query duplication complexity and dependency to minimize migration efforts Technology stack Oracle Hortonworks HDP cluster Attunity Visibility Cloudera Navigator Optimizer AWS Cloud and Dynamo DB Shared responsibility for administration of Hadoop Hive and Pig and managed and reviewed Hadoop log files and updating the configuration on each host Worked with Spark eco system using Scala Python and HIVE Queries on different data formats like Text file and parquet Tested raw data and executed performance scripts and configuring Cloudera Manager Agent heartbeat interval and timeouts Worked with teams in setting up AWS EC2 instances by using different AWS services like S3 EBS Elastic Load Balancer and Auto scaling groups VPC subnets and CloudWatch Implemented CDH3 Hadoop cluster on RedHat Enterprise Linux 64 assisted with performance tuning and monitoring Monitoring Hadoop Cluster through Cloudera Manager and Implementing alerts based on Error messages Used Spark Streaming API with Kafka to build live dashboards Worked on Transformations actions in RDD Spark Streaming Pair RDD Operations Checkpointing and SBT Providing reports to management on Cluster Usage Metrics and related HBase tables to load large sets of structured semistructured and unstructured data coming from UNIX NoSQL and a variety of portfolios Involved in different Hadoop distributions like Cloudera CDH3 CDH4 and Horton Works Distributions HDP and MapR Performed installation upgrade and configure tasks for impala on all machines in a cluster and supported codedesign analysis strategy development and project planning Created reports for the BI team using Sqoop to export data into HDFS and Hive and assisted with data capacity planning and node forecasting Managing Amazon Web Services AWS infrastructure with automation and configuration Administrator for Pig Hive and HBase installing updates patches and upgrades and performed both major and minor upgrades to the existing CDH cluster and upgraded the Hadoop cluster from CDH3 to CDH4 Developed a process for the Batch ingestion of CSV Files Sqoop from different sources and also generating views on the data source using Shell Scripting and Python Environment Hadoop HDFS Map Reduce Hive HBase Zookeeper Impala Java jdk16 Cloudera Oracle SQL Server UNIX Shell Scripting Flume Oozie Scala Spark ETL Sqoop Python kafka PySpark AWS S3 MongoDB Oracle SQL Hortonworks XML RedHat Linux 64 Java Developer Fresh Desk Chennai Tamil Nadu November 2015 to November 2015 Roles Responsibilities Implemented MultiThreaded Environment and used most of the interfaces under the collection framework by using Core Java Concepts Developed Graphical User Interfaces by using JSF JSP HTML DHTML Angularjs CSS and JavaScript and developed scripts in python for Financial Data coming from SQL Developer based on the requirements specified Implemented several JavaJ2EE design patterns like Spring MVC Singleton Spring Dependency Injection and Data Transfer Object Used JAXWS SOAP for producing web services and involved in writing programs to consume the web services using SOA with CXF framework and developed few web pages using JSP JSTL HTML CSS Java script Ajax and JSON Implemented business logic data exchange XML processing and created graphics using Python and Django Wrote code to fetch data from Web services using JQUERY AJAX via JSON response and updating the HTML pages and developed high traffic web applications using HTML CSS and JavaScript jQuery Bootstrap Ext JS AngularJS Nodejs and reactjs Write SQL queries and create PLSQL functionsprocedurespackages that are optimized for APEX and improve performance and response times of APEX pages and reports Used JQuery library NodeJS and AngularJS for creation of powerful dynamic WebPages and web applications by using its advanced and cross browser functionality Used Java Server Pages for content layout and presentation with Python and Extracted and loaded data using Python scripts and PLSQL packages Worked with various frameworks of JavaScript like BackboneJS AngularJS and EmberJS etc Written with objectoriented Python Flask SQL Beautiful Soup httplib2 Jinja2 HTMLCSS Bootstrap jQuery Linux Sublime Text GIT Developed GUI using JSP Struts HTML3 CSS3 XHTML JQuery Swing and JavaScript to simplify the complexities of the application Wrote and executed various MYSQL database queries from python using PythonMySQL connector and MySQLdb package and generated Python Django forms to record data of online users and used PyTest for writing test cases Developed and coordinated complex high quality solutions to clients using J2SE J2EE Servlets JSP HTML Struts Spring MVC SOAP JavaScript JQuery JSON and XML Exposed business functionality to external systems Interoperable clients using Web Services WSDLSOAP ApacheAxis Used Hibernate objectrelationalmapping ORM solution technique of mapping data representation from MVC model to Oracle10g Relational data model with a SQLbased schema and mapped using Hibernate Annotations Skilled in using collections in Python for manipulating and looping through different user defined objects Designed and developed intranet web applications using Ext JS Reactjs JavaScript and CSS and developed Merge jobs in Python to extract and load data into MySQL and Mango Db database Worked on Oracle SQL Server as the backend databases and integrated with Hibernate to retrieve Data Access Objects Developed Python batch processors to consume and produce various feeds and developed entire frontend and backend modules using Python on Django Web Framework and developed Business Logic using Python on Django Web Framework Designed and documented RESTHTTP APIs including JSON data formats and API versioning strategy and wrote Apex triggers apex classes developing Visual force pages batch classes Involved in AJAX driven application by invoking web servicesAPI and parsing the JSON response and involved in writing application level code to interact with APIs Web Services using JSON EnvironmentPython Django Java JSF MVC Spring IOC APEX Ruby on Rails Spring JDBC Hibernate ActiveMQ Log4j Ant MySQL JDK 16 J2EE JSP Servlets HTML LDAP Salesforce ESB Mule JDBC MongoDB DAO EJB 30 PLSQL reactjs Web Sphere Eclipse AngularJS and CVS Education Bachelors Skills JAVA 2 years LINUX 2 years ORACLE 2 years PYTHON 2 years SQL 2 years Additional Information Technical Competencies Hadoop Ecosystem ToolsMapReduce HDFS Pig Hive HBase Sqoop Zookeeper Oozie Hue Storm Kafka Spark Flume Languages Java core java HTML Programming C C Databases MySQL Oracle SQL Server MongoDB Platforms Linux RHEL Ubuntu open Solaris AIX Scripting Languages Shell Scripting HTML scripting Python Puppet Web Servers Apache Tomcat JBOSS windows server2003 2008 and 2012 Cluster Management Tools HDP Ambari Cloudera Manager Hue Solr Cloud",
    "entities": [
        "HDFS MapReduce Hive Impala Sqoop",
        "HDFSHBaseintoSparkRDD Involved",
        "Hadoop Clusters",
        "Oracle SQL Server",
        "HDFS Developed",
        "Data Lake Implementation",
        "Merge",
        "SQL Developer",
        "Nodejs",
        "Relational",
        "the Hadoop HDFS MapReduce Pig Hive",
        "BI",
        "HDFS",
        "UNIX",
        "RedHat 65 Sr Hadoop Developer AMEX",
        "MS SQL Server Experienced",
        "Developed Spark",
        "CVS Education",
        "Data Nodes NameNode",
        "Java Server Pages",
        "J2SE J2EE Servlets",
        "CDH47",
        "Flume Sqoop Pig",
        "IBM",
        "Ganglia Nagios",
        "Amazon Web Services AWS",
        "CPU OS",
        "Hadoop Hive and Pig",
        "node",
        "Python Environment Hadoop",
        "RDD",
        "Hadoop",
        "Commissioning Decommissioning of Data Nodes",
        "Kerberos Using the Wizard Developed Spark",
        "Installed and Configured MapRzookeeper MapRcldb MapPjobtracker MapRtasktracker",
        "Python Flask SQL Beautiful Soup httplib2 Jinja2",
        "Hadoop Distributed File System",
        "Cincinnati",
        "HBase",
        "Automated",
        "Oozie Scala Spark",
        "Hadoop Administration",
        "Implemented High Availability",
        "Sr Hadoop Developer Sr Hadoop",
        "JavaJ2EE",
        "Amazon",
        "server2003",
        "Oracle SQL Hortonworks",
        "CDH3",
        "Cloudera Hadoop",
        "Spark with",
        "Python",
        "Developed",
        "NameNode",
        "Cloudera Oracle SQL Server",
        "Kerberos",
        "Oracle Data",
        "DMCrypt",
        "PySpark Environment Hadoop MapReduce Hive PIG",
        "Solaris AIX Scripting Languages Shell Scripting",
        "Work Experience Sr Hadoop Developer",
        "Hibernate Annotations Skilled",
        "Spring MVC Singleton",
        "MapReducejob",
        "AIX Solaris",
        "AWS Cloud",
        "Horton Works",
        "Active Directory",
        "SOA",
        "the JCE Policy File to Create a Kerberos Principal",
        "Hadoop Supported",
        "Linux",
        "the Hadoop Distributed File System",
        "JSP",
        "Visual",
        "Shell Scripting",
        "Ext JS Reactjs",
        "Spark SQL MLib Spark",
        "HDP",
        "Cluster Usage Metrics",
        "Oozie services Load",
        "SQLbased",
        "Ambari Nagios Ganglia",
        "MVC",
        "Hive Pig HBase Zookeeper",
        "Spark",
        "Apex",
        "EJB",
        "Implementing",
        "Cronjobs Experienced",
        "Data Access Objects",
        "CDH",
        "HTML CSS",
        "API",
        "MapRwebserver Installed",
        "Sqoop",
        "QA",
        "HIVE",
        "MapRFS",
        "Created",
        "Install OS",
        "AWS",
        "Scala",
        "MapRfileserver",
        "Hadoop Architecture",
        "Oracle",
        "Architected Hadoop",
        "Apache",
        "PIG",
        "CDH3 Hadoop",
        "log data",
        "Oracle to Hive",
        "HTML",
        "Financial Data",
        "Oozie",
        "CXF",
        "WebPages",
        "Additional Information Technical Competencies Hadoop Ecosystem",
        "Relational Database Management",
        "IOC APEX Ruby on Rails Spring JDBC Hibernate",
        "Business Logic using Python on Django Web Framework Designed",
        "Worked with",
        "Data Frame",
        "Batch Analysis Monitored",
        "Dynamo DB Shared",
        "CDH5",
        "Hive",
        "SQL MS",
        "Amazon AWS",
        "Handson",
        "Big Data Hadoop Ecosystems",
        "the Data Model for HBase",
        "log metrics",
        "MapR",
        "MapReduce Programs",
        "Supported MapReduce Programs",
        "ETL",
        "CDH4 Developed",
        "Apache Hadoop",
        "Oracle Hortonworks HDP",
        "Monitoring Hadoop Cluster",
        "Impala",
        "CloudWatch Implemented",
        "InstalledConfiguredMaintained Apache Hadoop",
        "JavaScript",
        "HDFS Job Tracker",
        "Big Data Experienced",
        "CSS",
        "PySpark AWS S3",
        "Charles Schwab Littleton",
        "EDW",
        "MapReduce",
        "NoSQL",
        "Tableau",
        "HDP Distributions",
        "ODBC WebHcat",
        "the Big Data Architecture Team",
        "PyTest",
        "JQuery",
        "Hadoop Spark Kafka",
        "RedHat Enterprise Linux",
        "Sqoop Collaborating",
        "Created Hive External",
        "Cloudera"
    ],
    "experience": "Experience in Hadoop Administration activities such as installation configuration and management of clusters in Cloudera CDH4 CDH5 Hortonworks HDP Distributions using Cloudera Manager Ambari Hands on experience in installing configuring and using Hadoop ecosystem components like HDFS MapReduce Hive Impala Sqoop Pig Oozie Zookeeper Spark Solr Hue Flume Storm Kafka and Yarn distributions Very good Knowledge and experience in Amazon AWS concepts like EMR and EC2 web services which provides fast and efficient processing of Big Data Experienced in performance tuning of Yarn Spark and Hiveandexperienced in developing MapReduce Programs using Apache Hadoop for analyzing the big data as per the requirement Extensively worked on Spark using Scala on cluster for computational analytics installed it on top of Hadoop performed advanced analytical application by making use of Spark with Hive and SQLOracle Expert in big data ecosystem using Hadoop Spark Kafka with columnoriented big data systems on cloud platforms such as Amazon CLoud AWS Microsoft Azure and Google Cloud Platform Experienced in importing exporting data between HDFS and Relational Database Management systems using Sqoop and troubleshooting for any issues Extensive experience in working with various distributions of Hadoop like enterprise versions of Cloudera CDH4CDH5 Hortonworks and good knowledge on MapR distribution IBM Big Insights and Amazons EMR Elastic MapReduce Exposure to Data Lake Implementation using Apache Spark and developed Data pipe lines and applied business logics using Sparkandused Scala and Python to convert HiveSQL queries into RDD transformations in Apache Spark Good Understanding and experience on NameNode HA architecture and experience in monitoring the health of cluster using Ambari Nagios Ganglia and Cronjobs Experienced in Cluster maintenance and Commissioning Decommissioning of Data Nodes and good understandingknowledge of Hadoop Architecture and various components such as HDFS Job Tracker and Task Tracker NameNode DataNode and MapReduce concepts Experienced in implementation of security controls using Kerberos principals ACLs Data encryptions using DMCrypt to protect entire Hadoop clusters Wellversed in spark components like Spark SQL MLib Spark streaming and GraphX Expertise in installation administration patches upgrade configuration performance tuning and troubleshooting of Red hat Linux SUSE CentOS AIX Solaris Experienced Schedule Recurring Hadoop Jobs with Apache Oozie and experience in Jumpstart Kickstart Infrastructure setup and Installation Methods for Linux Good knowledge in troubleshooting skills understanding of systems capacity bottlenecks basics of memory CPU OS storage and network Experience in administration activities of RDBMS data bases such as MS SQL Server Experienced in Hadoop Distributed File System and Ecosystem MapReduce Pig Hive Sqoop YARN MongoDB and HBase and knowledge of NoSQL databases such as HBase Cassandra and MongoDB Major strengths are familiarity with multiple software systems ability to learn quickly new technologies adapt to new environments focused adaptive and quick learner with excellent interpersonal technical and communication skills Work Experience Sr Hadoop Developer 8451 Cincinnati OH June 2018 to Present Roles Responsibilities Worked directly with the Big Data Architecture Team which created the foundation of this Enterprise Analytics initiative in a Hadoopbased Data Lake Created multinode Hadoop and Spark clusters in AWS instances to generate terabytes of data and stored it in AWS HDFS InstalledConfiguredMaintained Apache Hadoop clusters for application development and Hadoop tools like Hive Pig HBase Zookeeper and Sqoop Extracted real time feed using Kafka and Spark streaming and convert it to RDD and process data in the form of Data Frame and save the data as Parquet format in HDFS Developed data pipeline using Flume Sqoop Pig and Java map reduce to ingest customer behavioral data and financial histories into HDFS for analysis Involved in collecting and aggregating large amounts of log data using Apache Flume and staging data in HDFS for further analysis Upgraded the Hadoop cluster from CDH47 to CDH52 and worked on installing cluster commissioning decommissioning of Data Nodes NameNode recovery capacity planning and slots configuration Developed Spark scripts to import large files from Amazon S3 buckets and imported the data from different sources like HDFSHBaseintoSparkRDD Involved in migration of ETL processes from Oracle to Hive to test the easy data manipulation and worked on importing and exporting data from Oracle and DB2 into HDFS and HIVE using Sqoop Worked on Installing Cloudera Manager CDH and install the JCE Policy File to Create a Kerberos Principal for the Cloudera Manager Server enabling Kerberos Using the Wizard Developed Spark jobs using Scala and Python on top of   for interactive and Batch Analysis Monitored cluster for performance and networking and data integrity issues and responsible for troubleshooting issues in the execution of MapReduce jobs by inspecting and reviewing log files Created 25 Linux Bash scripts for users groups data distribution capacity planning and system monitoring Install OS and administrated Hadoop stack with CDH5 with YARN Cloudera distribution including configuration management monitoring debugging and performance tuning Supported MapReduce Programs and distributed applications running on the Hadoop clusterand scripting Hadoop package installation and configuration to support fullyautomated deployments Migrated existing onpremises application to AWS and used AWS services like EC2 and S3 for large data sets processing and storage and worked with ELASTIC MAPREDUCE and setup Hadoop environment in AWS EC2 Instances Worked with systems engineering team to plan and deploy new Hadoop environments and expand existing Hadoop clusters Perform maintenance monitoring deployments and upgrades across infrastructure that supports all our Hadoop clustersand worked on Hive for further analysis and for generating transforming files from different analytical formats to text files Created Hive External tables and loaded the data in to tables and query data using HQL and worked with application teams to install operating system Hadoop updates patches version upgrades as required Worked and learned a great deal from AWS Cloud services like EC2 S3 EBS RDS and VPC Monitoring Hadoopcluster using tools like Nagios Ganglia and Cloudera Manager and maintaining the Cluster by adding and removing of nodes using tools like Ganglia Nagios and Cloudera Manager Worked on Hive for exposing data for further analysis and for generating transforming files from different analytical formats to text files Worked on migrating MapReduce programs into Spark transformations using Spark and Scala initially done using python PySpark Environment Hadoop MapReduce Hive PIG Sqoop Python Spark SparkStreaming Spark SQL AWS EMR AWS S3 AWS Redshift Python Scala Pyspark MapR Java Oozie Flume HBase Nagios Ganglia Hue Cloudera Manager Zookeeper Cloudera Oracle Kerberos and RedHat 65 Sr Hadoop Developer AMEX Phoenix AZ May 2018 to May 2018 Roles Responsibilities Collaborate in identifying the current problems constraints and root causes with data sets to identify the descriptive and predictive solution with support of the Hadoop HDFS MapReduce Pig Hive and Hbase and further to develop reports in Tableau Architect the Hadoop cluster in Pseudo distributed Mode working with Zookeeper and Apache andstoring and loading the data from HDFS to AmazonAWSS3 and backing up and Created tables in AWS cluster with S3 storage Evaluated existing infrastructure systems and technologies and provided gap analysis and documented requirements evaluation and recommendations of system upgrades technologies and created proposed architecture and specifications along with recommendations InstalledConfiguredMaintained Apache Hadoop clusters for application development and Hadoop tools like Hive Pig HBase Zookeeper and Sqoop Installed and Configured Sqoop to import and export the data into MapRFS HBase and Hive from Relational databases Administering large MapR Hadoop environments build and support cluster set up performance tuning and monitoring in an enterprise environment Installed and Configured MapRzookeeper MapRcldb MapPjobtracker MapRtasktracker MapRresourcemanager MapRnode manager MapRfileserver and MapRwebserver Installed and configured Knox gateway to secure HIVE through ODBC WebHcat and Oozie services Load data from relational databases into MapRFS filesystem and HBase using Sqoop and setting up MapR metrics with NoSQL database to log metrics data Close monitoring and analysis of the MapReducejob executions on cluster at task level and optimized Hadoop clusters components to achieve high performance Developed data pipeline using Flume Sqoop Pig and Java MapReduce to ingest customer behavioral data into HDFS for analysis Integrated HDP clusters with Active Directory and enabled Kerberos for Authentication Worked on commissioning decommissioning of Data Nodes NameNode recovery capacity planning and installed Oozie workflow engine to run multiple Hive and Pig Jobs Worked on creating the Data Model for HBase from the current Oracle Data model Implemented High Availability and automatic failover infrastructure to overcome single point of failure for Name node utilizing zookeeper services Leveraged Chef to manage and maintain builds in various environments and planned for hardware and software installation on production cluster and communicated with multiple teams to get it done Monitoring the Hadoop cluster functioning through MCS and worked on NoSQL databases including HBase Used Hive and created Hive tables and involved in data loading and writing Hive UDFs and worked with Linux server admin team in administering the server hardware and operating system Worked closely with data analysts to construct creative solutions for their analysis tasks and managed and reviewed Hadoop and HBase log files Exported the analyzed data to the relational databases using Sqoop for visualization and to generate reports and worked on importing and exporting data from Oracle into HDFS and HIVE using Sqoop Collaborating with application teams to install operating system and Hadoop updates patches version upgrades when required Automated workflows using shell scripts pull data from various databases into Hadoop Supported in setting up QA environment and updating configurations for implementing scripts with Pig and Sqoop EnvironmentHadoop HDFS Map Reduce Hive HBase Kafka Zookeeper Oozie Impala Java Cloudera Oracle Teradata SQL Server Python UNIX Shell Scripting ETL Flume Scala Spark Sqoop Python AWS S3 EC2 Kafka Oracle MySQL Hortonworks YARN Python Hadoop Developer Charles Schwab Littleton CO January 2016 to May 2017 Roles Responsibilities Evaluated suitability of Hadoop and its ecosystem to the above project and implementing validating with various proof of concept POC applications to eventually adopt them to benefit from the Big Data Hadoop initiative Architected Hadoop system pulling data from Linux systems and RDBMS database on a regular basis in order to ingest data using Sqoop Installation configuration supporting and managing Hadoop Clusters using Apache Cloudera CDH4 distributions and on Amazon web services AWS Developed MapReduce programs to parse the raw data populate staging tables and store the refined data in partitioned tables in the EDW Worked on Cloudera Hadoop Upgrades and Patches and Installation of Ecosystem Products through Cloudera manager along with Cloudera Manager Upgrade Created Hive queries that helped market analysts spot emerging trends by comparing fresh data with EDW reference tables and historical metrics Setting an Amazon Web Services AWS EC2 instance for the Cloudera Manager server Enabled speedy reviews and first mover advantages by using Oozie to automate data loading into the Hadoop Distributed File System and PIG to preprocess the data Provided design recommendations and thought leadership to sponsorsstakeholders that improved review processes and resolved technical problems Identify query duplication complexity and dependency to minimize migration efforts Technology stack Oracle Hortonworks HDP cluster Attunity Visibility Cloudera Navigator Optimizer AWS Cloud and Dynamo DB Shared responsibility for administration of Hadoop Hive and Pig and managed and reviewed Hadoop log files and updating the configuration on each host Worked with Spark eco system using Scala Python and HIVE Queries on different data formats like Text file and parquet Tested raw data and executed performance scripts and configuring Cloudera Manager Agent heartbeat interval and timeouts Worked with teams in setting up AWS EC2 instances by using different AWS services like S3 EBS Elastic Load Balancer and Auto scaling groups VPC subnets and CloudWatch Implemented CDH3 Hadoop cluster on RedHat Enterprise Linux 64 assisted with performance tuning and monitoring Monitoring Hadoop Cluster through Cloudera Manager and Implementing alerts based on Error messages Used Spark Streaming API with Kafka to build live dashboards Worked on Transformations actions in RDD Spark Streaming Pair RDD Operations Checkpointing and SBT Providing reports to management on Cluster Usage Metrics and related HBase tables to load large sets of structured semistructured and unstructured data coming from UNIX NoSQL and a variety of portfolios Involved in different Hadoop distributions like Cloudera CDH3 CDH4 and Horton Works Distributions HDP and MapR Performed installation upgrade and configure tasks for impala on all machines in a cluster and supported codedesign analysis strategy development and project planning Created reports for the BI team using Sqoop to export data into HDFS and Hive and assisted with data capacity planning and node forecasting Managing Amazon Web Services AWS infrastructure with automation and configuration Administrator for Pig Hive and HBase installing updates patches and upgrades and performed both major and minor upgrades to the existing CDH cluster and upgraded the Hadoop cluster from CDH3 to CDH4 Developed a process for the Batch ingestion of CSV Files Sqoop from different sources and also generating views on the data source using Shell Scripting and Python Environment Hadoop HDFS Map Reduce Hive HBase Zookeeper Impala Java jdk16 Cloudera Oracle SQL Server UNIX Shell Scripting Flume Oozie Scala Spark ETL Sqoop Python kafka PySpark AWS S3 MongoDB Oracle SQL Hortonworks XML RedHat Linux 64 Java Developer Fresh Desk Chennai Tamil Nadu November 2015 to November 2015 Roles Responsibilities Implemented MultiThreaded Environment and used most of the interfaces under the collection framework by using Core Java Concepts Developed Graphical User Interfaces by using JSF JSP HTML DHTML Angularjs CSS and JavaScript and developed scripts in python for Financial Data coming from SQL Developer based on the requirements specified Implemented several JavaJ2EE design patterns like Spring MVC Singleton Spring Dependency Injection and Data Transfer Object Used JAXWS SOAP for producing web services and involved in writing programs to consume the web services using SOA with CXF framework and developed few web pages using JSP JSTL HTML CSS Java script Ajax and JSON Implemented business logic data exchange XML processing and created graphics using Python and Django Wrote code to fetch data from Web services using JQUERY AJAX via JSON response and updating the HTML pages and developed high traffic web applications using HTML CSS and JavaScript jQuery Bootstrap Ext JS AngularJS Nodejs and reactjs Write SQL queries and create PLSQL functionsprocedurespackages that are optimized for APEX and improve performance and response times of APEX pages and reports Used JQuery library NodeJS and AngularJS for creation of powerful dynamic WebPages and web applications by using its advanced and cross browser functionality Used Java Server Pages for content layout and presentation with Python and Extracted and loaded data using Python scripts and PLSQL packages Worked with various frameworks of JavaScript like BackboneJS AngularJS and EmberJS etc Written with objectoriented Python Flask SQL Beautiful Soup httplib2 Jinja2 HTMLCSS Bootstrap jQuery Linux Sublime Text GIT Developed GUI using JSP Struts HTML3 CSS3 XHTML JQuery Swing and JavaScript to simplify the complexities of the application Wrote and executed various MYSQL database queries from python using PythonMySQL connector and MySQLdb package and generated Python Django forms to record data of online users and used PyTest for writing test cases Developed and coordinated complex high quality solutions to clients using J2SE J2EE Servlets JSP HTML Struts Spring MVC SOAP JavaScript JQuery JSON and XML Exposed business functionality to external systems Interoperable clients using Web Services WSDLSOAP ApacheAxis Used Hibernate objectrelationalmapping ORM solution technique of mapping data representation from MVC model to Oracle10 g Relational data model with a SQLbased schema and mapped using Hibernate Annotations Skilled in using collections in Python for manipulating and looping through different user defined objects Designed and developed intranet web applications using Ext JS Reactjs JavaScript and CSS and developed Merge jobs in Python to extract and load data into MySQL and Mango Db database Worked on Oracle SQL Server as the backend databases and integrated with Hibernate to retrieve Data Access Objects Developed Python batch processors to consume and produce various feeds and developed entire frontend and backend modules using Python on Django Web Framework and developed Business Logic using Python on Django Web Framework Designed and documented RESTHTTP APIs including JSON data formats and API versioning strategy and wrote Apex triggers apex classes developing Visual force pages batch classes Involved in AJAX driven application by invoking web servicesAPI and parsing the JSON response and involved in writing application level code to interact with APIs Web Services using JSON EnvironmentPython Django Java JSF MVC Spring IOC APEX Ruby on Rails Spring JDBC Hibernate ActiveMQ Log4j Ant MySQL JDK 16 J2EE JSP Servlets HTML LDAP Salesforce ESB Mule JDBC MongoDB DAO EJB 30 PLSQL reactjs Web Sphere Eclipse AngularJS and CVS Education Bachelors Skills JAVA 2 years LINUX 2 years ORACLE 2 years PYTHON 2 years SQL 2 years Additional Information Technical Competencies Hadoop Ecosystem ToolsMapReduce HDFS Pig Hive HBase Sqoop Zookeeper Oozie Hue Storm Kafka Spark Flume Languages Java core java HTML Programming C C Databases MySQL Oracle SQL Server MongoDB Platforms Linux RHEL Ubuntu open Solaris AIX Scripting Languages Shell Scripting HTML scripting Python Puppet Web Servers Apache Tomcat JBOSS windows server2003 2008 and 2012 Cluster Management Tools HDP Ambari Cloudera Manager Hue Solr Cloud",
    "extracted_keywords": [
        "Sr",
        "Hadoop",
        "Developer",
        "Sr",
        "Hadoop",
        "span",
        "lDeveloperspan",
        "Sr",
        "Hadoop",
        "Developer",
        "years",
        "IT",
        "experience",
        "Years",
        "Big",
        "Data",
        "Hadoop",
        "Ecosystems",
        "experience",
        "ingestion",
        "storage",
        "processing",
        "analysis",
        "data",
        "Handson",
        "experience",
        "Hadoop",
        "clusters",
        "Amazon",
        "AWS",
        "EMR",
        "S2",
        "S3",
        "Redshift",
        "Cassandra",
        "MangoDB",
        "CosmosDB",
        "SimpleDB",
        "DynamoDB",
        "Postgresql",
        "SQL",
        "MS",
        "SQL",
        "Experience",
        "Hadoop",
        "Administration",
        "activities",
        "installation",
        "configuration",
        "management",
        "clusters",
        "Cloudera",
        "CDH4",
        "CDH5",
        "Hortonworks",
        "HDP",
        "Distributions",
        "Cloudera",
        "Manager",
        "Ambari",
        "Hands",
        "experience",
        "configuring",
        "Hadoop",
        "ecosystem",
        "components",
        "MapReduce",
        "Hive",
        "Impala",
        "Sqoop",
        "Pig",
        "Oozie",
        "Zookeeper",
        "Spark",
        "Solr",
        "Hue",
        "Flume",
        "Storm",
        "Kafka",
        "Yarn",
        "distributions",
        "Knowledge",
        "experience",
        "Amazon",
        "AWS",
        "concepts",
        "EMR",
        "EC2",
        "web",
        "services",
        "processing",
        "Big",
        "Data",
        "performance",
        "tuning",
        "Yarn",
        "Spark",
        "Hiveandexperienced",
        "MapReduce",
        "Programs",
        "Apache",
        "Hadoop",
        "data",
        "requirement",
        "Spark",
        "Scala",
        "cluster",
        "analytics",
        "top",
        "Hadoop",
        "application",
        "use",
        "Spark",
        "Hive",
        "SQLOracle",
        "Expert",
        "data",
        "ecosystem",
        "Hadoop",
        "Spark",
        "Kafka",
        "data",
        "systems",
        "cloud",
        "platforms",
        "Amazon",
        "CLoud",
        "Microsoft",
        "Azure",
        "Google",
        "Cloud",
        "Platform",
        "data",
        "HDFS",
        "Relational",
        "Database",
        "Management",
        "systems",
        "Sqoop",
        "issues",
        "experience",
        "distributions",
        "Hadoop",
        "enterprise",
        "versions",
        "Cloudera",
        "CDH4CDH5",
        "Hortonworks",
        "knowledge",
        "MapR",
        "distribution",
        "IBM",
        "Big",
        "Insights",
        "Amazons",
        "EMR",
        "Elastic",
        "MapReduce",
        "Exposure",
        "Data",
        "Lake",
        "Implementation",
        "Apache",
        "Spark",
        "Data",
        "pipe",
        "lines",
        "business",
        "logics",
        "Sparkandused",
        "Scala",
        "Python",
        "HiveSQL",
        "queries",
        "RDD",
        "transformations",
        "Apache",
        "Spark",
        "Good",
        "Understanding",
        "experience",
        "NameNode",
        "HA",
        "architecture",
        "experience",
        "health",
        "cluster",
        "Ambari",
        "Nagios",
        "Ganglia",
        "Cronjobs",
        "Cluster",
        "maintenance",
        "Decommissioning",
        "Data",
        "Nodes",
        "understandingknowledge",
        "Hadoop",
        "Architecture",
        "components",
        "HDFS",
        "Job",
        "Tracker",
        "Task",
        "Tracker",
        "NameNode",
        "DataNode",
        "MapReduce",
        "concepts",
        "implementation",
        "security",
        "controls",
        "Kerberos",
        "principals",
        "ACLs",
        "Data",
        "encryptions",
        "DMCrypt",
        "Hadoop",
        "clusters",
        "spark",
        "components",
        "Spark",
        "SQL",
        "MLib",
        "streaming",
        "GraphX",
        "Expertise",
        "installation",
        "administration",
        "configuration",
        "performance",
        "tuning",
        "troubleshooting",
        "hat",
        "Linux",
        "SUSE",
        "CentOS",
        "AIX",
        "Solaris",
        "Schedule",
        "Recurring",
        "Hadoop",
        "Jobs",
        "Apache",
        "Oozie",
        "experience",
        "Jumpstart",
        "Kickstart",
        "Infrastructure",
        "setup",
        "Installation",
        "Methods",
        "Linux",
        "knowledge",
        "troubleshooting",
        "skills",
        "understanding",
        "systems",
        "capacity",
        "basics",
        "memory",
        "CPU",
        "OS",
        "storage",
        "network",
        "Experience",
        "administration",
        "activities",
        "RDBMS",
        "data",
        "bases",
        "MS",
        "SQL",
        "Server",
        "Hadoop",
        "Distributed",
        "File",
        "System",
        "Ecosystem",
        "MapReduce",
        "Pig",
        "Hive",
        "Sqoop",
        "YARN",
        "MongoDB",
        "HBase",
        "knowledge",
        "NoSQL",
        "HBase",
        "Cassandra",
        "strengths",
        "familiarity",
        "software",
        "systems",
        "ability",
        "technologies",
        "environments",
        "learner",
        "communication",
        "skills",
        "Work",
        "Experience",
        "Sr",
        "Hadoop",
        "Developer",
        "Cincinnati",
        "OH",
        "June",
        "Present",
        "Roles",
        "Responsibilities",
        "Big",
        "Data",
        "Architecture",
        "Team",
        "foundation",
        "Enterprise",
        "Analytics",
        "initiative",
        "Hadoopbased",
        "Data",
        "Lake",
        "Created",
        "multinode",
        "Hadoop",
        "Spark",
        "clusters",
        "AWS",
        "instances",
        "terabytes",
        "data",
        "AWS",
        "HDFS",
        "Apache",
        "Hadoop",
        "clusters",
        "application",
        "development",
        "Hadoop",
        "tools",
        "Hive",
        "Pig",
        "HBase",
        "Zookeeper",
        "Sqoop",
        "time",
        "feed",
        "Kafka",
        "Spark",
        "streaming",
        "process",
        "data",
        "form",
        "Data",
        "Frame",
        "data",
        "Parquet",
        "format",
        "HDFS",
        "data",
        "pipeline",
        "Flume",
        "Sqoop",
        "Pig",
        "Java",
        "map",
        "customer",
        "data",
        "histories",
        "HDFS",
        "analysis",
        "amounts",
        "log",
        "data",
        "Apache",
        "Flume",
        "data",
        "HDFS",
        "analysis",
        "Hadoop",
        "cluster",
        "CDH47",
        "CDH52",
        "cluster",
        "decommissioning",
        "Data",
        "Nodes",
        "NameNode",
        "recovery",
        "capacity",
        "planning",
        "slots",
        "configuration",
        "Spark",
        "scripts",
        "files",
        "Amazon",
        "S3",
        "buckets",
        "data",
        "sources",
        "HDFSHBaseintoSparkRDD",
        "migration",
        "ETL",
        "processes",
        "Oracle",
        "Hive",
        "data",
        "manipulation",
        "data",
        "Oracle",
        "DB2",
        "HDFS",
        "HIVE",
        "Sqoop",
        "Worked",
        "Installing",
        "Cloudera",
        "Manager",
        "CDH",
        "JCE",
        "Policy",
        "File",
        "Kerberos",
        "Principal",
        "Cloudera",
        "Manager",
        "Server",
        "Kerberos",
        "Wizard",
        "Developed",
        "Spark",
        "jobs",
        "Scala",
        "Python",
        "top",
        "Batch",
        "Analysis",
        "cluster",
        "performance",
        "networking",
        "data",
        "integrity",
        "issues",
        "troubleshooting",
        "issues",
        "execution",
        "MapReduce",
        "jobs",
        "log",
        "files",
        "Linux",
        "Bash",
        "scripts",
        "users",
        "groups",
        "data",
        "distribution",
        "capacity",
        "planning",
        "system",
        "Install",
        "OS",
        "Hadoop",
        "stack",
        "CDH5",
        "YARN",
        "Cloudera",
        "distribution",
        "configuration",
        "management",
        "monitoring",
        "debugging",
        "performance",
        "Supported",
        "MapReduce",
        "Programs",
        "applications",
        "Hadoop",
        "scripting",
        "Hadoop",
        "package",
        "installation",
        "configuration",
        "deployments",
        "onpremises",
        "application",
        "AWS",
        "AWS",
        "services",
        "EC2",
        "S3",
        "data",
        "sets",
        "processing",
        "storage",
        "ELASTIC",
        "MAPREDUCE",
        "setup",
        "Hadoop",
        "environment",
        "AWS",
        "EC2",
        "Instances",
        "systems",
        "engineering",
        "team",
        "Hadoop",
        "environments",
        "Hadoop",
        "clusters",
        "maintenance",
        "monitoring",
        "deployments",
        "upgrades",
        "infrastructure",
        "Hadoop",
        "clustersand",
        "Hive",
        "analysis",
        "transforming",
        "files",
        "formats",
        "text",
        "files",
        "Hive",
        "External",
        "tables",
        "data",
        "tables",
        "query",
        "data",
        "HQL",
        "application",
        "teams",
        "operating",
        "system",
        "Hadoop",
        "updates",
        "version",
        "upgrades",
        "Worked",
        "deal",
        "AWS",
        "Cloud",
        "services",
        "EC2",
        "S3",
        "EBS",
        "RDS",
        "VPC",
        "Monitoring",
        "Hadoopcluster",
        "tools",
        "Nagios",
        "Ganglia",
        "Cloudera",
        "Manager",
        "Cluster",
        "removing",
        "nodes",
        "tools",
        "Ganglia",
        "Nagios",
        "Cloudera",
        "Manager",
        "Hive",
        "data",
        "analysis",
        "transforming",
        "files",
        "formats",
        "text",
        "files",
        "MapReduce",
        "programs",
        "Spark",
        "transformations",
        "Spark",
        "Scala",
        "python",
        "PySpark",
        "Environment",
        "Hadoop",
        "MapReduce",
        "Hive",
        "PIG",
        "Sqoop",
        "Python",
        "Spark",
        "SparkStreaming",
        "Spark",
        "SQL",
        "AWS",
        "EMR",
        "S3",
        "AWS",
        "Python",
        "Scala",
        "Pyspark",
        "MapR",
        "Java",
        "Oozie",
        "Flume",
        "HBase",
        "Nagios",
        "Ganglia",
        "Hue",
        "Cloudera",
        "Manager",
        "Zookeeper",
        "Cloudera",
        "Oracle",
        "Kerberos",
        "RedHat",
        "Sr",
        "Hadoop",
        "Developer",
        "AMEX",
        "Phoenix",
        "AZ",
        "May",
        "May",
        "Roles",
        "Responsibilities",
        "Collaborate",
        "problems",
        "constraints",
        "root",
        "causes",
        "data",
        "sets",
        "solution",
        "support",
        "Hadoop",
        "HDFS",
        "MapReduce",
        "Pig",
        "Hive",
        "Hbase",
        "reports",
        "Tableau",
        "Architect",
        "Hadoop",
        "cluster",
        "Pseudo",
        "Mode",
        "Zookeeper",
        "Apache",
        "andstoring",
        "data",
        "HDFS",
        "AmazonAWSS3",
        "tables",
        "AWS",
        "cluster",
        "S3",
        "storage",
        "infrastructure",
        "systems",
        "technologies",
        "gap",
        "analysis",
        "requirements",
        "evaluation",
        "recommendations",
        "system",
        "upgrades",
        "technologies",
        "architecture",
        "specifications",
        "recommendations",
        "Apache",
        "Hadoop",
        "clusters",
        "application",
        "development",
        "Hadoop",
        "tools",
        "Hive",
        "Pig",
        "HBase",
        "Zookeeper",
        "Sqoop",
        "Installed",
        "Configured",
        "Sqoop",
        "data",
        "MapRFS",
        "HBase",
        "Hive",
        "Relational",
        "databases",
        "MapR",
        "Hadoop",
        "environments",
        "cluster",
        "performance",
        "tuning",
        "enterprise",
        "environment",
        "Configured",
        "MapRzookeeper",
        "MapRcldb",
        "MapPjobtracker",
        "MapRtasktracker",
        "MapRresourcemanager",
        "MapRnode",
        "manager",
        "MapRfileserver",
        "MapRwebserver",
        "Installed",
        "Knox",
        "gateway",
        "HIVE",
        "ODBC",
        "WebHcat",
        "Oozie",
        "services",
        "Load",
        "data",
        "databases",
        "MapRFS",
        "filesystem",
        "HBase",
        "Sqoop",
        "MapR",
        "metrics",
        "NoSQL",
        "database",
        "metrics",
        "data",
        "monitoring",
        "analysis",
        "MapReducejob",
        "executions",
        "cluster",
        "task",
        "level",
        "Hadoop",
        "clusters",
        "components",
        "performance",
        "data",
        "pipeline",
        "Flume",
        "Sqoop",
        "Pig",
        "Java",
        "MapReduce",
        "customer",
        "data",
        "HDFS",
        "analysis",
        "Integrated",
        "HDP",
        "clusters",
        "Active",
        "Directory",
        "Kerberos",
        "Authentication",
        "decommissioning",
        "Data",
        "Nodes",
        "NameNode",
        "recovery",
        "capacity",
        "planning",
        "Oozie",
        "workflow",
        "engine",
        "Hive",
        "Pig",
        "Jobs",
        "Data",
        "Model",
        "HBase",
        "Oracle",
        "Data",
        "model",
        "High",
        "Availability",
        "infrastructure",
        "point",
        "failure",
        "Name",
        "node",
        "zookeeper",
        "services",
        "Leveraged",
        "Chef",
        "builds",
        "environments",
        "hardware",
        "software",
        "installation",
        "production",
        "cluster",
        "teams",
        "Hadoop",
        "cluster",
        "MCS",
        "NoSQL",
        "databases",
        "HBase",
        "Hive",
        "Hive",
        "tables",
        "data",
        "loading",
        "Hive",
        "UDFs",
        "Linux",
        "server",
        "admin",
        "team",
        "server",
        "hardware",
        "operating",
        "system",
        "data",
        "analysts",
        "solutions",
        "analysis",
        "tasks",
        "Hadoop",
        "HBase",
        "files",
        "data",
        "databases",
        "Sqoop",
        "visualization",
        "reports",
        "data",
        "Oracle",
        "HDFS",
        "HIVE",
        "Sqoop",
        "Collaborating",
        "application",
        "teams",
        "operating",
        "system",
        "Hadoop",
        "updates",
        "version",
        "upgrades",
        "Automated",
        "workflows",
        "shell",
        "scripts",
        "data",
        "databases",
        "Hadoop",
        "Supported",
        "QA",
        "environment",
        "configurations",
        "scripts",
        "Pig",
        "Sqoop",
        "EnvironmentHadoop",
        "HDFS",
        "Map",
        "Reduce",
        "Hive",
        "HBase",
        "Kafka",
        "Zookeeper",
        "Oozie",
        "Impala",
        "Java",
        "Cloudera",
        "Oracle",
        "Teradata",
        "SQL",
        "Server",
        "Python",
        "UNIX",
        "Shell",
        "Scripting",
        "ETL",
        "Flume",
        "Scala",
        "Spark",
        "Sqoop",
        "Python",
        "S3",
        "EC2",
        "Kafka",
        "Oracle",
        "MySQL",
        "Hortonworks",
        "YARN",
        "Python",
        "Hadoop",
        "Developer",
        "Charles",
        "Schwab",
        "Littleton",
        "CO",
        "January",
        "May",
        "Roles",
        "Responsibilities",
        "suitability",
        "Hadoop",
        "ecosystem",
        "project",
        "proof",
        "concept",
        "POC",
        "applications",
        "Big",
        "Data",
        "Hadoop",
        "initiative",
        "Architected",
        "Hadoop",
        "system",
        "data",
        "Linux",
        "systems",
        "RDBMS",
        "database",
        "basis",
        "order",
        "data",
        "Sqoop",
        "Installation",
        "configuration",
        "Hadoop",
        "Clusters",
        "Apache",
        "Cloudera",
        "CDH4",
        "distributions",
        "Amazon",
        "web",
        "services",
        "AWS",
        "MapReduce",
        "programs",
        "data",
        "staging",
        "tables",
        "data",
        "tables",
        "EDW",
        "Cloudera",
        "Hadoop",
        "Upgrades",
        "Patches",
        "Installation",
        "Ecosystem",
        "Products",
        "Cloudera",
        "manager",
        "Cloudera",
        "Manager",
        "Upgrade",
        "Created",
        "Hive",
        "queries",
        "market",
        "analysts",
        "trends",
        "data",
        "EDW",
        "reference",
        "tables",
        "metrics",
        "Amazon",
        "Web",
        "Services",
        "AWS",
        "EC2",
        "instance",
        "Cloudera",
        "Manager",
        "server",
        "reviews",
        "advantages",
        "Oozie",
        "data",
        "loading",
        "Hadoop",
        "Distributed",
        "File",
        "System",
        "PIG",
        "data",
        "design",
        "recommendations",
        "leadership",
        "sponsorsstakeholders",
        "review",
        "processes",
        "problems",
        "query",
        "duplication",
        "complexity",
        "dependency",
        "migration",
        "efforts",
        "Technology",
        "stack",
        "Oracle",
        "Hortonworks",
        "HDP",
        "cluster",
        "Attunity",
        "Visibility",
        "Cloudera",
        "Navigator",
        "Optimizer",
        "AWS",
        "Cloud",
        "Dynamo",
        "DB",
        "responsibility",
        "administration",
        "Hadoop",
        "Hive",
        "Pig",
        "Hadoop",
        "log",
        "files",
        "configuration",
        "host",
        "Spark",
        "eco",
        "system",
        "Scala",
        "Python",
        "HIVE",
        "Queries",
        "data",
        "formats",
        "Text",
        "file",
        "parquet",
        "data",
        "performance",
        "scripts",
        "Cloudera",
        "Manager",
        "Agent",
        "heartbeat",
        "interval",
        "timeouts",
        "teams",
        "AWS",
        "EC2",
        "instances",
        "AWS",
        "services",
        "S3",
        "EBS",
        "Elastic",
        "Load",
        "Balancer",
        "Auto",
        "scaling",
        "groups",
        "subnets",
        "CloudWatch",
        "CDH3",
        "Hadoop",
        "cluster",
        "RedHat",
        "Enterprise",
        "Linux",
        "performance",
        "tuning",
        "Monitoring",
        "Hadoop",
        "Cluster",
        "Cloudera",
        "Manager",
        "Implementing",
        "alerts",
        "Error",
        "messages",
        "Spark",
        "Streaming",
        "API",
        "Kafka",
        "dashboards",
        "Transformations",
        "actions",
        "RDD",
        "Spark",
        "Streaming",
        "Pair",
        "RDD",
        "Operations",
        "Checkpointing",
        "SBT",
        "Providing",
        "management",
        "Cluster",
        "Usage",
        "Metrics",
        "HBase",
        "tables",
        "sets",
        "data",
        "UNIX",
        "NoSQL",
        "variety",
        "portfolios",
        "Hadoop",
        "distributions",
        "Cloudera",
        "CDH3",
        "CDH4",
        "Horton",
        "Distributions",
        "HDP",
        "MapR",
        "Performed",
        "installation",
        "upgrade",
        "configure",
        "tasks",
        "impala",
        "machines",
        "cluster",
        "analysis",
        "strategy",
        "development",
        "project",
        "reports",
        "BI",
        "team",
        "Sqoop",
        "data",
        "HDFS",
        "Hive",
        "data",
        "capacity",
        "planning",
        "node",
        "forecasting",
        "Amazon",
        "Web",
        "Services",
        "AWS",
        "infrastructure",
        "automation",
        "configuration",
        "Administrator",
        "Pig",
        "Hive",
        "HBase",
        "updates",
        "patches",
        "upgrades",
        "upgrades",
        "CDH",
        "cluster",
        "Hadoop",
        "cluster",
        "CDH3",
        "CDH4",
        "process",
        "Batch",
        "ingestion",
        "CSV",
        "Files",
        "Sqoop",
        "sources",
        "views",
        "data",
        "source",
        "Shell",
        "Scripting",
        "Python",
        "Environment",
        "Hadoop",
        "HDFS",
        "Map",
        "Reduce",
        "Hive",
        "HBase",
        "Zookeeper",
        "Impala",
        "Java",
        "jdk16",
        "Cloudera",
        "Oracle",
        "SQL",
        "Server",
        "UNIX",
        "Shell",
        "Scripting",
        "Flume",
        "Oozie",
        "Scala",
        "Spark",
        "ETL",
        "Sqoop",
        "Python",
        "PySpark",
        "S3",
        "MongoDB",
        "Oracle",
        "SQL",
        "Hortonworks",
        "XML",
        "RedHat",
        "Linux",
        "Java",
        "Developer",
        "Fresh",
        "Desk",
        "Chennai",
        "Tamil",
        "Nadu",
        "November",
        "November",
        "Roles",
        "Responsibilities",
        "MultiThreaded",
        "Environment",
        "interfaces",
        "collection",
        "framework",
        "Core",
        "Java",
        "Concepts",
        "Developed",
        "Graphical",
        "User",
        "Interfaces",
        "JSF",
        "JSP",
        "HTML",
        "DHTML",
        "Angularjs",
        "CSS",
        "JavaScript",
        "scripts",
        "python",
        "Financial",
        "Data",
        "SQL",
        "Developer",
        "requirements",
        "JavaJ2EE",
        "design",
        "patterns",
        "Spring",
        "MVC",
        "Singleton",
        "Spring",
        "Dependency",
        "Injection",
        "Data",
        "Transfer",
        "Object",
        "JAXWS",
        "SOAP",
        "web",
        "services",
        "programs",
        "web",
        "services",
        "SOA",
        "CXF",
        "framework",
        "web",
        "pages",
        "JSP",
        "JSTL",
        "HTML",
        "CSS",
        "Java",
        "script",
        "Ajax",
        "business",
        "logic",
        "data",
        "exchange",
        "XML",
        "processing",
        "graphics",
        "Python",
        "Django",
        "Wrote",
        "code",
        "data",
        "Web",
        "services",
        "JQUERY",
        "AJAX",
        "response",
        "HTML",
        "pages",
        "traffic",
        "web",
        "applications",
        "HTML",
        "CSS",
        "JavaScript",
        "jQuery",
        "Bootstrap",
        "Ext",
        "JS",
        "AngularJS",
        "Nodejs",
        "reactjs",
        "Write",
        "SQL",
        "queries",
        "PLSQL",
        "functionsprocedurespackages",
        "APEX",
        "performance",
        "response",
        "times",
        "APEX",
        "pages",
        "reports",
        "JQuery",
        "library",
        "NodeJS",
        "AngularJS",
        "creation",
        "WebPages",
        "web",
        "applications",
        "browser",
        "functionality",
        "Java",
        "Server",
        "Pages",
        "content",
        "layout",
        "presentation",
        "Python",
        "data",
        "Python",
        "scripts",
        "PLSQL",
        "packages",
        "frameworks",
        "JavaScript",
        "BackboneJS",
        "AngularJS",
        "EmberJS",
        "Python",
        "Flask",
        "SQL",
        "Beautiful",
        "Soup",
        "httplib2",
        "Jinja2",
        "HTMLCSS",
        "Bootstrap",
        "jQuery",
        "Linux",
        "Sublime",
        "Text",
        "GIT",
        "Developed",
        "GUI",
        "JSP",
        "Struts",
        "XHTML",
        "JQuery",
        "Swing",
        "JavaScript",
        "complexities",
        "application",
        "Wrote",
        "MYSQL",
        "database",
        "python",
        "PythonMySQL",
        "connector",
        "package",
        "Python",
        "Django",
        "data",
        "users",
        "PyTest",
        "test",
        "cases",
        "quality",
        "solutions",
        "clients",
        "J2SE",
        "J2EE",
        "Servlets",
        "JSP",
        "HTML",
        "Struts",
        "Spring",
        "MVC",
        "SOAP",
        "JavaScript",
        "JQuery",
        "JSON",
        "XML",
        "business",
        "functionality",
        "systems",
        "clients",
        "Web",
        "Services",
        "WSDLSOAP",
        "ApacheAxis",
        "Hibernate",
        "ORM",
        "solution",
        "technique",
        "mapping",
        "data",
        "representation",
        "MVC",
        "model",
        "Oracle10",
        "g",
        "data",
        "model",
        "schema",
        "Hibernate",
        "Annotations",
        "collections",
        "Python",
        "user",
        "objects",
        "intranet",
        "web",
        "applications",
        "Ext",
        "JS",
        "Reactjs",
        "JavaScript",
        "CSS",
        "Merge",
        "jobs",
        "Python",
        "data",
        "MySQL",
        "Mango",
        "Db",
        "database",
        "Oracle",
        "SQL",
        "Server",
        "databases",
        "Hibernate",
        "Data",
        "Access",
        "Python",
        "batch",
        "processors",
        "feeds",
        "frontend",
        "modules",
        "Python",
        "Django",
        "Web",
        "Framework",
        "Business",
        "Logic",
        "Python",
        "Django",
        "Web",
        "Framework",
        "RESTHTTP",
        "APIs",
        "data",
        "formats",
        "API",
        "strategy",
        "Apex",
        "apex",
        "classes",
        "force",
        "pages",
        "classes",
        "AJAX",
        "application",
        "web",
        "servicesAPI",
        "response",
        "application",
        "level",
        "code",
        "APIs",
        "Web",
        "Services",
        "JSON",
        "EnvironmentPython",
        "Django",
        "Java",
        "JSF",
        "MVC",
        "Spring",
        "IOC",
        "APEX",
        "Ruby",
        "Rails",
        "Spring",
        "JDBC",
        "Hibernate",
        "ActiveMQ",
        "Log4j",
        "Ant",
        "MySQL",
        "JDK",
        "J2EE",
        "JSP",
        "Servlets",
        "HTML",
        "LDAP",
        "Salesforce",
        "Mule",
        "JDBC",
        "MongoDB",
        "DAO",
        "EJB",
        "PLSQL",
        "reactjs",
        "Web",
        "Sphere",
        "Eclipse",
        "AngularJS",
        "CVS",
        "Education",
        "Bachelors",
        "Skills",
        "JAVA",
        "years",
        "LINUX",
        "years",
        "ORACLE",
        "years",
        "PYTHON",
        "years",
        "SQL",
        "years",
        "Additional",
        "Information",
        "Technical",
        "Competencies",
        "Hadoop",
        "Ecosystem",
        "HDFS",
        "Pig",
        "Hive",
        "HBase",
        "Sqoop",
        "Zookeeper",
        "Oozie",
        "Hue",
        "Storm",
        "Kafka",
        "Spark",
        "Flume",
        "Languages",
        "Java",
        "core",
        "HTML",
        "Programming",
        "C",
        "C",
        "MySQL",
        "Oracle",
        "SQL",
        "Server",
        "MongoDB",
        "Platforms",
        "Linux",
        "RHEL",
        "Ubuntu",
        "Solaris",
        "AIX",
        "Scripting",
        "Languages",
        "Shell",
        "Scripting",
        "HTML",
        "Python",
        "Puppet",
        "Web",
        "Servers",
        "Apache",
        "Tomcat",
        "JBOSS",
        "server2003",
        "Cluster",
        "Management",
        "Tools",
        "HDP",
        "Ambari",
        "Cloudera",
        "Manager",
        "Hue",
        "Solr",
        "Cloud"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T20:09:54.396495",
    "resume_data": "Sr Hadoop Developer Sr Hadoop span lDeveloperspan Sr Hadoop Developer 8451 Over 7 years of professional IT experience with 3 Years of Big Data Hadoop Ecosystems experience in ingestion storage querying processing and analysis of big data Handson experience architecting and implementing Hadoop clusters on Amazon AWS using EMR S2 S3 Redshift Cassandra MangoDB CosmosDB SimpleDB AmazonRDS DynamoDB Postgresql SQL MS SQL Experience in Hadoop Administration activities such as installation configuration and management of clusters in Cloudera CDH4 CDH5 Hortonworks HDP Distributions using Cloudera Manager Ambari Hands on experience in installing configuring and using Hadoop ecosystem components like HDFS MapReduce Hive Impala Sqoop Pig Oozie Zookeeper Spark Solr Hue Flume Storm Kafka and Yarn distributions Very good Knowledge and experience in Amazon AWS concepts like EMR and EC2 web services which provides fast and efficient processing of Big Data Experienced in performance tuning of Yarn Spark and Hiveandexperienced in developing MapReduce Programs using Apache Hadoop for analyzing the big data as per the requirement Extensively worked on Spark using Scala on cluster for computational analytics installed it on top of Hadoop performed advanced analytical application by making use of Spark with Hive and SQLOracle Expert in big data ecosystem using Hadoop Spark Kafka with columnoriented big data systems on cloud platforms such as Amazon CLoud AWS Microsoft Azure and Google Cloud Platform Experienced in importing exporting data between HDFS and Relational Database Management systems using Sqoop and troubleshooting for any issues Extensive experience in working with various distributions of Hadoop like enterprise versions of Cloudera CDH4CDH5 Hortonworks and good knowledge on MapR distribution IBM Big Insights and Amazons EMR Elastic MapReduce Exposure to Data Lake Implementation using Apache Spark and developed Data pipe lines and applied business logics using Sparkandused Scala and Python to convert HiveSQL queries into RDD transformations in Apache Spark Good Understanding and experience on NameNode HA architecture and experience in monitoring the health of cluster using Ambari Nagios Ganglia and Cronjobs Experienced in Cluster maintenance and Commissioning Decommissioning of Data Nodes and good understandingknowledge of Hadoop Architecture and various components such as HDFS Job Tracker and Task Tracker NameNode DataNode and MapReduce concepts Experienced in implementation of security controls using Kerberos principals ACLs Data encryptions using DMCrypt to protect entire Hadoop clusters Wellversed in spark components like Spark SQL MLib Spark streaming and GraphX Expertise in installation administration patches upgrade configuration performance tuning and troubleshooting of Red hat Linux SUSE CentOS AIX Solaris Experienced Schedule Recurring Hadoop Jobs with Apache Oozie and experience in Jumpstart Kickstart Infrastructure setup and Installation Methods for Linux Good knowledge in troubleshooting skills understanding of systems capacity bottlenecks basics of memory CPU OS storage and network Experience in administration activities of RDBMS data bases such as MS SQL Server Experienced in Hadoop Distributed File System and Ecosystem MapReduce Pig Hive Sqoop YARN MongoDB and HBase and knowledge of NoSQL databases such as HBase Cassandra and MongoDB Major strengths are familiarity with multiple software systems ability to learn quickly new technologies adapt to new environments focused adaptive and quick learner with excellent interpersonal technical and communication skills Work Experience Sr Hadoop Developer 8451 Cincinnati OH June 2018 to Present Roles Responsibilities Worked directly with the Big Data Architecture Team which created the foundation of this Enterprise Analytics initiative in a Hadoopbased Data Lake Created multinode Hadoop and Spark clusters in AWS instances to generate terabytes of data and stored it in AWS HDFS InstalledConfiguredMaintained Apache Hadoop clusters for application development and Hadoop tools like Hive Pig HBase Zookeeper and Sqoop Extracted real time feed using Kafka and Spark streaming and convert it to RDD and process data in the form of Data Frame and save the data as Parquet format in HDFS Developed data pipeline using Flume Sqoop Pig and Java map reduce to ingest customer behavioral data and financial histories into HDFS for analysis Involved in collecting and aggregating large amounts of log data using Apache Flume and staging data in HDFS for further analysis Upgraded the Hadoop cluster from CDH47 to CDH52 and worked on installing cluster commissioning decommissioning of Data Nodes NameNode recovery capacity planning and slots configuration Developed Spark scripts to import large files from Amazon S3 buckets and imported the data from different sources like HDFSHBaseintoSparkRDD Involved in migration of ETL processes from Oracle to Hive to test the easy data manipulation and worked on importing and exporting data from Oracle and DB2 into HDFS and HIVE using Sqoop Worked on Installing Cloudera Manager CDH and install the JCE Policy File to Create a Kerberos Principal for the Cloudera Manager Server enabling Kerberos Using the Wizard Developed Spark jobs using Scala and Python on top of YarnMRv2 for interactive and Batch Analysis Monitored cluster for performance and networking and data integrity issues and responsible for troubleshooting issues in the execution of MapReduce jobs by inspecting and reviewing log files Created 25 Linux Bash scripts for users groups data distribution capacity planning and system monitoring Install OS and administrated Hadoop stack with CDH5 with YARN Cloudera distribution including configuration management monitoring debugging and performance tuning Supported MapReduce Programs and distributed applications running on the Hadoop clusterand scripting Hadoop package installation and configuration to support fullyautomated deployments Migrated existing onpremises application to AWS and used AWS services like EC2 and S3 for large data sets processing and storage and worked with ELASTIC MAPREDUCE and setup Hadoop environment in AWS EC2 Instances Worked with systems engineering team to plan and deploy new Hadoop environments and expand existing Hadoop clusters Perform maintenance monitoring deployments and upgrades across infrastructure that supports all our Hadoop clustersand worked on Hive for further analysis and for generating transforming files from different analytical formats to text files Created Hive External tables and loaded the data in to tables and query data using HQL and worked with application teams to install operating system Hadoop updates patches version upgrades as required Worked and learned a great deal from AWS Cloud services like EC2 S3 EBS RDS and VPC Monitoring Hadoopcluster using tools like Nagios Ganglia and Cloudera Manager and maintaining the Cluster by adding and removing of nodes using tools like Ganglia Nagios and Cloudera Manager Worked on Hive for exposing data for further analysis and for generating transforming files from different analytical formats to text files Worked on migrating MapReduce programs into Spark transformations using Spark and Scala initially done using python PySpark Environment Hadoop MapReduce Hive PIG Sqoop Python Spark SparkStreaming Spark SQL AWS EMR AWS S3 AWS Redshift Python Scala Pyspark MapR Java Oozie Flume HBase Nagios Ganglia Hue Cloudera Manager Zookeeper Cloudera Oracle Kerberos and RedHat 65 Sr Hadoop Developer AMEX Phoenix AZ May 2018 to May 2018 Roles Responsibilities Collaborate in identifying the current problems constraints and root causes with data sets to identify the descriptive and predictive solution with support of the Hadoop HDFS MapReduce Pig Hive and Hbase and further to develop reports in Tableau Architect the Hadoop cluster in Pseudo distributed Mode working with Zookeeper and Apache andstoring and loading the data from HDFS to AmazonAWSS3 and backing up and Created tables in AWS cluster with S3 storage Evaluated existing infrastructure systems and technologies and provided gap analysis and documented requirements evaluation and recommendations of system upgrades technologies and created proposed architecture and specifications along with recommendations InstalledConfiguredMaintained Apache Hadoop clusters for application development and Hadoop tools like Hive Pig HBase Zookeeper and Sqoop Installed and Configured Sqoop to import and export the data into MapRFS HBase and Hive from Relational databases Administering large MapR Hadoop environments build and support cluster set up performance tuning and monitoring in an enterprise environment Installed and Configured MapRzookeeper MapRcldb MapPjobtracker MapRtasktracker MapRresourcemanager MapRnode manager MapRfileserver and MapRwebserver Installed and configured Knox gateway to secure HIVE through ODBC WebHcat and Oozie services Load data from relational databases into MapRFS filesystem and HBase using Sqoop and setting up MapR metrics with NoSQL database to log metrics data Close monitoring and analysis of the MapReducejob executions on cluster at task level and optimized Hadoop clusters components to achieve high performance Developed data pipeline using Flume Sqoop Pig and Java MapReduce to ingest customer behavioral data into HDFS for analysis Integrated HDP clusters with Active Directory and enabled Kerberos for Authentication Worked on commissioning decommissioning of Data Nodes NameNode recovery capacity planning and installed Oozie workflow engine to run multiple Hive and Pig Jobs Worked on creating the Data Model for HBase from the current Oracle Data model Implemented High Availability and automatic failover infrastructure to overcome single point of failure for Name node utilizing zookeeper services Leveraged Chef to manage and maintain builds in various environments and planned for hardware and software installation on production cluster and communicated with multiple teams to get it done Monitoring the Hadoop cluster functioning through MCS and worked on NoSQL databases including HBase Used Hive and created Hive tables and involved in data loading and writing Hive UDFs and worked with Linux server admin team in administering the server hardware and operating system Worked closely with data analysts to construct creative solutions for their analysis tasks and managed and reviewed Hadoop and HBase log files Exported the analyzed data to the relational databases using Sqoop for visualization and to generate reports and worked on importing and exporting data from Oracle into HDFS and HIVE using Sqoop Collaborating with application teams to install operating system and Hadoop updates patches version upgrades when required Automated workflows using shell scripts pull data from various databases into Hadoop Supported in setting up QA environment and updating configurations for implementing scripts with Pig and Sqoop EnvironmentHadoop HDFS Map Reduce Hive HBase Kafka Zookeeper Oozie Impala Java Cloudera Oracle Teradata SQL Server Python UNIX Shell Scripting ETL Flume Scala Spark Sqoop Python AWS S3 EC2 Kafka Oracle MySQL Hortonworks YARN Python Hadoop Developer Charles Schwab Littleton CO January 2016 to May 2017 Roles Responsibilities Evaluated suitability of Hadoop and its ecosystem to the above project and implementing validating with various proof of concept POC applications to eventually adopt them to benefit from the Big Data Hadoop initiative Architected Hadoop system pulling data from Linux systems and RDBMS database on a regular basis in order to ingest data using Sqoop Installation configuration supporting and managing Hadoop Clusters using Apache Cloudera CDH4 distributions and on Amazon web services AWS Developed MapReduce programs to parse the raw data populate staging tables and store the refined data in partitioned tables in the EDW Worked on Cloudera Hadoop Upgrades and Patches and Installation of Ecosystem Products through Cloudera manager along with Cloudera Manager Upgrade Created Hive queries that helped market analysts spot emerging trends by comparing fresh data with EDW reference tables and historical metrics Setting an Amazon Web Services AWS EC2 instance for the Cloudera Manager server Enabled speedy reviews and first mover advantages by using Oozie to automate data loading into the Hadoop Distributed File System and PIG to preprocess the data Provided design recommendations and thought leadership to sponsorsstakeholders that improved review processes and resolved technical problems Identify query duplication complexity and dependency to minimize migration efforts Technology stack Oracle Hortonworks HDP cluster Attunity Visibility Cloudera Navigator Optimizer AWS Cloud and Dynamo DB Shared responsibility for administration of Hadoop Hive and Pig and managed and reviewed Hadoop log files and updating the configuration on each host Worked with Spark eco system using Scala Python and HIVE Queries on different data formats like Text file and parquet Tested raw data and executed performance scripts and configuring Cloudera Manager Agent heartbeat interval and timeouts Worked with teams in setting up AWS EC2 instances by using different AWS services like S3 EBS Elastic Load Balancer and Auto scaling groups VPC subnets and CloudWatch Implemented CDH3 Hadoop cluster on RedHat Enterprise Linux 64 assisted with performance tuning and monitoring Monitoring Hadoop Cluster through Cloudera Manager and Implementing alerts based on Error messages Used Spark Streaming API with Kafka to build live dashboards Worked on Transformations actions in RDD Spark Streaming Pair RDD Operations Checkpointing and SBT Providing reports to management on Cluster Usage Metrics and related HBase tables to load large sets of structured semistructured and unstructured data coming from UNIX NoSQL and a variety of portfolios Involved in different Hadoop distributions like Cloudera CDH3 CDH4 and Horton Works Distributions HDP and MapR Performed installation upgrade and configure tasks for impala on all machines in a cluster and supported codedesign analysis strategy development and project planning Created reports for the BI team using Sqoop to export data into HDFS and Hive and assisted with data capacity planning and node forecasting Managing Amazon Web Services AWS infrastructure with automation and configuration Administrator for Pig Hive and HBase installing updates patches and upgrades and performed both major and minor upgrades to the existing CDH cluster and upgraded the Hadoop cluster from CDH3 to CDH4 Developed a process for the Batch ingestion of CSV Files Sqoop from different sources and also generating views on the data source using Shell Scripting and Python Environment Hadoop HDFS Map Reduce Hive HBase Zookeeper Impala Java jdk16 Cloudera Oracle SQL Server UNIX Shell Scripting Flume Oozie Scala Spark ETL Sqoop Python kafka PySpark AWS S3 MongoDB Oracle SQL Hortonworks XML RedHat Linux 64 Java Developer Fresh Desk Chennai Tamil Nadu November 2015 to November 2015 Roles Responsibilities Implemented MultiThreaded Environment and used most of the interfaces under the collection framework by using Core Java Concepts Developed Graphical User Interfaces by using JSF JSP HTML DHTML Angularjs CSS and JavaScript and developed scripts in python for Financial Data coming from SQL Developer based on the requirements specified Implemented several JavaJ2EE design patterns like Spring MVC Singleton Spring Dependency Injection and Data Transfer Object Used JAXWS SOAP for producing web services and involved in writing programs to consume the web services using SOA with CXF framework and developed few web pages using JSP JSTL HTML CSS Java script Ajax and JSON Implemented business logic data exchange XML processing and created graphics using Python and Django Wrote code to fetch data from Web services using JQUERY AJAX via JSON response and updating the HTML pages and developed high traffic web applications using HTML CSS and JavaScript jQuery Bootstrap Ext JS AngularJS Nodejs and reactjs Write SQL queries and create PLSQL functionsprocedurespackages that are optimized for APEX and improve performance and response times of APEX pages and reports Used JQuery library NodeJS and AngularJS for creation of powerful dynamic WebPages and web applications by using its advanced and cross browser functionality Used Java Server Pages for content layout and presentation with Python and Extracted and loaded data using Python scripts and PLSQL packages Worked with various frameworks of JavaScript like BackboneJS AngularJS and EmberJS etc Written with objectoriented Python Flask SQL Beautiful Soup httplib2 Jinja2 HTMLCSS Bootstrap jQuery Linux Sublime Text GIT Developed GUI using JSP Struts HTML3 CSS3 XHTML JQuery Swing and JavaScript to simplify the complexities of the application Wrote and executed various MYSQL database queries from python using PythonMySQL connector and MySQLdb package and generated Python Django forms to record data of online users and used PyTest for writing test cases Developed and coordinated complex high quality solutions to clients using J2SE J2EE Servlets JSP HTML Struts Spring MVC SOAP JavaScript JQuery JSON and XML Exposed business functionality to external systems Interoperable clients using Web Services WSDLSOAP ApacheAxis Used Hibernate objectrelationalmapping ORM solution technique of mapping data representation from MVC model to Oracle10g Relational data model with a SQLbased schema and mapped using Hibernate Annotations Skilled in using collections in Python for manipulating and looping through different user defined objects Designed and developed intranet web applications using Ext JS Reactjs JavaScript and CSS and developed Merge jobs in Python to extract and load data into MySQL and Mango Db database Worked on Oracle SQL Server as the backend databases and integrated with Hibernate to retrieve Data Access Objects Developed Python batch processors to consume and produce various feeds and developed entire frontend and backend modules using Python on Django Web Framework and developed Business Logic using Python on Django Web Framework Designed and documented RESTHTTP APIs including JSON data formats and API versioning strategy and wrote Apex triggers apex classes developing Visual force pages batch classes Involved in AJAX driven application by invoking web servicesAPI and parsing the JSON response and involved in writing application level code to interact with APIs Web Services using JSON EnvironmentPython Django Java JSF MVC Spring IOC APEX Ruby on Rails Spring JDBC Hibernate ActiveMQ Log4j Ant MySQL JDK 16 J2EE JSP Servlets HTML LDAP Salesforce ESB Mule JDBC MongoDB DAO EJB 30 PLSQL reactjs Web Sphere Eclipse AngularJS and CVS Education Bachelors Skills JAVA 2 years LINUX 2 years ORACLE 2 years PYTHON 2 years SQL 2 years Additional Information Technical Competencies Hadoop Ecosystem ToolsMapReduce HDFS Pig Hive HBase Sqoop Zookeeper Oozie Hue Storm Kafka Spark Flume Languages Java core java HTML Programming C C Databases MySQL Oracle SQL Server MongoDB Platforms Linux RHEL Ubuntu open Solaris AIX Scripting Languages Shell Scripting HTML scripting Python Puppet Web Servers Apache Tomcat JBOSS windows server2003 2008 and 2012 Cluster Management Tools HDP Ambari Cloudera Manager Hue Solr Cloud",
    "unique_id": "7220edae-d8e9-43e3-99cf-4008675364ed"
}