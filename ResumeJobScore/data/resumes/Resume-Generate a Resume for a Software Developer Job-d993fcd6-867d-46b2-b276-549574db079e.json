{
    "clean_data": "Bigdata developer Bigdata span ldeveloperspan Bigdata developer Caterpillar LLCPeoriaIL Peoria IL Having 5 years of experience in implementation Development Customization integration Testing applications using JavaJ2EE technologies Bigdata technologies like Hadoop Apache Spark using Python Apache Hive Pig Flume Apache Kafka Impala and support of business application system Worked on Cloudera Environments to Ingest cleanse and process huge data with different file formats Hands on with Cloudera Environment Setup Hadoop Administration and Configurations Hands on with Hadoop file systems services POC on Spark Projects using Spark SQL and Spark Streaming with Hadoop File System as Data Lake Experienced in Scrum and pair programming methodologies Experienced in writing test cases using Junit Experience in writing HIVE test cases for realtime Data Analytics Hand on experience with Big Data Hadoop Spark Kafka and Cloudera Administration Experience on AWS Lambda services and building Lambda function and Lambda layers using python Experience on AWS Cloud services along with building Hadoop Cluster which includes EC2 S3 ESF Glacier SWF etc Gained knowledge on SQL server MYSQL and 2012 R2 windows server while developing virtual machines using HyperV technology Experience in Java while working in my pervious projects Experience with various stages of Software Development Life cycle SDLC and agile Enthusiastic to learn new tools and technology with my adaptive learning skills Work Experience Bigdata developer Caterpillar LLCPeoriaIL February 2019 to Present Description This project is to build various data pipelines for extraction and transformation of data provided by dealers for client Caterpillar The pipelines are created around the data sent by dealers which are extracted and apply various transformations to cleanse the data and to provide back with business requirements The transformed data is stored in various data storage systems which are consumed back by various dealers Responsibilities Working on designing and implementing data pipelines by collaborating with Team leads Architect and Business Analysts Involved in coding testing integration and Deployment of various modules in a software development life cycle SDLC Built and Migrated the existing data processing framework to intuitive and scalable Spark applications using Scala as for dealer data processing Designed and implemented high performance and high availability data processing framework using various Hadoop Tools like Hive Impala Spark Sqoop Oozie Snowflake and AWS Working on migrating existing systems to AWS services like Lambda EMR EC2 instance Worked on data Integration and imported data from RDBMSOracle into Cloudera Hadoop HDFS and AWS Assisted in Data Migration from Cloudera Hadoop to AWS instance Designed developed and tested reusable framework and libraries using coding best practices and Improve performance Upgraded data ingestion workflows to store data in accessible performant secure and sustainable way Managed the ongoing data warehouse performance and service accessibility and troubleshoot ingestion jobs and workflow Failures proactively Supported Hadoop platform in identifying communicating and resolving data integration and consumption Issues Worked on Root Cause Analysis and Problem Management processes and assisted support team resolving issues in a data ingestion solution developed on Hadoop platform Built Smoke and regression test cases and performed Unit Testing on all the developed modules Environment Hive Impala Sqoop Oozie Spark Scala SQL Agile Java Cloudera AWS Putty Oracle Hadoop developer Ameriprise Financial Minneapolis MN January 2018 to January 2019 Description Worked as a Hadoop developer Data implementation project that consist data migration from source systems to Hadoop environment Aimed to achieve high performance for near real time Data Analytics using Spark Data lake was maintained in Hadoop by ingesting data from Oracle using Sqoop A data ware house was created in Hive to integrate with Spark and to measure performance using Impala Using Spark SQL and Data frames we have achieved high performance for Business analytics by loading data from Hive Spark jobs were submitted in Spark Job Server and connected to YARN as resource manager We have used Avro file format to transfer data between applications and stored data in parquet file formats in Hive We have started another POC to verify clusters performance on streaming data for real time analytics using Spark Streaming and Kafka Responsibilities Installed Clouderas Hadoop cluster configured NameNodes a secondary NameNode and configured following services on Edge node Spark Hive and Kafka and DataNodes Used Xen Center and Cent OS 7 Setup ssh permissions to accept request between each node to transfer huge volumes of data Provided proper FQDN to address the nodes from client applications Installed services from Cloudera Manager on Edge nodes Verified cluster performance and management of the cluster from Cloudera Manager to see the health of the services and nodes Used Sqoop to ingest data from Oracle and created Sqoop jobs for ingesting day to day updates Created Spark and Hive scripts for data analysis Created Test Strategy and Test Design documents and responsible for overall quality of the project Created test cases to validate the HIVE tables and data quality in different process areas Created use cases for test case creation to validate the business flow process Ingested data from Sqoop as Avro datafile Transferred Avro schema file from client using cron jobs Created Hive scripts to load data into data ware house and stored data as parquet files in Hive Cleansed data using Spark or Hive before inserting into Hive ware house Wrote Spark SQL jobs to achieve data analysis by loading data from Hive and writing data back to Hive and relational database Maintained and cleaned data in Hive warehouse using Spark Jobs Environment Cloudera manager Spark Hive Sqoop Kafka YARN MySQL Xen Center JavaHadoop Developer Kohls January 2017 to December 2017 Description This is a servicebased application which acts as a standard middleware application to orchestrate between different applications Our application will act like a communication barrier between frontend user applications and backend applications Also worked on Hadoop environment with other team to write Sqoop jobs to ingest data to HDFS Integrated Hive with HBase to overcome the limitation faced by Hive for daily updated Used Flume to input data streaming faster to HDFS Responsibilities Designed system per J2EE specifications Servlets were used as a Front Controller gateway into the system Helper classes were used to limit the business logic in the servlet EJB was used to talk to the database and JSP along with HTML XML was used to control the client view Designed and added new functionality extended existing application using J2EE XML Servlets JSP Clientside validations are done using JavaScript and serverside validations are done using Struts validator framework Developed Action Classes which acts as the controller in Struts framework Created table and different batch programs to clean up tables in DB2 database Extensively used Collections and Exceptions in the batch program for database cleanup Established JDBC connection using database connection pool Wrote complex SQL statements to retrieve data from the DB2 database Developed application using Eclipse on Windows XP Implemented Log4j to maintain system log Used Junit framework for Unit testing of application Used Clear Case for version control Used Python to perform data validation on the data ingested using Sqoop and Flume and the cleansed data set is pushed into HBase Used Hive to analyze data ingested into HBase by using HiveHBase integration and compute various metrics for reporting on the dashboard Created test cases to validate the HIVE tables and data quality in different process areas Environment JAVA J2EE HTML XML CSS JavaScript windowsUnix Struts Servlets JSP EJB Eclipse JDBC DB2 ClearCase Hive HBase Kafka Flume Creating templates on Virtual machine Concordia University Wisconsin May 2016 to December 2016 Description Developed and deployed virtual machines using windows virtualization technology to minimize the infrastructure cost I developed different VMs using either windows or ubuntu I created VMs based on customers requirements I installed different applications and setup those machines with user credentials Responsibilities Used Windows Virtualization to configure different VMs Used MYSQL server and configure user with secured user credentials Configured Telnet server to in VMs so that user can connect to VMs using Telnet Client application remotely JavaJ2EE Developer Cura Software Solutions July 2014 to November 2015 Title IVR system development Description IVR Processing System this system was developed based on java User can call in and select different options to navigate through and complete his transaction IVR system will send HTTP requests to this application which will process request and respond accordingly Responsibilities Used Maven to build the application Used HTTP protocol to receive and respond back with the response Used Eclipse as IDE and SVN as source code repository Attended agile scrum meetings to manage iterations Participated in daily stand ups sprint reviews sprint planning sessions and demo the changes to business Wrote unit test cases using JUnit Supported the application for UAT testing and with the bug fixes Environment Java Servlets Maven Junit Eclipse SVN UNIX Java developer Cura Software Solutions January 2014 to June 2014 Title Web application development Description Intranet based application to track User requests received from IVR systems like Billing Data customer data Once the request was solved then our application will generate conformation and send to the user Responsibilities Working on HTML CSS JavaScript and JSP for building user interface pages Using Tomcat to deploy the application in different environments Using Maven to build the application artifacts Using JDBC to access database Using Eclipse as IDE and SVN as source code repository Education Masters in Information Technology in Information Technology Concordia University Wisconsin Skills Impala Sqoop Kafka Cdh4 Flume Hadoop Avro C Hadoop Hive Html Javascript Jenkins Pig Eclipse Java Jsp Servlets Database Jdbc Additional Information Technical Skills Big Data Technologies Apache Hadoop Spark Hive Sqoop Kafka Pig Flume AVRO Parquet Impala AWS Technologies Lambda services EMR Cloudwatch Hadoop Distributions ClouderaCDH4CDH5 Programming Languages C C Java HTML CSS javascript Servlets JSP JDBC Junit log4j Maven Databases Oracle Database SQL server TSQL MYSQL IDE Tools Eclipse SoapUI Jenkins JIRA Confluence Applications MSOffice Word Excel PowerPoint MS Access MS outlook Internet Applications Operating Systems Windows 20002012 server R2 Windows XP Pro Vista 7810",
    "entities": [
        "IVR",
        "IDE",
        "MN",
        "Wisconsin",
        "HTTP",
        "ClouderaCDH4CDH5 Programming Languages C C",
        "AWS Technologies Lambda",
        "Cloudwatch Hadoop Distributions",
        "Telnet Client",
        "J2EE XML Servlets",
        "Hadoop Tools",
        "Oozie Snowflake",
        "Hadoop",
        "Hive Spark",
        "JavaJ2EE Developer Cura Software Solutions",
        "Maintained",
        "ClearCase",
        "HBase",
        "Python Apache Hive Pig Flume Apache",
        "JavaJ2EE",
        "Development Customization",
        "UAT",
        "Billing Data",
        "Cloudera Hadoop",
        "AWS Lambda services",
        "Enthusiastic",
        "Developed Action Classes",
        "NameNode",
        "Spark Data",
        "Created Test Strategy and Test Design",
        "AWS Working",
        "AWS Cloud",
        "HDFS Integrated Hive",
        "Hadoop File System",
        "Collections and Exceptions",
        "JSP",
        "Architect and Business Analysts Involved",
        "Minneapolis",
        "Data Analytics Hand",
        "Data Analytics",
        "Spark Streaming",
        "Hadoop Cluster",
        "Spark Jobs Environment",
        "AWS Assisted",
        "Struts Servlets JSP EJB",
        "Spark",
        "EJB",
        "Created Hive",
        "Big Data Hadoop Spark Kafka",
        "Improve",
        "Responsibilities Working on HTML CSS JavaScript",
        "Oracle Database SQL",
        "Impala Using Spark",
        "Cura Software Solutions",
        "Sqoop",
        "HIVE",
        "HTML XML",
        "RDBMSOracle",
        "Created",
        "Oracle",
        "Spark Projects",
        "Software Development Life",
        "SQL",
        "Xen Center",
        "Hadoop Apache Spark",
        "Responsibilities Working",
        "Hive",
        "Responsibilities Used Windows Virtualization",
        "NameNodes",
        "Virtual",
        "JUnit Supported",
        "Lambda EMR",
        "Maven",
        "Data Migration",
        "Impala",
        "Spark SQL",
        "JavaScript",
        "Helper",
        "Putty Oracle Hadoop",
        "parquet",
        "Bigdata",
        "Spark Hive Sqoop",
        "Description Developed",
        "SVN",
        "Caterpillar LLCPeoriaIL",
        "Data",
        "Concordia University",
        "Cloudera Environment Setup Hadoop Administration",
        "Cloudera Hadoop HDFS",
        "Team",
        "Edge",
        "Root Cause Analysis and Problem Management",
        "Created Spark",
        "Information Technology Concordia University",
        "Caterpillar",
        "Cloudera Administration",
        "Information Technology in",
        "Description IVR Processing System",
        "Xen Center JavaHadoop Developer"
    ],
    "experience": "Projects using Spark SQL and Spark Streaming with Hadoop File System as Data Lake Experienced in Scrum and pair programming methodologies Experienced in writing test cases using Junit Experience in writing HIVE test cases for realtime Data Analytics Hand on experience with Big Data Hadoop Spark Kafka and Cloudera Administration Experience on AWS Lambda services and building Lambda function and Lambda layers using python Experience on AWS Cloud services along with building Hadoop Cluster which includes EC2 S3 ESF Glacier SWF etc Gained knowledge on SQL server MYSQL and 2012 R2 windows server while developing virtual machines using HyperV technology Experience in Java while working in my pervious projects Experience with various stages of Software Development Life cycle SDLC and agile Enthusiastic to learn new tools and technology with my adaptive learning skills Work Experience Bigdata developer Caterpillar LLCPeoriaIL February 2019 to Present Description This project is to build various data pipelines for extraction and transformation of data provided by dealers for client Caterpillar The pipelines are created around the data sent by dealers which are extracted and apply various transformations to cleanse the data and to provide back with business requirements The transformed data is stored in various data storage systems which are consumed back by various dealers Responsibilities Working on designing and implementing data pipelines by collaborating with Team leads Architect and Business Analysts Involved in coding testing integration and Deployment of various modules in a software development life cycle SDLC Built and Migrated the existing data processing framework to intuitive and scalable Spark applications using Scala as for dealer data processing Designed and implemented high performance and high availability data processing framework using various Hadoop Tools like Hive Impala Spark Sqoop Oozie Snowflake and AWS Working on migrating existing systems to AWS services like Lambda EMR EC2 instance Worked on data Integration and imported data from RDBMSOracle into Cloudera Hadoop HDFS and AWS Assisted in Data Migration from Cloudera Hadoop to AWS instance Designed developed and tested reusable framework and libraries using coding best practices and Improve performance Upgraded data ingestion workflows to store data in accessible performant secure and sustainable way Managed the ongoing data warehouse performance and service accessibility and troubleshoot ingestion jobs and workflow Failures proactively Supported Hadoop platform in identifying communicating and resolving data integration and consumption Issues Worked on Root Cause Analysis and Problem Management processes and assisted support team resolving issues in a data ingestion solution developed on Hadoop platform Built Smoke and regression test cases and performed Unit Testing on all the developed modules Environment Hive Impala Sqoop Oozie Spark Scala SQL Agile Java Cloudera AWS Putty Oracle Hadoop developer Ameriprise Financial Minneapolis MN January 2018 to January 2019 Description Worked as a Hadoop developer Data implementation project that consist data migration from source systems to Hadoop environment Aimed to achieve high performance for near real time Data Analytics using Spark Data lake was maintained in Hadoop by ingesting data from Oracle using Sqoop A data ware house was created in Hive to integrate with Spark and to measure performance using Impala Using Spark SQL and Data frames we have achieved high performance for Business analytics by loading data from Hive Spark jobs were submitted in Spark Job Server and connected to YARN as resource manager We have used Avro file format to transfer data between applications and stored data in parquet file formats in Hive We have started another POC to verify clusters performance on streaming data for real time analytics using Spark Streaming and Kafka Responsibilities Installed Clouderas Hadoop cluster configured NameNodes a secondary NameNode and configured following services on Edge node Spark Hive and Kafka and DataNodes Used Xen Center and Cent OS 7 Setup ssh permissions to accept request between each node to transfer huge volumes of data Provided proper FQDN to address the nodes from client applications Installed services from Cloudera Manager on Edge nodes Verified cluster performance and management of the cluster from Cloudera Manager to see the health of the services and nodes Used Sqoop to ingest data from Oracle and created Sqoop jobs for ingesting day to day updates Created Spark and Hive scripts for data analysis Created Test Strategy and Test Design documents and responsible for overall quality of the project Created test cases to validate the HIVE tables and data quality in different process areas Created use cases for test case creation to validate the business flow process Ingested data from Sqoop as Avro datafile Transferred Avro schema file from client using cron jobs Created Hive scripts to load data into data ware house and stored data as parquet files in Hive Cleansed data using Spark or Hive before inserting into Hive ware house Wrote Spark SQL jobs to achieve data analysis by loading data from Hive and writing data back to Hive and relational database Maintained and cleaned data in Hive warehouse using Spark Jobs Environment Cloudera manager Spark Hive Sqoop Kafka YARN MySQL Xen Center JavaHadoop Developer Kohls January 2017 to December 2017 Description This is a servicebased application which acts as a standard middleware application to orchestrate between different applications Our application will act like a communication barrier between frontend user applications and backend applications Also worked on Hadoop environment with other team to write Sqoop jobs to ingest data to HDFS Integrated Hive with HBase to overcome the limitation faced by Hive for daily updated Used Flume to input data streaming faster to HDFS Responsibilities Designed system per J2EE specifications Servlets were used as a Front Controller gateway into the system Helper classes were used to limit the business logic in the servlet EJB was used to talk to the database and JSP along with HTML XML was used to control the client view Designed and added new functionality extended existing application using J2EE XML Servlets JSP Clientside validations are done using JavaScript and serverside validations are done using Struts validator framework Developed Action Classes which acts as the controller in Struts framework Created table and different batch programs to clean up tables in DB2 database Extensively used Collections and Exceptions in the batch program for database cleanup Established JDBC connection using database connection pool Wrote complex SQL statements to retrieve data from the DB2 database Developed application using Eclipse on Windows XP Implemented Log4j to maintain system log Used Junit framework for Unit testing of application Used Clear Case for version control Used Python to perform data validation on the data ingested using Sqoop and Flume and the cleansed data set is pushed into HBase Used Hive to analyze data ingested into HBase by using HiveHBase integration and compute various metrics for reporting on the dashboard Created test cases to validate the HIVE tables and data quality in different process areas Environment JAVA J2EE HTML XML CSS JavaScript windowsUnix Struts Servlets JSP EJB Eclipse JDBC DB2 ClearCase Hive HBase Kafka Flume Creating templates on Virtual machine Concordia University Wisconsin May 2016 to December 2016 Description Developed and deployed virtual machines using windows virtualization technology to minimize the infrastructure cost I developed different VMs using either windows or ubuntu I created VMs based on customers requirements I installed different applications and setup those machines with user credentials Responsibilities Used Windows Virtualization to configure different VMs Used MYSQL server and configure user with secured user credentials Configured Telnet server to in VMs so that user can connect to VMs using Telnet Client application remotely JavaJ2EE Developer Cura Software Solutions July 2014 to November 2015 Title IVR system development Description IVR Processing System this system was developed based on java User can call in and select different options to navigate through and complete his transaction IVR system will send HTTP requests to this application which will process request and respond accordingly Responsibilities Used Maven to build the application Used HTTP protocol to receive and respond back with the response Used Eclipse as IDE and SVN as source code repository Attended agile scrum meetings to manage iterations Participated in daily stand ups sprint reviews sprint planning sessions and demo the changes to business Wrote unit test cases using JUnit Supported the application for UAT testing and with the bug fixes Environment Java Servlets Maven Junit Eclipse SVN UNIX Java developer Cura Software Solutions January 2014 to June 2014 Title Web application development Description Intranet based application to track User requests received from IVR systems like Billing Data customer data Once the request was solved then our application will generate conformation and send to the user Responsibilities Working on HTML CSS JavaScript and JSP for building user interface pages Using Tomcat to deploy the application in different environments Using Maven to build the application artifacts Using JDBC to access database Using Eclipse as IDE and SVN as source code repository Education Masters in Information Technology in Information Technology Concordia University Wisconsin Skills Impala Sqoop Kafka Cdh4 Flume Hadoop Avro C Hadoop Hive Html Javascript Jenkins Pig Eclipse Java Jsp Servlets Database Jdbc Additional Information Technical Skills Big Data Technologies Apache Hadoop Spark Hive Sqoop Kafka Pig Flume AVRO Parquet Impala AWS Technologies Lambda services EMR Cloudwatch Hadoop Distributions ClouderaCDH4CDH5 Programming Languages C C Java HTML CSS javascript Servlets JSP JDBC Junit log4j Maven Databases Oracle Database SQL server TSQL MYSQL IDE Tools Eclipse SoapUI Jenkins JIRA Confluence Applications MSOffice Word Excel PowerPoint MS Access MS outlook Internet Applications Operating Systems Windows 20002012 server R2 Windows XP Pro Vista 7810",
    "extracted_keywords": [
        "Bigdata",
        "developer",
        "Bigdata",
        "span",
        "ldeveloperspan",
        "Bigdata",
        "developer",
        "Caterpillar",
        "LLCPeoriaIL",
        "Peoria",
        "IL",
        "years",
        "experience",
        "implementation",
        "Development",
        "Customization",
        "integration",
        "Testing",
        "applications",
        "JavaJ2EE",
        "technologies",
        "Bigdata",
        "technologies",
        "Hadoop",
        "Apache",
        "Spark",
        "Python",
        "Apache",
        "Hive",
        "Pig",
        "Flume",
        "Apache",
        "Kafka",
        "Impala",
        "support",
        "business",
        "application",
        "system",
        "Cloudera",
        "Environments",
        "cleanse",
        "data",
        "file",
        "formats",
        "Hands",
        "Cloudera",
        "Environment",
        "Setup",
        "Hadoop",
        "Administration",
        "Configurations",
        "Hands",
        "Hadoop",
        "file",
        "systems",
        "services",
        "POC",
        "Spark",
        "Projects",
        "Spark",
        "SQL",
        "Spark",
        "Streaming",
        "Hadoop",
        "File",
        "System",
        "Data",
        "Lake",
        "Experienced",
        "Scrum",
        "pair",
        "programming",
        "methodologies",
        "test",
        "cases",
        "Junit",
        "Experience",
        "HIVE",
        "test",
        "cases",
        "Data",
        "Analytics",
        "Hand",
        "experience",
        "Big",
        "Data",
        "Hadoop",
        "Spark",
        "Kafka",
        "Cloudera",
        "Administration",
        "Experience",
        "AWS",
        "Lambda",
        "services",
        "Lambda",
        "function",
        "Lambda",
        "layers",
        "python",
        "Experience",
        "AWS",
        "Cloud",
        "services",
        "Hadoop",
        "Cluster",
        "EC2",
        "S3",
        "ESF",
        "Glacier",
        "SWF",
        "knowledge",
        "SQL",
        "server",
        "MYSQL",
        "R2",
        "windows",
        "server",
        "machines",
        "HyperV",
        "technology",
        "Experience",
        "Java",
        "projects",
        "stages",
        "Software",
        "Development",
        "Life",
        "cycle",
        "SDLC",
        "Enthusiastic",
        "tools",
        "technology",
        "learning",
        "skills",
        "Work",
        "Experience",
        "Bigdata",
        "developer",
        "Caterpillar",
        "LLCPeoriaIL",
        "February",
        "Present",
        "Description",
        "project",
        "data",
        "pipelines",
        "extraction",
        "transformation",
        "data",
        "dealers",
        "client",
        "Caterpillar",
        "pipelines",
        "data",
        "dealers",
        "transformations",
        "data",
        "business",
        "requirements",
        "data",
        "data",
        "storage",
        "systems",
        "dealers",
        "Responsibilities",
        "data",
        "pipelines",
        "Team",
        "Architect",
        "Business",
        "Analysts",
        "testing",
        "integration",
        "Deployment",
        "modules",
        "software",
        "development",
        "life",
        "cycle",
        "SDLC",
        "data",
        "processing",
        "framework",
        "Spark",
        "applications",
        "Scala",
        "dealer",
        "data",
        "processing",
        "performance",
        "availability",
        "data",
        "processing",
        "framework",
        "Hadoop",
        "Tools",
        "Hive",
        "Impala",
        "Spark",
        "Sqoop",
        "Oozie",
        "Snowflake",
        "AWS",
        "systems",
        "AWS",
        "services",
        "Lambda",
        "EMR",
        "EC2",
        "instance",
        "data",
        "Integration",
        "data",
        "RDBMSOracle",
        "Cloudera",
        "Hadoop",
        "HDFS",
        "AWS",
        "Data",
        "Migration",
        "Cloudera",
        "Hadoop",
        "AWS",
        "instance",
        "framework",
        "libraries",
        "practices",
        "performance",
        "data",
        "ingestion",
        "workflows",
        "data",
        "way",
        "data",
        "warehouse",
        "performance",
        "service",
        "accessibility",
        "troubleshoot",
        "ingestion",
        "jobs",
        "Failures",
        "Hadoop",
        "platform",
        "data",
        "integration",
        "consumption",
        "Issues",
        "Root",
        "Cause",
        "Analysis",
        "Problem",
        "Management",
        "processes",
        "support",
        "team",
        "issues",
        "data",
        "ingestion",
        "solution",
        "Hadoop",
        "platform",
        "Smoke",
        "regression",
        "test",
        "cases",
        "Unit",
        "Testing",
        "modules",
        "Environment",
        "Hive",
        "Impala",
        "Sqoop",
        "Oozie",
        "Spark",
        "Scala",
        "SQL",
        "Agile",
        "Java",
        "Cloudera",
        "Putty",
        "Oracle",
        "Hadoop",
        "developer",
        "Ameriprise",
        "Financial",
        "Minneapolis",
        "MN",
        "January",
        "January",
        "Description",
        "Hadoop",
        "developer",
        "Data",
        "implementation",
        "project",
        "data",
        "migration",
        "source",
        "systems",
        "Hadoop",
        "environment",
        "performance",
        "time",
        "Data",
        "Analytics",
        "Spark",
        "Data",
        "lake",
        "Hadoop",
        "data",
        "Oracle",
        "Sqoop",
        "A",
        "data",
        "ware",
        "house",
        "Hive",
        "Spark",
        "performance",
        "Impala",
        "Spark",
        "SQL",
        "Data",
        "frames",
        "performance",
        "Business",
        "analytics",
        "loading",
        "data",
        "Hive",
        "Spark",
        "jobs",
        "Spark",
        "Job",
        "Server",
        "YARN",
        "resource",
        "manager",
        "Avro",
        "file",
        "format",
        "data",
        "applications",
        "data",
        "file",
        "formats",
        "Hive",
        "POC",
        "clusters",
        "performance",
        "data",
        "time",
        "analytics",
        "Spark",
        "Streaming",
        "Kafka",
        "Responsibilities",
        "Installed",
        "Clouderas",
        "Hadoop",
        "cluster",
        "NameNodes",
        "NameNode",
        "services",
        "Edge",
        "node",
        "Spark",
        "Hive",
        "Kafka",
        "DataNodes",
        "Xen",
        "Center",
        "Cent",
        "Setup",
        "ssh",
        "permissions",
        "request",
        "node",
        "volumes",
        "data",
        "FQDN",
        "nodes",
        "client",
        "applications",
        "services",
        "Cloudera",
        "Manager",
        "Edge",
        "nodes",
        "cluster",
        "performance",
        "management",
        "cluster",
        "Cloudera",
        "Manager",
        "health",
        "services",
        "nodes",
        "Used",
        "Sqoop",
        "data",
        "Oracle",
        "Sqoop",
        "jobs",
        "day",
        "day",
        "updates",
        "Created",
        "Spark",
        "Hive",
        "scripts",
        "data",
        "analysis",
        "Test",
        "Strategy",
        "Test",
        "Design",
        "documents",
        "quality",
        "project",
        "test",
        "cases",
        "HIVE",
        "tables",
        "data",
        "quality",
        "process",
        "areas",
        "use",
        "cases",
        "test",
        "case",
        "creation",
        "business",
        "flow",
        "process",
        "data",
        "Sqoop",
        "Avro",
        "Transferred",
        "Avro",
        "schema",
        "file",
        "client",
        "cron",
        "jobs",
        "Hive",
        "scripts",
        "data",
        "data",
        "ware",
        "house",
        "data",
        "files",
        "Hive",
        "data",
        "Spark",
        "Hive",
        "Hive",
        "ware",
        "house",
        "Wrote",
        "Spark",
        "SQL",
        "jobs",
        "data",
        "analysis",
        "loading",
        "data",
        "Hive",
        "data",
        "Hive",
        "database",
        "data",
        "Hive",
        "warehouse",
        "Spark",
        "Jobs",
        "Environment",
        "Cloudera",
        "manager",
        "Spark",
        "Hive",
        "Sqoop",
        "Kafka",
        "YARN",
        "MySQL",
        "Xen",
        "Center",
        "JavaHadoop",
        "Developer",
        "Kohls",
        "January",
        "December",
        "Description",
        "application",
        "middleware",
        "application",
        "applications",
        "application",
        "communication",
        "barrier",
        "frontend",
        "user",
        "applications",
        "applications",
        "Hadoop",
        "environment",
        "team",
        "Sqoop",
        "jobs",
        "data",
        "HDFS",
        "Integrated",
        "Hive",
        "HBase",
        "limitation",
        "Hive",
        "Used",
        "Flume",
        "data",
        "streaming",
        "HDFS",
        "Responsibilities",
        "system",
        "J2EE",
        "specifications",
        "Servlets",
        "Front",
        "Controller",
        "gateway",
        "system",
        "Helper",
        "classes",
        "business",
        "logic",
        "servlet",
        "EJB",
        "database",
        "JSP",
        "HTML",
        "XML",
        "client",
        "view",
        "functionality",
        "application",
        "J2EE",
        "XML",
        "Servlets",
        "JSP",
        "Clientside",
        "validations",
        "JavaScript",
        "serverside",
        "validations",
        "Struts",
        "validator",
        "framework",
        "Developed",
        "Action",
        "Classes",
        "controller",
        "Struts",
        "table",
        "batch",
        "programs",
        "tables",
        "DB2",
        "database",
        "Collections",
        "Exceptions",
        "batch",
        "program",
        "database",
        "cleanup",
        "Established",
        "JDBC",
        "connection",
        "database",
        "connection",
        "pool",
        "Wrote",
        "SQL",
        "statements",
        "data",
        "DB2",
        "database",
        "application",
        "Eclipse",
        "Windows",
        "XP",
        "Log4j",
        "system",
        "log",
        "Junit",
        "framework",
        "Unit",
        "testing",
        "application",
        "Clear",
        "Case",
        "version",
        "control",
        "Python",
        "data",
        "validation",
        "data",
        "Sqoop",
        "Flume",
        "data",
        "set",
        "HBase",
        "Hive",
        "data",
        "HBase",
        "HiveHBase",
        "integration",
        "metrics",
        "dashboard",
        "test",
        "cases",
        "HIVE",
        "tables",
        "data",
        "quality",
        "process",
        "areas",
        "Environment",
        "J2EE",
        "HTML",
        "XML",
        "CSS",
        "JavaScript",
        "windowsUnix",
        "Struts",
        "Servlets",
        "JSP",
        "EJB",
        "Eclipse",
        "JDBC",
        "DB2",
        "ClearCase",
        "Hive",
        "HBase",
        "Kafka",
        "Flume",
        "templates",
        "machine",
        "Concordia",
        "University",
        "Wisconsin",
        "May",
        "December",
        "Description",
        "Developed",
        "machines",
        "windows",
        "virtualization",
        "technology",
        "infrastructure",
        "cost",
        "VMs",
        "windows",
        "ubuntu",
        "VMs",
        "customers",
        "requirements",
        "applications",
        "machines",
        "user",
        "credentials",
        "Responsibilities",
        "Windows",
        "Virtualization",
        "VMs",
        "MYSQL",
        "server",
        "configure",
        "user",
        "user",
        "credentials",
        "Configured",
        "Telnet",
        "server",
        "VMs",
        "user",
        "VMs",
        "Telnet",
        "Client",
        "application",
        "JavaJ2EE",
        "Developer",
        "Cura",
        "Software",
        "Solutions",
        "July",
        "November",
        "Title",
        "IVR",
        "system",
        "development",
        "Description",
        "IVR",
        "Processing",
        "System",
        "system",
        "User",
        "options",
        "transaction",
        "IVR",
        "system",
        "HTTP",
        "requests",
        "application",
        "request",
        "Responsibilities",
        "Maven",
        "application",
        "HTTP",
        "protocol",
        "response",
        "Eclipse",
        "IDE",
        "SVN",
        "source",
        "code",
        "repository",
        "scrum",
        "meetings",
        "iterations",
        "stand",
        "ups",
        "sprint",
        "reviews",
        "sprint",
        "planning",
        "sessions",
        "changes",
        "business",
        "Wrote",
        "unit",
        "test",
        "cases",
        "JUnit",
        "application",
        "testing",
        "bug",
        "fixes",
        "Environment",
        "Java",
        "Servlets",
        "Maven",
        "Junit",
        "Eclipse",
        "SVN",
        "UNIX",
        "Java",
        "developer",
        "Cura",
        "Software",
        "Solutions",
        "January",
        "June",
        "Title",
        "Web",
        "application",
        "development",
        "Description",
        "Intranet",
        "application",
        "User",
        "requests",
        "IVR",
        "systems",
        "Billing",
        "Data",
        "customer",
        "data",
        "request",
        "application",
        "conformation",
        "user",
        "Responsibilities",
        "HTML",
        "CSS",
        "JavaScript",
        "JSP",
        "user",
        "interface",
        "pages",
        "Tomcat",
        "application",
        "environments",
        "Maven",
        "application",
        "artifacts",
        "JDBC",
        "access",
        "database",
        "Eclipse",
        "IDE",
        "SVN",
        "source",
        "code",
        "repository",
        "Education",
        "Masters",
        "Information",
        "Technology",
        "Information",
        "Technology",
        "Concordia",
        "University",
        "Wisconsin",
        "Skills",
        "Impala",
        "Sqoop",
        "Kafka",
        "Cdh4",
        "Flume",
        "Hadoop",
        "Avro",
        "C",
        "Hadoop",
        "Hive",
        "Html",
        "Javascript",
        "Jenkins",
        "Pig",
        "Eclipse",
        "Java",
        "Jsp",
        "Servlets",
        "Database",
        "Jdbc",
        "Additional",
        "Information",
        "Technical",
        "Skills",
        "Big",
        "Data",
        "Technologies",
        "Apache",
        "Hadoop",
        "Spark",
        "Hive",
        "Sqoop",
        "Kafka",
        "Pig",
        "Flume",
        "AVRO",
        "Parquet",
        "Impala",
        "AWS",
        "Technologies",
        "Lambda",
        "services",
        "EMR",
        "Cloudwatch",
        "Hadoop",
        "Distributions",
        "ClouderaCDH4CDH5",
        "Programming",
        "Languages",
        "C",
        "C",
        "Java",
        "HTML",
        "CSS",
        "javascript",
        "JSP",
        "JDBC",
        "Junit",
        "log4j",
        "Maven",
        "Databases",
        "Oracle",
        "Database",
        "SQL",
        "server",
        "TSQL",
        "MYSQL",
        "IDE",
        "Tools",
        "Eclipse",
        "SoapUI",
        "Jenkins",
        "JIRA",
        "Confluence",
        "Applications",
        "MSOffice",
        "Word",
        "Excel",
        "PowerPoint",
        "MS",
        "Access",
        "MS",
        "outlook",
        "Internet",
        "Applications",
        "Operating",
        "Systems",
        "Windows",
        "server",
        "R2",
        "Windows",
        "XP",
        "Pro",
        "Vista"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T22:01:49.357200",
    "resume_data": "Bigdata developer Bigdata span ldeveloperspan Bigdata developer Caterpillar LLCPeoriaIL Peoria IL Having 5 years of experience in implementation Development Customization integration Testing applications using JavaJ2EE technologies Bigdata technologies like Hadoop Apache Spark using Python Apache Hive Pig Flume Apache Kafka Impala and support of business application system Worked on Cloudera Environments to Ingest cleanse and process huge data with different file formats Hands on with Cloudera Environment Setup Hadoop Administration and Configurations Hands on with Hadoop file systems services POC on Spark Projects using Spark SQL and Spark Streaming with Hadoop File System as Data Lake Experienced in Scrum and pair programming methodologies Experienced in writing test cases using Junit Experience in writing HIVE test cases for realtime Data Analytics Hand on experience with Big Data Hadoop Spark Kafka and Cloudera Administration Experience on AWS Lambda services and building Lambda function and Lambda layers using python Experience on AWS Cloud services along with building Hadoop Cluster which includes EC2 S3 ESF Glacier SWF etc Gained knowledge on SQL server MYSQL and 2012 R2 windows server while developing virtual machines using HyperV technology Experience in Java while working in my pervious projects Experience with various stages of Software Development Life cycle SDLC and agile Enthusiastic to learn new tools and technology with my adaptive learning skills Work Experience Bigdata developer Caterpillar LLCPeoriaIL February 2019 to Present Description This project is to build various data pipelines for extraction and transformation of data provided by dealers for client Caterpillar The pipelines are created around the data sent by dealers which are extracted and apply various transformations to cleanse the data and to provide back with business requirements The transformed data is stored in various data storage systems which are consumed back by various dealers Responsibilities Working on designing and implementing data pipelines by collaborating with Team leads Architect and Business Analysts Involved in coding testing integration and Deployment of various modules in a software development life cycle SDLC Built and Migrated the existing data processing framework to intuitive and scalable Spark applications using Scala as for dealer data processing Designed and implemented high performance and high availability data processing framework using various Hadoop Tools like Hive Impala Spark Sqoop Oozie Snowflake and AWS Working on migrating existing systems to AWS services like Lambda EMR EC2 instance Worked on data Integration and imported data from RDBMSOracle into Cloudera Hadoop HDFS and AWS Assisted in Data Migration from Cloudera Hadoop to AWS instance Designed developed and tested reusable framework and libraries using coding best practices and Improve performance Upgraded data ingestion workflows to store data in accessible performant secure and sustainable way Managed the ongoing data warehouse performance and service accessibility and troubleshoot ingestion jobs and workflow Failures proactively Supported Hadoop platform in identifying communicating and resolving data integration and consumption Issues Worked on Root Cause Analysis and Problem Management processes and assisted support team resolving issues in a data ingestion solution developed on Hadoop platform Built Smoke and regression test cases and performed Unit Testing on all the developed modules Environment Hive Impala Sqoop Oozie Spark Scala SQL Agile Java Cloudera AWS Putty Oracle Hadoop developer Ameriprise Financial Minneapolis MN January 2018 to January 2019 Description Worked as a Hadoop developer Data implementation project that consist data migration from source systems to Hadoop environment Aimed to achieve high performance for near real time Data Analytics using Spark Data lake was maintained in Hadoop by ingesting data from Oracle using Sqoop A data ware house was created in Hive to integrate with Spark and to measure performance using Impala Using Spark SQL and Data frames we have achieved high performance for Business analytics by loading data from Hive Spark jobs were submitted in Spark Job Server and connected to YARN as resource manager We have used Avro file format to transfer data between applications and stored data in parquet file formats in Hive We have started another POC to verify clusters performance on streaming data for real time analytics using Spark Streaming and Kafka Responsibilities Installed Clouderas Hadoop cluster configured NameNodes a secondary NameNode and configured following services on Edge node Spark Hive and Kafka and DataNodes Used Xen Center and Cent OS 7 Setup ssh permissions to accept request between each node to transfer huge volumes of data Provided proper FQDN to address the nodes from client applications Installed services from Cloudera Manager on Edge nodes Verified cluster performance and management of the cluster from Cloudera Manager to see the health of the services and nodes Used Sqoop to ingest data from Oracle and created Sqoop jobs for ingesting day to day updates Created Spark and Hive scripts for data analysis Created Test Strategy and Test Design documents and responsible for overall quality of the project Created test cases to validate the HIVE tables and data quality in different process areas Created use cases for test case creation to validate the business flow process Ingested data from Sqoop as Avro datafile Transferred Avro schema file from client using cron jobs Created Hive scripts to load data into data ware house and stored data as parquet files in Hive Cleansed data using Spark or Hive before inserting into Hive ware house Wrote Spark SQL jobs to achieve data analysis by loading data from Hive and writing data back to Hive and relational database Maintained and cleaned data in Hive warehouse using Spark Jobs Environment Cloudera manager Spark Hive Sqoop Kafka YARN MySQL Xen Center JavaHadoop Developer Kohls January 2017 to December 2017 Description This is a servicebased application which acts as a standard middleware application to orchestrate between different applications Our application will act like a communication barrier between frontend user applications and backend applications Also worked on Hadoop environment with other team to write Sqoop jobs to ingest data to HDFS Integrated Hive with HBase to overcome the limitation faced by Hive for daily updated Used Flume to input data streaming faster to HDFS Responsibilities Designed system per J2EE specifications Servlets were used as a Front Controller gateway into the system Helper classes were used to limit the business logic in the servlet EJB was used to talk to the database and JSP along with HTML XML was used to control the client view Designed and added new functionality extended existing application using J2EE XML Servlets JSP Clientside validations are done using JavaScript and serverside validations are done using Struts validator framework Developed Action Classes which acts as the controller in Struts framework Created table and different batch programs to clean up tables in DB2 database Extensively used Collections and Exceptions in the batch program for database cleanup Established JDBC connection using database connection pool Wrote complex SQL statements to retrieve data from the DB2 database Developed application using Eclipse on Windows XP Implemented Log4j to maintain system log Used Junit framework for Unit testing of application Used Clear Case for version control Used Python to perform data validation on the data ingested using Sqoop and Flume and the cleansed data set is pushed into HBase Used Hive to analyze data ingested into HBase by using HiveHBase integration and compute various metrics for reporting on the dashboard Created test cases to validate the HIVE tables and data quality in different process areas Environment JAVA J2EE HTML XML CSS JavaScript windowsUnix Struts Servlets JSP EJB Eclipse JDBC DB2 ClearCase Hive HBase Kafka Flume Creating templates on Virtual machine Concordia University Wisconsin May 2016 to December 2016 Description Developed and deployed virtual machines using windows virtualization technology to minimize the infrastructure cost I developed different VMs using either windows or ubuntu I created VMs based on customers requirements I installed different applications and setup those machines with user credentials Responsibilities Used Windows Virtualization to configure different VMs Used MYSQL server and configure user with secured user credentials Configured Telnet server to in VMs so that user can connect to VMs using Telnet Client application remotely JavaJ2EE Developer Cura Software Solutions July 2014 to November 2015 Title IVR system development Description IVR Processing System this system was developed based on java User can call in and select different options to navigate through and complete his transaction IVR system will send HTTP requests to this application which will process request and respond accordingly Responsibilities Used Maven to build the application Used HTTP protocol to receive and respond back with the response Used Eclipse as IDE and SVN as source code repository Attended agile scrum meetings to manage iterations Participated in daily stand ups sprint reviews sprint planning sessions and demo the changes to business Wrote unit test cases using JUnit Supported the application for UAT testing and with the bug fixes Environment Java Servlets Maven Junit Eclipse SVN UNIX Java developer Cura Software Solutions January 2014 to June 2014 Title Web application development Description Intranet based application to track User requests received from IVR systems like Billing Data customer data Once the request was solved then our application will generate conformation and send to the user Responsibilities Working on HTML CSS JavaScript and JSP for building user interface pages Using Tomcat to deploy the application in different environments Using Maven to build the application artifacts Using JDBC to access database Using Eclipse as IDE and SVN as source code repository Education Masters in Information Technology in Information Technology Concordia University Wisconsin Skills Impala Sqoop Kafka Cdh4 Flume Hadoop Avro C Hadoop Hive Html Javascript Jenkins Pig Eclipse Java Jsp Servlets Database Jdbc Additional Information Technical Skills Big Data Technologies Apache Hadoop Spark Hive Sqoop Kafka Pig Flume AVRO Parquet Impala AWS Technologies Lambda services EMR Cloudwatch Hadoop Distributions ClouderaCDH4CDH5 Programming Languages C C Java HTML CSS javascript Servlets JSP JDBC Junit log4j Maven Databases Oracle Database SQL server TSQL MYSQL IDE Tools Eclipse SoapUI Jenkins JIRA Confluence Applications MSOffice Word Excel PowerPoint MS Access MS outlook Internet Applications Operating Systems Windows 20002012 server R2 Windows XP Pro Vista 7810",
    "unique_id": "d993fcd6-867d-46b2-b276-549574db079e"
}