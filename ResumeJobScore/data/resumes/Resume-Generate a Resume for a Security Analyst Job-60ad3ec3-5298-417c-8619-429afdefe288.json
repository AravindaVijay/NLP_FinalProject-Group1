{
    "clean_data": "Hadoop Admin Hadoop Admin Hadoop Administrator NCR Corp Over 7 years of professional IT experience which includes experience in Big Data ecosystem related technologies 3 years of experience in Hadoop Administration Expertise in Big Data technologies and Hadoop ecosystem HDFS Job Tracker Task Tracker NameNode Data Node andMapReduceprogramming paradigm Hands on experience in installing configuring and using Hadoop ecosystem components like Hadoop MapReduce HDFS HBase Oozie Hive Sqoop Pig Zookeeper and Flume Hands on experience with Apache Hadoop Map Reduce programming PIG Scripting and Distribute Application and HDFS Hands on experience in installation configuration management and deployment of Big Data solutions and the underlying infrastructure of Hadoop Cluster using Cloudera Horton works distributions and Map Excellent understanding of Hadoop Cluster architecture and monitoring the cluster Experience in managing and reviewing Hadoop log files Experience in NoSQLdatabase Monod and Cassandra Hands on experience importing and exporting data using Sqoop from HDFS to Relational Database Systems Over two years of experience in design development and maintenance and support of Big Data Analytics using Hadoop Ecosystem components like HDFS Hive Pig HBase Sqoop Flume Zookeeper MapReduce and Oozie Managing the health of Cluster resolving performance related issues coordinating with various parties for Infrastructure support Update Hadoop 121 to 252 Experience in Talend Big Data Studio 60 Push data as delimited files into HDFS using Talend Big Data studio Expertise in Commissioning and decommissioning the nodes in Hadoop Cluster using Cloudera Manager Enterprise Experience on Oracle OBIEE Setting up HDFS Quotas to enforce the fair share of computing resources Experience in Rebalance an HDFS Cluster Hadoop Cluster capacity planning performance tuning cluster Monitoring Troubleshooting Hands on experience in analyzing Log files for Hadoop and eco system services and finding root cause Expertise in benchmarking performing backup and disaster recovery of Name Node metadata and important and sensitive data residing on cluster Rack aware configuration for quick availability and processing of data Experience in designing and implementing of secure Hadoop cluster using Kerberos Successfully loaded files to Hive and HDFS from Oracle SQL Server MySQL and Teradata using Sqoop Loaded streaming log data from various web servers into HDFS using Flume Created Hive internal and external tables defined with appropriate static and dynamic partitions Experience in Creating and managing HBase clusters dynamically using Slider and Start Stop HBase clusters running on Slider Strong Knowledge on Spark concepts like RDD Operations Caching and Persistence Experience in Upgrading Apache Ambary CDH and HDP Cluster Extensive knowledge in using job scheduling by Oozie and Centralized Service Zookeeper Expertise in Collaborating across Multiple technology groups and getting things done Worked on both traditional Waterfall model and Agile methodology Sound knowledge of Data warehousing concepts Worked on Oracle Teradata and Vertica database systems with Good experience in UNIX Shell scripting Experience in modeling with both OLTPOLAP systems and Kimball and Inmon Data Warehousingenvironments Experience in extracting data from both Relational systems and Flat Files Analysis and development of mappings using the transformations in Informatica Handsome experience in Linux admin activities Worked in a 24x7 oncall Production Support Environment Work Experience Hadoop Admin Tracfone Wireless Miami FL May 2011 to August 2012 Description TracFone Wireless is known for its coverage in 996 of the cellular populations They are contracted with the major carriers to bring you the same level of coverage by providing connectivity through the same cellular towers Responsibilities Installed and configured Flume Hive Pig Sqoop and Oozie on the Hadoop cluster Launching and Setup of HADOOP HBASE Cluster which includes configuring different components of HADOOP and HBASE Cluster Experienced in loading data from the UNIX file system to HDFS Created HBase tables to load large sets of structured semistructured and unstructured data coming from UNIX NoSQL and a variety of portfolios Worked on writing transformermapping MapReduce pipelines using Java Involved in creating Hive tables loading them with data and writing Hive queries that will run internally in Map Reduce way Migrated ETL jobs to Pig scripts do Transformations even joins and some preaggregations before storing the data onto HDFS Worked on different file formats like Sequence files XML files and Map files using Map Reduce Programs Experienced in loading and transforming of large sets of structured semi structured and unstructured data Involved in creating Oozie workflow and Coordinator jobs to kick off the jobs on time for data availability Used Flume to collect aggregate and store the web log data from different sources like web servers network devices and pushed to HDFS Scripting to deploy monitors checks and critical sysadmin functions automation Managing and scheduling Jobs on a Hadoop cluster Performing tuning and troubleshooting of MapReduce jobs by analyzing and reviewing Hadoop log files Involved in defining job flows managing and reviewing log files Environment Map Reduce HDFS Hive Java SQL Cloudera Manager Pig Sqoop Oozie Database Administrator Convergys Cincinnati OH January 2010 to March 2011 Description Convergys Corporation is a corporation based in Cincinnati Ohio that sells customer management and information management products primarily to large corporations Information management provides convergent billing and business support system BSS products and services including revenue management product and order management and customer care management to telecom utilities and cablesatellitebroadband service providers Responsibilities Installed optimized and configured new servers and application upgrades in existing network environment as per requirements Extensively working on performance tuning working with developers every day and support production readiness testing in performance lab database Identified poorly written SQL statements and suggested code change implemented SQL profiles and SQL plan baselines Managed and administered WindowsLinux systems Performed periodic maintenance provided technical support and executed systems engineering Wrote PLSQL procedures to do the database jobs and other monthly weekly maintenance tasks Performed schema refreshes from Production to QA and other test environments for testing Develop new views and triggers for new software release Provide on call support for various database issues like Oracle errors slow performance and system maintenance issues Perform SQL server Databases Migration from One server to another Server during Maintenance window activities Scheduled Cron jobs for daytoday database jobs and other monitoring tasks at database and UNIX level Having High level of experience with different Interface systems supporting our applications and working on various environments based on requirements Involved in analyzing the real time data and doing performance tuning Supported Production A and development database servers Written SQL Scripts for generating reports Provided User training and support Worked on identifying and troubleshooting the bugs Environment Oracle PLSQL Linux UNIX Perl Shell Java Nagios Infants Rating Billing Linux System Administrator Ediko Systems Inc Hyderabad Andhra Pradesh June 2008 to December 2009 DescriptionEdiko Systems Integrators Pvt Ltd an IBM Premier Business Partner is a specialist company delivering world class business solutions leveraging IBM Technologies EDIKO ensures the delivery of high quality business integration solutions through the application of sound software architecture principles and using the latest IBM technologies together with agile project management techniques Responsibilities Administration package installation configuration of Oracle Enterprise Linux 5x Administration of RHEL which includes installation testing tuning upgrading and loading patches troubleshooting both physical and virtual server issues Creating cloning Linux Virtual Machines Installing RedHat Linux using kick start and applying security polices for hardening the server based on the company policies RPM and YUM package installations patch and other server management Managing systems routine backup scheduling jobs like disabling and enabling cron jobs enabling system logging network logging of servers for maintenance performance tuning testing Tech and nontech refresh of Linux servers which includes new hardware OS upgrade application installation testing Set up user and group login IDs printing parameters network configuration password resolving permissions issues and user and group quota Creating physical volumes volume groups and logical volumes Gathering requirements from customers and business partners and design implement and provide solutions in building the environment Installing and configuring Apache and supporting them on Linux production servers Environment Oracle Shell Perl PLSQL DNS TCPIP Apache Tomcat XML HTML and UNIXLinux Education Bachelor of Technology in Computer Science Osmania University Hyderabad Andhra Pradesh Additional Information Technical Skills HadoopBig Data HDFSMapReduceHBasePigHiveSqoopFlumeMongoDB Cassandra Power pivot Puppet Oozie Zookeeper Java J2EE Technologies Core Java Servlets JSP JDBC JNDI Java Beans IDEs Eclipse Net beans Big Data Analytics Defamer 205 Frameworks MVC Struts Hibernate Spring Programming languages CC Java Python Ant scripts Linux shell scripts Databases Oracle 11g10g9i MySQL DB2 MSSQL Server Web Servers Web Logic Web Sphere Apache Tomcat Web Technologies HTML XML JavaScript AJAX SOAP WSDL Network Protocols TCPIP UDP HTTP DNS DHCP ETL Tools Informatica Pentaho Testing Win Runner Load Runner QTP",
    "entities": [
        "Talend Big Data Studio",
        "Oracle SQL Server",
        "HADOOP",
        "Relational",
        "Oracle OBIEE Setting up HDFS",
        "UNIX",
        "UNIXLinux Education Bachelor of Technology",
        "IBM",
        "Update Hadoop",
        "Hadoop Ecosystem",
        "Hadoop",
        "XML",
        "RDD Operations Caching and Persistence Experience",
        "Oracle 11g10g9i MySQL DB2",
        "Launching and Setup of HADOOP HBASE Cluster",
        "Cincinnati",
        "Hadoop Administration Expertise",
        "Creating",
        "HBase",
        "HDP Cluster Extensive",
        "Distribute Application",
        "Inmon Data Warehousingenvironments",
        "Waterfall",
        "Sequence",
        "Map Reduce Programs Experienced",
        "Commissioning",
        "Perform SQL",
        "Flume Created Hive",
        "Develop",
        "Linux",
        "Map Excellent",
        "Hadoop Cluster",
        "Kimball",
        "Interface",
        "Slider",
        "Agile",
        "Oozie Managing",
        "HDFS Created HBase",
        "BSS",
        "Oozie and Centralized Service Zookeeper Expertise",
        "Responsibilities Administration",
        "Sqoop",
        "Oracle Teradata",
        "Vertica",
        "QA",
        "IBM Technologies EDIKO",
        "Hadoop Admin Hadoop Admin Hadoop Administrator NCR Corp",
        "Informatica Handsome",
        "Server",
        "Responsibilities Installed",
        "Oracle",
        "RPM",
        "Talend Big Data",
        "Big Data Analytics",
        "Cron",
        "SQL",
        "HDFS Hive Pig HBase Sqoop",
        "YUM",
        "DescriptionEdiko Systems Integrators Pvt Ltd",
        "Relational Database Systems",
        "Kerberos Successfully",
        "Information",
        "Hadoop MapReduce HDFS HBase Oozie Hive Sqoop",
        "Big Data",
        "Hive",
        "Oracle Enterprise Linux",
        "Computer Science Osmania University Hyderabad Andhra Pradesh Additional Information Technical Skills",
        "UNIX Shell",
        "FL",
        "WindowsLinux",
        "Apache Hadoop",
        "Performed",
        "PIG Scripting and",
        "Flat Files Analysis",
        "MapReduce",
        "NoSQL",
        "Linux Virtual Machines",
        "Tech",
        "Map",
        "Ohio",
        "DNS"
    ],
    "experience": "Experience in managing and reviewing Hadoop log files Experience in NoSQLdatabase Monod and Cassandra Hands on experience importing and exporting data using Sqoop from HDFS to Relational Database Systems Over two years of experience in design development and maintenance and support of Big Data Analytics using Hadoop Ecosystem components like HDFS Hive Pig HBase Sqoop Flume Zookeeper MapReduce and Oozie Managing the health of Cluster resolving performance related issues coordinating with various parties for Infrastructure support Update Hadoop 121 to 252 Experience in Talend Big Data Studio 60 Push data as delimited files into HDFS using Talend Big Data studio Expertise in Commissioning and decommissioning the nodes in Hadoop Cluster using Cloudera Manager Enterprise Experience on Oracle OBIEE Setting up HDFS Quotas to enforce the fair share of computing resources Experience in Rebalance an HDFS Cluster Hadoop Cluster capacity planning performance tuning cluster Monitoring Troubleshooting Hands on experience in analyzing Log files for Hadoop and eco system services and finding root cause Expertise in benchmarking performing backup and disaster recovery of Name Node metadata and important and sensitive data residing on cluster Rack aware configuration for quick availability and processing of data Experience in designing and implementing of secure Hadoop cluster using Kerberos Successfully loaded files to Hive and HDFS from Oracle SQL Server MySQL and Teradata using Sqoop Loaded streaming log data from various web servers into HDFS using Flume Created Hive internal and external tables defined with appropriate static and dynamic partitions Experience in Creating and managing HBase clusters dynamically using Slider and Start Stop HBase clusters running on Slider Strong Knowledge on Spark concepts like RDD Operations Caching and Persistence Experience in Upgrading Apache Ambary CDH and HDP Cluster Extensive knowledge in using job scheduling by Oozie and Centralized Service Zookeeper Expertise in Collaborating across Multiple technology groups and getting things done Worked on both traditional Waterfall model and Agile methodology Sound knowledge of Data warehousing concepts Worked on Oracle Teradata and Vertica database systems with Good experience in UNIX Shell scripting Experience in modeling with both OLTPOLAP systems and Kimball and Inmon Data Warehousingenvironments Experience in extracting data from both Relational systems and Flat Files Analysis and development of mappings using the transformations in Informatica Handsome experience in Linux admin activities Worked in a 24x7 oncall Production Support Environment Work Experience Hadoop Admin Tracfone Wireless Miami FL May 2011 to August 2012 Description TracFone Wireless is known for its coverage in 996 of the cellular populations They are contracted with the major carriers to bring you the same level of coverage by providing connectivity through the same cellular towers Responsibilities Installed and configured Flume Hive Pig Sqoop and Oozie on the Hadoop cluster Launching and Setup of HADOOP HBASE Cluster which includes configuring different components of HADOOP and HBASE Cluster Experienced in loading data from the UNIX file system to HDFS Created HBase tables to load large sets of structured semistructured and unstructured data coming from UNIX NoSQL and a variety of portfolios Worked on writing transformermapping MapReduce pipelines using Java Involved in creating Hive tables loading them with data and writing Hive queries that will run internally in Map Reduce way Migrated ETL jobs to Pig scripts do Transformations even joins and some preaggregations before storing the data onto HDFS Worked on different file formats like Sequence files XML files and Map files using Map Reduce Programs Experienced in loading and transforming of large sets of structured semi structured and unstructured data Involved in creating Oozie workflow and Coordinator jobs to kick off the jobs on time for data availability Used Flume to collect aggregate and store the web log data from different sources like web servers network devices and pushed to HDFS Scripting to deploy monitors checks and critical sysadmin functions automation Managing and scheduling Jobs on a Hadoop cluster Performing tuning and troubleshooting of MapReduce jobs by analyzing and reviewing Hadoop log files Involved in defining job flows managing and reviewing log files Environment Map Reduce HDFS Hive Java SQL Cloudera Manager Pig Sqoop Oozie Database Administrator Convergys Cincinnati OH January 2010 to March 2011 Description Convergys Corporation is a corporation based in Cincinnati Ohio that sells customer management and information management products primarily to large corporations Information management provides convergent billing and business support system BSS products and services including revenue management product and order management and customer care management to telecom utilities and cablesatellitebroadband service providers Responsibilities Installed optimized and configured new servers and application upgrades in existing network environment as per requirements Extensively working on performance tuning working with developers every day and support production readiness testing in performance lab database Identified poorly written SQL statements and suggested code change implemented SQL profiles and SQL plan baselines Managed and administered WindowsLinux systems Performed periodic maintenance provided technical support and executed systems engineering Wrote PLSQL procedures to do the database jobs and other monthly weekly maintenance tasks Performed schema refreshes from Production to QA and other test environments for testing Develop new views and triggers for new software release Provide on call support for various database issues like Oracle errors slow performance and system maintenance issues Perform SQL server Databases Migration from One server to another Server during Maintenance window activities Scheduled Cron jobs for daytoday database jobs and other monitoring tasks at database and UNIX level Having High level of experience with different Interface systems supporting our applications and working on various environments based on requirements Involved in analyzing the real time data and doing performance tuning Supported Production A and development database servers Written SQL Scripts for generating reports Provided User training and support Worked on identifying and troubleshooting the bugs Environment Oracle PLSQL Linux UNIX Perl Shell Java Nagios Infants Rating Billing Linux System Administrator Ediko Systems Inc Hyderabad Andhra Pradesh June 2008 to December 2009 DescriptionEdiko Systems Integrators Pvt Ltd an IBM Premier Business Partner is a specialist company delivering world class business solutions leveraging IBM Technologies EDIKO ensures the delivery of high quality business integration solutions through the application of sound software architecture principles and using the latest IBM technologies together with agile project management techniques Responsibilities Administration package installation configuration of Oracle Enterprise Linux 5x Administration of RHEL which includes installation testing tuning upgrading and loading patches troubleshooting both physical and virtual server issues Creating cloning Linux Virtual Machines Installing RedHat Linux using kick start and applying security polices for hardening the server based on the company policies RPM and YUM package installations patch and other server management Managing systems routine backup scheduling jobs like disabling and enabling cron jobs enabling system logging network logging of servers for maintenance performance tuning testing Tech and nontech refresh of Linux servers which includes new hardware OS upgrade application installation testing Set up user and group login IDs printing parameters network configuration password resolving permissions issues and user and group quota Creating physical volumes volume groups and logical volumes Gathering requirements from customers and business partners and design implement and provide solutions in building the environment Installing and configuring Apache and supporting them on Linux production servers Environment Oracle Shell Perl PLSQL DNS TCPIP Apache Tomcat XML HTML and UNIXLinux Education Bachelor of Technology in Computer Science Osmania University Hyderabad Andhra Pradesh Additional Information Technical Skills HadoopBig Data HDFSMapReduceHBasePigHiveSqoopFlumeMongoDB Cassandra Power pivot Puppet Oozie Zookeeper Java J2EE Technologies Core Java Servlets JSP JDBC JNDI Java Beans IDEs Eclipse Net beans Big Data Analytics Defamer 205 Frameworks MVC Struts Hibernate Spring Programming languages CC Java Python Ant scripts Linux shell scripts Databases Oracle 11g10g9i MySQL DB2 MSSQL Server Web Servers Web Logic Web Sphere Apache Tomcat Web Technologies HTML XML JavaScript AJAX SOAP WSDL Network Protocols TCPIP UDP HTTP DNS DHCP ETL Tools Informatica Pentaho Testing Win Runner Load Runner QTP",
    "extracted_keywords": [
        "Hadoop",
        "Admin",
        "Hadoop",
        "Admin",
        "Hadoop",
        "Administrator",
        "NCR",
        "Corp",
        "years",
        "IT",
        "experience",
        "experience",
        "Big",
        "Data",
        "ecosystem",
        "technologies",
        "years",
        "experience",
        "Hadoop",
        "Administration",
        "Expertise",
        "Big",
        "Data",
        "technologies",
        "Hadoop",
        "ecosystem",
        "HDFS",
        "Job",
        "Tracker",
        "Task",
        "Tracker",
        "NameNode",
        "Data",
        "Node",
        "paradigm",
        "Hands",
        "experience",
        "configuring",
        "Hadoop",
        "ecosystem",
        "components",
        "Hadoop",
        "MapReduce",
        "HDFS",
        "HBase",
        "Oozie",
        "Hive",
        "Sqoop",
        "Pig",
        "Zookeeper",
        "Flume",
        "Hands",
        "experience",
        "Apache",
        "Hadoop",
        "Map",
        "programming",
        "PIG",
        "Scripting",
        "Distribute",
        "Application",
        "HDFS",
        "Hands",
        "experience",
        "installation",
        "configuration",
        "management",
        "deployment",
        "Big",
        "Data",
        "solutions",
        "infrastructure",
        "Hadoop",
        "Cluster",
        "Cloudera",
        "Horton",
        "distributions",
        "Map",
        "Excellent",
        "understanding",
        "Hadoop",
        "Cluster",
        "architecture",
        "cluster",
        "Experience",
        "Hadoop",
        "log",
        "Experience",
        "Monod",
        "Cassandra",
        "Hands",
        "experience",
        "data",
        "Sqoop",
        "HDFS",
        "Relational",
        "Database",
        "Systems",
        "years",
        "experience",
        "design",
        "development",
        "maintenance",
        "support",
        "Big",
        "Data",
        "Analytics",
        "Hadoop",
        "Ecosystem",
        "components",
        "Hive",
        "Pig",
        "HBase",
        "Sqoop",
        "Flume",
        "Zookeeper",
        "MapReduce",
        "Oozie",
        "health",
        "Cluster",
        "performance",
        "issues",
        "parties",
        "Infrastructure",
        "support",
        "Update",
        "Hadoop",
        "Experience",
        "Talend",
        "Big",
        "Data",
        "Studio",
        "Push",
        "data",
        "files",
        "HDFS",
        "Talend",
        "Big",
        "Data",
        "studio",
        "Expertise",
        "nodes",
        "Hadoop",
        "Cluster",
        "Cloudera",
        "Manager",
        "Enterprise",
        "Experience",
        "Oracle",
        "OBIEE",
        "HDFS",
        "Quotas",
        "share",
        "resources",
        "Experience",
        "Rebalance",
        "HDFS",
        "Cluster",
        "Hadoop",
        "Cluster",
        "capacity",
        "performance",
        "cluster",
        "Monitoring",
        "Troubleshooting",
        "Hands",
        "experience",
        "Log",
        "files",
        "Hadoop",
        "eco",
        "system",
        "services",
        "root",
        "Expertise",
        "backup",
        "disaster",
        "recovery",
        "Name",
        "Node",
        "metadata",
        "data",
        "cluster",
        "Rack",
        "configuration",
        "availability",
        "processing",
        "data",
        "Experience",
        "Hadoop",
        "cluster",
        "Kerberos",
        "files",
        "Hive",
        "HDFS",
        "Oracle",
        "SQL",
        "Server",
        "MySQL",
        "Teradata",
        "Sqoop",
        "Loaded",
        "streaming",
        "log",
        "data",
        "web",
        "servers",
        "HDFS",
        "Flume",
        "Created",
        "Hive",
        "tables",
        "partitions",
        "Experience",
        "Creating",
        "HBase",
        "clusters",
        "Slider",
        "Start",
        "HBase",
        "clusters",
        "Slider",
        "Strong",
        "Knowledge",
        "Spark",
        "concepts",
        "RDD",
        "Operations",
        "Caching",
        "Persistence",
        "Experience",
        "Upgrading",
        "Apache",
        "Ambary",
        "CDH",
        "HDP",
        "Cluster",
        "knowledge",
        "job",
        "scheduling",
        "Oozie",
        "Centralized",
        "Service",
        "Zookeeper",
        "Expertise",
        "technology",
        "groups",
        "things",
        "Waterfall",
        "model",
        "methodology",
        "Sound",
        "knowledge",
        "Data",
        "warehousing",
        "concepts",
        "Oracle",
        "Teradata",
        "Vertica",
        "database",
        "systems",
        "experience",
        "UNIX",
        "Shell",
        "Experience",
        "modeling",
        "systems",
        "Kimball",
        "Inmon",
        "Data",
        "Warehousingenvironments",
        "Experience",
        "data",
        "systems",
        "Files",
        "Analysis",
        "development",
        "mappings",
        "transformations",
        "Informatica",
        "Handsome",
        "experience",
        "Linux",
        "admin",
        "activities",
        "oncall",
        "Production",
        "Support",
        "Environment",
        "Work",
        "Experience",
        "Hadoop",
        "Admin",
        "Tracfone",
        "Wireless",
        "Miami",
        "FL",
        "May",
        "August",
        "Description",
        "TracFone",
        "Wireless",
        "coverage",
        "populations",
        "carriers",
        "level",
        "coverage",
        "connectivity",
        "towers",
        "Responsibilities",
        "Flume",
        "Hive",
        "Pig",
        "Sqoop",
        "Oozie",
        "Hadoop",
        "cluster",
        "Launching",
        "Setup",
        "HADOOP",
        "HBASE",
        "Cluster",
        "components",
        "HADOOP",
        "HBASE",
        "Cluster",
        "loading",
        "data",
        "UNIX",
        "file",
        "system",
        "HDFS",
        "Created",
        "HBase",
        "sets",
        "data",
        "UNIX",
        "NoSQL",
        "variety",
        "portfolios",
        "writing",
        "MapReduce",
        "pipelines",
        "Java",
        "Hive",
        "tables",
        "data",
        "Hive",
        "queries",
        "Map",
        "Reduce",
        "way",
        "ETL",
        "jobs",
        "scripts",
        "Transformations",
        "preaggregations",
        "data",
        "HDFS",
        "file",
        "formats",
        "Sequence",
        "files",
        "XML",
        "files",
        "Map",
        "files",
        "Map",
        "Reduce",
        "Programs",
        "loading",
        "transforming",
        "sets",
        "data",
        "Oozie",
        "workflow",
        "Coordinator",
        "jobs",
        "jobs",
        "time",
        "data",
        "availability",
        "Flume",
        "aggregate",
        "web",
        "log",
        "data",
        "sources",
        "web",
        "servers",
        "network",
        "devices",
        "HDFS",
        "Scripting",
        "monitors",
        "checks",
        "sysadmin",
        "functions",
        "scheduling",
        "Jobs",
        "Hadoop",
        "cluster",
        "tuning",
        "troubleshooting",
        "MapReduce",
        "jobs",
        "Hadoop",
        "log",
        "files",
        "job",
        "log",
        "files",
        "Environment",
        "Map",
        "HDFS",
        "Hive",
        "Java",
        "SQL",
        "Cloudera",
        "Manager",
        "Pig",
        "Sqoop",
        "Oozie",
        "Database",
        "Administrator",
        "Convergys",
        "Cincinnati",
        "OH",
        "January",
        "March",
        "Description",
        "Convergys",
        "Corporation",
        "corporation",
        "Cincinnati",
        "Ohio",
        "customer",
        "management",
        "information",
        "management",
        "products",
        "corporations",
        "Information",
        "management",
        "convergent",
        "billing",
        "business",
        "support",
        "system",
        "BSS",
        "products",
        "services",
        "revenue",
        "management",
        "product",
        "order",
        "management",
        "customer",
        "care",
        "management",
        "telecom",
        "utilities",
        "cablesatellitebroadband",
        "service",
        "providers",
        "Responsibilities",
        "servers",
        "application",
        "upgrades",
        "network",
        "environment",
        "requirements",
        "performance",
        "developers",
        "day",
        "production",
        "readiness",
        "testing",
        "performance",
        "lab",
        "database",
        "SQL",
        "statements",
        "code",
        "change",
        "SQL",
        "profiles",
        "SQL",
        "plan",
        "baselines",
        "WindowsLinux",
        "systems",
        "maintenance",
        "support",
        "systems",
        "engineering",
        "Wrote",
        "PLSQL",
        "procedures",
        "database",
        "jobs",
        "maintenance",
        "tasks",
        "Performed",
        "schema",
        "Production",
        "QA",
        "test",
        "environments",
        "views",
        "triggers",
        "software",
        "release",
        "Provide",
        "call",
        "support",
        "database",
        "issues",
        "Oracle",
        "errors",
        "performance",
        "system",
        "maintenance",
        "issues",
        "Perform",
        "SQL",
        "server",
        "Databases",
        "Migration",
        "server",
        "Server",
        "Maintenance",
        "window",
        "activities",
        "Cron",
        "jobs",
        "daytoday",
        "database",
        "jobs",
        "monitoring",
        "tasks",
        "database",
        "UNIX",
        "level",
        "level",
        "experience",
        "Interface",
        "systems",
        "applications",
        "environments",
        "requirements",
        "time",
        "data",
        "performance",
        "Supported",
        "Production",
        "A",
        "development",
        "database",
        "SQL",
        "Scripts",
        "generating",
        "reports",
        "User",
        "training",
        "support",
        "bugs",
        "Environment",
        "Oracle",
        "PLSQL",
        "Linux",
        "UNIX",
        "Perl",
        "Shell",
        "Java",
        "Nagios",
        "Infants",
        "Rating",
        "Billing",
        "Linux",
        "System",
        "Administrator",
        "Ediko",
        "Systems",
        "Inc",
        "Hyderabad",
        "Andhra",
        "Pradesh",
        "June",
        "December",
        "DescriptionEdiko",
        "Systems",
        "Integrators",
        "Pvt",
        "Ltd",
        "IBM",
        "Premier",
        "Business",
        "Partner",
        "company",
        "world",
        "class",
        "business",
        "solutions",
        "IBM",
        "Technologies",
        "EDIKO",
        "delivery",
        "quality",
        "business",
        "integration",
        "solutions",
        "application",
        "software",
        "architecture",
        "principles",
        "IBM",
        "technologies",
        "project",
        "management",
        "techniques",
        "Responsibilities",
        "Administration",
        "package",
        "installation",
        "configuration",
        "Oracle",
        "Enterprise",
        "Linux",
        "5x",
        "Administration",
        "RHEL",
        "installation",
        "testing",
        "tuning",
        "upgrading",
        "loading",
        "patches",
        "server",
        "issues",
        "Linux",
        "Virtual",
        "Machines",
        "RedHat",
        "Linux",
        "kick",
        "security",
        "polices",
        "server",
        "company",
        "RPM",
        "YUM",
        "package",
        "installations",
        "patch",
        "server",
        "management",
        "Managing",
        "systems",
        "scheduling",
        "jobs",
        "cron",
        "jobs",
        "system",
        "network",
        "logging",
        "servers",
        "maintenance",
        "performance",
        "Tech",
        "nontech",
        "refresh",
        "Linux",
        "servers",
        "hardware",
        "OS",
        "upgrade",
        "application",
        "installation",
        "testing",
        "user",
        "group",
        "login",
        "parameters",
        "network",
        "configuration",
        "password",
        "permissions",
        "issues",
        "user",
        "group",
        "quota",
        "volumes",
        "volume",
        "groups",
        "volumes",
        "requirements",
        "customers",
        "business",
        "partners",
        "design",
        "implement",
        "solutions",
        "environment",
        "Apache",
        "Linux",
        "production",
        "servers",
        "Environment",
        "Oracle",
        "Shell",
        "Perl",
        "PLSQL",
        "DNS",
        "Apache",
        "Tomcat",
        "XML",
        "HTML",
        "UNIXLinux",
        "Education",
        "Bachelor",
        "Technology",
        "Computer",
        "Science",
        "Osmania",
        "University",
        "Hyderabad",
        "Andhra",
        "Pradesh",
        "Additional",
        "Information",
        "Technical",
        "Skills",
        "HadoopBig",
        "Data",
        "HDFSMapReduceHBasePigHiveSqoopFlumeMongoDB",
        "Cassandra",
        "Power",
        "pivot",
        "Puppet",
        "Oozie",
        "Zookeeper",
        "Java",
        "J2EE",
        "Technologies",
        "Core",
        "Java",
        "Servlets",
        "JSP",
        "JDBC",
        "JNDI",
        "Java",
        "Beans",
        "IDEs",
        "Eclipse",
        "Net",
        "beans",
        "Big",
        "Data",
        "Analytics",
        "Defamer",
        "Frameworks",
        "MVC",
        "Struts",
        "Hibernate",
        "Spring",
        "Programming",
        "CC",
        "Java",
        "Python",
        "Ant",
        "Linux",
        "shell",
        "scripts",
        "Databases",
        "Oracle",
        "MySQL",
        "DB2",
        "MSSQL",
        "Server",
        "Web",
        "Servers",
        "Web",
        "Logic",
        "Web",
        "Sphere",
        "Apache",
        "Tomcat",
        "Web",
        "Technologies",
        "HTML",
        "XML",
        "JavaScript",
        "AJAX",
        "SOAP",
        "WSDL",
        "Network",
        "Protocols",
        "UDP",
        "HTTP",
        "DNS",
        "ETL",
        "Tools",
        "Informatica",
        "Pentaho",
        "Testing",
        "Win",
        "Runner",
        "Load",
        "Runner",
        "QTP"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T21:35:00.227189",
    "resume_data": "Hadoop Admin Hadoop Admin Hadoop Administrator NCR Corp Over 7 years of professional IT experience which includes experience in Big Data ecosystem related technologies 3 years of experience in Hadoop Administration Expertise in Big Data technologies and Hadoop ecosystem HDFS Job Tracker Task Tracker NameNode Data Node andMapReduceprogramming paradigm Hands on experience in installing configuring and using Hadoop ecosystem components like Hadoop MapReduce HDFS HBase Oozie Hive Sqoop Pig Zookeeper and Flume Hands on experience with Apache Hadoop Map Reduce programming PIG Scripting and Distribute Application and HDFS Hands on experience in installation configuration management and deployment of Big Data solutions and the underlying infrastructure of Hadoop Cluster using Cloudera Horton works distributions and Map Excellent understanding of Hadoop Cluster architecture and monitoring the cluster Experience in managing and reviewing Hadoop log files Experience in NoSQLdatabase Monod and Cassandra Hands on experience importing and exporting data using Sqoop from HDFS to Relational Database Systems Over two years of experience in design development and maintenance and support of Big Data Analytics using Hadoop Ecosystem components like HDFS Hive Pig HBase Sqoop Flume Zookeeper MapReduce and Oozie Managing the health of Cluster resolving performance related issues coordinating with various parties for Infrastructure support Update Hadoop 121 to 252 Experience in Talend Big Data Studio 60 Push data as delimited files into HDFS using Talend Big Data studio Expertise in Commissioning and decommissioning the nodes in Hadoop Cluster using Cloudera Manager Enterprise Experience on Oracle OBIEE Setting up HDFS Quotas to enforce the fair share of computing resources Experience in Rebalance an HDFS Cluster Hadoop Cluster capacity planning performance tuning cluster Monitoring Troubleshooting Hands on experience in analyzing Log files for Hadoop and eco system services and finding root cause Expertise in benchmarking performing backup and disaster recovery of Name Node metadata and important and sensitive data residing on cluster Rack aware configuration for quick availability and processing of data Experience in designing and implementing of secure Hadoop cluster using Kerberos Successfully loaded files to Hive and HDFS from Oracle SQL Server MySQL and Teradata using Sqoop Loaded streaming log data from various web servers into HDFS using Flume Created Hive internal and external tables defined with appropriate static and dynamic partitions Experience in Creating and managing HBase clusters dynamically using Slider and Start Stop HBase clusters running on Slider Strong Knowledge on Spark concepts like RDD Operations Caching and Persistence Experience in Upgrading Apache Ambary CDH and HDP Cluster Extensive knowledge in using job scheduling by Oozie and Centralized Service Zookeeper Expertise in Collaborating across Multiple technology groups and getting things done Worked on both traditional Waterfall model and Agile methodology Sound knowledge of Data warehousing concepts Worked on Oracle Teradata and Vertica database systems with Good experience in UNIX Shell scripting Experience in modeling with both OLTPOLAP systems and Kimball and Inmon Data Warehousingenvironments Experience in extracting data from both Relational systems and Flat Files Analysis and development of mappings using the transformations in Informatica Handsome experience in Linux admin activities Worked in a 24x7 oncall Production Support Environment Work Experience Hadoop Admin Tracfone Wireless Miami FL May 2011 to August 2012 Description TracFone Wireless is known for its coverage in 996 of the cellular populations They are contracted with the major carriers to bring you the same level of coverage by providing connectivity through the same cellular towers Responsibilities Installed and configured Flume Hive Pig Sqoop and Oozie on the Hadoop cluster Launching and Setup of HADOOP HBASE Cluster which includes configuring different components of HADOOP and HBASE Cluster Experienced in loading data from the UNIX file system to HDFS Created HBase tables to load large sets of structured semistructured and unstructured data coming from UNIX NoSQL and a variety of portfolios Worked on writing transformermapping MapReduce pipelines using Java Involved in creating Hive tables loading them with data and writing Hive queries that will run internally in Map Reduce way Migrated ETL jobs to Pig scripts do Transformations even joins and some preaggregations before storing the data onto HDFS Worked on different file formats like Sequence files XML files and Map files using Map Reduce Programs Experienced in loading and transforming of large sets of structured semi structured and unstructured data Involved in creating Oozie workflow and Coordinator jobs to kick off the jobs on time for data availability Used Flume to collect aggregate and store the web log data from different sources like web servers network devices and pushed to HDFS Scripting to deploy monitors checks and critical sysadmin functions automation Managing and scheduling Jobs on a Hadoop cluster Performing tuning and troubleshooting of MapReduce jobs by analyzing and reviewing Hadoop log files Involved in defining job flows managing and reviewing log files Environment Map Reduce HDFS Hive Java SQL Cloudera Manager Pig Sqoop Oozie Database Administrator Convergys Cincinnati OH January 2010 to March 2011 Description Convergys Corporation is a corporation based in Cincinnati Ohio that sells customer management and information management products primarily to large corporations Information management provides convergent billing and business support system BSS products and services including revenue management product and order management and customer care management to telecom utilities and cablesatellitebroadband service providers Responsibilities Installed optimized and configured new servers and application upgrades in existing network environment as per requirements Extensively working on performance tuning working with developers every day and support production readiness testing in performance lab database Identified poorly written SQL statements and suggested code change implemented SQL profiles and SQL plan baselines Managed and administered WindowsLinux systems Performed periodic maintenance provided technical support and executed systems engineering Wrote PLSQL procedures to do the database jobs and other monthly weekly maintenance tasks Performed schema refreshes from Production to QA and other test environments for testing Develop new views and triggers for new software release Provide on call support for various database issues like Oracle errors slow performance and system maintenance issues Perform SQL server Databases Migration from One server to another Server during Maintenance window activities Scheduled Cron jobs for daytoday database jobs and other monitoring tasks at database and UNIX level Having High level of experience with different Interface systems supporting our applications and working on various environments based on requirements Involved in analyzing the real time data and doing performance tuning Supported Production A and development database servers Written SQL Scripts for generating reports Provided User training and support Worked on identifying and troubleshooting the bugs Environment Oracle PLSQL Linux UNIX Perl Shell Java Nagios Infants Rating Billing Linux System Administrator Ediko Systems Inc Hyderabad Andhra Pradesh June 2008 to December 2009 DescriptionEdiko Systems Integrators Pvt Ltd an IBM Premier Business Partner is a specialist company delivering world class business solutions leveraging IBM Technologies EDIKO ensures the delivery of high quality business integration solutions through the application of sound software architecture principles and using the latest IBM technologies together with agile project management techniques Responsibilities Administration package installation configuration of Oracle Enterprise Linux 5x Administration of RHEL which includes installation testing tuning upgrading and loading patches troubleshooting both physical and virtual server issues Creating cloning Linux Virtual Machines Installing RedHat Linux using kick start and applying security polices for hardening the server based on the company policies RPM and YUM package installations patch and other server management Managing systems routine backup scheduling jobs like disabling and enabling cron jobs enabling system logging network logging of servers for maintenance performance tuning testing Tech and nontech refresh of Linux servers which includes new hardware OS upgrade application installation testing Set up user and group login IDs printing parameters network configuration password resolving permissions issues and user and group quota Creating physical volumes volume groups and logical volumes Gathering requirements from customers and business partners and design implement and provide solutions in building the environment Installing and configuring Apache and supporting them on Linux production servers Environment Oracle Shell Perl PLSQL DNS TCPIP Apache Tomcat XML HTML and UNIXLinux Education Bachelor of Technology in Computer Science Osmania University Hyderabad Andhra Pradesh Additional Information Technical Skills HadoopBig Data HDFSMapReduceHBasePigHiveSqoopFlumeMongoDB Cassandra Power pivot Puppet Oozie Zookeeper Java J2EE Technologies Core Java Servlets JSP JDBC JNDI Java Beans IDEs Eclipse Net beans Big Data Analytics Defamer 205 Frameworks MVC Struts Hibernate Spring Programming languages CC Java Python Ant scripts Linux shell scripts Databases Oracle 11g10g9i MySQL DB2 MSSQL Server Web Servers Web Logic Web Sphere Apache Tomcat Web Technologies HTML XML JavaScript AJAX SOAP WSDL Network Protocols TCPIP UDP HTTP DNS DHCP ETL Tools Informatica Pentaho Testing Win Runner Load Runner QTP",
    "unique_id": "60ad3ec3-5298-417c-8619-429afdefe288"
}