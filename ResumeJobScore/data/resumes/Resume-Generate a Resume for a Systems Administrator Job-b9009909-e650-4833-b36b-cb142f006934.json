{
    "clean_data": "Senior Big Data Developer Spark Senior Big Data span lDeveloperspan Spark Senior Big Data Developer Spark Huntington Bank Piscataway NJ 6 years IT experience with 3 years in Big Data Hadoop development and 3 years in JavaJ2EE technologies Sound domain knowledge in the area of banking insurance healthcare and catering Certified Cloudera Spark and Hadoop Developer Oracle Java SE 8 Programmer I II Hands on Experience in Hadoop ecosystem including HDFS Spark Hive Pig Sqoop Impala Kafka Oozie Flume NiFi HBase ZooKeeper and MapReduce Expertise in Java Scala C C and scripting languages like Python Experience in Spark Streaming Spark SQL in a production environment Handson experience on RDD architecture implementing Spark operations and optimizing transformations Experienced with distributions including Amazon Web Service and Cloudera CHD 5 Worked on building configuring monitoring and supporting Cloudera Hadoop CHD 5 Extensive experience in data ingestion technologies such as Flume Kafka and Sqoop Utilize Kafka NiFi and Flume to gain realtime and nearreal time streaming data in HDFS from different sources Extensive experience in creating Hive tables and queries using HiveQL Experience in Hive partitions and bucketing to optimize performance Experience in designing time driven and data driven automated workflow using Oozie Experience in NoSQL databases such as HBase 098 and MongoDB 32 and ensured faster access to data on HDFS Worked with RDBMS including MySQL 55 Oracle 10g and PostgreSQL 9x Extracted data from log files and push into HDFS using Flume In depth understanding of Hadoop Architecture workload management schedulers scalability and various components such as HDFS MapReduce and YARN Good knowledge of Data Mining Machine Learning and Statistical Modeling algorithms including KMeans Decision tree Perceptron Winnow Linear regression SVM Adaboost Neural Network and Navie Bayes Experienced in Machine Learning and Data Mining with Python R and Java Skilled at Data Visualization with Tableau Good knowledge in UNIX shell commands Hands on experience in MVC architecture and J2EE frameworks like Struts 2 Spring MVC Experience in web development using HTML CSS Javascript JQuery and Hibernate Familiar with Agile methodology standards and Test Driven Development Extensive Experience in Unit Testing with JUnit MRUnit and Pytest Excellent communication skills Successfully working in fastpaced multitasking environment in collaborative team a selfmotivated enthusiastic learner TECHNICAL S KILLS Hadoop Ecosystems Web Technologies SOAP REST JSP 20 JavaScript Servlet PHP HTML5 HDFS MapReduce HBase Spark 13 Hive Pig Kafka 12 Sqoop Flume NiFi Impala Oozie ZooKeeper Operation Systems Programming Languages Linux CentOS Ubuntu Windows Mac OS Java 6 Scala 210 Python C C R PHP SQL JavaScript Pig Latin Machine Learning algorithms Regression Perceptron Naive Bayes Kmeans Relational NoSQL Databases Decision tree SVM MySQL Oracle PostgreSQL HBase MongoD Work Experience Senior Big Data Developer Spark Huntington Bank Indianapolis IN November 2015 to Present Description Huntington Bank is a fullservice banking provider Their business involves retail and commercial financial services to individuals families and business The purpose of this project is to build a real time solution for credit card fraud detection based on technologies including Hadoop Spark Streaming and Apache Kafka Also we built new processing pipelines over transaction records user profiles files and communication data ranging from emails instant messages social media feeds Responsibilities Designed and implemented scalable infrastructure and platform for large amounts of data ingestion aggregation integration and analytics in Hadoop including Spark Hive Pig and HBase Loaded large sets of structured semistructured and unstructured data with Sqoop and Flume Wrote Sqoop scripts to import export and update the data between HDFS and Relational databases Created Flume configure file to collect aggregate and store the web log and event data Loaded transformed and analyzed data using Hive queries HiveQL Configured the Spark cluster as well as integrating it with the existing Hadoop cluster Utilized Kafka to capture and process real time and nearreal time streaming data Developed Spark programs in Scala to perform data transformation data streaming and analysis Utilized historical data stored in HDFS and HBase to build machine learning model which can be used to make predictions on live events Worked with analytics team to build statistical model with Spark MLlib Worked with Oozie and Zookeeper to manage job workflow and job coordination in the cluster Performed unit testing using JUnit and PyTest Environment Hadoop HDFS Spark Streaming Zookeeper Oozie HBase Hive Sqoop Flume Kafka Junit PyTest Scala Big Data Developer Hadoop The Hanover Insurance Group Somerset NJ October 2013 to September 2015 Description The Hanover Insurance Group is the holding company for several property and casualty insurance This company mainly focused on home auto and business insurance it also offers wide variety of flexibility and claims The project is to build a fully distributed HDFS and integrate necessary Hadoop tools Meanwhile our team needs to support analytic team to build risk management models for insurance product analysis and innovation Responsibilities Responsible for building scalable distributed data solutions using Hadoop on Amazon EC2 Installed and configured Hadoop clusters and Hadoop tools for application development including Hive Pig Sqoop Flume Zookeeper and Oozie Created multiple Hive tables with partitioning and bucketing for efficient data access Extracted and loaded customer data from databases to HDFS and Hive tables using Sqoop Used Flume to transfer log source files to HDFS Performed data transformations cleaning and filtering using Pig and Hive Worked with analytic team to prepare and visualize tables in Tableau for reporting Developed workflow in Ooize to automate the tasks of loading the data into HDFS and preprocessing with Pig Performed unit testing using JUnit and MRUnit Environment Hadoop HDFS YARN MapReduce Sqoop Flume Hive Pig Zookeeper Oozie Oracle JUnit MRUnit Java Developer JPMorgan Chase Brooklyn NY February 2012 to August 2013 Description JPMorgan Chase is one of the largest banks in the United States It provides services including markets insurance investment products and standard banking transactions The aim of this project is to create a web application that bank phone officers will be able to view credit card information and query different transactions such as address change credit card summary and card reissue Responsibilities Developed the application implementing Spring MVC architecture with Hibernate as ORM framework Developed user interface by using JSP HTML5 CSS3 and JavaScript Implemented DAO using JDBC for database connectivity to MySQL database Wrote SQL for querying inserting and managing as required on the database object Implemented user input validations using JavaScript and JQuery Developed test cases and performed unit test using JUnit framework Used Agile methodology for the development of the project Environment Eclipse Java Spring MVC Hibernate JSP HTML CSS JavaScript MySQL JUnit Java Developer JDcom May 2011 to September 2011 Description JDcom is a Chinese electronic commerce company It is the largest B2C online retailers in China and a major competitor to Alibaba TaoBao The project is to design and implement different modules including product recommendation and some webpage implementation Responsibilities Designed and developed of application using Spring MVC framework with Agile methodlogy Developed JSP and HTML5 pages using CSS and JavaScript as part of the presentation layer Hibernate framework is used in persistence layer for mapping an objectoriented domain model to database Developed database schema and SQL queries for querying inserting and managing database Implemented various design patterns in the project such as Data Transfer Object Data Access Object and Singleton Used Maven scripts to fetch build and deploy application to development environment Created RESTful web service interface to Javabased runtime engine Used JUnit for functional and unit testing code Environment Eclipse Java Spring MVC Hibernate JSP HTML CSS JavaScript Maven RESTful Oracle JUnit Front End Developer Chung How Kitchen Stony Brook NY December 2010 to April 2011 Descriptions Chung How Kitchen is a Chinese restaurant in Stony Brook NY Chung How Kitchen managed to display the restaurant information to their customers This project implemented interactive navigation to the website Responsibilities Communicated with clients to clearly define project specifications plans and layouts Created layouts and wireframes by using Adobe Dreamweaver Developed UserInterface views using HTML CSS Bootstrap Implemented fundamental web functions using JavaScript and JQuery Fixed cross browser compatibility issues for Chrome Firefox Safari and IE Implemented dynamic web applications using AJAX and JSON Developed functional prototypes and iterations for testing Environment Eclipse Adobe Dreamweaver Java HTML CSS BootStrap JavaScript JQuery AJAX Education Master of Science in Computer Science Purdue University West Lafayette IN Bachelor of Science in Computer Science Stony Brook University Stony Brook NY",
    "entities": [
        "Data Transfer Object Data Access Object",
        "HTML CSS Bootstrap Implemented",
        "AJAX",
        "Relational",
        "HiveQL Configured",
        "Responsibilities Communicated",
        "HDFS",
        "UNIX",
        "The Hanover Insurance Group",
        "PyTest Environment Hadoop HDFS Spark Streaming Zookeeper Oozie HBase Hive",
        "Oozie Oracle JUnit",
        "Big Data Hadoop",
        "TECHNICAL S KILLS Hadoop Ecosystems Web Technologies SOAP REST JSP",
        "RDD",
        "Hadoop",
        "JUnit",
        "HBase",
        "Spark Streaming Spark",
        "JavaJ2EE",
        "JQuery Developed",
        "Cloudera Hadoop",
        "Created RESTful",
        "SVM Adaboost Neural Network",
        "Utilized",
        "Spring MVC",
        "HDFS Performed",
        "Data Mining",
        "JSP HTML5 CSS3",
        "HTML5",
        "TaoBao",
        "Present Description Huntington Bank",
        "JSP",
        "log files",
        "MVC",
        "Oozie Created",
        "Spark",
        "Agile",
        "Oracle JUnit Front End Developer Chung",
        "Data Visualization",
        "Description JPMorgan Chase",
        "Sqoop",
        "Oozie Experience",
        "Created",
        "IE Implemented",
        "China",
        "HTML CSS Javascript JQuery and Hibernate Familiar",
        "Hadoop Architecture",
        "Windows Mac",
        "Singleton",
        "Latin Machine Learning",
        "Hadoop Developer Oracle Java",
        "Responsibilities Responsible",
        "SQL",
        "Science in Computer Science Purdue University West Lafayette",
        "Amazon EC2 Installed",
        "the United States",
        "Indianapolis",
        "Hive",
        "MRUnit Environment Hadoop HDFS YARN MapReduce Sqoop",
        "Handson",
        "Regression Perceptron Naive",
        "Maven",
        "MapReduce Expertise",
        "Performed",
        "JavaScript",
        "Ooize",
        "Spark Hive Pig",
        "MapReduce HBase Spark",
        "JSON Developed",
        "CSS",
        "Hadoop Spark Streaming",
        "Amazon Web Service",
        "Test Driven Development Extensive",
        "Oracle PostgreSQL",
        "Data Mining Machine Learning",
        "NoSQL",
        "Tableau",
        "Spark MLlib Worked",
        "Brooklyn",
        "Machine Learning",
        "JQuery Fixed",
        "Impala Oozie ZooKeeper Operation Systems Programming Languages Linux",
        "Flume Wrote",
        "SVM"
    ],
    "experience": "Experience in Hadoop ecosystem including HDFS Spark Hive Pig Sqoop Impala Kafka Oozie Flume NiFi HBase ZooKeeper and MapReduce Expertise in Java Scala C C and scripting languages like Python Experience in Spark Streaming Spark SQL in a production environment Handson experience on RDD architecture implementing Spark operations and optimizing transformations Experienced with distributions including Amazon Web Service and Cloudera CHD 5 Worked on building configuring monitoring and supporting Cloudera Hadoop CHD 5 Extensive experience in data ingestion technologies such as Flume Kafka and Sqoop Utilize Kafka NiFi and Flume to gain realtime and nearreal time streaming data in HDFS from different sources Extensive experience in creating Hive tables and queries using HiveQL Experience in Hive partitions and bucketing to optimize performance Experience in designing time driven and data driven automated workflow using Oozie Experience in NoSQL databases such as HBase 098 and MongoDB 32 and ensured faster access to data on HDFS Worked with RDBMS including MySQL 55 Oracle 10 g and PostgreSQL 9x Extracted data from log files and push into HDFS using Flume In depth understanding of Hadoop Architecture workload management schedulers scalability and various components such as HDFS MapReduce and YARN Good knowledge of Data Mining Machine Learning and Statistical Modeling algorithms including KMeans Decision tree Perceptron Winnow Linear regression SVM Adaboost Neural Network and Navie Bayes Experienced in Machine Learning and Data Mining with Python R and Java Skilled at Data Visualization with Tableau Good knowledge in UNIX shell commands Hands on experience in MVC architecture and J2EE frameworks like Struts 2 Spring MVC Experience in web development using HTML CSS Javascript JQuery and Hibernate Familiar with Agile methodology standards and Test Driven Development Extensive Experience in Unit Testing with JUnit MRUnit and Pytest Excellent communication skills Successfully working in fastpaced multitasking environment in collaborative team a selfmotivated enthusiastic learner TECHNICAL S KILLS Hadoop Ecosystems Web Technologies SOAP REST JSP 20 JavaScript Servlet PHP HTML5 HDFS MapReduce HBase Spark 13 Hive Pig Kafka 12 Sqoop Flume NiFi Impala Oozie ZooKeeper Operation Systems Programming Languages Linux CentOS Ubuntu Windows Mac OS Java 6 Scala 210 Python C C R PHP SQL JavaScript Pig Latin Machine Learning algorithms Regression Perceptron Naive Bayes Kmeans Relational NoSQL Databases Decision tree SVM MySQL Oracle PostgreSQL HBase MongoD Work Experience Senior Big Data Developer Spark Huntington Bank Indianapolis IN November 2015 to Present Description Huntington Bank is a fullservice banking provider Their business involves retail and commercial financial services to individuals families and business The purpose of this project is to build a real time solution for credit card fraud detection based on technologies including Hadoop Spark Streaming and Apache Kafka Also we built new processing pipelines over transaction records user profiles files and communication data ranging from emails instant messages social media feeds Responsibilities Designed and implemented scalable infrastructure and platform for large amounts of data ingestion aggregation integration and analytics in Hadoop including Spark Hive Pig and HBase Loaded large sets of structured semistructured and unstructured data with Sqoop and Flume Wrote Sqoop scripts to import export and update the data between HDFS and Relational databases Created Flume configure file to collect aggregate and store the web log and event data Loaded transformed and analyzed data using Hive queries HiveQL Configured the Spark cluster as well as integrating it with the existing Hadoop cluster Utilized Kafka to capture and process real time and nearreal time streaming data Developed Spark programs in Scala to perform data transformation data streaming and analysis Utilized historical data stored in HDFS and HBase to build machine learning model which can be used to make predictions on live events Worked with analytics team to build statistical model with Spark MLlib Worked with Oozie and Zookeeper to manage job workflow and job coordination in the cluster Performed unit testing using JUnit and PyTest Environment Hadoop HDFS Spark Streaming Zookeeper Oozie HBase Hive Sqoop Flume Kafka Junit PyTest Scala Big Data Developer Hadoop The Hanover Insurance Group Somerset NJ October 2013 to September 2015 Description The Hanover Insurance Group is the holding company for several property and casualty insurance This company mainly focused on home auto and business insurance it also offers wide variety of flexibility and claims The project is to build a fully distributed HDFS and integrate necessary Hadoop tools Meanwhile our team needs to support analytic team to build risk management models for insurance product analysis and innovation Responsibilities Responsible for building scalable distributed data solutions using Hadoop on Amazon EC2 Installed and configured Hadoop clusters and Hadoop tools for application development including Hive Pig Sqoop Flume Zookeeper and Oozie Created multiple Hive tables with partitioning and bucketing for efficient data access Extracted and loaded customer data from databases to HDFS and Hive tables using Sqoop Used Flume to transfer log source files to HDFS Performed data transformations cleaning and filtering using Pig and Hive Worked with analytic team to prepare and visualize tables in Tableau for reporting Developed workflow in Ooize to automate the tasks of loading the data into HDFS and preprocessing with Pig Performed unit testing using JUnit and MRUnit Environment Hadoop HDFS YARN MapReduce Sqoop Flume Hive Pig Zookeeper Oozie Oracle JUnit MRUnit Java Developer JPMorgan Chase Brooklyn NY February 2012 to August 2013 Description JPMorgan Chase is one of the largest banks in the United States It provides services including markets insurance investment products and standard banking transactions The aim of this project is to create a web application that bank phone officers will be able to view credit card information and query different transactions such as address change credit card summary and card reissue Responsibilities Developed the application implementing Spring MVC architecture with Hibernate as ORM framework Developed user interface by using JSP HTML5 CSS3 and JavaScript Implemented DAO using JDBC for database connectivity to MySQL database Wrote SQL for querying inserting and managing as required on the database object Implemented user input validations using JavaScript and JQuery Developed test cases and performed unit test using JUnit framework Used Agile methodology for the development of the project Environment Eclipse Java Spring MVC Hibernate JSP HTML CSS JavaScript MySQL JUnit Java Developer JDcom May 2011 to September 2011 Description JDcom is a Chinese electronic commerce company It is the largest B2C online retailers in China and a major competitor to Alibaba TaoBao The project is to design and implement different modules including product recommendation and some webpage implementation Responsibilities Designed and developed of application using Spring MVC framework with Agile methodlogy Developed JSP and HTML5 pages using CSS and JavaScript as part of the presentation layer Hibernate framework is used in persistence layer for mapping an objectoriented domain model to database Developed database schema and SQL queries for querying inserting and managing database Implemented various design patterns in the project such as Data Transfer Object Data Access Object and Singleton Used Maven scripts to fetch build and deploy application to development environment Created RESTful web service interface to Javabased runtime engine Used JUnit for functional and unit testing code Environment Eclipse Java Spring MVC Hibernate JSP HTML CSS JavaScript Maven RESTful Oracle JUnit Front End Developer Chung How Kitchen Stony Brook NY December 2010 to April 2011 Descriptions Chung How Kitchen is a Chinese restaurant in Stony Brook NY Chung How Kitchen managed to display the restaurant information to their customers This project implemented interactive navigation to the website Responsibilities Communicated with clients to clearly define project specifications plans and layouts Created layouts and wireframes by using Adobe Dreamweaver Developed UserInterface views using HTML CSS Bootstrap Implemented fundamental web functions using JavaScript and JQuery Fixed cross browser compatibility issues for Chrome Firefox Safari and IE Implemented dynamic web applications using AJAX and JSON Developed functional prototypes and iterations for testing Environment Eclipse Adobe Dreamweaver Java HTML CSS BootStrap JavaScript JQuery AJAX Education Master of Science in Computer Science Purdue University West Lafayette IN Bachelor of Science in Computer Science Stony Brook University Stony Brook NY",
    "extracted_keywords": [
        "Big",
        "Data",
        "Developer",
        "Spark",
        "Senior",
        "Big",
        "Data",
        "span",
        "lDeveloperspan",
        "Spark",
        "Senior",
        "Big",
        "Data",
        "Developer",
        "Spark",
        "Huntington",
        "Bank",
        "Piscataway",
        "NJ",
        "years",
        "years",
        "Big",
        "Data",
        "Hadoop",
        "development",
        "years",
        "JavaJ2EE",
        "technologies",
        "domain",
        "knowledge",
        "area",
        "banking",
        "insurance",
        "healthcare",
        "Certified",
        "Cloudera",
        "Spark",
        "Hadoop",
        "Developer",
        "Oracle",
        "Java",
        "SE",
        "Programmer",
        "II",
        "Hands",
        "Experience",
        "Hadoop",
        "ecosystem",
        "HDFS",
        "Spark",
        "Hive",
        "Pig",
        "Sqoop",
        "Impala",
        "Kafka",
        "Oozie",
        "Flume",
        "NiFi",
        "HBase",
        "ZooKeeper",
        "MapReduce",
        "Expertise",
        "Java",
        "Scala",
        "C",
        "C",
        "scripting",
        "languages",
        "Python",
        "Experience",
        "Spark",
        "Streaming",
        "Spark",
        "SQL",
        "production",
        "environment",
        "Handson",
        "experience",
        "RDD",
        "architecture",
        "Spark",
        "operations",
        "transformations",
        "distributions",
        "Amazon",
        "Web",
        "Service",
        "Cloudera",
        "CHD",
        "monitoring",
        "Cloudera",
        "Hadoop",
        "CHD",
        "experience",
        "data",
        "ingestion",
        "technologies",
        "Flume",
        "Kafka",
        "Sqoop",
        "Utilize",
        "Kafka",
        "NiFi",
        "Flume",
        "time",
        "data",
        "HDFS",
        "sources",
        "experience",
        "Hive",
        "tables",
        "queries",
        "HiveQL",
        "Experience",
        "Hive",
        "partitions",
        "performance",
        "Experience",
        "time",
        "data",
        "workflow",
        "Oozie",
        "Experience",
        "NoSQL",
        "databases",
        "HBase",
        "access",
        "data",
        "HDFS",
        "RDBMS",
        "MySQL",
        "Oracle",
        "g",
        "PostgreSQL",
        "9x",
        "data",
        "log",
        "files",
        "HDFS",
        "Flume",
        "depth",
        "understanding",
        "Hadoop",
        "Architecture",
        "workload",
        "management",
        "scalability",
        "components",
        "HDFS",
        "MapReduce",
        "YARN",
        "knowledge",
        "Data",
        "Mining",
        "Machine",
        "Learning",
        "Statistical",
        "Modeling",
        "algorithms",
        "KMeans",
        "Decision",
        "tree",
        "Perceptron",
        "Winnow",
        "Linear",
        "regression",
        "SVM",
        "Adaboost",
        "Neural",
        "Network",
        "Navie",
        "Bayes",
        "Machine",
        "Learning",
        "Data",
        "Mining",
        "Python",
        "R",
        "Java",
        "Skilled",
        "Data",
        "Visualization",
        "Tableau",
        "knowledge",
        "UNIX",
        "shell",
        "Hands",
        "experience",
        "MVC",
        "architecture",
        "J2EE",
        "frameworks",
        "Struts",
        "Spring",
        "MVC",
        "Experience",
        "web",
        "development",
        "HTML",
        "CSS",
        "Javascript",
        "JQuery",
        "Hibernate",
        "Familiar",
        "methodology",
        "standards",
        "Test",
        "Driven",
        "Development",
        "Extensive",
        "Experience",
        "Unit",
        "Testing",
        "JUnit",
        "MRUnit",
        "Pytest",
        "Excellent",
        "communication",
        "skills",
        "environment",
        "team",
        "learner",
        "TECHNICAL",
        "S",
        "KILLS",
        "Hadoop",
        "Ecosystems",
        "Web",
        "Technologies",
        "SOAP",
        "REST",
        "JSP",
        "JavaScript",
        "Servlet",
        "PHP",
        "HTML5",
        "HDFS",
        "MapReduce",
        "HBase",
        "Spark",
        "Hive",
        "Pig",
        "Kafka",
        "Sqoop",
        "Flume",
        "NiFi",
        "Impala",
        "Oozie",
        "ZooKeeper",
        "Operation",
        "Systems",
        "Programming",
        "Languages",
        "Linux",
        "CentOS",
        "Ubuntu",
        "Windows",
        "Mac",
        "OS",
        "Java",
        "Scala",
        "Python",
        "C",
        "C",
        "R",
        "PHP",
        "SQL",
        "JavaScript",
        "Pig",
        "Latin",
        "Machine",
        "Learning",
        "Regression",
        "Perceptron",
        "Naive",
        "Bayes",
        "Kmeans",
        "Relational",
        "NoSQL",
        "Decision",
        "tree",
        "SVM",
        "MySQL",
        "Oracle",
        "PostgreSQL",
        "HBase",
        "MongoD",
        "Work",
        "Experience",
        "Senior",
        "Big",
        "Data",
        "Developer",
        "Spark",
        "Huntington",
        "Bank",
        "Indianapolis",
        "November",
        "Present",
        "Description",
        "Huntington",
        "Bank",
        "fullservice",
        "banking",
        "provider",
        "business",
        "services",
        "individuals",
        "families",
        "business",
        "purpose",
        "project",
        "time",
        "solution",
        "credit",
        "card",
        "fraud",
        "detection",
        "technologies",
        "Hadoop",
        "Spark",
        "Streaming",
        "Apache",
        "Kafka",
        "processing",
        "pipelines",
        "transaction",
        "records",
        "user",
        "profiles",
        "files",
        "communication",
        "data",
        "emails",
        "messages",
        "media",
        "Responsibilities",
        "infrastructure",
        "platform",
        "amounts",
        "data",
        "ingestion",
        "aggregation",
        "integration",
        "analytics",
        "Hadoop",
        "Spark",
        "Hive",
        "Pig",
        "HBase",
        "sets",
        "data",
        "Sqoop",
        "Flume",
        "Wrote",
        "Sqoop",
        "export",
        "data",
        "HDFS",
        "Relational",
        "databases",
        "Created",
        "Flume",
        "configure",
        "file",
        "aggregate",
        "web",
        "log",
        "event",
        "data",
        "data",
        "Hive",
        "HiveQL",
        "Configured",
        "Spark",
        "cluster",
        "Hadoop",
        "cluster",
        "Kafka",
        "time",
        "time",
        "streaming",
        "data",
        "Spark",
        "programs",
        "Scala",
        "data",
        "transformation",
        "data",
        "streaming",
        "analysis",
        "data",
        "HDFS",
        "HBase",
        "machine",
        "learning",
        "model",
        "predictions",
        "events",
        "analytics",
        "team",
        "model",
        "Spark",
        "MLlib",
        "Oozie",
        "Zookeeper",
        "job",
        "workflow",
        "job",
        "coordination",
        "cluster",
        "Performed",
        "unit",
        "testing",
        "JUnit",
        "PyTest",
        "Environment",
        "Hadoop",
        "HDFS",
        "Spark",
        "Streaming",
        "Zookeeper",
        "Oozie",
        "HBase",
        "Hive",
        "Sqoop",
        "Flume",
        "Kafka",
        "Junit",
        "PyTest",
        "Scala",
        "Big",
        "Data",
        "Developer",
        "Hadoop",
        "The",
        "Hanover",
        "Insurance",
        "Group",
        "Somerset",
        "NJ",
        "October",
        "September",
        "Description",
        "The",
        "Hanover",
        "Insurance",
        "Group",
        "company",
        "property",
        "casualty",
        "insurance",
        "company",
        "home",
        "auto",
        "business",
        "insurance",
        "variety",
        "flexibility",
        "project",
        "HDFS",
        "Hadoop",
        "tools",
        "team",
        "team",
        "risk",
        "management",
        "models",
        "insurance",
        "product",
        "analysis",
        "innovation",
        "Responsibilities",
        "data",
        "solutions",
        "Hadoop",
        "Amazon",
        "EC2",
        "Installed",
        "Hadoop",
        "clusters",
        "Hadoop",
        "tools",
        "application",
        "development",
        "Hive",
        "Pig",
        "Sqoop",
        "Flume",
        "Zookeeper",
        "Oozie",
        "Hive",
        "tables",
        "data",
        "access",
        "customer",
        "data",
        "databases",
        "HDFS",
        "Hive",
        "tables",
        "Sqoop",
        "Used",
        "Flume",
        "log",
        "source",
        "files",
        "HDFS",
        "Performed",
        "data",
        "transformations",
        "filtering",
        "Pig",
        "Hive",
        "Worked",
        "team",
        "tables",
        "Tableau",
        "Developed",
        "workflow",
        "Ooize",
        "tasks",
        "data",
        "HDFS",
        "Pig",
        "Performed",
        "unit",
        "testing",
        "JUnit",
        "MRUnit",
        "Environment",
        "Hadoop",
        "HDFS",
        "YARN",
        "MapReduce",
        "Sqoop",
        "Flume",
        "Hive",
        "Pig",
        "Zookeeper",
        "Oozie",
        "Oracle",
        "JUnit",
        "MRUnit",
        "Java",
        "Developer",
        "JPMorgan",
        "Chase",
        "Brooklyn",
        "NY",
        "February",
        "August",
        "Description",
        "JPMorgan",
        "Chase",
        "banks",
        "United",
        "States",
        "services",
        "markets",
        "insurance",
        "investment",
        "products",
        "banking",
        "transactions",
        "aim",
        "project",
        "web",
        "application",
        "bank",
        "phone",
        "officers",
        "credit",
        "card",
        "information",
        "transactions",
        "address",
        "change",
        "credit",
        "card",
        "summary",
        "card",
        "reissue",
        "Responsibilities",
        "application",
        "Spring",
        "MVC",
        "architecture",
        "Hibernate",
        "ORM",
        "framework",
        "user",
        "interface",
        "JSP",
        "HTML5",
        "CSS3",
        "JavaScript",
        "DAO",
        "JDBC",
        "database",
        "connectivity",
        "MySQL",
        "database",
        "Wrote",
        "SQL",
        "inserting",
        "database",
        "object",
        "user",
        "input",
        "validations",
        "JavaScript",
        "JQuery",
        "test",
        "cases",
        "unit",
        "test",
        "JUnit",
        "framework",
        "methodology",
        "development",
        "project",
        "Environment",
        "Eclipse",
        "Java",
        "Spring",
        "MVC",
        "Hibernate",
        "JSP",
        "HTML",
        "CSS",
        "JavaScript",
        "MySQL",
        "JUnit",
        "Java",
        "Developer",
        "JDcom",
        "May",
        "September",
        "Description",
        "JDcom",
        "commerce",
        "company",
        "B2C",
        "retailers",
        "China",
        "competitor",
        "Alibaba",
        "TaoBao",
        "project",
        "modules",
        "product",
        "recommendation",
        "webpage",
        "implementation",
        "Responsibilities",
        "application",
        "Spring",
        "MVC",
        "framework",
        "methodlogy",
        "JSP",
        "HTML5",
        "pages",
        "CSS",
        "JavaScript",
        "part",
        "presentation",
        "layer",
        "Hibernate",
        "framework",
        "persistence",
        "layer",
        "domain",
        "model",
        "database",
        "schema",
        "SQL",
        "inserting",
        "database",
        "design",
        "patterns",
        "project",
        "Data",
        "Transfer",
        "Object",
        "Data",
        "Access",
        "Object",
        "Singleton",
        "Maven",
        "scripts",
        "build",
        "application",
        "development",
        "environment",
        "web",
        "service",
        "interface",
        "runtime",
        "engine",
        "JUnit",
        "unit",
        "testing",
        "code",
        "Environment",
        "Eclipse",
        "Java",
        "Spring",
        "MVC",
        "Hibernate",
        "JSP",
        "HTML",
        "CSS",
        "JavaScript",
        "Maven",
        "RESTful",
        "Oracle",
        "JUnit",
        "Front",
        "End",
        "Developer",
        "Chung",
        "Kitchen",
        "Stony",
        "Brook",
        "NY",
        "December",
        "April",
        "Descriptions",
        "Chung",
        "Kitchen",
        "restaurant",
        "Stony",
        "Brook",
        "NY",
        "Chung",
        "Kitchen",
        "restaurant",
        "information",
        "customers",
        "project",
        "navigation",
        "website",
        "clients",
        "project",
        "specifications",
        "plans",
        "layouts",
        "layouts",
        "wireframes",
        "Adobe",
        "Dreamweaver",
        "Developed",
        "UserInterface",
        "views",
        "HTML",
        "CSS",
        "Bootstrap",
        "web",
        "functions",
        "JavaScript",
        "JQuery",
        "Fixed",
        "cross",
        "browser",
        "compatibility",
        "issues",
        "Chrome",
        "Firefox",
        "Safari",
        "IE",
        "web",
        "applications",
        "AJAX",
        "prototypes",
        "iterations",
        "Environment",
        "Eclipse",
        "Adobe",
        "Dreamweaver",
        "Java",
        "HTML",
        "CSS",
        "BootStrap",
        "JavaScript",
        "JQuery",
        "AJAX",
        "Education",
        "Master",
        "Science",
        "Computer",
        "Science",
        "Purdue",
        "University",
        "West",
        "Lafayette",
        "Bachelor",
        "Science",
        "Computer",
        "Science",
        "Stony",
        "Brook",
        "University",
        "Stony",
        "Brook",
        "NY"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T22:42:55.586462",
    "resume_data": "Senior Big Data Developer Spark Senior Big Data span lDeveloperspan Spark Senior Big Data Developer Spark Huntington Bank Piscataway NJ 6 years IT experience with 3 years in Big Data Hadoop development and 3 years in JavaJ2EE technologies Sound domain knowledge in the area of banking insurance healthcare and catering Certified Cloudera Spark and Hadoop Developer Oracle Java SE 8 Programmer I II Hands on Experience in Hadoop ecosystem including HDFS Spark Hive Pig Sqoop Impala Kafka Oozie Flume NiFi HBase ZooKeeper and MapReduce Expertise in Java Scala C C and scripting languages like Python Experience in Spark Streaming Spark SQL in a production environment Handson experience on RDD architecture implementing Spark operations and optimizing transformations Experienced with distributions including Amazon Web Service and Cloudera CHD 5 Worked on building configuring monitoring and supporting Cloudera Hadoop CHD 5 Extensive experience in data ingestion technologies such as Flume Kafka and Sqoop Utilize Kafka NiFi and Flume to gain realtime and nearreal time streaming data in HDFS from different sources Extensive experience in creating Hive tables and queries using HiveQL Experience in Hive partitions and bucketing to optimize performance Experience in designing time driven and data driven automated workflow using Oozie Experience in NoSQL databases such as HBase 098 and MongoDB 32 and ensured faster access to data on HDFS Worked with RDBMS including MySQL 55 Oracle 10g and PostgreSQL 9x Extracted data from log files and push into HDFS using Flume In depth understanding of Hadoop Architecture workload management schedulers scalability and various components such as HDFS MapReduce and YARN Good knowledge of Data Mining Machine Learning and Statistical Modeling algorithms including KMeans Decision tree Perceptron Winnow Linear regression SVM Adaboost Neural Network and Navie Bayes Experienced in Machine Learning and Data Mining with Python R and Java Skilled at Data Visualization with Tableau Good knowledge in UNIX shell commands Hands on experience in MVC architecture and J2EE frameworks like Struts 2 Spring MVC Experience in web development using HTML CSS Javascript JQuery and Hibernate Familiar with Agile methodology standards and Test Driven Development Extensive Experience in Unit Testing with JUnit MRUnit and Pytest Excellent communication skills Successfully working in fastpaced multitasking environment in collaborative team a selfmotivated enthusiastic learner TECHNICAL S KILLS Hadoop Ecosystems Web Technologies SOAP REST JSP 20 JavaScript Servlet PHP HTML5 HDFS MapReduce HBase Spark 13 Hive Pig Kafka 12 Sqoop Flume NiFi Impala Oozie ZooKeeper Operation Systems Programming Languages Linux CentOS Ubuntu Windows Mac OS Java 6 Scala 210 Python C C R PHP SQL JavaScript Pig Latin Machine Learning algorithms Regression Perceptron Naive Bayes Kmeans Relational NoSQL Databases Decision tree SVM MySQL Oracle PostgreSQL HBase MongoD Work Experience Senior Big Data Developer Spark Huntington Bank Indianapolis IN November 2015 to Present Description Huntington Bank is a fullservice banking provider Their business involves retail and commercial financial services to individuals families and business The purpose of this project is to build a real time solution for credit card fraud detection based on technologies including Hadoop Spark Streaming and Apache Kafka Also we built new processing pipelines over transaction records user profiles files and communication data ranging from emails instant messages social media feeds Responsibilities Designed and implemented scalable infrastructure and platform for large amounts of data ingestion aggregation integration and analytics in Hadoop including Spark Hive Pig and HBase Loaded large sets of structured semistructured and unstructured data with Sqoop and Flume Wrote Sqoop scripts to import export and update the data between HDFS and Relational databases Created Flume configure file to collect aggregate and store the web log and event data Loaded transformed and analyzed data using Hive queries HiveQL Configured the Spark cluster as well as integrating it with the existing Hadoop cluster Utilized Kafka to capture and process real time and nearreal time streaming data Developed Spark programs in Scala to perform data transformation data streaming and analysis Utilized historical data stored in HDFS and HBase to build machine learning model which can be used to make predictions on live events Worked with analytics team to build statistical model with Spark MLlib Worked with Oozie and Zookeeper to manage job workflow and job coordination in the cluster Performed unit testing using JUnit and PyTest Environment Hadoop HDFS Spark Streaming Zookeeper Oozie HBase Hive Sqoop Flume Kafka Junit PyTest Scala Big Data Developer Hadoop The Hanover Insurance Group Somerset NJ October 2013 to September 2015 Description The Hanover Insurance Group is the holding company for several property and casualty insurance This company mainly focused on home auto and business insurance it also offers wide variety of flexibility and claims The project is to build a fully distributed HDFS and integrate necessary Hadoop tools Meanwhile our team needs to support analytic team to build risk management models for insurance product analysis and innovation Responsibilities Responsible for building scalable distributed data solutions using Hadoop on Amazon EC2 Installed and configured Hadoop clusters and Hadoop tools for application development including Hive Pig Sqoop Flume Zookeeper and Oozie Created multiple Hive tables with partitioning and bucketing for efficient data access Extracted and loaded customer data from databases to HDFS and Hive tables using Sqoop Used Flume to transfer log source files to HDFS Performed data transformations cleaning and filtering using Pig and Hive Worked with analytic team to prepare and visualize tables in Tableau for reporting Developed workflow in Ooize to automate the tasks of loading the data into HDFS and preprocessing with Pig Performed unit testing using JUnit and MRUnit Environment Hadoop HDFS YARN MapReduce Sqoop Flume Hive Pig Zookeeper Oozie Oracle JUnit MRUnit Java Developer JPMorgan Chase Brooklyn NY February 2012 to August 2013 Description JPMorgan Chase is one of the largest banks in the United States It provides services including markets insurance investment products and standard banking transactions The aim of this project is to create a web application that bank phone officers will be able to view credit card information and query different transactions such as address change credit card summary and card reissue Responsibilities Developed the application implementing Spring MVC architecture with Hibernate as ORM framework Developed user interface by using JSP HTML5 CSS3 and JavaScript Implemented DAO using JDBC for database connectivity to MySQL database Wrote SQL for querying inserting and managing as required on the database object Implemented user input validations using JavaScript and JQuery Developed test cases and performed unit test using JUnit framework Used Agile methodology for the development of the project Environment Eclipse Java Spring MVC Hibernate JSP HTML CSS JavaScript MySQL JUnit Java Developer JDcom May 2011 to September 2011 Description JDcom is a Chinese electronic commerce company It is the largest B2C online retailers in China and a major competitor to Alibaba TaoBao The project is to design and implement different modules including product recommendation and some webpage implementation Responsibilities Designed and developed of application using Spring MVC framework with Agile methodlogy Developed JSP and HTML5 pages using CSS and JavaScript as part of the presentation layer Hibernate framework is used in persistence layer for mapping an objectoriented domain model to database Developed database schema and SQL queries for querying inserting and managing database Implemented various design patterns in the project such as Data Transfer Object Data Access Object and Singleton Used Maven scripts to fetch build and deploy application to development environment Created RESTful web service interface to Javabased runtime engine Used JUnit for functional and unit testing code Environment Eclipse Java Spring MVC Hibernate JSP HTML CSS JavaScript Maven RESTful Oracle JUnit Front End Developer Chung How Kitchen Stony Brook NY December 2010 to April 2011 Descriptions Chung How Kitchen is a Chinese restaurant in Stony Brook NY Chung How Kitchen managed to display the restaurant information to their customers This project implemented interactive navigation to the website Responsibilities Communicated with clients to clearly define project specifications plans and layouts Created layouts and wireframes by using Adobe Dreamweaver Developed UserInterface views using HTML CSS Bootstrap Implemented fundamental web functions using JavaScript and JQuery Fixed cross browser compatibility issues for Chrome Firefox Safari and IE Implemented dynamic web applications using AJAX and JSON Developed functional prototypes and iterations for testing Environment Eclipse Adobe Dreamweaver Java HTML CSS BootStrap JavaScript JQuery AJAX Education Master of Science in Computer Science Purdue University West Lafayette IN Bachelor of Science in Computer Science Stony Brook University Stony Brook NY",
    "unique_id": "b9009909-e650-4833-b36b-cb142f006934"
}