{
    "clean_data": "Sr Big Data Developer Sr Big Data span lDeveloperspan Sr Big Data Developer Anthem Inc Atlanta GA Overall 10 years of working experience as a Big DataHadoop Developer in designed and developed various applications like big data Hadoop JavaJ2EE opensource technologies Extensive knowledge in Software Development Lifecycle SDLC using Waterfall Agile methodologies Having Project experience on Microsoft Azure Cloud cluster environment and Azure Data lake store Experience in importing and exporting data using Sqoop from HDFS to Relational Database Systems and viceversa Experience in converting Hive queries into Spark transformations using Spark RDDs and Scala Good Knowledge in Amazon Web Service AWS concepts like EMR and EC2 web services which provides fast Good experience in Spark and its related technologies like Spark SQL and Spark Streaming Good knowledge of NOSQL databases like Mongo DB Cassandra and HBase Experience on building the applications using Spark Core Spark SQL Data Frames Spark Streaming Strong experience in developing the workflows using Apache Oozie framework to automate tasks Experience in deploying J2EE applications on Apache Tomcat web server and Web Logic WebSphere JBoss application server Experience in working with Eclipse IDE Net Beans and Rational Application Developer Expertise on usage of SQL queries to extract data from RDBMS databases MySQL DB2 and Oracle Experience with installing backup recovery configuration and development on multiple Hadoop distribution platforms Cloudera and Hortonworks including cloud platforms Amazon AWS and Google Cloud Experience on data ingestion tool Apache NiFi used to extract data from various data sources into Hadoop data lake Experience in working with NoSQL database Apache HBase and have implemented performance improvement as per project requirement Proficient in using and deploying applications to Web ServersApplication servers like Tomcat WebSphere Micro services Experience in generating logging by Log4j to identify the errors in production test environment Extensive knowledge in programming with Resilient Distributed Datasets RDDs Experience in working different version control tools like GIT SVN and Continuous Integration tool Jenkins Hands on Experience in developing and implementing the Spring Rest and Restful Web Services Expertise in writing Hadoop Jobs to analyze data using MapReduce Apache Crunch Hive Pig and SOLR Splunk Experience in working with Web Servers like Apache Tomcat and Application Servers like IBM WebSphere and JBOSS Experience in Apache Flume for collecting aggregating and moving huge chunks of data from various sources such as web server telnet sources etc Experience in working with different scripting technologies like Python UNIX shell scripts Strong team player ability to work independently and in a team as well ability to adapt to rapidly changing environment commitment towards learning Possess excellent Work Experience Sr Big Data Developer Anthem Inc Atlanta GA March 2018 to Present Responsibilities As a Sr Big Data Developer I worked on Hadoop ecosystems including Hive HBase Oozie Pig Zookeeper Spark Streaming MCS MapR Control System and so on with MapR distribution Developed Java and J2EE applications using Rapid Application Development RAD Eclipse Installed and configured Hadoop MapReduce HDFS Developed multiple MapReduce jobs in Java for data cleaning and Preprocessing Create data integration and technical solutions for Azure Data Lake Analytics Azure Data Lake Storage Azure Data Factory Azure SQL databases and Azure SQL Data Warehouse for providing analytics Built code for real time data ingestion using Java MapRStreams Kafka and STORM Involved in various phases of development analysed and developed the system going through Agile Scrum methodology Involved in Design development and testing of web application and integration projects using Object Oriented technologies such as Core Java J2EE Struts JSP JDBC Spring Framework Hibernate Java Beans Web Services RESTSOAP XML XSLT XSL and Ant Worked on Apache Solr which is used as indexing and search engine Primarily involved in Data Migration process using Azure by integrating with GitHub repository and Jenkins Used Spark API over Hortonworks Hadoop YARN to perform analytics on data in Hive Developed various Servlets and Java Interfaces as part of the integration and process flow required for the system Automated workflows using shell scripts and ControlM jobs to pull data from various databases into Hadoop Data Lake Developed Spring Framework Controllers and worked on spring application framework features Prepared Linux shell scripts to configure deploy and manage Oozie workflows of Big Data applications Worked with different data sources like Avro data files XML files JSON files SQL server and Oracle to load data into Hive tables Developed Use Case Diagrams Object Diagrams and Class Diagrams in UML using Rational Rose Exported event weblogs to HDFS by creating a HDFS sink which directly deposits the weblogs in HDFS Implemented a distributed messaging queue to integrate with Cassandra using Apache Kafka and Zookeeper Imported weblogs unstructured data using the Apache Flume and stores the data in Flume channel Developed scripts to run scheduled batch cycles using Oozie and present data for reports Managed real time data processing and real time Data Ingestion in MongoDB using Storm Developed using the framework builds the graphical components and define actions popup menus in XML Primarily involved in Data Migration process using Azure by integrating with Github repository and Jenkins Used Jenkins for build and continuous integration for software development Provided connections using JDBC to the database and developed SQL queries to manipulate the data Monitored Hadoop cluster using tools like Cloudera manager managing and scheduling the jobs on Hadoop cluster Used Spark to create the structured data from large amount of unstructured data from various sources Designed and implemented scalable Cloud Data and Analytical architecture solutions for various public and private cloud platforms using Azure Extracted Real time feed using Spark streaming and convert it to RDD and process data into Data Frame and load the data into Cassandra Performed importing data from various sources to the Cassandra cluster using Sqoop Environment Hadoop 30 Hive 23 HBase 22 Oozie 51 Pig 017 Zookeeper 34 Spark 24 MapR MapReduce HDFS Java MS Azure Kafka 22 Agile Jenkins 216 Hortonworks Cassandra 311 XML MongoDB Sr SparkHadoop Developer Pacific Life Newport Beach CA October 2015 to February 2018 Responsibilities Responsible for building scalable distributed data solutions using Hadoop Participate in all stages of Software Development Life Cycle SDLC including requirements gathering system Analysis system development unit testing and performance testing Installed Oozie workflow engine to run multiple Hive Shell Script Sqoop pig and Java jobs In depth understanding of Hadoop Architecture including YARN and various components such as HDFS Resource Manager Node Manager Name Node Data Node and MR v1 v2 concepts Data Ingestion Experience in importing and exporting data from different RDBMS Servers like MySQL Oracle and Teradata into HDFS and Hive using Sqoop Experience in ingesting data from FTPSFTP servers using Flume Experience in developing Kafka Consumer API using Spark Scala applications Data Processing Developed MapReduce programs in Java for data cleansing data filtering and data aggregation Experienced in analyzing the data using PIG Latin scripts Experience in designing table partitioning bucketing and optimized hive scripts using different performance utilities and techniques Experience in developing Hive UDFs and running hive scripts using different execution engines like Tez and Spark Hive on Spark Experience in designing tables and views for reporting using Impala Experienced in Developing Spark application using Spark Core Spark SQL and Spark Streaming APIs Developed Java Beans for business logic Design of REST APIs that allow sophisticated effective and low cost application integrations Implemented Spark using Scala and Spark SQL for faster testing and processing of data Involved in storydriven Agile development methodology and actively participated in daily scrum meetings Developed Apache sparkbased programs to implement complex business transformations Used Hive to analyze partitioned and bucketed data and compute various metrics for reporting Developed unloading micro services using Scala API in Spark Data frame API for the semantic layer Used SparkSQL and Spark Data frame extensively to cleanse and integrate imported data into more meaningful insights Migrated associated business logic PLSQL proceduresfunctions to Apache Spark Involved in Unit testing and delivered Unit test plans and results documents using JUnit and MrUnit Used Spark API over ClouderaHadoopYARN to perform analytics on data in Hive Created scripts to sync data between local MongoDB and Postgres databases with those on AWS Cloud Used Oozie engine for creating workflow and coordinator jobs that schedule and execute various Created Kafka producer API to send livestream json data into various Kafka topics Created concurrent access for Hive tables with shared and exclusive locking that can be enabled in Hive with the help of Zookeeper implementation in the cluster Created webbased User interface for creating monitoring and controlling data flows using Apache Nifi Worked on a POC to compare processing time of Impala with Apache Hive for batch applications to implement the former in project Extensively used Spark Cassandra connector to load data to and from Cassandra Involved in the Design Phase for getting live event data from the database to the frontend application using Spark Ecosystem Worked on various compression and file formats like Avro Parquet and Text formats Created a complete processing engine based on Cloudera distribution enhanced performance Worked with NoSQL databases like HBase in creating HBase tables to load large sets of semi structured data coming from various sources Written eventdriven link tracking system to capture user events and feed to Kafka to push it to HBase Implemented Spark using Scala and performed cleansing of data by applying Transformations and Actions Developed MapReduce programs in Java to search production logs and web analytics logs for application issues Involved in builddeploy applications using Maven and integrated with CICD server Jenkins Environment Hadoop 30 Oozie 50 Hive 23 Sqoop 14 Pig 017 Java Spark 24 Scala 213 Agile PLSQL JUnit 54 AWS Kafka 22 Zookeeper 34 Cassandra 311 HCatalog NoSQL Maven 36 Jenkins 216 Java Hadoop Developer Comcast Philadelphia PA January 2013 to September 2015 Responsibilities Worked on analyzing Hadoop cluster and different big data analytic tools including MapReduce Hive and Spark Implemented Spark RDD transformations to map business analysis and apply actions on top of transformations Developed the server side scripts using JMS JSP and Java Beans Adding and modifying Hibernate configuration code and JavaSQL statements depending upon the specific database access requirements Implemented POC to migrate MapReduce programs into Spark transformations using Spark and Scala Implemented Storm topologies to preprocess data before moving into HDFS system Worked with various Hadoop file formats including Text Sequence File RCFILE and ORC File Written a Pig Scripts to read data from HDFS and write into Hive Table Managed servers on the Amazon Web Services AWS platform instances using Puppet Chef Configuration management Interacted with Cloudera support and log the issues in Cloudera portal and fixing them as per the recommendations Migrated complex MapReduce programs into Spark RDD transformations actions Involved in installing configuring and managing Hadoop Ecosystem components like Pig Sqoop Kafka and Flume Involved in the process of data acquisition data preprocessing and data exploration of telecommunication project in Scala Developed and Deployed Oozie Workflows for recurring operations on Clusters Created POC on Hortonworks and suggested the best practice in terms HDP HDF platform Developed and implemented Apache NIFI across various environments written QA scripts in Python for tracking files Worked in AWS environment for development and deployment of Custom Hadoop Applications Converted text files into Avro then to parquet format for the file to be used with other Hadoop Ecosystem tools Implemented Apache Pig scripts to load data from and to store data into Hive using HCatalog Loaded data into the cluster from dynamically generated files using Flume and from relational database management systems using Sqoop Created HBase tables to store various data formats of incoming data from different portfolios Designed and implemented Cassandra and associated Restful webservice Expertise in writing Hadoop Jobs for analyzing data using Hive QL Queries Pig Latin Data flow language and custom MapReduce programs in Java Implemented AJAX JSON and JavaScript to create interactive web screens Environment Hadoop 23 MapReduce Hive 18 Spark 20 Scala 17 HDFS AWS Puppet Oozie 30 Hortonworks AJAX JSON JavaScript Java JavaJ2EE Developer Amica Mutual Insurance Company Lincoln RI August 2011 to December 2012 Responsibilities Developed and utilized J2EE Services and JMS components for messaging communication in WebSphere Application Server Developed the code based on the design using Tiles MVC Struts framework and using J2EE patterns Developed the MVC applicationmodel using Spring framework Spring Boot Micro services and used Hibernate framework to interact with the database Developed the custom Logging framework used to log transactions executed across the various applications using Log4j Used Maven in building the application and auto deploying it to the environment Extensively used JQuery to provide dynamic User Interface and for the clientside validations Developed dynamic proxies to consume the web services developed in JAXWS standards for CRM module Written and executed test cases for unit testing using Mockito JUNIT framework Designed and developed business components using Session and Entity Beans in EJB Extensively used Eclipse IDE for developing debugging integrating and deploying the application Used Bit bucket for the repository and version management through SourceTree for GIT Developed user interface using JSTL HTML JavaScript JQuery and CSS Created clean and readable API documentation using Slate to assist our current and future developers Involved in writing test cases using JUnit and integrated these tests with Jenkins Continuous integration tool Extensively used Eclipse IDE for developing debugging integrating and deploying the application Involved in exporting and importing integrations and jar files from development staging and production environments using WebLogic Developed real time tracking of class schedules using Node JS socketio based on socket technology Express JS framework Involved in Unit integration bug fixing acceptance testing with test cases Code reviews Extensively used Java MultiThreading concept for downloading files from a URL Implemented Service Oriented Architecture by developing Java web services using WSDL and SOAP Used frameworks such as Angular backbonejs to a handful of web applications Involved in fixing defects in application worked in JSF managed beans converters validator and configuration files Mapped business objects to database using Hibernate and used JPA annotations for mapping DB to objects Environment JQuery JavaScript HTML5 CSS3 JUNIT Maven MVC Struts Eclipse JUnit Java Developer Glansa Solutions Hyderabad Telangana April 2009 to July 2011 Responsibilities Worked as a Java Developer and involved in application and database design Involved in requirement analysis and participated in the design of the application using UML and OO Analysis Design and Development Around 6 years of IT Industry experience as java developer in software development life cycle core area of object oriented and webbased enterprise applications using javaj2ee technology Experience in applicationweb servers like IBM Web Sphere Web Logic Application Servers JBoss and Tomcat Web Servers Proficient in ntier application design and development using Java J2EE JSP Servlets Struts 20 Spring and Oracle Expertise in developing applications using Core Java Multithreading Collections Swing development Memory Management Application utilities J2EE JSP Servlets Java Beans Web Services SOAP WSDL UDDI AngularJS JMS JDBC JSON Ajax Expertise in HTML XHTML HTML5 ML5 CSS AJAX jQuery JSTL specification XML SAX DOM XSL XSLT JAXP JAXB DTD Resource properties Expert in Various Agile methodologies like SCRUM Test Driven Development TTD Incremental and Iteration methodology Agile Development Testing using Software Development Life Cycle Experienced in Development Testing and Deployment of enterprise applications on Windows Linux and UNIX platforms using IDEs such as Eclipse STS Rational Application Developer RAD NetBeans IntelliJ14x15 Experienced in developing complex PLSQL queries Procedures Triggers Stored Procedures Packages and Views in various databases such as Oracle DB2 and MySQL SQL Server Working Knowledge in JSON and XML technologies such as XSL XSLT XPath parsers like SAX DOM and JAXB Developed the Application using Spring MVC Framework by implementing Controller Service classes Performed JUnit testing to test the implemented services Used ANT scripts to build the application and deployed on Web Sphere Application Server Implemented spring javabased SOAP Web Services for authorization and JUnit tests for part of my code Managed Spring Core for dependency injection of control IOC and integrated with Hibernate Worked on core Java concepts like Collections and Exception Handling for writing the backend APIs Developed DAO and service layers using the Spring Dao support and Hibernate ORM mapping Created test cases for DAO Layer and service layer using JUnit and bug tracking using JIRA Decomposed existing monolithic code base into Spring Boot Microservices Extensively used Eclipse IDE for developing debugging integrating and deploying the application Involved in implementation of MVC pattern using Angular JS and Spring Controller Used JavaScript user input validated using regular expressions and also in the server side Developed business logic using Java Struts Action classes and deployed using Tomcat Used SVN for version control across common source code used by developers Implemented MVC web frameworks for the web applications using JSPServletTag libraries that were designed using JSP Integrated Subversion SVN into Jenkins to automate the code checkout process Implemented modules using Core Java APIs Java collection Threads XML and integrating the modules Environment Java MVC JUnit ANT Hibernate JUnit Eclipse Angular JS JavaScript Struts MVC Education Bachelors Skills CASSANDRA HDFS IMPALA MAPREDUCE OOZIE SQOOP HBASE KAFKA FLUME HADOOP JBOSS JMS MONGODB NOSQL ANALYSIS SERVICES APPLICATION SERVER Git Hadoop HBase Hive Additional Information Technical Skills HadoopBig Data Technologies Hadoop 30 HDFS MapReduce HBase 14 Apache Pig Hive 23 Sqoop 14 Apache Impala 21 Oozie 43 Yarn Apache Flume 18 Kafka 11 Zookeeper Cloud Platform Amazon AWS EC2 EC3 MS Azure Azure SQL Database Azure SQL Data Warehouse Azure Analysis Services HDInsight Azure Data Lake Data Factory Hadoop Distributions Cloudera Hortonworks MapR Programming Language Java Scala Python 36 SQL PLSQL Shell Scripting Storm 10 JSP Servlets Frameworks Spring 505 Hibernate 52 Struts 13 JSF EJB JMS Web Technologies HTML CSS JavaScript JQuery 33 Bootstrap 41 XML JSON AJAX Databases Oracle 12c11g SQL Operating Systems Linux Unix Windows 1087 IDE and Tools Eclipse 47 NetBeans 82 IntelliJ Maven NoSQL Databases HBase 14 Cassandra 311 MongoDB Accumulo WebApplication Server Apache Tomcat 907 JBoss Web Logic Web Sphere SDLC Methodologies Agile Waterfall Version Control GIT SVN CVS",
    "entities": [
        "Implemented Spark",
        "Installed Oozie",
        "Hive Table Managed",
        "JSP Servlets Frameworks",
        "Preprocessing Create",
        "MapReduce Hive",
        "Git Hadoop HBase Hive Additional Information Technical Skills",
        "Hortonworks Hadoop",
        "HDFS",
        "UNIX",
        "Core Java Multithreading Collections Swing",
        "Java J2EE JSP Servlets Struts",
        "JMS",
        "Text",
        "the Amazon Web Services AWS",
        "Hive Created",
        "JSON",
        "IBM",
        "Hadoop Ecosystem",
        "GIT Developed",
        "Waterfall Agile methodologies",
        "Jenkins Used Spark",
        "RDD",
        "Hadoop",
        "SAX DOM",
        "XML",
        "Atlanta",
        "NOSQL",
        "Created Kafka",
        "Impala Experienced",
        "JUnit",
        "Data Ingestion Experience",
        "Hive Developed",
        "Present Responsibilities",
        "HBase Implemented Spark",
        "JAXWS",
        "MrUnit Used Spark API",
        "Automated",
        "HBase",
        "Avro",
        "Sqoop Created HBase",
        "Storm Developed",
        "Java Implemented AJAX JSON",
        "Oozie 50 Hive 23",
        "Apache Spark Involved",
        "Developed",
        "SparkSQL",
        "Custom Hadoop Applications Converted",
        "Eclipse STS Rational Application Developer",
        "Spark Data",
        "Node Data",
        "Performed JUnit",
        "Scala Developed",
        "ORC File Written",
        "Possess",
        "Developing Spark",
        "Controller Service",
        "HadoopBig Data Technologies Hadoop",
        "UML",
        "OO Analysis Design and Development",
        "Servlets",
        "JSP Integrated Subversion",
        "Hadoop Participate",
        "Spark Streaming APIs Developed Java Beans",
        "Text Sequence File RCFILE",
        "IntelliJ14x15 Experienced",
        "Tools",
        "Puppet Chef Configuration",
        "SourceTree",
        "JSP",
        "Scala Implemented Storm",
        "Sr Big Data Developer Sr Big Data",
        "Oracle DB2",
        "Google Cloud",
        "IOC",
        "Apache Tomcat and Application Servers",
        "Java Struts Action",
        "Memory Management Application",
        "MapReduce Apache Crunch Hive Pig",
        "XML SAX DOM XSL XSLT JAXP JAXB DTD Resource",
        "HDP",
        "Amica Mutual Insurance Company",
        "Angular JS",
        "MVC",
        "Spark",
        "EJB",
        "Node JS",
        "CSS Created",
        "SOAP Web Services",
        "Hadoop Jobs",
        "API",
        "Sqoop",
        "HDFS Resource",
        "QA",
        "Web Sphere Application Server Implemented",
        "Created",
        "Spark Core Spark",
        "Hadoop Architecture",
        "Oracle",
        "Oracle Experience",
        "XML Primarily",
        "HCatalog Loaded",
        "JSF",
        "Postgres",
        "Clusters Created",
        "ControlM",
        "PIG",
        "FTPSFTP",
        "Slate",
        "Tiles MVC Struts",
        "Jenkins Continuous",
        "Oozie",
        "Hive HBase Oozie",
        "SQL",
        "Log4j",
        "GitHub",
        "Spark RDD",
        "WebLogic Developed",
        "Github",
        "Relational Database Systems",
        "Data Frame",
        "Azure Data",
        "Big Data",
        "Hive",
        "CICD",
        "Spark Implemented Spark",
        "Amazon AWS",
        "Jenkins Environment Hadoop",
        "Maven",
        "Zookeeper",
        "Data Migration",
        "Hibernate",
        "Hibernate Worked",
        "Impala",
        "Spark SQL",
        "Oozie 51",
        "Hadoop JavaJ2EE",
        "JavaScript",
        "Cloud Data",
        "ANT",
        "Azure Data Lake Analytics Azure Data Lake Storage Azure Data Factory",
        "Sr SparkHadoop Developer Pacific Life Newport Beach",
        "Cassandra Involved",
        "Microsoft",
        "Spark Hive on Spark",
        "Continuous Integration",
        "Tomcat WebSphere Micro services",
        "Data Ingestion",
        "Amazon Web Service AWS",
        "Spark Ecosystem Worked",
        "SVN",
        "Ant Worked",
        "Monitored Hadoop",
        "Hadoop Data Lake Developed Spring Framework Controllers",
        "Test Driven Development TTD Incremental and Iteration methodology",
        "MapReduce",
        "NetBeans",
        "WebSphere Application Server Developed",
        "RDBMS",
        "NoSQL",
        "Software Development Life Cycle",
        "JAXB Developed the Application",
        "JQuery",
        "J2EE Services",
        "a URL Implemented Service Oriented Architecture",
        "Oracle Expertise",
        "Cloudera",
        "Hadoop MapReduce HDFS Developed"
    ],
    "experience": "Experience in importing and exporting data using Sqoop from HDFS to Relational Database Systems and viceversa Experience in converting Hive queries into Spark transformations using Spark RDDs and Scala Good Knowledge in Amazon Web Service AWS concepts like EMR and EC2 web services which provides fast Good experience in Spark and its related technologies like Spark SQL and Spark Streaming Good knowledge of NOSQL databases like Mongo DB Cassandra and HBase Experience on building the applications using Spark Core Spark SQL Data Frames Spark Streaming Strong experience in developing the workflows using Apache Oozie framework to automate tasks Experience in deploying J2EE applications on Apache Tomcat web server and Web Logic WebSphere JBoss application server Experience in working with Eclipse IDE Net Beans and Rational Application Developer Expertise on usage of SQL queries to extract data from RDBMS databases MySQL DB2 and Oracle Experience with installing backup recovery configuration and development on multiple Hadoop distribution platforms Cloudera and Hortonworks including cloud platforms Amazon AWS and Google Cloud Experience on data ingestion tool Apache NiFi used to extract data from various data sources into Hadoop data lake Experience in working with NoSQL database Apache HBase and have implemented performance improvement as per project requirement Proficient in using and deploying applications to Web ServersApplication servers like Tomcat WebSphere Micro services Experience in generating logging by Log4j to identify the errors in production test environment Extensive knowledge in programming with Resilient Distributed Datasets RDDs Experience in working different version control tools like GIT SVN and Continuous Integration tool Jenkins Hands on Experience in developing and implementing the Spring Rest and Restful Web Services Expertise in writing Hadoop Jobs to analyze data using MapReduce Apache Crunch Hive Pig and SOLR Splunk Experience in working with Web Servers like Apache Tomcat and Application Servers like IBM WebSphere and JBOSS Experience in Apache Flume for collecting aggregating and moving huge chunks of data from various sources such as web server telnet sources etc Experience in working with different scripting technologies like Python UNIX shell scripts Strong team player ability to work independently and in a team as well ability to adapt to rapidly changing environment commitment towards learning Possess excellent Work Experience Sr Big Data Developer Anthem Inc Atlanta GA March 2018 to Present Responsibilities As a Sr Big Data Developer I worked on Hadoop ecosystems including Hive HBase Oozie Pig Zookeeper Spark Streaming MCS MapR Control System and so on with MapR distribution Developed Java and J2EE applications using Rapid Application Development RAD Eclipse Installed and configured Hadoop MapReduce HDFS Developed multiple MapReduce jobs in Java for data cleaning and Preprocessing Create data integration and technical solutions for Azure Data Lake Analytics Azure Data Lake Storage Azure Data Factory Azure SQL databases and Azure SQL Data Warehouse for providing analytics Built code for real time data ingestion using Java MapRStreams Kafka and STORM Involved in various phases of development analysed and developed the system going through Agile Scrum methodology Involved in Design development and testing of web application and integration projects using Object Oriented technologies such as Core Java J2EE Struts JSP JDBC Spring Framework Hibernate Java Beans Web Services RESTSOAP XML XSLT XSL and Ant Worked on Apache Solr which is used as indexing and search engine Primarily involved in Data Migration process using Azure by integrating with GitHub repository and Jenkins Used Spark API over Hortonworks Hadoop YARN to perform analytics on data in Hive Developed various Servlets and Java Interfaces as part of the integration and process flow required for the system Automated workflows using shell scripts and ControlM jobs to pull data from various databases into Hadoop Data Lake Developed Spring Framework Controllers and worked on spring application framework features Prepared Linux shell scripts to configure deploy and manage Oozie workflows of Big Data applications Worked with different data sources like Avro data files XML files JSON files SQL server and Oracle to load data into Hive tables Developed Use Case Diagrams Object Diagrams and Class Diagrams in UML using Rational Rose Exported event weblogs to HDFS by creating a HDFS sink which directly deposits the weblogs in HDFS Implemented a distributed messaging queue to integrate with Cassandra using Apache Kafka and Zookeeper Imported weblogs unstructured data using the Apache Flume and stores the data in Flume channel Developed scripts to run scheduled batch cycles using Oozie and present data for reports Managed real time data processing and real time Data Ingestion in MongoDB using Storm Developed using the framework builds the graphical components and define actions popup menus in XML Primarily involved in Data Migration process using Azure by integrating with Github repository and Jenkins Used Jenkins for build and continuous integration for software development Provided connections using JDBC to the database and developed SQL queries to manipulate the data Monitored Hadoop cluster using tools like Cloudera manager managing and scheduling the jobs on Hadoop cluster Used Spark to create the structured data from large amount of unstructured data from various sources Designed and implemented scalable Cloud Data and Analytical architecture solutions for various public and private cloud platforms using Azure Extracted Real time feed using Spark streaming and convert it to RDD and process data into Data Frame and load the data into Cassandra Performed importing data from various sources to the Cassandra cluster using Sqoop Environment Hadoop 30 Hive 23 HBase 22 Oozie 51 Pig 017 Zookeeper 34 Spark 24 MapR MapReduce HDFS Java MS Azure Kafka 22 Agile Jenkins 216 Hortonworks Cassandra 311 XML MongoDB Sr SparkHadoop Developer Pacific Life Newport Beach CA October 2015 to February 2018 Responsibilities Responsible for building scalable distributed data solutions using Hadoop Participate in all stages of Software Development Life Cycle SDLC including requirements gathering system Analysis system development unit testing and performance testing Installed Oozie workflow engine to run multiple Hive Shell Script Sqoop pig and Java jobs In depth understanding of Hadoop Architecture including YARN and various components such as HDFS Resource Manager Node Manager Name Node Data Node and MR v1 v2 concepts Data Ingestion Experience in importing and exporting data from different RDBMS Servers like MySQL Oracle and Teradata into HDFS and Hive using Sqoop Experience in ingesting data from FTPSFTP servers using Flume Experience in developing Kafka Consumer API using Spark Scala applications Data Processing Developed MapReduce programs in Java for data cleansing data filtering and data aggregation Experienced in analyzing the data using PIG Latin scripts Experience in designing table partitioning bucketing and optimized hive scripts using different performance utilities and techniques Experience in developing Hive UDFs and running hive scripts using different execution engines like Tez and Spark Hive on Spark Experience in designing tables and views for reporting using Impala Experienced in Developing Spark application using Spark Core Spark SQL and Spark Streaming APIs Developed Java Beans for business logic Design of REST APIs that allow sophisticated effective and low cost application integrations Implemented Spark using Scala and Spark SQL for faster testing and processing of data Involved in storydriven Agile development methodology and actively participated in daily scrum meetings Developed Apache sparkbased programs to implement complex business transformations Used Hive to analyze partitioned and bucketed data and compute various metrics for reporting Developed unloading micro services using Scala API in Spark Data frame API for the semantic layer Used SparkSQL and Spark Data frame extensively to cleanse and integrate imported data into more meaningful insights Migrated associated business logic PLSQL proceduresfunctions to Apache Spark Involved in Unit testing and delivered Unit test plans and results documents using JUnit and MrUnit Used Spark API over ClouderaHadoopYARN to perform analytics on data in Hive Created scripts to sync data between local MongoDB and Postgres databases with those on AWS Cloud Used Oozie engine for creating workflow and coordinator jobs that schedule and execute various Created Kafka producer API to send livestream json data into various Kafka topics Created concurrent access for Hive tables with shared and exclusive locking that can be enabled in Hive with the help of Zookeeper implementation in the cluster Created webbased User interface for creating monitoring and controlling data flows using Apache Nifi Worked on a POC to compare processing time of Impala with Apache Hive for batch applications to implement the former in project Extensively used Spark Cassandra connector to load data to and from Cassandra Involved in the Design Phase for getting live event data from the database to the frontend application using Spark Ecosystem Worked on various compression and file formats like Avro Parquet and Text formats Created a complete processing engine based on Cloudera distribution enhanced performance Worked with NoSQL databases like HBase in creating HBase tables to load large sets of semi structured data coming from various sources Written eventdriven link tracking system to capture user events and feed to Kafka to push it to HBase Implemented Spark using Scala and performed cleansing of data by applying Transformations and Actions Developed MapReduce programs in Java to search production logs and web analytics logs for application issues Involved in builddeploy applications using Maven and integrated with CICD server Jenkins Environment Hadoop 30 Oozie 50 Hive 23 Sqoop 14 Pig 017 Java Spark 24 Scala 213 Agile PLSQL JUnit 54 AWS Kafka 22 Zookeeper 34 Cassandra 311 HCatalog NoSQL Maven 36 Jenkins 216 Java Hadoop Developer Comcast Philadelphia PA January 2013 to September 2015 Responsibilities Worked on analyzing Hadoop cluster and different big data analytic tools including MapReduce Hive and Spark Implemented Spark RDD transformations to map business analysis and apply actions on top of transformations Developed the server side scripts using JMS JSP and Java Beans Adding and modifying Hibernate configuration code and JavaSQL statements depending upon the specific database access requirements Implemented POC to migrate MapReduce programs into Spark transformations using Spark and Scala Implemented Storm topologies to preprocess data before moving into HDFS system Worked with various Hadoop file formats including Text Sequence File RCFILE and ORC File Written a Pig Scripts to read data from HDFS and write into Hive Table Managed servers on the Amazon Web Services AWS platform instances using Puppet Chef Configuration management Interacted with Cloudera support and log the issues in Cloudera portal and fixing them as per the recommendations Migrated complex MapReduce programs into Spark RDD transformations actions Involved in installing configuring and managing Hadoop Ecosystem components like Pig Sqoop Kafka and Flume Involved in the process of data acquisition data preprocessing and data exploration of telecommunication project in Scala Developed and Deployed Oozie Workflows for recurring operations on Clusters Created POC on Hortonworks and suggested the best practice in terms HDP HDF platform Developed and implemented Apache NIFI across various environments written QA scripts in Python for tracking files Worked in AWS environment for development and deployment of Custom Hadoop Applications Converted text files into Avro then to parquet format for the file to be used with other Hadoop Ecosystem tools Implemented Apache Pig scripts to load data from and to store data into Hive using HCatalog Loaded data into the cluster from dynamically generated files using Flume and from relational database management systems using Sqoop Created HBase tables to store various data formats of incoming data from different portfolios Designed and implemented Cassandra and associated Restful webservice Expertise in writing Hadoop Jobs for analyzing data using Hive QL Queries Pig Latin Data flow language and custom MapReduce programs in Java Implemented AJAX JSON and JavaScript to create interactive web screens Environment Hadoop 23 MapReduce Hive 18 Spark 20 Scala 17 HDFS AWS Puppet Oozie 30 Hortonworks AJAX JSON JavaScript Java JavaJ2EE Developer Amica Mutual Insurance Company Lincoln RI August 2011 to December 2012 Responsibilities Developed and utilized J2EE Services and JMS components for messaging communication in WebSphere Application Server Developed the code based on the design using Tiles MVC Struts framework and using J2EE patterns Developed the MVC applicationmodel using Spring framework Spring Boot Micro services and used Hibernate framework to interact with the database Developed the custom Logging framework used to log transactions executed across the various applications using Log4j Used Maven in building the application and auto deploying it to the environment Extensively used JQuery to provide dynamic User Interface and for the clientside validations Developed dynamic proxies to consume the web services developed in JAXWS standards for CRM module Written and executed test cases for unit testing using Mockito JUNIT framework Designed and developed business components using Session and Entity Beans in EJB Extensively used Eclipse IDE for developing debugging integrating and deploying the application Used Bit bucket for the repository and version management through SourceTree for GIT Developed user interface using JSTL HTML JavaScript JQuery and CSS Created clean and readable API documentation using Slate to assist our current and future developers Involved in writing test cases using JUnit and integrated these tests with Jenkins Continuous integration tool Extensively used Eclipse IDE for developing debugging integrating and deploying the application Involved in exporting and importing integrations and jar files from development staging and production environments using WebLogic Developed real time tracking of class schedules using Node JS socketio based on socket technology Express JS framework Involved in Unit integration bug fixing acceptance testing with test cases Code reviews Extensively used Java MultiThreading concept for downloading files from a URL Implemented Service Oriented Architecture by developing Java web services using WSDL and SOAP Used frameworks such as Angular backbonejs to a handful of web applications Involved in fixing defects in application worked in JSF managed beans converters validator and configuration files Mapped business objects to database using Hibernate and used JPA annotations for mapping DB to objects Environment JQuery JavaScript HTML5 CSS3 JUNIT Maven MVC Struts Eclipse JUnit Java Developer Glansa Solutions Hyderabad Telangana April 2009 to July 2011 Responsibilities Worked as a Java Developer and involved in application and database design Involved in requirement analysis and participated in the design of the application using UML and OO Analysis Design and Development Around 6 years of IT Industry experience as java developer in software development life cycle core area of object oriented and webbased enterprise applications using javaj2ee technology Experience in applicationweb servers like IBM Web Sphere Web Logic Application Servers JBoss and Tomcat Web Servers Proficient in ntier application design and development using Java J2EE JSP Servlets Struts 20 Spring and Oracle Expertise in developing applications using Core Java Multithreading Collections Swing development Memory Management Application utilities J2EE JSP Servlets Java Beans Web Services SOAP WSDL UDDI AngularJS JMS JDBC JSON Ajax Expertise in HTML XHTML HTML5 ML5 CSS AJAX jQuery JSTL specification XML SAX DOM XSL XSLT JAXP JAXB DTD Resource properties Expert in Various Agile methodologies like SCRUM Test Driven Development TTD Incremental and Iteration methodology Agile Development Testing using Software Development Life Cycle Experienced in Development Testing and Deployment of enterprise applications on Windows Linux and UNIX platforms using IDEs such as Eclipse STS Rational Application Developer RAD NetBeans IntelliJ14x15 Experienced in developing complex PLSQL queries Procedures Triggers Stored Procedures Packages and Views in various databases such as Oracle DB2 and MySQL SQL Server Working Knowledge in JSON and XML technologies such as XSL XSLT XPath parsers like SAX DOM and JAXB Developed the Application using Spring MVC Framework by implementing Controller Service classes Performed JUnit testing to test the implemented services Used ANT scripts to build the application and deployed on Web Sphere Application Server Implemented spring javabased SOAP Web Services for authorization and JUnit tests for part of my code Managed Spring Core for dependency injection of control IOC and integrated with Hibernate Worked on core Java concepts like Collections and Exception Handling for writing the backend APIs Developed DAO and service layers using the Spring Dao support and Hibernate ORM mapping Created test cases for DAO Layer and service layer using JUnit and bug tracking using JIRA Decomposed existing monolithic code base into Spring Boot Microservices Extensively used Eclipse IDE for developing debugging integrating and deploying the application Involved in implementation of MVC pattern using Angular JS and Spring Controller Used JavaScript user input validated using regular expressions and also in the server side Developed business logic using Java Struts Action classes and deployed using Tomcat Used SVN for version control across common source code used by developers Implemented MVC web frameworks for the web applications using JSPServletTag libraries that were designed using JSP Integrated Subversion SVN into Jenkins to automate the code checkout process Implemented modules using Core Java APIs Java collection Threads XML and integrating the modules Environment Java MVC JUnit ANT Hibernate JUnit Eclipse Angular JS JavaScript Struts MVC Education Bachelors Skills CASSANDRA HDFS IMPALA MAPREDUCE OOZIE SQOOP HBASE KAFKA FLUME HADOOP JBOSS JMS MONGODB NOSQL ANALYSIS SERVICES APPLICATION SERVER Git Hadoop HBase Hive Additional Information Technical Skills HadoopBig Data Technologies Hadoop 30 HDFS MapReduce HBase 14 Apache Pig Hive 23 Sqoop 14 Apache Impala 21 Oozie 43 Yarn Apache Flume 18 Kafka 11 Zookeeper Cloud Platform Amazon AWS EC2 EC3 MS Azure Azure SQL Database Azure SQL Data Warehouse Azure Analysis Services HDInsight Azure Data Lake Data Factory Hadoop Distributions Cloudera Hortonworks MapR Programming Language Java Scala Python 36 SQL PLSQL Shell Scripting Storm 10 JSP Servlets Frameworks Spring 505 Hibernate 52 Struts 13 JSF EJB JMS Web Technologies HTML CSS JavaScript JQuery 33 Bootstrap 41 XML JSON AJAX Databases Oracle 12c11 g SQL Operating Systems Linux Unix Windows 1087 IDE and Tools Eclipse 47 NetBeans 82 IntelliJ Maven NoSQL Databases HBase 14 Cassandra 311 MongoDB Accumulo WebApplication Server Apache Tomcat 907 JBoss Web Logic Web Sphere SDLC Methodologies Agile Waterfall Version Control GIT SVN CVS",
    "extracted_keywords": [
        "Sr",
        "Big",
        "Data",
        "Developer",
        "Sr",
        "Big",
        "Data",
        "span",
        "lDeveloperspan",
        "Sr",
        "Big",
        "Data",
        "Developer",
        "Anthem",
        "Inc",
        "Atlanta",
        "GA",
        "Overall",
        "years",
        "working",
        "experience",
        "Big",
        "DataHadoop",
        "Developer",
        "applications",
        "data",
        "Hadoop",
        "JavaJ2EE",
        "opensource",
        "technologies",
        "knowledge",
        "Software",
        "Development",
        "Lifecycle",
        "SDLC",
        "Waterfall",
        "Agile",
        "methodologies",
        "Project",
        "experience",
        "Microsoft",
        "Azure",
        "Cloud",
        "cluster",
        "environment",
        "Azure",
        "Data",
        "lake",
        "store",
        "Experience",
        "data",
        "Sqoop",
        "HDFS",
        "Relational",
        "Database",
        "Systems",
        "viceversa",
        "Experience",
        "Hive",
        "queries",
        "Spark",
        "transformations",
        "Spark",
        "RDDs",
        "Scala",
        "Good",
        "Knowledge",
        "Amazon",
        "Web",
        "Service",
        "AWS",
        "concepts",
        "EMR",
        "EC2",
        "web",
        "services",
        "experience",
        "Spark",
        "technologies",
        "Spark",
        "SQL",
        "Spark",
        "Streaming",
        "knowledge",
        "NOSQL",
        "Mongo",
        "DB",
        "Cassandra",
        "HBase",
        "Experience",
        "applications",
        "Spark",
        "Core",
        "Spark",
        "SQL",
        "Data",
        "Frames",
        "Spark",
        "Streaming",
        "Strong",
        "experience",
        "workflows",
        "Apache",
        "Oozie",
        "framework",
        "tasks",
        "Experience",
        "J2EE",
        "applications",
        "Apache",
        "Tomcat",
        "web",
        "server",
        "Web",
        "Logic",
        "WebSphere",
        "JBoss",
        "application",
        "server",
        "Experience",
        "Eclipse",
        "IDE",
        "Net",
        "Beans",
        "Rational",
        "Application",
        "Developer",
        "Expertise",
        "usage",
        "SQL",
        "data",
        "RDBMS",
        "MySQL",
        "DB2",
        "Oracle",
        "Experience",
        "backup",
        "recovery",
        "configuration",
        "development",
        "Hadoop",
        "distribution",
        "Cloudera",
        "Hortonworks",
        "cloud",
        "Amazon",
        "AWS",
        "Google",
        "Cloud",
        "Experience",
        "data",
        "ingestion",
        "tool",
        "Apache",
        "NiFi",
        "data",
        "data",
        "sources",
        "Hadoop",
        "data",
        "lake",
        "Experience",
        "NoSQL",
        "database",
        "Apache",
        "HBase",
        "performance",
        "improvement",
        "project",
        "requirement",
        "Proficient",
        "applications",
        "Web",
        "ServersApplication",
        "servers",
        "Tomcat",
        "WebSphere",
        "Micro",
        "Experience",
        "Log4j",
        "errors",
        "production",
        "test",
        "environment",
        "knowledge",
        "programming",
        "Resilient",
        "Datasets",
        "RDDs",
        "Experience",
        "version",
        "control",
        "tools",
        "GIT",
        "SVN",
        "Continuous",
        "Integration",
        "tool",
        "Jenkins",
        "Hands",
        "Experience",
        "Spring",
        "Rest",
        "Restful",
        "Web",
        "Services",
        "Expertise",
        "Hadoop",
        "Jobs",
        "data",
        "MapReduce",
        "Apache",
        "Crunch",
        "Hive",
        "Pig",
        "SOLR",
        "Splunk",
        "Experience",
        "Web",
        "Servers",
        "Apache",
        "Tomcat",
        "Application",
        "Servers",
        "IBM",
        "WebSphere",
        "JBOSS",
        "Experience",
        "Apache",
        "Flume",
        "chunks",
        "data",
        "sources",
        "web",
        "server",
        "telnet",
        "sources",
        "Experience",
        "scripting",
        "technologies",
        "Python",
        "UNIX",
        "shell",
        "team",
        "player",
        "ability",
        "team",
        "ability",
        "environment",
        "commitment",
        "Possess",
        "Work",
        "Experience",
        "Sr",
        "Big",
        "Data",
        "Developer",
        "Anthem",
        "Inc",
        "Atlanta",
        "GA",
        "March",
        "Present",
        "Responsibilities",
        "Sr",
        "Big",
        "Data",
        "Developer",
        "Hadoop",
        "ecosystems",
        "Hive",
        "HBase",
        "Oozie",
        "Pig",
        "Zookeeper",
        "Spark",
        "Streaming",
        "MCS",
        "MapR",
        "Control",
        "System",
        "MapR",
        "distribution",
        "Developed",
        "Java",
        "J2EE",
        "applications",
        "Rapid",
        "Application",
        "Development",
        "RAD",
        "Eclipse",
        "Installed",
        "Hadoop",
        "MapReduce",
        "MapReduce",
        "jobs",
        "Java",
        "data",
        "cleaning",
        "Preprocessing",
        "Create",
        "data",
        "integration",
        "solutions",
        "Azure",
        "Data",
        "Lake",
        "Analytics",
        "Azure",
        "Data",
        "Lake",
        "Storage",
        "Azure",
        "Data",
        "Factory",
        "Azure",
        "SQL",
        "Azure",
        "SQL",
        "Data",
        "Warehouse",
        "analytics",
        "code",
        "time",
        "data",
        "ingestion",
        "Java",
        "MapRStreams",
        "Kafka",
        "STORM",
        "phases",
        "development",
        "system",
        "Agile",
        "Scrum",
        "methodology",
        "Design",
        "development",
        "testing",
        "web",
        "application",
        "integration",
        "projects",
        "Object",
        "technologies",
        "Core",
        "Java",
        "J2EE",
        "Struts",
        "JSP",
        "JDBC",
        "Spring",
        "Framework",
        "Hibernate",
        "Java",
        "Beans",
        "Web",
        "Services",
        "RESTSOAP",
        "XML",
        "XSLT",
        "XSL",
        "Ant",
        "Worked",
        "Apache",
        "Solr",
        "indexing",
        "search",
        "engine",
        "Data",
        "Migration",
        "process",
        "Azure",
        "GitHub",
        "repository",
        "Jenkins",
        "Spark",
        "API",
        "Hortonworks",
        "Hadoop",
        "YARN",
        "analytics",
        "data",
        "Hive",
        "Servlets",
        "Java",
        "Interfaces",
        "part",
        "integration",
        "process",
        "flow",
        "system",
        "Automated",
        "workflows",
        "scripts",
        "ControlM",
        "jobs",
        "data",
        "databases",
        "Hadoop",
        "Data",
        "Lake",
        "Developed",
        "Spring",
        "Framework",
        "Controllers",
        "spring",
        "application",
        "framework",
        "Prepared",
        "Linux",
        "shell",
        "scripts",
        "configure",
        "deploy",
        "Oozie",
        "workflows",
        "Data",
        "applications",
        "data",
        "sources",
        "Avro",
        "data",
        "XML",
        "files",
        "JSON",
        "files",
        "SQL",
        "server",
        "Oracle",
        "data",
        "Hive",
        "tables",
        "Developed",
        "Use",
        "Case",
        "Diagrams",
        "Object",
        "Diagrams",
        "Class",
        "Diagrams",
        "UML",
        "Rational",
        "Rose",
        "event",
        "HDFS",
        "HDFS",
        "sink",
        "weblogs",
        "HDFS",
        "queue",
        "Cassandra",
        "Apache",
        "Kafka",
        "Zookeeper",
        "Imported",
        "data",
        "Apache",
        "Flume",
        "data",
        "Flume",
        "channel",
        "scripts",
        "batch",
        "cycles",
        "Oozie",
        "data",
        "reports",
        "time",
        "data",
        "processing",
        "time",
        "Data",
        "Ingestion",
        "MongoDB",
        "Storm",
        "framework",
        "components",
        "actions",
        "popup",
        "menus",
        "XML",
        "Data",
        "Migration",
        "process",
        "Azure",
        "Github",
        "repository",
        "Jenkins",
        "Jenkins",
        "build",
        "integration",
        "software",
        "development",
        "connections",
        "JDBC",
        "database",
        "SQL",
        "queries",
        "data",
        "Monitored",
        "Hadoop",
        "cluster",
        "tools",
        "Cloudera",
        "manager",
        "jobs",
        "Hadoop",
        "cluster",
        "Spark",
        "data",
        "amount",
        "data",
        "sources",
        "Cloud",
        "Data",
        "Analytical",
        "architecture",
        "solutions",
        "cloud",
        "platforms",
        "Azure",
        "time",
        "feed",
        "Spark",
        "streaming",
        "process",
        "data",
        "Data",
        "Frame",
        "data",
        "Cassandra",
        "Performed",
        "data",
        "sources",
        "Cassandra",
        "cluster",
        "Sqoop",
        "Environment",
        "Hadoop",
        "Hive",
        "HBase",
        "Oozie",
        "Pig",
        "Zookeeper",
        "Spark",
        "MapR",
        "MapReduce",
        "HDFS",
        "Java",
        "MS",
        "Azure",
        "Kafka",
        "Agile",
        "Jenkins",
        "Hortonworks",
        "Cassandra",
        "XML",
        "Sr",
        "SparkHadoop",
        "Developer",
        "Pacific",
        "Life",
        "Newport",
        "Beach",
        "CA",
        "October",
        "February",
        "Responsibilities",
        "data",
        "solutions",
        "Hadoop",
        "Participate",
        "stages",
        "Software",
        "Development",
        "Life",
        "Cycle",
        "SDLC",
        "requirements",
        "system",
        "Analysis",
        "system",
        "development",
        "unit",
        "testing",
        "performance",
        "testing",
        "Oozie",
        "workflow",
        "engine",
        "Hive",
        "Shell",
        "Script",
        "Sqoop",
        "pig",
        "Java",
        "jobs",
        "depth",
        "understanding",
        "Hadoop",
        "Architecture",
        "YARN",
        "components",
        "HDFS",
        "Resource",
        "Manager",
        "Node",
        "Manager",
        "Name",
        "Node",
        "Data",
        "Node",
        "MR",
        "v1",
        "v2",
        "concepts",
        "Data",
        "Ingestion",
        "Experience",
        "data",
        "RDBMS",
        "Servers",
        "MySQL",
        "Oracle",
        "Teradata",
        "HDFS",
        "Hive",
        "Sqoop",
        "Experience",
        "data",
        "servers",
        "Flume",
        "Experience",
        "Kafka",
        "Consumer",
        "API",
        "Spark",
        "Scala",
        "applications",
        "Data",
        "Processing",
        "MapReduce",
        "programs",
        "Java",
        "data",
        "data",
        "filtering",
        "data",
        "aggregation",
        "data",
        "PIG",
        "Latin",
        "scripts",
        "Experience",
        "table",
        "bucketing",
        "hive",
        "scripts",
        "performance",
        "utilities",
        "techniques",
        "Experience",
        "Hive",
        "UDFs",
        "hive",
        "scripts",
        "execution",
        "engines",
        "Tez",
        "Spark",
        "Hive",
        "Spark",
        "Experience",
        "tables",
        "views",
        "reporting",
        "Impala",
        "Spark",
        "application",
        "Spark",
        "Core",
        "Spark",
        "SQL",
        "Spark",
        "Streaming",
        "APIs",
        "Java",
        "Beans",
        "business",
        "logic",
        "Design",
        "REST",
        "APIs",
        "cost",
        "application",
        "integrations",
        "Spark",
        "Scala",
        "Spark",
        "SQL",
        "testing",
        "processing",
        "data",
        "Agile",
        "development",
        "methodology",
        "scrum",
        "meetings",
        "Developed",
        "Apache",
        "programs",
        "business",
        "transformations",
        "Hive",
        "data",
        "metrics",
        "Developed",
        "unloading",
        "micro",
        "services",
        "Scala",
        "API",
        "Spark",
        "Data",
        "frame",
        "API",
        "layer",
        "SparkSQL",
        "Spark",
        "Data",
        "frame",
        "data",
        "insights",
        "business",
        "logic",
        "PLSQL",
        "proceduresfunctions",
        "Apache",
        "Spark",
        "Unit",
        "testing",
        "Unit",
        "test",
        "plans",
        "documents",
        "JUnit",
        "MrUnit",
        "Spark",
        "API",
        "ClouderaHadoopYARN",
        "analytics",
        "data",
        "Hive",
        "scripts",
        "data",
        "MongoDB",
        "Postgres",
        "AWS",
        "Cloud",
        "Oozie",
        "engine",
        "workflow",
        "coordinator",
        "jobs",
        "Created",
        "Kafka",
        "producer",
        "API",
        "livestream",
        "json",
        "data",
        "Kafka",
        "topics",
        "access",
        "Hive",
        "tables",
        "locking",
        "Hive",
        "help",
        "implementation",
        "cluster",
        "Created",
        "User",
        "interface",
        "monitoring",
        "data",
        "flows",
        "Apache",
        "Nifi",
        "POC",
        "processing",
        "time",
        "Impala",
        "Apache",
        "Hive",
        "batch",
        "applications",
        "project",
        "Spark",
        "Cassandra",
        "connector",
        "data",
        "Cassandra",
        "Design",
        "Phase",
        "event",
        "data",
        "database",
        "frontend",
        "application",
        "Spark",
        "Ecosystem",
        "compression",
        "file",
        "formats",
        "Avro",
        "Parquet",
        "Text",
        "formats",
        "processing",
        "engine",
        "Cloudera",
        "distribution",
        "performance",
        "databases",
        "HBase",
        "HBase",
        "tables",
        "sets",
        "data",
        "sources",
        "link",
        "tracking",
        "system",
        "user",
        "events",
        "feed",
        "Kafka",
        "HBase",
        "Spark",
        "Scala",
        "cleansing",
        "data",
        "Transformations",
        "Actions",
        "MapReduce",
        "programs",
        "Java",
        "production",
        "logs",
        "web",
        "analytics",
        "logs",
        "application",
        "issues",
        "builddeploy",
        "applications",
        "Maven",
        "CICD",
        "server",
        "Jenkins",
        "Environment",
        "Hadoop",
        "Oozie",
        "Hive",
        "Sqoop",
        "Pig",
        "Java",
        "Spark",
        "Scala",
        "Agile",
        "PLSQL",
        "JUnit",
        "AWS",
        "Kafka",
        "Zookeeper",
        "Cassandra",
        "HCatalog",
        "NoSQL",
        "Maven",
        "Jenkins",
        "Java",
        "Hadoop",
        "Developer",
        "Comcast",
        "Philadelphia",
        "PA",
        "January",
        "September",
        "Responsibilities",
        "Hadoop",
        "cluster",
        "data",
        "tools",
        "MapReduce",
        "Hive",
        "Spark",
        "Spark",
        "RDD",
        "transformations",
        "business",
        "analysis",
        "actions",
        "top",
        "transformations",
        "server",
        "side",
        "scripts",
        "JMS",
        "JSP",
        "Java",
        "Beans",
        "Hibernate",
        "configuration",
        "code",
        "statements",
        "database",
        "access",
        "requirements",
        "POC",
        "MapReduce",
        "programs",
        "Spark",
        "transformations",
        "Spark",
        "Scala",
        "Storm",
        "topologies",
        "data",
        "HDFS",
        "system",
        "Hadoop",
        "file",
        "formats",
        "Text",
        "Sequence",
        "File",
        "RCFILE",
        "ORC",
        "File",
        "Written",
        "Pig",
        "Scripts",
        "data",
        "HDFS",
        "Hive",
        "Table",
        "servers",
        "Amazon",
        "Web",
        "Services",
        "AWS",
        "platform",
        "instances",
        "Puppet",
        "Chef",
        "Configuration",
        "management",
        "Cloudera",
        "support",
        "issues",
        "Cloudera",
        "portal",
        "recommendations",
        "MapReduce",
        "programs",
        "Spark",
        "RDD",
        "transformations",
        "actions",
        "configuring",
        "Hadoop",
        "Ecosystem",
        "components",
        "Pig",
        "Sqoop",
        "Kafka",
        "Flume",
        "process",
        "data",
        "acquisition",
        "data",
        "data",
        "exploration",
        "telecommunication",
        "project",
        "Scala",
        "Developed",
        "Deployed",
        "Oozie",
        "Workflows",
        "operations",
        "Clusters",
        "Created",
        "POC",
        "Hortonworks",
        "practice",
        "terms",
        "HDP",
        "HDF",
        "platform",
        "Apache",
        "NIFI",
        "environments",
        "QA",
        "scripts",
        "Python",
        "files",
        "AWS",
        "environment",
        "development",
        "deployment",
        "Custom",
        "Hadoop",
        "Applications",
        "text",
        "files",
        "Avro",
        "format",
        "file",
        "Hadoop",
        "Ecosystem",
        "tools",
        "Apache",
        "Pig",
        "scripts",
        "data",
        "data",
        "Hive",
        "HCatalog",
        "Loaded",
        "data",
        "cluster",
        "files",
        "Flume",
        "database",
        "management",
        "systems",
        "Sqoop",
        "Created",
        "HBase",
        "data",
        "formats",
        "data",
        "portfolios",
        "Cassandra",
        "Restful",
        "webservice",
        "Expertise",
        "Hadoop",
        "Jobs",
        "data",
        "Hive",
        "QL",
        "Queries",
        "Pig",
        "Latin",
        "Data",
        "flow",
        "language",
        "custom",
        "MapReduce",
        "programs",
        "Java",
        "Implemented",
        "AJAX",
        "JSON",
        "JavaScript",
        "web",
        "screens",
        "Environment",
        "Hadoop",
        "MapReduce",
        "Hive",
        "Spark",
        "Scala",
        "HDFS",
        "AWS",
        "Puppet",
        "Oozie",
        "Hortonworks",
        "AJAX",
        "JSON",
        "JavaScript",
        "Java",
        "JavaJ2EE",
        "Developer",
        "Amica",
        "Mutual",
        "Insurance",
        "Company",
        "Lincoln",
        "RI",
        "August",
        "December",
        "Responsibilities",
        "J2EE",
        "Services",
        "JMS",
        "components",
        "communication",
        "WebSphere",
        "Application",
        "Server",
        "code",
        "design",
        "Tiles",
        "MVC",
        "Struts",
        "framework",
        "J2EE",
        "patterns",
        "MVC",
        "applicationmodel",
        "Spring",
        "framework",
        "Spring",
        "Boot",
        "Micro",
        "services",
        "Hibernate",
        "framework",
        "database",
        "custom",
        "Logging",
        "framework",
        "transactions",
        "applications",
        "Log4j",
        "Maven",
        "application",
        "auto",
        "environment",
        "JQuery",
        "User",
        "Interface",
        "clientside",
        "proxies",
        "web",
        "services",
        "JAXWS",
        "standards",
        "CRM",
        "module",
        "Written",
        "test",
        "cases",
        "unit",
        "testing",
        "Mockito",
        "JUNIT",
        "framework",
        "business",
        "components",
        "Session",
        "Entity",
        "Beans",
        "EJB",
        "Extensively",
        "Eclipse",
        "IDE",
        "debugging",
        "application",
        "Bit",
        "bucket",
        "repository",
        "version",
        "management",
        "SourceTree",
        "GIT",
        "user",
        "interface",
        "JSTL",
        "HTML",
        "JavaScript",
        "JQuery",
        "CSS",
        "API",
        "documentation",
        "Slate",
        "developers",
        "test",
        "cases",
        "JUnit",
        "tests",
        "Jenkins",
        "Continuous",
        "integration",
        "tool",
        "Eclipse",
        "IDE",
        "debugging",
        "application",
        "integrations",
        "jar",
        "files",
        "development",
        "staging",
        "production",
        "environments",
        "WebLogic",
        "time",
        "tracking",
        "class",
        "schedules",
        "Node",
        "JS",
        "socketio",
        "socket",
        "technology",
        "Express",
        "JS",
        "framework",
        "Unit",
        "integration",
        "bug",
        "acceptance",
        "testing",
        "test",
        "cases",
        "Code",
        "reviews",
        "Java",
        "MultiThreading",
        "concept",
        "files",
        "URL",
        "Service",
        "Oriented",
        "Architecture",
        "Java",
        "web",
        "services",
        "WSDL",
        "frameworks",
        "backbonejs",
        "handful",
        "web",
        "applications",
        "defects",
        "application",
        "JSF",
        "beans",
        "converters",
        "validator",
        "configuration",
        "files",
        "Mapped",
        "business",
        "database",
        "Hibernate",
        "JPA",
        "annotations",
        "mapping",
        "DB",
        "Environment",
        "JQuery",
        "JavaScript",
        "HTML5",
        "CSS3",
        "JUNIT",
        "Maven",
        "MVC",
        "Struts",
        "Eclipse",
        "JUnit",
        "Java",
        "Developer",
        "Glansa",
        "Solutions",
        "Hyderabad",
        "Telangana",
        "April",
        "July",
        "Responsibilities",
        "Java",
        "Developer",
        "application",
        "database",
        "design",
        "requirement",
        "analysis",
        "design",
        "application",
        "UML",
        "OO",
        "Analysis",
        "Design",
        "Development",
        "years",
        "IT",
        "Industry",
        "experience",
        "developer",
        "software",
        "development",
        "life",
        "cycle",
        "core",
        "area",
        "object",
        "enterprise",
        "applications",
        "technology",
        "Experience",
        "servers",
        "IBM",
        "Web",
        "Sphere",
        "Web",
        "Logic",
        "Application",
        "Servers",
        "JBoss",
        "Tomcat",
        "Web",
        "Servers",
        "Proficient",
        "ntier",
        "application",
        "design",
        "development",
        "Java",
        "J2EE",
        "JSP",
        "Servlets",
        "Struts",
        "Spring",
        "Oracle",
        "Expertise",
        "applications",
        "Core",
        "Java",
        "Multithreading",
        "Collections",
        "Swing",
        "development",
        "Memory",
        "Management",
        "Application",
        "utilities",
        "J2EE",
        "JSP",
        "Servlets",
        "Java",
        "Beans",
        "Web",
        "Services",
        "SOAP",
        "WSDL",
        "UDDI",
        "AngularJS",
        "JMS",
        "JDBC",
        "JSON",
        "Ajax",
        "Expertise",
        "HTML",
        "XHTML",
        "HTML5",
        "ML5",
        "CSS",
        "AJAX",
        "jQuery",
        "JSTL",
        "specification",
        "XML",
        "SAX",
        "DOM",
        "XSL",
        "XSLT",
        "JAXP",
        "JAXB",
        "DTD",
        "Resource",
        "properties",
        "Expert",
        "methodologies",
        "SCRUM",
        "Test",
        "Driven",
        "Development",
        "TTD",
        "Incremental",
        "Iteration",
        "methodology",
        "Agile",
        "Development",
        "Testing",
        "Software",
        "Development",
        "Life",
        "Cycle",
        "Development",
        "Testing",
        "Deployment",
        "enterprise",
        "applications",
        "Windows",
        "Linux",
        "UNIX",
        "platforms",
        "IDEs",
        "Eclipse",
        "STS",
        "Rational",
        "Application",
        "Developer",
        "RAD",
        "NetBeans",
        "PLSQL",
        "Procedures",
        "Triggers",
        "Stored",
        "Procedures",
        "Packages",
        "Views",
        "databases",
        "Oracle",
        "DB2",
        "MySQL",
        "SQL",
        "Server",
        "Working",
        "Knowledge",
        "XML",
        "technologies",
        "XSL",
        "XSLT",
        "XPath",
        "parsers",
        "SAX",
        "DOM",
        "JAXB",
        "Application",
        "Spring",
        "MVC",
        "Framework",
        "Controller",
        "Service",
        "classes",
        "Performed",
        "JUnit",
        "testing",
        "services",
        "ANT",
        "scripts",
        "application",
        "Web",
        "Sphere",
        "Application",
        "Server",
        "spring",
        "SOAP",
        "Web",
        "Services",
        "authorization",
        "JUnit",
        "tests",
        "part",
        "code",
        "Managed",
        "Spring",
        "Core",
        "dependency",
        "injection",
        "control",
        "IOC",
        "Hibernate",
        "Worked",
        "core",
        "Java",
        "concepts",
        "Collections",
        "Exception",
        "Handling",
        "APIs",
        "Developed",
        "DAO",
        "service",
        "layers",
        "Spring",
        "Dao",
        "support",
        "Hibernate",
        "ORM",
        "mapping",
        "test",
        "cases",
        "DAO",
        "Layer",
        "service",
        "layer",
        "JUnit",
        "bug",
        "tracking",
        "JIRA",
        "Decomposed",
        "code",
        "base",
        "Spring",
        "Boot",
        "Microservices",
        "Eclipse",
        "IDE",
        "debugging",
        "application",
        "implementation",
        "MVC",
        "pattern",
        "Angular",
        "JS",
        "Spring",
        "Controller",
        "JavaScript",
        "user",
        "input",
        "expressions",
        "server",
        "side",
        "business",
        "logic",
        "Java",
        "Struts",
        "Action",
        "classes",
        "Tomcat",
        "SVN",
        "version",
        "control",
        "source",
        "code",
        "developers",
        "MVC",
        "web",
        "frameworks",
        "web",
        "applications",
        "libraries",
        "JSP",
        "Integrated",
        "Subversion",
        "SVN",
        "Jenkins",
        "code",
        "checkout",
        "process",
        "modules",
        "Core",
        "Java",
        "APIs",
        "Java",
        "collection",
        "Threads",
        "XML",
        "modules",
        "Environment",
        "Java",
        "MVC",
        "JUnit",
        "ANT",
        "Hibernate",
        "JUnit",
        "Eclipse",
        "Angular",
        "JS",
        "JavaScript",
        "Struts",
        "MVC",
        "Education",
        "Bachelors",
        "Skills",
        "CASSANDRA",
        "HDFS",
        "IMPALA",
        "MAPREDUCE",
        "OOZIE",
        "SQOOP",
        "HBASE",
        "KAFKA",
        "FLUME",
        "HADOOP",
        "JBOSS",
        "JMS",
        "NOSQL",
        "ANALYSIS",
        "SERVICES",
        "APPLICATION",
        "SERVER",
        "Git",
        "Hadoop",
        "HBase",
        "Hive",
        "Additional",
        "Information",
        "Technical",
        "Skills",
        "HadoopBig",
        "Data",
        "Technologies",
        "Hadoop",
        "HDFS",
        "MapReduce",
        "HBase",
        "Apache",
        "Pig",
        "Hive",
        "Sqoop",
        "Apache",
        "Impala",
        "Oozie",
        "Yarn",
        "Apache",
        "Flume",
        "Kafka",
        "Zookeeper",
        "Cloud",
        "Platform",
        "Amazon",
        "EC2",
        "EC3",
        "MS",
        "Azure",
        "Azure",
        "SQL",
        "Database",
        "Azure",
        "SQL",
        "Data",
        "Warehouse",
        "Azure",
        "Analysis",
        "Services",
        "HDInsight",
        "Azure",
        "Data",
        "Lake",
        "Data",
        "Factory",
        "Hadoop",
        "Distributions",
        "Cloudera",
        "Hortonworks",
        "MapR",
        "Programming",
        "Language",
        "Java",
        "Scala",
        "Python",
        "SQL",
        "PLSQL",
        "Shell",
        "Scripting",
        "Storm",
        "JSP",
        "Servlets",
        "Frameworks",
        "Spring",
        "Hibernate",
        "Struts",
        "JSF",
        "EJB",
        "JMS",
        "Web",
        "Technologies",
        "HTML",
        "CSS",
        "JavaScript",
        "JQuery",
        "Bootstrap",
        "XML",
        "JSON",
        "AJAX",
        "Oracle",
        "g",
        "SQL",
        "Operating",
        "Systems",
        "Linux",
        "Unix",
        "Windows",
        "IDE",
        "Tools",
        "Eclipse",
        "NetBeans",
        "IntelliJ",
        "Maven",
        "NoSQL",
        "Databases",
        "HBase",
        "Cassandra",
        "Accumulo",
        "WebApplication",
        "Server",
        "Apache",
        "Tomcat",
        "JBoss",
        "Web",
        "Logic",
        "Web",
        "Sphere",
        "SDLC",
        "Methodologies",
        "Agile",
        "Waterfall",
        "Version",
        "Control",
        "GIT",
        "SVN",
        "CVS"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T23:08:32.169045",
    "resume_data": "Sr Big Data Developer Sr Big Data span lDeveloperspan Sr Big Data Developer Anthem Inc Atlanta GA Overall 10 years of working experience as a Big DataHadoop Developer in designed and developed various applications like big data Hadoop JavaJ2EE opensource technologies Extensive knowledge in Software Development Lifecycle SDLC using Waterfall Agile methodologies Having Project experience on Microsoft Azure Cloud cluster environment and Azure Data lake store Experience in importing and exporting data using Sqoop from HDFS to Relational Database Systems and viceversa Experience in converting Hive queries into Spark transformations using Spark RDDs and Scala Good Knowledge in Amazon Web Service AWS concepts like EMR and EC2 web services which provides fast Good experience in Spark and its related technologies like Spark SQL and Spark Streaming Good knowledge of NOSQL databases like Mongo DB Cassandra and HBase Experience on building the applications using Spark Core Spark SQL Data Frames Spark Streaming Strong experience in developing the workflows using Apache Oozie framework to automate tasks Experience in deploying J2EE applications on Apache Tomcat web server and Web Logic WebSphere JBoss application server Experience in working with Eclipse IDE Net Beans and Rational Application Developer Expertise on usage of SQL queries to extract data from RDBMS databases MySQL DB2 and Oracle Experience with installing backup recovery configuration and development on multiple Hadoop distribution platforms Cloudera and Hortonworks including cloud platforms Amazon AWS and Google Cloud Experience on data ingestion tool Apache NiFi used to extract data from various data sources into Hadoop data lake Experience in working with NoSQL database Apache HBase and have implemented performance improvement as per project requirement Proficient in using and deploying applications to Web ServersApplication servers like Tomcat WebSphere Micro services Experience in generating logging by Log4j to identify the errors in production test environment Extensive knowledge in programming with Resilient Distributed Datasets RDDs Experience in working different version control tools like GIT SVN and Continuous Integration tool Jenkins Hands on Experience in developing and implementing the Spring Rest and Restful Web Services Expertise in writing Hadoop Jobs to analyze data using MapReduce Apache Crunch Hive Pig and SOLR Splunk Experience in working with Web Servers like Apache Tomcat and Application Servers like IBM WebSphere and JBOSS Experience in Apache Flume for collecting aggregating and moving huge chunks of data from various sources such as web server telnet sources etc Experience in working with different scripting technologies like Python UNIX shell scripts Strong team player ability to work independently and in a team as well ability to adapt to rapidly changing environment commitment towards learning Possess excellent Work Experience Sr Big Data Developer Anthem Inc Atlanta GA March 2018 to Present Responsibilities As a Sr Big Data Developer I worked on Hadoop ecosystems including Hive HBase Oozie Pig Zookeeper Spark Streaming MCS MapR Control System and so on with MapR distribution Developed Java and J2EE applications using Rapid Application Development RAD Eclipse Installed and configured Hadoop MapReduce HDFS Developed multiple MapReduce jobs in Java for data cleaning and Preprocessing Create data integration and technical solutions for Azure Data Lake Analytics Azure Data Lake Storage Azure Data Factory Azure SQL databases and Azure SQL Data Warehouse for providing analytics Built code for real time data ingestion using Java MapRStreams Kafka and STORM Involved in various phases of development analysed and developed the system going through Agile Scrum methodology Involved in Design development and testing of web application and integration projects using Object Oriented technologies such as Core Java J2EE Struts JSP JDBC Spring Framework Hibernate Java Beans Web Services RESTSOAP XML XSLT XSL and Ant Worked on Apache Solr which is used as indexing and search engine Primarily involved in Data Migration process using Azure by integrating with GitHub repository and Jenkins Used Spark API over Hortonworks Hadoop YARN to perform analytics on data in Hive Developed various Servlets and Java Interfaces as part of the integration and process flow required for the system Automated workflows using shell scripts and ControlM jobs to pull data from various databases into Hadoop Data Lake Developed Spring Framework Controllers and worked on spring application framework features Prepared Linux shell scripts to configure deploy and manage Oozie workflows of Big Data applications Worked with different data sources like Avro data files XML files JSON files SQL server and Oracle to load data into Hive tables Developed Use Case Diagrams Object Diagrams and Class Diagrams in UML using Rational Rose Exported event weblogs to HDFS by creating a HDFS sink which directly deposits the weblogs in HDFS Implemented a distributed messaging queue to integrate with Cassandra using Apache Kafka and Zookeeper Imported weblogs unstructured data using the Apache Flume and stores the data in Flume channel Developed scripts to run scheduled batch cycles using Oozie and present data for reports Managed real time data processing and real time Data Ingestion in MongoDB using Storm Developed using the framework builds the graphical components and define actions popup menus in XML Primarily involved in Data Migration process using Azure by integrating with Github repository and Jenkins Used Jenkins for build and continuous integration for software development Provided connections using JDBC to the database and developed SQL queries to manipulate the data Monitored Hadoop cluster using tools like Cloudera manager managing and scheduling the jobs on Hadoop cluster Used Spark to create the structured data from large amount of unstructured data from various sources Designed and implemented scalable Cloud Data and Analytical architecture solutions for various public and private cloud platforms using Azure Extracted Real time feed using Spark streaming and convert it to RDD and process data into Data Frame and load the data into Cassandra Performed importing data from various sources to the Cassandra cluster using Sqoop Environment Hadoop 30 Hive 23 HBase 22 Oozie 51 Pig 017 Zookeeper 34 Spark 24 MapR MapReduce HDFS Java MS Azure Kafka 22 Agile Jenkins 216 Hortonworks Cassandra 311 XML MongoDB Sr SparkHadoop Developer Pacific Life Newport Beach CA October 2015 to February 2018 Responsibilities Responsible for building scalable distributed data solutions using Hadoop Participate in all stages of Software Development Life Cycle SDLC including requirements gathering system Analysis system development unit testing and performance testing Installed Oozie workflow engine to run multiple Hive Shell Script Sqoop pig and Java jobs In depth understanding of Hadoop Architecture including YARN and various components such as HDFS Resource Manager Node Manager Name Node Data Node and MR v1 v2 concepts Data Ingestion Experience in importing and exporting data from different RDBMS Servers like MySQL Oracle and Teradata into HDFS and Hive using Sqoop Experience in ingesting data from FTPSFTP servers using Flume Experience in developing Kafka Consumer API using Spark Scala applications Data Processing Developed MapReduce programs in Java for data cleansing data filtering and data aggregation Experienced in analyzing the data using PIG Latin scripts Experience in designing table partitioning bucketing and optimized hive scripts using different performance utilities and techniques Experience in developing Hive UDFs and running hive scripts using different execution engines like Tez and Spark Hive on Spark Experience in designing tables and views for reporting using Impala Experienced in Developing Spark application using Spark Core Spark SQL and Spark Streaming APIs Developed Java Beans for business logic Design of REST APIs that allow sophisticated effective and low cost application integrations Implemented Spark using Scala and Spark SQL for faster testing and processing of data Involved in storydriven Agile development methodology and actively participated in daily scrum meetings Developed Apache sparkbased programs to implement complex business transformations Used Hive to analyze partitioned and bucketed data and compute various metrics for reporting Developed unloading micro services using Scala API in Spark Data frame API for the semantic layer Used SparkSQL and Spark Data frame extensively to cleanse and integrate imported data into more meaningful insights Migrated associated business logic PLSQL proceduresfunctions to Apache Spark Involved in Unit testing and delivered Unit test plans and results documents using JUnit and MrUnit Used Spark API over ClouderaHadoopYARN to perform analytics on data in Hive Created scripts to sync data between local MongoDB and Postgres databases with those on AWS Cloud Used Oozie engine for creating workflow and coordinator jobs that schedule and execute various Created Kafka producer API to send livestream json data into various Kafka topics Created concurrent access for Hive tables with shared and exclusive locking that can be enabled in Hive with the help of Zookeeper implementation in the cluster Created webbased User interface for creating monitoring and controlling data flows using Apache Nifi Worked on a POC to compare processing time of Impala with Apache Hive for batch applications to implement the former in project Extensively used Spark Cassandra connector to load data to and from Cassandra Involved in the Design Phase for getting live event data from the database to the frontend application using Spark Ecosystem Worked on various compression and file formats like Avro Parquet and Text formats Created a complete processing engine based on Cloudera distribution enhanced performance Worked with NoSQL databases like HBase in creating HBase tables to load large sets of semi structured data coming from various sources Written eventdriven link tracking system to capture user events and feed to Kafka to push it to HBase Implemented Spark using Scala and performed cleansing of data by applying Transformations and Actions Developed MapReduce programs in Java to search production logs and web analytics logs for application issues Involved in builddeploy applications using Maven and integrated with CICD server Jenkins Environment Hadoop 30 Oozie 50 Hive 23 Sqoop 14 Pig 017 Java Spark 24 Scala 213 Agile PLSQL JUnit 54 AWS Kafka 22 Zookeeper 34 Cassandra 311 HCatalog NoSQL Maven 36 Jenkins 216 Java Hadoop Developer Comcast Philadelphia PA January 2013 to September 2015 Responsibilities Worked on analyzing Hadoop cluster and different big data analytic tools including MapReduce Hive and Spark Implemented Spark RDD transformations to map business analysis and apply actions on top of transformations Developed the server side scripts using JMS JSP and Java Beans Adding and modifying Hibernate configuration code and JavaSQL statements depending upon the specific database access requirements Implemented POC to migrate MapReduce programs into Spark transformations using Spark and Scala Implemented Storm topologies to preprocess data before moving into HDFS system Worked with various Hadoop file formats including Text Sequence File RCFILE and ORC File Written a Pig Scripts to read data from HDFS and write into Hive Table Managed servers on the Amazon Web Services AWS platform instances using Puppet Chef Configuration management Interacted with Cloudera support and log the issues in Cloudera portal and fixing them as per the recommendations Migrated complex MapReduce programs into Spark RDD transformations actions Involved in installing configuring and managing Hadoop Ecosystem components like Pig Sqoop Kafka and Flume Involved in the process of data acquisition data preprocessing and data exploration of telecommunication project in Scala Developed and Deployed Oozie Workflows for recurring operations on Clusters Created POC on Hortonworks and suggested the best practice in terms HDP HDF platform Developed and implemented Apache NIFI across various environments written QA scripts in Python for tracking files Worked in AWS environment for development and deployment of Custom Hadoop Applications Converted text files into Avro then to parquet format for the file to be used with other Hadoop Ecosystem tools Implemented Apache Pig scripts to load data from and to store data into Hive using HCatalog Loaded data into the cluster from dynamically generated files using Flume and from relational database management systems using Sqoop Created HBase tables to store various data formats of incoming data from different portfolios Designed and implemented Cassandra and associated Restful webservice Expertise in writing Hadoop Jobs for analyzing data using Hive QL Queries Pig Latin Data flow language and custom MapReduce programs in Java Implemented AJAX JSON and JavaScript to create interactive web screens Environment Hadoop 23 MapReduce Hive 18 Spark 20 Scala 17 HDFS AWS Puppet Oozie 30 Hortonworks AJAX JSON JavaScript Java JavaJ2EE Developer Amica Mutual Insurance Company Lincoln RI August 2011 to December 2012 Responsibilities Developed and utilized J2EE Services and JMS components for messaging communication in WebSphere Application Server Developed the code based on the design using Tiles MVC Struts framework and using J2EE patterns Developed the MVC applicationmodel using Spring framework Spring Boot Micro services and used Hibernate framework to interact with the database Developed the custom Logging framework used to log transactions executed across the various applications using Log4j Used Maven in building the application and auto deploying it to the environment Extensively used JQuery to provide dynamic User Interface and for the clientside validations Developed dynamic proxies to consume the web services developed in JAXWS standards for CRM module Written and executed test cases for unit testing using Mockito JUNIT framework Designed and developed business components using Session and Entity Beans in EJB Extensively used Eclipse IDE for developing debugging integrating and deploying the application Used Bit bucket for the repository and version management through SourceTree for GIT Developed user interface using JSTL HTML JavaScript JQuery and CSS Created clean and readable API documentation using Slate to assist our current and future developers Involved in writing test cases using JUnit and integrated these tests with Jenkins Continuous integration tool Extensively used Eclipse IDE for developing debugging integrating and deploying the application Involved in exporting and importing integrations and jar files from development staging and production environments using WebLogic Developed real time tracking of class schedules using Node JS socketio based on socket technology Express JS framework Involved in Unit integration bug fixing acceptance testing with test cases Code reviews Extensively used Java MultiThreading concept for downloading files from a URL Implemented Service Oriented Architecture by developing Java web services using WSDL and SOAP Used frameworks such as Angular backbonejs to a handful of web applications Involved in fixing defects in application worked in JSF managed beans converters validator and configuration files Mapped business objects to database using Hibernate and used JPA annotations for mapping DB to objects Environment JQuery JavaScript HTML5 CSS3 JUNIT Maven MVC Struts Eclipse JUnit Java Developer Glansa Solutions Hyderabad Telangana April 2009 to July 2011 Responsibilities Worked as a Java Developer and involved in application and database design Involved in requirement analysis and participated in the design of the application using UML and OO Analysis Design and Development Around 6 years of IT Industry experience as java developer in software development life cycle core area of object oriented and webbased enterprise applications using javaj2ee technology Experience in applicationweb servers like IBM Web Sphere Web Logic Application Servers JBoss and Tomcat Web Servers Proficient in ntier application design and development using Java J2EE JSP Servlets Struts 20 Spring and Oracle Expertise in developing applications using Core Java Multithreading Collections Swing development Memory Management Application utilities J2EE JSP Servlets Java Beans Web Services SOAP WSDL UDDI AngularJS JMS JDBC JSON Ajax Expertise in HTML XHTML HTML5 ML5 CSS AJAX jQuery JSTL specification XML SAX DOM XSL XSLT JAXP JAXB DTD Resource properties Expert in Various Agile methodologies like SCRUM Test Driven Development TTD Incremental and Iteration methodology Agile Development Testing using Software Development Life Cycle Experienced in Development Testing and Deployment of enterprise applications on Windows Linux and UNIX platforms using IDEs such as Eclipse STS Rational Application Developer RAD NetBeans IntelliJ14x15 Experienced in developing complex PLSQL queries Procedures Triggers Stored Procedures Packages and Views in various databases such as Oracle DB2 and MySQL SQL Server Working Knowledge in JSON and XML technologies such as XSL XSLT XPath parsers like SAX DOM and JAXB Developed the Application using Spring MVC Framework by implementing Controller Service classes Performed JUnit testing to test the implemented services Used ANT scripts to build the application and deployed on Web Sphere Application Server Implemented spring javabased SOAP Web Services for authorization and JUnit tests for part of my code Managed Spring Core for dependency injection of control IOC and integrated with Hibernate Worked on core Java concepts like Collections and Exception Handling for writing the backend APIs Developed DAO and service layers using the Spring Dao support and Hibernate ORM mapping Created test cases for DAO Layer and service layer using JUnit and bug tracking using JIRA Decomposed existing monolithic code base into Spring Boot Microservices Extensively used Eclipse IDE for developing debugging integrating and deploying the application Involved in implementation of MVC pattern using Angular JS and Spring Controller Used JavaScript user input validated using regular expressions and also in the server side Developed business logic using Java Struts Action classes and deployed using Tomcat Used SVN for version control across common source code used by developers Implemented MVC web frameworks for the web applications using JSPServletTag libraries that were designed using JSP Integrated Subversion SVN into Jenkins to automate the code checkout process Implemented modules using Core Java APIs Java collection Threads XML and integrating the modules Environment Java MVC JUnit ANT Hibernate JUnit Eclipse Angular JS JavaScript Struts MVC Education Bachelors Skills CASSANDRA HDFS IMPALA MAPREDUCE OOZIE SQOOP HBASE KAFKA FLUME HADOOP JBOSS JMS MONGODB NOSQL ANALYSIS SERVICES APPLICATION SERVER Git Hadoop HBase Hive Additional Information Technical Skills HadoopBig Data Technologies Hadoop 30 HDFS MapReduce HBase 14 Apache Pig Hive 23 Sqoop 14 Apache Impala 21 Oozie 43 Yarn Apache Flume 18 Kafka 11 Zookeeper Cloud Platform Amazon AWS EC2 EC3 MS Azure Azure SQL Database Azure SQL Data Warehouse Azure Analysis Services HDInsight Azure Data Lake Data Factory Hadoop Distributions Cloudera Hortonworks MapR Programming Language Java Scala Python 36 SQL PLSQL Shell Scripting Storm 10 JSP Servlets Frameworks Spring 505 Hibernate 52 Struts 13 JSF EJB JMS Web Technologies HTML CSS JavaScript JQuery 33 Bootstrap 41 XML JSON AJAX Databases Oracle 12c11g SQL Operating Systems Linux Unix Windows 1087 IDE and Tools Eclipse 47 NetBeans 82 IntelliJ Maven NoSQL Databases HBase 14 Cassandra 311 MongoDB Accumulo WebApplication Server Apache Tomcat 907 JBoss Web Logic Web Sphere SDLC Methodologies Agile Waterfall Version Control GIT SVN CVS",
    "unique_id": "6617fbfe-0201-463b-b6a2-c63a2a42ead2"
}