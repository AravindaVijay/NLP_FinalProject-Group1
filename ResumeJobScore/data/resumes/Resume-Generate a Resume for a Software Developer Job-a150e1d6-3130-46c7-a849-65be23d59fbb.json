{
    "clean_data": "Hadoop Developer Hadoop span lDeveloperspan Hadoop Developer Walmart 6 Years of experience in Big Data Analytics using Hadoop ecosystem components like Map Reduce Pig Hive HBase Flume Sqoop Oozie Spark and Kafka Solid understanding of Distributed Systems Architecture MapReduce and Sparx execution frameworks for large scale parallel processing Experience working with all major Hadoop distributions like ClouderaCDH Horton worksHDP and AWS EMR Developed Spark applications using Spark core anddata frames Used SparkSQL and Spark streaming APIs in Scala Implemented custom business logic and performed join optimization secondary sorting custom sorting using Map Reduce programs Experienced testing and running of Map Reduce pipelines on Apache Crunch Hands on experience with accessing and perform CURD operations against HBase data using Java API Performed Spark application troubleshooting and fine tuning Worked on data integration using Kafka Spark streaming and HBase Depth understanding of NoSQL databases such as Hbase and its integration with Hadoop cluster Strong work experience in extracting wrangling ingestion processing storing querying and analyzing structured semistructured and unstructured data Solid understanding of Hadoop MRv1 and Hadoop MRv2 YARN architecture Experience in creating Maps side join Reducer side join shuffle sort distributed cache compression techniques multiple Hadoop input and output formats Worked with various data formats like csv text sequential files parquet Avro Jason RC files Designed HIVE queries to perform data analysis data transfer and table design to load data into Hadoop environment Implemented Sqoop for large datasets transfer between HDFS and RDBMS and viceversa Extensive experience on importing and exporting data using stream processing platforms like Flume and Kafka Experienced with AWS where cluster was built using Ec2 instances store data in S3 and ran EMR jobs Sound knowledge over J2EE Design Patterns like MVC Architecture Singleton Factory Pattern Front Controller Session Facade Business Delegate and Data Access Object for building J2EE Applications Experienced in developing the unit test cases using MRUnit JUnit and Easy Mock Knowledge on Splunk for logging mechanism Experience in using Maven and ANT for build automation Experience in using version control and configuration management tools like SVN CVS Used Talend ETL tool to Extract the data from different data sources transform and load the data into the dataware house systems for reporting and analysis Strong understanding of SDLC and process methodologies like waterfall and Agile Involved in sprint planning project documentation story creations project related meetings Work Experience Hadoop Developer Walmart Bentonville AR January 2018 to Present Responsibilities Developed TDCH Scriptsfor improving and exporting data into HDFS and Hive Used Fair Schedule to allocate resources in Yarn Responsible to manage data coming from different sources Involved in creating Hive tables loading with data and writing Hive queries Implemented partitioning dynamic partitions buckets in HIVE Implemented the workflows using Apache Oozie framework to automate tasks Worked on CICD pipeline chipsmartcard interface devices integrating code changes to Git repository and build using Jenkins Read the ORC filesoptimized row columnar and create data frames to use in spark Experienced working with Spark core and Spark SQL using Pyspark Performed data transformation and analytics on large dataset using Spark Integrated spark jobs with MLP platforms Wrote Kafka procedures to stream the data from external rest APIs to Kafka topics Wrote Spark streaming application to consume the data from Kafka topics and write the processed streams to Hbase Used reporting tools like Tableau to connect with impala for generating daily reports of data Involved in Analysis Design Development and Testing process based on the new business requirements Environment Hadoop Hive S3 Spark Oozie Teradata Yarn Unix Hortonworks TDCH Kafka Tableau Verizon Wireless Fort Worth TX 2017 to December 2017 Responsibilities Responsible for building scalable distributed data solution using Apache Hadoop and Spark Involved in combining traditional transactional data and event data with social network data Involved in loading data from relational database into HDFS using Sqoop Deployed scalable Hadoop cluster on AWS using S3 as underlying file system for Hadoop Worked with Spark core Spark streaming and Spark SQL modules of Spark Involved in developing generic SparkScala functions for transformations aggregations and designing schema for rows Experienced in working with Spark APIs like RDDs Datasets Data Frames and DStreams to perform transportation on the data Used Spark SQL to perform interactive analysis on the data using SQL and HiveQL Experienced in processing live data streams using Spark streaming with high level functions like map reduce join and window Experienced in minimizing data transfers over Hadoop clusters using Spark optimization like broadcasts variables and accumulators Worked on troubleshooting Spark application to make them more error tolerant Involved in loading the processed data into Hive warehouse Stored the data in table formats using Hive tables and Hive Serdes Implemented static partitions dynamic partitions and buckets in Hive Used Oozie operational services for scheduling workflows dynamically Used reporting tools like Tableau to connect with Impala for generating daily reports of data Designed documented operational problems by following standards using JIRA Collaborated with the infrastructure network database application and other teams to ensure data quality and availability Environment Hadoop 2x Spark core Spark SQL Spark streaming Scala Hive Sqoop Oozie Amazon EMR Tableau Impala JIRA INDIAHadoop developer Infotech Enterprises Hyderabad Telangana April 2014 to June 2015 Responsibilities Worked on Spark SQL readingwriting data from JSON file text file parquet file schema RDD Identifying data sources and create appropriate data ingestion procedures Transformed the data using Spark Hive Pig for BI team to perform visual analytics according to the client requirement Developed the service to run the Mapreduce jobs as per the requirement basis Importing and exporting data into HDFS and HIVE PIG using Sqoop Populated big data customer marketing data structures Developed Spark scripts by using Python as per requirements Performed joins on tables in Hive with various optimization techniques Implemented lateral view in conjunction with UDFs in Hive according to the client requirements Created Hive tables as per internal requirements in static and dynamic partitions Implemented the workflows using Apache Oozie framework to automate the tasks Developing design documents considering all possible approaches and identifying best of them Developed scripts and automated data management from end to end sync up between the clusters Imported the data from different sources like HDFSHbase into Spark RDD Experienced with Spark context SparkSQL Data frame Pair RDDs Spark Yarn Involved in converting HiveSQL queries into Spark transformations using Spark RDD Scala Automated the jobs for pulling data from FTP server to load data into Hive tables using Oozie workflows EnvironmentHive Sqoop Python Shell scripting Spark Oozie Scala Java Java developer Virinchi Technologies Hyderabad Telangana July 2012 to March 2014 Responsibilities Designed the application using J2EE design patterns based on MVC architecture Designed the user interface using HTML CSS java script and jQuery Used JDBC for accessing data from Oracle database Validated web forms for clientside validation as per the requirements Developed custom tags JSTL to support custom user interfaces Handled business logic as a model using helper classes and servlets to control the flow of application as controller as serverside validations Developed code to convert JSON data to customize JavaScript objects Developed servlets and JSPs based on MVC pattern using Struts framework Developed EJBsession bean acts as Faade Used Maven for building application EAR for deploying on Web logic application server Used Eclipse development testing and code review Performed unit testing on application to verify and identify various scenarios EnvironmentJ2EE Java Eclipse EJB JDBC Web logic PlSQL Junit Maven Log4j UML Education Science Engineering JNT University Skills Apache 2 years APACHE HADOOP HDFS 2 years APACHE HADOOP OOZIE 2 years Java 2 years SQL 4 years",
    "entities": [
        "Wrote Spark",
        "Created Hive",
        "HDFSHbase",
        "FTP",
        "jQuery Used JDBC",
        "Involved in Analysis Design Development and Testing",
        "Scala Implemented",
        "HTML CSS",
        "SparkSQL",
        "Spark SQL Spark",
        "Scala Hive",
        "Developed",
        "HiveQL Experienced",
        "Sqoop",
        "HIVE",
        "Maven",
        "Apache Hadoop",
        "Performed",
        "Sound",
        "Sparx",
        "BI",
        "HDFS",
        "Impala",
        "Spark SQL",
        "MVC Architecture Singleton Factory",
        "AWS",
        "Hadoop Developer Hadoop",
        "ANT",
        "Git",
        "JIRA Collaborated",
        "Developed Spark",
        "Virinchi Technologies Hyderabad",
        "Agile Involved",
        "Hadoop Worked",
        "Big Data Analytics",
        "Infotech Enterprises Hyderabad",
        "Spark Involved",
        "JSON",
        "Spark Hive Pig",
        "Spark RDD Experienced",
        "Distributed Systems Architecture MapReduce",
        "SVN",
        "Work Experience Hadoop Developer Walmart Bentonville AR",
        "Oozie",
        "MVC",
        "Worked on CICD",
        "Sqoop Populated",
        "Implemented Sqoop",
        "SQL",
        "RDD",
        "Hadoop",
        "UML Education Science Engineering JNT University",
        "Mapreduce",
        "EAR",
        "SDLC",
        "Maps",
        "SparkSQL Data frame Pair RDDs Spark Yarn Involved",
        "NoSQL",
        "ORC",
        "Spark Integrated",
        "Tableau",
        "Shell",
        "Fort Worth",
        "HBase",
        "HBase Depth",
        "SparkScala",
        "Hive",
        "Pyspark Performed",
        "Spark",
        "Yarn Responsible"
    ],
    "experience": "Experience working with all major Hadoop distributions like ClouderaCDH Horton worksHDP and AWS EMR Developed Spark applications using Spark core anddata frames Used SparkSQL and Spark streaming APIs in Scala Implemented custom business logic and performed join optimization secondary sorting custom sorting using Map Reduce programs Experienced testing and running of Map Reduce pipelines on Apache Crunch Hands on experience with accessing and perform CURD operations against HBase data using Java API Performed Spark application troubleshooting and fine tuning Worked on data integration using Kafka Spark streaming and HBase Depth understanding of NoSQL databases such as Hbase and its integration with Hadoop cluster Strong work experience in extracting wrangling ingestion processing storing querying and analyzing structured semistructured and unstructured data Solid understanding of Hadoop MRv1 and Hadoop MRv2 YARN architecture Experience in creating Maps side join Reducer side join shuffle sort distributed cache compression techniques multiple Hadoop input and output formats Worked with various data formats like csv text sequential files parquet Avro Jason RC files Designed HIVE queries to perform data analysis data transfer and table design to load data into Hadoop environment Implemented Sqoop for large datasets transfer between HDFS and RDBMS and viceversa Extensive experience on importing and exporting data using stream processing platforms like Flume and Kafka Experienced with AWS where cluster was built using Ec2 instances store data in S3 and ran EMR jobs Sound knowledge over J2EE Design Patterns like MVC Architecture Singleton Factory Pattern Front Controller Session Facade Business Delegate and Data Access Object for building J2EE Applications Experienced in developing the unit test cases using MRUnit JUnit and Easy Mock Knowledge on Splunk for logging mechanism Experience in using Maven and ANT for build automation Experience in using version control and configuration management tools like SVN CVS Used Talend ETL tool to Extract the data from different data sources transform and load the data into the dataware house systems for reporting and analysis Strong understanding of SDLC and process methodologies like waterfall and Agile Involved in sprint planning project documentation story creations project related meetings Work Experience Hadoop Developer Walmart Bentonville AR January 2018 to Present Responsibilities Developed TDCH Scriptsfor improving and exporting data into HDFS and Hive Used Fair Schedule to allocate resources in Yarn Responsible to manage data coming from different sources Involved in creating Hive tables loading with data and writing Hive queries Implemented partitioning dynamic partitions buckets in HIVE Implemented the workflows using Apache Oozie framework to automate tasks Worked on CICD pipeline chipsmartcard interface devices integrating code changes to Git repository and build using Jenkins Read the ORC filesoptimized row columnar and create data frames to use in spark Experienced working with Spark core and Spark SQL using Pyspark Performed data transformation and analytics on large dataset using Spark Integrated spark jobs with MLP platforms Wrote Kafka procedures to stream the data from external rest APIs to Kafka topics Wrote Spark streaming application to consume the data from Kafka topics and write the processed streams to Hbase Used reporting tools like Tableau to connect with impala for generating daily reports of data Involved in Analysis Design Development and Testing process based on the new business requirements Environment Hadoop Hive S3 Spark Oozie Teradata Yarn Unix Hortonworks TDCH Kafka Tableau Verizon Wireless Fort Worth TX 2017 to December 2017 Responsibilities Responsible for building scalable distributed data solution using Apache Hadoop and Spark Involved in combining traditional transactional data and event data with social network data Involved in loading data from relational database into HDFS using Sqoop Deployed scalable Hadoop cluster on AWS using S3 as underlying file system for Hadoop Worked with Spark core Spark streaming and Spark SQL modules of Spark Involved in developing generic SparkScala functions for transformations aggregations and designing schema for rows Experienced in working with Spark APIs like RDDs Datasets Data Frames and DStreams to perform transportation on the data Used Spark SQL to perform interactive analysis on the data using SQL and HiveQL Experienced in processing live data streams using Spark streaming with high level functions like map reduce join and window Experienced in minimizing data transfers over Hadoop clusters using Spark optimization like broadcasts variables and accumulators Worked on troubleshooting Spark application to make them more error tolerant Involved in loading the processed data into Hive warehouse Stored the data in table formats using Hive tables and Hive Serdes Implemented static partitions dynamic partitions and buckets in Hive Used Oozie operational services for scheduling workflows dynamically Used reporting tools like Tableau to connect with Impala for generating daily reports of data Designed documented operational problems by following standards using JIRA Collaborated with the infrastructure network database application and other teams to ensure data quality and availability Environment Hadoop 2x Spark core Spark SQL Spark streaming Scala Hive Sqoop Oozie Amazon EMR Tableau Impala JIRA INDIAHadoop developer Infotech Enterprises Hyderabad Telangana April 2014 to June 2015 Responsibilities Worked on Spark SQL readingwriting data from JSON file text file parquet file schema RDD Identifying data sources and create appropriate data ingestion procedures Transformed the data using Spark Hive Pig for BI team to perform visual analytics according to the client requirement Developed the service to run the Mapreduce jobs as per the requirement basis Importing and exporting data into HDFS and HIVE PIG using Sqoop Populated big data customer marketing data structures Developed Spark scripts by using Python as per requirements Performed joins on tables in Hive with various optimization techniques Implemented lateral view in conjunction with UDFs in Hive according to the client requirements Created Hive tables as per internal requirements in static and dynamic partitions Implemented the workflows using Apache Oozie framework to automate the tasks Developing design documents considering all possible approaches and identifying best of them Developed scripts and automated data management from end to end sync up between the clusters Imported the data from different sources like HDFSHbase into Spark RDD Experienced with Spark context SparkSQL Data frame Pair RDDs Spark Yarn Involved in converting HiveSQL queries into Spark transformations using Spark RDD Scala Automated the jobs for pulling data from FTP server to load data into Hive tables using Oozie workflows EnvironmentHive Sqoop Python Shell scripting Spark Oozie Scala Java Java developer Virinchi Technologies Hyderabad Telangana July 2012 to March 2014 Responsibilities Designed the application using J2EE design patterns based on MVC architecture Designed the user interface using HTML CSS java script and jQuery Used JDBC for accessing data from Oracle database Validated web forms for clientside validation as per the requirements Developed custom tags JSTL to support custom user interfaces Handled business logic as a model using helper classes and servlets to control the flow of application as controller as serverside validations Developed code to convert JSON data to customize JavaScript objects Developed servlets and JSPs based on MVC pattern using Struts framework Developed EJBsession bean acts as Faade Used Maven for building application EAR for deploying on Web logic application server Used Eclipse development testing and code review Performed unit testing on application to verify and identify various scenarios EnvironmentJ2EE Java Eclipse EJB JDBC Web logic PlSQL Junit Maven Log4j UML Education Science Engineering JNT University Skills Apache 2 years APACHE HADOOP HDFS 2 years APACHE HADOOP OOZIE 2 years Java 2 years SQL 4 years",
    "extracted_keywords": [
        "Hadoop",
        "Developer",
        "Hadoop",
        "span",
        "lDeveloperspan",
        "Hadoop",
        "Developer",
        "Walmart",
        "Years",
        "experience",
        "Big",
        "Data",
        "Analytics",
        "Hadoop",
        "ecosystem",
        "components",
        "Map",
        "Reduce",
        "Pig",
        "Hive",
        "HBase",
        "Flume",
        "Sqoop",
        "Oozie",
        "Spark",
        "Kafka",
        "understanding",
        "Distributed",
        "Systems",
        "Architecture",
        "MapReduce",
        "execution",
        "frameworks",
        "scale",
        "parallel",
        "processing",
        "Experience",
        "Hadoop",
        "distributions",
        "ClouderaCDH",
        "Horton",
        "worksHDP",
        "AWS",
        "EMR",
        "Spark",
        "applications",
        "Spark",
        "core",
        "anddata",
        "frames",
        "SparkSQL",
        "Spark",
        "APIs",
        "Scala",
        "custom",
        "business",
        "logic",
        "join",
        "optimization",
        "custom",
        "Map",
        "Reduce",
        "programs",
        "testing",
        "running",
        "Map",
        "pipelines",
        "Apache",
        "Crunch",
        "Hands",
        "experience",
        "accessing",
        "CURD",
        "operations",
        "HBase",
        "data",
        "Java",
        "API",
        "Performed",
        "Spark",
        "application",
        "troubleshooting",
        "tuning",
        "data",
        "integration",
        "Kafka",
        "Spark",
        "streaming",
        "HBase",
        "Depth",
        "understanding",
        "NoSQL",
        "databases",
        "Hbase",
        "integration",
        "Hadoop",
        "cluster",
        "Strong",
        "work",
        "experience",
        "ingestion",
        "processing",
        "data",
        "understanding",
        "Hadoop",
        "MRv1",
        "Hadoop",
        "MRv2",
        "YARN",
        "architecture",
        "Experience",
        "Maps",
        "side",
        "join",
        "Reducer",
        "side",
        "join",
        "shuffle",
        "cache",
        "compression",
        "techniques",
        "Hadoop",
        "input",
        "output",
        "formats",
        "data",
        "formats",
        "csv",
        "text",
        "files",
        "parquet",
        "Avro",
        "Jason",
        "RC",
        "files",
        "HIVE",
        "data",
        "analysis",
        "data",
        "transfer",
        "table",
        "design",
        "data",
        "Hadoop",
        "environment",
        "Sqoop",
        "datasets",
        "transfer",
        "HDFS",
        "RDBMS",
        "viceversa",
        "experience",
        "data",
        "stream",
        "processing",
        "platforms",
        "Flume",
        "Kafka",
        "AWS",
        "cluster",
        "Ec2",
        "instances",
        "data",
        "S3",
        "EMR",
        "jobs",
        "knowledge",
        "J2EE",
        "Design",
        "Patterns",
        "MVC",
        "Architecture",
        "Singleton",
        "Factory",
        "Pattern",
        "Front",
        "Controller",
        "Session",
        "Facade",
        "Business",
        "Delegate",
        "Data",
        "Access",
        "Object",
        "J2EE",
        "Applications",
        "unit",
        "test",
        "cases",
        "MRUnit",
        "JUnit",
        "Easy",
        "Mock",
        "Knowledge",
        "Splunk",
        "mechanism",
        "Experience",
        "Maven",
        "ANT",
        "build",
        "automation",
        "Experience",
        "version",
        "control",
        "configuration",
        "management",
        "tools",
        "SVN",
        "CVS",
        "Talend",
        "ETL",
        "tool",
        "data",
        "data",
        "sources",
        "data",
        "dataware",
        "house",
        "systems",
        "reporting",
        "analysis",
        "understanding",
        "SDLC",
        "process",
        "methodologies",
        "waterfall",
        "Agile",
        "sprint",
        "planning",
        "project",
        "documentation",
        "story",
        "creations",
        "meetings",
        "Work",
        "Experience",
        "Hadoop",
        "Developer",
        "Walmart",
        "Bentonville",
        "AR",
        "January",
        "Present",
        "Responsibilities",
        "TDCH",
        "Scriptsfor",
        "data",
        "HDFS",
        "Hive",
        "Used",
        "Fair",
        "Schedule",
        "resources",
        "Yarn",
        "data",
        "sources",
        "Hive",
        "tables",
        "data",
        "Hive",
        "queries",
        "partitions",
        "buckets",
        "HIVE",
        "workflows",
        "Apache",
        "Oozie",
        "framework",
        "tasks",
        "CICD",
        "pipeline",
        "chipsmartcard",
        "interface",
        "devices",
        "code",
        "changes",
        "Git",
        "repository",
        "Jenkins",
        "Read",
        "ORC",
        "row",
        "columnar",
        "data",
        "frames",
        "spark",
        "Spark",
        "core",
        "Spark",
        "SQL",
        "Pyspark",
        "Performed",
        "data",
        "transformation",
        "analytics",
        "dataset",
        "Spark",
        "Integrated",
        "spark",
        "jobs",
        "MLP",
        "platforms",
        "Wrote",
        "Kafka",
        "data",
        "rest",
        "APIs",
        "Kafka",
        "topics",
        "Wrote",
        "Spark",
        "streaming",
        "application",
        "data",
        "Kafka",
        "topics",
        "streams",
        "Hbase",
        "reporting",
        "tools",
        "Tableau",
        "impala",
        "reports",
        "data",
        "Analysis",
        "Design",
        "Development",
        "Testing",
        "process",
        "business",
        "requirements",
        "Environment",
        "Hadoop",
        "Hive",
        "S3",
        "Spark",
        "Oozie",
        "Teradata",
        "Yarn",
        "Unix",
        "Hortonworks",
        "TDCH",
        "Kafka",
        "Tableau",
        "Verizon",
        "Wireless",
        "Fort",
        "Worth",
        "TX",
        "December",
        "Responsibilities",
        "data",
        "solution",
        "Apache",
        "Hadoop",
        "Spark",
        "data",
        "event",
        "data",
        "network",
        "data",
        "loading",
        "data",
        "database",
        "HDFS",
        "Sqoop",
        "Deployed",
        "Hadoop",
        "cluster",
        "AWS",
        "S3",
        "file",
        "system",
        "Hadoop",
        "Worked",
        "Spark",
        "core",
        "Spark",
        "streaming",
        "Spark",
        "SQL",
        "modules",
        "Spark",
        "SparkScala",
        "functions",
        "transformations",
        "aggregations",
        "schema",
        "rows",
        "Spark",
        "APIs",
        "RDDs",
        "Datasets",
        "Data",
        "Frames",
        "DStreams",
        "transportation",
        "data",
        "Spark",
        "SQL",
        "analysis",
        "data",
        "SQL",
        "HiveQL",
        "data",
        "streams",
        "Spark",
        "streaming",
        "level",
        "functions",
        "map",
        "join",
        "window",
        "data",
        "transfers",
        "Hadoop",
        "clusters",
        "Spark",
        "optimization",
        "broadcasts",
        "variables",
        "accumulators",
        "Spark",
        "application",
        "error",
        "tolerant",
        "data",
        "Hive",
        "warehouse",
        "data",
        "table",
        "formats",
        "Hive",
        "tables",
        "Hive",
        "Serdes",
        "partitions",
        "partitions",
        "buckets",
        "Hive",
        "Oozie",
        "services",
        "scheduling",
        "workflows",
        "reporting",
        "tools",
        "Tableau",
        "Impala",
        "reports",
        "data",
        "problems",
        "standards",
        "JIRA",
        "Collaborated",
        "infrastructure",
        "network",
        "database",
        "application",
        "teams",
        "data",
        "quality",
        "availability",
        "Environment",
        "Hadoop",
        "Spark",
        "core",
        "Spark",
        "SQL",
        "Spark",
        "Scala",
        "Hive",
        "Sqoop",
        "Oozie",
        "Amazon",
        "EMR",
        "Tableau",
        "Impala",
        "JIRA",
        "INDIAHadoop",
        "developer",
        "Infotech",
        "Enterprises",
        "Hyderabad",
        "Telangana",
        "April",
        "June",
        "Responsibilities",
        "Spark",
        "SQL",
        "data",
        "file",
        "text",
        "file",
        "parquet",
        "file",
        "schema",
        "data",
        "sources",
        "data",
        "ingestion",
        "procedures",
        "data",
        "Spark",
        "Hive",
        "Pig",
        "BI",
        "team",
        "analytics",
        "client",
        "requirement",
        "service",
        "Mapreduce",
        "jobs",
        "basis",
        "data",
        "HDFS",
        "HIVE",
        "PIG",
        "Sqoop",
        "Populated",
        "data",
        "customer",
        "marketing",
        "data",
        "structures",
        "Spark",
        "scripts",
        "Python",
        "requirements",
        "Performed",
        "tables",
        "Hive",
        "optimization",
        "techniques",
        "view",
        "conjunction",
        "UDFs",
        "Hive",
        "client",
        "requirements",
        "Hive",
        "tables",
        "requirements",
        "partitions",
        "workflows",
        "Apache",
        "Oozie",
        "framework",
        "tasks",
        "design",
        "documents",
        "approaches",
        "scripts",
        "data",
        "management",
        "end",
        "end",
        "sync",
        "clusters",
        "data",
        "sources",
        "HDFSHbase",
        "Spark",
        "RDD",
        "Spark",
        "context",
        "SparkSQL",
        "Data",
        "frame",
        "Pair",
        "RDDs",
        "Spark",
        "Yarn",
        "HiveSQL",
        "queries",
        "Spark",
        "transformations",
        "Spark",
        "RDD",
        "Scala",
        "jobs",
        "data",
        "FTP",
        "server",
        "data",
        "Hive",
        "tables",
        "Oozie",
        "workflows",
        "EnvironmentHive",
        "Sqoop",
        "Python",
        "Shell",
        "Spark",
        "Oozie",
        "Scala",
        "Java",
        "Java",
        "developer",
        "Virinchi",
        "Technologies",
        "Hyderabad",
        "Telangana",
        "July",
        "March",
        "Responsibilities",
        "application",
        "J2EE",
        "design",
        "patterns",
        "MVC",
        "architecture",
        "user",
        "interface",
        "HTML",
        "CSS",
        "script",
        "jQuery",
        "JDBC",
        "data",
        "Oracle",
        "database",
        "web",
        "forms",
        "validation",
        "requirements",
        "custom",
        "tags",
        "JSTL",
        "custom",
        "user",
        "interfaces",
        "business",
        "logic",
        "model",
        "helper",
        "classes",
        "servlets",
        "flow",
        "application",
        "controller",
        "serverside",
        "code",
        "data",
        "JavaScript",
        "servlets",
        "JSPs",
        "MVC",
        "pattern",
        "Struts",
        "framework",
        "EJBsession",
        "bean",
        "Faade",
        "Used",
        "Maven",
        "building",
        "application",
        "EAR",
        "Web",
        "logic",
        "application",
        "server",
        "Eclipse",
        "development",
        "testing",
        "code",
        "review",
        "Performed",
        "unit",
        "application",
        "scenarios",
        "Java",
        "Eclipse",
        "EJB",
        "JDBC",
        "Web",
        "logic",
        "PlSQL",
        "Junit",
        "Maven",
        "Log4j",
        "UML",
        "Education",
        "Science",
        "Engineering",
        "JNT",
        "University",
        "Skills",
        "Apache",
        "years",
        "APACHE",
        "HADOOP",
        "HDFS",
        "years",
        "APACHE",
        "HADOOP",
        "OOZIE",
        "years",
        "Java",
        "years",
        "SQL",
        "years"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T21:55:58.073099",
    "resume_data": "Hadoop Developer Hadoop span lDeveloperspan Hadoop Developer Walmart 6 Years of experience in Big Data Analytics using Hadoop ecosystem components like Map Reduce Pig Hive HBase Flume Sqoop Oozie Spark and Kafka Solid understanding of Distributed Systems Architecture MapReduce and Sparx execution frameworks for large scale parallel processing Experience working with all major Hadoop distributions like ClouderaCDH Horton worksHDP and AWS EMR Developed Spark applications using Spark core anddata frames Used SparkSQL and Spark streaming APIs in Scala Implemented custom business logic and performed join optimization secondary sorting custom sorting using Map Reduce programs Experienced testing and running of Map Reduce pipelines on Apache Crunch Hands on experience with accessing and perform CURD operations against HBase data using Java API Performed Spark application troubleshooting and fine tuning Worked on data integration using Kafka Spark streaming and HBase Depth understanding of NoSQL databases such as Hbase and its integration with Hadoop cluster Strong work experience in extracting wrangling ingestion processing storing querying and analyzing structured semistructured and unstructured data Solid understanding of Hadoop MRv1 and Hadoop MRv2 YARN architecture Experience in creating Maps side join Reducer side join shuffle sort distributed cache compression techniques multiple Hadoop input and output formats Worked with various data formats like csv text sequential files parquet Avro Jason RC files Designed HIVE queries to perform data analysis data transfer and table design to load data into Hadoop environment Implemented Sqoop for large datasets transfer between HDFS and RDBMS and viceversa Extensive experience on importing and exporting data using stream processing platforms like Flume and Kafka Experienced with AWS where cluster was built using Ec2 instances store data in S3 and ran EMR jobs Sound knowledge over J2EE Design Patterns like MVC Architecture Singleton Factory Pattern Front Controller Session Facade Business Delegate and Data Access Object for building J2EE Applications Experienced in developing the unit test cases using MRUnit JUnit and Easy Mock Knowledge on Splunk for logging mechanism Experience in using Maven and ANT for build automation Experience in using version control and configuration management tools like SVN CVS Used Talend ETL tool to Extract the data from different data sources transform and load the data into the dataware house systems for reporting and analysis Strong understanding of SDLC and process methodologies like waterfall and Agile Involved in sprint planning project documentation story creations project related meetings Work Experience Hadoop Developer Walmart Bentonville AR January 2018 to Present Responsibilities Developed TDCH Scriptsfor improving and exporting data into HDFS and Hive Used Fair Schedule to allocate resources in Yarn Responsible to manage data coming from different sources Involved in creating Hive tables loading with data and writing Hive queries Implemented partitioning dynamic partitions buckets in HIVE Implemented the workflows using Apache Oozie framework to automate tasks Worked on CICD pipeline chipsmartcard interface devices integrating code changes to Git repository and build using Jenkins Read the ORC filesoptimized row columnar and create data frames to use in spark Experienced working with Spark core and Spark SQL using Pyspark Performed data transformation and analytics on large dataset using Spark Integrated spark jobs with MLP platforms Wrote Kafka procedures to stream the data from external rest APIs to Kafka topics Wrote Spark streaming application to consume the data from Kafka topics and write the processed streams to Hbase Used reporting tools like Tableau to connect with impala for generating daily reports of data Involved in Analysis Design Development and Testing process based on the new business requirements Environment Hadoop Hive S3 Spark Oozie Teradata Yarn Unix Hortonworks TDCH Kafka Tableau Verizon Wireless Fort Worth TX 2017 to December 2017 Responsibilities Responsible for building scalable distributed data solution using Apache Hadoop and Spark Involved in combining traditional transactional data and event data with social network data Involved in loading data from relational database into HDFS using Sqoop Deployed scalable Hadoop cluster on AWS using S3 as underlying file system for Hadoop Worked with Spark core Spark streaming and Spark SQL modules of Spark Involved in developing generic SparkScala functions for transformations aggregations and designing schema for rows Experienced in working with Spark APIs like RDDs Datasets Data Frames and DStreams to perform transportation on the data Used Spark SQL to perform interactive analysis on the data using SQL and HiveQL Experienced in processing live data streams using Spark streaming with high level functions like map reduce join and window Experienced in minimizing data transfers over Hadoop clusters using Spark optimization like broadcasts variables and accumulators Worked on troubleshooting Spark application to make them more error tolerant Involved in loading the processed data into Hive warehouse Stored the data in table formats using Hive tables and Hive Serdes Implemented static partitions dynamic partitions and buckets in Hive Used Oozie operational services for scheduling workflows dynamically Used reporting tools like Tableau to connect with Impala for generating daily reports of data Designed documented operational problems by following standards using JIRA Collaborated with the infrastructure network database application and other teams to ensure data quality and availability Environment Hadoop 2x Spark core Spark SQL Spark streaming Scala Hive Sqoop Oozie Amazon EMR Tableau Impala JIRA INDIAHadoop developer Infotech Enterprises Hyderabad Telangana April 2014 to June 2015 Responsibilities Worked on Spark SQL readingwriting data from JSON file text file parquet file schema RDD Identifying data sources and create appropriate data ingestion procedures Transformed the data using Spark Hive Pig for BI team to perform visual analytics according to the client requirement Developed the service to run the Mapreduce jobs as per the requirement basis Importing and exporting data into HDFS and HIVE PIG using Sqoop Populated big data customer marketing data structures Developed Spark scripts by using Python as per requirements Performed joins on tables in Hive with various optimization techniques Implemented lateral view in conjunction with UDFs in Hive according to the client requirements Created Hive tables as per internal requirements in static and dynamic partitions Implemented the workflows using Apache Oozie framework to automate the tasks Developing design documents considering all possible approaches and identifying best of them Developed scripts and automated data management from end to end sync up between the clusters Imported the data from different sources like HDFSHbase into Spark RDD Experienced with Spark context SparkSQL Data frame Pair RDDs Spark Yarn Involved in converting HiveSQL queries into Spark transformations using Spark RDD Scala Automated the jobs for pulling data from FTP server to load data into Hive tables using Oozie workflows EnvironmentHive Sqoop Python Shell scripting Spark Oozie Scala Java Java developer Virinchi Technologies Hyderabad Telangana July 2012 to March 2014 Responsibilities Designed the application using J2EE design patterns based on MVC architecture Designed the user interface using HTML CSS java script and jQuery Used JDBC for accessing data from Oracle database Validated web forms for clientside validation as per the requirements Developed custom tags JSTL to support custom user interfaces Handled business logic as a model using helper classes and servlets to control the flow of application as controller as serverside validations Developed code to convert JSON data to customize JavaScript objects Developed servlets and JSPs based on MVC pattern using Struts framework Developed EJBsession bean acts as Faade Used Maven for building application EAR for deploying on Web logic application server Used Eclipse development testing and code review Performed unit testing on application to verify and identify various scenarios EnvironmentJ2EE Java Eclipse EJB JDBC Web logic PlSQL Junit Maven Log4j UML Education Science Engineering JNT University Skills Apache 2 years APACHE HADOOP HDFS 2 years APACHE HADOOP OOZIE 2 years Java 2 years SQL 4 years",
    "unique_id": "a150e1d6-3130-46c7-a849-65be23d59fbb"
}