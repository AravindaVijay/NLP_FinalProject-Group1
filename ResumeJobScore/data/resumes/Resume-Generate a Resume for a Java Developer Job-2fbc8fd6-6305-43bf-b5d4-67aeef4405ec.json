{
    "clean_data": "Hadoop Developer Hadoop span lDeveloperspan Hadoop Developer TIAACREF Around 7 years of experience in full Software Development Life Cycle SDLC AGILE Methodology and analysis design development testing implementation and maintenance in Hadoop Data Warehousing Linux and Java 3 years of experience in providing solutions for Big Data using Hadoop 2x HDFS MR2 YARN Kafka PIG Hive Sqoop HBase Cloudera Manager Zoo keeper Oozie Hue CDH5 HDP 2x Experienced in Big data Hadoop NoSQL and various components such as HDFS Job Tracker Task Tracker Name Node Data Node and Map Reduce2 YARN programming paradigm Working experience on Cloudera Horton Works Hadoop distribution Implementation of Big data batch processes using Hadoop Map Reduce2 YARN PIG and Hive Experienced in using Kafka as a distributed publishersubscriber messaging system Experience in importing and exporting data using Sqoop from HDFSHiveHBase to Relational Database Systems and viceversa Hands on experience in inmemory data processing with Apache Spark Experience in integration of various data sources like Oracle DB2 Sybase SQL server and MS access and nonrelational sources like flat files into staging area Good experience in writing PIG scripts and Hive Queries for processing and analyzing large volumes of data Experience in optimization of Map Reduce algorithm using Combiners and Partitioners to deliver best results Experienced in designing developing and implementing connectivity products that allow efficient exchange of data between the core database engine and the Hadoop ecosystem Extending Hive and Pig core functionality by writing custom UDFs Hands on experience in using BI tools like SplunkHunk Tableau Experience in understanding the security requirements for Hadoop and integrate with Kerberos authentication and authorization infrastructure Experience in managing and reviewing Hadoop log files Hands on experience in application development using Core JAVA RDBMS and Linux shell scripting Having good working experience in AgileScrum methodologies technical discussion with client and Communication using scrum calls daily for project analysis specs and development aspects Ability to work independently as well as in a team and able to effectively communicate with customers peers and management at all levels in and outside the organization Work Experience Hadoop Developer TIAACREF Charlotte NC December 2014 to Present The Project deals with developing a centralized data repository using Hadoop ecosystem for maintaining data flowing in from client locations and provide data querying capabilities to the Pathologists Roles Responsibilities Worked on analyzing Hadoop cluster using different big data analytic tools including Kafka Pig Hive and Map Reduce Developing parser and loader map reduce application to retrieve data from HDFS and store to HBase and Hive Importing the data from the MySql and Oracle into the HDFS using Sqoop Importing the unstructured data into the HDFS using Flume Written Map Reduce java programs to analyze the log data for largescale data sets Involved in creating Hive tables loading and analyzing data using hive queries Involved in using HBase Java API on Java application Deployed Hadoop Cluster in Pseudodistributed and Fully Distributed modes Involved in running AdHoc query through PIG Latin language Hive or Java Map Reduce Real time streaming the data using Spark with Kafka Configured Spark streaming to receive real time data from the Kafka and store the stream data to HDFS using Scale Wrote the shell scripts to monitor the health check of Hadoop daemon services and respond accordingly to any warning or failure conditions Importing and exporting data into HDFS using Sqoop and Kafka Developed PIG Latin scripts to extract the data from the web server output files to load into HDFS Automated all the jobs for extracting the data from different Data Sources like MySQL to pushing the result set data to Hadoop Distributed File System Environment Hadoop 100 Oracle 11g10g Python Map Reduce Hive HBase Flume Sqoop Pig Zookeeper Java ETL SQL Server CentOS UNIX Linux Windows 7 Vista XP Hadoop Developer Shell Oil Company Houston TX January 2013 to November 2014 Shell Oil Company is a multinational oil company which is amongst the largest oil companies in the world It is one of Americas largest oil and natural gas producers natural gas marketers gasoline marketers and petrochemical manufactures Big data analytics transform big data in shell Exploration and production into sound exploration decisions high quality wells reduced costs and lower environment impact The ability to collect more data and analyze it helped the company to locate oil and gas reserves in areas that were once inaccessible or thought to be depleted Roles Responsibilities Involved in review of functional and nonfunctional requirements Develop Map Reduce jobs for the users Maintain update and schedule the periodic jobs which range from updates on periodic Map Reduce jobs to creating adhoc jobs for the business users Importing and exporting data into HDFS and Hive using Sqoop Experienced in defining job flows Scheduling all HadoopHive Sqoop Hbase jobs using Oozie Experienced in managing and reviewing Hadoop log files Experienced in running Hadoop streaming jobs to process terabytes of xml format data Responsible to manage data coming from different sources Involved in loading data from UNIX file system to HDFS Involved in creating Hive tables loading with data and writing hive queries which will run internally in map reduce way Designed and implemented Map reducebased largescale parallel relationlearning system Setup and benchmarked HadoopHBase clusters for internal use Understanding functional specifications and documenting technical design documents Performed unit testing for all the components using JUnit Implemented data access using Hibernate persistence framework Environment Java Eclipse Oracle 10g Sub Version Hadoop HBase Linux Map Reduce HDFS Hive Java JDK 16 Hadoop Distribution Cloudera Map Reduce SQL Windows UNIX Shell Scripting Hadoop Developer Celgene Township of Warren NJ November 2012 to December 2013 Celgene Pharmaceuticals is a global integrated biopharmaceutical company that delivers truly innovative and life changing drugs for various patients They are primarily engaged in the discovery development and commercialization of innovative therapies designed to treat cancer and immune inflammatory related diseases Roles Responsibilities Worked on analyzing Hadoop cluster using different big data analytic tools including Pig HBase and Sqoop Developed Message Handler Adapter which converts the data objects into XML message and invoke an enterprise service and viceversa using Java JMS and MQ Series Business logic is implemented using Struts action components in the Struts and Hibernate framework Migrating the needed data from MySQL into HDFS using Sqoop and importing various formats of unstructured data from logs into HDFS using Flume Used Multithreading for invoking the database and also implemented complex modules which contain business logics using Collection Reflection and Generics API Involved in Pig Latin programming Importing And Exporting Data from MySQLOracle to HiveQL using SQOOP Experienced in analyzing data with Hive and Pig Responsible for operational support of Production system Loading log data directly into HDFS using Flume Environment Apache Hadoop HDFS Java Map Reduce Eclipse Hive PIG Sqoop Flume Oozie JavaJ2EE Oracle 10g SQL PLSQL JSP EJB Struts Hibernate Weblogic 80 HTML AJAX Java Script JDBC XML JMS Java Developer ICICI Finance HYD September 2010 to September 2012 ICICI Finance uses real time application hosted across the globe for its loans and finance division This multitier architecture application facilitates settlement between various merchant systems and transaction processing units in ICICI Roles Responsibilities Worked on both Web Logic Portal 92 for Portal development and Web Logic 81 for Data Services Programming Worked on creating EJBs that implemented business logic Developed the presentation layer using JSP HTML CSS and client validations using JavaScript Involved in designing and development of the ecommerce site using JSP Servlet EJBs JavaScript and JDBC Used Eclipse 60 as IDE for application development Validated all forms using Struts validation framework and implemented Tiles framework in the presentation layer Configured Struts framework to implement MVC design patterns Designed and developed GUI using JSP HTML DHTML and CSS Worked with JMS for messaging interface Environment Java J2EE HTML DHTML CSS JavaScript JSP Servlets XML EJB Struts Weblogic 81 SQL Server 2008R2 CentOS UNIX LINUX Windows 7VistaXP Java Developer Nalinsoft Pvt Ltd Hyderabad Andhra Pradesh May 2009 to August 2010 Roles Responsibilities Designed Use Case Diagrams Class Diagrams and Sequence Diagrams and Object Diagrams using IBM Rational Rose model the detail design of the application Involved in designing user screens using HTMLas per user requirements Used SpringHibernate integration in the back end to fetch data from Oracle and MYSQL databases Used Spring Dependency Injection properties to provide loosecoupling between layers Implemented the Web Service client for the login authentication credit reports and applicant information Used Web services SOAP for transmission of large blocks of XML data over HTTP Used Hibernate object relational data mapping framework to persist and retrieve the data from database Wrote SQL queries stored procedures and triggers to perform backend database operations by using SQL Server 2005 Implemented the logging mechanism using Log4j framework Wrote test cases in JUnit for unit testing of classes Developed application to be implemented on Windows XP Created application using Eclipse IDE Installed Web Logic Server for handling HTTP RequestResponse Used Subversion for version control and created automated build scripts Environment JDK 15 Rational Rose Spring Web Services JAXWS Hibernate Log4j Weblogic SQL Server 2005 Windows XP HTML Eclipse Log4J XML JUnit SVN Additional Information TECHNICAL SKILLS Hadoop Ecosystem Hadoop MapReduce Sqoop Hive Oozie PIG HDFS Zookeeper Flume Spark Kafka NoSQL Databases Hbase Cassandra Mongo DB Java J2EE Technologies Core Java Servlets JSP JDBC JNDI Java Beans Languages C C JAVA SQL PLSQL PIG Latin HiveQL Unix shell scripting Frameworks MVC Spring Hibernate Struts 12 EJB JMS JUnit MRUnit Databases Oracle 11g10g9i My SQL DB2 MS SQL Server Application Server Apache Tomcat JBoss IBM Web sphere Web Logic Web Services WSDL SOAP Apache CXF Apache Axis REST Methodologies Scrum Agile Waterfall",
    "entities": [
        "Implementation of Big data",
        "Combiners",
        "Nalinsoft Pvt Ltd Hyderabad Andhra",
        "SQL Server",
        "Oracle DB2 Sybase SQL",
        "Hive Importing",
        "Cloudera Horton Works Hadoop",
        "Developed",
        "GUI",
        "HiveQL",
        "Sqoop",
        "Kerberos",
        "Oozie Experienced",
        "Node Data",
        "Performed",
        "BI",
        "HDFS",
        "UNIX",
        "Oozie JavaJ2EE Oracle",
        "Sequence Diagrams",
        "Web Logic Web Services WSDL SOAP Apache CXF",
        "Hadoop Developer Hadoop",
        "Oracle",
        "Data Sources",
        "SQOOP Experienced",
        "Data Services Programming Worked",
        "PIG",
        "Flume Environment Apache Hadoop",
        "Work Experience Hadoop Developer",
        "HadoopHBase",
        "Hive",
        "SVN Additional Information TECHNICAL SKILLS Hadoop Ecosystem Hadoop MapReduce Sqoop",
        "IBM",
        "HDFS Job Tracker Task Tracker",
        "Vista XP Hadoop Developer",
        "SpringHibernate",
        "JavaScript Involved",
        "MQ Series Business",
        "MVC",
        "Weblogic SQL Server",
        "JSP",
        "Hive Queries",
        "SQL",
        "Hadoop",
        "Windows XP Created",
        "Communication",
        "SplunkHunk Tableau",
        "HDFS Involved",
        "XML",
        "Shell Scripting Hadoop Developer Celgene Township",
        "DHTML",
        "Shell Oil Company",
        "Oracle 11g10",
        "Frameworks MVC",
        "Relational Database Systems",
        "Hive and Pig Responsible",
        "JUnit",
        "Hadoop Data",
        "HTTP Used Hibernate",
        "MS",
        "HBase",
        "Apache Spark",
        "Big Data",
        "Hadoop Distributed File System Environment Hadoop",
        "TX",
        "MS SQL Server Application",
        "Spark",
        "HadoopHive Sqoop Hbase",
        "Ability",
        "Present The Project"
    ],
    "experience": "Experience in importing and exporting data using Sqoop from HDFSHiveHBase to Relational Database Systems and viceversa Hands on experience in inmemory data processing with Apache Spark Experience in integration of various data sources like Oracle DB2 Sybase SQL server and MS access and nonrelational sources like flat files into staging area Good experience in writing PIG scripts and Hive Queries for processing and analyzing large volumes of data Experience in optimization of Map Reduce algorithm using Combiners and Partitioners to deliver best results Experienced in designing developing and implementing connectivity products that allow efficient exchange of data between the core database engine and the Hadoop ecosystem Extending Hive and Pig core functionality by writing custom UDFs Hands on experience in using BI tools like SplunkHunk Tableau Experience in understanding the security requirements for Hadoop and integrate with Kerberos authentication and authorization infrastructure Experience in managing and reviewing Hadoop log files Hands on experience in application development using Core JAVA RDBMS and Linux shell scripting Having good working experience in AgileScrum methodologies technical discussion with client and Communication using scrum calls daily for project analysis specs and development aspects Ability to work independently as well as in a team and able to effectively communicate with customers peers and management at all levels in and outside the organization Work Experience Hadoop Developer TIAACREF Charlotte NC December 2014 to Present The Project deals with developing a centralized data repository using Hadoop ecosystem for maintaining data flowing in from client locations and provide data querying capabilities to the Pathologists Roles Responsibilities Worked on analyzing Hadoop cluster using different big data analytic tools including Kafka Pig Hive and Map Reduce Developing parser and loader map reduce application to retrieve data from HDFS and store to HBase and Hive Importing the data from the MySql and Oracle into the HDFS using Sqoop Importing the unstructured data into the HDFS using Flume Written Map Reduce java programs to analyze the log data for largescale data sets Involved in creating Hive tables loading and analyzing data using hive queries Involved in using HBase Java API on Java application Deployed Hadoop Cluster in Pseudodistributed and Fully Distributed modes Involved in running AdHoc query through PIG Latin language Hive or Java Map Reduce Real time streaming the data using Spark with Kafka Configured Spark streaming to receive real time data from the Kafka and store the stream data to HDFS using Scale Wrote the shell scripts to monitor the health check of Hadoop daemon services and respond accordingly to any warning or failure conditions Importing and exporting data into HDFS using Sqoop and Kafka Developed PIG Latin scripts to extract the data from the web server output files to load into HDFS Automated all the jobs for extracting the data from different Data Sources like MySQL to pushing the result set data to Hadoop Distributed File System Environment Hadoop 100 Oracle 11g10 g Python Map Reduce Hive HBase Flume Sqoop Pig Zookeeper Java ETL SQL Server CentOS UNIX Linux Windows 7 Vista XP Hadoop Developer Shell Oil Company Houston TX January 2013 to November 2014 Shell Oil Company is a multinational oil company which is amongst the largest oil companies in the world It is one of Americas largest oil and natural gas producers natural gas marketers gasoline marketers and petrochemical manufactures Big data analytics transform big data in shell Exploration and production into sound exploration decisions high quality wells reduced costs and lower environment impact The ability to collect more data and analyze it helped the company to locate oil and gas reserves in areas that were once inaccessible or thought to be depleted Roles Responsibilities Involved in review of functional and nonfunctional requirements Develop Map Reduce jobs for the users Maintain update and schedule the periodic jobs which range from updates on periodic Map Reduce jobs to creating adhoc jobs for the business users Importing and exporting data into HDFS and Hive using Sqoop Experienced in defining job flows Scheduling all HadoopHive Sqoop Hbase jobs using Oozie Experienced in managing and reviewing Hadoop log files Experienced in running Hadoop streaming jobs to process terabytes of xml format data Responsible to manage data coming from different sources Involved in loading data from UNIX file system to HDFS Involved in creating Hive tables loading with data and writing hive queries which will run internally in map reduce way Designed and implemented Map reducebased largescale parallel relationlearning system Setup and benchmarked HadoopHBase clusters for internal use Understanding functional specifications and documenting technical design documents Performed unit testing for all the components using JUnit Implemented data access using Hibernate persistence framework Environment Java Eclipse Oracle 10 g Sub Version Hadoop HBase Linux Map Reduce HDFS Hive Java JDK 16 Hadoop Distribution Cloudera Map Reduce SQL Windows UNIX Shell Scripting Hadoop Developer Celgene Township of Warren NJ November 2012 to December 2013 Celgene Pharmaceuticals is a global integrated biopharmaceutical company that delivers truly innovative and life changing drugs for various patients They are primarily engaged in the discovery development and commercialization of innovative therapies designed to treat cancer and immune inflammatory related diseases Roles Responsibilities Worked on analyzing Hadoop cluster using different big data analytic tools including Pig HBase and Sqoop Developed Message Handler Adapter which converts the data objects into XML message and invoke an enterprise service and viceversa using Java JMS and MQ Series Business logic is implemented using Struts action components in the Struts and Hibernate framework Migrating the needed data from MySQL into HDFS using Sqoop and importing various formats of unstructured data from logs into HDFS using Flume Used Multithreading for invoking the database and also implemented complex modules which contain business logics using Collection Reflection and Generics API Involved in Pig Latin programming Importing And Exporting Data from MySQLOracle to HiveQL using SQOOP Experienced in analyzing data with Hive and Pig Responsible for operational support of Production system Loading log data directly into HDFS using Flume Environment Apache Hadoop HDFS Java Map Reduce Eclipse Hive PIG Sqoop Flume Oozie JavaJ2EE Oracle 10 g SQL PLSQL JSP EJB Struts Hibernate Weblogic 80 HTML AJAX Java Script JDBC XML JMS Java Developer ICICI Finance HYD September 2010 to September 2012 ICICI Finance uses real time application hosted across the globe for its loans and finance division This multitier architecture application facilitates settlement between various merchant systems and transaction processing units in ICICI Roles Responsibilities Worked on both Web Logic Portal 92 for Portal development and Web Logic 81 for Data Services Programming Worked on creating EJBs that implemented business logic Developed the presentation layer using JSP HTML CSS and client validations using JavaScript Involved in designing and development of the ecommerce site using JSP Servlet EJBs JavaScript and JDBC Used Eclipse 60 as IDE for application development Validated all forms using Struts validation framework and implemented Tiles framework in the presentation layer Configured Struts framework to implement MVC design patterns Designed and developed GUI using JSP HTML DHTML and CSS Worked with JMS for messaging interface Environment Java J2EE HTML DHTML CSS JavaScript JSP Servlets XML EJB Struts Weblogic 81 SQL Server 2008R2 CentOS UNIX LINUX Windows 7VistaXP Java Developer Nalinsoft Pvt Ltd Hyderabad Andhra Pradesh May 2009 to August 2010 Roles Responsibilities Designed Use Case Diagrams Class Diagrams and Sequence Diagrams and Object Diagrams using IBM Rational Rose model the detail design of the application Involved in designing user screens using HTMLas per user requirements Used SpringHibernate integration in the back end to fetch data from Oracle and MYSQL databases Used Spring Dependency Injection properties to provide loosecoupling between layers Implemented the Web Service client for the login authentication credit reports and applicant information Used Web services SOAP for transmission of large blocks of XML data over HTTP Used Hibernate object relational data mapping framework to persist and retrieve the data from database Wrote SQL queries stored procedures and triggers to perform backend database operations by using SQL Server 2005 Implemented the logging mechanism using Log4j framework Wrote test cases in JUnit for unit testing of classes Developed application to be implemented on Windows XP Created application using Eclipse IDE Installed Web Logic Server for handling HTTP RequestResponse Used Subversion for version control and created automated build scripts Environment JDK 15 Rational Rose Spring Web Services JAXWS Hibernate Log4j Weblogic SQL Server 2005 Windows XP HTML Eclipse Log4J XML JUnit SVN Additional Information TECHNICAL SKILLS Hadoop Ecosystem Hadoop MapReduce Sqoop Hive Oozie PIG HDFS Zookeeper Flume Spark Kafka NoSQL Databases Hbase Cassandra Mongo DB Java J2EE Technologies Core Java Servlets JSP JDBC JNDI Java Beans Languages C C JAVA SQL PLSQL PIG Latin HiveQL Unix shell scripting Frameworks MVC Spring Hibernate Struts 12 EJB JMS JUnit MRUnit Databases Oracle 11g10g9i My SQL DB2 MS SQL Server Application Server Apache Tomcat JBoss IBM Web sphere Web Logic Web Services WSDL SOAP Apache CXF Apache Axis REST Methodologies Scrum Agile Waterfall",
    "extracted_keywords": [
        "Hadoop",
        "Developer",
        "Hadoop",
        "span",
        "lDeveloperspan",
        "Hadoop",
        "Developer",
        "TIAACREF",
        "years",
        "experience",
        "Software",
        "Development",
        "Life",
        "Cycle",
        "SDLC",
        "AGILE",
        "Methodology",
        "analysis",
        "design",
        "development",
        "testing",
        "implementation",
        "maintenance",
        "Hadoop",
        "Data",
        "Warehousing",
        "Linux",
        "Java",
        "years",
        "experience",
        "solutions",
        "Big",
        "Data",
        "Hadoop",
        "HDFS",
        "MR2",
        "YARN",
        "Kafka",
        "PIG",
        "Hive",
        "Sqoop",
        "HBase",
        "Cloudera",
        "Manager",
        "Zoo",
        "keeper",
        "Oozie",
        "Hue",
        "CDH5",
        "HDP",
        "data",
        "Hadoop",
        "NoSQL",
        "components",
        "HDFS",
        "Job",
        "Tracker",
        "Task",
        "Tracker",
        "Name",
        "Node",
        "Data",
        "Node",
        "Map",
        "Reduce2",
        "YARN",
        "programming",
        "paradigm",
        "Working",
        "experience",
        "Cloudera",
        "Horton",
        "Works",
        "Hadoop",
        "distribution",
        "Implementation",
        "data",
        "batch",
        "processes",
        "Hadoop",
        "Map",
        "Reduce2",
        "YARN",
        "PIG",
        "Hive",
        "Kafka",
        "publishersubscriber",
        "system",
        "Experience",
        "data",
        "Sqoop",
        "HDFSHiveHBase",
        "Relational",
        "Database",
        "Systems",
        "viceversa",
        "Hands",
        "experience",
        "data",
        "processing",
        "Apache",
        "Spark",
        "Experience",
        "integration",
        "data",
        "sources",
        "Oracle",
        "DB2",
        "Sybase",
        "SQL",
        "server",
        "MS",
        "access",
        "sources",
        "files",
        "staging",
        "area",
        "experience",
        "PIG",
        "scripts",
        "Hive",
        "Queries",
        "processing",
        "volumes",
        "data",
        "Experience",
        "optimization",
        "Map",
        "Reduce",
        "algorithm",
        "Combiners",
        "Partitioners",
        "results",
        "connectivity",
        "products",
        "exchange",
        "data",
        "core",
        "database",
        "engine",
        "Hadoop",
        "ecosystem",
        "Hive",
        "Pig",
        "core",
        "functionality",
        "custom",
        "UDFs",
        "Hands",
        "experience",
        "BI",
        "tools",
        "SplunkHunk",
        "Tableau",
        "Experience",
        "security",
        "requirements",
        "Hadoop",
        "authentication",
        "authorization",
        "infrastructure",
        "Experience",
        "Hadoop",
        "log",
        "Hands",
        "experience",
        "application",
        "development",
        "Core",
        "JAVA",
        "RDBMS",
        "Linux",
        "shell",
        "scripting",
        "working",
        "experience",
        "AgileScrum",
        "methodologies",
        "discussion",
        "client",
        "Communication",
        "scrum",
        "calls",
        "project",
        "analysis",
        "specs",
        "development",
        "Ability",
        "team",
        "customers",
        "peers",
        "management",
        "levels",
        "organization",
        "Work",
        "Experience",
        "Hadoop",
        "Developer",
        "TIAACREF",
        "Charlotte",
        "NC",
        "December",
        "Present",
        "Project",
        "data",
        "repository",
        "Hadoop",
        "ecosystem",
        "data",
        "client",
        "locations",
        "data",
        "capabilities",
        "Pathologists",
        "Roles",
        "Responsibilities",
        "Hadoop",
        "cluster",
        "data",
        "tools",
        "Kafka",
        "Pig",
        "Hive",
        "Map",
        "Reduce",
        "parser",
        "loader",
        "map",
        "application",
        "data",
        "HDFS",
        "store",
        "HBase",
        "Hive",
        "data",
        "MySql",
        "Oracle",
        "HDFS",
        "Sqoop",
        "data",
        "HDFS",
        "Flume",
        "Written",
        "Map",
        "programs",
        "log",
        "data",
        "largescale",
        "data",
        "sets",
        "Hive",
        "tables",
        "data",
        "queries",
        "HBase",
        "Java",
        "API",
        "Java",
        "application",
        "Deployed",
        "Hadoop",
        "Cluster",
        "Pseudodistributed",
        "modes",
        "AdHoc",
        "query",
        "PIG",
        "Latin",
        "language",
        "Hive",
        "Java",
        "Map",
        "time",
        "data",
        "Spark",
        "Kafka",
        "Configured",
        "Spark",
        "streaming",
        "time",
        "data",
        "Kafka",
        "stream",
        "data",
        "HDFS",
        "Scale",
        "Wrote",
        "shell",
        "scripts",
        "health",
        "check",
        "Hadoop",
        "daemon",
        "services",
        "warning",
        "failure",
        "conditions",
        "data",
        "HDFS",
        "Sqoop",
        "Kafka",
        "Developed",
        "PIG",
        "scripts",
        "data",
        "web",
        "server",
        "output",
        "files",
        "HDFS",
        "jobs",
        "data",
        "Data",
        "Sources",
        "MySQL",
        "result",
        "data",
        "Hadoop",
        "Distributed",
        "File",
        "System",
        "Environment",
        "Hadoop",
        "Oracle",
        "g",
        "Python",
        "Map",
        "Reduce",
        "Hive",
        "HBase",
        "Flume",
        "Sqoop",
        "Pig",
        "Zookeeper",
        "Java",
        "ETL",
        "SQL",
        "Server",
        "CentOS",
        "UNIX",
        "Linux",
        "Windows",
        "Vista",
        "XP",
        "Hadoop",
        "Developer",
        "Shell",
        "Oil",
        "Company",
        "Houston",
        "TX",
        "January",
        "November",
        "Shell",
        "Oil",
        "Company",
        "oil",
        "company",
        "oil",
        "companies",
        "world",
        "Americas",
        "oil",
        "gas",
        "producers",
        "gas",
        "marketers",
        "gasoline",
        "marketers",
        "petrochemical",
        "manufactures",
        "data",
        "analytics",
        "data",
        "shell",
        "Exploration",
        "production",
        "exploration",
        "decisions",
        "quality",
        "wells",
        "costs",
        "environment",
        "ability",
        "data",
        "company",
        "oil",
        "gas",
        "reserves",
        "areas",
        "Roles",
        "Responsibilities",
        "review",
        "requirements",
        "Develop",
        "Map",
        "Reduce",
        "jobs",
        "users",
        "update",
        "jobs",
        "updates",
        "Map",
        "Reduce",
        "jobs",
        "jobs",
        "business",
        "users",
        "data",
        "HDFS",
        "Hive",
        "Sqoop",
        "Experienced",
        "job",
        "flows",
        "Scheduling",
        "HadoopHive",
        "Sqoop",
        "Hbase",
        "jobs",
        "Oozie",
        "Hadoop",
        "log",
        "files",
        "Hadoop",
        "streaming",
        "jobs",
        "terabytes",
        "xml",
        "format",
        "data",
        "data",
        "sources",
        "loading",
        "data",
        "UNIX",
        "file",
        "system",
        "HDFS",
        "Hive",
        "tables",
        "data",
        "hive",
        "queries",
        "map",
        "way",
        "Map",
        "largescale",
        "system",
        "Setup",
        "HadoopHBase",
        "clusters",
        "use",
        "specifications",
        "design",
        "documents",
        "Performed",
        "unit",
        "components",
        "JUnit",
        "data",
        "access",
        "Hibernate",
        "persistence",
        "framework",
        "Environment",
        "Java",
        "Eclipse",
        "Oracle",
        "g",
        "Sub",
        "Version",
        "Hadoop",
        "HBase",
        "Linux",
        "Map",
        "HDFS",
        "Hive",
        "Java",
        "JDK",
        "Hadoop",
        "Distribution",
        "Cloudera",
        "Map",
        "SQL",
        "Windows",
        "UNIX",
        "Shell",
        "Scripting",
        "Hadoop",
        "Developer",
        "Celgene",
        "Township",
        "Warren",
        "NJ",
        "November",
        "December",
        "Celgene",
        "Pharmaceuticals",
        "company",
        "life",
        "drugs",
        "patients",
        "discovery",
        "development",
        "commercialization",
        "therapies",
        "cancer",
        "diseases",
        "Roles",
        "Responsibilities",
        "Hadoop",
        "cluster",
        "data",
        "tools",
        "Pig",
        "HBase",
        "Sqoop",
        "Developed",
        "Message",
        "Handler",
        "Adapter",
        "data",
        "XML",
        "message",
        "enterprise",
        "service",
        "viceversa",
        "Java",
        "JMS",
        "MQ",
        "Series",
        "Business",
        "logic",
        "Struts",
        "action",
        "components",
        "Struts",
        "Hibernate",
        "framework",
        "data",
        "MySQL",
        "HDFS",
        "Sqoop",
        "formats",
        "data",
        "logs",
        "HDFS",
        "Flume",
        "Multithreading",
        "database",
        "modules",
        "business",
        "logics",
        "Collection",
        "Reflection",
        "Generics",
        "API",
        "Pig",
        "Latin",
        "programming",
        "Importing",
        "Exporting",
        "Data",
        "MySQLOracle",
        "HiveQL",
        "SQOOP",
        "data",
        "Hive",
        "Pig",
        "Responsible",
        "support",
        "Production",
        "system",
        "Loading",
        "log",
        "data",
        "HDFS",
        "Flume",
        "Environment",
        "Apache",
        "Hadoop",
        "HDFS",
        "Java",
        "Map",
        "Eclipse",
        "Hive",
        "PIG",
        "Sqoop",
        "Flume",
        "Oozie",
        "JavaJ2EE",
        "Oracle",
        "g",
        "SQL",
        "PLSQL",
        "JSP",
        "EJB",
        "Struts",
        "Hibernate",
        "Weblogic",
        "HTML",
        "AJAX",
        "Java",
        "Script",
        "JDBC",
        "XML",
        "JMS",
        "Java",
        "Developer",
        "ICICI",
        "Finance",
        "HYD",
        "September",
        "September",
        "ICICI",
        "Finance",
        "time",
        "application",
        "globe",
        "loans",
        "finance",
        "division",
        "architecture",
        "application",
        "settlement",
        "merchant",
        "systems",
        "transaction",
        "processing",
        "units",
        "ICICI",
        "Roles",
        "Responsibilities",
        "Web",
        "Logic",
        "Portal",
        "Portal",
        "development",
        "Web",
        "Logic",
        "Data",
        "Services",
        "Programming",
        "EJBs",
        "business",
        "logic",
        "presentation",
        "layer",
        "JSP",
        "HTML",
        "CSS",
        "client",
        "validations",
        "JavaScript",
        "designing",
        "development",
        "ecommerce",
        "site",
        "JSP",
        "Servlet",
        "EJBs",
        "JavaScript",
        "JDBC",
        "Eclipse",
        "IDE",
        "application",
        "development",
        "forms",
        "Struts",
        "validation",
        "framework",
        "Tiles",
        "framework",
        "presentation",
        "layer",
        "Configured",
        "Struts",
        "framework",
        "MVC",
        "design",
        "patterns",
        "GUI",
        "JSP",
        "HTML",
        "DHTML",
        "CSS",
        "JMS",
        "interface",
        "Environment",
        "Java",
        "J2EE",
        "HTML",
        "DHTML",
        "CSS",
        "JavaScript",
        "JSP",
        "Servlets",
        "XML",
        "EJB",
        "Struts",
        "Weblogic",
        "SQL",
        "Server",
        "2008R2",
        "CentOS",
        "UNIX",
        "LINUX",
        "Java",
        "Developer",
        "Nalinsoft",
        "Pvt",
        "Ltd",
        "Hyderabad",
        "Andhra",
        "Pradesh",
        "May",
        "August",
        "Roles",
        "Responsibilities",
        "Use",
        "Case",
        "Diagrams",
        "Class",
        "Diagrams",
        "Sequence",
        "Diagrams",
        "Object",
        "Diagrams",
        "IBM",
        "Rational",
        "Rose",
        "model",
        "detail",
        "design",
        "application",
        "user",
        "screens",
        "HTMLas",
        "user",
        "requirements",
        "SpringHibernate",
        "integration",
        "end",
        "data",
        "Oracle",
        "MYSQL",
        "Spring",
        "Dependency",
        "Injection",
        "properties",
        "layers",
        "Web",
        "Service",
        "client",
        "login",
        "authentication",
        "credit",
        "reports",
        "information",
        "Web",
        "services",
        "SOAP",
        "transmission",
        "blocks",
        "XML",
        "data",
        "HTTP",
        "Hibernate",
        "object",
        "data",
        "mapping",
        "framework",
        "data",
        "database",
        "Wrote",
        "SQL",
        "procedures",
        "triggers",
        "database",
        "operations",
        "SQL",
        "Server",
        "logging",
        "mechanism",
        "Log4j",
        "framework",
        "test",
        "cases",
        "JUnit",
        "unit",
        "testing",
        "classes",
        "application",
        "Windows",
        "XP",
        "application",
        "Eclipse",
        "IDE",
        "Installed",
        "Web",
        "Logic",
        "Server",
        "HTTP",
        "RequestResponse",
        "Subversion",
        "version",
        "control",
        "build",
        "scripts",
        "Environment",
        "JDK",
        "Rational",
        "Rose",
        "Spring",
        "Web",
        "Services",
        "JAXWS",
        "Hibernate",
        "Log4j",
        "Weblogic",
        "SQL",
        "Server",
        "Windows",
        "XP",
        "HTML",
        "Eclipse",
        "XML",
        "JUnit",
        "SVN",
        "Information",
        "TECHNICAL",
        "SKILLS",
        "Hadoop",
        "Ecosystem",
        "Hadoop",
        "MapReduce",
        "Sqoop",
        "Hive",
        "Oozie",
        "PIG",
        "HDFS",
        "Zookeeper",
        "Flume",
        "Spark",
        "Kafka",
        "NoSQL",
        "Hbase",
        "Cassandra",
        "Mongo",
        "DB",
        "Java",
        "J2EE",
        "Technologies",
        "Core",
        "Java",
        "Servlets",
        "JSP",
        "JDBC",
        "JNDI",
        "Java",
        "Beans",
        "Languages",
        "C",
        "C",
        "SQL",
        "PLSQL",
        "PIG",
        "Latin",
        "HiveQL",
        "Unix",
        "shell",
        "Frameworks",
        "MVC",
        "Spring",
        "Hibernate",
        "Struts",
        "EJB",
        "JMS",
        "JUnit",
        "MRUnit",
        "Oracle",
        "SQL",
        "DB2",
        "MS",
        "SQL",
        "Server",
        "Application",
        "Server",
        "Apache",
        "Tomcat",
        "JBoss",
        "IBM",
        "Web",
        "sphere",
        "Web",
        "Logic",
        "Web",
        "Services",
        "WSDL",
        "SOAP",
        "Apache",
        "CXF",
        "Apache",
        "Axis",
        "REST",
        "Methodologies",
        "Scrum",
        "Agile",
        "Waterfall"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T20:16:25.346860",
    "resume_data": "Hadoop Developer Hadoop span lDeveloperspan Hadoop Developer TIAACREF Around 7 years of experience in full Software Development Life Cycle SDLC AGILE Methodology and analysis design development testing implementation and maintenance in Hadoop Data Warehousing Linux and Java 3 years of experience in providing solutions for Big Data using Hadoop 2x HDFS MR2 YARN Kafka PIG Hive Sqoop HBase Cloudera Manager Zoo keeper Oozie Hue CDH5 HDP 2x Experienced in Big data Hadoop NoSQL and various components such as HDFS Job Tracker Task Tracker Name Node Data Node and Map Reduce2 YARN programming paradigm Working experience on Cloudera Horton Works Hadoop distribution Implementation of Big data batch processes using Hadoop Map Reduce2 YARN PIG and Hive Experienced in using Kafka as a distributed publishersubscriber messaging system Experience in importing and exporting data using Sqoop from HDFSHiveHBase to Relational Database Systems and viceversa Hands on experience in inmemory data processing with Apache Spark Experience in integration of various data sources like Oracle DB2 Sybase SQL server and MS access and nonrelational sources like flat files into staging area Good experience in writing PIG scripts and Hive Queries for processing and analyzing large volumes of data Experience in optimization of Map Reduce algorithm using Combiners and Partitioners to deliver best results Experienced in designing developing and implementing connectivity products that allow efficient exchange of data between the core database engine and the Hadoop ecosystem Extending Hive and Pig core functionality by writing custom UDFs Hands on experience in using BI tools like SplunkHunk Tableau Experience in understanding the security requirements for Hadoop and integrate with Kerberos authentication and authorization infrastructure Experience in managing and reviewing Hadoop log files Hands on experience in application development using Core JAVA RDBMS and Linux shell scripting Having good working experience in AgileScrum methodologies technical discussion with client and Communication using scrum calls daily for project analysis specs and development aspects Ability to work independently as well as in a team and able to effectively communicate with customers peers and management at all levels in and outside the organization Work Experience Hadoop Developer TIAACREF Charlotte NC December 2014 to Present The Project deals with developing a centralized data repository using Hadoop ecosystem for maintaining data flowing in from client locations and provide data querying capabilities to the Pathologists Roles Responsibilities Worked on analyzing Hadoop cluster using different big data analytic tools including Kafka Pig Hive and Map Reduce Developing parser and loader map reduce application to retrieve data from HDFS and store to HBase and Hive Importing the data from the MySql and Oracle into the HDFS using Sqoop Importing the unstructured data into the HDFS using Flume Written Map Reduce java programs to analyze the log data for largescale data sets Involved in creating Hive tables loading and analyzing data using hive queries Involved in using HBase Java API on Java application Deployed Hadoop Cluster in Pseudodistributed and Fully Distributed modes Involved in running AdHoc query through PIG Latin language Hive or Java Map Reduce Real time streaming the data using Spark with Kafka Configured Spark streaming to receive real time data from the Kafka and store the stream data to HDFS using Scale Wrote the shell scripts to monitor the health check of Hadoop daemon services and respond accordingly to any warning or failure conditions Importing and exporting data into HDFS using Sqoop and Kafka Developed PIG Latin scripts to extract the data from the web server output files to load into HDFS Automated all the jobs for extracting the data from different Data Sources like MySQL to pushing the result set data to Hadoop Distributed File System Environment Hadoop 100 Oracle 11g10g Python Map Reduce Hive HBase Flume Sqoop Pig Zookeeper Java ETL SQL Server CentOS UNIX Linux Windows 7 Vista XP Hadoop Developer Shell Oil Company Houston TX January 2013 to November 2014 Shell Oil Company is a multinational oil company which is amongst the largest oil companies in the world It is one of Americas largest oil and natural gas producers natural gas marketers gasoline marketers and petrochemical manufactures Big data analytics transform big data in shell Exploration and production into sound exploration decisions high quality wells reduced costs and lower environment impact The ability to collect more data and analyze it helped the company to locate oil and gas reserves in areas that were once inaccessible or thought to be depleted Roles Responsibilities Involved in review of functional and nonfunctional requirements Develop Map Reduce jobs for the users Maintain update and schedule the periodic jobs which range from updates on periodic Map Reduce jobs to creating adhoc jobs for the business users Importing and exporting data into HDFS and Hive using Sqoop Experienced in defining job flows Scheduling all HadoopHive Sqoop Hbase jobs using Oozie Experienced in managing and reviewing Hadoop log files Experienced in running Hadoop streaming jobs to process terabytes of xml format data Responsible to manage data coming from different sources Involved in loading data from UNIX file system to HDFS Involved in creating Hive tables loading with data and writing hive queries which will run internally in map reduce way Designed and implemented Map reducebased largescale parallel relationlearning system Setup and benchmarked HadoopHBase clusters for internal use Understanding functional specifications and documenting technical design documents Performed unit testing for all the components using JUnit Implemented data access using Hibernate persistence framework Environment Java Eclipse Oracle 10g Sub Version Hadoop HBase Linux Map Reduce HDFS Hive Java JDK 16 Hadoop Distribution Cloudera Map Reduce SQL Windows UNIX Shell Scripting Hadoop Developer Celgene Township of Warren NJ November 2012 to December 2013 Celgene Pharmaceuticals is a global integrated biopharmaceutical company that delivers truly innovative and life changing drugs for various patients They are primarily engaged in the discovery development and commercialization of innovative therapies designed to treat cancer and immune inflammatory related diseases Roles Responsibilities Worked on analyzing Hadoop cluster using different big data analytic tools including Pig HBase and Sqoop Developed Message Handler Adapter which converts the data objects into XML message and invoke an enterprise service and viceversa using Java JMS and MQ Series Business logic is implemented using Struts action components in the Struts and Hibernate framework Migrating the needed data from MySQL into HDFS using Sqoop and importing various formats of unstructured data from logs into HDFS using Flume Used Multithreading for invoking the database and also implemented complex modules which contain business logics using Collection Reflection and Generics API Involved in Pig Latin programming Importing And Exporting Data from MySQLOracle to HiveQL using SQOOP Experienced in analyzing data with Hive and Pig Responsible for operational support of Production system Loading log data directly into HDFS using Flume Environment Apache Hadoop HDFS Java Map Reduce Eclipse Hive PIG Sqoop Flume Oozie JavaJ2EE Oracle 10g SQL PLSQL JSP EJB Struts Hibernate Weblogic 80 HTML AJAX Java Script JDBC XML JMS Java Developer ICICI Finance HYD September 2010 to September 2012 ICICI Finance uses real time application hosted across the globe for its loans and finance division This multitier architecture application facilitates settlement between various merchant systems and transaction processing units in ICICI Roles Responsibilities Worked on both Web Logic Portal 92 for Portal development and Web Logic 81 for Data Services Programming Worked on creating EJBs that implemented business logic Developed the presentation layer using JSP HTML CSS and client validations using JavaScript Involved in designing and development of the ecommerce site using JSP Servlet EJBs JavaScript and JDBC Used Eclipse 60 as IDE for application development Validated all forms using Struts validation framework and implemented Tiles framework in the presentation layer Configured Struts framework to implement MVC design patterns Designed and developed GUI using JSP HTML DHTML and CSS Worked with JMS for messaging interface Environment Java J2EE HTML DHTML CSS JavaScript JSP Servlets XML EJB Struts Weblogic 81 SQL Server 2008R2 CentOS UNIX LINUX Windows 7VistaXP Java Developer Nalinsoft Pvt Ltd Hyderabad Andhra Pradesh May 2009 to August 2010 Roles Responsibilities Designed Use Case Diagrams Class Diagrams and Sequence Diagrams and Object Diagrams using IBM Rational Rose model the detail design of the application Involved in designing user screens using HTMLas per user requirements Used SpringHibernate integration in the back end to fetch data from Oracle and MYSQL databases Used Spring Dependency Injection properties to provide loosecoupling between layers Implemented the Web Service client for the login authentication credit reports and applicant information Used Web services SOAP for transmission of large blocks of XML data over HTTP Used Hibernate object relational data mapping framework to persist and retrieve the data from database Wrote SQL queries stored procedures and triggers to perform backend database operations by using SQL Server 2005 Implemented the logging mechanism using Log4j framework Wrote test cases in JUnit for unit testing of classes Developed application to be implemented on Windows XP Created application using Eclipse IDE Installed Web Logic Server for handling HTTP RequestResponse Used Subversion for version control and created automated build scripts Environment JDK 15 Rational Rose Spring Web Services JAXWS Hibernate Log4j Weblogic SQL Server 2005 Windows XP HTML Eclipse Log4J XML JUnit SVN Additional Information TECHNICAL SKILLS Hadoop Ecosystem Hadoop MapReduce Sqoop Hive Oozie PIG HDFS Zookeeper Flume Spark Kafka NoSQL Databases Hbase Cassandra Mongo DB Java J2EE Technologies Core Java Servlets JSP JDBC JNDI Java Beans Languages C C JAVA SQL PLSQL PIG Latin HiveQL Unix shell scripting Frameworks MVC Spring Hibernate Struts 12 EJB JMS JUnit MRUnit Databases Oracle 11g10g9i My SQL DB2 MS SQL Server Application Server Apache Tomcat JBoss IBM Web sphere Web Logic Web Services WSDL SOAP Apache CXF Apache Axis REST Methodologies Scrum Agile Waterfall",
    "unique_id": "2fbc8fd6-6305-43bf-b5d4-67aeef4405ec"
}