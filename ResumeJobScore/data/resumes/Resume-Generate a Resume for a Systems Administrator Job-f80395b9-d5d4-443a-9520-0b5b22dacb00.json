{
    "clean_data": "Big Data Hadoop Lead Big Data Hadoop Lead Big Data Hadoop Lead JPMC Dallas TX Over all 10 years of experience in data analysis data modeling and implementation of enterprise class systems spanning Big Data Data Integration Object Oriented programming Data warehousing and Advanced Analytics 4 years of experience with Hadoop HDFS Map Reduce and Hadoop Ecosystem Hive Hive Oozie Kafka Impala Spark AVRO JSON Good knowledge of Hive optimization with ORC Partitions and Bucketing Data ingestion schedulers have been created using Sqoop and Oozie scheduler Have hands on experience in writing MapReduce jobs using Java Hands on experience in writing pig Latin scripts and pig commands and hive queries Having good knowledge and experience in Spark and Kafka Hands on experience in installing configuring and using ecosystem components like Hadoop MapReduce HDFS Sqoop Pig Scala Hive Impala Spark Experience in database development using SQL and PLSQL and experience working on databases like Oracle 9i10g Informix and SQL Server Experience working on NoSQL databases including HBase MongoDB Experience using Sqoop to import data into HDFS from RDBMS and viceversa Experience in Database Design and Development using Relational Databases Oracle MSSQL MySQL Server 20052008 and NoSQL Databases MongoDB Cassandra HBase Effective team player and excellent communication skills with insight to determine priorities schedule work and meet critical deadlines Having good work experience in file formats such as AVRO JSON and Parquet etc with Hadoop tools using SerDe concepts Experience in analyzing data in Spark using Scala and Pyspark Optimized the Hive tables using optimization techniques like partitions and bucketing to provide better performance with HiveQL queries Experience in importing streaming data into HDFS using flume sources and flume sinks and transforming the data using flume interceptors Experience utilizing Java tools in Business Web and ClientServer environments including Java Jdbc Servlets Jsp Struts Framework Jasper Reports and Sql Experienced with different scripting languages like Python and shell scripting Proficient in using various IDEs like Eclipse Net beans Experienced with different scripting languages like Python and shell scripting TECHNICAL SUMARY Big Data Ecosystem Hadoop Map Reduce HDFS HBase Spark Scala Impala Hive Pig Oozie sqoop Flume Kafka CDH4 JSON AVRO Java Technologies Java 5 Java 6 JAXP AJAX I18N JFC Swing Log4j Java Help API Work Experience Big Data Hadoop Lead JPMC Dallas TX May 2018 to Present Project AML Anti Money Laundering cards AML Cards is a compliance project handling all credit card transactions both retail and consumer The main goal is to detect fraud transactions and generate alerts on such transactions over a data about 400 GBMonth for USA Canada alone The project is divided into two parts 1 Segmentation 12month historical data is provided to analysts 2 Transaction Monitoring alerts are generated on 12 months recurring feed data This is a rule based alert generation model Responsibilities Lead the AML Cards North America development and DQ team successfully to implement the compliance project Involved in the project from POC and worked from data staging till saturation of DataMart and reporting Extensive experience in Amazon Web Services Amazon EC2 Amazon S3 Amazon Simple  Amazon RDS Amazon Elastic Load Balancing Amazon SQS AWS Identity and access management AWS Cloud Watch Amazon EBS and Amazon CloudFront Deployment performance scalability finetuning webapplication servers like WebLogic WebSphere JBoss and Pramati Tomcat Expertise in Spring framework including Spring IoC Spring DAO support Spring ORM Spring Microservices Spring AOP Spring Security Spring MVC Spring Cache Spring Integration Spring Boot and Spring REST Expertise in Developing applications using Restful Web Services SOAP Java J2EE Servlets EJB JPA WebSphere Commerce Hibernate Spring Framework Jasper Reports Server Ext js JSP JMS Struts XML Eclipse NetBeans jQuery Visual Source Safe CVS SVN JDBC JNDI JIRA ANT Maven IReport Apache Tiles Spring Batch Spring Security Spring Web flow Spring Data JPA JSF ICE faces HTML and Java Scripts Expertise in developing Microservices using Spring Boot and Node JS to build more physically separated modular applications which will improve scalability Availability Agility of application Experience and familiarity building modern Spring applications with Spring Boot Worked in an onsiteoffshore environment Completely responsible for creating data model for storing processing data and for generating reporting alerts This model is being implemented as standard across all regions as a global solution Involved in discussions and guiding other region teams on Citi Big data platform and AML cards data model and strategy Responsible for technical design and review of data dictionary Business requirement Responsible for providing technical solutions and work arounds Migrating the needed data from Data warehouse and Product processors into HDFS using Talend and Sqoop and importing various formats of flat files in to HDFS Using Spark Streaming to bring all credit card transactions in the Hadoop environment Involved in design of overall Citi Group Big data architecture Involved in discussion with source systems for issues related to DQ in data Integrated the hive warehouse with Spark Impala We replaced impala with spark due to impalas security issue Comfortable with SCALA functional programming idioms and very familiar with Iterate Enumerate streaming patterns Almost entire DQ and end to end reconciliation is done in SCALA SPARK Implemented partitioning dynamic partitions indexing and buckets HIVE Created Custom UDFs in JAVA to overcome HIVE limitations on cloudera CDH5 Used Hive to process data and Batch data filtering Used SparkImpala for any other value centric data filtering Supported and Monitored Map Reduce Programs running on the cluster Monitored logs and responded accordingly to any warning or failure conditions Responsible for preserving code and design integrity using SVN and SharePoint Gave a demo to business users on using Datameer for analytics Environment Apache Hadoop HDFS Hive Map Reduce Java Talend Spark Impala Scala Sqoop Cloudera CDH5 Platform SVN SharePoint Data Meer and Maven Big Data Hadoop Developer Drobo Santa Clara CA May 2015 to April 2018 Project Big Data as Service Description Big Data as a service is a project particularly designed for serving the business partners which involves in collecting the data of all the historic claims and loads that into Big Data platform It also involves assigning a personal SAN Security ID for each and every customer respective to particular secure area Responsibilities Have setup the 64 node cluster and configured the entire Hadoop platform Migrating the needed data from MySQL Mongo DB into HDFS using Sqoop and importing various formats of flat files into HDFS Mainly worked on Hive queries to categorize data of different claims Integrated the hive warehouse with HBase Used Kafka to store all online communications into Hbase Written customized HiveUDFs in Java where the functionality is too complex Designed and created Hive external tables using shared metastore instead of derby with partitioning dynamic partitioning and buckets Experience in Continuous delivery Continuous Integration CDCI tools Docker Jenkins to deploy this application to AWS Experience in making Junit and Test NG test cases and executed as part of auto build process from Jenkins Jobs Hands on experience in designing and implementation of Selenium WebDriver Automation Framework for Smoke test and Regression test using TestNG Experience in developing end to end automation using Selenium WebDriver Grid POM Junit TestNG Cucumber Object Repository Web Services REST SOAP Excellent knowledge and experience in SQL queries PLSQL stored procedures functions and triggers to interact with SQL MySQL Oracle databases Experience in Maven pomxml and as CICD tool Jenkins CI and Configured Log4j for logging mechanism HiveQL scripts to create load and query tables in a Hive Generate final reporting data using Tableau for testing by connecting to the corresponding Hive tables using Hive ODBC connector Supported Map Reduce Programs those are running on the cluster Maintain System integrity of all subcomponents related to Hadoop Maintained System integrity of all subcomponents primarily HDFS MR HBase and Hive Monitored System health and logs and respond accordingly to any warning or failure conditions Presented data and dataflow using Talend for reusability Environment Apache Hadoop HDFS Hive Map Reduce Java Pig Sqoop Cloudera CDH4 MySQL Tableau Talend Kafka SFTP Java J2EE Developer MetLife Insurance Somerset NJ January 2013 to April 2015 Project Disability Income Maintenance and Enhancement Application Disability Income Application is used for the Underwriting and Administration of Disabilities products This is a Maintenance and Enhancement Project for Individual and Institutional policies of MetLife Responsibilities Coded the business methods according to the IBM Rational Rose UML model Extensively used Core Java Servlets JSP and XML Experience implementing Struts Model View Controller framework spring frameworks and Object Relational mapping ORM tools such as Hibernate Extensive knowledge on Core Java technologies such as MultiThreading Exception Handling Reflection Collections Streams File IO Strong experience in developing JAVA J2EE applications using IDEs like Eclipse IntelliJ and Web servers like JBoss Tomcat WebLogic Strong working experience on mapping tools like Hibernate Hibernate Connection Pooling HQL Hibernate Caching Transactions Implemented Business Logic using POJOs and used Tomcat and WebLogic to deploy the applications Good perception of Object Oriented Programming concepts OOPS Good experience in spring modules like Spring AOP IOC etc Working experience on Multithreading synchronous and event based programming Extensive knowledge in deploying and maintaining the application on Tomcat and WebLogic servers Used Maven for the project management like build install Hands on experience in Object Oriented Design and Core Java concepts like Design Patterns Multithreading Exception Handling and Collection APIs Good experience in Spring like Spring Core IOC AOP Spring MVC Expertise in configuring the Spring Application Context with dependency injection and using Spring Framework that can integrate Hibernate and Web Services Developed various Spring starter POMs for Spring Boot based Rest services Used Struts 12 in presentation tier Generated the Hibernate XML and Java Mappings for the schemas Used DB2 Database to store the system data Used Rational Application Developer RAD as Integrated Development Environment IDE Used unit testing for all the components using JUnit Used Apache log 4j Logging framework for logging of trace and Auditing Used Asynchronous JavaScript and XML AJAX for better and faster interactive FrontEnd Used IBM WebSphere as the Application Server Used IBM Rational Clearcase as the version controller Environments Java 16 Servlets JSP Struts12 IBM Rational Application Developer RAD 6 Web sphere 60 iText AJAX Rational Clear case Rational Rose Oracle 9i log4j JAVA Developer VULAB Chicago IL July 2009 to December 2012 Order Management Software Order Management Software manages the sales operation of organization It manages the customers orders invoices and returns Responsibilities Involved in the complete SDLC software development life cycle of the application from requirement analysis to testing Developed the modules based on struts MVC Architecture Developed The UI using JavaScript JSP HTML and CSS for interactive cross browser functionality and complex user interface Proficient knowledge in developing the webbased applications design and development using JAVA J2EE Servlets JSP Spring JDBC Hibernate ANT Eclipse XML JDBC and Databases Experience in web application design using open source Struts Spring MVC Frameworks and J2EE Design Patterns Developed ANT Maven scripts in to build and deploy J2EE Applications Experience implementing Struts Model View Controller framework spring frameworks and Object Relational mapping ORM tools such as Hibernate Extensive knowledge on Core Java technologies such as MultiThreading Exception Handling Reflection Collections Streams File IO Strong experience in developing JAVAJ2EE applications using IDEs like Eclipse IntelliJ and Web servers like JBoss Tomcat WebLogic Strong working experience on mapping tools like Hibernate Hibernate Connection Pooling HQL Hibernate Caching Transactions Implemented Business Logic using POJOs and used Tomcat and WebLogic to deploy the applications Created Business Logic using Servlets Session beans and deployed them on WebLogic server Used MVC struts framework for application design Created complex SQL Queries PLSQL Stored procedures Functions for back end Prepared the Functional Design and Test case specifications Involved in writing Stored Procedures in Oracle to do some database side validations Performed unit testing system testing and integration testing Developed Unit Test Cases Used JUnit for unit testing of the application Provided Technical support for production environments resolving the issues analyzing the defects providing and implementing the solution defects Resolved more priority defects as per the schedule Environment J2EE JSP PLSQL HTML CSS Struts JUnit Used Eclipse IDE for all coding in Java Servlets and JSPs Coordinate with the QA lead for development of test plan test cases test code and actual testing responsible for defects allocation and ensuring that the defects are resolved Used Flex Styles and CSS to manage the Look and Feel of the application Deployed the application on Web Sphere Application server Environment Java 6 Eclipse Apache Tomcat Web Server JSP JavaScript AWT Servlets JDBC HTML Front Page 2000 Oracle CVS Education Bachelors Skills Eclipse Java Jboss Wsad Api Application server Design patterns Dtd Hbase Html Python Xml Xsl Xslt Database Db2 Mysql Oracle Oracle 10g Sql Additional Information Methodologies Agile UML Design Patterns Database Oracle 10g DB2 MySQL No Sql MongoDB Hbase Application Server Apache Tomcat 5x 60 Jboss 40 Web Tools HTML Java Script XML DTD Schemas XSL XSLT XPath DOM XQuery Tools SQL developer DB visualize Hortonworks IDE Testing Tools NetBeans Eclipse WSAD RAD Mat lab Operating System Windows Linux Scripts Bash Python ANT Testing API JUNIT",
    "entities": [
        "Citi Big",
        "Maven Big Data Hadoop Developer",
        "MVC Expertise",
        "Oracle CVS Education",
        "SVN SharePoint Data Meer",
        "JAVA Developer VULAB Chicago IL",
        "Used Rational Application Developer RAD",
        "Responsibilities",
        "Design Patterns Multithreading Exception Handling and Collection APIs Good",
        "IBM Rational Application Developer RAD 6",
        "Hibernate Extensive",
        "IBM",
        "Database Design and Development",
        "JAVA J2EE Servlets",
        "Hadoop",
        "XML",
        "WebLogic",
        "JUnit",
        "HBase",
        "FrontEnd Used IBM WebSphere",
        "TX",
        "Amazon",
        "Developed Unit Test Cases Used JUnit",
        "JAVAJ2EE",
        "Hibernate Spring Framework Jasper",
        "SQL Server",
        "Iterate Enumerate",
        "Project Big Data as Service Description Big Data",
        "Stored Procedures",
        "Developed",
        "Hadoop Maintained System",
        "Dallas",
        "JSP Struts12",
        "Continuous",
        "Provided Technical",
        "DQ",
        "Monitored",
        "Pyspark Optimized the Hive",
        "the IBM Rational Rose UML",
        "the Application Server Used",
        "Big Data Hadoop Lead Big Data Hadoop Lead Big Data Hadoop Lead",
        "Talend",
        "the Underwriting and Administration of Disabilities",
        "Present Project AML Anti Money Laundering cards AML Cards",
        "Hive Monitored System",
        "MultiThreading Exception Handling Reflection Collections Streams File IO Strong",
        "Advanced Analytics",
        "MVC",
        "Spark",
        "Supported",
        "Node JS",
        "AML",
        "Big Data Data Integration Object Oriented",
        "Object Oriented Design",
        "Sqoop",
        "QA",
        "HIVE",
        "Santa Clara",
        "SCALA SPARK Implemented",
        "MVC Spring Cache Spring Integration Spring Boot",
        "Created",
        "Mysql Oracle Oracle",
        "Hadoop MapReduce HDFS Sqoop",
        "Scala",
        "SAN Security",
        "Business Web",
        "Oracle",
        "ClientServer",
        "HIVE Created Custom",
        "Selenium WebDriver Grid",
        "HTML",
        "Oozie",
        "Citi Group Big",
        "HDFS MR HBase",
        "SQL Queries PLSQL Stored",
        "SQL",
        "SerDe",
        "Amazon Web Services",
        "ORC Partitions",
        "Created Business Logic using Servlets Session",
        "USA Canada",
        "Integrated Development Environment IDE Used",
        "Cassandra HBase Effective",
        "MetLife Responsibilities Coded",
        "Web Sphere Application",
        "Big Data",
        "Hive",
        "CICD",
        "Schemas",
        "WebLogic WebSphere JBoss",
        "Pramati Tomcat Expertise",
        "Test NG",
        "Regression",
        "Core Java Servlets JSP",
        "DB",
        "NetBeans jQuery Visual Source Safe CVS SVN",
        "JAVA",
        "Maven",
        "Performed",
        "XSLT",
        "Impala",
        "Scala Sqoop",
        "Selenium WebDriver Automation Framework",
        "Bucketing Data",
        "Availability Agility",
        "Sql Additional Information Methodologies Agile UML Design Patterns Database Oracle",
        "WebSphere Commerce",
        "Continuous Integration",
        "Hibernate Hibernate Connection Pooling HQL Hibernate Caching Transactions Implemented Business Logic",
        "SVN",
        "CSS",
        "Restful Web Services",
        "HDFS Mainly",
        "Tomcat",
        "Data",
        "MapReduce",
        "SCALA",
        "Order Management Software Order Management Software",
        "NoSQL",
        "Tableau",
        "Spring Core IOC",
        "Amazon CloudFront Deployment",
        "Informix",
        "HDFS Using Spark Streaming"
    ],
    "experience": "Experience in database development using SQL and PLSQL and experience working on databases like Oracle 9i10 g Informix and SQL Server Experience working on NoSQL databases including HBase MongoDB Experience using Sqoop to import data into HDFS from RDBMS and viceversa Experience in Database Design and Development using Relational Databases Oracle MSSQL MySQL Server 20052008 and NoSQL Databases MongoDB Cassandra HBase Effective team player and excellent communication skills with insight to determine priorities schedule work and meet critical deadlines Having good work experience in file formats such as AVRO JSON and Parquet etc with Hadoop tools using SerDe concepts Experience in analyzing data in Spark using Scala and Pyspark Optimized the Hive tables using optimization techniques like partitions and bucketing to provide better performance with HiveQL queries Experience in importing streaming data into HDFS using flume sources and flume sinks and transforming the data using flume interceptors Experience utilizing Java tools in Business Web and ClientServer environments including Java Jdbc Servlets Jsp Struts Framework Jasper Reports and Sql Experienced with different scripting languages like Python and shell scripting Proficient in using various IDEs like Eclipse Net beans Experienced with different scripting languages like Python and shell scripting TECHNICAL SUMARY Big Data Ecosystem Hadoop Map Reduce HDFS HBase Spark Scala Impala Hive Pig Oozie sqoop Flume Kafka CDH4 JSON AVRO Java Technologies Java 5 Java 6 JAXP AJAX I18N JFC Swing Log4j Java Help API Work Experience Big Data Hadoop Lead JPMC Dallas TX May 2018 to Present Project AML Anti Money Laundering cards AML Cards is a compliance project handling all credit card transactions both retail and consumer The main goal is to detect fraud transactions and generate alerts on such transactions over a data about 400 GBMonth for USA Canada alone The project is divided into two parts 1 Segmentation 12month historical data is provided to analysts 2 Transaction Monitoring alerts are generated on 12 months recurring feed data This is a rule based alert generation model Responsibilities Lead the AML Cards North America development and DQ team successfully to implement the compliance project Involved in the project from POC and worked from data staging till saturation of DataMart and reporting Extensive experience in Amazon Web Services Amazon EC2 Amazon S3 Amazon Simple   Amazon RDS Amazon Elastic Load Balancing Amazon SQS AWS Identity and access management AWS Cloud Watch Amazon EBS and Amazon CloudFront Deployment performance scalability finetuning webapplication servers like WebLogic WebSphere JBoss and Pramati Tomcat Expertise in Spring framework including Spring IoC Spring DAO support Spring ORM Spring Microservices Spring AOP Spring Security Spring MVC Spring Cache Spring Integration Spring Boot and Spring REST Expertise in Developing applications using Restful Web Services SOAP Java J2EE Servlets EJB JPA WebSphere Commerce Hibernate Spring Framework Jasper Reports Server Ext js JSP JMS Struts XML Eclipse NetBeans jQuery Visual Source Safe CVS SVN JDBC JNDI JIRA ANT Maven IReport Apache Tiles Spring Batch Spring Security Spring Web flow Spring Data JPA JSF ICE faces HTML and Java Scripts Expertise in developing Microservices using Spring Boot and Node JS to build more physically separated modular applications which will improve scalability Availability Agility of application Experience and familiarity building modern Spring applications with Spring Boot Worked in an onsiteoffshore environment Completely responsible for creating data model for storing processing data and for generating reporting alerts This model is being implemented as standard across all regions as a global solution Involved in discussions and guiding other region teams on Citi Big data platform and AML cards data model and strategy Responsible for technical design and review of data dictionary Business requirement Responsible for providing technical solutions and work arounds Migrating the needed data from Data warehouse and Product processors into HDFS using Talend and Sqoop and importing various formats of flat files in to HDFS Using Spark Streaming to bring all credit card transactions in the Hadoop environment Involved in design of overall Citi Group Big data architecture Involved in discussion with source systems for issues related to DQ in data Integrated the hive warehouse with Spark Impala We replaced impala with spark due to impalas security issue Comfortable with SCALA functional programming idioms and very familiar with Iterate Enumerate streaming patterns Almost entire DQ and end to end reconciliation is done in SCALA SPARK Implemented partitioning dynamic partitions indexing and buckets HIVE Created Custom UDFs in JAVA to overcome HIVE limitations on cloudera CDH5 Used Hive to process data and Batch data filtering Used SparkImpala for any other value centric data filtering Supported and Monitored Map Reduce Programs running on the cluster Monitored logs and responded accordingly to any warning or failure conditions Responsible for preserving code and design integrity using SVN and SharePoint Gave a demo to business users on using Datameer for analytics Environment Apache Hadoop HDFS Hive Map Reduce Java Talend Spark Impala Scala Sqoop Cloudera CDH5 Platform SVN SharePoint Data Meer and Maven Big Data Hadoop Developer Drobo Santa Clara CA May 2015 to April 2018 Project Big Data as Service Description Big Data as a service is a project particularly designed for serving the business partners which involves in collecting the data of all the historic claims and loads that into Big Data platform It also involves assigning a personal SAN Security ID for each and every customer respective to particular secure area Responsibilities Have setup the 64 node cluster and configured the entire Hadoop platform Migrating the needed data from MySQL Mongo DB into HDFS using Sqoop and importing various formats of flat files into HDFS Mainly worked on Hive queries to categorize data of different claims Integrated the hive warehouse with HBase Used Kafka to store all online communications into Hbase Written customized HiveUDFs in Java where the functionality is too complex Designed and created Hive external tables using shared metastore instead of derby with partitioning dynamic partitioning and buckets Experience in Continuous delivery Continuous Integration CDCI tools Docker Jenkins to deploy this application to AWS Experience in making Junit and Test NG test cases and executed as part of auto build process from Jenkins Jobs Hands on experience in designing and implementation of Selenium WebDriver Automation Framework for Smoke test and Regression test using TestNG Experience in developing end to end automation using Selenium WebDriver Grid POM Junit TestNG Cucumber Object Repository Web Services REST SOAP Excellent knowledge and experience in SQL queries PLSQL stored procedures functions and triggers to interact with SQL MySQL Oracle databases Experience in Maven pomxml and as CICD tool Jenkins CI and Configured Log4j for logging mechanism HiveQL scripts to create load and query tables in a Hive Generate final reporting data using Tableau for testing by connecting to the corresponding Hive tables using Hive ODBC connector Supported Map Reduce Programs those are running on the cluster Maintain System integrity of all subcomponents related to Hadoop Maintained System integrity of all subcomponents primarily HDFS MR HBase and Hive Monitored System health and logs and respond accordingly to any warning or failure conditions Presented data and dataflow using Talend for reusability Environment Apache Hadoop HDFS Hive Map Reduce Java Pig Sqoop Cloudera CDH4 MySQL Tableau Talend Kafka SFTP Java J2EE Developer MetLife Insurance Somerset NJ January 2013 to April 2015 Project Disability Income Maintenance and Enhancement Application Disability Income Application is used for the Underwriting and Administration of Disabilities products This is a Maintenance and Enhancement Project for Individual and Institutional policies of MetLife Responsibilities Coded the business methods according to the IBM Rational Rose UML model Extensively used Core Java Servlets JSP and XML Experience implementing Struts Model View Controller framework spring frameworks and Object Relational mapping ORM tools such as Hibernate Extensive knowledge on Core Java technologies such as MultiThreading Exception Handling Reflection Collections Streams File IO Strong experience in developing JAVA J2EE applications using IDEs like Eclipse IntelliJ and Web servers like JBoss Tomcat WebLogic Strong working experience on mapping tools like Hibernate Hibernate Connection Pooling HQL Hibernate Caching Transactions Implemented Business Logic using POJOs and used Tomcat and WebLogic to deploy the applications Good perception of Object Oriented Programming concepts OOPS Good experience in spring modules like Spring AOP IOC etc Working experience on Multithreading synchronous and event based programming Extensive knowledge in deploying and maintaining the application on Tomcat and WebLogic servers Used Maven for the project management like build install Hands on experience in Object Oriented Design and Core Java concepts like Design Patterns Multithreading Exception Handling and Collection APIs Good experience in Spring like Spring Core IOC AOP Spring MVC Expertise in configuring the Spring Application Context with dependency injection and using Spring Framework that can integrate Hibernate and Web Services Developed various Spring starter POMs for Spring Boot based Rest services Used Struts 12 in presentation tier Generated the Hibernate XML and Java Mappings for the schemas Used DB2 Database to store the system data Used Rational Application Developer RAD as Integrated Development Environment IDE Used unit testing for all the components using JUnit Used Apache log 4j Logging framework for logging of trace and Auditing Used Asynchronous JavaScript and XML AJAX for better and faster interactive FrontEnd Used IBM WebSphere as the Application Server Used IBM Rational Clearcase as the version controller Environments Java 16 Servlets JSP Struts12 IBM Rational Application Developer RAD 6 Web sphere 60 iText AJAX Rational Clear case Rational Rose Oracle 9i log4j JAVA Developer VULAB Chicago IL July 2009 to December 2012 Order Management Software Order Management Software manages the sales operation of organization It manages the customers orders invoices and returns Responsibilities Involved in the complete SDLC software development life cycle of the application from requirement analysis to testing Developed the modules based on struts MVC Architecture Developed The UI using JavaScript JSP HTML and CSS for interactive cross browser functionality and complex user interface Proficient knowledge in developing the webbased applications design and development using JAVA J2EE Servlets JSP Spring JDBC Hibernate ANT Eclipse XML JDBC and Databases Experience in web application design using open source Struts Spring MVC Frameworks and J2EE Design Patterns Developed ANT Maven scripts in to build and deploy J2EE Applications Experience implementing Struts Model View Controller framework spring frameworks and Object Relational mapping ORM tools such as Hibernate Extensive knowledge on Core Java technologies such as MultiThreading Exception Handling Reflection Collections Streams File IO Strong experience in developing JAVAJ2EE applications using IDEs like Eclipse IntelliJ and Web servers like JBoss Tomcat WebLogic Strong working experience on mapping tools like Hibernate Hibernate Connection Pooling HQL Hibernate Caching Transactions Implemented Business Logic using POJOs and used Tomcat and WebLogic to deploy the applications Created Business Logic using Servlets Session beans and deployed them on WebLogic server Used MVC struts framework for application design Created complex SQL Queries PLSQL Stored procedures Functions for back end Prepared the Functional Design and Test case specifications Involved in writing Stored Procedures in Oracle to do some database side validations Performed unit testing system testing and integration testing Developed Unit Test Cases Used JUnit for unit testing of the application Provided Technical support for production environments resolving the issues analyzing the defects providing and implementing the solution defects Resolved more priority defects as per the schedule Environment J2EE JSP PLSQL HTML CSS Struts JUnit Used Eclipse IDE for all coding in Java Servlets and JSPs Coordinate with the QA lead for development of test plan test cases test code and actual testing responsible for defects allocation and ensuring that the defects are resolved Used Flex Styles and CSS to manage the Look and Feel of the application Deployed the application on Web Sphere Application server Environment Java 6 Eclipse Apache Tomcat Web Server JSP JavaScript AWT Servlets JDBC HTML Front Page 2000 Oracle CVS Education Bachelors Skills Eclipse Java Jboss Wsad Api Application server Design patterns Dtd Hbase Html Python Xml Xsl Xslt Database Db2 Mysql Oracle Oracle 10 g Sql Additional Information Methodologies Agile UML Design Patterns Database Oracle 10 g DB2 MySQL No Sql MongoDB Hbase Application Server Apache Tomcat 5x 60 Jboss 40 Web Tools HTML Java Script XML DTD Schemas XSL XSLT XPath DOM XQuery Tools SQL developer DB visualize Hortonworks IDE Testing Tools NetBeans Eclipse WSAD RAD Mat lab Operating System Windows Linux Scripts Bash Python ANT Testing API JUNIT",
    "extracted_keywords": [
        "Big",
        "Data",
        "Hadoop",
        "Lead",
        "Big",
        "Data",
        "Hadoop",
        "Lead",
        "Big",
        "Data",
        "Hadoop",
        "Lead",
        "JPMC",
        "Dallas",
        "TX",
        "years",
        "experience",
        "data",
        "analysis",
        "data",
        "modeling",
        "implementation",
        "enterprise",
        "class",
        "systems",
        "Big",
        "Data",
        "Data",
        "Integration",
        "Object",
        "programming",
        "Data",
        "warehousing",
        "Advanced",
        "Analytics",
        "years",
        "experience",
        "Hadoop",
        "HDFS",
        "Map",
        "Reduce",
        "Hadoop",
        "Ecosystem",
        "Hive",
        "Hive",
        "Oozie",
        "Kafka",
        "Impala",
        "Spark",
        "AVRO",
        "JSON",
        "knowledge",
        "Hive",
        "optimization",
        "ORC",
        "Partitions",
        "Bucketing",
        "Data",
        "ingestion",
        "schedulers",
        "Sqoop",
        "Oozie",
        "scheduler",
        "hands",
        "experience",
        "MapReduce",
        "jobs",
        "Java",
        "Hands",
        "experience",
        "pig",
        "scripts",
        "pig",
        "commands",
        "hive",
        "queries",
        "knowledge",
        "experience",
        "Spark",
        "Kafka",
        "Hands",
        "experience",
        "configuring",
        "ecosystem",
        "components",
        "Hadoop",
        "MapReduce",
        "HDFS",
        "Sqoop",
        "Pig",
        "Scala",
        "Hive",
        "Impala",
        "Spark",
        "Experience",
        "database",
        "development",
        "SQL",
        "PLSQL",
        "experience",
        "databases",
        "Oracle",
        "g",
        "Informix",
        "SQL",
        "Server",
        "Experience",
        "NoSQL",
        "databases",
        "HBase",
        "MongoDB",
        "Experience",
        "Sqoop",
        "data",
        "HDFS",
        "RDBMS",
        "viceversa",
        "Experience",
        "Database",
        "Design",
        "Development",
        "Relational",
        "Databases",
        "Oracle",
        "MSSQL",
        "MySQL",
        "Server",
        "NoSQL",
        "MongoDB",
        "Cassandra",
        "HBase",
        "team",
        "player",
        "communication",
        "skills",
        "insight",
        "priorities",
        "schedule",
        "work",
        "deadlines",
        "work",
        "experience",
        "file",
        "formats",
        "AVRO",
        "JSON",
        "Parquet",
        "Hadoop",
        "tools",
        "SerDe",
        "concepts",
        "Experience",
        "data",
        "Spark",
        "Scala",
        "Pyspark",
        "Hive",
        "tables",
        "optimization",
        "techniques",
        "partitions",
        "bucketing",
        "performance",
        "HiveQL",
        "queries",
        "Experience",
        "data",
        "HDFS",
        "flume",
        "sources",
        "sinks",
        "data",
        "flume",
        "interceptors",
        "Experience",
        "Java",
        "tools",
        "Business",
        "Web",
        "ClientServer",
        "environments",
        "Java",
        "Jdbc",
        "Servlets",
        "Jsp",
        "Struts",
        "Framework",
        "Jasper",
        "Reports",
        "Sql",
        "scripting",
        "languages",
        "Python",
        "shell",
        "Proficient",
        "IDEs",
        "Eclipse",
        "Net",
        "beans",
        "scripting",
        "languages",
        "Python",
        "shell",
        "TECHNICAL",
        "SUMARY",
        "Big",
        "Data",
        "Ecosystem",
        "Hadoop",
        "Map",
        "HDFS",
        "HBase",
        "Spark",
        "Scala",
        "Impala",
        "Hive",
        "Pig",
        "Oozie",
        "sqoop",
        "Flume",
        "Kafka",
        "CDH4",
        "AVRO",
        "Java",
        "Technologies",
        "Java",
        "Java",
        "JAXP",
        "AJAX",
        "I18N",
        "JFC",
        "Swing",
        "Log4j",
        "Java",
        "Help",
        "API",
        "Work",
        "Experience",
        "Big",
        "Data",
        "Hadoop",
        "Lead",
        "JPMC",
        "Dallas",
        "TX",
        "May",
        "Present",
        "Project",
        "AML",
        "Anti",
        "Money",
        "Laundering",
        "cards",
        "AML",
        "Cards",
        "compliance",
        "project",
        "credit",
        "card",
        "transactions",
        "consumer",
        "goal",
        "fraud",
        "transactions",
        "alerts",
        "transactions",
        "data",
        "GBMonth",
        "USA",
        "Canada",
        "project",
        "parts",
        "Segmentation",
        "data",
        "analysts",
        "Transaction",
        "Monitoring",
        "alerts",
        "months",
        "feed",
        "data",
        "rule",
        "generation",
        "model",
        "Responsibilities",
        "AML",
        "Cards",
        "North",
        "America",
        "development",
        "DQ",
        "team",
        "compliance",
        "project",
        "project",
        "POC",
        "data",
        "staging",
        "saturation",
        "DataMart",
        "experience",
        "Amazon",
        "Web",
        "Services",
        "Amazon",
        "EC2",
        "Amazon",
        "S3",
        "Amazon",
        "Simple",
        "Amazon",
        "RDS",
        "Amazon",
        "Elastic",
        "Load",
        "Amazon",
        "SQS",
        "AWS",
        "Identity",
        "access",
        "management",
        "AWS",
        "Cloud",
        "Amazon",
        "EBS",
        "Amazon",
        "CloudFront",
        "Deployment",
        "performance",
        "scalability",
        "webapplication",
        "servers",
        "WebLogic",
        "WebSphere",
        "JBoss",
        "Pramati",
        "Tomcat",
        "Expertise",
        "Spring",
        "framework",
        "Spring",
        "IoC",
        "Spring",
        "DAO",
        "support",
        "Spring",
        "ORM",
        "Spring",
        "Microservices",
        "Spring",
        "AOP",
        "Spring",
        "Security",
        "Spring",
        "MVC",
        "Spring",
        "Cache",
        "Spring",
        "Integration",
        "Spring",
        "Boot",
        "Spring",
        "REST",
        "Expertise",
        "applications",
        "Restful",
        "Web",
        "Services",
        "SOAP",
        "Java",
        "J2EE",
        "Servlets",
        "EJB",
        "JPA",
        "WebSphere",
        "Commerce",
        "Hibernate",
        "Spring",
        "Framework",
        "Jasper",
        "Reports",
        "Server",
        "Ext",
        "JSP",
        "JMS",
        "Struts",
        "XML",
        "Eclipse",
        "NetBeans",
        "jQuery",
        "Visual",
        "Source",
        "Safe",
        "CVS",
        "SVN",
        "JDBC",
        "JNDI",
        "JIRA",
        "ANT",
        "Maven",
        "IReport",
        "Apache",
        "Tiles",
        "Spring",
        "Batch",
        "Spring",
        "Security",
        "Spring",
        "Web",
        "flow",
        "Spring",
        "Data",
        "JPA",
        "JSF",
        "ICE",
        "HTML",
        "Java",
        "Scripts",
        "Expertise",
        "Microservices",
        "Spring",
        "Boot",
        "Node",
        "JS",
        "applications",
        "scalability",
        "Availability",
        "Agility",
        "application",
        "Experience",
        "familiarity",
        "Spring",
        "applications",
        "Spring",
        "Boot",
        "environment",
        "data",
        "model",
        "processing",
        "data",
        "reporting",
        "alerts",
        "model",
        "standard",
        "regions",
        "solution",
        "discussions",
        "region",
        "teams",
        "Citi",
        "data",
        "platform",
        "AML",
        "cards",
        "data",
        "model",
        "strategy",
        "design",
        "review",
        "data",
        "Business",
        "requirement",
        "solutions",
        "work",
        "data",
        "Data",
        "warehouse",
        "Product",
        "processors",
        "HDFS",
        "Talend",
        "Sqoop",
        "formats",
        "files",
        "HDFS",
        "Spark",
        "Streaming",
        "credit",
        "card",
        "transactions",
        "Hadoop",
        "environment",
        "design",
        "Citi",
        "Group",
        "data",
        "architecture",
        "discussion",
        "source",
        "systems",
        "issues",
        "DQ",
        "data",
        "Integrated",
        "hive",
        "warehouse",
        "Spark",
        "Impala",
        "impala",
        "spark",
        "impalas",
        "security",
        "issue",
        "SCALA",
        "programming",
        "idioms",
        "Iterate",
        "Enumerate",
        "streaming",
        "patterns",
        "DQ",
        "end",
        "reconciliation",
        "SCALA",
        "SPARK",
        "partitions",
        "indexing",
        "buckets",
        "Custom",
        "UDFs",
        "JAVA",
        "HIVE",
        "limitations",
        "cloudera",
        "CDH5",
        "Hive",
        "data",
        "Batch",
        "data",
        "filtering",
        "SparkImpala",
        "value",
        "data",
        "Supported",
        "Monitored",
        "Map",
        "Reduce",
        "Programs",
        "cluster",
        "Monitored",
        "logs",
        "warning",
        "failure",
        "conditions",
        "code",
        "design",
        "integrity",
        "SVN",
        "SharePoint",
        "demo",
        "business",
        "users",
        "Datameer",
        "analytics",
        "Environment",
        "Apache",
        "Hadoop",
        "HDFS",
        "Hive",
        "Map",
        "Reduce",
        "Java",
        "Talend",
        "Spark",
        "Impala",
        "Scala",
        "Sqoop",
        "Cloudera",
        "CDH5",
        "Platform",
        "SVN",
        "SharePoint",
        "Data",
        "Meer",
        "Maven",
        "Big",
        "Data",
        "Hadoop",
        "Developer",
        "Drobo",
        "Santa",
        "Clara",
        "CA",
        "May",
        "April",
        "Project",
        "Big",
        "Data",
        "Service",
        "Description",
        "Big",
        "Data",
        "service",
        "project",
        "business",
        "partners",
        "data",
        "claims",
        "Data",
        "platform",
        "SAN",
        "Security",
        "ID",
        "customer",
        "area",
        "Responsibilities",
        "setup",
        "node",
        "cluster",
        "Hadoop",
        "platform",
        "data",
        "MySQL",
        "Mongo",
        "DB",
        "HDFS",
        "Sqoop",
        "formats",
        "files",
        "HDFS",
        "Hive",
        "queries",
        "data",
        "claims",
        "hive",
        "warehouse",
        "HBase",
        "Kafka",
        "communications",
        "Hbase",
        "Written",
        "HiveUDFs",
        "Java",
        "functionality",
        "Hive",
        "tables",
        "metastore",
        "derby",
        "partitioning",
        "Experience",
        "delivery",
        "Continuous",
        "Integration",
        "CDCI",
        "tools",
        "Docker",
        "Jenkins",
        "application",
        "AWS",
        "Experience",
        "Junit",
        "Test",
        "NG",
        "test",
        "cases",
        "part",
        "auto",
        "build",
        "process",
        "Jenkins",
        "Jobs",
        "Hands",
        "experience",
        "designing",
        "implementation",
        "Selenium",
        "WebDriver",
        "Automation",
        "Framework",
        "Smoke",
        "test",
        "Regression",
        "test",
        "TestNG",
        "Experience",
        "end",
        "automation",
        "Selenium",
        "WebDriver",
        "Grid",
        "POM",
        "Junit",
        "TestNG",
        "Cucumber",
        "Object",
        "Repository",
        "Web",
        "Services",
        "REST",
        "Excellent",
        "knowledge",
        "experience",
        "SQL",
        "queries",
        "procedures",
        "functions",
        "triggers",
        "SQL",
        "MySQL",
        "Oracle",
        "Experience",
        "Maven",
        "pomxml",
        "CICD",
        "tool",
        "Jenkins",
        "CI",
        "Configured",
        "Log4j",
        "mechanism",
        "scripts",
        "load",
        "query",
        "tables",
        "Hive",
        "Generate",
        "reporting",
        "data",
        "Tableau",
        "Hive",
        "tables",
        "Hive",
        "ODBC",
        "connector",
        "Supported",
        "Map",
        "Reduce",
        "Programs",
        "cluster",
        "Maintain",
        "System",
        "integrity",
        "subcomponents",
        "Hadoop",
        "Maintained",
        "System",
        "integrity",
        "subcomponents",
        "MR",
        "HBase",
        "Hive",
        "Monitored",
        "System",
        "health",
        "logs",
        "warning",
        "failure",
        "conditions",
        "data",
        "Talend",
        "reusability",
        "Environment",
        "Apache",
        "Hadoop",
        "HDFS",
        "Hive",
        "Map",
        "Reduce",
        "Java",
        "Pig",
        "Sqoop",
        "Cloudera",
        "CDH4",
        "MySQL",
        "Tableau",
        "Talend",
        "Kafka",
        "SFTP",
        "Java",
        "J2EE",
        "Developer",
        "MetLife",
        "Insurance",
        "Somerset",
        "NJ",
        "January",
        "April",
        "Project",
        "Disability",
        "Income",
        "Maintenance",
        "Enhancement",
        "Application",
        "Disability",
        "Income",
        "Application",
        "Underwriting",
        "Administration",
        "Disabilities",
        "products",
        "Maintenance",
        "Enhancement",
        "Project",
        "policies",
        "MetLife",
        "Responsibilities",
        "business",
        "methods",
        "IBM",
        "Rational",
        "Rose",
        "UML",
        "model",
        "Core",
        "Java",
        "Servlets",
        "JSP",
        "XML",
        "Experience",
        "Struts",
        "Model",
        "View",
        "Controller",
        "framework",
        "spring",
        "frameworks",
        "Object",
        "Relational",
        "mapping",
        "ORM",
        "tools",
        "Hibernate",
        "knowledge",
        "Core",
        "Java",
        "technologies",
        "MultiThreading",
        "Exception",
        "Handling",
        "Reflection",
        "Collections",
        "Streams",
        "File",
        "IO",
        "experience",
        "J2EE",
        "applications",
        "IDEs",
        "Eclipse",
        "IntelliJ",
        "Web",
        "servers",
        "JBoss",
        "Tomcat",
        "WebLogic",
        "Strong",
        "working",
        "experience",
        "mapping",
        "tools",
        "Hibernate",
        "Hibernate",
        "Connection",
        "HQL",
        "Hibernate",
        "Caching",
        "Transactions",
        "Business",
        "Logic",
        "POJOs",
        "Tomcat",
        "WebLogic",
        "applications",
        "perception",
        "Object",
        "Programming",
        "concepts",
        "experience",
        "spring",
        "modules",
        "Spring",
        "AOP",
        "IOC",
        "Working",
        "experience",
        "event",
        "knowledge",
        "application",
        "Tomcat",
        "WebLogic",
        "servers",
        "Maven",
        "project",
        "management",
        "Hands",
        "experience",
        "Object",
        "Oriented",
        "Design",
        "Core",
        "Java",
        "concepts",
        "Design",
        "Patterns",
        "Multithreading",
        "Exception",
        "Handling",
        "Collection",
        "APIs",
        "experience",
        "Spring",
        "Spring",
        "Core",
        "IOC",
        "AOP",
        "Spring",
        "MVC",
        "Expertise",
        "Spring",
        "Application",
        "Context",
        "dependency",
        "injection",
        "Spring",
        "Framework",
        "Hibernate",
        "Web",
        "Services",
        "Spring",
        "POMs",
        "Spring",
        "Boot",
        "Rest",
        "services",
        "Struts",
        "presentation",
        "tier",
        "Hibernate",
        "XML",
        "Java",
        "Mappings",
        "schemas",
        "Used",
        "DB2",
        "Database",
        "system",
        "data",
        "Rational",
        "Application",
        "Developer",
        "RAD",
        "Integrated",
        "Development",
        "Environment",
        "IDE",
        "unit",
        "testing",
        "components",
        "JUnit",
        "Used",
        "Apache",
        "4j",
        "framework",
        "trace",
        "Auditing",
        "Asynchronous",
        "JavaScript",
        "XML",
        "AJAX",
        "FrontEnd",
        "IBM",
        "WebSphere",
        "Application",
        "Server",
        "IBM",
        "Rational",
        "Clearcase",
        "version",
        "controller",
        "Environments",
        "Java",
        "Servlets",
        "JSP",
        "Struts12",
        "IBM",
        "Rational",
        "Application",
        "Developer",
        "RAD",
        "Web",
        "sphere",
        "iText",
        "AJAX",
        "Rational",
        "Clear",
        "case",
        "Rational",
        "Rose",
        "Oracle",
        "9i",
        "log4j",
        "Developer",
        "VULAB",
        "Chicago",
        "IL",
        "July",
        "December",
        "Order",
        "Management",
        "Software",
        "Order",
        "Management",
        "Software",
        "sales",
        "operation",
        "organization",
        "customers",
        "invoices",
        "Responsibilities",
        "SDLC",
        "software",
        "development",
        "life",
        "cycle",
        "application",
        "requirement",
        "analysis",
        "Developed",
        "modules",
        "struts",
        "MVC",
        "Architecture",
        "UI",
        "JavaScript",
        "JSP",
        "HTML",
        "CSS",
        "cross",
        "browser",
        "functionality",
        "user",
        "knowledge",
        "applications",
        "design",
        "development",
        "JAVA",
        "J2EE",
        "Servlets",
        "JSP",
        "Spring",
        "JDBC",
        "Hibernate",
        "ANT",
        "Eclipse",
        "XML",
        "JDBC",
        "Databases",
        "Experience",
        "web",
        "application",
        "design",
        "source",
        "Struts",
        "Spring",
        "MVC",
        "Frameworks",
        "J2EE",
        "Design",
        "Patterns",
        "ANT",
        "Maven",
        "scripts",
        "J2EE",
        "Applications",
        "Experience",
        "Struts",
        "Model",
        "View",
        "Controller",
        "framework",
        "spring",
        "frameworks",
        "Object",
        "Relational",
        "mapping",
        "ORM",
        "tools",
        "Hibernate",
        "knowledge",
        "Core",
        "Java",
        "technologies",
        "MultiThreading",
        "Exception",
        "Handling",
        "Reflection",
        "Collections",
        "Streams",
        "File",
        "IO",
        "experience",
        "JAVAJ2EE",
        "applications",
        "IDEs",
        "Eclipse",
        "IntelliJ",
        "Web",
        "servers",
        "JBoss",
        "Tomcat",
        "WebLogic",
        "Strong",
        "working",
        "experience",
        "mapping",
        "tools",
        "Hibernate",
        "Hibernate",
        "Connection",
        "HQL",
        "Hibernate",
        "Caching",
        "Transactions",
        "Business",
        "Logic",
        "POJOs",
        "Tomcat",
        "WebLogic",
        "applications",
        "Business",
        "Logic",
        "Servlets",
        "Session",
        "beans",
        "WebLogic",
        "server",
        "MVC",
        "struts",
        "framework",
        "application",
        "design",
        "SQL",
        "Queries",
        "PLSQL",
        "procedures",
        "Functions",
        "end",
        "Functional",
        "Design",
        "Test",
        "case",
        "specifications",
        "Procedures",
        "Oracle",
        "database",
        "side",
        "Performed",
        "unit",
        "testing",
        "system",
        "testing",
        "integration",
        "testing",
        "Developed",
        "Unit",
        "Test",
        "Cases",
        "JUnit",
        "unit",
        "testing",
        "application",
        "support",
        "production",
        "environments",
        "issues",
        "defects",
        "solution",
        "defects",
        "priority",
        "defects",
        "schedule",
        "Environment",
        "J2EE",
        "JSP",
        "PLSQL",
        "HTML",
        "CSS",
        "Struts",
        "JUnit",
        "Eclipse",
        "IDE",
        "Java",
        "Servlets",
        "JSPs",
        "QA",
        "lead",
        "development",
        "test",
        "plan",
        "test",
        "cases",
        "code",
        "testing",
        "defects",
        "allocation",
        "defects",
        "Flex",
        "Styles",
        "CSS",
        "Look",
        "Feel",
        "application",
        "application",
        "Web",
        "Sphere",
        "Application",
        "server",
        "Environment",
        "Java",
        "Eclipse",
        "Apache",
        "Tomcat",
        "Web",
        "Server",
        "JSP",
        "JavaScript",
        "AWT",
        "Servlets",
        "JDBC",
        "HTML",
        "Front",
        "Page",
        "Oracle",
        "CVS",
        "Education",
        "Bachelors",
        "Skills",
        "Eclipse",
        "Java",
        "Jboss",
        "Wsad",
        "Api",
        "Application",
        "server",
        "Design",
        "patterns",
        "Dtd",
        "Hbase",
        "Html",
        "Python",
        "Xml",
        "Xsl",
        "Xslt",
        "Database",
        "Db2",
        "Mysql",
        "Oracle",
        "Oracle",
        "g",
        "Sql",
        "Additional",
        "Information",
        "Methodologies",
        "Agile",
        "UML",
        "Design",
        "Patterns",
        "Database",
        "Oracle",
        "g",
        "DB2",
        "MySQL",
        "Sql",
        "MongoDB",
        "Hbase",
        "Application",
        "Server",
        "Apache",
        "Tomcat",
        "Jboss",
        "Web",
        "Tools",
        "HTML",
        "Java",
        "Script",
        "XML",
        "DTD",
        "Schemas",
        "XSL",
        "XSLT",
        "XPath",
        "DOM",
        "XQuery",
        "Tools",
        "SQL",
        "developer",
        "DB",
        "visualize",
        "Hortonworks",
        "IDE",
        "Testing",
        "Tools",
        "NetBeans",
        "Eclipse",
        "WSAD",
        "RAD",
        "Mat",
        "lab",
        "System",
        "Linux",
        "Scripts",
        "Bash",
        "Python",
        "ANT",
        "Testing",
        "API",
        "JUNIT"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T22:37:11.692729",
    "resume_data": "Big Data Hadoop Lead Big Data Hadoop Lead Big Data Hadoop Lead JPMC Dallas TX Over all 10 years of experience in data analysis data modeling and implementation of enterprise class systems spanning Big Data Data Integration Object Oriented programming Data warehousing and Advanced Analytics 4 years of experience with Hadoop HDFS Map Reduce and Hadoop Ecosystem Hive Hive Oozie Kafka Impala Spark AVRO JSON Good knowledge of Hive optimization with ORC Partitions and Bucketing Data ingestion schedulers have been created using Sqoop and Oozie scheduler Have hands on experience in writing MapReduce jobs using Java Hands on experience in writing pig Latin scripts and pig commands and hive queries Having good knowledge and experience in Spark and Kafka Hands on experience in installing configuring and using ecosystem components like Hadoop MapReduce HDFS Sqoop Pig Scala Hive Impala Spark Experience in database development using SQL and PLSQL and experience working on databases like Oracle 9i10g Informix and SQL Server Experience working on NoSQL databases including HBase MongoDB Experience using Sqoop to import data into HDFS from RDBMS and viceversa Experience in Database Design and Development using Relational Databases Oracle MSSQL MySQL Server 20052008 and NoSQL Databases MongoDB Cassandra HBase Effective team player and excellent communication skills with insight to determine priorities schedule work and meet critical deadlines Having good work experience in file formats such as AVRO JSON and Parquet etc with Hadoop tools using SerDe concepts Experience in analyzing data in Spark using Scala and Pyspark Optimized the Hive tables using optimization techniques like partitions and bucketing to provide better performance with HiveQL queries Experience in importing streaming data into HDFS using flume sources and flume sinks and transforming the data using flume interceptors Experience utilizing Java tools in Business Web and ClientServer environments including Java Jdbc Servlets Jsp Struts Framework Jasper Reports and Sql Experienced with different scripting languages like Python and shell scripting Proficient in using various IDEs like Eclipse Net beans Experienced with different scripting languages like Python and shell scripting TECHNICAL SUMARY Big Data Ecosystem Hadoop Map Reduce HDFS HBase Spark Scala Impala Hive Pig Oozie sqoop Flume Kafka CDH4 JSON AVRO Java Technologies Java 5 Java 6 JAXP AJAX I18N JFC Swing Log4j Java Help API Work Experience Big Data Hadoop Lead JPMC Dallas TX May 2018 to Present Project AML Anti Money Laundering cards AML Cards is a compliance project handling all credit card transactions both retail and consumer The main goal is to detect fraud transactions and generate alerts on such transactions over a data about 400 GBMonth for USA Canada alone The project is divided into two parts 1 Segmentation 12month historical data is provided to analysts 2 Transaction Monitoring alerts are generated on 12 months recurring feed data This is a rule based alert generation model Responsibilities Lead the AML Cards North America development and DQ team successfully to implement the compliance project Involved in the project from POC and worked from data staging till saturation of DataMart and reporting Extensive experience in Amazon Web Services Amazon EC2 Amazon S3 Amazon Simple dB Amazon RDS Amazon Elastic Load Balancing Amazon SQS AWS Identity and access management AWS Cloud Watch Amazon EBS and Amazon CloudFront Deployment performance scalability finetuning webapplication servers like WebLogic WebSphere JBoss and Pramati Tomcat Expertise in Spring framework including Spring IoC Spring DAO support Spring ORM Spring Microservices Spring AOP Spring Security Spring MVC Spring Cache Spring Integration Spring Boot and Spring REST Expertise in Developing applications using Restful Web Services SOAP Java J2EE Servlets EJB JPA WebSphere Commerce Hibernate Spring Framework Jasper Reports Server Ext js JSP JMS Struts XML Eclipse NetBeans jQuery Visual Source Safe CVS SVN JDBC JNDI JIRA ANT Maven IReport Apache Tiles Spring Batch Spring Security Spring Web flow Spring Data JPA JSF ICE faces HTML and Java Scripts Expertise in developing Microservices using Spring Boot and Node JS to build more physically separated modular applications which will improve scalability Availability Agility of application Experience and familiarity building modern Spring applications with Spring Boot Worked in an onsiteoffshore environment Completely responsible for creating data model for storing processing data and for generating reporting alerts This model is being implemented as standard across all regions as a global solution Involved in discussions and guiding other region teams on Citi Big data platform and AML cards data model and strategy Responsible for technical design and review of data dictionary Business requirement Responsible for providing technical solutions and work arounds Migrating the needed data from Data warehouse and Product processors into HDFS using Talend and Sqoop and importing various formats of flat files in to HDFS Using Spark Streaming to bring all credit card transactions in the Hadoop environment Involved in design of overall Citi Group Big data architecture Involved in discussion with source systems for issues related to DQ in data Integrated the hive warehouse with Spark Impala We replaced impala with spark due to impalas security issue Comfortable with SCALA functional programming idioms and very familiar with Iterate Enumerate streaming patterns Almost entire DQ and end to end reconciliation is done in SCALA SPARK Implemented partitioning dynamic partitions indexing and buckets HIVE Created Custom UDFs in JAVA to overcome HIVE limitations on cloudera CDH5 Used Hive to process data and Batch data filtering Used SparkImpala for any other value centric data filtering Supported and Monitored Map Reduce Programs running on the cluster Monitored logs and responded accordingly to any warning or failure conditions Responsible for preserving code and design integrity using SVN and SharePoint Gave a demo to business users on using Datameer for analytics Environment Apache Hadoop HDFS Hive Map Reduce Java Talend Spark Impala Scala Sqoop Cloudera CDH5 Platform SVN SharePoint Data Meer and Maven Big Data Hadoop Developer Drobo Santa Clara CA May 2015 to April 2018 Project Big Data as Service Description Big Data as a service is a project particularly designed for serving the business partners which involves in collecting the data of all the historic claims and loads that into Big Data platform It also involves assigning a personal SAN Security ID for each and every customer respective to particular secure area Responsibilities Have setup the 64 node cluster and configured the entire Hadoop platform Migrating the needed data from MySQL Mongo DB into HDFS using Sqoop and importing various formats of flat files into HDFS Mainly worked on Hive queries to categorize data of different claims Integrated the hive warehouse with HBase Used Kafka to store all online communications into Hbase Written customized HiveUDFs in Java where the functionality is too complex Designed and created Hive external tables using shared metastore instead of derby with partitioning dynamic partitioning and buckets Experience in Continuous delivery Continuous Integration CDCI tools Docker Jenkins to deploy this application to AWS Experience in making Junit and Test NG test cases and executed as part of auto build process from Jenkins Jobs Hands on experience in designing and implementation of Selenium WebDriver Automation Framework for Smoke test and Regression test using TestNG Experience in developing end to end automation using Selenium WebDriver Grid POM Junit TestNG Cucumber Object Repository Web Services REST SOAP Excellent knowledge and experience in SQL queries PLSQL stored procedures functions and triggers to interact with SQL MySQL Oracle databases Experience in Maven pomxml and as CICD tool Jenkins CI and Configured Log4j for logging mechanism HiveQL scripts to create load and query tables in a Hive Generate final reporting data using Tableau for testing by connecting to the corresponding Hive tables using Hive ODBC connector Supported Map Reduce Programs those are running on the cluster Maintain System integrity of all subcomponents related to Hadoop Maintained System integrity of all subcomponents primarily HDFS MR HBase and Hive Monitored System health and logs and respond accordingly to any warning or failure conditions Presented data and dataflow using Talend for reusability Environment Apache Hadoop HDFS Hive Map Reduce Java Pig Sqoop Cloudera CDH4 MySQL Tableau Talend Kafka SFTP Java J2EE Developer MetLife Insurance Somerset NJ January 2013 to April 2015 Project Disability Income Maintenance and Enhancement Application Disability Income Application is used for the Underwriting and Administration of Disabilities products This is a Maintenance and Enhancement Project for Individual and Institutional policies of MetLife Responsibilities Coded the business methods according to the IBM Rational Rose UML model Extensively used Core Java Servlets JSP and XML Experience implementing Struts Model View Controller framework spring frameworks and Object Relational mapping ORM tools such as Hibernate Extensive knowledge on Core Java technologies such as MultiThreading Exception Handling Reflection Collections Streams File IO Strong experience in developing JAVA J2EE applications using IDEs like Eclipse IntelliJ and Web servers like JBoss Tomcat WebLogic Strong working experience on mapping tools like Hibernate Hibernate Connection Pooling HQL Hibernate Caching Transactions Implemented Business Logic using POJOs and used Tomcat and WebLogic to deploy the applications Good perception of Object Oriented Programming concepts OOPS Good experience in spring modules like Spring AOP IOC etc Working experience on Multithreading synchronous and event based programming Extensive knowledge in deploying and maintaining the application on Tomcat and WebLogic servers Used Maven for the project management like build install Hands on experience in Object Oriented Design and Core Java concepts like Design Patterns Multithreading Exception Handling and Collection APIs Good experience in Spring like Spring Core IOC AOP Spring MVC Expertise in configuring the Spring Application Context with dependency injection and using Spring Framework that can integrate Hibernate and Web Services Developed various Spring starter POMs for Spring Boot based Rest services Used Struts 12 in presentation tier Generated the Hibernate XML and Java Mappings for the schemas Used DB2 Database to store the system data Used Rational Application Developer RAD as Integrated Development Environment IDE Used unit testing for all the components using JUnit Used Apache log 4j Logging framework for logging of trace and Auditing Used Asynchronous JavaScript and XML AJAX for better and faster interactive FrontEnd Used IBM WebSphere as the Application Server Used IBM Rational Clearcase as the version controller Environments Java 16 Servlets JSP Struts12 IBM Rational Application Developer RAD 6 Web sphere 60 iText AJAX Rational Clear case Rational Rose Oracle 9i log4j JAVA Developer VULAB Chicago IL July 2009 to December 2012 Order Management Software Order Management Software manages the sales operation of organization It manages the customers orders invoices and returns Responsibilities Involved in the complete SDLC software development life cycle of the application from requirement analysis to testing Developed the modules based on struts MVC Architecture Developed The UI using JavaScript JSP HTML and CSS for interactive cross browser functionality and complex user interface Proficient knowledge in developing the webbased applications design and development using JAVA J2EE Servlets JSP Spring JDBC Hibernate ANT Eclipse XML JDBC and Databases Experience in web application design using open source Struts Spring MVC Frameworks and J2EE Design Patterns Developed ANT Maven scripts in to build and deploy J2EE Applications Experience implementing Struts Model View Controller framework spring frameworks and Object Relational mapping ORM tools such as Hibernate Extensive knowledge on Core Java technologies such as MultiThreading Exception Handling Reflection Collections Streams File IO Strong experience in developing JAVAJ2EE applications using IDEs like Eclipse IntelliJ and Web servers like JBoss Tomcat WebLogic Strong working experience on mapping tools like Hibernate Hibernate Connection Pooling HQL Hibernate Caching Transactions Implemented Business Logic using POJOs and used Tomcat and WebLogic to deploy the applications Created Business Logic using Servlets Session beans and deployed them on WebLogic server Used MVC struts framework for application design Created complex SQL Queries PLSQL Stored procedures Functions for back end Prepared the Functional Design and Test case specifications Involved in writing Stored Procedures in Oracle to do some database side validations Performed unit testing system testing and integration testing Developed Unit Test Cases Used JUnit for unit testing of the application Provided Technical support for production environments resolving the issues analyzing the defects providing and implementing the solution defects Resolved more priority defects as per the schedule Environment J2EE JSP PLSQL HTML CSS Struts JUnit Used Eclipse IDE for all coding in Java Servlets and JSPs Coordinate with the QA lead for development of test plan test cases test code and actual testing responsible for defects allocation and ensuring that the defects are resolved Used Flex Styles and CSS to manage the Look and Feel of the application Deployed the application on Web Sphere Application server Environment Java 6 Eclipse Apache Tomcat Web Server JSP JavaScript AWT Servlets JDBC HTML Front Page 2000 Oracle CVS Education Bachelors Skills Eclipse Java Jboss Wsad Api Application server Design patterns Dtd Hbase Html Python Xml Xsl Xslt Database Db2 Mysql Oracle Oracle 10g Sql Additional Information Methodologies Agile UML Design Patterns Database Oracle 10g DB2 MySQL No Sql MongoDB Hbase Application Server Apache Tomcat 5x 60 Jboss 40 Web Tools HTML Java Script XML DTD Schemas XSL XSLT XPath DOM XQuery Tools SQL developer DB visualize Hortonworks IDE Testing Tools NetBeans Eclipse WSAD RAD Mat lab Operating System Windows Linux Scripts Bash Python ANT Testing API JUNIT",
    "unique_id": "f80395b9-d5d4-443a-9520-0b5b22dacb00"
}