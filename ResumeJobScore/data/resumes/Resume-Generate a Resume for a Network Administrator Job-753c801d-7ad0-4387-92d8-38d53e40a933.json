{
    "clean_data": "Sr Hadoop Developer Sr Hadoop span lDeveloperspan Sr Hadoop Developer Genuent LLC Philadelphia PA Having 8 years of overall IT experience of comprehensive experience as an Apache Hadoop Developer Expertise in writing Hadoop Jobs for analyzing structured and unstructured data using HDFS Hive HBase Pig Spark Kafka Scala Oozie and Talend ETL Good knowledge of Hadoop Architecture and various components such as HDFS Job Tracker Task Tracker Name Node Data Node YARN and MapReduce concepts Experience in working with different kind of MapReduce programs using Hadoop for working with Big Data analysis Experience in analyzing data using Hive QL Pig Latin and custom MapReduce programs in Java Experience in importingexporting data using Sqoop into HDFS from Relational Database Systems and viceversa Extensive knowledge and experience on real time data streaming techniques like Kafka Storm and Spark Streaming Working experience on designing and implementing complete endtoend Hadoop Infrastructure including PIG HIVE Sqoop Oozie Flume and zookeeper Good Knowledge in providing support to data analyst in running Pig and Hive queries Experience in writing shell scripts to dump the shared data from MySQL servers to HDFS Experience in designing both time driven and data driven automated workflows using Oozie Knowledge in performance tuning the Hadoop cluster by gathering and analyzing the existing infrastructure Experience in working with various Cloudera distributions CDH4CDH5 Hortonworks and Amazon EMR Hadoop Distributions Extensively worked on Hive and Sqoop for sourcing and transformations Experience in automating the Hadoop Installation configuration and maintaining the cluster by using the tools like Puppet Hands on experience knowledge in NoSQL databases like HBase Cassandra Mongo db Experience in working with flume to load the log data from multiple sources directly into HDFS Strong debugging and problem solving skills with excellent understanding of system development methodologies techniques and tools Wrote Flume configuration files for importing streaming log data into HBase with Flume Processing this data using Spark Streaming API with Scala Worked in complete Software Development Life Cycle analysis design development testing implementation and support in different application domain involving different technologies varying from object oriented technology to Internet programming on Windows NT Linux and UNIX Solaris platforms and RUP methodologies Familiar with RDBMS concepts and worked on Oracle 8i9i SQL Server 70 DB2 8x7x Involved in writing shell scripts Ant scripts for Unix OS for application deployments to production region Used Spark API over Hortonworks Hadoop YARN to perform analytics on data in Hive Experience in Hadoop Distributions like Cloudera Hortonworks Big Insights MapR Windows Azure and Impala Hands on experience in developing the applications with Java J2EE J2EE Servlets JSP EJB SOAP Web Services JNDI JMS JDBC2 Hibernate Struts Spring XML HTML XSD XSLT PLSQL Oracle10g and MSSQL Server RDBMS Having very good POC and Development experience on Apache Flume Kafka Spark Storm and Scala Good understanding in using data ingestion tools such as Kafka Sqoop and Flume Good working knowledge on Hadoop hue ecosystems Good knowledge in evaluating big data analytics libraries and use of SparkSQL for data exploratory Exceptional ability to quickly master new concepts and capable of working ingroup as well as independently with excellent communication skills Work Experience Sr Hadoop Developer Genuent LLC Houston TX October 2017 to Present The Northwestern Mutual is a financial services mutual organization Its products include life insurance longterm care insurance disability insurance annuities mutual funds stocks bonds and employee benefit services Responsibilities Good at working on Hadoop MapReduce and  developed multiple MapReduce jobs for structured semistructured and unstructured data in java Involved in Configuring Hadoop cluster and load balancing across the nodes Developed MapReduce programs in Java for parsing the raw data and populating staging Tables Created Hive queries to compare the raw data with EDW reference tables and performing aggregates Experienced in developing custom input formats and data types to parse and process unstructured and semi structured input data and mapped them into key value pairs to implement business logic in MapReduce Experience in implementing custom sterilizer interceptor source and sink as per the requirement in Flume to ingest data from multiple sources Experience in setting up Fanout workflow in flume to design v shaped architecture to take data from many sources and ingest into single sink Analyzing the requirement to setup a cluster Importing and exporting data into HDFS and Hive using Sqoop Experienced in analyzing data with Hive and Pig Experienced knowledge over designing Restful services using java based APIs like JERSEY Developed Pig Latin scripts to extract the data from the web server output files to load into HDFS Integrating bulk data into Cassandra file system using MapReduce programs Got good experience with NoSQL databases HBase Cassandra Involved in HBase setup and storing data into HBase which will be used for further analysis Expertise in designing data modelling for Cassandra NoSQL database Experienced in managing and reviewing Hadoop log files Experienced in defining job flows using Oozie workflow Involved in working with Spark on top of  for interactive and Batch Analysis Worked closely with AWS EC2 infrastructure teams to troubleshoot complex issues Expertise in writing the Scala code using higher order functions for the iterative algorithms in spark for performance consideration Experienced in analyzing and Optimizing RDDs by controlling partitions for the given data Good understanding on DAG cycle for entire spark application flow on Spark application WebUI Experienced in writing live Realtime Processing using Spark Streaming with Kafka Developed custom mappers in python script and Hive UDFs and UDAFs based on the given requirement Used HiveQL to analyze the partitioned and bucketed data and compute various metrics for reporting Experienced in querying data using SparkSQL on top of Spark engine Experience in managing and monitoring Hadoop cluster using Cloudera Manager Supported in setting up QA environment and updating configurations for implementing scripts with Pig Hive and Sqoop Environment CDH JavaJDK17 Hadoop MapReduce HDFS Hive Sqoop Flume HBase Cassandra Pig Oozie Kerberos Scala Spark SparkSQL Spark Streaming Kafka Linux AWS Shell Scripting MySQL Oracle 11g PLSQL SQLPLUS Hadoop Developer Saint Lukes Health System Kansas City MO September 2015 to October 2017 Saint Lukes Health System includes 10 hospitals and campuses across the Kansas City region home care and hospice behavioural health care dozens of physician practices a life care senior living community and more Responsibilities Installed and configured Hadoop Mapreduce HDFS Developed multiple MapReduce jobs in java for data cleaning and preprocessing Supported Map Reduce Programs those are running on the cluster Involved in loading data from UNIX file system to HDFS Involved in creating Hive tables loading with data and writing hive queries which will run internally in map reduce way Develop MapReduce jobs for the users Maintain update and schedule the periodic jobs which range from updates on periodic MapReduce jobs to creating adhoc jobs for the business users Importing and exporting data into HDFS and Hive using Sqoop Experienced in defining job flows Experienced in managing and reviewing Hadoop log files Extracted files from Couch DB through Sqoop and placed in HDFS and processed Experienced in running Hadoop streaming jobs to process terabytes of xml format data Load and transform large sets of structured semi structured and unstructured data Responsible to manage data coming from different sources Got good experience with NOSQL database Developed a custom File System plug in for Hadoop so it can access files on Data Platform This plugin allows Hadoop MapReduce programs HBase Pig and Hive to work unmodified and access files directly Designed and implemented Mapreducebased largescale parallel relationlearning system Extracted feeds form social media sites such as Facebook Twitter using Python scripts Setup and benchmarked HadoopHBase clusters for internal use Gained very good business knowledge on health insurance claim processing fraud suspect identification appeals process etc Involved in review of functional and nonfunctional requirements Facilitated knowledge transfer sessions Environment Java Eclipse Oracle Sub Version Hadoop Hive HBase Linux MapReduce HDFS Hive Java JDK Hadoop Distribution of Horton Works Cloudera MapReduce DataStax IBM DataStage Oracle PLSQL SQLPLUS UNIX Shell Scripting Hadoop Developer Utica National Insurance Group Utica NY August 2013 to September 2015 Utica National Insurance Group is a Top 100 nationally recognized insurer providing personal and commercial insurance products and services with the secondlargest errors and omissions business in the United States Utica National sells its products through more than 2200 independent insurance agents and employs over 1200 people countrywide Responsibilities Worked on Spark and Cassandra for the User behavior analysis and lightning speed execution Installed and configured Hadoop MapReduce HDFS and developed multiple MapReduce jobs in Java for data cleansing and preprocessing Importing and exporting data into HDFS and Hive using Sqoop Used UDFs to implement business logic in Hadoop Extracted files from Oracle and DB2through Sqoop and placed in HDFS and processed Load and transform large sets of structured semi structured and unstructured data Responsible to manage data coming from different sources Supported MapReduce Programs those are running on the cluster Involved in loading data from UNIX file system to HDFS Involved in creating Hive tables loading with data and writing hive queries which will run internally in map reduce way Developed mapping parameters and variables to support SQL override Used existing ETL standards to develop these mappings Extracted the data from the flat files and other RDBMS databases into staging area and populated onto Data warehouse Worked on JVM performance tuning to improve MapReduce jobs performance Environment Hadoop MapReduce HDFS Hive Oracle Java Struts Servlets HTML XML SQL J2EE JUnit Tomcat Java Developer AOP and Spring Security Chicago IL July 2011 to August 2013 Reva has a history of success in reselling architecting designing developing implementing and supporting solutions across multiplatforms With partnerships across Tier1 platforms in enterprise content management and mobility Reva is known for delivering ontime and onbudget Responsibilities Used Spring framework for implementing IOCJDBCORM AOP and Spring Security to implement business layer Developed and Consumed Web services securely using JAXWS API and tested using SOAP UI Extensively used Action Dispatch Action Action Forms Struts Tag libraries Struts Configuration from Struts Extensively used the Hibernate Query Language for data retrieval from the database and process the data in the business methods Developed pages using JSP JSTL Spring tags JQuery Java Script Used JQuery to make AJAX calls Used Jenkins continuous integration tool to do the deployments Worked on JDBC for database connections Worked on multithreaded middleware using socket programming to introduce whole set of new business rules implementing OOPS design and principles Involved in implementing Java multithreading concepts Developed several REST web services supporting both XML and JSON to perform task such as demand response management Used Servlet Java and Spring for server side business logic Implemented the log functionality by using Log4j and internal logging APIs Used Junit for server side testing Used Maven build tools and SVN for version control Developed frontend of application using Bootstrap AngularJS and NodeJS frameworks Implemented SOA architecture using Enterprise Service Bus ESB Designed frontend data driven GUI using JSF HTML4 JavaScript and CSS Used IBM MQ Series as the JMS provider Responsible for writing SQL Queries and Procedures using DB2 Connection with Oracle MySQL Database is implemented using Hibernate ORM Configured hibernate entities using annotations from scratch Environment Core Java EJB Hibernate AWS JSF Struts Spring JPA REST JBoss DB2 Oracle XML JUnit HTML4 CSS JavaScript Apache Tomcat 5x Log4j Java Developer Oracle Corporation Santa Clara CA June 2009 to July 2011 GTL a Global Group Enterprise is a leading Infrastructure Services company focused on telecom and Power In the telecom segment the company provides Network Services to Telecom Operators OEMs and Tower Companies In the power sector the company offers EPC services Distribution Franchisee and Smart Grid solutions to Utilities and distribution companies Responsibilities Used Struts framework to generate Forms and actions for validating the user request data Developed Server side validation checks using Struts validators and Java Script validations With JSPs and Struts custom tags developed and implemented validations of data Developed applications which access the database with JDBC to execute queries prepared statements and procedures Developed programs to manipulate the data and perform CRUD operations on request to the database Used message driven beans for asynchronous processing alerts to the customer Worked on developing Use Cases Class Diagrams Sequence diagrams and Data Models Developed and Deployed SOAP Based Web Services on Tomcat Server Coding of SQL PLSQL and Views using IBMDB2 for the database Working on issues while converting JAVA to AJAX Supported in developing business tier using the stateless session bean Extensively used JDBC to access the database objects Using Clear case for source code control and JUNIT testing tool for unit testing Reviewing the code and perform integrated module testing Environment Java J2EE AJAX Struts Web Services SOAP HTML XML JSP JDBC ANT XML IBM Tomcat JUNIT DB2 Rational Rose Eclipse Helios CVS Education Bachelors Skills ORACLE 7 years SQL 9 years XML 8 years JAVA 9 years TOMCAT 6 years Additional Information Technical Skills BigData Technologies Hadoop MapReduce HDFS Hive Pig Zookeeper Sqoop Oozie Flume IMPALA HBASE Kafka Storm Big Data Frameworks HDFS YARN Spark Hadoop Distributions Cloudera CDH3 CDH4 CDH5 Horton works Amazon EMR EC2 Programming Languages Java shell scripting Scala Databases RDBMS MySQL Oracle Microsoft SQL Server Teradata DB2 PLSQL CASSANDRA MongoDB IDE and Tools Eclipse NetBeans Tableau Operating System Windows LinuxUnix Frameworks Spring Hibernate JSF EJB JMS Scripting Languages JSP Servlets JavaScript XML HTML Python Application Servers Apache Tomcat Web Sphere Web logic JBoss Methodologies Agile SDLC Waterfall Web Services Restful SOAP ETL Tools Talend Informatica Others Solr elastic search",
    "entities": [
        "Java Developer Oracle Corporation",
        "AJAX",
        "GUI",
        "Present The Northwestern Mutual",
        "Hortonworks Hadoop",
        "Flume Processing",
        "HDFS",
        "UNIX",
        "java Involved",
        "Additional Information Technical Skills BigData Technologies Hadoop MapReduce HDFS Hive Pig Zookeeper Sqoop Oozie Flume",
        "Utica National Insurance Group",
        "IBM",
        "UDAFs",
        "Realtime Processing using Spark Streaming",
        "Hadoop",
        "HDFS Involved",
        "SOAP",
        "XML",
        "NOSQL",
        "Kansas City",
        "HBase",
        "Amazon EMR EC2",
        "Sr Hadoop Developer Sr Hadoop",
        "TX",
        "Spark Streaming API",
        "File System",
        "Utica National",
        "Oozie Knowledge",
        "CDH3",
        "HBase Pig",
        "SQL Server",
        "SparkSQL",
        "Developed",
        "Network Services",
        "UNIX Solaris",
        "IBMDB2",
        "Hadoop MapReduce",
        "Hibernate ORM Configured",
        "RUP",
        "Restful",
        "Hadoop Distributions",
        "SQL Queries and Procedures",
        "Utilities",
        "Hadoop Mapreduce HDFS Developed",
        "Spring Security",
        "Action Dispatch Action Action Forms Struts Tag",
        "JSP",
        "the Hibernate Query Language",
        "Hive Experience",
        "Worked",
        "Oracle XML",
        "Distribution Franchisee",
        "Amazon EMR Hadoop",
        "Views",
        "Spark",
        "Hadoop Infrastructure",
        "Oracle Sub Version Hadoop",
        "Use Cases Class Diagrams Sequence",
        "Hadoop Jobs",
        "Sqoop",
        "QA",
        "Apache Hadoop Developer Expertise",
        "Houston",
        "Infrastructure Services",
        "Develop MapReduce",
        "AWS",
        "Hadoop MapReduce HDFS",
        "Hadoop Architecture",
        "Responsibilities Installed",
        "Implemented",
        "Oracle",
        "JSF",
        "PIG",
        "HadoopHBase",
        "Environment Hadoop MapReduce HDFS Hive Oracle",
        "HDFS Job Tracker Task Tracker",
        "java",
        "Struts Configuration",
        "SQL",
        "Facilitated",
        "Log4j",
        "HDFS Hive HBase",
        "Pig Hive and Sqoop Environment",
        "Flume",
        "Bootstrap",
        "Relational Database Systems",
        "MapReduce Experience",
        "Chicago",
        "the United States",
        "NetBeans Tableau",
        "Big Data",
        "Hive",
        "JUNIT",
        "HiveQL",
        "DAG",
        "Configuring Hadoop",
        "Supported MapReduce Programs",
        "ETL",
        "CRUD",
        "JAVA",
        "Oracle Microsoft",
        "Maven",
        "Tower Companies",
        "Data Models Developed",
        "Puppet Hands",
        "Tables Created Hive",
        "XSD",
        "JAXWS API",
        "SVN",
        "Smart Grid",
        "Expertise",
        "CSS",
        "Developed MapReduce",
        "Work Experience Sr Hadoop Developer Genuent",
        "EDW",
        "REST",
        "MapReduce",
        "Impala Hands",
        "AJAX Supported",
        "NoSQL",
        "Telecom Operators",
        "Software Development Life Cycle",
        "Node",
        "ANT XML",
        "Cloudera"
    ],
    "experience": "Experience in working with different kind of MapReduce programs using Hadoop for working with Big Data analysis Experience in analyzing data using Hive QL Pig Latin and custom MapReduce programs in Java Experience in importingexporting data using Sqoop into HDFS from Relational Database Systems and viceversa Extensive knowledge and experience on real time data streaming techniques like Kafka Storm and Spark Streaming Working experience on designing and implementing complete endtoend Hadoop Infrastructure including PIG HIVE Sqoop Oozie Flume and zookeeper Good Knowledge in providing support to data analyst in running Pig and Hive queries Experience in writing shell scripts to dump the shared data from MySQL servers to HDFS Experience in designing both time driven and data driven automated workflows using Oozie Knowledge in performance tuning the Hadoop cluster by gathering and analyzing the existing infrastructure Experience in working with various Cloudera distributions CDH4CDH5 Hortonworks and Amazon EMR Hadoop Distributions Extensively worked on Hive and Sqoop for sourcing and transformations Experience in automating the Hadoop Installation configuration and maintaining the cluster by using the tools like Puppet Hands on experience knowledge in NoSQL databases like HBase Cassandra Mongo db Experience in working with flume to load the log data from multiple sources directly into HDFS Strong debugging and problem solving skills with excellent understanding of system development methodologies techniques and tools Wrote Flume configuration files for importing streaming log data into HBase with Flume Processing this data using Spark Streaming API with Scala Worked in complete Software Development Life Cycle analysis design development testing implementation and support in different application domain involving different technologies varying from object oriented technology to Internet programming on Windows NT Linux and UNIX Solaris platforms and RUP methodologies Familiar with RDBMS concepts and worked on Oracle 8i9i SQL Server 70 DB2 8x7x Involved in writing shell scripts Ant scripts for Unix OS for application deployments to production region Used Spark API over Hortonworks Hadoop YARN to perform analytics on data in Hive Experience in Hadoop Distributions like Cloudera Hortonworks Big Insights MapR Windows Azure and Impala Hands on experience in developing the applications with Java J2EE J2EE Servlets JSP EJB SOAP Web Services JNDI JMS JDBC2 Hibernate Struts Spring XML HTML XSD XSLT PLSQL Oracle10 g and MSSQL Server RDBMS Having very good POC and Development experience on Apache Flume Kafka Spark Storm and Scala Good understanding in using data ingestion tools such as Kafka Sqoop and Flume Good working knowledge on Hadoop hue ecosystems Good knowledge in evaluating big data analytics libraries and use of SparkSQL for data exploratory Exceptional ability to quickly master new concepts and capable of working ingroup as well as independently with excellent communication skills Work Experience Sr Hadoop Developer Genuent LLC Houston TX October 2017 to Present The Northwestern Mutual is a financial services mutual organization Its products include life insurance longterm care insurance disability insurance annuities mutual funds stocks bonds and employee benefit services Responsibilities Good at working on Hadoop MapReduce and   developed multiple MapReduce jobs for structured semistructured and unstructured data in java Involved in Configuring Hadoop cluster and load balancing across the nodes Developed MapReduce programs in Java for parsing the raw data and populating staging Tables Created Hive queries to compare the raw data with EDW reference tables and performing aggregates Experienced in developing custom input formats and data types to parse and process unstructured and semi structured input data and mapped them into key value pairs to implement business logic in MapReduce Experience in implementing custom sterilizer interceptor source and sink as per the requirement in Flume to ingest data from multiple sources Experience in setting up Fanout workflow in flume to design v shaped architecture to take data from many sources and ingest into single sink Analyzing the requirement to setup a cluster Importing and exporting data into HDFS and Hive using Sqoop Experienced in analyzing data with Hive and Pig Experienced knowledge over designing Restful services using java based APIs like JERSEY Developed Pig Latin scripts to extract the data from the web server output files to load into HDFS Integrating bulk data into Cassandra file system using MapReduce programs Got good experience with NoSQL databases HBase Cassandra Involved in HBase setup and storing data into HBase which will be used for further analysis Expertise in designing data modelling for Cassandra NoSQL database Experienced in managing and reviewing Hadoop log files Experienced in defining job flows using Oozie workflow Involved in working with Spark on top of   for interactive and Batch Analysis Worked closely with AWS EC2 infrastructure teams to troubleshoot complex issues Expertise in writing the Scala code using higher order functions for the iterative algorithms in spark for performance consideration Experienced in analyzing and Optimizing RDDs by controlling partitions for the given data Good understanding on DAG cycle for entire spark application flow on Spark application WebUI Experienced in writing live Realtime Processing using Spark Streaming with Kafka Developed custom mappers in python script and Hive UDFs and UDAFs based on the given requirement Used HiveQL to analyze the partitioned and bucketed data and compute various metrics for reporting Experienced in querying data using SparkSQL on top of Spark engine Experience in managing and monitoring Hadoop cluster using Cloudera Manager Supported in setting up QA environment and updating configurations for implementing scripts with Pig Hive and Sqoop Environment CDH JavaJDK17 Hadoop MapReduce HDFS Hive Sqoop Flume HBase Cassandra Pig Oozie Kerberos Scala Spark SparkSQL Spark Streaming Kafka Linux AWS Shell Scripting MySQL Oracle 11 g PLSQL SQLPLUS Hadoop Developer Saint Lukes Health System Kansas City MO September 2015 to October 2017 Saint Lukes Health System includes 10 hospitals and campuses across the Kansas City region home care and hospice behavioural health care dozens of physician practices a life care senior living community and more Responsibilities Installed and configured Hadoop Mapreduce HDFS Developed multiple MapReduce jobs in java for data cleaning and preprocessing Supported Map Reduce Programs those are running on the cluster Involved in loading data from UNIX file system to HDFS Involved in creating Hive tables loading with data and writing hive queries which will run internally in map reduce way Develop MapReduce jobs for the users Maintain update and schedule the periodic jobs which range from updates on periodic MapReduce jobs to creating adhoc jobs for the business users Importing and exporting data into HDFS and Hive using Sqoop Experienced in defining job flows Experienced in managing and reviewing Hadoop log files Extracted files from Couch DB through Sqoop and placed in HDFS and processed Experienced in running Hadoop streaming jobs to process terabytes of xml format data Load and transform large sets of structured semi structured and unstructured data Responsible to manage data coming from different sources Got good experience with NOSQL database Developed a custom File System plug in for Hadoop so it can access files on Data Platform This plugin allows Hadoop MapReduce programs HBase Pig and Hive to work unmodified and access files directly Designed and implemented Mapreducebased largescale parallel relationlearning system Extracted feeds form social media sites such as Facebook Twitter using Python scripts Setup and benchmarked HadoopHBase clusters for internal use Gained very good business knowledge on health insurance claim processing fraud suspect identification appeals process etc Involved in review of functional and nonfunctional requirements Facilitated knowledge transfer sessions Environment Java Eclipse Oracle Sub Version Hadoop Hive HBase Linux MapReduce HDFS Hive Java JDK Hadoop Distribution of Horton Works Cloudera MapReduce DataStax IBM DataStage Oracle PLSQL SQLPLUS UNIX Shell Scripting Hadoop Developer Utica National Insurance Group Utica NY August 2013 to September 2015 Utica National Insurance Group is a Top 100 nationally recognized insurer providing personal and commercial insurance products and services with the secondlargest errors and omissions business in the United States Utica National sells its products through more than 2200 independent insurance agents and employs over 1200 people countrywide Responsibilities Worked on Spark and Cassandra for the User behavior analysis and lightning speed execution Installed and configured Hadoop MapReduce HDFS and developed multiple MapReduce jobs in Java for data cleansing and preprocessing Importing and exporting data into HDFS and Hive using Sqoop Used UDFs to implement business logic in Hadoop Extracted files from Oracle and DB2through Sqoop and placed in HDFS and processed Load and transform large sets of structured semi structured and unstructured data Responsible to manage data coming from different sources Supported MapReduce Programs those are running on the cluster Involved in loading data from UNIX file system to HDFS Involved in creating Hive tables loading with data and writing hive queries which will run internally in map reduce way Developed mapping parameters and variables to support SQL override Used existing ETL standards to develop these mappings Extracted the data from the flat files and other RDBMS databases into staging area and populated onto Data warehouse Worked on JVM performance tuning to improve MapReduce jobs performance Environment Hadoop MapReduce HDFS Hive Oracle Java Struts Servlets HTML XML SQL J2EE JUnit Tomcat Java Developer AOP and Spring Security Chicago IL July 2011 to August 2013 Reva has a history of success in reselling architecting designing developing implementing and supporting solutions across multiplatforms With partnerships across Tier1 platforms in enterprise content management and mobility Reva is known for delivering ontime and onbudget Responsibilities Used Spring framework for implementing IOCJDBCORM AOP and Spring Security to implement business layer Developed and Consumed Web services securely using JAXWS API and tested using SOAP UI Extensively used Action Dispatch Action Action Forms Struts Tag libraries Struts Configuration from Struts Extensively used the Hibernate Query Language for data retrieval from the database and process the data in the business methods Developed pages using JSP JSTL Spring tags JQuery Java Script Used JQuery to make AJAX calls Used Jenkins continuous integration tool to do the deployments Worked on JDBC for database connections Worked on multithreaded middleware using socket programming to introduce whole set of new business rules implementing OOPS design and principles Involved in implementing Java multithreading concepts Developed several REST web services supporting both XML and JSON to perform task such as demand response management Used Servlet Java and Spring for server side business logic Implemented the log functionality by using Log4j and internal logging APIs Used Junit for server side testing Used Maven build tools and SVN for version control Developed frontend of application using Bootstrap AngularJS and NodeJS frameworks Implemented SOA architecture using Enterprise Service Bus ESB Designed frontend data driven GUI using JSF HTML4 JavaScript and CSS Used IBM MQ Series as the JMS provider Responsible for writing SQL Queries and Procedures using DB2 Connection with Oracle MySQL Database is implemented using Hibernate ORM Configured hibernate entities using annotations from scratch Environment Core Java EJB Hibernate AWS JSF Struts Spring JPA REST JBoss DB2 Oracle XML JUnit HTML4 CSS JavaScript Apache Tomcat 5x Log4j Java Developer Oracle Corporation Santa Clara CA June 2009 to July 2011 GTL a Global Group Enterprise is a leading Infrastructure Services company focused on telecom and Power In the telecom segment the company provides Network Services to Telecom Operators OEMs and Tower Companies In the power sector the company offers EPC services Distribution Franchisee and Smart Grid solutions to Utilities and distribution companies Responsibilities Used Struts framework to generate Forms and actions for validating the user request data Developed Server side validation checks using Struts validators and Java Script validations With JSPs and Struts custom tags developed and implemented validations of data Developed applications which access the database with JDBC to execute queries prepared statements and procedures Developed programs to manipulate the data and perform CRUD operations on request to the database Used message driven beans for asynchronous processing alerts to the customer Worked on developing Use Cases Class Diagrams Sequence diagrams and Data Models Developed and Deployed SOAP Based Web Services on Tomcat Server Coding of SQL PLSQL and Views using IBMDB2 for the database Working on issues while converting JAVA to AJAX Supported in developing business tier using the stateless session bean Extensively used JDBC to access the database objects Using Clear case for source code control and JUNIT testing tool for unit testing Reviewing the code and perform integrated module testing Environment Java J2EE AJAX Struts Web Services SOAP HTML XML JSP JDBC ANT XML IBM Tomcat JUNIT DB2 Rational Rose Eclipse Helios CVS Education Bachelors Skills ORACLE 7 years SQL 9 years XML 8 years JAVA 9 years TOMCAT 6 years Additional Information Technical Skills BigData Technologies Hadoop MapReduce HDFS Hive Pig Zookeeper Sqoop Oozie Flume IMPALA HBASE Kafka Storm Big Data Frameworks HDFS YARN Spark Hadoop Distributions Cloudera CDH3 CDH4 CDH5 Horton works Amazon EMR EC2 Programming Languages Java shell scripting Scala Databases RDBMS MySQL Oracle Microsoft SQL Server Teradata DB2 PLSQL CASSANDRA MongoDB IDE and Tools Eclipse NetBeans Tableau Operating System Windows LinuxUnix Frameworks Spring Hibernate JSF EJB JMS Scripting Languages JSP Servlets JavaScript XML HTML Python Application Servers Apache Tomcat Web Sphere Web logic JBoss Methodologies Agile SDLC Waterfall Web Services Restful SOAP ETL Tools Talend Informatica Others Solr elastic search",
    "extracted_keywords": [
        "Sr",
        "Hadoop",
        "Developer",
        "Sr",
        "Hadoop",
        "span",
        "lDeveloperspan",
        "Sr",
        "Hadoop",
        "Developer",
        "Genuent",
        "LLC",
        "Philadelphia",
        "PA",
        "years",
        "IT",
        "experience",
        "experience",
        "Apache",
        "Hadoop",
        "Developer",
        "Expertise",
        "Hadoop",
        "Jobs",
        "data",
        "HDFS",
        "Hive",
        "HBase",
        "Pig",
        "Spark",
        "Kafka",
        "Scala",
        "Oozie",
        "Talend",
        "ETL",
        "knowledge",
        "Hadoop",
        "Architecture",
        "components",
        "HDFS",
        "Job",
        "Tracker",
        "Task",
        "Tracker",
        "Name",
        "Node",
        "Data",
        "Node",
        "YARN",
        "MapReduce",
        "concepts",
        "Experience",
        "kind",
        "MapReduce",
        "programs",
        "Hadoop",
        "Data",
        "analysis",
        "Experience",
        "data",
        "Hive",
        "QL",
        "Pig",
        "Latin",
        "MapReduce",
        "programs",
        "Java",
        "Experience",
        "data",
        "Sqoop",
        "HDFS",
        "Relational",
        "Database",
        "Systems",
        "viceversa",
        "knowledge",
        "experience",
        "time",
        "data",
        "techniques",
        "Kafka",
        "Storm",
        "Spark",
        "Streaming",
        "Working",
        "experience",
        "endtoend",
        "Hadoop",
        "Infrastructure",
        "PIG",
        "HIVE",
        "Sqoop",
        "Oozie",
        "Flume",
        "zookeeper",
        "Good",
        "Knowledge",
        "support",
        "data",
        "analyst",
        "Pig",
        "Hive",
        "Experience",
        "shell",
        "scripts",
        "data",
        "MySQL",
        "servers",
        "HDFS",
        "Experience",
        "time",
        "data",
        "workflows",
        "Oozie",
        "Knowledge",
        "performance",
        "Hadoop",
        "cluster",
        "infrastructure",
        "Experience",
        "Cloudera",
        "distributions",
        "CDH4CDH5",
        "Hortonworks",
        "Amazon",
        "EMR",
        "Hadoop",
        "Distributions",
        "Hive",
        "Sqoop",
        "transformations",
        "Hadoop",
        "Installation",
        "configuration",
        "cluster",
        "tools",
        "Puppet",
        "Hands",
        "experience",
        "knowledge",
        "NoSQL",
        "HBase",
        "Cassandra",
        "Mongo",
        "db",
        "Experience",
        "flume",
        "log",
        "data",
        "sources",
        "HDFS",
        "debugging",
        "problem",
        "skills",
        "understanding",
        "system",
        "development",
        "methodologies",
        "techniques",
        "tools",
        "Wrote",
        "Flume",
        "configuration",
        "files",
        "streaming",
        "log",
        "data",
        "HBase",
        "Flume",
        "Processing",
        "data",
        "Spark",
        "Streaming",
        "API",
        "Scala",
        "Software",
        "Development",
        "Life",
        "Cycle",
        "analysis",
        "design",
        "development",
        "testing",
        "implementation",
        "support",
        "application",
        "domain",
        "technologies",
        "object",
        "technology",
        "Internet",
        "programming",
        "Windows",
        "NT",
        "Linux",
        "UNIX",
        "Solaris",
        "platforms",
        "RUP",
        "methodologies",
        "concepts",
        "Oracle",
        "8i9i",
        "SQL",
        "Server",
        "DB2",
        "8x7x",
        "shell",
        "scripts",
        "Ant",
        "scripts",
        "Unix",
        "OS",
        "application",
        "deployments",
        "production",
        "region",
        "Spark",
        "API",
        "Hortonworks",
        "Hadoop",
        "YARN",
        "analytics",
        "data",
        "Hive",
        "Experience",
        "Hadoop",
        "Distributions",
        "Cloudera",
        "Hortonworks",
        "Big",
        "Insights",
        "MapR",
        "Azure",
        "Impala",
        "Hands",
        "experience",
        "applications",
        "Java",
        "J2EE",
        "J2EE",
        "Servlets",
        "JSP",
        "EJB",
        "SOAP",
        "Web",
        "Services",
        "JNDI",
        "JMS",
        "JDBC2",
        "Hibernate",
        "Struts",
        "Spring",
        "XML",
        "HTML",
        "XSD",
        "XSLT",
        "PLSQL",
        "Oracle10",
        "g",
        "MSSQL",
        "Server",
        "RDBMS",
        "POC",
        "Development",
        "experience",
        "Apache",
        "Flume",
        "Kafka",
        "Spark",
        "Storm",
        "Scala",
        "Good",
        "understanding",
        "data",
        "ingestion",
        "tools",
        "Kafka",
        "Sqoop",
        "Flume",
        "knowledge",
        "Hadoop",
        "hue",
        "knowledge",
        "data",
        "analytics",
        "libraries",
        "use",
        "SparkSQL",
        "data",
        "ability",
        "concepts",
        "ingroup",
        "communication",
        "skills",
        "Work",
        "Experience",
        "Sr",
        "Hadoop",
        "Developer",
        "Genuent",
        "LLC",
        "Houston",
        "TX",
        "October",
        "Present",
        "Northwestern",
        "Mutual",
        "services",
        "organization",
        "products",
        "life",
        "insurance",
        "longterm",
        "care",
        "insurance",
        "disability",
        "insurance",
        "annuities",
        "funds",
        "stocks",
        "bonds",
        "employee",
        "benefit",
        "services",
        "Responsibilities",
        "Hadoop",
        "MapReduce",
        "MapReduce",
        "jobs",
        "data",
        "java",
        "Configuring",
        "Hadoop",
        "cluster",
        "load",
        "nodes",
        "MapReduce",
        "programs",
        "Java",
        "data",
        "staging",
        "Tables",
        "Created",
        "Hive",
        "data",
        "EDW",
        "reference",
        "tables",
        "aggregates",
        "custom",
        "input",
        "formats",
        "data",
        "types",
        "process",
        "input",
        "data",
        "value",
        "pairs",
        "business",
        "logic",
        "MapReduce",
        "Experience",
        "custom",
        "sterilizer",
        "interceptor",
        "source",
        "sink",
        "requirement",
        "Flume",
        "data",
        "sources",
        "Experience",
        "Fanout",
        "flume",
        "architecture",
        "data",
        "sources",
        "sink",
        "requirement",
        "cluster",
        "Importing",
        "data",
        "HDFS",
        "Hive",
        "Sqoop",
        "Experienced",
        "data",
        "Hive",
        "Pig",
        "knowledge",
        "services",
        "APIs",
        "JERSEY",
        "Developed",
        "Pig",
        "Latin",
        "scripts",
        "data",
        "web",
        "server",
        "output",
        "files",
        "HDFS",
        "Integrating",
        "data",
        "Cassandra",
        "file",
        "system",
        "MapReduce",
        "programs",
        "experience",
        "HBase",
        "Cassandra",
        "HBase",
        "setup",
        "data",
        "HBase",
        "analysis",
        "Expertise",
        "data",
        "Cassandra",
        "NoSQL",
        "database",
        "Hadoop",
        "log",
        "files",
        "job",
        "flows",
        "Oozie",
        "workflow",
        "Spark",
        "top",
        "Batch",
        "Analysis",
        "AWS",
        "EC2",
        "infrastructure",
        "teams",
        "issues",
        "Expertise",
        "Scala",
        "code",
        "order",
        "functions",
        "algorithms",
        "spark",
        "performance",
        "consideration",
        "RDDs",
        "partitions",
        "data",
        "understanding",
        "DAG",
        "cycle",
        "spark",
        "application",
        "flow",
        "Spark",
        "application",
        "WebUI",
        "Realtime",
        "Processing",
        "Spark",
        "Streaming",
        "Kafka",
        "custom",
        "mappers",
        "python",
        "script",
        "Hive",
        "UDFs",
        "UDAFs",
        "requirement",
        "HiveQL",
        "data",
        "metrics",
        "data",
        "SparkSQL",
        "top",
        "Spark",
        "engine",
        "Experience",
        "Hadoop",
        "cluster",
        "Cloudera",
        "Manager",
        "QA",
        "environment",
        "configurations",
        "scripts",
        "Pig",
        "Hive",
        "Sqoop",
        "Environment",
        "CDH",
        "JavaJDK17",
        "Hadoop",
        "MapReduce",
        "HDFS",
        "Hive",
        "Sqoop",
        "Flume",
        "HBase",
        "Cassandra",
        "Pig",
        "Oozie",
        "Kerberos",
        "Scala",
        "Spark",
        "SparkSQL",
        "Spark",
        "Streaming",
        "Kafka",
        "Linux",
        "Shell",
        "Scripting",
        "MySQL",
        "Oracle",
        "g",
        "PLSQL",
        "SQLPLUS",
        "Hadoop",
        "Developer",
        "Saint",
        "Lukes",
        "Health",
        "System",
        "Kansas",
        "City",
        "MO",
        "September",
        "October",
        "Saint",
        "Lukes",
        "Health",
        "System",
        "hospitals",
        "campuses",
        "Kansas",
        "City",
        "region",
        "home",
        "care",
        "hospice",
        "health",
        "care",
        "dozens",
        "physician",
        "practices",
        "life",
        "care",
        "community",
        "Responsibilities",
        "Hadoop",
        "Mapreduce",
        "HDFS",
        "MapReduce",
        "jobs",
        "java",
        "data",
        "Map",
        "Programs",
        "cluster",
        "loading",
        "data",
        "UNIX",
        "file",
        "system",
        "HDFS",
        "Hive",
        "tables",
        "data",
        "hive",
        "queries",
        "map",
        "way",
        "Develop",
        "MapReduce",
        "jobs",
        "users",
        "update",
        "jobs",
        "updates",
        "MapReduce",
        "jobs",
        "jobs",
        "business",
        "users",
        "data",
        "HDFS",
        "Hive",
        "Sqoop",
        "Experienced",
        "job",
        "flows",
        "Hadoop",
        "log",
        "files",
        "Couch",
        "DB",
        "Sqoop",
        "HDFS",
        "Hadoop",
        "streaming",
        "jobs",
        "terabytes",
        "xml",
        "format",
        "data",
        "Load",
        "sets",
        "data",
        "data",
        "sources",
        "experience",
        "NOSQL",
        "database",
        "custom",
        "File",
        "System",
        "plug",
        "Hadoop",
        "files",
        "Data",
        "Platform",
        "plugin",
        "Hadoop",
        "MapReduce",
        "programs",
        "HBase",
        "Pig",
        "Hive",
        "access",
        "files",
        "Mapreducebased",
        "largescale",
        "system",
        "media",
        "sites",
        "Facebook",
        "Twitter",
        "Python",
        "scripts",
        "Setup",
        "HadoopHBase",
        "clusters",
        "use",
        "business",
        "knowledge",
        "health",
        "insurance",
        "claim",
        "processing",
        "fraud",
        "suspect",
        "identification",
        "appeals",
        "process",
        "review",
        "requirements",
        "knowledge",
        "transfer",
        "sessions",
        "Environment",
        "Java",
        "Eclipse",
        "Oracle",
        "Sub",
        "Version",
        "Hadoop",
        "Hive",
        "HBase",
        "Linux",
        "MapReduce",
        "HDFS",
        "Hive",
        "Java",
        "JDK",
        "Hadoop",
        "Distribution",
        "Horton",
        "Cloudera",
        "MapReduce",
        "DataStax",
        "IBM",
        "DataStage",
        "Oracle",
        "PLSQL",
        "SQLPLUS",
        "UNIX",
        "Shell",
        "Scripting",
        "Hadoop",
        "Developer",
        "Utica",
        "National",
        "Insurance",
        "Group",
        "Utica",
        "NY",
        "August",
        "September",
        "Utica",
        "National",
        "Insurance",
        "Group",
        "Top",
        "insurer",
        "insurance",
        "products",
        "services",
        "errors",
        "omissions",
        "business",
        "United",
        "States",
        "Utica",
        "National",
        "products",
        "insurance",
        "agents",
        "people",
        "countrywide",
        "Responsibilities",
        "Spark",
        "Cassandra",
        "User",
        "behavior",
        "analysis",
        "lightning",
        "speed",
        "execution",
        "Hadoop",
        "MapReduce",
        "HDFS",
        "MapReduce",
        "jobs",
        "Java",
        "data",
        "cleansing",
        "Importing",
        "data",
        "HDFS",
        "Hive",
        "Sqoop",
        "Used",
        "UDFs",
        "business",
        "logic",
        "Hadoop",
        "files",
        "Oracle",
        "DB2through",
        "Sqoop",
        "HDFS",
        "Load",
        "sets",
        "data",
        "data",
        "sources",
        "MapReduce",
        "Programs",
        "cluster",
        "loading",
        "data",
        "UNIX",
        "file",
        "system",
        "HDFS",
        "Hive",
        "tables",
        "data",
        "hive",
        "queries",
        "map",
        "way",
        "mapping",
        "parameters",
        "variables",
        "SQL",
        "override",
        "ETL",
        "standards",
        "mappings",
        "data",
        "files",
        "RDBMS",
        "databases",
        "staging",
        "area",
        "Data",
        "warehouse",
        "JVM",
        "performance",
        "MapReduce",
        "jobs",
        "performance",
        "Environment",
        "Hadoop",
        "MapReduce",
        "HDFS",
        "Hive",
        "Oracle",
        "Java",
        "Struts",
        "Servlets",
        "HTML",
        "XML",
        "SQL",
        "J2EE",
        "JUnit",
        "Tomcat",
        "Java",
        "Developer",
        "AOP",
        "Spring",
        "Security",
        "Chicago",
        "IL",
        "July",
        "August",
        "Reva",
        "history",
        "success",
        "solutions",
        "multiplatforms",
        "partnerships",
        "Tier1",
        "platforms",
        "enterprise",
        "content",
        "management",
        "mobility",
        "Reva",
        "ontime",
        "Responsibilities",
        "Spring",
        "framework",
        "IOCJDBCORM",
        "AOP",
        "Spring",
        "Security",
        "business",
        "layer",
        "Consumed",
        "Web",
        "services",
        "JAXWS",
        "API",
        "SOAP",
        "UI",
        "Action",
        "Dispatch",
        "Action",
        "Action",
        "Forms",
        "Struts",
        "Tag",
        "Struts",
        "Configuration",
        "Struts",
        "Hibernate",
        "Query",
        "Language",
        "data",
        "retrieval",
        "database",
        "data",
        "business",
        "methods",
        "pages",
        "JSP",
        "JSTL",
        "Spring",
        "JQuery",
        "Java",
        "Script",
        "JQuery",
        "AJAX",
        "calls",
        "Jenkins",
        "integration",
        "tool",
        "deployments",
        "JDBC",
        "database",
        "connections",
        "middleware",
        "socket",
        "programming",
        "set",
        "business",
        "rules",
        "design",
        "principles",
        "Java",
        "concepts",
        "REST",
        "web",
        "services",
        "XML",
        "JSON",
        "task",
        "demand",
        "response",
        "management",
        "Servlet",
        "Java",
        "Spring",
        "server",
        "side",
        "business",
        "logic",
        "log",
        "functionality",
        "Log4j",
        "APIs",
        "Junit",
        "server",
        "side",
        "testing",
        "Maven",
        "tools",
        "SVN",
        "version",
        "control",
        "frontend",
        "application",
        "Bootstrap",
        "AngularJS",
        "NodeJS",
        "frameworks",
        "SOA",
        "architecture",
        "Enterprise",
        "Service",
        "Bus",
        "ESB",
        "frontend",
        "data",
        "GUI",
        "JSF",
        "HTML4",
        "JavaScript",
        "CSS",
        "IBM",
        "MQ",
        "Series",
        "JMS",
        "provider",
        "SQL",
        "Queries",
        "Procedures",
        "DB2",
        "Connection",
        "Oracle",
        "MySQL",
        "Database",
        "Hibernate",
        "ORM",
        "hibernate",
        "entities",
        "annotations",
        "scratch",
        "Environment",
        "Core",
        "Java",
        "EJB",
        "Hibernate",
        "JSF",
        "Struts",
        "Spring",
        "JPA",
        "REST",
        "JBoss",
        "DB2",
        "Oracle",
        "XML",
        "JUnit",
        "HTML4",
        "CSS",
        "JavaScript",
        "Apache",
        "Tomcat",
        "Log4j",
        "Java",
        "Developer",
        "Oracle",
        "Corporation",
        "Santa",
        "Clara",
        "CA",
        "June",
        "July",
        "GTL",
        "Global",
        "Group",
        "Enterprise",
        "Infrastructure",
        "Services",
        "company",
        "telecom",
        "Power",
        "telecom",
        "segment",
        "company",
        "Network",
        "Services",
        "Telecom",
        "Operators",
        "OEMs",
        "Tower",
        "Companies",
        "power",
        "sector",
        "company",
        "EPC",
        "services",
        "Distribution",
        "Franchisee",
        "Smart",
        "Grid",
        "solutions",
        "Utilities",
        "distribution",
        "companies",
        "Responsibilities",
        "Struts",
        "framework",
        "Forms",
        "actions",
        "user",
        "request",
        "data",
        "Server",
        "side",
        "validation",
        "checks",
        "Struts",
        "validators",
        "Java",
        "Script",
        "JSPs",
        "Struts",
        "custom",
        "tags",
        "validations",
        "data",
        "applications",
        "database",
        "JDBC",
        "queries",
        "statements",
        "procedures",
        "programs",
        "data",
        "CRUD",
        "operations",
        "request",
        "database",
        "message",
        "beans",
        "processing",
        "alerts",
        "customer",
        "Use",
        "Cases",
        "Class",
        "Diagrams",
        "Sequence",
        "diagrams",
        "Data",
        "Models",
        "Deployed",
        "SOAP",
        "Based",
        "Web",
        "Services",
        "Tomcat",
        "Server",
        "Coding",
        "SQL",
        "PLSQL",
        "Views",
        "IBMDB2",
        "database",
        "issues",
        "AJAX",
        "business",
        "tier",
        "session",
        "bean",
        "JDBC",
        "database",
        "Clear",
        "case",
        "source",
        "code",
        "control",
        "JUNIT",
        "testing",
        "tool",
        "unit",
        "testing",
        "code",
        "module",
        "testing",
        "Environment",
        "Java",
        "J2EE",
        "AJAX",
        "Struts",
        "Web",
        "Services",
        "SOAP",
        "HTML",
        "XML",
        "JSP",
        "JDBC",
        "ANT",
        "XML",
        "IBM",
        "Tomcat",
        "JUNIT",
        "DB2",
        "Rational",
        "Rose",
        "Eclipse",
        "Helios",
        "CVS",
        "Education",
        "Bachelors",
        "Skills",
        "ORACLE",
        "years",
        "SQL",
        "years",
        "XML",
        "years",
        "years",
        "TOMCAT",
        "years",
        "Additional",
        "Information",
        "Technical",
        "Skills",
        "BigData",
        "Technologies",
        "Hadoop",
        "MapReduce",
        "HDFS",
        "Hive",
        "Pig",
        "Zookeeper",
        "Sqoop",
        "Oozie",
        "Flume",
        "IMPALA",
        "HBASE",
        "Kafka",
        "Storm",
        "Big",
        "Data",
        "Frameworks",
        "HDFS",
        "YARN",
        "Spark",
        "Hadoop",
        "Distributions",
        "Cloudera",
        "CDH3",
        "CDH4",
        "CDH5",
        "Horton",
        "Amazon",
        "EMR",
        "EC2",
        "Programming",
        "Languages",
        "Java",
        "shell",
        "Scala",
        "Databases",
        "RDBMS",
        "MySQL",
        "Oracle",
        "Microsoft",
        "SQL",
        "Server",
        "Teradata",
        "DB2",
        "PLSQL",
        "CASSANDRA",
        "MongoDB",
        "IDE",
        "Tools",
        "Eclipse",
        "NetBeans",
        "Tableau",
        "Operating",
        "System",
        "Windows",
        "LinuxUnix",
        "Frameworks",
        "Spring",
        "Hibernate",
        "JSF",
        "EJB",
        "JMS",
        "Scripting",
        "Languages",
        "JSP",
        "Servlets",
        "JavaScript",
        "XML",
        "HTML",
        "Python",
        "Application",
        "Servers",
        "Apache",
        "Tomcat",
        "Web",
        "Sphere",
        "Web",
        "logic",
        "JBoss",
        "Methodologies",
        "Agile",
        "SDLC",
        "Waterfall",
        "Web",
        "Services",
        "Restful",
        "SOAP",
        "ETL",
        "Tools",
        "Talend",
        "Informatica",
        "Others",
        "Solr",
        "search"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T20:32:46.035154",
    "resume_data": "Sr Hadoop Developer Sr Hadoop span lDeveloperspan Sr Hadoop Developer Genuent LLC Philadelphia PA Having 8 years of overall IT experience of comprehensive experience as an Apache Hadoop Developer Expertise in writing Hadoop Jobs for analyzing structured and unstructured data using HDFS Hive HBase Pig Spark Kafka Scala Oozie and Talend ETL Good knowledge of Hadoop Architecture and various components such as HDFS Job Tracker Task Tracker Name Node Data Node YARN and MapReduce concepts Experience in working with different kind of MapReduce programs using Hadoop for working with Big Data analysis Experience in analyzing data using Hive QL Pig Latin and custom MapReduce programs in Java Experience in importingexporting data using Sqoop into HDFS from Relational Database Systems and viceversa Extensive knowledge and experience on real time data streaming techniques like Kafka Storm and Spark Streaming Working experience on designing and implementing complete endtoend Hadoop Infrastructure including PIG HIVE Sqoop Oozie Flume and zookeeper Good Knowledge in providing support to data analyst in running Pig and Hive queries Experience in writing shell scripts to dump the shared data from MySQL servers to HDFS Experience in designing both time driven and data driven automated workflows using Oozie Knowledge in performance tuning the Hadoop cluster by gathering and analyzing the existing infrastructure Experience in working with various Cloudera distributions CDH4CDH5 Hortonworks and Amazon EMR Hadoop Distributions Extensively worked on Hive and Sqoop for sourcing and transformations Experience in automating the Hadoop Installation configuration and maintaining the cluster by using the tools like Puppet Hands on experience knowledge in NoSQL databases like HBase Cassandra Mongo db Experience in working with flume to load the log data from multiple sources directly into HDFS Strong debugging and problem solving skills with excellent understanding of system development methodologies techniques and tools Wrote Flume configuration files for importing streaming log data into HBase with Flume Processing this data using Spark Streaming API with Scala Worked in complete Software Development Life Cycle analysis design development testing implementation and support in different application domain involving different technologies varying from object oriented technology to Internet programming on Windows NT Linux and UNIX Solaris platforms and RUP methodologies Familiar with RDBMS concepts and worked on Oracle 8i9i SQL Server 70 DB2 8x7x Involved in writing shell scripts Ant scripts for Unix OS for application deployments to production region Used Spark API over Hortonworks Hadoop YARN to perform analytics on data in Hive Experience in Hadoop Distributions like Cloudera Hortonworks Big Insights MapR Windows Azure and Impala Hands on experience in developing the applications with Java J2EE J2EE Servlets JSP EJB SOAP Web Services JNDI JMS JDBC2 Hibernate Struts Spring XML HTML XSD XSLT PLSQL Oracle10g and MSSQL Server RDBMS Having very good POC and Development experience on Apache Flume Kafka Spark Storm and Scala Good understanding in using data ingestion tools such as Kafka Sqoop and Flume Good working knowledge on Hadoop hue ecosystems Good knowledge in evaluating big data analytics libraries and use of SparkSQL for data exploratory Exceptional ability to quickly master new concepts and capable of working ingroup as well as independently with excellent communication skills Work Experience Sr Hadoop Developer Genuent LLC Houston TX October 2017 to Present The Northwestern Mutual is a financial services mutual organization Its products include life insurance longterm care insurance disability insurance annuities mutual funds stocks bonds and employee benefit services Responsibilities Good at working on Hadoop MapReduce and YarnMRv2 developed multiple MapReduce jobs for structured semistructured and unstructured data in java Involved in Configuring Hadoop cluster and load balancing across the nodes Developed MapReduce programs in Java for parsing the raw data and populating staging Tables Created Hive queries to compare the raw data with EDW reference tables and performing aggregates Experienced in developing custom input formats and data types to parse and process unstructured and semi structured input data and mapped them into key value pairs to implement business logic in MapReduce Experience in implementing custom sterilizer interceptor source and sink as per the requirement in Flume to ingest data from multiple sources Experience in setting up Fanout workflow in flume to design v shaped architecture to take data from many sources and ingest into single sink Analyzing the requirement to setup a cluster Importing and exporting data into HDFS and Hive using Sqoop Experienced in analyzing data with Hive and Pig Experienced knowledge over designing Restful services using java based APIs like JERSEY Developed Pig Latin scripts to extract the data from the web server output files to load into HDFS Integrating bulk data into Cassandra file system using MapReduce programs Got good experience with NoSQL databases HBase Cassandra Involved in HBase setup and storing data into HBase which will be used for further analysis Expertise in designing data modelling for Cassandra NoSQL database Experienced in managing and reviewing Hadoop log files Experienced in defining job flows using Oozie workflow Involved in working with Spark on top of YarnMRv2 for interactive and Batch Analysis Worked closely with AWS EC2 infrastructure teams to troubleshoot complex issues Expertise in writing the Scala code using higher order functions for the iterative algorithms in spark for performance consideration Experienced in analyzing and Optimizing RDDs by controlling partitions for the given data Good understanding on DAG cycle for entire spark application flow on Spark application WebUI Experienced in writing live Realtime Processing using Spark Streaming with Kafka Developed custom mappers in python script and Hive UDFs and UDAFs based on the given requirement Used HiveQL to analyze the partitioned and bucketed data and compute various metrics for reporting Experienced in querying data using SparkSQL on top of Spark engine Experience in managing and monitoring Hadoop cluster using Cloudera Manager Supported in setting up QA environment and updating configurations for implementing scripts with Pig Hive and Sqoop Environment CDH JavaJDK17 Hadoop MapReduce HDFS Hive Sqoop Flume HBase Cassandra Pig Oozie Kerberos Scala Spark SparkSQL Spark Streaming Kafka Linux AWS Shell Scripting MySQL Oracle 11g PLSQL SQLPLUS Hadoop Developer Saint Lukes Health System Kansas City MO September 2015 to October 2017 Saint Lukes Health System includes 10 hospitals and campuses across the Kansas City region home care and hospice behavioural health care dozens of physician practices a life care senior living community and more Responsibilities Installed and configured Hadoop Mapreduce HDFS Developed multiple MapReduce jobs in java for data cleaning and preprocessing Supported Map Reduce Programs those are running on the cluster Involved in loading data from UNIX file system to HDFS Involved in creating Hive tables loading with data and writing hive queries which will run internally in map reduce way Develop MapReduce jobs for the users Maintain update and schedule the periodic jobs which range from updates on periodic MapReduce jobs to creating adhoc jobs for the business users Importing and exporting data into HDFS and Hive using Sqoop Experienced in defining job flows Experienced in managing and reviewing Hadoop log files Extracted files from Couch DB through Sqoop and placed in HDFS and processed Experienced in running Hadoop streaming jobs to process terabytes of xml format data Load and transform large sets of structured semi structured and unstructured data Responsible to manage data coming from different sources Got good experience with NOSQL database Developed a custom File System plug in for Hadoop so it can access files on Data Platform This plugin allows Hadoop MapReduce programs HBase Pig and Hive to work unmodified and access files directly Designed and implemented Mapreducebased largescale parallel relationlearning system Extracted feeds form social media sites such as Facebook Twitter using Python scripts Setup and benchmarked HadoopHBase clusters for internal use Gained very good business knowledge on health insurance claim processing fraud suspect identification appeals process etc Involved in review of functional and nonfunctional requirements Facilitated knowledge transfer sessions Environment Java Eclipse Oracle Sub Version Hadoop Hive HBase Linux MapReduce HDFS Hive Java JDK Hadoop Distribution of Horton Works Cloudera MapReduce DataStax IBM DataStage Oracle PLSQL SQLPLUS UNIX Shell Scripting Hadoop Developer Utica National Insurance Group Utica NY August 2013 to September 2015 Utica National Insurance Group is a Top 100 nationally recognized insurer providing personal and commercial insurance products and services with the secondlargest errors and omissions business in the United States Utica National sells its products through more than 2200 independent insurance agents and employs over 1200 people countrywide Responsibilities Worked on Spark and Cassandra for the User behavior analysis and lightning speed execution Installed and configured Hadoop MapReduce HDFS and developed multiple MapReduce jobs in Java for data cleansing and preprocessing Importing and exporting data into HDFS and Hive using Sqoop Used UDFs to implement business logic in Hadoop Extracted files from Oracle and DB2through Sqoop and placed in HDFS and processed Load and transform large sets of structured semi structured and unstructured data Responsible to manage data coming from different sources Supported MapReduce Programs those are running on the cluster Involved in loading data from UNIX file system to HDFS Involved in creating Hive tables loading with data and writing hive queries which will run internally in map reduce way Developed mapping parameters and variables to support SQL override Used existing ETL standards to develop these mappings Extracted the data from the flat files and other RDBMS databases into staging area and populated onto Data warehouse Worked on JVM performance tuning to improve MapReduce jobs performance Environment Hadoop MapReduce HDFS Hive Oracle Java Struts Servlets HTML XML SQL J2EE JUnit Tomcat Java Developer AOP and Spring Security Chicago IL July 2011 to August 2013 Reva has a history of success in reselling architecting designing developing implementing and supporting solutions across multiplatforms With partnerships across Tier1 platforms in enterprise content management and mobility Reva is known for delivering ontime and onbudget Responsibilities Used Spring framework for implementing IOCJDBCORM AOP and Spring Security to implement business layer Developed and Consumed Web services securely using JAXWS API and tested using SOAP UI Extensively used Action Dispatch Action Action Forms Struts Tag libraries Struts Configuration from Struts Extensively used the Hibernate Query Language for data retrieval from the database and process the data in the business methods Developed pages using JSP JSTL Spring tags JQuery Java Script Used JQuery to make AJAX calls Used Jenkins continuous integration tool to do the deployments Worked on JDBC for database connections Worked on multithreaded middleware using socket programming to introduce whole set of new business rules implementing OOPS design and principles Involved in implementing Java multithreading concepts Developed several REST web services supporting both XML and JSON to perform task such as demand response management Used Servlet Java and Spring for server side business logic Implemented the log functionality by using Log4j and internal logging APIs Used Junit for server side testing Used Maven build tools and SVN for version control Developed frontend of application using Bootstrap AngularJS and NodeJS frameworks Implemented SOA architecture using Enterprise Service Bus ESB Designed frontend data driven GUI using JSF HTML4 JavaScript and CSS Used IBM MQ Series as the JMS provider Responsible for writing SQL Queries and Procedures using DB2 Connection with Oracle MySQL Database is implemented using Hibernate ORM Configured hibernate entities using annotations from scratch Environment Core Java EJB Hibernate AWS JSF Struts Spring JPA REST JBoss DB2 Oracle XML JUnit HTML4 CSS JavaScript Apache Tomcat 5x Log4j Java Developer Oracle Corporation Santa Clara CA June 2009 to July 2011 GTL a Global Group Enterprise is a leading Infrastructure Services company focused on telecom and Power In the telecom segment the company provides Network Services to Telecom Operators OEMs and Tower Companies In the power sector the company offers EPC services Distribution Franchisee and Smart Grid solutions to Utilities and distribution companies Responsibilities Used Struts framework to generate Forms and actions for validating the user request data Developed Server side validation checks using Struts validators and Java Script validations With JSPs and Struts custom tags developed and implemented validations of data Developed applications which access the database with JDBC to execute queries prepared statements and procedures Developed programs to manipulate the data and perform CRUD operations on request to the database Used message driven beans for asynchronous processing alerts to the customer Worked on developing Use Cases Class Diagrams Sequence diagrams and Data Models Developed and Deployed SOAP Based Web Services on Tomcat Server Coding of SQL PLSQL and Views using IBMDB2 for the database Working on issues while converting JAVA to AJAX Supported in developing business tier using the stateless session bean Extensively used JDBC to access the database objects Using Clear case for source code control and JUNIT testing tool for unit testing Reviewing the code and perform integrated module testing Environment Java J2EE AJAX Struts Web Services SOAP HTML XML JSP JDBC ANT XML IBM Tomcat JUNIT DB2 Rational Rose Eclipse Helios CVS Education Bachelors Skills ORACLE 7 years SQL 9 years XML 8 years JAVA 9 years TOMCAT 6 years Additional Information Technical Skills BigData Technologies Hadoop MapReduce HDFS Hive Pig Zookeeper Sqoop Oozie Flume IMPALA HBASE Kafka Storm Big Data Frameworks HDFS YARN Spark Hadoop Distributions Cloudera CDH3 CDH4 CDH5 Horton works Amazon EMR EC2 Programming Languages Java shell scripting Scala Databases RDBMS MySQL Oracle Microsoft SQL Server Teradata DB2 PLSQL CASSANDRA MongoDB IDE and Tools Eclipse NetBeans Tableau Operating System Windows LinuxUnix Frameworks Spring Hibernate JSF EJB JMS Scripting Languages JSP Servlets JavaScript XML HTML Python Application Servers Apache Tomcat Web Sphere Web logic JBoss Methodologies Agile SDLC Waterfall Web Services Restful SOAP ETL Tools Talend Informatica Others Solr elastic search",
    "unique_id": "753c801d-7ad0-4387-92d8-38d53e40a933"
}