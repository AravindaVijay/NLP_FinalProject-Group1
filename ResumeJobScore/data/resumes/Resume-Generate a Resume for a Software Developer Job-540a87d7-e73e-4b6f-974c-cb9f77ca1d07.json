{
    "clean_data": "Sr Hadoop Developer Sr Hadoop span lDeveloperspan Sr Hadoop Developer Mattel Inc El Segando CA Overall 8 years of overall experience with strong emphasis on Design Development Implementation Testing and Deployment of Software Applications Over 4 years of comprehensive IT experience in BigData and BigData Analytics Hadoop HDFS MapReduce YARN Hadoop Ecosystem and Shell Scripting 5 years of development experience using Java J2EE JSP and Servlets Highly capable for processing large sets of Structured Semistructured and Unstructured datasets and supporting BigData applications Hands on experience with Hadoop Ecosystem components like Map Reduce Processing HDFS Storage YARN Sqoop Pig Hive HBase Oozie ZooKeeper and Spark for data storage and analysis Expertise in transferring data between a Hadoop ecosystem and structured data storage in a RDBMS such as MY SQL Oracle Teradata and DB2 using Sqoop Experience in NoSQL databases like Mongo DB HBase and Cassandra Experience in Apache Spark cluster and streams processing using Spark Streaming Expertise in moving large amounts of log streaming event data and Transactional data using Flume Experience in developing MapReduce jobs in Java for data cleaning and preprocessing Expertise in writing Pig Latin Hive Scripts and extended their functionality using User Defined Functions UDFs Expertise in handling arrangement of data within certain limits Data Layouts using Partitions and Bucketing in Hive Expertise in preparing Interactive Data Visualizations using Tableau Software from different sources Hands on experience in developing workflows execute MapReduce Sqoop Pig Hive and Shell Scripts using Oozie Experience working with Cloudera Hue Interface and Impala Hands on experience developing Solr Indexes using MapReduce Indexer Tool Expertise in ObjectOriented Analysis and Design OOAD like UML and use of various design patterns Experience in Java JSP Servlets EJB Web Logic Web Sphere Hibernate Spring JBoss JDBC RMI Java Script Ajax JQuery XML and HTML Fluent with the core Java concepts like IO MultiThreading Exceptions Reg Ex Data Structures and Serialization Performed Unit Testing using Junit Testing Framework and Log4J to monitor the error logs Experience in process Improvement NormalizationDenormalization Data extraction cleansing and Manipulation Converting requirement specification Source system understanding into Conceptual Logical and Physical Data Model Data flow DFD Expertise in working with Transactional Databases like Oracle SQL server My SQL and Db2 Expertise in developing SQL queries Stored Procedures and excellent development experience with Agile Methodology Ability to adapt to evolving technology Strong sense of Responsibility and Accomplishment Excellent leadership interpersonal problem solving and time management skills Excellent communication skills both Written documentation and Verbal presentation Authorized to work in the US for any employer Work Experience Sr Hadoop Developer Mattel Inc October 2018 to Present Description FFM Federally Facilitated MarketplaceHealthcare Health Partners like other healthcare organizations has a legacy system clinicians and researchers that needed access to the data The data types include EMR generated data genomic data financial data patient and caregiver data incremental physiological monitoring ventilator data temperature and humidity data Any electronically generated data in the healthcare environment can be ingested and stored in the hdfs which will be used for analytic to aid in the delivery of quality care at the lowest possible cost and an environment to enable clinical researchers to examine healthcare data Key Achievements Performed performance tuning and troubleshooting of MapReduce jobs by analyzing and reviewing Hadoop log files Involved Low level design for MR Hive Impala Shell scripts to process data Involved in complete Big Data flow of the application starting from data ingestion upstream to HDFS processing the data in HDFS and analyzing the data Knowledge on handling Hive queries using Spark SQL that integrate with Spark environment implemented in Scala Used Spark Streaming API with Kafka to build live dashboards Worked on Transformations actions in RDD Spark Streaming Pair RDD Operations Checkpointing and SBT Implemented POC to migrate map reduce jobs into Spark RDD transformation using Scala IDE for Eclipse Creating Hive tables to import large data sets from various relational databases using Sqoop and export the analyzed data back for visualization and report generation by the BI team Installing and configuring Hive Sqoop Flume Oozie on the Hadoop clusters Involved in scheduling Oozie workflow engine to run multiple Hive and Pig jobs Developed a process for the Batch ingestion of CSV Files Sqoop from different sources and also generating views on the data source using Shell Scripting and Python Integrated a shell script to create Collectionsmorphline SolrIndexes on top of table directories using MapReduce Indexer Tool within Batch Ingestion Framework Implemented partitioning dynamic partitions and buckets in HIVE Developed Hive Scripts to create the views and apply transformation logic in the Target Database Involved in the design of Data Mart and Data Lake to provide faster insight into the Data Involved in using Stream Sets Data Collector tool and created Data Flows for one of the streaming application Experienced in using Kafka as a data pipeline between JMS Producer and Spark Streaming Application Consumer Involved in the development of Spark Streaming application for one of the data source using Scala Spark by applying the transformations Developed a script in Scala to read all the Parquet Tables in a Database and parse them as Json files another script to parse them as structured tables in Hive Designed and Maintained Oozie workflows to manage the flow of jobs in the cluster Configured Zookeeper for Cluster coordination services Developed a unit test script to read a Parquet file for testing PySpark on the cluster Involved in exploration of new technologies like AWS Apache Flink and Apache NIFIetc which can increase the business value Environment Hadoop HDFS Map Reduce Hive HBase Zookeeper Impala Javajdk16 Cloudera Oracle SQL Server UNIX Shell Scripting Flume Oozie Scala Spark Sqoop Python kafka PySpark Sr Hadoop Developer Intel Santa Clara CA January 2018 to September 2018 Description The purpose of the project is to perform the analysis on the Effectiveness and validity of controls and to store terabytes of log information generated by the source providers as part of the analysis and extract meaningful information out of it The solution is based on the open source Big Data software Hadoop The data will be stored in Hadoop file system and processed using Map Reduce jobs which intern includes getting the raw data process the data to obtain controls and redesignchange history information extract various reports out of the controls history and Export the information for further processing Key Achievements Responsible for Writing MapReduce jobs to perform operations like copying data on HDFS and defining job flows on EC2 server load and transform large sets of structured semistructured and unstructured data Developed a process for Sqooping data from multiple sources like SQL Server Oracle and Teradata Responsible for creation of mapping document from source fields to destination fields mapping Developed a shell script to create staging landing tables with the same schema like the source and generate the properties which are used by Oozie jobs Developed Oozie workflows for executing Sqoop and Hive actions Worked with NoSQL databases like Hbase in creating Hbase tables to load large sets of semi structured data coming from various sources Performance optimizations on SparkScala Diagnose and resolve performance issues Responsible for developing Python wrapper scripts which will extract specific date range using Sqoop by passing custom properties required for the workflow Developed scripts to run Oozie workflows capture the logs of all jobs that run on cluster and create a metadata table which specifies the execution times of each job Developed Hive scripts for performing transformation logic and also loading the data from staging zone to final landing zone Worked on Parquet File format to get a better storage and performance for publish tables Involved in loading transactional data into HDFS using Flume for Fraud Analytics Developed Python utility to validate HDFS tables with source tables Designed and developed UDFS to extend the functionality in both PIG and HIVE Import and Export of data using Sqoop between MySQL to HDFS on regular basis Responsible for developing multiple Kafka Producers and Consumers from scratch as per the software requirement specifications Involved in using CA7 tool to setup dependencies at each level Table Data File and Time Automated all the jobs for pulling data from FTP server to load data into Hive tables using Oozie workflows Involved in developing Spark code using Scala and  for faster testing and processing of data and exploring of optimizing it using Spark Context  Pair RDDs Spark YARN Migrating the needed data from Oracle MySQL in to HDFS using Sqoop and importing various formats of flat files in to HDFS Environment Hadoop HDFS Map Reduce Hive HBase Kafka Zookeeper Oozie Impala Javajdk16 Cloudera Oracle Teradata SQL Server UNIX Shell Scripting Flume Scala Spark Sqoop Python Sr Hadoop Developer Health partners Bloomington MN January 2016 to December 2017 Description The purpose of this project is to acquire Core metrics data export and process in Hadoop The Core metrics data files to be loaded in to Hadoop data warehouse daily to enable business reporting and outbound feeds We use Hadoop to store vast volumes of unstructured data allows the company to collect web logs transaction data and social media data Key Achievements Responsible for Managing Analyzing and Transforming petabytes of data and also quick validation check on FTP file arrival from S3 Bucket to HDFS Responsible for analyzing large data sets and derive customer usage patterns by developing new MapReduce programs Experienced in creation of Hive tables and loading data incrementally into the tables using Dynamic Partitioning and Worked on Avro Files JSON Records Experienced in using Pig for data cleansing and developed Pig Latin scripts to extract the data from web server output files to load into HDFS Worked on Hive by creating external and internal tables loading it with data and writing Hive queries Involved in development and usage of UDTFs and UDAFs for decoding Log Record Fields and Conversions Generating Minute Buckets for the specified Time Intervals and JSON Field Extractor Developed Pig and Hive UDFs to analyze the complex data to find specific user behavior Responsible for Debug Optimization of Hive Scripts and also implementing Deduplication Logic in Hive using a Rank Key Function UDF Experienced in writing Hive Validation Scripts which are used in validation framework for daily analysis through graphs and presented to business users Developed workflow in Oozie to automate the tasks of loading data into HDFS and preprocessing with Pig and Hive Involved for Cassandra Database Schema design Using BULK LOAD Utility data pushed to Cassandra databases Responsible for creating Dashboards on Tableau Server Generated reports for hive tables in different scenarios using Tableau Responsible for Scheduling using Active Batchjobs and Cron jobs Experienced in Jar builds that can be triggered by commits to Github using Jenkins Exploring new tools for data tagging like Tealium POC Report Actively updated the upper management with daily updates on the progress of project that include the classification levels that were achieved on the data Environment Hadoop Map Reduce HDFS Pig Hive HBase Zookeeper Oozie Impala Cassandra Javajdk16 Cloudera Oracle 11g10g Windows NTUNIX Shell Scripting Tableau Tealium Sr Java Developer Well Point INC Cincinnati OH January 2013 to December 2015 Description Well Point Health Insurance portal provides ability for end customersusers to request a quote provide quote details purchase health insurance and payment of insurance amount This platform is built using a Struts framework in the presentation layer The back end functionality is built using Oracle as data persistence layer and Hibernate as data access layer Key Achievements Responsible for understanding the scope of the project and requirements gathering Used MapReduce to Index the large amount of data to easily access specific records Supported MapReduce Programs which are running on the cluster Developed MapReduce programs to perform data filtering for unstructured data Designed the application by implementing Struts Framework based on MVC Architecture Designed and developed the front end using JSP HTML and JavaScript and JQuery Developed framework for data processing using Design patterns Java XML Implemented J2EE standards MVC2 architecture using Struts Framework Implementing Servlets JSP and Ajax to design the user interface Used JSP Java Script HTML5 and CSS for manipulating validating customizing error messages to the User Interface Used the light weight container of the Spring Framework to provide architectural flexibility for Inversion of Controller IOC Used SpringIOC for dependency injection to Hibernate and Spring Frameworks Designed and developed Session beans to implement the Business logic Developed EJB components that are deployed on Web logic Application Server Written unit tests using Junit Framework and Logging is done using Log4J Framework Used Html CSS JavaScript and JQuery to develop front end pages Designed and developed various configuration files for Hibernate mappings Designed and Developed SQL queries and Stored Procedures Used XML XSLT XPATH to extract data from Web Services output XML Extensively used JavaScript JQuery and AJAX for clientside validation Used ANT scripts to fetch build and deploy application to development environment Developed Web Services for sending and getting data from different applications using SOAP messages Actively involved in code reviews and bug fixing Applied CSS Cascading style Sheets for entire site for standardization of the site Offshore coordination and User acceptance testing support Environment Java 50 Struts Spring 20 Hibernate 32 WebLogic 70 Eclipse 33 Oracle 10g Junit 42Maven Windows XPJ2EE JSP JDBC Hibernate spring HTML XMLCSS JavaScript and JQuery Software Programmer Blue Pal Solutions Pvt Ltd April 2011 to December 2012 Description Created a home page for the use of internal employees This home page enables employees to punch in and punch out so the activity tracking report is generated beginning of the week for past week and sent to employees managers Description Key Achievements Involved in the analysis design of the application using Rational Rose Developed the various action classes to handle the requests and responses Designed and created Java Objects JSP pages JSF JavaBeans and Servlets to achieve various business functionalities and created validation methods using JavaScript and Backing Beans Involved in writing client side validations using JavaScript CSS Involved in the design of the Referential Data Service module to interface with various databases using JDBC Used Hibernate framework to persist the employee work hours to the database Developed classes and interface with underlying web services layer Prepared documentation and participated in preparing users manual for the application Prepared Use Cases Business Process Models and Data flow diagrams User Interface models Gathered analyzed requirements for EAuto designed process flow diagrams Defined business processes related to the project and provided technical direction to development workgroup Analyzed the legacy and the Financial Data Warehouse Participated in Data base design sessions Database normalization meetings Managed Change Request Management and Defect Management Managed UAT testing and developed test strategies test plans reviewed QA test plans for appropriate test coverage Involved in Developing JSPs action classes form beans response beans EJBs Extensively used XML to code configuration files Developed PLSQL stored procedures triggers Performed functional integration system and validation testing Environment Java J2EE JSP JCL DB2 Struts SQL PLDSQL Eclipse Oracle Windows XP HTML CSS JavaScript and XML Education Bachelors in Computers Science in Computers Science JNTU Skills APACHE HADOOP HDFS 3 years APACHE HADOOP IMPALA 3 years APACHE HADOOP MAPREDUCE 6 years APACHE HADOOP OOZIE 3 years APACHE HBASE 3 years databases 5 years Hadoop 3 years HADOOP DISTRIBUTED FILE SYSTEM 3 years HBase 3 years HTML 4 years J2EE 4 years JavaScript 4 years JSP 4 years MapReduce 6 years Oracle 8 years Servlets 4 years SQL 6 years Struts 4 years Web Services 4 years XML 4 years",
    "entities": [
        "log information",
        "AJAX",
        "User Defined Functions UDFs Expertise",
        "Conceptual Logical and Physical Data Model Data",
        "MN",
        "Oracle MySQL",
        "Tableau Software",
        "Struts Framework Implementing Servlets JSP",
        "Transactional Databases",
        "Data Mart",
        "BI",
        "HDFS",
        "Data Layouts using Partitions",
        "Hive Validation Scripts",
        "the Data Involved",
        "Data Lake",
        "Flume for Fraud Analytics Developed Python",
        "Hadoop Ecosystem",
        "HDFS Environment Hadoop HDFS Map Reduce Hive HBase",
        "Ajax",
        "Hibernate and Spring Frameworks Designed",
        "UDAFs",
        "Hadoop",
        "Effectiveness",
        "XML",
        "SOAP",
        "Data Flows",
        "Cincinnati",
        "WebLogic",
        "MapReduce Indexer Tool",
        "Responsible for Debug Optimization of Hive Scripts",
        "Shell",
        "the Referential Data Service",
        "Oozie Scala Spark",
        "Apache Spark",
        "Sr Hadoop Developer Sr Hadoop",
        "Tealium POC Report Actively",
        "Time Intervals",
        "JavaScript CSS Involved",
        "HTML Fluent",
        "Tableau Responsible for Scheduling using Active Batchjobs",
        "Managed Change Request Management and Defect Management Managed UAT",
        "Developed",
        "Offshore",
        "Interactive Data Visualizations",
        "Work Experience Sr Hadoop Developer",
        "UML",
        "JDBC Used Hibernate",
        "Servlets",
        "Structured Semistructured",
        "Core",
        "Applied CSS Cascading",
        "Python Sr Hadoop Developer Health",
        "Struts SQL PLDSQL",
        "Developed Web Services",
        "JSP",
        "Unstructured",
        "Github using Jenkins Exploring",
        "Worked",
        "Shell Scripting",
        "Spark Streaming",
        "AWS Apache Flink",
        "Junit Framework",
        "BigData",
        "Interface",
        "Spark",
        "Inversion of Controller IOC Used SpringIOC",
        "EJB",
        "Collectionsmorphline SolrIndexes",
        "Description Created",
        "Mattel Inc",
        "US",
        "Database",
        "Sqoop",
        "Oozie Experience",
        "QA",
        "Intel",
        "Parquet File",
        "Avro Files JSON Records Experienced",
        "CA",
        "Oracle",
        "Verbal",
        "the Financial Data Warehouse Participated",
        "MY SQL Oracle Teradata",
        "Spark Context  Pair RDDs Spark YARN Migrating",
        "PIG",
        "Consumers",
        "Cron",
        "Oozie",
        "Apache NIFIetc",
        "Deduplication Logic",
        "EAuto",
        "ObjectOriented Analysis and Design OOAD",
        "SQL",
        "Oracle SQL",
        "Description Key Achievements Involved",
        "Spark RDD",
        "Transactional",
        "PySpark Sr Hadoop Developer",
        "Big Data",
        "Hive",
        "JQuery Software Programmer Blue Pal Solutions Pvt Ltd",
        "Table Data File",
        "FTP",
        "MVC Architecture Designed",
        "Kafka Producers",
        "Supported MapReduce Programs",
        "Computers Science in Computers Science JNTU Skills APACHE HADOOP",
        "the Spring Framework",
        "Application Server Written",
        "SQL Server Oracle",
        "Transforming",
        "Cassandra Experience",
        "Impala",
        "Spark SQL",
        "Maintained Oozie",
        "Windows XPJ2EE",
        "Spark Streaming Application Consumer Involved",
        "ANT",
        "HDFS Worked on Hive",
        "Spark Streaming Expertise",
        "Agile Methodology Ability",
        "CSS",
        "MapReduce Sqoop",
        "Hadoop The Core",
        "Rank Key Function UDF Experienced",
        "Java JSP Servlets",
        "Design Development Implementation Testing and Deployment of Software Applications",
        "Servlets Highly",
        "BigData Analytics Hadoop",
        "NormalizationDenormalization Data",
        "MapReduce",
        "Oracle Windows XP",
        "Responsibility and Accomplishment Excellent",
        "Impala Hands",
        "NoSQL",
        "HIVE Import and Export",
        "Stream Sets Data Collector",
        "JQuery"
    ],
    "experience": "Experience in NoSQL databases like Mongo DB HBase and Cassandra Experience in Apache Spark cluster and streams processing using Spark Streaming Expertise in moving large amounts of log streaming event data and Transactional data using Flume Experience in developing MapReduce jobs in Java for data cleaning and preprocessing Expertise in writing Pig Latin Hive Scripts and extended their functionality using User Defined Functions UDFs Expertise in handling arrangement of data within certain limits Data Layouts using Partitions and Bucketing in Hive Expertise in preparing Interactive Data Visualizations using Tableau Software from different sources Hands on experience in developing workflows execute MapReduce Sqoop Pig Hive and Shell Scripts using Oozie Experience working with Cloudera Hue Interface and Impala Hands on experience developing Solr Indexes using MapReduce Indexer Tool Expertise in ObjectOriented Analysis and Design OOAD like UML and use of various design patterns Experience in Java JSP Servlets EJB Web Logic Web Sphere Hibernate Spring JBoss JDBC RMI Java Script Ajax JQuery XML and HTML Fluent with the core Java concepts like IO MultiThreading Exceptions Reg Ex Data Structures and Serialization Performed Unit Testing using Junit Testing Framework and Log4J to monitor the error logs Experience in process Improvement NormalizationDenormalization Data extraction cleansing and Manipulation Converting requirement specification Source system understanding into Conceptual Logical and Physical Data Model Data flow DFD Expertise in working with Transactional Databases like Oracle SQL server My SQL and Db2 Expertise in developing SQL queries Stored Procedures and excellent development experience with Agile Methodology Ability to adapt to evolving technology Strong sense of Responsibility and Accomplishment Excellent leadership interpersonal problem solving and time management skills Excellent communication skills both Written documentation and Verbal presentation Authorized to work in the US for any employer Work Experience Sr Hadoop Developer Mattel Inc October 2018 to Present Description FFM Federally Facilitated MarketplaceHealthcare Health Partners like other healthcare organizations has a legacy system clinicians and researchers that needed access to the data The data types include EMR generated data genomic data financial data patient and caregiver data incremental physiological monitoring ventilator data temperature and humidity data Any electronically generated data in the healthcare environment can be ingested and stored in the hdfs which will be used for analytic to aid in the delivery of quality care at the lowest possible cost and an environment to enable clinical researchers to examine healthcare data Key Achievements Performed performance tuning and troubleshooting of MapReduce jobs by analyzing and reviewing Hadoop log files Involved Low level design for MR Hive Impala Shell scripts to process data Involved in complete Big Data flow of the application starting from data ingestion upstream to HDFS processing the data in HDFS and analyzing the data Knowledge on handling Hive queries using Spark SQL that integrate with Spark environment implemented in Scala Used Spark Streaming API with Kafka to build live dashboards Worked on Transformations actions in RDD Spark Streaming Pair RDD Operations Checkpointing and SBT Implemented POC to migrate map reduce jobs into Spark RDD transformation using Scala IDE for Eclipse Creating Hive tables to import large data sets from various relational databases using Sqoop and export the analyzed data back for visualization and report generation by the BI team Installing and configuring Hive Sqoop Flume Oozie on the Hadoop clusters Involved in scheduling Oozie workflow engine to run multiple Hive and Pig jobs Developed a process for the Batch ingestion of CSV Files Sqoop from different sources and also generating views on the data source using Shell Scripting and Python Integrated a shell script to create Collectionsmorphline SolrIndexes on top of table directories using MapReduce Indexer Tool within Batch Ingestion Framework Implemented partitioning dynamic partitions and buckets in HIVE Developed Hive Scripts to create the views and apply transformation logic in the Target Database Involved in the design of Data Mart and Data Lake to provide faster insight into the Data Involved in using Stream Sets Data Collector tool and created Data Flows for one of the streaming application Experienced in using Kafka as a data pipeline between JMS Producer and Spark Streaming Application Consumer Involved in the development of Spark Streaming application for one of the data source using Scala Spark by applying the transformations Developed a script in Scala to read all the Parquet Tables in a Database and parse them as Json files another script to parse them as structured tables in Hive Designed and Maintained Oozie workflows to manage the flow of jobs in the cluster Configured Zookeeper for Cluster coordination services Developed a unit test script to read a Parquet file for testing PySpark on the cluster Involved in exploration of new technologies like AWS Apache Flink and Apache NIFIetc which can increase the business value Environment Hadoop HDFS Map Reduce Hive HBase Zookeeper Impala Javajdk16 Cloudera Oracle SQL Server UNIX Shell Scripting Flume Oozie Scala Spark Sqoop Python kafka PySpark Sr Hadoop Developer Intel Santa Clara CA January 2018 to September 2018 Description The purpose of the project is to perform the analysis on the Effectiveness and validity of controls and to store terabytes of log information generated by the source providers as part of the analysis and extract meaningful information out of it The solution is based on the open source Big Data software Hadoop The data will be stored in Hadoop file system and processed using Map Reduce jobs which intern includes getting the raw data process the data to obtain controls and redesignchange history information extract various reports out of the controls history and Export the information for further processing Key Achievements Responsible for Writing MapReduce jobs to perform operations like copying data on HDFS and defining job flows on EC2 server load and transform large sets of structured semistructured and unstructured data Developed a process for Sqooping data from multiple sources like SQL Server Oracle and Teradata Responsible for creation of mapping document from source fields to destination fields mapping Developed a shell script to create staging landing tables with the same schema like the source and generate the properties which are used by Oozie jobs Developed Oozie workflows for executing Sqoop and Hive actions Worked with NoSQL databases like Hbase in creating Hbase tables to load large sets of semi structured data coming from various sources Performance optimizations on SparkScala Diagnose and resolve performance issues Responsible for developing Python wrapper scripts which will extract specific date range using Sqoop by passing custom properties required for the workflow Developed scripts to run Oozie workflows capture the logs of all jobs that run on cluster and create a metadata table which specifies the execution times of each job Developed Hive scripts for performing transformation logic and also loading the data from staging zone to final landing zone Worked on Parquet File format to get a better storage and performance for publish tables Involved in loading transactional data into HDFS using Flume for Fraud Analytics Developed Python utility to validate HDFS tables with source tables Designed and developed UDFS to extend the functionality in both PIG and HIVE Import and Export of data using Sqoop between MySQL to HDFS on regular basis Responsible for developing multiple Kafka Producers and Consumers from scratch as per the software requirement specifications Involved in using CA7 tool to setup dependencies at each level Table Data File and Time Automated all the jobs for pulling data from FTP server to load data into Hive tables using Oozie workflows Involved in developing Spark code using Scala and   for faster testing and processing of data and exploring of optimizing it using Spark Context   Pair RDDs Spark YARN Migrating the needed data from Oracle MySQL in to HDFS using Sqoop and importing various formats of flat files in to HDFS Environment Hadoop HDFS Map Reduce Hive HBase Kafka Zookeeper Oozie Impala Javajdk16 Cloudera Oracle Teradata SQL Server UNIX Shell Scripting Flume Scala Spark Sqoop Python Sr Hadoop Developer Health partners Bloomington MN January 2016 to December 2017 Description The purpose of this project is to acquire Core metrics data export and process in Hadoop The Core metrics data files to be loaded in to Hadoop data warehouse daily to enable business reporting and outbound feeds We use Hadoop to store vast volumes of unstructured data allows the company to collect web logs transaction data and social media data Key Achievements Responsible for Managing Analyzing and Transforming petabytes of data and also quick validation check on FTP file arrival from S3 Bucket to HDFS Responsible for analyzing large data sets and derive customer usage patterns by developing new MapReduce programs Experienced in creation of Hive tables and loading data incrementally into the tables using Dynamic Partitioning and Worked on Avro Files JSON Records Experienced in using Pig for data cleansing and developed Pig Latin scripts to extract the data from web server output files to load into HDFS Worked on Hive by creating external and internal tables loading it with data and writing Hive queries Involved in development and usage of UDTFs and UDAFs for decoding Log Record Fields and Conversions Generating Minute Buckets for the specified Time Intervals and JSON Field Extractor Developed Pig and Hive UDFs to analyze the complex data to find specific user behavior Responsible for Debug Optimization of Hive Scripts and also implementing Deduplication Logic in Hive using a Rank Key Function UDF Experienced in writing Hive Validation Scripts which are used in validation framework for daily analysis through graphs and presented to business users Developed workflow in Oozie to automate the tasks of loading data into HDFS and preprocessing with Pig and Hive Involved for Cassandra Database Schema design Using BULK LOAD Utility data pushed to Cassandra databases Responsible for creating Dashboards on Tableau Server Generated reports for hive tables in different scenarios using Tableau Responsible for Scheduling using Active Batchjobs and Cron jobs Experienced in Jar builds that can be triggered by commits to Github using Jenkins Exploring new tools for data tagging like Tealium POC Report Actively updated the upper management with daily updates on the progress of project that include the classification levels that were achieved on the data Environment Hadoop Map Reduce HDFS Pig Hive HBase Zookeeper Oozie Impala Cassandra Javajdk16 Cloudera Oracle 11g10 g Windows NTUNIX Shell Scripting Tableau Tealium Sr Java Developer Well Point INC Cincinnati OH January 2013 to December 2015 Description Well Point Health Insurance portal provides ability for end customersusers to request a quote provide quote details purchase health insurance and payment of insurance amount This platform is built using a Struts framework in the presentation layer The back end functionality is built using Oracle as data persistence layer and Hibernate as data access layer Key Achievements Responsible for understanding the scope of the project and requirements gathering Used MapReduce to Index the large amount of data to easily access specific records Supported MapReduce Programs which are running on the cluster Developed MapReduce programs to perform data filtering for unstructured data Designed the application by implementing Struts Framework based on MVC Architecture Designed and developed the front end using JSP HTML and JavaScript and JQuery Developed framework for data processing using Design patterns Java XML Implemented J2EE standards MVC2 architecture using Struts Framework Implementing Servlets JSP and Ajax to design the user interface Used JSP Java Script HTML5 and CSS for manipulating validating customizing error messages to the User Interface Used the light weight container of the Spring Framework to provide architectural flexibility for Inversion of Controller IOC Used SpringIOC for dependency injection to Hibernate and Spring Frameworks Designed and developed Session beans to implement the Business logic Developed EJB components that are deployed on Web logic Application Server Written unit tests using Junit Framework and Logging is done using Log4J Framework Used Html CSS JavaScript and JQuery to develop front end pages Designed and developed various configuration files for Hibernate mappings Designed and Developed SQL queries and Stored Procedures Used XML XSLT XPATH to extract data from Web Services output XML Extensively used JavaScript JQuery and AJAX for clientside validation Used ANT scripts to fetch build and deploy application to development environment Developed Web Services for sending and getting data from different applications using SOAP messages Actively involved in code reviews and bug fixing Applied CSS Cascading style Sheets for entire site for standardization of the site Offshore coordination and User acceptance testing support Environment Java 50 Struts Spring 20 Hibernate 32 WebLogic 70 Eclipse 33 Oracle 10 g Junit 42Maven Windows XPJ2EE JSP JDBC Hibernate spring HTML XMLCSS JavaScript and JQuery Software Programmer Blue Pal Solutions Pvt Ltd April 2011 to December 2012 Description Created a home page for the use of internal employees This home page enables employees to punch in and punch out so the activity tracking report is generated beginning of the week for past week and sent to employees managers Description Key Achievements Involved in the analysis design of the application using Rational Rose Developed the various action classes to handle the requests and responses Designed and created Java Objects JSP pages JSF JavaBeans and Servlets to achieve various business functionalities and created validation methods using JavaScript and Backing Beans Involved in writing client side validations using JavaScript CSS Involved in the design of the Referential Data Service module to interface with various databases using JDBC Used Hibernate framework to persist the employee work hours to the database Developed classes and interface with underlying web services layer Prepared documentation and participated in preparing users manual for the application Prepared Use Cases Business Process Models and Data flow diagrams User Interface models Gathered analyzed requirements for EAuto designed process flow diagrams Defined business processes related to the project and provided technical direction to development workgroup Analyzed the legacy and the Financial Data Warehouse Participated in Data base design sessions Database normalization meetings Managed Change Request Management and Defect Management Managed UAT testing and developed test strategies test plans reviewed QA test plans for appropriate test coverage Involved in Developing JSPs action classes form beans response beans EJBs Extensively used XML to code configuration files Developed PLSQL stored procedures triggers Performed functional integration system and validation testing Environment Java J2EE JSP JCL DB2 Struts SQL PLDSQL Eclipse Oracle Windows XP HTML CSS JavaScript and XML Education Bachelors in Computers Science in Computers Science JNTU Skills APACHE HADOOP HDFS 3 years APACHE HADOOP IMPALA 3 years APACHE HADOOP MAPREDUCE 6 years APACHE HADOOP OOZIE 3 years APACHE HBASE 3 years databases 5 years Hadoop 3 years HADOOP DISTRIBUTED FILE SYSTEM 3 years HBase 3 years HTML 4 years J2EE 4 years JavaScript 4 years JSP 4 years MapReduce 6 years Oracle 8 years Servlets 4 years SQL 6 years Struts 4 years Web Services 4 years XML 4 years",
    "extracted_keywords": [
        "Sr",
        "Hadoop",
        "Developer",
        "Sr",
        "Hadoop",
        "span",
        "lDeveloperspan",
        "Sr",
        "Hadoop",
        "Developer",
        "Mattel",
        "Inc",
        "El",
        "Segando",
        "CA",
        "Overall",
        "years",
        "experience",
        "emphasis",
        "Design",
        "Development",
        "Implementation",
        "Testing",
        "Deployment",
        "Software",
        "Applications",
        "years",
        "IT",
        "experience",
        "BigData",
        "BigData",
        "Analytics",
        "Hadoop",
        "HDFS",
        "MapReduce",
        "YARN",
        "Hadoop",
        "Ecosystem",
        "Shell",
        "Scripting",
        "years",
        "development",
        "experience",
        "Java",
        "J2EE",
        "JSP",
        "Servlets",
        "sets",
        "Structured",
        "Semistructured",
        "datasets",
        "BigData",
        "applications",
        "Hands",
        "experience",
        "Hadoop",
        "Ecosystem",
        "components",
        "Map",
        "Reduce",
        "Processing",
        "HDFS",
        "Storage",
        "YARN",
        "Sqoop",
        "Pig",
        "Hive",
        "HBase",
        "Oozie",
        "ZooKeeper",
        "Spark",
        "data",
        "storage",
        "analysis",
        "Expertise",
        "data",
        "Hadoop",
        "ecosystem",
        "data",
        "storage",
        "RDBMS",
        "MY",
        "SQL",
        "Oracle",
        "Teradata",
        "DB2",
        "Sqoop",
        "Experience",
        "NoSQL",
        "databases",
        "Mongo",
        "DB",
        "HBase",
        "Cassandra",
        "Experience",
        "Apache",
        "Spark",
        "cluster",
        "streams",
        "processing",
        "Spark",
        "Streaming",
        "Expertise",
        "amounts",
        "log",
        "event",
        "data",
        "data",
        "Flume",
        "Experience",
        "MapReduce",
        "jobs",
        "Java",
        "data",
        "Expertise",
        "Pig",
        "Latin",
        "Hive",
        "Scripts",
        "functionality",
        "User",
        "Defined",
        "Functions",
        "UDFs",
        "Expertise",
        "arrangement",
        "data",
        "limits",
        "Data",
        "Layouts",
        "Partitions",
        "Bucketing",
        "Hive",
        "Expertise",
        "Interactive",
        "Data",
        "Visualizations",
        "Tableau",
        "Software",
        "sources",
        "Hands",
        "experience",
        "workflows",
        "MapReduce",
        "Sqoop",
        "Pig",
        "Hive",
        "Shell",
        "Scripts",
        "Oozie",
        "Experience",
        "Cloudera",
        "Hue",
        "Interface",
        "Impala",
        "Hands",
        "experience",
        "Solr",
        "Indexes",
        "MapReduce",
        "Indexer",
        "Tool",
        "Expertise",
        "ObjectOriented",
        "Analysis",
        "Design",
        "OOAD",
        "UML",
        "use",
        "design",
        "patterns",
        "Experience",
        "Java",
        "JSP",
        "Servlets",
        "EJB",
        "Web",
        "Logic",
        "Web",
        "Sphere",
        "Hibernate",
        "Spring",
        "JBoss",
        "JDBC",
        "RMI",
        "Java",
        "Script",
        "Ajax",
        "JQuery",
        "XML",
        "HTML",
        "Fluent",
        "core",
        "Java",
        "concepts",
        "IO",
        "MultiThreading",
        "Exceptions",
        "Reg",
        "Ex",
        "Data",
        "Structures",
        "Serialization",
        "Performed",
        "Unit",
        "Testing",
        "Junit",
        "Testing",
        "Framework",
        "Log4J",
        "error",
        "logs",
        "Experience",
        "process",
        "Improvement",
        "NormalizationDenormalization",
        "Data",
        "extraction",
        "cleansing",
        "Manipulation",
        "requirement",
        "specification",
        "Source",
        "system",
        "Conceptual",
        "Logical",
        "Physical",
        "Data",
        "Model",
        "Data",
        "DFD",
        "Expertise",
        "Transactional",
        "Databases",
        "Oracle",
        "SQL",
        "server",
        "SQL",
        "Db2",
        "Expertise",
        "SQL",
        "Procedures",
        "development",
        "experience",
        "Agile",
        "Methodology",
        "Ability",
        "technology",
        "sense",
        "Responsibility",
        "Accomplishment",
        "Excellent",
        "leadership",
        "problem",
        "time",
        "management",
        "communication",
        "skills",
        "documentation",
        "presentation",
        "US",
        "employer",
        "Work",
        "Experience",
        "Sr",
        "Hadoop",
        "Developer",
        "Mattel",
        "Inc",
        "October",
        "Present",
        "Description",
        "FFM",
        "Federally",
        "Facilitated",
        "MarketplaceHealthcare",
        "Health",
        "Partners",
        "healthcare",
        "organizations",
        "legacy",
        "system",
        "clinicians",
        "researchers",
        "access",
        "data",
        "data",
        "types",
        "EMR",
        "data",
        "data",
        "data",
        "patient",
        "caregiver",
        "data",
        "monitoring",
        "ventilator",
        "data",
        "temperature",
        "humidity",
        "data",
        "data",
        "healthcare",
        "environment",
        "hdfs",
        "delivery",
        "quality",
        "care",
        "cost",
        "environment",
        "researchers",
        "healthcare",
        "data",
        "Key",
        "Achievements",
        "Performed",
        "performance",
        "tuning",
        "troubleshooting",
        "MapReduce",
        "jobs",
        "Hadoop",
        "log",
        "files",
        "level",
        "design",
        "MR",
        "Hive",
        "Impala",
        "Shell",
        "scripts",
        "data",
        "Big",
        "Data",
        "flow",
        "application",
        "data",
        "ingestion",
        "HDFS",
        "data",
        "HDFS",
        "data",
        "Knowledge",
        "Hive",
        "queries",
        "Spark",
        "SQL",
        "Spark",
        "environment",
        "Scala",
        "Spark",
        "Streaming",
        "API",
        "Kafka",
        "dashboards",
        "Transformations",
        "actions",
        "RDD",
        "Spark",
        "Streaming",
        "Pair",
        "RDD",
        "Operations",
        "Checkpointing",
        "SBT",
        "Implemented",
        "POC",
        "map",
        "jobs",
        "Spark",
        "RDD",
        "transformation",
        "Scala",
        "IDE",
        "Eclipse",
        "Creating",
        "Hive",
        "tables",
        "data",
        "sets",
        "databases",
        "Sqoop",
        "data",
        "visualization",
        "report",
        "generation",
        "BI",
        "team",
        "Hive",
        "Sqoop",
        "Flume",
        "Oozie",
        "Hadoop",
        "clusters",
        "Oozie",
        "workflow",
        "engine",
        "Hive",
        "Pig",
        "jobs",
        "process",
        "Batch",
        "ingestion",
        "CSV",
        "Files",
        "Sqoop",
        "sources",
        "views",
        "data",
        "source",
        "Shell",
        "Scripting",
        "Python",
        "shell",
        "script",
        "Collectionsmorphline",
        "SolrIndexes",
        "top",
        "table",
        "directories",
        "MapReduce",
        "Indexer",
        "Tool",
        "Batch",
        "Ingestion",
        "Framework",
        "partitions",
        "buckets",
        "HIVE",
        "Hive",
        "Scripts",
        "views",
        "transformation",
        "logic",
        "Target",
        "Database",
        "design",
        "Data",
        "Mart",
        "Data",
        "Lake",
        "insight",
        "Data",
        "Stream",
        "Sets",
        "Data",
        "Collector",
        "tool",
        "Data",
        "Flows",
        "streaming",
        "application",
        "Kafka",
        "data",
        "pipeline",
        "JMS",
        "Producer",
        "Spark",
        "Streaming",
        "Application",
        "Consumer",
        "development",
        "Spark",
        "Streaming",
        "application",
        "data",
        "source",
        "Scala",
        "Spark",
        "transformations",
        "script",
        "Scala",
        "Parquet",
        "Tables",
        "Database",
        "Json",
        "script",
        "tables",
        "Hive",
        "Designed",
        "Oozie",
        "workflows",
        "flow",
        "jobs",
        "cluster",
        "Configured",
        "Zookeeper",
        "Cluster",
        "coordination",
        "services",
        "unit",
        "test",
        "script",
        "Parquet",
        "file",
        "PySpark",
        "cluster",
        "exploration",
        "technologies",
        "AWS",
        "Apache",
        "Flink",
        "Apache",
        "NIFIetc",
        "business",
        "value",
        "Environment",
        "Hadoop",
        "HDFS",
        "Map",
        "Reduce",
        "Hive",
        "HBase",
        "Zookeeper",
        "Impala",
        "Javajdk16",
        "Cloudera",
        "Oracle",
        "SQL",
        "Server",
        "UNIX",
        "Shell",
        "Scripting",
        "Flume",
        "Oozie",
        "Scala",
        "Spark",
        "Sqoop",
        "Python",
        "kafka",
        "PySpark",
        "Sr",
        "Hadoop",
        "Developer",
        "Intel",
        "Santa",
        "Clara",
        "CA",
        "January",
        "September",
        "Description",
        "purpose",
        "project",
        "analysis",
        "Effectiveness",
        "validity",
        "controls",
        "terabytes",
        "log",
        "information",
        "source",
        "providers",
        "part",
        "analysis",
        "information",
        "solution",
        "source",
        "Big",
        "Data",
        "software",
        "Hadoop",
        "data",
        "Hadoop",
        "file",
        "system",
        "Map",
        "Reduce",
        "jobs",
        "intern",
        "data",
        "process",
        "data",
        "controls",
        "redesignchange",
        "history",
        "information",
        "reports",
        "controls",
        "history",
        "information",
        "Key",
        "Achievements",
        "MapReduce",
        "jobs",
        "operations",
        "data",
        "HDFS",
        "job",
        "flows",
        "EC2",
        "server",
        "load",
        "sets",
        "data",
        "process",
        "data",
        "sources",
        "SQL",
        "Server",
        "Oracle",
        "Teradata",
        "Responsible",
        "creation",
        "mapping",
        "document",
        "source",
        "fields",
        "destination",
        "fields",
        "mapping",
        "shell",
        "script",
        "staging",
        "landing",
        "tables",
        "schema",
        "source",
        "properties",
        "Oozie",
        "jobs",
        "Developed",
        "Oozie",
        "workflows",
        "Sqoop",
        "Hive",
        "actions",
        "databases",
        "Hbase",
        "Hbase",
        "tables",
        "sets",
        "data",
        "sources",
        "Performance",
        "optimizations",
        "SparkScala",
        "Diagnose",
        "performance",
        "issues",
        "Python",
        "wrapper",
        "scripts",
        "date",
        "range",
        "Sqoop",
        "custom",
        "properties",
        "scripts",
        "Oozie",
        "workflows",
        "logs",
        "jobs",
        "cluster",
        "metadata",
        "table",
        "execution",
        "times",
        "job",
        "Hive",
        "scripts",
        "transformation",
        "logic",
        "data",
        "zone",
        "landing",
        "zone",
        "Parquet",
        "File",
        "format",
        "storage",
        "performance",
        "tables",
        "data",
        "HDFS",
        "Flume",
        "Fraud",
        "Analytics",
        "Python",
        "utility",
        "HDFS",
        "tables",
        "source",
        "tables",
        "UDFS",
        "functionality",
        "PIG",
        "HIVE",
        "Import",
        "Export",
        "data",
        "Sqoop",
        "MySQL",
        "HDFS",
        "basis",
        "Kafka",
        "Producers",
        "Consumers",
        "scratch",
        "software",
        "requirement",
        "specifications",
        "CA7",
        "tool",
        "dependencies",
        "level",
        "Table",
        "Data",
        "File",
        "Time",
        "jobs",
        "data",
        "FTP",
        "server",
        "data",
        "Hive",
        "tables",
        "Oozie",
        "workflows",
        "Spark",
        "code",
        "Scala",
        "testing",
        "processing",
        "data",
        "Spark",
        "Context",
        "Pair",
        "RDDs",
        "Spark",
        "YARN",
        "data",
        "Oracle",
        "MySQL",
        "HDFS",
        "Sqoop",
        "formats",
        "files",
        "HDFS",
        "Environment",
        "Hadoop",
        "HDFS",
        "Map",
        "Reduce",
        "Hive",
        "HBase",
        "Kafka",
        "Zookeeper",
        "Oozie",
        "Impala",
        "Javajdk16",
        "Cloudera",
        "Oracle",
        "Teradata",
        "SQL",
        "Server",
        "UNIX",
        "Shell",
        "Scripting",
        "Flume",
        "Scala",
        "Spark",
        "Sqoop",
        "Python",
        "Sr",
        "Hadoop",
        "Developer",
        "Health",
        "Bloomington",
        "MN",
        "January",
        "December",
        "Description",
        "purpose",
        "project",
        "Core",
        "metrics",
        "data",
        "export",
        "process",
        "Hadoop",
        "Core",
        "metrics",
        "data",
        "files",
        "Hadoop",
        "data",
        "warehouse",
        "business",
        "reporting",
        "outbound",
        "feeds",
        "Hadoop",
        "volumes",
        "data",
        "company",
        "web",
        "logs",
        "transaction",
        "data",
        "media",
        "data",
        "Key",
        "Achievements",
        "Analyzing",
        "Transforming",
        "petabytes",
        "data",
        "validation",
        "check",
        "FTP",
        "file",
        "arrival",
        "S3",
        "Bucket",
        "data",
        "sets",
        "customer",
        "usage",
        "patterns",
        "MapReduce",
        "programs",
        "creation",
        "Hive",
        "tables",
        "loading",
        "data",
        "tables",
        "Dynamic",
        "Partitioning",
        "Worked",
        "Avro",
        "Files",
        "JSON",
        "Records",
        "Pig",
        "data",
        "cleansing",
        "Pig",
        "Latin",
        "scripts",
        "data",
        "web",
        "server",
        "output",
        "files",
        "HDFS",
        "Worked",
        "Hive",
        "tables",
        "data",
        "Hive",
        "queries",
        "development",
        "usage",
        "UDTFs",
        "UDAFs",
        "Log",
        "Record",
        "Fields",
        "Conversions",
        "Generating",
        "Minute",
        "Buckets",
        "Time",
        "Intervals",
        "JSON",
        "Field",
        "Extractor",
        "Developed",
        "Pig",
        "Hive",
        "UDFs",
        "data",
        "user",
        "behavior",
        "Debug",
        "Optimization",
        "Hive",
        "Scripts",
        "Deduplication",
        "Logic",
        "Hive",
        "Rank",
        "Key",
        "Function",
        "UDF",
        "Hive",
        "Validation",
        "Scripts",
        "validation",
        "framework",
        "analysis",
        "graphs",
        "business",
        "users",
        "workflow",
        "Oozie",
        "tasks",
        "loading",
        "data",
        "HDFS",
        "Pig",
        "Hive",
        "Cassandra",
        "Database",
        "Schema",
        "design",
        "BULK",
        "LOAD",
        "Utility",
        "data",
        "Cassandra",
        "Dashboards",
        "Tableau",
        "Server",
        "Generated",
        "hive",
        "tables",
        "scenarios",
        "Tableau",
        "Responsible",
        "Scheduling",
        "Active",
        "Batchjobs",
        "Cron",
        "jobs",
        "Jar",
        "builds",
        "commits",
        "Github",
        "Jenkins",
        "tools",
        "data",
        "Tealium",
        "POC",
        "Report",
        "management",
        "updates",
        "progress",
        "project",
        "classification",
        "levels",
        "data",
        "Environment",
        "Hadoop",
        "Map",
        "Reduce",
        "HDFS",
        "Pig",
        "Hive",
        "HBase",
        "Zookeeper",
        "Oozie",
        "Impala",
        "Cassandra",
        "Javajdk16",
        "Cloudera",
        "Oracle",
        "g",
        "Windows",
        "NTUNIX",
        "Shell",
        "Scripting",
        "Tableau",
        "Tealium",
        "Sr",
        "Java",
        "Developer",
        "Well",
        "Point",
        "INC",
        "Cincinnati",
        "OH",
        "January",
        "December",
        "Description",
        "Point",
        "Health",
        "Insurance",
        "portal",
        "ability",
        "end",
        "customersusers",
        "quote",
        "details",
        "health",
        "insurance",
        "payment",
        "insurance",
        "amount",
        "platform",
        "Struts",
        "framework",
        "presentation",
        "layer",
        "end",
        "functionality",
        "Oracle",
        "data",
        "persistence",
        "layer",
        "Hibernate",
        "data",
        "access",
        "layer",
        "Key",
        "Achievements",
        "Responsible",
        "scope",
        "project",
        "requirements",
        "MapReduce",
        "amount",
        "data",
        "records",
        "MapReduce",
        "Programs",
        "cluster",
        "MapReduce",
        "programs",
        "data",
        "filtering",
        "data",
        "application",
        "Struts",
        "Framework",
        "MVC",
        "Architecture",
        "end",
        "JSP",
        "HTML",
        "JavaScript",
        "JQuery",
        "framework",
        "data",
        "processing",
        "Design",
        "patterns",
        "Java",
        "XML",
        "J2EE",
        "standards",
        "MVC2",
        "architecture",
        "Struts",
        "Framework",
        "Implementing",
        "Servlets",
        "JSP",
        "Ajax",
        "user",
        "interface",
        "JSP",
        "Java",
        "Script",
        "HTML5",
        "CSS",
        "error",
        "messages",
        "User",
        "Interface",
        "weight",
        "container",
        "Spring",
        "Framework",
        "flexibility",
        "Inversion",
        "Controller",
        "IOC",
        "SpringIOC",
        "dependency",
        "injection",
        "Hibernate",
        "Spring",
        "Frameworks",
        "Session",
        "beans",
        "Business",
        "logic",
        "Developed",
        "EJB",
        "components",
        "Web",
        "logic",
        "Application",
        "Server",
        "Written",
        "unit",
        "tests",
        "Junit",
        "Framework",
        "Logging",
        "Framework",
        "Html",
        "CSS",
        "JavaScript",
        "JQuery",
        "end",
        "pages",
        "configuration",
        "files",
        "Hibernate",
        "mappings",
        "Developed",
        "SQL",
        "queries",
        "Procedures",
        "XML",
        "XSLT",
        "XPATH",
        "data",
        "Web",
        "Services",
        "output",
        "XML",
        "JavaScript",
        "JQuery",
        "AJAX",
        "validation",
        "ANT",
        "scripts",
        "build",
        "application",
        "development",
        "environment",
        "Developed",
        "Web",
        "Services",
        "data",
        "applications",
        "SOAP",
        "messages",
        "code",
        "reviews",
        "bug",
        "CSS",
        "style",
        "Sheets",
        "site",
        "standardization",
        "site",
        "Offshore",
        "coordination",
        "User",
        "acceptance",
        "testing",
        "support",
        "Environment",
        "Java",
        "Struts",
        "Spring",
        "Hibernate",
        "WebLogic",
        "Eclipse",
        "Oracle",
        "g",
        "Junit",
        "42Maven",
        "Windows",
        "XPJ2EE",
        "JSP",
        "JDBC",
        "Hibernate",
        "spring",
        "HTML",
        "XMLCSS",
        "JavaScript",
        "JQuery",
        "Software",
        "Programmer",
        "Blue",
        "Pal",
        "Solutions",
        "Pvt",
        "Ltd",
        "April",
        "December",
        "Description",
        "home",
        "page",
        "use",
        "employees",
        "home",
        "page",
        "employees",
        "activity",
        "tracking",
        "report",
        "beginning",
        "week",
        "week",
        "employees",
        "managers",
        "Description",
        "Key",
        "Achievements",
        "analysis",
        "design",
        "application",
        "Rational",
        "Rose",
        "action",
        "classes",
        "requests",
        "responses",
        "Java",
        "Objects",
        "JSP",
        "JSF",
        "JavaBeans",
        "Servlets",
        "business",
        "functionalities",
        "validation",
        "methods",
        "JavaScript",
        "Backing",
        "Beans",
        "client",
        "side",
        "validations",
        "JavaScript",
        "CSS",
        "design",
        "Referential",
        "Data",
        "Service",
        "module",
        "databases",
        "JDBC",
        "Hibernate",
        "framework",
        "employee",
        "work",
        "hours",
        "database",
        "classes",
        "interface",
        "web",
        "services",
        "layer",
        "documentation",
        "users",
        "manual",
        "application",
        "Prepared",
        "Use",
        "Cases",
        "Business",
        "Process",
        "Models",
        "Data",
        "flow",
        "diagrams",
        "User",
        "Interface",
        "models",
        "Gathered",
        "requirements",
        "EAuto",
        "process",
        "flow",
        "diagrams",
        "business",
        "processes",
        "project",
        "direction",
        "development",
        "workgroup",
        "legacy",
        "Financial",
        "Data",
        "Warehouse",
        "Data",
        "base",
        "design",
        "sessions",
        "Database",
        "normalization",
        "meetings",
        "Managed",
        "Change",
        "Request",
        "Management",
        "Defect",
        "Management",
        "UAT",
        "testing",
        "test",
        "strategies",
        "test",
        "plans",
        "QA",
        "test",
        "plans",
        "test",
        "coverage",
        "JSPs",
        "action",
        "classes",
        "beans",
        "response",
        "beans",
        "EJBs",
        "XML",
        "configuration",
        "files",
        "procedures",
        "integration",
        "system",
        "validation",
        "testing",
        "Environment",
        "Java",
        "J2EE",
        "JSP",
        "JCL",
        "DB2",
        "Struts",
        "SQL",
        "PLDSQL",
        "Eclipse",
        "Oracle",
        "Windows",
        "XP",
        "HTML",
        "CSS",
        "JavaScript",
        "XML",
        "Education",
        "Bachelors",
        "Computers",
        "Science",
        "Computers",
        "Science",
        "JNTU",
        "Skills",
        "APACHE",
        "HADOOP",
        "HDFS",
        "years",
        "APACHE",
        "HADOOP",
        "IMPALA",
        "years",
        "APACHE",
        "HADOOP",
        "MAPREDUCE",
        "years",
        "APACHE",
        "HADOOP",
        "OOZIE",
        "years",
        "APACHE",
        "HBASE",
        "years",
        "years",
        "Hadoop",
        "years",
        "HADOOP",
        "FILE",
        "SYSTEM",
        "years",
        "HBase",
        "years",
        "HTML",
        "years",
        "J2EE",
        "years",
        "JavaScript",
        "years",
        "JSP",
        "years",
        "MapReduce",
        "years",
        "Oracle",
        "years",
        "Servlets",
        "years",
        "SQL",
        "years",
        "Struts",
        "years",
        "Web",
        "Services",
        "years",
        "XML",
        "years"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T21:40:38.349187",
    "resume_data": "Sr Hadoop Developer Sr Hadoop span lDeveloperspan Sr Hadoop Developer Mattel Inc El Segando CA Overall 8 years of overall experience with strong emphasis on Design Development Implementation Testing and Deployment of Software Applications Over 4 years of comprehensive IT experience in BigData and BigData Analytics Hadoop HDFS MapReduce YARN Hadoop Ecosystem and Shell Scripting 5 years of development experience using Java J2EE JSP and Servlets Highly capable for processing large sets of Structured Semistructured and Unstructured datasets and supporting BigData applications Hands on experience with Hadoop Ecosystem components like Map Reduce Processing HDFS Storage YARN Sqoop Pig Hive HBase Oozie ZooKeeper and Spark for data storage and analysis Expertise in transferring data between a Hadoop ecosystem and structured data storage in a RDBMS such as MY SQL Oracle Teradata and DB2 using Sqoop Experience in NoSQL databases like Mongo DB HBase and Cassandra Experience in Apache Spark cluster and streams processing using Spark Streaming Expertise in moving large amounts of log streaming event data and Transactional data using Flume Experience in developing MapReduce jobs in Java for data cleaning and preprocessing Expertise in writing Pig Latin Hive Scripts and extended their functionality using User Defined Functions UDFs Expertise in handling arrangement of data within certain limits Data Layouts using Partitions and Bucketing in Hive Expertise in preparing Interactive Data Visualizations using Tableau Software from different sources Hands on experience in developing workflows execute MapReduce Sqoop Pig Hive and Shell Scripts using Oozie Experience working with Cloudera Hue Interface and Impala Hands on experience developing Solr Indexes using MapReduce Indexer Tool Expertise in ObjectOriented Analysis and Design OOAD like UML and use of various design patterns Experience in Java JSP Servlets EJB Web Logic Web Sphere Hibernate Spring JBoss JDBC RMI Java Script Ajax JQuery XML and HTML Fluent with the core Java concepts like IO MultiThreading Exceptions Reg Ex Data Structures and Serialization Performed Unit Testing using Junit Testing Framework and Log4J to monitor the error logs Experience in process Improvement NormalizationDenormalization Data extraction cleansing and Manipulation Converting requirement specification Source system understanding into Conceptual Logical and Physical Data Model Data flow DFD Expertise in working with Transactional Databases like Oracle SQL server My SQL and Db2 Expertise in developing SQL queries Stored Procedures and excellent development experience with Agile Methodology Ability to adapt to evolving technology Strong sense of Responsibility and Accomplishment Excellent leadership interpersonal problem solving and time management skills Excellent communication skills both Written documentation and Verbal presentation Authorized to work in the US for any employer Work Experience Sr Hadoop Developer Mattel Inc October 2018 to Present Description FFM Federally Facilitated MarketplaceHealthcare Health Partners like other healthcare organizations has a legacy system clinicians and researchers that needed access to the data The data types include EMR generated data genomic data financial data patient and caregiver data incremental physiological monitoring ventilator data temperature and humidity data Any electronically generated data in the healthcare environment can be ingested and stored in the hdfs which will be used for analytic to aid in the delivery of quality care at the lowest possible cost and an environment to enable clinical researchers to examine healthcare data Key Achievements Performed performance tuning and troubleshooting of MapReduce jobs by analyzing and reviewing Hadoop log files Involved Low level design for MR Hive Impala Shell scripts to process data Involved in complete Big Data flow of the application starting from data ingestion upstream to HDFS processing the data in HDFS and analyzing the data Knowledge on handling Hive queries using Spark SQL that integrate with Spark environment implemented in Scala Used Spark Streaming API with Kafka to build live dashboards Worked on Transformations actions in RDD Spark Streaming Pair RDD Operations Checkpointing and SBT Implemented POC to migrate map reduce jobs into Spark RDD transformation using Scala IDE for Eclipse Creating Hive tables to import large data sets from various relational databases using Sqoop and export the analyzed data back for visualization and report generation by the BI team Installing and configuring Hive Sqoop Flume Oozie on the Hadoop clusters Involved in scheduling Oozie workflow engine to run multiple Hive and Pig jobs Developed a process for the Batch ingestion of CSV Files Sqoop from different sources and also generating views on the data source using Shell Scripting and Python Integrated a shell script to create Collectionsmorphline SolrIndexes on top of table directories using MapReduce Indexer Tool within Batch Ingestion Framework Implemented partitioning dynamic partitions and buckets in HIVE Developed Hive Scripts to create the views and apply transformation logic in the Target Database Involved in the design of Data Mart and Data Lake to provide faster insight into the Data Involved in using Stream Sets Data Collector tool and created Data Flows for one of the streaming application Experienced in using Kafka as a data pipeline between JMS Producer and Spark Streaming Application Consumer Involved in the development of Spark Streaming application for one of the data source using Scala Spark by applying the transformations Developed a script in Scala to read all the Parquet Tables in a Database and parse them as Json files another script to parse them as structured tables in Hive Designed and Maintained Oozie workflows to manage the flow of jobs in the cluster Configured Zookeeper for Cluster coordination services Developed a unit test script to read a Parquet file for testing PySpark on the cluster Involved in exploration of new technologies like AWS Apache Flink and Apache NIFIetc which can increase the business value Environment Hadoop HDFS Map Reduce Hive HBase Zookeeper Impala Javajdk16 Cloudera Oracle SQL Server UNIX Shell Scripting Flume Oozie Scala Spark Sqoop Python kafka PySpark Sr Hadoop Developer Intel Santa Clara CA January 2018 to September 2018 Description The purpose of the project is to perform the analysis on the Effectiveness and validity of controls and to store terabytes of log information generated by the source providers as part of the analysis and extract meaningful information out of it The solution is based on the open source Big Data software Hadoop The data will be stored in Hadoop file system and processed using Map Reduce jobs which intern includes getting the raw data process the data to obtain controls and redesignchange history information extract various reports out of the controls history and Export the information for further processing Key Achievements Responsible for Writing MapReduce jobs to perform operations like copying data on HDFS and defining job flows on EC2 server load and transform large sets of structured semistructured and unstructured data Developed a process for Sqooping data from multiple sources like SQL Server Oracle and Teradata Responsible for creation of mapping document from source fields to destination fields mapping Developed a shell script to create staging landing tables with the same schema like the source and generate the properties which are used by Oozie jobs Developed Oozie workflows for executing Sqoop and Hive actions Worked with NoSQL databases like Hbase in creating Hbase tables to load large sets of semi structured data coming from various sources Performance optimizations on SparkScala Diagnose and resolve performance issues Responsible for developing Python wrapper scripts which will extract specific date range using Sqoop by passing custom properties required for the workflow Developed scripts to run Oozie workflows capture the logs of all jobs that run on cluster and create a metadata table which specifies the execution times of each job Developed Hive scripts for performing transformation logic and also loading the data from staging zone to final landing zone Worked on Parquet File format to get a better storage and performance for publish tables Involved in loading transactional data into HDFS using Flume for Fraud Analytics Developed Python utility to validate HDFS tables with source tables Designed and developed UDFS to extend the functionality in both PIG and HIVE Import and Export of data using Sqoop between MySQL to HDFS on regular basis Responsible for developing multiple Kafka Producers and Consumers from scratch as per the software requirement specifications Involved in using CA7 tool to setup dependencies at each level Table Data File and Time Automated all the jobs for pulling data from FTP server to load data into Hive tables using Oozie workflows Involved in developing Spark code using Scala and SparkSQL for faster testing and processing of data and exploring of optimizing it using Spark Context SparkSQL Pair RDDs Spark YARN Migrating the needed data from Oracle MySQL in to HDFS using Sqoop and importing various formats of flat files in to HDFS Environment Hadoop HDFS Map Reduce Hive HBase Kafka Zookeeper Oozie Impala Javajdk16 Cloudera Oracle Teradata SQL Server UNIX Shell Scripting Flume Scala Spark Sqoop Python Sr Hadoop Developer Health partners Bloomington MN January 2016 to December 2017 Description The purpose of this project is to acquire Core metrics data export and process in Hadoop The Core metrics data files to be loaded in to Hadoop data warehouse daily to enable business reporting and outbound feeds We use Hadoop to store vast volumes of unstructured data allows the company to collect web logs transaction data and social media data Key Achievements Responsible for Managing Analyzing and Transforming petabytes of data and also quick validation check on FTP file arrival from S3 Bucket to HDFS Responsible for analyzing large data sets and derive customer usage patterns by developing new MapReduce programs Experienced in creation of Hive tables and loading data incrementally into the tables using Dynamic Partitioning and Worked on Avro Files JSON Records Experienced in using Pig for data cleansing and developed Pig Latin scripts to extract the data from web server output files to load into HDFS Worked on Hive by creating external and internal tables loading it with data and writing Hive queries Involved in development and usage of UDTFs and UDAFs for decoding Log Record Fields and Conversions Generating Minute Buckets for the specified Time Intervals and JSON Field Extractor Developed Pig and Hive UDFs to analyze the complex data to find specific user behavior Responsible for Debug Optimization of Hive Scripts and also implementing Deduplication Logic in Hive using a Rank Key Function UDF Experienced in writing Hive Validation Scripts which are used in validation framework for daily analysis through graphs and presented to business users Developed workflow in Oozie to automate the tasks of loading data into HDFS and preprocessing with Pig and Hive Involved for Cassandra Database Schema design Using BULK LOAD Utility data pushed to Cassandra databases Responsible for creating Dashboards on Tableau Server Generated reports for hive tables in different scenarios using Tableau Responsible for Scheduling using Active Batchjobs and Cron jobs Experienced in Jar builds that can be triggered by commits to Github using Jenkins Exploring new tools for data tagging like Tealium POC Report Actively updated the upper management with daily updates on the progress of project that include the classification levels that were achieved on the data Environment Hadoop Map Reduce HDFS Pig Hive HBase Zookeeper Oozie Impala Cassandra Javajdk16 Cloudera Oracle 11g10g Windows NTUNIX Shell Scripting Tableau Tealium Sr Java Developer Well Point INC Cincinnati OH January 2013 to December 2015 Description Well Point Health Insurance portal provides ability for end customersusers to request a quote provide quote details purchase health insurance and payment of insurance amount This platform is built using a Struts framework in the presentation layer The back end functionality is built using Oracle as data persistence layer and Hibernate as data access layer Key Achievements Responsible for understanding the scope of the project and requirements gathering Used MapReduce to Index the large amount of data to easily access specific records Supported MapReduce Programs which are running on the cluster Developed MapReduce programs to perform data filtering for unstructured data Designed the application by implementing Struts Framework based on MVC Architecture Designed and developed the front end using JSP HTML and JavaScript and JQuery Developed framework for data processing using Design patterns Java XML Implemented J2EE standards MVC2 architecture using Struts Framework Implementing Servlets JSP and Ajax to design the user interface Used JSP Java Script HTML5 and CSS for manipulating validating customizing error messages to the User Interface Used the light weight container of the Spring Framework to provide architectural flexibility for Inversion of Controller IOC Used SpringIOC for dependency injection to Hibernate and Spring Frameworks Designed and developed Session beans to implement the Business logic Developed EJB components that are deployed on Web logic Application Server Written unit tests using Junit Framework and Logging is done using Log4J Framework Used Html CSS JavaScript and JQuery to develop front end pages Designed and developed various configuration files for Hibernate mappings Designed and Developed SQL queries and Stored Procedures Used XML XSLT XPATH to extract data from Web Services output XML Extensively used JavaScript JQuery and AJAX for clientside validation Used ANT scripts to fetch build and deploy application to development environment Developed Web Services for sending and getting data from different applications using SOAP messages Actively involved in code reviews and bug fixing Applied CSS Cascading style Sheets for entire site for standardization of the site Offshore coordination and User acceptance testing support Environment Java 50 Struts Spring 20 Hibernate 32 WebLogic 70 Eclipse 33 Oracle 10g Junit 42Maven Windows XPJ2EE JSP JDBC Hibernate spring HTML XMLCSS JavaScript and JQuery Software Programmer Blue Pal Solutions Pvt Ltd April 2011 to December 2012 Description Created a home page for the use of internal employees This home page enables employees to punch in and punch out so the activity tracking report is generated beginning of the week for past week and sent to employees managers Description Key Achievements Involved in the analysis design of the application using Rational Rose Developed the various action classes to handle the requests and responses Designed and created Java Objects JSP pages JSF JavaBeans and Servlets to achieve various business functionalities and created validation methods using JavaScript and Backing Beans Involved in writing client side validations using JavaScript CSS Involved in the design of the Referential Data Service module to interface with various databases using JDBC Used Hibernate framework to persist the employee work hours to the database Developed classes and interface with underlying web services layer Prepared documentation and participated in preparing users manual for the application Prepared Use Cases Business Process Models and Data flow diagrams User Interface models Gathered analyzed requirements for EAuto designed process flow diagrams Defined business processes related to the project and provided technical direction to development workgroup Analyzed the legacy and the Financial Data Warehouse Participated in Data base design sessions Database normalization meetings Managed Change Request Management and Defect Management Managed UAT testing and developed test strategies test plans reviewed QA test plans for appropriate test coverage Involved in Developing JSPs action classes form beans response beans EJBs Extensively used XML to code configuration files Developed PLSQL stored procedures triggers Performed functional integration system and validation testing Environment Java J2EE JSP JCL DB2 Struts SQL PLDSQL Eclipse Oracle Windows XP HTML CSS JavaScript and XML Education Bachelors in Computers Science in Computers Science JNTU Skills APACHE HADOOP HDFS 3 years APACHE HADOOP IMPALA 3 years APACHE HADOOP MAPREDUCE 6 years APACHE HADOOP OOZIE 3 years APACHE HBASE 3 years databases 5 years Hadoop 3 years HADOOP DISTRIBUTED FILE SYSTEM 3 years HBase 3 years HTML 4 years J2EE 4 years JavaScript 4 years JSP 4 years MapReduce 6 years Oracle 8 years Servlets 4 years SQL 6 years Struts 4 years Web Services 4 years XML 4 years",
    "unique_id": "540a87d7-e73e-4b6f-974c-cb9f77ca1d07"
}