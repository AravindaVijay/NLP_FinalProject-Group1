{
    "clean_data": "Data Engineer Data Engineer Data Engineer RDOPS Wayne PA Have around 7 years of work experience into development Actively looking for opportunities in Big Data industry as a Big Data DeveloperData EngineerTableau Developer Extensive experience with different phases of project life cycle Analysis Design Implementation Testing and debugging new and existing clientserver based applications Extensive experience as SQL Server Developer in utilizing triggers cursors functions and stored procedures Strong technical knowledge in TSQL including ETL Microsoft Analysis Services Performance tuning Reporting Designing logicalphysical databases and Troubleshooting Expertise in Database development for OLTP Batch Processing Online Processing OLAP ETL Data warehousing Data mining DBMS and Data Modeling Experience in creating Views Constraints Triggers joins Functions Batch Scripts and numerous stored procedure consumed at various stages of migrationcleansing process reports or data validation Proficient in writing complex TSQL DDL DML Ranking Functions TOPn PIVOT XML PATH SQL programming physical logical database design and SQL Performance tuning Excellent knowledge in ETL validation Data Model Validations and troubleshoot migration issues Experience in Query tuning and performancetuning using Clustered and NonClustered indexes on tables for faster query retrieval Extensively worked on Normalized and denormalized database design and RDBMS concepts to design Relational Database model as per Business requirement Experience in developing Dashboard Parameterized Reports linked reports Sub reports Create complex Reports ex drill down reports using different objects drill through reports using SSRS Experience in Migrating databases from SQL 2008 R2 to SQL 2014 and Extraction Migration of Data from heterogeneous data sources like MS Access Teradata and DB2 etc to MS SQL Server High experiencecomfort level interpreting ETL Data Mapping documents transformation logic reusable logic etc Worked on administration tasks such as creating logins user mapping batch jobs backup and Recovery Maintenance tasks table management upgrades creating databasesFile groupsfilesTransaction logs Experience in using Data Modeling Tools Microsoft Visio and Erwin designing and developing periodic and ondemand reports using Crystal Reports and Report Builder Ability to comprehend the requirements and formulate solutions in a deadline oriented setting Excellent track record as a team player with effective communication skills Strong writing and documentation skills for the management and development Authorized to work in the US for any employer Work Experience Data Engineer RDOPS July 2018 to Present Responsibilities Working on LIA a Coordination of Benefits solution with intelligent analytics for Medicare and Medicaid claims Clearing off the Protected Health Information PHI cleaning data from EDI file formats eg 837834 flat files client databases and load into the MS SQL database Used Sqoop import to move data from MSSQL to HDFS in formats parquet avro textfile with compression codecs gzip snappyuncompressed on incremental append with splits on for boundary conditions Changing the number of mappers delimiters of fields and lines writing custom queries in Sqoop Developed maven projects as KIE sessions with dependencies and plugins for the running environments Implemented business rules for MedicareMedicaid insurance programs Run Spark jobs using maven jar files and required files and rules are passed as arguments This filters recoverable claims and gets saved as text files Launching an EMR cluster Release version 5x with Hive Presto Pig Zoo Keeper HBase Sqoop Spark Creating steps for running custom maven jars and storing output files to S3 bucket security groups restrictions of authorized systems Performed DDL operations in HQL on AWS EMR Worked with TBLPROPERTIES bucketing SerDe formats vectorization compression sizes and types indexing clustering partitioning storage formats for tables Worked on external tables skewed tables managed tables temporary tables transaction tables in Hive Export data from SQL tables using Sqoop to Hive on AWS EMR cluster and applying DML operations Use AWS Presto for querying by connecting to Hive metastore which was used to create schema Create AWS S3 buckets and pass path as arguments in spark jobs Store output flat files into output bucket in the AWS S3 Use Tableau for joins blends and unions depending on the cardinality Connecting data from different sources custom SQL queries creating pivot tables stored procedures Using live andor extract type connections Developed a data pipeline for streaming applications to get live data with kafka and process it in pysparkscala and store in required format in HDFS Write pyspark scala streaming scripts as required Implemented Oozie workflows to automate the data processing of spark jobs and streaming applications Developed charts plots continuousdiscrete Dimensions Measures splitting on delimiters used splits with data interpreter Customizing filters aggregations colors sizes tooltip label analytic functions like reference average forecast Created calculated fields new parameters KPIs aliases text tables with calculations like table pane acrossdown Developed views dashboards workbooks and projects Optimizing screen resolutions layouts scaling page orientations Pushing data to Tableau online and creating user accesses to the workbooks customizing start pages upon login Environment Microsoft SQL Apache Maven Tableau Drools Spark AWS S3 AWS Presto AWS EMR HIPAA compliance Hadoop Developer UAASC Western Illinois University June 2016 to July 2018 Responsibilities Worked on various projects as a softwaredata engineer to collect data required for research projects of various departments on fields like consumer products data historical stock prices data business products reviews etc Running spark applications on both local mode on test sample and on clusters in yarn mode in clusters Used Spark on text file datasets from HDFS using python text file reading operations on files in local file system and converting them to RDD applying transformations such as filtering mapping union and intersection using lambda operations Setting up accumulator and broadcast variables using spark context shuffling number of partitions Converting data sets into key value pairs and applying actions performing joins on RDDs applying coalesce to produce single output files repartition the RDD data on a partitioner Saving and loading tofrom textsequenceparquetavrojson file formats with compression codecs like uncompressed snappy gzip Developed global temporary views temporary tables on the dataframes and using Spark Sql to query output Performing aggregations functions using spark sql Created Hive UDFs and UDAFs in java as maven projects with dependencies in pomxml Used Hive UDFUDAF interface in java importing jar into Hive environment and added it to class path Automated OS operations in python scripts for combining files in folders directory creation deletions autorunning scripts Worked on creating Python 3 APIs for parsing data from a webpage and filter out the data and save in the required format Used python libraries such as beautiful soup requests to mine data from URL with python multiprocessing spoofing headers iterating pages and filtering content from html tags Created a framework which has many APIs to scrapping data from different web pages Developed Django applications to display analysis results on a web application Used HTML 5 and created pages to display the results used Bootstrap and CSS 3 Input fields send HTTP responses the respective files are queried from the database and sent as HTTP response calls and are injected to the Frontend display Was responsible for creating Tableau Dashboards for the data collected for the Financemarketing department Creating views from the data generated after cleaning as per the business requirements Environment Python Django MSSQL AWS HTML CSS Spark Tableau Software Engineer First Republic Bank May 2012 to June 2016 Responsibilities Worked in an Agile environment for the project tools used were JIRA GIT Following up with the activities on JIRA regarding issues reports related documents coordinating with the development business and testing teams from various locations Creating an issue tracking sub tasks changing and updating fix versions Creating Scrum projects discussing about sprints to be added standup meetings burndown charts summary reports Dealing with use cases sequence and activity diagrams as a part of UML diagrams Software Requirements Specification Deliver test artifacts adhering to customers needs Performed Requirement based testing following the DO 178B guidelines Software Verification and Validation included Analyzing and correct the bugs identified during Testing performed White box and Black box testing Modified Condition Decision Coverage MCDC code coverage functionality check by setting T F conditions in the code Used GIT to maintain repository creating and merging branches commit changes checking out moving and removing files Creating data models stored procedures queries for data analysis and manipulations views functions Maintain upgrade databases and creating backups in SQL Develop automated python scripts for repetitive task like delimiters splitting characters joining stray values filtering date and data format conversions regex operations like code matching replacing pattern matching Environment Python development MS SQL C Education MS in Computer Science Western Illinois University Additional Information SKILLS AWS Real time working knowledge on AWS services like EMR clusters EC2 Security Configurations storing data in S3 Querying the tables using Athena Presto HQL Databases like AWS RedShift Snowflake Apache Cassandra implementing JDBC connectors to AWS AWS CodeBuild and AWS CodePipeline for Continous Integration and Continous Deployement Hadoop ecosystems Experience on HDFS environment and Big data technologies like Apache Spark Apache Hive Sqoop Apache Kafka Flume NoSQL database like Apache HBase writing scripts in Apache Pig Working with file formats textavrojsonparquet and file compressions gzipsnappyuncompressed Good understanding of Hadoop components like HDFS Hadoop MapReduce and YARN Implemented batch processing live streaming using Kafka and spark streaming using pysparkscala Automating workflow of jobs using Apache Oozie through cli and GUI Tableau Proficient with creating interactive dashboards in Tableau accessing data from multiple sources Created custom SQL queries for liveextract connections in Tableau Worked and maintained Tableau online such as creating projects workbooks views user access privileges Experience on calculated fields KPIs creating views charts plots and trees Worked on saving in different formats joins blends unions pivot tables multisource connectors SQL Creating data models creating and maintaining data lakes databases Creating joins indexes clusters creating constraints stored procedures DRL DDL and DML Experience of migrating data from traditional databases like MSSQL to AWS RedShift Maven Projects Worked on creating Spark jobs Java Maven projects with Drools engine MS SQL Developed stateful and stateless KIE sessions working with relative dependencies and plugins for pomxml Python Developed my own Python Framework worked on APIs and developed web applications on Django Experience of working with HTML5 CSS3 Bootstrap DOM Worked on date format file formats string manipulations file and directory accesses different file formats OS and IO level operations concurrent execution internet protocols libraries etc Version Control and SDLC Knowledge of version control in Git GitHub worked in Agile environments Committing moving and removing files checkout branch and merge",
    "entities": [
        "HDFS Hadoop MapReduce",
        "Tableau Drools Spark AWS S3 AWS Presto AWS EMR",
        "AWS CodePipeline for Continous Integration and Continous Deployement Hadoop",
        "HTTP",
        "Dimensions Measures",
        "Query",
        "AWS EMR",
        "UDAFs",
        "Release",
        "Views Constraints Triggers",
        "RDD",
        "Hadoop",
        "Troubleshooting Expertise",
        "Apache Spark",
        "Present Responsibilities Working",
        "Athena Presto HQL Databases",
        "Medicaid",
        "SQL Develop",
        "HDFS Write",
        "Developed",
        "Run Spark",
        "Version Control",
        "Software Verification and Validation",
        "Django",
        "ETL Microsoft Analysis Services",
        "UML",
        "Tableau Worked",
        "AWS AWS CodeBuild",
        "Used Spark",
        "Create AWS S3",
        "the Protected Health Information PHI",
        "pysparkscala",
        "Software Requirements Specification Deliver",
        "EMR",
        "Implemented Oozie",
        "Crystal Reports and Report Builder Ability",
        "Spark",
        "MS Access Teradata",
        "Created Hive",
        "GIT",
        "US",
        "Database",
        "Sqoop",
        "GUI Tableau Proficient",
        "Drools",
        "Created",
        "SQL Creating",
        "First Republic Bank",
        "Data Engineer Data Engineer Data",
        "DBMS",
        "Computer Science Western Illinois University Additional Information SKILLS AWS",
        "java",
        "Hive Export",
        "DRL DDL",
        "SSRS",
        "SQL",
        "SerDe",
        "DML",
        "Bootstrap",
        "Extraction Migration of Data",
        "Hadoop Developer UAASC Western Illinois University",
        "SQL Server Developer",
        "Big Data",
        "Hive",
        "MedicareMedicaid",
        "Clearing",
        "S3 Querying",
        "Automated OS",
        "ETL",
        "Medicare",
        "Maven",
        "EDI",
        "Normalized",
        "Microsoft",
        "NonClustered",
        "TBLPROPERTIES",
        "Data Modeling",
        "Relational Database",
        "Performed Requirement",
        "Use AWS Presto",
        "Tableau",
        "java importing"
    ],
    "experience": "Experience in creating Views Constraints Triggers joins Functions Batch Scripts and numerous stored procedure consumed at various stages of migrationcleansing process reports or data validation Proficient in writing complex TSQL DDL DML Ranking Functions TOPn PIVOT XML PATH SQL programming physical logical database design and SQL Performance tuning Excellent knowledge in ETL validation Data Model Validations and troubleshoot migration issues Experience in Query tuning and performancetuning using Clustered and NonClustered indexes on tables for faster query retrieval Extensively worked on Normalized and denormalized database design and RDBMS concepts to design Relational Database model as per Business requirement Experience in developing Dashboard Parameterized Reports linked reports Sub reports Create complex Reports ex drill down reports using different objects drill through reports using SSRS Experience in Migrating databases from SQL 2008 R2 to SQL 2014 and Extraction Migration of Data from heterogeneous data sources like MS Access Teradata and DB2 etc to MS SQL Server High experiencecomfort level interpreting ETL Data Mapping documents transformation logic reusable logic etc Worked on administration tasks such as creating logins user mapping batch jobs backup and Recovery Maintenance tasks table management upgrades creating databasesFile groupsfilesTransaction logs Experience in using Data Modeling Tools Microsoft Visio and Erwin designing and developing periodic and ondemand reports using Crystal Reports and Report Builder Ability to comprehend the requirements and formulate solutions in a deadline oriented setting Excellent track record as a team player with effective communication skills Strong writing and documentation skills for the management and development Authorized to work in the US for any employer Work Experience Data Engineer RDOPS July 2018 to Present Responsibilities Working on LIA a Coordination of Benefits solution with intelligent analytics for Medicare and Medicaid claims Clearing off the Protected Health Information PHI cleaning data from EDI file formats eg 837834 flat files client databases and load into the MS SQL database Used Sqoop import to move data from MSSQL to HDFS in formats parquet avro textfile with compression codecs gzip snappyuncompressed on incremental append with splits on for boundary conditions Changing the number of mappers delimiters of fields and lines writing custom queries in Sqoop Developed maven projects as KIE sessions with dependencies and plugins for the running environments Implemented business rules for MedicareMedicaid insurance programs Run Spark jobs using maven jar files and required files and rules are passed as arguments This filters recoverable claims and gets saved as text files Launching an EMR cluster Release version 5x with Hive Presto Pig Zoo Keeper HBase Sqoop Spark Creating steps for running custom maven jars and storing output files to S3 bucket security groups restrictions of authorized systems Performed DDL operations in HQL on AWS EMR Worked with TBLPROPERTIES bucketing SerDe formats vectorization compression sizes and types indexing clustering partitioning storage formats for tables Worked on external tables skewed tables managed tables temporary tables transaction tables in Hive Export data from SQL tables using Sqoop to Hive on AWS EMR cluster and applying DML operations Use AWS Presto for querying by connecting to Hive metastore which was used to create schema Create AWS S3 buckets and pass path as arguments in spark jobs Store output flat files into output bucket in the AWS S3 Use Tableau for joins blends and unions depending on the cardinality Connecting data from different sources custom SQL queries creating pivot tables stored procedures Using live andor extract type connections Developed a data pipeline for streaming applications to get live data with kafka and process it in pysparkscala and store in required format in HDFS Write pyspark scala streaming scripts as required Implemented Oozie workflows to automate the data processing of spark jobs and streaming applications Developed charts plots continuousdiscrete Dimensions Measures splitting on delimiters used splits with data interpreter Customizing filters aggregations colors sizes tooltip label analytic functions like reference average forecast Created calculated fields new parameters KPIs aliases text tables with calculations like table pane acrossdown Developed views dashboards workbooks and projects Optimizing screen resolutions layouts scaling page orientations Pushing data to Tableau online and creating user accesses to the workbooks customizing start pages upon login Environment Microsoft SQL Apache Maven Tableau Drools Spark AWS S3 AWS Presto AWS EMR HIPAA compliance Hadoop Developer UAASC Western Illinois University June 2016 to July 2018 Responsibilities Worked on various projects as a softwaredata engineer to collect data required for research projects of various departments on fields like consumer products data historical stock prices data business products reviews etc Running spark applications on both local mode on test sample and on clusters in yarn mode in clusters Used Spark on text file datasets from HDFS using python text file reading operations on files in local file system and converting them to RDD applying transformations such as filtering mapping union and intersection using lambda operations Setting up accumulator and broadcast variables using spark context shuffling number of partitions Converting data sets into key value pairs and applying actions performing joins on RDDs applying coalesce to produce single output files repartition the RDD data on a partitioner Saving and loading tofrom textsequenceparquetavrojson file formats with compression codecs like uncompressed snappy gzip Developed global temporary views temporary tables on the dataframes and using Spark Sql to query output Performing aggregations functions using spark sql Created Hive UDFs and UDAFs in java as maven projects with dependencies in pomxml Used Hive UDFUDAF interface in java importing jar into Hive environment and added it to class path Automated OS operations in python scripts for combining files in folders directory creation deletions autorunning scripts Worked on creating Python 3 APIs for parsing data from a webpage and filter out the data and save in the required format Used python libraries such as beautiful soup requests to mine data from URL with python multiprocessing spoofing headers iterating pages and filtering content from html tags Created a framework which has many APIs to scrapping data from different web pages Developed Django applications to display analysis results on a web application Used HTML 5 and created pages to display the results used Bootstrap and CSS 3 Input fields send HTTP responses the respective files are queried from the database and sent as HTTP response calls and are injected to the Frontend display Was responsible for creating Tableau Dashboards for the data collected for the Financemarketing department Creating views from the data generated after cleaning as per the business requirements Environment Python Django MSSQL AWS HTML CSS Spark Tableau Software Engineer First Republic Bank May 2012 to June 2016 Responsibilities Worked in an Agile environment for the project tools used were JIRA GIT Following up with the activities on JIRA regarding issues reports related documents coordinating with the development business and testing teams from various locations Creating an issue tracking sub tasks changing and updating fix versions Creating Scrum projects discussing about sprints to be added standup meetings burndown charts summary reports Dealing with use cases sequence and activity diagrams as a part of UML diagrams Software Requirements Specification Deliver test artifacts adhering to customers needs Performed Requirement based testing following the DO 178B guidelines Software Verification and Validation included Analyzing and correct the bugs identified during Testing performed White box and Black box testing Modified Condition Decision Coverage MCDC code coverage functionality check by setting T F conditions in the code Used GIT to maintain repository creating and merging branches commit changes checking out moving and removing files Creating data models stored procedures queries for data analysis and manipulations views functions Maintain upgrade databases and creating backups in SQL Develop automated python scripts for repetitive task like delimiters splitting characters joining stray values filtering date and data format conversions regex operations like code matching replacing pattern matching Environment Python development MS SQL C Education MS in Computer Science Western Illinois University Additional Information SKILLS AWS Real time working knowledge on AWS services like EMR clusters EC2 Security Configurations storing data in S3 Querying the tables using Athena Presto HQL Databases like AWS RedShift Snowflake Apache Cassandra implementing JDBC connectors to AWS AWS CodeBuild and AWS CodePipeline for Continous Integration and Continous Deployement Hadoop ecosystems Experience on HDFS environment and Big data technologies like Apache Spark Apache Hive Sqoop Apache Kafka Flume NoSQL database like Apache HBase writing scripts in Apache Pig Working with file formats textavrojsonparquet and file compressions gzipsnappyuncompressed Good understanding of Hadoop components like HDFS Hadoop MapReduce and YARN Implemented batch processing live streaming using Kafka and spark streaming using pysparkscala Automating workflow of jobs using Apache Oozie through cli and GUI Tableau Proficient with creating interactive dashboards in Tableau accessing data from multiple sources Created custom SQL queries for liveextract connections in Tableau Worked and maintained Tableau online such as creating projects workbooks views user access privileges Experience on calculated fields KPIs creating views charts plots and trees Worked on saving in different formats joins blends unions pivot tables multisource connectors SQL Creating data models creating and maintaining data lakes databases Creating joins indexes clusters creating constraints stored procedures DRL DDL and DML Experience of migrating data from traditional databases like MSSQL to AWS RedShift Maven Projects Worked on creating Spark jobs Java Maven projects with Drools engine MS SQL Developed stateful and stateless KIE sessions working with relative dependencies and plugins for pomxml Python Developed my own Python Framework worked on APIs and developed web applications on Django Experience of working with HTML5 CSS3 Bootstrap DOM Worked on date format file formats string manipulations file and directory accesses different file formats OS and IO level operations concurrent execution internet protocols libraries etc Version Control and SDLC Knowledge of version control in Git GitHub worked in Agile environments Committing moving and removing files checkout branch and merge",
    "extracted_keywords": [
        "Data",
        "Engineer",
        "Data",
        "Engineer",
        "Data",
        "Engineer",
        "RDOPS",
        "Wayne",
        "PA",
        "years",
        "work",
        "experience",
        "development",
        "opportunities",
        "Data",
        "industry",
        "Big",
        "Data",
        "DeveloperData",
        "EngineerTableau",
        "Developer",
        "experience",
        "phases",
        "project",
        "life",
        "cycle",
        "Analysis",
        "Design",
        "Implementation",
        "Testing",
        "applications",
        "experience",
        "SQL",
        "Server",
        "Developer",
        "triggers",
        "cursors",
        "functions",
        "procedures",
        "knowledge",
        "TSQL",
        "ETL",
        "Microsoft",
        "Analysis",
        "Services",
        "Performance",
        "Designing",
        "databases",
        "Troubleshooting",
        "Expertise",
        "Database",
        "development",
        "OLTP",
        "Batch",
        "Processing",
        "Online",
        "Processing",
        "OLAP",
        "ETL",
        "Data",
        "warehousing",
        "Data",
        "mining",
        "DBMS",
        "Data",
        "Modeling",
        "Experience",
        "Views",
        "Constraints",
        "Triggers",
        "Functions",
        "Batch",
        "Scripts",
        "procedure",
        "stages",
        "process",
        "reports",
        "data",
        "validation",
        "Proficient",
        "TSQL",
        "DDL",
        "DML",
        "Ranking",
        "Functions",
        "TOPn",
        "PIVOT",
        "PATH",
        "SQL",
        "programming",
        "database",
        "design",
        "SQL",
        "Performance",
        "knowledge",
        "ETL",
        "validation",
        "Data",
        "Model",
        "Validations",
        "troubleshoot",
        "migration",
        "issues",
        "Experience",
        "Query",
        "performancetuning",
        "Clustered",
        "NonClustered",
        "indexes",
        "tables",
        "query",
        "retrieval",
        "Normalized",
        "database",
        "design",
        "RDBMS",
        "concepts",
        "Relational",
        "Database",
        "model",
        "Business",
        "requirement",
        "Experience",
        "Dashboard",
        "Parameterized",
        "Reports",
        "reports",
        "Sub",
        "reports",
        "Reports",
        "ex",
        "drill",
        "reports",
        "objects",
        "drill",
        "reports",
        "SSRS",
        "Experience",
        "Migrating",
        "databases",
        "SQL",
        "R2",
        "SQL",
        "Extraction",
        "Migration",
        "Data",
        "data",
        "sources",
        "MS",
        "Access",
        "Teradata",
        "DB2",
        "MS",
        "SQL",
        "Server",
        "experiencecomfort",
        "level",
        "interpreting",
        "ETL",
        "Data",
        "Mapping",
        "documents",
        "transformation",
        "logic",
        "logic",
        "administration",
        "tasks",
        "logins",
        "user",
        "mapping",
        "batch",
        "jobs",
        "backup",
        "Recovery",
        "Maintenance",
        "tasks",
        "table",
        "management",
        "upgrades",
        "groupsfilesTransaction",
        "Experience",
        "Data",
        "Modeling",
        "Tools",
        "Microsoft",
        "Visio",
        "Erwin",
        "ondemand",
        "reports",
        "Crystal",
        "Reports",
        "Report",
        "Builder",
        "Ability",
        "requirements",
        "solutions",
        "deadline",
        "track",
        "record",
        "team",
        "player",
        "communication",
        "skills",
        "writing",
        "documentation",
        "skills",
        "management",
        "development",
        "US",
        "employer",
        "Work",
        "Experience",
        "Data",
        "Engineer",
        "RDOPS",
        "July",
        "Present",
        "Responsibilities",
        "LIA",
        "Coordination",
        "Benefits",
        "solution",
        "analytics",
        "Medicare",
        "Medicaid",
        "Clearing",
        "Protected",
        "Health",
        "Information",
        "PHI",
        "data",
        "EDI",
        "file",
        "formats",
        "files",
        "client",
        "databases",
        "load",
        "MS",
        "SQL",
        "database",
        "Sqoop",
        "import",
        "data",
        "MSSQL",
        "HDFS",
        "formats",
        "parquet",
        "avro",
        "textfile",
        "compression",
        "codecs",
        "gzip",
        "append",
        "splits",
        "conditions",
        "number",
        "mappers",
        "delimiters",
        "fields",
        "lines",
        "custom",
        "queries",
        "Sqoop",
        "Developed",
        "maven",
        "projects",
        "KIE",
        "sessions",
        "dependencies",
        "plugins",
        "environments",
        "business",
        "rules",
        "MedicareMedicaid",
        "insurance",
        "programs",
        "Spark",
        "jobs",
        "maven",
        "jar",
        "files",
        "files",
        "rules",
        "arguments",
        "claims",
        "text",
        "files",
        "EMR",
        "cluster",
        "Release",
        "version",
        "5x",
        "Hive",
        "Presto",
        "Pig",
        "Zoo",
        "Keeper",
        "HBase",
        "Sqoop",
        "Spark",
        "steps",
        "custom",
        "maven",
        "jars",
        "output",
        "files",
        "S3",
        "bucket",
        "security",
        "groups",
        "restrictions",
        "systems",
        "Performed",
        "DDL",
        "operations",
        "HQL",
        "AWS",
        "EMR",
        "TBLPROPERTIES",
        "SerDe",
        "vectorization",
        "compression",
        "sizes",
        "types",
        "storage",
        "formats",
        "tables",
        "tables",
        "tables",
        "tables",
        "tables",
        "transaction",
        "tables",
        "Hive",
        "Export",
        "data",
        "SQL",
        "tables",
        "Sqoop",
        "Hive",
        "AWS",
        "EMR",
        "cluster",
        "DML",
        "operations",
        "Use",
        "AWS",
        "Presto",
        "Hive",
        "metastore",
        "schema",
        "AWS",
        "S3",
        "buckets",
        "path",
        "arguments",
        "spark",
        "jobs",
        "Store",
        "output",
        "files",
        "output",
        "bucket",
        "AWS",
        "S3",
        "Use",
        "Tableau",
        "joins",
        "blends",
        "unions",
        "cardinality",
        "data",
        "sources",
        "custom",
        "SQL",
        "tables",
        "procedures",
        "extract",
        "type",
        "connections",
        "data",
        "pipeline",
        "streaming",
        "applications",
        "data",
        "kafka",
        "pysparkscala",
        "store",
        "format",
        "HDFS",
        "Write",
        "pyspark",
        "streaming",
        "scripts",
        "Implemented",
        "Oozie",
        "workflows",
        "data",
        "processing",
        "spark",
        "jobs",
        "streaming",
        "applications",
        "charts",
        "plots",
        "continuousdiscrete",
        "Dimensions",
        "Measures",
        "splitting",
        "delimiters",
        "splits",
        "data",
        "interpreter",
        "Customizing",
        "filters",
        "aggregations",
        "colors",
        "tooltip",
        "label",
        "functions",
        "reference",
        "forecast",
        "fields",
        "parameters",
        "aliases",
        "text",
        "tables",
        "calculations",
        "table",
        "pane",
        "acrossdown",
        "views",
        "dashboards",
        "workbooks",
        "projects",
        "screen",
        "resolutions",
        "layouts",
        "page",
        "orientations",
        "data",
        "Tableau",
        "user",
        "workbooks",
        "pages",
        "login",
        "Environment",
        "Microsoft",
        "SQL",
        "Apache",
        "Maven",
        "Tableau",
        "Drools",
        "Spark",
        "S3",
        "AWS",
        "Presto",
        "AWS",
        "EMR",
        "compliance",
        "Hadoop",
        "Developer",
        "UAASC",
        "Western",
        "Illinois",
        "University",
        "June",
        "July",
        "Responsibilities",
        "projects",
        "softwaredata",
        "engineer",
        "data",
        "research",
        "projects",
        "departments",
        "fields",
        "consumer",
        "products",
        "data",
        "stock",
        "prices",
        "data",
        "business",
        "products",
        "spark",
        "applications",
        "mode",
        "test",
        "sample",
        "clusters",
        "yarn",
        "mode",
        "clusters",
        "Spark",
        "text",
        "file",
        "datasets",
        "text",
        "file",
        "operations",
        "files",
        "file",
        "system",
        "transformations",
        "mapping",
        "union",
        "intersection",
        "operations",
        "accumulator",
        "variables",
        "spark",
        "context",
        "number",
        "partitions",
        "data",
        "sets",
        "value",
        "pairs",
        "actions",
        "joins",
        "RDDs",
        "coalesce",
        "output",
        "files",
        "repartition",
        "RDD",
        "data",
        "partitioner",
        "Saving",
        "loading",
        "textsequenceparquetavrojson",
        "file",
        "formats",
        "compression",
        "codecs",
        "gzip",
        "views",
        "tables",
        "dataframes",
        "Spark",
        "Sql",
        "output",
        "aggregations",
        "functions",
        "spark",
        "sql",
        "Created",
        "Hive",
        "UDFs",
        "UDAFs",
        "maven",
        "projects",
        "dependencies",
        "pomxml",
        "Hive",
        "UDFUDAF",
        "interface",
        "jar",
        "Hive",
        "environment",
        "class",
        "path",
        "Automated",
        "OS",
        "operations",
        "scripts",
        "files",
        "folders",
        "directory",
        "creation",
        "deletions",
        "scripts",
        "Python",
        "APIs",
        "data",
        "webpage",
        "data",
        "format",
        "python",
        "libraries",
        "soup",
        "requests",
        "data",
        "URL",
        "python",
        "multiprocessing",
        "headers",
        "pages",
        "content",
        "html",
        "tags",
        "framework",
        "APIs",
        "data",
        "web",
        "pages",
        "Django",
        "applications",
        "analysis",
        "results",
        "web",
        "application",
        "HTML",
        "pages",
        "results",
        "Bootstrap",
        "CSS",
        "Input",
        "fields",
        "HTTP",
        "responses",
        "files",
        "database",
        "HTTP",
        "response",
        "Frontend",
        "display",
        "Tableau",
        "Dashboards",
        "data",
        "Financemarketing",
        "department",
        "views",
        "data",
        "business",
        "requirements",
        "Environment",
        "Python",
        "Django",
        "MSSQL",
        "AWS",
        "HTML",
        "CSS",
        "Spark",
        "Tableau",
        "Software",
        "Engineer",
        "First",
        "Republic",
        "Bank",
        "May",
        "June",
        "Responsibilities",
        "environment",
        "project",
        "tools",
        "JIRA",
        "GIT",
        "activities",
        "JIRA",
        "issues",
        "reports",
        "documents",
        "development",
        "business",
        "testing",
        "teams",
        "locations",
        "issue",
        "sub",
        "tasks",
        "fix",
        "versions",
        "Scrum",
        "projects",
        "sprints",
        "standup",
        "meetings",
        "charts",
        "summary",
        "reports",
        "use",
        "cases",
        "sequence",
        "activity",
        "diagrams",
        "part",
        "UML",
        "diagrams",
        "Software",
        "Requirements",
        "Specification",
        "Deliver",
        "test",
        "artifacts",
        "customers",
        "Performed",
        "Requirement",
        "testing",
        "DO",
        "178B",
        "Software",
        "Verification",
        "Validation",
        "Analyzing",
        "bugs",
        "Testing",
        "White",
        "box",
        "Black",
        "box",
        "Modified",
        "Condition",
        "Decision",
        "Coverage",
        "MCDC",
        "code",
        "coverage",
        "functionality",
        "check",
        "T",
        "F",
        "conditions",
        "code",
        "GIT",
        "repository",
        "creating",
        "branches",
        "changes",
        "files",
        "data",
        "models",
        "procedures",
        "data",
        "analysis",
        "manipulations",
        "views",
        "functions",
        "upgrade",
        "databases",
        "backups",
        "SQL",
        "Develop",
        "python",
        "scripts",
        "task",
        "delimiters",
        "characters",
        "values",
        "filtering",
        "date",
        "data",
        "format",
        "conversions",
        "operations",
        "code",
        "matching",
        "replacing",
        "pattern",
        "Environment",
        "Python",
        "development",
        "MS",
        "SQL",
        "C",
        "Education",
        "MS",
        "Computer",
        "Science",
        "Western",
        "Illinois",
        "University",
        "Additional",
        "Information",
        "SKILLS",
        "AWS",
        "time",
        "knowledge",
        "AWS",
        "services",
        "EMR",
        "clusters",
        "EC2",
        "Security",
        "Configurations",
        "data",
        "S3",
        "Querying",
        "tables",
        "Athena",
        "Presto",
        "HQL",
        "Databases",
        "AWS",
        "RedShift",
        "Snowflake",
        "Apache",
        "Cassandra",
        "JDBC",
        "connectors",
        "AWS",
        "AWS",
        "CodeBuild",
        "AWS",
        "CodePipeline",
        "Continous",
        "Integration",
        "Continous",
        "Deployement",
        "Hadoop",
        "Experience",
        "HDFS",
        "environment",
        "data",
        "technologies",
        "Apache",
        "Spark",
        "Apache",
        "Hive",
        "Sqoop",
        "Apache",
        "Kafka",
        "Flume",
        "database",
        "Apache",
        "HBase",
        "scripts",
        "Apache",
        "Pig",
        "Working",
        "file",
        "formats",
        "textavrojsonparquet",
        "file",
        "compressions",
        "understanding",
        "Hadoop",
        "components",
        "HDFS",
        "Hadoop",
        "MapReduce",
        "YARN",
        "batch",
        "streaming",
        "Kafka",
        "streaming",
        "pysparkscala",
        "workflow",
        "jobs",
        "Apache",
        "Oozie",
        "cli",
        "GUI",
        "Tableau",
        "Proficient",
        "dashboards",
        "Tableau",
        "data",
        "sources",
        "custom",
        "SQL",
        "liveextract",
        "connections",
        "Tableau",
        "Worked",
        "Tableau",
        "projects",
        "workbooks",
        "user",
        "access",
        "privileges",
        "Experience",
        "fields",
        "KPIs",
        "views",
        "charts",
        "plots",
        "trees",
        "formats",
        "blends",
        "unions",
        "pivot",
        "multisource",
        "connectors",
        "SQL",
        "data",
        "models",
        "data",
        "lakes",
        "joins",
        "indexes",
        "clusters",
        "constraints",
        "procedures",
        "DRL",
        "DDL",
        "DML",
        "Experience",
        "data",
        "databases",
        "MSSQL",
        "AWS",
        "RedShift",
        "Maven",
        "Projects",
        "Spark",
        "jobs",
        "Java",
        "Maven",
        "Drools",
        "engine",
        "MS",
        "SQL",
        "KIE",
        "sessions",
        "dependencies",
        "plugins",
        "pomxml",
        "Python",
        "Python",
        "Framework",
        "APIs",
        "web",
        "applications",
        "Django",
        "Experience",
        "HTML5",
        "CSS3",
        "Bootstrap",
        "DOM",
        "date",
        "format",
        "file",
        "formats",
        "string",
        "manipulations",
        "file",
        "directory",
        "file",
        "formats",
        "OS",
        "IO",
        "level",
        "operations",
        "execution",
        "internet",
        "protocols",
        "Version",
        "Control",
        "SDLC",
        "Knowledge",
        "version",
        "control",
        "Git",
        "GitHub",
        "environments",
        "files",
        "checkout",
        "branch"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T22:11:49.034627",
    "resume_data": "Data Engineer Data Engineer Data Engineer RDOPS Wayne PA Have around 7 years of work experience into development Actively looking for opportunities in Big Data industry as a Big Data DeveloperData EngineerTableau Developer Extensive experience with different phases of project life cycle Analysis Design Implementation Testing and debugging new and existing clientserver based applications Extensive experience as SQL Server Developer in utilizing triggers cursors functions and stored procedures Strong technical knowledge in TSQL including ETL Microsoft Analysis Services Performance tuning Reporting Designing logicalphysical databases and Troubleshooting Expertise in Database development for OLTP Batch Processing Online Processing OLAP ETL Data warehousing Data mining DBMS and Data Modeling Experience in creating Views Constraints Triggers joins Functions Batch Scripts and numerous stored procedure consumed at various stages of migrationcleansing process reports or data validation Proficient in writing complex TSQL DDL DML Ranking Functions TOPn PIVOT XML PATH SQL programming physical logical database design and SQL Performance tuning Excellent knowledge in ETL validation Data Model Validations and troubleshoot migration issues Experience in Query tuning and performancetuning using Clustered and NonClustered indexes on tables for faster query retrieval Extensively worked on Normalized and denormalized database design and RDBMS concepts to design Relational Database model as per Business requirement Experience in developing Dashboard Parameterized Reports linked reports Sub reports Create complex Reports ex drill down reports using different objects drill through reports using SSRS Experience in Migrating databases from SQL 2008 R2 to SQL 2014 and Extraction Migration of Data from heterogeneous data sources like MS Access Teradata and DB2 etc to MS SQL Server High experiencecomfort level interpreting ETL Data Mapping documents transformation logic reusable logic etc Worked on administration tasks such as creating logins user mapping batch jobs backup and Recovery Maintenance tasks table management upgrades creating databasesFile groupsfilesTransaction logs Experience in using Data Modeling Tools Microsoft Visio and Erwin designing and developing periodic and ondemand reports using Crystal Reports and Report Builder Ability to comprehend the requirements and formulate solutions in a deadline oriented setting Excellent track record as a team player with effective communication skills Strong writing and documentation skills for the management and development Authorized to work in the US for any employer Work Experience Data Engineer RDOPS July 2018 to Present Responsibilities Working on LIA a Coordination of Benefits solution with intelligent analytics for Medicare and Medicaid claims Clearing off the Protected Health Information PHI cleaning data from EDI file formats eg 837834 flat files client databases and load into the MS SQL database Used Sqoop import to move data from MSSQL to HDFS in formats parquet avro textfile with compression codecs gzip snappyuncompressed on incremental append with splits on for boundary conditions Changing the number of mappers delimiters of fields and lines writing custom queries in Sqoop Developed maven projects as KIE sessions with dependencies and plugins for the running environments Implemented business rules for MedicareMedicaid insurance programs Run Spark jobs using maven jar files and required files and rules are passed as arguments This filters recoverable claims and gets saved as text files Launching an EMR cluster Release version 5x with Hive Presto Pig Zoo Keeper HBase Sqoop Spark Creating steps for running custom maven jars and storing output files to S3 bucket security groups restrictions of authorized systems Performed DDL operations in HQL on AWS EMR Worked with TBLPROPERTIES bucketing SerDe formats vectorization compression sizes and types indexing clustering partitioning storage formats for tables Worked on external tables skewed tables managed tables temporary tables transaction tables in Hive Export data from SQL tables using Sqoop to Hive on AWS EMR cluster and applying DML operations Use AWS Presto for querying by connecting to Hive metastore which was used to create schema Create AWS S3 buckets and pass path as arguments in spark jobs Store output flat files into output bucket in the AWS S3 Use Tableau for joins blends and unions depending on the cardinality Connecting data from different sources custom SQL queries creating pivot tables stored procedures Using live andor extract type connections Developed a data pipeline for streaming applications to get live data with kafka and process it in pysparkscala and store in required format in HDFS Write pyspark scala streaming scripts as required Implemented Oozie workflows to automate the data processing of spark jobs and streaming applications Developed charts plots continuousdiscrete Dimensions Measures splitting on delimiters used splits with data interpreter Customizing filters aggregations colors sizes tooltip label analytic functions like reference average forecast Created calculated fields new parameters KPIs aliases text tables with calculations like table pane acrossdown Developed views dashboards workbooks and projects Optimizing screen resolutions layouts scaling page orientations Pushing data to Tableau online and creating user accesses to the workbooks customizing start pages upon login Environment Microsoft SQL Apache Maven Tableau Drools Spark AWS S3 AWS Presto AWS EMR HIPAA compliance Hadoop Developer UAASC Western Illinois University June 2016 to July 2018 Responsibilities Worked on various projects as a softwaredata engineer to collect data required for research projects of various departments on fields like consumer products data historical stock prices data business products reviews etc Running spark applications on both local mode on test sample and on clusters in yarn mode in clusters Used Spark on text file datasets from HDFS using python text file reading operations on files in local file system and converting them to RDD applying transformations such as filtering mapping union and intersection using lambda operations Setting up accumulator and broadcast variables using spark context shuffling number of partitions Converting data sets into key value pairs and applying actions performing joins on RDDs applying coalesce to produce single output files repartition the RDD data on a partitioner Saving and loading tofrom textsequenceparquetavrojson file formats with compression codecs like uncompressed snappy gzip Developed global temporary views temporary tables on the dataframes and using Spark Sql to query output Performing aggregations functions using spark sql Created Hive UDFs and UDAFs in java as maven projects with dependencies in pomxml Used Hive UDFUDAF interface in java importing jar into Hive environment and added it to class path Automated OS operations in python scripts for combining files in folders directory creation deletions autorunning scripts Worked on creating Python 3 APIs for parsing data from a webpage and filter out the data and save in the required format Used python libraries such as beautiful soup requests to mine data from URL with python multiprocessing spoofing headers iterating pages and filtering content from html tags Created a framework which has many APIs to scrapping data from different web pages Developed Django applications to display analysis results on a web application Used HTML 5 and created pages to display the results used Bootstrap and CSS 3 Input fields send HTTP responses the respective files are queried from the database and sent as HTTP response calls and are injected to the Frontend display Was responsible for creating Tableau Dashboards for the data collected for the Financemarketing department Creating views from the data generated after cleaning as per the business requirements Environment Python Django MSSQL AWS HTML CSS Spark Tableau Software Engineer First Republic Bank May 2012 to June 2016 Responsibilities Worked in an Agile environment for the project tools used were JIRA GIT Following up with the activities on JIRA regarding issues reports related documents coordinating with the development business and testing teams from various locations Creating an issue tracking sub tasks changing and updating fix versions Creating Scrum projects discussing about sprints to be added standup meetings burndown charts summary reports Dealing with use cases sequence and activity diagrams as a part of UML diagrams Software Requirements Specification Deliver test artifacts adhering to customers needs Performed Requirement based testing following the DO 178B guidelines Software Verification and Validation included Analyzing and correct the bugs identified during Testing performed White box and Black box testing Modified Condition Decision Coverage MCDC code coverage functionality check by setting T F conditions in the code Used GIT to maintain repository creating and merging branches commit changes checking out moving and removing files Creating data models stored procedures queries for data analysis and manipulations views functions Maintain upgrade databases and creating backups in SQL Develop automated python scripts for repetitive task like delimiters splitting characters joining stray values filtering date and data format conversions regex operations like code matching replacing pattern matching Environment Python development MS SQL C Education MS in Computer Science Western Illinois University Additional Information SKILLS AWS Real time working knowledge on AWS services like EMR clusters EC2 Security Configurations storing data in S3 Querying the tables using Athena Presto HQL Databases like AWS RedShift Snowflake Apache Cassandra implementing JDBC connectors to AWS AWS CodeBuild and AWS CodePipeline for Continous Integration and Continous Deployement Hadoop ecosystems Experience on HDFS environment and Big data technologies like Apache Spark Apache Hive Sqoop Apache Kafka Flume NoSQL database like Apache HBase writing scripts in Apache Pig Working with file formats textavrojsonparquet and file compressions gzipsnappyuncompressed Good understanding of Hadoop components like HDFS Hadoop MapReduce and YARN Implemented batch processing live streaming using Kafka and spark streaming using pysparkscala Automating workflow of jobs using Apache Oozie through cli and GUI Tableau Proficient with creating interactive dashboards in Tableau accessing data from multiple sources Created custom SQL queries for liveextract connections in Tableau Worked and maintained Tableau online such as creating projects workbooks views user access privileges Experience on calculated fields KPIs creating views charts plots and trees Worked on saving in different formats joins blends unions pivot tables multisource connectors SQL Creating data models creating and maintaining data lakes databases Creating joins indexes clusters creating constraints stored procedures DRL DDL and DML Experience of migrating data from traditional databases like MSSQL to AWS RedShift Maven Projects Worked on creating Spark jobs Java Maven projects with Drools engine MS SQL Developed stateful and stateless KIE sessions working with relative dependencies and plugins for pomxml Python Developed my own Python Framework worked on APIs and developed web applications on Django Experience of working with HTML5 CSS3 Bootstrap DOM Worked on date format file formats string manipulations file and directory accesses different file formats OS and IO level operations concurrent execution internet protocols libraries etc Version Control and SDLC Knowledge of version control in Git GitHub worked in Agile environments Committing moving and removing files checkout branch and merge",
    "unique_id": "dab7da20-3223-429e-a007-edc9931027a2"
}