{
    "clean_data": "Hadoop Developer Hadoop span lDeveloperspan Hadoop Developer Dallas TX Experience around 9 years in IT industry with complete software development of life cycle SDLC which includes business requirements gathering system analysis design data modeling development testing and implementation of the projects Experienced in configuration deployments and managing of different Hadoop distributions like Cloudera CDH4 CDH5 and Hortonworks HDP Experience of importexport data using Sqoop from Hadoop distributed file systems to relational database systems and vice versa Experience in handling various file formats like AVRO Sequential text xml JSON and Parquet with different compression techniques such as gzip LZO Snappy etc Experienced on Spark Architecture including Spark Core Spark SQL Data Frames Spark Streaming and Spark MLlib Imported the data from source HDFS into Spark Data Frame for inmemory data computation to generate the optimized output response and better visualizations Expertise in writing Spark RDD transformations actions Data Frames case classes for the required input data and performed the data transformations using SparkCore also convert RRD to Data Frame Experienced on collection the real time streaming data and creating the pipeline for row data from different source using Kafka and store data into HDFS and NoSQL using Spark Extending HIVE core functionality by using custom User Defined Functions UDF and User Defined Aggregating Functions UDAF Implemented POC for using Impala for data processing on top of HIVE for better utilization of C executions engines Experience in NoSQL Databases HBase Cassandra and its integrated with Hadoop cluster Implemented Cluster for NoSQL tools HBase as a part of POC to address HBase limitations Exploring with Spark Beta version API to improve the performance and optimization of the existing algorithms with different modes such as YARN Mesos and standalone for POC Expertise in using ETL Tool Informatica Power Center designer workflow manager repository manager data quality and ETL concepts Experienced with NiFi to automate the data movement between different Hadoop systems Worked with different Hadoop Security such as Knox and Ranger integrated LDAP store with Kerberos KDC Good understanding on security requirements for Hadoop and integrate with Kerberos authentication and authorization infrastructure Experienced on cloud integration with AWS using Elastic Map Reduce EMR Simple Storage Service S3 EC2 Redshift and Microsoft Azure Experienced on different Relational Data Base Management Systems like Teradata PostgreSQL DB2 Oracle and SQL Server Experienced in scheduling and monitoring the production jobs using Oozie and Azkaban Authorized to work in the US for any employer Work Experience Hadoop Developer Quant Systems Irving TX US October 2016 to March 2018 Responsibilities Involved in installation configuration and Design of Hadoop Distributed using Cloudera and Hortonworks of Hadoop Involved in complete Big Data flow of the application data ingestion from upstream to HDFS processing the data in HDFS and analyzing the data using several tools Imported the data from various formats like JSON Sequential Text CSV AVRO and Parquet to HDFS cluster with compressed for optimization Experienced on ingesting data from RDBMS sources like Oracle SQL Server and Teradata into HDFS using Sqoop Configured Hive and written Hive UDFs and UDAFs Also created partitions such as Static and Dynamic with bucketing Implemented advanced procedures like text analytics and processing using the inmemory computing capabilities like spark Importing and exporting data into HDFS and hive using Sqoop and Kafka with batch and streaming Experienced with SparkStreaming APIs to perform transformations and actions on the fly for building the common learner data model which gets the data from Kafka in near real time and Persists into HBase Performance analysis of Spark streaming and batch jobs by using Spark tuning parameters Enhanced and optimized product Spark code to aggregate group and run data mining tasks using the Spark framework Developed Spark scripts by using Python shell commands as per the requirement Using Hive join queries to join multiple tables of a source system and load them to Elastic search tables Experience in managing and reviewing huge Hadoop log files Expertise in designing and creating various analytical reports and Automated Dashboards to help users to identify critical KPIs and facilitate strategic planning in the organization Involved in Cluster maintenance Cluster Monitoring and Troubleshooting Created Data Pipelines as per the business requirements and scheduled it using Oozie Coordinators Maintaining technical documentation for each and every step of development environment and launching Hadoop clusters Built the automated build and deployment framework using GitHub and Maven etc Worked on IntelliJ IDEA to develop the code and dubbing Worked on BI tools as Tableau to create dashboards like weekly monthly daily reports using tableau desktop and publish them to HDFS cluster Environment Scala Hadoop HDFS Hive Oozie Sqoop NiFi Spark Kafka Elastic Search Shell Scripting HBase Python GitHub Tableau Oracle MySQL Teradata and AWS Hadoop Developer ReqRoute Inc Milpitas CA US August 2014 to October 2016 Responsibilities Involved in requirement gathered narrates the stories and worked with complete Software Development Life Cycle SDLC methodologies based on Agile Involved in installation configuration supporting and managing Hadoop Clusters using Hortonworks Distribution HDP to Cloudera Distributions Hadoop CDH Worked on Hadoop MapReduce HDFS developed multiple MapReduce jobs in java for data cleaning and preprocessing Experienced in managing and reviewing Hadoop log files and documenting the issues on daily basis to the resolution portal Implemented Dynamic Partitions Buckets in HIVE Involved in converting HiveSQL queries into Spark transformations using Spark RDDs Scala and Python Experience configuring spouts and bolts in various Storm topologies and validating data in the bolts Experienced in running Hadoop streaming jobs to process terabytes of xml format data Have an experience to load and transform large sets of structured semi structured and unstructured data using Sqoop from Hadoop Distributed File Systems to Relational Database Systems and also Relational Database Systems to Hadoop Distributed File Systems Used Spark streaming to receive real time data from the Kafka and store the stream data to HDFS using Scala and databases such as HBase Establishedimplemented firewall rules validated rules with vulnerability scanning tools Used Oozie workflow engine to manage interdependent Hadoop jobs and to automate several types of Hadoop jobs such as Java MapReduce Hive Pig and Sqoop Implemented Storm builder topologies to perform cleansing operations before moving data into HBase Involved in creating Hive tables loading with data and writing hive queries which will run internally in map reduce way Developed a custom File System plug in for Hadoop so it can access files on Data Platform Used Spark to create APIs in Java and Python for Big Data analysis Experience in troubleshooting errors in Cassandra Hive and MapReduce This plugin allows Hadoop MapReduce programs Cassandra Pig and Hive to work unmodified and access files directly Used versions controls tools such as GitHub to pull data from Upstream to local branch check conflict cleaning also reviewing the codes of other developers Involved with development teams to discuss JIRA stories and understand the requirements Actively involved in complete life cycle of agile methodology to design develop deploy and support solutions Environment Hadoop Hive Pig Strom Cassandra Sqoop Impala Oozie Java Python Shell Scripting MapReduce Java Collection MySQL Java Developer Netserv Applications Inc Alpharetta GA US June 2012 to August 2014 Responsibilities Responsible and active in the analysis design implementation and deployment of full Software Development Lifecycle SDLC of the project Designed and developed user interface using JSP HTML and JavaScript Defined the search criteria and pulled out the record of the customer from the database Make the required changes and save the updated record back to the database Validated the fields of user registration screen and login screen by writing JavaScript and jQuery validations Used DAO and JDBC for database access Developed stored procedures and triggers using PLSQL in order to calculate and update the tables to implement business logic Design and develop XML processing components for dynamic menus on the application Involved in postproduction support and maintenance of the application Involved in the analysis design implementation and testing of the project modules Implemented the presentation layer with HTML XHTML and JavaScript Developed web components using JSP and JDBC Implemented database using SQL Server Designed tables and indexes Wrote complex SQL queries and stored procedures Involved in fixing bugs and unit testing with test cases using JUnit Created user and technical documentation Environment Java Oracle HTML XML SQL J2EE JUnit JDBC JSP Tomcat SQL Server MongoDB JavaScript GitHub SourceTree NetBeans Java Developer DHI Group Inc Greenwood Village CO US April 2009 to June 2012 Responsibilities Analyzing and preparing the requirement Analysis Document Deploying the Application to the JBOSS Application Server Requirement gatherings from various stakeholders of the project Effortestimation and estimating timelines for development tasks Used to J2EE and EJB to handle the business flow and Functionality Interact with Client to get the confirmation on the functionalities and implementation Involved in the complete SDLC of the Development with full system dependency Actively coordinated with deployment manager for application production launch Provide Support and update for the period under warranty Produce detailed lowlevel designs from high level design Specifications for components of low level complexity Develops builds and unit tests components of low level Complexity from detailed lowlevel designs Developed user and technical documentation Monitoring of test cases to verify actual results against expected results Performed Functional User Interface test and Regression Test Carrying out Regression testing to track the problem tracking Implemented Model View Controller MVC architecture at the Web tier level to isolate each layer of the application to avoid the complexity of integration and ease of maintenance along with Validation Framework Environment Java JEE CSS HTML SVN EJB UNIX XML Work Flow MyEclipse JMS JIRA Oracle JBOSS Education Bachelors",
    "entities": [
        "Hadoop Clusters",
        "Oracle SQL Server",
        "Produce",
        "Work Experience Hadoop Developer Quant Systems",
        "Informatica Power Center",
        "Relational Data Base Management Systems",
        "SparkStreaming",
        "Troubleshooting Created Data Pipelines",
        "BI",
        "HDFS",
        "Performed Functional User Interface",
        "Hadoop Developer Hadoop",
        "Developed Spark",
        "NiFi",
        "Regression Test Carrying out Regression",
        "UDAFs",
        "Hadoop",
        "XML",
        "JSON Sequential Text CSV",
        "HBase Involved",
        "AWS Hadoop Developer ReqRoute Inc Milpitas",
        "HBase",
        "File System",
        "SQL Server",
        "Oozie Coordinators Maintaining",
        "Kerberos",
        "Dallas",
        "Hadoop MapReduce",
        "Client",
        "SparkCore",
        "Hadoop Distributed File Systems",
        "Spark MLlib Imported",
        "JSP",
        "DHI Group",
        "GitHub Tableau Oracle",
        "Data Frame Experienced",
        "Worked",
        "Spark",
        "EJB",
        "API",
        "US",
        "Sqoop",
        "HIVE",
        "Spark Data Frame",
        "Storm",
        "Spark Core Spark",
        "AWS",
        "Oracle",
        "Agile Involved",
        "Hadoop Distributed File Systems Used Spark",
        "JUnit Created",
        "IDEA",
        "java",
        "Implemented Dynamic Partitions Buckets",
        "SQL",
        "Spark RDD",
        "GitHub",
        "Teradata PostgreSQL DB2",
        "Relational Database Systems",
        "Validation Framework",
        "JDBC Implemented",
        "Oozie and Azkaban Authorized",
        "AVRO Sequential",
        "CDH5",
        "Cloudera Distributions Hadoop",
        "Big Data",
        "RRD",
        "Functionality Interact",
        "ETL",
        "Data Frames",
        "Maven",
        "Hortonworks HDP",
        "Impala",
        "JavaScript",
        "Design of Hadoop",
        "SQL Server Experienced",
        "Spark Beta",
        "Upstream",
        "Microsoft",
        "LZO",
        "SVN",
        "jQuery",
        "Spark Extending",
        "MapReduce",
        "NoSQL",
        "Spark Architecture",
        "Tableau",
        "Software Development Life Cycle",
        "Hadoop Security"
    ],
    "experience": "Experience around 9 years in IT industry with complete software development of life cycle SDLC which includes business requirements gathering system analysis design data modeling development testing and implementation of the projects Experienced in configuration deployments and managing of different Hadoop distributions like Cloudera CDH4 CDH5 and Hortonworks HDP Experience of importexport data using Sqoop from Hadoop distributed file systems to relational database systems and vice versa Experience in handling various file formats like AVRO Sequential text xml JSON and Parquet with different compression techniques such as gzip LZO Snappy etc Experienced on Spark Architecture including Spark Core Spark SQL Data Frames Spark Streaming and Spark MLlib Imported the data from source HDFS into Spark Data Frame for inmemory data computation to generate the optimized output response and better visualizations Expertise in writing Spark RDD transformations actions Data Frames case classes for the required input data and performed the data transformations using SparkCore also convert RRD to Data Frame Experienced on collection the real time streaming data and creating the pipeline for row data from different source using Kafka and store data into HDFS and NoSQL using Spark Extending HIVE core functionality by using custom User Defined Functions UDF and User Defined Aggregating Functions UDAF Implemented POC for using Impala for data processing on top of HIVE for better utilization of C executions engines Experience in NoSQL Databases HBase Cassandra and its integrated with Hadoop cluster Implemented Cluster for NoSQL tools HBase as a part of POC to address HBase limitations Exploring with Spark Beta version API to improve the performance and optimization of the existing algorithms with different modes such as YARN Mesos and standalone for POC Expertise in using ETL Tool Informatica Power Center designer workflow manager repository manager data quality and ETL concepts Experienced with NiFi to automate the data movement between different Hadoop systems Worked with different Hadoop Security such as Knox and Ranger integrated LDAP store with Kerberos KDC Good understanding on security requirements for Hadoop and integrate with Kerberos authentication and authorization infrastructure Experienced on cloud integration with AWS using Elastic Map Reduce EMR Simple Storage Service S3 EC2 Redshift and Microsoft Azure Experienced on different Relational Data Base Management Systems like Teradata PostgreSQL DB2 Oracle and SQL Server Experienced in scheduling and monitoring the production jobs using Oozie and Azkaban Authorized to work in the US for any employer Work Experience Hadoop Developer Quant Systems Irving TX US October 2016 to March 2018 Responsibilities Involved in installation configuration and Design of Hadoop Distributed using Cloudera and Hortonworks of Hadoop Involved in complete Big Data flow of the application data ingestion from upstream to HDFS processing the data in HDFS and analyzing the data using several tools Imported the data from various formats like JSON Sequential Text CSV AVRO and Parquet to HDFS cluster with compressed for optimization Experienced on ingesting data from RDBMS sources like Oracle SQL Server and Teradata into HDFS using Sqoop Configured Hive and written Hive UDFs and UDAFs Also created partitions such as Static and Dynamic with bucketing Implemented advanced procedures like text analytics and processing using the inmemory computing capabilities like spark Importing and exporting data into HDFS and hive using Sqoop and Kafka with batch and streaming Experienced with SparkStreaming APIs to perform transformations and actions on the fly for building the common learner data model which gets the data from Kafka in near real time and Persists into HBase Performance analysis of Spark streaming and batch jobs by using Spark tuning parameters Enhanced and optimized product Spark code to aggregate group and run data mining tasks using the Spark framework Developed Spark scripts by using Python shell commands as per the requirement Using Hive join queries to join multiple tables of a source system and load them to Elastic search tables Experience in managing and reviewing huge Hadoop log files Expertise in designing and creating various analytical reports and Automated Dashboards to help users to identify critical KPIs and facilitate strategic planning in the organization Involved in Cluster maintenance Cluster Monitoring and Troubleshooting Created Data Pipelines as per the business requirements and scheduled it using Oozie Coordinators Maintaining technical documentation for each and every step of development environment and launching Hadoop clusters Built the automated build and deployment framework using GitHub and Maven etc Worked on IntelliJ IDEA to develop the code and dubbing Worked on BI tools as Tableau to create dashboards like weekly monthly daily reports using tableau desktop and publish them to HDFS cluster Environment Scala Hadoop HDFS Hive Oozie Sqoop NiFi Spark Kafka Elastic Search Shell Scripting HBase Python GitHub Tableau Oracle MySQL Teradata and AWS Hadoop Developer ReqRoute Inc Milpitas CA US August 2014 to October 2016 Responsibilities Involved in requirement gathered narrates the stories and worked with complete Software Development Life Cycle SDLC methodologies based on Agile Involved in installation configuration supporting and managing Hadoop Clusters using Hortonworks Distribution HDP to Cloudera Distributions Hadoop CDH Worked on Hadoop MapReduce HDFS developed multiple MapReduce jobs in java for data cleaning and preprocessing Experienced in managing and reviewing Hadoop log files and documenting the issues on daily basis to the resolution portal Implemented Dynamic Partitions Buckets in HIVE Involved in converting HiveSQL queries into Spark transformations using Spark RDDs Scala and Python Experience configuring spouts and bolts in various Storm topologies and validating data in the bolts Experienced in running Hadoop streaming jobs to process terabytes of xml format data Have an experience to load and transform large sets of structured semi structured and unstructured data using Sqoop from Hadoop Distributed File Systems to Relational Database Systems and also Relational Database Systems to Hadoop Distributed File Systems Used Spark streaming to receive real time data from the Kafka and store the stream data to HDFS using Scala and databases such as HBase Establishedimplemented firewall rules validated rules with vulnerability scanning tools Used Oozie workflow engine to manage interdependent Hadoop jobs and to automate several types of Hadoop jobs such as Java MapReduce Hive Pig and Sqoop Implemented Storm builder topologies to perform cleansing operations before moving data into HBase Involved in creating Hive tables loading with data and writing hive queries which will run internally in map reduce way Developed a custom File System plug in for Hadoop so it can access files on Data Platform Used Spark to create APIs in Java and Python for Big Data analysis Experience in troubleshooting errors in Cassandra Hive and MapReduce This plugin allows Hadoop MapReduce programs Cassandra Pig and Hive to work unmodified and access files directly Used versions controls tools such as GitHub to pull data from Upstream to local branch check conflict cleaning also reviewing the codes of other developers Involved with development teams to discuss JIRA stories and understand the requirements Actively involved in complete life cycle of agile methodology to design develop deploy and support solutions Environment Hadoop Hive Pig Strom Cassandra Sqoop Impala Oozie Java Python Shell Scripting MapReduce Java Collection MySQL Java Developer Netserv Applications Inc Alpharetta GA US June 2012 to August 2014 Responsibilities Responsible and active in the analysis design implementation and deployment of full Software Development Lifecycle SDLC of the project Designed and developed user interface using JSP HTML and JavaScript Defined the search criteria and pulled out the record of the customer from the database Make the required changes and save the updated record back to the database Validated the fields of user registration screen and login screen by writing JavaScript and jQuery validations Used DAO and JDBC for database access Developed stored procedures and triggers using PLSQL in order to calculate and update the tables to implement business logic Design and develop XML processing components for dynamic menus on the application Involved in postproduction support and maintenance of the application Involved in the analysis design implementation and testing of the project modules Implemented the presentation layer with HTML XHTML and JavaScript Developed web components using JSP and JDBC Implemented database using SQL Server Designed tables and indexes Wrote complex SQL queries and stored procedures Involved in fixing bugs and unit testing with test cases using JUnit Created user and technical documentation Environment Java Oracle HTML XML SQL J2EE JUnit JDBC JSP Tomcat SQL Server MongoDB JavaScript GitHub SourceTree NetBeans Java Developer DHI Group Inc Greenwood Village CO US April 2009 to June 2012 Responsibilities Analyzing and preparing the requirement Analysis Document Deploying the Application to the JBOSS Application Server Requirement gatherings from various stakeholders of the project Effortestimation and estimating timelines for development tasks Used to J2EE and EJB to handle the business flow and Functionality Interact with Client to get the confirmation on the functionalities and implementation Involved in the complete SDLC of the Development with full system dependency Actively coordinated with deployment manager for application production launch Provide Support and update for the period under warranty Produce detailed lowlevel designs from high level design Specifications for components of low level complexity Develops builds and unit tests components of low level Complexity from detailed lowlevel designs Developed user and technical documentation Monitoring of test cases to verify actual results against expected results Performed Functional User Interface test and Regression Test Carrying out Regression testing to track the problem tracking Implemented Model View Controller MVC architecture at the Web tier level to isolate each layer of the application to avoid the complexity of integration and ease of maintenance along with Validation Framework Environment Java JEE CSS HTML SVN EJB UNIX XML Work Flow MyEclipse JMS JIRA Oracle JBOSS Education Bachelors",
    "extracted_keywords": [
        "Hadoop",
        "Developer",
        "Hadoop",
        "span",
        "lDeveloperspan",
        "Hadoop",
        "Developer",
        "Dallas",
        "TX",
        "Experience",
        "years",
        "IT",
        "industry",
        "software",
        "development",
        "life",
        "cycle",
        "SDLC",
        "business",
        "requirements",
        "system",
        "analysis",
        "design",
        "data",
        "development",
        "testing",
        "implementation",
        "projects",
        "configuration",
        "deployments",
        "managing",
        "Hadoop",
        "distributions",
        "Cloudera",
        "CDH4",
        "CDH5",
        "Hortonworks",
        "HDP",
        "Experience",
        "importexport",
        "data",
        "Sqoop",
        "Hadoop",
        "file",
        "systems",
        "database",
        "systems",
        "vice",
        "versa",
        "Experience",
        "file",
        "formats",
        "AVRO",
        "Sequential",
        "text",
        "xml",
        "JSON",
        "Parquet",
        "compression",
        "techniques",
        "gzip",
        "LZO",
        "Snappy",
        "Spark",
        "Architecture",
        "Spark",
        "Core",
        "Spark",
        "SQL",
        "Data",
        "Frames",
        "Spark",
        "Streaming",
        "Spark",
        "MLlib",
        "data",
        "source",
        "HDFS",
        "Spark",
        "Data",
        "Frame",
        "data",
        "computation",
        "output",
        "response",
        "visualizations",
        "Expertise",
        "Spark",
        "RDD",
        "transformations",
        "actions",
        "Data",
        "Frames",
        "case",
        "classes",
        "input",
        "data",
        "data",
        "transformations",
        "SparkCore",
        "RRD",
        "Data",
        "Frame",
        "collection",
        "time",
        "data",
        "pipeline",
        "row",
        "data",
        "source",
        "Kafka",
        "data",
        "HDFS",
        "NoSQL",
        "Spark",
        "HIVE",
        "core",
        "functionality",
        "custom",
        "User",
        "Defined",
        "Functions",
        "UDF",
        "User",
        "Defined",
        "Aggregating",
        "Functions",
        "UDAF",
        "POC",
        "Impala",
        "data",
        "processing",
        "top",
        "HIVE",
        "utilization",
        "C",
        "executions",
        "engines",
        "NoSQL",
        "HBase",
        "Cassandra",
        "Hadoop",
        "cluster",
        "Cluster",
        "NoSQL",
        "tools",
        "HBase",
        "part",
        "POC",
        "HBase",
        "limitations",
        "Spark",
        "Beta",
        "version",
        "API",
        "performance",
        "optimization",
        "algorithms",
        "modes",
        "YARN",
        "Mesos",
        "POC",
        "Expertise",
        "ETL",
        "Tool",
        "Informatica",
        "Power",
        "Center",
        "designer",
        "workflow",
        "manager",
        "repository",
        "manager",
        "data",
        "quality",
        "ETL",
        "concepts",
        "NiFi",
        "data",
        "movement",
        "Hadoop",
        "systems",
        "Hadoop",
        "Security",
        "Knox",
        "Ranger",
        "LDAP",
        "store",
        "Kerberos",
        "KDC",
        "understanding",
        "security",
        "requirements",
        "Hadoop",
        "authentication",
        "authorization",
        "infrastructure",
        "integration",
        "AWS",
        "Elastic",
        "Map",
        "EMR",
        "Simple",
        "Storage",
        "Service",
        "S3",
        "EC2",
        "Redshift",
        "Microsoft",
        "Azure",
        "Relational",
        "Data",
        "Base",
        "Management",
        "Systems",
        "Teradata",
        "PostgreSQL",
        "DB2",
        "Oracle",
        "SQL",
        "Server",
        "scheduling",
        "production",
        "jobs",
        "Oozie",
        "Azkaban",
        "Authorized",
        "US",
        "employer",
        "Work",
        "Experience",
        "Hadoop",
        "Developer",
        "Quant",
        "Systems",
        "Irving",
        "TX",
        "US",
        "October",
        "March",
        "Responsibilities",
        "installation",
        "configuration",
        "Design",
        "Hadoop",
        "Cloudera",
        "Hortonworks",
        "Hadoop",
        "Big",
        "Data",
        "flow",
        "application",
        "data",
        "ingestion",
        "HDFS",
        "data",
        "HDFS",
        "data",
        "tools",
        "data",
        "formats",
        "JSON",
        "Sequential",
        "Text",
        "CSV",
        "AVRO",
        "Parquet",
        "HDFS",
        "cluster",
        "optimization",
        "data",
        "sources",
        "Oracle",
        "SQL",
        "Server",
        "Teradata",
        "HDFS",
        "Sqoop",
        "Configured",
        "Hive",
        "Hive",
        "UDFs",
        "UDAFs",
        "partitions",
        "Static",
        "Dynamic",
        "procedures",
        "text",
        "analytics",
        "processing",
        "inmemory",
        "computing",
        "capabilities",
        "spark",
        "Importing",
        "data",
        "HDFS",
        "hive",
        "Sqoop",
        "Kafka",
        "batch",
        "streaming",
        "SparkStreaming",
        "APIs",
        "transformations",
        "actions",
        "fly",
        "learner",
        "data",
        "model",
        "data",
        "Kafka",
        "time",
        "Persists",
        "HBase",
        "Performance",
        "analysis",
        "Spark",
        "streaming",
        "batch",
        "jobs",
        "Spark",
        "parameters",
        "product",
        "Spark",
        "code",
        "group",
        "data",
        "mining",
        "tasks",
        "Spark",
        "framework",
        "Developed",
        "Spark",
        "scripts",
        "Python",
        "shell",
        "commands",
        "requirement",
        "Hive",
        "join",
        "queries",
        "tables",
        "source",
        "system",
        "search",
        "tables",
        "Experience",
        "Hadoop",
        "log",
        "Expertise",
        "reports",
        "Automated",
        "Dashboards",
        "users",
        "KPIs",
        "planning",
        "organization",
        "Cluster",
        "maintenance",
        "Cluster",
        "Monitoring",
        "Troubleshooting",
        "Created",
        "Data",
        "Pipelines",
        "business",
        "requirements",
        "Oozie",
        "Coordinators",
        "documentation",
        "step",
        "development",
        "environment",
        "Hadoop",
        "clusters",
        "build",
        "deployment",
        "framework",
        "GitHub",
        "Maven",
        "IntelliJ",
        "IDEA",
        "code",
        "Worked",
        "BI",
        "tools",
        "Tableau",
        "dashboards",
        "reports",
        "tableau",
        "desktop",
        "HDFS",
        "cluster",
        "Environment",
        "Scala",
        "Hadoop",
        "HDFS",
        "Hive",
        "Oozie",
        "Sqoop",
        "NiFi",
        "Spark",
        "Kafka",
        "Elastic",
        "Search",
        "Shell",
        "Scripting",
        "HBase",
        "Python",
        "GitHub",
        "Tableau",
        "Oracle",
        "MySQL",
        "Teradata",
        "AWS",
        "Hadoop",
        "Developer",
        "ReqRoute",
        "Inc",
        "Milpitas",
        "CA",
        "US",
        "August",
        "October",
        "Responsibilities",
        "requirement",
        "narrates",
        "stories",
        "Software",
        "Development",
        "Life",
        "Cycle",
        "SDLC",
        "methodologies",
        "Agile",
        "installation",
        "configuration",
        "Hadoop",
        "Clusters",
        "Hortonworks",
        "Distribution",
        "HDP",
        "Cloudera",
        "Distributions",
        "Hadoop",
        "CDH",
        "Hadoop",
        "MapReduce",
        "MapReduce",
        "jobs",
        "java",
        "data",
        "Hadoop",
        "log",
        "files",
        "issues",
        "basis",
        "resolution",
        "Implemented",
        "Dynamic",
        "Partitions",
        "Buckets",
        "HIVE",
        "HiveSQL",
        "queries",
        "Spark",
        "transformations",
        "Spark",
        "RDDs",
        "Scala",
        "Python",
        "Experience",
        "spouts",
        "bolts",
        "Storm",
        "topologies",
        "data",
        "bolts",
        "Hadoop",
        "streaming",
        "jobs",
        "terabytes",
        "xml",
        "format",
        "data",
        "experience",
        "sets",
        "data",
        "Sqoop",
        "Hadoop",
        "Distributed",
        "File",
        "Systems",
        "Relational",
        "Database",
        "Systems",
        "Relational",
        "Database",
        "Systems",
        "Hadoop",
        "Distributed",
        "File",
        "Systems",
        "Spark",
        "streaming",
        "time",
        "data",
        "Kafka",
        "stream",
        "data",
        "HDFS",
        "Scala",
        "databases",
        "HBase",
        "Establishedimplemented",
        "firewall",
        "rules",
        "rules",
        "vulnerability",
        "tools",
        "Oozie",
        "workflow",
        "engine",
        "Hadoop",
        "jobs",
        "types",
        "Hadoop",
        "jobs",
        "Java",
        "MapReduce",
        "Hive",
        "Pig",
        "Sqoop",
        "Storm",
        "builder",
        "topologies",
        "cleansing",
        "operations",
        "data",
        "HBase",
        "Hive",
        "tables",
        "data",
        "hive",
        "queries",
        "map",
        "way",
        "custom",
        "File",
        "System",
        "plug",
        "Hadoop",
        "files",
        "Data",
        "Platform",
        "Spark",
        "APIs",
        "Java",
        "Python",
        "Big",
        "Data",
        "analysis",
        "Experience",
        "troubleshooting",
        "errors",
        "Cassandra",
        "Hive",
        "MapReduce",
        "plugin",
        "Hadoop",
        "MapReduce",
        "programs",
        "Cassandra",
        "Pig",
        "Hive",
        "access",
        "files",
        "versions",
        "controls",
        "tools",
        "GitHub",
        "data",
        "Upstream",
        "branch",
        "check",
        "conflict",
        "cleaning",
        "codes",
        "developers",
        "development",
        "teams",
        "JIRA",
        "stories",
        "requirements",
        "life",
        "cycle",
        "methodology",
        "deploy",
        "support",
        "solutions",
        "Environment",
        "Hadoop",
        "Hive",
        "Pig",
        "Strom",
        "Cassandra",
        "Sqoop",
        "Impala",
        "Oozie",
        "Java",
        "Python",
        "Shell",
        "Scripting",
        "MapReduce",
        "Java",
        "Collection",
        "MySQL",
        "Java",
        "Developer",
        "Netserv",
        "Applications",
        "Inc",
        "Alpharetta",
        "GA",
        "US",
        "June",
        "August",
        "Responsibilities",
        "analysis",
        "design",
        "implementation",
        "deployment",
        "Software",
        "Development",
        "Lifecycle",
        "SDLC",
        "project",
        "user",
        "interface",
        "JSP",
        "HTML",
        "JavaScript",
        "search",
        "criteria",
        "record",
        "customer",
        "database",
        "changes",
        "record",
        "database",
        "fields",
        "user",
        "registration",
        "screen",
        "login",
        "screen",
        "JavaScript",
        "jQuery",
        "validations",
        "DAO",
        "JDBC",
        "database",
        "access",
        "procedures",
        "triggers",
        "PLSQL",
        "order",
        "tables",
        "business",
        "logic",
        "Design",
        "XML",
        "processing",
        "components",
        "menus",
        "application",
        "postproduction",
        "support",
        "maintenance",
        "application",
        "analysis",
        "design",
        "implementation",
        "testing",
        "project",
        "modules",
        "presentation",
        "layer",
        "HTML",
        "XHTML",
        "JavaScript",
        "Developed",
        "web",
        "components",
        "JSP",
        "JDBC",
        "database",
        "SQL",
        "Server",
        "tables",
        "indexes",
        "Wrote",
        "SQL",
        "queries",
        "procedures",
        "bugs",
        "unit",
        "testing",
        "test",
        "cases",
        "JUnit",
        "Created",
        "user",
        "documentation",
        "Environment",
        "Java",
        "Oracle",
        "HTML",
        "XML",
        "SQL",
        "J2EE",
        "JUnit",
        "JDBC",
        "JSP",
        "Tomcat",
        "SQL",
        "Server",
        "MongoDB",
        "JavaScript",
        "GitHub",
        "SourceTree",
        "NetBeans",
        "Java",
        "Developer",
        "DHI",
        "Group",
        "Inc",
        "Greenwood",
        "Village",
        "CO",
        "US",
        "April",
        "June",
        "Responsibilities",
        "requirement",
        "Analysis",
        "Document",
        "Application",
        "JBOSS",
        "Application",
        "Server",
        "Requirement",
        "gatherings",
        "stakeholders",
        "project",
        "Effortestimation",
        "timelines",
        "development",
        "tasks",
        "J2EE",
        "EJB",
        "business",
        "flow",
        "Functionality",
        "Interact",
        "Client",
        "confirmation",
        "functionalities",
        "implementation",
        "SDLC",
        "Development",
        "system",
        "dependency",
        "deployment",
        "manager",
        "application",
        "production",
        "launch",
        "Support",
        "period",
        "warranty",
        "lowlevel",
        "designs",
        "level",
        "design",
        "Specifications",
        "components",
        "level",
        "complexity",
        "Develops",
        "builds",
        "unit",
        "components",
        "level",
        "Complexity",
        "lowlevel",
        "designs",
        "user",
        "documentation",
        "Monitoring",
        "test",
        "cases",
        "results",
        "results",
        "Performed",
        "Functional",
        "User",
        "Interface",
        "test",
        "Regression",
        "Test",
        "Regression",
        "testing",
        "problem",
        "Model",
        "View",
        "Controller",
        "MVC",
        "architecture",
        "Web",
        "tier",
        "level",
        "layer",
        "application",
        "complexity",
        "integration",
        "ease",
        "maintenance",
        "Validation",
        "Framework",
        "Environment",
        "Java",
        "JEE",
        "CSS",
        "HTML",
        "SVN",
        "EJB",
        "UNIX",
        "XML",
        "Work",
        "Flow",
        "MyEclipse",
        "JMS",
        "JIRA",
        "Oracle",
        "JBOSS",
        "Education",
        "Bachelors"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T21:31:02.577688",
    "resume_data": "Hadoop Developer Hadoop span lDeveloperspan Hadoop Developer Dallas TX Experience around 9 years in IT industry with complete software development of life cycle SDLC which includes business requirements gathering system analysis design data modeling development testing and implementation of the projects Experienced in configuration deployments and managing of different Hadoop distributions like Cloudera CDH4 CDH5 and Hortonworks HDP Experience of importexport data using Sqoop from Hadoop distributed file systems to relational database systems and vice versa Experience in handling various file formats like AVRO Sequential text xml JSON and Parquet with different compression techniques such as gzip LZO Snappy etc Experienced on Spark Architecture including Spark Core Spark SQL Data Frames Spark Streaming and Spark MLlib Imported the data from source HDFS into Spark Data Frame for inmemory data computation to generate the optimized output response and better visualizations Expertise in writing Spark RDD transformations actions Data Frames case classes for the required input data and performed the data transformations using SparkCore also convert RRD to Data Frame Experienced on collection the real time streaming data and creating the pipeline for row data from different source using Kafka and store data into HDFS and NoSQL using Spark Extending HIVE core functionality by using custom User Defined Functions UDF and User Defined Aggregating Functions UDAF Implemented POC for using Impala for data processing on top of HIVE for better utilization of C executions engines Experience in NoSQL Databases HBase Cassandra and its integrated with Hadoop cluster Implemented Cluster for NoSQL tools HBase as a part of POC to address HBase limitations Exploring with Spark Beta version API to improve the performance and optimization of the existing algorithms with different modes such as YARN Mesos and standalone for POC Expertise in using ETL Tool Informatica Power Center designer workflow manager repository manager data quality and ETL concepts Experienced with NiFi to automate the data movement between different Hadoop systems Worked with different Hadoop Security such as Knox and Ranger integrated LDAP store with Kerberos KDC Good understanding on security requirements for Hadoop and integrate with Kerberos authentication and authorization infrastructure Experienced on cloud integration with AWS using Elastic Map Reduce EMR Simple Storage Service S3 EC2 Redshift and Microsoft Azure Experienced on different Relational Data Base Management Systems like Teradata PostgreSQL DB2 Oracle and SQL Server Experienced in scheduling and monitoring the production jobs using Oozie and Azkaban Authorized to work in the US for any employer Work Experience Hadoop Developer Quant Systems Irving TX US October 2016 to March 2018 Responsibilities Involved in installation configuration and Design of Hadoop Distributed using Cloudera and Hortonworks of Hadoop Involved in complete Big Data flow of the application data ingestion from upstream to HDFS processing the data in HDFS and analyzing the data using several tools Imported the data from various formats like JSON Sequential Text CSV AVRO and Parquet to HDFS cluster with compressed for optimization Experienced on ingesting data from RDBMS sources like Oracle SQL Server and Teradata into HDFS using Sqoop Configured Hive and written Hive UDFs and UDAFs Also created partitions such as Static and Dynamic with bucketing Implemented advanced procedures like text analytics and processing using the inmemory computing capabilities like spark Importing and exporting data into HDFS and hive using Sqoop and Kafka with batch and streaming Experienced with SparkStreaming APIs to perform transformations and actions on the fly for building the common learner data model which gets the data from Kafka in near real time and Persists into HBase Performance analysis of Spark streaming and batch jobs by using Spark tuning parameters Enhanced and optimized product Spark code to aggregate group and run data mining tasks using the Spark framework Developed Spark scripts by using Python shell commands as per the requirement Using Hive join queries to join multiple tables of a source system and load them to Elastic search tables Experience in managing and reviewing huge Hadoop log files Expertise in designing and creating various analytical reports and Automated Dashboards to help users to identify critical KPIs and facilitate strategic planning in the organization Involved in Cluster maintenance Cluster Monitoring and Troubleshooting Created Data Pipelines as per the business requirements and scheduled it using Oozie Coordinators Maintaining technical documentation for each and every step of development environment and launching Hadoop clusters Built the automated build and deployment framework using GitHub and Maven etc Worked on IntelliJ IDEA to develop the code and dubbing Worked on BI tools as Tableau to create dashboards like weekly monthly daily reports using tableau desktop and publish them to HDFS cluster Environment Scala Hadoop HDFS Hive Oozie Sqoop NiFi Spark Kafka Elastic Search Shell Scripting HBase Python GitHub Tableau Oracle MySQL Teradata and AWS Hadoop Developer ReqRoute Inc Milpitas CA US August 2014 to October 2016 Responsibilities Involved in requirement gathered narrates the stories and worked with complete Software Development Life Cycle SDLC methodologies based on Agile Involved in installation configuration supporting and managing Hadoop Clusters using Hortonworks Distribution HDP to Cloudera Distributions Hadoop CDH Worked on Hadoop MapReduce HDFS developed multiple MapReduce jobs in java for data cleaning and preprocessing Experienced in managing and reviewing Hadoop log files and documenting the issues on daily basis to the resolution portal Implemented Dynamic Partitions Buckets in HIVE Involved in converting HiveSQL queries into Spark transformations using Spark RDDs Scala and Python Experience configuring spouts and bolts in various Storm topologies and validating data in the bolts Experienced in running Hadoop streaming jobs to process terabytes of xml format data Have an experience to load and transform large sets of structured semi structured and unstructured data using Sqoop from Hadoop Distributed File Systems to Relational Database Systems and also Relational Database Systems to Hadoop Distributed File Systems Used Spark streaming to receive real time data from the Kafka and store the stream data to HDFS using Scala and databases such as HBase Establishedimplemented firewall rules validated rules with vulnerability scanning tools Used Oozie workflow engine to manage interdependent Hadoop jobs and to automate several types of Hadoop jobs such as Java MapReduce Hive Pig and Sqoop Implemented Storm builder topologies to perform cleansing operations before moving data into HBase Involved in creating Hive tables loading with data and writing hive queries which will run internally in map reduce way Developed a custom File System plug in for Hadoop so it can access files on Data Platform Used Spark to create APIs in Java and Python for Big Data analysis Experience in troubleshooting errors in Cassandra Hive and MapReduce This plugin allows Hadoop MapReduce programs Cassandra Pig and Hive to work unmodified and access files directly Used versions controls tools such as GitHub to pull data from Upstream to local branch check conflict cleaning also reviewing the codes of other developers Involved with development teams to discuss JIRA stories and understand the requirements Actively involved in complete life cycle of agile methodology to design develop deploy and support solutions Environment Hadoop Hive Pig Strom Cassandra Sqoop Impala Oozie Java Python Shell Scripting MapReduce Java Collection MySQL Java Developer Netserv Applications Inc Alpharetta GA US June 2012 to August 2014 Responsibilities Responsible and active in the analysis design implementation and deployment of full Software Development Lifecycle SDLC of the project Designed and developed user interface using JSP HTML and JavaScript Defined the search criteria and pulled out the record of the customer from the database Make the required changes and save the updated record back to the database Validated the fields of user registration screen and login screen by writing JavaScript and jQuery validations Used DAO and JDBC for database access Developed stored procedures and triggers using PLSQL in order to calculate and update the tables to implement business logic Design and develop XML processing components for dynamic menus on the application Involved in postproduction support and maintenance of the application Involved in the analysis design implementation and testing of the project modules Implemented the presentation layer with HTML XHTML and JavaScript Developed web components using JSP and JDBC Implemented database using SQL Server Designed tables and indexes Wrote complex SQL queries and stored procedures Involved in fixing bugs and unit testing with test cases using JUnit Created user and technical documentation Environment Java Oracle HTML XML SQL J2EE JUnit JDBC JSP Tomcat SQL Server MongoDB JavaScript GitHub SourceTree NetBeans Java Developer DHI Group Inc Greenwood Village CO US April 2009 to June 2012 Responsibilities Analyzing and preparing the requirement Analysis Document Deploying the Application to the JBOSS Application Server Requirement gatherings from various stakeholders of the project Effortestimation and estimating timelines for development tasks Used to J2EE and EJB to handle the business flow and Functionality Interact with Client to get the confirmation on the functionalities and implementation Involved in the complete SDLC of the Development with full system dependency Actively coordinated with deployment manager for application production launch Provide Support and update for the period under warranty Produce detailed lowlevel designs from high level design Specifications for components of low level complexity Develops builds and unit tests components of low level Complexity from detailed lowlevel designs Developed user and technical documentation Monitoring of test cases to verify actual results against expected results Performed Functional User Interface test and Regression Test Carrying out Regression testing to track the problem tracking Implemented Model View Controller MVC architecture at the Web tier level to isolate each layer of the application to avoid the complexity of integration and ease of maintenance along with Validation Framework Environment Java JEE CSS HTML SVN EJB UNIX XML Work Flow MyEclipse JMS JIRA Oracle JBOSS Education Bachelors",
    "unique_id": "b7fa4fd9-5d0f-4fe5-a033-5993ed886247"
}