{
    "clean_data": "Data Scientist Data Scientist Data Scientist CVS Health Scottsdale AZ Around 9 years of IT experience includes in Data Science Machine Learning Deep Learning NLP Text Mining DataBusiness Analytics Data Visualization Data Operations and BI A deep understanding of Statistical Modelling Multivariate Analysis Bigdata analytics and Standard Procedures Highly efficient in Dimensionality Reduction methods such as PCA Principal component Analysis Factor Analysis etc Implemented bootstrapping methods such as Random Forests Classification KMeans Clustering KNN Nave Bayes SVM Decision Tree BFS Linear and Logistic Regression Methods Experience in text understanding classification pattern recognition recommendation systems targeting systems and ranking systems using Python Strong skills in statistical methodologies such as AB test Experiment design Hypothesis test ANOVA CrossTabs T tests and Correlation Techniques Worked with applications like R SPSS and Python to develop predictive models Experience with Natural Language Processing NLP Extensively worked on Python 3527 Numpy Pandas Matplotlib NLTK and Scikitlearn Experience in implementing data analysis with various analytic tools such as Anaconda 40 Jupyter Notebook 4X R 30ggplot2 and Excel Worked on Tableau to create dashboards and visualizations Extensive experience in Text Analytics developing different Statistical Machine Learning Data Mining solutions to various business problems and generating data visualizations using R Python Power BI and Tableau Hands on experience in business understanding data understanding and preparation of large databases Expertise in transforming business requirements into analytical models designing algorithms building models developing datamining and reporting solutions that scales across massive volume of structured and unstructured data Documenting new data to help source to target mapping Also updating the documentation for existing data assisting with data profiling to maintain data sanitation validation Identifies what data is available and relevant including internal and external data sources leveraging new data collection processes such geolocation information Good understanding on data preparation techniques Experienced working on large volume of Data using BASE SAS programming Proficiency in application of statistical prediction modeling machine learning classification techniques and econometric forecasting techniques Proficiency in various type of optimization Market Mix modeling Segmentation Time Series Price Promo models etc Experience in the application of Neural Network Support Vector Machines SVM and Random Forest Creative thinking and propose innovative ways to look at problems by using data mining approaches on the set of information available Identifiescreates the appropriate algorithm to discover patterns validate their findings using an experimental and iterative approach Applies advanced statistical and predictive modeling techniques to build maintain and improve on multiple realtime decision systems Closely works with product managers Service development managers and product development team in productizing the algorithms developed Experience in designing star schema Snowflake schema for Data Warehouse ODS architecture Experience in designing stunning visualizations using Tableau and Power BI software and publishing and presenting dashboards Storyline on web and desktop platforms Experience in working with relational databases Teradata Oracle with advanced SQL programming skills Indepth knowledge of statistical procedures that are applied in Supervised Unsupervised problems Proficiency in SAS Base SAS Enterprise Guide Enterprise Miner Familiar with graphical models and deep learning models including deep learning frameworks such as TensorFlow Experienced in working with advanced analytical teams to design build validate and refresh data models that enable the next generation of sophisticated solutions for global clients Excellent communication skills verbal and written to communicate with clients and team prepare deliver effective presentations Strong experience in Software Development Life Cycle SDLC including Requirements Analysis Design Specification and Testing as per Cycle in both Waterfall and Agile methodologies Strong experience in interacting with stakeholderscustomers gathering requirements through interviews workshops and existing system documentation or procedures defining business processes identifying and analyzing risks using appropriate templates and analysis tools Mapping and tracing data from system to system in order to establish data hierarchy and lineage Used Data Lineage and reverse engineering as a way to track back errors in data till the data source Authorized to work in the US for any employer Work Experience Data Scientist CVS Health Scottsdale AZ August 2018 to Present CVS Health is an American pharmacy and healthcare company with nearly 10000 stores in its network Its healthfocused business includes pharmacy services retail instore health clinics and its own Digital Innovation Lab aimed at creating smart devices and apps to improve healthcare Involved in multiple projects with the primary objective of providing better services to the customers Performed behavioral analysis personalization and customer targeting Data Scientist Sherwin Williams Minneapolis MN January 2017 to July 2018 Minneapolis MN Jan 2017 Jul 2018 Data Scientist The SherwinWilliams Company is an American Fortune 500 company It is a global leader in the manufacture development distribution and sale of paints coatings and related products to professional industrial commercial and retail customers The primary objective of the project is to perform predictive analytics using the production and client data to estimate the production quantities Responsibilities Used the Classification machine learning algorithms Nave Bayes Linear regression Logistic regression SVM Neural Networks and used Clustering Algorithm K Means Analyzed business requirements and developed the applications models used appropriate algorithms for arriving at the required insights Established partnerships with product and engineering teams and work closely with other teams Work collaboratively with senior management to develop strategy and approach to defining business challenges to be answered by data science Used unsupervised Kmeans DBSCAN and supervised learning techniques Regression Classification for feature engineering and did Principal Component Analysis for dimensionality reduction of features Used SparkStreaming APIs to perform necessary transformations and actions on the fly for building the common learner data model which gets the data from Kafka in near real time Worked on data cleaning data preparation and feature engineering with Python 3X Used ML toolkit H2O for model fitting and to discover patterns in data Used H20 for data visualization and for different machine learning algorithms Used TensorFlow library for development and evaluation of Deep Learning Models Performed text classification task using NLTK package and implemented various natural language processing techniques Used Tableau for data visualization to create reports dashboards for insights and business process improvement Extensively used Pythons multiple data science packages like Pandas NumPy Matplotlib SciPy Scikitlearn and NLTK Used TensorFlow Python APIs to perform TensorFlow graphs Worked on Spark Python modules for machine learning and predictive analytics in Spark on AWS Worked on end to end pipe line in Spark Explored and analyzed the customer specific features by using Spark SQL Performed data imputation using Scikitlearn package in Python Created the dashboards and reports in tableau for visualizing the data in required format Collaborated with team members and translated functional requirements to technical requirements for development Conducted Code Review for the Fit Gap done by the team members Created Hive scripts to create external internal data tables on Hive Worked on creating datasets to load data into HIVE Environment Spark Apache Spark Hive Machine learning Python Numpy NLTK Pandas Scipy SQL Tableau HDFS Tableau  Mongo DB SQL Server and ETL Data Scientist TracFone Miami FL March 2015 to December 2016 TracFone Wireless Inc is a prepaid mobile virtual network operator in the United States Puerto Rico and the US Virgin Islands The objective of the project is to analyze the Customer data work on Churn and build recommendation engines and automated customer scoring systems Responsibilities Enhancing data collection procedures to include information that is relevant for building analytic systems Processing cleansing and verifying the integrity of data used for analysis Doing adhoc analysis and presenting results in a clear manner Constant tracking of model performance Excellent understanding of machine learning techniques and algorithms such as Logistic Regression SVM Random Forests Deep Learning etc Worked with Data governance Data quality Data lineage Data architect to design various models Independently coded new programs and designed Tables to load and test the program effectively for the given POCs Extending companys data with third party sources of information when needed Designed data models and data flow diagrams using Erwin and MS Visio Developed Implemented maintained the Conceptual Logical Physical Data Models using Erwin for ForwardReverse Engineered Databases Experience with common data science toolkits such as R Python Spark etc Good applied statistics skills such as statistical sampling testing regression etc Build analytic models using a variety of techniques such as logistic regression risk scorecards and pattern recognition technologies Analyze and understand large amounts of data to determine suitability for use in models and then work to segment the data create variables build models and test those models Work with technical and development teams to deploy models Build Model Performance Reports and Modeling Technical Documentation to support each of the models for the product line Developed new reports in SAS using SAS ODS PROC REPORT PROC TABULATE and PROC SQL Imported Data from relational database into SAS files per detailed specifications Responsible for the development and maintenance of SAS Information Maps for Analytics and Business Forecasting Team Performed Exploratory Data Analysis and Data Visualizations using R and Tableau Perform a proper EDA Univariate and bivariate analysis to understand the intrinsic effectcombined Established Data architecture strategy best practices standards and roadmaps Lead the development and presentation of a data analytics datahub prototype with the help of the other members of the emerging solutions team Experience in SAS programming for auditing data developing data performing data validation QA and improve efficiency of SAS programs Involved in analysis of Business requirement Design and Development of High level and Lowlevel designs Unit and Integration testing Worked with several R packages including knitr dplyr SparkR CausalInfer spacetime Interacted with the other departments to understand and identify data needs and requirements Environment UNIX Python 352 MLLib SAS regression logistic regression Hadoop NoSQL Teradata OLTP Random forest OLAP HDFS ODS Data Science Analyst Kindred Health Louisville KY April 2013 to February 2015 Kindred Healthcare Incorporated is a healthcare services company that operates hospitals nursing centers and contract rehabilitation services across the United States The project involved in predictive analysis and trend analysis using the patient historical data Responsibilities Extracted Transformed and loaded data from given source to analysis Handson implementation of R Python Hadoop Tableau and SAS to extract and import data Had a pleasure working experience on Spark spark streaming spark SQL Scala and Kafka Also converting HiveSQL queries into Spark transformations using Spark RDDs and Scala Used Kafka to load data in to HDFS and move data into NoSQL databases Worked on Spark SQL and Data frames for faster execution of Hive queries using Spark Sql Context Gained extreme knowledge on Map Reduce using Python Hive queries using Oozie workflows Performed Data cleaning process applied Backward Forward filling methods on dataset for handling missing values Design built and deployed a set of python modelling APIs for customer analytics which integrate multiple machine learning techniques for various user behavior prediction and support multiple marketing segmentation programs Segmented the customers based on demographics using Kmeans Clustering Explored different regression and ensemble models in machine learning to perform forecasting Used classification techniques including Random Forest and Logistic Regression to quantify the likelihood of each user referring Performed Boosting method on predicted model for the improve efficiency of the model Environment RR studio Informatica SQLPLSQL Oracle 10g MSOffice Teradata Data Analyst SunTrust Bank Atlanta GA October 2011 to March 2013 Worked for Credit Card Management Team and involved in calculating the Credit Card Performance by dividing the data into different DataMarts and performing analysis on the data and credit risk calculation for the Customers and lending money Mortgaging Responsibilities The team of Data analysts focused on providing analytics insights and decision support tools for executives for accurate decision making Applied highly advanced data access routines to extract data from source systems for monitoring operations compliance to banking Laws Rules and Regulations using Visual Basic Apps VBA SQL Server SSIS SAS and SQL Identified measured and recommended improvement strategies for KPIs across all business areas Assisted in defining implementing and utilizing business metrics calculations and methodologies Designed and provided complex excel reports including summaries charts and graphs to interpret findings to team and stakeholders Assisted the team for standardization of reports using SAS macros and SQL Responsible for creation of Credit data related warehouse to help with Risk Assessment for Commercial loans Performed competitor and customer analysis risk and pricing analysis and forecasted results for credit card holders on demographical basis Created macros and used existing macros to develop SAS programs for data analysis Created and manipulated various management reports in MS Excel for sales metrics using VLOOKUP and Pivot tables Developed transformation logic for BI tools Informatica for data transformation into various layers in Data warehouse Utilized SQL to develop stored procedures views to create result sets to meet varying reporting requirements Used advanced excel formulas lookup functions pivot table If Statements etc for analyzing data Identified process improvements that significantly reduce workloads or improve quality Worked for BI Analytics team to conduct AB testing data extraction and exploratory analysis Generated dashboards and presented the analysis to researchers explaining insights on the data Environment Excel 2010 R Informatica Power Center 90 MS SQL Server 200 Data Analyst Python Developer Teleparadigm Networks Private Limited May 2009 to September 2011 Teleparadigm Networks delivers software and IT services to clientele across the globe under service models Have a wide client base in Healthcare and Telecom Industries Responsibilities Brought in and implemented updated analytical methods such as regression modelling classification tree statistical tests and data visualization techniques with Python Analyzed customer Help data contact volumes and other operational data in MySQL to provide insights that enable improvements to Help content and customer experience Maintained and updated existing automated solutions Improved data collection and distribution processes by using pandas and Numpy packages in Python while enhancing reporting capabilities to provide clear line of sight into key performance trends and metrics Analysed historical demand filter out outliersexceptions identify the most appropriate statistical forecasting algorithm develop base plan understand variance propose improvement opportunities and incorporate demand signal into forecast and executed data visualization by using plotly package in Python Interacted with QA to develop test plans from highlevel design documentation Environment MySQL Statistical modelling Python libraries pandas and Numpy packages Education Bachelors in Feature Engineering and Selection Stanford NER Tagger Skills APACHE HADOOP HDFS 5 years HADOOP DISTRIBUTED FILE SYSTEM 5 years HDFS 5 years Python 7 years SQL 6 years",
    "entities": [
        "Statistical Machine Learning Data Mining",
        "NLTK",
        "Informatica",
        "Informatica Power Center",
        "MN",
        "Clustering Algorithm K Means Analyzed",
        "CausalInfer",
        "Customer",
        "BI",
        "Spark Sql Context Gained",
        "Applied",
        "Spark SQL Performed",
        "DataMarts",
        "Requirements Analysis Design Specification",
        "Indepth",
        "Laws Rules and Regulations using Visual Basic",
        "Louisville",
        "Kindred Healthcare Incorporated",
        "Atlanta",
        "Puerto Rico",
        "SVM Neural Networks",
        "Software Development Life Cycle SDLC",
        "Natural Language Processing",
        "Maintained",
        "Telecom Industries Responsibilities Brought",
        "Principal Component Analysis",
        "Digital Innovation Lab",
        "Credit",
        "Statistical Modelling Multivariate Analysis Bigdata",
        "Python Power BI",
        "Generated",
        "Assisted",
        "Dimensionality Reduction",
        "Random Forest Creative",
        "the Conceptual Logical Physical Data Models",
        "ETL Data Scientist",
        "MS Excel",
        "Healthcare",
        "Waterfall",
        "Data Warehouse ODS",
        "Lowlevel",
        "Python Hadoop Tableau",
        "Data Scientist Data Scientist Data Scientist CVS Health Scottsdale",
        "The SherwinWilliams Company",
        "SQL Tableau",
        "Minneapolis",
        "Collaborated",
        "VLOOKUP",
        "Churn",
        "Customers",
        "Tables",
        "Spark",
        "Created Hive",
        "Random Forest and Logistic Regression",
        "HIVE Environment Spark Apache Spark Hive Machine",
        "US",
        "QA",
        "Power BI",
        "Present CVS Health",
        "Created",
        "Hypothesis",
        "Data Science Machine Learning Deep Learning",
        "Text Analytics",
        "PCA Principal",
        "EDA",
        "Performed Data",
        "SAS Information Maps for Analytics and Business Forecasting Team Performed Exploratory Data Analysis",
        "SAS",
        "Market Mix",
        "Oozie",
        "SQL",
        "SunTrust Bank",
        "Standard Procedures Highly",
        "NLP",
        "Worked for BI Analytics",
        "ANOVA",
        "Deep Learning Models Performed",
        "the United States",
        "Identifiescreates",
        "Anaconda",
        "Hive",
        "Regression Classification",
        "Risk Assessment for Commercial",
        "Handson",
        "Established Data",
        "Stanford",
        "Conducted Code Review",
        "TracFone Wireless Inc",
        "Build",
        "Performed",
        "SQL Responsible",
        "Credit Card Management Team",
        "Miami FL",
        "Feature Engineering",
        "ML",
        "SQL Identified",
        "AB test Experiment",
        "the US Virgin Islands",
        "Data",
        "Segmented",
        "Neural Network Support",
        "Informatica SQLPLSQL Oracle",
        "NoSQL",
        "Tableau",
        "ForwardReverse Engineered Databases",
        "Random Forests Classification",
        "Applies",
        "Python Created"
    ],
    "experience": "Experience in text understanding classification pattern recognition recommendation systems targeting systems and ranking systems using Python Strong skills in statistical methodologies such as AB test Experiment design Hypothesis test ANOVA CrossTabs T tests and Correlation Techniques Worked with applications like R SPSS and Python to develop predictive models Experience with Natural Language Processing NLP Extensively worked on Python 3527 Numpy Pandas Matplotlib NLTK and Scikitlearn Experience in implementing data analysis with various analytic tools such as Anaconda 40 Jupyter Notebook 4X R 30ggplot2 and Excel Worked on Tableau to create dashboards and visualizations Extensive experience in Text Analytics developing different Statistical Machine Learning Data Mining solutions to various business problems and generating data visualizations using R Python Power BI and Tableau Hands on experience in business understanding data understanding and preparation of large databases Expertise in transforming business requirements into analytical models designing algorithms building models developing datamining and reporting solutions that scales across massive volume of structured and unstructured data Documenting new data to help source to target mapping Also updating the documentation for existing data assisting with data profiling to maintain data sanitation validation Identifies what data is available and relevant including internal and external data sources leveraging new data collection processes such geolocation information Good understanding on data preparation techniques Experienced working on large volume of Data using BASE SAS programming Proficiency in application of statistical prediction modeling machine learning classification techniques and econometric forecasting techniques Proficiency in various type of optimization Market Mix modeling Segmentation Time Series Price Promo models etc Experience in the application of Neural Network Support Vector Machines SVM and Random Forest Creative thinking and propose innovative ways to look at problems by using data mining approaches on the set of information available Identifiescreates the appropriate algorithm to discover patterns validate their findings using an experimental and iterative approach Applies advanced statistical and predictive modeling techniques to build maintain and improve on multiple realtime decision systems Closely works with product managers Service development managers and product development team in productizing the algorithms developed Experience in designing star schema Snowflake schema for Data Warehouse ODS architecture Experience in designing stunning visualizations using Tableau and Power BI software and publishing and presenting dashboards Storyline on web and desktop platforms Experience in working with relational databases Teradata Oracle with advanced SQL programming skills Indepth knowledge of statistical procedures that are applied in Supervised Unsupervised problems Proficiency in SAS Base SAS Enterprise Guide Enterprise Miner Familiar with graphical models and deep learning models including deep learning frameworks such as TensorFlow Experienced in working with advanced analytical teams to design build validate and refresh data models that enable the next generation of sophisticated solutions for global clients Excellent communication skills verbal and written to communicate with clients and team prepare deliver effective presentations Strong experience in Software Development Life Cycle SDLC including Requirements Analysis Design Specification and Testing as per Cycle in both Waterfall and Agile methodologies Strong experience in interacting with stakeholderscustomers gathering requirements through interviews workshops and existing system documentation or procedures defining business processes identifying and analyzing risks using appropriate templates and analysis tools Mapping and tracing data from system to system in order to establish data hierarchy and lineage Used Data Lineage and reverse engineering as a way to track back errors in data till the data source Authorized to work in the US for any employer Work Experience Data Scientist CVS Health Scottsdale AZ August 2018 to Present CVS Health is an American pharmacy and healthcare company with nearly 10000 stores in its network Its healthfocused business includes pharmacy services retail instore health clinics and its own Digital Innovation Lab aimed at creating smart devices and apps to improve healthcare Involved in multiple projects with the primary objective of providing better services to the customers Performed behavioral analysis personalization and customer targeting Data Scientist Sherwin Williams Minneapolis MN January 2017 to July 2018 Minneapolis MN Jan 2017 Jul 2018 Data Scientist The SherwinWilliams Company is an American Fortune 500 company It is a global leader in the manufacture development distribution and sale of paints coatings and related products to professional industrial commercial and retail customers The primary objective of the project is to perform predictive analytics using the production and client data to estimate the production quantities Responsibilities Used the Classification machine learning algorithms Nave Bayes Linear regression Logistic regression SVM Neural Networks and used Clustering Algorithm K Means Analyzed business requirements and developed the applications models used appropriate algorithms for arriving at the required insights Established partnerships with product and engineering teams and work closely with other teams Work collaboratively with senior management to develop strategy and approach to defining business challenges to be answered by data science Used unsupervised Kmeans DBSCAN and supervised learning techniques Regression Classification for feature engineering and did Principal Component Analysis for dimensionality reduction of features Used SparkStreaming APIs to perform necessary transformations and actions on the fly for building the common learner data model which gets the data from Kafka in near real time Worked on data cleaning data preparation and feature engineering with Python 3X Used ML toolkit H2O for model fitting and to discover patterns in data Used H20 for data visualization and for different machine learning algorithms Used TensorFlow library for development and evaluation of Deep Learning Models Performed text classification task using NLTK package and implemented various natural language processing techniques Used Tableau for data visualization to create reports dashboards for insights and business process improvement Extensively used Pythons multiple data science packages like Pandas NumPy Matplotlib SciPy Scikitlearn and NLTK Used TensorFlow Python APIs to perform TensorFlow graphs Worked on Spark Python modules for machine learning and predictive analytics in Spark on AWS Worked on end to end pipe line in Spark Explored and analyzed the customer specific features by using Spark SQL Performed data imputation using Scikitlearn package in Python Created the dashboards and reports in tableau for visualizing the data in required format Collaborated with team members and translated functional requirements to technical requirements for development Conducted Code Review for the Fit Gap done by the team members Created Hive scripts to create external internal data tables on Hive Worked on creating datasets to load data into HIVE Environment Spark Apache Spark Hive Machine learning Python Numpy NLTK Pandas Scipy SQL Tableau HDFS Tableau   Mongo DB SQL Server and ETL Data Scientist TracFone Miami FL March 2015 to December 2016 TracFone Wireless Inc is a prepaid mobile virtual network operator in the United States Puerto Rico and the US Virgin Islands The objective of the project is to analyze the Customer data work on Churn and build recommendation engines and automated customer scoring systems Responsibilities Enhancing data collection procedures to include information that is relevant for building analytic systems Processing cleansing and verifying the integrity of data used for analysis Doing adhoc analysis and presenting results in a clear manner Constant tracking of model performance Excellent understanding of machine learning techniques and algorithms such as Logistic Regression SVM Random Forests Deep Learning etc Worked with Data governance Data quality Data lineage Data architect to design various models Independently coded new programs and designed Tables to load and test the program effectively for the given POCs Extending companys data with third party sources of information when needed Designed data models and data flow diagrams using Erwin and MS Visio Developed Implemented maintained the Conceptual Logical Physical Data Models using Erwin for ForwardReverse Engineered Databases Experience with common data science toolkits such as R Python Spark etc Good applied statistics skills such as statistical sampling testing regression etc Build analytic models using a variety of techniques such as logistic regression risk scorecards and pattern recognition technologies Analyze and understand large amounts of data to determine suitability for use in models and then work to segment the data create variables build models and test those models Work with technical and development teams to deploy models Build Model Performance Reports and Modeling Technical Documentation to support each of the models for the product line Developed new reports in SAS using SAS ODS PROC REPORT PROC TABULATE and PROC SQL Imported Data from relational database into SAS files per detailed specifications Responsible for the development and maintenance of SAS Information Maps for Analytics and Business Forecasting Team Performed Exploratory Data Analysis and Data Visualizations using R and Tableau Perform a proper EDA Univariate and bivariate analysis to understand the intrinsic effectcombined Established Data architecture strategy best practices standards and roadmaps Lead the development and presentation of a data analytics datahub prototype with the help of the other members of the emerging solutions team Experience in SAS programming for auditing data developing data performing data validation QA and improve efficiency of SAS programs Involved in analysis of Business requirement Design and Development of High level and Lowlevel designs Unit and Integration testing Worked with several R packages including knitr dplyr SparkR CausalInfer spacetime Interacted with the other departments to understand and identify data needs and requirements Environment UNIX Python 352 MLLib SAS regression logistic regression Hadoop NoSQL Teradata OLTP Random forest OLAP HDFS ODS Data Science Analyst Kindred Health Louisville KY April 2013 to February 2015 Kindred Healthcare Incorporated is a healthcare services company that operates hospitals nursing centers and contract rehabilitation services across the United States The project involved in predictive analysis and trend analysis using the patient historical data Responsibilities Extracted Transformed and loaded data from given source to analysis Handson implementation of R Python Hadoop Tableau and SAS to extract and import data Had a pleasure working experience on Spark spark streaming spark SQL Scala and Kafka Also converting HiveSQL queries into Spark transformations using Spark RDDs and Scala Used Kafka to load data in to HDFS and move data into NoSQL databases Worked on Spark SQL and Data frames for faster execution of Hive queries using Spark Sql Context Gained extreme knowledge on Map Reduce using Python Hive queries using Oozie workflows Performed Data cleaning process applied Backward Forward filling methods on dataset for handling missing values Design built and deployed a set of python modelling APIs for customer analytics which integrate multiple machine learning techniques for various user behavior prediction and support multiple marketing segmentation programs Segmented the customers based on demographics using Kmeans Clustering Explored different regression and ensemble models in machine learning to perform forecasting Used classification techniques including Random Forest and Logistic Regression to quantify the likelihood of each user referring Performed Boosting method on predicted model for the improve efficiency of the model Environment RR studio Informatica SQLPLSQL Oracle 10 g MSOffice Teradata Data Analyst SunTrust Bank Atlanta GA October 2011 to March 2013 Worked for Credit Card Management Team and involved in calculating the Credit Card Performance by dividing the data into different DataMarts and performing analysis on the data and credit risk calculation for the Customers and lending money Mortgaging Responsibilities The team of Data analysts focused on providing analytics insights and decision support tools for executives for accurate decision making Applied highly advanced data access routines to extract data from source systems for monitoring operations compliance to banking Laws Rules and Regulations using Visual Basic Apps VBA SQL Server SSIS SAS and SQL Identified measured and recommended improvement strategies for KPIs across all business areas Assisted in defining implementing and utilizing business metrics calculations and methodologies Designed and provided complex excel reports including summaries charts and graphs to interpret findings to team and stakeholders Assisted the team for standardization of reports using SAS macros and SQL Responsible for creation of Credit data related warehouse to help with Risk Assessment for Commercial loans Performed competitor and customer analysis risk and pricing analysis and forecasted results for credit card holders on demographical basis Created macros and used existing macros to develop SAS programs for data analysis Created and manipulated various management reports in MS Excel for sales metrics using VLOOKUP and Pivot tables Developed transformation logic for BI tools Informatica for data transformation into various layers in Data warehouse Utilized SQL to develop stored procedures views to create result sets to meet varying reporting requirements Used advanced excel formulas lookup functions pivot table If Statements etc for analyzing data Identified process improvements that significantly reduce workloads or improve quality Worked for BI Analytics team to conduct AB testing data extraction and exploratory analysis Generated dashboards and presented the analysis to researchers explaining insights on the data Environment Excel 2010 R Informatica Power Center 90 MS SQL Server 200 Data Analyst Python Developer Teleparadigm Networks Private Limited May 2009 to September 2011 Teleparadigm Networks delivers software and IT services to clientele across the globe under service models Have a wide client base in Healthcare and Telecom Industries Responsibilities Brought in and implemented updated analytical methods such as regression modelling classification tree statistical tests and data visualization techniques with Python Analyzed customer Help data contact volumes and other operational data in MySQL to provide insights that enable improvements to Help content and customer experience Maintained and updated existing automated solutions Improved data collection and distribution processes by using pandas and Numpy packages in Python while enhancing reporting capabilities to provide clear line of sight into key performance trends and metrics Analysed historical demand filter out outliersexceptions identify the most appropriate statistical forecasting algorithm develop base plan understand variance propose improvement opportunities and incorporate demand signal into forecast and executed data visualization by using plotly package in Python Interacted with QA to develop test plans from highlevel design documentation Environment MySQL Statistical modelling Python libraries pandas and Numpy packages Education Bachelors in Feature Engineering and Selection Stanford NER Tagger Skills APACHE HADOOP HDFS 5 years HADOOP DISTRIBUTED FILE SYSTEM 5 years HDFS 5 years Python 7 years SQL 6 years",
    "extracted_keywords": [
        "Data",
        "Scientist",
        "Data",
        "Scientist",
        "Data",
        "Scientist",
        "CVS",
        "Health",
        "Scottsdale",
        "AZ",
        "years",
        "IT",
        "experience",
        "Data",
        "Science",
        "Machine",
        "Learning",
        "Deep",
        "Learning",
        "NLP",
        "Text",
        "Mining",
        "DataBusiness",
        "Analytics",
        "Data",
        "Visualization",
        "Data",
        "Operations",
        "BI",
        "understanding",
        "Statistical",
        "Modelling",
        "Multivariate",
        "Analysis",
        "Bigdata",
        "analytics",
        "Standard",
        "Procedures",
        "Dimensionality",
        "Reduction",
        "methods",
        "PCA",
        "Principal",
        "component",
        "Analysis",
        "Factor",
        "Analysis",
        "methods",
        "Random",
        "Forests",
        "Classification",
        "KMeans",
        "KNN",
        "Nave",
        "Bayes",
        "SVM",
        "Decision",
        "Tree",
        "Linear",
        "Logistic",
        "Regression",
        "Methods",
        "Experience",
        "text",
        "classification",
        "pattern",
        "recognition",
        "recommendation",
        "systems",
        "systems",
        "systems",
        "Python",
        "Strong",
        "skills",
        "methodologies",
        "AB",
        "test",
        "Experiment",
        "design",
        "Hypothesis",
        "test",
        "ANOVA",
        "CrossTabs",
        "T",
        "tests",
        "Correlation",
        "Techniques",
        "applications",
        "R",
        "SPSS",
        "Python",
        "models",
        "Experience",
        "Natural",
        "Language",
        "Processing",
        "NLP",
        "Python",
        "Numpy",
        "Pandas",
        "Matplotlib",
        "NLTK",
        "Scikitlearn",
        "Experience",
        "data",
        "analysis",
        "tools",
        "Anaconda",
        "Jupyter",
        "Notebook",
        "4X",
        "R",
        "30ggplot2",
        "Excel",
        "Worked",
        "Tableau",
        "dashboards",
        "visualizations",
        "experience",
        "Text",
        "Analytics",
        "Machine",
        "Learning",
        "Data",
        "Mining",
        "solutions",
        "business",
        "problems",
        "data",
        "visualizations",
        "R",
        "Python",
        "Power",
        "BI",
        "Tableau",
        "Hands",
        "experience",
        "business",
        "data",
        "understanding",
        "preparation",
        "databases",
        "Expertise",
        "business",
        "requirements",
        "models",
        "algorithms",
        "building",
        "models",
        "solutions",
        "volume",
        "data",
        "data",
        "source",
        "mapping",
        "documentation",
        "data",
        "data",
        "profiling",
        "data",
        "sanitation",
        "validation",
        "Identifies",
        "data",
        "data",
        "sources",
        "data",
        "collection",
        "geolocation",
        "information",
        "understanding",
        "data",
        "preparation",
        "techniques",
        "volume",
        "Data",
        "BASE",
        "SAS",
        "programming",
        "Proficiency",
        "application",
        "prediction",
        "machine",
        "classification",
        "techniques",
        "forecasting",
        "techniques",
        "Proficiency",
        "type",
        "optimization",
        "Market",
        "Mix",
        "Segmentation",
        "Time",
        "Series",
        "Price",
        "Promo",
        "models",
        "Experience",
        "application",
        "Neural",
        "Network",
        "Support",
        "Vector",
        "Machines",
        "SVM",
        "Random",
        "Forest",
        "thinking",
        "ways",
        "problems",
        "data",
        "mining",
        "approaches",
        "set",
        "information",
        "Identifiescreates",
        "algorithm",
        "patterns",
        "findings",
        "approach",
        "modeling",
        "techniques",
        "maintain",
        "decision",
        "systems",
        "product",
        "managers",
        "Service",
        "development",
        "managers",
        "product",
        "development",
        "team",
        "algorithms",
        "Experience",
        "star",
        "schema",
        "Snowflake",
        "schema",
        "Data",
        "Warehouse",
        "ODS",
        "architecture",
        "Experience",
        "visualizations",
        "Tableau",
        "Power",
        "BI",
        "software",
        "publishing",
        "dashboards",
        "Storyline",
        "web",
        "desktop",
        "platforms",
        "databases",
        "Teradata",
        "Oracle",
        "SQL",
        "programming",
        "skills",
        "knowledge",
        "procedures",
        "Supervised",
        "Unsupervised",
        "problems",
        "Proficiency",
        "SAS",
        "Base",
        "SAS",
        "Enterprise",
        "Guide",
        "Enterprise",
        "Miner",
        "Familiar",
        "models",
        "learning",
        "models",
        "frameworks",
        "TensorFlow",
        "teams",
        "build",
        "validate",
        "data",
        "models",
        "generation",
        "solutions",
        "clients",
        "communication",
        "clients",
        "team",
        "presentations",
        "experience",
        "Software",
        "Development",
        "Life",
        "Cycle",
        "SDLC",
        "Requirements",
        "Analysis",
        "Design",
        "Specification",
        "Testing",
        "Cycle",
        "Waterfall",
        "methodologies",
        "experience",
        "stakeholderscustomers",
        "requirements",
        "interviews",
        "workshops",
        "system",
        "documentation",
        "procedures",
        "business",
        "processes",
        "risks",
        "templates",
        "analysis",
        "tools",
        "Mapping",
        "data",
        "system",
        "system",
        "order",
        "data",
        "hierarchy",
        "Data",
        "Lineage",
        "engineering",
        "way",
        "errors",
        "data",
        "data",
        "source",
        "US",
        "employer",
        "Work",
        "Experience",
        "Data",
        "Scientist",
        "CVS",
        "Health",
        "Scottsdale",
        "AZ",
        "August",
        "Present",
        "CVS",
        "Health",
        "pharmacy",
        "healthcare",
        "company",
        "stores",
        "network",
        "business",
        "pharmacy",
        "services",
        "instore",
        "health",
        "clinics",
        "Digital",
        "Innovation",
        "Lab",
        "devices",
        "apps",
        "healthcare",
        "projects",
        "objective",
        "services",
        "customers",
        "analysis",
        "personalization",
        "customer",
        "Data",
        "Scientist",
        "Sherwin",
        "Williams",
        "Minneapolis",
        "MN",
        "January",
        "July",
        "Minneapolis",
        "MN",
        "Jan",
        "Jul",
        "Data",
        "Scientist",
        "SherwinWilliams",
        "Company",
        "American",
        "Fortune",
        "company",
        "leader",
        "manufacture",
        "development",
        "distribution",
        "sale",
        "paints",
        "coatings",
        "products",
        "customers",
        "objective",
        "project",
        "analytics",
        "production",
        "client",
        "data",
        "production",
        "quantities",
        "Responsibilities",
        "Classification",
        "machine",
        "learning",
        "algorithms",
        "Nave",
        "Bayes",
        "Linear",
        "regression",
        "regression",
        "SVM",
        "Neural",
        "Networks",
        "Clustering",
        "Algorithm",
        "K",
        "business",
        "requirements",
        "applications",
        "models",
        "algorithms",
        "insights",
        "partnerships",
        "product",
        "engineering",
        "teams",
        "teams",
        "management",
        "strategy",
        "approach",
        "business",
        "challenges",
        "data",
        "science",
        "Kmeans",
        "DBSCAN",
        "techniques",
        "Regression",
        "Classification",
        "feature",
        "engineering",
        "Principal",
        "Component",
        "Analysis",
        "dimensionality",
        "reduction",
        "features",
        "SparkStreaming",
        "APIs",
        "transformations",
        "actions",
        "fly",
        "learner",
        "data",
        "model",
        "data",
        "Kafka",
        "time",
        "data",
        "data",
        "preparation",
        "feature",
        "engineering",
        "Python",
        "3X",
        "ML",
        "toolkit",
        "H2O",
        "model",
        "patterns",
        "data",
        "H20",
        "data",
        "visualization",
        "machine",
        "learning",
        "algorithms",
        "TensorFlow",
        "library",
        "development",
        "evaluation",
        "Deep",
        "Learning",
        "Models",
        "text",
        "classification",
        "task",
        "NLTK",
        "package",
        "language",
        "processing",
        "techniques",
        "Tableau",
        "data",
        "visualization",
        "reports",
        "dashboards",
        "insights",
        "business",
        "process",
        "improvement",
        "Pythons",
        "data",
        "science",
        "packages",
        "Pandas",
        "NumPy",
        "Matplotlib",
        "SciPy",
        "Scikitlearn",
        "NLTK",
        "TensorFlow",
        "Python",
        "APIs",
        "graphs",
        "Spark",
        "Python",
        "modules",
        "machine",
        "learning",
        "analytics",
        "Spark",
        "AWS",
        "end",
        "pipe",
        "line",
        "Spark",
        "Explored",
        "customer",
        "features",
        "Spark",
        "SQL",
        "Performed",
        "data",
        "imputation",
        "Scikitlearn",
        "package",
        "Python",
        "dashboards",
        "reports",
        "tableau",
        "data",
        "format",
        "team",
        "members",
        "requirements",
        "requirements",
        "development",
        "Conducted",
        "Code",
        "Review",
        "Fit",
        "Gap",
        "team",
        "members",
        "Hive",
        "scripts",
        "data",
        "tables",
        "Hive",
        "Worked",
        "datasets",
        "data",
        "HIVE",
        "Environment",
        "Spark",
        "Apache",
        "Spark",
        "Hive",
        "Machine",
        "Python",
        "Numpy",
        "NLTK",
        "Pandas",
        "Scipy",
        "SQL",
        "Tableau",
        "HDFS",
        "Tableau",
        "Mongo",
        "DB",
        "SQL",
        "Server",
        "ETL",
        "Data",
        "Scientist",
        "TracFone",
        "Miami",
        "FL",
        "March",
        "December",
        "TracFone",
        "Wireless",
        "Inc",
        "network",
        "operator",
        "United",
        "States",
        "Puerto",
        "Rico",
        "US",
        "Virgin",
        "Islands",
        "objective",
        "project",
        "Customer",
        "data",
        "work",
        "Churn",
        "recommendation",
        "engines",
        "customer",
        "scoring",
        "systems",
        "Responsibilities",
        "data",
        "collection",
        "procedures",
        "information",
        "systems",
        "Processing",
        "cleansing",
        "integrity",
        "data",
        "analysis",
        "adhoc",
        "analysis",
        "results",
        "manner",
        "tracking",
        "model",
        "performance",
        "Excellent",
        "understanding",
        "machine",
        "techniques",
        "algorithms",
        "Logistic",
        "Regression",
        "SVM",
        "Random",
        "Forests",
        "Deep",
        "Learning",
        "Data",
        "governance",
        "Data",
        "quality",
        "Data",
        "lineage",
        "Data",
        "architect",
        "models",
        "programs",
        "Tables",
        "program",
        "POCs",
        "companys",
        "data",
        "party",
        "sources",
        "information",
        "data",
        "models",
        "data",
        "flow",
        "diagrams",
        "Erwin",
        "MS",
        "Visio",
        "Developed",
        "Implemented",
        "Conceptual",
        "Logical",
        "Physical",
        "Data",
        "Models",
        "Erwin",
        "ForwardReverse",
        "Engineered",
        "Databases",
        "Experience",
        "data",
        "science",
        "toolkits",
        "R",
        "Python",
        "Spark",
        "statistics",
        "skills",
        "sampling",
        "testing",
        "regression",
        "models",
        "variety",
        "techniques",
        "regression",
        "risk",
        "scorecards",
        "pattern",
        "recognition",
        "technologies",
        "Analyze",
        "amounts",
        "data",
        "suitability",
        "use",
        "models",
        "data",
        "variables",
        "models",
        "models",
        "development",
        "teams",
        "models",
        "Build",
        "Model",
        "Performance",
        "Reports",
        "Modeling",
        "Technical",
        "Documentation",
        "models",
        "product",
        "line",
        "reports",
        "SAS",
        "SAS",
        "ODS",
        "PROC",
        "REPORT",
        "PROC",
        "TABULATE",
        "PROC",
        "SQL",
        "Imported",
        "Data",
        "database",
        "SAS",
        "files",
        "specifications",
        "development",
        "maintenance",
        "SAS",
        "Information",
        "Maps",
        "Analytics",
        "Business",
        "Forecasting",
        "Team",
        "Exploratory",
        "Data",
        "Analysis",
        "Data",
        "Visualizations",
        "R",
        "Tableau",
        "EDA",
        "Univariate",
        "analysis",
        "Established",
        "Data",
        "architecture",
        "strategy",
        "practices",
        "standards",
        "roadmaps",
        "development",
        "presentation",
        "data",
        "analytics",
        "datahub",
        "prototype",
        "help",
        "members",
        "solutions",
        "team",
        "Experience",
        "SAS",
        "programming",
        "auditing",
        "data",
        "data",
        "data",
        "validation",
        "QA",
        "efficiency",
        "SAS",
        "programs",
        "analysis",
        "Business",
        "requirement",
        "Design",
        "Development",
        "level",
        "Lowlevel",
        "designs",
        "Unit",
        "Integration",
        "testing",
        "R",
        "packages",
        "knitr",
        "dplyr",
        "CausalInfer",
        "spacetime",
        "departments",
        "data",
        "needs",
        "requirements",
        "Environment",
        "UNIX",
        "Python",
        "MLLib",
        "SAS",
        "regression",
        "regression",
        "Hadoop",
        "NoSQL",
        "Teradata",
        "OLTP",
        "Random",
        "forest",
        "OLAP",
        "HDFS",
        "ODS",
        "Data",
        "Science",
        "Analyst",
        "Kindred",
        "Health",
        "Louisville",
        "KY",
        "April",
        "February",
        "Kindred",
        "Healthcare",
        "Incorporated",
        "healthcare",
        "services",
        "company",
        "hospitals",
        "nursing",
        "centers",
        "contract",
        "rehabilitation",
        "services",
        "United",
        "States",
        "project",
        "analysis",
        "analysis",
        "data",
        "Responsibilities",
        "Transformed",
        "data",
        "source",
        "Handson",
        "implementation",
        "R",
        "Python",
        "Hadoop",
        "Tableau",
        "SAS",
        "import",
        "data",
        "pleasure",
        "working",
        "experience",
        "Spark",
        "spark",
        "streaming",
        "spark",
        "SQL",
        "Scala",
        "Kafka",
        "HiveSQL",
        "queries",
        "Spark",
        "transformations",
        "Spark",
        "RDDs",
        "Scala",
        "Kafka",
        "data",
        "HDFS",
        "data",
        "NoSQL",
        "databases",
        "Spark",
        "SQL",
        "Data",
        "frames",
        "execution",
        "Hive",
        "queries",
        "Spark",
        "Sql",
        "Context",
        "knowledge",
        "Map",
        "Reduce",
        "Python",
        "Hive",
        "queries",
        "Oozie",
        "workflows",
        "Performed",
        "Data",
        "cleaning",
        "process",
        "Backward",
        "Forward",
        "methods",
        "dataset",
        "values",
        "Design",
        "set",
        "python",
        "APIs",
        "customer",
        "analytics",
        "machine",
        "techniques",
        "user",
        "behavior",
        "prediction",
        "marketing",
        "segmentation",
        "programs",
        "customers",
        "demographics",
        "Kmeans",
        "Clustering",
        "regression",
        "models",
        "machine",
        "classification",
        "techniques",
        "Random",
        "Forest",
        "Logistic",
        "Regression",
        "likelihood",
        "user",
        "Performed",
        "Boosting",
        "method",
        "model",
        "efficiency",
        "model",
        "Environment",
        "RR",
        "studio",
        "Informatica",
        "SQLPLSQL",
        "Oracle",
        "g",
        "MSOffice",
        "Teradata",
        "Data",
        "Analyst",
        "SunTrust",
        "Bank",
        "Atlanta",
        "GA",
        "October",
        "March",
        "Credit",
        "Card",
        "Management",
        "Team",
        "Credit",
        "Card",
        "Performance",
        "data",
        "DataMarts",
        "analysis",
        "data",
        "credit",
        "risk",
        "calculation",
        "Customers",
        "money",
        "Mortgaging",
        "Responsibilities",
        "team",
        "Data",
        "analysts",
        "analytics",
        "insights",
        "decision",
        "support",
        "tools",
        "executives",
        "decision",
        "data",
        "access",
        "routines",
        "data",
        "source",
        "systems",
        "operations",
        "compliance",
        "Laws",
        "Rules",
        "Regulations",
        "Visual",
        "Basic",
        "Apps",
        "VBA",
        "SQL",
        "Server",
        "SSIS",
        "SAS",
        "SQL",
        "improvement",
        "strategies",
        "KPIs",
        "business",
        "areas",
        "business",
        "metrics",
        "calculations",
        "methodologies",
        "excel",
        "reports",
        "summaries",
        "charts",
        "graphs",
        "findings",
        "stakeholders",
        "team",
        "standardization",
        "reports",
        "SAS",
        "macros",
        "SQL",
        "Responsible",
        "creation",
        "Credit",
        "data",
        "warehouse",
        "Risk",
        "Assessment",
        "loans",
        "Performed",
        "competitor",
        "customer",
        "analysis",
        "risk",
        "pricing",
        "analysis",
        "results",
        "credit",
        "card",
        "holders",
        "basis",
        "macros",
        "macros",
        "SAS",
        "programs",
        "data",
        "analysis",
        "management",
        "reports",
        "MS",
        "Excel",
        "sales",
        "metrics",
        "VLOOKUP",
        "Pivot",
        "transformation",
        "logic",
        "BI",
        "tools",
        "Informatica",
        "data",
        "transformation",
        "layers",
        "Data",
        "warehouse",
        "SQL",
        "procedures",
        "views",
        "result",
        "sets",
        "reporting",
        "requirements",
        "excel",
        "formulas",
        "lookup",
        "functions",
        "pivot",
        "table",
        "Statements",
        "data",
        "process",
        "improvements",
        "workloads",
        "quality",
        "Worked",
        "BI",
        "Analytics",
        "team",
        "AB",
        "testing",
        "data",
        "extraction",
        "analysis",
        "dashboards",
        "analysis",
        "researchers",
        "insights",
        "data",
        "Environment",
        "Excel",
        "R",
        "Informatica",
        "Power",
        "Center",
        "MS",
        "SQL",
        "Server",
        "Data",
        "Analyst",
        "Python",
        "Developer",
        "Teleparadigm",
        "Networks",
        "Private",
        "Limited",
        "May",
        "September",
        "Teleparadigm",
        "Networks",
        "software",
        "IT",
        "services",
        "globe",
        "service",
        "models",
        "client",
        "base",
        "Healthcare",
        "Telecom",
        "Industries",
        "Responsibilities",
        "methods",
        "regression",
        "classification",
        "tree",
        "tests",
        "data",
        "visualization",
        "techniques",
        "Python",
        "customer",
        "data",
        "contact",
        "volumes",
        "data",
        "MySQL",
        "insights",
        "improvements",
        "content",
        "customer",
        "experience",
        "solutions",
        "data",
        "collection",
        "distribution",
        "processes",
        "pandas",
        "packages",
        "Python",
        "capabilities",
        "line",
        "sight",
        "performance",
        "trends",
        "metrics",
        "demand",
        "filter",
        "outliersexceptions",
        "forecasting",
        "algorithm",
        "base",
        "plan",
        "variance",
        "improvement",
        "opportunities",
        "demand",
        "signal",
        "forecast",
        "data",
        "visualization",
        "package",
        "Python",
        "QA",
        "test",
        "plans",
        "highlevel",
        "design",
        "documentation",
        "Environment",
        "MySQL",
        "modelling",
        "Python",
        "pandas",
        "Numpy",
        "packages",
        "Education",
        "Bachelors",
        "Feature",
        "Engineering",
        "Selection",
        "Stanford",
        "NER",
        "Tagger",
        "Skills",
        "APACHE",
        "HADOOP",
        "HDFS",
        "years",
        "HADOOP",
        "FILE",
        "SYSTEM",
        "years",
        "years",
        "Python",
        "years",
        "SQL",
        "years"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T21:37:27.011221",
    "resume_data": "Data Scientist Data Scientist Data Scientist CVS Health Scottsdale AZ Around 9 years of IT experience includes in Data Science Machine Learning Deep Learning NLP Text Mining DataBusiness Analytics Data Visualization Data Operations and BI A deep understanding of Statistical Modelling Multivariate Analysis Bigdata analytics and Standard Procedures Highly efficient in Dimensionality Reduction methods such as PCA Principal component Analysis Factor Analysis etc Implemented bootstrapping methods such as Random Forests Classification KMeans Clustering KNN Nave Bayes SVM Decision Tree BFS Linear and Logistic Regression Methods Experience in text understanding classification pattern recognition recommendation systems targeting systems and ranking systems using Python Strong skills in statistical methodologies such as AB test Experiment design Hypothesis test ANOVA CrossTabs T tests and Correlation Techniques Worked with applications like R SPSS and Python to develop predictive models Experience with Natural Language Processing NLP Extensively worked on Python 3527 Numpy Pandas Matplotlib NLTK and Scikitlearn Experience in implementing data analysis with various analytic tools such as Anaconda 40 Jupyter Notebook 4X R 30ggplot2 and Excel Worked on Tableau to create dashboards and visualizations Extensive experience in Text Analytics developing different Statistical Machine Learning Data Mining solutions to various business problems and generating data visualizations using R Python Power BI and Tableau Hands on experience in business understanding data understanding and preparation of large databases Expertise in transforming business requirements into analytical models designing algorithms building models developing datamining and reporting solutions that scales across massive volume of structured and unstructured data Documenting new data to help source to target mapping Also updating the documentation for existing data assisting with data profiling to maintain data sanitation validation Identifies what data is available and relevant including internal and external data sources leveraging new data collection processes such geolocation information Good understanding on data preparation techniques Experienced working on large volume of Data using BASE SAS programming Proficiency in application of statistical prediction modeling machine learning classification techniques and econometric forecasting techniques Proficiency in various type of optimization Market Mix modeling Segmentation Time Series Price Promo models etc Experience in the application of Neural Network Support Vector Machines SVM and Random Forest Creative thinking and propose innovative ways to look at problems by using data mining approaches on the set of information available Identifiescreates the appropriate algorithm to discover patterns validate their findings using an experimental and iterative approach Applies advanced statistical and predictive modeling techniques to build maintain and improve on multiple realtime decision systems Closely works with product managers Service development managers and product development team in productizing the algorithms developed Experience in designing star schema Snowflake schema for Data Warehouse ODS architecture Experience in designing stunning visualizations using Tableau and Power BI software and publishing and presenting dashboards Storyline on web and desktop platforms Experience in working with relational databases Teradata Oracle with advanced SQL programming skills Indepth knowledge of statistical procedures that are applied in Supervised Unsupervised problems Proficiency in SAS Base SAS Enterprise Guide Enterprise Miner Familiar with graphical models and deep learning models including deep learning frameworks such as TensorFlow Experienced in working with advanced analytical teams to design build validate and refresh data models that enable the next generation of sophisticated solutions for global clients Excellent communication skills verbal and written to communicate with clients and team prepare deliver effective presentations Strong experience in Software Development Life Cycle SDLC including Requirements Analysis Design Specification and Testing as per Cycle in both Waterfall and Agile methodologies Strong experience in interacting with stakeholderscustomers gathering requirements through interviews workshops and existing system documentation or procedures defining business processes identifying and analyzing risks using appropriate templates and analysis tools Mapping and tracing data from system to system in order to establish data hierarchy and lineage Used Data Lineage and reverse engineering as a way to track back errors in data till the data source Authorized to work in the US for any employer Work Experience Data Scientist CVS Health Scottsdale AZ August 2018 to Present CVS Health is an American pharmacy and healthcare company with nearly 10000 stores in its network Its healthfocused business includes pharmacy services retail instore health clinics and its own Digital Innovation Lab aimed at creating smart devices and apps to improve healthcare Involved in multiple projects with the primary objective of providing better services to the customers Performed behavioral analysis personalization and customer targeting Data Scientist Sherwin Williams Minneapolis MN January 2017 to July 2018 Minneapolis MN Jan 2017 Jul 2018 Data Scientist The SherwinWilliams Company is an American Fortune 500 company It is a global leader in the manufacture development distribution and sale of paints coatings and related products to professional industrial commercial and retail customers The primary objective of the project is to perform predictive analytics using the production and client data to estimate the production quantities Responsibilities Used the Classification machine learning algorithms Nave Bayes Linear regression Logistic regression SVM Neural Networks and used Clustering Algorithm K Means Analyzed business requirements and developed the applications models used appropriate algorithms for arriving at the required insights Established partnerships with product and engineering teams and work closely with other teams Work collaboratively with senior management to develop strategy and approach to defining business challenges to be answered by data science Used unsupervised Kmeans DBSCAN and supervised learning techniques Regression Classification for feature engineering and did Principal Component Analysis for dimensionality reduction of features Used SparkStreaming APIs to perform necessary transformations and actions on the fly for building the common learner data model which gets the data from Kafka in near real time Worked on data cleaning data preparation and feature engineering with Python 3X Used ML toolkit H2O for model fitting and to discover patterns in data Used H20 for data visualization and for different machine learning algorithms Used TensorFlow library for development and evaluation of Deep Learning Models Performed text classification task using NLTK package and implemented various natural language processing techniques Used Tableau for data visualization to create reports dashboards for insights and business process improvement Extensively used Pythons multiple data science packages like Pandas NumPy Matplotlib SciPy Scikitlearn and NLTK Used TensorFlow Python APIs to perform TensorFlow graphs Worked on Spark Python modules for machine learning and predictive analytics in Spark on AWS Worked on end to end pipe line in Spark Explored and analyzed the customer specific features by using Spark SQL Performed data imputation using Scikitlearn package in Python Created the dashboards and reports in tableau for visualizing the data in required format Collaborated with team members and translated functional requirements to technical requirements for development Conducted Code Review for the Fit Gap done by the team members Created Hive scripts to create external internal data tables on Hive Worked on creating datasets to load data into HIVE Environment Spark Apache Spark Hive Machine learning Python Numpy NLTK Pandas Scipy SQL Tableau HDFS Tableau DynamoDB Mongo DB SQL Server and ETL Data Scientist TracFone Miami FL March 2015 to December 2016 TracFone Wireless Inc is a prepaid mobile virtual network operator in the United States Puerto Rico and the US Virgin Islands The objective of the project is to analyze the Customer data work on Churn and build recommendation engines and automated customer scoring systems Responsibilities Enhancing data collection procedures to include information that is relevant for building analytic systems Processing cleansing and verifying the integrity of data used for analysis Doing adhoc analysis and presenting results in a clear manner Constant tracking of model performance Excellent understanding of machine learning techniques and algorithms such as Logistic Regression SVM Random Forests Deep Learning etc Worked with Data governance Data quality Data lineage Data architect to design various models Independently coded new programs and designed Tables to load and test the program effectively for the given POCs Extending companys data with third party sources of information when needed Designed data models and data flow diagrams using Erwin and MS Visio Developed Implemented maintained the Conceptual Logical Physical Data Models using Erwin for ForwardReverse Engineered Databases Experience with common data science toolkits such as R Python Spark etc Good applied statistics skills such as statistical sampling testing regression etc Build analytic models using a variety of techniques such as logistic regression risk scorecards and pattern recognition technologies Analyze and understand large amounts of data to determine suitability for use in models and then work to segment the data create variables build models and test those models Work with technical and development teams to deploy models Build Model Performance Reports and Modeling Technical Documentation to support each of the models for the product line Developed new reports in SAS using SAS ODS PROC REPORT PROC TABULATE and PROC SQL Imported Data from relational database into SAS files per detailed specifications Responsible for the development and maintenance of SAS Information Maps for Analytics and Business Forecasting Team Performed Exploratory Data Analysis and Data Visualizations using R and Tableau Perform a proper EDA Univariate and bivariate analysis to understand the intrinsic effectcombined Established Data architecture strategy best practices standards and roadmaps Lead the development and presentation of a data analytics datahub prototype with the help of the other members of the emerging solutions team Experience in SAS programming for auditing data developing data performing data validation QA and improve efficiency of SAS programs Involved in analysis of Business requirement Design and Development of High level and Lowlevel designs Unit and Integration testing Worked with several R packages including knitr dplyr SparkR CausalInfer spacetime Interacted with the other departments to understand and identify data needs and requirements Environment UNIX Python 352 MLLib SAS regression logistic regression Hadoop NoSQL Teradata OLTP Random forest OLAP HDFS ODS Data Science Analyst Kindred Health Louisville KY April 2013 to February 2015 Kindred Healthcare Incorporated is a healthcare services company that operates hospitals nursing centers and contract rehabilitation services across the United States The project involved in predictive analysis and trend analysis using the patient historical data Responsibilities Extracted Transformed and loaded data from given source to analysis Handson implementation of R Python Hadoop Tableau and SAS to extract and import data Had a pleasure working experience on Spark spark streaming spark SQL Scala and Kafka Also converting HiveSQL queries into Spark transformations using Spark RDDs and Scala Used Kafka to load data in to HDFS and move data into NoSQL databases Worked on Spark SQL and Data frames for faster execution of Hive queries using Spark Sql Context Gained extreme knowledge on Map Reduce using Python Hive queries using Oozie workflows Performed Data cleaning process applied Backward Forward filling methods on dataset for handling missing values Design built and deployed a set of python modelling APIs for customer analytics which integrate multiple machine learning techniques for various user behavior prediction and support multiple marketing segmentation programs Segmented the customers based on demographics using Kmeans Clustering Explored different regression and ensemble models in machine learning to perform forecasting Used classification techniques including Random Forest and Logistic Regression to quantify the likelihood of each user referring Performed Boosting method on predicted model for the improve efficiency of the model Environment RR studio Informatica SQLPLSQL Oracle 10g MSOffice Teradata Data Analyst SunTrust Bank Atlanta GA October 2011 to March 2013 Worked for Credit Card Management Team and involved in calculating the Credit Card Performance by dividing the data into different DataMarts and performing analysis on the data and credit risk calculation for the Customers and lending money Mortgaging Responsibilities The team of Data analysts focused on providing analytics insights and decision support tools for executives for accurate decision making Applied highly advanced data access routines to extract data from source systems for monitoring operations compliance to banking Laws Rules and Regulations using Visual Basic Apps VBA SQL Server SSIS SAS and SQL Identified measured and recommended improvement strategies for KPIs across all business areas Assisted in defining implementing and utilizing business metrics calculations and methodologies Designed and provided complex excel reports including summaries charts and graphs to interpret findings to team and stakeholders Assisted the team for standardization of reports using SAS macros and SQL Responsible for creation of Credit data related warehouse to help with Risk Assessment for Commercial loans Performed competitor and customer analysis risk and pricing analysis and forecasted results for credit card holders on demographical basis Created macros and used existing macros to develop SAS programs for data analysis Created and manipulated various management reports in MS Excel for sales metrics using VLOOKUP and Pivot tables Developed transformation logic for BI tools Informatica for data transformation into various layers in Data warehouse Utilized SQL to develop stored procedures views to create result sets to meet varying reporting requirements Used advanced excel formulas lookup functions pivot table If Statements etc for analyzing data Identified process improvements that significantly reduce workloads or improve quality Worked for BI Analytics team to conduct AB testing data extraction and exploratory analysis Generated dashboards and presented the analysis to researchers explaining insights on the data Environment Excel 2010 R Informatica Power Center 90 MS SQL Server 200 Data Analyst Python Developer Teleparadigm Networks Private Limited May 2009 to September 2011 Teleparadigm Networks delivers software and IT services to clientele across the globe under service models Have a wide client base in Healthcare and Telecom Industries Responsibilities Brought in and implemented updated analytical methods such as regression modelling classification tree statistical tests and data visualization techniques with Python Analyzed customer Help data contact volumes and other operational data in MySQL to provide insights that enable improvements to Help content and customer experience Maintained and updated existing automated solutions Improved data collection and distribution processes by using pandas and Numpy packages in Python while enhancing reporting capabilities to provide clear line of sight into key performance trends and metrics Analysed historical demand filter out outliersexceptions identify the most appropriate statistical forecasting algorithm develop base plan understand variance propose improvement opportunities and incorporate demand signal into forecast and executed data visualization by using plotly package in Python Interacted with QA to develop test plans from highlevel design documentation Environment MySQL Statistical modelling Python libraries pandas and Numpy packages Education Bachelors in Feature Engineering and Selection Stanford NER Tagger Skills APACHE HADOOP HDFS 5 years HADOOP DISTRIBUTED FILE SYSTEM 5 years HDFS 5 years Python 7 years SQL 6 years",
    "unique_id": "6f1a0b72-e1be-438c-9857-e7dc77b9ca56"
}