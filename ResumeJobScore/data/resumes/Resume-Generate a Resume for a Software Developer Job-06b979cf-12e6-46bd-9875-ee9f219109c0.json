{
    "clean_data": "Data ScientistMachine Learning Engineer Data ScientistMachine Learning Engineer Data ScientistMachine Learning Engineer EBay San Jose CA Close to 10 years of expert involvement in IT in which I have 3 years of knowledge in Data Mining Machine Learning and Spark Development with big datasets of Structured and Unstructured Data Data Acquisition Data Validation Predictive demonstrating Data Visualization Capable in measurable programming languages like R and Python Proficient in managing entire data science project life cycle and actively involved in all the phases of project life cycle Extensive experience in Text Analytics developing different Statistical Machine Learning Data mining solutions to various business problems and generating data visualizations using R Python and Tableau Adept and deep understanding of Statistical modeling Multivariate Analysis model testing problem analysis model comparison and validation Skilled in performing data parsing data manipulation and data preparation with methods including describe data contents compute descriptive statistics of data regex split and combine Remap merge subset reindex melt and reshape Experience in using various packages in R and libraries in Python Working knowledge in Hadoop Hive and NOSQL databases like Cassandra and HBase Hands on experience in implementing LDA Naive Bayes and skilled in Random Forests Decision Trees Linear and Logistic Regression SVM Clustering neural networks Principle Component Analysis and good knowledge on Recommender Systems Good industry knowledge analytical and problemsolving skills and ability to work well within a team as well as an individual Highly creative innovative committed intellectually curious business savvy with effective communication and interpersonal skills I can be able to quickly adapt the new work pace and learning Authorized to work in the US for any employer Work Experience Data ScientistMachine Learning Engineer EBay San Jose CA January 2017 to Present Roles Responsibilities Analyzed Trading mechanism for realtime transactions and build collateral management tools Compiled data from various sources to perform complex analysis for actionable results Utilized machine learning algorithms such as linear regression multivariate regression naive bayes Random Forests Kmeans KNN for data analysis Measured Efficiency of HadoopHive environment ensuring SLA is met Developed Spark code using Scala and SparkSQLStreaming for faster processing of data Prepared Spark build from the source code and ran the PIG Scripts using Spark rather using MR jobs for better performance Analyzing the system for new enhancementsfunctionalities and perform Impact analysis of the application for implementing ETL changes Imported data using Sqoop to load data from MySQL to HDFS on regular basis Developed Scripts and Batch Job to schedule various Hadoop Program Used TensorFlow to train the model from insightful data and look at thousands of examples Designing developing and optimizing SQL code DDL DML Building performant scalable ETL processes to load cleanse and validate data Expertise in Data archival and Data migration adhoc reporting and code utilizing SAS on UNIX and Windows Environments Tested and debugged SAS programs against the test data Processed the data in SAS for the given requirement using SAS programming concepts Imported and Exported data files to and from SAS using Proc Import and Proc Export from Excel and various delimited textbased data files such as TXT tab delimited and CSV comma delimited files into SAS datasets for analysis Expertise in producing RTF PDF HTML files using SAS ODS facility Providing support for data processes This will involve monitoring data profiling database usage trouble shooting tuning and ensuring data integrity Participating in the full software development lifecycle with requirements solution design development QA implementation and product support using Scrum and other Agile methodologies Collaborate with team members and stakeholders in design and development of data environment Learning new tools and skillsets as needs arise Preparing associated documentation for specifications requirements and testing Optimizing the Tensorflow Model for an efficiency Used Tensorflow for text summarization Used Spark API over Cloudera Hadoop YARN to perform analytics on data in Hive Wrote Hive queries for data analysis to meet the business requirements Developed Kafka producer and consumers for message handling Responsible for analyzing multiplatform applications using python Used storm for an automatic mechanism to analyze large amounts of nonunique data points with low latency and high throughput Developed MapReduce jobs in Python for data cleaning and data processing Environment Machine learning AWS MS Azure Cassandra SAS Spark HDFS Hive Pig Linux Anaconda Python MySQL Eclipse PLSQL SQL connector SparkML Data ScientistMachine Learning Engineer Macys Inc Duluth GA August 2015 to December 2017 Roles Responsibilities Performed Data Profiling to learn about user behaviour and merged data from multiple data sources Participated in all phases of data mining data collection data cleaning developing models validation visualization and performed Gap analysis Performed Kmeans clustering Multivariate analysis and Support Vector Machines in Python and R Developed Clustering algorithms and Support Vector Machines that improved Customer segmentation and Market Expansion Professional Tableau user Desktop Online and Server Data Story teller Mining Data from different Data Source such as SQL Server Oracle Cube Database Web Analytics Business Object and hadoop Providing AD hoc analysis and reports to Executive level management team Data Manipulation and Aggregation from different source using Nexus Toad Business Objects Power BI and Smart View In Unix development environment for Financial application reports used batch processes and models using Perl and Korn shell scripts with partitions and subpartitions on oracle database Developed analytics and strategy to integrate B2B analytics in outbound calling operations Implemented analytics delivery on cloudbased visualization using shiny tool for Business Object and Google analytics platform SPOC Data Scientist and predictive analyst to create annual and quarterly Business forecast reports Main source of Business Regression report Creating various B2B Predictive and descriptive analytics using R and Tableau Creating and automating ad hoc reports Responsible for planning scheduling new product releases and promotional offers Worked on NOSQL databases like Cassandra Experienced in Agile methodologies and SCRUM process Parsing data producing concise conclusions from raw data in a clean wellstructured and easily maintainable format Extracted data from HDFS and prepared data for exploratory analysis using data munging Worked on Text Analytics Naive Bayes Sentiment analysis creating word clouds and retrieving data from Twitter and other social networking platforms Extensive experience and proficiency in using SAS ODS to create output files in a variety of formats including RTF HTML and PDF Worked on different data formats such as JSON XML and performed machine learning algorithms in R and Python Environment R Python UNIX Scripting SAS Cassandra Java Hadoop MapReduce HDFS Pig Sqoop Hive Oracle Eclipse Python Developer Myntra Bengaluru Karnataka April 2012 to June 2015 Roles Responsibilities Worked on the project from gathering requirements to developing the entire application Worked on Anaconda Python Environment Created activated and programmed in Anaconda environment Wrote programs for performance calculations using NumPy and SQLAlchemy Wrote python routines to log into the websites and fetch data for selected options Used python modules of urllib urllib2 Requests for web crawling Experience using all these ML techniques clustering regression classification graphical models Extensive experience in Text Analytics developing different Statistical Machine Learning Data mining solutions to various business problems and generating data visualizations using R Python and Tableau Used with other packages such as Beautiful Soup for data parsing Involved in development of Web Services using SOAP for sending and getting data from the external interface in the XML format Used with other packages such as Beautiful Soup for data parsing Worked on development of SQL and stored procedures on MYSQL Analyzed the code completely and have reduced the code redundancy to the optimal level Design and build a text classification application using different text classification models Used Jira for defect tracking and project management Worked on writing and as well as read data from CSV and excel file formats Involved in Sprint planning sessions and participated in the daily Agile SCRUM meetings Conducted every day scrum as part of the SCRUM Master role Developed the project in Linux environment Worked on resulting reports of the application Performed QA testing on the application Held meetings with client and worked for the entire project with limited help from the client Environment Python Anaconda Sypder IDE Windows 7 Teradata Requests urllib urllib2 Beautiful Soup Tableau python libraries such as NumPy SQL Alchemy  Python developer Credit Sussie Pune Maharashtra August 2008 to March 2012 Roles Responsibilities Using python libraries for machine learning like pandas numpy matplotlib sklearn scipy to Load the dataset summarizing the dataset visualizing the dataset evaluating some algorithms and making some predictions Perform application development maintenance and ensure adherence to process and information security control AWS vpc EC2 S3 route 53 A key part of API design and development Unit test and debugging Maintain and extend the existing PythonFlask REST A strong eye on code reusability and maintainability Strong background in objectoriented programming design patterns algorithms and data structures Used Python scripts to update the content in database and manipulate files Generated Python Django forms to maintain the record of online users Used Django APIs to access the database Writing Unit Functional and Integration test cases for Cloud Computing applications on AWS Writing Python scripts with Cloud Formation templates to automate installation of Auto scaling EC2 VPC and other services Designed and managed API system deployment using fast http server and AWS architecture Developed Restful APIs using Python Flask and SQL Alchemy data models as well as ensured code Designed and managed API system deployment using fast http server and Amazon AWS architecture Designed and developed a horizontally scalable APIs using Python Flask Involved in back end development using Python with framework Flask Wrote Python modules to view and connect the Apache Cassandra instance Created Unit test regression test framework for workingnew code Responsible for designing developing testing deploying and maintaining the web application Wrote and executed various MySQL database queries from Python MySQL connector and MySQL Db package Involved in debugging and troubleshooting issues and fixed many bugs in two of the main applications Which is main source of data for customers and internal customer service team Implemented SOAPRESTful web services in JSON format Attended many days to day meetings with developers and users and performed QA testing on the application Environment Python Django API HTML CSS AJAX Git AWS Apache HTTP Flask XML OOD Shell Scripting MYSQL Cassandra Education Bachelor ECE in JNTUH University Bachelors Skills algorithms 8 years API 5 years Machine Learning 10 years python 10 years SQL 10 years Expertise Scikitlearn NLTK spaCy NumPy SciPy OpenCv Deep learning NLP RNN CNN Tensor flow Keras matplotlib Microsoft Visual Studio Microsoft Office 10 years Additional Information TECHNICAL SKILLS Expertise Scikitlearn NLTK spaCy NumPy SciPy OpenCv Deep learning NLP RNN CNN Tensor flow Keras matplotlib Microsoft Visual Studio Microsoft Office Machine Learning Algorithms Linear Regression Logistic Regression Decision Trees Random Forest KMeans Clustering Support Vector Machines Gradient Boost Machines XGBoost Neural Networks Data Analysis Skills Data Cleaning Data Visualization Feature Selection Pandas Operating Systems Windows Mac and Linux Unix Programming Languages Python SQL R Matlab Torch C C Java Octave Apache Spark Hadoop Spark ML Other Programming Knowledge and Skills ElasticSearch Data Scraping RESTfulApi using Django Web Frame work Tools Toad Erwin AWS AzureD3 Mule Soft Alteryx Tableau Shiny Adobe Analytics Anaconda",
    "entities": [
        "Python Proficient",
        "Customer",
        "HDFS",
        "UNIX",
        "Business Regression",
        "Writing Unit Functional and Integration",
        "Keras",
        "JSON",
        "Business Object",
        "Beautiful Soup",
        "Responsibilities Performed Data Profiling",
        "TensorFlow",
        "Google",
        "SOAP",
        "RTF HTML",
        "XML",
        "NOSQL",
        "Python Working",
        "PDF Worked",
        "Cloudera Hadoop",
        "Data Source",
        "Data ScientistMachine Learning Engineer",
        "Hadoop Program",
        "Utilized",
        "Work Experience Data ScientistMachine Learning Engineer",
        "Hive Wrote Hive",
        "San Jose",
        "LDA Naive Bayes",
        "Processed",
        "PDF HTML",
        "Linux",
        "Data ScientistMachine Learning Engineer Data ScientistMachine Learning Engineer Data ScientistMachine Learning Engineer",
        "Worked",
        "Support Vector Machines",
        "Created Unit",
        "Spark Development",
        "Optimizing the Tensorflow Model",
        "Spark",
        "Measured Efficiency of HadoopHive",
        "linear",
        "CSV",
        "Additional Information TECHNICAL SKILLS Expertise Scikitlearn NLTK",
        "API",
        "Tableau Adept",
        "Multivariate Analysis",
        "Hadoop Hive",
        "US",
        "Sqoop",
        "Perform",
        "QA",
        "Worked on Anaconda Python Environment Created",
        "KNN",
        "Proc Import",
        "Skills ElasticSearch Data Scraping",
        "CA",
        "B2B Predictive",
        "MR",
        "Text Analytics",
        "Unstructured Data Data Acquisition Data Validation Predictive",
        "Market Expansion Professional Tableau",
        "SAS",
        "SQL",
        "Microsoft Office",
        "NLP",
        "SQL Alchemy",
        "Anaconda",
        "Nexus Toad Business Objects Power BI",
        "Amazon AWS",
        "Statistical Machine Learning Data",
        "ETL",
        "Learning",
        "Desktop Online",
        "SQL Server Oracle",
        "NumPy SQL Alchemy",
        "Random Forests Decision Trees Linear and Logistic Regression SVM Clustering",
        "Django Web Frame",
        "SPOC Data Scientist",
        "Principle Component Analysis",
        "Recommender Systems Good",
        "CNN",
        "ML",
        "Mining Data",
        "Linux Unix Programming Languages",
        "Data",
        "Structured",
        "Data Mining Machine Learning",
        "PythonFlask",
        "Sprint",
        "RTF",
        "SLA",
        "Data Visualization Capable",
        "Microsoft Visual Studio",
        "TXT",
        "JSON XML"
    ],
    "experience": "Experience in using various packages in R and libraries in Python Working knowledge in Hadoop Hive and NOSQL databases like Cassandra and HBase Hands on experience in implementing LDA Naive Bayes and skilled in Random Forests Decision Trees Linear and Logistic Regression SVM Clustering neural networks Principle Component Analysis and good knowledge on Recommender Systems Good industry knowledge analytical and problemsolving skills and ability to work well within a team as well as an individual Highly creative innovative committed intellectually curious business savvy with effective communication and interpersonal skills I can be able to quickly adapt the new work pace and learning Authorized to work in the US for any employer Work Experience Data ScientistMachine Learning Engineer EBay San Jose CA January 2017 to Present Roles Responsibilities Analyzed Trading mechanism for realtime transactions and build collateral management tools Compiled data from various sources to perform complex analysis for actionable results Utilized machine learning algorithms such as linear regression multivariate regression naive bayes Random Forests Kmeans KNN for data analysis Measured Efficiency of HadoopHive environment ensuring SLA is met Developed Spark code using Scala and SparkSQLStreaming for faster processing of data Prepared Spark build from the source code and ran the PIG Scripts using Spark rather using MR jobs for better performance Analyzing the system for new enhancementsfunctionalities and perform Impact analysis of the application for implementing ETL changes Imported data using Sqoop to load data from MySQL to HDFS on regular basis Developed Scripts and Batch Job to schedule various Hadoop Program Used TensorFlow to train the model from insightful data and look at thousands of examples Designing developing and optimizing SQL code DDL DML Building performant scalable ETL processes to load cleanse and validate data Expertise in Data archival and Data migration adhoc reporting and code utilizing SAS on UNIX and Windows Environments Tested and debugged SAS programs against the test data Processed the data in SAS for the given requirement using SAS programming concepts Imported and Exported data files to and from SAS using Proc Import and Proc Export from Excel and various delimited textbased data files such as TXT tab delimited and CSV comma delimited files into SAS datasets for analysis Expertise in producing RTF PDF HTML files using SAS ODS facility Providing support for data processes This will involve monitoring data profiling database usage trouble shooting tuning and ensuring data integrity Participating in the full software development lifecycle with requirements solution design development QA implementation and product support using Scrum and other Agile methodologies Collaborate with team members and stakeholders in design and development of data environment Learning new tools and skillsets as needs arise Preparing associated documentation for specifications requirements and testing Optimizing the Tensorflow Model for an efficiency Used Tensorflow for text summarization Used Spark API over Cloudera Hadoop YARN to perform analytics on data in Hive Wrote Hive queries for data analysis to meet the business requirements Developed Kafka producer and consumers for message handling Responsible for analyzing multiplatform applications using python Used storm for an automatic mechanism to analyze large amounts of nonunique data points with low latency and high throughput Developed MapReduce jobs in Python for data cleaning and data processing Environment Machine learning AWS MS Azure Cassandra SAS Spark HDFS Hive Pig Linux Anaconda Python MySQL Eclipse PLSQL SQL connector SparkML Data ScientistMachine Learning Engineer Macys Inc Duluth GA August 2015 to December 2017 Roles Responsibilities Performed Data Profiling to learn about user behaviour and merged data from multiple data sources Participated in all phases of data mining data collection data cleaning developing models validation visualization and performed Gap analysis Performed Kmeans clustering Multivariate analysis and Support Vector Machines in Python and R Developed Clustering algorithms and Support Vector Machines that improved Customer segmentation and Market Expansion Professional Tableau user Desktop Online and Server Data Story teller Mining Data from different Data Source such as SQL Server Oracle Cube Database Web Analytics Business Object and hadoop Providing AD hoc analysis and reports to Executive level management team Data Manipulation and Aggregation from different source using Nexus Toad Business Objects Power BI and Smart View In Unix development environment for Financial application reports used batch processes and models using Perl and Korn shell scripts with partitions and subpartitions on oracle database Developed analytics and strategy to integrate B2B analytics in outbound calling operations Implemented analytics delivery on cloudbased visualization using shiny tool for Business Object and Google analytics platform SPOC Data Scientist and predictive analyst to create annual and quarterly Business forecast reports Main source of Business Regression report Creating various B2B Predictive and descriptive analytics using R and Tableau Creating and automating ad hoc reports Responsible for planning scheduling new product releases and promotional offers Worked on NOSQL databases like Cassandra Experienced in Agile methodologies and SCRUM process Parsing data producing concise conclusions from raw data in a clean wellstructured and easily maintainable format Extracted data from HDFS and prepared data for exploratory analysis using data munging Worked on Text Analytics Naive Bayes Sentiment analysis creating word clouds and retrieving data from Twitter and other social networking platforms Extensive experience and proficiency in using SAS ODS to create output files in a variety of formats including RTF HTML and PDF Worked on different data formats such as JSON XML and performed machine learning algorithms in R and Python Environment R Python UNIX Scripting SAS Cassandra Java Hadoop MapReduce HDFS Pig Sqoop Hive Oracle Eclipse Python Developer Myntra Bengaluru Karnataka April 2012 to June 2015 Roles Responsibilities Worked on the project from gathering requirements to developing the entire application Worked on Anaconda Python Environment Created activated and programmed in Anaconda environment Wrote programs for performance calculations using NumPy and SQLAlchemy Wrote python routines to log into the websites and fetch data for selected options Used python modules of urllib urllib2 Requests for web crawling Experience using all these ML techniques clustering regression classification graphical models Extensive experience in Text Analytics developing different Statistical Machine Learning Data mining solutions to various business problems and generating data visualizations using R Python and Tableau Used with other packages such as Beautiful Soup for data parsing Involved in development of Web Services using SOAP for sending and getting data from the external interface in the XML format Used with other packages such as Beautiful Soup for data parsing Worked on development of SQL and stored procedures on MYSQL Analyzed the code completely and have reduced the code redundancy to the optimal level Design and build a text classification application using different text classification models Used Jira for defect tracking and project management Worked on writing and as well as read data from CSV and excel file formats Involved in Sprint planning sessions and participated in the daily Agile SCRUM meetings Conducted every day scrum as part of the SCRUM Master role Developed the project in Linux environment Worked on resulting reports of the application Performed QA testing on the application Held meetings with client and worked for the entire project with limited help from the client Environment Python Anaconda Sypder IDE Windows 7 Teradata Requests urllib urllib2 Beautiful Soup Tableau python libraries such as NumPy SQL Alchemy   Python developer Credit Sussie Pune Maharashtra August 2008 to March 2012 Roles Responsibilities Using python libraries for machine learning like pandas numpy matplotlib sklearn scipy to Load the dataset summarizing the dataset visualizing the dataset evaluating some algorithms and making some predictions Perform application development maintenance and ensure adherence to process and information security control AWS vpc EC2 S3 route 53 A key part of API design and development Unit test and debugging Maintain and extend the existing PythonFlask REST A strong eye on code reusability and maintainability Strong background in objectoriented programming design patterns algorithms and data structures Used Python scripts to update the content in database and manipulate files Generated Python Django forms to maintain the record of online users Used Django APIs to access the database Writing Unit Functional and Integration test cases for Cloud Computing applications on AWS Writing Python scripts with Cloud Formation templates to automate installation of Auto scaling EC2 VPC and other services Designed and managed API system deployment using fast http server and AWS architecture Developed Restful APIs using Python Flask and SQL Alchemy data models as well as ensured code Designed and managed API system deployment using fast http server and Amazon AWS architecture Designed and developed a horizontally scalable APIs using Python Flask Involved in back end development using Python with framework Flask Wrote Python modules to view and connect the Apache Cassandra instance Created Unit test regression test framework for workingnew code Responsible for designing developing testing deploying and maintaining the web application Wrote and executed various MySQL database queries from Python MySQL connector and MySQL Db package Involved in debugging and troubleshooting issues and fixed many bugs in two of the main applications Which is main source of data for customers and internal customer service team Implemented SOAPRESTful web services in JSON format Attended many days to day meetings with developers and users and performed QA testing on the application Environment Python Django API HTML CSS AJAX Git AWS Apache HTTP Flask XML OOD Shell Scripting MYSQL Cassandra Education Bachelor ECE in JNTUH University Bachelors Skills algorithms 8 years API 5 years Machine Learning 10 years python 10 years SQL 10 years Expertise Scikitlearn NLTK spaCy NumPy SciPy OpenCv Deep learning NLP RNN CNN Tensor flow Keras matplotlib Microsoft Visual Studio Microsoft Office 10 years Additional Information TECHNICAL SKILLS Expertise Scikitlearn NLTK spaCy NumPy SciPy OpenCv Deep learning NLP RNN CNN Tensor flow Keras matplotlib Microsoft Visual Studio Microsoft Office Machine Learning Algorithms Linear Regression Logistic Regression Decision Trees Random Forest KMeans Clustering Support Vector Machines Gradient Boost Machines XGBoost Neural Networks Data Analysis Skills Data Cleaning Data Visualization Feature Selection Pandas Operating Systems Windows Mac and Linux Unix Programming Languages Python SQL R Matlab Torch C C Java Octave Apache Spark Hadoop Spark ML Other Programming Knowledge and Skills ElasticSearch Data Scraping RESTfulApi using Django Web Frame work Tools Toad Erwin AWS AzureD3 Mule Soft Alteryx Tableau Shiny Adobe Analytics Anaconda",
    "extracted_keywords": [
        "Data",
        "ScientistMachine",
        "Learning",
        "Engineer",
        "Data",
        "ScientistMachine",
        "Learning",
        "Engineer",
        "Data",
        "ScientistMachine",
        "Learning",
        "Engineer",
        "EBay",
        "San",
        "Jose",
        "CA",
        "years",
        "expert",
        "involvement",
        "IT",
        "years",
        "knowledge",
        "Data",
        "Mining",
        "Machine",
        "Learning",
        "Spark",
        "Development",
        "datasets",
        "Structured",
        "Unstructured",
        "Data",
        "Data",
        "Acquisition",
        "Data",
        "Validation",
        "Predictive",
        "demonstrating",
        "Data",
        "Visualization",
        "Capable",
        "programming",
        "languages",
        "R",
        "Python",
        "Proficient",
        "data",
        "science",
        "project",
        "life",
        "cycle",
        "phases",
        "project",
        "life",
        "cycle",
        "experience",
        "Text",
        "Analytics",
        "Machine",
        "Learning",
        "Data",
        "mining",
        "solutions",
        "business",
        "problems",
        "data",
        "visualizations",
        "R",
        "Python",
        "Tableau",
        "Adept",
        "understanding",
        "modeling",
        "Multivariate",
        "Analysis",
        "model",
        "testing",
        "problem",
        "analysis",
        "model",
        "comparison",
        "validation",
        "data",
        "data",
        "manipulation",
        "data",
        "preparation",
        "methods",
        "describe",
        "data",
        "contents",
        "statistics",
        "data",
        "regex",
        "Remap",
        "merge",
        "subset",
        "reindex",
        "melt",
        "Experience",
        "packages",
        "R",
        "libraries",
        "Python",
        "Working",
        "knowledge",
        "Hadoop",
        "Hive",
        "NOSQL",
        "Cassandra",
        "HBase",
        "Hands",
        "experience",
        "LDA",
        "Naive",
        "Bayes",
        "Random",
        "Forests",
        "Decision",
        "Trees",
        "Linear",
        "Logistic",
        "Regression",
        "SVM",
        "networks",
        "Principle",
        "Component",
        "Analysis",
        "knowledge",
        "Recommender",
        "Systems",
        "Good",
        "industry",
        "knowledge",
        "skills",
        "ability",
        "team",
        "business",
        "communication",
        "skills",
        "work",
        "pace",
        "Authorized",
        "US",
        "employer",
        "Work",
        "Experience",
        "Data",
        "ScientistMachine",
        "Learning",
        "Engineer",
        "EBay",
        "San",
        "Jose",
        "CA",
        "January",
        "Present",
        "Roles",
        "Responsibilities",
        "Trading",
        "mechanism",
        "transactions",
        "management",
        "tools",
        "data",
        "sources",
        "analysis",
        "results",
        "machine",
        "learning",
        "algorithms",
        "linear",
        "regression",
        "multivariate",
        "regression",
        "bayes",
        "Random",
        "Forests",
        "Kmeans",
        "KNN",
        "data",
        "analysis",
        "Efficiency",
        "HadoopHive",
        "environment",
        "SLA",
        "Developed",
        "Spark",
        "code",
        "Scala",
        "SparkSQLStreaming",
        "processing",
        "data",
        "Prepared",
        "Spark",
        "source",
        "code",
        "PIG",
        "Scripts",
        "Spark",
        "MR",
        "jobs",
        "performance",
        "system",
        "enhancementsfunctionalities",
        "Impact",
        "analysis",
        "application",
        "ETL",
        "changes",
        "data",
        "Sqoop",
        "data",
        "MySQL",
        "HDFS",
        "basis",
        "Scripts",
        "Batch",
        "Job",
        "Hadoop",
        "Program",
        "TensorFlow",
        "model",
        "data",
        "thousands",
        "examples",
        "SQL",
        "code",
        "DDL",
        "DML",
        "ETL",
        "processes",
        "cleanse",
        "data",
        "Expertise",
        "Data",
        "archival",
        "Data",
        "migration",
        "adhoc",
        "reporting",
        "code",
        "SAS",
        "UNIX",
        "Windows",
        "Environments",
        "SAS",
        "programs",
        "test",
        "data",
        "data",
        "SAS",
        "requirement",
        "SAS",
        "programming",
        "concepts",
        "data",
        "files",
        "SAS",
        "Proc",
        "Import",
        "Proc",
        "Export",
        "Excel",
        "data",
        "files",
        "TXT",
        "tab",
        "CSV",
        "comma",
        "files",
        "SAS",
        "datasets",
        "analysis",
        "Expertise",
        "RTF",
        "PDF",
        "HTML",
        "files",
        "SAS",
        "ODS",
        "facility",
        "support",
        "data",
        "processes",
        "data",
        "profiling",
        "database",
        "usage",
        "trouble",
        "data",
        "integrity",
        "software",
        "development",
        "lifecycle",
        "requirements",
        "solution",
        "design",
        "development",
        "QA",
        "implementation",
        "product",
        "support",
        "Scrum",
        "methodologies",
        "team",
        "members",
        "stakeholders",
        "design",
        "development",
        "data",
        "environment",
        "tools",
        "skillsets",
        "needs",
        "documentation",
        "specifications",
        "requirements",
        "testing",
        "Tensorflow",
        "Model",
        "efficiency",
        "Tensorflow",
        "text",
        "summarization",
        "Spark",
        "API",
        "Cloudera",
        "Hadoop",
        "YARN",
        "analytics",
        "data",
        "Hive",
        "Wrote",
        "Hive",
        "data",
        "analysis",
        "business",
        "requirements",
        "Kafka",
        "producer",
        "consumers",
        "message",
        "multiplatform",
        "applications",
        "python",
        "storm",
        "mechanism",
        "amounts",
        "data",
        "points",
        "latency",
        "throughput",
        "Developed",
        "MapReduce",
        "jobs",
        "Python",
        "data",
        "cleaning",
        "data",
        "Environment",
        "Machine",
        "AWS",
        "MS",
        "Azure",
        "Cassandra",
        "SAS",
        "Spark",
        "HDFS",
        "Hive",
        "Pig",
        "Linux",
        "Anaconda",
        "Python",
        "MySQL",
        "Eclipse",
        "PLSQL",
        "SQL",
        "connector",
        "SparkML",
        "Data",
        "ScientistMachine",
        "Learning",
        "Engineer",
        "Macys",
        "Inc",
        "Duluth",
        "GA",
        "August",
        "December",
        "Roles",
        "Responsibilities",
        "Performed",
        "Data",
        "Profiling",
        "user",
        "behaviour",
        "data",
        "data",
        "sources",
        "phases",
        "data",
        "mining",
        "data",
        "collection",
        "data",
        "models",
        "validation",
        "visualization",
        "analysis",
        "Performed",
        "Kmeans",
        "Multivariate",
        "analysis",
        "Support",
        "Vector",
        "Machines",
        "Python",
        "R",
        "algorithms",
        "Support",
        "Vector",
        "Machines",
        "Customer",
        "segmentation",
        "Market",
        "Expansion",
        "Professional",
        "Tableau",
        "user",
        "Desktop",
        "Online",
        "Server",
        "Data",
        "Story",
        "teller",
        "Mining",
        "Data",
        "Data",
        "Source",
        "SQL",
        "Server",
        "Oracle",
        "Cube",
        "Database",
        "Web",
        "Analytics",
        "Business",
        "Object",
        "hadoop",
        "AD",
        "analysis",
        "reports",
        "level",
        "management",
        "team",
        "Data",
        "Manipulation",
        "Aggregation",
        "source",
        "Nexus",
        "Toad",
        "Business",
        "Objects",
        "Power",
        "BI",
        "Smart",
        "View",
        "Unix",
        "development",
        "environment",
        "application",
        "reports",
        "batch",
        "processes",
        "models",
        "Perl",
        "Korn",
        "shell",
        "scripts",
        "partitions",
        "subpartitions",
        "oracle",
        "database",
        "Developed",
        "analytics",
        "strategy",
        "B2B",
        "analytics",
        "outbound",
        "calling",
        "operations",
        "analytics",
        "delivery",
        "visualization",
        "tool",
        "Business",
        "Object",
        "Google",
        "analytics",
        "platform",
        "SPOC",
        "Data",
        "Scientist",
        "analyst",
        "Business",
        "forecast",
        "source",
        "Business",
        "Regression",
        "report",
        "B2B",
        "analytics",
        "R",
        "Tableau",
        "Creating",
        "ad",
        "scheduling",
        "product",
        "releases",
        "offers",
        "NOSQL",
        "databases",
        "Cassandra",
        "methodologies",
        "SCRUM",
        "process",
        "data",
        "conclusions",
        "data",
        "format",
        "data",
        "HDFS",
        "data",
        "analysis",
        "data",
        "munging",
        "Text",
        "Analytics",
        "Naive",
        "Bayes",
        "Sentiment",
        "analysis",
        "word",
        "clouds",
        "data",
        "Twitter",
        "networking",
        "platforms",
        "experience",
        "proficiency",
        "SAS",
        "ODS",
        "output",
        "files",
        "variety",
        "formats",
        "RTF",
        "HTML",
        "PDF",
        "data",
        "formats",
        "JSON",
        "XML",
        "machine",
        "algorithms",
        "R",
        "Python",
        "Environment",
        "R",
        "Python",
        "UNIX",
        "Scripting",
        "SAS",
        "Cassandra",
        "Java",
        "Hadoop",
        "MapReduce",
        "HDFS",
        "Pig",
        "Sqoop",
        "Hive",
        "Oracle",
        "Eclipse",
        "Python",
        "Developer",
        "Myntra",
        "Bengaluru",
        "Karnataka",
        "April",
        "June",
        "Roles",
        "Responsibilities",
        "project",
        "requirements",
        "application",
        "Anaconda",
        "Python",
        "Environment",
        "Anaconda",
        "environment",
        "Wrote",
        "programs",
        "performance",
        "calculations",
        "NumPy",
        "SQLAlchemy",
        "python",
        "routines",
        "websites",
        "data",
        "options",
        "modules",
        "Requests",
        "web",
        "Experience",
        "ML",
        "techniques",
        "regression",
        "classification",
        "models",
        "experience",
        "Text",
        "Analytics",
        "Machine",
        "Learning",
        "Data",
        "mining",
        "solutions",
        "business",
        "problems",
        "data",
        "visualizations",
        "R",
        "Python",
        "Tableau",
        "packages",
        "Beautiful",
        "Soup",
        "data",
        "development",
        "Web",
        "Services",
        "SOAP",
        "data",
        "interface",
        "XML",
        "format",
        "packages",
        "Beautiful",
        "Soup",
        "data",
        "development",
        "SQL",
        "procedures",
        "MYSQL",
        "code",
        "code",
        "redundancy",
        "level",
        "Design",
        "text",
        "classification",
        "application",
        "text",
        "classification",
        "models",
        "Jira",
        "tracking",
        "project",
        "management",
        "writing",
        "data",
        "CSV",
        "file",
        "formats",
        "Sprint",
        "planning",
        "sessions",
        "Agile",
        "SCRUM",
        "meetings",
        "day",
        "part",
        "SCRUM",
        "Master",
        "role",
        "project",
        "Linux",
        "environment",
        "reports",
        "application",
        "Performed",
        "QA",
        "testing",
        "application",
        "meetings",
        "client",
        "project",
        "help",
        "client",
        "Environment",
        "Python",
        "Anaconda",
        "Sypder",
        "IDE",
        "Windows",
        "Teradata",
        "Requests",
        "Beautiful",
        "Soup",
        "Tableau",
        "NumPy",
        "SQL",
        "Alchemy",
        "Python",
        "developer",
        "Credit",
        "Sussie",
        "Pune",
        "Maharashtra",
        "August",
        "March",
        "Roles",
        "Responsibilities",
        "python",
        "libraries",
        "machine",
        "learning",
        "pandas",
        "numpy",
        "matplotlib",
        "dataset",
        "dataset",
        "dataset",
        "algorithms",
        "predictions",
        "application",
        "development",
        "maintenance",
        "adherence",
        "process",
        "information",
        "security",
        "control",
        "AWS",
        "vpc",
        "EC2",
        "S3",
        "route",
        "part",
        "API",
        "design",
        "development",
        "Unit",
        "test",
        "Maintain",
        "PythonFlask",
        "REST",
        "eye",
        "code",
        "reusability",
        "maintainability",
        "background",
        "programming",
        "design",
        "patterns",
        "algorithms",
        "data",
        "structures",
        "Python",
        "scripts",
        "content",
        "database",
        "manipulate",
        "files",
        "Python",
        "Django",
        "record",
        "users",
        "Django",
        "APIs",
        "database",
        "Writing",
        "Unit",
        "Functional",
        "Integration",
        "test",
        "cases",
        "Cloud",
        "Computing",
        "applications",
        "AWS",
        "Python",
        "scripts",
        "Cloud",
        "Formation",
        "installation",
        "Auto",
        "EC2",
        "VPC",
        "services",
        "API",
        "system",
        "deployment",
        "http",
        "server",
        "AWS",
        "APIs",
        "Python",
        "Flask",
        "SQL",
        "Alchemy",
        "data",
        "models",
        "code",
        "API",
        "system",
        "deployment",
        "http",
        "server",
        "Amazon",
        "AWS",
        "architecture",
        "APIs",
        "Python",
        "Flask",
        "end",
        "development",
        "Python",
        "framework",
        "Flask",
        "Wrote",
        "Python",
        "modules",
        "Apache",
        "Cassandra",
        "instance",
        "Created",
        "Unit",
        "test",
        "regression",
        "test",
        "framework",
        "code",
        "testing",
        "web",
        "application",
        "Wrote",
        "MySQL",
        "database",
        "Python",
        "MySQL",
        "connector",
        "MySQL",
        "Db",
        "package",
        "troubleshooting",
        "issues",
        "bugs",
        "applications",
        "source",
        "data",
        "customers",
        "customer",
        "service",
        "team",
        "web",
        "services",
        "format",
        "days",
        "day",
        "meetings",
        "developers",
        "users",
        "QA",
        "testing",
        "application",
        "Environment",
        "Python",
        "Django",
        "API",
        "HTML",
        "CSS",
        "AJAX",
        "Git",
        "AWS",
        "Apache",
        "HTTP",
        "Flask",
        "XML",
        "OOD",
        "Shell",
        "Scripting",
        "MYSQL",
        "Cassandra",
        "Education",
        "Bachelor",
        "ECE",
        "JNTUH",
        "University",
        "Bachelors",
        "Skills",
        "years",
        "API",
        "years",
        "Machine",
        "Learning",
        "years",
        "years",
        "SQL",
        "years",
        "Expertise",
        "Scikitlearn",
        "NLTK",
        "spaCy",
        "NumPy",
        "SciPy",
        "OpenCv",
        "Deep",
        "NLP",
        "RNN",
        "CNN",
        "Tensor",
        "flow",
        "Keras",
        "matplotlib",
        "Microsoft",
        "Visual",
        "Studio",
        "Microsoft",
        "Office",
        "years",
        "Information",
        "TECHNICAL",
        "SKILLS",
        "Expertise",
        "Scikitlearn",
        "NLTK",
        "spaCy",
        "NumPy",
        "SciPy",
        "OpenCv",
        "Deep",
        "NLP",
        "RNN",
        "CNN",
        "Tensor",
        "flow",
        "Keras",
        "matplotlib",
        "Microsoft",
        "Visual",
        "Studio",
        "Microsoft",
        "Office",
        "Machine",
        "Learning",
        "Algorithms",
        "Linear",
        "Regression",
        "Logistic",
        "Regression",
        "Decision",
        "Trees",
        "Random",
        "Forest",
        "KMeans",
        "Support",
        "Vector",
        "Machines",
        "Gradient",
        "Boost",
        "Machines",
        "XGBoost",
        "Neural",
        "Networks",
        "Data",
        "Analysis",
        "Skills",
        "Data",
        "Cleaning",
        "Data",
        "Visualization",
        "Feature",
        "Selection",
        "Pandas",
        "Operating",
        "Systems",
        "Windows",
        "Mac",
        "Linux",
        "Unix",
        "Programming",
        "Languages",
        "Python",
        "SQL",
        "R",
        "Matlab",
        "Torch",
        "C",
        "C",
        "Java",
        "Octave",
        "Apache",
        "Spark",
        "Hadoop",
        "Spark",
        "ML",
        "Programming",
        "Knowledge",
        "Skills",
        "ElasticSearch",
        "Data",
        "RESTfulApi",
        "Django",
        "Web",
        "Frame",
        "work",
        "Tools",
        "Toad",
        "Erwin",
        "AWS",
        "Mule",
        "Soft",
        "Alteryx",
        "Tableau",
        "Shiny",
        "Adobe",
        "Analytics",
        "Anaconda"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T22:02:43.820995",
    "resume_data": "Data ScientistMachine Learning Engineer Data ScientistMachine Learning Engineer Data ScientistMachine Learning Engineer EBay San Jose CA Close to 10 years of expert involvement in IT in which I have 3 years of knowledge in Data Mining Machine Learning and Spark Development with big datasets of Structured and Unstructured Data Data Acquisition Data Validation Predictive demonstrating Data Visualization Capable in measurable programming languages like R and Python Proficient in managing entire data science project life cycle and actively involved in all the phases of project life cycle Extensive experience in Text Analytics developing different Statistical Machine Learning Data mining solutions to various business problems and generating data visualizations using R Python and Tableau Adept and deep understanding of Statistical modeling Multivariate Analysis model testing problem analysis model comparison and validation Skilled in performing data parsing data manipulation and data preparation with methods including describe data contents compute descriptive statistics of data regex split and combine Remap merge subset reindex melt and reshape Experience in using various packages in R and libraries in Python Working knowledge in Hadoop Hive and NOSQL databases like Cassandra and HBase Hands on experience in implementing LDA Naive Bayes and skilled in Random Forests Decision Trees Linear and Logistic Regression SVM Clustering neural networks Principle Component Analysis and good knowledge on Recommender Systems Good industry knowledge analytical and problemsolving skills and ability to work well within a team as well as an individual Highly creative innovative committed intellectually curious business savvy with effective communication and interpersonal skills I can be able to quickly adapt the new work pace and learning Authorized to work in the US for any employer Work Experience Data ScientistMachine Learning Engineer EBay San Jose CA January 2017 to Present Roles Responsibilities Analyzed Trading mechanism for realtime transactions and build collateral management tools Compiled data from various sources to perform complex analysis for actionable results Utilized machine learning algorithms such as linear regression multivariate regression naive bayes Random Forests Kmeans KNN for data analysis Measured Efficiency of HadoopHive environment ensuring SLA is met Developed Spark code using Scala and SparkSQLStreaming for faster processing of data Prepared Spark build from the source code and ran the PIG Scripts using Spark rather using MR jobs for better performance Analyzing the system for new enhancementsfunctionalities and perform Impact analysis of the application for implementing ETL changes Imported data using Sqoop to load data from MySQL to HDFS on regular basis Developed Scripts and Batch Job to schedule various Hadoop Program Used TensorFlow to train the model from insightful data and look at thousands of examples Designing developing and optimizing SQL code DDL DML Building performant scalable ETL processes to load cleanse and validate data Expertise in Data archival and Data migration adhoc reporting and code utilizing SAS on UNIX and Windows Environments Tested and debugged SAS programs against the test data Processed the data in SAS for the given requirement using SAS programming concepts Imported and Exported data files to and from SAS using Proc Import and Proc Export from Excel and various delimited textbased data files such as TXT tab delimited and CSV comma delimited files into SAS datasets for analysis Expertise in producing RTF PDF HTML files using SAS ODS facility Providing support for data processes This will involve monitoring data profiling database usage trouble shooting tuning and ensuring data integrity Participating in the full software development lifecycle with requirements solution design development QA implementation and product support using Scrum and other Agile methodologies Collaborate with team members and stakeholders in design and development of data environment Learning new tools and skillsets as needs arise Preparing associated documentation for specifications requirements and testing Optimizing the Tensorflow Model for an efficiency Used Tensorflow for text summarization Used Spark API over Cloudera Hadoop YARN to perform analytics on data in Hive Wrote Hive queries for data analysis to meet the business requirements Developed Kafka producer and consumers for message handling Responsible for analyzing multiplatform applications using python Used storm for an automatic mechanism to analyze large amounts of nonunique data points with low latency and high throughput Developed MapReduce jobs in Python for data cleaning and data processing Environment Machine learning AWS MS Azure Cassandra SAS Spark HDFS Hive Pig Linux Anaconda Python MySQL Eclipse PLSQL SQL connector SparkML Data ScientistMachine Learning Engineer Macys Inc Duluth GA August 2015 to December 2017 Roles Responsibilities Performed Data Profiling to learn about user behaviour and merged data from multiple data sources Participated in all phases of data mining data collection data cleaning developing models validation visualization and performed Gap analysis Performed Kmeans clustering Multivariate analysis and Support Vector Machines in Python and R Developed Clustering algorithms and Support Vector Machines that improved Customer segmentation and Market Expansion Professional Tableau user Desktop Online and Server Data Story teller Mining Data from different Data Source such as SQL Server Oracle Cube Database Web Analytics Business Object and hadoop Providing AD hoc analysis and reports to Executive level management team Data Manipulation and Aggregation from different source using Nexus Toad Business Objects Power BI and Smart View In Unix development environment for Financial application reports used batch processes and models using Perl and Korn shell scripts with partitions and subpartitions on oracle database Developed analytics and strategy to integrate B2B analytics in outbound calling operations Implemented analytics delivery on cloudbased visualization using shiny tool for Business Object and Google analytics platform SPOC Data Scientist and predictive analyst to create annual and quarterly Business forecast reports Main source of Business Regression report Creating various B2B Predictive and descriptive analytics using R and Tableau Creating and automating ad hoc reports Responsible for planning scheduling new product releases and promotional offers Worked on NOSQL databases like Cassandra Experienced in Agile methodologies and SCRUM process Parsing data producing concise conclusions from raw data in a clean wellstructured and easily maintainable format Extracted data from HDFS and prepared data for exploratory analysis using data munging Worked on Text Analytics Naive Bayes Sentiment analysis creating word clouds and retrieving data from Twitter and other social networking platforms Extensive experience and proficiency in using SAS ODS to create output files in a variety of formats including RTF HTML and PDF Worked on different data formats such as JSON XML and performed machine learning algorithms in R and Python Environment R Python UNIX Scripting SAS Cassandra Java Hadoop MapReduce HDFS Pig Sqoop Hive Oracle Eclipse Python Developer Myntra Bengaluru Karnataka April 2012 to June 2015 Roles Responsibilities Worked on the project from gathering requirements to developing the entire application Worked on Anaconda Python Environment Created activated and programmed in Anaconda environment Wrote programs for performance calculations using NumPy and SQLAlchemy Wrote python routines to log into the websites and fetch data for selected options Used python modules of urllib urllib2 Requests for web crawling Experience using all these ML techniques clustering regression classification graphical models Extensive experience in Text Analytics developing different Statistical Machine Learning Data mining solutions to various business problems and generating data visualizations using R Python and Tableau Used with other packages such as Beautiful Soup for data parsing Involved in development of Web Services using SOAP for sending and getting data from the external interface in the XML format Used with other packages such as Beautiful Soup for data parsing Worked on development of SQL and stored procedures on MYSQL Analyzed the code completely and have reduced the code redundancy to the optimal level Design and build a text classification application using different text classification models Used Jira for defect tracking and project management Worked on writing and as well as read data from CSV and excel file formats Involved in Sprint planning sessions and participated in the daily Agile SCRUM meetings Conducted every day scrum as part of the SCRUM Master role Developed the project in Linux environment Worked on resulting reports of the application Performed QA testing on the application Held meetings with client and worked for the entire project with limited help from the client Environment Python Anaconda Sypder IDE Windows 7 Teradata Requests urllib urllib2 Beautiful Soup Tableau python libraries such as NumPy SQL Alchemy MySQLdb Python developer Credit Sussie Pune Maharashtra August 2008 to March 2012 Roles Responsibilities Using python libraries for machine learning like pandas numpy matplotlib sklearn scipy to Load the dataset summarizing the dataset visualizing the dataset evaluating some algorithms and making some predictions Perform application development maintenance and ensure adherence to process and information security control AWS vpc EC2 S3 route 53 A key part of API design and development Unit test and debugging Maintain and extend the existing PythonFlask REST A strong eye on code reusability and maintainability Strong background in objectoriented programming design patterns algorithms and data structures Used Python scripts to update the content in database and manipulate files Generated Python Django forms to maintain the record of online users Used Django APIs to access the database Writing Unit Functional and Integration test cases for Cloud Computing applications on AWS Writing Python scripts with Cloud Formation templates to automate installation of Auto scaling EC2 VPC and other services Designed and managed API system deployment using fast http server and AWS architecture Developed Restful APIs using Python Flask and SQL Alchemy data models as well as ensured code Designed and managed API system deployment using fast http server and Amazon AWS architecture Designed and developed a horizontally scalable APIs using Python Flask Involved in back end development using Python with framework Flask Wrote Python modules to view and connect the Apache Cassandra instance Created Unit test regression test framework for workingnew code Responsible for designing developing testing deploying and maintaining the web application Wrote and executed various MySQL database queries from Python MySQL connector and MySQL Db package Involved in debugging and troubleshooting issues and fixed many bugs in two of the main applications Which is main source of data for customers and internal customer service team Implemented SOAPRESTful web services in JSON format Attended many days to day meetings with developers and users and performed QA testing on the application Environment Python Django API HTML CSS AJAX Git AWS Apache HTTP Flask XML OOD Shell Scripting MYSQL Cassandra Education Bachelor ECE in JNTUH University Bachelors Skills algorithms 8 years API 5 years Machine Learning 10 years python 10 years SQL 10 years Expertise Scikitlearn NLTK spaCy NumPy SciPy OpenCv Deep learning NLP RNN CNN Tensor flow Keras matplotlib Microsoft Visual Studio Microsoft Office 10 years Additional Information TECHNICAL SKILLS Expertise Scikitlearn NLTK spaCy NumPy SciPy OpenCv Deep learning NLP RNN CNN Tensor flow Keras matplotlib Microsoft Visual Studio Microsoft Office Machine Learning Algorithms Linear Regression Logistic Regression Decision Trees Random Forest KMeans Clustering Support Vector Machines Gradient Boost Machines XGBoost Neural Networks Data Analysis Skills Data Cleaning Data Visualization Feature Selection Pandas Operating Systems Windows Mac and Linux Unix Programming Languages Python SQL R Matlab Torch C C Java Octave Apache Spark Hadoop Spark ML Other Programming Knowledge and Skills ElasticSearch Data Scraping RESTfulApi using Django Web Frame work Tools Toad Erwin AWS AzureD3 Mule Soft Alteryx Tableau Shiny Adobe Analytics Anaconda",
    "unique_id": "06b979cf-12e6-46bd-9875-ee9f219109c0"
}