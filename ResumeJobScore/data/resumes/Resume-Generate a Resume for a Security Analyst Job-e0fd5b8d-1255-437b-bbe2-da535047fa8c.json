{
    "clean_data": "Sr Big DataHadoop Developer Sr Big DataHadoop span lDeveloperspan Sr Big DataHadoop Developer Aetna Inc Hartford CT Over 9 years of IT experience in software development and support with experience in developing strategic methods for deploying Big Data technologies to efficiently solve Big Data processing requirement Proficient in installing configuring and using Apache Hadoop ecosystems such as MapReduce Hive Pig Flume Yarn HBase Sqoop Spark Storm Kafka Oozie and Zookeeper Strong comprehension of Hadoop daemons and MapReduce topics Strong knowledge of Spark for handling large data processing in streaming process along with Scala Hands On experience on developing UDF DATA Frames and SQL Queries in Spark SQL Experience in integrating Kafka with Spark streaming for high speed data processing Ability to develop MapReduce program using Java and Python Good understanding and exposure to Python programming Exporting and importing data to and from Oracle using SQL developer for analysis Extensive use of Open Source Software and WebApplication Servers like Eclipse IDE and Apache Tomcat Experience in designing a component using UML DesignUse Case Class Sequence and Development Component diagrams for the requirements Experience in understanding the security requirements for Hadoop and integrate with Kerberos authentication and authorization infrastructure Strong experience in various phases of Software Development Life Cycle SDLC as requirement gathering modeling analysis architecture design development testing and implementation Strong experience on designing Big data pipelines such as Data Ingestion Data Processing Transformations enrichment and aggregations and Reporting Strong experience in submitting Spark applications in different clusters such as Spark Standalone and Hadoop Yarn Good knowledge on various Amazon Web Services AWS such as S3 EC2 VPC RDS SQS ELB Experience in tuning and improving the performance of spark jobs by exploring various options Strong Experience in migrating data using Sqoop from HDFS to Relational Database Systems and viceversa Strong experience in developing the workflows using Apache Oozie framework to automate tasks Experience in working with MapReduce programs using Apache Hadoop to analyze large data sets efficiently Strong experience in working with Core Hadoop components like HDFS Yarn and MapReduce Strong experience in Cloudera Hadoop distribution with Cloudera manager Experience in launching spark applications by using the Kerberos authentication Solid frontend developer experience in developing UI applications using JSP Ajax CSS HTML Java Script XML Hands on Experience in developing and implementing the Spring Rest and Restful Web Services Strong knowledge in JAVA Messaging Service JMS Experience in generating logging by Log4j to identify the errors in production test environment Efficient in developing java applications in various Integrated Development Environment IDE tools like Eclipse My Eclipse and RAD Excellent knowledge on Hadoop Architecture and ecosystems such as HDFS MapReduce Job Tracker Task Tracker NameNode DataNode and Secondary NameNode concepts Excellent knowledge in building and scheduling Big Data workflows with the help of Oozie and Autosys Experience in Elastic Search used for Faster Indexing Kibana Creating Dashboards Splunk Log Analysis and Dashboards Excellent working experience in ScrumAgile framework and Waterfall project execution methodologies Good Knowledge in Amazon AWS concepts like EMR and EC2 web services which provides fast and efficient processing Experience in using Web Services Technologies Web Services RESTFUL SOAP UI Proficient in using and deploying applications to Web ServersApplication servers like Tomcat WebSphere Micro services Working experience with distributed systems using JMS Message Queues Spring JMS Integration Hands on Experience in integration with Maven JUnit and Log4j frameworks Experience in writing Build Scripts using Shell Scripts MAVEN and using CI Continuous Integration tools like Continuum Jenkins Work Experience Sr Big DataHadoop Developer Aetna Inc Hartford CT May 2017 to Present Responsibilities Worked as a Sr Big DataHadoop Developer with Hadoop Ecosystems components like HBase Sqoop Zookeeper Oozie Hive and Pig with Cloudera Hadoop distribution Involved in Agile development methodology active member in scrum meetings Worked in Azure environment for development and deployment of Custom Hadoop Applications Designed and implemented scalable Cloud Data and Analytical architecture solutions for various public and private cloud platforms using Azure Involved in start to end process of Hadoop jobs that used various technologies such as Sqoop PIG Hive MapReduce Spark and Shells scripts for scheduling of few jobs Implemented various Azure platforms such as Azure SQL Database Azure SQL Data Warehouse Azure Analysis Services HDInsight Azure Data Lake and Data Factory Developed SQL scripts using Spark for handling different data sets and verifying the performance over MapReduce jobs Used J2EE design patterns like Factory pattern Singleton Pattern Involved in converting MapReduce programs into Spark transformations using Spark RDDs using Scala and Python Extracted and loaded data into Data Lake environment MS Azure by using Sqoop which was accessed by business users Manage and support of enterprise Data Warehouse operation big data advanced predictive application development using Cloudera Hortonworks HDP Developed PIG scripts to transform the raw data into intelligent data as specified by business users Utilized Apache Spark with Python to develop and execute Big Data Analytics and Machine learning applications executed machine learning use cases under Spark ML and MLlib Installed Hadoop Map Reduce HDFS Azure to develop multiple MapReduce jobs in PIG and Hive for data cleaning and preprocessing Used Spark API over Hortonworks Hadoop YARN to perform analytics on data in Hive Improved the performance and optimization of the existing algorithms in Hadoop using Spark Context SparkSQL Data Frame Pair RDDs Spark YARN Developed Spark code using Scala and SparkSQLStreaming for faster testing and processing of data Developed a Spark job in Java which indexes data into Elastic Search from external Hive tables which are in HDFS Explored MLlib algorithms in Spark to understand the possible Machine Learning functionalities that can be used for our use case Used windows Azure SQL reporting services to create reports with tables charts and maps Executed Hive queries on Parquet tables stored in Hive to perform data analysis to meet the business requirements Used Oozie and Zookeeper operational services for coordinating cluster and scheduling workflows Configured Spark streaming to receive real time data from Kafka and store the stream data to HDFS using Scala Continuous coordination with QA team production support team and deployment Performed transformations cleaning and filtering on imported data using Hive Map Reduce and loaded final data into HDFS Explored with the Spark improving the performance and optimization of the existing algorithms in Hadoop using Spark Context SparkSQL Data Frame Pair RDDs Spark YARN Import the data from different sources like HDFSHBase into Spark RDD and developed a data pipeline using Kafka and Storm to store data into HDFS Used Spark streaming to receive real time data from the Kafka and store the stream data to HDFS using Scala and NoSQL databases such as HBase and Cassandra Documented the requirements including the available code which should be implemented using Spark Hive HDFS HBase and Elastic Search Performed transformations like event joins filter boot traffic and some preaggregations using Pig Developed code in Java which creates mapping in Elastic Search even before data is indexed into Configured Oozie workflow to run multiple Hive and Pig jobs which run independently with time and data availability Imported and exported the analyzed data to the relational databases using Sqoop for visualization and to generate reports for the BI team Support Cloud Strategy team to integrate analytical capabilities into an overall cloud architecture and business case development Developed customized Hive UDFs and UDAFs in Java JDBC connectivity with hive development and execution of Pig scripts and Pig UDFs Used Hadoop YARN to perform analytics on data in Hive Developed and maintained batch data flow using HiveQL and Unix scripting Developed and execute data pipeline testing processes and validate business rules and policies Built code for real time data ingestion using Java MapRStreams Kafka and STORM Environment Hadoop 30 HBase Sqoop Zookeeper Oozie Hive 12 Pig 017 Agile Azure MapReduce Spark 23 J2EE Java Zookeeper 34 Oozie Cassandra 311 NoSQL Big DataHadoop Developer HCSC Chicago IL July 2015 to April 2017 Responsibilities Extensively worked on Hadoop ecosystems including Hive MongoDB Zookeeper Spark Streaming with MapR distribution Developed Big Data solutions focused on pattern matching and predictive modeling Worked on analyzing Hadoop cluster and different big data analytic tools including Pig HBase database and Sqoop Involved in Agile methodologies daily scrum meetings spring planning Created RDDs and applied data filters in Spark and created Cassandra tables and Hive tables for user access Worked on NoSQL support enterprise production and loading data into HBase using Impala and Sqoop Performed multiple MapReduce jobs in Pig and Hive for data cleaning and preprocessing Build Hadoop solutions for big data problems using MR1 and MR2 in YARN Handled importing of data from various data sources performed transformations using Hive Pig and loaded data into HDFS Involved in identifying job dependencies to design workflow for Oozie YARN resource management Performed Hadoop installation configuration of multiple nodes in AWSEC2 using Hortonworks platform Worked with NoSQL databases like HBase in creating HBase tables to load large sets of semistructured data coming from various sources Involved in Hadoop cluster administration and successful in maintenance of large volumes of storage Upgraded the Hadoop Cluster from CDH3 to CDH4 setting up High Availability Cluster and integrating Hive with existing applications Designed Developed a Flattened View Merge and Flattened dataset denormalizing several Datasets in HiveHDFS which consists of key attributes consumed by Business and other down streams Analyzed the existing data flow to the warehouses and taking the similar approach to migrate the data into HDFS Involved in PLSQL query optimization to reduce the overall run time of stored procedures Exported data from HDFS to RDBMS via Sqoop for Business Intelligence visualization and user report generation Involved in designing schema writing CQLs and loading data using Cassandra Built the automated build and deployment framework using Jenkins Maven Implemented MapReduce jobs in HIVE by querying the available data Configured Hive Meta store with MySQL which stores the metadata for Hive tables Performance tuning of Hive queries MapReduce programs for different applications Proactively involved in ongoing maintenance support and improvements in Hadoop cluster Worked with cloud provisioning team on a capacity planning and sizing of the nodes Master and Slave for an AWS EMR Cluster Worked on data using Sqoop from HDFS to Relational Database Systems and viceversa Maintaining and troubleshooting Exploring with Spark to improve the performance and optimization of the existing algorithms in Hadoop using Spark context SparkSQL Data Frame pair RDDs Created Hive Tables loaded claims data from Oracle using Sqoop and loaded the processed data into target database Used Cloudera Manager for installation and management of Hadoop Cluster Worked on MongoDB HBase databases which differ from classic relational databases Involved in converting HiveQL into Spark transformations using Spark RDD and through Scala programming Integrated KafkaSpark streaming for high efficiency throughput and reliability Worked in tuning Hive Pig to improve performance and solved performance issues in both scripts Environment Hadoop 30 Hive 12 MongoDB Zookeeper 34 Pig 017 HBase Sqoop Agile NoSQL Impala 30 MapReduce YARN Oozie AWS Hortonworks Kafka Cassandra 311 Sr JavaHadoop Developer TMobile Bellevue WA November 2013 to June 2015 Responsibilities Worked on analyzing Hadoop cluster and different big data analytic tools including Pig Hive and Sqoop Worked on implementation and maintenance of Cloudera Hadoop cluster Assisted in upgrading configuration and maintenance of various Hadoop infrastructures like Pig Hive and HBase Developed presentation layer components comprising of JSP Servlets and JavaBeans using the Spring framework Responsible for developing various modules frontend and backend components using several design patterns based on clients business requirements Designed and Developed application modules using spring and Hibernate frameworks Involved in developing Use case diagrams Class diagrams Sequence diagrams and process flow diagrams for the modules using UML and Rational Rose Developed the presentation layer using JSP AJAX HTML Bootstrap XHTML AngularJS CSS and client validations using JavaScript Designed and developed SOAP RESTFUL web services Developed Java classes confirming J2EE design patterns JNDI packaged with J2EE specifications Developed and executed custom MapReduce programs Pig Latin scripts and HQL queries Used Hadoop FS scripts for HDFS Hadoop File System data loading and manipulation Developed simple to complex MapReduce streaming jobs using Java language for processing and validating the data Implemented Spark using Python and Spark SQL for faster processing of data Developed Spark jobs and Hive Jobs to summarize and transform data Involved in converting HiveSQL queries into Spark transformations using Spark data frames Scala and Python Consumed Web Services using WSDL SOAP and UDDI from third party for authorizing payments tofrom customers Developed and implemented the MVC Architectural Pattern using Spring Framework Extensively used Spring JDBC in data access layer to access and update information in the database Developed a RESTful API that provided account management capability as well as security role lookup and management for all downstream dependencies Responsible for the design and development of the application framework Used springs test framework to create integration tests for various spring boot and spring batch applications Implemented data access using Hibernate ORM Tool persistence framework Environment Pig 014 Hive 10 Sqoop HBase Spring 45 JavaBeans Hibernate 48 JavaScript AJAX HTML Bootstrap AngularJS CSS J2EE Java HDFS Spark Python Scala MVC Sr JavaFull Stack Developer CoreLogic Austin TX April 2012 to October 2013 Responsibilities Involved in various phases of Software Development Life Cycle SDLC of the application like Requirement gathering Design Analysis and Code development Implemented MVC architecture and DAO design pattern for maximum abstraction of the application and code reusability Developed web application using JSP custom tag libraries Spring Action classes and Action Designed Java Servlets and Objects using J2EE standards Configured the application using Spring framework annotations and developed Spring Controllers for request and response processing and implemented RESTful Web Service Migrated Spring based application to cloud based Micro services Designed and developed the REST based Micro services using the Spring Boot Spring Data with JPA Participated in coding Spring AOP components for the Transactional Model to handle many requests Involved in developing Java APIs which communicates with the JavaBeans Implemented JavaJ2EE Design patterns like Business Delegate and Data Transfer Object DTO Data Access Object Used JSP for presentation layer developed high performance objectrelational persistence and query service for entire application utilizing Hibernate Developed views using Bootstrap components AngularUI and involved in configuring routing for various modules using angular UI router Extensively used HTML JavaScript Angularjs and Ajax for client side development and validations Developed Web services SOAP through WSDL in Apache Axis to interact with other components Created Stateless Session EJBs for retrieving data and Entity Beans for maintaining User Profile Used Log4j as logging framework to capture the log traces of applications in debugging the issues Used ANT automated build scripts to compile and package the application Designed database and created tables written the complex SQL Queries and stored procedures as per the requirements Involved in Unit Integration and Performance Testing for the new enhancements Developed the application using Java Beans Servlets and EJBs Environment SDLC MVC JSP J2EE Java JavaBeans Hibernate 35 Bootstrap HTML JavaScript Angularjs Ajax Log4j ANT Java Developer Impetus October 2009 to March 2012 Responsibilities As a Developer in Java involved in System Requirements analysis and conceptual design Experienced in Web development with JavaScript JQuery HTML Bootstrap CSS and Ajax Implemented Java batch jobs for nightly runs and worked heavily on concurrency API for a low latency high throughput application Worked on applications included in requirement analysis Design Development and Testing Used ANT Maven to build deploy applications helped to deployment for CI using Jenkins and Maven Designed and developed Application based on Spring Framework Spring MVC and Spring templates Created JUnit test cases for unit testing the code at minute level and used Eclipse IDE Developed and designed new crossbrowser accessible interfaces using JQuery and JavaScript Involved in writing JSPs JavaScript and Servlets to generate dynamic web pages and web content Developed Use Case Diagrams Object Diagrams and Class Diagrams in UML using Rational Rose Worked on Spring IoC Spring MVC framework Spring Messaging Framework and Spring AOP to develop application service components Used Hibernate ORM framework for persistence to database by integrating it with Spring framework using Spring Hibernate template Used JavaScript and JQuery for providing clientside validation and Spring Validators for serverside validation Used JQuery to make the frontend components interact with the JavaScript functions to add dynamism to the web pages at the client side Developed User interface using JSP Angular JS JSP Tag libraries third party libraries and JavaScript Used JavaScript JQuery and Ajax API for intensive user operations and clientside validations Used Hibernate framework in persistence layer for mapping an objectoriented domain model to a relational database Used Spring Core for concept Inversion of control IOC implemented using dependency injection Developed Object Model and UML design models for developing Use cases and created Sequence diagram class diagram and active diagrams for application components and interfaces Defined and developed the applications presentation layer using HTML CSS and JavaScript Developed the User Interface using JSP and used CSS for style setting of the Web Pages Involved in developing applications for workflow using JSPs spring MVC module Hibernate AJAX JavaScript technologies using Apache Tomcat Implemented design patterns like DAO singleton factory to achieve design principles Used Jenkins for continuous integration and Maven for building the EAR file Generated JUnit test cases for testing various Java components Environment JavaScript JQuery HTML 4 Bootstrap CSS Ajax Java ANT Maven Jenkins Hibernate MVC JUnit Education Bachelors Skills AJAX 5 years Apache 6 years APACHE HADOOP HDFS 5 years APACHE HADOOP MAPREDUCE 5 years APACHE HADOOP SQOOP 5 years APACHE HBASE 5 years Bootstrap 5 years Database 9 years Hadoop 5 years HADOOP DISTRIBUTED FILE SYSTEM 5 years HBase 5 years HDFS 5 years Hive 5 years HTML 5 years Java 7 years JavaScript 5 years JSP 5 years MapReduce 5 years Servlets 5 years SQL 7 years",
    "entities": [
        "Implemented Spark",
        "Spring Framework",
        "the Transactional Model",
        "Used Hibernate",
        "Spark Context",
        "SparkSQL Data Frame",
        "Objects",
        "Java Beans Servlets",
        "Web Services Technologies Web Services RESTFUL",
        "Custom Hadoop Applications Designed",
        "HDFS Hadoop File System",
        "Hortonworks Hadoop",
        "BI",
        "HDFS",
        "Apache Tomcat Experience",
        "ScrumAgile",
        "Data Lake",
        "Hibernate AJAX JavaScript",
        "Amazon Web Services AWS",
        "UDAFs",
        "Sqoop for Business Intelligence",
        "Design Development and Testing Used ANT Maven",
        "Hadoop",
        "Sqoop Involved",
        "HDFS Involved",
        "EAR",
        "the Hadoop Cluster",
        "Software Development Life Cycle SDLC",
        "JUnit",
        "Hive Developed",
        "HBase",
        "TX",
        "Core Hadoop",
        "Spark Standalone",
        "Spark ML",
        "CDH3",
        "CI Continuous Integration",
        "Cloudera Hadoop",
        "Hadoop Ecosystems",
        "Hadoop Cluster Worked",
        "Python",
        "Hartford CT",
        "Developed",
        "DAO",
        "Data Warehouse",
        "Jenkins",
        "Kerberos",
        "Data Ingestion Data Processing Transformations",
        "HDP Developed",
        "System Requirements",
        "Created Hive Tables",
        "UML",
        "Dashboards Excellent",
        "Waterfall",
        "Sr Big DataHadoop Developer Sr Big DataHadoop",
        "Build Hadoop",
        "Sequence",
        "Integrated Development Environment IDE",
        "Singleton Pattern Involved",
        "WebApplication Servers",
        "JNDI",
        "JSP",
        "Micro",
        "Utilized Apache Spark",
        "SQL Queries",
        "Azure Involved",
        "IOC",
        "the JavaBeans Implemented JavaJ2EE Design",
        "Hibernate ORM Tool",
        "Autosys",
        "RAD Excellent",
        "MVC",
        "Spark",
        "Developed User",
        "MLlib Installed Hadoop Map Reduce HDFS Azure",
        "Maven Designed",
        "HTML CSS",
        "API",
        "Sqoop",
        "QA",
        "HIVE",
        "Executed Hive",
        "Oozie YARN",
        "STORM Environment Hadoop",
        "Business Delegate",
        "IDE Developed",
        "Storm",
        "Hadoop Architecture",
        "Oracle",
        "Secondary NameNode",
        "Faster Indexing Kibana Creating Dashboards Splunk Log Analysis",
        "Big Data Analytics",
        "PIG",
        "Created JUnit",
        "ELB",
        "Hive MongoDB Zookeeper Spark",
        "Tomcat WebSphere Micro",
        "Python Consumed Web Services",
        "Oozie",
        "RESTFUL",
        "HDFS MapReduce Job Tracker Task Tracker NameNode DataNode",
        "SQL",
        "Log4j",
        "UDF",
        "Spark RDD",
        "MapReduce Hive Pig Flume Yarn HBase",
        "Hive Pig",
        "Bootstrap",
        "Relational Database Systems",
        "Performed Hadoop",
        "Maven JUnit",
        "CI",
        "Big Data",
        "Hive",
        "HiveQL",
        "Created Stateless Session",
        "Amazon AWS",
        "Used Spark API",
        "the Web Pages Involved",
        "Hive Improved",
        "Hadoop FS",
        "CDH4",
        "UDDI",
        "Integrated KafkaSpark",
        "Spring Hibernate",
        "Apache Hadoop",
        "Maven",
        "Performed",
        "JavaBeans",
        "Impala",
        "JMS Message Queues Spring JMS Integration Hands",
        "JavaScript",
        "Cloud Data",
        "ANT",
        "Shells",
        "UI",
        "JSP Ajax CSS",
        "JSP Servlets",
        "Python programming Exporting",
        "Restful Web Services Strong",
        "JAVA Messaging Service JMS Experience",
        "CSS",
        "Imported",
        "REST",
        "MapReduce",
        "NoSQL",
        "Spark Hive HDFS HBase",
        "Application",
        "JQuery",
        "Ajax Implemented Java",
        "Java JavaBeans Hibernate"
    ],
    "experience": "Experience in integrating Kafka with Spark streaming for high speed data processing Ability to develop MapReduce program using Java and Python Good understanding and exposure to Python programming Exporting and importing data to and from Oracle using SQL developer for analysis Extensive use of Open Source Software and WebApplication Servers like Eclipse IDE and Apache Tomcat Experience in designing a component using UML DesignUse Case Class Sequence and Development Component diagrams for the requirements Experience in understanding the security requirements for Hadoop and integrate with Kerberos authentication and authorization infrastructure Strong experience in various phases of Software Development Life Cycle SDLC as requirement gathering modeling analysis architecture design development testing and implementation Strong experience on designing Big data pipelines such as Data Ingestion Data Processing Transformations enrichment and aggregations and Reporting Strong experience in submitting Spark applications in different clusters such as Spark Standalone and Hadoop Yarn Good knowledge on various Amazon Web Services AWS such as S3 EC2 VPC RDS SQS ELB Experience in tuning and improving the performance of spark jobs by exploring various options Strong Experience in migrating data using Sqoop from HDFS to Relational Database Systems and viceversa Strong experience in developing the workflows using Apache Oozie framework to automate tasks Experience in working with MapReduce programs using Apache Hadoop to analyze large data sets efficiently Strong experience in working with Core Hadoop components like HDFS Yarn and MapReduce Strong experience in Cloudera Hadoop distribution with Cloudera manager Experience in launching spark applications by using the Kerberos authentication Solid frontend developer experience in developing UI applications using JSP Ajax CSS HTML Java Script XML Hands on Experience in developing and implementing the Spring Rest and Restful Web Services Strong knowledge in JAVA Messaging Service JMS Experience in generating logging by Log4j to identify the errors in production test environment Efficient in developing java applications in various Integrated Development Environment IDE tools like Eclipse My Eclipse and RAD Excellent knowledge on Hadoop Architecture and ecosystems such as HDFS MapReduce Job Tracker Task Tracker NameNode DataNode and Secondary NameNode concepts Excellent knowledge in building and scheduling Big Data workflows with the help of Oozie and Autosys Experience in Elastic Search used for Faster Indexing Kibana Creating Dashboards Splunk Log Analysis and Dashboards Excellent working experience in ScrumAgile framework and Waterfall project execution methodologies Good Knowledge in Amazon AWS concepts like EMR and EC2 web services which provides fast and efficient processing Experience in using Web Services Technologies Web Services RESTFUL SOAP UI Proficient in using and deploying applications to Web ServersApplication servers like Tomcat WebSphere Micro services Working experience with distributed systems using JMS Message Queues Spring JMS Integration Hands on Experience in integration with Maven JUnit and Log4j frameworks Experience in writing Build Scripts using Shell Scripts MAVEN and using CI Continuous Integration tools like Continuum Jenkins Work Experience Sr Big DataHadoop Developer Aetna Inc Hartford CT May 2017 to Present Responsibilities Worked as a Sr Big DataHadoop Developer with Hadoop Ecosystems components like HBase Sqoop Zookeeper Oozie Hive and Pig with Cloudera Hadoop distribution Involved in Agile development methodology active member in scrum meetings Worked in Azure environment for development and deployment of Custom Hadoop Applications Designed and implemented scalable Cloud Data and Analytical architecture solutions for various public and private cloud platforms using Azure Involved in start to end process of Hadoop jobs that used various technologies such as Sqoop PIG Hive MapReduce Spark and Shells scripts for scheduling of few jobs Implemented various Azure platforms such as Azure SQL Database Azure SQL Data Warehouse Azure Analysis Services HDInsight Azure Data Lake and Data Factory Developed SQL scripts using Spark for handling different data sets and verifying the performance over MapReduce jobs Used J2EE design patterns like Factory pattern Singleton Pattern Involved in converting MapReduce programs into Spark transformations using Spark RDDs using Scala and Python Extracted and loaded data into Data Lake environment MS Azure by using Sqoop which was accessed by business users Manage and support of enterprise Data Warehouse operation big data advanced predictive application development using Cloudera Hortonworks HDP Developed PIG scripts to transform the raw data into intelligent data as specified by business users Utilized Apache Spark with Python to develop and execute Big Data Analytics and Machine learning applications executed machine learning use cases under Spark ML and MLlib Installed Hadoop Map Reduce HDFS Azure to develop multiple MapReduce jobs in PIG and Hive for data cleaning and preprocessing Used Spark API over Hortonworks Hadoop YARN to perform analytics on data in Hive Improved the performance and optimization of the existing algorithms in Hadoop using Spark Context SparkSQL Data Frame Pair RDDs Spark YARN Developed Spark code using Scala and SparkSQLStreaming for faster testing and processing of data Developed a Spark job in Java which indexes data into Elastic Search from external Hive tables which are in HDFS Explored MLlib algorithms in Spark to understand the possible Machine Learning functionalities that can be used for our use case Used windows Azure SQL reporting services to create reports with tables charts and maps Executed Hive queries on Parquet tables stored in Hive to perform data analysis to meet the business requirements Used Oozie and Zookeeper operational services for coordinating cluster and scheduling workflows Configured Spark streaming to receive real time data from Kafka and store the stream data to HDFS using Scala Continuous coordination with QA team production support team and deployment Performed transformations cleaning and filtering on imported data using Hive Map Reduce and loaded final data into HDFS Explored with the Spark improving the performance and optimization of the existing algorithms in Hadoop using Spark Context SparkSQL Data Frame Pair RDDs Spark YARN Import the data from different sources like HDFSHBase into Spark RDD and developed a data pipeline using Kafka and Storm to store data into HDFS Used Spark streaming to receive real time data from the Kafka and store the stream data to HDFS using Scala and NoSQL databases such as HBase and Cassandra Documented the requirements including the available code which should be implemented using Spark Hive HDFS HBase and Elastic Search Performed transformations like event joins filter boot traffic and some preaggregations using Pig Developed code in Java which creates mapping in Elastic Search even before data is indexed into Configured Oozie workflow to run multiple Hive and Pig jobs which run independently with time and data availability Imported and exported the analyzed data to the relational databases using Sqoop for visualization and to generate reports for the BI team Support Cloud Strategy team to integrate analytical capabilities into an overall cloud architecture and business case development Developed customized Hive UDFs and UDAFs in Java JDBC connectivity with hive development and execution of Pig scripts and Pig UDFs Used Hadoop YARN to perform analytics on data in Hive Developed and maintained batch data flow using HiveQL and Unix scripting Developed and execute data pipeline testing processes and validate business rules and policies Built code for real time data ingestion using Java MapRStreams Kafka and STORM Environment Hadoop 30 HBase Sqoop Zookeeper Oozie Hive 12 Pig 017 Agile Azure MapReduce Spark 23 J2EE Java Zookeeper 34 Oozie Cassandra 311 NoSQL Big DataHadoop Developer HCSC Chicago IL July 2015 to April 2017 Responsibilities Extensively worked on Hadoop ecosystems including Hive MongoDB Zookeeper Spark Streaming with MapR distribution Developed Big Data solutions focused on pattern matching and predictive modeling Worked on analyzing Hadoop cluster and different big data analytic tools including Pig HBase database and Sqoop Involved in Agile methodologies daily scrum meetings spring planning Created RDDs and applied data filters in Spark and created Cassandra tables and Hive tables for user access Worked on NoSQL support enterprise production and loading data into HBase using Impala and Sqoop Performed multiple MapReduce jobs in Pig and Hive for data cleaning and preprocessing Build Hadoop solutions for big data problems using MR1 and MR2 in YARN Handled importing of data from various data sources performed transformations using Hive Pig and loaded data into HDFS Involved in identifying job dependencies to design workflow for Oozie YARN resource management Performed Hadoop installation configuration of multiple nodes in AWSEC2 using Hortonworks platform Worked with NoSQL databases like HBase in creating HBase tables to load large sets of semistructured data coming from various sources Involved in Hadoop cluster administration and successful in maintenance of large volumes of storage Upgraded the Hadoop Cluster from CDH3 to CDH4 setting up High Availability Cluster and integrating Hive with existing applications Designed Developed a Flattened View Merge and Flattened dataset denormalizing several Datasets in HiveHDFS which consists of key attributes consumed by Business and other down streams Analyzed the existing data flow to the warehouses and taking the similar approach to migrate the data into HDFS Involved in PLSQL query optimization to reduce the overall run time of stored procedures Exported data from HDFS to RDBMS via Sqoop for Business Intelligence visualization and user report generation Involved in designing schema writing CQLs and loading data using Cassandra Built the automated build and deployment framework using Jenkins Maven Implemented MapReduce jobs in HIVE by querying the available data Configured Hive Meta store with MySQL which stores the metadata for Hive tables Performance tuning of Hive queries MapReduce programs for different applications Proactively involved in ongoing maintenance support and improvements in Hadoop cluster Worked with cloud provisioning team on a capacity planning and sizing of the nodes Master and Slave for an AWS EMR Cluster Worked on data using Sqoop from HDFS to Relational Database Systems and viceversa Maintaining and troubleshooting Exploring with Spark to improve the performance and optimization of the existing algorithms in Hadoop using Spark context SparkSQL Data Frame pair RDDs Created Hive Tables loaded claims data from Oracle using Sqoop and loaded the processed data into target database Used Cloudera Manager for installation and management of Hadoop Cluster Worked on MongoDB HBase databases which differ from classic relational databases Involved in converting HiveQL into Spark transformations using Spark RDD and through Scala programming Integrated KafkaSpark streaming for high efficiency throughput and reliability Worked in tuning Hive Pig to improve performance and solved performance issues in both scripts Environment Hadoop 30 Hive 12 MongoDB Zookeeper 34 Pig 017 HBase Sqoop Agile NoSQL Impala 30 MapReduce YARN Oozie AWS Hortonworks Kafka Cassandra 311 Sr JavaHadoop Developer TMobile Bellevue WA November 2013 to June 2015 Responsibilities Worked on analyzing Hadoop cluster and different big data analytic tools including Pig Hive and Sqoop Worked on implementation and maintenance of Cloudera Hadoop cluster Assisted in upgrading configuration and maintenance of various Hadoop infrastructures like Pig Hive and HBase Developed presentation layer components comprising of JSP Servlets and JavaBeans using the Spring framework Responsible for developing various modules frontend and backend components using several design patterns based on clients business requirements Designed and Developed application modules using spring and Hibernate frameworks Involved in developing Use case diagrams Class diagrams Sequence diagrams and process flow diagrams for the modules using UML and Rational Rose Developed the presentation layer using JSP AJAX HTML Bootstrap XHTML AngularJS CSS and client validations using JavaScript Designed and developed SOAP RESTFUL web services Developed Java classes confirming J2EE design patterns JNDI packaged with J2EE specifications Developed and executed custom MapReduce programs Pig Latin scripts and HQL queries Used Hadoop FS scripts for HDFS Hadoop File System data loading and manipulation Developed simple to complex MapReduce streaming jobs using Java language for processing and validating the data Implemented Spark using Python and Spark SQL for faster processing of data Developed Spark jobs and Hive Jobs to summarize and transform data Involved in converting HiveSQL queries into Spark transformations using Spark data frames Scala and Python Consumed Web Services using WSDL SOAP and UDDI from third party for authorizing payments tofrom customers Developed and implemented the MVC Architectural Pattern using Spring Framework Extensively used Spring JDBC in data access layer to access and update information in the database Developed a RESTful API that provided account management capability as well as security role lookup and management for all downstream dependencies Responsible for the design and development of the application framework Used springs test framework to create integration tests for various spring boot and spring batch applications Implemented data access using Hibernate ORM Tool persistence framework Environment Pig 014 Hive 10 Sqoop HBase Spring 45 JavaBeans Hibernate 48 JavaScript AJAX HTML Bootstrap AngularJS CSS J2EE Java HDFS Spark Python Scala MVC Sr JavaFull Stack Developer CoreLogic Austin TX April 2012 to October 2013 Responsibilities Involved in various phases of Software Development Life Cycle SDLC of the application like Requirement gathering Design Analysis and Code development Implemented MVC architecture and DAO design pattern for maximum abstraction of the application and code reusability Developed web application using JSP custom tag libraries Spring Action classes and Action Designed Java Servlets and Objects using J2EE standards Configured the application using Spring framework annotations and developed Spring Controllers for request and response processing and implemented RESTful Web Service Migrated Spring based application to cloud based Micro services Designed and developed the REST based Micro services using the Spring Boot Spring Data with JPA Participated in coding Spring AOP components for the Transactional Model to handle many requests Involved in developing Java APIs which communicates with the JavaBeans Implemented JavaJ2EE Design patterns like Business Delegate and Data Transfer Object DTO Data Access Object Used JSP for presentation layer developed high performance objectrelational persistence and query service for entire application utilizing Hibernate Developed views using Bootstrap components AngularUI and involved in configuring routing for various modules using angular UI router Extensively used HTML JavaScript Angularjs and Ajax for client side development and validations Developed Web services SOAP through WSDL in Apache Axis to interact with other components Created Stateless Session EJBs for retrieving data and Entity Beans for maintaining User Profile Used Log4j as logging framework to capture the log traces of applications in debugging the issues Used ANT automated build scripts to compile and package the application Designed database and created tables written the complex SQL Queries and stored procedures as per the requirements Involved in Unit Integration and Performance Testing for the new enhancements Developed the application using Java Beans Servlets and EJBs Environment SDLC MVC JSP J2EE Java JavaBeans Hibernate 35 Bootstrap HTML JavaScript Angularjs Ajax Log4j ANT Java Developer Impetus October 2009 to March 2012 Responsibilities As a Developer in Java involved in System Requirements analysis and conceptual design Experienced in Web development with JavaScript JQuery HTML Bootstrap CSS and Ajax Implemented Java batch jobs for nightly runs and worked heavily on concurrency API for a low latency high throughput application Worked on applications included in requirement analysis Design Development and Testing Used ANT Maven to build deploy applications helped to deployment for CI using Jenkins and Maven Designed and developed Application based on Spring Framework Spring MVC and Spring templates Created JUnit test cases for unit testing the code at minute level and used Eclipse IDE Developed and designed new crossbrowser accessible interfaces using JQuery and JavaScript Involved in writing JSPs JavaScript and Servlets to generate dynamic web pages and web content Developed Use Case Diagrams Object Diagrams and Class Diagrams in UML using Rational Rose Worked on Spring IoC Spring MVC framework Spring Messaging Framework and Spring AOP to develop application service components Used Hibernate ORM framework for persistence to database by integrating it with Spring framework using Spring Hibernate template Used JavaScript and JQuery for providing clientside validation and Spring Validators for serverside validation Used JQuery to make the frontend components interact with the JavaScript functions to add dynamism to the web pages at the client side Developed User interface using JSP Angular JS JSP Tag libraries third party libraries and JavaScript Used JavaScript JQuery and Ajax API for intensive user operations and clientside validations Used Hibernate framework in persistence layer for mapping an objectoriented domain model to a relational database Used Spring Core for concept Inversion of control IOC implemented using dependency injection Developed Object Model and UML design models for developing Use cases and created Sequence diagram class diagram and active diagrams for application components and interfaces Defined and developed the applications presentation layer using HTML CSS and JavaScript Developed the User Interface using JSP and used CSS for style setting of the Web Pages Involved in developing applications for workflow using JSPs spring MVC module Hibernate AJAX JavaScript technologies using Apache Tomcat Implemented design patterns like DAO singleton factory to achieve design principles Used Jenkins for continuous integration and Maven for building the EAR file Generated JUnit test cases for testing various Java components Environment JavaScript JQuery HTML 4 Bootstrap CSS Ajax Java ANT Maven Jenkins Hibernate MVC JUnit Education Bachelors Skills AJAX 5 years Apache 6 years APACHE HADOOP HDFS 5 years APACHE HADOOP MAPREDUCE 5 years APACHE HADOOP SQOOP 5 years APACHE HBASE 5 years Bootstrap 5 years Database 9 years Hadoop 5 years HADOOP DISTRIBUTED FILE SYSTEM 5 years HBase 5 years HDFS 5 years Hive 5 years HTML 5 years Java 7 years JavaScript 5 years JSP 5 years MapReduce 5 years Servlets 5 years SQL 7 years",
    "extracted_keywords": [
        "Sr",
        "Big",
        "DataHadoop",
        "Developer",
        "Sr",
        "Big",
        "DataHadoop",
        "span",
        "lDeveloperspan",
        "Sr",
        "Big",
        "DataHadoop",
        "Developer",
        "Aetna",
        "Inc",
        "Hartford",
        "CT",
        "years",
        "IT",
        "experience",
        "software",
        "development",
        "support",
        "experience",
        "methods",
        "Big",
        "Data",
        "technologies",
        "Big",
        "Data",
        "processing",
        "requirement",
        "Proficient",
        "configuring",
        "Apache",
        "Hadoop",
        "ecosystems",
        "MapReduce",
        "Hive",
        "Pig",
        "Flume",
        "Yarn",
        "HBase",
        "Sqoop",
        "Spark",
        "Storm",
        "Kafka",
        "Oozie",
        "Zookeeper",
        "Strong",
        "comprehension",
        "Hadoop",
        "daemons",
        "MapReduce",
        "topics",
        "knowledge",
        "Spark",
        "data",
        "processing",
        "streaming",
        "process",
        "Scala",
        "Hands",
        "experience",
        "UDF",
        "DATA",
        "Frames",
        "SQL",
        "Queries",
        "Spark",
        "SQL",
        "Experience",
        "Kafka",
        "Spark",
        "streaming",
        "speed",
        "data",
        "Ability",
        "MapReduce",
        "program",
        "Java",
        "Python",
        "understanding",
        "exposure",
        "Python",
        "programming",
        "Exporting",
        "data",
        "Oracle",
        "SQL",
        "developer",
        "analysis",
        "use",
        "Open",
        "Source",
        "Software",
        "WebApplication",
        "Servers",
        "Eclipse",
        "IDE",
        "Apache",
        "Tomcat",
        "Experience",
        "component",
        "UML",
        "DesignUse",
        "Case",
        "Class",
        "Sequence",
        "Development",
        "Component",
        "diagrams",
        "requirements",
        "Experience",
        "security",
        "requirements",
        "Hadoop",
        "authentication",
        "authorization",
        "infrastructure",
        "experience",
        "phases",
        "Software",
        "Development",
        "Life",
        "Cycle",
        "SDLC",
        "requirement",
        "gathering",
        "analysis",
        "architecture",
        "design",
        "development",
        "testing",
        "implementation",
        "experience",
        "data",
        "pipelines",
        "Data",
        "Ingestion",
        "Data",
        "Processing",
        "Transformations",
        "enrichment",
        "aggregations",
        "experience",
        "Spark",
        "applications",
        "clusters",
        "Spark",
        "Standalone",
        "Hadoop",
        "Yarn",
        "knowledge",
        "Amazon",
        "Web",
        "Services",
        "AWS",
        "S3",
        "EC2",
        "VPC",
        "RDS",
        "SQS",
        "ELB",
        "Experience",
        "performance",
        "spark",
        "jobs",
        "options",
        "Strong",
        "Experience",
        "data",
        "Sqoop",
        "HDFS",
        "Relational",
        "Database",
        "Systems",
        "viceversa",
        "Strong",
        "experience",
        "workflows",
        "Apache",
        "Oozie",
        "framework",
        "tasks",
        "Experience",
        "MapReduce",
        "programs",
        "Apache",
        "Hadoop",
        "data",
        "experience",
        "Core",
        "Hadoop",
        "components",
        "HDFS",
        "Yarn",
        "MapReduce",
        "Strong",
        "experience",
        "Cloudera",
        "Hadoop",
        "distribution",
        "Cloudera",
        "manager",
        "Experience",
        "spark",
        "applications",
        "authentication",
        "frontend",
        "developer",
        "experience",
        "UI",
        "applications",
        "JSP",
        "Ajax",
        "CSS",
        "HTML",
        "Java",
        "Script",
        "XML",
        "Hands",
        "Experience",
        "Spring",
        "Rest",
        "Restful",
        "Web",
        "Services",
        "knowledge",
        "JAVA",
        "Messaging",
        "Service",
        "JMS",
        "Experience",
        "Log4j",
        "errors",
        "production",
        "test",
        "environment",
        "Efficient",
        "applications",
        "Integrated",
        "Development",
        "Environment",
        "IDE",
        "tools",
        "Eclipse",
        "Eclipse",
        "RAD",
        "Excellent",
        "knowledge",
        "Hadoop",
        "Architecture",
        "ecosystems",
        "HDFS",
        "MapReduce",
        "Job",
        "Tracker",
        "Task",
        "Tracker",
        "NameNode",
        "DataNode",
        "Secondary",
        "NameNode",
        "knowledge",
        "building",
        "scheduling",
        "Big",
        "Data",
        "help",
        "Oozie",
        "Autosys",
        "Experience",
        "Elastic",
        "Search",
        "Indexing",
        "Kibana",
        "Dashboards",
        "Splunk",
        "Log",
        "Analysis",
        "Dashboards",
        "Excellent",
        "working",
        "experience",
        "ScrumAgile",
        "framework",
        "Waterfall",
        "project",
        "execution",
        "methodologies",
        "Good",
        "Knowledge",
        "Amazon",
        "AWS",
        "concepts",
        "EMR",
        "EC2",
        "web",
        "services",
        "processing",
        "Experience",
        "Web",
        "Services",
        "Technologies",
        "Web",
        "Services",
        "RESTFUL",
        "SOAP",
        "UI",
        "Proficient",
        "applications",
        "Web",
        "ServersApplication",
        "servers",
        "Tomcat",
        "WebSphere",
        "Micro",
        "Working",
        "experience",
        "systems",
        "JMS",
        "Message",
        "Queues",
        "Spring",
        "JMS",
        "Integration",
        "Hands",
        "Experience",
        "integration",
        "Maven",
        "JUnit",
        "Log4j",
        "frameworks",
        "Experience",
        "Build",
        "Scripts",
        "Shell",
        "Scripts",
        "MAVEN",
        "CI",
        "Continuous",
        "Integration",
        "tools",
        "Continuum",
        "Jenkins",
        "Work",
        "Experience",
        "Sr",
        "Big",
        "DataHadoop",
        "Developer",
        "Aetna",
        "Inc",
        "Hartford",
        "CT",
        "May",
        "Present",
        "Responsibilities",
        "Sr",
        "Big",
        "DataHadoop",
        "Developer",
        "Hadoop",
        "Ecosystems",
        "components",
        "HBase",
        "Sqoop",
        "Zookeeper",
        "Oozie",
        "Hive",
        "Pig",
        "Cloudera",
        "Hadoop",
        "distribution",
        "development",
        "methodology",
        "member",
        "scrum",
        "meetings",
        "Azure",
        "environment",
        "development",
        "deployment",
        "Custom",
        "Hadoop",
        "Applications",
        "Cloud",
        "Data",
        "Analytical",
        "architecture",
        "solutions",
        "cloud",
        "platforms",
        "Azure",
        "start",
        "end",
        "process",
        "Hadoop",
        "jobs",
        "technologies",
        "Sqoop",
        "PIG",
        "Hive",
        "MapReduce",
        "Spark",
        "Shells",
        "scripts",
        "scheduling",
        "jobs",
        "platforms",
        "Azure",
        "SQL",
        "Database",
        "Azure",
        "SQL",
        "Data",
        "Warehouse",
        "Azure",
        "Analysis",
        "Services",
        "HDInsight",
        "Azure",
        "Data",
        "Lake",
        "Data",
        "Factory",
        "SQL",
        "scripts",
        "Spark",
        "data",
        "sets",
        "performance",
        "MapReduce",
        "jobs",
        "J2EE",
        "design",
        "patterns",
        "Factory",
        "pattern",
        "Singleton",
        "Pattern",
        "MapReduce",
        "programs",
        "Spark",
        "transformations",
        "Spark",
        "RDDs",
        "Scala",
        "Python",
        "data",
        "Data",
        "Lake",
        "environment",
        "MS",
        "Azure",
        "Sqoop",
        "business",
        "users",
        "support",
        "enterprise",
        "Data",
        "Warehouse",
        "operation",
        "data",
        "application",
        "development",
        "Cloudera",
        "Hortonworks",
        "HDP",
        "PIG",
        "scripts",
        "data",
        "data",
        "business",
        "users",
        "Apache",
        "Spark",
        "Python",
        "Big",
        "Data",
        "Analytics",
        "Machine",
        "learning",
        "applications",
        "machine",
        "learning",
        "use",
        "cases",
        "Spark",
        "ML",
        "MLlib",
        "Installed",
        "Hadoop",
        "Map",
        "HDFS",
        "Azure",
        "MapReduce",
        "jobs",
        "PIG",
        "Hive",
        "data",
        "Spark",
        "API",
        "Hortonworks",
        "Hadoop",
        "YARN",
        "analytics",
        "data",
        "Hive",
        "performance",
        "optimization",
        "algorithms",
        "Hadoop",
        "Spark",
        "Context",
        "SparkSQL",
        "Data",
        "Frame",
        "Pair",
        "Spark",
        "YARN",
        "Spark",
        "code",
        "Scala",
        "SparkSQLStreaming",
        "testing",
        "processing",
        "data",
        "Spark",
        "job",
        "Java",
        "data",
        "Elastic",
        "Search",
        "Hive",
        "tables",
        "HDFS",
        "MLlib",
        "algorithms",
        "Spark",
        "Machine",
        "Learning",
        "functionalities",
        "use",
        "case",
        "windows",
        "Azure",
        "SQL",
        "services",
        "reports",
        "tables",
        "charts",
        "maps",
        "Hive",
        "queries",
        "Parquet",
        "tables",
        "Hive",
        "data",
        "analysis",
        "business",
        "requirements",
        "Oozie",
        "Zookeeper",
        "services",
        "cluster",
        "scheduling",
        "workflows",
        "Configured",
        "Spark",
        "streaming",
        "time",
        "data",
        "Kafka",
        "stream",
        "data",
        "HDFS",
        "Scala",
        "Continuous",
        "coordination",
        "QA",
        "team",
        "production",
        "support",
        "team",
        "deployment",
        "Performed",
        "transformations",
        "filtering",
        "data",
        "Hive",
        "Map",
        "Reduce",
        "data",
        "HDFS",
        "Spark",
        "performance",
        "optimization",
        "algorithms",
        "Hadoop",
        "Spark",
        "Context",
        "SparkSQL",
        "Data",
        "Frame",
        "Pair",
        "Spark",
        "YARN",
        "data",
        "sources",
        "HDFSHBase",
        "Spark",
        "RDD",
        "data",
        "pipeline",
        "Kafka",
        "Storm",
        "data",
        "HDFS",
        "Spark",
        "streaming",
        "time",
        "data",
        "Kafka",
        "stream",
        "data",
        "HDFS",
        "Scala",
        "NoSQL",
        "HBase",
        "Cassandra",
        "requirements",
        "code",
        "Spark",
        "Hive",
        "HDFS",
        "HBase",
        "Elastic",
        "Search",
        "transformations",
        "event",
        "filter",
        "boot",
        "traffic",
        "preaggregations",
        "Pig",
        "Developed",
        "code",
        "Java",
        "mapping",
        "Elastic",
        "Search",
        "data",
        "Configured",
        "Oozie",
        "workflow",
        "Hive",
        "Pig",
        "jobs",
        "time",
        "data",
        "availability",
        "data",
        "databases",
        "Sqoop",
        "visualization",
        "reports",
        "BI",
        "team",
        "Support",
        "Cloud",
        "Strategy",
        "team",
        "capabilities",
        "cloud",
        "architecture",
        "business",
        "case",
        "development",
        "Developed",
        "Hive",
        "UDFs",
        "UDAFs",
        "Java",
        "JDBC",
        "connectivity",
        "hive",
        "development",
        "execution",
        "Pig",
        "scripts",
        "Pig",
        "UDFs",
        "Used",
        "Hadoop",
        "YARN",
        "analytics",
        "data",
        "Hive",
        "Developed",
        "batch",
        "data",
        "flow",
        "HiveQL",
        "Unix",
        "Developed",
        "data",
        "pipeline",
        "testing",
        "processes",
        "business",
        "rules",
        "policies",
        "code",
        "time",
        "data",
        "ingestion",
        "Java",
        "MapRStreams",
        "Kafka",
        "STORM",
        "Environment",
        "Hadoop",
        "HBase",
        "Sqoop",
        "Zookeeper",
        "Oozie",
        "Hive",
        "Pig",
        "Agile",
        "Azure",
        "MapReduce",
        "Spark",
        "J2EE",
        "Java",
        "Zookeeper",
        "Oozie",
        "Cassandra",
        "NoSQL",
        "Big",
        "DataHadoop",
        "Developer",
        "HCSC",
        "Chicago",
        "IL",
        "July",
        "April",
        "Responsibilities",
        "Hadoop",
        "ecosystems",
        "Hive",
        "MongoDB",
        "Zookeeper",
        "Spark",
        "Streaming",
        "MapR",
        "distribution",
        "Big",
        "Data",
        "solutions",
        "pattern",
        "matching",
        "modeling",
        "Hadoop",
        "cluster",
        "data",
        "tools",
        "Pig",
        "HBase",
        "database",
        "Sqoop",
        "methodologies",
        "meetings",
        "spring",
        "RDDs",
        "data",
        "filters",
        "Spark",
        "Cassandra",
        "tables",
        "Hive",
        "tables",
        "user",
        "access",
        "NoSQL",
        "support",
        "enterprise",
        "production",
        "loading",
        "data",
        "HBase",
        "Impala",
        "Sqoop",
        "MapReduce",
        "jobs",
        "Pig",
        "Hive",
        "data",
        "Build",
        "Hadoop",
        "solutions",
        "data",
        "problems",
        "MR1",
        "MR2",
        "YARN",
        "importing",
        "data",
        "data",
        "sources",
        "transformations",
        "Hive",
        "Pig",
        "data",
        "HDFS",
        "job",
        "dependencies",
        "workflow",
        "Oozie",
        "YARN",
        "resource",
        "management",
        "Performed",
        "Hadoop",
        "installation",
        "configuration",
        "nodes",
        "AWSEC2",
        "Hortonworks",
        "platform",
        "databases",
        "HBase",
        "HBase",
        "tables",
        "sets",
        "data",
        "sources",
        "Hadoop",
        "cluster",
        "administration",
        "maintenance",
        "volumes",
        "storage",
        "Hadoop",
        "Cluster",
        "CDH3",
        "CDH4",
        "High",
        "Availability",
        "Cluster",
        "Hive",
        "applications",
        "Flattened",
        "View",
        "Merge",
        "Flattened",
        "dataset",
        "Datasets",
        "HiveHDFS",
        "attributes",
        "Business",
        "streams",
        "data",
        "flow",
        "warehouses",
        "approach",
        "data",
        "HDFS",
        "PLSQL",
        "query",
        "optimization",
        "time",
        "procedures",
        "data",
        "HDFS",
        "Sqoop",
        "Business",
        "Intelligence",
        "visualization",
        "user",
        "report",
        "generation",
        "schema",
        "CQLs",
        "loading",
        "data",
        "Cassandra",
        "build",
        "deployment",
        "framework",
        "Jenkins",
        "Maven",
        "MapReduce",
        "jobs",
        "HIVE",
        "data",
        "Configured",
        "Hive",
        "Meta",
        "store",
        "MySQL",
        "metadata",
        "Hive",
        "tables",
        "Performance",
        "tuning",
        "Hive",
        "MapReduce",
        "programs",
        "applications",
        "maintenance",
        "support",
        "improvements",
        "Hadoop",
        "cluster",
        "team",
        "capacity",
        "planning",
        "sizing",
        "nodes",
        "Master",
        "Slave",
        "AWS",
        "EMR",
        "Cluster",
        "Worked",
        "data",
        "Sqoop",
        "HDFS",
        "Relational",
        "Database",
        "Systems",
        "viceversa",
        "Maintaining",
        "Spark",
        "performance",
        "optimization",
        "algorithms",
        "Hadoop",
        "Spark",
        "context",
        "SparkSQL",
        "Data",
        "Frame",
        "pair",
        "RDDs",
        "Hive",
        "Tables",
        "claims",
        "data",
        "Oracle",
        "Sqoop",
        "data",
        "target",
        "database",
        "Cloudera",
        "Manager",
        "installation",
        "management",
        "Hadoop",
        "Cluster",
        "MongoDB",
        "HBase",
        "databases",
        "HiveQL",
        "Spark",
        "transformations",
        "Spark",
        "RDD",
        "Scala",
        "Integrated",
        "KafkaSpark",
        "streaming",
        "efficiency",
        "throughput",
        "reliability",
        "Hive",
        "Pig",
        "performance",
        "performance",
        "issues",
        "scripts",
        "Environment",
        "Hadoop",
        "Hive",
        "Zookeeper",
        "Pig",
        "HBase",
        "Sqoop",
        "Agile",
        "NoSQL",
        "Impala",
        "MapReduce",
        "YARN",
        "Oozie",
        "AWS",
        "Hortonworks",
        "Kafka",
        "Cassandra",
        "Sr",
        "JavaHadoop",
        "Developer",
        "TMobile",
        "Bellevue",
        "WA",
        "November",
        "June",
        "Responsibilities",
        "Hadoop",
        "cluster",
        "data",
        "tools",
        "Pig",
        "Hive",
        "Sqoop",
        "implementation",
        "maintenance",
        "Cloudera",
        "Hadoop",
        "cluster",
        "Assisted",
        "configuration",
        "maintenance",
        "Hadoop",
        "infrastructures",
        "Pig",
        "Hive",
        "HBase",
        "presentation",
        "layer",
        "components",
        "JSP",
        "Servlets",
        "JavaBeans",
        "Spring",
        "framework",
        "modules",
        "components",
        "design",
        "patterns",
        "clients",
        "business",
        "requirements",
        "application",
        "modules",
        "spring",
        "Hibernate",
        "frameworks",
        "Use",
        "case",
        "diagrams",
        "Class",
        "diagrams",
        "Sequence",
        "diagrams",
        "process",
        "flow",
        "diagrams",
        "modules",
        "UML",
        "Rational",
        "Rose",
        "presentation",
        "layer",
        "JSP",
        "AJAX",
        "HTML",
        "Bootstrap",
        "CSS",
        "client",
        "validations",
        "JavaScript",
        "SOAP",
        "RESTFUL",
        "web",
        "services",
        "Java",
        "classes",
        "J2EE",
        "design",
        "patterns",
        "JNDI",
        "J2EE",
        "specifications",
        "custom",
        "MapReduce",
        "programs",
        "Pig",
        "Latin",
        "scripts",
        "HQL",
        "Used",
        "Hadoop",
        "FS",
        "scripts",
        "HDFS",
        "Hadoop",
        "File",
        "System",
        "data",
        "loading",
        "manipulation",
        "MapReduce",
        "streaming",
        "jobs",
        "Java",
        "language",
        "processing",
        "data",
        "Spark",
        "Python",
        "Spark",
        "SQL",
        "processing",
        "data",
        "Spark",
        "jobs",
        "Hive",
        "Jobs",
        "data",
        "HiveSQL",
        "queries",
        "Spark",
        "transformations",
        "Spark",
        "data",
        "Scala",
        "Python",
        "Consumed",
        "Web",
        "Services",
        "WSDL",
        "SOAP",
        "UDDI",
        "party",
        "payments",
        "customers",
        "MVC",
        "Architectural",
        "Pattern",
        "Spring",
        "Framework",
        "Spring",
        "JDBC",
        "data",
        "access",
        "layer",
        "information",
        "database",
        "API",
        "account",
        "management",
        "capability",
        "security",
        "role",
        "lookup",
        "management",
        "dependencies",
        "design",
        "development",
        "application",
        "framework",
        "springs",
        "test",
        "framework",
        "integration",
        "tests",
        "spring",
        "boot",
        "spring",
        "batch",
        "applications",
        "data",
        "access",
        "Hibernate",
        "ORM",
        "Tool",
        "persistence",
        "framework",
        "Environment",
        "Pig",
        "Hive",
        "Sqoop",
        "HBase",
        "Spring",
        "JavaBeans",
        "Hibernate",
        "JavaScript",
        "AJAX",
        "HTML",
        "Bootstrap",
        "AngularJS",
        "CSS",
        "J2EE",
        "Java",
        "HDFS",
        "Spark",
        "Python",
        "Scala",
        "MVC",
        "Sr",
        "JavaFull",
        "Stack",
        "Developer",
        "CoreLogic",
        "Austin",
        "TX",
        "April",
        "October",
        "Responsibilities",
        "phases",
        "Software",
        "Development",
        "Life",
        "Cycle",
        "SDLC",
        "application",
        "Requirement",
        "Design",
        "Analysis",
        "Code",
        "development",
        "MVC",
        "architecture",
        "DAO",
        "design",
        "pattern",
        "abstraction",
        "application",
        "code",
        "reusability",
        "web",
        "application",
        "JSP",
        "custom",
        "tag",
        "Spring",
        "Action",
        "classes",
        "Action",
        "Java",
        "Servlets",
        "Objects",
        "J2EE",
        "standards",
        "application",
        "Spring",
        "framework",
        "annotations",
        "Spring",
        "Controllers",
        "request",
        "response",
        "processing",
        "RESTful",
        "Web",
        "Service",
        "Spring",
        "application",
        "cloud",
        "Micro",
        "services",
        "REST",
        "Micro",
        "services",
        "Spring",
        "Boot",
        "Spring",
        "Data",
        "JPA",
        "Participated",
        "Spring",
        "AOP",
        "components",
        "Transactional",
        "Model",
        "requests",
        "Java",
        "APIs",
        "JavaBeans",
        "JavaJ2EE",
        "Design",
        "patterns",
        "Business",
        "Delegate",
        "Data",
        "Transfer",
        "Object",
        "DTO",
        "Data",
        "Access",
        "Object",
        "JSP",
        "presentation",
        "layer",
        "performance",
        "persistence",
        "query",
        "service",
        "application",
        "Hibernate",
        "views",
        "Bootstrap",
        "components",
        "modules",
        "UI",
        "router",
        "HTML",
        "JavaScript",
        "Angularjs",
        "Ajax",
        "client",
        "side",
        "development",
        "Web",
        "services",
        "SOAP",
        "WSDL",
        "Apache",
        "Axis",
        "components",
        "Stateless",
        "Session",
        "EJBs",
        "data",
        "Entity",
        "Beans",
        "User",
        "Profile",
        "Log4j",
        "framework",
        "log",
        "traces",
        "applications",
        "issues",
        "ANT",
        "build",
        "scripts",
        "compile",
        "application",
        "database",
        "tables",
        "SQL",
        "Queries",
        "procedures",
        "requirements",
        "Unit",
        "Integration",
        "Performance",
        "Testing",
        "enhancements",
        "application",
        "Java",
        "Beans",
        "Servlets",
        "EJBs",
        "Environment",
        "SDLC",
        "MVC",
        "JSP",
        "J2EE",
        "Java",
        "JavaBeans",
        "Hibernate",
        "Bootstrap",
        "HTML",
        "JavaScript",
        "Angularjs",
        "Ajax",
        "Log4j",
        "ANT",
        "Java",
        "Developer",
        "Impetus",
        "October",
        "March",
        "Responsibilities",
        "Developer",
        "Java",
        "System",
        "Requirements",
        "analysis",
        "design",
        "Web",
        "development",
        "JavaScript",
        "JQuery",
        "HTML",
        "Bootstrap",
        "CSS",
        "Ajax",
        "Java",
        "batch",
        "jobs",
        "runs",
        "concurrency",
        "API",
        "latency",
        "throughput",
        "application",
        "applications",
        "requirement",
        "analysis",
        "Design",
        "Development",
        "Testing",
        "ANT",
        "Maven",
        "deploy",
        "applications",
        "deployment",
        "CI",
        "Jenkins",
        "Maven",
        "Designed",
        "Application",
        "Spring",
        "Framework",
        "Spring",
        "MVC",
        "Spring",
        "templates",
        "JUnit",
        "test",
        "cases",
        "unit",
        "code",
        "minute",
        "level",
        "Eclipse",
        "IDE",
        "Developed",
        "crossbrowser",
        "interfaces",
        "JQuery",
        "JavaScript",
        "JSPs",
        "JavaScript",
        "Servlets",
        "web",
        "pages",
        "web",
        "content",
        "Developed",
        "Use",
        "Case",
        "Diagrams",
        "Object",
        "Diagrams",
        "Class",
        "Diagrams",
        "UML",
        "Rational",
        "Rose",
        "Spring",
        "IoC",
        "Spring",
        "MVC",
        "framework",
        "Spring",
        "Messaging",
        "Framework",
        "Spring",
        "AOP",
        "application",
        "service",
        "components",
        "Hibernate",
        "ORM",
        "framework",
        "persistence",
        "database",
        "Spring",
        "framework",
        "Spring",
        "Hibernate",
        "template",
        "JavaScript",
        "JQuery",
        "validation",
        "Spring",
        "Validators",
        "serverside",
        "validation",
        "JQuery",
        "frontend",
        "components",
        "JavaScript",
        "functions",
        "dynamism",
        "web",
        "pages",
        "client",
        "side",
        "Developed",
        "User",
        "interface",
        "JSP",
        "Angular",
        "JS",
        "JSP",
        "Tag",
        "party",
        "libraries",
        "JavaScript",
        "JavaScript",
        "JQuery",
        "API",
        "user",
        "operations",
        "validations",
        "Hibernate",
        "framework",
        "persistence",
        "layer",
        "domain",
        "model",
        "database",
        "Spring",
        "Core",
        "concept",
        "Inversion",
        "control",
        "IOC",
        "dependency",
        "injection",
        "Object",
        "Model",
        "UML",
        "design",
        "models",
        "Use",
        "cases",
        "Sequence",
        "diagram",
        "class",
        "diagram",
        "diagrams",
        "application",
        "components",
        "interfaces",
        "applications",
        "presentation",
        "layer",
        "HTML",
        "CSS",
        "JavaScript",
        "User",
        "Interface",
        "JSP",
        "CSS",
        "style",
        "setting",
        "Web",
        "Pages",
        "applications",
        "workflow",
        "JSPs",
        "spring",
        "MVC",
        "module",
        "Hibernate",
        "AJAX",
        "JavaScript",
        "technologies",
        "Apache",
        "Tomcat",
        "design",
        "patterns",
        "DAO",
        "singleton",
        "factory",
        "design",
        "principles",
        "Jenkins",
        "integration",
        "Maven",
        "EAR",
        "file",
        "JUnit",
        "test",
        "cases",
        "Java",
        "components",
        "Environment",
        "JavaScript",
        "JQuery",
        "HTML",
        "Bootstrap",
        "CSS",
        "Ajax",
        "Java",
        "ANT",
        "Maven",
        "Jenkins",
        "Hibernate",
        "MVC",
        "JUnit",
        "Education",
        "Bachelors",
        "Skills",
        "AJAX",
        "years",
        "Apache",
        "years",
        "APACHE",
        "HADOOP",
        "HDFS",
        "years",
        "APACHE",
        "HADOOP",
        "MAPREDUCE",
        "years",
        "APACHE",
        "HADOOP",
        "SQOOP",
        "years",
        "APACHE",
        "HBASE",
        "years",
        "Bootstrap",
        "years",
        "Database",
        "years",
        "Hadoop",
        "years",
        "HADOOP",
        "FILE",
        "SYSTEM",
        "years",
        "HBase",
        "years",
        "years",
        "Hive",
        "years",
        "HTML",
        "years",
        "Java",
        "years",
        "JavaScript",
        "years",
        "JSP",
        "years",
        "MapReduce",
        "years",
        "Servlets",
        "years",
        "SQL",
        "years"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T21:37:58.145662",
    "resume_data": "Sr Big DataHadoop Developer Sr Big DataHadoop span lDeveloperspan Sr Big DataHadoop Developer Aetna Inc Hartford CT Over 9 years of IT experience in software development and support with experience in developing strategic methods for deploying Big Data technologies to efficiently solve Big Data processing requirement Proficient in installing configuring and using Apache Hadoop ecosystems such as MapReduce Hive Pig Flume Yarn HBase Sqoop Spark Storm Kafka Oozie and Zookeeper Strong comprehension of Hadoop daemons and MapReduce topics Strong knowledge of Spark for handling large data processing in streaming process along with Scala Hands On experience on developing UDF DATA Frames and SQL Queries in Spark SQL Experience in integrating Kafka with Spark streaming for high speed data processing Ability to develop MapReduce program using Java and Python Good understanding and exposure to Python programming Exporting and importing data to and from Oracle using SQL developer for analysis Extensive use of Open Source Software and WebApplication Servers like Eclipse IDE and Apache Tomcat Experience in designing a component using UML DesignUse Case Class Sequence and Development Component diagrams for the requirements Experience in understanding the security requirements for Hadoop and integrate with Kerberos authentication and authorization infrastructure Strong experience in various phases of Software Development Life Cycle SDLC as requirement gathering modeling analysis architecture design development testing and implementation Strong experience on designing Big data pipelines such as Data Ingestion Data Processing Transformations enrichment and aggregations and Reporting Strong experience in submitting Spark applications in different clusters such as Spark Standalone and Hadoop Yarn Good knowledge on various Amazon Web Services AWS such as S3 EC2 VPC RDS SQS ELB Experience in tuning and improving the performance of spark jobs by exploring various options Strong Experience in migrating data using Sqoop from HDFS to Relational Database Systems and viceversa Strong experience in developing the workflows using Apache Oozie framework to automate tasks Experience in working with MapReduce programs using Apache Hadoop to analyze large data sets efficiently Strong experience in working with Core Hadoop components like HDFS Yarn and MapReduce Strong experience in Cloudera Hadoop distribution with Cloudera manager Experience in launching spark applications by using the Kerberos authentication Solid frontend developer experience in developing UI applications using JSP Ajax CSS HTML Java Script XML Hands on Experience in developing and implementing the Spring Rest and Restful Web Services Strong knowledge in JAVA Messaging Service JMS Experience in generating logging by Log4j to identify the errors in production test environment Efficient in developing java applications in various Integrated Development Environment IDE tools like Eclipse My Eclipse and RAD Excellent knowledge on Hadoop Architecture and ecosystems such as HDFS MapReduce Job Tracker Task Tracker NameNode DataNode and Secondary NameNode concepts Excellent knowledge in building and scheduling Big Data workflows with the help of Oozie and Autosys Experience in Elastic Search used for Faster Indexing Kibana Creating Dashboards Splunk Log Analysis and Dashboards Excellent working experience in ScrumAgile framework and Waterfall project execution methodologies Good Knowledge in Amazon AWS concepts like EMR and EC2 web services which provides fast and efficient processing Experience in using Web Services Technologies Web Services RESTFUL SOAP UI Proficient in using and deploying applications to Web ServersApplication servers like Tomcat WebSphere Micro services Working experience with distributed systems using JMS Message Queues Spring JMS Integration Hands on Experience in integration with Maven JUnit and Log4j frameworks Experience in writing Build Scripts using Shell Scripts MAVEN and using CI Continuous Integration tools like Continuum Jenkins Work Experience Sr Big DataHadoop Developer Aetna Inc Hartford CT May 2017 to Present Responsibilities Worked as a Sr Big DataHadoop Developer with Hadoop Ecosystems components like HBase Sqoop Zookeeper Oozie Hive and Pig with Cloudera Hadoop distribution Involved in Agile development methodology active member in scrum meetings Worked in Azure environment for development and deployment of Custom Hadoop Applications Designed and implemented scalable Cloud Data and Analytical architecture solutions for various public and private cloud platforms using Azure Involved in start to end process of Hadoop jobs that used various technologies such as Sqoop PIG Hive MapReduce Spark and Shells scripts for scheduling of few jobs Implemented various Azure platforms such as Azure SQL Database Azure SQL Data Warehouse Azure Analysis Services HDInsight Azure Data Lake and Data Factory Developed SQL scripts using Spark for handling different data sets and verifying the performance over MapReduce jobs Used J2EE design patterns like Factory pattern Singleton Pattern Involved in converting MapReduce programs into Spark transformations using Spark RDDs using Scala and Python Extracted and loaded data into Data Lake environment MS Azure by using Sqoop which was accessed by business users Manage and support of enterprise Data Warehouse operation big data advanced predictive application development using Cloudera Hortonworks HDP Developed PIG scripts to transform the raw data into intelligent data as specified by business users Utilized Apache Spark with Python to develop and execute Big Data Analytics and Machine learning applications executed machine learning use cases under Spark ML and MLlib Installed Hadoop Map Reduce HDFS Azure to develop multiple MapReduce jobs in PIG and Hive for data cleaning and preprocessing Used Spark API over Hortonworks Hadoop YARN to perform analytics on data in Hive Improved the performance and optimization of the existing algorithms in Hadoop using Spark Context SparkSQL Data Frame Pair RDDs Spark YARN Developed Spark code using Scala and SparkSQLStreaming for faster testing and processing of data Developed a Spark job in Java which indexes data into Elastic Search from external Hive tables which are in HDFS Explored MLlib algorithms in Spark to understand the possible Machine Learning functionalities that can be used for our use case Used windows Azure SQL reporting services to create reports with tables charts and maps Executed Hive queries on Parquet tables stored in Hive to perform data analysis to meet the business requirements Used Oozie and Zookeeper operational services for coordinating cluster and scheduling workflows Configured Spark streaming to receive real time data from Kafka and store the stream data to HDFS using Scala Continuous coordination with QA team production support team and deployment Performed transformations cleaning and filtering on imported data using Hive Map Reduce and loaded final data into HDFS Explored with the Spark improving the performance and optimization of the existing algorithms in Hadoop using Spark Context SparkSQL Data Frame Pair RDDs Spark YARN Import the data from different sources like HDFSHBase into Spark RDD and developed a data pipeline using Kafka and Storm to store data into HDFS Used Spark streaming to receive real time data from the Kafka and store the stream data to HDFS using Scala and NoSQL databases such as HBase and Cassandra Documented the requirements including the available code which should be implemented using Spark Hive HDFS HBase and Elastic Search Performed transformations like event joins filter boot traffic and some preaggregations using Pig Developed code in Java which creates mapping in Elastic Search even before data is indexed into Configured Oozie workflow to run multiple Hive and Pig jobs which run independently with time and data availability Imported and exported the analyzed data to the relational databases using Sqoop for visualization and to generate reports for the BI team Support Cloud Strategy team to integrate analytical capabilities into an overall cloud architecture and business case development Developed customized Hive UDFs and UDAFs in Java JDBC connectivity with hive development and execution of Pig scripts and Pig UDFs Used Hadoop YARN to perform analytics on data in Hive Developed and maintained batch data flow using HiveQL and Unix scripting Developed and execute data pipeline testing processes and validate business rules and policies Built code for real time data ingestion using Java MapRStreams Kafka and STORM Environment Hadoop 30 HBase Sqoop Zookeeper Oozie Hive 12 Pig 017 Agile Azure MapReduce Spark 23 J2EE Java Zookeeper 34 Oozie Cassandra 311 NoSQL Big DataHadoop Developer HCSC Chicago IL July 2015 to April 2017 Responsibilities Extensively worked on Hadoop ecosystems including Hive MongoDB Zookeeper Spark Streaming with MapR distribution Developed Big Data solutions focused on pattern matching and predictive modeling Worked on analyzing Hadoop cluster and different big data analytic tools including Pig HBase database and Sqoop Involved in Agile methodologies daily scrum meetings spring planning Created RDDs and applied data filters in Spark and created Cassandra tables and Hive tables for user access Worked on NoSQL support enterprise production and loading data into HBase using Impala and Sqoop Performed multiple MapReduce jobs in Pig and Hive for data cleaning and preprocessing Build Hadoop solutions for big data problems using MR1 and MR2 in YARN Handled importing of data from various data sources performed transformations using Hive Pig and loaded data into HDFS Involved in identifying job dependencies to design workflow for Oozie YARN resource management Performed Hadoop installation configuration of multiple nodes in AWSEC2 using Hortonworks platform Worked with NoSQL databases like HBase in creating HBase tables to load large sets of semistructured data coming from various sources Involved in Hadoop cluster administration and successful in maintenance of large volumes of storage Upgraded the Hadoop Cluster from CDH3 to CDH4 setting up High Availability Cluster and integrating Hive with existing applications Designed Developed a Flattened View Merge and Flattened dataset denormalizing several Datasets in HiveHDFS which consists of key attributes consumed by Business and other down streams Analyzed the existing data flow to the warehouses and taking the similar approach to migrate the data into HDFS Involved in PLSQL query optimization to reduce the overall run time of stored procedures Exported data from HDFS to RDBMS via Sqoop for Business Intelligence visualization and user report generation Involved in designing schema writing CQLs and loading data using Cassandra Built the automated build and deployment framework using Jenkins Maven Implemented MapReduce jobs in HIVE by querying the available data Configured Hive Meta store with MySQL which stores the metadata for Hive tables Performance tuning of Hive queries MapReduce programs for different applications Proactively involved in ongoing maintenance support and improvements in Hadoop cluster Worked with cloud provisioning team on a capacity planning and sizing of the nodes Master and Slave for an AWS EMR Cluster Worked on data using Sqoop from HDFS to Relational Database Systems and viceversa Maintaining and troubleshooting Exploring with Spark to improve the performance and optimization of the existing algorithms in Hadoop using Spark context SparkSQL Data Frame pair RDDs Created Hive Tables loaded claims data from Oracle using Sqoop and loaded the processed data into target database Used Cloudera Manager for installation and management of Hadoop Cluster Worked on MongoDB HBase databases which differ from classic relational databases Involved in converting HiveQL into Spark transformations using Spark RDD and through Scala programming Integrated KafkaSpark streaming for high efficiency throughput and reliability Worked in tuning Hive Pig to improve performance and solved performance issues in both scripts Environment Hadoop 30 Hive 12 MongoDB Zookeeper 34 Pig 017 HBase Sqoop Agile NoSQL Impala 30 MapReduce YARN Oozie AWS Hortonworks Kafka Cassandra 311 Sr JavaHadoop Developer TMobile Bellevue WA November 2013 to June 2015 Responsibilities Worked on analyzing Hadoop cluster and different big data analytic tools including Pig Hive and Sqoop Worked on implementation and maintenance of Cloudera Hadoop cluster Assisted in upgrading configuration and maintenance of various Hadoop infrastructures like Pig Hive and HBase Developed presentation layer components comprising of JSP Servlets and JavaBeans using the Spring framework Responsible for developing various modules frontend and backend components using several design patterns based on clients business requirements Designed and Developed application modules using spring and Hibernate frameworks Involved in developing Use case diagrams Class diagrams Sequence diagrams and process flow diagrams for the modules using UML and Rational Rose Developed the presentation layer using JSP AJAX HTML Bootstrap XHTML AngularJS CSS and client validations using JavaScript Designed and developed SOAP RESTFUL web services Developed Java classes confirming J2EE design patterns JNDI packaged with J2EE specifications Developed and executed custom MapReduce programs Pig Latin scripts and HQL queries Used Hadoop FS scripts for HDFS Hadoop File System data loading and manipulation Developed simple to complex MapReduce streaming jobs using Java language for processing and validating the data Implemented Spark using Python and Spark SQL for faster processing of data Developed Spark jobs and Hive Jobs to summarize and transform data Involved in converting HiveSQL queries into Spark transformations using Spark data frames Scala and Python Consumed Web Services using WSDL SOAP and UDDI from third party for authorizing payments tofrom customers Developed and implemented the MVC Architectural Pattern using Spring Framework Extensively used Spring JDBC in data access layer to access and update information in the database Developed a RESTful API that provided account management capability as well as security role lookup and management for all downstream dependencies Responsible for the design and development of the application framework Used springs test framework to create integration tests for various spring boot and spring batch applications Implemented data access using Hibernate ORM Tool persistence framework Environment Pig 014 Hive 10 Sqoop HBase Spring 45 JavaBeans Hibernate 48 JavaScript AJAX HTML Bootstrap AngularJS CSS J2EE Java HDFS Spark Python Scala MVC Sr JavaFull Stack Developer CoreLogic Austin TX April 2012 to October 2013 Responsibilities Involved in various phases of Software Development Life Cycle SDLC of the application like Requirement gathering Design Analysis and Code development Implemented MVC architecture and DAO design pattern for maximum abstraction of the application and code reusability Developed web application using JSP custom tag libraries Spring Action classes and Action Designed Java Servlets and Objects using J2EE standards Configured the application using Spring framework annotations and developed Spring Controllers for request and response processing and implemented RESTful Web Service Migrated Spring based application to cloud based Micro services Designed and developed the REST based Micro services using the Spring Boot Spring Data with JPA Participated in coding Spring AOP components for the Transactional Model to handle many requests Involved in developing Java APIs which communicates with the JavaBeans Implemented JavaJ2EE Design patterns like Business Delegate and Data Transfer Object DTO Data Access Object Used JSP for presentation layer developed high performance objectrelational persistence and query service for entire application utilizing Hibernate Developed views using Bootstrap components AngularUI and involved in configuring routing for various modules using angular UI router Extensively used HTML JavaScript Angularjs and Ajax for client side development and validations Developed Web services SOAP through WSDL in Apache Axis to interact with other components Created Stateless Session EJBs for retrieving data and Entity Beans for maintaining User Profile Used Log4j as logging framework to capture the log traces of applications in debugging the issues Used ANT automated build scripts to compile and package the application Designed database and created tables written the complex SQL Queries and stored procedures as per the requirements Involved in Unit Integration and Performance Testing for the new enhancements Developed the application using Java Beans Servlets and EJBs Environment SDLC MVC JSP J2EE Java JavaBeans Hibernate 35 Bootstrap HTML JavaScript Angularjs Ajax Log4j ANT Java Developer Impetus October 2009 to March 2012 Responsibilities As a Developer in Java involved in System Requirements analysis and conceptual design Experienced in Web development with JavaScript JQuery HTML Bootstrap CSS and Ajax Implemented Java batch jobs for nightly runs and worked heavily on concurrency API for a low latency high throughput application Worked on applications included in requirement analysis Design Development and Testing Used ANT Maven to build deploy applications helped to deployment for CI using Jenkins and Maven Designed and developed Application based on Spring Framework Spring MVC and Spring templates Created JUnit test cases for unit testing the code at minute level and used Eclipse IDE Developed and designed new crossbrowser accessible interfaces using JQuery and JavaScript Involved in writing JSPs JavaScript and Servlets to generate dynamic web pages and web content Developed Use Case Diagrams Object Diagrams and Class Diagrams in UML using Rational Rose Worked on Spring IoC Spring MVC framework Spring Messaging Framework and Spring AOP to develop application service components Used Hibernate ORM framework for persistence to database by integrating it with Spring framework using Spring Hibernate template Used JavaScript and JQuery for providing clientside validation and Spring Validators for serverside validation Used JQuery to make the frontend components interact with the JavaScript functions to add dynamism to the web pages at the client side Developed User interface using JSP Angular JS JSP Tag libraries third party libraries and JavaScript Used JavaScript JQuery and Ajax API for intensive user operations and clientside validations Used Hibernate framework in persistence layer for mapping an objectoriented domain model to a relational database Used Spring Core for concept Inversion of control IOC implemented using dependency injection Developed Object Model and UML design models for developing Use cases and created Sequence diagram class diagram and active diagrams for application components and interfaces Defined and developed the applications presentation layer using HTML CSS and JavaScript Developed the User Interface using JSP and used CSS for style setting of the Web Pages Involved in developing applications for workflow using JSPs spring MVC module Hibernate AJAX JavaScript technologies using Apache Tomcat Implemented design patterns like DAO singleton factory to achieve design principles Used Jenkins for continuous integration and Maven for building the EAR file Generated JUnit test cases for testing various Java components Environment JavaScript JQuery HTML 4 Bootstrap CSS Ajax Java ANT Maven Jenkins Hibernate MVC JUnit Education Bachelors Skills AJAX 5 years Apache 6 years APACHE HADOOP HDFS 5 years APACHE HADOOP MAPREDUCE 5 years APACHE HADOOP SQOOP 5 years APACHE HBASE 5 years Bootstrap 5 years Database 9 years Hadoop 5 years HADOOP DISTRIBUTED FILE SYSTEM 5 years HBase 5 years HDFS 5 years Hive 5 years HTML 5 years Java 7 years JavaScript 5 years JSP 5 years MapReduce 5 years Servlets 5 years SQL 7 years",
    "unique_id": "e0fd5b8d-1255-437b-bbe2-da535047fa8c"
}