{
    "clean_data": "Big Data Engineer Big Data Engineer Big Data Engineer ATT Atlanta GA 9 years of experience in IT industry with extensive experience in Java J2ee and Big data technologies 4 years working of exclusive experience on Big Data technologies and Hadoop stack Strong experience working with HDFS MapReduce Spark Hive Pig Sqoop Flume Kafka Oozie and HBase Good understanding of distributed systems HDFS architecture internal working details of MapReduce and Spark processing frameworks More than two years of hands on experience using Spark framework with Scala Good exposure to performance tuning hive queries mapreduce jobs spark jobs Expertise in Inbound and Outbound importingexporting data formto traditional RDBMS using SQOOP Tuned PIG and HIVE scripts by understanding the joins group and aggregation between them Extensively worked on HiveQL join operations writing custom UDFs and having good experience in optimizing Hive Queries Worked on various Hadoop Distributions Cloudera Hortonworks Amazon AWS to implement and make use of those Participated in design development and system migration of high performance metadata driven data pipeline with Kafka and HivePresto on Qubole providing data export capability through API and UI Experience data processing like collecting aggregating moving from various sources using Apache Flume and Kafka Hands on experience in NOSQL databases and SQL databases Authorized to work in the US for any employer Work Experience Big Data Engineer ATT Atlanta GA July 2016 to Present Responsibilities Extensively involved in Design phase and delivered Design documents in Hadoop eco system with HDFS HIVE PIG SQOOP and SPARK with SCALA Collected the logs from the physical machines and the Open Stack controller and integrated into HDFS using Kafka Involved in the highlevel design of the Hadoop architecture for the existing data structure and Business process Part of Configuring deployment of Hadoop Cluster in the AWS cloud Worked with clients to better understand their reporting and dash boarding needs and present solutions using structured Agile project methodology approach Worked on analyzing Hadoop cluster and different Big Data Components including Pig Hive Spark HBase Kafka Elastic Search database and SQOOP Involved in loading disparate datasets into Hadoop Data Lake this would be available to the data science team to predict the future Implemented Partitioning Dynamic Partitions and Buckets in HIVE for increasing performance benefit and helping in organizing data in a logical fashion Installed Hadoop Map Reduce HDFS and developed multiple MapReduce jobs in PIG and Hive for data cleaning and preprocessing Created tables in HBase to store variable data formats of PII data coming from different portfolios Worked on Sequence files RC files Map side joins bucketing partitioning forHive performance enhancement and storage improvement Experienced in pulling the data from Amazon S3 bucket to Data Lake and builtHive tables on top of it and created data frames in Spark to perform further analysis Used cloud computing on the multinode cluster and deployed Hadoop application on cloud S3 and used Elastic Map Reduce EMR to run aMapReduce Explored MLlib algorithms in Spark to understand the possible Machine Learning functionalities that can be used for use case In preprocessing phase of data extraction we used Spark to remove all the missing data for transforming of data to create new features Developed data pipeline using Flume Sqoop Pig and Java map reduce to ingest customer behavioral data and financial histories into HDFS for analysis Involved in loading data from UNIX file system to HDFS using Flume and HDFSAPI Configured Spark Streaming to receive real time data from the Kafka and storethe stream data to HDFS Developed RDDsData Frames in Spark using Scala and Python and applied several transformation logics to load data from Hadoop Data Lake to Cassandra DB Exported the analyzed data to the NoSQL Database using HBase for visualization and to generate reports for the Business Intelligence team using SAS Used various HBase commands and generated different Datasets as per requirements and provided access to the data when required using grant and Revoke Created Hive tables as per requirement as internal or external tables intended for efficiency Developed MapReduce programs for the files generated by hive query processing to generate key value pairs and upload the data to NoSQL database HBase Implemented installation and configuration of multinode cluster on the cloud using Amazon Web Services AWS on EC2 Created and maintained Technical documentation for launching Hadoop Clusters and for executing Hive queries and Pig Scripts Worked in tuning Hive Pig to improve performance and solved performance issues in both scripts Worked with Elastic MapReduce EMR and setting up environments on Amazon AWS EC2 instances Developed various data connections from data source to SSIS Tableau Server for report and dashboard development Involved unit testing interface testing system testing and user acceptance testing of the workflow tool Used JIRA for bug tracking and GIT for version control Involved in storydriven agile development methodology and actively participated in daily scrum meetings Environment Apache Hadoop 30 AWS MLlib MYSQL Kafka HDFS 12 Hive 23 Pig017 MapReduce Flume 18 Cloudera Oozie UNIX Oracle 12c Tableau 7 GIT UNIX Developer Big Data January 2015 to June 2016 Confidential NJ Responsibilities Involved in complete Big Data flow of the application data ingestion from upstream to HDFS processing the data in HDFS and analyzing the data using several tools Imported the data from various formats like JSON Sequential Text CSV AVRO and Parquet to HDFS cluster with compressed for optimization Experienced on ingesting data from RDBMS sources like Oracle SQL Server and Teradata into HDFS using Sqoop Configured Hive and written Hive UDFs and UDAFs Also created partitions such as Static and Dynamic with bucketing Implemented advanced procedures like text analytics and processing using the inmemory computing capabilities like spark Importing and exporting data into HDFS and hive using Sqoop and Kafka with batch and streaming Experienced with SparkStreaming APIs to perform transformations and actions on the fly for building the common learner data model which gets the data from Kafka in near real time and Persists into HBase Performance analysis of Spark streaming and batch jobs by using Spark tuning parameters Enhanced and optimized product Spark code to aggregate group and run data mining tasks using the Spark framework Developed Spark scripts by using Python shell commands as per the requirement Using Hive join queries to join multiple tables of a source system and load them to Elastic search tables Experience in managing and reviewing huge Hadoop log files Expertise in designing and creating various analytical reports and Automated Dashboards to help users to identify critical KPIs and facilitate strategic planning in the organization Involved in Cluster maintenance Cluster Monitoring and Troubleshooting Created Data Pipelines as per the business requirements and scheduled it using Oozie Coordinators Maintaining technical documentation for each and every step of development environment and launching Hadoop clusters Worked on BI tools as Tableau to create dashboards like weekly monthly daily reports using tableau desktop and publish them to HDFS cluster Environment Scala Hadoop HDFS Hive Oozie SqoopKafka Elastic Search Shell Scripting HBase Python GitHub Tableau Oracle MySQL Teradata and AWS Hadoop Developer Nationwide Mutual Insurance company Columbus OH June 2012 to November 2014 Responsibilities Experience on AWSEMR Spark installation HDFS and MapReduce Architecture Participated in Hadoop Deployment and infrastructure scaling Involved in creating Hive tables and loading and analyzing data using hive queries Developed Simple to complex Map Reduce Jobs using Hive and Pig Developed workflow in Oozie to automate the tasks of loading the data into HDFS and preprocessing with Pig Parsed highlevel design spec to simple ETL coding and mapping standards Maintained warehouse metadata naming standards and warehouse standards for future application development Worked with Linux systems and RDBMS database on a regular basis to ingest data using Sqoop Implemented Kafka consumers to move data from Kafka partitions into Cassandra for near realtime analysis Involved in Hadoop cluster task like adding and removing nodes Managed and reviewed Hadoop log files and loaded log data into HDFS using Sqoop Created and maintained Technical documentation for launching HADOOP Clusters and for executing Hive queries Pig Scripts Sqoop jobs Environment HadoopHortonworks stack HDFS Oozie Pig Hive MapReduce Sqoop Cassandra Linux Hadoop Developer US Bank Denver CO November 2010 to May 2012 Responsibilities Worked on analyzing writing HadoopMapReduce jobs using JavaAPI Pig and Hive Responsible for building scalable distributed data solutions using Hadoop Involved in loading data from edge node to HDFS using shell scripting Created HBase tables to store variable data formats of PII data coming from different portfolios Exported the analyzed data to the relational databases using Sqoop for visualization and to generate reports for the BI team Worked with using different kind of compression techniques to save data and optimize data transfer over network using LZO Snappy and Bzip etc Analyze large and critical datasets using Cloudera HDFS HBase MapReduce Hive HiveUDF Pig Sqoop Zookeeper Spark Developed custom aggregate functions using SparkSQL and performed interactive querying Used Scoop to store the data into HBase and Hive Worked on installing cluster commissioning decommissioning of DataNode NameNode high availability capacity planning and slots configuration Creating Hive tables dynamic partitions buckets for sampling and working on them using HiveQL Used Pig to parse the data and Store in Avro format Stored the data in tabular formats using Hive tables and Hive Serdes Collecting and aggregating large amounts of log data using Apache Flume and staging data in HDFS for further analysis Worked with NoSQL databases like HBase for creating HBase tables to load large sets of semi structured data coming from various sources Implemented a script to transmit information from Oracle to HBase using Sqoop Implemented MapReduce programs to handle semiunstructured data like XML JSON and sequence files for log files Finetuned Pig queries for better performance Involved in writing the shell scripts for exporting log files to Hadoop cluster through automated process Installed Oozie workflow engine to run multiple Hive and pig jobs Analyzed large amounts of data sets to determine optimal way to aggregate and report on it Environment Hadoop Map Reduce HDFS Yarn Sqoop Oozie Pig Hive HBase Java Eclipse UNIX shell scripting python Horton works Java Developer GE Healthcare Richmond TX January 2010 to October 2010 Responsibilities Effectively interacted with team members and business users for requirements gathering Involved in analysis design and implementation phases of the software development lifecycle SDLC Implementation of spring core J2EE patterns like MVC Dependency Injection DI and Inversion of Control IOC Implemented REST Web Services with Jersey API to deal with customer requests Developed test cases using J Unit and used Log4j as the logging framework Worked with HQL and Criteria API from retrieving the data elements from database Developed user interface using HTML Spring Tags JavaScript JQuery and CSS Developed the application using Eclipse IDE and worked under Agile Environment Design and implementation of front end web pages using CSS JSP HTML java Script Ajax and Struts Utilized Eclipse IDE as improvement environment to plan create and convey Spring segments on Web Logic Environment Java J2EE HTML JavaScript CSS J Query Spring 30 JNDI Hibernate 30 Java Mail Web Services REST Oracle 10g JUnit Log4j Eclipse Web logic 103 Java Developer BMO Harris Bank Milwaukee WI June 2008 to December 2009 Responsibilities Responsible for Daily maintenance and improvement of live website using HTML5 CSS3 JavaScript JQuery Involved in Software Development Life Cycle phases like requirements gathering Analysis Design Development and Testing Participated in requirement gathering and worked closely with the architect in designing and modeling Worked on development of SQL and stored procedures trigger and function on MYSQL Developed all the UI pages using HTML5 DHTML XSLXSLT XHTML DOM CSS3 JSON JavaScript JQuery Ajax Adobe Creative suite Updated billing pages using HTML CSS in Angularjs framework Performed form validations using reactive forms from Angular framework Integrated the Java code API in JSP pages and responsible for setting up AngularJS framework for UI development Developed html views with HTML5 CSS3 bootstrap and AngularJS Implemented code according to coding standards and Created AngularJS Controller which isolate scopes perform operations Created the Application using AngularJS and NodeJS libraries and used NPM to manage dependencies and gulp to minify rectify and babelify the code Involved in developing the web pages using Angular which are powerful in building the Single page web applications Used the Nodejs backbonejs and Requirejs MVC Frameworks in the development of the web applications Used advanced level of HTML5 JavaScript CSS3 and pure CSS layouts table less layout Produced content pages with CSS3 layout and style markup presentations and also used JavaScript methods Involved in Consuming Restful Services using Angular http Service Developed and consumed Restful services using WEB API Used AJAX coding techniques to update parts of a web page Developed the Client side validation using Java Script and JQuery and server side using Server side validations Built efficient Angularjs backend for client web application Also participated in server side programming with java using JDBC Servlets and JSP Designed Frontend with in object oriented JavaScript Framework like BackboneJS AngularJS and ExtJS Used Angularjs to create server side applications Used its workhorse connectors and libraries relating to HTTP SSL compression file system access etc Developed certain features of the application functionality ie CRUD Create read update delete features using Backbonejs Requirejs and Responsive Design Designed and Developed Java Script frame work which is wrapper on top of JQUERY frame work and AJAX based UI frame work for UI Configuration widgets Involved with Mobile development team to make mobile website responsive and fast added AJAX functionality Involved in developing XML HTML and JavaScript for client side presentation and data validation on the client side with in the forms Involved in redesigning the entire site with CSS styles for consistent look and feel across all browsers and all pages Worked with the team of architects and backend Developers to gather requirements and enhance the application functionality and add new features Participated in the status meetings and status updating to the management team Environment SOAP Restful web services JavaScript Angular JS SQL MySQL JQuery Apache web server PHP SQL Developer tool JDBC CSS3 HTML5 Education Bachelors Skills APACHE HADOOP HDFS 7 years APACHE HADOOP SQOOP 7 years Hadoop 7 years HADOOP 7 years HADOOP DISTRIBUTED FILE SYSTEM 7 years Additional Information TECHNICAL SKILLS Big Data Ecosystem Hadoop Spark Scala Map Reduce HDFS Hive Pig Sqoop Flume Kafka HBase Java Technologies JSP Servlets Junit Spring Hibernate Database Technologies MySQL SQL server Oracle MS Access Programming Languages Scala Python Java and Linux shell scripting Operating Systems Windows LINUX",
    "entities": [
        "the Business Intelligence",
        "CSS Developed",
        "Hadoop Clusters",
        "Oracle SQL Server",
        "Created the Application",
        "AJAX",
        "Angular",
        "SparkStreaming",
        "SPARK",
        "Nodejs",
        "US Bank",
        "Troubleshooting Created Data Pipelines",
        "BI",
        "CSS JSP",
        "HDFS",
        "UNIX",
        "Sqoop Created",
        "Developed Spark",
        "Data Lake",
        "Revoke Created Hive",
        "Agile Environment Design",
        "Flume Sqoop Pig",
        "Amazon AWS EC2",
        "Amazon Web Services AWS",
        "UDAFs",
        "Hadoop",
        "Atlanta",
        "NOSQL",
        "Backbonejs Requirejs and Responsive Design Designed",
        "JSON Sequential Text CSV",
        "Maintained",
        "Present Responsibilities",
        "HBase",
        "Inbound",
        "TX",
        "Amazon",
        "SSIS",
        "PHP SQL Developer",
        "Inversion of Control IOC Implemented REST Web Services",
        "Oozie Coordinators Maintaining",
        "Developed",
        "SparkSQL",
        "HDFS Developed RDDsData Frames",
        "Restful",
        "Hadoop Deployment",
        "Big Data Engineer Big Data Engineer Big Data",
        "Hadoop Data Lake",
        "AWSEMR Spark",
        "Hadoop Involved",
        "JSP",
        "Elastic MapReduce EMR",
        "GitHub Tableau Oracle",
        "HivePresto",
        "SQOOP Involved",
        "UI Experience",
        "Work Experience Big Data",
        "Worked",
        "Denver",
        "Built",
        "Hadoop Cluster",
        "J Unit",
        "HDFS MapReduce Spark Hive Pig",
        "Oracle MS Access Programming Languages Scala Python",
        "MVC",
        "Spark",
        "Agile",
        "GIT",
        "GE Healthcare",
        "Script Ajax",
        "HTML CSS",
        "API",
        "US",
        "Sqoop",
        "HIVE",
        "Big Data Components",
        "Created",
        "AWS",
        "Analyzed",
        "HTML Spring Tags",
        "Server",
        "PIG",
        "GIT UNIX Developer",
        "MapReduce Architecture Participated",
        "log data",
        "SAS",
        "Oozie",
        "SQL",
        "Design",
        "Flume",
        "Oracle to HBase",
        "NPM",
        "HADOOP Clusters",
        "Big Data",
        "Hive",
        "SQOOP",
        "Amazon AWS",
        "ETL",
        "Additional Information TECHNICAL SKILLS Big Data Ecosystem Hadoop",
        "PII",
        "JavaScript",
        "UI",
        "Java Mail Web Services",
        "JNDI Hibernate",
        "LZO",
        "Cloudera HDFS HBase MapReduce Hive HiveUDF Pig Sqoop Zookeeper Spark Developed",
        "Expertise",
        "CSS",
        "Requirejs MVC Frameworks",
        "Created HBase",
        "Consuming Restful Services",
        "MapReduce",
        "Hive Queries Worked",
        "NoSQL",
        "Tableau",
        "AWS Hadoop Developer Nationwide Mutual Insurance",
        "SCALA Collected",
        "UI Configuration",
        "NoSQL Database",
        "JQuery"
    ],
    "experience": "Experience data processing like collecting aggregating moving from various sources using Apache Flume and Kafka Hands on experience in NOSQL databases and SQL databases Authorized to work in the US for any employer Work Experience Big Data Engineer ATT Atlanta GA July 2016 to Present Responsibilities Extensively involved in Design phase and delivered Design documents in Hadoop eco system with HDFS HIVE PIG SQOOP and SPARK with SCALA Collected the logs from the physical machines and the Open Stack controller and integrated into HDFS using Kafka Involved in the highlevel design of the Hadoop architecture for the existing data structure and Business process Part of Configuring deployment of Hadoop Cluster in the AWS cloud Worked with clients to better understand their reporting and dash boarding needs and present solutions using structured Agile project methodology approach Worked on analyzing Hadoop cluster and different Big Data Components including Pig Hive Spark HBase Kafka Elastic Search database and SQOOP Involved in loading disparate datasets into Hadoop Data Lake this would be available to the data science team to predict the future Implemented Partitioning Dynamic Partitions and Buckets in HIVE for increasing performance benefit and helping in organizing data in a logical fashion Installed Hadoop Map Reduce HDFS and developed multiple MapReduce jobs in PIG and Hive for data cleaning and preprocessing Created tables in HBase to store variable data formats of PII data coming from different portfolios Worked on Sequence files RC files Map side joins bucketing partitioning forHive performance enhancement and storage improvement Experienced in pulling the data from Amazon S3 bucket to Data Lake and builtHive tables on top of it and created data frames in Spark to perform further analysis Used cloud computing on the multinode cluster and deployed Hadoop application on cloud S3 and used Elastic Map Reduce EMR to run aMapReduce Explored MLlib algorithms in Spark to understand the possible Machine Learning functionalities that can be used for use case In preprocessing phase of data extraction we used Spark to remove all the missing data for transforming of data to create new features Developed data pipeline using Flume Sqoop Pig and Java map reduce to ingest customer behavioral data and financial histories into HDFS for analysis Involved in loading data from UNIX file system to HDFS using Flume and HDFSAPI Configured Spark Streaming to receive real time data from the Kafka and storethe stream data to HDFS Developed RDDsData Frames in Spark using Scala and Python and applied several transformation logics to load data from Hadoop Data Lake to Cassandra DB Exported the analyzed data to the NoSQL Database using HBase for visualization and to generate reports for the Business Intelligence team using SAS Used various HBase commands and generated different Datasets as per requirements and provided access to the data when required using grant and Revoke Created Hive tables as per requirement as internal or external tables intended for efficiency Developed MapReduce programs for the files generated by hive query processing to generate key value pairs and upload the data to NoSQL database HBase Implemented installation and configuration of multinode cluster on the cloud using Amazon Web Services AWS on EC2 Created and maintained Technical documentation for launching Hadoop Clusters and for executing Hive queries and Pig Scripts Worked in tuning Hive Pig to improve performance and solved performance issues in both scripts Worked with Elastic MapReduce EMR and setting up environments on Amazon AWS EC2 instances Developed various data connections from data source to SSIS Tableau Server for report and dashboard development Involved unit testing interface testing system testing and user acceptance testing of the workflow tool Used JIRA for bug tracking and GIT for version control Involved in storydriven agile development methodology and actively participated in daily scrum meetings Environment Apache Hadoop 30 AWS MLlib MYSQL Kafka HDFS 12 Hive 23 Pig017 MapReduce Flume 18 Cloudera Oozie UNIX Oracle 12c Tableau 7 GIT UNIX Developer Big Data January 2015 to June 2016 Confidential NJ Responsibilities Involved in complete Big Data flow of the application data ingestion from upstream to HDFS processing the data in HDFS and analyzing the data using several tools Imported the data from various formats like JSON Sequential Text CSV AVRO and Parquet to HDFS cluster with compressed for optimization Experienced on ingesting data from RDBMS sources like Oracle SQL Server and Teradata into HDFS using Sqoop Configured Hive and written Hive UDFs and UDAFs Also created partitions such as Static and Dynamic with bucketing Implemented advanced procedures like text analytics and processing using the inmemory computing capabilities like spark Importing and exporting data into HDFS and hive using Sqoop and Kafka with batch and streaming Experienced with SparkStreaming APIs to perform transformations and actions on the fly for building the common learner data model which gets the data from Kafka in near real time and Persists into HBase Performance analysis of Spark streaming and batch jobs by using Spark tuning parameters Enhanced and optimized product Spark code to aggregate group and run data mining tasks using the Spark framework Developed Spark scripts by using Python shell commands as per the requirement Using Hive join queries to join multiple tables of a source system and load them to Elastic search tables Experience in managing and reviewing huge Hadoop log files Expertise in designing and creating various analytical reports and Automated Dashboards to help users to identify critical KPIs and facilitate strategic planning in the organization Involved in Cluster maintenance Cluster Monitoring and Troubleshooting Created Data Pipelines as per the business requirements and scheduled it using Oozie Coordinators Maintaining technical documentation for each and every step of development environment and launching Hadoop clusters Worked on BI tools as Tableau to create dashboards like weekly monthly daily reports using tableau desktop and publish them to HDFS cluster Environment Scala Hadoop HDFS Hive Oozie SqoopKafka Elastic Search Shell Scripting HBase Python GitHub Tableau Oracle MySQL Teradata and AWS Hadoop Developer Nationwide Mutual Insurance company Columbus OH June 2012 to November 2014 Responsibilities Experience on AWSEMR Spark installation HDFS and MapReduce Architecture Participated in Hadoop Deployment and infrastructure scaling Involved in creating Hive tables and loading and analyzing data using hive queries Developed Simple to complex Map Reduce Jobs using Hive and Pig Developed workflow in Oozie to automate the tasks of loading the data into HDFS and preprocessing with Pig Parsed highlevel design spec to simple ETL coding and mapping standards Maintained warehouse metadata naming standards and warehouse standards for future application development Worked with Linux systems and RDBMS database on a regular basis to ingest data using Sqoop Implemented Kafka consumers to move data from Kafka partitions into Cassandra for near realtime analysis Involved in Hadoop cluster task like adding and removing nodes Managed and reviewed Hadoop log files and loaded log data into HDFS using Sqoop Created and maintained Technical documentation for launching HADOOP Clusters and for executing Hive queries Pig Scripts Sqoop jobs Environment HadoopHortonworks stack HDFS Oozie Pig Hive MapReduce Sqoop Cassandra Linux Hadoop Developer US Bank Denver CO November 2010 to May 2012 Responsibilities Worked on analyzing writing HadoopMapReduce jobs using JavaAPI Pig and Hive Responsible for building scalable distributed data solutions using Hadoop Involved in loading data from edge node to HDFS using shell scripting Created HBase tables to store variable data formats of PII data coming from different portfolios Exported the analyzed data to the relational databases using Sqoop for visualization and to generate reports for the BI team Worked with using different kind of compression techniques to save data and optimize data transfer over network using LZO Snappy and Bzip etc Analyze large and critical datasets using Cloudera HDFS HBase MapReduce Hive HiveUDF Pig Sqoop Zookeeper Spark Developed custom aggregate functions using SparkSQL and performed interactive querying Used Scoop to store the data into HBase and Hive Worked on installing cluster commissioning decommissioning of DataNode NameNode high availability capacity planning and slots configuration Creating Hive tables dynamic partitions buckets for sampling and working on them using HiveQL Used Pig to parse the data and Store in Avro format Stored the data in tabular formats using Hive tables and Hive Serdes Collecting and aggregating large amounts of log data using Apache Flume and staging data in HDFS for further analysis Worked with NoSQL databases like HBase for creating HBase tables to load large sets of semi structured data coming from various sources Implemented a script to transmit information from Oracle to HBase using Sqoop Implemented MapReduce programs to handle semiunstructured data like XML JSON and sequence files for log files Finetuned Pig queries for better performance Involved in writing the shell scripts for exporting log files to Hadoop cluster through automated process Installed Oozie workflow engine to run multiple Hive and pig jobs Analyzed large amounts of data sets to determine optimal way to aggregate and report on it Environment Hadoop Map Reduce HDFS Yarn Sqoop Oozie Pig Hive HBase Java Eclipse UNIX shell scripting python Horton works Java Developer GE Healthcare Richmond TX January 2010 to October 2010 Responsibilities Effectively interacted with team members and business users for requirements gathering Involved in analysis design and implementation phases of the software development lifecycle SDLC Implementation of spring core J2EE patterns like MVC Dependency Injection DI and Inversion of Control IOC Implemented REST Web Services with Jersey API to deal with customer requests Developed test cases using J Unit and used Log4j as the logging framework Worked with HQL and Criteria API from retrieving the data elements from database Developed user interface using HTML Spring Tags JavaScript JQuery and CSS Developed the application using Eclipse IDE and worked under Agile Environment Design and implementation of front end web pages using CSS JSP HTML java Script Ajax and Struts Utilized Eclipse IDE as improvement environment to plan create and convey Spring segments on Web Logic Environment Java J2EE HTML JavaScript CSS J Query Spring 30 JNDI Hibernate 30 Java Mail Web Services REST Oracle 10 g JUnit Log4j Eclipse Web logic 103 Java Developer BMO Harris Bank Milwaukee WI June 2008 to December 2009 Responsibilities Responsible for Daily maintenance and improvement of live website using HTML5 CSS3 JavaScript JQuery Involved in Software Development Life Cycle phases like requirements gathering Analysis Design Development and Testing Participated in requirement gathering and worked closely with the architect in designing and modeling Worked on development of SQL and stored procedures trigger and function on MYSQL Developed all the UI pages using HTML5 DHTML XSLXSLT XHTML DOM CSS3 JSON JavaScript JQuery Ajax Adobe Creative suite Updated billing pages using HTML CSS in Angularjs framework Performed form validations using reactive forms from Angular framework Integrated the Java code API in JSP pages and responsible for setting up AngularJS framework for UI development Developed html views with HTML5 CSS3 bootstrap and AngularJS Implemented code according to coding standards and Created AngularJS Controller which isolate scopes perform operations Created the Application using AngularJS and NodeJS libraries and used NPM to manage dependencies and gulp to minify rectify and babelify the code Involved in developing the web pages using Angular which are powerful in building the Single page web applications Used the Nodejs backbonejs and Requirejs MVC Frameworks in the development of the web applications Used advanced level of HTML5 JavaScript CSS3 and pure CSS layouts table less layout Produced content pages with CSS3 layout and style markup presentations and also used JavaScript methods Involved in Consuming Restful Services using Angular http Service Developed and consumed Restful services using WEB API Used AJAX coding techniques to update parts of a web page Developed the Client side validation using Java Script and JQuery and server side using Server side validations Built efficient Angularjs backend for client web application Also participated in server side programming with java using JDBC Servlets and JSP Designed Frontend with in object oriented JavaScript Framework like BackboneJS AngularJS and ExtJS Used Angularjs to create server side applications Used its workhorse connectors and libraries relating to HTTP SSL compression file system access etc Developed certain features of the application functionality ie CRUD Create read update delete features using Backbonejs Requirejs and Responsive Design Designed and Developed Java Script frame work which is wrapper on top of JQUERY frame work and AJAX based UI frame work for UI Configuration widgets Involved with Mobile development team to make mobile website responsive and fast added AJAX functionality Involved in developing XML HTML and JavaScript for client side presentation and data validation on the client side with in the forms Involved in redesigning the entire site with CSS styles for consistent look and feel across all browsers and all pages Worked with the team of architects and backend Developers to gather requirements and enhance the application functionality and add new features Participated in the status meetings and status updating to the management team Environment SOAP Restful web services JavaScript Angular JS SQL MySQL JQuery Apache web server PHP SQL Developer tool JDBC CSS3 HTML5 Education Bachelors Skills APACHE HADOOP HDFS 7 years APACHE HADOOP SQOOP 7 years Hadoop 7 years HADOOP 7 years HADOOP DISTRIBUTED FILE SYSTEM 7 years Additional Information TECHNICAL SKILLS Big Data Ecosystem Hadoop Spark Scala Map Reduce HDFS Hive Pig Sqoop Flume Kafka HBase Java Technologies JSP Servlets Junit Spring Hibernate Database Technologies MySQL SQL server Oracle MS Access Programming Languages Scala Python Java and Linux shell scripting Operating Systems Windows LINUX",
    "extracted_keywords": [
        "Data",
        "Engineer",
        "Big",
        "Data",
        "Engineer",
        "Big",
        "Data",
        "Engineer",
        "ATT",
        "Atlanta",
        "GA",
        "years",
        "experience",
        "IT",
        "industry",
        "experience",
        "Java",
        "J2ee",
        "data",
        "technologies",
        "years",
        "experience",
        "Big",
        "Data",
        "technologies",
        "Hadoop",
        "Strong",
        "experience",
        "HDFS",
        "MapReduce",
        "Spark",
        "Hive",
        "Pig",
        "Sqoop",
        "Flume",
        "Kafka",
        "Oozie",
        "HBase",
        "understanding",
        "systems",
        "HDFS",
        "architecture",
        "working",
        "details",
        "MapReduce",
        "Spark",
        "processing",
        "frameworks",
        "years",
        "hands",
        "experience",
        "Spark",
        "framework",
        "Scala",
        "Good",
        "exposure",
        "performance",
        "hive",
        "queries",
        "jobs",
        "spark",
        "jobs",
        "Expertise",
        "Inbound",
        "data",
        "formto",
        "RDBMS",
        "SQOOP",
        "PIG",
        "HIVE",
        "scripts",
        "joins",
        "group",
        "aggregation",
        "HiveQL",
        "join",
        "operations",
        "custom",
        "UDFs",
        "experience",
        "Hive",
        "Queries",
        "Hadoop",
        "Distributions",
        "Cloudera",
        "Hortonworks",
        "Amazon",
        "AWS",
        "use",
        "design",
        "development",
        "system",
        "migration",
        "performance",
        "metadata",
        "data",
        "pipeline",
        "Kafka",
        "HivePresto",
        "Qubole",
        "data",
        "export",
        "capability",
        "API",
        "UI",
        "Experience",
        "data",
        "processing",
        "sources",
        "Apache",
        "Flume",
        "Kafka",
        "Hands",
        "experience",
        "NOSQL",
        "databases",
        "SQL",
        "US",
        "employer",
        "Work",
        "Experience",
        "Big",
        "Data",
        "Engineer",
        "ATT",
        "Atlanta",
        "GA",
        "July",
        "Present",
        "Responsibilities",
        "Design",
        "phase",
        "Design",
        "documents",
        "Hadoop",
        "eco",
        "system",
        "HDFS",
        "HIVE",
        "PIG",
        "SQOOP",
        "SPARK",
        "SCALA",
        "logs",
        "machines",
        "Open",
        "Stack",
        "controller",
        "HDFS",
        "Kafka",
        "highlevel",
        "design",
        "Hadoop",
        "architecture",
        "data",
        "structure",
        "Business",
        "process",
        "Part",
        "Configuring",
        "deployment",
        "Hadoop",
        "Cluster",
        "AWS",
        "cloud",
        "clients",
        "reporting",
        "dash",
        "boarding",
        "needs",
        "solutions",
        "project",
        "methodology",
        "approach",
        "Hadoop",
        "cluster",
        "Big",
        "Data",
        "Components",
        "Pig",
        "Hive",
        "Spark",
        "HBase",
        "Kafka",
        "Elastic",
        "Search",
        "database",
        "SQOOP",
        "datasets",
        "Hadoop",
        "Data",
        "Lake",
        "data",
        "science",
        "team",
        "Partitioning",
        "Dynamic",
        "Partitions",
        "Buckets",
        "HIVE",
        "performance",
        "benefit",
        "data",
        "fashion",
        "Installed",
        "Hadoop",
        "Map",
        "HDFS",
        "MapReduce",
        "jobs",
        "PIG",
        "Hive",
        "data",
        "tables",
        "HBase",
        "data",
        "formats",
        "PII",
        "data",
        "portfolios",
        "Sequence",
        "files",
        "RC",
        "files",
        "Map",
        "side",
        "performance",
        "enhancement",
        "storage",
        "improvement",
        "data",
        "Amazon",
        "S3",
        "bucket",
        "Data",
        "Lake",
        "tables",
        "top",
        "data",
        "frames",
        "Spark",
        "analysis",
        "cloud",
        "computing",
        "multinode",
        "cluster",
        "Hadoop",
        "application",
        "cloud",
        "S3",
        "Elastic",
        "Map",
        "EMR",
        "Explored",
        "MLlib",
        "algorithms",
        "Spark",
        "Machine",
        "Learning",
        "functionalities",
        "use",
        "case",
        "phase",
        "data",
        "extraction",
        "Spark",
        "data",
        "transforming",
        "data",
        "features",
        "data",
        "pipeline",
        "Flume",
        "Sqoop",
        "Pig",
        "Java",
        "map",
        "customer",
        "data",
        "histories",
        "HDFS",
        "analysis",
        "loading",
        "data",
        "UNIX",
        "file",
        "system",
        "HDFS",
        "Flume",
        "HDFSAPI",
        "Spark",
        "Streaming",
        "time",
        "data",
        "Kafka",
        "stream",
        "data",
        "RDDsData",
        "Frames",
        "Spark",
        "Scala",
        "Python",
        "transformation",
        "logics",
        "data",
        "Hadoop",
        "Data",
        "Lake",
        "Cassandra",
        "DB",
        "data",
        "NoSQL",
        "Database",
        "HBase",
        "visualization",
        "reports",
        "Business",
        "Intelligence",
        "team",
        "SAS",
        "HBase",
        "commands",
        "Datasets",
        "requirements",
        "access",
        "data",
        "grant",
        "Hive",
        "tables",
        "requirement",
        "tables",
        "efficiency",
        "MapReduce",
        "programs",
        "files",
        "query",
        "processing",
        "value",
        "pairs",
        "data",
        "NoSQL",
        "database",
        "HBase",
        "installation",
        "configuration",
        "multinode",
        "cluster",
        "cloud",
        "Amazon",
        "Web",
        "Services",
        "AWS",
        "EC2",
        "Created",
        "documentation",
        "Hadoop",
        "Clusters",
        "Hive",
        "queries",
        "Pig",
        "Scripts",
        "Hive",
        "Pig",
        "performance",
        "performance",
        "issues",
        "scripts",
        "MapReduce",
        "EMR",
        "environments",
        "Amazon",
        "AWS",
        "EC2",
        "instances",
        "data",
        "connections",
        "data",
        "source",
        "SSIS",
        "Tableau",
        "Server",
        "report",
        "dashboard",
        "development",
        "unit",
        "testing",
        "interface",
        "testing",
        "system",
        "testing",
        "user",
        "acceptance",
        "testing",
        "tool",
        "JIRA",
        "bug",
        "tracking",
        "GIT",
        "version",
        "control",
        "development",
        "methodology",
        "scrum",
        "meetings",
        "Environment",
        "Apache",
        "Hadoop",
        "AWS",
        "MLlib",
        "MYSQL",
        "Kafka",
        "HDFS",
        "Hive",
        "Pig017",
        "MapReduce",
        "Flume",
        "Cloudera",
        "Oozie",
        "UNIX",
        "Oracle",
        "12c",
        "Tableau",
        "GIT",
        "UNIX",
        "Developer",
        "Big",
        "Data",
        "January",
        "June",
        "Confidential",
        "NJ",
        "Responsibilities",
        "Big",
        "Data",
        "flow",
        "application",
        "data",
        "ingestion",
        "HDFS",
        "data",
        "HDFS",
        "data",
        "tools",
        "data",
        "formats",
        "JSON",
        "Sequential",
        "Text",
        "CSV",
        "AVRO",
        "Parquet",
        "HDFS",
        "cluster",
        "optimization",
        "data",
        "sources",
        "Oracle",
        "SQL",
        "Server",
        "Teradata",
        "HDFS",
        "Sqoop",
        "Configured",
        "Hive",
        "Hive",
        "UDFs",
        "UDAFs",
        "partitions",
        "Static",
        "Dynamic",
        "procedures",
        "text",
        "analytics",
        "processing",
        "inmemory",
        "computing",
        "capabilities",
        "spark",
        "Importing",
        "data",
        "HDFS",
        "hive",
        "Sqoop",
        "Kafka",
        "batch",
        "streaming",
        "SparkStreaming",
        "APIs",
        "transformations",
        "actions",
        "fly",
        "learner",
        "data",
        "model",
        "data",
        "Kafka",
        "time",
        "Persists",
        "HBase",
        "Performance",
        "analysis",
        "Spark",
        "streaming",
        "batch",
        "jobs",
        "Spark",
        "parameters",
        "product",
        "Spark",
        "code",
        "group",
        "data",
        "mining",
        "tasks",
        "Spark",
        "framework",
        "Developed",
        "Spark",
        "scripts",
        "Python",
        "shell",
        "commands",
        "requirement",
        "Hive",
        "join",
        "queries",
        "tables",
        "source",
        "system",
        "search",
        "tables",
        "Experience",
        "Hadoop",
        "log",
        "Expertise",
        "reports",
        "Automated",
        "Dashboards",
        "users",
        "KPIs",
        "planning",
        "organization",
        "Cluster",
        "maintenance",
        "Cluster",
        "Monitoring",
        "Troubleshooting",
        "Created",
        "Data",
        "Pipelines",
        "business",
        "requirements",
        "Oozie",
        "Coordinators",
        "documentation",
        "step",
        "development",
        "environment",
        "Hadoop",
        "clusters",
        "BI",
        "tools",
        "Tableau",
        "dashboards",
        "reports",
        "tableau",
        "desktop",
        "HDFS",
        "cluster",
        "Environment",
        "Scala",
        "Hadoop",
        "HDFS",
        "Hive",
        "Oozie",
        "SqoopKafka",
        "Elastic",
        "Search",
        "Shell",
        "Scripting",
        "HBase",
        "Python",
        "GitHub",
        "Tableau",
        "Oracle",
        "MySQL",
        "Teradata",
        "AWS",
        "Hadoop",
        "Developer",
        "Nationwide",
        "Mutual",
        "Insurance",
        "company",
        "Columbus",
        "OH",
        "June",
        "November",
        "Responsibilities",
        "Experience",
        "AWSEMR",
        "Spark",
        "installation",
        "HDFS",
        "MapReduce",
        "Architecture",
        "Hadoop",
        "Deployment",
        "infrastructure",
        "scaling",
        "Hive",
        "tables",
        "loading",
        "data",
        "hive",
        "queries",
        "Simple",
        "Map",
        "Reduce",
        "Jobs",
        "Hive",
        "Pig",
        "workflow",
        "Oozie",
        "tasks",
        "data",
        "HDFS",
        "Pig",
        "highlevel",
        "design",
        "spec",
        "ETL",
        "mapping",
        "standards",
        "warehouse",
        "metadata",
        "naming",
        "standards",
        "warehouse",
        "standards",
        "application",
        "development",
        "Linux",
        "systems",
        "RDBMS",
        "database",
        "basis",
        "data",
        "Sqoop",
        "Kafka",
        "consumers",
        "data",
        "Kafka",
        "partitions",
        "Cassandra",
        "analysis",
        "Hadoop",
        "cluster",
        "task",
        "nodes",
        "Managed",
        "Hadoop",
        "log",
        "files",
        "log",
        "data",
        "HDFS",
        "Sqoop",
        "Created",
        "documentation",
        "HADOOP",
        "Clusters",
        "Hive",
        "queries",
        "Pig",
        "Scripts",
        "Sqoop",
        "jobs",
        "Environment",
        "HadoopHortonworks",
        "HDFS",
        "Oozie",
        "Pig",
        "Hive",
        "MapReduce",
        "Sqoop",
        "Cassandra",
        "Linux",
        "Hadoop",
        "Developer",
        "US",
        "Bank",
        "Denver",
        "CO",
        "November",
        "May",
        "Responsibilities",
        "HadoopMapReduce",
        "jobs",
        "JavaAPI",
        "Pig",
        "Hive",
        "Responsible",
        "data",
        "solutions",
        "Hadoop",
        "loading",
        "data",
        "edge",
        "node",
        "HDFS",
        "shell",
        "scripting",
        "Created",
        "HBase",
        "data",
        "formats",
        "PII",
        "data",
        "portfolios",
        "data",
        "databases",
        "Sqoop",
        "visualization",
        "reports",
        "BI",
        "team",
        "kind",
        "compression",
        "techniques",
        "data",
        "data",
        "transfer",
        "network",
        "LZO",
        "Snappy",
        "Bzip",
        "datasets",
        "Cloudera",
        "HDFS",
        "HBase",
        "MapReduce",
        "Hive",
        "HiveUDF",
        "Pig",
        "Sqoop",
        "Zookeeper",
        "Spark",
        "custom",
        "aggregate",
        "functions",
        "SparkSQL",
        "Used",
        "Scoop",
        "data",
        "HBase",
        "Hive",
        "Worked",
        "cluster",
        "decommissioning",
        "DataNode",
        "NameNode",
        "availability",
        "capacity",
        "planning",
        "slots",
        "configuration",
        "Hive",
        "tables",
        "partitions",
        "buckets",
        "HiveQL",
        "Used",
        "Pig",
        "data",
        "Store",
        "Avro",
        "format",
        "data",
        "formats",
        "Hive",
        "tables",
        "Hive",
        "Serdes",
        "Collecting",
        "amounts",
        "log",
        "data",
        "Apache",
        "Flume",
        "data",
        "HDFS",
        "analysis",
        "databases",
        "HBase",
        "HBase",
        "tables",
        "sets",
        "data",
        "sources",
        "script",
        "information",
        "Oracle",
        "HBase",
        "Sqoop",
        "MapReduce",
        "programs",
        "data",
        "XML",
        "JSON",
        "sequence",
        "files",
        "log",
        "files",
        "Finetuned",
        "Pig",
        "performance",
        "shell",
        "scripts",
        "log",
        "files",
        "Hadoop",
        "cluster",
        "process",
        "Oozie",
        "workflow",
        "engine",
        "Hive",
        "pig",
        "jobs",
        "amounts",
        "data",
        "sets",
        "way",
        "Environment",
        "Hadoop",
        "Map",
        "Reduce",
        "HDFS",
        "Yarn",
        "Sqoop",
        "Oozie",
        "Pig",
        "Hive",
        "HBase",
        "Java",
        "Eclipse",
        "UNIX",
        "shell",
        "scripting",
        "python",
        "Horton",
        "Java",
        "Developer",
        "GE",
        "Healthcare",
        "Richmond",
        "TX",
        "January",
        "October",
        "Responsibilities",
        "team",
        "members",
        "business",
        "users",
        "requirements",
        "analysis",
        "design",
        "implementation",
        "phases",
        "software",
        "development",
        "lifecycle",
        "SDLC",
        "Implementation",
        "spring",
        "core",
        "J2EE",
        "patterns",
        "MVC",
        "Dependency",
        "Injection",
        "DI",
        "Inversion",
        "Control",
        "IOC",
        "REST",
        "Web",
        "Services",
        "Jersey",
        "API",
        "customer",
        "requests",
        "test",
        "cases",
        "J",
        "Unit",
        "Log4j",
        "framework",
        "HQL",
        "Criteria",
        "API",
        "data",
        "elements",
        "database",
        "user",
        "interface",
        "HTML",
        "Spring",
        "Tags",
        "JavaScript",
        "JQuery",
        "CSS",
        "application",
        "Eclipse",
        "IDE",
        "Agile",
        "Environment",
        "Design",
        "implementation",
        "end",
        "web",
        "pages",
        "CSS",
        "JSP",
        "HTML",
        "java",
        "Script",
        "Ajax",
        "Struts",
        "Eclipse",
        "IDE",
        "improvement",
        "environment",
        "Spring",
        "segments",
        "Web",
        "Logic",
        "Environment",
        "Java",
        "J2EE",
        "HTML",
        "JavaScript",
        "CSS",
        "J",
        "Query",
        "Spring",
        "JNDI",
        "Hibernate",
        "Java",
        "Mail",
        "Web",
        "Services",
        "REST",
        "Oracle",
        "g",
        "JUnit",
        "Log4j",
        "Eclipse",
        "Web",
        "logic",
        "Java",
        "Developer",
        "BMO",
        "Harris",
        "Bank",
        "Milwaukee",
        "WI",
        "June",
        "December",
        "Responsibilities",
        "Daily",
        "maintenance",
        "improvement",
        "website",
        "HTML5",
        "CSS3",
        "JavaScript",
        "JQuery",
        "Software",
        "Development",
        "Life",
        "Cycle",
        "requirements",
        "Analysis",
        "Design",
        "Development",
        "Testing",
        "Participated",
        "requirement",
        "gathering",
        "architect",
        "development",
        "SQL",
        "procedures",
        "trigger",
        "function",
        "MYSQL",
        "UI",
        "pages",
        "HTML5",
        "DHTML",
        "XSLXSLT",
        "XHTML",
        "DOM",
        "CSS3",
        "JSON",
        "JavaScript",
        "JQuery",
        "Ajax",
        "Adobe",
        "Creative",
        "suite",
        "billing",
        "pages",
        "HTML",
        "CSS",
        "Angularjs",
        "framework",
        "Performed",
        "form",
        "validations",
        "forms",
        "framework",
        "Integrated",
        "Java",
        "code",
        "API",
        "JSP",
        "pages",
        "framework",
        "UI",
        "development",
        "Developed",
        "html",
        "HTML5",
        "CSS3",
        "bootstrap",
        "code",
        "standards",
        "Controller",
        "scopes",
        "operations",
        "Application",
        "AngularJS",
        "NodeJS",
        "libraries",
        "NPM",
        "dependencies",
        "gulp",
        "code",
        "web",
        "pages",
        "Angular",
        "page",
        "web",
        "applications",
        "Nodejs",
        "backbonejs",
        "Requirejs",
        "MVC",
        "Frameworks",
        "development",
        "web",
        "applications",
        "level",
        "HTML5",
        "JavaScript",
        "CSS3",
        "CSS",
        "layouts",
        "table",
        "content",
        "pages",
        "CSS3",
        "layout",
        "style",
        "presentations",
        "JavaScript",
        "methods",
        "Restful",
        "Services",
        "http",
        "Service",
        "Developed",
        "Restful",
        "services",
        "WEB",
        "API",
        "AJAX",
        "techniques",
        "parts",
        "web",
        "page",
        "Client",
        "side",
        "validation",
        "Java",
        "Script",
        "JQuery",
        "server",
        "side",
        "Server",
        "side",
        "validations",
        "Angularjs",
        "backend",
        "client",
        "web",
        "application",
        "server",
        "side",
        "programming",
        "JDBC",
        "Servlets",
        "JSP",
        "Frontend",
        "object",
        "JavaScript",
        "Framework",
        "BackboneJS",
        "AngularJS",
        "ExtJS",
        "Angularjs",
        "server",
        "side",
        "applications",
        "connectors",
        "libraries",
        "HTTP",
        "SSL",
        "compression",
        "file",
        "system",
        "access",
        "features",
        "application",
        "functionality",
        "CRUD",
        "Create",
        "features",
        "Backbonejs",
        "Requirejs",
        "Responsive",
        "Design",
        "Designed",
        "Developed",
        "Java",
        "Script",
        "frame",
        "work",
        "wrapper",
        "top",
        "JQUERY",
        "frame",
        "work",
        "AJAX",
        "UI",
        "frame",
        "work",
        "UI",
        "Configuration",
        "widgets",
        "Mobile",
        "development",
        "team",
        "website",
        "AJAX",
        "functionality",
        "XML",
        "HTML",
        "JavaScript",
        "client",
        "side",
        "presentation",
        "data",
        "validation",
        "client",
        "side",
        "forms",
        "site",
        "CSS",
        "styles",
        "look",
        "browsers",
        "pages",
        "team",
        "architects",
        "Developers",
        "requirements",
        "application",
        "functionality",
        "features",
        "status",
        "meetings",
        "status",
        "management",
        "team",
        "Environment",
        "SOAP",
        "Restful",
        "web",
        "services",
        "JavaScript",
        "Angular",
        "JS",
        "SQL",
        "MySQL",
        "JQuery",
        "Apache",
        "web",
        "server",
        "PHP",
        "SQL",
        "Developer",
        "tool",
        "JDBC",
        "CSS3",
        "HTML5",
        "Education",
        "Bachelors",
        "Skills",
        "APACHE",
        "HADOOP",
        "HDFS",
        "years",
        "APACHE",
        "HADOOP",
        "SQOOP",
        "years",
        "Hadoop",
        "years",
        "HADOOP",
        "years",
        "HADOOP",
        "FILE",
        "SYSTEM",
        "years",
        "Information",
        "TECHNICAL",
        "SKILLS",
        "Big",
        "Data",
        "Ecosystem",
        "Hadoop",
        "Spark",
        "Scala",
        "Map",
        "HDFS",
        "Hive",
        "Pig",
        "Sqoop",
        "Flume",
        "Kafka",
        "HBase",
        "Java",
        "Technologies",
        "JSP",
        "Servlets",
        "Junit",
        "Spring",
        "Hibernate",
        "Database",
        "Technologies",
        "MySQL",
        "SQL",
        "server",
        "Oracle",
        "MS",
        "Access",
        "Programming",
        "Languages",
        "Scala",
        "Python",
        "Java",
        "Linux",
        "shell",
        "scripting",
        "Operating",
        "Systems",
        "Windows",
        "LINUX"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T23:15:27.321021",
    "resume_data": "Big Data Engineer Big Data Engineer Big Data Engineer ATT Atlanta GA 9 years of experience in IT industry with extensive experience in Java J2ee and Big data technologies 4 years working of exclusive experience on Big Data technologies and Hadoop stack Strong experience working with HDFS MapReduce Spark Hive Pig Sqoop Flume Kafka Oozie and HBase Good understanding of distributed systems HDFS architecture internal working details of MapReduce and Spark processing frameworks More than two years of hands on experience using Spark framework with Scala Good exposure to performance tuning hive queries mapreduce jobs spark jobs Expertise in Inbound and Outbound importingexporting data formto traditional RDBMS using SQOOP Tuned PIG and HIVE scripts by understanding the joins group and aggregation between them Extensively worked on HiveQL join operations writing custom UDFs and having good experience in optimizing Hive Queries Worked on various Hadoop Distributions Cloudera Hortonworks Amazon AWS to implement and make use of those Participated in design development and system migration of high performance metadata driven data pipeline with Kafka and HivePresto on Qubole providing data export capability through API and UI Experience data processing like collecting aggregating moving from various sources using Apache Flume and Kafka Hands on experience in NOSQL databases and SQL databases Authorized to work in the US for any employer Work Experience Big Data Engineer ATT Atlanta GA July 2016 to Present Responsibilities Extensively involved in Design phase and delivered Design documents in Hadoop eco system with HDFS HIVE PIG SQOOP and SPARK with SCALA Collected the logs from the physical machines and the Open Stack controller and integrated into HDFS using Kafka Involved in the highlevel design of the Hadoop architecture for the existing data structure and Business process Part of Configuring deployment of Hadoop Cluster in the AWS cloud Worked with clients to better understand their reporting and dash boarding needs and present solutions using structured Agile project methodology approach Worked on analyzing Hadoop cluster and different Big Data Components including Pig Hive Spark HBase Kafka Elastic Search database and SQOOP Involved in loading disparate datasets into Hadoop Data Lake this would be available to the data science team to predict the future Implemented Partitioning Dynamic Partitions and Buckets in HIVE for increasing performance benefit and helping in organizing data in a logical fashion Installed Hadoop Map Reduce HDFS and developed multiple MapReduce jobs in PIG and Hive for data cleaning and preprocessing Created tables in HBase to store variable data formats of PII data coming from different portfolios Worked on Sequence files RC files Map side joins bucketing partitioning forHive performance enhancement and storage improvement Experienced in pulling the data from Amazon S3 bucket to Data Lake and builtHive tables on top of it and created data frames in Spark to perform further analysis Used cloud computing on the multinode cluster and deployed Hadoop application on cloud S3 and used Elastic Map Reduce EMR to run aMapReduce Explored MLlib algorithms in Spark to understand the possible Machine Learning functionalities that can be used for use case In preprocessing phase of data extraction we used Spark to remove all the missing data for transforming of data to create new features Developed data pipeline using Flume Sqoop Pig and Java map reduce to ingest customer behavioral data and financial histories into HDFS for analysis Involved in loading data from UNIX file system to HDFS using Flume and HDFSAPI Configured Spark Streaming to receive real time data from the Kafka and storethe stream data to HDFS Developed RDDsData Frames in Spark using Scala and Python and applied several transformation logics to load data from Hadoop Data Lake to Cassandra DB Exported the analyzed data to the NoSQL Database using HBase for visualization and to generate reports for the Business Intelligence team using SAS Used various HBase commands and generated different Datasets as per requirements and provided access to the data when required using grant and Revoke Created Hive tables as per requirement as internal or external tables intended for efficiency Developed MapReduce programs for the files generated by hive query processing to generate key value pairs and upload the data to NoSQL database HBase Implemented installation and configuration of multinode cluster on the cloud using Amazon Web Services AWS on EC2 Created and maintained Technical documentation for launching Hadoop Clusters and for executing Hive queries and Pig Scripts Worked in tuning Hive Pig to improve performance and solved performance issues in both scripts Worked with Elastic MapReduce EMR and setting up environments on Amazon AWS EC2 instances Developed various data connections from data source to SSIS Tableau Server for report and dashboard development Involved unit testing interface testing system testing and user acceptance testing of the workflow tool Used JIRA for bug tracking and GIT for version control Involved in storydriven agile development methodology and actively participated in daily scrum meetings Environment Apache Hadoop 30 AWS MLlib MYSQL Kafka HDFS 12 Hive 23 Pig017 MapReduce Flume 18 Cloudera Oozie UNIX Oracle 12c Tableau 7 GIT UNIX Developer Big Data January 2015 to June 2016 Confidential NJ Responsibilities Involved in complete Big Data flow of the application data ingestion from upstream to HDFS processing the data in HDFS and analyzing the data using several tools Imported the data from various formats like JSON Sequential Text CSV AVRO and Parquet to HDFS cluster with compressed for optimization Experienced on ingesting data from RDBMS sources like Oracle SQL Server and Teradata into HDFS using Sqoop Configured Hive and written Hive UDFs and UDAFs Also created partitions such as Static and Dynamic with bucketing Implemented advanced procedures like text analytics and processing using the inmemory computing capabilities like spark Importing and exporting data into HDFS and hive using Sqoop and Kafka with batch and streaming Experienced with SparkStreaming APIs to perform transformations and actions on the fly for building the common learner data model which gets the data from Kafka in near real time and Persists into HBase Performance analysis of Spark streaming and batch jobs by using Spark tuning parameters Enhanced and optimized product Spark code to aggregate group and run data mining tasks using the Spark framework Developed Spark scripts by using Python shell commands as per the requirement Using Hive join queries to join multiple tables of a source system and load them to Elastic search tables Experience in managing and reviewing huge Hadoop log files Expertise in designing and creating various analytical reports and Automated Dashboards to help users to identify critical KPIs and facilitate strategic planning in the organization Involved in Cluster maintenance Cluster Monitoring and Troubleshooting Created Data Pipelines as per the business requirements and scheduled it using Oozie Coordinators Maintaining technical documentation for each and every step of development environment and launching Hadoop clusters Worked on BI tools as Tableau to create dashboards like weekly monthly daily reports using tableau desktop and publish them to HDFS cluster Environment Scala Hadoop HDFS Hive Oozie SqoopKafka Elastic Search Shell Scripting HBase Python GitHub Tableau Oracle MySQL Teradata and AWS Hadoop Developer Nationwide Mutual Insurance company Columbus OH June 2012 to November 2014 Responsibilities Experience on AWSEMR Spark installation HDFS and MapReduce Architecture Participated in Hadoop Deployment and infrastructure scaling Involved in creating Hive tables and loading and analyzing data using hive queries Developed Simple to complex Map Reduce Jobs using Hive and Pig Developed workflow in Oozie to automate the tasks of loading the data into HDFS and preprocessing with Pig Parsed highlevel design spec to simple ETL coding and mapping standards Maintained warehouse metadata naming standards and warehouse standards for future application development Worked with Linux systems and RDBMS database on a regular basis to ingest data using Sqoop Implemented Kafka consumers to move data from Kafka partitions into Cassandra for near realtime analysis Involved in Hadoop cluster task like adding and removing nodes Managed and reviewed Hadoop log files and loaded log data into HDFS using Sqoop Created and maintained Technical documentation for launching HADOOP Clusters and for executing Hive queries Pig Scripts Sqoop jobs Environment HadoopHortonworks stack HDFS Oozie Pig Hive MapReduce Sqoop Cassandra Linux Hadoop Developer US Bank Denver CO November 2010 to May 2012 Responsibilities Worked on analyzing writing HadoopMapReduce jobs using JavaAPI Pig and Hive Responsible for building scalable distributed data solutions using Hadoop Involved in loading data from edge node to HDFS using shell scripting Created HBase tables to store variable data formats of PII data coming from different portfolios Exported the analyzed data to the relational databases using Sqoop for visualization and to generate reports for the BI team Worked with using different kind of compression techniques to save data and optimize data transfer over network using LZO Snappy and Bzip etc Analyze large and critical datasets using Cloudera HDFS HBase MapReduce Hive HiveUDF Pig Sqoop Zookeeper Spark Developed custom aggregate functions using SparkSQL and performed interactive querying Used Scoop to store the data into HBase and Hive Worked on installing cluster commissioning decommissioning of DataNode NameNode high availability capacity planning and slots configuration Creating Hive tables dynamic partitions buckets for sampling and working on them using HiveQL Used Pig to parse the data and Store in Avro format Stored the data in tabular formats using Hive tables and Hive Serdes Collecting and aggregating large amounts of log data using Apache Flume and staging data in HDFS for further analysis Worked with NoSQL databases like HBase for creating HBase tables to load large sets of semi structured data coming from various sources Implemented a script to transmit information from Oracle to HBase using Sqoop Implemented MapReduce programs to handle semiunstructured data like XML JSON and sequence files for log files Finetuned Pig queries for better performance Involved in writing the shell scripts for exporting log files to Hadoop cluster through automated process Installed Oozie workflow engine to run multiple Hive and pig jobs Analyzed large amounts of data sets to determine optimal way to aggregate and report on it Environment Hadoop Map Reduce HDFS Yarn Sqoop Oozie Pig Hive HBase Java Eclipse UNIX shell scripting python Horton works Java Developer GE Healthcare Richmond TX January 2010 to October 2010 Responsibilities Effectively interacted with team members and business users for requirements gathering Involved in analysis design and implementation phases of the software development lifecycle SDLC Implementation of spring core J2EE patterns like MVC Dependency Injection DI and Inversion of Control IOC Implemented REST Web Services with Jersey API to deal with customer requests Developed test cases using J Unit and used Log4j as the logging framework Worked with HQL and Criteria API from retrieving the data elements from database Developed user interface using HTML Spring Tags JavaScript JQuery and CSS Developed the application using Eclipse IDE and worked under Agile Environment Design and implementation of front end web pages using CSS JSP HTML java Script Ajax and Struts Utilized Eclipse IDE as improvement environment to plan create and convey Spring segments on Web Logic Environment Java J2EE HTML JavaScript CSS J Query Spring 30 JNDI Hibernate 30 Java Mail Web Services REST Oracle 10g JUnit Log4j Eclipse Web logic 103 Java Developer BMO Harris Bank Milwaukee WI June 2008 to December 2009 Responsibilities Responsible for Daily maintenance and improvement of live website using HTML5 CSS3 JavaScript JQuery Involved in Software Development Life Cycle phases like requirements gathering Analysis Design Development and Testing Participated in requirement gathering and worked closely with the architect in designing and modeling Worked on development of SQL and stored procedures trigger and function on MYSQL Developed all the UI pages using HTML5 DHTML XSLXSLT XHTML DOM CSS3 JSON JavaScript JQuery Ajax Adobe Creative suite Updated billing pages using HTML CSS in Angularjs framework Performed form validations using reactive forms from Angular framework Integrated the Java code API in JSP pages and responsible for setting up AngularJS framework for UI development Developed html views with HTML5 CSS3 bootstrap and AngularJS Implemented code according to coding standards and Created AngularJS Controller which isolate scopes perform operations Created the Application using AngularJS and NodeJS libraries and used NPM to manage dependencies and gulp to minify rectify and babelify the code Involved in developing the web pages using Angular which are powerful in building the Single page web applications Used the Nodejs backbonejs and Requirejs MVC Frameworks in the development of the web applications Used advanced level of HTML5 JavaScript CSS3 and pure CSS layouts table less layout Produced content pages with CSS3 layout and style markup presentations and also used JavaScript methods Involved in Consuming Restful Services using Angular http Service Developed and consumed Restful services using WEB API Used AJAX coding techniques to update parts of a web page Developed the Client side validation using Java Script and JQuery and server side using Server side validations Built efficient Angularjs backend for client web application Also participated in server side programming with java using JDBC Servlets and JSP Designed Frontend with in object oriented JavaScript Framework like BackboneJS AngularJS and ExtJS Used Angularjs to create server side applications Used its workhorse connectors and libraries relating to HTTP SSL compression file system access etc Developed certain features of the application functionality ie CRUD Create read update delete features using Backbonejs Requirejs and Responsive Design Designed and Developed Java Script frame work which is wrapper on top of JQUERY frame work and AJAX based UI frame work for UI Configuration widgets Involved with Mobile development team to make mobile website responsive and fast added AJAX functionality Involved in developing XML HTML and JavaScript for client side presentation and data validation on the client side with in the forms Involved in redesigning the entire site with CSS styles for consistent look and feel across all browsers and all pages Worked with the team of architects and backend Developers to gather requirements and enhance the application functionality and add new features Participated in the status meetings and status updating to the management team Environment SOAP Restful web services JavaScript Angular JS SQL MySQL JQuery Apache web server PHP SQL Developer tool JDBC CSS3 HTML5 Education Bachelors Skills APACHE HADOOP HDFS 7 years APACHE HADOOP SQOOP 7 years Hadoop 7 years HADOOP 7 years HADOOP DISTRIBUTED FILE SYSTEM 7 years Additional Information TECHNICAL SKILLS Big Data Ecosystem Hadoop Spark Scala Map Reduce HDFS Hive Pig Sqoop Flume Kafka HBase Java Technologies JSP Servlets Junit Spring Hibernate Database Technologies MySQL SQL server Oracle MS Access Programming Languages Scala Python Java and Linux shell scripting Operating Systems Windows LINUX",
    "unique_id": "974eb5f3-c4d5-4a60-9a89-a86dccaf9946"
}