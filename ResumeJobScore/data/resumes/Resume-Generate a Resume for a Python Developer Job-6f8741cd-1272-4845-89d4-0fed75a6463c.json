{
    "clean_data": "Hadoopspark Developer Hadoopspark span lDeveloperspan Hadoopspark Developer BCBSA Chicago IL Over 4 years of IT experience including Big Data technologies Web Application development and Business Intelligence Experience in deploying and managing a multinode Cloudera Hadoop cluster with different components MFS NFS CLDB Web server Spark Resource Manager Node Manager Hive HBase Zookeeper History Server Exposure to Spark Spark Streaming Scala and Implementing Spark using Scala and utilizing Data frames and Spark SQL API Data Frames and Pair RDDs for faster processing of data Experienced with the Scala Spark for improving the performance and optimization of the existing algorithms in Hadoop using Spark Context  Pair RDDs Spark YARN Extensive experience in working with various distributions of Hadoop Enterprise versions of Cloudera good knowledge on Amazons EMR Elastic MapReduce Used AWS S3 and Local Hard Disk as underlying File System HDFS for Hadoop Experience in deploying scalable Hadoop cluster on AWS using S3 as underlying file system for Hadoop Hands on experience in writing Python scripts Experience in converting SQL queries into Spark Transformations using Spark RDDs Scala and Performed mapside joins on RDDs Extensive Experience on importing and exporting data using stream processing platforms like Flume and Kafka Hands on experience in configuring and working with Flume to load the data from multiple sources directly into HDFS Good knowledge in job workflow scheduling and monitoring tools like Oozie Zookeeper Good working experience using Sqoop to import data into HDFS from RDBMS and viceversa Experience with distributed systems largescale nonrelational data stores MapReduce systems data modeling and big data systems Experienced in loading data to hive partitions and creating buckets in Hive Experience in handling messaging services using Apache Kafka Experience in finetuning MapReduce jobs for better scalability and performance Developed various Map Reduce applications to perform ETL workloads on terabytes of data Experience working with Cloudera Hortonworks Distribution of Hadoop Highly motivated team player with zeal to learn new technologies Experience in all Phases of Software Development Life Cycle Analysis Design Development Testing and Maintenance using Waterfall and Agile methodologies Authorized to work in the US for any employer Work Experience Hadoopspark Developer BCBSA Chicago IL January 2019 to Present Project Description This project is based on web services and FHIR response to get the prior member coverage for members when provided with member id plan id and product id It utilizes the Hadoop cluster for data storage and uses Responsibilities Work closely with the business and analytics team in gathering the system requirements Data ingestion into HDFS from various mainframe Db2 table using Sqoop Skilled on migrating the data from different databases to Hadoop HDFS and Hive using Sqoop Analyzed large structured datasets using Hives data warehousing infrastructure Extensive Knowledge of creating manages tables and external tables in Eco system Implement Hive UDFs for evaluation filtering loading and storing of data Involved in writing the Hive scripts to reduce the job execution time Importing exporting historical data from Db2 to HDFS Hive Monitoring Hadoop jobs on performance and productions cluster Provided Production support for few successful runs Design managed and External tables in Hive to optimize performance to improve performance Using API interface HUE to query data and managed tables Installed Oozie workflow engine and scheduled it to run datatime dependent Hive Involved in Agile methodologies daily Scrum meetings Sprint planning Environment HDFS Hive Sqoop ImportExport Spark Hue Oozie ETL Datawarehouse UNIX Cloudera spark Developer Chicago IL December 2017 to November 2018 Project Description all scripts are specialized in designing and developing the highperformance production software with stateof art computer vision capabilities The increase of data made the existing databases un accommodable So all scripts Leap wants to move it to Hadoop where exactly we can handle massive amount of data by means of its cluster nodes and to satisfy the scaling needs of the business operation Responsibilities Extracted the data from the flat files and other RDBMS databases into staging area and ingested to Hadoop Installed and configured Hadoop MapReduce HDFS and developed multiple MapReduce jobs in Java for data cleansing and preprocessing Importing and exporting data into HDFS and Hive using Sqoop Responsible for Coding batch pipelines Restful Service Map Reduce program Hive querys testing debugging Peer code review troubleshooting and maintain status report Implemented Map Reduce programs to classified data organizations into different classifieds based on different type of records Implemented complex map reduce programs to perform joins on the Map side using Distributed Cache in Java Wrote Flume configuration files for importing streaming log data into HBase with Flume Performed masking on customer sensitive data using Flume interceptors Involved in migrating tables from RDBMS into Hive tables using SQOOP and later generate visualizations using Tableau Involved in creating Hive tables loading with data and writing hive queries which will run internally in map reduce way Installed Oozie workflow engine and scheduled it to run datatime dependent Hive and Pig jobs Involved in Agile methodologies daily Scrum meetings Sprint planning Environment HDFS MapReduce Cassandra Hive Pig Sqoop Tableau NoSQL Shell Scripting Maven Git HDP Distribution Eclipse Log4j JUnit Linux Hadoop Developer Waukegan IL December 2016 to November 2017 Project Description AbbVie is a global researchdriven biopharmaceutical company committed to developing innovative advanced therapies for some of the worlds most complex and critical conditions The companys mission is to use its expertise to markedly improve treatments across four primary therapeutic areas immunology oncology virology and neuroscience The project aims at importing supply chain data from SAP source onto HDFS Also providing and supporting a self service portal for business users for their daytoday operations analysis and reporting Responsibilities Installed and configured Hadoop YARN MapReduce Flume HDFS Hadoop Distributed File System developed multiple MapReduce jobs in Python for data cleaning Developed data pipeline using Flume Sqoop Pig and Python MapReduce to ingest customer behavioral data and financial histories into HDFS for analysis Extensive experience in working with various distributions of Hadoop Enterprise versions of Cloudera good knowledge on Amazons EMR Elastic MapReduce Used AWS S3 and Local Hard Disk as underlying File System HDFS for Hadoop Experience in deploying scalable Hadoop cluster on AWS using S3 as underlying file system for Hadoop Developed Python scripts to extract the data from the web server output files to load into HDFS Involved in HBASE setup and storing data into HBASE which will be used for further analysis Used Pig as ETL tool to do transformations event joins and some preaggregations before storing the data onto HDFS Wrote Python MapReduce scripts for processing the unstructured data Load log data into HDFS using Flume Worked extensively in creating MapReduce jobs to power data for search and aggregation Worked on Hive for exposing data for further analysis and for generating transforming files from different analytical formats to text files Responsible for creating Hive tables loading data and writing hive queries Used forward engineering to create a Physical Data Model with DDL that best suits the requirements Worked with Sqoop to export analyzed data from HDFS environment into RDBMS for report generation and visualization purpose Implemented Fair schedulers on the Job tracker to share the resources of the Cluster for the Map Reduce jobs given by the users Maintaining and monitoring clusters Loaded data into the cluster from dynamically generated files using Flume and from relational database management systems using Sqoop Environment Cloudera Cloudera Manager HDFS Map Reduce Hive Impala Pig Python SQL Sqoop Flume Yarn Linux Centos HBase Java Developer NebuLogic Technologies Hyderabad Telangana January 2014 to December 2015 Project Description Nebulogic is a provider of business outsourcing solutions They provide Services such as Human Capital Management Tax Compliance Electronic Payment Solutions Dealer Services and Medical Practice Services This project is focused on Human Capital Management and particularly on Payroll Services Human Resource Management Talent Management Benefits Administration Time Attendance and International Solutions This frontend application has interface with various Java applications which are used to send and receive information to third party vendors Responsibilities Involved in Full Life Cycle Development in Distributed Environment Using Java and J2EE framework Responsible for developing and modifying the existing service layer based on the business requirements Involved in designing developing webservices using SOAP and WSDL Involved in database design Created tables views triggers stored procedures in SQL for data manipulation and retrieval Developed Web Services for Payment Transaction and Payment Release Involved in Requirement Analysis Development and Documentation Developed frontend using JSP HTML CSS and JavaScript Coding for DAO Objects using JDBC using DAO pattern XML and XSDs are used to define data formats Implemented J2EE design patterns such as singleton DAO for the presentation tier business tier and Integration Tier layers of the project Involved in Bug fixing and functionality enhancements Followed coding and documentation standards and best practices Participated in project planning discussions and worked with team members to analyze the requirements and translate them into working software modules Environment Java J2EE JSP SOAP WSDL SQL PLSQL XML JDBC Eclipse Windows XP Oracle Education Master of Science in Computer Science in Computer Science Virginia international university 2018 Skills APACHE HADOOP HDFS 2 years database 2 years Eclipse 2 years Java 2 years SQL 2 years CertificationsLicenses Drivers License Additional Information TECHNICAL SKILLS HadoopMapR Ecosystem Hive HBase MFS HDFS MapReduce YARN Spark MapR Control System MCS Sqoop Pig Cloudera Manager Zookeeper Tools Apache Tomcat 70 Maven Git Hibernate Microsoft SQL Server Management Studio SQL Developer MySQL Workbench Eclipse Languages Java 78 Scala C CC JavaScript ABAP4 Ruby HTML5 CSS3 Databases Oracle 10g11g12c MySQL Microsoft SQL Server 20052008 Frameworks Spring MVC Spring Boot Hibernate",
    "entities": [
        "Business Intelligence Experience",
        "Hadoop Installed",
        "Installed Oozie",
        "JDBC",
        "Spark Transformations",
        "Human Capital Management Tax Compliance Electronic Payment Solutions Dealer Services",
        "Implemented Fair",
        "Microsoft SQL Server Management Studio SQL Developer",
        "ETL",
        "API",
        "Developed",
        "the Cluster for the Map Reduce",
        "DAO",
        "US",
        "Sqoop",
        "DAO Objects",
        "Performed",
        "Implementing Spark",
        "HDFS",
        "Git Hibernate",
        "AWS",
        "Sqoop Analyzed",
        "Hadoop MapReduce HDFS",
        "Responsibilities Installed",
        "Medical Practice Services",
        "Human Capital Management",
        "Waterfall",
        "Responsibilities Involved",
        "DDL",
        "Spark SQL API Data Frames",
        "Microsoft",
        "Tableau Involved",
        "Work Experience Hadoopspark Developer BCBSA",
        "the Scala Spark",
        "Payroll Services Human Resource Management Talent Management Benefits Administration Time Attendance and International Solutions",
        "CertificationsLicenses Drivers License Additional Information TECHNICAL SKILLS HadoopMapR Ecosystem Hive HBase",
        "Hadoop Enterprise",
        "Flume Sqoop Pig",
        "Flume Performed",
        "CSS",
        "SAP",
        "FHIR",
        "Hadoopspark Developer Hadoopspark",
        "JSP",
        "Control System",
        "SQL",
        "Hive Experience",
        "Hadoop",
        "External",
        "Hive Involved",
        "HDFS Involved",
        "Spark Context  Pair RDDs Spark YARN Extensive",
        "Developer NebuLogic Technologies Hyderabad",
        "SOAP",
        "MapReduce",
        "XML",
        "Hadoop Hands",
        "Cloudera Hortonworks Distribution of Hadoop Highly",
        "Science in Computer Science in Computer Science Virginia",
        "Maintaining",
        "Project Description",
        "MFS",
        "Physical Data Model",
        "Chicago",
        "Spark Resource",
        "HBase",
        "Developed Web Services for Payment Transaction",
        "CC",
        "Boot Hibernate",
        "Big Data",
        "Hive",
        "SQOOP",
        "Present Project Description",
        "Spark"
    ],
    "experience": "Experience in deploying and managing a multinode Cloudera Hadoop cluster with different components MFS NFS CLDB Web server Spark Resource Manager Node Manager Hive HBase Zookeeper History Server Exposure to Spark Spark Streaming Scala and Implementing Spark using Scala and utilizing Data frames and Spark SQL API Data Frames and Pair RDDs for faster processing of data Experienced with the Scala Spark for improving the performance and optimization of the existing algorithms in Hadoop using Spark Context   Pair RDDs Spark YARN Extensive experience in working with various distributions of Hadoop Enterprise versions of Cloudera good knowledge on Amazons EMR Elastic MapReduce Used AWS S3 and Local Hard Disk as underlying File System HDFS for Hadoop Experience in deploying scalable Hadoop cluster on AWS using S3 as underlying file system for Hadoop Hands on experience in writing Python scripts Experience in converting SQL queries into Spark Transformations using Spark RDDs Scala and Performed mapside joins on RDDs Extensive Experience on importing and exporting data using stream processing platforms like Flume and Kafka Hands on experience in configuring and working with Flume to load the data from multiple sources directly into HDFS Good knowledge in job workflow scheduling and monitoring tools like Oozie Zookeeper Good working experience using Sqoop to import data into HDFS from RDBMS and viceversa Experience with distributed systems largescale nonrelational data stores MapReduce systems data modeling and big data systems Experienced in loading data to hive partitions and creating buckets in Hive Experience in handling messaging services using Apache Kafka Experience in finetuning MapReduce jobs for better scalability and performance Developed various Map Reduce applications to perform ETL workloads on terabytes of data Experience working with Cloudera Hortonworks Distribution of Hadoop Highly motivated team player with zeal to learn new technologies Experience in all Phases of Software Development Life Cycle Analysis Design Development Testing and Maintenance using Waterfall and Agile methodologies Authorized to work in the US for any employer Work Experience Hadoopspark Developer BCBSA Chicago IL January 2019 to Present Project Description This project is based on web services and FHIR response to get the prior member coverage for members when provided with member i d plan i d and product i d It utilizes the Hadoop cluster for data storage and uses Responsibilities Work closely with the business and analytics team in gathering the system requirements Data ingestion into HDFS from various mainframe Db2 table using Sqoop Skilled on migrating the data from different databases to Hadoop HDFS and Hive using Sqoop Analyzed large structured datasets using Hives data warehousing infrastructure Extensive Knowledge of creating manages tables and external tables in Eco system Implement Hive UDFs for evaluation filtering loading and storing of data Involved in writing the Hive scripts to reduce the job execution time Importing exporting historical data from Db2 to HDFS Hive Monitoring Hadoop jobs on performance and productions cluster Provided Production support for few successful runs Design managed and External tables in Hive to optimize performance to improve performance Using API interface HUE to query data and managed tables Installed Oozie workflow engine and scheduled it to run datatime dependent Hive Involved in Agile methodologies daily Scrum meetings Sprint planning Environment HDFS Hive Sqoop ImportExport Spark Hue Oozie ETL Datawarehouse UNIX Cloudera spark Developer Chicago IL December 2017 to November 2018 Project Description all scripts are specialized in designing and developing the highperformance production software with stateof art computer vision capabilities The increase of data made the existing databases un accommodable So all scripts Leap wants to move it to Hadoop where exactly we can handle massive amount of data by means of its cluster nodes and to satisfy the scaling needs of the business operation Responsibilities Extracted the data from the flat files and other RDBMS databases into staging area and ingested to Hadoop Installed and configured Hadoop MapReduce HDFS and developed multiple MapReduce jobs in Java for data cleansing and preprocessing Importing and exporting data into HDFS and Hive using Sqoop Responsible for Coding batch pipelines Restful Service Map Reduce program Hive querys testing debugging Peer code review troubleshooting and maintain status report Implemented Map Reduce programs to classified data organizations into different classifieds based on different type of records Implemented complex map reduce programs to perform joins on the Map side using Distributed Cache in Java Wrote Flume configuration files for importing streaming log data into HBase with Flume Performed masking on customer sensitive data using Flume interceptors Involved in migrating tables from RDBMS into Hive tables using SQOOP and later generate visualizations using Tableau Involved in creating Hive tables loading with data and writing hive queries which will run internally in map reduce way Installed Oozie workflow engine and scheduled it to run datatime dependent Hive and Pig jobs Involved in Agile methodologies daily Scrum meetings Sprint planning Environment HDFS MapReduce Cassandra Hive Pig Sqoop Tableau NoSQL Shell Scripting Maven Git HDP Distribution Eclipse Log4j JUnit Linux Hadoop Developer Waukegan IL December 2016 to November 2017 Project Description AbbVie is a global researchdriven biopharmaceutical company committed to developing innovative advanced therapies for some of the worlds most complex and critical conditions The companys mission is to use its expertise to markedly improve treatments across four primary therapeutic areas immunology oncology virology and neuroscience The project aims at importing supply chain data from SAP source onto HDFS Also providing and supporting a self service portal for business users for their daytoday operations analysis and reporting Responsibilities Installed and configured Hadoop YARN MapReduce Flume HDFS Hadoop Distributed File System developed multiple MapReduce jobs in Python for data cleaning Developed data pipeline using Flume Sqoop Pig and Python MapReduce to ingest customer behavioral data and financial histories into HDFS for analysis Extensive experience in working with various distributions of Hadoop Enterprise versions of Cloudera good knowledge on Amazons EMR Elastic MapReduce Used AWS S3 and Local Hard Disk as underlying File System HDFS for Hadoop Experience in deploying scalable Hadoop cluster on AWS using S3 as underlying file system for Hadoop Developed Python scripts to extract the data from the web server output files to load into HDFS Involved in HBASE setup and storing data into HBASE which will be used for further analysis Used Pig as ETL tool to do transformations event joins and some preaggregations before storing the data onto HDFS Wrote Python MapReduce scripts for processing the unstructured data Load log data into HDFS using Flume Worked extensively in creating MapReduce jobs to power data for search and aggregation Worked on Hive for exposing data for further analysis and for generating transforming files from different analytical formats to text files Responsible for creating Hive tables loading data and writing hive queries Used forward engineering to create a Physical Data Model with DDL that best suits the requirements Worked with Sqoop to export analyzed data from HDFS environment into RDBMS for report generation and visualization purpose Implemented Fair schedulers on the Job tracker to share the resources of the Cluster for the Map Reduce jobs given by the users Maintaining and monitoring clusters Loaded data into the cluster from dynamically generated files using Flume and from relational database management systems using Sqoop Environment Cloudera Cloudera Manager HDFS Map Reduce Hive Impala Pig Python SQL Sqoop Flume Yarn Linux Centos HBase Java Developer NebuLogic Technologies Hyderabad Telangana January 2014 to December 2015 Project Description Nebulogic is a provider of business outsourcing solutions They provide Services such as Human Capital Management Tax Compliance Electronic Payment Solutions Dealer Services and Medical Practice Services This project is focused on Human Capital Management and particularly on Payroll Services Human Resource Management Talent Management Benefits Administration Time Attendance and International Solutions This frontend application has interface with various Java applications which are used to send and receive information to third party vendors Responsibilities Involved in Full Life Cycle Development in Distributed Environment Using Java and J2EE framework Responsible for developing and modifying the existing service layer based on the business requirements Involved in designing developing webservices using SOAP and WSDL Involved in database design Created tables views triggers stored procedures in SQL for data manipulation and retrieval Developed Web Services for Payment Transaction and Payment Release Involved in Requirement Analysis Development and Documentation Developed frontend using JSP HTML CSS and JavaScript Coding for DAO Objects using JDBC using DAO pattern XML and XSDs are used to define data formats Implemented J2EE design patterns such as singleton DAO for the presentation tier business tier and Integration Tier layers of the project Involved in Bug fixing and functionality enhancements Followed coding and documentation standards and best practices Participated in project planning discussions and worked with team members to analyze the requirements and translate them into working software modules Environment Java J2EE JSP SOAP WSDL SQL PLSQL XML JDBC Eclipse Windows XP Oracle Education Master of Science in Computer Science in Computer Science Virginia international university 2018 Skills APACHE HADOOP HDFS 2 years database 2 years Eclipse 2 years Java 2 years SQL 2 years CertificationsLicenses Drivers License Additional Information TECHNICAL SKILLS HadoopMapR Ecosystem Hive HBase MFS HDFS MapReduce YARN Spark MapR Control System MCS Sqoop Pig Cloudera Manager Zookeeper Tools Apache Tomcat 70 Maven Git Hibernate Microsoft SQL Server Management Studio SQL Developer MySQL Workbench Eclipse Languages Java 78 Scala C CC JavaScript ABAP4 Ruby HTML5 CSS3 Databases Oracle 10g11g12c MySQL Microsoft SQL Server 20052008 Frameworks Spring MVC Spring Boot Hibernate",
    "extracted_keywords": [
        "Hadoopspark",
        "Developer",
        "Hadoopspark",
        "span",
        "lDeveloperspan",
        "Hadoopspark",
        "Developer",
        "BCBSA",
        "Chicago",
        "IL",
        "years",
        "IT",
        "experience",
        "Big",
        "Data",
        "technologies",
        "Web",
        "Application",
        "development",
        "Business",
        "Intelligence",
        "Experience",
        "multinode",
        "Cloudera",
        "Hadoop",
        "cluster",
        "components",
        "MFS",
        "NFS",
        "CLDB",
        "Web",
        "server",
        "Spark",
        "Resource",
        "Manager",
        "Node",
        "Manager",
        "Hive",
        "HBase",
        "Zookeeper",
        "History",
        "Server",
        "Exposure",
        "Spark",
        "Spark",
        "Streaming",
        "Scala",
        "Implementing",
        "Spark",
        "Scala",
        "Data",
        "frames",
        "Spark",
        "SQL",
        "API",
        "Data",
        "Frames",
        "Pair",
        "RDDs",
        "processing",
        "data",
        "Scala",
        "Spark",
        "performance",
        "optimization",
        "algorithms",
        "Hadoop",
        "Spark",
        "Context",
        "Pair",
        "RDDs",
        "Spark",
        "YARN",
        "experience",
        "distributions",
        "Hadoop",
        "Enterprise",
        "versions",
        "Cloudera",
        "knowledge",
        "Amazons",
        "EMR",
        "Elastic",
        "MapReduce",
        "AWS",
        "S3",
        "Local",
        "Hard",
        "Disk",
        "File",
        "System",
        "HDFS",
        "Hadoop",
        "Experience",
        "Hadoop",
        "cluster",
        "AWS",
        "S3",
        "file",
        "system",
        "Hadoop",
        "Hands",
        "experience",
        "Python",
        "scripts",
        "Experience",
        "SQL",
        "queries",
        "Spark",
        "Transformations",
        "Spark",
        "RDDs",
        "Scala",
        "Performed",
        "mapside",
        "RDDs",
        "Experience",
        "data",
        "stream",
        "processing",
        "platforms",
        "Flume",
        "Kafka",
        "Hands",
        "experience",
        "configuring",
        "Flume",
        "data",
        "sources",
        "HDFS",
        "knowledge",
        "job",
        "workflow",
        "scheduling",
        "monitoring",
        "tools",
        "Oozie",
        "Zookeeper",
        "working",
        "experience",
        "Sqoop",
        "data",
        "HDFS",
        "RDBMS",
        "viceversa",
        "Experience",
        "systems",
        "largescale",
        "data",
        "stores",
        "MapReduce",
        "systems",
        "data",
        "modeling",
        "data",
        "systems",
        "loading",
        "data",
        "partitions",
        "buckets",
        "Hive",
        "Experience",
        "services",
        "Apache",
        "Kafka",
        "Experience",
        "MapReduce",
        "jobs",
        "scalability",
        "performance",
        "Map",
        "Reduce",
        "applications",
        "ETL",
        "workloads",
        "terabytes",
        "data",
        "Experience",
        "Cloudera",
        "Hortonworks",
        "Distribution",
        "Hadoop",
        "team",
        "player",
        "zeal",
        "technologies",
        "Experience",
        "Phases",
        "Software",
        "Development",
        "Life",
        "Cycle",
        "Analysis",
        "Design",
        "Development",
        "Testing",
        "Maintenance",
        "Waterfall",
        "methodologies",
        "US",
        "employer",
        "Work",
        "Experience",
        "Hadoopspark",
        "Developer",
        "BCBSA",
        "Chicago",
        "IL",
        "January",
        "Present",
        "Project",
        "Description",
        "project",
        "web",
        "services",
        "FHIR",
        "response",
        "member",
        "coverage",
        "members",
        "member",
        "i",
        "d",
        "plan",
        "i",
        "d",
        "product",
        "i",
        "d",
        "Hadoop",
        "cluster",
        "data",
        "storage",
        "Responsibilities",
        "business",
        "analytics",
        "team",
        "system",
        "requirements",
        "Data",
        "ingestion",
        "HDFS",
        "mainframe",
        "Db2",
        "table",
        "Sqoop",
        "Skilled",
        "data",
        "databases",
        "Hadoop",
        "HDFS",
        "Hive",
        "Sqoop",
        "datasets",
        "Hives",
        "data",
        "warehousing",
        "infrastructure",
        "Knowledge",
        "manages",
        "tables",
        "tables",
        "Eco",
        "system",
        "Implement",
        "Hive",
        "UDFs",
        "evaluation",
        "loading",
        "storing",
        "data",
        "Hive",
        "scripts",
        "job",
        "execution",
        "time",
        "data",
        "HDFS",
        "Hive",
        "Monitoring",
        "Hadoop",
        "jobs",
        "performance",
        "productions",
        "cluster",
        "Production",
        "support",
        "runs",
        "Design",
        "tables",
        "Hive",
        "performance",
        "performance",
        "API",
        "interface",
        "HUE",
        "data",
        "tables",
        "Oozie",
        "workflow",
        "engine",
        "datatime",
        "Hive",
        "methodologies",
        "Scrum",
        "meetings",
        "Sprint",
        "planning",
        "Environment",
        "HDFS",
        "Hive",
        "Sqoop",
        "ImportExport",
        "Spark",
        "Hue",
        "Oozie",
        "ETL",
        "Datawarehouse",
        "UNIX",
        "Cloudera",
        "spark",
        "Developer",
        "Chicago",
        "IL",
        "December",
        "November",
        "Project",
        "Description",
        "scripts",
        "highperformance",
        "production",
        "software",
        "art",
        "computer",
        "vision",
        "increase",
        "data",
        "databases",
        "un",
        "scripts",
        "Leap",
        "Hadoop",
        "amount",
        "data",
        "means",
        "cluster",
        "nodes",
        "scaling",
        "needs",
        "business",
        "operation",
        "Responsibilities",
        "data",
        "files",
        "RDBMS",
        "databases",
        "staging",
        "area",
        "Hadoop",
        "Installed",
        "Hadoop",
        "MapReduce",
        "HDFS",
        "MapReduce",
        "jobs",
        "Java",
        "data",
        "cleansing",
        "Importing",
        "data",
        "HDFS",
        "Hive",
        "Sqoop",
        "Responsible",
        "batch",
        "pipelines",
        "Restful",
        "Service",
        "Map",
        "Reduce",
        "program",
        "Hive",
        "testing",
        "Peer",
        "code",
        "review",
        "troubleshooting",
        "status",
        "report",
        "Map",
        "programs",
        "data",
        "organizations",
        "classifieds",
        "type",
        "records",
        "map",
        "programs",
        "joins",
        "Map",
        "side",
        "Cache",
        "Java",
        "Wrote",
        "Flume",
        "configuration",
        "files",
        "streaming",
        "log",
        "data",
        "HBase",
        "Flume",
        "Performed",
        "masking",
        "customer",
        "data",
        "Flume",
        "interceptors",
        "tables",
        "RDBMS",
        "Hive",
        "tables",
        "SQOOP",
        "visualizations",
        "Tableau",
        "Hive",
        "tables",
        "data",
        "hive",
        "queries",
        "map",
        "way",
        "Oozie",
        "workflow",
        "engine",
        "datatime",
        "Hive",
        "Pig",
        "jobs",
        "methodologies",
        "Scrum",
        "meetings",
        "Sprint",
        "planning",
        "Environment",
        "HDFS",
        "MapReduce",
        "Cassandra",
        "Hive",
        "Pig",
        "Sqoop",
        "Tableau",
        "NoSQL",
        "Shell",
        "Scripting",
        "Maven",
        "Git",
        "HDP",
        "Distribution",
        "Eclipse",
        "Log4j",
        "JUnit",
        "Linux",
        "Hadoop",
        "Developer",
        "Waukegan",
        "IL",
        "December",
        "November",
        "Project",
        "Description",
        "AbbVie",
        "researchdriven",
        "company",
        "therapies",
        "worlds",
        "conditions",
        "companys",
        "mission",
        "expertise",
        "treatments",
        "areas",
        "immunology",
        "oncology",
        "virology",
        "neuroscience",
        "project",
        "supply",
        "chain",
        "data",
        "SAP",
        "source",
        "HDFS",
        "self",
        "service",
        "portal",
        "business",
        "users",
        "daytoday",
        "operations",
        "analysis",
        "Responsibilities",
        "Hadoop",
        "YARN",
        "MapReduce",
        "Flume",
        "HDFS",
        "Hadoop",
        "Distributed",
        "File",
        "System",
        "MapReduce",
        "jobs",
        "Python",
        "data",
        "data",
        "pipeline",
        "Flume",
        "Sqoop",
        "Pig",
        "Python",
        "MapReduce",
        "customer",
        "data",
        "histories",
        "HDFS",
        "analysis",
        "experience",
        "distributions",
        "Hadoop",
        "Enterprise",
        "versions",
        "Cloudera",
        "knowledge",
        "Amazons",
        "EMR",
        "Elastic",
        "MapReduce",
        "AWS",
        "S3",
        "Local",
        "Hard",
        "Disk",
        "File",
        "System",
        "HDFS",
        "Hadoop",
        "Experience",
        "Hadoop",
        "cluster",
        "AWS",
        "S3",
        "file",
        "system",
        "Hadoop",
        "Developed",
        "Python",
        "scripts",
        "data",
        "web",
        "server",
        "output",
        "files",
        "HDFS",
        "HBASE",
        "setup",
        "data",
        "HBASE",
        "analysis",
        "Pig",
        "ETL",
        "tool",
        "transformations",
        "event",
        "preaggregations",
        "data",
        "HDFS",
        "Wrote",
        "Python",
        "MapReduce",
        "scripts",
        "data",
        "Load",
        "data",
        "HDFS",
        "Flume",
        "Worked",
        "MapReduce",
        "jobs",
        "power",
        "data",
        "search",
        "aggregation",
        "Hive",
        "data",
        "analysis",
        "transforming",
        "files",
        "formats",
        "text",
        "files",
        "Hive",
        "tables",
        "loading",
        "data",
        "hive",
        "queries",
        "engineering",
        "Physical",
        "Data",
        "Model",
        "DDL",
        "requirements",
        "Sqoop",
        "data",
        "HDFS",
        "environment",
        "RDBMS",
        "report",
        "generation",
        "visualization",
        "purpose",
        "Fair",
        "schedulers",
        "Job",
        "tracker",
        "resources",
        "Cluster",
        "Map",
        "Reduce",
        "jobs",
        "users",
        "clusters",
        "data",
        "cluster",
        "files",
        "Flume",
        "database",
        "management",
        "systems",
        "Sqoop",
        "Environment",
        "Cloudera",
        "Cloudera",
        "Manager",
        "HDFS",
        "Map",
        "Reduce",
        "Hive",
        "Impala",
        "Pig",
        "Python",
        "SQL",
        "Sqoop",
        "Flume",
        "Yarn",
        "Linux",
        "Centos",
        "HBase",
        "Java",
        "Developer",
        "NebuLogic",
        "Technologies",
        "Hyderabad",
        "Telangana",
        "January",
        "December",
        "Project",
        "Description",
        "Nebulogic",
        "provider",
        "business",
        "outsourcing",
        "solutions",
        "Services",
        "Human",
        "Capital",
        "Management",
        "Tax",
        "Compliance",
        "Electronic",
        "Payment",
        "Solutions",
        "Dealer",
        "Services",
        "Medical",
        "Practice",
        "Services",
        "project",
        "Human",
        "Capital",
        "Management",
        "Payroll",
        "Services",
        "Human",
        "Resource",
        "Management",
        "Talent",
        "Management",
        "Benefits",
        "Administration",
        "Time",
        "Attendance",
        "International",
        "Solutions",
        "frontend",
        "application",
        "interface",
        "Java",
        "applications",
        "information",
        "party",
        "vendors",
        "Responsibilities",
        "Full",
        "Life",
        "Cycle",
        "Development",
        "Distributed",
        "Environment",
        "Java",
        "J2EE",
        "framework",
        "service",
        "layer",
        "business",
        "requirements",
        "webservices",
        "SOAP",
        "WSDL",
        "database",
        "design",
        "tables",
        "views",
        "triggers",
        "procedures",
        "SQL",
        "data",
        "manipulation",
        "retrieval",
        "Developed",
        "Web",
        "Services",
        "Payment",
        "Transaction",
        "Payment",
        "Release",
        "Requirement",
        "Analysis",
        "Development",
        "Documentation",
        "frontend",
        "JSP",
        "HTML",
        "CSS",
        "JavaScript",
        "Coding",
        "DAO",
        "Objects",
        "JDBC",
        "DAO",
        "pattern",
        "XML",
        "XSDs",
        "data",
        "formats",
        "J2EE",
        "design",
        "patterns",
        "singleton",
        "DAO",
        "presentation",
        "tier",
        "business",
        "tier",
        "Integration",
        "Tier",
        "layers",
        "project",
        "Bug",
        "fixing",
        "functionality",
        "enhancements",
        "documentation",
        "standards",
        "practices",
        "project",
        "planning",
        "discussions",
        "team",
        "members",
        "requirements",
        "software",
        "modules",
        "Environment",
        "Java",
        "J2EE",
        "JSP",
        "SOAP",
        "WSDL",
        "SQL",
        "PLSQL",
        "XML",
        "JDBC",
        "Eclipse",
        "Windows",
        "XP",
        "Oracle",
        "Education",
        "Master",
        "Science",
        "Computer",
        "Science",
        "Computer",
        "Science",
        "Virginia",
        "university",
        "Skills",
        "APACHE",
        "HADOOP",
        "HDFS",
        "years",
        "database",
        "years",
        "Eclipse",
        "years",
        "Java",
        "years",
        "SQL",
        "years",
        "CertificationsLicenses",
        "Drivers",
        "License",
        "Additional",
        "Information",
        "TECHNICAL",
        "SKILLS",
        "HadoopMapR",
        "Ecosystem",
        "Hive",
        "HBase",
        "MFS",
        "HDFS",
        "MapReduce",
        "YARN",
        "Spark",
        "MapR",
        "Control",
        "System",
        "MCS",
        "Sqoop",
        "Pig",
        "Cloudera",
        "Manager",
        "Zookeeper",
        "Tools",
        "Apache",
        "Tomcat",
        "Maven",
        "Git",
        "Hibernate",
        "Microsoft",
        "SQL",
        "Server",
        "Management",
        "Studio",
        "SQL",
        "Developer",
        "MySQL",
        "Workbench",
        "Eclipse",
        "Languages",
        "Java",
        "Scala",
        "C",
        "CC",
        "JavaScript",
        "ABAP4",
        "Ruby",
        "HTML5",
        "CSS3",
        "Databases",
        "Oracle",
        "MySQL",
        "Microsoft",
        "SQL",
        "Server",
        "Frameworks",
        "Spring",
        "MVC",
        "Spring",
        "Boot",
        "Hibernate"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T21:06:42.804824",
    "resume_data": "Hadoopspark Developer Hadoopspark span lDeveloperspan Hadoopspark Developer BCBSA Chicago IL Over 4 years of IT experience including Big Data technologies Web Application development and Business Intelligence Experience in deploying and managing a multinode Cloudera Hadoop cluster with different components MFS NFS CLDB Web server Spark Resource Manager Node Manager Hive HBase Zookeeper History Server Exposure to Spark Spark Streaming Scala and Implementing Spark using Scala and utilizing Data frames and Spark SQL API Data Frames and Pair RDDs for faster processing of data Experienced with the Scala Spark for improving the performance and optimization of the existing algorithms in Hadoop using Spark Context SparkSQL Pair RDDs Spark YARN Extensive experience in working with various distributions of Hadoop Enterprise versions of Cloudera good knowledge on Amazons EMR Elastic MapReduce Used AWS S3 and Local Hard Disk as underlying File System HDFS for Hadoop Experience in deploying scalable Hadoop cluster on AWS using S3 as underlying file system for Hadoop Hands on experience in writing Python scripts Experience in converting SQL queries into Spark Transformations using Spark RDDs Scala and Performed mapside joins on RDDs Extensive Experience on importing and exporting data using stream processing platforms like Flume and Kafka Hands on experience in configuring and working with Flume to load the data from multiple sources directly into HDFS Good knowledge in job workflow scheduling and monitoring tools like Oozie Zookeeper Good working experience using Sqoop to import data into HDFS from RDBMS and viceversa Experience with distributed systems largescale nonrelational data stores MapReduce systems data modeling and big data systems Experienced in loading data to hive partitions and creating buckets in Hive Experience in handling messaging services using Apache Kafka Experience in finetuning MapReduce jobs for better scalability and performance Developed various Map Reduce applications to perform ETL workloads on terabytes of data Experience working with Cloudera Hortonworks Distribution of Hadoop Highly motivated team player with zeal to learn new technologies Experience in all Phases of Software Development Life Cycle Analysis Design Development Testing and Maintenance using Waterfall and Agile methodologies Authorized to work in the US for any employer Work Experience Hadoopspark Developer BCBSA Chicago IL January 2019 to Present Project Description This project is based on web services and FHIR response to get the prior member coverage for members when provided with member id plan id and product id It utilizes the Hadoop cluster for data storage and uses Responsibilities Work closely with the business and analytics team in gathering the system requirements Data ingestion into HDFS from various mainframe Db2 table using Sqoop Skilled on migrating the data from different databases to Hadoop HDFS and Hive using Sqoop Analyzed large structured datasets using Hives data warehousing infrastructure Extensive Knowledge of creating manages tables and external tables in Eco system Implement Hive UDFs for evaluation filtering loading and storing of data Involved in writing the Hive scripts to reduce the job execution time Importing exporting historical data from Db2 to HDFS Hive Monitoring Hadoop jobs on performance and productions cluster Provided Production support for few successful runs Design managed and External tables in Hive to optimize performance to improve performance Using API interface HUE to query data and managed tables Installed Oozie workflow engine and scheduled it to run datatime dependent Hive Involved in Agile methodologies daily Scrum meetings Sprint planning Environment HDFS Hive Sqoop ImportExport Spark Hue Oozie ETL Datawarehouse UNIX Cloudera spark Developer Chicago IL December 2017 to November 2018 Project Description all scripts are specialized in designing and developing the highperformance production software with stateof art computer vision capabilities The increase of data made the existing databases un accommodable So all scripts Leap wants to move it to Hadoop where exactly we can handle massive amount of data by means of its cluster nodes and to satisfy the scaling needs of the business operation Responsibilities Extracted the data from the flat files and other RDBMS databases into staging area and ingested to Hadoop Installed and configured Hadoop MapReduce HDFS and developed multiple MapReduce jobs in Java for data cleansing and preprocessing Importing and exporting data into HDFS and Hive using Sqoop Responsible for Coding batch pipelines Restful Service Map Reduce program Hive querys testing debugging Peer code review troubleshooting and maintain status report Implemented Map Reduce programs to classified data organizations into different classifieds based on different type of records Implemented complex map reduce programs to perform joins on the Map side using Distributed Cache in Java Wrote Flume configuration files for importing streaming log data into HBase with Flume Performed masking on customer sensitive data using Flume interceptors Involved in migrating tables from RDBMS into Hive tables using SQOOP and later generate visualizations using Tableau Involved in creating Hive tables loading with data and writing hive queries which will run internally in map reduce way Installed Oozie workflow engine and scheduled it to run datatime dependent Hive and Pig jobs Involved in Agile methodologies daily Scrum meetings Sprint planning Environment HDFS MapReduce Cassandra Hive Pig Sqoop Tableau NoSQL Shell Scripting Maven Git HDP Distribution Eclipse Log4j JUnit Linux Hadoop Developer Waukegan IL December 2016 to November 2017 Project Description AbbVie is a global researchdriven biopharmaceutical company committed to developing innovative advanced therapies for some of the worlds most complex and critical conditions The companys mission is to use its expertise to markedly improve treatments across four primary therapeutic areas immunology oncology virology and neuroscience The project aims at importing supply chain data from SAP source onto HDFS Also providing and supporting a self service portal for business users for their daytoday operations analysis and reporting Responsibilities Installed and configured Hadoop YARN MapReduce Flume HDFS Hadoop Distributed File System developed multiple MapReduce jobs in Python for data cleaning Developed data pipeline using Flume Sqoop Pig and Python MapReduce to ingest customer behavioral data and financial histories into HDFS for analysis Extensive experience in working with various distributions of Hadoop Enterprise versions of Cloudera good knowledge on Amazons EMR Elastic MapReduce Used AWS S3 and Local Hard Disk as underlying File System HDFS for Hadoop Experience in deploying scalable Hadoop cluster on AWS using S3 as underlying file system for Hadoop Developed Python scripts to extract the data from the web server output files to load into HDFS Involved in HBASE setup and storing data into HBASE which will be used for further analysis Used Pig as ETL tool to do transformations event joins and some preaggregations before storing the data onto HDFS Wrote Python MapReduce scripts for processing the unstructured data Load log data into HDFS using Flume Worked extensively in creating MapReduce jobs to power data for search and aggregation Worked on Hive for exposing data for further analysis and for generating transforming files from different analytical formats to text files Responsible for creating Hive tables loading data and writing hive queries Used forward engineering to create a Physical Data Model with DDL that best suits the requirements Worked with Sqoop to export analyzed data from HDFS environment into RDBMS for report generation and visualization purpose Implemented Fair schedulers on the Job tracker to share the resources of the Cluster for the Map Reduce jobs given by the users Maintaining and monitoring clusters Loaded data into the cluster from dynamically generated files using Flume and from relational database management systems using Sqoop Environment Cloudera Cloudera Manager HDFS Map Reduce Hive Impala Pig Python SQL Sqoop Flume Yarn Linux Centos HBase Java Developer NebuLogic Technologies Hyderabad Telangana January 2014 to December 2015 Project Description Nebulogic is a provider of business outsourcing solutions They provide Services such as Human Capital Management Tax Compliance Electronic Payment Solutions Dealer Services and Medical Practice Services This project is focused on Human Capital Management and particularly on Payroll Services Human Resource Management Talent Management Benefits Administration Time Attendance and International Solutions This frontend application has interface with various Java applications which are used to send and receive information to third party vendors Responsibilities Involved in Full Life Cycle Development in Distributed Environment Using Java and J2EE framework Responsible for developing and modifying the existing service layer based on the business requirements Involved in designing developing webservices using SOAP and WSDL Involved in database design Created tables views triggers stored procedures in SQL for data manipulation and retrieval Developed Web Services for Payment Transaction and Payment Release Involved in Requirement Analysis Development and Documentation Developed frontend using JSP HTML CSS and JavaScript Coding for DAO Objects using JDBC using DAO pattern XML and XSDs are used to define data formats Implemented J2EE design patterns such as singleton DAO for the presentation tier business tier and Integration Tier layers of the project Involved in Bug fixing and functionality enhancements Followed coding and documentation standards and best practices Participated in project planning discussions and worked with team members to analyze the requirements and translate them into working software modules Environment Java J2EE JSP SOAP WSDL SQL PLSQL XML JDBC Eclipse Windows XP Oracle Education Master of Science in Computer Science in Computer Science Virginia international university 2018 Skills APACHE HADOOP HDFS 2 years database 2 years Eclipse 2 years Java 2 years SQL 2 years CertificationsLicenses Drivers License Additional Information TECHNICAL SKILLS HadoopMapR Ecosystem Hive HBase MFS HDFS MapReduce YARN Spark MapR Control System MCS Sqoop Pig Cloudera Manager Zookeeper Tools Apache Tomcat 70 Maven Git Hibernate Microsoft SQL Server Management Studio SQL Developer MySQL Workbench Eclipse Languages Java 78 Scala C CC JavaScript ABAP4 Ruby HTML5 CSS3 Databases Oracle 10g11g12c MySQL Microsoft SQL Server 20052008 Frameworks Spring MVC Spring Boot Hibernate",
    "unique_id": "6f8741cd-1272-4845-89d4-0fed75a6463c"
}