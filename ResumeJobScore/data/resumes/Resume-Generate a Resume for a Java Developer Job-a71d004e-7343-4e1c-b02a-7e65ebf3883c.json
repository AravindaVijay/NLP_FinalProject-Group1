{
    "clean_data": "spark python developer spark amp span lpythonspan span ldeveloperspan spark python developer TATA CONSULTANCY SERVICES LIMITED Hyderabad A result oriented professional with over 2 years of experience as HADOOP DEVELOPER Presently working with TATA CONSULTANCY SERVICES LIMITED as Developer in Banking Financial Domain 2 years of extensive experience on Big Data and Big Data analytics Having 2 years of hands on experience on Python Having hands on experience in using Hadoop Technologies such as HDFS HIVE SQOOP Impala Flume Solr Having hands on experience in writing Map Reduce jobs in Hive Pig Having experience on importing and exporting data from different systems to Hadoop file system using SQOOP Using Hadoop ecosystem components for storage and processing data exported data into Tableau using Live connection Having experience on creating databases tables and views in HIVEQL IMPALA and PIG LATIN Having experience on using OOZIE to define and schedule the jobs Having experience on Storage and Processing in Hue covering all Hadoop ecosystem components Having good experience on using Tableau Qlikview Reporting Tools Having knowledge on Zookeeper Experience in working with different data sources like Flat files XML files and Databases Having experience on Sentiment Analysis Having experience on Managing HDFS file system End to end system implementation and post production support Having good skill for project development using methodologies agile waterfall etc Excellence in learning new technologies team building team management and smart working To be part of an organization which progresses dynamically renders creative and challenging environment Put my expertise to the best use towards the growth of the Organization Work Experience spark python developer TATA CONSULTANCY SERVICES LIMITED Hyderabad January 2018 to Present Description USAA a leading financial services group of companies have widely horizontal industries in Insurance Banking and Finacial services They have planned to migrate their customer and other related informations to migrate to Hadoop and do some data analysis and process the large data with the help of spark Data migration from different relational database like Netezza DB2 and through datastage With the help of spark sql we are processing the large amount of data and storing into a staging area then as per our service agreement we have migrated the data from staging area to the Final table using Hive Responsibilities Data migration from DB2 Netezza using Sqoop and keep data in Hive Data migration from datastage and parking it in Hive Creating spark dataframes according to SLA for processing the data and storing into staging area Writing spark code to apply transformations using Python Using Pythons different modules for integrate Hive with spark Using pythons class approach for wrapping the script and move that into productio Hadoop Developer TATA CONSULTANCY SERVICES LIMITED Hyderabad September 2016 to Present Accomplishments Single point of contact between Business team and Developers from solution planning sizing to fulfilment Working on various Projects with High end experience in Hadoop and Python development Working as a developer Hadoop Ecosystems HDFS Hive Sqoop Tableau Spark developer TATA CONSULTANCY SERVICES LIMITED Hyderabad December 2016 to December 2017 Client Trans America US Description Transamerica a leading provider of life insurance retirement and investment solutions to enable the transformation of administration of its US insurance and annuity business linesTCS BaNCS which is a insurance digital platform is partner with Transamerica in its ongoing transformation TCS BanCs push the datas Structure data in the form of flat files to edge node Edge node work as the gateway from other network to hadoop environment so that it will not face the other network issue other network cant touch the data Then from edge node or also known as sftp server all the flat files in being pushed to hadoop server All the files has been placed in database known as hbasePhoenix on top of hbase is use because Phoenix is an open source SQL skin for HBaseWe can use standard JDBC APIs instead of the regular HBase client APIs to create tables insert data and query our HBase dataBased on the business requirement some logics and trasformations should apply on Phoenix tables and need to store in warehouse called as hiveFinally busniess extract need to created and given to customers Responsibilities Acquire the data from source connections FileDB and Keep the data in Hive Transform the data by applying predefined transformation rules as well as custom rules Write Spark code to apply the transformation as well as frame SQL Query using java Created custom rules by using Java Execute queries using SQLHQL and generate the resultant data Project 2 Education BTech Gandhi Institute of Engineering and Technology 2012 to 2016 Additional Information TECHNICAL SKILLS Databases Oracle 11g MYSQL Netezza DB2 Operating systems Windows Linux Servers Weblogic 12c Integrations Webservices Hadoop Technologies HDFS Hive Impala Sqoop Pig Flume Solr oozie Zookeeper HBase SPARK PYTHON Methodology Agile Methodology Reporting Tools Tableau IBM Cognos",
    "entities": [
        "SQOOP Using Hadoop",
        "Zookeeper Experience",
        "Present Description USAA",
        "OOZIE",
        "TATA CONSULTANCY SERVICES LIMITED as Developer in Banking Financial Domain",
        "Hive Data",
        "TATA CONSULTANCY SERVICES LIMITED Hyderabad",
        "US",
        "Sqoop",
        "Hive Transform",
        "BaNCS",
        "12c Integrations Webservices Hadoop Technologies HDFS Hive",
        "Python Using Pythons",
        "BTech Gandhi Institute of Engineering and Technology 2012",
        "Transamerica",
        "Storage",
        "Working",
        "Netezza DB2",
        "PIG",
        "java Created",
        "2016 Additional Information TECHNICAL SKILLS",
        "Hive Responsibilities Data",
        "HIVEQL",
        "Phoenix",
        "Project 2 Education",
        "Tableau Qlikview Reporting Tools Having",
        "SQL",
        "Hadoop",
        "Data",
        "the Organization Work Experience",
        "Hadoop Developer TATA CONSULTANCY SERVICES LIMITED Hyderabad",
        "Python Having",
        "Tableau",
        "Trans America US Description Transamerica",
        "SQL Query",
        "Responsibilities Acquire",
        "HBase",
        "Insurance Banking and Finacial services",
        "Big Data",
        "Hive",
        "Hadoop Ecosystems HDFS Hive Sqoop Tableau Spark",
        "Hadoop Technologies"
    ],
    "experience": "Experience in working with different data sources like Flat files XML files and Databases Having experience on Sentiment Analysis Having experience on Managing HDFS file system End to end system implementation and post production support Having good skill for project development using methodologies agile waterfall etc Excellence in learning new technologies team building team management and smart working To be part of an organization which progresses dynamically renders creative and challenging environment Put my expertise to the best use towards the growth of the Organization Work Experience spark python developer TATA CONSULTANCY SERVICES LIMITED Hyderabad January 2018 to Present Description USAA a leading financial services group of companies have widely horizontal industries in Insurance Banking and Finacial services They have planned to migrate their customer and other related informations to migrate to Hadoop and do some data analysis and process the large data with the help of spark Data migration from different relational database like Netezza DB2 and through datastage With the help of spark sql we are processing the large amount of data and storing into a staging area then as per our service agreement we have migrated the data from staging area to the Final table using Hive Responsibilities Data migration from DB2 Netezza using Sqoop and keep data in Hive Data migration from datastage and parking it in Hive Creating spark dataframes according to SLA for processing the data and storing into staging area Writing spark code to apply transformations using Python Using Pythons different modules for integrate Hive with spark Using pythons class approach for wrapping the script and move that into productio Hadoop Developer TATA CONSULTANCY SERVICES LIMITED Hyderabad September 2016 to Present Accomplishments Single point of contact between Business team and Developers from solution planning sizing to fulfilment Working on various Projects with High end experience in Hadoop and Python development Working as a developer Hadoop Ecosystems HDFS Hive Sqoop Tableau Spark developer TATA CONSULTANCY SERVICES LIMITED Hyderabad December 2016 to December 2017 Client Trans America US Description Transamerica a leading provider of life insurance retirement and investment solutions to enable the transformation of administration of its US insurance and annuity business linesTCS BaNCS which is a insurance digital platform is partner with Transamerica in its ongoing transformation TCS BanCs push the datas Structure data in the form of flat files to edge node Edge node work as the gateway from other network to hadoop environment so that it will not face the other network issue other network ca nt touch the data Then from edge node or also known as sftp server all the flat files in being pushed to hadoop server All the files has been placed in database known as hbasePhoenix on top of hbase is use because Phoenix is an open source SQL skin for HBaseWe can use standard JDBC APIs instead of the regular HBase client APIs to create tables insert data and query our HBase dataBased on the business requirement some logics and trasformations should apply on Phoenix tables and need to store in warehouse called as hiveFinally busniess extract need to created and given to customers Responsibilities Acquire the data from source connections FileDB and Keep the data in Hive Transform the data by applying predefined transformation rules as well as custom rules Write Spark code to apply the transformation as well as frame SQL Query using java Created custom rules by using Java Execute queries using SQLHQL and generate the resultant data Project 2 Education BTech Gandhi Institute of Engineering and Technology 2012 to 2016 Additional Information TECHNICAL SKILLS Databases Oracle 11 g MYSQL Netezza DB2 Operating systems Windows Linux Servers Weblogic 12c Integrations Webservices Hadoop Technologies HDFS Hive Impala Sqoop Pig Flume Solr oozie Zookeeper HBase SPARK PYTHON Methodology Agile Methodology Reporting Tools Tableau IBM Cognos",
    "extracted_keywords": [
        "spark",
        "python",
        "developer",
        "spark",
        "amp",
        "span",
        "lpythonspan",
        "span",
        "ldeveloperspan",
        "spark",
        "python",
        "developer",
        "TATA",
        "CONSULTANCY",
        "SERVICES",
        "LIMITED",
        "Hyderabad",
        "result",
        "years",
        "experience",
        "HADOOP",
        "DEVELOPER",
        "TATA",
        "CONSULTANCY",
        "SERVICES",
        "LIMITED",
        "Developer",
        "Banking",
        "Financial",
        "Domain",
        "years",
        "experience",
        "Big",
        "Data",
        "Big",
        "Data",
        "analytics",
        "years",
        "hands",
        "experience",
        "Python",
        "hands",
        "experience",
        "Hadoop",
        "Technologies",
        "HDFS",
        "HIVE",
        "SQOOP",
        "Impala",
        "Flume",
        "Solr",
        "hands",
        "experience",
        "Map",
        "Reduce",
        "jobs",
        "Hive",
        "Pig",
        "experience",
        "data",
        "systems",
        "Hadoop",
        "file",
        "system",
        "SQOOP",
        "Using",
        "Hadoop",
        "ecosystem",
        "components",
        "storage",
        "processing",
        "data",
        "data",
        "Tableau",
        "connection",
        "experience",
        "databases",
        "tables",
        "views",
        "HIVEQL",
        "IMPALA",
        "PIG",
        "LATIN",
        "experience",
        "OOZIE",
        "jobs",
        "experience",
        "Storage",
        "Processing",
        "Hue",
        "Hadoop",
        "ecosystem",
        "components",
        "experience",
        "Tableau",
        "Qlikview",
        "Reporting",
        "Tools",
        "knowledge",
        "Zookeeper",
        "Experience",
        "data",
        "sources",
        "files",
        "XML",
        "files",
        "Databases",
        "experience",
        "Sentiment",
        "Analysis",
        "experience",
        "HDFS",
        "file",
        "system",
        "End",
        "system",
        "implementation",
        "production",
        "support",
        "skill",
        "project",
        "development",
        "methodologies",
        "waterfall",
        "Excellence",
        "technologies",
        "team",
        "team",
        "management",
        "part",
        "organization",
        "environment",
        "expertise",
        "use",
        "growth",
        "Organization",
        "Work",
        "Experience",
        "spark",
        "python",
        "developer",
        "TATA",
        "CONSULTANCY",
        "SERVICES",
        "LIMITED",
        "Hyderabad",
        "January",
        "Present",
        "Description",
        "USAA",
        "services",
        "group",
        "companies",
        "industries",
        "Insurance",
        "Banking",
        "Finacial",
        "services",
        "customer",
        "informations",
        "Hadoop",
        "data",
        "analysis",
        "data",
        "help",
        "spark",
        "Data",
        "migration",
        "database",
        "Netezza",
        "DB2",
        "datastage",
        "help",
        "spark",
        "sql",
        "amount",
        "data",
        "staging",
        "area",
        "service",
        "agreement",
        "data",
        "area",
        "table",
        "Hive",
        "Responsibilities",
        "Data",
        "migration",
        "DB2",
        "Netezza",
        "Sqoop",
        "data",
        "Hive",
        "Data",
        "migration",
        "datastage",
        "Hive",
        "Creating",
        "spark",
        "SLA",
        "data",
        "staging",
        "area",
        "spark",
        "code",
        "transformations",
        "Python",
        "Pythons",
        "modules",
        "Hive",
        "spark",
        "pythons",
        "class",
        "approach",
        "script",
        "productio",
        "Hadoop",
        "Developer",
        "TATA",
        "CONSULTANCY",
        "SERVICES",
        "LIMITED",
        "Hyderabad",
        "September",
        "Present",
        "Accomplishments",
        "point",
        "contact",
        "Business",
        "team",
        "Developers",
        "solution",
        "planning",
        "fulfilment",
        "Working",
        "Projects",
        "end",
        "experience",
        "Hadoop",
        "Python",
        "development",
        "Working",
        "developer",
        "Hadoop",
        "Ecosystems",
        "HDFS",
        "Hive",
        "Sqoop",
        "Tableau",
        "Spark",
        "developer",
        "TATA",
        "CONSULTANCY",
        "SERVICES",
        "LIMITED",
        "Hyderabad",
        "December",
        "December",
        "Client",
        "Trans",
        "America",
        "US",
        "Description",
        "Transamerica",
        "provider",
        "life",
        "insurance",
        "retirement",
        "investment",
        "solutions",
        "transformation",
        "administration",
        "US",
        "insurance",
        "annuity",
        "business",
        "linesTCS",
        "BaNCS",
        "insurance",
        "platform",
        "partner",
        "Transamerica",
        "transformation",
        "TCS",
        "BanCs",
        "datas",
        "Structure",
        "data",
        "form",
        "files",
        "node",
        "Edge",
        "node",
        "work",
        "gateway",
        "network",
        "hadoop",
        "environment",
        "network",
        "issue",
        "network",
        "data",
        "edge",
        "node",
        "sftp",
        "server",
        "files",
        "hadoop",
        "server",
        "files",
        "database",
        "hbasePhoenix",
        "top",
        "hbase",
        "use",
        "Phoenix",
        "source",
        "SQL",
        "skin",
        "HBaseWe",
        "JDBC",
        "APIs",
        "HBase",
        "client",
        "APIs",
        "tables",
        "data",
        "HBase",
        "business",
        "requirement",
        "logics",
        "trasformations",
        "Phoenix",
        "tables",
        "warehouse",
        "extract",
        "customers",
        "Responsibilities",
        "data",
        "source",
        "connections",
        "FileDB",
        "data",
        "Hive",
        "Transform",
        "data",
        "transformation",
        "rules",
        "custom",
        "rules",
        "Spark",
        "code",
        "transformation",
        "frame",
        "SQL",
        "Query",
        "custom",
        "rules",
        "Java",
        "Execute",
        "SQLHQL",
        "data",
        "Project",
        "Education",
        "BTech",
        "Gandhi",
        "Institute",
        "Engineering",
        "Technology",
        "Information",
        "TECHNICAL",
        "SKILLS",
        "Oracle",
        "g",
        "MYSQL",
        "Netezza",
        "DB2",
        "Operating",
        "systems",
        "Windows",
        "Linux",
        "Servers",
        "Weblogic",
        "12c",
        "Integrations",
        "Webservices",
        "Hadoop",
        "Technologies",
        "HDFS",
        "Hive",
        "Impala",
        "Sqoop",
        "Pig",
        "Flume",
        "Solr",
        "oozie",
        "Zookeeper",
        "HBase",
        "SPARK",
        "PYTHON",
        "Methodology",
        "Agile",
        "Methodology",
        "Reporting",
        "Tools",
        "Tableau",
        "IBM",
        "Cognos"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T20:05:05.586822",
    "resume_data": "spark python developer spark amp span lpythonspan span ldeveloperspan spark python developer TATA CONSULTANCY SERVICES LIMITED Hyderabad A result oriented professional with over 2 years of experience as HADOOP DEVELOPER Presently working with TATA CONSULTANCY SERVICES LIMITED as Developer in Banking Financial Domain 2 years of extensive experience on Big Data and Big Data analytics Having 2 years of hands on experience on Python Having hands on experience in using Hadoop Technologies such as HDFS HIVE SQOOP Impala Flume Solr Having hands on experience in writing Map Reduce jobs in Hive Pig Having experience on importing and exporting data from different systems to Hadoop file system using SQOOP Using Hadoop ecosystem components for storage and processing data exported data into Tableau using Live connection Having experience on creating databases tables and views in HIVEQL IMPALA and PIG LATIN Having experience on using OOZIE to define and schedule the jobs Having experience on Storage and Processing in Hue covering all Hadoop ecosystem components Having good experience on using Tableau Qlikview Reporting Tools Having knowledge on Zookeeper Experience in working with different data sources like Flat files XML files and Databases Having experience on Sentiment Analysis Having experience on Managing HDFS file system End to end system implementation and post production support Having good skill for project development using methodologies agile waterfall etc Excellence in learning new technologies team building team management and smart working To be part of an organization which progresses dynamically renders creative and challenging environment Put my expertise to the best use towards the growth of the Organization Work Experience spark python developer TATA CONSULTANCY SERVICES LIMITED Hyderabad January 2018 to Present Description USAA a leading financial services group of companies have widely horizontal industries in Insurance Banking and Finacial services They have planned to migrate their customer and other related informations to migrate to Hadoop and do some data analysis and process the large data with the help of spark Data migration from different relational database like Netezza DB2 and through datastage With the help of spark sql we are processing the large amount of data and storing into a staging area then as per our service agreement we have migrated the data from staging area to the Final table using Hive Responsibilities Data migration from DB2 Netezza using Sqoop and keep data in Hive Data migration from datastage and parking it in Hive Creating spark dataframes according to SLA for processing the data and storing into staging area Writing spark code to apply transformations using Python Using Pythons different modules for integrate Hive with spark Using pythons class approach for wrapping the script and move that into productio Hadoop Developer TATA CONSULTANCY SERVICES LIMITED Hyderabad September 2016 to Present Accomplishments Single point of contact between Business team and Developers from solution planning sizing to fulfilment Working on various Projects with High end experience in Hadoop and Python development Working as a developer Hadoop Ecosystems HDFS Hive Sqoop Tableau Spark developer TATA CONSULTANCY SERVICES LIMITED Hyderabad December 2016 to December 2017 Client Trans America US Description Transamerica a leading provider of life insurance retirement and investment solutions to enable the transformation of administration of its US insurance and annuity business linesTCS BaNCS which is a insurance digital platform is partner with Transamerica in its ongoing transformation TCS BanCs push the datas Structure data in the form of flat files to edge node Edge node work as the gateway from other network to hadoop environment so that it will not face the other network issue other network cant touch the data Then from edge node or also known as sftp server all the flat files in being pushed to hadoop server All the files has been placed in database known as hbasePhoenix on top of hbase is use because Phoenix is an open source SQL skin for HBaseWe can use standard JDBC APIs instead of the regular HBase client APIs to create tables insert data and query our HBase dataBased on the business requirement some logics and trasformations should apply on Phoenix tables and need to store in warehouse called as hiveFinally busniess extract need to created and given to customers Responsibilities Acquire the data from source connections FileDB and Keep the data in Hive Transform the data by applying predefined transformation rules as well as custom rules Write Spark code to apply the transformation as well as frame SQL Query using java Created custom rules by using Java Execute queries using SQLHQL and generate the resultant data Project 2 Education BTech Gandhi Institute of Engineering and Technology 2012 to 2016 Additional Information TECHNICAL SKILLS Databases Oracle 11g MYSQL Netezza DB2 Operating systems Windows Linux Servers Weblogic 12c Integrations Webservices Hadoop Technologies HDFS Hive Impala Sqoop Pig Flume Solr oozie Zookeeper HBase SPARK PYTHON Methodology Agile Methodology Reporting Tools Tableau IBM Cognos",
    "unique_id": "a71d004e-7343-4e1c-b02a-7e65ebf3883c"
}