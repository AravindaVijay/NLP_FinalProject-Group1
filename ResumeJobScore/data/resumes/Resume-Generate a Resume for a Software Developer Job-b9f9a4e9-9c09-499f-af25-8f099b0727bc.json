{
    "clean_data": "Bigdata Developer Bigdata span lDeveloperspan Bigdata Developer Brillant Edison NJ Over 5 Years of professional IT experience in analysis architectural design prototyping development Integration and testing of applications using JavaJ2EE Technologies and Good Working experience in Big Data Technologies Experience in developing Map Reduce Programs using Apache Hadoop for analyzing the big data as per the requirement Experienced on major Hadoop ecosystems projects such as Pig Hive HBase and monitoring them with Cloudera Manager Extensive experience in developing Pig Latin Scripts and using Hive Query Language for data analytics Hands on experience working on NoSQL databases including HBase Cassandra and its integration with Hadoop cluster Experience in implementing Spark Scala application using higher order functions for both batch and interactive analysis requirement Good working experience using Sqoop to import data into HDFS from RDBMS and viceversa Good knowledge in using job scheduling and monitoring tools like Oozie and Zookeeper Experience in Hadoop administration activities such as installation and configuration of clusters using Apache Cloudera and AWS Experienced in designing built and deploying a multitude application utilizing almost all the AWS stack Including EC2 R53 S3 RDS DynamoDB SQS IAM and EMR focussing on highavailability fault tolerance and autoscaling Developed UML Diagrams for Object Oriented Design Use Cases Sequence Diagrams and Class Diagrams using Rational Rose Visual Paradigm and Visio Hands on experience in solving software design issues by applying design patterns including Singleton Pattern Business Delegator Pattern Controller Pattern MVC Pattern Factory Pattern Abstract Factory Pattern DAO Pattern and Template Pattern Developed Webbased applications using Python Amazon Web Services jQuery CSS and Model View control frameworks like Django Flask and JavaScript Good experience with design coding debug operations reporting and data analysis utilizing python and using python libraries to speed up development Hands on experience with Bid Data environment on technologies including Hadoop Experienced in creative and effective frontend development using JSP JavaScript HTML 5 DHTML XHTML Ajax and CSS Good Working experience in using different Spring modules like Spring Core Container Module Spring Application Context Module Spring MVC Framework module Spring ORM Module in Web applications Used jQuery to select HTML elements to manipulate HTML elements and to implement AJAX in Web applications Used available plugins for extension of jQuery functionality Working knowledge of database such as Oracle10g11g12c Microsoft SQL Server DB2 Experience in writing numerous test cases using JUnit framework with Selenium Strong experience in database design writing complex SQL Queries and Stored Procedures Experienced in using Version Control Tools like Subversion Git Experience in Building Deploying and Integrating with Ant Maven Experience in development of logging standards and mechanism based on Log4J Strong problemsolving skills good communication interpersonal skills and a good team player Have the motivation to take independent responsibility as well as ability to contribute and be a productive team member Work Experience Bigdata Developer Brillant Edison NJ May 2018 to Present Description Brilliant Understand concepts and build your problem solving skills with thousands of free problems and examples in math science and engineering Responsibilities Solid understanding and experience in applying and implementing machine learning algorithms and concepts such as Classification and Regression Resampling statistics and bootstrapping using R language Experience in working with Hadoop 2x version and Spark 2x Python and Scala Extending HIVE and PIG core functionality by using custom User Defined Functions UDF User Defined TableGenerating Functions UDTF and User Defined Aggregating Functions UDAF in Java Experience in Hadoop Production support tasks by analysing the Application and cluster logs Implemented Partitioning Dynamic Partitions and Buckets in Hive on Avro files to meet the business requirements Expertise in designing and deployment of Hadoop cluster and different Big Data analytic tools including Pig Hive HBase Oozie Zookeeper SQOOP flume Spark Kafka HBase with MapR Distribution Assist in upgrading configuration and maintenance of various Hadoop infrastructures like Pig Hive and HBase Used Spark for interactive queries processing of streaming data and integration with NoSQL database for huge volume of data Developed Scala scripts using both Data framesSQL and RDDMapReduce in Spark 1x2x for Data Aggregation queries and writing data back into OLTP system through Sqoop Used SparkStreaming APIs to perform necessary transformations and actions on the fly for building the common learner data model which gets the data from Kafka in near real time and Persists into HBase Performed advanced procedures like text analytics and processing using the inmemory computing capabilities of Spark using Scala Experienced in handling large datasets using Partitions Spark in Memory capabilities Broadcasts in Spark Effective efficient Joins Transformations and other during ingestion process itself Worked on migrating Map Reduce programs into Spark transformations using Spark and Scala Coordinate with Administration team to enhance Spark Jobs performance by analyzing them Implemented design patterns in Scala for the Spark application Developed quality code adhering to Scala coding Standards and best practices Used Spark API over MapR Hadoop YARN to perform analytics on data in Hive Explored with the Spark improving the performance and optimization of the existing algorithms in Hadoop using Spark Context SparkSQL Data Frame Pair RDDs Spark YARN Worked on Loading log data into HDFS using Flume Kafka and performing ETL integrations Used Reporting Tool Tableau to connect with Hive for generating daily reports of data Collaborated with the infrastructure network database application and BA teams to ensure data quality Worked with different File Formats like TEXTFILE SEQUENCE FILE AVROFILE ORC and PARQUET for Hive querying and processing Developed Spark code using Scala and SparkSQLStreaming for faster testing and processing of data Import the data from different sources like HDFSHBase into Spark RDD Load the data into Spark RDD and do in memory data Computation to generate the Output response Experience in Oozie and workflow scheduler to manage Hadoop jobs by Direct Acyclic Graph DAG of actions with control flows Configure Oozie workflow to run multiple Hive and Pig jobs which run independently with time and data availability Collected and aggregated large amounts of log data using Apache Flume and staging data in HDFS for further analysis Developed a data pipeline using Kafka and Storm to store data into HDFS Developed business specific Custom UDFs in Hive Pig Responsible for developing Python wrapper scripts which will extract specific date range using Sqoop by passing custom properties required for the workflow Skilled in using collections in Python for manipulating and looping through different user defined objects Worked with different kind of compression techniques like LZO GZip Snappy Worked on various configurations of Oozie bundles for Orchestrating Pig Hive Spark Sqoop Utilized for Apache Nifi data migration from various sources to HDFS destination Worked with the Apache Nifi flow to perform the conversion of Raw XML data into JSON AVRO Ingested streaming data Apache Nifi with into Kafka Environment RedHat Linux MapR Scala Python Rlanguage Python HDFS Hive Pig Sqoop Flume Oozie HBase Spark Core Spark SQL Spark streaming Kafka Tableau BigData Developer Eli  Indianapolis IN March 2017 to April 2018 Description Eli Lilly and Company is a global pharmaceutical company headquartered in Indianapolis Indiana with offices in 18 countries Its products are sold in approximately 125 countries The company was founded in 1876 by and named after Col Eli Lilly a pharmaceutical chemist and veteran of the American Civil War Responsibilities Interact with Solution Architects and Business Analysts to gather requirements and update Solution Architect Document Handled importing of data from various data sources performed transformations using Hive MapReduce Spark and loaded data into HDFS Submitted Talend jobs for scheduling using Talend scheduler Extensively Worked in Agile software development approach using JIRA Leveraged Talend to ingest data into the datalake of datafabric Unit tested Talend workflows for the correctness of the data Developed jobs to expose HDFS files to Hive tables and Views depending up on the schema versions Created Hive tables partitions and implemented incremental imports to perform adhoc queries on structured data Developed jobs to move inbound files to HDFS file location based on monthly weekly daily and hourly partitioning Optimizing Hive queries improve performance by configuring Hive Query parameters Implemented ORC data format for Apache Hive computations to handle the custom business requirements Imported data from RDBMS MySQL Oracle to HDFS and vice versa using Sqoop Big Data ETL tool for Business Intelligence visualization and report generation Mapping source to target data and converted data JSON to XML 229 Accord Format using Talend data mapper Developed jobs to expose HDFS files to Hive tables and Views depending up on the schema versions Created Hive tables partitions and implemented incremental imports to perform adhoc queries on structured data Developed jobs to move inbound files to HDFS file location based on monthly weekly daily and hourly partitioning Created Hive External tables and loaded the data in to tables and query data using HQL Implemented ORC data format for Apache Hive computations to handle the custom business requirements Implemented a centralized Data Lake in Hadoop with data from various sources Analyzing and understanding the legacy code and making recommendations on how the new system can be designed Developed Scala scripts using both Data framesSQL and RDDMapReduce in Spark 1x2x for Data Aggregation queries and writing data back into OLTP system through Sqoop Used SparkStreaming APIs to perform necessary transformations and actions on the fly for building the common learner data model which gets the data from Kafka in near real time and Persists into HBase Performed advanced procedures like text analytics and processing using the inmemory computing capabilities of Spark using Scala Stored MapReduce program output in Amazon S3 and developed a script to move the data to RedShift for generating a dashboard using QlikView Worked on Talend Administrator Console TAC for scheduling jobs and adding users EnvironmentHive MapReduce SparkRDBMSTACData LakeAmazon S3RedShiftJSON XML Hadoop Developer Comcast Herndon VA May 2016 to February 2017 Description Comcast Corporation is a global media and technology company with two primary businesses Comcast Cable and NBCUniversal Comcast Cable is one of the nations largest video highspeed internet and phone providers to residential customers under the XFINITY brand and provides these services to businesses Responsibilities Provided application demo to the client by designing and developing search engine report analysis trends application administration prototype screens using AngularJS and BootstrapJS Took the ownership of complete application Design of Java part Hadoop integration Apart from the normal requirement gathering participated in Business meeting with the client to gather security requirements Assisted with the architect to analyze the existing system and future system Prepared design blue pints and application flow documentation Experienced in managing and reviewing Hadoop log files Load and transform large sets of structured semi structured and unstructured data Responsible to manage data coming from different sources and application Supported Map Reduce Programs those are running on the cluster Responsible in working with Message broker system such as Kafka Extracted data from mainframes and feed to KAFKA and ingested to HBASE to perform Analytics Written event driven link tracking system to capture user events and feed to KAFKA to push it to HBASE Created MapReduce jobs to extracts the contents from HBASE and configured in OOZIE workflow to generate analytical reports Developed the JAX RS web services code using apache CXF framework to fetch data from SOLR when user performed the search for documents Participated in SOLR schema and ingested data into SOLR for data indexing Written MapReduce programs to organize the data and ingest the data to suitable for analytics in client specified format Hands on experience in writing python scripts to optimize the performance Implemented Storm builder topologies to perform cleansing operations before moving data into Cassandra Extracted files from Cassandra through Sqoop and placed in HDFS and processed Implemented Bloom filters in Cassandra using key space creation Involved in writing Cassandra CQL statements God hands on experience in developing concurrency using spark and Cassandra together Involved in writing spark applications using Scala Hands on experience in creating RDDs transformations and Actions while implementing spark applications Good knowledge in creating data frames using Spark SQL Involved in loading data into Cassandra NoSQL Database Implemented record level atomicity on writes using Cassandra Written PIG Scripts to query and process the Data sets to figure out the patterns of trends by applying client specific criteria and configured OOZIE workflows to run the jobs along with the MR jobs Stored the derived the results in HBASE from analysis and make it available to data ingestion for SOLR for indexing data Involved in integration of java search UI SOLR and HDFS Involved in code deployments using continuous integration tool using Jenkins Documented all the challenges issues involved to deal with the security system and Implemented best practices Created Project structures and configurations according to the project architecture and made it available to the junior developer to continue their work Handled onsite coordinator role to deliver work to offshore Involved in core reviews and application lead supported activities Environment Java J2EE Python Cassandra Spring 32 MVC HTML5 CSS AngularJS Restful services using CXF web services framework spring data SOLR 521 PIG HIVE apache AVRO Map Reduce Sqoop Zookeeper SVN Jenkins windows AD windows KDC Hortonworks distribution of Hadoop 23 YARN Ambari Java Developer Magna Quest Technologies Pvt LtdHyd March 2015 to January 2016 Description Magna Quest Technologies limited is a trendsetting and fascinating innovative Enterprise Productbased Solutions Company which has established its leadership over 15 years in three broad lines of businesses Responsibilities Involved in Analysis design and coding on Java and J2EE Environment Implemented struts MVC framework Designed developed and implemented the business logic required for Security presentation controller Set up the deployment environment on Web Logic Developed system preferences UI screens using JSP and HTML Developed UI screens using Swing components like JLabel JTable JScrollPane JButtons JTextFields etc Used JDBC to connect to Oracle database and get the results that are required Designed asynchronous messaging using Java Message Service JMS Consumed web services through SOAP protocol Developed web Components using JSP Servlets and Serverside components using EJB under J2EE Environment Designing JSP using Java Beans Implemented Struts framework 20 Action and Controller classes for dispatching request to appropriate class Design and implementation of frontend web pages using CSS DHTML JavaScript JSP HTML XHTML JSTL Ajax and Struts Tag Library Designed table structure and coded scripts to create tables indexes views sequence synonyms and database triggers Involved in writing Database procedures Triggers PLSQL statements for data retrieving Developed using Web 20 to interact with other users and changing the contents of websites Implemented AOP and IOC concept using UI Spring 20 Framework Developed using Transaction Management API of Spring 20 and coordinates transactions for Java objects Generated WSDL files using AXIS2 tool Developed using CVS as a version controlling tool for managing the module developments Configured and Tested Application on the IBM Web Sphere App Server Used Hibernate ORM tools which automate the mapping between SQL databases and objects in Java Developed using XML XPDL BPEL and XML parsers like DOM SAX Developed using XSLT to convert XML documents into XHTML and PDF documents Written JUnit test cases for Business Objects and prepared code documentation for future reference and upgrades Deployed applications using WebSphere Application Server and Used IDE RAD Rational Application Developer Environment Java J2EE JSP Servlets MVC Hibernate Spring 30 Web Services Maven 32x Eclipse SOAP WSDL EclipsejQuery Java Script Swings Oracle REST API PLSQL Oracle 11g UNIX Java Developer Impact Software SolutionsHYD January 2014 to February 2015 Description IMPACT IT SOLUTIONS is an Expert IT Solutions Company providing all kind of Web Solution like Custom Web development Ecommerce Application Development Web Application development Ecommerce Solution Development Webbased application Development web webbased IT solutions to a wide variety of clients across India rest of the Globe Responsibilities Designed developed the application using Spring Framework Developed class diagrams sequence and use case diagrams using UML Rational Rose Designed the application with reusable J2EE design patterns Designed DAO objects for accessing RDBMS Developed web pages using JSP HTML DHTML and JSTL Designed and developed a webbased client using Servlets JSP Tag Libraries JavaScript HTML and XML using Struts Framework Involved in developing JSP forms Designed and developed web pages using HTML and JSP Designed various applets using JBuilder Designed and developed Servlets to communicate between presentation and business layer Used EJB as a middleware in developing a threetier distributed application Developed Session Beans and Entity beans for business and data process Used JMS in the project for sending and receiving the messages on the queue Developed the Servlets for processing the data on the server Developed views and controllers for client and manager modules using Spring MVC and Spring Core Used Spring Security for securing the web tier Access Business logic is implemented using Hibernate Developed and modified database objects as per the requirements Involved in Unit integration bug fixing acceptance testing with test cases Code reviews Interaction with customers and identified System Requirements and developed Software Requirement Specifications Implemented Java design patterns wherever required Involved in development maintenance implementation and support of the System Involved in initial project setup and guidelines Implemented Multithreading concepts Developed test cases for Unit testing using JUnit and performed integration and system testing Involved in coding for the presentation layer using Struts Framework JSP AJAX XML XSLT and JavaScript Closely worked and supported the creation of database schema objects tables stored procedures and triggers using Oracle SQLPLSQL Environment Java J2EE JSP CSS JavaScript AJAX Hibernate Spring XML EJB Web Services SOAP Eclipse Rational Rose HTML XPATH XSLT DOM and JDBC IntelliJ Eclipse NetBeans 2010 to 2012 Development Methodologies AgileScrum UML Design Patterns Waterfall Build Tools Jenkins Toad SQL Loader Maven ANT RTC RSA ControlM Oziee Hue SOAP UI Reporting Tools MS Office WordExcelPower Point VisioOutlook Crystal reports XI SSRS cognos 7060 Databases Microsoft SQL Server 12 MySQL 4x5x Oracle 11g 12c DB2 Teradata Netezza Operating Systems All versions of Windows UNIX LINUX Macintosh HD Sun Solaris Skills Sql Cassandra Hdfs Impala Mapreduce Oozie Sqoop Hbase Kafka Flume Hadoop Mongodb Splunk Tableau server C Dtd Fortran Hadoop Hbase Hive Additional Information TECHNICAL SKILLS BigdataHadoop Technologies Hadoop HDFS YARN MapReduce Hive Pig Impala Sqoop Flume Spark Kafka Storm Drill Zookeeper and Oozie Languages HTML5 DHTML WSDL CSS3 C C XML RR Studio SAS Enterprise Guide SAS R Caret Weka ggplot Perl MATLAB Mathematica FORTRAN DTD Schemas JSON Ajax Java Scala Python NumPy SciPy Pandas Java Script Shell Scripting NO SQL Databases Cassandra HBase MongoDB MariaDB Business Intelligence Tools Tableau server Tableau Reader Tableau Splunk SAP Business Objects OBIEE SAP Business Intelligence QlikView Amazon Redshift or Azure Data Warehouse",
    "entities": [
        "Spark Context",
        "Classification and Regression Resampling",
        "Spark Effective",
        "HTML Developed UI",
        "Cassandra",
        "Developed UML Diagrams for Object Oriented Design Use Cases Sequence Diagrams",
        "Bid Data",
        "UNIX",
        "Submitted Talend",
        "Developed Spark",
        "SOLR",
        "Implemented Storm",
        "MapR Distribution Assist",
        "Data Lake",
        "HBASE Created MapReduce",
        "JSON",
        "IBM",
        "Comcast Cable",
        "CVS",
        "JavaScript Closely",
        "Used Reporting Tool Tableau",
        "JavaJ2EE Technologies",
        "the American Civil War Responsibilities Interact with",
        "Apache Nifi",
        "Cassandra Written",
        "Hadoop",
        "Controller",
        "XML",
        "HDFS Involved",
        "Description Comcast Corporation",
        "Spring Core Container",
        "XFINITY",
        "Work Experience Bigdata Developer Brillant Edison NJ",
        "JUnit",
        "SVN Jenkins",
        "NBCUniversal Comcast Cable",
        "Building Deploying",
        "Amazon S3",
        "Hadoop Production",
        "Maven ANT RTC",
        "Solution Architects and Business Analysts",
        "HBase Performed",
        "Created Project",
        "the Globe Responsibilities Designed",
        "Oozie HBase Spark",
        "Developed",
        "DAO",
        "Hive MapReduce Spark",
        "Cassandra CQL",
        "Singleton Pattern Business",
        "Scala Experienced",
        "IDE RAD Rational Application Developer Environment",
        "Servlets JSP Tag Libraries",
        "System Requirements",
        "Spring MVC",
        "Responsibilities Involved",
        "PARQUET",
        "TEXTFILE SEQUENCE FILE AVROFILE",
        "Software Requirement Specifications Implemented Java",
        "Pig Hive HBase",
        "Including EC2 R53 S3 RDS",
        "Ant Maven",
        "Pig Hive HBase Oozie Zookeeper",
        "MapR Hadoop",
        "DOM SAX Developed",
        "JSP",
        "Version Control Tools",
        "Hibernate Developed",
        "Collaborated",
        "Implemented AOP",
        "SQL Queries",
        "IOC",
        "Talend",
        "ORC",
        "Col Eli Lilly",
        "Views",
        "Hive Pig Responsible",
        "Raw XML",
        "MVC",
        "Oracle SQLPLSQL Environment",
        "Spark",
        "File Formats",
        "EJB",
        "Hive Query Language",
        "Created Hive",
        "Indiana",
        "Solution Architect Document",
        "Direct Acyclic Graph DAG",
        "Database",
        "Sqoop",
        "Transaction Management API",
        "Eli",
        "Hadoop Experienced",
        "Pattern Factory Pattern Abstract Factory Pattern",
        "HIVE",
        "PDF",
        "Storm",
        "Spark SQL Involved",
        "Class Diagrams",
        "Data Aggregation",
        "the System Involved",
        "PIG",
        "MVC Framework",
        "log data",
        "HTML",
        "java",
        "Business Intelligence",
        "Description Eli Lilly and Company",
        "SAS",
        "Intelligence Tools Tableau",
        "CXF",
        "Collected",
        "SQL",
        "OLTP",
        "Spark RDD",
        "Kafka Tableau BigData Developer",
        "RedShift",
        "JIRA Leveraged Talend",
        "Written MapReduce",
        "Big Data",
        "Indianapolis",
        "Hive",
        "Macintosh",
        "Tableau Reader Tableau Splunk SAP Business Objects OBIEE SAP Business Intelligence QlikView Amazon Redshift or Azure Data Warehouse",
        "Python Amazon Web Services jQuery CSS",
        "Oracle10g11g12c",
        "CSS Good Working",
        "Serverside",
        "XHTML",
        "OOZIE",
        "Partitions Spark",
        "Sqoop Big Data ETL",
        "Sun Solaris",
        "Bigdata Developer Bigdata",
        "HBASE",
        "ETL",
        "Reporting Tools MS Office",
        "India",
        "Apache Hadoop",
        "LakeAmazon",
        "Description Magna Quest Technologies",
        "XML Hadoop",
        "XSLT",
        "Impala",
        "BA",
        "UI",
        "Studio SAS Enterprise Guide",
        "Microsoft",
        "JSP Servlets",
        "Ecommerce Application Development Web Application",
        "UML Rational Rose Designed",
        "UI Spring",
        "LZO",
        "SQS IAM",
        "Business Objects",
        "Expertise",
        "CSS",
        "Big Data Technologies",
        "jQuery",
        "Hive Query",
        "Java Message Service",
        "Actions",
        "Data",
        "AWS Experienced",
        "XI SSRS",
        "NetBeans",
        "NoSQL",
        "Tableau",
        "Access Business",
        "Application",
        "Selenium Strong",
        "WebSphere Application Server",
        "KAFKA",
        "Created Hive External"
    ],
    "experience": "Experience in developing Map Reduce Programs using Apache Hadoop for analyzing the big data as per the requirement Experienced on major Hadoop ecosystems projects such as Pig Hive HBase and monitoring them with Cloudera Manager Extensive experience in developing Pig Latin Scripts and using Hive Query Language for data analytics Hands on experience working on NoSQL databases including HBase Cassandra and its integration with Hadoop cluster Experience in implementing Spark Scala application using higher order functions for both batch and interactive analysis requirement Good working experience using Sqoop to import data into HDFS from RDBMS and viceversa Good knowledge in using job scheduling and monitoring tools like Oozie and Zookeeper Experience in Hadoop administration activities such as installation and configuration of clusters using Apache Cloudera and AWS Experienced in designing built and deploying a multitude application utilizing almost all the AWS stack Including EC2 R53 S3 RDS DynamoDB SQS IAM and EMR focussing on highavailability fault tolerance and autoscaling Developed UML Diagrams for Object Oriented Design Use Cases Sequence Diagrams and Class Diagrams using Rational Rose Visual Paradigm and Visio Hands on experience in solving software design issues by applying design patterns including Singleton Pattern Business Delegator Pattern Controller Pattern MVC Pattern Factory Pattern Abstract Factory Pattern DAO Pattern and Template Pattern Developed Webbased applications using Python Amazon Web Services jQuery CSS and Model View control frameworks like Django Flask and JavaScript Good experience with design coding debug operations reporting and data analysis utilizing python and using python libraries to speed up development Hands on experience with Bid Data environment on technologies including Hadoop Experienced in creative and effective frontend development using JSP JavaScript HTML 5 DHTML XHTML Ajax and CSS Good Working experience in using different Spring modules like Spring Core Container Module Spring Application Context Module Spring MVC Framework module Spring ORM Module in Web applications Used jQuery to select HTML elements to manipulate HTML elements and to implement AJAX in Web applications Used available plugins for extension of jQuery functionality Working knowledge of database such as Oracle10g11g12c Microsoft SQL Server DB2 Experience in writing numerous test cases using JUnit framework with Selenium Strong experience in database design writing complex SQL Queries and Stored Procedures Experienced in using Version Control Tools like Subversion Git Experience in Building Deploying and Integrating with Ant Maven Experience in development of logging standards and mechanism based on Log4J Strong problemsolving skills good communication interpersonal skills and a good team player Have the motivation to take independent responsibility as well as ability to contribute and be a productive team member Work Experience Bigdata Developer Brillant Edison NJ May 2018 to Present Description Brilliant Understand concepts and build your problem solving skills with thousands of free problems and examples in math science and engineering Responsibilities Solid understanding and experience in applying and implementing machine learning algorithms and concepts such as Classification and Regression Resampling statistics and bootstrapping using R language Experience in working with Hadoop 2x version and Spark 2x Python and Scala Extending HIVE and PIG core functionality by using custom User Defined Functions UDF User Defined TableGenerating Functions UDTF and User Defined Aggregating Functions UDAF in Java Experience in Hadoop Production support tasks by analysing the Application and cluster logs Implemented Partitioning Dynamic Partitions and Buckets in Hive on Avro files to meet the business requirements Expertise in designing and deployment of Hadoop cluster and different Big Data analytic tools including Pig Hive HBase Oozie Zookeeper SQOOP flume Spark Kafka HBase with MapR Distribution Assist in upgrading configuration and maintenance of various Hadoop infrastructures like Pig Hive and HBase Used Spark for interactive queries processing of streaming data and integration with NoSQL database for huge volume of data Developed Scala scripts using both Data framesSQL and RDDMapReduce in Spark 1x2x for Data Aggregation queries and writing data back into OLTP system through Sqoop Used SparkStreaming APIs to perform necessary transformations and actions on the fly for building the common learner data model which gets the data from Kafka in near real time and Persists into HBase Performed advanced procedures like text analytics and processing using the inmemory computing capabilities of Spark using Scala Experienced in handling large datasets using Partitions Spark in Memory capabilities Broadcasts in Spark Effective efficient Joins Transformations and other during ingestion process itself Worked on migrating Map Reduce programs into Spark transformations using Spark and Scala Coordinate with Administration team to enhance Spark Jobs performance by analyzing them Implemented design patterns in Scala for the Spark application Developed quality code adhering to Scala coding Standards and best practices Used Spark API over MapR Hadoop YARN to perform analytics on data in Hive Explored with the Spark improving the performance and optimization of the existing algorithms in Hadoop using Spark Context SparkSQL Data Frame Pair RDDs Spark YARN Worked on Loading log data into HDFS using Flume Kafka and performing ETL integrations Used Reporting Tool Tableau to connect with Hive for generating daily reports of data Collaborated with the infrastructure network database application and BA teams to ensure data quality Worked with different File Formats like TEXTFILE SEQUENCE FILE AVROFILE ORC and PARQUET for Hive querying and processing Developed Spark code using Scala and SparkSQLStreaming for faster testing and processing of data Import the data from different sources like HDFSHBase into Spark RDD Load the data into Spark RDD and do in memory data Computation to generate the Output response Experience in Oozie and workflow scheduler to manage Hadoop jobs by Direct Acyclic Graph DAG of actions with control flows Configure Oozie workflow to run multiple Hive and Pig jobs which run independently with time and data availability Collected and aggregated large amounts of log data using Apache Flume and staging data in HDFS for further analysis Developed a data pipeline using Kafka and Storm to store data into HDFS Developed business specific Custom UDFs in Hive Pig Responsible for developing Python wrapper scripts which will extract specific date range using Sqoop by passing custom properties required for the workflow Skilled in using collections in Python for manipulating and looping through different user defined objects Worked with different kind of compression techniques like LZO GZip Snappy Worked on various configurations of Oozie bundles for Orchestrating Pig Hive Spark Sqoop Utilized for Apache Nifi data migration from various sources to HDFS destination Worked with the Apache Nifi flow to perform the conversion of Raw XML data into JSON AVRO Ingested streaming data Apache Nifi with into Kafka Environment RedHat Linux MapR Scala Python Rlanguage Python HDFS Hive Pig Sqoop Flume Oozie HBase Spark Core Spark SQL Spark streaming Kafka Tableau BigData Developer Eli   Indianapolis IN March 2017 to April 2018 Description Eli Lilly and Company is a global pharmaceutical company headquartered in Indianapolis Indiana with offices in 18 countries Its products are sold in approximately 125 countries The company was founded in 1876 by and named after Col Eli Lilly a pharmaceutical chemist and veteran of the American Civil War Responsibilities Interact with Solution Architects and Business Analysts to gather requirements and update Solution Architect Document Handled importing of data from various data sources performed transformations using Hive MapReduce Spark and loaded data into HDFS Submitted Talend jobs for scheduling using Talend scheduler Extensively Worked in Agile software development approach using JIRA Leveraged Talend to ingest data into the datalake of datafabric Unit tested Talend workflows for the correctness of the data Developed jobs to expose HDFS files to Hive tables and Views depending up on the schema versions Created Hive tables partitions and implemented incremental imports to perform adhoc queries on structured data Developed jobs to move inbound files to HDFS file location based on monthly weekly daily and hourly partitioning Optimizing Hive queries improve performance by configuring Hive Query parameters Implemented ORC data format for Apache Hive computations to handle the custom business requirements Imported data from RDBMS MySQL Oracle to HDFS and vice versa using Sqoop Big Data ETL tool for Business Intelligence visualization and report generation Mapping source to target data and converted data JSON to XML 229 Accord Format using Talend data mapper Developed jobs to expose HDFS files to Hive tables and Views depending up on the schema versions Created Hive tables partitions and implemented incremental imports to perform adhoc queries on structured data Developed jobs to move inbound files to HDFS file location based on monthly weekly daily and hourly partitioning Created Hive External tables and loaded the data in to tables and query data using HQL Implemented ORC data format for Apache Hive computations to handle the custom business requirements Implemented a centralized Data Lake in Hadoop with data from various sources Analyzing and understanding the legacy code and making recommendations on how the new system can be designed Developed Scala scripts using both Data framesSQL and RDDMapReduce in Spark 1x2x for Data Aggregation queries and writing data back into OLTP system through Sqoop Used SparkStreaming APIs to perform necessary transformations and actions on the fly for building the common learner data model which gets the data from Kafka in near real time and Persists into HBase Performed advanced procedures like text analytics and processing using the inmemory computing capabilities of Spark using Scala Stored MapReduce program output in Amazon S3 and developed a script to move the data to RedShift for generating a dashboard using QlikView Worked on Talend Administrator Console TAC for scheduling jobs and adding users EnvironmentHive MapReduce SparkRDBMSTACData LakeAmazon S3RedShiftJSON XML Hadoop Developer Comcast Herndon VA May 2016 to February 2017 Description Comcast Corporation is a global media and technology company with two primary businesses Comcast Cable and NBCUniversal Comcast Cable is one of the nations largest video highspeed internet and phone providers to residential customers under the XFINITY brand and provides these services to businesses Responsibilities Provided application demo to the client by designing and developing search engine report analysis trends application administration prototype screens using AngularJS and BootstrapJS Took the ownership of complete application Design of Java part Hadoop integration Apart from the normal requirement gathering participated in Business meeting with the client to gather security requirements Assisted with the architect to analyze the existing system and future system Prepared design blue pints and application flow documentation Experienced in managing and reviewing Hadoop log files Load and transform large sets of structured semi structured and unstructured data Responsible to manage data coming from different sources and application Supported Map Reduce Programs those are running on the cluster Responsible in working with Message broker system such as Kafka Extracted data from mainframes and feed to KAFKA and ingested to HBASE to perform Analytics Written event driven link tracking system to capture user events and feed to KAFKA to push it to HBASE Created MapReduce jobs to extracts the contents from HBASE and configured in OOZIE workflow to generate analytical reports Developed the JAX RS web services code using apache CXF framework to fetch data from SOLR when user performed the search for documents Participated in SOLR schema and ingested data into SOLR for data indexing Written MapReduce programs to organize the data and ingest the data to suitable for analytics in client specified format Hands on experience in writing python scripts to optimize the performance Implemented Storm builder topologies to perform cleansing operations before moving data into Cassandra Extracted files from Cassandra through Sqoop and placed in HDFS and processed Implemented Bloom filters in Cassandra using key space creation Involved in writing Cassandra CQL statements God hands on experience in developing concurrency using spark and Cassandra together Involved in writing spark applications using Scala Hands on experience in creating RDDs transformations and Actions while implementing spark applications Good knowledge in creating data frames using Spark SQL Involved in loading data into Cassandra NoSQL Database Implemented record level atomicity on writes using Cassandra Written PIG Scripts to query and process the Data sets to figure out the patterns of trends by applying client specific criteria and configured OOZIE workflows to run the jobs along with the MR jobs Stored the derived the results in HBASE from analysis and make it available to data ingestion for SOLR for indexing data Involved in integration of java search UI SOLR and HDFS Involved in code deployments using continuous integration tool using Jenkins Documented all the challenges issues involved to deal with the security system and Implemented best practices Created Project structures and configurations according to the project architecture and made it available to the junior developer to continue their work Handled onsite coordinator role to deliver work to offshore Involved in core reviews and application lead supported activities Environment Java J2EE Python Cassandra Spring 32 MVC HTML5 CSS AngularJS Restful services using CXF web services framework spring data SOLR 521 PIG HIVE apache AVRO Map Reduce Sqoop Zookeeper SVN Jenkins windows AD windows KDC Hortonworks distribution of Hadoop 23 YARN Ambari Java Developer Magna Quest Technologies Pvt LtdHyd March 2015 to January 2016 Description Magna Quest Technologies limited is a trendsetting and fascinating innovative Enterprise Productbased Solutions Company which has established its leadership over 15 years in three broad lines of businesses Responsibilities Involved in Analysis design and coding on Java and J2EE Environment Implemented struts MVC framework Designed developed and implemented the business logic required for Security presentation controller Set up the deployment environment on Web Logic Developed system preferences UI screens using JSP and HTML Developed UI screens using Swing components like JLabel JTable JScrollPane JButtons JTextFields etc Used JDBC to connect to Oracle database and get the results that are required Designed asynchronous messaging using Java Message Service JMS Consumed web services through SOAP protocol Developed web Components using JSP Servlets and Serverside components using EJB under J2EE Environment Designing JSP using Java Beans Implemented Struts framework 20 Action and Controller classes for dispatching request to appropriate class Design and implementation of frontend web pages using CSS DHTML JavaScript JSP HTML XHTML JSTL Ajax and Struts Tag Library Designed table structure and coded scripts to create tables indexes views sequence synonyms and database triggers Involved in writing Database procedures Triggers PLSQL statements for data retrieving Developed using Web 20 to interact with other users and changing the contents of websites Implemented AOP and IOC concept using UI Spring 20 Framework Developed using Transaction Management API of Spring 20 and coordinates transactions for Java objects Generated WSDL files using AXIS2 tool Developed using CVS as a version controlling tool for managing the module developments Configured and Tested Application on the IBM Web Sphere App Server Used Hibernate ORM tools which automate the mapping between SQL databases and objects in Java Developed using XML XPDL BPEL and XML parsers like DOM SAX Developed using XSLT to convert XML documents into XHTML and PDF documents Written JUnit test cases for Business Objects and prepared code documentation for future reference and upgrades Deployed applications using WebSphere Application Server and Used IDE RAD Rational Application Developer Environment Java J2EE JSP Servlets MVC Hibernate Spring 30 Web Services Maven 32x Eclipse SOAP WSDL EclipsejQuery Java Script Swings Oracle REST API PLSQL Oracle 11 g UNIX Java Developer Impact Software SolutionsHYD January 2014 to February 2015 Description IMPACT IT SOLUTIONS is an Expert IT Solutions Company providing all kind of Web Solution like Custom Web development Ecommerce Application Development Web Application development Ecommerce Solution Development Webbased application Development web webbased IT solutions to a wide variety of clients across India rest of the Globe Responsibilities Designed developed the application using Spring Framework Developed class diagrams sequence and use case diagrams using UML Rational Rose Designed the application with reusable J2EE design patterns Designed DAO objects for accessing RDBMS Developed web pages using JSP HTML DHTML and JSTL Designed and developed a webbased client using Servlets JSP Tag Libraries JavaScript HTML and XML using Struts Framework Involved in developing JSP forms Designed and developed web pages using HTML and JSP Designed various applets using JBuilder Designed and developed Servlets to communicate between presentation and business layer Used EJB as a middleware in developing a threetier distributed application Developed Session Beans and Entity beans for business and data process Used JMS in the project for sending and receiving the messages on the queue Developed the Servlets for processing the data on the server Developed views and controllers for client and manager modules using Spring MVC and Spring Core Used Spring Security for securing the web tier Access Business logic is implemented using Hibernate Developed and modified database objects as per the requirements Involved in Unit integration bug fixing acceptance testing with test cases Code reviews Interaction with customers and identified System Requirements and developed Software Requirement Specifications Implemented Java design patterns wherever required Involved in development maintenance implementation and support of the System Involved in initial project setup and guidelines Implemented Multithreading concepts Developed test cases for Unit testing using JUnit and performed integration and system testing Involved in coding for the presentation layer using Struts Framework JSP AJAX XML XSLT and JavaScript Closely worked and supported the creation of database schema objects tables stored procedures and triggers using Oracle SQLPLSQL Environment Java J2EE JSP CSS JavaScript AJAX Hibernate Spring XML EJB Web Services SOAP Eclipse Rational Rose HTML XPATH XSLT DOM and JDBC IntelliJ Eclipse NetBeans 2010 to 2012 Development Methodologies AgileScrum UML Design Patterns Waterfall Build Tools Jenkins Toad SQL Loader Maven ANT RTC RSA ControlM Oziee Hue SOAP UI Reporting Tools MS Office WordExcelPower Point VisioOutlook Crystal reports XI SSRS cognos 7060 Databases Microsoft SQL Server 12 MySQL 4x5x Oracle 11 g 12c DB2 Teradata Netezza Operating Systems All versions of Windows UNIX LINUX Macintosh HD Sun Solaris Skills Sql Cassandra Hdfs Impala Mapreduce Oozie Sqoop Hbase Kafka Flume Hadoop Mongodb Splunk Tableau server C Dtd Fortran Hadoop Hbase Hive Additional Information TECHNICAL SKILLS BigdataHadoop Technologies Hadoop HDFS YARN MapReduce Hive Pig Impala Sqoop Flume Spark Kafka Storm Drill Zookeeper and Oozie Languages HTML5 DHTML WSDL CSS3 C C XML RR Studio SAS Enterprise Guide SAS R Caret Weka ggplot Perl MATLAB Mathematica FORTRAN DTD Schemas JSON Ajax Java Scala Python NumPy SciPy Pandas Java Script Shell Scripting NO SQL Databases Cassandra HBase MongoDB MariaDB Business Intelligence Tools Tableau server Tableau Reader Tableau Splunk SAP Business Objects OBIEE SAP Business Intelligence QlikView Amazon Redshift or Azure Data Warehouse",
    "extracted_keywords": [
        "Bigdata",
        "Developer",
        "Bigdata",
        "span",
        "lDeveloperspan",
        "Bigdata",
        "Developer",
        "Brillant",
        "Edison",
        "NJ",
        "Years",
        "IT",
        "experience",
        "analysis",
        "design",
        "development",
        "Integration",
        "testing",
        "applications",
        "JavaJ2EE",
        "Technologies",
        "Good",
        "Working",
        "experience",
        "Big",
        "Data",
        "Technologies",
        "Experience",
        "Map",
        "Reduce",
        "Programs",
        "Apache",
        "Hadoop",
        "data",
        "requirement",
        "Hadoop",
        "ecosystems",
        "projects",
        "Pig",
        "Hive",
        "HBase",
        "Cloudera",
        "Manager",
        "Extensive",
        "experience",
        "Pig",
        "Latin",
        "Scripts",
        "Hive",
        "Query",
        "Language",
        "data",
        "analytics",
        "Hands",
        "experience",
        "NoSQL",
        "databases",
        "HBase",
        "Cassandra",
        "integration",
        "Hadoop",
        "cluster",
        "Experience",
        "Spark",
        "Scala",
        "application",
        "order",
        "functions",
        "batch",
        "analysis",
        "requirement",
        "working",
        "experience",
        "Sqoop",
        "data",
        "HDFS",
        "RDBMS",
        "viceversa",
        "knowledge",
        "job",
        "scheduling",
        "monitoring",
        "tools",
        "Oozie",
        "Zookeeper",
        "Experience",
        "Hadoop",
        "administration",
        "activities",
        "installation",
        "configuration",
        "clusters",
        "Apache",
        "Cloudera",
        "AWS",
        "designing",
        "multitude",
        "application",
        "AWS",
        "stack",
        "EC2",
        "R53",
        "S3",
        "RDS",
        "DynamoDB",
        "SQS",
        "IAM",
        "EMR",
        "highavailability",
        "fault",
        "tolerance",
        "Developed",
        "UML",
        "Diagrams",
        "Object",
        "Oriented",
        "Design",
        "Use",
        "Cases",
        "Sequence",
        "Diagrams",
        "Class",
        "Diagrams",
        "Rational",
        "Rose",
        "Visual",
        "Paradigm",
        "Visio",
        "Hands",
        "experience",
        "software",
        "design",
        "issues",
        "design",
        "patterns",
        "Singleton",
        "Pattern",
        "Business",
        "Delegator",
        "Pattern",
        "Controller",
        "Pattern",
        "MVC",
        "Pattern",
        "Factory",
        "Pattern",
        "Abstract",
        "Factory",
        "Pattern",
        "DAO",
        "Pattern",
        "Template",
        "Pattern",
        "applications",
        "Python",
        "Amazon",
        "Web",
        "Services",
        "jQuery",
        "CSS",
        "Model",
        "View",
        "control",
        "frameworks",
        "Django",
        "Flask",
        "JavaScript",
        "Good",
        "experience",
        "design",
        "debug",
        "operations",
        "data",
        "analysis",
        "python",
        "python",
        "libraries",
        "development",
        "Hands",
        "experience",
        "Bid",
        "Data",
        "environment",
        "technologies",
        "Hadoop",
        "Experienced",
        "frontend",
        "development",
        "JSP",
        "JavaScript",
        "HTML",
        "DHTML",
        "XHTML",
        "CSS",
        "Good",
        "Working",
        "experience",
        "Spring",
        "modules",
        "Spring",
        "Core",
        "Container",
        "Module",
        "Spring",
        "Application",
        "Context",
        "Module",
        "Spring",
        "MVC",
        "Framework",
        "module",
        "Spring",
        "ORM",
        "Module",
        "Web",
        "applications",
        "jQuery",
        "HTML",
        "elements",
        "HTML",
        "elements",
        "AJAX",
        "Web",
        "applications",
        "plugins",
        "extension",
        "jQuery",
        "functionality",
        "Working",
        "knowledge",
        "database",
        "Oracle10g11g12c",
        "Microsoft",
        "SQL",
        "Server",
        "DB2",
        "Experience",
        "test",
        "cases",
        "JUnit",
        "framework",
        "Selenium",
        "Strong",
        "experience",
        "database",
        "design",
        "SQL",
        "Queries",
        "Stored",
        "Procedures",
        "Version",
        "Control",
        "Tools",
        "Subversion",
        "Git",
        "Experience",
        "Building",
        "Deploying",
        "Integrating",
        "Ant",
        "Maven",
        "Experience",
        "development",
        "standards",
        "mechanism",
        "skills",
        "communication",
        "skills",
        "team",
        "player",
        "motivation",
        "responsibility",
        "ability",
        "team",
        "member",
        "Work",
        "Experience",
        "Bigdata",
        "Developer",
        "Brillant",
        "Edison",
        "NJ",
        "May",
        "Present",
        "Description",
        "Brilliant",
        "Understand",
        "concepts",
        "problem",
        "skills",
        "thousands",
        "problems",
        "examples",
        "math",
        "science",
        "engineering",
        "Responsibilities",
        "understanding",
        "experience",
        "machine",
        "learning",
        "algorithms",
        "concepts",
        "Classification",
        "Regression",
        "Resampling",
        "statistics",
        "R",
        "language",
        "Experience",
        "Hadoop",
        "version",
        "Spark",
        "Python",
        "Scala",
        "HIVE",
        "PIG",
        "core",
        "functionality",
        "custom",
        "User",
        "Defined",
        "Functions",
        "UDF",
        "User",
        "Defined",
        "TableGenerating",
        "Functions",
        "UDTF",
        "User",
        "Defined",
        "Aggregating",
        "Functions",
        "UDAF",
        "Java",
        "Experience",
        "Hadoop",
        "Production",
        "support",
        "tasks",
        "Application",
        "cluster",
        "logs",
        "Partitioning",
        "Dynamic",
        "Partitions",
        "Buckets",
        "Hive",
        "Avro",
        "files",
        "business",
        "requirements",
        "Expertise",
        "designing",
        "deployment",
        "Hadoop",
        "cluster",
        "Big",
        "Data",
        "tools",
        "Pig",
        "Hive",
        "HBase",
        "Oozie",
        "Zookeeper",
        "SQOOP",
        "Spark",
        "Kafka",
        "HBase",
        "MapR",
        "Distribution",
        "Assist",
        "configuration",
        "maintenance",
        "Hadoop",
        "infrastructures",
        "Pig",
        "Hive",
        "HBase",
        "Spark",
        "queries",
        "processing",
        "data",
        "integration",
        "NoSQL",
        "database",
        "volume",
        "data",
        "Scala",
        "scripts",
        "Data",
        "framesSQL",
        "RDDMapReduce",
        "Spark",
        "Data",
        "Aggregation",
        "data",
        "OLTP",
        "system",
        "Sqoop",
        "SparkStreaming",
        "APIs",
        "transformations",
        "actions",
        "fly",
        "learner",
        "data",
        "model",
        "data",
        "Kafka",
        "time",
        "Persists",
        "HBase",
        "procedures",
        "text",
        "analytics",
        "processing",
        "inmemory",
        "computing",
        "capabilities",
        "Spark",
        "Scala",
        "datasets",
        "Partitions",
        "Spark",
        "Memory",
        "capabilities",
        "Broadcasts",
        "Spark",
        "Effective",
        "Joins",
        "Transformations",
        "ingestion",
        "process",
        "Map",
        "Reduce",
        "programs",
        "Spark",
        "transformations",
        "Spark",
        "Scala",
        "Coordinate",
        "Administration",
        "team",
        "Spark",
        "Jobs",
        "performance",
        "design",
        "patterns",
        "Scala",
        "Spark",
        "application",
        "quality",
        "code",
        "Scala",
        "Standards",
        "practices",
        "Spark",
        "API",
        "MapR",
        "Hadoop",
        "YARN",
        "analytics",
        "data",
        "Hive",
        "Explored",
        "Spark",
        "performance",
        "optimization",
        "algorithms",
        "Hadoop",
        "Spark",
        "Context",
        "SparkSQL",
        "Data",
        "Frame",
        "Pair",
        "Spark",
        "YARN",
        "Loading",
        "log",
        "data",
        "HDFS",
        "Flume",
        "Kafka",
        "ETL",
        "integrations",
        "Reporting",
        "Tool",
        "Tableau",
        "Hive",
        "reports",
        "data",
        "infrastructure",
        "network",
        "database",
        "application",
        "BA",
        "teams",
        "data",
        "quality",
        "File",
        "Formats",
        "TEXTFILE",
        "SEQUENCE",
        "AVROFILE",
        "ORC",
        "PARQUET",
        "Hive",
        "Developed",
        "Spark",
        "code",
        "Scala",
        "SparkSQLStreaming",
        "testing",
        "processing",
        "data",
        "Import",
        "data",
        "sources",
        "HDFSHBase",
        "Spark",
        "RDD",
        "Load",
        "data",
        "Spark",
        "RDD",
        "memory",
        "data",
        "Computation",
        "Output",
        "response",
        "Experience",
        "Oozie",
        "workflow",
        "scheduler",
        "Hadoop",
        "jobs",
        "Direct",
        "Acyclic",
        "Graph",
        "DAG",
        "actions",
        "control",
        "Configure",
        "Oozie",
        "Hive",
        "Pig",
        "jobs",
        "time",
        "data",
        "availability",
        "amounts",
        "log",
        "data",
        "Apache",
        "Flume",
        "data",
        "HDFS",
        "analysis",
        "data",
        "pipeline",
        "Kafka",
        "Storm",
        "data",
        "HDFS",
        "business",
        "Custom",
        "UDFs",
        "Hive",
        "Pig",
        "Responsible",
        "Python",
        "wrapper",
        "scripts",
        "date",
        "range",
        "Sqoop",
        "custom",
        "properties",
        "workflow",
        "collections",
        "Python",
        "user",
        "objects",
        "kind",
        "compression",
        "techniques",
        "LZO",
        "GZip",
        "Snappy",
        "configurations",
        "Oozie",
        "bundles",
        "Orchestrating",
        "Pig",
        "Hive",
        "Spark",
        "Sqoop",
        "Apache",
        "Nifi",
        "data",
        "migration",
        "sources",
        "HDFS",
        "destination",
        "Apache",
        "Nifi",
        "flow",
        "conversion",
        "Raw",
        "XML",
        "data",
        "AVRO",
        "streaming",
        "data",
        "Apache",
        "Nifi",
        "Kafka",
        "Environment",
        "RedHat",
        "Linux",
        "MapR",
        "Scala",
        "Python",
        "Rlanguage",
        "Python",
        "HDFS",
        "Hive",
        "Pig",
        "Sqoop",
        "Flume",
        "Oozie",
        "HBase",
        "Spark",
        "Core",
        "Spark",
        "SQL",
        "Spark",
        "Kafka",
        "Tableau",
        "BigData",
        "Developer",
        "Eli",
        "Indianapolis",
        "March",
        "April",
        "Description",
        "Eli",
        "Lilly",
        "Company",
        "company",
        "Indianapolis",
        "Indiana",
        "offices",
        "countries",
        "products",
        "countries",
        "company",
        "Col",
        "Eli",
        "Lilly",
        "chemist",
        "veteran",
        "American",
        "Civil",
        "War",
        "Responsibilities",
        "Interact",
        "Solution",
        "Architects",
        "Business",
        "Analysts",
        "requirements",
        "Solution",
        "Architect",
        "Document",
        "importing",
        "data",
        "data",
        "sources",
        "transformations",
        "Hive",
        "MapReduce",
        "Spark",
        "data",
        "HDFS",
        "Talend",
        "jobs",
        "scheduling",
        "Talend",
        "scheduler",
        "software",
        "development",
        "approach",
        "JIRA",
        "Leveraged",
        "Talend",
        "data",
        "datalake",
        "Unit",
        "Talend",
        "workflows",
        "correctness",
        "data",
        "jobs",
        "HDFS",
        "files",
        "Hive",
        "tables",
        "Views",
        "schema",
        "versions",
        "Hive",
        "tables",
        "partitions",
        "imports",
        "queries",
        "data",
        "jobs",
        "files",
        "HDFS",
        "file",
        "location",
        "Optimizing",
        "Hive",
        "queries",
        "performance",
        "Hive",
        "Query",
        "parameters",
        "ORC",
        "data",
        "format",
        "Apache",
        "Hive",
        "computations",
        "custom",
        "business",
        "requirements",
        "data",
        "RDBMS",
        "MySQL",
        "Oracle",
        "HDFS",
        "vice",
        "Sqoop",
        "Big",
        "Data",
        "ETL",
        "tool",
        "Business",
        "Intelligence",
        "visualization",
        "report",
        "generation",
        "Mapping",
        "source",
        "data",
        "data",
        "JSON",
        "Accord",
        "Format",
        "Talend",
        "data",
        "jobs",
        "HDFS",
        "files",
        "Hive",
        "tables",
        "Views",
        "schema",
        "versions",
        "Hive",
        "tables",
        "partitions",
        "imports",
        "queries",
        "data",
        "jobs",
        "files",
        "HDFS",
        "file",
        "location",
        "Created",
        "Hive",
        "External",
        "tables",
        "data",
        "tables",
        "query",
        "data",
        "HQL",
        "ORC",
        "data",
        "format",
        "Apache",
        "Hive",
        "computations",
        "custom",
        "business",
        "requirements",
        "Data",
        "Lake",
        "Hadoop",
        "data",
        "sources",
        "legacy",
        "code",
        "recommendations",
        "system",
        "Scala",
        "scripts",
        "Data",
        "framesSQL",
        "RDDMapReduce",
        "Spark",
        "Data",
        "Aggregation",
        "data",
        "OLTP",
        "system",
        "Sqoop",
        "SparkStreaming",
        "APIs",
        "transformations",
        "actions",
        "fly",
        "learner",
        "data",
        "model",
        "data",
        "Kafka",
        "time",
        "Persists",
        "HBase",
        "procedures",
        "text",
        "analytics",
        "processing",
        "inmemory",
        "computing",
        "capabilities",
        "Spark",
        "Scala",
        "Stored",
        "MapReduce",
        "program",
        "output",
        "Amazon",
        "S3",
        "script",
        "data",
        "RedShift",
        "dashboard",
        "QlikView",
        "Worked",
        "Talend",
        "Administrator",
        "Console",
        "TAC",
        "scheduling",
        "jobs",
        "users",
        "EnvironmentHive",
        "MapReduce",
        "SparkRDBMSTACData",
        "LakeAmazon",
        "S3RedShiftJSON",
        "XML",
        "Hadoop",
        "Developer",
        "Comcast",
        "Herndon",
        "VA",
        "May",
        "February",
        "Description",
        "Comcast",
        "Corporation",
        "media",
        "technology",
        "company",
        "businesses",
        "Comcast",
        "Cable",
        "NBCUniversal",
        "Comcast",
        "Cable",
        "nations",
        "video",
        "highspeed",
        "internet",
        "phone",
        "providers",
        "customers",
        "XFINITY",
        "brand",
        "services",
        "businesses",
        "Responsibilities",
        "application",
        "demo",
        "client",
        "search",
        "engine",
        "report",
        "analysis",
        "trends",
        "application",
        "administration",
        "prototype",
        "screens",
        "AngularJS",
        "ownership",
        "application",
        "Design",
        "Java",
        "part",
        "Hadoop",
        "integration",
        "requirement",
        "gathering",
        "Business",
        "meeting",
        "client",
        "security",
        "requirements",
        "architect",
        "system",
        "system",
        "Prepared",
        "design",
        "pints",
        "application",
        "flow",
        "documentation",
        "Hadoop",
        "log",
        "Load",
        "sets",
        "data",
        "data",
        "sources",
        "application",
        "Map",
        "Reduce",
        "Programs",
        "cluster",
        "Message",
        "broker",
        "system",
        "Kafka",
        "data",
        "mainframes",
        "feed",
        "KAFKA",
        "HBASE",
        "Analytics",
        "event",
        "link",
        "tracking",
        "system",
        "user",
        "events",
        "feed",
        "KAFKA",
        "HBASE",
        "MapReduce",
        "jobs",
        "contents",
        "HBASE",
        "OOZIE",
        "reports",
        "JAX",
        "RS",
        "web",
        "services",
        "code",
        "apache",
        "CXF",
        "framework",
        "data",
        "SOLR",
        "user",
        "search",
        "documents",
        "SOLR",
        "schema",
        "data",
        "SOLR",
        "data",
        "indexing",
        "Written",
        "MapReduce",
        "programs",
        "data",
        "data",
        "analytics",
        "client",
        "format",
        "Hands",
        "experience",
        "scripts",
        "performance",
        "Storm",
        "builder",
        "topologies",
        "cleansing",
        "operations",
        "data",
        "Cassandra",
        "files",
        "Cassandra",
        "Sqoop",
        "HDFS",
        "Implemented",
        "Bloom",
        "filters",
        "Cassandra",
        "space",
        "creation",
        "Cassandra",
        "CQL",
        "God",
        "experience",
        "concurrency",
        "spark",
        "Cassandra",
        "spark",
        "applications",
        "Scala",
        "Hands",
        "experience",
        "RDDs",
        "transformations",
        "Actions",
        "spark",
        "applications",
        "knowledge",
        "data",
        "frames",
        "Spark",
        "SQL",
        "loading",
        "data",
        "Cassandra",
        "NoSQL",
        "Database",
        "record",
        "level",
        "atomicity",
        "writes",
        "Cassandra",
        "Written",
        "PIG",
        "Scripts",
        "query",
        "Data",
        "patterns",
        "trends",
        "client",
        "criteria",
        "OOZIE",
        "workflows",
        "jobs",
        "MR",
        "jobs",
        "results",
        "HBASE",
        "analysis",
        "data",
        "ingestion",
        "SOLR",
        "indexing",
        "data",
        "integration",
        "search",
        "UI",
        "SOLR",
        "HDFS",
        "code",
        "deployments",
        "integration",
        "tool",
        "Jenkins",
        "challenges",
        "issues",
        "security",
        "system",
        "practices",
        "Project",
        "structures",
        "configurations",
        "project",
        "architecture",
        "developer",
        "work",
        "coordinator",
        "role",
        "work",
        "core",
        "reviews",
        "application",
        "lead",
        "activities",
        "Environment",
        "Java",
        "J2EE",
        "Python",
        "Cassandra",
        "Spring",
        "MVC",
        "HTML5",
        "CSS",
        "services",
        "CXF",
        "web",
        "services",
        "framework",
        "spring",
        "data",
        "SOLR",
        "PIG",
        "HIVE",
        "apache",
        "AVRO",
        "Map",
        "Reduce",
        "Sqoop",
        "Zookeeper",
        "SVN",
        "Jenkins",
        "AD",
        "KDC",
        "Hortonworks",
        "distribution",
        "Hadoop",
        "YARN",
        "Ambari",
        "Java",
        "Developer",
        "Magna",
        "Quest",
        "Technologies",
        "Pvt",
        "LtdHyd",
        "March",
        "January",
        "Description",
        "Magna",
        "Quest",
        "Technologies",
        "trendsetting",
        "Enterprise",
        "Productbased",
        "Solutions",
        "Company",
        "leadership",
        "years",
        "lines",
        "businesses",
        "Responsibilities",
        "Analysis",
        "design",
        "Java",
        "J2EE",
        "Environment",
        "struts",
        "MVC",
        "framework",
        "business",
        "logic",
        "Security",
        "presentation",
        "controller",
        "deployment",
        "environment",
        "Web",
        "system",
        "preferences",
        "UI",
        "screens",
        "JSP",
        "HTML",
        "UI",
        "screens",
        "Swing",
        "components",
        "JLabel",
        "JTable",
        "JScrollPane",
        "JButtons",
        "JTextFields",
        "JDBC",
        "Oracle",
        "database",
        "results",
        "messaging",
        "Java",
        "Message",
        "Service",
        "JMS",
        "Consumed",
        "web",
        "services",
        "SOAP",
        "protocol",
        "web",
        "Components",
        "JSP",
        "Servlets",
        "Serverside",
        "components",
        "EJB",
        "J2EE",
        "Environment",
        "Designing",
        "JSP",
        "Java",
        "Beans",
        "Struts",
        "Action",
        "Controller",
        "classes",
        "request",
        "class",
        "Design",
        "implementation",
        "frontend",
        "web",
        "pages",
        "CSS",
        "DHTML",
        "JavaScript",
        "JSP",
        "HTML",
        "XHTML",
        "JSTL",
        "Ajax",
        "Struts",
        "Tag",
        "Library",
        "table",
        "structure",
        "scripts",
        "tables",
        "indexes",
        "views",
        "sequence",
        "synonyms",
        "database",
        "triggers",
        "Database",
        "procedures",
        "Triggers",
        "PLSQL",
        "statements",
        "data",
        "Developed",
        "Web",
        "users",
        "contents",
        "websites",
        "AOP",
        "IOC",
        "concept",
        "UI",
        "Spring",
        "Framework",
        "Transaction",
        "Management",
        "API",
        "Spring",
        "transactions",
        "Java",
        "objects",
        "WSDL",
        "files",
        "tool",
        "CVS",
        "version",
        "tool",
        "module",
        "developments",
        "Tested",
        "Application",
        "IBM",
        "Web",
        "Sphere",
        "App",
        "Server",
        "Hibernate",
        "ORM",
        "tools",
        "mapping",
        "SQL",
        "databases",
        "objects",
        "Java",
        "Developed",
        "XML",
        "XPDL",
        "BPEL",
        "XML",
        "parsers",
        "DOM",
        "SAX",
        "XSLT",
        "XML",
        "documents",
        "XHTML",
        "PDF",
        "documents",
        "Written",
        "JUnit",
        "test",
        "cases",
        "Business",
        "Objects",
        "code",
        "documentation",
        "reference",
        "upgrades",
        "Deployed",
        "applications",
        "WebSphere",
        "Application",
        "Server",
        "IDE",
        "RAD",
        "Rational",
        "Application",
        "Developer",
        "Environment",
        "Java",
        "J2EE",
        "JSP",
        "Servlets",
        "MVC",
        "Hibernate",
        "Spring",
        "Web",
        "Services",
        "Maven",
        "32x",
        "Eclipse",
        "SOAP",
        "WSDL",
        "EclipsejQuery",
        "Java",
        "Script",
        "Swings",
        "Oracle",
        "REST",
        "API",
        "PLSQL",
        "Oracle",
        "g",
        "UNIX",
        "Java",
        "Developer",
        "Impact",
        "Software",
        "January",
        "February",
        "Description",
        "IMPACT",
        "IT",
        "SOLUTIONS",
        "Expert",
        "IT",
        "Solutions",
        "Company",
        "kind",
        "Web",
        "Solution",
        "Custom",
        "Web",
        "development",
        "Ecommerce",
        "Application",
        "Development",
        "Web",
        "Application",
        "development",
        "Ecommerce",
        "Solution",
        "Development",
        "Webbased",
        "application",
        "Development",
        "web",
        "IT",
        "solutions",
        "variety",
        "clients",
        "India",
        "rest",
        "Globe",
        "Responsibilities",
        "application",
        "Spring",
        "Framework",
        "class",
        "diagrams",
        "sequence",
        "case",
        "diagrams",
        "UML",
        "Rational",
        "Rose",
        "application",
        "J2EE",
        "design",
        "patterns",
        "DAO",
        "RDBMS",
        "web",
        "pages",
        "JSP",
        "HTML",
        "DHTML",
        "JSTL",
        "client",
        "Servlets",
        "JSP",
        "Tag",
        "Libraries",
        "JavaScript",
        "HTML",
        "XML",
        "Struts",
        "Framework",
        "JSP",
        "forms",
        "web",
        "pages",
        "HTML",
        "JSP",
        "applets",
        "JBuilder",
        "Servlets",
        "presentation",
        "business",
        "layer",
        "EJB",
        "middleware",
        "application",
        "Developed",
        "Session",
        "Beans",
        "Entity",
        "beans",
        "business",
        "data",
        "process",
        "JMS",
        "project",
        "messages",
        "queue",
        "Servlets",
        "data",
        "server",
        "views",
        "controllers",
        "client",
        "manager",
        "modules",
        "Spring",
        "MVC",
        "Spring",
        "Core",
        "Spring",
        "Security",
        "web",
        "tier",
        "Access",
        "Business",
        "logic",
        "Hibernate",
        "Developed",
        "database",
        "requirements",
        "Unit",
        "integration",
        "bug",
        "acceptance",
        "testing",
        "test",
        "cases",
        "Code",
        "Interaction",
        "customers",
        "System",
        "Requirements",
        "Software",
        "Requirement",
        "Specifications",
        "Java",
        "design",
        "patterns",
        "development",
        "maintenance",
        "implementation",
        "support",
        "System",
        "project",
        "setup",
        "guidelines",
        "Multithreading",
        "concepts",
        "test",
        "cases",
        "Unit",
        "testing",
        "JUnit",
        "integration",
        "system",
        "testing",
        "presentation",
        "layer",
        "Struts",
        "Framework",
        "JSP",
        "AJAX",
        "XML",
        "XSLT",
        "JavaScript",
        "creation",
        "database",
        "schema",
        "tables",
        "procedures",
        "triggers",
        "Oracle",
        "SQLPLSQL",
        "Environment",
        "Java",
        "J2EE",
        "JSP",
        "CSS",
        "JavaScript",
        "AJAX",
        "Hibernate",
        "Spring",
        "XML",
        "EJB",
        "Web",
        "Services",
        "SOAP",
        "Eclipse",
        "Rational",
        "Rose",
        "HTML",
        "XPATH",
        "XSLT",
        "DOM",
        "JDBC",
        "IntelliJ",
        "Eclipse",
        "NetBeans",
        "Development",
        "Methodologies",
        "AgileScrum",
        "UML",
        "Design",
        "Patterns",
        "Waterfall",
        "Build",
        "Tools",
        "Jenkins",
        "Toad",
        "SQL",
        "Loader",
        "Maven",
        "ANT",
        "RTC",
        "RSA",
        "ControlM",
        "Oziee",
        "Hue",
        "SOAP",
        "UI",
        "Reporting",
        "Tools",
        "MS",
        "Office",
        "WordExcelPower",
        "Point",
        "VisioOutlook",
        "Crystal",
        "XI",
        "SSRS",
        "cognos",
        "Microsoft",
        "SQL",
        "Server",
        "MySQL",
        "Oracle",
        "g",
        "DB2",
        "Teradata",
        "Netezza",
        "Operating",
        "Systems",
        "versions",
        "Windows",
        "UNIX",
        "LINUX",
        "Macintosh",
        "HD",
        "Sun",
        "Solaris",
        "Skills",
        "Sql",
        "Cassandra",
        "Hdfs",
        "Impala",
        "Mapreduce",
        "Oozie",
        "Sqoop",
        "Hbase",
        "Kafka",
        "Flume",
        "Hadoop",
        "Mongodb",
        "Splunk",
        "Tableau",
        "server",
        "C",
        "Fortran",
        "Hadoop",
        "Hbase",
        "Hive",
        "Additional",
        "Information",
        "TECHNICAL",
        "SKILLS",
        "BigdataHadoop",
        "Technologies",
        "Hadoop",
        "HDFS",
        "YARN",
        "MapReduce",
        "Hive",
        "Pig",
        "Impala",
        "Sqoop",
        "Flume",
        "Spark",
        "Kafka",
        "Storm",
        "Drill",
        "Zookeeper",
        "Oozie",
        "Languages",
        "HTML5",
        "DHTML",
        "WSDL",
        "CSS3",
        "C",
        "C",
        "XML",
        "RR",
        "Studio",
        "SAS",
        "Enterprise",
        "Guide",
        "SAS",
        "R",
        "Caret",
        "Weka",
        "ggplot",
        "Perl",
        "MATLAB",
        "Mathematica",
        "FORTRAN",
        "DTD",
        "Schemas",
        "JSON",
        "Ajax",
        "Java",
        "Scala",
        "Python",
        "NumPy",
        "SciPy",
        "Pandas",
        "Java",
        "Script",
        "Shell",
        "Scripting",
        "SQL",
        "Databases",
        "Cassandra",
        "HBase",
        "MongoDB",
        "MariaDB",
        "Business",
        "Intelligence",
        "Tools",
        "Tableau",
        "server",
        "Tableau",
        "Reader",
        "Tableau",
        "Splunk",
        "SAP",
        "Business",
        "OBIEE",
        "SAP",
        "Business",
        "Intelligence",
        "QlikView",
        "Amazon",
        "Redshift",
        "Azure",
        "Data",
        "Warehouse"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T22:19:45.692267",
    "resume_data": "Bigdata Developer Bigdata span lDeveloperspan Bigdata Developer Brillant Edison NJ Over 5 Years of professional IT experience in analysis architectural design prototyping development Integration and testing of applications using JavaJ2EE Technologies and Good Working experience in Big Data Technologies Experience in developing Map Reduce Programs using Apache Hadoop for analyzing the big data as per the requirement Experienced on major Hadoop ecosystems projects such as Pig Hive HBase and monitoring them with Cloudera Manager Extensive experience in developing Pig Latin Scripts and using Hive Query Language for data analytics Hands on experience working on NoSQL databases including HBase Cassandra and its integration with Hadoop cluster Experience in implementing Spark Scala application using higher order functions for both batch and interactive analysis requirement Good working experience using Sqoop to import data into HDFS from RDBMS and viceversa Good knowledge in using job scheduling and monitoring tools like Oozie and Zookeeper Experience in Hadoop administration activities such as installation and configuration of clusters using Apache Cloudera and AWS Experienced in designing built and deploying a multitude application utilizing almost all the AWS stack Including EC2 R53 S3 RDS DynamoDB SQS IAM and EMR focussing on highavailability fault tolerance and autoscaling Developed UML Diagrams for Object Oriented Design Use Cases Sequence Diagrams and Class Diagrams using Rational Rose Visual Paradigm and Visio Hands on experience in solving software design issues by applying design patterns including Singleton Pattern Business Delegator Pattern Controller Pattern MVC Pattern Factory Pattern Abstract Factory Pattern DAO Pattern and Template Pattern Developed Webbased applications using Python Amazon Web Services jQuery CSS and Model View control frameworks like Django Flask and JavaScript Good experience with design coding debug operations reporting and data analysis utilizing python and using python libraries to speed up development Hands on experience with Bid Data environment on technologies including Hadoop Experienced in creative and effective frontend development using JSP JavaScript HTML 5 DHTML XHTML Ajax and CSS Good Working experience in using different Spring modules like Spring Core Container Module Spring Application Context Module Spring MVC Framework module Spring ORM Module in Web applications Used jQuery to select HTML elements to manipulate HTML elements and to implement AJAX in Web applications Used available plugins for extension of jQuery functionality Working knowledge of database such as Oracle10g11g12c Microsoft SQL Server DB2 Experience in writing numerous test cases using JUnit framework with Selenium Strong experience in database design writing complex SQL Queries and Stored Procedures Experienced in using Version Control Tools like Subversion Git Experience in Building Deploying and Integrating with Ant Maven Experience in development of logging standards and mechanism based on Log4J Strong problemsolving skills good communication interpersonal skills and a good team player Have the motivation to take independent responsibility as well as ability to contribute and be a productive team member Work Experience Bigdata Developer Brillant Edison NJ May 2018 to Present Description Brilliant Understand concepts and build your problem solving skills with thousands of free problems and examples in math science and engineering Responsibilities Solid understanding and experience in applying and implementing machine learning algorithms and concepts such as Classification and Regression Resampling statistics and bootstrapping using R language Experience in working with Hadoop 2x version and Spark 2x Python and Scala Extending HIVE and PIG core functionality by using custom User Defined Functions UDF User Defined TableGenerating Functions UDTF and User Defined Aggregating Functions UDAF in Java Experience in Hadoop Production support tasks by analysing the Application and cluster logs Implemented Partitioning Dynamic Partitions and Buckets in Hive on Avro files to meet the business requirements Expertise in designing and deployment of Hadoop cluster and different Big Data analytic tools including Pig Hive HBase Oozie Zookeeper SQOOP flume Spark Kafka HBase with MapR Distribution Assist in upgrading configuration and maintenance of various Hadoop infrastructures like Pig Hive and HBase Used Spark for interactive queries processing of streaming data and integration with NoSQL database for huge volume of data Developed Scala scripts using both Data framesSQL and RDDMapReduce in Spark 1x2x for Data Aggregation queries and writing data back into OLTP system through Sqoop Used SparkStreaming APIs to perform necessary transformations and actions on the fly for building the common learner data model which gets the data from Kafka in near real time and Persists into HBase Performed advanced procedures like text analytics and processing using the inmemory computing capabilities of Spark using Scala Experienced in handling large datasets using Partitions Spark in Memory capabilities Broadcasts in Spark Effective efficient Joins Transformations and other during ingestion process itself Worked on migrating Map Reduce programs into Spark transformations using Spark and Scala Coordinate with Administration team to enhance Spark Jobs performance by analyzing them Implemented design patterns in Scala for the Spark application Developed quality code adhering to Scala coding Standards and best practices Used Spark API over MapR Hadoop YARN to perform analytics on data in Hive Explored with the Spark improving the performance and optimization of the existing algorithms in Hadoop using Spark Context SparkSQL Data Frame Pair RDDs Spark YARN Worked on Loading log data into HDFS using Flume Kafka and performing ETL integrations Used Reporting Tool Tableau to connect with Hive for generating daily reports of data Collaborated with the infrastructure network database application and BA teams to ensure data quality Worked with different File Formats like TEXTFILE SEQUENCE FILE AVROFILE ORC and PARQUET for Hive querying and processing Developed Spark code using Scala and SparkSQLStreaming for faster testing and processing of data Import the data from different sources like HDFSHBase into Spark RDD Load the data into Spark RDD and do in memory data Computation to generate the Output response Experience in Oozie and workflow scheduler to manage Hadoop jobs by Direct Acyclic Graph DAG of actions with control flows Configure Oozie workflow to run multiple Hive and Pig jobs which run independently with time and data availability Collected and aggregated large amounts of log data using Apache Flume and staging data in HDFS for further analysis Developed a data pipeline using Kafka and Storm to store data into HDFS Developed business specific Custom UDFs in Hive Pig Responsible for developing Python wrapper scripts which will extract specific date range using Sqoop by passing custom properties required for the workflow Skilled in using collections in Python for manipulating and looping through different user defined objects Worked with different kind of compression techniques like LZO GZip Snappy Worked on various configurations of Oozie bundles for Orchestrating Pig Hive Spark Sqoop Utilized for Apache Nifi data migration from various sources to HDFS destination Worked with the Apache Nifi flow to perform the conversion of Raw XML data into JSON AVRO Ingested streaming data Apache Nifi with into Kafka Environment RedHat Linux MapR Scala Python Rlanguage Python HDFS Hive Pig Sqoop Flume Oozie HBase Spark Core Spark SQL Spark streaming Kafka Tableau BigData Developer Eli LillyCO Indianapolis IN March 2017 to April 2018 Description Eli Lilly and Company is a global pharmaceutical company headquartered in Indianapolis Indiana with offices in 18 countries Its products are sold in approximately 125 countries The company was founded in 1876 by and named after Col Eli Lilly a pharmaceutical chemist and veteran of the American Civil War Responsibilities Interact with Solution Architects and Business Analysts to gather requirements and update Solution Architect Document Handled importing of data from various data sources performed transformations using Hive MapReduce Spark and loaded data into HDFS Submitted Talend jobs for scheduling using Talend scheduler Extensively Worked in Agile software development approach using JIRA Leveraged Talend to ingest data into the datalake of datafabric Unit tested Talend workflows for the correctness of the data Developed jobs to expose HDFS files to Hive tables and Views depending up on the schema versions Created Hive tables partitions and implemented incremental imports to perform adhoc queries on structured data Developed jobs to move inbound files to HDFS file location based on monthly weekly daily and hourly partitioning Optimizing Hive queries improve performance by configuring Hive Query parameters Implemented ORC data format for Apache Hive computations to handle the custom business requirements Imported data from RDBMS MySQL Oracle to HDFS and vice versa using Sqoop Big Data ETL tool for Business Intelligence visualization and report generation Mapping source to target data and converted data JSON to XML 229 Accord Format using Talend data mapper Developed jobs to expose HDFS files to Hive tables and Views depending up on the schema versions Created Hive tables partitions and implemented incremental imports to perform adhoc queries on structured data Developed jobs to move inbound files to HDFS file location based on monthly weekly daily and hourly partitioning Created Hive External tables and loaded the data in to tables and query data using HQL Implemented ORC data format for Apache Hive computations to handle the custom business requirements Implemented a centralized Data Lake in Hadoop with data from various sources Analyzing and understanding the legacy code and making recommendations on how the new system can be designed Developed Scala scripts using both Data framesSQL and RDDMapReduce in Spark 1x2x for Data Aggregation queries and writing data back into OLTP system through Sqoop Used SparkStreaming APIs to perform necessary transformations and actions on the fly for building the common learner data model which gets the data from Kafka in near real time and Persists into HBase Performed advanced procedures like text analytics and processing using the inmemory computing capabilities of Spark using Scala Stored MapReduce program output in Amazon S3 and developed a script to move the data to RedShift for generating a dashboard using QlikView Worked on Talend Administrator Console TAC for scheduling jobs and adding users EnvironmentHive MapReduce SparkRDBMSTACData LakeAmazon S3RedShiftJSON XML Hadoop Developer Comcast Herndon VA May 2016 to February 2017 Description Comcast Corporation is a global media and technology company with two primary businesses Comcast Cable and NBCUniversal Comcast Cable is one of the nations largest video highspeed internet and phone providers to residential customers under the XFINITY brand and provides these services to businesses Responsibilities Provided application demo to the client by designing and developing search engine report analysis trends application administration prototype screens using AngularJS and BootstrapJS Took the ownership of complete application Design of Java part Hadoop integration Apart from the normal requirement gathering participated in Business meeting with the client to gather security requirements Assisted with the architect to analyze the existing system and future system Prepared design blue pints and application flow documentation Experienced in managing and reviewing Hadoop log files Load and transform large sets of structured semi structured and unstructured data Responsible to manage data coming from different sources and application Supported Map Reduce Programs those are running on the cluster Responsible in working with Message broker system such as Kafka Extracted data from mainframes and feed to KAFKA and ingested to HBASE to perform Analytics Written event driven link tracking system to capture user events and feed to KAFKA to push it to HBASE Created MapReduce jobs to extracts the contents from HBASE and configured in OOZIE workflow to generate analytical reports Developed the JAX RS web services code using apache CXF framework to fetch data from SOLR when user performed the search for documents Participated in SOLR schema and ingested data into SOLR for data indexing Written MapReduce programs to organize the data and ingest the data to suitable for analytics in client specified format Hands on experience in writing python scripts to optimize the performance Implemented Storm builder topologies to perform cleansing operations before moving data into Cassandra Extracted files from Cassandra through Sqoop and placed in HDFS and processed Implemented Bloom filters in Cassandra using key space creation Involved in writing Cassandra CQL statements God hands on experience in developing concurrency using spark and Cassandra together Involved in writing spark applications using Scala Hands on experience in creating RDDs transformations and Actions while implementing spark applications Good knowledge in creating data frames using Spark SQL Involved in loading data into Cassandra NoSQL Database Implemented record level atomicity on writes using Cassandra Written PIG Scripts to query and process the Data sets to figure out the patterns of trends by applying client specific criteria and configured OOZIE workflows to run the jobs along with the MR jobs Stored the derived the results in HBASE from analysis and make it available to data ingestion for SOLR for indexing data Involved in integration of java search UI SOLR and HDFS Involved in code deployments using continuous integration tool using Jenkins Documented all the challenges issues involved to deal with the security system and Implemented best practices Created Project structures and configurations according to the project architecture and made it available to the junior developer to continue their work Handled onsite coordinator role to deliver work to offshore Involved in core reviews and application lead supported activities Environment Java J2EE Python Cassandra Spring 32 MVC HTML5 CSS AngularJS Restful services using CXF web services framework spring data SOLR 521 PIG HIVE apache AVRO Map Reduce Sqoop Zookeeper SVN Jenkins windows AD windows KDC Hortonworks distribution of Hadoop 23 YARN Ambari Java Developer Magna Quest Technologies Pvt LtdHyd March 2015 to January 2016 Description Magna Quest Technologies limited is a trendsetting and fascinating innovative Enterprise Productbased Solutions Company which has established its leadership over 15 years in three broad lines of businesses Responsibilities Involved in Analysis design and coding on Java and J2EE Environment Implemented struts MVC framework Designed developed and implemented the business logic required for Security presentation controller Set up the deployment environment on Web Logic Developed system preferences UI screens using JSP and HTML Developed UI screens using Swing components like JLabel JTable JScrollPane JButtons JTextFields etc Used JDBC to connect to Oracle database and get the results that are required Designed asynchronous messaging using Java Message Service JMS Consumed web services through SOAP protocol Developed web Components using JSP Servlets and Serverside components using EJB under J2EE Environment Designing JSP using Java Beans Implemented Struts framework 20 Action and Controller classes for dispatching request to appropriate class Design and implementation of frontend web pages using CSS DHTML JavaScript JSP HTML XHTML JSTL Ajax and Struts Tag Library Designed table structure and coded scripts to create tables indexes views sequence synonyms and database triggers Involved in writing Database procedures Triggers PLSQL statements for data retrieving Developed using Web 20 to interact with other users and changing the contents of websites Implemented AOP and IOC concept using UI Spring 20 Framework Developed using Transaction Management API of Spring 20 and coordinates transactions for Java objects Generated WSDL files using AXIS2 tool Developed using CVS as a version controlling tool for managing the module developments Configured and Tested Application on the IBM Web Sphere App Server Used Hibernate ORM tools which automate the mapping between SQL databases and objects in Java Developed using XML XPDL BPEL and XML parsers like DOM SAX Developed using XSLT to convert XML documents into XHTML and PDF documents Written JUnit test cases for Business Objects and prepared code documentation for future reference and upgrades Deployed applications using WebSphere Application Server and Used IDE RAD Rational Application Developer Environment Java J2EE JSP Servlets MVC Hibernate Spring 30 Web Services Maven 32x Eclipse SOAP WSDL EclipsejQuery Java Script Swings Oracle REST API PLSQL Oracle 11g UNIX Java Developer Impact Software SolutionsHYD January 2014 to February 2015 Description IMPACT IT SOLUTIONS is an Expert IT Solutions Company providing all kind of Web Solution like Custom Web development Ecommerce Application Development Web Application development Ecommerce Solution Development Webbased application Development web webbased IT solutions to a wide variety of clients across India rest of the Globe Responsibilities Designed developed the application using Spring Framework Developed class diagrams sequence and use case diagrams using UML Rational Rose Designed the application with reusable J2EE design patterns Designed DAO objects for accessing RDBMS Developed web pages using JSP HTML DHTML and JSTL Designed and developed a webbased client using Servlets JSP Tag Libraries JavaScript HTML and XML using Struts Framework Involved in developing JSP forms Designed and developed web pages using HTML and JSP Designed various applets using JBuilder Designed and developed Servlets to communicate between presentation and business layer Used EJB as a middleware in developing a threetier distributed application Developed Session Beans and Entity beans for business and data process Used JMS in the project for sending and receiving the messages on the queue Developed the Servlets for processing the data on the server Developed views and controllers for client and manager modules using Spring MVC and Spring Core Used Spring Security for securing the web tier Access Business logic is implemented using Hibernate Developed and modified database objects as per the requirements Involved in Unit integration bug fixing acceptance testing with test cases Code reviews Interaction with customers and identified System Requirements and developed Software Requirement Specifications Implemented Java design patterns wherever required Involved in development maintenance implementation and support of the System Involved in initial project setup and guidelines Implemented Multithreading concepts Developed test cases for Unit testing using JUnit and performed integration and system testing Involved in coding for the presentation layer using Struts Framework JSP AJAX XML XSLT and JavaScript Closely worked and supported the creation of database schema objects tables stored procedures and triggers using Oracle SQLPLSQL Environment Java J2EE JSP CSS JavaScript AJAX Hibernate Spring XML EJB Web Services SOAP Eclipse Rational Rose HTML XPATH XSLT DOM and JDBC IntelliJ Eclipse NetBeans 2010 to 2012 Development Methodologies AgileScrum UML Design Patterns Waterfall Build Tools Jenkins Toad SQL Loader Maven ANT RTC RSA ControlM Oziee Hue SOAP UI Reporting Tools MS Office WordExcelPower Point VisioOutlook Crystal reports XI SSRS cognos 7060 Databases Microsoft SQL Server 200820102012 MySQL 4x5x Oracle 11g 12c DB2 Teradata Netezza Operating Systems All versions of Windows UNIX LINUX Macintosh HD Sun Solaris Skills Sql Cassandra Hdfs Impala Mapreduce Oozie Sqoop Hbase Kafka Flume Hadoop Mongodb Splunk Tableau server C Dtd Fortran Hadoop Hbase Hive Additional Information TECHNICAL SKILLS BigdataHadoop Technologies Hadoop HDFS YARN MapReduce Hive Pig Impala Sqoop Flume Spark Kafka Storm Drill Zookeeper and Oozie Languages HTML5 DHTML WSDL CSS3 C C XML RR Studio SAS Enterprise Guide SAS R Caret Weka ggplot Perl MATLAB Mathematica FORTRAN DTD Schemas JSON Ajax Java Scala Python NumPy SciPy Pandas Java Script Shell Scripting NO SQL Databases Cassandra HBase MongoDB MariaDB Business Intelligence Tools Tableau server Tableau Reader Tableau Splunk SAP Business Objects OBIEE SAP Business Intelligence QlikView Amazon Redshift or Azure Data Warehouse",
    "unique_id": "b9f9a4e9-9c09-499f-af25-8f099b0727bc"
}