{
    "clean_data": "Data Resarcher Data Scientist Data Resarcher Data Scientist Data Resarcher Data Scientist USAA San Antonio TX Professional qualified Data ScientistData Analyst with experience in Data Science and Analytics including Data Mining Deep LearningMachine Learning and Statistical Analysis Involved in the entire data science project life cycle and actively involved in all the phases including data cleaning data extractionand datavisualization with large data sets of structured and unstructured data created ER diagrams and schema Experienced with machine learning algorithm such as logistic regression KNN SVM random forest neural network linear regression lasso regression and kmeans Implemented Bagging and Boosting to enhance the model performance Experience in implementing data analysis with various analytic tools such as Anaconda Jupiter Notebook 4X R 30 ggplot2 dplyr Caret and Excel Solid ability to write and optimize diverse SQL queries working knowledge of RDBMS like SQL Server 12 NoSql databases like MongoDB 32 Excellent understanding Agile and Scrum development methodology Used the version control tools like Git 2X and build tools like Apache MavenAnt Ability to maintain a fun casual professional and productive team atmosphere Experienced the full software life cycle in SDLC Agile DevOps and Scrum methodologies including creating requirements test plans Skilled in Advanced Regression Modeling Correlation Multivariate Analysis Model Building Business Intelligence tools and application of Statistical Concepts Developed predictive models using Decision Tree Naive Bayes Logistic Regression Random Forest Social Network Analysis Cluster Analysis and Neural Networks Experienced in Machine Learning and Statistical Analysis with Python ScikitLearn Experienced in Python to manipulate data for data loading and extraction and worked with python libraries like Matplotlib Scipy Numpy and Pandas for data analysis Strong SQL programming skills with experience in working with functions packages and triggers Expertise in transforming business requirements into designing algorithms analytical models building models developing data mining and reporting solutions that scales across massive volume of structured and unstructured data Skilled in performing data parsing data manipulation data architecture data ingestion and data preparation with methods including describe data contents compute descriptive statistics of data regex split and combine merge Remap subset reindex melt and reshape Worked with NoSQLDatabase including Hbase Cassandra and MongoDB Experienced in Big Data with Hadoop MapReduce HDFS and Spark Experienced in Data Integration Validation and Data Quality controls for ETL process and Data Warehousing using MS Visual Studio SSAS SSISand SSRS Proficient in Tableau and RShiny data visualization tools to analyze and obtain insights into large datasets create visually powerful and actionable interactive reports and dashboards Automated recurring reports using SQL andPython and visualized them on BI platform like Tableau Worked in development environment like Git and VM Excellent communication skills Successfully working in fastpaced multitasking environment both independently and in collaborative team a selfmotivated enthusiastic learner Work Experience Data Resarcher Data Scientist USAA San Antonio TX April 2019 to Present Description The United Services Automobile Association USAA is a San Antonio Texasbased Fortune 500 diversified financial services group of companies including a Texas Department of Insuranceregulated reciprocal interinsurance exchange and subsidiaries offering banking investing and insurance to people and families who serve or served in the United States Armed ForcesAt the end of 2017 there were 124 million members Responsibilities Test and determine whether new technology is potentially useful for USAA Extracting Opearations data from Workday and storing it in Hadoop Getting the data in Pickle file Format and Parsing it by CSV Build a graphic database using DataStax in Python ObjectOriented Programming Query datainformation from graphic database using Gremlin to support model validation Work in a group to develop Machine Learning guidance for model development and validation Build analytic tools to prepare data and graphs using Power Query and GraphX Draft the procedures for reporting Worked on ETL tools such as Apache Airflow Build a auto DAG system which would automatically trigger the workflow whenever the Airflow will receive HTTP request Write python scripts to parse documents Take online courses as a means to continuously learn new subjects Did intensive research on the tools like Graph Networks Scheduling Tools and Data Wrangling Tools to determine the best tool available in the market that would be best fit for companys need Environment Windows Python 3 DataStax GraphX Apache Airflow SQL Data Scientist Machine Learning Rauxa New York NY August 2018 to March 2019 Description Makers of results Rauxa applies data technology and content to create measurable impact at maximum speed for clients that include Gap Inc TGI Fridays and Verizon The countrys largest womanowned independent advertising agency Responsibilities Built models using Statistical techniques like Bayesian HMM and Machine Learning classification models like XGBoost SVM and Random Forest Participated in all phases of data mining data cleaning data collection developing models validation visualization and performed Gap analysis A highly immersive Data Science program involving Data ManipulationVisualization Web Scraping Machine Learning Python programming SQL GIT MongoDB Hadoop Setup storage and data analysis tools in AWS cloud computing infrastructure Installed and used Tensorflow a Deep Learning Framework Used pandas numpy seaborn matplotlib scikitlearn scipy NLTK in Python for developing various machine learning algorithms Data Manipulation and Aggregation from different source using Nexus Business Objects Toad Power BI and Smart View Programmed a utility in Python that used multiple packages numpy scipy pandas Implemented Classification using supervised algorithms like Logistic Regression Decision trees Naive Bayes KNN As Architect delivered various complex OLAP databasescubes scorecards dashboards and reports Updated Python scripts to match training data with our database stored in AWS Cloud Search so that we would be able to assign each document a response label for further classification Data transformation from various resources data organization features extraction from raw and stored Validated the machine learning classifiers using ROC Curves and Lift Charts Environment Unix Python 352 MLLib SAS regression logistic regression Hadoop 274 NoSQL Teradata OLTP random forest OLAP HDFS ODS NLTK SVM JSON XML and MapReduce Data Scientist Charter communication St Louis MO May 2017 to July 2018 Description Charter Communications Inc is an American telecommunications and mass media company that offers its services to consumers and businesses under the branding of Spectrum Responsibilities Utilized Scala Hadoop pySpark Data Lake TensorFlow MongoDB AWS Python a broad variety of machine learning methods including classifications regressions dimensionally reduction etc Developed sentiment analysis framework for email conversation and Customer Satisfaction scoreCSAT correlation Application of various machine learning algorithms and statistical modeling like decision trees text analytics natural language processing NLP supervised and unsupervised regression models social network analysis neural networks deep learning SVM clustering to identify Volume using scikitlearn package in python Matlab Sentiment Analytics engine was completely built in Python3 Email chain was considered for analysis Emails were pulled into DB from a tool Email2DB Each email transactions was preprocessed sentiment calculation subjectivity polarity was taken and linked to the CSAT score for dashboarding Emite was used as a tool of choice for visualization RapidMiner and PredictionIO was used as tool of choice for predictive analysis ITSM data was taken and build a engine for predicting CPU failure Memory Issue Disk Space Issue Server failure Errors Training data was build from either of proactive and reactive ticket data which was in turn to be used to make Classification Model for prediction Identifying and evaluating potential vendors for technology fitments performing proof of value exercise conducting commercial and contract negotiations Performed Data Profiling to learn about behavior with various features such as traffic pattern location Date and Time etc Categorized comments into positive and negative clusters from different social networking sites using Sentiment Analysis and Text Analytics Performed Multinomial Logistic Regression Decision Tree Random forest SVM to classify package is going to deliver on time for the new route Performed data analysis by using Hive to retrieve the data from Hadoop cluster Sql to retrieve datafrom Oracle database and used ETL for data transformation Performed Data Cleaning features scaling features engineering using pandas and numpy packages in python Exploring DAGs their dependencies and logs using AirFlow pipelines for automation Performed data cleaning and feature selection using MLlib package in PySpark and working with deep learning frameworks such as Tensorflow and PyTorch Developed SparkScalaR Python for regular expression regex project in the HadoopHive environment with LinuxWindows for big data resources Used clustering technique KMeans to identify outliers and to classify unlabeled data Communicated the results with operations team for taking best decisions Collected data needs and requirements by Interacting with the other departments Environment Python 2x CDH5 HDFS Hadoop 23 Hive Impala AWS Linux Spark Tableau Desktop SQL Server 2014 Microsoft Excel Matlab Spark SQL Pyspark Data analyst EDAC Technologies Corp Cheshire CT January 2016 to April 2017 Description EDAC Technologies Corporation provides design manufacturing and services for tooling fixtures molds jet engine components and machine spindles in the aerospace industrial semiconductor and medical device markets Responsibilities Worked with BI team in gathering the report requirements and also Sqoop to export data into HDFS and Hive Involved in the below phases of Analytics using R Python and Jupyter notebook Data collection and treatment Analysed existing internal data and external data worked on entry errorsclassification errors and defined criteria for missing values Data Mining Used cluster analysis for identifying customer segments Decision trees used for profitable and nonprofitable customers Market Basket Analysis used for customer purchasing behaviour and partproduct association Emite was used as a tool of choice for visualization Assisted with data capacity planning and node forecasting Installed Configured and managed Flume Infrastructure Worked closely with the claims processing team to obtain patterns in filing of fraudulent claims Worked on performing major upgrade of cluster from CDH3u6 to CDH440 Developed Map Reduce programs to extract and transform the data sets and results were exported back to RDBMS using Sqoop Patterns were observed in fraudulent claims using text mining in R and Hive Exported the data required information to RDBMS using Sqoop to make the data available for the claims processing team to assist in processing a claim based on the data Developed Map Reduce programs to parse the raw data populate staging tables and store the refined data in partitioned tables in the EDW Created tables in Hive and loaded the structured resulted from Map Reduce jobs data Created Hive queries that helped market analysts spot emerging trends by comparing fresh data with EDW reference tables and historical metrics Enabled speedy reviews and first mover advantages by using Oozie to automate data loading into the Hadoop Distributed File System and PIG to preprocess the data Provided design recommendations and thought leadership to sponsorsstakeholders that improved review processes and resolved technical problems Managed and reviewed Hadoop log files Tested raw data and executed performance scripts Environment HDFS PIG HIVE Map Reduce Linux HBase Flume Sqoop R VMware Eclipse Cloudera Python Data Analyst Dorman Products Inc Colmar PA March 2014 to December 2015 Description Dorman Products Inc supplies automotive replacement parts automotive hardware and brake products to the automotive aftermarket and mass merchandise markets in the United States Canada Mexico Europe the Middle East and Australia Responsibilities Created and maintained Logical and Physical models for the data mart Created partitions and indexes for the tables in the data mart Performed data profiling and analysis applied various data cleansing rules designed data standards and architecturedesigned the relational models Maintained metadata data definitions of table structures and version controlling for the data model Developed SQL scripts for creating tables Sequences Triggers views and materialized views Worked on query optimization and performance tuning using SQL Profiler and performance monitoring Developed mappings to load Fact and Dimension tables SCD Type 1 and SCD Type 2 dimensions and Incremental loading and unit tested the mappings Utilized Erwins forward reverse engineering tools and target database schema conversion process Worked on creating enterprise wide Model EDM for products and services in Teradata Environment based on the data from PDM Conceived designed developed and implemented this model from the scratch Building publishing customized interactive reports and dashboards report scheduling using Tableau server Write SQL scripts to test the mappings and Developed Traceability Matrix of Business Requirements mapped to Test Scripts to ensure any Change Control in requirements leads to test case update Responsible for development and testing of conversion programs for importing Data from text files into map Oracle Database utilizing PERL shell scripts SQLLoader Involved in extensive DATA validation by writing several complex SQL queries and Involved in backend testing and worked with data quality issues Developed and executed load scripts using Teradata client utilities MULTILOAD FASTLOAD and BTEQ Exporting and importing the data between different platforms such as SAS MSExcel Generated periodic reports based on the statistical analysis of the data using SQL Server Reporting Services SSRS Worked with the ETL team to document the Transformation Rules for Data Migration from OLTP to Warehouse Environment for reporting purposes Created SQL scripts to find data quality issues and to identify keys data anomalies and data validation issues Formatting the data sets read into SAS by using Format statement in the data step as well as Proc Format Applied Business Objects best practices during development with a strong focus on reusability and better performance Developed Tableau visualizations and dashboards using Tableau Desktop Used Graphical Entity Relationship Diagramming to create new database design via easy to use graphical interface Designed different type of STAR schemas for detailed data marts and plan data marts in the OLAP environment Environment Erwin MS SQL Server 2008 DB2 Oracle SQL Developer PLSQL Business Objects Erwin MS office suite Windows XP TOAD SQLPLUS SQLLOADER Teradata Netezza SAS Tableau Business Objects SSRS tableau SQL Assistant Informatica XML Python Developer Dabur India Ltd Ghaziabad Uttar Pradesh December 2012 to February 2014 Description Dabur is one of the Indias largest Ayurvedic medicine natural consumer products manufacturer Dabur demerged its Pharma business in 2003 and hived it off into a separate company Dabur Pharma Ltd German company Fresenius SE bought a 7327 equity stake in Dabur Pharma in June 2008 at Rs 7650 a share Responsibilities Involved in the design development and testing phases of application using AGILE methodology Designed and maintained databases using Python and developed Python based API RESTful Web Service using Flask SQLAlchemy and PostgreSQL Designed and developed the UI of the website using HTML XHTML AJAX CSS and JavaScript Participated in requirement gathering and worked closely with the architect in designing and modeling Worked on Restful web services which enforced a stateless client server and support JSON few changes from SOAP to RESTFUL Technology Involved in detailed analysis based on the requirement documents Involved in writing SQL queries implementing functions triggers cursors object types sequences indexes etc Created and managed all of hosted or local repositories through Source Trees simple interface of GIT client collaborated with GIT command lines and Stash Responsible for setting up Python REST API framework and spring frame work using Django Develope consumer based features and applications using Python Django HTML behavior Driven Development BDD and pair based programming Designed and developed components using Python with Django framework Implemented code in python to retrieve and manipulate data Involved in development of the enterprise social network application using Python Twisted and Cassandra Used Python and Django creating graphics XML processing of documents data exchange and business logic implementation between servers orked closely with backend developer to find ways to push the limits of existing Web technology Designed and developed the UI for the website with HTML XHTML CSS Java Script and AJAX Used AJAXJSON communication for accessing RESTfulweb services data payload Designed dynamic clientside JavaScript codes to build web forms and performed simulations for web application page Created and implemented SQL Queries Stored procedures Functions Packages and Triggers in SQL Server Successfully implemented Auto CompleteAuto Suggest functionality using JQuery Ajax Web Service and JSON Environment Python 25 JavaJ2EE Django10 HTMLCSS Linux Shell Scripting Java Script Ajax JQuery JSON XML PostgreSQL Jenkins ANT Maven Subversion Python Data Analyst Kotak Mahindra Bank Mumbai Maharashtra January 2011 to November 2012 Description Kotak Mahindra Bank is an Indian private sector bank headquartered in Mumbai Maharashtra India In February 2003 Reserve Bank of India issued the licence to Kotak Mahindra Finance Ltd the groups flagship company to carry on banking business Responsibilities Analyzed data sources and requirements and business rules to perform logical and physical data modeling Analyzed and designed best fit logical and physical data models and relational database definitions using DB2 Generated reports of data definitions Involved in NormalizationDenormalization Normal Form and database design methodology Maintained existing ETL procedures fixed bugs and restored software to production environment Developed the code as per the clients requirements using SQL PLSQL and Data Warehousing concepts Involved in Dimensional modeling Star Schema of the Data warehouse and used Erwin to design the business process dimensions and measured facts Worked with Data Warehouse Extract and load developers to design mappings for Data Capture Staging Cleansing Loading and Auditing Developed enterprise data model management process to manage multiple data models developed by different groups Designed and created Data Marts as part of a data warehouse Wrote complex SQL queries for validating the data against different kinds of reports generated by Business Objects XIR2 Using Erwin modeling tool publishing of a data dictionary review of the model and dictionary with subject matter experts and generation of data definition language Coordinated with DBA in implementing the Database changes and also updating Data Models with changes implemented in development QA and Production Worked Extensively with DBA and Reporting team for improving the Report Performance with the Use of appropriate indexes and Partitioning Developed Data Mapping Transformation and Cleansing rules for the Master Data Management Architecture involved OLTP ODS and OLAP Tuned and coded optimization using different techniques like dynamic SQL dynamic cursors and tuning SQL queries writing generic procedures functions and packages Experienced in GUI Relational Database Management System RDBMS designing of OLAP system environment as well as Report Development Extensively used SQL TSQL and PLSQL to write stored procedures functions packages and triggers Analyzed of data report were prepared weekly biweekly monthly using MS Excel SQL UNIX Environment ER Studio Informatica Power Center 8191 Power Connect Power exchange Oracle 11g MainframesDB2 MS SQL Server 2008 SQLPLSQL XML Windows NT 40 Tableau Workday SPSS SAS Business Objects XML Tableau Unix Shell Scripting Teradata Netezza Aginity Education Bachelors Skills SQL",
    "entities": [
        "USAA Extracting Opearations",
        "MLlib",
        "Informatica XML Python",
        "New York",
        "RESTFUL Technology Involved",
        "SDLC Agile DevOps",
        "BI",
        "MapReduce Data Scientist Charter",
        "HDFS",
        "OLTP ODS",
        "Write SQL",
        "Driven Development BDD",
        "HTTP",
        "Hadoop Getting",
        "Data Science",
        "Spectrum Responsibilities Utilized Scala Hadoop pySpark Data Lake TensorFlow",
        "EDAC Technologies Corp Cheshire",
        "Logical",
        "GUI Relational Database Management System",
        "USAA",
        "Data Marts",
        "ER",
        "Statistical Concepts Developed",
        "node",
        "MS Excel SQL UNIX Environment",
        "Hadoop",
        "Kotak Mahindra Finance Ltd",
        "SOAP",
        "XML",
        "Data Science and Analytics",
        "PyTorch Developed SparkScalaR Python",
        "Maintained",
        "kmeans Implemented Bagging and Boosting",
        "RapidMiner",
        "Reserve Bank of India",
        "Fresenius",
        "Texas Department",
        "TX",
        "Rauxa",
        "Data Models",
        "Pickle",
        "Oracle SQL Developer PLSQL Business",
        "Installed Configured",
        "SQL Server Reporting Services",
        "SQL Server",
        "Dabur India Ltd",
        "Australia Responsibilities Created",
        "Performed Data Cleaning",
        "Generated",
        "Developed",
        "Implemented Classification",
        "Pyspark Data",
        "VM Excellent",
        "Utilized",
        "Data Warehouse Extract",
        "Power Query",
        "VMware",
        "Stash Responsible",
        "Tableau Worked",
        "Studio Informatica Power Center 8191 Power Connect Power exchange",
        "PDM",
        "AirFlow",
        "Windows XP",
        "AGILE",
        "San Antonio",
        "Developed Traceability Matrix of Business Requirements",
        "API RESTful Web Service",
        "the Hadoop Distributed File System",
        "Airflow",
        "Format",
        "TX Professional",
        "Python Twisted",
        "CPU",
        "DBA",
        "ROC",
        "Developed SQL",
        "RShiny",
        "Spark Experienced",
        "XGBoost SVM",
        "Oracle Database",
        "Data Analyst Dorman Products Inc Colmar PA March 2014 to December 2015",
        "Description EDAC Technologies Corporation",
        "GIT",
        "linear",
        "Dabur",
        "MULTILOAD",
        "Sqoop",
        "LinuxWindows",
        "the Master Data Management Architecture",
        "Hadoop Setup",
        "Sqoop Patterns",
        "KNN",
        "NoSql",
        "Statistical",
        "Created",
        "AWS",
        "Created SQL",
        "Analyzed",
        "Data Resarcher Data",
        "SQL Profiler",
        "Workday",
        "Oracle",
        "Present Description",
        "Tensorflow a Deep Learning Framework",
        "Data Capture Staging Cleansing Loading and Auditing Developed",
        "PySpark",
        "Developed Tableau",
        "Apache Airflow",
        "PIG",
        "Sql",
        "Dabur Pharma Ltd German",
        "Apache MavenAnt Ability",
        "SAS",
        "Oozie",
        "Data Integration Validation",
        "SSRS",
        "SQLPLSQL XML Windows NT",
        "Market Basket Analysis",
        "CDH3u6",
        "SQL",
        "SCD Type 1",
        "Hive Involved",
        "OLTP",
        "Incremental",
        "BTEQ Exporting",
        "NLP",
        "Verizon",
        "Data Quality",
        "Errors Training",
        "the United States",
        "Anaconda",
        "Big Data",
        "Hive",
        "SQL GIT",
        "Memory Issue Disk Space Issue Server",
        "SCD Type",
        "Wrote",
        "DAG",
        "Work Experience Data Resarcher Data Scientist",
        "Tableau Desktop Used Graphical Entity Relationship Diagramming",
        "Pandas",
        "Mumbai",
        "ETL",
        "DB",
        "Random Forest Participated",
        "India",
        "Performed",
        "OLAP Tuned",
        "OLAP",
        "Description Kotak Mahindra Bank",
        "Flume Infrastructure Worked",
        "Graph Networks Scheduling Tools",
        "Partitioning Developed Data Mapping Transformation and Cleansing",
        "Performed Data Profiling",
        "JavaScript",
        "Customer Satisfaction scoreCSAT correlation Application",
        "Responsibilities Analyzed",
        "UI",
        "The United Services Automobile Association",
        "Logistic Regression Decision",
        "Python ScikitLearn Experienced",
        "Microsoft",
        "Jenkins ANT Maven Subversion Python",
        "Dabur Pharma",
        "CSAT",
        "Data Mining Used",
        "Nexus Business Objects Toad Power BI",
        "EDW",
        "Teradata Environment",
        "the Transformation Rules for Data Migration",
        "Data",
        "Data ScientistData Analyst",
        "Data Warehousing",
        "Python ObjectOriented Programming Query",
        "NoSQLDatabase",
        "Tableau",
        "ITSM",
        "Machine Learning",
        "JQuery Ajax Web Service and JSON Environment Python",
        "GIT command lines",
        "Description Dorman Products Inc",
        "HMM",
        "SQL Server Successfully",
        "Teradata",
        "Change Control",
        "SVM",
        "SQL andPython",
        "Fact and Dimension",
        "Advanced Regression Modeling Correlation Multivariate Analysis Model Building Business Intelligence",
        "Gap Inc"
    ],
    "experience": "Experience in implementing data analysis with various analytic tools such as Anaconda Jupiter Notebook 4X R 30 ggplot2 dplyr Caret and Excel Solid ability to write and optimize diverse SQL queries working knowledge of RDBMS like SQL Server 12 NoSql databases like MongoDB 32 Excellent understanding Agile and Scrum development methodology Used the version control tools like Git 2X and build tools like Apache MavenAnt Ability to maintain a fun casual professional and productive team atmosphere Experienced the full software life cycle in SDLC Agile DevOps and Scrum methodologies including creating requirements test plans Skilled in Advanced Regression Modeling Correlation Multivariate Analysis Model Building Business Intelligence tools and application of Statistical Concepts Developed predictive models using Decision Tree Naive Bayes Logistic Regression Random Forest Social Network Analysis Cluster Analysis and Neural Networks Experienced in Machine Learning and Statistical Analysis with Python ScikitLearn Experienced in Python to manipulate data for data loading and extraction and worked with python libraries like Matplotlib Scipy Numpy and Pandas for data analysis Strong SQL programming skills with experience in working with functions packages and triggers Expertise in transforming business requirements into designing algorithms analytical models building models developing data mining and reporting solutions that scales across massive volume of structured and unstructured data Skilled in performing data parsing data manipulation data architecture data ingestion and data preparation with methods including describe data contents compute descriptive statistics of data regex split and combine merge Remap subset reindex melt and reshape Worked with NoSQLDatabase including Hbase Cassandra and MongoDB Experienced in Big Data with Hadoop MapReduce HDFS and Spark Experienced in Data Integration Validation and Data Quality controls for ETL process and Data Warehousing using MS Visual Studio SSAS SSISand SSRS Proficient in Tableau and RShiny data visualization tools to analyze and obtain insights into large datasets create visually powerful and actionable interactive reports and dashboards Automated recurring reports using SQL andPython and visualized them on BI platform like Tableau Worked in development environment like Git and VM Excellent communication skills Successfully working in fastpaced multitasking environment both independently and in collaborative team a selfmotivated enthusiastic learner Work Experience Data Resarcher Data Scientist USAA San Antonio TX April 2019 to Present Description The United Services Automobile Association USAA is a San Antonio Texasbased Fortune 500 diversified financial services group of companies including a Texas Department of Insuranceregulated reciprocal interinsurance exchange and subsidiaries offering banking investing and insurance to people and families who serve or served in the United States Armed ForcesAt the end of 2017 there were 124 million members Responsibilities Test and determine whether new technology is potentially useful for USAA Extracting Opearations data from Workday and storing it in Hadoop Getting the data in Pickle file Format and Parsing it by CSV Build a graphic database using DataStax in Python ObjectOriented Programming Query datainformation from graphic database using Gremlin to support model validation Work in a group to develop Machine Learning guidance for model development and validation Build analytic tools to prepare data and graphs using Power Query and GraphX Draft the procedures for reporting Worked on ETL tools such as Apache Airflow Build a auto DAG system which would automatically trigger the workflow whenever the Airflow will receive HTTP request Write python scripts to parse documents Take online courses as a means to continuously learn new subjects Did intensive research on the tools like Graph Networks Scheduling Tools and Data Wrangling Tools to determine the best tool available in the market that would be best fit for companys need Environment Windows Python 3 DataStax GraphX Apache Airflow SQL Data Scientist Machine Learning Rauxa New York NY August 2018 to March 2019 Description Makers of results Rauxa applies data technology and content to create measurable impact at maximum speed for clients that include Gap Inc TGI Fridays and Verizon The countrys largest womanowned independent advertising agency Responsibilities Built models using Statistical techniques like Bayesian HMM and Machine Learning classification models like XGBoost SVM and Random Forest Participated in all phases of data mining data cleaning data collection developing models validation visualization and performed Gap analysis A highly immersive Data Science program involving Data ManipulationVisualization Web Scraping Machine Learning Python programming SQL GIT MongoDB Hadoop Setup storage and data analysis tools in AWS cloud computing infrastructure Installed and used Tensorflow a Deep Learning Framework Used pandas numpy seaborn matplotlib scikitlearn scipy NLTK in Python for developing various machine learning algorithms Data Manipulation and Aggregation from different source using Nexus Business Objects Toad Power BI and Smart View Programmed a utility in Python that used multiple packages numpy scipy pandas Implemented Classification using supervised algorithms like Logistic Regression Decision trees Naive Bayes KNN As Architect delivered various complex OLAP databasescubes scorecards dashboards and reports Updated Python scripts to match training data with our database stored in AWS Cloud Search so that we would be able to assign each document a response label for further classification Data transformation from various resources data organization features extraction from raw and stored Validated the machine learning classifiers using ROC Curves and Lift Charts Environment Unix Python 352 MLLib SAS regression logistic regression Hadoop 274 NoSQL Teradata OLTP random forest OLAP HDFS ODS NLTK SVM JSON XML and MapReduce Data Scientist Charter communication St Louis MO May 2017 to July 2018 Description Charter Communications Inc is an American telecommunications and mass media company that offers its services to consumers and businesses under the branding of Spectrum Responsibilities Utilized Scala Hadoop pySpark Data Lake TensorFlow MongoDB AWS Python a broad variety of machine learning methods including classifications regressions dimensionally reduction etc Developed sentiment analysis framework for email conversation and Customer Satisfaction scoreCSAT correlation Application of various machine learning algorithms and statistical modeling like decision trees text analytics natural language processing NLP supervised and unsupervised regression models social network analysis neural networks deep learning SVM clustering to identify Volume using scikitlearn package in python Matlab Sentiment Analytics engine was completely built in Python3 Email chain was considered for analysis Emails were pulled into DB from a tool Email2DB Each email transactions was preprocessed sentiment calculation subjectivity polarity was taken and linked to the CSAT score for dashboarding Emite was used as a tool of choice for visualization RapidMiner and PredictionIO was used as tool of choice for predictive analysis ITSM data was taken and build a engine for predicting CPU failure Memory Issue Disk Space Issue Server failure Errors Training data was build from either of proactive and reactive ticket data which was in turn to be used to make Classification Model for prediction Identifying and evaluating potential vendors for technology fitments performing proof of value exercise conducting commercial and contract negotiations Performed Data Profiling to learn about behavior with various features such as traffic pattern location Date and Time etc Categorized comments into positive and negative clusters from different social networking sites using Sentiment Analysis and Text Analytics Performed Multinomial Logistic Regression Decision Tree Random forest SVM to classify package is going to deliver on time for the new route Performed data analysis by using Hive to retrieve the data from Hadoop cluster Sql to retrieve datafrom Oracle database and used ETL for data transformation Performed Data Cleaning features scaling features engineering using pandas and numpy packages in python Exploring DAGs their dependencies and logs using AirFlow pipelines for automation Performed data cleaning and feature selection using MLlib package in PySpark and working with deep learning frameworks such as Tensorflow and PyTorch Developed SparkScalaR Python for regular expression regex project in the HadoopHive environment with LinuxWindows for big data resources Used clustering technique KMeans to identify outliers and to classify unlabeled data Communicated the results with operations team for taking best decisions Collected data needs and requirements by Interacting with the other departments Environment Python 2x CDH5 HDFS Hadoop 23 Hive Impala AWS Linux Spark Tableau Desktop SQL Server 2014 Microsoft Excel Matlab Spark SQL Pyspark Data analyst EDAC Technologies Corp Cheshire CT January 2016 to April 2017 Description EDAC Technologies Corporation provides design manufacturing and services for tooling fixtures molds jet engine components and machine spindles in the aerospace industrial semiconductor and medical device markets Responsibilities Worked with BI team in gathering the report requirements and also Sqoop to export data into HDFS and Hive Involved in the below phases of Analytics using R Python and Jupyter notebook Data collection and treatment Analysed existing internal data and external data worked on entry errorsclassification errors and defined criteria for missing values Data Mining Used cluster analysis for identifying customer segments Decision trees used for profitable and nonprofitable customers Market Basket Analysis used for customer purchasing behaviour and partproduct association Emite was used as a tool of choice for visualization Assisted with data capacity planning and node forecasting Installed Configured and managed Flume Infrastructure Worked closely with the claims processing team to obtain patterns in filing of fraudulent claims Worked on performing major upgrade of cluster from CDH3u6 to CDH440 Developed Map Reduce programs to extract and transform the data sets and results were exported back to RDBMS using Sqoop Patterns were observed in fraudulent claims using text mining in R and Hive Exported the data required information to RDBMS using Sqoop to make the data available for the claims processing team to assist in processing a claim based on the data Developed Map Reduce programs to parse the raw data populate staging tables and store the refined data in partitioned tables in the EDW Created tables in Hive and loaded the structured resulted from Map Reduce jobs data Created Hive queries that helped market analysts spot emerging trends by comparing fresh data with EDW reference tables and historical metrics Enabled speedy reviews and first mover advantages by using Oozie to automate data loading into the Hadoop Distributed File System and PIG to preprocess the data Provided design recommendations and thought leadership to sponsorsstakeholders that improved review processes and resolved technical problems Managed and reviewed Hadoop log files Tested raw data and executed performance scripts Environment HDFS PIG HIVE Map Reduce Linux HBase Flume Sqoop R VMware Eclipse Cloudera Python Data Analyst Dorman Products Inc Colmar PA March 2014 to December 2015 Description Dorman Products Inc supplies automotive replacement parts automotive hardware and brake products to the automotive aftermarket and mass merchandise markets in the United States Canada Mexico Europe the Middle East and Australia Responsibilities Created and maintained Logical and Physical models for the data mart Created partitions and indexes for the tables in the data mart Performed data profiling and analysis applied various data cleansing rules designed data standards and architecturedesigned the relational models Maintained metadata data definitions of table structures and version controlling for the data model Developed SQL scripts for creating tables Sequences Triggers views and materialized views Worked on query optimization and performance tuning using SQL Profiler and performance monitoring Developed mappings to load Fact and Dimension tables SCD Type 1 and SCD Type 2 dimensions and Incremental loading and unit tested the mappings Utilized Erwins forward reverse engineering tools and target database schema conversion process Worked on creating enterprise wide Model EDM for products and services in Teradata Environment based on the data from PDM Conceived designed developed and implemented this model from the scratch Building publishing customized interactive reports and dashboards report scheduling using Tableau server Write SQL scripts to test the mappings and Developed Traceability Matrix of Business Requirements mapped to Test Scripts to ensure any Change Control in requirements leads to test case update Responsible for development and testing of conversion programs for importing Data from text files into map Oracle Database utilizing PERL shell scripts SQLLoader Involved in extensive DATA validation by writing several complex SQL queries and Involved in backend testing and worked with data quality issues Developed and executed load scripts using Teradata client utilities MULTILOAD FASTLOAD and BTEQ Exporting and importing the data between different platforms such as SAS MSExcel Generated periodic reports based on the statistical analysis of the data using SQL Server Reporting Services SSRS Worked with the ETL team to document the Transformation Rules for Data Migration from OLTP to Warehouse Environment for reporting purposes Created SQL scripts to find data quality issues and to identify keys data anomalies and data validation issues Formatting the data sets read into SAS by using Format statement in the data step as well as Proc Format Applied Business Objects best practices during development with a strong focus on reusability and better performance Developed Tableau visualizations and dashboards using Tableau Desktop Used Graphical Entity Relationship Diagramming to create new database design via easy to use graphical interface Designed different type of STAR schemas for detailed data marts and plan data marts in the OLAP environment Environment Erwin MS SQL Server 2008 DB2 Oracle SQL Developer PLSQL Business Objects Erwin MS office suite Windows XP TOAD SQLPLUS SQLLOADER Teradata Netezza SAS Tableau Business Objects SSRS tableau SQL Assistant Informatica XML Python Developer Dabur India Ltd Ghaziabad Uttar Pradesh December 2012 to February 2014 Description Dabur is one of the Indias largest Ayurvedic medicine natural consumer products manufacturer Dabur demerged its Pharma business in 2003 and hived it off into a separate company Dabur Pharma Ltd German company Fresenius SE bought a 7327 equity stake in Dabur Pharma in June 2008 at Rs 7650 a share Responsibilities Involved in the design development and testing phases of application using AGILE methodology Designed and maintained databases using Python and developed Python based API RESTful Web Service using Flask SQLAlchemy and PostgreSQL Designed and developed the UI of the website using HTML XHTML AJAX CSS and JavaScript Participated in requirement gathering and worked closely with the architect in designing and modeling Worked on Restful web services which enforced a stateless client server and support JSON few changes from SOAP to RESTFUL Technology Involved in detailed analysis based on the requirement documents Involved in writing SQL queries implementing functions triggers cursors object types sequences indexes etc Created and managed all of hosted or local repositories through Source Trees simple interface of GIT client collaborated with GIT command lines and Stash Responsible for setting up Python REST API framework and spring frame work using Django Develope consumer based features and applications using Python Django HTML behavior Driven Development BDD and pair based programming Designed and developed components using Python with Django framework Implemented code in python to retrieve and manipulate data Involved in development of the enterprise social network application using Python Twisted and Cassandra Used Python and Django creating graphics XML processing of documents data exchange and business logic implementation between servers orked closely with backend developer to find ways to push the limits of existing Web technology Designed and developed the UI for the website with HTML XHTML CSS Java Script and AJAX Used AJAXJSON communication for accessing RESTfulweb services data payload Designed dynamic clientside JavaScript codes to build web forms and performed simulations for web application page Created and implemented SQL Queries Stored procedures Functions Packages and Triggers in SQL Server Successfully implemented Auto CompleteAuto Suggest functionality using JQuery Ajax Web Service and JSON Environment Python 25 JavaJ2EE Django10 HTMLCSS Linux Shell Scripting Java Script Ajax JQuery JSON XML PostgreSQL Jenkins ANT Maven Subversion Python Data Analyst Kotak Mahindra Bank Mumbai Maharashtra January 2011 to November 2012 Description Kotak Mahindra Bank is an Indian private sector bank headquartered in Mumbai Maharashtra India In February 2003 Reserve Bank of India issued the licence to Kotak Mahindra Finance Ltd the groups flagship company to carry on banking business Responsibilities Analyzed data sources and requirements and business rules to perform logical and physical data modeling Analyzed and designed best fit logical and physical data models and relational database definitions using DB2 Generated reports of data definitions Involved in NormalizationDenormalization Normal Form and database design methodology Maintained existing ETL procedures fixed bugs and restored software to production environment Developed the code as per the clients requirements using SQL PLSQL and Data Warehousing concepts Involved in Dimensional modeling Star Schema of the Data warehouse and used Erwin to design the business process dimensions and measured facts Worked with Data Warehouse Extract and load developers to design mappings for Data Capture Staging Cleansing Loading and Auditing Developed enterprise data model management process to manage multiple data models developed by different groups Designed and created Data Marts as part of a data warehouse Wrote complex SQL queries for validating the data against different kinds of reports generated by Business Objects XIR2 Using Erwin modeling tool publishing of a data dictionary review of the model and dictionary with subject matter experts and generation of data definition language Coordinated with DBA in implementing the Database changes and also updating Data Models with changes implemented in development QA and Production Worked Extensively with DBA and Reporting team for improving the Report Performance with the Use of appropriate indexes and Partitioning Developed Data Mapping Transformation and Cleansing rules for the Master Data Management Architecture involved OLTP ODS and OLAP Tuned and coded optimization using different techniques like dynamic SQL dynamic cursors and tuning SQL queries writing generic procedures functions and packages Experienced in GUI Relational Database Management System RDBMS designing of OLAP system environment as well as Report Development Extensively used SQL TSQL and PLSQL to write stored procedures functions packages and triggers Analyzed of data report were prepared weekly biweekly monthly using MS Excel SQL UNIX Environment ER Studio Informatica Power Center 8191 Power Connect Power exchange Oracle 11 g MainframesDB2 MS SQL Server 2008 SQLPLSQL XML Windows NT 40 Tableau Workday SPSS SAS Business Objects XML Tableau Unix Shell Scripting Teradata Netezza Aginity Education Bachelors Skills SQL",
    "extracted_keywords": [
        "Data",
        "Resarcher",
        "Data",
        "Scientist",
        "Data",
        "Resarcher",
        "Data",
        "Scientist",
        "Data",
        "Resarcher",
        "Data",
        "Scientist",
        "USAA",
        "San",
        "Antonio",
        "TX",
        "Professional",
        "Data",
        "ScientistData",
        "Analyst",
        "experience",
        "Data",
        "Science",
        "Analytics",
        "Data",
        "Mining",
        "Deep",
        "LearningMachine",
        "Learning",
        "Statistical",
        "Analysis",
        "data",
        "science",
        "project",
        "life",
        "cycle",
        "phases",
        "data",
        "data",
        "datavisualization",
        "data",
        "sets",
        "data",
        "ER",
        "diagrams",
        "schema",
        "machine",
        "learning",
        "algorithm",
        "regression",
        "KNN",
        "SVM",
        "forest",
        "network",
        "linear",
        "regression",
        "lasso",
        "regression",
        "kmeans",
        "Bagging",
        "model",
        "performance",
        "Experience",
        "data",
        "analysis",
        "tools",
        "Anaconda",
        "Jupiter",
        "Notebook",
        "4X",
        "ggplot2",
        "dplyr",
        "Caret",
        "Excel",
        "ability",
        "SQL",
        "knowledge",
        "RDBMS",
        "SQL",
        "Server",
        "NoSql",
        "Excellent",
        "understanding",
        "Agile",
        "Scrum",
        "development",
        "methodology",
        "version",
        "control",
        "tools",
        "Git",
        "2X",
        "tools",
        "Apache",
        "MavenAnt",
        "Ability",
        "team",
        "atmosphere",
        "software",
        "life",
        "cycle",
        "SDLC",
        "Agile",
        "DevOps",
        "Scrum",
        "methodologies",
        "requirements",
        "test",
        "plans",
        "Regression",
        "Modeling",
        "Correlation",
        "Multivariate",
        "Analysis",
        "Model",
        "Building",
        "Business",
        "Intelligence",
        "tools",
        "application",
        "Statistical",
        "Concepts",
        "models",
        "Decision",
        "Tree",
        "Naive",
        "Bayes",
        "Logistic",
        "Regression",
        "Random",
        "Forest",
        "Social",
        "Network",
        "Analysis",
        "Cluster",
        "Analysis",
        "Neural",
        "Networks",
        "Machine",
        "Learning",
        "Statistical",
        "Analysis",
        "Python",
        "ScikitLearn",
        "Python",
        "data",
        "data",
        "loading",
        "extraction",
        "python",
        "libraries",
        "Matplotlib",
        "Scipy",
        "Numpy",
        "Pandas",
        "data",
        "analysis",
        "Strong",
        "SQL",
        "programming",
        "skills",
        "experience",
        "functions",
        "packages",
        "Expertise",
        "business",
        "requirements",
        "algorithms",
        "models",
        "building",
        "models",
        "data",
        "mining",
        "reporting",
        "solutions",
        "volume",
        "data",
        "data",
        "data",
        "manipulation",
        "data",
        "architecture",
        "data",
        "ingestion",
        "data",
        "preparation",
        "methods",
        "describe",
        "data",
        "contents",
        "statistics",
        "data",
        "regex",
        "Remap",
        "subset",
        "reindex",
        "melt",
        "reshape",
        "NoSQLDatabase",
        "Hbase",
        "Cassandra",
        "MongoDB",
        "Big",
        "Data",
        "Hadoop",
        "MapReduce",
        "HDFS",
        "Spark",
        "Data",
        "Integration",
        "Validation",
        "Data",
        "Quality",
        "ETL",
        "process",
        "Data",
        "Warehousing",
        "MS",
        "Visual",
        "Studio",
        "SSAS",
        "SSISand",
        "SSRS",
        "Proficient",
        "Tableau",
        "RShiny",
        "data",
        "visualization",
        "tools",
        "insights",
        "datasets",
        "reports",
        "dashboards",
        "reports",
        "SQL",
        "andPython",
        "BI",
        "platform",
        "Tableau",
        "development",
        "environment",
        "Git",
        "VM",
        "Excellent",
        "communication",
        "skills",
        "environment",
        "team",
        "learner",
        "Work",
        "Experience",
        "Data",
        "Resarcher",
        "Data",
        "Scientist",
        "USAA",
        "San",
        "Antonio",
        "TX",
        "April",
        "Present",
        "Description",
        "United",
        "Services",
        "Automobile",
        "Association",
        "USAA",
        "San",
        "Antonio",
        "Texasbased",
        "Fortune",
        "services",
        "group",
        "companies",
        "Texas",
        "Department",
        "Insuranceregulated",
        "interinsurance",
        "exchange",
        "subsidiaries",
        "banking",
        "investing",
        "insurance",
        "people",
        "families",
        "United",
        "States",
        "Armed",
        "ForcesAt",
        "end",
        "members",
        "Responsibilities",
        "technology",
        "USAA",
        "Extracting",
        "Opearations",
        "data",
        "Workday",
        "Hadoop",
        "data",
        "Pickle",
        "file",
        "Format",
        "CSV",
        "Build",
        "database",
        "DataStax",
        "Python",
        "ObjectOriented",
        "Programming",
        "Query",
        "datainformation",
        "database",
        "Gremlin",
        "model",
        "validation",
        "Work",
        "group",
        "Machine",
        "Learning",
        "guidance",
        "model",
        "development",
        "validation",
        "Build",
        "tools",
        "data",
        "graphs",
        "Power",
        "Query",
        "GraphX",
        "Draft",
        "procedures",
        "Worked",
        "ETL",
        "tools",
        "Apache",
        "Airflow",
        "Build",
        "auto",
        "DAG",
        "system",
        "workflow",
        "Airflow",
        "HTTP",
        "request",
        "python",
        "scripts",
        "documents",
        "courses",
        "means",
        "subjects",
        "research",
        "tools",
        "Graph",
        "Networks",
        "Scheduling",
        "Tools",
        "Data",
        "Wrangling",
        "Tools",
        "tool",
        "market",
        "companys",
        "Environment",
        "Windows",
        "Python",
        "DataStax",
        "GraphX",
        "Apache",
        "Airflow",
        "SQL",
        "Data",
        "Scientist",
        "Machine",
        "Learning",
        "Rauxa",
        "New",
        "York",
        "NY",
        "August",
        "March",
        "Description",
        "Makers",
        "results",
        "Rauxa",
        "data",
        "technology",
        "content",
        "impact",
        "speed",
        "clients",
        "Gap",
        "Inc",
        "TGI",
        "Fridays",
        "Verizon",
        "countrys",
        "advertising",
        "agency",
        "models",
        "techniques",
        "Bayesian",
        "HMM",
        "Machine",
        "Learning",
        "classification",
        "models",
        "XGBoost",
        "SVM",
        "Random",
        "Forest",
        "phases",
        "data",
        "mining",
        "data",
        "data",
        "collection",
        "models",
        "validation",
        "visualization",
        "analysis",
        "Data",
        "Science",
        "program",
        "Data",
        "ManipulationVisualization",
        "Web",
        "Machine",
        "Learning",
        "Python",
        "programming",
        "SQL",
        "GIT",
        "MongoDB",
        "Hadoop",
        "Setup",
        "storage",
        "data",
        "analysis",
        "tools",
        "AWS",
        "cloud",
        "infrastructure",
        "Tensorflow",
        "Deep",
        "Learning",
        "Framework",
        "pandas",
        "numpy",
        "matplotlib",
        "NLTK",
        "Python",
        "machine",
        "learning",
        "Data",
        "Manipulation",
        "Aggregation",
        "source",
        "Nexus",
        "Business",
        "Toad",
        "Power",
        "BI",
        "Smart",
        "View",
        "utility",
        "Python",
        "packages",
        "scipy",
        "Implemented",
        "Classification",
        "algorithms",
        "Logistic",
        "Regression",
        "Decision",
        "trees",
        "Bayes",
        "KNN",
        "Architect",
        "OLAP",
        "scorecards",
        "dashboards",
        "reports",
        "Updated",
        "Python",
        "scripts",
        "training",
        "data",
        "database",
        "AWS",
        "Cloud",
        "Search",
        "document",
        "response",
        "label",
        "classification",
        "Data",
        "transformation",
        "resources",
        "data",
        "organization",
        "extraction",
        "machine",
        "classifiers",
        "ROC",
        "Curves",
        "Lift",
        "Charts",
        "Environment",
        "Unix",
        "Python",
        "MLLib",
        "SAS",
        "regression",
        "regression",
        "Hadoop",
        "NoSQL",
        "Teradata",
        "OLTP",
        "forest",
        "OLAP",
        "HDFS",
        "ODS",
        "NLTK",
        "SVM",
        "JSON",
        "XML",
        "MapReduce",
        "Data",
        "Scientist",
        "Charter",
        "communication",
        "St",
        "Louis",
        "MO",
        "May",
        "July",
        "Description",
        "Charter",
        "Communications",
        "Inc",
        "telecommunications",
        "media",
        "company",
        "services",
        "consumers",
        "businesses",
        "branding",
        "Spectrum",
        "Responsibilities",
        "Scala",
        "Hadoop",
        "pySpark",
        "Data",
        "Lake",
        "TensorFlow",
        "MongoDB",
        "AWS",
        "Python",
        "variety",
        "machine",
        "learning",
        "methods",
        "classifications",
        "regressions",
        "reduction",
        "sentiment",
        "analysis",
        "framework",
        "email",
        "conversation",
        "Customer",
        "Satisfaction",
        "correlation",
        "Application",
        "machine",
        "algorithms",
        "modeling",
        "decision",
        "trees",
        "text",
        "analytics",
        "language",
        "processing",
        "NLP",
        "regression",
        "models",
        "network",
        "analysis",
        "networks",
        "SVM",
        "Volume",
        "package",
        "python",
        "Matlab",
        "Sentiment",
        "Analytics",
        "engine",
        "Python3",
        "Email",
        "chain",
        "analysis",
        "Emails",
        "DB",
        "tool",
        "Email2DB",
        "email",
        "transactions",
        "sentiment",
        "calculation",
        "subjectivity",
        "polarity",
        "CSAT",
        "score",
        "dashboarding",
        "Emite",
        "tool",
        "choice",
        "visualization",
        "RapidMiner",
        "PredictionIO",
        "tool",
        "choice",
        "analysis",
        "ITSM",
        "data",
        "engine",
        "CPU",
        "failure",
        "Memory",
        "Issue",
        "Disk",
        "Space",
        "Issue",
        "Server",
        "failure",
        "Errors",
        "Training",
        "data",
        "ticket",
        "data",
        "turn",
        "Classification",
        "Model",
        "prediction",
        "vendors",
        "technology",
        "fitments",
        "proof",
        "value",
        "exercise",
        "contract",
        "negotiations",
        "Performed",
        "Data",
        "Profiling",
        "behavior",
        "features",
        "traffic",
        "pattern",
        "location",
        "Date",
        "Time",
        "comments",
        "clusters",
        "networking",
        "sites",
        "Sentiment",
        "Analysis",
        "Text",
        "Analytics",
        "Performed",
        "Multinomial",
        "Logistic",
        "Regression",
        "Decision",
        "Tree",
        "Random",
        "forest",
        "SVM",
        "package",
        "time",
        "route",
        "Performed",
        "data",
        "analysis",
        "Hive",
        "data",
        "Hadoop",
        "cluster",
        "Sql",
        "Oracle",
        "database",
        "ETL",
        "data",
        "transformation",
        "Performed",
        "Data",
        "Cleaning",
        "features",
        "engineering",
        "pandas",
        "packages",
        "python",
        "DAGs",
        "dependencies",
        "logs",
        "AirFlow",
        "pipelines",
        "automation",
        "Performed",
        "data",
        "cleaning",
        "feature",
        "selection",
        "MLlib",
        "package",
        "PySpark",
        "frameworks",
        "Tensorflow",
        "SparkScalaR",
        "Python",
        "expression",
        "regex",
        "project",
        "environment",
        "LinuxWindows",
        "data",
        "resources",
        "clustering",
        "technique",
        "KMeans",
        "outliers",
        "data",
        "results",
        "operations",
        "team",
        "decisions",
        "data",
        "needs",
        "requirements",
        "departments",
        "Environment",
        "Python",
        "CDH5",
        "HDFS",
        "Hadoop",
        "Hive",
        "Impala",
        "Linux",
        "Spark",
        "Tableau",
        "Desktop",
        "SQL",
        "Server",
        "Microsoft",
        "Excel",
        "Matlab",
        "Spark",
        "SQL",
        "Pyspark",
        "Data",
        "analyst",
        "EDAC",
        "Technologies",
        "Corp",
        "Cheshire",
        "CT",
        "January",
        "April",
        "Description",
        "EDAC",
        "Technologies",
        "Corporation",
        "design",
        "manufacturing",
        "services",
        "tooling",
        "fixtures",
        "molds",
        "jet",
        "engine",
        "components",
        "machine",
        "spindles",
        "semiconductor",
        "device",
        "markets",
        "Responsibilities",
        "BI",
        "team",
        "report",
        "requirements",
        "Sqoop",
        "data",
        "HDFS",
        "Hive",
        "phases",
        "Analytics",
        "R",
        "Python",
        "Jupyter",
        "notebook",
        "Data",
        "collection",
        "treatment",
        "data",
        "data",
        "entry",
        "errorsclassification",
        "errors",
        "criteria",
        "values",
        "Data",
        "Mining",
        "cluster",
        "analysis",
        "customer",
        "Decision",
        "trees",
        "customers",
        "Market",
        "Basket",
        "Analysis",
        "customer",
        "behaviour",
        "partproduct",
        "association",
        "Emite",
        "tool",
        "choice",
        "visualization",
        "data",
        "capacity",
        "planning",
        "node",
        "forecasting",
        "Configured",
        "Flume",
        "Infrastructure",
        "claims",
        "processing",
        "team",
        "patterns",
        "filing",
        "claims",
        "upgrade",
        "cluster",
        "CDH3u6",
        "CDH440",
        "Developed",
        "Map",
        "programs",
        "data",
        "sets",
        "results",
        "Sqoop",
        "Patterns",
        "claims",
        "text",
        "mining",
        "R",
        "Hive",
        "data",
        "information",
        "Sqoop",
        "data",
        "claims",
        "processing",
        "team",
        "claim",
        "data",
        "Developed",
        "Map",
        "programs",
        "data",
        "staging",
        "tables",
        "data",
        "tables",
        "EDW",
        "Created",
        "tables",
        "Hive",
        "Map",
        "Reduce",
        "jobs",
        "data",
        "Hive",
        "queries",
        "market",
        "analysts",
        "trends",
        "data",
        "EDW",
        "reference",
        "tables",
        "metrics",
        "reviews",
        "advantages",
        "Oozie",
        "data",
        "loading",
        "Hadoop",
        "Distributed",
        "File",
        "System",
        "PIG",
        "data",
        "design",
        "recommendations",
        "leadership",
        "sponsorsstakeholders",
        "review",
        "processes",
        "problems",
        "Hadoop",
        "log",
        "files",
        "data",
        "performance",
        "scripts",
        "Environment",
        "HDFS",
        "PIG",
        "HIVE",
        "Map",
        "Linux",
        "HBase",
        "Flume",
        "Sqoop",
        "R",
        "VMware",
        "Eclipse",
        "Cloudera",
        "Python",
        "Data",
        "Analyst",
        "Dorman",
        "Products",
        "Inc",
        "Colmar",
        "PA",
        "March",
        "December",
        "Description",
        "Dorman",
        "Products",
        "Inc",
        "replacement",
        "parts",
        "hardware",
        "brake",
        "products",
        "aftermarket",
        "merchandise",
        "markets",
        "United",
        "States",
        "Canada",
        "Mexico",
        "Europe",
        "Middle",
        "East",
        "Australia",
        "Responsibilities",
        "models",
        "data",
        "mart",
        "Created",
        "partitions",
        "indexes",
        "tables",
        "data",
        "mart",
        "Performed",
        "data",
        "profiling",
        "analysis",
        "data",
        "cleansing",
        "rules",
        "data",
        "standards",
        "models",
        "metadata",
        "data",
        "definitions",
        "table",
        "structures",
        "version",
        "data",
        "model",
        "Developed",
        "SQL",
        "scripts",
        "tables",
        "Sequences",
        "Triggers",
        "views",
        "views",
        "query",
        "optimization",
        "performance",
        "SQL",
        "Profiler",
        "performance",
        "mappings",
        "Fact",
        "Dimension",
        "SCD",
        "Type",
        "SCD",
        "Type",
        "dimensions",
        "loading",
        "unit",
        "mappings",
        "Erwins",
        "engineering",
        "tools",
        "target",
        "database",
        "schema",
        "conversion",
        "process",
        "enterprise",
        "Model",
        "EDM",
        "products",
        "services",
        "Teradata",
        "Environment",
        "data",
        "PDM",
        "Conceived",
        "model",
        "scratch",
        "Building",
        "reports",
        "dashboards",
        "scheduling",
        "Tableau",
        "server",
        "Write",
        "SQL",
        "scripts",
        "mappings",
        "Traceability",
        "Matrix",
        "Business",
        "Requirements",
        "Scripts",
        "Change",
        "Control",
        "requirements",
        "test",
        "case",
        "development",
        "testing",
        "conversion",
        "programs",
        "Data",
        "text",
        "files",
        "map",
        "Oracle",
        "Database",
        "PERL",
        "shell",
        "scripts",
        "SQLLoader",
        "DATA",
        "validation",
        "SQL",
        "queries",
        "testing",
        "data",
        "quality",
        "issues",
        "load",
        "scripts",
        "Teradata",
        "client",
        "utilities",
        "MULTILOAD",
        "FASTLOAD",
        "BTEQ",
        "Exporting",
        "data",
        "platforms",
        "SAS",
        "MSExcel",
        "reports",
        "analysis",
        "data",
        "SQL",
        "Server",
        "Reporting",
        "Services",
        "SSRS",
        "ETL",
        "team",
        "Transformation",
        "Rules",
        "Data",
        "Migration",
        "OLTP",
        "Warehouse",
        "Environment",
        "purposes",
        "SQL",
        "scripts",
        "data",
        "quality",
        "issues",
        "keys",
        "data",
        "anomalies",
        "data",
        "validation",
        "issues",
        "data",
        "sets",
        "SAS",
        "Format",
        "statement",
        "data",
        "step",
        "Proc",
        "Format",
        "Applied",
        "Business",
        "practices",
        "development",
        "focus",
        "reusability",
        "performance",
        "Developed",
        "Tableau",
        "visualizations",
        "dashboards",
        "Tableau",
        "Desktop",
        "Graphical",
        "Entity",
        "Relationship",
        "database",
        "design",
        "interface",
        "type",
        "STAR",
        "schemas",
        "data",
        "marts",
        "plan",
        "data",
        "marts",
        "OLAP",
        "environment",
        "Environment",
        "Erwin",
        "MS",
        "SQL",
        "Server",
        "DB2",
        "Oracle",
        "SQL",
        "Developer",
        "PLSQL",
        "Business",
        "Erwin",
        "MS",
        "office",
        "suite",
        "Windows",
        "XP",
        "TOAD",
        "SQLPLUS",
        "SQLLOADER",
        "Teradata",
        "Netezza",
        "SAS",
        "Tableau",
        "Business",
        "Objects",
        "SSRS",
        "tableau",
        "SQL",
        "Assistant",
        "Informatica",
        "XML",
        "Python",
        "Developer",
        "Dabur",
        "India",
        "Ltd",
        "Ghaziabad",
        "Uttar",
        "Pradesh",
        "December",
        "February",
        "Description",
        "Dabur",
        "Indias",
        "medicine",
        "consumer",
        "products",
        "manufacturer",
        "Dabur",
        "Pharma",
        "business",
        "company",
        "Dabur",
        "Pharma",
        "Ltd",
        "company",
        "Fresenius",
        "SE",
        "equity",
        "stake",
        "Dabur",
        "Pharma",
        "June",
        "Rs",
        "share",
        "Responsibilities",
        "design",
        "development",
        "phases",
        "application",
        "methodology",
        "databases",
        "Python",
        "Python",
        "API",
        "RESTful",
        "Web",
        "Service",
        "Flask",
        "SQLAlchemy",
        "PostgreSQL",
        "UI",
        "website",
        "HTML",
        "XHTML",
        "AJAX",
        "CSS",
        "JavaScript",
        "requirement",
        "gathering",
        "architect",
        "modeling",
        "Restful",
        "web",
        "services",
        "client",
        "server",
        "changes",
        "SOAP",
        "RESTFUL",
        "Technology",
        "analysis",
        "requirement",
        "documents",
        "SQL",
        "functions",
        "triggers",
        "cursors",
        "types",
        "sequences",
        "indexes",
        "repositories",
        "Source",
        "Trees",
        "interface",
        "GIT",
        "client",
        "GIT",
        "command",
        "lines",
        "Stash",
        "Responsible",
        "Python",
        "REST",
        "API",
        "framework",
        "spring",
        "frame",
        "work",
        "Django",
        "Develope",
        "consumer",
        "features",
        "applications",
        "Python",
        "Django",
        "HTML",
        "behavior",
        "Driven",
        "Development",
        "BDD",
        "programming",
        "components",
        "Python",
        "Django",
        "framework",
        "code",
        "python",
        "manipulate",
        "data",
        "development",
        "enterprise",
        "network",
        "application",
        "Python",
        "Twisted",
        "Cassandra",
        "Python",
        "Django",
        "graphics",
        "XML",
        "processing",
        "documents",
        "data",
        "exchange",
        "business",
        "logic",
        "implementation",
        "servers",
        "developer",
        "ways",
        "limits",
        "Web",
        "technology",
        "UI",
        "website",
        "HTML",
        "XHTML",
        "CSS",
        "Java",
        "Script",
        "AJAX",
        "AJAXJSON",
        "communication",
        "RESTfulweb",
        "services",
        "data",
        "payload",
        "JavaScript",
        "codes",
        "web",
        "forms",
        "simulations",
        "web",
        "application",
        "page",
        "SQL",
        "Queries",
        "procedures",
        "Functions",
        "Packages",
        "Triggers",
        "SQL",
        "Server",
        "Successfully",
        "Auto",
        "CompleteAuto",
        "Suggest",
        "functionality",
        "JQuery",
        "Ajax",
        "Web",
        "Service",
        "JSON",
        "Environment",
        "Python",
        "JavaJ2EE",
        "Django10",
        "HTMLCSS",
        "Linux",
        "Shell",
        "Scripting",
        "Java",
        "Script",
        "Ajax",
        "JQuery",
        "JSON",
        "XML",
        "PostgreSQL",
        "Jenkins",
        "ANT",
        "Maven",
        "Subversion",
        "Python",
        "Data",
        "Analyst",
        "Kotak",
        "Mahindra",
        "Bank",
        "Mumbai",
        "Maharashtra",
        "January",
        "November",
        "Description",
        "Kotak",
        "Mahindra",
        "Bank",
        "sector",
        "bank",
        "Mumbai",
        "Maharashtra",
        "India",
        "February",
        "Reserve",
        "Bank",
        "India",
        "licence",
        "Kotak",
        "Mahindra",
        "Finance",
        "Ltd",
        "groups",
        "flagship",
        "company",
        "banking",
        "business",
        "Responsibilities",
        "data",
        "sources",
        "requirements",
        "business",
        "rules",
        "data",
        "data",
        "models",
        "database",
        "definitions",
        "DB2",
        "Generated",
        "reports",
        "data",
        "definitions",
        "NormalizationDenormalization",
        "Normal",
        "Form",
        "database",
        "design",
        "methodology",
        "ETL",
        "procedures",
        "bugs",
        "software",
        "production",
        "environment",
        "code",
        "clients",
        "requirements",
        "SQL",
        "PLSQL",
        "Data",
        "Warehousing",
        "concepts",
        "modeling",
        "Star",
        "Schema",
        "Data",
        "warehouse",
        "Erwin",
        "business",
        "process",
        "dimensions",
        "facts",
        "Data",
        "Warehouse",
        "Extract",
        "developers",
        "mappings",
        "Data",
        "Capture",
        "Staging",
        "Cleansing",
        "Loading",
        "Auditing",
        "enterprise",
        "data",
        "model",
        "management",
        "process",
        "data",
        "models",
        "groups",
        "Data",
        "Marts",
        "part",
        "data",
        "warehouse",
        "Wrote",
        "SQL",
        "data",
        "kinds",
        "reports",
        "Business",
        "Objects",
        "XIR2",
        "Erwin",
        "modeling",
        "tool",
        "publishing",
        "data",
        "review",
        "model",
        "subject",
        "matter",
        "experts",
        "generation",
        "data",
        "definition",
        "language",
        "DBA",
        "Database",
        "changes",
        "Data",
        "Models",
        "changes",
        "development",
        "QA",
        "Production",
        "DBA",
        "Reporting",
        "team",
        "Report",
        "Performance",
        "Use",
        "indexes",
        "Developed",
        "Data",
        "Mapping",
        "Transformation",
        "Cleansing",
        "rules",
        "Master",
        "Data",
        "Management",
        "Architecture",
        "OLTP",
        "ODS",
        "OLAP",
        "optimization",
        "techniques",
        "SQL",
        "cursors",
        "SQL",
        "queries",
        "procedures",
        "functions",
        "packages",
        "GUI",
        "Relational",
        "Database",
        "Management",
        "System",
        "RDBMS",
        "designing",
        "OLAP",
        "system",
        "environment",
        "Report",
        "Development",
        "SQL",
        "TSQL",
        "PLSQL",
        "procedures",
        "functions",
        "packages",
        "triggers",
        "data",
        "report",
        "MS",
        "Excel",
        "SQL",
        "UNIX",
        "Environment",
        "ER",
        "Studio",
        "Informatica",
        "Power",
        "Center",
        "8191",
        "Power",
        "Connect",
        "Power",
        "exchange",
        "Oracle",
        "g",
        "MainframesDB2",
        "MS",
        "SQL",
        "Server",
        "SQLPLSQL",
        "XML",
        "Windows",
        "NT",
        "Tableau",
        "Workday",
        "SPSS",
        "SAS",
        "Business",
        "XML",
        "Tableau",
        "Unix",
        "Shell",
        "Scripting",
        "Teradata",
        "Netezza",
        "Aginity",
        "Education",
        "Bachelors",
        "Skills",
        "SQL"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T20:05:03.690763",
    "resume_data": "Data Resarcher Data Scientist Data Resarcher Data Scientist Data Resarcher Data Scientist USAA San Antonio TX Professional qualified Data ScientistData Analyst with experience in Data Science and Analytics including Data Mining Deep LearningMachine Learning and Statistical Analysis Involved in the entire data science project life cycle and actively involved in all the phases including data cleaning data extractionand datavisualization with large data sets of structured and unstructured data created ER diagrams and schema Experienced with machine learning algorithm such as logistic regression KNN SVM random forest neural network linear regression lasso regression and kmeans Implemented Bagging and Boosting to enhance the model performance Experience in implementing data analysis with various analytic tools such as Anaconda Jupiter Notebook 4X R 30 ggplot2 dplyr Caret and Excel Solid ability to write and optimize diverse SQL queries working knowledge of RDBMS like SQL Server 200820102012 NoSql databases like MongoDB 32 Excellent understanding Agile and Scrum development methodology Used the version control tools like Git 2X and build tools like Apache MavenAnt Ability to maintain a fun casual professional and productive team atmosphere Experienced the full software life cycle in SDLC Agile DevOps and Scrum methodologies including creating requirements test plans Skilled in Advanced Regression Modeling Correlation Multivariate Analysis Model Building Business Intelligence tools and application of Statistical Concepts Developed predictive models using Decision Tree Naive Bayes Logistic Regression Random Forest Social Network Analysis Cluster Analysis and Neural Networks Experienced in Machine Learning and Statistical Analysis with Python ScikitLearn Experienced in Python to manipulate data for data loading and extraction and worked with python libraries like Matplotlib Scipy Numpy and Pandas for data analysis Strong SQL programming skills with experience in working with functions packages and triggers Expertise in transforming business requirements into designing algorithms analytical models building models developing data mining and reporting solutions that scales across massive volume of structured and unstructured data Skilled in performing data parsing data manipulation data architecture data ingestion and data preparation with methods including describe data contents compute descriptive statistics of data regex split and combine merge Remap subset reindex melt and reshape Worked with NoSQLDatabase including Hbase Cassandra and MongoDB Experienced in Big Data with Hadoop MapReduce HDFS and Spark Experienced in Data Integration Validation and Data Quality controls for ETL process and Data Warehousing using MS Visual Studio SSAS SSISand SSRS Proficient in Tableau and RShiny data visualization tools to analyze and obtain insights into large datasets create visually powerful and actionable interactive reports and dashboards Automated recurring reports using SQL andPython and visualized them on BI platform like Tableau Worked in development environment like Git and VM Excellent communication skills Successfully working in fastpaced multitasking environment both independently and in collaborative team a selfmotivated enthusiastic learner Work Experience Data Resarcher Data Scientist USAA San Antonio TX April 2019 to Present Description The United Services Automobile Association USAA is a San Antonio Texasbased Fortune 500 diversified financial services group of companies including a Texas Department of Insuranceregulated reciprocal interinsurance exchange and subsidiaries offering banking investing and insurance to people and families who serve or served in the United States Armed ForcesAt the end of 2017 there were 124 million members Responsibilities Test and determine whether new technology is potentially useful for USAA Extracting Opearations data from Workday and storing it in Hadoop Getting the data in Pickle file Format and Parsing it by CSV Build a graphic database using DataStax in Python ObjectOriented Programming Query datainformation from graphic database using Gremlin to support model validation Work in a group to develop Machine Learning guidance for model development and validation Build analytic tools to prepare data and graphs using Power Query and GraphX Draft the procedures for reporting Worked on ETL tools such as Apache Airflow Build a auto DAG system which would automatically trigger the workflow whenever the Airflow will receive HTTP request Write python scripts to parse documents Take online courses as a means to continuously learn new subjects Did intensive research on the tools like Graph Networks Scheduling Tools and Data Wrangling Tools to determine the best tool available in the market that would be best fit for companys need Environment Windows Python 3 DataStax GraphX Apache Airflow SQL Data Scientist Machine Learning Rauxa New York NY August 2018 to March 2019 Description Makers of results Rauxa applies data technology and content to create measurable impact at maximum speed for clients that include Gap Inc TGI Fridays and Verizon The countrys largest womanowned independent advertising agency Responsibilities Built models using Statistical techniques like Bayesian HMM and Machine Learning classification models like XGBoost SVM and Random Forest Participated in all phases of data mining data cleaning data collection developing models validation visualization and performed Gap analysis A highly immersive Data Science program involving Data ManipulationVisualization Web Scraping Machine Learning Python programming SQL GIT MongoDB Hadoop Setup storage and data analysis tools in AWS cloud computing infrastructure Installed and used Tensorflow a Deep Learning Framework Used pandas numpy seaborn matplotlib scikitlearn scipy NLTK in Python for developing various machine learning algorithms Data Manipulation and Aggregation from different source using Nexus Business Objects Toad Power BI and Smart View Programmed a utility in Python that used multiple packages numpy scipy pandas Implemented Classification using supervised algorithms like Logistic Regression Decision trees Naive Bayes KNN As Architect delivered various complex OLAP databasescubes scorecards dashboards and reports Updated Python scripts to match training data with our database stored in AWS Cloud Search so that we would be able to assign each document a response label for further classification Data transformation from various resources data organization features extraction from raw and stored Validated the machine learning classifiers using ROC Curves and Lift Charts Environment Unix Python 352 MLLib SAS regression logistic regression Hadoop 274 NoSQL Teradata OLTP random forest OLAP HDFS ODS NLTK SVM JSON XML and MapReduce Data Scientist Charter communication St Louis MO May 2017 to July 2018 Description Charter Communications Inc is an American telecommunications and mass media company that offers its services to consumers and businesses under the branding of Spectrum Responsibilities Utilized Scala Hadoop pySpark Data Lake TensorFlow MongoDB AWS Python a broad variety of machine learning methods including classifications regressions dimensionally reduction etc Developed sentiment analysis framework for email conversation and Customer Satisfaction scoreCSAT correlation Application of various machine learning algorithms and statistical modeling like decision trees text analytics natural language processing NLP supervised and unsupervised regression models social network analysis neural networks deep learning SVM clustering to identify Volume using scikitlearn package in python Matlab Sentiment Analytics engine was completely built in Python3 Email chain was considered for analysis Emails were pulled into DB from a tool Email2DB Each email transactions was preprocessed sentiment calculation subjectivity polarity was taken and linked to the CSAT score for dashboarding Emite was used as a tool of choice for visualization RapidMiner and PredictionIO was used as tool of choice for predictive analysis ITSM data was taken and build a engine for predicting CPU failure Memory Issue Disk Space Issue Server failure Errors Training data was build from either of proactive and reactive ticket data which was in turn to be used to make Classification Model for prediction Identifying and evaluating potential vendors for technology fitments performing proof of value exercise conducting commercial and contract negotiations Performed Data Profiling to learn about behavior with various features such as traffic pattern location Date and Time etc Categorized comments into positive and negative clusters from different social networking sites using Sentiment Analysis and Text Analytics Performed Multinomial Logistic Regression Decision Tree Random forest SVM to classify package is going to deliver on time for the new route Performed data analysis by using Hive to retrieve the data from Hadoop cluster Sql to retrieve datafrom Oracle database and used ETL for data transformation Performed Data Cleaning features scaling features engineering using pandas and numpy packages in python Exploring DAGs their dependencies and logs using AirFlow pipelines for automation Performed data cleaning and feature selection using MLlib package in PySpark and working with deep learning frameworks such as Tensorflow and PyTorch Developed SparkScalaR Python for regular expression regex project in the HadoopHive environment with LinuxWindows for big data resources Used clustering technique KMeans to identify outliers and to classify unlabeled data Communicated the results with operations team for taking best decisions Collected data needs and requirements by Interacting with the other departments Environment Python 2x CDH5 HDFS Hadoop 23 Hive Impala AWS Linux Spark Tableau Desktop SQL Server 2014 Microsoft Excel Matlab Spark SQL Pyspark Data analyst EDAC Technologies Corp Cheshire CT January 2016 to April 2017 Description EDAC Technologies Corporation provides design manufacturing and services for tooling fixtures molds jet engine components and machine spindles in the aerospace industrial semiconductor and medical device markets Responsibilities Worked with BI team in gathering the report requirements and also Sqoop to export data into HDFS and Hive Involved in the below phases of Analytics using R Python and Jupyter notebook Data collection and treatment Analysed existing internal data and external data worked on entry errorsclassification errors and defined criteria for missing values Data Mining Used cluster analysis for identifying customer segments Decision trees used for profitable and nonprofitable customers Market Basket Analysis used for customer purchasing behaviour and partproduct association Emite was used as a tool of choice for visualization Assisted with data capacity planning and node forecasting Installed Configured and managed Flume Infrastructure Worked closely with the claims processing team to obtain patterns in filing of fraudulent claims Worked on performing major upgrade of cluster from CDH3u6 to CDH440 Developed Map Reduce programs to extract and transform the data sets and results were exported back to RDBMS using Sqoop Patterns were observed in fraudulent claims using text mining in R and Hive Exported the data required information to RDBMS using Sqoop to make the data available for the claims processing team to assist in processing a claim based on the data Developed Map Reduce programs to parse the raw data populate staging tables and store the refined data in partitioned tables in the EDW Created tables in Hive and loaded the structured resulted from Map Reduce jobs data Created Hive queries that helped market analysts spot emerging trends by comparing fresh data with EDW reference tables and historical metrics Enabled speedy reviews and first mover advantages by using Oozie to automate data loading into the Hadoop Distributed File System and PIG to preprocess the data Provided design recommendations and thought leadership to sponsorsstakeholders that improved review processes and resolved technical problems Managed and reviewed Hadoop log files Tested raw data and executed performance scripts Environment HDFS PIG HIVE Map Reduce Linux HBase Flume Sqoop R VMware Eclipse Cloudera Python Data Analyst Dorman Products Inc Colmar PA March 2014 to December 2015 Description Dorman Products Inc supplies automotive replacement parts automotive hardware and brake products to the automotive aftermarket and mass merchandise markets in the United States Canada Mexico Europe the Middle East and Australia Responsibilities Created and maintained Logical and Physical models for the data mart Created partitions and indexes for the tables in the data mart Performed data profiling and analysis applied various data cleansing rules designed data standards and architecturedesigned the relational models Maintained metadata data definitions of table structures and version controlling for the data model Developed SQL scripts for creating tables Sequences Triggers views and materialized views Worked on query optimization and performance tuning using SQL Profiler and performance monitoring Developed mappings to load Fact and Dimension tables SCD Type 1 and SCD Type 2 dimensions and Incremental loading and unit tested the mappings Utilized Erwins forward reverse engineering tools and target database schema conversion process Worked on creating enterprise wide Model EDM for products and services in Teradata Environment based on the data from PDM Conceived designed developed and implemented this model from the scratch Building publishing customized interactive reports and dashboards report scheduling using Tableau server Write SQL scripts to test the mappings and Developed Traceability Matrix of Business Requirements mapped to Test Scripts to ensure any Change Control in requirements leads to test case update Responsible for development and testing of conversion programs for importing Data from text files into map Oracle Database utilizing PERL shell scripts SQLLoader Involved in extensive DATA validation by writing several complex SQL queries and Involved in backend testing and worked with data quality issues Developed and executed load scripts using Teradata client utilities MULTILOAD FASTLOAD and BTEQ Exporting and importing the data between different platforms such as SAS MSExcel Generated periodic reports based on the statistical analysis of the data using SQL Server Reporting Services SSRS Worked with the ETL team to document the Transformation Rules for Data Migration from OLTP to Warehouse Environment for reporting purposes Created SQL scripts to find data quality issues and to identify keys data anomalies and data validation issues Formatting the data sets read into SAS by using Format statement in the data step as well as Proc Format Applied Business Objects best practices during development with a strong focus on reusability and better performance Developed Tableau visualizations and dashboards using Tableau Desktop Used Graphical Entity Relationship Diagramming to create new database design via easy to use graphical interface Designed different type of STAR schemas for detailed data marts and plan data marts in the OLAP environment Environment Erwin MS SQL Server 2008 DB2 Oracle SQL Developer PLSQL Business Objects Erwin MS office suite Windows XP TOAD SQLPLUS SQLLOADER Teradata Netezza SAS Tableau Business Objects SSRS tableau SQL Assistant Informatica XML Python Developer Dabur India Ltd Ghaziabad Uttar Pradesh December 2012 to February 2014 Description Dabur is one of the Indias largest Ayurvedic medicine natural consumer products manufacturer Dabur demerged its Pharma business in 2003 and hived it off into a separate company Dabur Pharma Ltd German company Fresenius SE bought a 7327 equity stake in Dabur Pharma in June 2008 at Rs 7650 a share Responsibilities Involved in the design development and testing phases of application using AGILE methodology Designed and maintained databases using Python and developed Python based API RESTful Web Service using Flask SQLAlchemy and PostgreSQL Designed and developed the UI of the website using HTML XHTML AJAX CSS and JavaScript Participated in requirement gathering and worked closely with the architect in designing and modeling Worked on Restful web services which enforced a stateless client server and support JSON few changes from SOAP to RESTFUL Technology Involved in detailed analysis based on the requirement documents Involved in writing SQL queries implementing functions triggers cursors object types sequences indexes etc Created and managed all of hosted or local repositories through Source Trees simple interface of GIT client collaborated with GIT command lines and Stash Responsible for setting up Python REST API framework and spring frame work using Django Develope consumer based features and applications using Python Django HTML behavior Driven Development BDD and pair based programming Designed and developed components using Python with Django framework Implemented code in python to retrieve and manipulate data Involved in development of the enterprise social network application using Python Twisted and Cassandra Used Python and Django creating graphics XML processing of documents data exchange and business logic implementation between servers orked closely with backend developer to find ways to push the limits of existing Web technology Designed and developed the UI for the website with HTML XHTML CSS Java Script and AJAX Used AJAXJSON communication for accessing RESTfulweb services data payload Designed dynamic clientside JavaScript codes to build web forms and performed simulations for web application page Created and implemented SQL Queries Stored procedures Functions Packages and Triggers in SQL Server Successfully implemented Auto CompleteAuto Suggest functionality using JQuery Ajax Web Service and JSON Environment Python 25 JavaJ2EE Django10 HTMLCSS Linux Shell Scripting Java Script Ajax JQuery JSON XML PostgreSQL Jenkins ANT Maven Subversion Python Data Analyst Kotak Mahindra Bank Mumbai Maharashtra January 2011 to November 2012 Description Kotak Mahindra Bank is an Indian private sector bank headquartered in Mumbai Maharashtra India In February 2003 Reserve Bank of India issued the licence to Kotak Mahindra Finance Ltd the groups flagship company to carry on banking business Responsibilities Analyzed data sources and requirements and business rules to perform logical and physical data modeling Analyzed and designed best fit logical and physical data models and relational database definitions using DB2 Generated reports of data definitions Involved in NormalizationDenormalization Normal Form and database design methodology Maintained existing ETL procedures fixed bugs and restored software to production environment Developed the code as per the clients requirements using SQL PLSQL and Data Warehousing concepts Involved in Dimensional modeling Star Schema of the Data warehouse and used Erwin to design the business process dimensions and measured facts Worked with Data Warehouse Extract and load developers to design mappings for Data Capture Staging Cleansing Loading and Auditing Developed enterprise data model management process to manage multiple data models developed by different groups Designed and created Data Marts as part of a data warehouse Wrote complex SQL queries for validating the data against different kinds of reports generated by Business Objects XIR2 Using Erwin modeling tool publishing of a data dictionary review of the model and dictionary with subject matter experts and generation of data definition language Coordinated with DBA in implementing the Database changes and also updating Data Models with changes implemented in development QA and Production Worked Extensively with DBA and Reporting team for improving the Report Performance with the Use of appropriate indexes and Partitioning Developed Data Mapping Transformation and Cleansing rules for the Master Data Management Architecture involved OLTP ODS and OLAP Tuned and coded optimization using different techniques like dynamic SQL dynamic cursors and tuning SQL queries writing generic procedures functions and packages Experienced in GUI Relational Database Management System RDBMS designing of OLAP system environment as well as Report Development Extensively used SQL TSQL and PLSQL to write stored procedures functions packages and triggers Analyzed of data report were prepared weekly biweekly monthly using MS Excel SQL UNIX Environment ER Studio Informatica Power Center 8191 Power Connect Power exchange Oracle 11g MainframesDB2 MS SQL Server 2008 SQLPLSQL XML Windows NT 40 Tableau Workday SPSS SAS Business Objects XML Tableau Unix Shell Scripting Teradata Netezza Aginity Education Bachelors Skills SQL",
    "unique_id": "f2144f3b-ba2a-42d5-8242-549411df6524"
}