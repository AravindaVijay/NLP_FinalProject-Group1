{
    "clean_data": "Sr Hadoop Developer Sr Hadoop span lDeveloperspan Sr Hadoop Developer Wells Fargo Charlotte NC Work Experience Sr Hadoop Developer Wells Fargo Charlotte NC January 2018 to Present Responsibilities Multiple Spark Jobs were written to perform Data Quality checks on data before files were moved to Data Processing Layer Implemented Spark using Scala and Spark SQL for faster testing and processing of data Designed and Modified Database tables and used HBASE Queries to insert and fetch data from tables Responsible for creating data pipeline using Kafka Flume and Spark Streaming for Twitter source to collect the sentiment tweets of Eaton customers about the reviews Involved in moving all log files generated from various sources to HDFS for further processing through Flume170 Involved in deploying the applications in AWS and maintains the EC2 Elastic Computing Cloud and RDS Relational Database Services in amazon web services Implemented the file validation framework UDFs UDTFs and DAOs Strong experienced in working with UNIXLINUX environments writing UNIX shell scripts Python Created reporting views in Impala using Sentry Policy files different databases like MySQL RDBMS into HDFS and HBASE using Sqoop Advanced knowledge in performance troubleshooting and tuning Cassandra clusters Analyzing the source data to know the quality of data by using Talend Data Quality Involved in creating Hive tables loading with data and writing hive queries Developed REST APIs using Java Play framework and Akka Model and Create the consolidated Cassandra Filo DB and Spark tables based on the data profiling Used OOZIE121Operational Services for batch processing and scheduling workflows dynamically and created UDFs to store specialized data structures in HBase and Cassandra Developed multiple MapReduce jobs in Java for data cleaning and preprocessing Used Impala to read write and query the Hadoop data in HDFS from Cassandra and configured Kafka to read and write messages from external programs Optimizing existing algorithms in Hadoop using Spark Context SparkSQL Data Frames and Pair RDDs Create a complete processing engine based on Cloudera distribution enhanced to performance Environment Hadoop HDFS MapReduce Yarn Hive Pig HBase Oozie Sqoop Kafka Flume Oracle 11g FiloDB Spark Akka Scala Cloudera HDFS Talend Eclipse Oozie Nodejs UnixLinux Aws JQuery Ajax Python Zookeeper Hadoop Bigdata Developer Target MN April 2016 to January 2018 Responsibilities Developed efficient MapReduce programs for filtering out the unstructured data and developed multiple MapReduce jobs to perform data cleaning and preprocessing on Hortonworks Implemented Data Interface to get information of customers using RestAPIand PreProcessdata using MapReduce20 and store into HDFS Hortonworks Extracted files from MySQL Oracle and Teradata2 through Sqoop146and placed in HDFS Cloudera Distribution and processed Worked with various HDFS file formats like Avro176 Sequence File Json and various compression formats like Snappy bzip2 Successfully written Spark Streaming application to read streaming twitter data and analyze twitter records in real time using kafka and flume to measure performance of Apache spark streaming Proficient in designing Row keys and Schema Design for NoSQL Database Hbaseand knowledge of other NOSQL database Cassandra Used Hive to perform data validation on the data ingested using scoop and flume and the cleansed data set is pushed into Hbase Good understanding of Cassandra Data Modeling based on applications Wrote ETL jobs to read from web APIs using REST and HTTP calls and loaded into HDFS using java and Talend Developed the Pig 0150UDFs to preprocess the data for analysis and Migrated ETL operations into Hadoop system using Pig Latin scripts and Python Scripts351 Used Pig as ETL tool to do transformations event joins filtering and some preaggregations before storing the data into HDFS Troubleshooting debugging altering Talend issues while maintaining the health and performance of the ETL environment Loaded data into the cluster from dynamically generated files using Flume and from relational database management systems using Sqoop Used spark to parse XML files and extract values from tags and load it into multiple hive tables Experienced in running Hadoop streaming jobs to process terabytes of formatted data using Pythonscripts Developed small distributed applications in our projects using Zookeeper347and scheduled the workflows using Oozie 420 Proficiency in writing the UnixLinux shell commands Developed a SCP Stimulator which emulates the behavior of intelligent networking and Interacts with SSF Environment Hadoop HDFS MapReduce Yarn Hive Pig HBase Oozie Sqoop Kafka Flume Oracle 11g Spark Scala Cloudera HDFS Talend Eclipse Oozie UnixLinux Aws Python Perl Zookeeper Hadoop Bigdata Developer Middle by Corp Elgin IL October 2014 to March 2016 Responsibilities Developed multiple MapReduce jobs in java for data cleaning and preprocessing Performed Map Reduce Programs those are running on the cluster Involved in loading data from RDBMS and web logs into HDFS using Sqoop and Flume Worked on loading the data from MySQL to HBase where necessary using Sqoop Configured Hadoop cluster with Name node and slaves and formatted HDFS Performed Importing and exporting data from Oracle to HDFS and Hive using Sqoop Performed source data ingestion cleansing and transformation in Hadoop Supported MapReduce Programs running on the cluster Wrote Pig Scripts to perform ETL procedures on the data in HDFS Used Oozie workflow engine to run multiple Hive and Pig jobs Analyzed the partitioned and bucketed data and compute various metrics for reporting Created HBase tables to store various data formats of data coming from different portfolios Worked on improving the performance of existing Pig and Hive Queries Involved in developing HiveUDFs and reused in some other requirements Worked on performing Join operations Developed fingerprinting rules on HIVE which help in uniquely identifying a driver profile Evaluated business requirements and prepared detailed specifications that follow project guidelines required to develop written programs Exported the result set from Hive to MySQL using Sqoop after processing the data Analyzed the data by performing Hive queries and running Pig scripts to study customer behavior Used Hive to partition and bucket data Environment Hadoop MapReduce HDFS HBase HDP Horton Sqoop Data Processing Layer HUE AZURE Erwin MS Visio Tableau SQL MongoDB Oozie UNIX MySQL RDBMS Ambari Solr Cloud Lily HBase Cron Java developer YahSat Abu Dhabi June 2011 to September 2014 Responsibilities Involved in complete requirement analysis design coding and testing phases of the project Developed the application using which is based on Model View Controller design pattern Extensively used Hibernate in data access layer to perform database operations Used Spring Framework for Dependency Injection and integrated it with the Struts Framework and Hibernate Developed front end using Struts framework Configured Struts DynaAction Forms Message Resources Action Messages Action Errors Validationxml and Validatorrulesxml Designed and developed frontend using struts framework Used JSP JavaScript JSTL EL Custom Tag libraries and Validations provided by struts framework Used Web services WSDL and SOAP for getting credit card information from third party Worked on advanced Hibernate associations with multiple levels of Caching lazy loading Created Use case Sequence diagrams functional specifications and User Interface diagrams using Star UML Worked in Agile development environment in sprint cycles of two weeks by dividing and organizing tasks Participated in daily scrum and other design related meetings Designed various tables required for the project in Oracle 9i database and used Stored Procedures and Triggers in the application Involved in consuming RESTful Web services to render the data to the front page Performed unit testing using JUnit framework Environment HTML JSP Servlets JDBC JavaScript Java API Spring 30 Spring MVC JDBC Maven SVN Servlets Struts Amazon WS RESTful Web Services Bootstrap Java developer ST Electronics Singapore July 2010 to May 2011 Responsibilities Created Use case Sequence diagrams functional specifications and User Interface diagrams using Star UML Involved in complete requirement analysis design coding and testing phases of the project Participated in JAD meetings to gather the requirements and understand the End Users System Developed user interfaces using JSP HTML XML and JavaScript Created Stored Procedures Functions Used JDBC to process database calls for DB2AS400 and SQL Server databases Developed the code which will create XML files and Flat files with the data retrieved from Databases and XML files Created Data sources and Helper classes which will be utilized by all the interfaces to access the data and manipulate the data Used Servlets to implement business components Designed and Developed required service classes for database operation Developed web application called iHUB integration hub to initiate all the interface processes using Struts Framework JSP and HTML Used Java Script validation in JSP pages Developed the interfaces using Eclipse 311 and JBoss 41 Involved in integrated testing Bug fixing and in Production Support Environment HTML JSP Servlets JDBC JavaScript Tomcat Eclipse IDE XML XSL Tomcat 5 Education Bachelors Skills Apache cassandra Cassandra Hdfs Impala Mapreduce",
    "entities": [
        "Used OOZIE121Operational Services",
        "Spark Context",
        "Snappy bzip2 Successfully",
        "Used Spring Framework for Dependency Injection",
        "HDFS",
        "UNIX",
        "Validatorrulesxml Designed",
        "HTTP",
        "Eaton",
        "Migrated ETL",
        "Star UML Worked",
        "Hadoop",
        "XML",
        "NOSQL",
        "JAD",
        "Horton Sqoop Data Processing Layer HUE",
        "JUnit",
        "Talend Data Quality Involved",
        "JavaScript Created Stored Procedures Functions",
        "HBase",
        "Sr Hadoop Developer Sr Hadoop",
        "IDE XML",
        "SQL Server",
        "Environment Hadoop HDFS MapReduce Yarn Hive Pig HBase Oozie",
        "UNIXLINUX",
        "Servlets",
        "Sequence",
        "Data Processing Layer Implemented Spark",
        "Teradata2",
        "Oracle 9i",
        "Corp Elgin",
        "Hortonworks Implemented Data Interface",
        "the End Users System Developed",
        "Created Data",
        "JSP",
        "Star UML Involved",
        "Spark Streaming",
        "Talend",
        "RDS Relational Database Services",
        "the Struts Framework and Hibernate Developed",
        "MVC",
        "UnixLinux",
        "Spark",
        "User Interface",
        "Sqoop146and",
        "Sqoop",
        "HIVE",
        "Talend Developed the Pig",
        "SSF Environment Hadoop HDFS MapReduce Yarn Hive Pig HBase Oozie",
        "Created Use",
        "Tomcat 5 Education",
        "AWS",
        "PreProcessdata",
        "Present Responsibilities Multiple Spark Jobs",
        "java",
        "Data Quality",
        "Flume170 Involved",
        "Hive",
        "the EC2 Elastic Computing Cloud",
        "EL Custom Tag",
        "Wells Fargo Charlotte NC",
        "ETL",
        "Maven",
        "Performed",
        "Hadoop Supported MapReduce Programs",
        "Hibernate",
        "Impala",
        "ST Electronics",
        "Helper",
        "SVN",
        "Created HBase",
        "Cassandra Data Modeling",
        "REST",
        "MapReduce",
        "RDBMS",
        "Python Created",
        "Wells Fargo Charlotte",
        "Zookeeper Hadoop Bigdata Developer Target"
    ],
    "experience": "Experience Sr Hadoop Developer Wells Fargo Charlotte NC January 2018 to Present Responsibilities Multiple Spark Jobs were written to perform Data Quality checks on data before files were moved to Data Processing Layer Implemented Spark using Scala and Spark SQL for faster testing and processing of data Designed and Modified Database tables and used HBASE Queries to insert and fetch data from tables Responsible for creating data pipeline using Kafka Flume and Spark Streaming for Twitter source to collect the sentiment tweets of Eaton customers about the reviews Involved in moving all log files generated from various sources to HDFS for further processing through Flume170 Involved in deploying the applications in AWS and maintains the EC2 Elastic Computing Cloud and RDS Relational Database Services in amazon web services Implemented the file validation framework UDFs UDTFs and DAOs Strong experienced in working with UNIXLINUX environments writing UNIX shell scripts Python Created reporting views in Impala using Sentry Policy files different databases like MySQL RDBMS into HDFS and HBASE using Sqoop Advanced knowledge in performance troubleshooting and tuning Cassandra clusters Analyzing the source data to know the quality of data by using Talend Data Quality Involved in creating Hive tables loading with data and writing hive queries Developed REST APIs using Java Play framework and Akka Model and Create the consolidated Cassandra Filo DB and Spark tables based on the data profiling Used OOZIE121Operational Services for batch processing and scheduling workflows dynamically and created UDFs to store specialized data structures in HBase and Cassandra Developed multiple MapReduce jobs in Java for data cleaning and preprocessing Used Impala to read write and query the Hadoop data in HDFS from Cassandra and configured Kafka to read and write messages from external programs Optimizing existing algorithms in Hadoop using Spark Context SparkSQL Data Frames and Pair RDDs Create a complete processing engine based on Cloudera distribution enhanced to performance Environment Hadoop HDFS MapReduce Yarn Hive Pig HBase Oozie Sqoop Kafka Flume Oracle 11 g FiloDB Spark Akka Scala Cloudera HDFS Talend Eclipse Oozie Nodejs UnixLinux Aws JQuery Ajax Python Zookeeper Hadoop Bigdata Developer Target MN April 2016 to January 2018 Responsibilities Developed efficient MapReduce programs for filtering out the unstructured data and developed multiple MapReduce jobs to perform data cleaning and preprocessing on Hortonworks Implemented Data Interface to get information of customers using RestAPIand PreProcessdata using MapReduce20 and store into HDFS Hortonworks Extracted files from MySQL Oracle and Teradata2 through Sqoop146and placed in HDFS Cloudera Distribution and processed Worked with various HDFS file formats like Avro176 Sequence File Json and various compression formats like Snappy bzip2 Successfully written Spark Streaming application to read streaming twitter data and analyze twitter records in real time using kafka and flume to measure performance of Apache spark streaming Proficient in designing Row keys and Schema Design for NoSQL Database Hbaseand knowledge of other NOSQL database Cassandra Used Hive to perform data validation on the data ingested using scoop and flume and the cleansed data set is pushed into Hbase Good understanding of Cassandra Data Modeling based on applications Wrote ETL jobs to read from web APIs using REST and HTTP calls and loaded into HDFS using java and Talend Developed the Pig 0150UDFs to preprocess the data for analysis and Migrated ETL operations into Hadoop system using Pig Latin scripts and Python Scripts351 Used Pig as ETL tool to do transformations event joins filtering and some preaggregations before storing the data into HDFS Troubleshooting debugging altering Talend issues while maintaining the health and performance of the ETL environment Loaded data into the cluster from dynamically generated files using Flume and from relational database management systems using Sqoop Used spark to parse XML files and extract values from tags and load it into multiple hive tables Experienced in running Hadoop streaming jobs to process terabytes of formatted data using Pythonscripts Developed small distributed applications in our projects using Zookeeper347and scheduled the workflows using Oozie 420 Proficiency in writing the UnixLinux shell commands Developed a SCP Stimulator which emulates the behavior of intelligent networking and Interacts with SSF Environment Hadoop HDFS MapReduce Yarn Hive Pig HBase Oozie Sqoop Kafka Flume Oracle 11 g Spark Scala Cloudera HDFS Talend Eclipse Oozie UnixLinux Aws Python Perl Zookeeper Hadoop Bigdata Developer Middle by Corp Elgin IL October 2014 to March 2016 Responsibilities Developed multiple MapReduce jobs in java for data cleaning and preprocessing Performed Map Reduce Programs those are running on the cluster Involved in loading data from RDBMS and web logs into HDFS using Sqoop and Flume Worked on loading the data from MySQL to HBase where necessary using Sqoop Configured Hadoop cluster with Name node and slaves and formatted HDFS Performed Importing and exporting data from Oracle to HDFS and Hive using Sqoop Performed source data ingestion cleansing and transformation in Hadoop Supported MapReduce Programs running on the cluster Wrote Pig Scripts to perform ETL procedures on the data in HDFS Used Oozie workflow engine to run multiple Hive and Pig jobs Analyzed the partitioned and bucketed data and compute various metrics for reporting Created HBase tables to store various data formats of data coming from different portfolios Worked on improving the performance of existing Pig and Hive Queries Involved in developing HiveUDFs and reused in some other requirements Worked on performing Join operations Developed fingerprinting rules on HIVE which help in uniquely identifying a driver profile Evaluated business requirements and prepared detailed specifications that follow project guidelines required to develop written programs Exported the result set from Hive to MySQL using Sqoop after processing the data Analyzed the data by performing Hive queries and running Pig scripts to study customer behavior Used Hive to partition and bucket data Environment Hadoop MapReduce HDFS HBase HDP Horton Sqoop Data Processing Layer HUE AZURE Erwin MS Visio Tableau SQL MongoDB Oozie UNIX MySQL RDBMS Ambari Solr Cloud Lily HBase Cron Java developer YahSat Abu Dhabi June 2011 to September 2014 Responsibilities Involved in complete requirement analysis design coding and testing phases of the project Developed the application using which is based on Model View Controller design pattern Extensively used Hibernate in data access layer to perform database operations Used Spring Framework for Dependency Injection and integrated it with the Struts Framework and Hibernate Developed front end using Struts framework Configured Struts DynaAction Forms Message Resources Action Messages Action Errors Validationxml and Validatorrulesxml Designed and developed frontend using struts framework Used JSP JavaScript JSTL EL Custom Tag libraries and Validations provided by struts framework Used Web services WSDL and SOAP for getting credit card information from third party Worked on advanced Hibernate associations with multiple levels of Caching lazy loading Created Use case Sequence diagrams functional specifications and User Interface diagrams using Star UML Worked in Agile development environment in sprint cycles of two weeks by dividing and organizing tasks Participated in daily scrum and other design related meetings Designed various tables required for the project in Oracle 9i database and used Stored Procedures and Triggers in the application Involved in consuming RESTful Web services to render the data to the front page Performed unit testing using JUnit framework Environment HTML JSP Servlets JDBC JavaScript Java API Spring 30 Spring MVC JDBC Maven SVN Servlets Struts Amazon WS RESTful Web Services Bootstrap Java developer ST Electronics Singapore July 2010 to May 2011 Responsibilities Created Use case Sequence diagrams functional specifications and User Interface diagrams using Star UML Involved in complete requirement analysis design coding and testing phases of the project Participated in JAD meetings to gather the requirements and understand the End Users System Developed user interfaces using JSP HTML XML and JavaScript Created Stored Procedures Functions Used JDBC to process database calls for DB2AS400 and SQL Server databases Developed the code which will create XML files and Flat files with the data retrieved from Databases and XML files Created Data sources and Helper classes which will be utilized by all the interfaces to access the data and manipulate the data Used Servlets to implement business components Designed and Developed required service classes for database operation Developed web application called iHUB integration hub to initiate all the interface processes using Struts Framework JSP and HTML Used Java Script validation in JSP pages Developed the interfaces using Eclipse 311 and JBoss 41 Involved in integrated testing Bug fixing and in Production Support Environment HTML JSP Servlets JDBC JavaScript Tomcat Eclipse IDE XML XSL Tomcat 5 Education Bachelors Skills Apache cassandra Cassandra Hdfs Impala Mapreduce",
    "extracted_keywords": [
        "Sr",
        "Hadoop",
        "Developer",
        "Sr",
        "Hadoop",
        "span",
        "lDeveloperspan",
        "Sr",
        "Hadoop",
        "Developer",
        "Wells",
        "Fargo",
        "Charlotte",
        "NC",
        "Work",
        "Experience",
        "Sr",
        "Hadoop",
        "Developer",
        "Wells",
        "Fargo",
        "Charlotte",
        "NC",
        "January",
        "Present",
        "Responsibilities",
        "Multiple",
        "Spark",
        "Jobs",
        "Data",
        "Quality",
        "checks",
        "data",
        "files",
        "Data",
        "Processing",
        "Layer",
        "Spark",
        "Scala",
        "Spark",
        "SQL",
        "testing",
        "processing",
        "data",
        "Modified",
        "Database",
        "tables",
        "HBASE",
        "Queries",
        "data",
        "tables",
        "data",
        "pipeline",
        "Kafka",
        "Flume",
        "Spark",
        "Streaming",
        "Twitter",
        "source",
        "sentiment",
        "tweets",
        "Eaton",
        "customers",
        "reviews",
        "log",
        "files",
        "sources",
        "HDFS",
        "processing",
        "Flume170",
        "applications",
        "AWS",
        "EC2",
        "Elastic",
        "Computing",
        "Cloud",
        "RDS",
        "Relational",
        "Database",
        "Services",
        "amazon",
        "web",
        "services",
        "file",
        "validation",
        "framework",
        "UDFs",
        "UDTFs",
        "DAOs",
        "Strong",
        "UNIXLINUX",
        "environments",
        "UNIX",
        "shell",
        "scripts",
        "Python",
        "reporting",
        "views",
        "Impala",
        "Sentry",
        "Policy",
        "files",
        "databases",
        "MySQL",
        "RDBMS",
        "HDFS",
        "HBASE",
        "Sqoop",
        "knowledge",
        "performance",
        "Cassandra",
        "clusters",
        "source",
        "data",
        "quality",
        "data",
        "Talend",
        "Data",
        "Quality",
        "Hive",
        "tables",
        "data",
        "hive",
        "REST",
        "APIs",
        "Java",
        "Play",
        "framework",
        "Akka",
        "Model",
        "Cassandra",
        "Filo",
        "DB",
        "Spark",
        "tables",
        "data",
        "profiling",
        "OOZIE121Operational",
        "Services",
        "batch",
        "processing",
        "scheduling",
        "workflows",
        "UDFs",
        "data",
        "structures",
        "HBase",
        "Cassandra",
        "Developed",
        "MapReduce",
        "jobs",
        "Java",
        "data",
        "Impala",
        "Hadoop",
        "data",
        "HDFS",
        "Cassandra",
        "Kafka",
        "messages",
        "programs",
        "algorithms",
        "Hadoop",
        "Spark",
        "Context",
        "SparkSQL",
        "Data",
        "Frames",
        "Pair",
        "RDDs",
        "processing",
        "engine",
        "Cloudera",
        "distribution",
        "performance",
        "Environment",
        "Hadoop",
        "HDFS",
        "MapReduce",
        "Yarn",
        "Hive",
        "Pig",
        "HBase",
        "Oozie",
        "Sqoop",
        "Kafka",
        "Flume",
        "Oracle",
        "g",
        "FiloDB",
        "Spark",
        "Akka",
        "Scala",
        "Cloudera",
        "HDFS",
        "Talend",
        "Eclipse",
        "Oozie",
        "Nodejs",
        "UnixLinux",
        "Aws",
        "JQuery",
        "Ajax",
        "Python",
        "Zookeeper",
        "Hadoop",
        "Bigdata",
        "Developer",
        "Target",
        "MN",
        "April",
        "January",
        "Responsibilities",
        "MapReduce",
        "programs",
        "data",
        "MapReduce",
        "jobs",
        "data",
        "cleaning",
        "Hortonworks",
        "Implemented",
        "Data",
        "Interface",
        "information",
        "customers",
        "PreProcessdata",
        "HDFS",
        "Hortonworks",
        "files",
        "MySQL",
        "Oracle",
        "Teradata2",
        "Sqoop146and",
        "HDFS",
        "Cloudera",
        "Distribution",
        "Worked",
        "HDFS",
        "file",
        "formats",
        "Avro176",
        "Sequence",
        "File",
        "Json",
        "compression",
        "formats",
        "bzip2",
        "Spark",
        "Streaming",
        "application",
        "twitter",
        "data",
        "twitter",
        "records",
        "time",
        "kafka",
        "performance",
        "Apache",
        "spark",
        "Proficient",
        "Row",
        "keys",
        "Schema",
        "Design",
        "NoSQL",
        "Database",
        "Hbaseand",
        "knowledge",
        "NOSQL",
        "database",
        "Cassandra",
        "Hive",
        "data",
        "validation",
        "data",
        "scoop",
        "flume",
        "data",
        "set",
        "Hbase",
        "understanding",
        "Cassandra",
        "Data",
        "Modeling",
        "applications",
        "Wrote",
        "ETL",
        "jobs",
        "web",
        "APIs",
        "REST",
        "HTTP",
        "HDFS",
        "java",
        "Talend",
        "Pig",
        "0150UDFs",
        "data",
        "analysis",
        "ETL",
        "operations",
        "Hadoop",
        "system",
        "Pig",
        "Latin",
        "scripts",
        "Python",
        "Scripts351",
        "Pig",
        "ETL",
        "tool",
        "transformations",
        "event",
        "preaggregations",
        "data",
        "HDFS",
        "Troubleshooting",
        "debugging",
        "Talend",
        "issues",
        "health",
        "performance",
        "ETL",
        "environment",
        "data",
        "cluster",
        "files",
        "Flume",
        "database",
        "management",
        "systems",
        "Sqoop",
        "spark",
        "XML",
        "files",
        "values",
        "tags",
        "tables",
        "Hadoop",
        "streaming",
        "jobs",
        "terabytes",
        "data",
        "Pythonscripts",
        "applications",
        "projects",
        "Zookeeper347and",
        "workflows",
        "Oozie",
        "Proficiency",
        "UnixLinux",
        "shell",
        "SCP",
        "Stimulator",
        "behavior",
        "networking",
        "Interacts",
        "SSF",
        "Environment",
        "Hadoop",
        "HDFS",
        "MapReduce",
        "Yarn",
        "Hive",
        "Pig",
        "HBase",
        "Oozie",
        "Sqoop",
        "Kafka",
        "Flume",
        "Oracle",
        "g",
        "Spark",
        "Scala",
        "Cloudera",
        "HDFS",
        "Talend",
        "Eclipse",
        "Oozie",
        "UnixLinux",
        "Aws",
        "Python",
        "Perl",
        "Zookeeper",
        "Hadoop",
        "Bigdata",
        "Developer",
        "Middle",
        "Corp",
        "Elgin",
        "IL",
        "October",
        "March",
        "Responsibilities",
        "MapReduce",
        "jobs",
        "java",
        "data",
        "Performed",
        "Map",
        "Programs",
        "cluster",
        "loading",
        "data",
        "RDBMS",
        "web",
        "logs",
        "HDFS",
        "Sqoop",
        "Flume",
        "data",
        "MySQL",
        "HBase",
        "Sqoop",
        "Configured",
        "Hadoop",
        "cluster",
        "Name",
        "node",
        "slaves",
        "HDFS",
        "Performed",
        "Importing",
        "data",
        "Oracle",
        "HDFS",
        "Hive",
        "Sqoop",
        "Performed",
        "source",
        "data",
        "ingestion",
        "cleansing",
        "transformation",
        "Hadoop",
        "Supported",
        "MapReduce",
        "Programs",
        "cluster",
        "Wrote",
        "Pig",
        "Scripts",
        "ETL",
        "procedures",
        "data",
        "HDFS",
        "Oozie",
        "workflow",
        "engine",
        "Hive",
        "Pig",
        "jobs",
        "data",
        "metrics",
        "Created",
        "HBase",
        "data",
        "formats",
        "data",
        "portfolios",
        "performance",
        "Pig",
        "Hive",
        "Queries",
        "HiveUDFs",
        "requirements",
        "Join",
        "operations",
        "rules",
        "HIVE",
        "driver",
        "profile",
        "business",
        "requirements",
        "specifications",
        "project",
        "guidelines",
        "programs",
        "result",
        "Hive",
        "MySQL",
        "Sqoop",
        "data",
        "data",
        "Hive",
        "queries",
        "Pig",
        "scripts",
        "customer",
        "behavior",
        "Hive",
        "partition",
        "bucket",
        "data",
        "Environment",
        "Hadoop",
        "MapReduce",
        "HDFS",
        "HBase",
        "HDP",
        "Horton",
        "Sqoop",
        "Data",
        "Processing",
        "Layer",
        "HUE",
        "AZURE",
        "Erwin",
        "MS",
        "Visio",
        "Tableau",
        "SQL",
        "MongoDB",
        "Oozie",
        "UNIX",
        "MySQL",
        "RDBMS",
        "Ambari",
        "Solr",
        "Cloud",
        "Lily",
        "HBase",
        "Cron",
        "Java",
        "developer",
        "YahSat",
        "Abu",
        "Dhabi",
        "June",
        "September",
        "Responsibilities",
        "requirement",
        "analysis",
        "design",
        "phases",
        "project",
        "application",
        "Model",
        "View",
        "Controller",
        "design",
        "pattern",
        "Hibernate",
        "data",
        "access",
        "layer",
        "database",
        "operations",
        "Spring",
        "Framework",
        "Dependency",
        "Injection",
        "Struts",
        "Framework",
        "Hibernate",
        "end",
        "Struts",
        "framework",
        "Configured",
        "Struts",
        "DynaAction",
        "Forms",
        "Message",
        "Resources",
        "Action",
        "Messages",
        "Action",
        "Errors",
        "Validationxml",
        "Validatorrulesxml",
        "frontend",
        "struts",
        "framework",
        "JSP",
        "JavaScript",
        "JSTL",
        "EL",
        "Custom",
        "Tag",
        "libraries",
        "Validations",
        "struts",
        "framework",
        "Web",
        "services",
        "WSDL",
        "SOAP",
        "credit",
        "card",
        "information",
        "party",
        "Hibernate",
        "associations",
        "levels",
        "loading",
        "Use",
        "case",
        "Sequence",
        "diagrams",
        "specifications",
        "User",
        "Interface",
        "diagrams",
        "Star",
        "UML",
        "Worked",
        "Agile",
        "development",
        "environment",
        "sprint",
        "cycles",
        "weeks",
        "organizing",
        "tasks",
        "scrum",
        "design",
        "meetings",
        "tables",
        "project",
        "Oracle",
        "9i",
        "database",
        "Stored",
        "Procedures",
        "Triggers",
        "application",
        "Web",
        "services",
        "data",
        "page",
        "Performed",
        "unit",
        "testing",
        "JUnit",
        "framework",
        "Environment",
        "HTML",
        "JSP",
        "Servlets",
        "JDBC",
        "JavaScript",
        "Java",
        "API",
        "Spring",
        "Spring",
        "MVC",
        "JDBC",
        "Maven",
        "SVN",
        "Servlets",
        "Struts",
        "Amazon",
        "WS",
        "Web",
        "Services",
        "Bootstrap",
        "Java",
        "developer",
        "ST",
        "Electronics",
        "Singapore",
        "July",
        "May",
        "Responsibilities",
        "Use",
        "case",
        "Sequence",
        "diagrams",
        "specifications",
        "User",
        "Interface",
        "diagrams",
        "Star",
        "UML",
        "requirement",
        "analysis",
        "design",
        "phases",
        "project",
        "JAD",
        "meetings",
        "requirements",
        "End",
        "Users",
        "System",
        "user",
        "interfaces",
        "JSP",
        "HTML",
        "XML",
        "JavaScript",
        "Created",
        "Stored",
        "Procedures",
        "Functions",
        "JDBC",
        "database",
        "DB2AS400",
        "SQL",
        "Server",
        "code",
        "XML",
        "files",
        "files",
        "data",
        "Databases",
        "XML",
        "files",
        "Created",
        "Data",
        "sources",
        "Helper",
        "classes",
        "interfaces",
        "data",
        "data",
        "Servlets",
        "business",
        "components",
        "Developed",
        "service",
        "classes",
        "database",
        "operation",
        "Developed",
        "web",
        "application",
        "iHUB",
        "integration",
        "hub",
        "interface",
        "processes",
        "Struts",
        "Framework",
        "JSP",
        "HTML",
        "Java",
        "Script",
        "validation",
        "JSP",
        "pages",
        "interfaces",
        "Eclipse",
        "JBoss",
        "testing",
        "Bug",
        "fixing",
        "Production",
        "Support",
        "Environment",
        "HTML",
        "JSP",
        "Servlets",
        "JDBC",
        "JavaScript",
        "Tomcat",
        "Eclipse",
        "IDE",
        "XML",
        "XSL",
        "Tomcat",
        "Education",
        "Bachelors",
        "Skills",
        "Apache",
        "cassandra",
        "Cassandra",
        "Hdfs",
        "Impala",
        "Mapreduce"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T20:31:12.760589",
    "resume_data": "Sr Hadoop Developer Sr Hadoop span lDeveloperspan Sr Hadoop Developer Wells Fargo Charlotte NC Work Experience Sr Hadoop Developer Wells Fargo Charlotte NC January 2018 to Present Responsibilities Multiple Spark Jobs were written to perform Data Quality checks on data before files were moved to Data Processing Layer Implemented Spark using Scala and Spark SQL for faster testing and processing of data Designed and Modified Database tables and used HBASE Queries to insert and fetch data from tables Responsible for creating data pipeline using Kafka Flume and Spark Streaming for Twitter source to collect the sentiment tweets of Eaton customers about the reviews Involved in moving all log files generated from various sources to HDFS for further processing through Flume170 Involved in deploying the applications in AWS and maintains the EC2 Elastic Computing Cloud and RDS Relational Database Services in amazon web services Implemented the file validation framework UDFs UDTFs and DAOs Strong experienced in working with UNIXLINUX environments writing UNIX shell scripts Python Created reporting views in Impala using Sentry Policy files different databases like MySQL RDBMS into HDFS and HBASE using Sqoop Advanced knowledge in performance troubleshooting and tuning Cassandra clusters Analyzing the source data to know the quality of data by using Talend Data Quality Involved in creating Hive tables loading with data and writing hive queries Developed REST APIs using Java Play framework and Akka Model and Create the consolidated Cassandra Filo DB and Spark tables based on the data profiling Used OOZIE121Operational Services for batch processing and scheduling workflows dynamically and created UDFs to store specialized data structures in HBase and Cassandra Developed multiple MapReduce jobs in Java for data cleaning and preprocessing Used Impala to read write and query the Hadoop data in HDFS from Cassandra and configured Kafka to read and write messages from external programs Optimizing existing algorithms in Hadoop using Spark Context SparkSQL Data Frames and Pair RDDs Create a complete processing engine based on Cloudera distribution enhanced to performance Environment Hadoop HDFS MapReduce Yarn Hive Pig HBase Oozie Sqoop Kafka Flume Oracle 11g FiloDB Spark Akka Scala Cloudera HDFS Talend Eclipse Oozie Nodejs UnixLinux Aws JQuery Ajax Python Zookeeper Hadoop Bigdata Developer Target MN April 2016 to January 2018 Responsibilities Developed efficient MapReduce programs for filtering out the unstructured data and developed multiple MapReduce jobs to perform data cleaning and preprocessing on Hortonworks Implemented Data Interface to get information of customers using RestAPIand PreProcessdata using MapReduce20 and store into HDFS Hortonworks Extracted files from MySQL Oracle and Teradata2 through Sqoop146and placed in HDFS Cloudera Distribution and processed Worked with various HDFS file formats like Avro176 Sequence File Json and various compression formats like Snappy bzip2 Successfully written Spark Streaming application to read streaming twitter data and analyze twitter records in real time using kafka and flume to measure performance of Apache spark streaming Proficient in designing Row keys and Schema Design for NoSQL Database Hbaseand knowledge of other NOSQL database Cassandra Used Hive to perform data validation on the data ingested using scoop and flume and the cleansed data set is pushed into Hbase Good understanding of Cassandra Data Modeling based on applications Wrote ETL jobs to read from web APIs using REST and HTTP calls and loaded into HDFS using java and Talend Developed the Pig 0150UDFs to preprocess the data for analysis and Migrated ETL operations into Hadoop system using Pig Latin scripts and Python Scripts351 Used Pig as ETL tool to do transformations event joins filtering and some preaggregations before storing the data into HDFS Troubleshooting debugging altering Talend issues while maintaining the health and performance of the ETL environment Loaded data into the cluster from dynamically generated files using Flume and from relational database management systems using Sqoop Used spark to parse XML files and extract values from tags and load it into multiple hive tables Experienced in running Hadoop streaming jobs to process terabytes of formatted data using Pythonscripts Developed small distributed applications in our projects using Zookeeper347and scheduled the workflows using Oozie 420 Proficiency in writing the UnixLinux shell commands Developed a SCP Stimulator which emulates the behavior of intelligent networking and Interacts with SSF Environment Hadoop HDFS MapReduce Yarn Hive Pig HBase Oozie Sqoop Kafka Flume Oracle 11g Spark Scala Cloudera HDFS Talend Eclipse Oozie UnixLinux Aws Python Perl Zookeeper Hadoop Bigdata Developer Middle by Corp Elgin IL October 2014 to March 2016 Responsibilities Developed multiple MapReduce jobs in java for data cleaning and preprocessing Performed Map Reduce Programs those are running on the cluster Involved in loading data from RDBMS and web logs into HDFS using Sqoop and Flume Worked on loading the data from MySQL to HBase where necessary using Sqoop Configured Hadoop cluster with Name node and slaves and formatted HDFS Performed Importing and exporting data from Oracle to HDFS and Hive using Sqoop Performed source data ingestion cleansing and transformation in Hadoop Supported MapReduce Programs running on the cluster Wrote Pig Scripts to perform ETL procedures on the data in HDFS Used Oozie workflow engine to run multiple Hive and Pig jobs Analyzed the partitioned and bucketed data and compute various metrics for reporting Created HBase tables to store various data formats of data coming from different portfolios Worked on improving the performance of existing Pig and Hive Queries Involved in developing HiveUDFs and reused in some other requirements Worked on performing Join operations Developed fingerprinting rules on HIVE which help in uniquely identifying a driver profile Evaluated business requirements and prepared detailed specifications that follow project guidelines required to develop written programs Exported the result set from Hive to MySQL using Sqoop after processing the data Analyzed the data by performing Hive queries and running Pig scripts to study customer behavior Used Hive to partition and bucket data Environment Hadoop MapReduce HDFS HBase HDP Horton Sqoop Data Processing Layer HUE AZURE Erwin MS Visio Tableau SQL MongoDB Oozie UNIX MySQL RDBMS Ambari Solr Cloud Lily HBase Cron Java developer YahSat Abu Dhabi June 2011 to September 2014 Responsibilities Involved in complete requirement analysis design coding and testing phases of the project Developed the application using which is based on Model View Controller design pattern Extensively used Hibernate in data access layer to perform database operations Used Spring Framework for Dependency Injection and integrated it with the Struts Framework and Hibernate Developed front end using Struts framework Configured Struts DynaAction Forms Message Resources Action Messages Action Errors Validationxml and Validatorrulesxml Designed and developed frontend using struts framework Used JSP JavaScript JSTL EL Custom Tag libraries and Validations provided by struts framework Used Web services WSDL and SOAP for getting credit card information from third party Worked on advanced Hibernate associations with multiple levels of Caching lazy loading Created Use case Sequence diagrams functional specifications and User Interface diagrams using Star UML Worked in Agile development environment in sprint cycles of two weeks by dividing and organizing tasks Participated in daily scrum and other design related meetings Designed various tables required for the project in Oracle 9i database and used Stored Procedures and Triggers in the application Involved in consuming RESTful Web services to render the data to the front page Performed unit testing using JUnit framework Environment HTML JSP Servlets JDBC JavaScript Java API Spring 30 Spring MVC JDBC Maven SVN Servlets Struts Amazon WS RESTful Web Services Bootstrap Java developer ST Electronics Singapore July 2010 to May 2011 Responsibilities Created Use case Sequence diagrams functional specifications and User Interface diagrams using Star UML Involved in complete requirement analysis design coding and testing phases of the project Participated in JAD meetings to gather the requirements and understand the End Users System Developed user interfaces using JSP HTML XML and JavaScript Created Stored Procedures Functions Used JDBC to process database calls for DB2AS400 and SQL Server databases Developed the code which will create XML files and Flat files with the data retrieved from Databases and XML files Created Data sources and Helper classes which will be utilized by all the interfaces to access the data and manipulate the data Used Servlets to implement business components Designed and Developed required service classes for database operation Developed web application called iHUB integration hub to initiate all the interface processes using Struts Framework JSP and HTML Used Java Script validation in JSP pages Developed the interfaces using Eclipse 311 and JBoss 41 Involved in integrated testing Bug fixing and in Production Support Environment HTML JSP Servlets JDBC JavaScript Tomcat Eclipse IDE XML XSL Tomcat 5 Education Bachelors Skills Apache cassandra Cassandra Hdfs Impala Mapreduce",
    "unique_id": "e6ffde45-c3b9-4cd9-8ee2-9e49cffd1ddb"
}