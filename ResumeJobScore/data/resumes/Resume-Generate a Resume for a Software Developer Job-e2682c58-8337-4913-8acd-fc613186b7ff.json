{
    "clean_data": "Sr Hadoop Developer Sr Hadoop span lDeveloperspan Sr Hadoop Developer Walmart Bentonville AR 4 years of experience in developing applications that perform large scale Distributed Data Processing using Big Data ecosystem tools Hadoop Hive Pig Sqoop Hbase Cassandra Spark Spark Streaming MLLib Mahout Oozie Zoo Keeper Flume Yarn and Avro Passionate about Big Data Analytics and skilled in exploring data content and Expert in distributed computing algorithms and data analytics Hands on experience in using various Hadoop distributions Apache Cloudera Horton works MapR InDepth knowledge and experience in design development and deployments of Big Data projects using Hadoop Data Analytics NoSQL Distributed Machine Learning frameworks Solid understanding of SQL NOSQL databases such as Oracle Postgre SQL MySQL Mongo DB HBase Cassandra Knowledge on Mongo DB NoSQL data modeling tuning disaster recovery backup used it for distributed storage and processing using CRUD Worked on Mongo DB database concepts such as locking transactions indexes Sharing replication schema design Very good understanding Cassandra cluster mechanism that includes replication strategies snitch gossip consistent hashing and consistency levels Working knowledge in installing and maintaining Cassandra by configuring the cassandrayaml file as per the business requirement and performed readswrites using Java JDBC connectivity Experience with Cloudera Manager for management of Hadoop cluster Good conceptual understanding and experience in cloud computing applications using Amazon EC2 S3 EMR Experience in analyzing data using HiveQL Pig Latin HBase and custom Map Reduce programs in Java Extensive Experience on Teradata database analyzing business needs of clients developing effective and efficient solutions and ensuring client deliverables with in committed timelines Expertise in maintaining data quality data organization and metadata Experience in Coding Teradata SQL Teradata Stored Procedures Macros and Triggers Strong experience in Creating Database Objects such as Tables Views Functions Stored Procedures Indexes Triggers Cursors in Teradata Experience data processing like collecting aggregating moving from various sources using Apache Flume and Kafka Developed a data pipeline using Kafka and Spark Streaming to store data into HDFS and performed the realtime analytics on the incoming data Experience in importing the realtime data to Hadoop using Kafka and implemented the Oozie job for daily imports Experienced in working with inmemory processing framework like Spark Transformations SparkQL MLib and Spark Streaming Expertise in creating Custom Serdes in Hive Good working experience on using Sqoop to import data into HDFS from RDBMS and viceversa Expertise in job scheduling and monitoring tools like Oozie and Zoo Keeper Experience in design and development of Map Reduce Programs using Apache Hadoop for analyzing the big data as per the requirement Experience in performing adhoc queries on structured data using Hive QL and used Partition and Bucketing techniques and joins with HIVE for faster data access Experience in performing ETL operations using Pig Latin scripts Implemented Java APIs and created custom Java programs for fullfledged utilization of Hadoop and its related tools Implemented work flows that involve Hadoop actions using Oozie coordinators Experienced in implementing POC using Spark Sql and Mlib libraries Improving the performance and optimization of the existing algorithms in Hadoop using Spark Context SparkSQL Data Frame Pair RDDs YARN Hands on experience in handling Hive tables using Spark SQL Created User Defined Functions UDFs User Defined Aggregated Functions UDAFs in PIG and Hive Experience in dealing with log files to extract data and to copy into HDFS using Flume Experience with Testing Map Reduce programs using MR Unit JUnit and Easy Mock Implemented distributed searching capabilities using Solr to empower the geospatial search and navigation feature Experienced in Java Application Development ClientServer Applications InternetIntranet based applications using Core Java J2EE patterns Spring Hibernate Struts JMS Web Services SOAPREST Oracle SQL Server and other relational databases Experience writing Shell scripts in Linux OS and integrating them with other solutions Expert in developing web page interfaces using JSP Java Swings and HTML scripting languages Excellent understanding on Java beans and Hibernate framework to implement model logic to interact with RDBMS databases Experience in using IDEs like Eclipse NetBeans and Maven Hands on experienced working with source control tools such as Rational Clear Case and Clear Quest Hands on experience on writing Queries Stored procedures Functions and Triggers by using SQL Used EMR Elastic Map Reducing to perform bigdata operations in AWS Proficient using version control tools like GIT VSS SVN and PVCS Involvement in all stages of software development life cycle SDLC and follow agile methodologies and continuous delivery Strong skills in Object Oriented Analysis and Design OOAD Well versed in enterprise software development methodologies and practices including TDD BDD design patterns and performance testing Authorized to work in the US for any employer Work Experience Sr Hadoop Developer Walmart Bentonville AR May 2018 to Present Description Walmart Inc is an American multinational retail corporation that operates a chain of hypermarkets discount department stores and grocery storesweve been dedicated to making a difference in the lives of our customers Our business is the result of Sam Waltons visionary leadership along with generations of associates focused on helping customers and communities save money and live better Responsibilities Analyzed large and critical datasets using Cloudera HDFS HBase MapReduce Hive Hive UDF Pig Sqoop Zookeeper and Spark Gathered the business requirements from the Business Partners and Subject Matter Experts Developed environmental search engine using JAVA Apache SOLR and MYSQL Managed works including indexing data tuning relevance developing custom tokenizers and filters adding functionality includes playlist custom sorting and regionalization with SOLR Search Engine Ingested data from RDBMS and performed data transformations and then export the transformed data to Cassandra as per the business requirement Written multiple Map Reduce programs for data extraction transformation and aggregation from multiple file formats including XML JSON CSV other compressed file formats Developed automated processes for flattening the upstream data from Cassandra which in JSON format Used Hive UDFs to flatten the JSON Data Optimized Map Reduce Jobs to use HDFS efficiently by using various compression mechanisms Developed PIG UDFs to provide Pig capabilities for manipulating the data according to Business Requirements and worked on developing custom PIG Loaders and Implemented various requirements using Pig scripts Experienced on loading and transforming of large sets of structured semi structured and unstructured data Created POC using Spark Sql and Mlib libraries Developed a Spark Streaming module for consumption of Avro messages from Kafka Implementing different machine learning techniques in Scala using Scala machine learning library and created POC using SparkSql and Mlib libraries Experienced in Querying data using SparkSQL on top of Spark Engine implementing Spark RDDs in Scala Expertise in writing Scala code using Higher order functions for iterative algorithms in Spark for Performance considerations Experienced in managing and reviewing Hadoop log files Worked with different File Formats like TEXTFILE AVROFILE ORC and PARQUET for HIVE querying and processing Loading data by using the Teradata loader connection writing Teradata utilities scripts Fast Load Multiload and working with loader logs To monitor query run times using Teradata Performance Monitor Involved in loading of data into Teradata from legacy systems and flat files using complex MLOAD scripts and FASTLOAD scripts Create and Maintain Teradata Tables Views Macros Triggers and Stored Procedures Monitored workload job performance and capacity planning using Cloudera Distribution Worked on Data loading into Hive for Data Ingestion history and Data content summary Involved in developing Impala scripts for extraction transformation loading of data into data warehouse Used Hive and Impala to query the data in HBase Created Impala tables and SFTP scripts and Shell scripts to import data into Hadoop Developed Hbase java client API for CRUD Operations Created Hive tables and involved in data loading and writing Hive UDFs Developed Hive UDFs for rating aggregation Generated Java APIs for retrieval and analysis on NoSQL database such as HBase and Cassandra Provided adhoc queries and data metrics to the Business Users using Hive Pig Did various performance optimizations like using distributed cache for small datasets partition and bucketing in hive doing map side joins etc Worked on importing and exporting data from Oracle and DB2 into HDFS and HIVE using Sqoop for analysis visualization and to generate reports Worked on Apache spark writing python applications to convert txt xls files and parse Developed Python scripts UDFs using both Data framesSQL and RDDMapReduce in Spark for Data Aggregation queries and writing data back into RDBMS through Sqoop Use different components of Talend tOracleInput tOracleOutput tHiveInput tHiveOutput tHiveInputRow tUniqeRow tAggregateRow tRunJob tPreJob tPostJob tMap tJavaRow tJavaFlex tFilterRow etc to develop standard jobs Loading data from different source database files into Hive using Talend tool Load and transform data into HDFS from large set of structured data OracleSql server using Talend Big data studio Implemented Spark using PythonScala and utilizing Spark Core Spark Streaming and Spark SQL for faster processing of data instead of Map Reduce in Java Experience in integrating Apache Kafka with Apache Spark for real time processing Exposure on usage of Apache Kafka develop data pipeline of logs as a stream of messages using producers and consumers Scheduled Oozie workflow engine to run multiple Hive and Pig jobs which independently run with time and data availability Worked on custom Pig Loaders and Storage classes to work with a variety of data formats such as JSON Compressed CSV etc Involved in running Hadoop Streaming jobs to process Terabytes of data Used JIRA for bug tracking and CVS for version control Environment Hadoop Map Reduce Hive HDFS PIG Sqoop Oozie Cloudera Flume HBase SOLR CDH3 Cassandra Oracle UnixLinux Hadoop Hive PIG SQOOP Flume HDFS J2EE OracleSQL DB2 UnixLinux JavaScript Ajax Eclipse IDE CVS JIRA Hadoop Java developer University of Arkansas January 2017 to May 2017 Global Learning Department The University of Arkansas performs analysis on the various data they have The Global Learning Department is one of the projects that perform Data analytics on the data related to the Student applications received students admitted and other student activities such as enrollments and on the courses offered by each department The aim of the project is to perform data analytics and generate the reports for every month and semester Responsibilities Involved in all Phases of application development Followed the Agile Scrum development methodology Developed the web application using JavaJ2EE and Spring framework Developed the Business logic in the middletier using Java Beans and Java classes Implemented the Java beans autowiring and annotations in Apache Spring framework Connecting to the Hive from Java layer using the Hive JDBC driver Conducted Hibernate and created POJO classes for establishing Object Relation Mapping ORM between objects and relational database Experienced with JDBC API in establishing connection Between databases and java POJO classes Developed SQL queries to retrieve and validate Oracle data and executed the same using OJDBC drivers Played a key role in designing the Fact and dimension table for the Star Schema Created Hive tables and loaded retrieved the data from tables and developed Hive UDFs using Java Performing testing on the spark modules developed with the help of sample data sets Used Oozie workflow engine to manage interdependent Hadoop jobs and to automate several types of Hadoop jobs such as Hive Pig and Sqoop Environment JavaJ2EE Apache Spring Spark Hive PIG Tableau Oracle OJDBC Pig Sqoop Scala GIT Hadoop Developer Arteria Technologies Pvt Ltd August 2014 to November 2016 Responsibilities Installed and configured Hadoop MapReduce HDFS and developed multiple MapReduce jobs in Java for data cleansing and preprocessing Responsible to manage data coming from different sources Involved in gathering the business requirements from the Business Partners and Subject Matter Experts Proactively monitored systems and services architecture design and implementation of Hadoop deployment configuration management backup and disaster recovery systems and procedures Involved in works including indexing data tuning relevance developing custom token izers and filters adding functionality includes playlist custom sorting Supported Map Reduce Programs those are running on the cluster Involved in HDFS maintenance and loading of structured and unstructured data Involved in file movements between HDFS and AWS S3 and extensively worked with S3 bucket in AWS Worked extensively with importing metadata into Hive using Python and migrated existing tables and applications to work on AWS cloudS3 Installed and configured Pig and written Pig Latin scripts Imported data using Sqoop to load data from MySQL to HDFS on regular basis Expert knowledge on Mongo DB NoSQL data modeling tuning disaster recovery backup used it for distributed storage and processing using CRUD Extracted and restructured the data into Mongo DB using import and export command line utility tool Designed and Maintained Tez workflows to manage the flow of jobs in the cluster Developed Scripts and Batch Job to schedule various Hadoop Program Installation of Oozie workflow engine to run multiple Hive and pig jobs Writing Hive queries for data analysis to meet the business requirements Loading log data into HDFS using Flume and performing ETL Integration Collecting and aggregating large amounts of log data using Flume and staging data in HDFS for further analysis Developed Flume ETL job for handling data from HTTP Source and Sink as HDFS Created Hive tables and working on them using Hive QL Good Understanding of DAG cycle for entire Spark application flow on Spark application Web UI Developed Spark SQL scripts and involved in converting Hive UDFs to Spark SQL UDFs Implemented procedures like text analytics and processing using the inmemory computing capabilities like Apache Spark written in Scala Developed multiple Spark jobs in ScalaPython for Data cleaning preprocessing and Aggregating Developed Spark programs using Scala Involved in Creating SparkSQL Queries and Developed Oozie workflow for Spark jobs Push data as delimited files into HDFS using Talend Big data studio Analyzed and performed data integration using Talend open integration suite Wrote complex SQL queries to take data from various sources and integrated it with Talend Used Storm for an automatic mechanism for repeating attempts to download and manipulate the data when there is a hiccup Designing and development of technical architecture requirements and statistical models using R Used Storm to analyze large amounts of nonunique data points with low latency and high throughput Developed UI application using AngularJS integrated with Elastic Search to consume REST Writing the shell scripts to monitor the health check of Hadoop daemon services and respond accordingly to any warning or failure conditions Utilized Agile and Scrum Methodology to help manage and organize a team of developers with regular code review sessions Environment Hadoop Hive Linux Map Reduce HDFS Hive Pig HBase Sqoop Kafka Flume Shell Scripting Storm Java JDK 16 Java 6 Eclipse Oracle 10g PLSQL SQLPLUS Toad 96 Linux JIRA 51 Storm CVS JIRA 52 Java Developer Maveric Systems IN June 2013 to June 2014 Responsibilities Interacted with Team and Analysis Design and Develop database using ER Diagram Normalization and relational database concept Involved in Design Development and testing of the system Developed SQL Server Stored Procedures Tuned SQL Queries using Indexes and Execution Plan Developed User Defined Functions and created Views Created Triggers to maintain the Referential Integrity Implemented Exceptional Handling Involved in Requirement Analysis Development and Documentation Developed frontend using JSP HTML CSSand JavaScript Coding for DAO Objects using JDBC using DAO pattern XML and XSDs are used to define data formats Implemented J2EE design patterns such as singleton DAO for the presentation tier business tier and Integration Tier layers of the project Worked on client requirement and wrote Complex SQL Queries to generate Crystal Reports Creating and automating the regular Jobs Tuned and Optimized SQL Queries using Execution Plan and Profiler Developed user interface view component of MVC architecture with JSP Struts Custom Tag libraries HTML5 and JavaScript Used DOJO toolkit to construct Ajax requests and build dynamic web pages using JSPs DHTML and JavaScripts extensively used jQuery in web based applications Developed the controller component with Servlets and action classes Business Components are developed model components using Enterprise Java Beans EJB Established schedule and resource requirements by planning analyzing and documenting development effort to include time lines risks test requirements and performance targets Analyzing System Requirements and preparing System Design document Developing dynamic User Interface with HTML and JavaScript using JSP and Servlet Technology Designed and developed a sub system where Java Messaging Service JMS applications are developed to communicate with MQ in data exchange between different systems Used JMS elements for sending and receiving messages Used hibernate for mapping from Java classes to database tables Created and executed Test Plans using Quality Center by Test Director Mapped requirements with the Test cases in the Quality Center Supporting System Test and User acceptance test Rebuilding Indexes and Tables as part of Performance Tuning Exercise Involved in performing database Backup and Recovery Worked on Documentation using MS word Environment SQL Server 702000 SQL TSQL BCP Visual Basic 6050 Crystal Reports 745 Java J2ee JDBC EJB JSP EL JSTL JUNIT XML SOAP WSDL Eclipse Windows XP Oracle Education Masters Additional Information TECHNICAL SKILL SET LanguagesTools Java C C C Scala VB XML HTMLXHTML HDML DHTML Big Data HDFS Map Reduce HIVE PIG HBase SQOOP Oozie Zookeeper Spark Mahout Kafka Storm Cassandra Solr Impala Green plum Mongo DB WebDistributed Technologies J2EE Servlets JSP Struts Hibernate JSF JSTLEJBRMIJNI XMLJAXPXSLXSLT UML MVC STRUTS Spring Corba Java Threads J2EE Technologies J2EE Servlets JSP 2122 EJB2130 JDBC MVC Architecture Java Beans JNDI RMI JMS Java ANT 18 JavaScript Spring Browser LanguagesScripting HTML XHTML CSS XML XSL XSD XSLT Java script HTML DOM DHTML AJAX AppWeb Servers IBM Web sphere BEA Web logic Jdeveloper Apache Tomcat JBoss GUI Environment Swing AWT Applets Messaging Web Services Technology SOAP WSDL UDDI XML SOA JAXRPC IBM Web Sphere MQ v53 JMS Testing Case Tools JUnit Log4j Rational Clear case CVS ANT Maven JBuilder Configuration Management Chef Puppet Ansible Docker Build Tools CVS Subversion GIT Ant Maven Gradle Hudson TeamCity Jenkins Chef Puppet Ansible Docker CI Tools Jenkins Bamboo Scripting Languages Python Shell Bash Perl PowerShell Ruby Groovy PowerShell Monitoring Tools Nagios Cloud Watch JIIRA Bugzilla and Remedy Databases NO SQL Oracle MS SQL Server 2000 DB2 MS Access My SQL Teradata Cassandra Greenplum and Mongo DB Operating systems Windows Solaris Unix Linux Red Hat SUSE Linux Sun Solaris Ubuntu CentOS",
    "entities": [
        "Implemented Spark",
        "PythonScala",
        "Spark Transformations",
        "TDD BDD",
        "Oozie Zookeeper Spark",
        "The University of Arkansas",
        "Spark Context",
        "Apache Spring",
        "Talend Used Storm",
        "HDFS",
        "the Business Partners",
        "Java Messaging Service JMS",
        "Hadoop Streaming",
        "JSON",
        "ER Diagram Normalization",
        "IBM",
        "SparkSql",
        "Tables Views Functions Stored Procedures Indexes Triggers Cursors",
        "CVS",
        "Object Oriented Analysis and Design OOAD Well",
        "AWS Proficient",
        "Spark Hive PIG",
        "Servlet Technology Designed",
        "AWS cloudS3 Installed",
        "Spark Sql and Mlib",
        "Hadoop",
        "XML",
        "DHTML",
        "Utilized Agile",
        "ETL Integration Collecting",
        "Shell",
        "Spark for Performance",
        "Spark Engine",
        "HBase",
        "Developed Oozie",
        "JavaScripts",
        "Apache Spark",
        "Sr Hadoop Developer Sr Hadoop",
        "JavaJ2EE",
        "Amazon",
        "Developed",
        "Distributed Data Processing using Big Data",
        "SparkSQL",
        "J2EE OracleSQL DB2",
        "DAO",
        "University of Arkansas",
        "AWS S3",
        "JSP EL JSTL",
        "Work Experience Sr Hadoop Developer",
        "Complex SQL Queries",
        "tMap",
        "Talend Big",
        "Analyzing System Requirements",
        "Responsibilities Involved",
        "PARQUET",
        "PVCS Involvement",
        "Conducted Hibernate",
        "Servlets",
        "ScalaPython",
        "Rebuilding Indexes and",
        "Spark for Data Aggregation",
        "MQ",
        "the Quality Center Supporting System Test",
        "SOLR Search Engine Ingested",
        "Creating SparkSQL Queries",
        "Linux",
        "JSP",
        "Hive Experience",
        "Quality Center by Test",
        "MR Unit JUnit",
        "Talend",
        "ORC",
        "Views Created Triggers",
        "Test Plans",
        "InDepth",
        "MS",
        "Hadoop Hive Pig Sqoop Hbase",
        "FASTLOAD",
        "MVC",
        "Spark",
        "Java Application Development ClientServer Applications InternetIntranet",
        "Teradata Performance Monitor Involved",
        "File Formats",
        "GIT",
        "Created Hive",
        "GIT Hadoop",
        "API for CRUD Operations Created Hive",
        "the Referential Integrity Implemented Exceptional Handling Involved",
        "Spark SQL Created User Defined Functions",
        "HTTP Source",
        "US",
        "Sqoop",
        "HIVE",
        "MLOAD",
        "POJO",
        "DAO Objects",
        "Optimized SQL Queries",
        "Hadoop Program Installation of Oozie",
        "OracleSql",
        "JSP Struts Custom Tag",
        "Created",
        "Mlib",
        "Scala",
        "Analyzed",
        "Hadoop MapReduce HDFS",
        "Responsibilities Installed",
        "Business Components",
        "Oracle",
        "Java Developer Maveric Systems",
        "Hadoop Developed Hbase",
        "Big Data Analytics",
        "PIG",
        "Hive for Data Ingestion",
        "log data",
        "HTML",
        "JAVA Apache SOLR",
        "Oozie",
        "SQL",
        "Cassandra Oracle UnixLinux Hadoop Hive",
        "Created POC",
        "Spark Core Spark Streaming",
        "Easy Mock Implemented",
        "Elastic Search",
        "Mongo DB WebDistributed Technologies J2EE Servlets",
        "Big Data",
        "CRUD Worked on",
        "Hive",
        "JDBC",
        "OJDBC",
        "DAG",
        "Hadoop Data Analytics NoSQL Distributed Machine Learning",
        "Present Description Walmart Inc",
        "Developed a Spark",
        "Querying",
        "ETL",
        "the Business Users using Hive Pig",
        "CRUD Extracted",
        "Oracle Postgre",
        "HIVE PIG HBase",
        "Apache Hadoop",
        "Impala",
        "HBase Created",
        "JavaScript",
        "Responsibilities Analyzed",
        "The Global Learning Department",
        "the Star Schema Created Hive",
        "Hive Pig and Sqoop Environment JavaJ2EE Apache Spring",
        "Spark Streaming Expertise",
        "Business Requirements",
        "SVN",
        "Profiler Developed",
        "Expertise",
        "Fact",
        "jQuery",
        "Data",
        "MapReduce",
        "CVS ANT Maven JBuilder Configuration Management Chef Puppet Ansible Docker",
        "Developed UI",
        "RDBMS",
        "Stored Procedures Monitored",
        "NoSQL",
        "Global Learning Department",
        "Cloudera HDFS HBase MapReduce Hive Hive",
        "Teradata",
        "Maven Hands"
    ],
    "experience": "Experience with Cloudera Manager for management of Hadoop cluster Good conceptual understanding and experience in cloud computing applications using Amazon EC2 S3 EMR Experience in analyzing data using HiveQL Pig Latin HBase and custom Map Reduce programs in Java Extensive Experience on Teradata database analyzing business needs of clients developing effective and efficient solutions and ensuring client deliverables with in committed timelines Expertise in maintaining data quality data organization and metadata Experience in Coding Teradata SQL Teradata Stored Procedures Macros and Triggers Strong experience in Creating Database Objects such as Tables Views Functions Stored Procedures Indexes Triggers Cursors in Teradata Experience data processing like collecting aggregating moving from various sources using Apache Flume and Kafka Developed a data pipeline using Kafka and Spark Streaming to store data into HDFS and performed the realtime analytics on the incoming data Experience in importing the realtime data to Hadoop using Kafka and implemented the Oozie job for daily imports Experienced in working with inmemory processing framework like Spark Transformations SparkQL MLib and Spark Streaming Expertise in creating Custom Serdes in Hive Good working experience on using Sqoop to import data into HDFS from RDBMS and viceversa Expertise in job scheduling and monitoring tools like Oozie and Zoo Keeper Experience in design and development of Map Reduce Programs using Apache Hadoop for analyzing the big data as per the requirement Experience in performing adhoc queries on structured data using Hive QL and used Partition and Bucketing techniques and joins with HIVE for faster data access Experience in performing ETL operations using Pig Latin scripts Implemented Java APIs and created custom Java programs for fullfledged utilization of Hadoop and its related tools Implemented work flows that involve Hadoop actions using Oozie coordinators Experienced in implementing POC using Spark Sql and Mlib libraries Improving the performance and optimization of the existing algorithms in Hadoop using Spark Context SparkSQL Data Frame Pair RDDs YARN Hands on experience in handling Hive tables using Spark SQL Created User Defined Functions UDFs User Defined Aggregated Functions UDAFs in PIG and Hive Experience in dealing with log files to extract data and to copy into HDFS using Flume Experience with Testing Map Reduce programs using MR Unit JUnit and Easy Mock Implemented distributed searching capabilities using Solr to empower the geospatial search and navigation feature Experienced in Java Application Development ClientServer Applications InternetIntranet based applications using Core Java J2EE patterns Spring Hibernate Struts JMS Web Services SOAPREST Oracle SQL Server and other relational databases Experience writing Shell scripts in Linux OS and integrating them with other solutions Expert in developing web page interfaces using JSP Java Swings and HTML scripting languages Excellent understanding on Java beans and Hibernate framework to implement model logic to interact with RDBMS databases Experience in using IDEs like Eclipse NetBeans and Maven Hands on experienced working with source control tools such as Rational Clear Case and Clear Quest Hands on experience on writing Queries Stored procedures Functions and Triggers by using SQL Used EMR Elastic Map Reducing to perform bigdata operations in AWS Proficient using version control tools like GIT VSS SVN and PVCS Involvement in all stages of software development life cycle SDLC and follow agile methodologies and continuous delivery Strong skills in Object Oriented Analysis and Design OOAD Well versed in enterprise software development methodologies and practices including TDD BDD design patterns and performance testing Authorized to work in the US for any employer Work Experience Sr Hadoop Developer Walmart Bentonville AR May 2018 to Present Description Walmart Inc is an American multinational retail corporation that operates a chain of hypermarkets discount department stores and grocery storesweve been dedicated to making a difference in the lives of our customers Our business is the result of Sam Waltons visionary leadership along with generations of associates focused on helping customers and communities save money and live better Responsibilities Analyzed large and critical datasets using Cloudera HDFS HBase MapReduce Hive Hive UDF Pig Sqoop Zookeeper and Spark Gathered the business requirements from the Business Partners and Subject Matter Experts Developed environmental search engine using JAVA Apache SOLR and MYSQL Managed works including indexing data tuning relevance developing custom tokenizers and filters adding functionality includes playlist custom sorting and regionalization with SOLR Search Engine Ingested data from RDBMS and performed data transformations and then export the transformed data to Cassandra as per the business requirement Written multiple Map Reduce programs for data extraction transformation and aggregation from multiple file formats including XML JSON CSV other compressed file formats Developed automated processes for flattening the upstream data from Cassandra which in JSON format Used Hive UDFs to flatten the JSON Data Optimized Map Reduce Jobs to use HDFS efficiently by using various compression mechanisms Developed PIG UDFs to provide Pig capabilities for manipulating the data according to Business Requirements and worked on developing custom PIG Loaders and Implemented various requirements using Pig scripts Experienced on loading and transforming of large sets of structured semi structured and unstructured data Created POC using Spark Sql and Mlib libraries Developed a Spark Streaming module for consumption of Avro messages from Kafka Implementing different machine learning techniques in Scala using Scala machine learning library and created POC using SparkSql and Mlib libraries Experienced in Querying data using SparkSQL on top of Spark Engine implementing Spark RDDs in Scala Expertise in writing Scala code using Higher order functions for iterative algorithms in Spark for Performance considerations Experienced in managing and reviewing Hadoop log files Worked with different File Formats like TEXTFILE AVROFILE ORC and PARQUET for HIVE querying and processing Loading data by using the Teradata loader connection writing Teradata utilities scripts Fast Load Multiload and working with loader logs To monitor query run times using Teradata Performance Monitor Involved in loading of data into Teradata from legacy systems and flat files using complex MLOAD scripts and FASTLOAD scripts Create and Maintain Teradata Tables Views Macros Triggers and Stored Procedures Monitored workload job performance and capacity planning using Cloudera Distribution Worked on Data loading into Hive for Data Ingestion history and Data content summary Involved in developing Impala scripts for extraction transformation loading of data into data warehouse Used Hive and Impala to query the data in HBase Created Impala tables and SFTP scripts and Shell scripts to import data into Hadoop Developed Hbase java client API for CRUD Operations Created Hive tables and involved in data loading and writing Hive UDFs Developed Hive UDFs for rating aggregation Generated Java APIs for retrieval and analysis on NoSQL database such as HBase and Cassandra Provided adhoc queries and data metrics to the Business Users using Hive Pig Did various performance optimizations like using distributed cache for small datasets partition and bucketing in hive doing map side joins etc Worked on importing and exporting data from Oracle and DB2 into HDFS and HIVE using Sqoop for analysis visualization and to generate reports Worked on Apache spark writing python applications to convert txt xls files and parse Developed Python scripts UDFs using both Data framesSQL and RDDMapReduce in Spark for Data Aggregation queries and writing data back into RDBMS through Sqoop Use different components of Talend tOracleInput tOracleOutput tHiveInput tHiveOutput tHiveInputRow tUniqeRow tAggregateRow tRunJob tPreJob tPostJob tMap tJavaRow tJavaFlex tFilterRow etc to develop standard jobs Loading data from different source database files into Hive using Talend tool Load and transform data into HDFS from large set of structured data OracleSql server using Talend Big data studio Implemented Spark using PythonScala and utilizing Spark Core Spark Streaming and Spark SQL for faster processing of data instead of Map Reduce in Java Experience in integrating Apache Kafka with Apache Spark for real time processing Exposure on usage of Apache Kafka develop data pipeline of logs as a stream of messages using producers and consumers Scheduled Oozie workflow engine to run multiple Hive and Pig jobs which independently run with time and data availability Worked on custom Pig Loaders and Storage classes to work with a variety of data formats such as JSON Compressed CSV etc Involved in running Hadoop Streaming jobs to process Terabytes of data Used JIRA for bug tracking and CVS for version control Environment Hadoop Map Reduce Hive HDFS PIG Sqoop Oozie Cloudera Flume HBase SOLR CDH3 Cassandra Oracle UnixLinux Hadoop Hive PIG SQOOP Flume HDFS J2EE OracleSQL DB2 UnixLinux JavaScript Ajax Eclipse IDE CVS JIRA Hadoop Java developer University of Arkansas January 2017 to May 2017 Global Learning Department The University of Arkansas performs analysis on the various data they have The Global Learning Department is one of the projects that perform Data analytics on the data related to the Student applications received students admitted and other student activities such as enrollments and on the courses offered by each department The aim of the project is to perform data analytics and generate the reports for every month and semester Responsibilities Involved in all Phases of application development Followed the Agile Scrum development methodology Developed the web application using JavaJ2EE and Spring framework Developed the Business logic in the middletier using Java Beans and Java classes Implemented the Java beans autowiring and annotations in Apache Spring framework Connecting to the Hive from Java layer using the Hive JDBC driver Conducted Hibernate and created POJO classes for establishing Object Relation Mapping ORM between objects and relational database Experienced with JDBC API in establishing connection Between databases and java POJO classes Developed SQL queries to retrieve and validate Oracle data and executed the same using OJDBC drivers Played a key role in designing the Fact and dimension table for the Star Schema Created Hive tables and loaded retrieved the data from tables and developed Hive UDFs using Java Performing testing on the spark modules developed with the help of sample data sets Used Oozie workflow engine to manage interdependent Hadoop jobs and to automate several types of Hadoop jobs such as Hive Pig and Sqoop Environment JavaJ2EE Apache Spring Spark Hive PIG Tableau Oracle OJDBC Pig Sqoop Scala GIT Hadoop Developer Arteria Technologies Pvt Ltd August 2014 to November 2016 Responsibilities Installed and configured Hadoop MapReduce HDFS and developed multiple MapReduce jobs in Java for data cleansing and preprocessing Responsible to manage data coming from different sources Involved in gathering the business requirements from the Business Partners and Subject Matter Experts Proactively monitored systems and services architecture design and implementation of Hadoop deployment configuration management backup and disaster recovery systems and procedures Involved in works including indexing data tuning relevance developing custom token izers and filters adding functionality includes playlist custom sorting Supported Map Reduce Programs those are running on the cluster Involved in HDFS maintenance and loading of structured and unstructured data Involved in file movements between HDFS and AWS S3 and extensively worked with S3 bucket in AWS Worked extensively with importing metadata into Hive using Python and migrated existing tables and applications to work on AWS cloudS3 Installed and configured Pig and written Pig Latin scripts Imported data using Sqoop to load data from MySQL to HDFS on regular basis Expert knowledge on Mongo DB NoSQL data modeling tuning disaster recovery backup used it for distributed storage and processing using CRUD Extracted and restructured the data into Mongo DB using import and export command line utility tool Designed and Maintained Tez workflows to manage the flow of jobs in the cluster Developed Scripts and Batch Job to schedule various Hadoop Program Installation of Oozie workflow engine to run multiple Hive and pig jobs Writing Hive queries for data analysis to meet the business requirements Loading log data into HDFS using Flume and performing ETL Integration Collecting and aggregating large amounts of log data using Flume and staging data in HDFS for further analysis Developed Flume ETL job for handling data from HTTP Source and Sink as HDFS Created Hive tables and working on them using Hive QL Good Understanding of DAG cycle for entire Spark application flow on Spark application Web UI Developed Spark SQL scripts and involved in converting Hive UDFs to Spark SQL UDFs Implemented procedures like text analytics and processing using the inmemory computing capabilities like Apache Spark written in Scala Developed multiple Spark jobs in ScalaPython for Data cleaning preprocessing and Aggregating Developed Spark programs using Scala Involved in Creating SparkSQL Queries and Developed Oozie workflow for Spark jobs Push data as delimited files into HDFS using Talend Big data studio Analyzed and performed data integration using Talend open integration suite Wrote complex SQL queries to take data from various sources and integrated it with Talend Used Storm for an automatic mechanism for repeating attempts to download and manipulate the data when there is a hiccup Designing and development of technical architecture requirements and statistical models using R Used Storm to analyze large amounts of nonunique data points with low latency and high throughput Developed UI application using AngularJS integrated with Elastic Search to consume REST Writing the shell scripts to monitor the health check of Hadoop daemon services and respond accordingly to any warning or failure conditions Utilized Agile and Scrum Methodology to help manage and organize a team of developers with regular code review sessions Environment Hadoop Hive Linux Map Reduce HDFS Hive Pig HBase Sqoop Kafka Flume Shell Scripting Storm Java JDK 16 Java 6 Eclipse Oracle 10 g PLSQL SQLPLUS Toad 96 Linux JIRA 51 Storm CVS JIRA 52 Java Developer Maveric Systems IN June 2013 to June 2014 Responsibilities Interacted with Team and Analysis Design and Develop database using ER Diagram Normalization and relational database concept Involved in Design Development and testing of the system Developed SQL Server Stored Procedures Tuned SQL Queries using Indexes and Execution Plan Developed User Defined Functions and created Views Created Triggers to maintain the Referential Integrity Implemented Exceptional Handling Involved in Requirement Analysis Development and Documentation Developed frontend using JSP HTML CSSand JavaScript Coding for DAO Objects using JDBC using DAO pattern XML and XSDs are used to define data formats Implemented J2EE design patterns such as singleton DAO for the presentation tier business tier and Integration Tier layers of the project Worked on client requirement and wrote Complex SQL Queries to generate Crystal Reports Creating and automating the regular Jobs Tuned and Optimized SQL Queries using Execution Plan and Profiler Developed user interface view component of MVC architecture with JSP Struts Custom Tag libraries HTML5 and JavaScript Used DOJO toolkit to construct Ajax requests and build dynamic web pages using JSPs DHTML and JavaScripts extensively used jQuery in web based applications Developed the controller component with Servlets and action classes Business Components are developed model components using Enterprise Java Beans EJB Established schedule and resource requirements by planning analyzing and documenting development effort to include time lines risks test requirements and performance targets Analyzing System Requirements and preparing System Design document Developing dynamic User Interface with HTML and JavaScript using JSP and Servlet Technology Designed and developed a sub system where Java Messaging Service JMS applications are developed to communicate with MQ in data exchange between different systems Used JMS elements for sending and receiving messages Used hibernate for mapping from Java classes to database tables Created and executed Test Plans using Quality Center by Test Director Mapped requirements with the Test cases in the Quality Center Supporting System Test and User acceptance test Rebuilding Indexes and Tables as part of Performance Tuning Exercise Involved in performing database Backup and Recovery Worked on Documentation using MS word Environment SQL Server 702000 SQL TSQL BCP Visual Basic 6050 Crystal Reports 745 Java J2ee JDBC EJB JSP EL JSTL JUNIT XML SOAP WSDL Eclipse Windows XP Oracle Education Masters Additional Information TECHNICAL SKILL SET LanguagesTools Java C C C Scala VB XML HTMLXHTML HDML DHTML Big Data HDFS Map Reduce HIVE PIG HBase SQOOP Oozie Zookeeper Spark Mahout Kafka Storm Cassandra Solr Impala Green plum Mongo DB WebDistributed Technologies J2EE Servlets JSP Struts Hibernate JSF JSTLEJBRMIJNI XMLJAXPXSLXSLT UML MVC STRUTS Spring Corba Java Threads J2EE Technologies J2EE Servlets JSP 2122 EJB2130 JDBC MVC Architecture Java Beans JNDI RMI JMS Java ANT 18 JavaScript Spring Browser LanguagesScripting HTML XHTML CSS XML XSL XSD XSLT Java script HTML DOM DHTML AJAX AppWeb Servers IBM Web sphere BEA Web logic Jdeveloper Apache Tomcat JBoss GUI Environment Swing AWT Applets Messaging Web Services Technology SOAP WSDL UDDI XML SOA JAXRPC IBM Web Sphere MQ v53 JMS Testing Case Tools JUnit Log4j Rational Clear case CVS ANT Maven JBuilder Configuration Management Chef Puppet Ansible Docker Build Tools CVS Subversion GIT Ant Maven Gradle Hudson TeamCity Jenkins Chef Puppet Ansible Docker CI Tools Jenkins Bamboo Scripting Languages Python Shell Bash Perl PowerShell Ruby Groovy PowerShell Monitoring Tools Nagios Cloud Watch JIIRA Bugzilla and Remedy Databases NO SQL Oracle MS SQL Server 2000 DB2 MS Access My SQL Teradata Cassandra Greenplum and Mongo DB Operating systems Windows Solaris Unix Linux Red Hat SUSE Linux Sun Solaris Ubuntu CentOS",
    "extracted_keywords": [
        "Sr",
        "Hadoop",
        "Developer",
        "Sr",
        "Hadoop",
        "span",
        "lDeveloperspan",
        "Sr",
        "Hadoop",
        "Developer",
        "Walmart",
        "Bentonville",
        "AR",
        "years",
        "experience",
        "applications",
        "scale",
        "Data",
        "Processing",
        "Big",
        "Data",
        "ecosystem",
        "tools",
        "Hadoop",
        "Hive",
        "Pig",
        "Sqoop",
        "Hbase",
        "Cassandra",
        "Spark",
        "Spark",
        "Streaming",
        "MLLib",
        "Mahout",
        "Oozie",
        "Zoo",
        "Keeper",
        "Flume",
        "Yarn",
        "Avro",
        "Passionate",
        "Big",
        "Data",
        "Analytics",
        "data",
        "content",
        "Expert",
        "algorithms",
        "data",
        "Hands",
        "experience",
        "Hadoop",
        "distributions",
        "Apache",
        "Cloudera",
        "Horton",
        "MapR",
        "InDepth",
        "knowledge",
        "experience",
        "design",
        "development",
        "deployments",
        "Big",
        "Data",
        "projects",
        "Hadoop",
        "Data",
        "Analytics",
        "NoSQL",
        "Machine",
        "Learning",
        "frameworks",
        "understanding",
        "SQL",
        "NOSQL",
        "Oracle",
        "Postgre",
        "SQL",
        "MySQL",
        "Mongo",
        "DB",
        "HBase",
        "Cassandra",
        "Knowledge",
        "Mongo",
        "DB",
        "NoSQL",
        "data",
        "modeling",
        "disaster",
        "recovery",
        "backup",
        "storage",
        "processing",
        "CRUD",
        "Worked",
        "Mongo",
        "DB",
        "database",
        "concepts",
        "transactions",
        "indexes",
        "replication",
        "schema",
        "Cassandra",
        "cluster",
        "mechanism",
        "replication",
        "strategies",
        "snitch",
        "gossip",
        "hashing",
        "consistency",
        "levels",
        "knowledge",
        "Cassandra",
        "cassandrayaml",
        "file",
        "business",
        "requirement",
        "readswrites",
        "Java",
        "JDBC",
        "connectivity",
        "Experience",
        "Cloudera",
        "Manager",
        "management",
        "Hadoop",
        "cluster",
        "understanding",
        "experience",
        "computing",
        "applications",
        "Amazon",
        "EC2",
        "S3",
        "EMR",
        "Experience",
        "data",
        "HiveQL",
        "Pig",
        "Latin",
        "HBase",
        "custom",
        "Map",
        "Reduce",
        "programs",
        "Java",
        "Extensive",
        "Experience",
        "Teradata",
        "database",
        "business",
        "needs",
        "clients",
        "solutions",
        "client",
        "deliverables",
        "timelines",
        "Expertise",
        "data",
        "quality",
        "data",
        "organization",
        "metadata",
        "Experience",
        "Coding",
        "Teradata",
        "SQL",
        "Teradata",
        "Stored",
        "Procedures",
        "Macros",
        "Triggers",
        "experience",
        "Creating",
        "Database",
        "Objects",
        "Tables",
        "Views",
        "Functions",
        "Stored",
        "Procedures",
        "Indexes",
        "Triggers",
        "Cursors",
        "Teradata",
        "Experience",
        "data",
        "processing",
        "sources",
        "Apache",
        "Flume",
        "Kafka",
        "data",
        "pipeline",
        "Kafka",
        "Spark",
        "Streaming",
        "data",
        "HDFS",
        "analytics",
        "data",
        "Experience",
        "data",
        "Hadoop",
        "Kafka",
        "Oozie",
        "job",
        "imports",
        "inmemory",
        "processing",
        "framework",
        "Spark",
        "Transformations",
        "MLib",
        "Spark",
        "Streaming",
        "Expertise",
        "Custom",
        "Serdes",
        "Hive",
        "Good",
        "working",
        "experience",
        "Sqoop",
        "data",
        "HDFS",
        "RDBMS",
        "viceversa",
        "Expertise",
        "job",
        "scheduling",
        "monitoring",
        "tools",
        "Oozie",
        "Zoo",
        "Keeper",
        "Experience",
        "design",
        "development",
        "Map",
        "Reduce",
        "Programs",
        "Apache",
        "Hadoop",
        "data",
        "requirement",
        "Experience",
        "queries",
        "data",
        "Hive",
        "QL",
        "Partition",
        "Bucketing",
        "techniques",
        "HIVE",
        "data",
        "access",
        "Experience",
        "ETL",
        "operations",
        "Pig",
        "Latin",
        "scripts",
        "Java",
        "APIs",
        "custom",
        "Java",
        "programs",
        "utilization",
        "Hadoop",
        "tools",
        "work",
        "flows",
        "Hadoop",
        "actions",
        "Oozie",
        "coordinators",
        "POC",
        "Spark",
        "Sql",
        "Mlib",
        "performance",
        "optimization",
        "algorithms",
        "Hadoop",
        "Spark",
        "Context",
        "SparkSQL",
        "Data",
        "Frame",
        "Pair",
        "RDDs",
        "Hands",
        "experience",
        "Hive",
        "tables",
        "Spark",
        "SQL",
        "Created",
        "User",
        "Defined",
        "Functions",
        "UDFs",
        "User",
        "Aggregated",
        "Functions",
        "UDAFs",
        "PIG",
        "Hive",
        "Experience",
        "log",
        "files",
        "data",
        "HDFS",
        "Flume",
        "Experience",
        "Testing",
        "Map",
        "Reduce",
        "programs",
        "MR",
        "Unit",
        "JUnit",
        "Easy",
        "Mock",
        "capabilities",
        "Solr",
        "search",
        "navigation",
        "feature",
        "Java",
        "Application",
        "Development",
        "ClientServer",
        "Applications",
        "InternetIntranet",
        "applications",
        "Core",
        "Java",
        "J2EE",
        "Spring",
        "Hibernate",
        "Struts",
        "JMS",
        "Web",
        "Services",
        "SOAPREST",
        "Oracle",
        "SQL",
        "Server",
        "databases",
        "Experience",
        "Shell",
        "scripts",
        "Linux",
        "OS",
        "solutions",
        "Expert",
        "web",
        "page",
        "interfaces",
        "JSP",
        "Java",
        "Swings",
        "HTML",
        "scripting",
        "languages",
        "understanding",
        "Java",
        "beans",
        "Hibernate",
        "framework",
        "model",
        "logic",
        "RDBMS",
        "Experience",
        "IDEs",
        "Eclipse",
        "NetBeans",
        "Maven",
        "Hands",
        "source",
        "control",
        "tools",
        "Rational",
        "Clear",
        "Case",
        "Clear",
        "Quest",
        "Hands",
        "experience",
        "Queries",
        "procedures",
        "Functions",
        "Triggers",
        "SQL",
        "EMR",
        "Elastic",
        "Map",
        "bigdata",
        "operations",
        "AWS",
        "Proficient",
        "version",
        "control",
        "tools",
        "GIT",
        "VSS",
        "SVN",
        "PVCS",
        "Involvement",
        "stages",
        "software",
        "development",
        "life",
        "cycle",
        "SDLC",
        "methodologies",
        "delivery",
        "skills",
        "Object",
        "Oriented",
        "Analysis",
        "Design",
        "OOAD",
        "enterprise",
        "software",
        "development",
        "methodologies",
        "practices",
        "TDD",
        "BDD",
        "design",
        "patterns",
        "performance",
        "testing",
        "US",
        "employer",
        "Work",
        "Experience",
        "Sr",
        "Hadoop",
        "Developer",
        "Walmart",
        "Bentonville",
        "AR",
        "May",
        "Present",
        "Description",
        "Walmart",
        "Inc",
        "corporation",
        "chain",
        "hypermarkets",
        "discount",
        "department",
        "stores",
        "grocery",
        "storesweve",
        "difference",
        "lives",
        "customers",
        "business",
        "result",
        "Sam",
        "Waltons",
        "leadership",
        "generations",
        "associates",
        "customers",
        "communities",
        "money",
        "Responsibilities",
        "datasets",
        "Cloudera",
        "HDFS",
        "HBase",
        "MapReduce",
        "Hive",
        "Hive",
        "UDF",
        "Pig",
        "Sqoop",
        "Zookeeper",
        "Spark",
        "business",
        "requirements",
        "Business",
        "Partners",
        "Subject",
        "Matter",
        "Experts",
        "search",
        "engine",
        "JAVA",
        "Apache",
        "SOLR",
        "MYSQL",
        "Managed",
        "indexing",
        "data",
        "relevance",
        "custom",
        "tokenizers",
        "filters",
        "functionality",
        "playlist",
        "custom",
        "sorting",
        "regionalization",
        "SOLR",
        "Search",
        "Engine",
        "data",
        "RDBMS",
        "data",
        "transformations",
        "data",
        "Cassandra",
        "business",
        "requirement",
        "Written",
        "Map",
        "Reduce",
        "programs",
        "data",
        "extraction",
        "transformation",
        "aggregation",
        "file",
        "formats",
        "XML",
        "CSV",
        "file",
        "formats",
        "processes",
        "data",
        "Cassandra",
        "format",
        "Hive",
        "UDFs",
        "JSON",
        "Data",
        "Map",
        "Reduce",
        "Jobs",
        "HDFS",
        "compression",
        "mechanisms",
        "PIG",
        "UDFs",
        "Pig",
        "capabilities",
        "data",
        "Business",
        "Requirements",
        "custom",
        "PIG",
        "Loaders",
        "requirements",
        "Pig",
        "scripts",
        "loading",
        "transforming",
        "sets",
        "data",
        "Created",
        "POC",
        "Spark",
        "Sql",
        "Mlib",
        "Spark",
        "Streaming",
        "module",
        "consumption",
        "Avro",
        "messages",
        "Kafka",
        "machine",
        "techniques",
        "Scala",
        "Scala",
        "machine",
        "library",
        "POC",
        "SparkSql",
        "Mlib",
        "libraries",
        "Querying",
        "data",
        "SparkSQL",
        "top",
        "Spark",
        "Engine",
        "Spark",
        "RDDs",
        "Scala",
        "Expertise",
        "Scala",
        "code",
        "order",
        "functions",
        "algorithms",
        "Spark",
        "Performance",
        "considerations",
        "Hadoop",
        "log",
        "files",
        "File",
        "Formats",
        "TEXTFILE",
        "AVROFILE",
        "ORC",
        "PARQUET",
        "HIVE",
        "Loading",
        "data",
        "Teradata",
        "loader",
        "connection",
        "Teradata",
        "utilities",
        "Fast",
        "Load",
        "Multiload",
        "loader",
        "logs",
        "query",
        "times",
        "Teradata",
        "Performance",
        "Monitor",
        "loading",
        "data",
        "Teradata",
        "legacy",
        "systems",
        "files",
        "MLOAD",
        "scripts",
        "FASTLOAD",
        "scripts",
        "Maintain",
        "Teradata",
        "Tables",
        "Views",
        "Macros",
        "Triggers",
        "Stored",
        "Procedures",
        "workload",
        "job",
        "performance",
        "capacity",
        "planning",
        "Cloudera",
        "Distribution",
        "Worked",
        "Data",
        "Hive",
        "Data",
        "Ingestion",
        "history",
        "Data",
        "content",
        "summary",
        "Impala",
        "scripts",
        "extraction",
        "transformation",
        "loading",
        "data",
        "data",
        "warehouse",
        "Hive",
        "Impala",
        "data",
        "HBase",
        "Impala",
        "tables",
        "scripts",
        "Shell",
        "scripts",
        "data",
        "Hadoop",
        "Developed",
        "Hbase",
        "client",
        "API",
        "CRUD",
        "Operations",
        "Hive",
        "tables",
        "data",
        "loading",
        "Hive",
        "UDFs",
        "Developed",
        "Hive",
        "UDFs",
        "rating",
        "aggregation",
        "Java",
        "APIs",
        "retrieval",
        "analysis",
        "NoSQL",
        "database",
        "HBase",
        "Cassandra",
        "queries",
        "data",
        "metrics",
        "Business",
        "Users",
        "Hive",
        "Pig",
        "performance",
        "optimizations",
        "cache",
        "datasets",
        "partition",
        "bucketing",
        "hive",
        "map",
        "side",
        "data",
        "Oracle",
        "DB2",
        "HDFS",
        "HIVE",
        "Sqoop",
        "analysis",
        "visualization",
        "reports",
        "Apache",
        "spark",
        "python",
        "applications",
        "txt",
        "xls",
        "files",
        "Developed",
        "Python",
        "scripts",
        "UDFs",
        "Data",
        "framesSQL",
        "RDDMapReduce",
        "Spark",
        "Data",
        "Aggregation",
        "data",
        "RDBMS",
        "Sqoop",
        "Use",
        "components",
        "Talend",
        "tOracleInput",
        "tOracleOutput",
        "tHiveInput",
        "tHiveOutput",
        "tUniqeRow",
        "tAggregateRow",
        "tRunJob",
        "tPreJob",
        "tPostJob",
        "tMap",
        "tFilterRow",
        "jobs",
        "Loading",
        "data",
        "source",
        "database",
        "files",
        "Hive",
        "tool",
        "Load",
        "data",
        "HDFS",
        "set",
        "data",
        "OracleSql",
        "server",
        "Talend",
        "data",
        "studio",
        "Spark",
        "PythonScala",
        "Spark",
        "Core",
        "Spark",
        "Streaming",
        "Spark",
        "SQL",
        "processing",
        "data",
        "Map",
        "Reduce",
        "Java",
        "Experience",
        "Apache",
        "Kafka",
        "Apache",
        "Spark",
        "time",
        "Exposure",
        "usage",
        "Apache",
        "Kafka",
        "data",
        "pipeline",
        "logs",
        "stream",
        "messages",
        "producers",
        "consumers",
        "Oozie",
        "workflow",
        "engine",
        "Hive",
        "Pig",
        "jobs",
        "time",
        "data",
        "availability",
        "custom",
        "Pig",
        "Loaders",
        "Storage",
        "classes",
        "variety",
        "data",
        "formats",
        "JSON",
        "Compressed",
        "CSV",
        "Hadoop",
        "Streaming",
        "jobs",
        "Terabytes",
        "data",
        "JIRA",
        "bug",
        "tracking",
        "CVS",
        "version",
        "control",
        "Environment",
        "Hadoop",
        "Map",
        "Hive",
        "HDFS",
        "PIG",
        "Sqoop",
        "Oozie",
        "Cloudera",
        "Flume",
        "HBase",
        "SOLR",
        "CDH3",
        "Cassandra",
        "Oracle",
        "UnixLinux",
        "Hadoop",
        "Hive",
        "PIG",
        "SQOOP",
        "Flume",
        "HDFS",
        "J2EE",
        "DB2",
        "UnixLinux",
        "JavaScript",
        "Ajax",
        "Eclipse",
        "IDE",
        "CVS",
        "JIRA",
        "Hadoop",
        "Java",
        "developer",
        "University",
        "Arkansas",
        "January",
        "May",
        "Global",
        "Learning",
        "Department",
        "University",
        "Arkansas",
        "analysis",
        "data",
        "Global",
        "Learning",
        "Department",
        "projects",
        "Data",
        "analytics",
        "data",
        "Student",
        "applications",
        "students",
        "student",
        "activities",
        "enrollments",
        "courses",
        "department",
        "aim",
        "project",
        "data",
        "analytics",
        "reports",
        "month",
        "semester",
        "Responsibilities",
        "Phases",
        "application",
        "development",
        "Agile",
        "Scrum",
        "development",
        "methodology",
        "web",
        "application",
        "JavaJ2EE",
        "Spring",
        "framework",
        "Business",
        "logic",
        "middletier",
        "Java",
        "Beans",
        "Java",
        "classes",
        "Java",
        "beans",
        "autowiring",
        "annotations",
        "Apache",
        "Spring",
        "framework",
        "Hive",
        "Java",
        "layer",
        "Hive",
        "JDBC",
        "driver",
        "Hibernate",
        "POJO",
        "classes",
        "Object",
        "Relation",
        "Mapping",
        "ORM",
        "objects",
        "database",
        "JDBC",
        "API",
        "connection",
        "databases",
        "POJO",
        "classes",
        "SQL",
        "Oracle",
        "data",
        "OJDBC",
        "drivers",
        "role",
        "Fact",
        "dimension",
        "table",
        "Star",
        "Schema",
        "Hive",
        "tables",
        "data",
        "tables",
        "Hive",
        "UDFs",
        "Java",
        "Performing",
        "testing",
        "spark",
        "modules",
        "help",
        "sample",
        "data",
        "sets",
        "Oozie",
        "workflow",
        "engine",
        "Hadoop",
        "jobs",
        "types",
        "Hadoop",
        "jobs",
        "Hive",
        "Pig",
        "Sqoop",
        "Environment",
        "JavaJ2EE",
        "Apache",
        "Spring",
        "Spark",
        "Hive",
        "PIG",
        "Tableau",
        "Oracle",
        "OJDBC",
        "Pig",
        "Sqoop",
        "Scala",
        "GIT",
        "Hadoop",
        "Developer",
        "Arteria",
        "Technologies",
        "Pvt",
        "Ltd",
        "August",
        "November",
        "Responsibilities",
        "Hadoop",
        "MapReduce",
        "HDFS",
        "MapReduce",
        "jobs",
        "Java",
        "data",
        "cleansing",
        "Responsible",
        "data",
        "sources",
        "business",
        "requirements",
        "Business",
        "Partners",
        "Subject",
        "Matter",
        "Experts",
        "systems",
        "services",
        "architecture",
        "design",
        "implementation",
        "Hadoop",
        "deployment",
        "configuration",
        "management",
        "backup",
        "disaster",
        "recovery",
        "systems",
        "procedures",
        "works",
        "indexing",
        "data",
        "relevance",
        "custom",
        "token",
        "izers",
        "filters",
        "functionality",
        "playlist",
        "custom",
        "Map",
        "Programs",
        "cluster",
        "HDFS",
        "maintenance",
        "loading",
        "data",
        "file",
        "movements",
        "HDFS",
        "AWS",
        "S3",
        "S3",
        "bucket",
        "AWS",
        "metadata",
        "Hive",
        "Python",
        "tables",
        "applications",
        "AWS",
        "Installed",
        "Pig",
        "Pig",
        "Latin",
        "data",
        "Sqoop",
        "data",
        "MySQL",
        "HDFS",
        "basis",
        "Expert",
        "knowledge",
        "Mongo",
        "DB",
        "NoSQL",
        "data",
        "modeling",
        "disaster",
        "recovery",
        "backup",
        "storage",
        "processing",
        "CRUD",
        "Extracted",
        "data",
        "Mongo",
        "DB",
        "import",
        "export",
        "command",
        "line",
        "utility",
        "tool",
        "Maintained",
        "Tez",
        "workflows",
        "flow",
        "jobs",
        "cluster",
        "Developed",
        "Scripts",
        "Batch",
        "Job",
        "Hadoop",
        "Program",
        "Installation",
        "Oozie",
        "workflow",
        "engine",
        "Hive",
        "pig",
        "jobs",
        "Hive",
        "queries",
        "data",
        "analysis",
        "business",
        "requirements",
        "Loading",
        "log",
        "data",
        "HDFS",
        "Flume",
        "ETL",
        "Integration",
        "Collecting",
        "amounts",
        "log",
        "data",
        "Flume",
        "data",
        "HDFS",
        "analysis",
        "Developed",
        "Flume",
        "ETL",
        "job",
        "data",
        "HTTP",
        "Source",
        "Sink",
        "HDFS",
        "Hive",
        "tables",
        "Hive",
        "QL",
        "Understanding",
        "DAG",
        "cycle",
        "Spark",
        "application",
        "flow",
        "Spark",
        "application",
        "Web",
        "UI",
        "Spark",
        "SQL",
        "scripts",
        "Hive",
        "UDFs",
        "Spark",
        "SQL",
        "UDFs",
        "procedures",
        "text",
        "analytics",
        "processing",
        "inmemory",
        "computing",
        "capabilities",
        "Apache",
        "Spark",
        "Scala",
        "Spark",
        "jobs",
        "ScalaPython",
        "Data",
        "preprocessing",
        "Spark",
        "programs",
        "Scala",
        "SparkSQL",
        "Queries",
        "Developed",
        "Oozie",
        "workflow",
        "Spark",
        "jobs",
        "data",
        "files",
        "HDFS",
        "data",
        "studio",
        "data",
        "integration",
        "Talend",
        "integration",
        "suite",
        "Wrote",
        "SQL",
        "data",
        "sources",
        "Talend",
        "Storm",
        "mechanism",
        "attempts",
        "data",
        "hiccup",
        "Designing",
        "development",
        "architecture",
        "requirements",
        "models",
        "R",
        "Storm",
        "amounts",
        "data",
        "points",
        "latency",
        "throughput",
        "UI",
        "application",
        "Elastic",
        "Search",
        "REST",
        "shell",
        "scripts",
        "health",
        "check",
        "Hadoop",
        "daemon",
        "services",
        "warning",
        "failure",
        "conditions",
        "Agile",
        "Scrum",
        "Methodology",
        "team",
        "developers",
        "code",
        "review",
        "sessions",
        "Environment",
        "Hadoop",
        "Hive",
        "Linux",
        "Map",
        "HDFS",
        "Hive",
        "Pig",
        "HBase",
        "Sqoop",
        "Kafka",
        "Flume",
        "Shell",
        "Scripting",
        "Storm",
        "Java",
        "JDK",
        "Java",
        "Eclipse",
        "Oracle",
        "g",
        "PLSQL",
        "SQLPLUS",
        "Toad",
        "Linux",
        "JIRA",
        "Storm",
        "CVS",
        "JIRA",
        "Java",
        "Developer",
        "Maveric",
        "Systems",
        "June",
        "June",
        "Responsibilities",
        "Team",
        "Analysis",
        "Design",
        "Develop",
        "database",
        "ER",
        "Diagram",
        "Normalization",
        "database",
        "concept",
        "Design",
        "Development",
        "testing",
        "system",
        "SQL",
        "Server",
        "Stored",
        "Procedures",
        "SQL",
        "Queries",
        "Indexes",
        "Execution",
        "Plan",
        "User",
        "Defined",
        "Functions",
        "Views",
        "Created",
        "Triggers",
        "Referential",
        "Integrity",
        "Exceptional",
        "Handling",
        "Requirement",
        "Analysis",
        "Development",
        "Documentation",
        "frontend",
        "JSP",
        "HTML",
        "CSSand",
        "JavaScript",
        "Coding",
        "DAO",
        "Objects",
        "JDBC",
        "DAO",
        "pattern",
        "XML",
        "XSDs",
        "data",
        "formats",
        "J2EE",
        "design",
        "patterns",
        "singleton",
        "DAO",
        "presentation",
        "tier",
        "business",
        "tier",
        "Integration",
        "Tier",
        "layers",
        "project",
        "client",
        "requirement",
        "Complex",
        "SQL",
        "Queries",
        "Crystal",
        "Reports",
        "Jobs",
        "SQL",
        "Queries",
        "Execution",
        "Plan",
        "Profiler",
        "Developed",
        "user",
        "interface",
        "view",
        "component",
        "MVC",
        "architecture",
        "JSP",
        "Struts",
        "Custom",
        "Tag",
        "HTML5",
        "JavaScript",
        "DOJO",
        "toolkit",
        "requests",
        "web",
        "pages",
        "JSPs",
        "DHTML",
        "JavaScripts",
        "jQuery",
        "web",
        "applications",
        "controller",
        "component",
        "Servlets",
        "action",
        "classes",
        "Business",
        "Components",
        "model",
        "components",
        "Enterprise",
        "Java",
        "Beans",
        "EJB",
        "Established",
        "schedule",
        "resource",
        "requirements",
        "development",
        "effort",
        "time",
        "lines",
        "test",
        "requirements",
        "performance",
        "targets",
        "System",
        "Requirements",
        "System",
        "Design",
        "document",
        "User",
        "Interface",
        "HTML",
        "JavaScript",
        "JSP",
        "Servlet",
        "Technology",
        "sub",
        "system",
        "Java",
        "Messaging",
        "Service",
        "JMS",
        "applications",
        "MQ",
        "data",
        "exchange",
        "systems",
        "JMS",
        "elements",
        "messages",
        "hibernate",
        "mapping",
        "Java",
        "classes",
        "tables",
        "Test",
        "Plans",
        "Quality",
        "Center",
        "Test",
        "Director",
        "Mapped",
        "requirements",
        "Test",
        "cases",
        "Quality",
        "Center",
        "Supporting",
        "System",
        "Test",
        "User",
        "acceptance",
        "test",
        "Rebuilding",
        "Indexes",
        "Tables",
        "part",
        "Performance",
        "Exercise",
        "database",
        "Backup",
        "Recovery",
        "Documentation",
        "MS",
        "word",
        "Environment",
        "SQL",
        "Server",
        "SQL",
        "TSQL",
        "BCP",
        "Visual",
        "Basic",
        "Crystal",
        "Reports",
        "Java",
        "J2ee",
        "JDBC",
        "EJB",
        "JSP",
        "EL",
        "JSTL",
        "JUNIT",
        "XML",
        "SOAP",
        "WSDL",
        "Eclipse",
        "Windows",
        "XP",
        "Oracle",
        "Education",
        "Masters",
        "Additional",
        "Information",
        "TECHNICAL",
        "SKILL",
        "SET",
        "LanguagesTools",
        "Java",
        "C",
        "C",
        "C",
        "Scala",
        "VB",
        "XML",
        "HTMLXHTML",
        "HDML",
        "DHTML",
        "Big",
        "Data",
        "HDFS",
        "Map",
        "HIVE",
        "PIG",
        "HBase",
        "Oozie",
        "Zookeeper",
        "Spark",
        "Mahout",
        "Kafka",
        "Storm",
        "Cassandra",
        "Solr",
        "Impala",
        "Green",
        "plum",
        "Mongo",
        "DB",
        "WebDistributed",
        "Technologies",
        "J2EE",
        "Servlets",
        "JSP",
        "Struts",
        "Hibernate",
        "JSF",
        "JSTLEJBRMIJNI",
        "XMLJAXPXSLXSLT",
        "UML",
        "MVC",
        "STRUTS",
        "Spring",
        "Corba",
        "Java",
        "Threads",
        "J2EE",
        "Technologies",
        "J2EE",
        "Servlets",
        "JSP",
        "EJB2130",
        "JDBC",
        "MVC",
        "Architecture",
        "Java",
        "Beans",
        "JNDI",
        "RMI",
        "JMS",
        "Java",
        "ANT",
        "JavaScript",
        "Spring",
        "Browser",
        "LanguagesScripting",
        "HTML",
        "XHTML",
        "CSS",
        "XML",
        "XSL",
        "XSD",
        "XSLT",
        "Java",
        "script",
        "HTML",
        "DOM",
        "DHTML",
        "AJAX",
        "AppWeb",
        "Servers",
        "IBM",
        "Web",
        "sphere",
        "BEA",
        "Web",
        "logic",
        "Jdeveloper",
        "Apache",
        "Tomcat",
        "JBoss",
        "GUI",
        "Environment",
        "Swing",
        "AWT",
        "Applets",
        "Web",
        "Services",
        "Technology",
        "SOAP",
        "WSDL",
        "UDDI",
        "XML",
        "SOA",
        "JAXRPC",
        "IBM",
        "Web",
        "Sphere",
        "MQ",
        "v53",
        "JMS",
        "Testing",
        "Case",
        "Tools",
        "JUnit",
        "Log4j",
        "Rational",
        "Clear",
        "case",
        "CVS",
        "ANT",
        "Maven",
        "JBuilder",
        "Configuration",
        "Management",
        "Chef",
        "Puppet",
        "Ansible",
        "Docker",
        "Build",
        "Tools",
        "CVS",
        "Subversion",
        "GIT",
        "Ant",
        "Maven",
        "Gradle",
        "Hudson",
        "TeamCity",
        "Jenkins",
        "Chef",
        "Puppet",
        "Ansible",
        "Docker",
        "CI",
        "Tools",
        "Jenkins",
        "Bamboo",
        "Scripting",
        "Languages",
        "Python",
        "Shell",
        "Bash",
        "Perl",
        "PowerShell",
        "Ruby",
        "Groovy",
        "PowerShell",
        "Monitoring",
        "Tools",
        "Nagios",
        "Cloud",
        "Watch",
        "JIIRA",
        "Bugzilla",
        "Remedy",
        "SQL",
        "Oracle",
        "MS",
        "SQL",
        "Server",
        "DB2",
        "MS",
        "Access",
        "SQL",
        "Teradata",
        "Cassandra",
        "Greenplum",
        "Mongo",
        "DB",
        "Operating",
        "systems",
        "Windows",
        "Solaris",
        "Unix",
        "Linux",
        "Red",
        "Hat",
        "Linux",
        "Sun",
        "Solaris",
        "Ubuntu",
        "CentOS"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T21:39:50.490489",
    "resume_data": "Sr Hadoop Developer Sr Hadoop span lDeveloperspan Sr Hadoop Developer Walmart Bentonville AR 4 years of experience in developing applications that perform large scale Distributed Data Processing using Big Data ecosystem tools Hadoop Hive Pig Sqoop Hbase Cassandra Spark Spark Streaming MLLib Mahout Oozie Zoo Keeper Flume Yarn and Avro Passionate about Big Data Analytics and skilled in exploring data content and Expert in distributed computing algorithms and data analytics Hands on experience in using various Hadoop distributions Apache Cloudera Horton works MapR InDepth knowledge and experience in design development and deployments of Big Data projects using Hadoop Data Analytics NoSQL Distributed Machine Learning frameworks Solid understanding of SQL NOSQL databases such as Oracle Postgre SQL MySQL Mongo DB HBase Cassandra Knowledge on Mongo DB NoSQL data modeling tuning disaster recovery backup used it for distributed storage and processing using CRUD Worked on Mongo DB database concepts such as locking transactions indexes Sharing replication schema design Very good understanding Cassandra cluster mechanism that includes replication strategies snitch gossip consistent hashing and consistency levels Working knowledge in installing and maintaining Cassandra by configuring the cassandrayaml file as per the business requirement and performed readswrites using Java JDBC connectivity Experience with Cloudera Manager for management of Hadoop cluster Good conceptual understanding and experience in cloud computing applications using Amazon EC2 S3 EMR Experience in analyzing data using HiveQL Pig Latin HBase and custom Map Reduce programs in Java Extensive Experience on Teradata database analyzing business needs of clients developing effective and efficient solutions and ensuring client deliverables with in committed timelines Expertise in maintaining data quality data organization and metadata Experience in Coding Teradata SQL Teradata Stored Procedures Macros and Triggers Strong experience in Creating Database Objects such as Tables Views Functions Stored Procedures Indexes Triggers Cursors in Teradata Experience data processing like collecting aggregating moving from various sources using Apache Flume and Kafka Developed a data pipeline using Kafka and Spark Streaming to store data into HDFS and performed the realtime analytics on the incoming data Experience in importing the realtime data to Hadoop using Kafka and implemented the Oozie job for daily imports Experienced in working with inmemory processing framework like Spark Transformations SparkQL MLib and Spark Streaming Expertise in creating Custom Serdes in Hive Good working experience on using Sqoop to import data into HDFS from RDBMS and viceversa Expertise in job scheduling and monitoring tools like Oozie and Zoo Keeper Experience in design and development of Map Reduce Programs using Apache Hadoop for analyzing the big data as per the requirement Experience in performing adhoc queries on structured data using Hive QL and used Partition and Bucketing techniques and joins with HIVE for faster data access Experience in performing ETL operations using Pig Latin scripts Implemented Java APIs and created custom Java programs for fullfledged utilization of Hadoop and its related tools Implemented work flows that involve Hadoop actions using Oozie coordinators Experienced in implementing POC using Spark Sql and Mlib libraries Improving the performance and optimization of the existing algorithms in Hadoop using Spark Context SparkSQL Data Frame Pair RDDs YARN Hands on experience in handling Hive tables using Spark SQL Created User Defined Functions UDFs User Defined Aggregated Functions UDAFs in PIG and Hive Experience in dealing with log files to extract data and to copy into HDFS using Flume Experience with Testing Map Reduce programs using MR Unit JUnit and Easy Mock Implemented distributed searching capabilities using Solr to empower the geospatial search and navigation feature Experienced in Java Application Development ClientServer Applications InternetIntranet based applications using Core Java J2EE patterns Spring Hibernate Struts JMS Web Services SOAPREST Oracle SQL Server and other relational databases Experience writing Shell scripts in Linux OS and integrating them with other solutions Expert in developing web page interfaces using JSP Java Swings and HTML scripting languages Excellent understanding on Java beans and Hibernate framework to implement model logic to interact with RDBMS databases Experience in using IDEs like Eclipse NetBeans and Maven Hands on experienced working with source control tools such as Rational Clear Case and Clear Quest Hands on experience on writing Queries Stored procedures Functions and Triggers by using SQL Used EMR Elastic Map Reducing to perform bigdata operations in AWS Proficient using version control tools like GIT VSS SVN and PVCS Involvement in all stages of software development life cycle SDLC and follow agile methodologies and continuous delivery Strong skills in Object Oriented Analysis and Design OOAD Well versed in enterprise software development methodologies and practices including TDD BDD design patterns and performance testing Authorized to work in the US for any employer Work Experience Sr Hadoop Developer Walmart Bentonville AR May 2018 to Present Description Walmart Inc is an American multinational retail corporation that operates a chain of hypermarkets discount department stores and grocery storesweve been dedicated to making a difference in the lives of our customers Our business is the result of Sam Waltons visionary leadership along with generations of associates focused on helping customers and communities save money and live better Responsibilities Analyzed large and critical datasets using Cloudera HDFS HBase MapReduce Hive Hive UDF Pig Sqoop Zookeeper and Spark Gathered the business requirements from the Business Partners and Subject Matter Experts Developed environmental search engine using JAVA Apache SOLR and MYSQL Managed works including indexing data tuning relevance developing custom tokenizers and filters adding functionality includes playlist custom sorting and regionalization with SOLR Search Engine Ingested data from RDBMS and performed data transformations and then export the transformed data to Cassandra as per the business requirement Written multiple Map Reduce programs for data extraction transformation and aggregation from multiple file formats including XML JSON CSV other compressed file formats Developed automated processes for flattening the upstream data from Cassandra which in JSON format Used Hive UDFs to flatten the JSON Data Optimized Map Reduce Jobs to use HDFS efficiently by using various compression mechanisms Developed PIG UDFs to provide Pig capabilities for manipulating the data according to Business Requirements and worked on developing custom PIG Loaders and Implemented various requirements using Pig scripts Experienced on loading and transforming of large sets of structured semi structured and unstructured data Created POC using Spark Sql and Mlib libraries Developed a Spark Streaming module for consumption of Avro messages from Kafka Implementing different machine learning techniques in Scala using Scala machine learning library and created POC using SparkSql and Mlib libraries Experienced in Querying data using SparkSQL on top of Spark Engine implementing Spark RDDs in Scala Expertise in writing Scala code using Higher order functions for iterative algorithms in Spark for Performance considerations Experienced in managing and reviewing Hadoop log files Worked with different File Formats like TEXTFILE AVROFILE ORC and PARQUET for HIVE querying and processing Loading data by using the Teradata loader connection writing Teradata utilities scripts Fast Load Multiload and working with loader logs To monitor query run times using Teradata Performance Monitor Involved in loading of data into Teradata from legacy systems and flat files using complex MLOAD scripts and FASTLOAD scripts Create and Maintain Teradata Tables Views Macros Triggers and Stored Procedures Monitored workload job performance and capacity planning using Cloudera Distribution Worked on Data loading into Hive for Data Ingestion history and Data content summary Involved in developing Impala scripts for extraction transformation loading of data into data warehouse Used Hive and Impala to query the data in HBase Created Impala tables and SFTP scripts and Shell scripts to import data into Hadoop Developed Hbase java client API for CRUD Operations Created Hive tables and involved in data loading and writing Hive UDFs Developed Hive UDFs for rating aggregation Generated Java APIs for retrieval and analysis on NoSQL database such as HBase and Cassandra Provided adhoc queries and data metrics to the Business Users using Hive Pig Did various performance optimizations like using distributed cache for small datasets partition and bucketing in hive doing map side joins etc Worked on importing and exporting data from Oracle and DB2 into HDFS and HIVE using Sqoop for analysis visualization and to generate reports Worked on Apache spark writing python applications to convert txt xls files and parse Developed Python scripts UDFs using both Data framesSQL and RDDMapReduce in Spark for Data Aggregation queries and writing data back into RDBMS through Sqoop Use different components of Talend tOracleInput tOracleOutput tHiveInput tHiveOutput tHiveInputRow tUniqeRow tAggregateRow tRunJob tPreJob tPostJob tMap tJavaRow tJavaFlex tFilterRow etc to develop standard jobs Loading data from different source database files into Hive using Talend tool Load and transform data into HDFS from large set of structured data OracleSql server using Talend Big data studio Implemented Spark using PythonScala and utilizing Spark Core Spark Streaming and Spark SQL for faster processing of data instead of Map Reduce in Java Experience in integrating Apache Kafka with Apache Spark for real time processing Exposure on usage of Apache Kafka develop data pipeline of logs as a stream of messages using producers and consumers Scheduled Oozie workflow engine to run multiple Hive and Pig jobs which independently run with time and data availability Worked on custom Pig Loaders and Storage classes to work with a variety of data formats such as JSON Compressed CSV etc Involved in running Hadoop Streaming jobs to process Terabytes of data Used JIRA for bug tracking and CVS for version control Environment Hadoop Map Reduce Hive HDFS PIG Sqoop Oozie Cloudera Flume HBase SOLR CDH3 Cassandra Oracle UnixLinux Hadoop Hive PIG SQOOP Flume HDFS J2EE OracleSQL DB2 UnixLinux JavaScript Ajax Eclipse IDE CVS JIRA Hadoop Java developer University of Arkansas January 2017 to May 2017 Global Learning Department The University of Arkansas performs analysis on the various data they have The Global Learning Department is one of the projects that perform Data analytics on the data related to the Student applications received students admitted and other student activities such as enrollments and on the courses offered by each department The aim of the project is to perform data analytics and generate the reports for every month and semester Responsibilities Involved in all Phases of application development Followed the Agile Scrum development methodology Developed the web application using JavaJ2EE and Spring framework Developed the Business logic in the middletier using Java Beans and Java classes Implemented the Java beans autowiring and annotations in Apache Spring framework Connecting to the Hive from Java layer using the Hive JDBC driver Conducted Hibernate and created POJO classes for establishing Object Relation Mapping ORM between objects and relational database Experienced with JDBC API in establishing connection Between databases and java POJO classes Developed SQL queries to retrieve and validate Oracle data and executed the same using OJDBC drivers Played a key role in designing the Fact and dimension table for the Star Schema Created Hive tables and loaded retrieved the data from tables and developed Hive UDFs using Java Performing testing on the spark modules developed with the help of sample data sets Used Oozie workflow engine to manage interdependent Hadoop jobs and to automate several types of Hadoop jobs such as Hive Pig and Sqoop Environment JavaJ2EE Apache Spring Spark Hive PIG Tableau Oracle OJDBC Pig Sqoop Scala GIT Hadoop Developer Arteria Technologies Pvt Ltd August 2014 to November 2016 Responsibilities Installed and configured Hadoop MapReduce HDFS and developed multiple MapReduce jobs in Java for data cleansing and preprocessing Responsible to manage data coming from different sources Involved in gathering the business requirements from the Business Partners and Subject Matter Experts Proactively monitored systems and services architecture design and implementation of Hadoop deployment configuration management backup and disaster recovery systems and procedures Involved in works including indexing data tuning relevance developing custom token izers and filters adding functionality includes playlist custom sorting Supported Map Reduce Programs those are running on the cluster Involved in HDFS maintenance and loading of structured and unstructured data Involved in file movements between HDFS and AWS S3 and extensively worked with S3 bucket in AWS Worked extensively with importing metadata into Hive using Python and migrated existing tables and applications to work on AWS cloudS3 Installed and configured Pig and written Pig Latin scripts Imported data using Sqoop to load data from MySQL to HDFS on regular basis Expert knowledge on Mongo DB NoSQL data modeling tuning disaster recovery backup used it for distributed storage and processing using CRUD Extracted and restructured the data into Mongo DB using import and export command line utility tool Designed and Maintained Tez workflows to manage the flow of jobs in the cluster Developed Scripts and Batch Job to schedule various Hadoop Program Installation of Oozie workflow engine to run multiple Hive and pig jobs Writing Hive queries for data analysis to meet the business requirements Loading log data into HDFS using Flume and performing ETL Integration Collecting and aggregating large amounts of log data using Flume and staging data in HDFS for further analysis Developed Flume ETL job for handling data from HTTP Source and Sink as HDFS Created Hive tables and working on them using Hive QL Good Understanding of DAG cycle for entire Spark application flow on Spark application Web UI Developed Spark SQL scripts and involved in converting Hive UDFs to Spark SQL UDFs Implemented procedures like text analytics and processing using the inmemory computing capabilities like Apache Spark written in Scala Developed multiple Spark jobs in ScalaPython for Data cleaning preprocessing and Aggregating Developed Spark programs using Scala Involved in Creating SparkSQL Queries and Developed Oozie workflow for Spark jobs Push data as delimited files into HDFS using Talend Big data studio Analyzed and performed data integration using Talend open integration suite Wrote complex SQL queries to take data from various sources and integrated it with Talend Used Storm for an automatic mechanism for repeating attempts to download and manipulate the data when there is a hiccup Designing and development of technical architecture requirements and statistical models using R Used Storm to analyze large amounts of nonunique data points with low latency and high throughput Developed UI application using AngularJS integrated with Elastic Search to consume REST Writing the shell scripts to monitor the health check of Hadoop daemon services and respond accordingly to any warning or failure conditions Utilized Agile and Scrum Methodology to help manage and organize a team of developers with regular code review sessions Environment Hadoop Hive Linux Map Reduce HDFS Hive Pig HBase Sqoop Kafka Flume Shell Scripting Storm Java JDK 16 Java 6 Eclipse Oracle 10g PLSQL SQLPLUS Toad 96 Linux JIRA 51 Storm CVS JIRA 52 Java Developer Maveric Systems IN June 2013 to June 2014 Responsibilities Interacted with Team and Analysis Design and Develop database using ER Diagram Normalization and relational database concept Involved in Design Development and testing of the system Developed SQL Server Stored Procedures Tuned SQL Queries using Indexes and Execution Plan Developed User Defined Functions and created Views Created Triggers to maintain the Referential Integrity Implemented Exceptional Handling Involved in Requirement Analysis Development and Documentation Developed frontend using JSP HTML CSSand JavaScript Coding for DAO Objects using JDBC using DAO pattern XML and XSDs are used to define data formats Implemented J2EE design patterns such as singleton DAO for the presentation tier business tier and Integration Tier layers of the project Worked on client requirement and wrote Complex SQL Queries to generate Crystal Reports Creating and automating the regular Jobs Tuned and Optimized SQL Queries using Execution Plan and Profiler Developed user interface view component of MVC architecture with JSP Struts Custom Tag libraries HTML5 and JavaScript Used DOJO toolkit to construct Ajax requests and build dynamic web pages using JSPs DHTML and JavaScripts extensively used jQuery in web based applications Developed the controller component with Servlets and action classes Business Components are developed model components using Enterprise Java Beans EJB Established schedule and resource requirements by planning analyzing and documenting development effort to include time lines risks test requirements and performance targets Analyzing System Requirements and preparing System Design document Developing dynamic User Interface with HTML and JavaScript using JSP and Servlet Technology Designed and developed a sub system where Java Messaging Service JMS applications are developed to communicate with MQ in data exchange between different systems Used JMS elements for sending and receiving messages Used hibernate for mapping from Java classes to database tables Created and executed Test Plans using Quality Center by Test Director Mapped requirements with the Test cases in the Quality Center Supporting System Test and User acceptance test Rebuilding Indexes and Tables as part of Performance Tuning Exercise Involved in performing database Backup and Recovery Worked on Documentation using MS word Environment SQL Server 702000 SQL TSQL BCP Visual Basic 6050 Crystal Reports 745 Java J2ee JDBC EJB JSP EL JSTL JUNIT XML SOAP WSDL Eclipse Windows XP Oracle Education Masters Additional Information TECHNICAL SKILL SET LanguagesTools Java C C C Scala VB XML HTMLXHTML HDML DHTML Big Data HDFS Map Reduce HIVE PIG HBase SQOOP Oozie Zookeeper Spark Mahout Kafka Storm Cassandra Solr Impala Green plum Mongo DB WebDistributed Technologies J2EE Servlets JSP Struts Hibernate JSF JSTLEJBRMIJNI XMLJAXPXSLXSLT UML MVC STRUTS Spring Corba Java Threads J2EE Technologies J2EE Servlets JSP 2122 EJB2130 JDBC MVC Architecture Java Beans JNDI RMI JMS Java ANT 18 JavaScript Spring Browser LanguagesScripting HTML XHTML CSS XML XSL XSD XSLT Java script HTML DOM DHTML AJAX AppWeb Servers IBM Web sphere BEA Web logic Jdeveloper Apache Tomcat JBoss GUI Environment Swing AWT Applets Messaging Web Services Technology SOAP WSDL UDDI XML SOA JAXRPC IBM Web Sphere MQ v53 JMS Testing Case Tools JUnit Log4j Rational Clear case CVS ANT Maven JBuilder Configuration Management Chef Puppet Ansible Docker Build Tools CVS Subversion GIT Ant Maven Gradle Hudson TeamCity Jenkins Chef Puppet Ansible Docker CI Tools Jenkins Bamboo Scripting Languages Python Shell Bash Perl PowerShell Ruby Groovy PowerShell Monitoring Tools Nagios Cloud Watch JIIRA Bugzilla and Remedy Databases NO SQL Oracle MS SQL Server 2000 DB2 MS Access My SQL Teradata Cassandra Greenplum and Mongo DB Operating systems Windows Solaris Unix Linux Red Hat SUSE Linux Sun Solaris Ubuntu CentOS",
    "unique_id": "e2682c58-8337-4913-8acd-fc613186b7ff"
}