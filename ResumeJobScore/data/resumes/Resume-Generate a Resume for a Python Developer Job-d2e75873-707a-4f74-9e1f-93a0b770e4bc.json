{
    "clean_data": "PythonSpark Developer span lPythonspanSpark span lDeveloperspan Data Engineer Python developer Verisk Analytics 10 Years IT experience 4 Years of experience in Python Knowledge on AWS EC2 S3 and EMR Worked with Apache Spark and python for transforming the data and loading to HDFS Experience in working with Perl CGI and UNIX Shell Scripting 8 years of experience in Perl Scripting Knowledge on Hadoop ecosystem Used HDFS for storage and querying with Hive 1 Year of experience in Core Java Experience working on various databases MS SQL Server Teradata Oracle and MySQL Experience working in NoSQLMongoDB and Python Experience in reading and writing xml reports with Perl XML modules Experience working with repositories SVN and CVS Experience in working with Templating HTMLTemplate Template Toolkit Experience working with large databases with high volumes of data Experience working in all phases of SDLC System Development Life Cycle planning analysis design coding implementation testing maintenance and documentation Experience implementing SCDs Slowly Changing Dimensions types I II III Experience Enforcing Coding Standards Best Practices and User Security Possess Physical Logical and Dimensional Data Modeling skills Familiar with Data Modeling concepts Star Schema Snowflake Schema Facts and Dimensions Knowledge of OLTP OLAP and other data warehousing concepts Knowledge and experience in SQL programming Experience creating Tables Views Indexes Partitions Procedures Functions Triggers etc Experience devising SQL queries for Unit Testing Data Validation Integration Testing Experience in Job Monitoring Scheduling in TWS Tivoli Workload Scheduler Autosys Experience creating HLDs LLDs High Level Low Level Designs and Detailed Design Documents Experience with OnsiteOffshore Model Experience performing Maintenance Support and Administrator related activities Strong technical analytical logical and communication skills Work Experience PythonSpark Developer Verisk Analytics Jersey City NJ July 2017 to Present Forms Analytics First part To slice insurance documentsCoverage forms and Endorsements and get the similarity and differences in the documents Second part Perform ETL operations on vendor files which are published to S3 and load aggregated data back to S3 First Part Used python for end to end processing of documents with Spark StreamingKafka and AWS Documents are stored in MongoDb gridfs and vendor data stored in AWS S3 buckets in the source system By using python scripts read mongo gridfs data and queue the document data to kafka Kafka consumer reads the data from queue and fetches the corresponding document in Mongodb Gridfs Slice the received document with BeautifulSoup and load the data to Mongodb MS SQL and S3 Used Spark streaming using Python and Spark for calculating similarity and diff of documents Second Part Second part of project is processing of vendor documents from S3 Created AWS Lambda when vendor document is published to S3 bucket lambda sends it to Kafka topic Created AWS EMR cluster with Spark and using this cluster process the data coming from Kafka Have experience in creating RDDs DataFrames and manipulating them Have working knowledge on Spark SQL Used Apache Spark and python to perform aggregations and transformations on the loaded data and load back to S3 Used Pymongo to perform CRUD operations on Mongodb Used pandas library to manipulate the data Consuming APIs and get the JSON and parse the required information and load to MongoDb and SQL Server Used pythonmodules such as requests urllib urllib2 for web crawling in Python Python developer Citadel LLC New York NY September 2016 to July 2017 Project Data Production CDO DataProduction CDO team helps data analysts providing data scraped from websites and loading to MongoDb and vendor data to database Used python module Scrapy to scrape websites and get the data required information Used python modules such as requests urllib urllib2 for web crawling in Python Used other packages such as Beautifulsoup for data parsing in Python Used pandas to load vendor data to MSSQL after doing transformations Worked with data architects by creating scripts for transforming and loading data to Mongodb and MSSQL for their analysis Worked on connecting to MongoDb with pymongo module to perform CRUD operations Writing linux shell scripts for scheduling jobs in tidal Code deployment using Git and bitbucket Using tidal job scheduler to schedule crawler and parser for loading data to mongo and mssql Creating subscription process for loading data from vendor ftp to mssql and providing data to analysts Environment Python MSSQL Unix shell scripting Putty UNIX Linux WinXP Git Mongodb Bit bucket Tidal job scheduler PythonPerl Scripting Engineer CISCO San Jose CA October 2012 to September 2016 Project CEC Cisco Employee Connection Migration CEC Cisco Connection is an integrated intranet website It is Ciscos intranet starting point providing the primary gateway to all Cisco internal websites and applications Cisco CEC wwwin platform which currently hosts many applications in Cisco San Jose Data Centers Used python modules such as requests urllib urllib2 for web crawling in Python Used other packages such as Beautifulsoup for data parsing in Python Worked on writing and as well as read data from csv and excel file formats with Perl scripts Wrote Perl scripts to automate manual activities Wrote python routines to log into the websites and fetch data for selected options Loading vendor data into HDFS for storage and used Hive to query that data Worked on connecting to Oracle database and fetch the data with Python Worked on connecting to MongoDb with pymongo module to perform CRUD operations Wrote PythonPerl scripts to parse XMLJSON documents and load the data in database Managing offshore team and assigning the work and monitoring with Rally Publish code and the supporting files in Stage and Production using Web updater tool Code deployment using Kintana Testing the code on State and Production environment Checking the code by unit testing Documentation of User Requirement Specifications and System Requirement Specification Sending and Receiving Handovers to and from Offshore Knowledge Transfer to the newly hired Environment Python Perl Oracle Unix shell scripting WinSCP Putty UNIX Linux WinXP SVN CVS Hive Hadoop Perl Scripting Engineer Morgan Stanley New York NY July 2011 to September 2012 Project EDW OPS Enterprise Data Warehouse Operations Support Team supports the applications that come under the MSSB Morgan Stanley Smith Barney group under GWMG Global Wealth Management Group It supports Production BCP Business Continuity Plan Dev and QA regions It is an OnsiteOffshore model with onsite mostly supporting Prod and BCP Support activities include but not limited to Batch Monitoring in TWS Tivoli Workload Scheduler Job Recovery Code Deployment and Ticket Implementation Roles and Responsibilities Performed Administrator related tasks Granting access to App Dev teams to Informatica Repositories Bringing Infa services and repositories Up when they go Down Removing Object Locks in Infa Admin Console whenever needed Handled Directory Space Issues Analyzed logs in Infa Admin Console during outages and documented findings in Outage Tracker Created new Relational Mload Connections as per requirement Stay on top of emails sent to our group Served as the Owner of CM Change Management Scheduled conducted weekly CM meetingscalls with App Dev teams for all Prod BCP tickets Gathered Listed Verified Assigned and Implemented daily Change Request Service Request Tickets Ensured all open tickets were implemented and closed by EOD Deployed Informatica Mappings Workflows to their target folders in Prod BCP and QA Repositories Recovered Failed Jobs in Informatica Used WinSCP to migrate shell scripts Ini Config SQL files to their target directories in UNIX Monitored the Daily Batch around 1500 jobs that runs in TWS Tivoli Workload Scheduler Sending status emails to group with Batch Completion every 2 hours during system outages Running Jobs On Demand in TWS and Shell Scripts in UNIX as per App Dev teams requirement Analyzed Logs Recovered Failed Jobs in TWS promptly and ensured a continuous and smooth batch run Finding and removing invalid jobtime dependencies in TWS Sending and Receiving Handovers to and from Offshore Knowledge Transfer to the newly hired Environment Perl Informatica PowerCenter 91 Teradata TWS Tivoli Workload Scheduler Teradata SQL Assistant WinSCP Putty UNIX WinXP Perl Scripting Engineer WISE Pune Maharashtra September 2007 to June 2011 Project WISE Migration The FWP Migration QA team was formed to focus on building a scalable repeatable testing model SDLC process improvement environment provisioning management as well as promoting greater education awareness of QA best practices in use across Morgan Stanley for the back office warehouse migration Automating daily activities with Perl and Teradata Developing HTML reports with Perl CGI Extracting data from Teradata server by creating perl modules Creating modules for baselining the project Verify data files transaction log table spaces configuration etc Verify whether tables views stored procedures triggers and other database objects created successfully Verify schema row counts data reconciliation data integrity data correctness and validity Verify ETL Jobs middleware application functionality Bin to MSX Utility testing Migration Suite testing Verify performance meet user expectationsmatch or exceed existing system performance Verify logins users created correctly permissions assigned properly data security at various levels Generating test data from test specifications using TESTIFY tool Checking coding compliance using Assent tool This tool helps in checking if code is as per Clients standard It helps in streamlining the entire process Implementing Quality Standards as per IQMS standards This system provides guidelines for the conduct of every project and the means for monitoring it It integrates the various internal processes and intends to provide a process approach for project execution Setting up jobs in Autosys Resolved the defects raised by the testing team in Mercury Quality Center Technical mentoring of the ETL team and delegation of work Defect Analysis and Interaction with Business Users during UATSIT Created HLDs LLDs High Level Low Level Designs and Detailed Design Documents Environment Perl UNIX shell scripting Teradata Autosys Putty Mercury Quality Center Windows XP UNIX Education Master of Computer Applications in Computer Applications Pondicherry central university Puducherry Puducherry 2007",
    "entities": [
        "Spark StreamingKafka",
        "PythonPerl",
        "Created AWS EMR",
        "Python",
        "Python Worked",
        "AWS Documents",
        "Outage Tracker Created",
        "Morgan Stanley",
        "PythonPerl Scripting Engineer CISCO",
        "CRUD",
        "Global Wealth Management Group",
        "ETL",
        "User Security Possess Physical Logical",
        "BeautifulSoup",
        "Dimensional Data Modeling",
        "XMLJSON",
        "OnsiteOffshore Model",
        "AWS S3",
        "Tables Views Indexes Partitions Procedures Functions Triggers",
        "Mongodb MS SQL",
        "New York",
        "QA",
        "Unit Testing Data Validation Integration Testing",
        "CM",
        "Relational Mload Connections",
        "UNIX",
        "Job Monitoring Scheduling",
        "Cisco",
        "CA",
        "Perl",
        "OnsiteOffshore",
        "Informatica Repositories Bringing Infa",
        "Down Removing Object Locks",
        "Project EDW",
        "Responsibilities Performed Administrator",
        "Tivoli",
        "the MSSB Morgan Stanley Smith Barney",
        "Infa Admin Console",
        "Informatica Mappings Workflows",
        "NoSQLMongoDB",
        "Handled Directory Space",
        "San Jose",
        "SVN",
        "CVS",
        "Defect Analysis and Interaction with Business Users",
        "CEC",
        "Perform ETL",
        "Autosys Resolved",
        "Data Modeling",
        "PythonSpark Developer",
        "Granting",
        "Python Python",
        "SQL",
        "DataFrames",
        "Prod BCP",
        "IQMS",
        "S3 Created AWS Lambda",
        "Spark SQL Used",
        "Batch Completion",
        "Mercury Quality Center Technical",
        "Project Data Production CDO DataProduction",
        "Shell",
        "csv",
        "EOD",
        "State",
        "S3 Used Spark",
        "SDLC System Development Life Cycle",
        "CM Change Management Scheduled",
        "Documentation of User Requirement Specifications and System Requirement Specification Sending and Receiving Handovers",
        "UNIX Monitored the Daily Batch",
        "Apache Spark",
        "Teradata",
        "Verify",
        "MSX Utility",
        "Spark",
        "Rally Publish"
    ],
    "experience": "Experience in working with Perl CGI and UNIX Shell Scripting 8 years of experience in Perl Scripting Knowledge on Hadoop ecosystem Used HDFS for storage and querying with Hive 1 Year of experience in Core Java Experience working on various databases MS SQL Server Teradata Oracle and MySQL Experience working in NoSQLMongoDB and Python Experience in reading and writing xml reports with Perl XML modules Experience working with repositories SVN and CVS Experience in working with Templating HTMLTemplate Template Toolkit Experience working with large databases with high volumes of data Experience working in all phases of SDLC System Development Life Cycle planning analysis design coding implementation testing maintenance and documentation Experience implementing SCDs Slowly Changing Dimensions types I II III Experience Enforcing Coding Standards Best Practices and User Security Possess Physical Logical and Dimensional Data Modeling skills Familiar with Data Modeling concepts Star Schema Snowflake Schema Facts and Dimensions Knowledge of OLTP OLAP and other data warehousing concepts Knowledge and experience in SQL programming Experience creating Tables Views Indexes Partitions Procedures Functions Triggers etc Experience devising SQL queries for Unit Testing Data Validation Integration Testing Experience in Job Monitoring Scheduling in TWS Tivoli Workload Scheduler Autosys Experience creating HLDs LLDs High Level Low Level Designs and Detailed Design Documents Experience with OnsiteOffshore Model Experience performing Maintenance Support and Administrator related activities Strong technical analytical logical and communication skills Work Experience PythonSpark Developer Verisk Analytics Jersey City NJ July 2017 to Present Forms Analytics First part To slice insurance documentsCoverage forms and Endorsements and get the similarity and differences in the documents Second part Perform ETL operations on vendor files which are published to S3 and load aggregated data back to S3 First Part Used python for end to end processing of documents with Spark StreamingKafka and AWS Documents are stored in MongoDb gridfs and vendor data stored in AWS S3 buckets in the source system By using python scripts read mongo gridfs data and queue the document data to kafka Kafka consumer reads the data from queue and fetches the corresponding document in Mongodb Gridfs Slice the received document with BeautifulSoup and load the data to Mongodb MS SQL and S3 Used Spark streaming using Python and Spark for calculating similarity and diff of documents Second Part Second part of project is processing of vendor documents from S3 Created AWS Lambda when vendor document is published to S3 bucket lambda sends it to Kafka topic Created AWS EMR cluster with Spark and using this cluster process the data coming from Kafka Have experience in creating RDDs DataFrames and manipulating them Have working knowledge on Spark SQL Used Apache Spark and python to perform aggregations and transformations on the loaded data and load back to S3 Used Pymongo to perform CRUD operations on Mongodb Used pandas library to manipulate the data Consuming APIs and get the JSON and parse the required information and load to MongoDb and SQL Server Used pythonmodules such as requests urllib urllib2 for web crawling in Python Python developer Citadel LLC New York NY September 2016 to July 2017 Project Data Production CDO DataProduction CDO team helps data analysts providing data scraped from websites and loading to MongoDb and vendor data to database Used python module Scrapy to scrape websites and get the data required information Used python modules such as requests urllib urllib2 for web crawling in Python Used other packages such as Beautifulsoup for data parsing in Python Used pandas to load vendor data to MSSQL after doing transformations Worked with data architects by creating scripts for transforming and loading data to Mongodb and MSSQL for their analysis Worked on connecting to MongoDb with pymongo module to perform CRUD operations Writing linux shell scripts for scheduling jobs in tidal Code deployment using Git and bitbucket Using tidal job scheduler to schedule crawler and parser for loading data to mongo and mssql Creating subscription process for loading data from vendor ftp to mssql and providing data to analysts Environment Python MSSQL Unix shell scripting Putty UNIX Linux WinXP Git Mongodb Bit bucket Tidal job scheduler PythonPerl Scripting Engineer CISCO San Jose CA October 2012 to September 2016 Project CEC Cisco Employee Connection Migration CEC Cisco Connection is an integrated intranet website It is Ciscos intranet starting point providing the primary gateway to all Cisco internal websites and applications Cisco CEC wwwin platform which currently hosts many applications in Cisco San Jose Data Centers Used python modules such as requests urllib urllib2 for web crawling in Python Used other packages such as Beautifulsoup for data parsing in Python Worked on writing and as well as read data from csv and excel file formats with Perl scripts Wrote Perl scripts to automate manual activities Wrote python routines to log into the websites and fetch data for selected options Loading vendor data into HDFS for storage and used Hive to query that data Worked on connecting to Oracle database and fetch the data with Python Worked on connecting to MongoDb with pymongo module to perform CRUD operations Wrote PythonPerl scripts to parse XMLJSON documents and load the data in database Managing offshore team and assigning the work and monitoring with Rally Publish code and the supporting files in Stage and Production using Web updater tool Code deployment using Kintana Testing the code on State and Production environment Checking the code by unit testing Documentation of User Requirement Specifications and System Requirement Specification Sending and Receiving Handovers to and from Offshore Knowledge Transfer to the newly hired Environment Python Perl Oracle Unix shell scripting WinSCP Putty UNIX Linux WinXP SVN CVS Hive Hadoop Perl Scripting Engineer Morgan Stanley New York NY July 2011 to September 2012 Project EDW OPS Enterprise Data Warehouse Operations Support Team supports the applications that come under the MSSB Morgan Stanley Smith Barney group under GWMG Global Wealth Management Group It supports Production BCP Business Continuity Plan Dev and QA regions It is an OnsiteOffshore model with onsite mostly supporting Prod and BCP Support activities include but not limited to Batch Monitoring in TWS Tivoli Workload Scheduler Job Recovery Code Deployment and Ticket Implementation Roles and Responsibilities Performed Administrator related tasks Granting access to App Dev teams to Informatica Repositories Bringing Infa services and repositories Up when they go Down Removing Object Locks in Infa Admin Console whenever needed Handled Directory Space Issues Analyzed logs in Infa Admin Console during outages and documented findings in Outage Tracker Created new Relational Mload Connections as per requirement Stay on top of emails sent to our group Served as the Owner of CM Change Management Scheduled conducted weekly CM meetingscalls with App Dev teams for all Prod BCP tickets Gathered Listed Verified Assigned and Implemented daily Change Request Service Request Tickets Ensured all open tickets were implemented and closed by EOD Deployed Informatica Mappings Workflows to their target folders in Prod BCP and QA Repositories Recovered Failed Jobs in Informatica Used WinSCP to migrate shell scripts Ini Config SQL files to their target directories in UNIX Monitored the Daily Batch around 1500 jobs that runs in TWS Tivoli Workload Scheduler Sending status emails to group with Batch Completion every 2 hours during system outages Running Jobs On Demand in TWS and Shell Scripts in UNIX as per App Dev teams requirement Analyzed Logs Recovered Failed Jobs in TWS promptly and ensured a continuous and smooth batch run Finding and removing invalid jobtime dependencies in TWS Sending and Receiving Handovers to and from Offshore Knowledge Transfer to the newly hired Environment Perl Informatica PowerCenter 91 Teradata TWS Tivoli Workload Scheduler Teradata SQL Assistant WinSCP Putty UNIX WinXP Perl Scripting Engineer WISE Pune Maharashtra September 2007 to June 2011 Project WISE Migration The FWP Migration QA team was formed to focus on building a scalable repeatable testing model SDLC process improvement environment provisioning management as well as promoting greater education awareness of QA best practices in use across Morgan Stanley for the back office warehouse migration Automating daily activities with Perl and Teradata Developing HTML reports with Perl CGI Extracting data from Teradata server by creating perl modules Creating modules for baselining the project Verify data files transaction log table spaces configuration etc Verify whether tables views stored procedures triggers and other database objects created successfully Verify schema row counts data reconciliation data integrity data correctness and validity Verify ETL Jobs middleware application functionality Bin to MSX Utility testing Migration Suite testing Verify performance meet user expectationsmatch or exceed existing system performance Verify logins users created correctly permissions assigned properly data security at various levels Generating test data from test specifications using TESTIFY tool Checking coding compliance using Assent tool This tool helps in checking if code is as per Clients standard It helps in streamlining the entire process Implementing Quality Standards as per IQMS standards This system provides guidelines for the conduct of every project and the means for monitoring it It integrates the various internal processes and intends to provide a process approach for project execution Setting up jobs in Autosys Resolved the defects raised by the testing team in Mercury Quality Center Technical mentoring of the ETL team and delegation of work Defect Analysis and Interaction with Business Users during UATSIT Created HLDs LLDs High Level Low Level Designs and Detailed Design Documents Environment Perl UNIX shell scripting Teradata Autosys Putty Mercury Quality Center Windows XP UNIX Education Master of Computer Applications in Computer Applications Pondicherry central university Puducherry Puducherry 2007",
    "extracted_keywords": [
        "PythonSpark",
        "Developer",
        "span",
        "lDeveloperspan",
        "Data",
        "Engineer",
        "Python",
        "developer",
        "Verisk",
        "Analytics",
        "Years",
        "IT",
        "Years",
        "experience",
        "Python",
        "Knowledge",
        "AWS",
        "EC2",
        "S3",
        "EMR",
        "Apache",
        "Spark",
        "data",
        "loading",
        "HDFS",
        "Experience",
        "Perl",
        "CGI",
        "UNIX",
        "Shell",
        "Scripting",
        "years",
        "experience",
        "Perl",
        "Scripting",
        "Knowledge",
        "Hadoop",
        "ecosystem",
        "HDFS",
        "storage",
        "Hive",
        "Year",
        "experience",
        "Core",
        "Java",
        "Experience",
        "databases",
        "MS",
        "SQL",
        "Server",
        "Teradata",
        "Oracle",
        "MySQL",
        "Experience",
        "NoSQLMongoDB",
        "Python",
        "Experience",
        "reading",
        "xml",
        "reports",
        "Perl",
        "XML",
        "modules",
        "Experience",
        "repositories",
        "SVN",
        "CVS",
        "Experience",
        "Templating",
        "HTMLTemplate",
        "Template",
        "Toolkit",
        "Experience",
        "databases",
        "volumes",
        "data",
        "Experience",
        "phases",
        "SDLC",
        "System",
        "Development",
        "Life",
        "Cycle",
        "planning",
        "analysis",
        "design",
        "implementation",
        "testing",
        "maintenance",
        "documentation",
        "Experience",
        "SCDs",
        "Dimensions",
        "types",
        "II",
        "Experience",
        "Enforcing",
        "Coding",
        "Standards",
        "Best",
        "Practices",
        "User",
        "Security",
        "Possess",
        "Physical",
        "Logical",
        "Dimensional",
        "Data",
        "Modeling",
        "Data",
        "Modeling",
        "concepts",
        "Star",
        "Schema",
        "Snowflake",
        "Schema",
        "Facts",
        "Dimensions",
        "Knowledge",
        "OLTP",
        "OLAP",
        "data",
        "warehousing",
        "concepts",
        "Knowledge",
        "experience",
        "SQL",
        "programming",
        "Experience",
        "Tables",
        "Views",
        "Indexes",
        "Partitions",
        "Procedures",
        "Functions",
        "Triggers",
        "Experience",
        "SQL",
        "queries",
        "Unit",
        "Testing",
        "Data",
        "Validation",
        "Integration",
        "Testing",
        "Experience",
        "Job",
        "Monitoring",
        "Scheduling",
        "TWS",
        "Tivoli",
        "Workload",
        "Scheduler",
        "Autosys",
        "Experience",
        "HLDs",
        "LLDs",
        "High",
        "Level",
        "Low",
        "Level",
        "Designs",
        "Detailed",
        "Design",
        "Documents",
        "Experience",
        "OnsiteOffshore",
        "Model",
        "Experience",
        "Maintenance",
        "Support",
        "Administrator",
        "activities",
        "communication",
        "skills",
        "Work",
        "Experience",
        "PythonSpark",
        "Developer",
        "Verisk",
        "Analytics",
        "Jersey",
        "City",
        "NJ",
        "July",
        "Present",
        "Forms",
        "Analytics",
        "part",
        "insurance",
        "documentsCoverage",
        "forms",
        "Endorsements",
        "similarity",
        "differences",
        "documents",
        "part",
        "Perform",
        "ETL",
        "operations",
        "vendor",
        "files",
        "S3",
        "load",
        "data",
        "S3",
        "First",
        "Part",
        "python",
        "end",
        "end",
        "processing",
        "documents",
        "Spark",
        "StreamingKafka",
        "AWS",
        "Documents",
        "MongoDb",
        "gridfs",
        "vendor",
        "data",
        "AWS",
        "S3",
        "buckets",
        "source",
        "system",
        "scripts",
        "mongo",
        "gridfs",
        "data",
        "queue",
        "document",
        "data",
        "Kafka",
        "consumer",
        "data",
        "queue",
        "document",
        "Mongodb",
        "Gridfs",
        "Slice",
        "document",
        "BeautifulSoup",
        "data",
        "Mongodb",
        "MS",
        "SQL",
        "S3",
        "Spark",
        "streaming",
        "Python",
        "Spark",
        "similarity",
        "diff",
        "documents",
        "Second",
        "Part",
        "part",
        "project",
        "vendor",
        "documents",
        "S3",
        "Created",
        "AWS",
        "Lambda",
        "vendor",
        "document",
        "S3",
        "bucket",
        "lambda",
        "Kafka",
        "topic",
        "AWS",
        "EMR",
        "cluster",
        "Spark",
        "cluster",
        "process",
        "data",
        "Kafka",
        "experience",
        "RDDs",
        "DataFrames",
        "knowledge",
        "Spark",
        "SQL",
        "Apache",
        "Spark",
        "aggregations",
        "transformations",
        "data",
        "load",
        "S3",
        "Pymongo",
        "CRUD",
        "operations",
        "Mongodb",
        "Used",
        "pandas",
        "library",
        "data",
        "APIs",
        "JSON",
        "information",
        "load",
        "MongoDb",
        "SQL",
        "Server",
        "pythonmodules",
        "requests",
        "urllib2",
        "web",
        "Python",
        "Python",
        "developer",
        "Citadel",
        "LLC",
        "New",
        "York",
        "NY",
        "September",
        "July",
        "Project",
        "Data",
        "Production",
        "CDO",
        "DataProduction",
        "CDO",
        "team",
        "data",
        "analysts",
        "data",
        "websites",
        "loading",
        "MongoDb",
        "vendor",
        "data",
        "python",
        "module",
        "Scrapy",
        "websites",
        "data",
        "information",
        "modules",
        "requests",
        "urllib2",
        "web",
        "Python",
        "packages",
        "Beautifulsoup",
        "data",
        "Python",
        "pandas",
        "vendor",
        "data",
        "MSSQL",
        "transformations",
        "data",
        "architects",
        "scripts",
        "loading",
        "data",
        "Mongodb",
        "MSSQL",
        "analysis",
        "pymongo",
        "module",
        "CRUD",
        "operations",
        "linux",
        "shell",
        "scripts",
        "scheduling",
        "jobs",
        "Code",
        "deployment",
        "Git",
        "bitbucket",
        "job",
        "scheduler",
        "crawler",
        "parser",
        "loading",
        "data",
        "mongo",
        "mssql",
        "subscription",
        "process",
        "loading",
        "data",
        "vendor",
        "ftp",
        "data",
        "analysts",
        "Environment",
        "Python",
        "MSSQL",
        "Unix",
        "shell",
        "Putty",
        "UNIX",
        "Linux",
        "WinXP",
        "Git",
        "Mongodb",
        "Bit",
        "bucket",
        "job",
        "scheduler",
        "PythonPerl",
        "Scripting",
        "Engineer",
        "CISCO",
        "San",
        "Jose",
        "CA",
        "October",
        "September",
        "Project",
        "CEC",
        "Cisco",
        "Employee",
        "Connection",
        "Migration",
        "CEC",
        "Cisco",
        "Connection",
        "intranet",
        "website",
        "Ciscos",
        "intranet",
        "point",
        "gateway",
        "Cisco",
        "websites",
        "applications",
        "Cisco",
        "CEC",
        "wwwin",
        "platform",
        "applications",
        "Cisco",
        "San",
        "Jose",
        "Data",
        "Centers",
        "modules",
        "requests",
        "urllib2",
        "web",
        "Python",
        "packages",
        "Beautifulsoup",
        "data",
        "Python",
        "writing",
        "data",
        "csv",
        "file",
        "formats",
        "Perl",
        "scripts",
        "Wrote",
        "Perl",
        "scripts",
        "activities",
        "python",
        "routines",
        "websites",
        "data",
        "options",
        "Loading",
        "vendor",
        "data",
        "HDFS",
        "storage",
        "Hive",
        "data",
        "Oracle",
        "database",
        "data",
        "Python",
        "Worked",
        "pymongo",
        "module",
        "CRUD",
        "operations",
        "Wrote",
        "PythonPerl",
        "XMLJSON",
        "documents",
        "data",
        "database",
        "team",
        "work",
        "monitoring",
        "Rally",
        "Publish",
        "code",
        "files",
        "Stage",
        "Production",
        "Web",
        "updater",
        "tool",
        "Code",
        "deployment",
        "Kintana",
        "Testing",
        "code",
        "State",
        "Production",
        "environment",
        "code",
        "unit",
        "testing",
        "Documentation",
        "User",
        "Requirement",
        "Specifications",
        "System",
        "Requirement",
        "Specification",
        "Sending",
        "Handovers",
        "Offshore",
        "Knowledge",
        "Transfer",
        "Environment",
        "Python",
        "Perl",
        "Oracle",
        "Unix",
        "shell",
        "WinSCP",
        "Putty",
        "UNIX",
        "Linux",
        "WinXP",
        "SVN",
        "CVS",
        "Hive",
        "Hadoop",
        "Perl",
        "Scripting",
        "Engineer",
        "Morgan",
        "Stanley",
        "New",
        "York",
        "NY",
        "July",
        "September",
        "Project",
        "EDW",
        "OPS",
        "Enterprise",
        "Data",
        "Warehouse",
        "Operations",
        "Support",
        "Team",
        "applications",
        "MSSB",
        "Morgan",
        "Stanley",
        "Smith",
        "Barney",
        "group",
        "GWMG",
        "Global",
        "Wealth",
        "Management",
        "Group",
        "Production",
        "BCP",
        "Business",
        "Continuity",
        "Plan",
        "Dev",
        "QA",
        "regions",
        "OnsiteOffshore",
        "model",
        "Prod",
        "BCP",
        "Support",
        "activities",
        "Batch",
        "Monitoring",
        "TWS",
        "Tivoli",
        "Workload",
        "Scheduler",
        "Job",
        "Recovery",
        "Code",
        "Deployment",
        "Ticket",
        "Implementation",
        "Roles",
        "Responsibilities",
        "Performed",
        "Administrator",
        "tasks",
        "access",
        "App",
        "Dev",
        "teams",
        "Informatica",
        "Repositories",
        "Bringing",
        "Infa",
        "services",
        "repositories",
        "Object",
        "Locks",
        "Infa",
        "Admin",
        "Console",
        "Directory",
        "Space",
        "Issues",
        "logs",
        "Infa",
        "Admin",
        "Console",
        "outages",
        "findings",
        "Outage",
        "Tracker",
        "Relational",
        "Mload",
        "Connections",
        "requirement",
        "top",
        "emails",
        "group",
        "Owner",
        "CM",
        "Change",
        "Management",
        "CM",
        "meetingscalls",
        "App",
        "Dev",
        "teams",
        "Prod",
        "BCP",
        "tickets",
        "Gathered",
        "Listed",
        "Verified",
        "Change",
        "Request",
        "Service",
        "Request",
        "Tickets",
        "tickets",
        "EOD",
        "Deployed",
        "Informatica",
        "Mappings",
        "Workflows",
        "target",
        "folders",
        "Prod",
        "BCP",
        "QA",
        "Repositories",
        "Failed",
        "Jobs",
        "Informatica",
        "WinSCP",
        "shell",
        "scripts",
        "Ini",
        "Config",
        "SQL",
        "target",
        "directories",
        "UNIX",
        "Monitored",
        "Daily",
        "Batch",
        "jobs",
        "TWS",
        "Tivoli",
        "Workload",
        "Scheduler",
        "status",
        "emails",
        "group",
        "Batch",
        "Completion",
        "hours",
        "system",
        "outages",
        "Jobs",
        "Demand",
        "TWS",
        "Shell",
        "Scripts",
        "UNIX",
        "App",
        "Dev",
        "teams",
        "requirement",
        "Analyzed",
        "Logs",
        "Failed",
        "Jobs",
        "TWS",
        "batch",
        "Finding",
        "jobtime",
        "dependencies",
        "TWS",
        "Handovers",
        "Offshore",
        "Knowledge",
        "Transfer",
        "Environment",
        "Perl",
        "Informatica",
        "PowerCenter",
        "Teradata",
        "TWS",
        "Tivoli",
        "Workload",
        "Scheduler",
        "Teradata",
        "SQL",
        "Assistant",
        "WinSCP",
        "Putty",
        "UNIX",
        "WinXP",
        "Perl",
        "Scripting",
        "Engineer",
        "WISE",
        "Pune",
        "Maharashtra",
        "September",
        "June",
        "Project",
        "WISE",
        "Migration",
        "FWP",
        "Migration",
        "QA",
        "team",
        "testing",
        "model",
        "SDLC",
        "process",
        "improvement",
        "environment",
        "management",
        "education",
        "awareness",
        "QA",
        "practices",
        "use",
        "Morgan",
        "Stanley",
        "office",
        "warehouse",
        "migration",
        "Automating",
        "activities",
        "Perl",
        "Teradata",
        "Developing",
        "HTML",
        "reports",
        "Perl",
        "CGI",
        "data",
        "Teradata",
        "server",
        "perl",
        "modules",
        "modules",
        "project",
        "Verify",
        "data",
        "files",
        "transaction",
        "log",
        "table",
        "configuration",
        "tables",
        "views",
        "procedures",
        "triggers",
        "database",
        "objects",
        "schema",
        "row",
        "data",
        "reconciliation",
        "data",
        "integrity",
        "data",
        "correctness",
        "validity",
        "Verify",
        "ETL",
        "Jobs",
        "middleware",
        "application",
        "functionality",
        "Bin",
        "MSX",
        "Utility",
        "testing",
        "Migration",
        "Suite",
        "testing",
        "Verify",
        "performance",
        "user",
        "expectationsmatch",
        "system",
        "performance",
        "Verify",
        "users",
        "permissions",
        "data",
        "security",
        "levels",
        "test",
        "data",
        "test",
        "specifications",
        "TESTIFY",
        "tool",
        "compliance",
        "Assent",
        "tool",
        "tool",
        "code",
        "Clients",
        "standard",
        "process",
        "Quality",
        "Standards",
        "IQMS",
        "system",
        "guidelines",
        "conduct",
        "project",
        "means",
        "processes",
        "process",
        "approach",
        "project",
        "execution",
        "jobs",
        "Autosys",
        "defects",
        "testing",
        "team",
        "Mercury",
        "Quality",
        "Center",
        "Technical",
        "mentoring",
        "ETL",
        "team",
        "delegation",
        "work",
        "Defect",
        "Analysis",
        "Interaction",
        "Business",
        "Users",
        "UATSIT",
        "HLDs",
        "LLDs",
        "High",
        "Level",
        "Low",
        "Level",
        "Designs",
        "Detailed",
        "Design",
        "Documents",
        "Environment",
        "Perl",
        "UNIX",
        "shell",
        "Teradata",
        "Autosys",
        "Putty",
        "Mercury",
        "Quality",
        "Center",
        "Windows",
        "XP",
        "UNIX",
        "Education",
        "Master",
        "Computer",
        "Applications",
        "Computer",
        "Applications",
        "Pondicherry",
        "university",
        "Puducherry",
        "Puducherry"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T21:17:48.137624",
    "resume_data": "PythonSpark Developer span lPythonspanSpark span lDeveloperspan Data Engineer Python developer Verisk Analytics 10 Years IT experience 4 Years of experience in Python Knowledge on AWS EC2 S3 and EMR Worked with Apache Spark and python for transforming the data and loading to HDFS Experience in working with Perl CGI and UNIX Shell Scripting 8 years of experience in Perl Scripting Knowledge on Hadoop ecosystem Used HDFS for storage and querying with Hive 1 Year of experience in Core Java Experience working on various databases MS SQL Server Teradata Oracle and MySQL Experience working in NoSQLMongoDB and Python Experience in reading and writing xml reports with Perl XML modules Experience working with repositories SVN and CVS Experience in working with Templating HTMLTemplate Template Toolkit Experience working with large databases with high volumes of data Experience working in all phases of SDLC System Development Life Cycle planning analysis design coding implementation testing maintenance and documentation Experience implementing SCDs Slowly Changing Dimensions types I II III Experience Enforcing Coding Standards Best Practices and User Security Possess Physical Logical and Dimensional Data Modeling skills Familiar with Data Modeling concepts Star Schema Snowflake Schema Facts and Dimensions Knowledge of OLTP OLAP and other data warehousing concepts Knowledge and experience in SQL programming Experience creating Tables Views Indexes Partitions Procedures Functions Triggers etc Experience devising SQL queries for Unit Testing Data Validation Integration Testing Experience in Job Monitoring Scheduling in TWS Tivoli Workload Scheduler Autosys Experience creating HLDs LLDs High Level Low Level Designs and Detailed Design Documents Experience with OnsiteOffshore Model Experience performing Maintenance Support and Administrator related activities Strong technical analytical logical and communication skills Work Experience PythonSpark Developer Verisk Analytics Jersey City NJ July 2017 to Present Forms Analytics First part To slice insurance documentsCoverage forms and Endorsements and get the similarity and differences in the documents Second part Perform ETL operations on vendor files which are published to S3 and load aggregated data back to S3 First Part Used python for end to end processing of documents with Spark StreamingKafka and AWS Documents are stored in MongoDb gridfs and vendor data stored in AWS S3 buckets in the source system By using python scripts read mongo gridfs data and queue the document data to kafka Kafka consumer reads the data from queue and fetches the corresponding document in Mongodb Gridfs Slice the received document with BeautifulSoup and load the data to Mongodb MS SQL and S3 Used Spark streaming using Python and Spark for calculating similarity and diff of documents Second Part Second part of project is processing of vendor documents from S3 Created AWS Lambda when vendor document is published to S3 bucket lambda sends it to Kafka topic Created AWS EMR cluster with Spark and using this cluster process the data coming from Kafka Have experience in creating RDDs DataFrames and manipulating them Have working knowledge on Spark SQL Used Apache Spark and python to perform aggregations and transformations on the loaded data and load back to S3 Used Pymongo to perform CRUD operations on Mongodb Used pandas library to manipulate the data Consuming APIs and get the JSON and parse the required information and load to MongoDb and SQL Server Used pythonmodules such as requests urllib urllib2 for web crawling in Python Python developer Citadel LLC New York NY September 2016 to July 2017 Project Data Production CDO DataProduction CDO team helps data analysts providing data scraped from websites and loading to MongoDb and vendor data to database Used python module Scrapy to scrape websites and get the data required information Used python modules such as requests urllib urllib2 for web crawling in Python Used other packages such as Beautifulsoup for data parsing in Python Used pandas to load vendor data to MSSQL after doing transformations Worked with data architects by creating scripts for transforming and loading data to Mongodb and MSSQL for their analysis Worked on connecting to MongoDb with pymongo module to perform CRUD operations Writing linux shell scripts for scheduling jobs in tidal Code deployment using Git and bitbucket Using tidal job scheduler to schedule crawler and parser for loading data to mongo and mssql Creating subscription process for loading data from vendor ftp to mssql and providing data to analysts Environment Python MSSQL Unix shell scripting Putty UNIX Linux WinXP Git Mongodb Bit bucket Tidal job scheduler PythonPerl Scripting Engineer CISCO San Jose CA October 2012 to September 2016 Project CEC Cisco Employee Connection Migration CEC Cisco Connection is an integrated intranet website It is Ciscos intranet starting point providing the primary gateway to all Cisco internal websites and applications Cisco CEC wwwin platform which currently hosts many applications in Cisco San Jose Data Centers Used python modules such as requests urllib urllib2 for web crawling in Python Used other packages such as Beautifulsoup for data parsing in Python Worked on writing and as well as read data from csv and excel file formats with Perl scripts Wrote Perl scripts to automate manual activities Wrote python routines to log into the websites and fetch data for selected options Loading vendor data into HDFS for storage and used Hive to query that data Worked on connecting to Oracle database and fetch the data with Python Worked on connecting to MongoDb with pymongo module to perform CRUD operations Wrote PythonPerl scripts to parse XMLJSON documents and load the data in database Managing offshore team and assigning the work and monitoring with Rally Publish code and the supporting files in Stage and Production using Web updater tool Code deployment using Kintana Testing the code on State and Production environment Checking the code by unit testing Documentation of User Requirement Specifications and System Requirement Specification Sending and Receiving Handovers to and from Offshore Knowledge Transfer to the newly hired Environment Python Perl Oracle Unix shell scripting WinSCP Putty UNIX Linux WinXP SVN CVS Hive Hadoop Perl Scripting Engineer Morgan Stanley New York NY July 2011 to September 2012 Project EDW OPS Enterprise Data Warehouse Operations Support Team supports the applications that come under the MSSB Morgan Stanley Smith Barney group under GWMG Global Wealth Management Group It supports Production BCP Business Continuity Plan Dev and QA regions It is an OnsiteOffshore model with onsite mostly supporting Prod and BCP Support activities include but not limited to Batch Monitoring in TWS Tivoli Workload Scheduler Job Recovery Code Deployment and Ticket Implementation Roles and Responsibilities Performed Administrator related tasks Granting access to App Dev teams to Informatica Repositories Bringing Infa services and repositories Up when they go Down Removing Object Locks in Infa Admin Console whenever needed Handled Directory Space Issues Analyzed logs in Infa Admin Console during outages and documented findings in Outage Tracker Created new Relational Mload Connections as per requirement Stay on top of emails sent to our group Served as the Owner of CM Change Management Scheduled conducted weekly CM meetingscalls with App Dev teams for all Prod BCP tickets Gathered Listed Verified Assigned and Implemented daily Change Request Service Request Tickets Ensured all open tickets were implemented and closed by EOD Deployed Informatica Mappings Workflows to their target folders in Prod BCP and QA Repositories Recovered Failed Jobs in Informatica Used WinSCP to migrate shell scripts Ini Config SQL files to their target directories in UNIX Monitored the Daily Batch around 1500 jobs that runs in TWS Tivoli Workload Scheduler Sending status emails to group with Batch Completion every 2 hours during system outages Running Jobs On Demand in TWS and Shell Scripts in UNIX as per App Dev teams requirement Analyzed Logs Recovered Failed Jobs in TWS promptly and ensured a continuous and smooth batch run Finding and removing invalid jobtime dependencies in TWS Sending and Receiving Handovers to and from Offshore Knowledge Transfer to the newly hired Environment Perl Informatica PowerCenter 91 Teradata TWS Tivoli Workload Scheduler Teradata SQL Assistant WinSCP Putty UNIX WinXP Perl Scripting Engineer WISE Pune Maharashtra September 2007 to June 2011 Project WISE Migration The FWP Migration QA team was formed to focus on building a scalable repeatable testing model SDLC process improvement environment provisioning management as well as promoting greater education awareness of QA best practices in use across Morgan Stanley for the back office warehouse migration Automating daily activities with Perl and Teradata Developing HTML reports with Perl CGI Extracting data from Teradata server by creating perl modules Creating modules for baselining the project Verify data files transaction log table spaces configuration etc Verify whether tables views stored procedures triggers and other database objects created successfully Verify schema row counts data reconciliation data integrity data correctness and validity Verify ETL Jobs middleware application functionality Bin to MSX Utility testing Migration Suite testing Verify performance meet user expectationsmatch or exceed existing system performance Verify logins users created correctly permissions assigned properly data security at various levels Generating test data from test specifications using TESTIFY tool Checking coding compliance using Assent tool This tool helps in checking if code is as per Clients standard It helps in streamlining the entire process Implementing Quality Standards as per IQMS standards This system provides guidelines for the conduct of every project and the means for monitoring it It integrates the various internal processes and intends to provide a process approach for project execution Setting up jobs in Autosys Resolved the defects raised by the testing team in Mercury Quality Center Technical mentoring of the ETL team and delegation of work Defect Analysis and Interaction with Business Users during UATSIT Created HLDs LLDs High Level Low Level Designs and Detailed Design Documents Environment Perl UNIX shell scripting Teradata Autosys Putty Mercury Quality Center Windows XP UNIX Education Master of Computer Applications in Computer Applications Pondicherry central university Puducherry Puducherry 2007",
    "unique_id": "d2e75873-707a-4f74-9e1f-93a0b770e4bc"
}