{
    "clean_data": "Data Scientist Data Scientist Data Scientist Zoetis Inc Kalamazoo MI Efficient Data Scientist with around 6 years of experience in Statistical Modeling Machine Learning Data Mining with Large Data Sets of Structured and Unstructured Data and Performed Data Acquisition Data Validation Predictive Modeling and Data Visualization Significant industry experience and domain knowledge in Healthcare Retail Banking Energy and got some domain knowledge in Telecom industries Experience in feature extraction creating Regression models Classification Predictive data modeling and Cluster analysis Expertise in Python 2x3x programming with multiple packages including NumPy Pandas Matplotlib SciPy Seaborn and Scikitlearn handson experience with all Python libraries for Data Acquisition Data Cleaning Data Validation Predictive modeling and Data Visualization tools Experience in designing stunning visualizations using Tableau software and publishing and presenting dashboards Storyline on web and desktop platforms Worked on technologies like slack Git SVN and Openpyxl for reading and writing Strong business judgment and ability to take ambiguous problems and solve them in a structured hypothesisdriven and datasupported way Hands on experience in implementing LDA Nave Bayes and skilled in Random Forests Decision Trees Linear and Logistic Regression SVM Clustering Neural Networks Principle Component Analysis and good knowledge on Recommender Systems Implementation experiences in Machine Learning and deep learning including Regression Classification Neural network object tracking Natural Language Processing NLP using packages like Tensor Flow Keras NLTK Spacy Highly skilled in advanced Regression Modelling Time Series Analysis Correlation and Multivariate Analysis Experienced in Machine Learning Classification Algorithms like Logistic Regression KNN SVM Kernel SVM Naive Bayes and Decision Tree Experience in tuning algorithms using methods such as Grid Search Randomized Search KFold Cross Validation and Error Analysis Worked with outlier analysis with various methods like ZScore value analysis Liner regression Dbscan Density Based Spatial Clustering of Applications with Noise and Isolation forest Worked on Gradient Boosting decision trees with XGBoostto improve performance and accuracy in solving problems Also worked with several boosting methodologies like ADA Boost Gradient Boosting and XGBoost Implemented various statistical tests like ANOVA AB testing ZTest TTest for various business cases Validated the machine learning classifiers using Accuracy AUC ROCCurves and Lift Charts Worked on Artificial Neural Networks and Deep Learning models using Theano and Keras packages using Python Implemented and analyzed RNN based approaches for automatically predicting implicit relations in text The disclosure relation has potential applications in NLP tasks like Text Parsing Text Analytics Text Summarization Conversational systems Worked with various text analytics or Word Embedding libraries like Word2Vec Count Vectorizer GloVe LDA etc Solid knowledge and experience in Deep Learning techniques including Feed forward Neural Network Convolutional Neural Network CNN Recursive Neural Network RNN Worked with numerous data visualization tools in python like Matplotlib Seaborn ggplot pygal Worked and extracted data from various database sources like Oracle SQL Server DB2 MongoDB and Teradata Highly skilled in using Hadoop HBase Spark and Hive for basic analysis and extraction of data in the infrastructure to provide data summarization Good knowledge of Hadoop Architecture and various components such as HDFS Job Tracker Task Tracker Name Node Data Node Secondary Name Node MapReduce concepts and ecosystems including Hive and Pig Handled importing data from various data sources performed transformations using Hive MapReduce and loaded data into HDFS Experience working with MS Word MS Excel MS PowerPoint MS SharePoint and MS Project Work Experience Data Scientist Zoetis Inc Kalamazoo MI September 2018 to Present Zoetis Inc is the worlds largest producer of medicine and vaccinations for pets and livestock Zoetis delivers quality medicines vaccines and diagnostic products which are complemented by genetic tests bio devices and a range of services The project is to collect data from different sources and create a master data set And we do predictions on sales and profits Measures to be taken for improving the sales by applying machine learning strategies and statistical analyses to support animal health projects and products Responsibilities Developing data analytical databases from different sources and create a master data set Responsible for data identification collection exploration cleaning for modeling Data entry data auditing creating data reports and monitoring all data for accuracy We do predictions on sales and profits using machine learning and deep learning strategies Performed Time Series analysis on sales data to consider what measures to be taken for improve the Sales Manage large data sets from a wide variety of sources and apply analytics and statistical analyses to support animal health projects and products Analysis of biological and spatial data to develop insights into precision animal management and precision medicine Implementing analytics algorithms in Python Reprogramming languages Used Pandas NumPy seaborn SciPy Matplotlib Scikitlearn to visualization of the data after removing missing and outliers to fit in the model Applied isolation forest local outlier factor from Sklearn where local filters are used unsupervised outlier detection and score each sample Applied deep learning libraries Tensor Flow Theano Torch etc and scalable event stream processing architectures eg Lambda CEP etc Performed training Natural Language models and reinforcement learning engines to optimize intelligent agents that automate task execution Worked with dimensionality reduction techniques like PCA LDA and ICA Performed kMeans clustering in order to understand customer itemized bought products and segment the customers based on the customer products for animal medicine and vaccines behavior information for customized product offering customized and priority service to improve existing profitable relationships and to avoid customer churn etc using Python Applying Clustering algorithms to group the data on their similar behavior patterns Performed animal medicines and vaccines sales Predictive Modelling by using Decision Trees and Regressions in order to get the risk involved by giving individual scores to the customers Work with data analytics team to develop time series and optimization Performed Time Series Analysis on animal medicine and vaccine product sales datain order to extract meaningful statistics and other characteristics of the data to predict future values based on previously observed values Used Expert level understanding of different databases in combinations for Data extraction and loading joiningdata extracted from different databases and loading to a specific database in SQL Performed Advanced SQL queries for script executions like Update Insert and Delete Worked on Hadoop ecosystem components like HadoopMapReduce HDFS HBase Hive Sqoop Pig including their installation and configuration Used Hive to store the data and perform data cleaning steps for huge datasets Used self service environment Cloudera Data Science Workbench CDSW to manage the data analytics pipelines including builtin scheduling monitoring and email alerting Created various Proof of Concepts PoC and gap analysis and gathered necessary data for analysis from different sources prepared data for data exploration using data munging Implemented Agile Methodology for building an internal application Used Tableau to generate reports with internal records secondary sources of data JSON CSV and moreWhich helped the support team for better marketing Data Scientist Mars Solutions Group WI March 2017 to August 2018 The Client is the largest Healthcare Company and offers health care products insurance services Data Analytics Payment Integrity and The project was to build predictive models for customer value analysis by applying machine learning methods principal component analysis and regression on large dataset Responsibilities Creating statistical machine learning models for implementing Customer Churn Ticket routing techniques invoice premium predictions and claim classification Collaborated with other departments to collect and understand client business requirements Collaborated with Data Engineers to gathered business requirements and filtered the data according to project requirements Worked in importing and cleansing of data from various sources like Teradata Oracle flat files SQLServer 2005 with high volume data Performed feature engineering including feature intersection generating feature normalize and labelencoding with Scikitlearn preprocessing Congregated data from multiple sources and performed resampling to handle the issue of imbalanced data Treated missing values and outliers with several techniques Boxplots ZScore and DB Scan Explored and visualized the data to check the pattern distribution descriptive statistic and correlation using Python Matplotlib and Seaborn Performed NLP tasks with NLP library CoreNLP NLTK and Gensim Performed text representation techniques such as ngrams bag of words sentiment analysis etc Installed HDFS storage and data analysis tools in Amazon Web Services AWS cloud environment computing infrastructure Developed ETL processes for data conversions and construction of data warehouse using INFORMATICA Updated Python scripts to match training data with our database stored in AWS Cloud Search so that we would be able to assign each document a response label for further classification Importexport data from Teradata database to HDFS using Sqoop Performed data analysis by using Hive to retrieve the data from Hadoop cluster Sql to retrieve data from Oracle database Creating data pipelines using big data tools like Hadoop spark etc Good knowledge on Hadoop components such as HDFS Job Tracker TaskTracker Name Node Data Node and Map Reduce concepts Responsible for managing and reviewing Hadoop Log files Created bucketing and partitions in HIVE to handle the data Applied different dimensionality reduction techniques like principle component analysis PCA and tstochastic neighborhood embedding tSNE on feature matrix Worked with various customer analytics such as segmenting the customers Product Recommendations and NLP Tasks Worked with Clustering algorithms like KMeans KMeans DBSCAN and Agglomerative Hierarchical Clustering to target specific group of customers to generate profitable revenue Using NLP to sorting the email to automatically updating the records in Customer Relationship management CRM We can run natural language processing algorithms against the data and automatically extract the features or risk factors from the notes in the medical record Performed Multinomial Logistic Regression Random forest Decision Tree SVM and more machine learning algorithms Using graphical packages produced ROC Curve to visually represent True Positive Rate versus FalsePositive Rate Equally produced visualization of Precision Recall Curve for Area under the Curve Used Market Basket Analysis association rules analysis to identified patterns data quality issues and leveraged insights Addressed over fitting by implementing of the algorithm regularization methods like L2 and L1 Improved models accuracy by using Gradient Boosting technique like Light GBM and gained around 82 accuracy with Random Forest and 77 with Logistic Regression Used Kfold cross validation technique to increase the model performance and worked with hyper parameter tuning methods like Grid Search Worked with visualization tools like Tableau Cognos and Micro Strategy to create business reports for higher management and used Python visualization libraries like Seaborn Matplotlib and ggplot depending on business requirements Provided schedules status reports and issue resolutions to the Project team Business Users and Project Managers Data Analyst Data Scientist CMS Energy Jackson MI January 2016 to February 2017 CMS Energy is an energy company that is focused principally on utility operations I was responsible for building a new data science department with the help of other departments and I was able to learn how the business is operated and helped the company to grow and stay ahead of the competition By using machine learning we improvised the predictive algorithm for pricing strategy And we creating alerts that would notify customers of potential issues that their system has solely based on the data available to me Responsibilities Worked on Data Manipulation Visualization Machine Learning Python SQL NoSQL MongoDB Hadoop Performed Advanced SQL queries for script executions like Update Insert and Delete Used Expert level understanding of different databases in combinations for Data extraction and loading joining data extracted from different databases and loading to a specific database in SQL Programmed utilities in Python that uses packages like SciPy NumPy pandas stats model scikit learn XG boost matplotlib plotly NLTK seaborn bokeh Transformed the business requirements into analytical models designing algorithms building models developing data mining and reporting solutions that scales across massive volume of structured and unstructured data Have done Normalization Denormalization techniques for optimum performance in relational and dimensional database environments Worked on customer segmentation using an unsupervised learning technique clustering Implemented Classification using supervised learninglike Logistic Regression Decision trees KNN Naive Bayes Built models using Statistical techniques and Machine Learning classification models like XG Boost SVM and Random Forest Created various Proof of Concepts PoC and gap analysis and gathered necessary data for analysis from different sources prepared data for data exploration using data munging Used jupyter notebook for spark to make data manipulations Developed ETL processes for data conversions and construction of data warehouse using INFORMATICA Worked on Hadoop ecosystem components like Hadoop MapReduce HDFS HBase Hive Sqoop and Pig including their installation and configuration Handled importing data from various data sources performed transformations using Hive Map Reduce and loaded data into HDFS Used Hive to store the data and perform data cleaning steps for huge datasets Extracted data from source XML in HDFS preparing data for exploratory analysis using data munging Interacted with the other departments to understand and identify data needs and requirements and work with other members of the IT organization to deliver data visualization and reporting solutions to address those needs Used visualization tools like Tableau for the interactive graphs Used python libraries Matplotlib and Seaborn for creating dashboards Data Analyst Karvy Financial Services Limited November 2014 to December 2015 Karvy Financial Services Limited is a company which has been playing a very proactive role in the economic growth of India by providing loans to Micro Small Business segments and individuals like credit for the requirements of different sectors of economy Industries exports trading agriculture infrastructure and the individual segments We worked on various projects which handle customer analytics Credit Risk analysis and assessing risks associated with loans like identify and prevent fraudulent loans identify and prevent fraud detection for transactions Responsibilities Compiled data from various sources public and private databases to perform complex analysis and data manipulation for actionable results Applied concepts of probability distribution and statistical inference on the given dataset to unearth interesting findings using comparison Ttest Ftest Rsquared Pvalue etc Applied linear regression multiple regressions ordinary least square method meanvariance the theory of large numbers logistic regression dummy variable residuals Poisson distribution Naive Bayes fitting function etc to data with help of Scikit SciPy NumPy and Pandas module of Python Applied Principal Component Analysis PCA based unsupervised technique to determine unusual VPN logon time Performed Clustering with historical demographic and behavioral data as features to implement the personalized marketing to the customers Also created classification model using Logistic Regression Random Forests to classify dependent variable into two classes which are risky and okay Used FScore Precision recall evaluating model performance Built user behavior models for finding activity patterns and evaluating risk scores for every transaction using historic data to train the supervised learning models such as Decision trees Random Forests and SVM Real time analysis of customers financial profile and providing recommendation for financial products best suited Collected historical data and thirdparty data from different data sources and performed data integration using Alteryx Forecasted demand for loans and interest rates using Time Series analysis like ARIMAX VARMAX and HoltWinters Obtained better predictive performance of 81 accuracy using ensemble methods like Bootstrap aggregation Bagging and Boosting Light GBM Gradient Tested complex ETL mappings and sessions based on business user requirements and business rules to load data from source flat files and RDBMS tables to target tables Developed visualizations and dashboards using ggplot Tableau Prepared and presented data quality report to stakeholders to give understanding of data Python Developer Data Analyst Symbiosys Technologies Visakhapatnam Andhra Pradesh January 2014 to October 2014 Genius Brands International is our client and we performed exploratory data analysis on corporate purchase orders contracts and projects data using sampling and statistical methods Identified strata improved precision and accuracy Works with other team members including DBAs Other ETL developers Technical Architects QA and Business Analysts Project Managers Responsibilities The work will involve the development of workflows triggered by events from other systems Develop easy to use documentation for the frameworks and tools developed for adaption by other teams Applied kmeans and hierarchical clustering on the data Identified and analyzed business insights Developed Hive UDFs and Pig UDFs using Python in Microsoft HDInsight environment Implemented endtoend systems for Data Analytics Data Automation and customized visualization tools using Python R Hadoop and MongoDB Used pandas NumPy seaborn SciPy matplotlib scikitlearn in Python for developing various machine learning algorithms Performed data profiling to merge the data from multiple data sources Worked on csv json excel different types of files for the data cleaning and data analysis Used Python for statistical operations on the data and ggplot2 for the visualizing the data Participated in feature engineering such as feature intersection generating for adding potential powerful features plotting feature correlation matrix for feature selection and reducing feature normalization for ease to implement machine algorithms Principal Component Analysis PCA for dimensionality reduction and label encoding with Scikitlearn preprocessing Worked with several use cases like campaign sales analysis forecasting sales KPI analysis and NLP models Worked with Clustering algorithms to target specific group of customers to generate profitable revenue Worked with word embedding techniques like Word2Vec GloVe for sentiment analysis and text classifications Worked with text to vector representation methods including Counter Vectorizer Tfidf and Latent Dirichlet Allocation LDA for topic modeling Performed time series analysis using Tableau Developed and executed Ad hoc reportings according to the business needs Managed offshore projects and coordinated work for 24 hour productivity cycle ETL Developer Sutherland Global Services Hyderabad Telangana February 2013 to December 2013 Sutherland builds processes for the digital age by combining the speed and insight of design thinking with the scale and accuracy of data analytics Sutherland has customers across industries like financial services to Healthcare My role is to assist Analytics department for the data extraction and cleaning as a data preprocessing steps to build models Responsibilities Involved with Business Analysts team in requirements gathering and in preparing functional specifications and changing them into technical specifications Involved in Data mapping specifications to create and execute detailed system test plans The data mapping specifies what data will be extracted from an internal data warehouse transformed and sent to an external entity Managed full SDLC processes involving requirements management workflow analysis source data analysis data mapping metadata management data quality testing strategy and maintenance of the model Involved in extensive DATA validation by writing several complex SQL queries and Involved in backend testing and worked with data quality issues Designed SSIS packages to extract transform and load existing data into SQL Server used lots of components of SSIS such as Pivot Transformation Fuzzy Lookup Merge Merge Join Data Conversion Row Count Sort Derived Columns Conditional Split Execute SQL Task Data Flow Task and Execute Package Task Created SSIS Packages that involved dealing with different source formats flat files Excel XML OLE DB and different destination formats Debugged and troubleshot the ETL packages by using a breakpoint analyzing the process catching error information by SQL command in SSIS Developed SQL queries in SQL Server management studio Toad and generated complex reports forth end users Automated and scheduled recurring reporting processes using UNIX shell scripting and Teradata utilities such as MLOAD BTEQ and Fast Load Experience with Perl Performed data analysis and data profiling using complex SQL on various sources systems including Oracle and Teradata Education Master of Science in Information Technology Management Campbellsville University Bachelor of Technology in Electronics and Communication Engineering Jawaharlal Nehru Technological University Kakinada Andhra Pradesh",
    "entities": [
        "Oracle SQL Server",
        "HDFS Used Hive",
        "Oracle and Teradata Education Master of Science in Information Technology Management Campbellsville University Bachelor of Technology in",
        "Python Applied Principal Component Analysis",
        "INFORMATICA Updated Python",
        "Data Analytics Payment Integrity",
        "KNN Naive Bayes Built",
        "XGBoostto",
        "INFORMATICA Worked on Hadoop",
        "Sutherland",
        "UNIX",
        "Applied",
        "Performed Time Series Analysis",
        "Precision Recall Curve for Area",
        "Teradata Oracle",
        "Kakinada Andhra Pradesh",
        "ANOVA AB",
        "Interacted",
        "metadata",
        "XG Boost SVM",
        "RNN",
        "Amazon Web Services AWS",
        "Hadoop",
        "XML",
        "Nehru Technological University",
        "Tableau Developed",
        "Telangana",
        "Perl Performed",
        "Micro Small Business",
        "Principal Component Analysis",
        "Automated",
        "Healthcare Company",
        "Random Forests Decision Trees Linear and Logistic Regression SVM Clustering Neural Networks Principle Component Analysis",
        "Performed Data Acquisition Data Validation Predictive Modeling",
        "SSIS",
        "ICA Performed",
        "Python",
        "SQL Server",
        "builtin",
        "Liner",
        "Statistical Modeling Machine Learning Data Mining with Large Data Sets of Structured",
        "Implemented Classification",
        "Random Forest Created",
        "Node Data",
        "ZTest TTest",
        "Healthcare",
        "Recommender Systems Implementation",
        "Hadoop Log",
        "Ttest Ftest Rsquared Pvalue",
        "Regression Modelling Time Series Analysis Correlation and Multivariate Analysis Experienced",
        "SQL Programmed",
        "Machine Learning Classification Algorithms",
        "Collaborated with Data Engineers",
        "Telecom",
        "Congregated",
        "PowerPoint MS SharePoint",
        "CMS Energy",
        "Technical Architects QA",
        "Works",
        "KPI",
        "ADA",
        "Collaborated",
        "Product Recommendations",
        "Built",
        "Unstructured Data",
        "Python R Hadoop",
        "Cloudera Data",
        "VARMAX",
        "Genius Brands International",
        "Dbscan Density Based Spatial Clustering of Applications with Noise and Isolation",
        "Hadoop HBase Spark",
        "Hadoop MapReduce HDFS HBase Hive Sqoop",
        "Credit Risk",
        "Data Visualization",
        "Healthcare Retail Banking Energy",
        "Business Analysts Project Managers Responsibilities",
        "Developer Data",
        "HIVE",
        "Statistical",
        "Created",
        "Project Managers Data Analyst Data",
        "Developed ETL",
        "Hadoop Architecture",
        "Financial Services Limited",
        "Oracle",
        "PCA",
        "Developer Sutherland Global Services Hyderabad",
        "Text Parsing Text Analytics Text Summarization Conversational",
        "Teradata Highly",
        "MS Word MS Excel",
        "Sqoop Performed",
        "Sql",
        "Random Forest",
        "Responsibilities Involved with Business Analysts",
        "HDFS Job Tracker Task Tracker",
        "Performed Multinomial Logistic Regression Random",
        "SQL",
        "Boxplots ZScore",
        "Customer Relationship",
        "GBM",
        "Bootstrap",
        "SQL Performed",
        "NLP",
        "Openpyxl",
        "Hive",
        "Regression",
        "Karvy Financial Services Limited",
        "Pandas",
        "Present Zoetis Inc",
        "Tableau Cognos",
        "ETL",
        "Update Insert",
        "Regression Classification Neural",
        "India",
        "Performed",
        "Predictive Modelling",
        "MS Project Work Experience Data Scientist Zoetis Inc Kalamazoo MI",
        "Responsibilities Worked on Data Manipulation Visualization Machine Learning",
        "SSIS Developed",
        "Data Acquisition Data Cleaning Data Validation Predictive",
        "True Positive Rate versus FalsePositive Rate Equally",
        "Delete Worked",
        "Logistic Regression Decision",
        "Microsoft",
        "Data Analytics Data Automation",
        "OLE DB",
        "Execute Package Task Created",
        "Deep Learning",
        "DB Scan Explored",
        "Data Scientist Data Scientist Data",
        "Data Scientist Mars Solutions Group",
        "Implemented Agile Methodology",
        "Identified",
        "XG",
        "Data",
        "Toad",
        "Proof of Concepts",
        "Tableau",
        "Machine Learning",
        "Decision Trees and Regressions",
        "Artificial Neural Networks",
        "Pivot Transformation",
        "Normalization Denormalization",
        "SVM",
        "Node"
    ],
    "experience": "Experience in feature extraction creating Regression models Classification Predictive data modeling and Cluster analysis Expertise in Python 2x3x programming with multiple packages including NumPy Pandas Matplotlib SciPy Seaborn and Scikitlearn handson experience with all Python libraries for Data Acquisition Data Cleaning Data Validation Predictive modeling and Data Visualization tools Experience in designing stunning visualizations using Tableau software and publishing and presenting dashboards Storyline on web and desktop platforms Worked on technologies like slack Git SVN and Openpyxl for reading and writing Strong business judgment and ability to take ambiguous problems and solve them in a structured hypothesisdriven and datasupported way Hands on experience in implementing LDA Nave Bayes and skilled in Random Forests Decision Trees Linear and Logistic Regression SVM Clustering Neural Networks Principle Component Analysis and good knowledge on Recommender Systems Implementation experiences in Machine Learning and deep learning including Regression Classification Neural network object tracking Natural Language Processing NLP using packages like Tensor Flow Keras NLTK Spacy Highly skilled in advanced Regression Modelling Time Series Analysis Correlation and Multivariate Analysis Experienced in Machine Learning Classification Algorithms like Logistic Regression KNN SVM Kernel SVM Naive Bayes and Decision Tree Experience in tuning algorithms using methods such as Grid Search Randomized Search KFold Cross Validation and Error Analysis Worked with outlier analysis with various methods like ZScore value analysis Liner regression Dbscan Density Based Spatial Clustering of Applications with Noise and Isolation forest Worked on Gradient Boosting decision trees with XGBoostto improve performance and accuracy in solving problems Also worked with several boosting methodologies like ADA Boost Gradient Boosting and XGBoost Implemented various statistical tests like ANOVA AB testing ZTest TTest for various business cases Validated the machine learning classifiers using Accuracy AUC ROCCurves and Lift Charts Worked on Artificial Neural Networks and Deep Learning models using Theano and Keras packages using Python Implemented and analyzed RNN based approaches for automatically predicting implicit relations in text The disclosure relation has potential applications in NLP tasks like Text Parsing Text Analytics Text Summarization Conversational systems Worked with various text analytics or Word Embedding libraries like Word2Vec Count Vectorizer GloVe LDA etc Solid knowledge and experience in Deep Learning techniques including Feed forward Neural Network Convolutional Neural Network CNN Recursive Neural Network RNN Worked with numerous data visualization tools in python like Matplotlib Seaborn ggplot pygal Worked and extracted data from various database sources like Oracle SQL Server DB2 MongoDB and Teradata Highly skilled in using Hadoop HBase Spark and Hive for basic analysis and extraction of data in the infrastructure to provide data summarization Good knowledge of Hadoop Architecture and various components such as HDFS Job Tracker Task Tracker Name Node Data Node Secondary Name Node MapReduce concepts and ecosystems including Hive and Pig Handled importing data from various data sources performed transformations using Hive MapReduce and loaded data into HDFS Experience working with MS Word MS Excel MS PowerPoint MS SharePoint and MS Project Work Experience Data Scientist Zoetis Inc Kalamazoo MI September 2018 to Present Zoetis Inc is the worlds largest producer of medicine and vaccinations for pets and livestock Zoetis delivers quality medicines vaccines and diagnostic products which are complemented by genetic tests bio devices and a range of services The project is to collect data from different sources and create a master data set And we do predictions on sales and profits Measures to be taken for improving the sales by applying machine learning strategies and statistical analyses to support animal health projects and products Responsibilities Developing data analytical databases from different sources and create a master data set Responsible for data identification collection exploration cleaning for modeling Data entry data auditing creating data reports and monitoring all data for accuracy We do predictions on sales and profits using machine learning and deep learning strategies Performed Time Series analysis on sales data to consider what measures to be taken for improve the Sales Manage large data sets from a wide variety of sources and apply analytics and statistical analyses to support animal health projects and products Analysis of biological and spatial data to develop insights into precision animal management and precision medicine Implementing analytics algorithms in Python Reprogramming languages Used Pandas NumPy seaborn SciPy Matplotlib Scikitlearn to visualization of the data after removing missing and outliers to fit in the model Applied isolation forest local outlier factor from Sklearn where local filters are used unsupervised outlier detection and score each sample Applied deep learning libraries Tensor Flow Theano Torch etc and scalable event stream processing architectures eg Lambda CEP etc Performed training Natural Language models and reinforcement learning engines to optimize intelligent agents that automate task execution Worked with dimensionality reduction techniques like PCA LDA and ICA Performed kMeans clustering in order to understand customer itemized bought products and segment the customers based on the customer products for animal medicine and vaccines behavior information for customized product offering customized and priority service to improve existing profitable relationships and to avoid customer churn etc using Python Applying Clustering algorithms to group the data on their similar behavior patterns Performed animal medicines and vaccines sales Predictive Modelling by using Decision Trees and Regressions in order to get the risk involved by giving individual scores to the customers Work with data analytics team to develop time series and optimization Performed Time Series Analysis on animal medicine and vaccine product sales datain order to extract meaningful statistics and other characteristics of the data to predict future values based on previously observed values Used Expert level understanding of different databases in combinations for Data extraction and loading joiningdata extracted from different databases and loading to a specific database in SQL Performed Advanced SQL queries for script executions like Update Insert and Delete Worked on Hadoop ecosystem components like HadoopMapReduce HDFS HBase Hive Sqoop Pig including their installation and configuration Used Hive to store the data and perform data cleaning steps for huge datasets Used self service environment Cloudera Data Science Workbench CDSW to manage the data analytics pipelines including builtin scheduling monitoring and email alerting Created various Proof of Concepts PoC and gap analysis and gathered necessary data for analysis from different sources prepared data for data exploration using data munging Implemented Agile Methodology for building an internal application Used Tableau to generate reports with internal records secondary sources of data JSON CSV and moreWhich helped the support team for better marketing Data Scientist Mars Solutions Group WI March 2017 to August 2018 The Client is the largest Healthcare Company and offers health care products insurance services Data Analytics Payment Integrity and The project was to build predictive models for customer value analysis by applying machine learning methods principal component analysis and regression on large dataset Responsibilities Creating statistical machine learning models for implementing Customer Churn Ticket routing techniques invoice premium predictions and claim classification Collaborated with other departments to collect and understand client business requirements Collaborated with Data Engineers to gathered business requirements and filtered the data according to project requirements Worked in importing and cleansing of data from various sources like Teradata Oracle flat files SQLServer 2005 with high volume data Performed feature engineering including feature intersection generating feature normalize and labelencoding with Scikitlearn preprocessing Congregated data from multiple sources and performed resampling to handle the issue of imbalanced data Treated missing values and outliers with several techniques Boxplots ZScore and DB Scan Explored and visualized the data to check the pattern distribution descriptive statistic and correlation using Python Matplotlib and Seaborn Performed NLP tasks with NLP library CoreNLP NLTK and Gensim Performed text representation techniques such as ngrams bag of words sentiment analysis etc Installed HDFS storage and data analysis tools in Amazon Web Services AWS cloud environment computing infrastructure Developed ETL processes for data conversions and construction of data warehouse using INFORMATICA Updated Python scripts to match training data with our database stored in AWS Cloud Search so that we would be able to assign each document a response label for further classification Importexport data from Teradata database to HDFS using Sqoop Performed data analysis by using Hive to retrieve the data from Hadoop cluster Sql to retrieve data from Oracle database Creating data pipelines using big data tools like Hadoop spark etc Good knowledge on Hadoop components such as HDFS Job Tracker TaskTracker Name Node Data Node and Map Reduce concepts Responsible for managing and reviewing Hadoop Log files Created bucketing and partitions in HIVE to handle the data Applied different dimensionality reduction techniques like principle component analysis PCA and tstochastic neighborhood embedding tSNE on feature matrix Worked with various customer analytics such as segmenting the customers Product Recommendations and NLP Tasks Worked with Clustering algorithms like KMeans KMeans DBSCAN and Agglomerative Hierarchical Clustering to target specific group of customers to generate profitable revenue Using NLP to sorting the email to automatically updating the records in Customer Relationship management CRM We can run natural language processing algorithms against the data and automatically extract the features or risk factors from the notes in the medical record Performed Multinomial Logistic Regression Random forest Decision Tree SVM and more machine learning algorithms Using graphical packages produced ROC Curve to visually represent True Positive Rate versus FalsePositive Rate Equally produced visualization of Precision Recall Curve for Area under the Curve Used Market Basket Analysis association rules analysis to identified patterns data quality issues and leveraged insights Addressed over fitting by implementing of the algorithm regularization methods like L2 and L1 Improved models accuracy by using Gradient Boosting technique like Light GBM and gained around 82 accuracy with Random Forest and 77 with Logistic Regression Used Kfold cross validation technique to increase the model performance and worked with hyper parameter tuning methods like Grid Search Worked with visualization tools like Tableau Cognos and Micro Strategy to create business reports for higher management and used Python visualization libraries like Seaborn Matplotlib and ggplot depending on business requirements Provided schedules status reports and issue resolutions to the Project team Business Users and Project Managers Data Analyst Data Scientist CMS Energy Jackson MI January 2016 to February 2017 CMS Energy is an energy company that is focused principally on utility operations I was responsible for building a new data science department with the help of other departments and I was able to learn how the business is operated and helped the company to grow and stay ahead of the competition By using machine learning we improvised the predictive algorithm for pricing strategy And we creating alerts that would notify customers of potential issues that their system has solely based on the data available to me Responsibilities Worked on Data Manipulation Visualization Machine Learning Python SQL NoSQL MongoDB Hadoop Performed Advanced SQL queries for script executions like Update Insert and Delete Used Expert level understanding of different databases in combinations for Data extraction and loading joining data extracted from different databases and loading to a specific database in SQL Programmed utilities in Python that uses packages like SciPy NumPy pandas stats model scikit learn XG boost matplotlib plotly NLTK seaborn bokeh Transformed the business requirements into analytical models designing algorithms building models developing data mining and reporting solutions that scales across massive volume of structured and unstructured data Have done Normalization Denormalization techniques for optimum performance in relational and dimensional database environments Worked on customer segmentation using an unsupervised learning technique clustering Implemented Classification using supervised learninglike Logistic Regression Decision trees KNN Naive Bayes Built models using Statistical techniques and Machine Learning classification models like XG Boost SVM and Random Forest Created various Proof of Concepts PoC and gap analysis and gathered necessary data for analysis from different sources prepared data for data exploration using data munging Used jupyter notebook for spark to make data manipulations Developed ETL processes for data conversions and construction of data warehouse using INFORMATICA Worked on Hadoop ecosystem components like Hadoop MapReduce HDFS HBase Hive Sqoop and Pig including their installation and configuration Handled importing data from various data sources performed transformations using Hive Map Reduce and loaded data into HDFS Used Hive to store the data and perform data cleaning steps for huge datasets Extracted data from source XML in HDFS preparing data for exploratory analysis using data munging Interacted with the other departments to understand and identify data needs and requirements and work with other members of the IT organization to deliver data visualization and reporting solutions to address those needs Used visualization tools like Tableau for the interactive graphs Used python libraries Matplotlib and Seaborn for creating dashboards Data Analyst Karvy Financial Services Limited November 2014 to December 2015 Karvy Financial Services Limited is a company which has been playing a very proactive role in the economic growth of India by providing loans to Micro Small Business segments and individuals like credit for the requirements of different sectors of economy Industries exports trading agriculture infrastructure and the individual segments We worked on various projects which handle customer analytics Credit Risk analysis and assessing risks associated with loans like identify and prevent fraudulent loans identify and prevent fraud detection for transactions Responsibilities Compiled data from various sources public and private databases to perform complex analysis and data manipulation for actionable results Applied concepts of probability distribution and statistical inference on the given dataset to unearth interesting findings using comparison Ttest Ftest Rsquared Pvalue etc Applied linear regression multiple regressions ordinary least square method meanvariance the theory of large numbers logistic regression dummy variable residuals Poisson distribution Naive Bayes fitting function etc to data with help of Scikit SciPy NumPy and Pandas module of Python Applied Principal Component Analysis PCA based unsupervised technique to determine unusual VPN logon time Performed Clustering with historical demographic and behavioral data as features to implement the personalized marketing to the customers Also created classification model using Logistic Regression Random Forests to classify dependent variable into two classes which are risky and okay Used FScore Precision recall evaluating model performance Built user behavior models for finding activity patterns and evaluating risk scores for every transaction using historic data to train the supervised learning models such as Decision trees Random Forests and SVM Real time analysis of customers financial profile and providing recommendation for financial products best suited Collected historical data and thirdparty data from different data sources and performed data integration using Alteryx Forecasted demand for loans and interest rates using Time Series analysis like ARIMAX VARMAX and HoltWinters Obtained better predictive performance of 81 accuracy using ensemble methods like Bootstrap aggregation Bagging and Boosting Light GBM Gradient Tested complex ETL mappings and sessions based on business user requirements and business rules to load data from source flat files and RDBMS tables to target tables Developed visualizations and dashboards using ggplot Tableau Prepared and presented data quality report to stakeholders to give understanding of data Python Developer Data Analyst Symbiosys Technologies Visakhapatnam Andhra Pradesh January 2014 to October 2014 Genius Brands International is our client and we performed exploratory data analysis on corporate purchase orders contracts and projects data using sampling and statistical methods Identified strata improved precision and accuracy Works with other team members including DBAs Other ETL developers Technical Architects QA and Business Analysts Project Managers Responsibilities The work will involve the development of workflows triggered by events from other systems Develop easy to use documentation for the frameworks and tools developed for adaption by other teams Applied kmeans and hierarchical clustering on the data Identified and analyzed business insights Developed Hive UDFs and Pig UDFs using Python in Microsoft HDInsight environment Implemented endtoend systems for Data Analytics Data Automation and customized visualization tools using Python R Hadoop and MongoDB Used pandas NumPy seaborn SciPy matplotlib scikitlearn in Python for developing various machine learning algorithms Performed data profiling to merge the data from multiple data sources Worked on csv json excel different types of files for the data cleaning and data analysis Used Python for statistical operations on the data and ggplot2 for the visualizing the data Participated in feature engineering such as feature intersection generating for adding potential powerful features plotting feature correlation matrix for feature selection and reducing feature normalization for ease to implement machine algorithms Principal Component Analysis PCA for dimensionality reduction and label encoding with Scikitlearn preprocessing Worked with several use cases like campaign sales analysis forecasting sales KPI analysis and NLP models Worked with Clustering algorithms to target specific group of customers to generate profitable revenue Worked with word embedding techniques like Word2Vec GloVe for sentiment analysis and text classifications Worked with text to vector representation methods including Counter Vectorizer Tfidf and Latent Dirichlet Allocation LDA for topic modeling Performed time series analysis using Tableau Developed and executed Ad hoc reportings according to the business needs Managed offshore projects and coordinated work for 24 hour productivity cycle ETL Developer Sutherland Global Services Hyderabad Telangana February 2013 to December 2013 Sutherland builds processes for the digital age by combining the speed and insight of design thinking with the scale and accuracy of data analytics Sutherland has customers across industries like financial services to Healthcare My role is to assist Analytics department for the data extraction and cleaning as a data preprocessing steps to build models Responsibilities Involved with Business Analysts team in requirements gathering and in preparing functional specifications and changing them into technical specifications Involved in Data mapping specifications to create and execute detailed system test plans The data mapping specifies what data will be extracted from an internal data warehouse transformed and sent to an external entity Managed full SDLC processes involving requirements management workflow analysis source data analysis data mapping metadata management data quality testing strategy and maintenance of the model Involved in extensive DATA validation by writing several complex SQL queries and Involved in backend testing and worked with data quality issues Designed SSIS packages to extract transform and load existing data into SQL Server used lots of components of SSIS such as Pivot Transformation Fuzzy Lookup Merge Merge Join Data Conversion Row Count Sort Derived Columns Conditional Split Execute SQL Task Data Flow Task and Execute Package Task Created SSIS Packages that involved dealing with different source formats flat files Excel XML OLE DB and different destination formats Debugged and troubleshot the ETL packages by using a breakpoint analyzing the process catching error information by SQL command in SSIS Developed SQL queries in SQL Server management studio Toad and generated complex reports forth end users Automated and scheduled recurring reporting processes using UNIX shell scripting and Teradata utilities such as MLOAD BTEQ and Fast Load Experience with Perl Performed data analysis and data profiling using complex SQL on various sources systems including Oracle and Teradata Education Master of Science in Information Technology Management Campbellsville University Bachelor of Technology in Electronics and Communication Engineering Jawaharlal Nehru Technological University Kakinada Andhra Pradesh",
    "extracted_keywords": [
        "Data",
        "Scientist",
        "Data",
        "Scientist",
        "Data",
        "Scientist",
        "Zoetis",
        "Inc",
        "Kalamazoo",
        "MI",
        "Efficient",
        "Data",
        "Scientist",
        "years",
        "experience",
        "Statistical",
        "Modeling",
        "Machine",
        "Learning",
        "Data",
        "Mining",
        "Data",
        "Sets",
        "Structured",
        "Unstructured",
        "Data",
        "Performed",
        "Data",
        "Acquisition",
        "Data",
        "Validation",
        "Predictive",
        "Modeling",
        "Data",
        "Visualization",
        "Significant",
        "industry",
        "experience",
        "domain",
        "knowledge",
        "Healthcare",
        "Retail",
        "Banking",
        "Energy",
        "domain",
        "knowledge",
        "Telecom",
        "industries",
        "Experience",
        "feature",
        "extraction",
        "Regression",
        "models",
        "Classification",
        "Predictive",
        "data",
        "modeling",
        "Cluster",
        "analysis",
        "Expertise",
        "Python",
        "2x3x",
        "programming",
        "packages",
        "NumPy",
        "Pandas",
        "Matplotlib",
        "SciPy",
        "Seaborn",
        "Scikitlearn",
        "handson",
        "experience",
        "Python",
        "libraries",
        "Data",
        "Acquisition",
        "Data",
        "Cleaning",
        "Data",
        "Validation",
        "Predictive",
        "modeling",
        "Data",
        "Visualization",
        "tools",
        "Experience",
        "visualizations",
        "Tableau",
        "software",
        "publishing",
        "dashboards",
        "Storyline",
        "web",
        "desktop",
        "platforms",
        "technologies",
        "slack",
        "Git",
        "SVN",
        "Openpyxl",
        "business",
        "judgment",
        "ability",
        "problems",
        "hypothesisdriven",
        "way",
        "Hands",
        "experience",
        "LDA",
        "Nave",
        "Bayes",
        "Random",
        "Forests",
        "Decision",
        "Trees",
        "Linear",
        "Logistic",
        "Regression",
        "SVM",
        "Clustering",
        "Neural",
        "Networks",
        "Principle",
        "Component",
        "Analysis",
        "knowledge",
        "Recommender",
        "Systems",
        "Implementation",
        "experiences",
        "Machine",
        "Learning",
        "learning",
        "Regression",
        "Classification",
        "Neural",
        "network",
        "object",
        "Natural",
        "Language",
        "Processing",
        "NLP",
        "packages",
        "Tensor",
        "Flow",
        "Keras",
        "NLTK",
        "Spacy",
        "Regression",
        "Modelling",
        "Time",
        "Series",
        "Analysis",
        "Correlation",
        "Multivariate",
        "Analysis",
        "Machine",
        "Learning",
        "Classification",
        "Algorithms",
        "Logistic",
        "Regression",
        "KNN",
        "SVM",
        "Kernel",
        "SVM",
        "Naive",
        "Bayes",
        "Decision",
        "Tree",
        "Experience",
        "algorithms",
        "methods",
        "Grid",
        "Search",
        "Randomized",
        "Search",
        "KFold",
        "Cross",
        "Validation",
        "Error",
        "Analysis",
        "outlier",
        "analysis",
        "methods",
        "value",
        "analysis",
        "Liner",
        "regression",
        "Dbscan",
        "Density",
        "Based",
        "Spatial",
        "Clustering",
        "Applications",
        "Noise",
        "Isolation",
        "forest",
        "Gradient",
        "decision",
        "trees",
        "XGBoostto",
        "performance",
        "accuracy",
        "problems",
        "methodologies",
        "ADA",
        "Boost",
        "Gradient",
        "Boosting",
        "XGBoost",
        "tests",
        "ANOVA",
        "AB",
        "testing",
        "ZTest",
        "TTest",
        "business",
        "cases",
        "machine",
        "classifiers",
        "Accuracy",
        "AUC",
        "ROCCurves",
        "Lift",
        "Charts",
        "Worked",
        "Artificial",
        "Neural",
        "Networks",
        "Deep",
        "Learning",
        "models",
        "Theano",
        "Keras",
        "packages",
        "Python",
        "RNN",
        "approaches",
        "relations",
        "text",
        "disclosure",
        "relation",
        "applications",
        "NLP",
        "tasks",
        "Text",
        "Text",
        "Analytics",
        "Text",
        "Summarization",
        "Conversational",
        "systems",
        "text",
        "analytics",
        "Word",
        "libraries",
        "Word2Vec",
        "Count",
        "Vectorizer",
        "GloVe",
        "LDA",
        "knowledge",
        "experience",
        "Deep",
        "Learning",
        "techniques",
        "Feed",
        "Neural",
        "Network",
        "Convolutional",
        "Neural",
        "Network",
        "CNN",
        "Recursive",
        "Neural",
        "Network",
        "RNN",
        "data",
        "visualization",
        "tools",
        "python",
        "Matplotlib",
        "Seaborn",
        "ggplot",
        "pygal",
        "Worked",
        "data",
        "database",
        "sources",
        "Oracle",
        "SQL",
        "Server",
        "DB2",
        "MongoDB",
        "Teradata",
        "Hadoop",
        "HBase",
        "Spark",
        "Hive",
        "analysis",
        "extraction",
        "data",
        "infrastructure",
        "data",
        "summarization",
        "knowledge",
        "Hadoop",
        "Architecture",
        "components",
        "HDFS",
        "Job",
        "Tracker",
        "Task",
        "Tracker",
        "Name",
        "Node",
        "Data",
        "Node",
        "Secondary",
        "Name",
        "Node",
        "MapReduce",
        "concepts",
        "ecosystems",
        "Hive",
        "Pig",
        "data",
        "data",
        "sources",
        "transformations",
        "Hive",
        "MapReduce",
        "data",
        "HDFS",
        "Experience",
        "MS",
        "Word",
        "MS",
        "Excel",
        "MS",
        "PowerPoint",
        "MS",
        "SharePoint",
        "MS",
        "Project",
        "Work",
        "Experience",
        "Data",
        "Scientist",
        "Zoetis",
        "Inc",
        "Kalamazoo",
        "MI",
        "September",
        "Present",
        "Zoetis",
        "Inc",
        "worlds",
        "producer",
        "medicine",
        "vaccinations",
        "pets",
        "livestock",
        "Zoetis",
        "quality",
        "medicines",
        "vaccines",
        "products",
        "tests",
        "devices",
        "range",
        "services",
        "project",
        "data",
        "sources",
        "master",
        "data",
        "predictions",
        "sales",
        "Measures",
        "sales",
        "machine",
        "learning",
        "strategies",
        "analyses",
        "animal",
        "health",
        "projects",
        "products",
        "Responsibilities",
        "data",
        "databases",
        "sources",
        "master",
        "data",
        "data",
        "identification",
        "collection",
        "exploration",
        "cleaning",
        "Data",
        "entry",
        "data",
        "auditing",
        "data",
        "reports",
        "data",
        "accuracy",
        "predictions",
        "sales",
        "profits",
        "machine",
        "learning",
        "learning",
        "strategies",
        "Performed",
        "Time",
        "Series",
        "analysis",
        "sales",
        "data",
        "measures",
        "Sales",
        "Manage",
        "data",
        "sets",
        "variety",
        "sources",
        "analytics",
        "analyses",
        "animal",
        "health",
        "projects",
        "products",
        "Analysis",
        "data",
        "insights",
        "precision",
        "animal",
        "management",
        "precision",
        "medicine",
        "Implementing",
        "analytics",
        "algorithms",
        "Python",
        "languages",
        "Pandas",
        "NumPy",
        "SciPy",
        "Matplotlib",
        "Scikitlearn",
        "visualization",
        "data",
        "outliers",
        "model",
        "isolation",
        "forest",
        "outlier",
        "factor",
        "Sklearn",
        "filters",
        "outlier",
        "detection",
        "sample",
        "learning",
        "Tensor",
        "Flow",
        "Theano",
        "Torch",
        "etc",
        "event",
        "stream",
        "processing",
        "architectures",
        "eg",
        "Lambda",
        "CEP",
        "Performed",
        "training",
        "Natural",
        "Language",
        "models",
        "reinforcement",
        "learning",
        "engines",
        "agents",
        "task",
        "execution",
        "dimensionality",
        "reduction",
        "techniques",
        "PCA",
        "LDA",
        "ICA",
        "Performed",
        "kMeans",
        "order",
        "customer",
        "products",
        "customers",
        "customer",
        "products",
        "animal",
        "medicine",
        "vaccines",
        "behavior",
        "information",
        "product",
        "priority",
        "service",
        "relationships",
        "customer",
        "churn",
        "Python",
        "algorithms",
        "data",
        "behavior",
        "animal",
        "medicines",
        "vaccines",
        "sales",
        "Predictive",
        "Modelling",
        "Decision",
        "Trees",
        "Regressions",
        "order",
        "risk",
        "scores",
        "customers",
        "data",
        "analytics",
        "team",
        "time",
        "series",
        "optimization",
        "Performed",
        "Time",
        "Series",
        "Analysis",
        "animal",
        "medicine",
        "vaccine",
        "product",
        "sales",
        "order",
        "statistics",
        "characteristics",
        "data",
        "values",
        "values",
        "Expert",
        "level",
        "understanding",
        "databases",
        "combinations",
        "Data",
        "extraction",
        "loading",
        "joiningdata",
        "databases",
        "database",
        "SQL",
        "Performed",
        "Advanced",
        "SQL",
        "script",
        "executions",
        "Update",
        "Insert",
        "Delete",
        "Worked",
        "Hadoop",
        "ecosystem",
        "components",
        "HadoopMapReduce",
        "HDFS",
        "HBase",
        "Hive",
        "Sqoop",
        "Pig",
        "installation",
        "configuration",
        "Hive",
        "data",
        "data",
        "steps",
        "datasets",
        "self",
        "service",
        "environment",
        "Cloudera",
        "Data",
        "Science",
        "Workbench",
        "CDSW",
        "data",
        "analytics",
        "pipelines",
        "scheduling",
        "monitoring",
        "email",
        "Proof",
        "Concepts",
        "PoC",
        "gap",
        "analysis",
        "data",
        "analysis",
        "sources",
        "data",
        "data",
        "exploration",
        "data",
        "munging",
        "Agile",
        "Methodology",
        "application",
        "Tableau",
        "reports",
        "records",
        "sources",
        "data",
        "JSON",
        "CSV",
        "moreWhich",
        "support",
        "team",
        "marketing",
        "Data",
        "Scientist",
        "Mars",
        "Solutions",
        "Group",
        "WI",
        "March",
        "August",
        "Client",
        "Healthcare",
        "Company",
        "health",
        "care",
        "products",
        "insurance",
        "services",
        "Data",
        "Analytics",
        "Payment",
        "Integrity",
        "project",
        "models",
        "customer",
        "value",
        "analysis",
        "machine",
        "learning",
        "methods",
        "component",
        "analysis",
        "regression",
        "Responsibilities",
        "machine",
        "learning",
        "models",
        "Customer",
        "Churn",
        "Ticket",
        "techniques",
        "premium",
        "predictions",
        "classification",
        "Collaborated",
        "departments",
        "client",
        "business",
        "requirements",
        "Data",
        "Engineers",
        "business",
        "requirements",
        "data",
        "project",
        "requirements",
        "cleansing",
        "data",
        "sources",
        "Teradata",
        "Oracle",
        "files",
        "SQLServer",
        "volume",
        "data",
        "Performed",
        "feature",
        "engineering",
        "feature",
        "intersection",
        "feature",
        "normalize",
        "Scikitlearn",
        "Congregated",
        "data",
        "sources",
        "resampling",
        "issue",
        "data",
        "values",
        "outliers",
        "techniques",
        "Boxplots",
        "ZScore",
        "DB",
        "Scan",
        "Explored",
        "data",
        "pattern",
        "distribution",
        "statistic",
        "correlation",
        "Python",
        "Matplotlib",
        "Seaborn",
        "Performed",
        "NLP",
        "tasks",
        "NLP",
        "library",
        "CoreNLP",
        "NLTK",
        "Gensim",
        "Performed",
        "text",
        "representation",
        "techniques",
        "ngrams",
        "bag",
        "words",
        "sentiment",
        "analysis",
        "HDFS",
        "storage",
        "data",
        "analysis",
        "tools",
        "Amazon",
        "Web",
        "Services",
        "AWS",
        "cloud",
        "environment",
        "infrastructure",
        "Developed",
        "ETL",
        "processes",
        "data",
        "conversions",
        "construction",
        "data",
        "warehouse",
        "INFORMATICA",
        "Updated",
        "Python",
        "scripts",
        "training",
        "data",
        "database",
        "AWS",
        "Cloud",
        "Search",
        "document",
        "response",
        "label",
        "classification",
        "Importexport",
        "data",
        "Teradata",
        "database",
        "HDFS",
        "Sqoop",
        "Performed",
        "data",
        "analysis",
        "Hive",
        "data",
        "Hadoop",
        "cluster",
        "Sql",
        "data",
        "Oracle",
        "database",
        "data",
        "pipelines",
        "data",
        "tools",
        "Hadoop",
        "spark",
        "knowledge",
        "Hadoop",
        "components",
        "HDFS",
        "Job",
        "Tracker",
        "TaskTracker",
        "Name",
        "Node",
        "Data",
        "Node",
        "Map",
        "Reduce",
        "concepts",
        "Hadoop",
        "Log",
        "bucketing",
        "partitions",
        "HIVE",
        "data",
        "dimensionality",
        "reduction",
        "techniques",
        "component",
        "analysis",
        "PCA",
        "neighborhood",
        "tSNE",
        "feature",
        "matrix",
        "customer",
        "analytics",
        "customers",
        "Product",
        "Recommendations",
        "NLP",
        "Tasks",
        "algorithms",
        "KMeans",
        "KMeans",
        "DBSCAN",
        "Agglomerative",
        "Hierarchical",
        "Clustering",
        "group",
        "customers",
        "revenue",
        "NLP",
        "email",
        "records",
        "Customer",
        "Relationship",
        "management",
        "CRM",
        "language",
        "processing",
        "algorithms",
        "data",
        "features",
        "risk",
        "factors",
        "notes",
        "record",
        "Performed",
        "Multinomial",
        "Logistic",
        "Regression",
        "Random",
        "forest",
        "Decision",
        "Tree",
        "SVM",
        "machine",
        "learning",
        "algorithms",
        "packages",
        "ROC",
        "Curve",
        "Positive",
        "Rate",
        "FalsePositive",
        "Rate",
        "visualization",
        "Precision",
        "Recall",
        "Curve",
        "Area",
        "Curve",
        "Used",
        "Market",
        "Basket",
        "Analysis",
        "association",
        "analysis",
        "patterns",
        "data",
        "quality",
        "issues",
        "insights",
        "algorithm",
        "regularization",
        "methods",
        "L2",
        "L1",
        "models",
        "accuracy",
        "Gradient",
        "Boosting",
        "technique",
        "Light",
        "GBM",
        "accuracy",
        "Random",
        "Forest",
        "Logistic",
        "Regression",
        "Kfold",
        "cross",
        "validation",
        "technique",
        "model",
        "performance",
        "hyper",
        "parameter",
        "methods",
        "Grid",
        "Search",
        "visualization",
        "tools",
        "Tableau",
        "Cognos",
        "Micro",
        "Strategy",
        "business",
        "reports",
        "management",
        "Python",
        "visualization",
        "libraries",
        "Seaborn",
        "Matplotlib",
        "ggplot",
        "business",
        "requirements",
        "schedules",
        "status",
        "reports",
        "issue",
        "resolutions",
        "Project",
        "team",
        "Business",
        "Users",
        "Project",
        "Managers",
        "Data",
        "Analyst",
        "Data",
        "Scientist",
        "CMS",
        "Energy",
        "Jackson",
        "MI",
        "January",
        "February",
        "CMS",
        "Energy",
        "energy",
        "company",
        "utility",
        "operations",
        "data",
        "science",
        "department",
        "help",
        "departments",
        "business",
        "company",
        "competition",
        "machine",
        "learning",
        "algorithm",
        "pricing",
        "strategy",
        "alerts",
        "customers",
        "issues",
        "system",
        "data",
        "Responsibilities",
        "Data",
        "Manipulation",
        "Visualization",
        "Machine",
        "Learning",
        "Python",
        "SQL",
        "NoSQL",
        "MongoDB",
        "Hadoop",
        "Performed",
        "Advanced",
        "SQL",
        "script",
        "executions",
        "Update",
        "Insert",
        "Delete",
        "Expert",
        "level",
        "understanding",
        "databases",
        "combinations",
        "Data",
        "extraction",
        "loading",
        "data",
        "databases",
        "database",
        "SQL",
        "utilities",
        "Python",
        "packages",
        "SciPy",
        "NumPy",
        "stats",
        "model",
        "scikit",
        "XG",
        "boost",
        "matplotlib",
        "NLTK",
        "bokeh",
        "business",
        "requirements",
        "models",
        "algorithms",
        "building",
        "models",
        "data",
        "mining",
        "reporting",
        "solutions",
        "volume",
        "data",
        "Normalization",
        "Denormalization",
        "techniques",
        "performance",
        "database",
        "environments",
        "customer",
        "segmentation",
        "learning",
        "technique",
        "Classification",
        "learninglike",
        "Logistic",
        "Regression",
        "Decision",
        "KNN",
        "Naive",
        "Bayes",
        "models",
        "techniques",
        "Machine",
        "Learning",
        "classification",
        "models",
        "XG",
        "Boost",
        "SVM",
        "Random",
        "Forest",
        "Proof",
        "Concepts",
        "PoC",
        "gap",
        "analysis",
        "data",
        "analysis",
        "sources",
        "data",
        "data",
        "exploration",
        "data",
        "jupyter",
        "notebook",
        "spark",
        "data",
        "manipulations",
        "ETL",
        "processes",
        "data",
        "conversions",
        "construction",
        "data",
        "warehouse",
        "INFORMATICA",
        "Worked",
        "Hadoop",
        "ecosystem",
        "components",
        "Hadoop",
        "MapReduce",
        "HDFS",
        "HBase",
        "Hive",
        "Sqoop",
        "Pig",
        "installation",
        "configuration",
        "data",
        "data",
        "sources",
        "transformations",
        "Hive",
        "Map",
        "Reduce",
        "data",
        "HDFS",
        "Hive",
        "data",
        "data",
        "steps",
        "datasets",
        "data",
        "source",
        "XML",
        "HDFS",
        "data",
        "analysis",
        "data",
        "Interacted",
        "departments",
        "data",
        "needs",
        "requirements",
        "work",
        "members",
        "IT",
        "organization",
        "data",
        "visualization",
        "solutions",
        "visualization",
        "tools",
        "Tableau",
        "graphs",
        "python",
        "Matplotlib",
        "Seaborn",
        "dashboards",
        "Data",
        "Analyst",
        "Karvy",
        "Financial",
        "Services",
        "Limited",
        "November",
        "December",
        "Karvy",
        "Financial",
        "Services",
        "Limited",
        "company",
        "role",
        "growth",
        "India",
        "loans",
        "Micro",
        "Small",
        "Business",
        "segments",
        "individuals",
        "credit",
        "requirements",
        "sectors",
        "economy",
        "Industries",
        "exports",
        "trading",
        "agriculture",
        "infrastructure",
        "segments",
        "projects",
        "customer",
        "analytics",
        "Credit",
        "Risk",
        "analysis",
        "risks",
        "loans",
        "loans",
        "fraud",
        "detection",
        "transactions",
        "Responsibilities",
        "data",
        "sources",
        "databases",
        "analysis",
        "data",
        "manipulation",
        "results",
        "concepts",
        "probability",
        "distribution",
        "inference",
        "dataset",
        "findings",
        "comparison",
        "Ttest",
        "Ftest",
        "Rsquared",
        "Pvalue",
        "regression",
        "regressions",
        "method",
        "meanvariance",
        "theory",
        "numbers",
        "regression",
        "residuals",
        "Poisson",
        "distribution",
        "Naive",
        "Bayes",
        "function",
        "data",
        "help",
        "Scikit",
        "SciPy",
        "NumPy",
        "Pandas",
        "module",
        "Python",
        "Applied",
        "Principal",
        "Component",
        "Analysis",
        "PCA",
        "technique",
        "VPN",
        "logon",
        "time",
        "Performed",
        "Clustering",
        "data",
        "features",
        "marketing",
        "customers",
        "classification",
        "model",
        "Logistic",
        "Regression",
        "Random",
        "Forests",
        "variable",
        "classes",
        "FScore",
        "Precision",
        "recall",
        "model",
        "performance",
        "user",
        "behavior",
        "models",
        "activity",
        "patterns",
        "risk",
        "scores",
        "transaction",
        "data",
        "learning",
        "models",
        "Decision",
        "trees",
        "Random",
        "Forests",
        "SVM",
        "time",
        "analysis",
        "customers",
        "profile",
        "recommendation",
        "products",
        "Collected",
        "data",
        "thirdparty",
        "data",
        "data",
        "sources",
        "data",
        "integration",
        "Alteryx",
        "Forecasted",
        "demand",
        "loans",
        "interest",
        "rates",
        "Time",
        "Series",
        "analysis",
        "ARIMAX",
        "VARMAX",
        "HoltWinters",
        "performance",
        "accuracy",
        "methods",
        "Bootstrap",
        "aggregation",
        "Bagging",
        "Boosting",
        "Light",
        "GBM",
        "Gradient",
        "ETL",
        "mappings",
        "sessions",
        "business",
        "user",
        "requirements",
        "business",
        "rules",
        "data",
        "source",
        "files",
        "RDBMS",
        "tables",
        "tables",
        "visualizations",
        "dashboards",
        "ggplot",
        "Tableau",
        "Prepared",
        "data",
        "quality",
        "report",
        "stakeholders",
        "understanding",
        "data",
        "Python",
        "Developer",
        "Data",
        "Analyst",
        "Symbiosys",
        "Technologies",
        "Visakhapatnam",
        "Andhra",
        "Pradesh",
        "January",
        "October",
        "Genius",
        "Brands",
        "International",
        "client",
        "data",
        "analysis",
        "purchase",
        "orders",
        "contracts",
        "projects",
        "data",
        "sampling",
        "methods",
        "strata",
        "precision",
        "accuracy",
        "team",
        "members",
        "DBAs",
        "ETL",
        "developers",
        "Technical",
        "Architects",
        "QA",
        "Business",
        "Analysts",
        "Project",
        "Managers",
        "Responsibilities",
        "work",
        "development",
        "workflows",
        "events",
        "systems",
        "documentation",
        "frameworks",
        "tools",
        "adaption",
        "teams",
        "kmeans",
        "clustering",
        "data",
        "business",
        "insights",
        "Hive",
        "UDFs",
        "Pig",
        "UDFs",
        "Python",
        "Microsoft",
        "HDInsight",
        "environment",
        "systems",
        "Data",
        "Analytics",
        "Data",
        "Automation",
        "visualization",
        "tools",
        "Python",
        "R",
        "Hadoop",
        "NumPy",
        "SciPy",
        "matplotlib",
        "Python",
        "machine",
        "learning",
        "data",
        "profiling",
        "data",
        "data",
        "sources",
        "csv",
        "json",
        "types",
        "files",
        "data",
        "cleaning",
        "data",
        "analysis",
        "Python",
        "operations",
        "data",
        "ggplot2",
        "data",
        "feature",
        "engineering",
        "feature",
        "intersection",
        "features",
        "feature",
        "correlation",
        "matrix",
        "feature",
        "selection",
        "feature",
        "normalization",
        "ease",
        "machine",
        "algorithms",
        "Principal",
        "Component",
        "Analysis",
        "PCA",
        "dimensionality",
        "reduction",
        "label",
        "encoding",
        "Scikitlearn",
        "preprocessing",
        "use",
        "cases",
        "campaign",
        "sales",
        "analysis",
        "forecasting",
        "sales",
        "KPI",
        "analysis",
        "NLP",
        "models",
        "algorithms",
        "group",
        "customers",
        "revenue",
        "word",
        "techniques",
        "Word2Vec",
        "GloVe",
        "sentiment",
        "analysis",
        "text",
        "classifications",
        "text",
        "vector",
        "representation",
        "methods",
        "Counter",
        "Vectorizer",
        "Tfidf",
        "Latent",
        "Dirichlet",
        "Allocation",
        "LDA",
        "topic",
        "time",
        "series",
        "analysis",
        "Tableau",
        "Developed",
        "Ad",
        "reportings",
        "business",
        "projects",
        "work",
        "hour",
        "productivity",
        "cycle",
        "ETL",
        "Developer",
        "Sutherland",
        "Global",
        "Services",
        "Hyderabad",
        "Telangana",
        "February",
        "December",
        "Sutherland",
        "processes",
        "age",
        "speed",
        "insight",
        "design",
        "thinking",
        "scale",
        "accuracy",
        "data",
        "analytics",
        "Sutherland",
        "customers",
        "industries",
        "services",
        "Healthcare",
        "role",
        "Analytics",
        "department",
        "data",
        "extraction",
        "cleaning",
        "data",
        "steps",
        "models",
        "Responsibilities",
        "Business",
        "Analysts",
        "team",
        "requirements",
        "specifications",
        "specifications",
        "Data",
        "mapping",
        "specifications",
        "system",
        "test",
        "data",
        "mapping",
        "specifies",
        "data",
        "data",
        "warehouse",
        "entity",
        "SDLC",
        "processes",
        "requirements",
        "management",
        "workflow",
        "analysis",
        "source",
        "data",
        "analysis",
        "data",
        "mapping",
        "metadata",
        "management",
        "data",
        "quality",
        "testing",
        "strategy",
        "maintenance",
        "model",
        "DATA",
        "validation",
        "SQL",
        "queries",
        "testing",
        "data",
        "quality",
        "issues",
        "SSIS",
        "packages",
        "transform",
        "data",
        "SQL",
        "Server",
        "lots",
        "components",
        "SSIS",
        "Pivot",
        "Transformation",
        "Fuzzy",
        "Lookup",
        "Merge",
        "Merge",
        "Join",
        "Data",
        "Conversion",
        "Row",
        "Count",
        "Columns",
        "Conditional",
        "Split",
        "Execute",
        "SQL",
        "Task",
        "Data",
        "Flow",
        "Task",
        "Execute",
        "Package",
        "Task",
        "SSIS",
        "Packages",
        "source",
        "files",
        "Excel",
        "XML",
        "OLE",
        "DB",
        "destination",
        "formats",
        "ETL",
        "packages",
        "breakpoint",
        "process",
        "error",
        "information",
        "SQL",
        "command",
        "SSIS",
        "Developed",
        "SQL",
        "queries",
        "SQL",
        "Server",
        "management",
        "studio",
        "Toad",
        "reports",
        "users",
        "reporting",
        "processes",
        "UNIX",
        "shell",
        "scripting",
        "utilities",
        "MLOAD",
        "BTEQ",
        "Fast",
        "Load",
        "Experience",
        "Perl",
        "Performed",
        "data",
        "analysis",
        "data",
        "profiling",
        "SQL",
        "sources",
        "systems",
        "Oracle",
        "Teradata",
        "Education",
        "Master",
        "Science",
        "Information",
        "Technology",
        "Management",
        "Campbellsville",
        "University",
        "Bachelor",
        "Technology",
        "Electronics",
        "Communication",
        "Engineering",
        "Jawaharlal",
        "Nehru",
        "Technological",
        "University",
        "Kakinada",
        "Andhra",
        "Pradesh"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T20:42:55.090737",
    "resume_data": "Data Scientist Data Scientist Data Scientist Zoetis Inc Kalamazoo MI Efficient Data Scientist with around 6 years of experience in Statistical Modeling Machine Learning Data Mining with Large Data Sets of Structured and Unstructured Data and Performed Data Acquisition Data Validation Predictive Modeling and Data Visualization Significant industry experience and domain knowledge in Healthcare Retail Banking Energy and got some domain knowledge in Telecom industries Experience in feature extraction creating Regression models Classification Predictive data modeling and Cluster analysis Expertise in Python 2x3x programming with multiple packages including NumPy Pandas Matplotlib SciPy Seaborn and Scikitlearn handson experience with all Python libraries for Data Acquisition Data Cleaning Data Validation Predictive modeling and Data Visualization tools Experience in designing stunning visualizations using Tableau software and publishing and presenting dashboards Storyline on web and desktop platforms Worked on technologies like slack Git SVN and Openpyxl for reading and writing Strong business judgment and ability to take ambiguous problems and solve them in a structured hypothesisdriven and datasupported way Hands on experience in implementing LDA Nave Bayes and skilled in Random Forests Decision Trees Linear and Logistic Regression SVM Clustering Neural Networks Principle Component Analysis and good knowledge on Recommender Systems Implementation experiences in Machine Learning and deep learning including Regression Classification Neural network object tracking Natural Language Processing NLP using packages like Tensor Flow Keras NLTK Spacy Highly skilled in advanced Regression Modelling Time Series Analysis Correlation and Multivariate Analysis Experienced in Machine Learning Classification Algorithms like Logistic Regression KNN SVM Kernel SVM Naive Bayes and Decision Tree Experience in tuning algorithms using methods such as Grid Search Randomized Search KFold Cross Validation and Error Analysis Worked with outlier analysis with various methods like ZScore value analysis Liner regression Dbscan Density Based Spatial Clustering of Applications with Noise and Isolation forest Worked on Gradient Boosting decision trees with XGBoostto improve performance and accuracy in solving problems Also worked with several boosting methodologies like ADA Boost Gradient Boosting and XGBoost Implemented various statistical tests like ANOVA AB testing ZTest TTest for various business cases Validated the machine learning classifiers using Accuracy AUC ROCCurves and Lift Charts Worked on Artificial Neural Networks and Deep Learning models using Theano and Keras packages using Python Implemented and analyzed RNN based approaches for automatically predicting implicit relations in text The disclosure relation has potential applications in NLP tasks like Text Parsing Text Analytics Text Summarization Conversational systems Worked with various text analytics or Word Embedding libraries like Word2Vec Count Vectorizer GloVe LDA etc Solid knowledge and experience in Deep Learning techniques including Feed forward Neural Network Convolutional Neural Network CNN Recursive Neural Network RNN Worked with numerous data visualization tools in python like Matplotlib Seaborn ggplot pygal Worked and extracted data from various database sources like Oracle SQL Server DB2 MongoDB and Teradata Highly skilled in using Hadoop HBase Spark and Hive for basic analysis and extraction of data in the infrastructure to provide data summarization Good knowledge of Hadoop Architecture and various components such as HDFS Job Tracker Task Tracker Name Node Data Node Secondary Name Node MapReduce concepts and ecosystems including Hive and Pig Handled importing data from various data sources performed transformations using Hive MapReduce and loaded data into HDFS Experience working with MS Word MS Excel MS PowerPoint MS SharePoint and MS Project Work Experience Data Scientist Zoetis Inc Kalamazoo MI September 2018 to Present Zoetis Inc is the worlds largest producer of medicine and vaccinations for pets and livestock Zoetis delivers quality medicines vaccines and diagnostic products which are complemented by genetic tests bio devices and a range of services The project is to collect data from different sources and create a master data set And we do predictions on sales and profits Measures to be taken for improving the sales by applying machine learning strategies and statistical analyses to support animal health projects and products Responsibilities Developing data analytical databases from different sources and create a master data set Responsible for data identification collection exploration cleaning for modeling Data entry data auditing creating data reports and monitoring all data for accuracy We do predictions on sales and profits using machine learning and deep learning strategies Performed Time Series analysis on sales data to consider what measures to be taken for improve the Sales Manage large data sets from a wide variety of sources and apply analytics and statistical analyses to support animal health projects and products Analysis of biological and spatial data to develop insights into precision animal management and precision medicine Implementing analytics algorithms in Python Reprogramming languages Used Pandas NumPy seaborn SciPy Matplotlib Scikitlearn to visualization of the data after removing missing and outliers to fit in the model Applied isolation forest local outlier factor from Sklearn where local filters are used unsupervised outlier detection and score each sample Applied deep learning libraries Tensor Flow Theano Torch etc and scalable event stream processing architectures eg Lambda CEP etc Performed training Natural Language models and reinforcement learning engines to optimize intelligent agents that automate task execution Worked with dimensionality reduction techniques like PCA LDA and ICA Performed kMeans clustering in order to understand customer itemized bought products and segment the customers based on the customer products for animal medicine and vaccines behavior information for customized product offering customized and priority service to improve existing profitable relationships and to avoid customer churn etc using Python Applying Clustering algorithms to group the data on their similar behavior patterns Performed animal medicines and vaccines sales Predictive Modelling by using Decision Trees and Regressions in order to get the risk involved by giving individual scores to the customers Work with data analytics team to develop time series and optimization Performed Time Series Analysis on animal medicine and vaccine product sales datain order to extract meaningful statistics and other characteristics of the data to predict future values based on previously observed values Used Expert level understanding of different databases in combinations for Data extraction and loading joiningdata extracted from different databases and loading to a specific database in SQL Performed Advanced SQL queries for script executions like Update Insert and Delete Worked on Hadoop ecosystem components like HadoopMapReduce HDFS HBase Hive Sqoop Pig including their installation and configuration Used Hive to store the data and perform data cleaning steps for huge datasets Used self service environment Cloudera Data Science Workbench CDSW to manage the data analytics pipelines including builtin scheduling monitoring and email alerting Created various Proof of Concepts PoC and gap analysis and gathered necessary data for analysis from different sources prepared data for data exploration using data munging Implemented Agile Methodology for building an internal application Used Tableau to generate reports with internal records secondary sources of data JSON CSV and moreWhich helped the support team for better marketing Data Scientist Mars Solutions Group WI March 2017 to August 2018 The Client is the largest Healthcare Company and offers health care products insurance services Data Analytics Payment Integrity and The project was to build predictive models for customer value analysis by applying machine learning methods principal component analysis and regression on large dataset Responsibilities Creating statistical machine learning models for implementing Customer Churn Ticket routing techniques invoice premium predictions and claim classification Collaborated with other departments to collect and understand client business requirements Collaborated with Data Engineers to gathered business requirements and filtered the data according to project requirements Worked in importing and cleansing of data from various sources like Teradata Oracle flat files SQLServer 2005 with high volume data Performed feature engineering including feature intersection generating feature normalize and labelencoding with Scikitlearn preprocessing Congregated data from multiple sources and performed resampling to handle the issue of imbalanced data Treated missing values and outliers with several techniques Boxplots ZScore and DB Scan Explored and visualized the data to check the pattern distribution descriptive statistic and correlation using Python Matplotlib and Seaborn Performed NLP tasks with NLP library CoreNLP NLTK and Gensim Performed text representation techniques such as ngrams bag of words sentiment analysis etc Installed HDFS storage and data analysis tools in Amazon Web Services AWS cloud environment computing infrastructure Developed ETL processes for data conversions and construction of data warehouse using INFORMATICA Updated Python scripts to match training data with our database stored in AWS Cloud Search so that we would be able to assign each document a response label for further classification Importexport data from Teradata database to HDFS using Sqoop Performed data analysis by using Hive to retrieve the data from Hadoop cluster Sql to retrieve data from Oracle database Creating data pipelines using big data tools like Hadoop spark etc Good knowledge on Hadoop components such as HDFS Job Tracker TaskTracker Name Node Data Node and Map Reduce concepts Responsible for managing and reviewing Hadoop Log files Created bucketing and partitions in HIVE to handle the data Applied different dimensionality reduction techniques like principle component analysis PCA and tstochastic neighborhood embedding tSNE on feature matrix Worked with various customer analytics such as segmenting the customers Product Recommendations and NLP Tasks Worked with Clustering algorithms like KMeans KMeans DBSCAN and Agglomerative Hierarchical Clustering to target specific group of customers to generate profitable revenue Using NLP to sorting the email to automatically updating the records in Customer Relationship management CRM We can run natural language processing algorithms against the data and automatically extract the features or risk factors from the notes in the medical record Performed Multinomial Logistic Regression Random forest Decision Tree SVM and more machine learning algorithms Using graphical packages produced ROC Curve to visually represent True Positive Rate versus FalsePositive Rate Equally produced visualization of Precision Recall Curve for Area under the Curve Used Market Basket Analysis association rules analysis to identified patterns data quality issues and leveraged insights Addressed over fitting by implementing of the algorithm regularization methods like L2 and L1 Improved models accuracy by using Gradient Boosting technique like Light GBM and gained around 82 accuracy with Random Forest and 77 with Logistic Regression Used Kfold cross validation technique to increase the model performance and worked with hyper parameter tuning methods like Grid Search Worked with visualization tools like Tableau Cognos and Micro Strategy to create business reports for higher management and used Python visualization libraries like Seaborn Matplotlib and ggplot depending on business requirements Provided schedules status reports and issue resolutions to the Project team Business Users and Project Managers Data Analyst Data Scientist CMS Energy Jackson MI January 2016 to February 2017 CMS Energy is an energy company that is focused principally on utility operations I was responsible for building a new data science department with the help of other departments and I was able to learn how the business is operated and helped the company to grow and stay ahead of the competition By using machine learning we improvised the predictive algorithm for pricing strategy And we creating alerts that would notify customers of potential issues that their system has solely based on the data available to me Responsibilities Worked on Data Manipulation Visualization Machine Learning Python SQL NoSQL MongoDB Hadoop Performed Advanced SQL queries for script executions like Update Insert and Delete Used Expert level understanding of different databases in combinations for Data extraction and loading joining data extracted from different databases and loading to a specific database in SQL Programmed utilities in Python that uses packages like SciPy NumPy pandas stats model scikit learn XG boost matplotlib plotly NLTK seaborn bokeh Transformed the business requirements into analytical models designing algorithms building models developing data mining and reporting solutions that scales across massive volume of structured and unstructured data Have done Normalization Denormalization techniques for optimum performance in relational and dimensional database environments Worked on customer segmentation using an unsupervised learning technique clustering Implemented Classification using supervised learninglike Logistic Regression Decision trees KNN Naive Bayes Built models using Statistical techniques and Machine Learning classification models like XG Boost SVM and Random Forest Created various Proof of Concepts PoC and gap analysis and gathered necessary data for analysis from different sources prepared data for data exploration using data munging Used jupyter notebook for spark to make data manipulations Developed ETL processes for data conversions and construction of data warehouse using INFORMATICA Worked on Hadoop ecosystem components like Hadoop MapReduce HDFS HBase Hive Sqoop and Pig including their installation and configuration Handled importing data from various data sources performed transformations using Hive Map Reduce and loaded data into HDFS Used Hive to store the data and perform data cleaning steps for huge datasets Extracted data from source XML in HDFS preparing data for exploratory analysis using data munging Interacted with the other departments to understand and identify data needs and requirements and work with other members of the IT organization to deliver data visualization and reporting solutions to address those needs Used visualization tools like Tableau for the interactive graphs Used python libraries Matplotlib and Seaborn for creating dashboards Data Analyst Karvy Financial Services Limited November 2014 to December 2015 Karvy Financial Services Limited is a company which has been playing a very proactive role in the economic growth of India by providing loans to Micro Small Business segments and individuals like credit for the requirements of different sectors of economy Industries exports trading agriculture infrastructure and the individual segments We worked on various projects which handle customer analytics Credit Risk analysis and assessing risks associated with loans like identify and prevent fraudulent loans identify and prevent fraud detection for transactions Responsibilities Compiled data from various sources public and private databases to perform complex analysis and data manipulation for actionable results Applied concepts of probability distribution and statistical inference on the given dataset to unearth interesting findings using comparison Ttest Ftest Rsquared Pvalue etc Applied linear regression multiple regressions ordinary least square method meanvariance the theory of large numbers logistic regression dummy variable residuals Poisson distribution Naive Bayes fitting function etc to data with help of Scikit SciPy NumPy and Pandas module of Python Applied Principal Component Analysis PCA based unsupervised technique to determine unusual VPN logon time Performed Clustering with historical demographic and behavioral data as features to implement the personalized marketing to the customers Also created classification model using Logistic Regression Random Forests to classify dependent variable into two classes which are risky and okay Used FScore Precision recall evaluating model performance Built user behavior models for finding activity patterns and evaluating risk scores for every transaction using historic data to train the supervised learning models such as Decision trees Random Forests and SVM Real time analysis of customers financial profile and providing recommendation for financial products best suited Collected historical data and thirdparty data from different data sources and performed data integration using Alteryx Forecasted demand for loans and interest rates using Time Series analysis like ARIMAX VARMAX and HoltWinters Obtained better predictive performance of 81 accuracy using ensemble methods like Bootstrap aggregation Bagging and Boosting Light GBM Gradient Tested complex ETL mappings and sessions based on business user requirements and business rules to load data from source flat files and RDBMS tables to target tables Developed visualizations and dashboards using ggplot Tableau Prepared and presented data quality report to stakeholders to give understanding of data Python Developer Data Analyst Symbiosys Technologies Visakhapatnam Andhra Pradesh January 2014 to October 2014 Genius Brands International is our client and we performed exploratory data analysis on corporate purchase orders contracts and projects data using sampling and statistical methods Identified strata improved precision and accuracy Works with other team members including DBAs Other ETL developers Technical Architects QA and Business Analysts Project Managers Responsibilities The work will involve the development of workflows triggered by events from other systems Develop easy to use documentation for the frameworks and tools developed for adaption by other teams Applied kmeans and hierarchical clustering on the data Identified and analyzed business insights Developed Hive UDFs and Pig UDFs using Python in Microsoft HDInsight environment Implemented endtoend systems for Data Analytics Data Automation and customized visualization tools using Python R Hadoop and MongoDB Used pandas NumPy seaborn SciPy matplotlib scikitlearn in Python for developing various machine learning algorithms Performed data profiling to merge the data from multiple data sources Worked on csv json excel different types of files for the data cleaning and data analysis Used Python for statistical operations on the data and ggplot2 for the visualizing the data Participated in feature engineering such as feature intersection generating for adding potential powerful features plotting feature correlation matrix for feature selection and reducing feature normalization for ease to implement machine algorithms Principal Component Analysis PCA for dimensionality reduction and label encoding with Scikitlearn preprocessing Worked with several use cases like campaign sales analysis forecasting sales KPI analysis and NLP models Worked with Clustering algorithms to target specific group of customers to generate profitable revenue Worked with word embedding techniques like Word2Vec GloVe for sentiment analysis and text classifications Worked with text to vector representation methods including Counter Vectorizer Tfidf and Latent Dirichlet Allocation LDA for topic modeling Performed time series analysis using Tableau Developed and executed Ad hoc reportings according to the business needs Managed offshore projects and coordinated work for 24 hour productivity cycle ETL Developer Sutherland Global Services Hyderabad Telangana February 2013 to December 2013 Sutherland builds processes for the digital age by combining the speed and insight of design thinking with the scale and accuracy of data analytics Sutherland has customers across industries like financial services to Healthcare My role is to assist Analytics department for the data extraction and cleaning as a data preprocessing steps to build models Responsibilities Involved with Business Analysts team in requirements gathering and in preparing functional specifications and changing them into technical specifications Involved in Data mapping specifications to create and execute detailed system test plans The data mapping specifies what data will be extracted from an internal data warehouse transformed and sent to an external entity Managed full SDLC processes involving requirements management workflow analysis source data analysis data mapping metadata management data quality testing strategy and maintenance of the model Involved in extensive DATA validation by writing several complex SQL queries and Involved in backend testing and worked with data quality issues Designed SSIS packages to extract transform and load existing data into SQL Server used lots of components of SSIS such as Pivot Transformation Fuzzy Lookup Merge Merge Join Data Conversion Row Count Sort Derived Columns Conditional Split Execute SQL Task Data Flow Task and Execute Package Task Created SSIS Packages that involved dealing with different source formats flat files Excel XML OLE DB and different destination formats Debugged and troubleshot the ETL packages by using a breakpoint analyzing the process catching error information by SQL command in SSIS Developed SQL queries in SQL Server management studio Toad and generated complex reports forth end users Automated and scheduled recurring reporting processes using UNIX shell scripting and Teradata utilities such as MLOAD BTEQ and Fast Load Experience with Perl Performed data analysis and data profiling using complex SQL on various sources systems including Oracle and Teradata Education Master of Science in Information Technology Management Campbellsville University Bachelor of Technology in Electronics and Communication Engineering Jawaharlal Nehru Technological University Kakinada Andhra Pradesh",
    "unique_id": "5af0ee04-898c-497c-81f0-f30ba139bf31"
}