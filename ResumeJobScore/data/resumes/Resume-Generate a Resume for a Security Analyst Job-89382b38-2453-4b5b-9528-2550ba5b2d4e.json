{
    "clean_data": "Sr Hadoop Developer Sr Hadoop span lDeveloperspan Sr Hadoop Developer Grace Note Emeryville CA Authorized to work in the US for any employer Work Experience Sr Hadoop Developer Grace Note Emeryville CA September 2018 to Present Grace Notes video business has several different products serving our core video guide business each with their own unique format and delivery mechanism There is strong demand from our customers to support a consistent data schema standardized output format and modern data delivery across the different regions in more modern scalable and dynamic ways Responsibilities Multiple Spark Jobs were written to perform Data Quality checks on data before files were moved to Data Processing Layer Implemented Spark using Scala and Spark SQL for faster testing and processing of data Designed and Modified Database tables and used HBASE Queries to insert and fetch data from tables Involved in moving all log files generated from various sources to HDFS for further processing through Flume170 Involved in deploying the applications in AWSand maintains the EC2 Elastic Computing Cloud and RDS Relational Database Services in amazon web services Implemented the file validation framework UDFs UDTFs and DAOs Strong experienced in working with UNIXLINUX environments writing Unix shell scripts Python and Perl Build REST web service by building Nodejs Server in the backend to handle requests sent from the frontend JQuery Ajax calls Importing and exporting data from different databases like MySQL RDBMS into HDFS and HBASE using Sqoop Involved in creating Hive tables loading with data and writing hive queries Model and Create the consolidated Cassandra FiloDB and Spark tables based on the data profiling Used OOZIE121Operational Services for batch processing and scheduling workflows dynamically and created UDFs to store specialized data structures in HBase and Cassandra Developed multiple MapReduce jobs in Java for data cleaning and preprocessing Used Impala to read write and query the Hadoop data in HDFS from Cassandra and configured Kafka to read and write messages from external programs Optimizing existing algorithms in Hadoop using Spark Context SparkSQL Data Frames and Pair RDDs Create a complete processing engine based on Cloudera distribution enhanced to performance Environment Hadoop HDFS MapReduce Yarn Hive Pig HBase Oozie Sqoop Kafka Flume Oracle 11g Core Java FiloDB Spark Scala Cloudera HDFS Eclipse Web Services SOAP WSDL Oozie Nodejs UnixLinux Aws JQuery Ajax Python Perl Zookeeper Hadoop Bigdata Developer Vanguard Charlotte NC April 2016 to August 2018 Vanguard is one of the worlds largest investment companies offering a large selection of lowcost mutual funds advice and related services This project designed by big data analytics technology platform which brought together data from various structured and unstructured data sources into Hadoop platform enabling the near realtime collection and analysis of customer data This solution offered efficient ways to draw new customers and improve their user experience Responsibilities Developed efficient MapReduce programs for filtering out the unstructured data and developed multiple MapReduce jobs to perform datacleaning and preprocessing on Hortonworks Implemented Data Interface to get information of customers using RestAPIand PreProcessdata using MapReduce 20 and store into HDFS Hortonworks Extracted files from MySQL Oracle and Teradata 2 through Sqoop 146and placed in HDFS Cloudera Distribution and processed Worked with various HDFS file formats like Avro176 Sequence File Jsonandvarious compression formats like Snappy bzip2 Proficient in designing Row keys and Schema Design for NoSQL DatabaseHbaseand knowledge of other NOSQL database Cassandra Used Hive to perform data validation on the data ingested using scoop and flume and the cleansed data set is pushed intoHbase Developed the Pig 0150UDFs to preprocess the data for analysis and Migrated ETL operations into Hadoopsystem using Pig Latin scripts and Python Scripts351 Used Pig as ETL tool to do transformations event joins filtering and some preaggregations before storing the data into HDFS Troubleshooting debugging altering Talend issues while maintaining the health and performance of theETLenvironment Loaded data into the cluster from dynamically generated files usingFlume and from relationaldatabase management systems using Sqoop Used spark to parse XML files and extract values from tags and load it into multiple hive tables Experienced in runningHadoop streaming jobs to process terabytes of formatted data usingPythonscripts Developed small distributed applications in our projects using Zookeeper347and scheduled the workflows using Oozie 420 Proficiency in writing the UnixLinux shell commands Developed a SCP Stimulator which emulates the behavior of intelligent networking and Interacts with SSF Environment Hadoop HDFS MapReduce Yarn Hive Pig HBase Oozie Sqoop Kafka Flume Oracle 11g Core Java Spark Scala Cloudera HDFS EclipseOozie Nodejs UnixLinux Aws JQueryAjax Python Perl Zookeeper Hadoop Bigdata Developer Middle by Corp Elgin IL January 2014 to March 2016 The Middle by Corporation through its subsidiaries engages in the design manufacture marketing distribution and service of commercial foodservice and food processing equipment in the United States Canada Asia Europe the Middle East and Latin America Responsibilities Developed multiple MapReduce jobs in java for data cleaning and preprocessing Performed Map Reduce Programs those are running on the cluster Involved in loading data from RDBMS and web logs into HDFS using Sqoop and Flume Worked on loading the data from MySQL to HBase where necessary using Sqoop Configured Hadoop cluster with Namenode and slaves and formatted HDFS Performed Importing and exporting data from Oracle to HDFS and Hive using Sqoop Performed source data ingestion cleansing and transformation in Hadoop Supported MapReduce Programs running on the cluster Wrote Pig Scripts to perform ETL procedures on the data in HDFS Used Oozie workflow engine to run multiple Hive and Pig jobs Analyzed the partitioned and bucketed data and compute various metrics for reporting Created HBase tables to store various data formats of data coming from different portfolios Worked on improving the performance of existing Pig and Hive Queries Involved in developing HiveUDFs and reused in some other requirements Worked on performing Join operations Developed fingerprinting rules on HIVE which help in uniquely identifying a driver profile Evaluated business requirements and prepared detailed specifications that follow project guidelines required to develop written programs Exported the result set from Hive to MySQL using Sqoop after processing the data Analyzed the data by performing Hive queries and running Pig scripts to study customer behaviour Used Hive to partition and bucket data Environment Hadoop MapReduce HDFS HBase HDP Horton Sqoop Data Processing Layer HUE AZURE Erwin MS Visio Tableau SQL MongoDB Oozie UNIX MySQL RDBMS Ambari Solr Cloud Lily HBaseCron JavaHadoop Developer Fiserv Noida Uttar Pradesh January 2012 to December 2013 ACUMEN is software product that provides solution to credit unionsThis project was based on agile methodology Implemented using core java Oracle DWR and spring technology Responsibilities Worked on AGITAR tool which is Junit generating software to increase the code coverage Code coverage was major quality issue faced in Acumen at that time It was a critical short term project Analyze the generated Junit and add proper asserts and make it more code specific along with increasing the code coverage This helped to boast my product knowledge as well as my Junit writing skills Which Improved Code Quality to a commendable level Joined EFT team in ACUMEN This team basically dealt with the Electronic Fund Transfer ATM Credit Cards and online banking Explored almost all the areas of EFT Learned DWR Worked with various challenging aspects like JPOS for ATM and online banking Various logger applications for the cards Worked on all the layers of the product enhancing knowledge on Core Java Domain knowledge gain was tremendous in this assignment Environment Core Java Oracle DWR spring MVC Agitar Tomcat Glassfish ClearCase JIRA Java Developer Xoriant Mumbai Maharashtra July 2011 to December 2012 Get Insured as the name suggest was an insurance domain projectIt basically dealt with contracts with various State of US under Obama Health care for providing online Insurance to the user This included showing various plans to the user according to their wages need age health existing illness and other such parameters This health insurance plans falls in various categories of health care like children must have a dental plan So if a person has a family member less than 18 he is shown all the dental plans Responsibilities Worked on one of the most critical module for project right from the beginning phase which included requirement gathering analysis design review and development Module lead located to another location had KT from him about roughly 2 weeks Lead was absorbed by client Took initiative in building a new team of more than 6 members with proper knowledge transfer sessions assigning and managing tasks with JIRA Learned Backbone JS and worked with UI team on UI enhancements Actively participating in the daily Scrums understanding new user stories Implementing new requirements after discussion with Scrum masters Working with BAQA to identify and fix bugs raise new feature and enhancements Was greatly appreciated by client with appreciation certificate and client bonus of 10k and 50k respectively Environment JavaJ2EE spring MVC Hibernate Oracle Backbonejs HTML Tomcat WebSphere SVN JIRA Education Bachelors Skills APACHE CASSANDRA CASSANDRA HDFS IMPALA MAPREDUCE OOZIE SQOOP HBASE KAFKA ETL FLUME HADOOP INFORMATICA MONGODB NOSQL Avro Hadoop HBase Hive HTML Additional Information Technical Skills Programming Languages Java J2EE C SQLPLSQL PIG LATIN Scala HTML XML Hadoop HDFS MapReduce HBase Hive Pig Impala SQOOP Flume OOZIE Spark SparkQL and Zookeeper AWS Cloudera Hortonworks Kafka Avro Web Technologies JDBC JSP JavaScript AJAX SOAP Scripting Languages Java Script Pig Latin Python 27and Scala RDBMS Languages Oracle Microsoft SQL Server MYSQL NoSQL MongoDB HBase Apache Cassandra FiloDB SOA Web Services SOAP WSDL IDES MyEclipse Eclipse and RAD Operating System Linux Windows UNIX CentOS Methodologies Agile Waterfall model ETL Tools Talend Informatica Testing Hadoop MR UNIT Testing Quality Center Hive Testing Other Tools SVN Apache Ant Junit and Star UML TOAD PlSQL Developer JIRA Visual Source QC Agile Methodology",
    "entities": [
        "NOSQL Avro Hadoop HBase Hive HTML Additional Information Technical Skills Programming Languages",
        "Used OOZIE121Operational Services",
        "Zookeeper AWS",
        "Spark Context",
        "HTML XML Hadoop",
        "ETL",
        "Environment Hadoop HDFS MapReduce Yarn Hive Pig HBase Oozie",
        "US",
        "DWR",
        "Sqoop",
        "HIVE",
        "Cassandra FiloDB",
        "Namenode",
        "SSF Environment Hadoop HDFS MapReduce Yarn Hive Pig HBase Oozie",
        "Hadoop Supported MapReduce Programs",
        "Work Experience Sr Hadoop Developer",
        "UNIXLINUX",
        "KT",
        "Impala",
        "Obama Health",
        "PreProcessdata",
        "Acumen",
        "EFT",
        "UI",
        "the Electronic Fund Transfer",
        "OOZIE Spark",
        "Which Improved Code Quality",
        "Microsoft",
        "Data Processing Layer Implemented Spark",
        "Star UML TOAD PlSQL",
        "Snappy bzip2 Proficient",
        "Core Java FiloDB Spark",
        "Corp Elgin",
        "Migrated ETL",
        "Hortonworks Implemented Data Interface",
        "java",
        "Created HBase",
        "EFT Learned DWR Worked",
        "AWSand",
        "MVC",
        "ETL Tools Talend Informatica Testing Hadoop MR",
        "Hadoop",
        "XML",
        "Oracle DWR",
        "MapReduce",
        "NOSQL",
        "Data Quality",
        "Nodejs Server",
        "Talend",
        "RDBMS",
        "Flume170 Involved",
        "Horton Sqoop Data Processing Layer HUE",
        "BAQA",
        "NoSQL",
        "The Middle by Corporation",
        "State",
        "RDS Relational Database Services",
        "the United States",
        "Responsibilities Multiple Spark Jobs",
        "HBase",
        "Module",
        "KAFKA",
        "Sr Hadoop Developer Sr Hadoop",
        "Hive",
        "the EC2 Elastic Computing Cloud",
        "UnixLinux",
        "Spark",
        "ATM"
    ],
    "experience": "Experience Sr Hadoop Developer Grace Note Emeryville CA September 2018 to Present Grace Notes video business has several different products serving our core video guide business each with their own unique format and delivery mechanism There is strong demand from our customers to support a consistent data schema standardized output format and modern data delivery across the different regions in more modern scalable and dynamic ways Responsibilities Multiple Spark Jobs were written to perform Data Quality checks on data before files were moved to Data Processing Layer Implemented Spark using Scala and Spark SQL for faster testing and processing of data Designed and Modified Database tables and used HBASE Queries to insert and fetch data from tables Involved in moving all log files generated from various sources to HDFS for further processing through Flume170 Involved in deploying the applications in AWSand maintains the EC2 Elastic Computing Cloud and RDS Relational Database Services in amazon web services Implemented the file validation framework UDFs UDTFs and DAOs Strong experienced in working with UNIXLINUX environments writing Unix shell scripts Python and Perl Build REST web service by building Nodejs Server in the backend to handle requests sent from the frontend JQuery Ajax calls Importing and exporting data from different databases like MySQL RDBMS into HDFS and HBASE using Sqoop Involved in creating Hive tables loading with data and writing hive queries Model and Create the consolidated Cassandra FiloDB and Spark tables based on the data profiling Used OOZIE121Operational Services for batch processing and scheduling workflows dynamically and created UDFs to store specialized data structures in HBase and Cassandra Developed multiple MapReduce jobs in Java for data cleaning and preprocessing Used Impala to read write and query the Hadoop data in HDFS from Cassandra and configured Kafka to read and write messages from external programs Optimizing existing algorithms in Hadoop using Spark Context SparkSQL Data Frames and Pair RDDs Create a complete processing engine based on Cloudera distribution enhanced to performance Environment Hadoop HDFS MapReduce Yarn Hive Pig HBase Oozie Sqoop Kafka Flume Oracle 11 g Core Java FiloDB Spark Scala Cloudera HDFS Eclipse Web Services SOAP WSDL Oozie Nodejs UnixLinux Aws JQuery Ajax Python Perl Zookeeper Hadoop Bigdata Developer Vanguard Charlotte NC April 2016 to August 2018 Vanguard is one of the worlds largest investment companies offering a large selection of lowcost mutual funds advice and related services This project designed by big data analytics technology platform which brought together data from various structured and unstructured data sources into Hadoop platform enabling the near realtime collection and analysis of customer data This solution offered efficient ways to draw new customers and improve their user experience Responsibilities Developed efficient MapReduce programs for filtering out the unstructured data and developed multiple MapReduce jobs to perform datacleaning and preprocessing on Hortonworks Implemented Data Interface to get information of customers using RestAPIand PreProcessdata using MapReduce 20 and store into HDFS Hortonworks Extracted files from MySQL Oracle and Teradata 2 through Sqoop 146and placed in HDFS Cloudera Distribution and processed Worked with various HDFS file formats like Avro176 Sequence File Jsonandvarious compression formats like Snappy bzip2 Proficient in designing Row keys and Schema Design for NoSQL DatabaseHbaseand knowledge of other NOSQL database Cassandra Used Hive to perform data validation on the data ingested using scoop and flume and the cleansed data set is pushed intoHbase Developed the Pig 0150UDFs to preprocess the data for analysis and Migrated ETL operations into Hadoopsystem using Pig Latin scripts and Python Scripts351 Used Pig as ETL tool to do transformations event joins filtering and some preaggregations before storing the data into HDFS Troubleshooting debugging altering Talend issues while maintaining the health and performance of theETLenvironment Loaded data into the cluster from dynamically generated files usingFlume and from relationaldatabase management systems using Sqoop Used spark to parse XML files and extract values from tags and load it into multiple hive tables Experienced in runningHadoop streaming jobs to process terabytes of formatted data usingPythonscripts Developed small distributed applications in our projects using Zookeeper347and scheduled the workflows using Oozie 420 Proficiency in writing the UnixLinux shell commands Developed a SCP Stimulator which emulates the behavior of intelligent networking and Interacts with SSF Environment Hadoop HDFS MapReduce Yarn Hive Pig HBase Oozie Sqoop Kafka Flume Oracle 11 g Core Java Spark Scala Cloudera HDFS EclipseOozie Nodejs UnixLinux Aws JQueryAjax Python Perl Zookeeper Hadoop Bigdata Developer Middle by Corp Elgin IL January 2014 to March 2016 The Middle by Corporation through its subsidiaries engages in the design manufacture marketing distribution and service of commercial foodservice and food processing equipment in the United States Canada Asia Europe the Middle East and Latin America Responsibilities Developed multiple MapReduce jobs in java for data cleaning and preprocessing Performed Map Reduce Programs those are running on the cluster Involved in loading data from RDBMS and web logs into HDFS using Sqoop and Flume Worked on loading the data from MySQL to HBase where necessary using Sqoop Configured Hadoop cluster with Namenode and slaves and formatted HDFS Performed Importing and exporting data from Oracle to HDFS and Hive using Sqoop Performed source data ingestion cleansing and transformation in Hadoop Supported MapReduce Programs running on the cluster Wrote Pig Scripts to perform ETL procedures on the data in HDFS Used Oozie workflow engine to run multiple Hive and Pig jobs Analyzed the partitioned and bucketed data and compute various metrics for reporting Created HBase tables to store various data formats of data coming from different portfolios Worked on improving the performance of existing Pig and Hive Queries Involved in developing HiveUDFs and reused in some other requirements Worked on performing Join operations Developed fingerprinting rules on HIVE which help in uniquely identifying a driver profile Evaluated business requirements and prepared detailed specifications that follow project guidelines required to develop written programs Exported the result set from Hive to MySQL using Sqoop after processing the data Analyzed the data by performing Hive queries and running Pig scripts to study customer behaviour Used Hive to partition and bucket data Environment Hadoop MapReduce HDFS HBase HDP Horton Sqoop Data Processing Layer HUE AZURE Erwin MS Visio Tableau SQL MongoDB Oozie UNIX MySQL RDBMS Ambari Solr Cloud Lily HBaseCron JavaHadoop Developer Fiserv Noida Uttar Pradesh January 2012 to December 2013 ACUMEN is software product that provides solution to credit unionsThis project was based on agile methodology Implemented using core java Oracle DWR and spring technology Responsibilities Worked on AGITAR tool which is Junit generating software to increase the code coverage Code coverage was major quality issue faced in Acumen at that time It was a critical short term project Analyze the generated Junit and add proper asserts and make it more code specific along with increasing the code coverage This helped to boast my product knowledge as well as my Junit writing skills Which Improved Code Quality to a commendable level Joined EFT team in ACUMEN This team basically dealt with the Electronic Fund Transfer ATM Credit Cards and online banking Explored almost all the areas of EFT Learned DWR Worked with various challenging aspects like JPOS for ATM and online banking Various logger applications for the cards Worked on all the layers of the product enhancing knowledge on Core Java Domain knowledge gain was tremendous in this assignment Environment Core Java Oracle DWR spring MVC Agitar Tomcat Glassfish ClearCase JIRA Java Developer Xoriant Mumbai Maharashtra July 2011 to December 2012 Get Insured as the name suggest was an insurance domain projectIt basically dealt with contracts with various State of US under Obama Health care for providing online Insurance to the user This included showing various plans to the user according to their wages need age health existing illness and other such parameters This health insurance plans falls in various categories of health care like children must have a dental plan So if a person has a family member less than 18 he is shown all the dental plans Responsibilities Worked on one of the most critical module for project right from the beginning phase which included requirement gathering analysis design review and development Module lead located to another location had KT from him about roughly 2 weeks Lead was absorbed by client Took initiative in building a new team of more than 6 members with proper knowledge transfer sessions assigning and managing tasks with JIRA Learned Backbone JS and worked with UI team on UI enhancements Actively participating in the daily Scrums understanding new user stories Implementing new requirements after discussion with Scrum masters Working with BAQA to identify and fix bugs raise new feature and enhancements Was greatly appreciated by client with appreciation certificate and client bonus of 10k and 50k respectively Environment JavaJ2EE spring MVC Hibernate Oracle Backbonejs HTML Tomcat WebSphere SVN JIRA Education Bachelors Skills APACHE CASSANDRA CASSANDRA HDFS IMPALA MAPREDUCE OOZIE SQOOP HBASE KAFKA ETL FLUME HADOOP INFORMATICA MONGODB NOSQL Avro Hadoop HBase Hive HTML Additional Information Technical Skills Programming Languages Java J2EE C SQLPLSQL PIG LATIN Scala HTML XML Hadoop HDFS MapReduce HBase Hive Pig Impala SQOOP Flume OOZIE Spark SparkQL and Zookeeper AWS Cloudera Hortonworks Kafka Avro Web Technologies JDBC JSP JavaScript AJAX SOAP Scripting Languages Java Script Pig Latin Python 27and Scala RDBMS Languages Oracle Microsoft SQL Server MYSQL NoSQL MongoDB HBase Apache Cassandra FiloDB SOA Web Services SOAP WSDL IDES MyEclipse Eclipse and RAD Operating System Linux Windows UNIX CentOS Methodologies Agile Waterfall model ETL Tools Talend Informatica Testing Hadoop MR UNIT Testing Quality Center Hive Testing Other Tools SVN Apache Ant Junit and Star UML TOAD PlSQL Developer JIRA Visual Source QC Agile Methodology",
    "extracted_keywords": [
        "Sr",
        "Hadoop",
        "Developer",
        "Sr",
        "Hadoop",
        "span",
        "lDeveloperspan",
        "Sr",
        "Hadoop",
        "Developer",
        "Grace",
        "Note",
        "Emeryville",
        "CA",
        "Authorized",
        "US",
        "employer",
        "Work",
        "Experience",
        "Sr",
        "Hadoop",
        "Developer",
        "Grace",
        "Note",
        "Emeryville",
        "CA",
        "September",
        "Present",
        "Grace",
        "Notes",
        "video",
        "business",
        "products",
        "video",
        "guide",
        "business",
        "format",
        "delivery",
        "mechanism",
        "demand",
        "customers",
        "data",
        "schema",
        "output",
        "format",
        "data",
        "delivery",
        "regions",
        "ways",
        "Responsibilities",
        "Multiple",
        "Spark",
        "Jobs",
        "Data",
        "Quality",
        "checks",
        "data",
        "files",
        "Data",
        "Processing",
        "Layer",
        "Spark",
        "Scala",
        "Spark",
        "SQL",
        "testing",
        "processing",
        "data",
        "Modified",
        "Database",
        "tables",
        "HBASE",
        "Queries",
        "data",
        "tables",
        "log",
        "files",
        "sources",
        "HDFS",
        "processing",
        "Flume170",
        "applications",
        "AWSand",
        "EC2",
        "Elastic",
        "Computing",
        "Cloud",
        "RDS",
        "Relational",
        "Database",
        "Services",
        "amazon",
        "web",
        "services",
        "file",
        "validation",
        "framework",
        "UDFs",
        "UDTFs",
        "DAOs",
        "Strong",
        "UNIXLINUX",
        "environments",
        "Unix",
        "shell",
        "scripts",
        "Python",
        "Perl",
        "Build",
        "REST",
        "web",
        "service",
        "Nodejs",
        "Server",
        "backend",
        "requests",
        "frontend",
        "JQuery",
        "Ajax",
        "data",
        "databases",
        "MySQL",
        "RDBMS",
        "HDFS",
        "HBASE",
        "Sqoop",
        "Hive",
        "tables",
        "data",
        "hive",
        "queries",
        "Model",
        "Cassandra",
        "FiloDB",
        "Spark",
        "tables",
        "data",
        "profiling",
        "OOZIE121Operational",
        "Services",
        "batch",
        "processing",
        "scheduling",
        "workflows",
        "UDFs",
        "data",
        "structures",
        "HBase",
        "Cassandra",
        "Developed",
        "MapReduce",
        "jobs",
        "Java",
        "data",
        "Impala",
        "Hadoop",
        "data",
        "HDFS",
        "Cassandra",
        "Kafka",
        "messages",
        "programs",
        "algorithms",
        "Hadoop",
        "Spark",
        "Context",
        "SparkSQL",
        "Data",
        "Frames",
        "Pair",
        "RDDs",
        "processing",
        "engine",
        "Cloudera",
        "distribution",
        "performance",
        "Environment",
        "Hadoop",
        "HDFS",
        "MapReduce",
        "Yarn",
        "Hive",
        "Pig",
        "HBase",
        "Oozie",
        "Sqoop",
        "Kafka",
        "Flume",
        "Oracle",
        "g",
        "Core",
        "Java",
        "FiloDB",
        "Spark",
        "Scala",
        "Cloudera",
        "HDFS",
        "Eclipse",
        "Web",
        "Services",
        "SOAP",
        "WSDL",
        "Oozie",
        "Nodejs",
        "UnixLinux",
        "Aws",
        "JQuery",
        "Ajax",
        "Python",
        "Perl",
        "Zookeeper",
        "Hadoop",
        "Bigdata",
        "Developer",
        "Vanguard",
        "Charlotte",
        "NC",
        "April",
        "August",
        "Vanguard",
        "worlds",
        "investment",
        "companies",
        "selection",
        "funds",
        "advice",
        "services",
        "project",
        "data",
        "analytics",
        "technology",
        "platform",
        "data",
        "data",
        "sources",
        "Hadoop",
        "platform",
        "collection",
        "analysis",
        "customer",
        "data",
        "solution",
        "ways",
        "customers",
        "user",
        "experience",
        "Responsibilities",
        "MapReduce",
        "programs",
        "data",
        "MapReduce",
        "jobs",
        "Hortonworks",
        "Implemented",
        "Data",
        "Interface",
        "information",
        "customers",
        "PreProcessdata",
        "MapReduce",
        "store",
        "HDFS",
        "Hortonworks",
        "files",
        "MySQL",
        "Oracle",
        "Teradata",
        "Sqoop",
        "HDFS",
        "Cloudera",
        "Distribution",
        "Worked",
        "HDFS",
        "file",
        "formats",
        "Avro176",
        "Sequence",
        "File",
        "compression",
        "formats",
        "bzip2",
        "Row",
        "keys",
        "Schema",
        "Design",
        "NoSQL",
        "DatabaseHbaseand",
        "knowledge",
        "NOSQL",
        "database",
        "Cassandra",
        "Hive",
        "data",
        "validation",
        "data",
        "scoop",
        "flume",
        "data",
        "set",
        "intoHbase",
        "Pig",
        "0150UDFs",
        "data",
        "analysis",
        "ETL",
        "operations",
        "Hadoopsystem",
        "Pig",
        "Latin",
        "scripts",
        "Python",
        "Scripts351",
        "Pig",
        "ETL",
        "tool",
        "transformations",
        "event",
        "preaggregations",
        "data",
        "HDFS",
        "Troubleshooting",
        "debugging",
        "Talend",
        "issues",
        "health",
        "performance",
        "theETLenvironment",
        "data",
        "cluster",
        "files",
        "relationaldatabase",
        "management",
        "systems",
        "Sqoop",
        "spark",
        "XML",
        "files",
        "values",
        "tags",
        "tables",
        "runningHadoop",
        "streaming",
        "jobs",
        "terabytes",
        "data",
        "usingPythonscripts",
        "applications",
        "projects",
        "Zookeeper347and",
        "workflows",
        "Oozie",
        "Proficiency",
        "UnixLinux",
        "shell",
        "SCP",
        "Stimulator",
        "behavior",
        "networking",
        "Interacts",
        "SSF",
        "Environment",
        "Hadoop",
        "HDFS",
        "MapReduce",
        "Yarn",
        "Hive",
        "Pig",
        "HBase",
        "Oozie",
        "Sqoop",
        "Kafka",
        "Flume",
        "Oracle",
        "g",
        "Core",
        "Java",
        "Spark",
        "Scala",
        "Cloudera",
        "HDFS",
        "EclipseOozie",
        "Nodejs",
        "UnixLinux",
        "Aws",
        "JQueryAjax",
        "Python",
        "Perl",
        "Zookeeper",
        "Hadoop",
        "Bigdata",
        "Developer",
        "Middle",
        "Corp",
        "Elgin",
        "IL",
        "January",
        "March",
        "Middle",
        "Corporation",
        "subsidiaries",
        "design",
        "manufacture",
        "marketing",
        "distribution",
        "service",
        "foodservice",
        "food",
        "processing",
        "equipment",
        "United",
        "States",
        "Canada",
        "Asia",
        "Europe",
        "Middle",
        "East",
        "Latin",
        "America",
        "Responsibilities",
        "MapReduce",
        "jobs",
        "java",
        "data",
        "Performed",
        "Map",
        "Programs",
        "cluster",
        "loading",
        "data",
        "RDBMS",
        "web",
        "logs",
        "HDFS",
        "Sqoop",
        "Flume",
        "data",
        "MySQL",
        "HBase",
        "Sqoop",
        "Configured",
        "Hadoop",
        "cluster",
        "Namenode",
        "slaves",
        "HDFS",
        "Performed",
        "Importing",
        "data",
        "Oracle",
        "HDFS",
        "Hive",
        "Sqoop",
        "Performed",
        "source",
        "data",
        "ingestion",
        "cleansing",
        "transformation",
        "Hadoop",
        "Supported",
        "MapReduce",
        "Programs",
        "cluster",
        "Wrote",
        "Pig",
        "Scripts",
        "ETL",
        "procedures",
        "data",
        "HDFS",
        "Oozie",
        "workflow",
        "engine",
        "Hive",
        "Pig",
        "jobs",
        "data",
        "metrics",
        "Created",
        "HBase",
        "data",
        "formats",
        "data",
        "portfolios",
        "performance",
        "Pig",
        "Hive",
        "Queries",
        "HiveUDFs",
        "requirements",
        "Join",
        "operations",
        "rules",
        "HIVE",
        "driver",
        "profile",
        "business",
        "requirements",
        "specifications",
        "project",
        "guidelines",
        "programs",
        "result",
        "Hive",
        "MySQL",
        "Sqoop",
        "data",
        "data",
        "Hive",
        "queries",
        "Pig",
        "scripts",
        "customer",
        "behaviour",
        "Hive",
        "partition",
        "bucket",
        "data",
        "Environment",
        "Hadoop",
        "MapReduce",
        "HDFS",
        "HBase",
        "HDP",
        "Horton",
        "Sqoop",
        "Data",
        "Processing",
        "Layer",
        "HUE",
        "AZURE",
        "Erwin",
        "MS",
        "Visio",
        "Tableau",
        "SQL",
        "MongoDB",
        "Oozie",
        "UNIX",
        "MySQL",
        "RDBMS",
        "Ambari",
        "Solr",
        "Cloud",
        "Lily",
        "HBaseCron",
        "JavaHadoop",
        "Developer",
        "Fiserv",
        "Noida",
        "Uttar",
        "Pradesh",
        "January",
        "December",
        "ACUMEN",
        "software",
        "product",
        "solution",
        "credit",
        "project",
        "methodology",
        "core",
        "java",
        "Oracle",
        "DWR",
        "spring",
        "technology",
        "Responsibilities",
        "tool",
        "Junit",
        "software",
        "code",
        "coverage",
        "Code",
        "coverage",
        "quality",
        "issue",
        "Acumen",
        "time",
        "term",
        "project",
        "Analyze",
        "Junit",
        "asserts",
        "code",
        "code",
        "coverage",
        "product",
        "knowledge",
        "Junit",
        "skills",
        "Code",
        "Quality",
        "level",
        "EFT",
        "team",
        "ACUMEN",
        "team",
        "Electronic",
        "Fund",
        "Transfer",
        "ATM",
        "Credit",
        "Cards",
        "banking",
        "areas",
        "EFT",
        "Learned",
        "DWR",
        "aspects",
        "JPOS",
        "ATM",
        "banking",
        "applications",
        "cards",
        "layers",
        "product",
        "knowledge",
        "Core",
        "Java",
        "Domain",
        "knowledge",
        "gain",
        "assignment",
        "Environment",
        "Core",
        "Java",
        "Oracle",
        "DWR",
        "spring",
        "MVC",
        "Agitar",
        "Tomcat",
        "Glassfish",
        "ClearCase",
        "JIRA",
        "Java",
        "Developer",
        "Xoriant",
        "Mumbai",
        "Maharashtra",
        "July",
        "December",
        "name",
        "insurance",
        "domain",
        "projectIt",
        "contracts",
        "State",
        "US",
        "Obama",
        "Health",
        "care",
        "Insurance",
        "user",
        "plans",
        "user",
        "wages",
        "age",
        "health",
        "illness",
        "parameters",
        "health",
        "insurance",
        "plans",
        "categories",
        "health",
        "care",
        "children",
        "plan",
        "person",
        "family",
        "member",
        "plans",
        "Responsibilities",
        "module",
        "project",
        "beginning",
        "phase",
        "requirement",
        "analysis",
        "design",
        "review",
        "development",
        "Module",
        "lead",
        "location",
        "KT",
        "weeks",
        "Lead",
        "client",
        "initiative",
        "team",
        "members",
        "knowledge",
        "transfer",
        "sessions",
        "tasks",
        "JIRA",
        "Backbone",
        "JS",
        "UI",
        "team",
        "UI",
        "enhancements",
        "Scrums",
        "user",
        "stories",
        "requirements",
        "discussion",
        "Scrum",
        "masters",
        "BAQA",
        "bugs",
        "feature",
        "enhancements",
        "client",
        "appreciation",
        "certificate",
        "client",
        "bonus",
        "10k",
        "50k",
        "Environment",
        "JavaJ2EE",
        "spring",
        "MVC",
        "Hibernate",
        "Oracle",
        "Backbonejs",
        "HTML",
        "Tomcat",
        "WebSphere",
        "SVN",
        "JIRA",
        "Education",
        "Bachelors",
        "Skills",
        "APACHE",
        "CASSANDRA",
        "CASSANDRA",
        "HDFS",
        "IMPALA",
        "MAPREDUCE",
        "OOZIE",
        "SQOOP",
        "HBASE",
        "KAFKA",
        "ETL",
        "FLUME",
        "HADOOP",
        "INFORMATICA",
        "NOSQL",
        "Avro",
        "Hadoop",
        "HBase",
        "Hive",
        "HTML",
        "Additional",
        "Information",
        "Technical",
        "Skills",
        "Programming",
        "Languages",
        "Java",
        "J2EE",
        "C",
        "SQLPLSQL",
        "PIG",
        "LATIN",
        "Scala",
        "HTML",
        "XML",
        "Hadoop",
        "HDFS",
        "MapReduce",
        "HBase",
        "Hive",
        "Pig",
        "Impala",
        "SQOOP",
        "Flume",
        "OOZIE",
        "Spark",
        "SparkQL",
        "Zookeeper",
        "AWS",
        "Cloudera",
        "Hortonworks",
        "Kafka",
        "Avro",
        "Web",
        "Technologies",
        "JDBC",
        "JSP",
        "JavaScript",
        "AJAX",
        "SOAP",
        "Scripting",
        "Languages",
        "Java",
        "Script",
        "Pig",
        "Latin",
        "Python",
        "27and",
        "Scala",
        "RDBMS",
        "Languages",
        "Oracle",
        "Microsoft",
        "SQL",
        "Server",
        "MYSQL",
        "NoSQL",
        "MongoDB",
        "HBase",
        "Apache",
        "Cassandra",
        "FiloDB",
        "SOA",
        "Web",
        "Services",
        "SOAP",
        "WSDL",
        "IDES",
        "MyEclipse",
        "Eclipse",
        "RAD",
        "Operating",
        "System",
        "Linux",
        "UNIX",
        "CentOS",
        "Methodologies",
        "Agile",
        "Waterfall",
        "model",
        "ETL",
        "Tools",
        "Talend",
        "Informatica",
        "Testing",
        "Hadoop",
        "MR",
        "UNIT",
        "Testing",
        "Quality",
        "Center",
        "Hive",
        "Testing",
        "Tools",
        "SVN",
        "Apache",
        "Ant",
        "Junit",
        "Star",
        "UML",
        "TOAD",
        "PlSQL",
        "Developer",
        "JIRA",
        "Visual",
        "Source",
        "QC",
        "Agile",
        "Methodology"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T21:24:18.381946",
    "resume_data": "Sr Hadoop Developer Sr Hadoop span lDeveloperspan Sr Hadoop Developer Grace Note Emeryville CA Authorized to work in the US for any employer Work Experience Sr Hadoop Developer Grace Note Emeryville CA September 2018 to Present Grace Notes video business has several different products serving our core video guide business each with their own unique format and delivery mechanism There is strong demand from our customers to support a consistent data schema standardized output format and modern data delivery across the different regions in more modern scalable and dynamic ways Responsibilities Multiple Spark Jobs were written to perform Data Quality checks on data before files were moved to Data Processing Layer Implemented Spark using Scala and Spark SQL for faster testing and processing of data Designed and Modified Database tables and used HBASE Queries to insert and fetch data from tables Involved in moving all log files generated from various sources to HDFS for further processing through Flume170 Involved in deploying the applications in AWSand maintains the EC2 Elastic Computing Cloud and RDS Relational Database Services in amazon web services Implemented the file validation framework UDFs UDTFs and DAOs Strong experienced in working with UNIXLINUX environments writing Unix shell scripts Python and Perl Build REST web service by building Nodejs Server in the backend to handle requests sent from the frontend JQuery Ajax calls Importing and exporting data from different databases like MySQL RDBMS into HDFS and HBASE using Sqoop Involved in creating Hive tables loading with data and writing hive queries Model and Create the consolidated Cassandra FiloDB and Spark tables based on the data profiling Used OOZIE121Operational Services for batch processing and scheduling workflows dynamically and created UDFs to store specialized data structures in HBase and Cassandra Developed multiple MapReduce jobs in Java for data cleaning and preprocessing Used Impala to read write and query the Hadoop data in HDFS from Cassandra and configured Kafka to read and write messages from external programs Optimizing existing algorithms in Hadoop using Spark Context SparkSQL Data Frames and Pair RDDs Create a complete processing engine based on Cloudera distribution enhanced to performance Environment Hadoop HDFS MapReduce Yarn Hive Pig HBase Oozie Sqoop Kafka Flume Oracle 11g Core Java FiloDB Spark Scala Cloudera HDFS Eclipse Web Services SOAP WSDL Oozie Nodejs UnixLinux Aws JQuery Ajax Python Perl Zookeeper Hadoop Bigdata Developer Vanguard Charlotte NC April 2016 to August 2018 Vanguard is one of the worlds largest investment companies offering a large selection of lowcost mutual funds advice and related services This project designed by big data analytics technology platform which brought together data from various structured and unstructured data sources into Hadoop platform enabling the near realtime collection and analysis of customer data This solution offered efficient ways to draw new customers and improve their user experience Responsibilities Developed efficient MapReduce programs for filtering out the unstructured data and developed multiple MapReduce jobs to perform datacleaning and preprocessing on Hortonworks Implemented Data Interface to get information of customers using RestAPIand PreProcessdata using MapReduce 20 and store into HDFS Hortonworks Extracted files from MySQL Oracle and Teradata 2 through Sqoop 146and placed in HDFS Cloudera Distribution and processed Worked with various HDFS file formats like Avro176 Sequence File Jsonandvarious compression formats like Snappy bzip2 Proficient in designing Row keys and Schema Design for NoSQL DatabaseHbaseand knowledge of other NOSQL database Cassandra Used Hive to perform data validation on the data ingested using scoop and flume and the cleansed data set is pushed intoHbase Developed the Pig 0150UDFs to preprocess the data for analysis and Migrated ETL operations into Hadoopsystem using Pig Latin scripts and Python Scripts351 Used Pig as ETL tool to do transformations event joins filtering and some preaggregations before storing the data into HDFS Troubleshooting debugging altering Talend issues while maintaining the health and performance of theETLenvironment Loaded data into the cluster from dynamically generated files usingFlume and from relationaldatabase management systems using Sqoop Used spark to parse XML files and extract values from tags and load it into multiple hive tables Experienced in runningHadoop streaming jobs to process terabytes of formatted data usingPythonscripts Developed small distributed applications in our projects using Zookeeper347and scheduled the workflows using Oozie 420 Proficiency in writing the UnixLinux shell commands Developed a SCP Stimulator which emulates the behavior of intelligent networking and Interacts with SSF Environment Hadoop HDFS MapReduce Yarn Hive Pig HBase Oozie Sqoop Kafka Flume Oracle 11g Core Java Spark Scala Cloudera HDFS EclipseOozie Nodejs UnixLinux Aws JQueryAjax Python Perl Zookeeper Hadoop Bigdata Developer Middle by Corp Elgin IL January 2014 to March 2016 The Middle by Corporation through its subsidiaries engages in the design manufacture marketing distribution and service of commercial foodservice and food processing equipment in the United States Canada Asia Europe the Middle East and Latin America Responsibilities Developed multiple MapReduce jobs in java for data cleaning and preprocessing Performed Map Reduce Programs those are running on the cluster Involved in loading data from RDBMS and web logs into HDFS using Sqoop and Flume Worked on loading the data from MySQL to HBase where necessary using Sqoop Configured Hadoop cluster with Namenode and slaves and formatted HDFS Performed Importing and exporting data from Oracle to HDFS and Hive using Sqoop Performed source data ingestion cleansing and transformation in Hadoop Supported MapReduce Programs running on the cluster Wrote Pig Scripts to perform ETL procedures on the data in HDFS Used Oozie workflow engine to run multiple Hive and Pig jobs Analyzed the partitioned and bucketed data and compute various metrics for reporting Created HBase tables to store various data formats of data coming from different portfolios Worked on improving the performance of existing Pig and Hive Queries Involved in developing HiveUDFs and reused in some other requirements Worked on performing Join operations Developed fingerprinting rules on HIVE which help in uniquely identifying a driver profile Evaluated business requirements and prepared detailed specifications that follow project guidelines required to develop written programs Exported the result set from Hive to MySQL using Sqoop after processing the data Analyzed the data by performing Hive queries and running Pig scripts to study customer behaviour Used Hive to partition and bucket data Environment Hadoop MapReduce HDFS HBase HDP Horton Sqoop Data Processing Layer HUE AZURE Erwin MS Visio Tableau SQL MongoDB Oozie UNIX MySQL RDBMS Ambari Solr Cloud Lily HBaseCron JavaHadoop Developer Fiserv Noida Uttar Pradesh January 2012 to December 2013 ACUMEN is software product that provides solution to credit unionsThis project was based on agile methodology Implemented using core java Oracle DWR and spring technology Responsibilities Worked on AGITAR tool which is Junit generating software to increase the code coverage Code coverage was major quality issue faced in Acumen at that time It was a critical short term project Analyze the generated Junit and add proper asserts and make it more code specific along with increasing the code coverage This helped to boast my product knowledge as well as my Junit writing skills Which Improved Code Quality to a commendable level Joined EFT team in ACUMEN This team basically dealt with the Electronic Fund Transfer ATM Credit Cards and online banking Explored almost all the areas of EFT Learned DWR Worked with various challenging aspects like JPOS for ATM and online banking Various logger applications for the cards Worked on all the layers of the product enhancing knowledge on Core Java Domain knowledge gain was tremendous in this assignment Environment Core Java Oracle DWR spring MVC Agitar Tomcat Glassfish ClearCase JIRA Java Developer Xoriant Mumbai Maharashtra July 2011 to December 2012 Get Insured as the name suggest was an insurance domain projectIt basically dealt with contracts with various State of US under Obama Health care for providing online Insurance to the user This included showing various plans to the user according to their wages need age health existing illness and other such parameters This health insurance plans falls in various categories of health care like children must have a dental plan So if a person has a family member less than 18 he is shown all the dental plans Responsibilities Worked on one of the most critical module for project right from the beginning phase which included requirement gathering analysis design review and development Module lead located to another location had KT from him about roughly 2 weeks Lead was absorbed by client Took initiative in building a new team of more than 6 members with proper knowledge transfer sessions assigning and managing tasks with JIRA Learned Backbone JS and worked with UI team on UI enhancements Actively participating in the daily Scrums understanding new user stories Implementing new requirements after discussion with Scrum masters Working with BAQA to identify and fix bugs raise new feature and enhancements Was greatly appreciated by client with appreciation certificate and client bonus of 10k and 50k respectively Environment JavaJ2EE spring MVC Hibernate Oracle Backbonejs HTML Tomcat WebSphere SVN JIRA Education Bachelors Skills APACHE CASSANDRA CASSANDRA HDFS IMPALA MAPREDUCE OOZIE SQOOP HBASE KAFKA ETL FLUME HADOOP INFORMATICA MONGODB NOSQL Avro Hadoop HBase Hive HTML Additional Information Technical Skills Programming Languages Java J2EE C SQLPLSQL PIG LATIN Scala HTML XML Hadoop HDFS MapReduce HBase Hive Pig Impala SQOOP Flume OOZIE Spark SparkQL and Zookeeper AWS Cloudera Hortonworks Kafka Avro Web Technologies JDBC JSP JavaScript AJAX SOAP Scripting Languages Java Script Pig Latin Python 27and Scala RDBMS Languages Oracle Microsoft SQL Server MYSQL NoSQL MongoDB HBase Apache Cassandra FiloDB SOA Web Services SOAP WSDL IDES MyEclipse Eclipse and RAD Operating System Linux Windows UNIX CentOS Methodologies Agile Waterfall model ETL Tools Talend Informatica Testing Hadoop MR UNIT Testing Quality Center Hive Testing Other Tools SVN Apache Ant Junit and Star UML TOAD PlSQL Developer JIRA Visual Source QC Agile Methodology",
    "unique_id": "89382b38-2453-4b5b-9528-2550ba5b2d4e"
}