{
    "clean_data": "Big dataHadoop Developer Big dataHadoop span lDeveloperspan Big dataHadoop Developer Bank of America Avenel NJ Above 8 years of experience in Analysis Design Development Testing Implementation Maintenance and Enhancements on various IT Projects and experience in Big Data in implementing endtoend Hadoop solutions Experience in working in environments using Agile SCRUM RUP and TestDriven development methodologies Good Knowledge in Amazon AWS concepts like EMR and EC2 web services which provides fast and efficient processing Extensive experience in installing configuring and using ecosystem components like Hadoop MapReduce HDFS Sqoop Pig Hive Impala Spark Expertise in using J2EE application servers such as IBM Web Sphere JBoss and web servers like Apache Tomcat Experience in different Hadoop distributions like Cloudera CDH3 CDH4 and Horton Works Distributions HDP Experience in analyzing data using HIVEQL PIG Latin and custom Map Reduce programs in JAVA Extending HIVE and PIG core functionality by using custom UDFs Experienced in configuring and administering the Hadoop Cluster using major Hadoop Distributions like Apache Hadoop and Cloudera Diverse experience utilizing Java tools in business Web and clientserver environments including Java Platform J2EE EJB JSP Java Servlets Struts and Java database Connectivity JDBC technologies Good understanding in integration of various data sources like RDBMS Spreadsheets Text files JSON and XML files Good working experience on using Sqoop to import data into HDFS from RDBMS and viceversa Implemented Service Oriented Architecture SOA using Web Services and JMS Java Messaging Service Implemented J2EE Design Patterns such as MVC Session Faade DAO DTO Singleton Pattern Front Controller and Business Delegate Experienced in developing web services with XML based protocols such as SOAP Axis UDDI and WSDL Experienced in MVC Model View Controller architecture and various J2EE design patterns like singleton and factory design patterns Extensive experience in loading and analyzing large datasets with Hadoop framework Map Reduce HDFS PIG HIVE Flume Sqoop SPARK Impala NoSQL databases like Mongo DB HBase Cassandra Solid understanding of Hadoop MRV1 and Hadoop MRV2 or YARN Architecture Hands on experience in configuring and administering the Hadoop Cluster using major Hadoop Distributions like Apache Hadoop and Cloudera Good knowledge in SQL and PLSQL to write Stored Procedures and Functions and writing unit test cases using JUnit Extensive experience in Extraction Transformation and Loading ETL of data from multiple sources into Data Warehouse and Data Mart Strong knowledge in Object oriented designanalysis UML modeling Classic design patterns and J2EE patterns Hands on experience working with databases like Oracle 12g SQL Server 2010 and MySQL Hands on experience on the entire latest UI stack including HTML CSS mobile friendly responsive design usercentric design etc Experience in developing webbased enterprise applications using Java J2EE Servlets JSP EJB JDBC Hibernate Spring IOC Spring AOP Spring MVC Spring Web Flow Spring Boot Spring Security Spring Batch Spring Integration Web Services SOAP and REST and ORM frameworks like Hibernate Strong knowledge on Hadoopeco systems including HDFS Hive Oozie HBase Pig Sqoop Zookeeper etc Extensive experience with advanced J2EE Frameworks such as spring Struts JSF and Hibernate Expertise in JavaScript JavaScriptMVC patterns ObjectOrientedJavaScriptDesign Patterns and AJAX calls Installation configuration and administration experience in Big Data platforms Cloudera Manager of Cloudera MCS of MapR Expertise in using XML related technologies such as XML DTD XSD XPATH XSLT DOM SAX JAXP JSON and JAXB Experience in using ANT and Maven for building and deploying the projects in servers and also using Junit and log4j for debugging Work Experience Big dataHadoop Developer Bank of America August 2018 to Present Description Bank of America Merrill Lynch is the marketing name for the global banking and global markets businesses of Bank of America Corporation Lending derivatives and other commercial banking activities are performed globally by banking affiliates of Bank of America Corporation including Bank of America NA and Member FDIC Responsibilities Responsible for installation and configuration of Hive Pig Hbase and Sqoop on the Hadoop cluster and created hive tables to store the processed results in a tabular format Configured Spark Streaming to receive real time data from the Apache Kafka and store the stream data to HDFS using Scala Developed the Sqoop scripts to make the interaction between Hive and vertica Database Processed data into HDFS by developing solutions and analyzed the data using Map Reduce PIG and Hive to produce summary results from Hadoop to downstream systems Build servers using AWS Importing volumes launching EC2 creating security groups autoscaling load balancers Route 53 SES and SNS in the defined virtual private connection Written Map Reduce code to process and parsing the data from various sources and storing parsed data into HBase and Hive using HBaseHive Integration Streamed AWS log group into Lambda function to create service now incident Involved in loading and transforming large sets of Structured SemiStructured and Unstructured data and analyzed them by running Hive queries and Pig scripts Created Managed tables and External tables in Hive and loaded data from HDFS Developed Spark code by using Scala and SparkSQL for faster processing and testing and performed complex HiveQL queries on Hive tables Scheduled several times based Oozie workflow by developing Python scripts Developed Pig Latin scripts using operators such as LOAD STORE DUMP FILTER DISTINCT FOREACH GENERATE GROUP COGROUP ORDER LIMIT UNION SPLIT to extract data from data files to load into HDFS Exporting the data using Sqoop to RDBMS servers and processed that data for ETL operations Worked on S3 buckets on AWS to store Cloud Formation Templates and worked on AWS to create EC2 instances Designing ETL Data Pipeline flow to ingest the data from RDBMS source to Hadoop using shell script sqoop package and MySQL Endtoend architecture and implementation of clientserver systems using Scala Akka Java JavaScript and related Linux Optimized the Hive tables using optimization techniques like partitions and bucketing to provide better Used Oozie workflow engine to manage interdependent Hadoop jobs and to automate several types of Hadoop jobs such as Java mapreduce Hive Pig and Sqoop Implementing Hadoop with the AWS EC2 system using a few instances in gathering and analyzing data log files Involved in Spark and Spark Streaming creating RDDs applying operations Transformation and Actions Created partitioned tables and loaded data using both static partition and dynamic partition method Developed custom Apache Spark programs in Scala to analyze and transform unstructured data Handled importing of data from various data sources performed transformations using Hive MapReduce loaded data into HDFS and Extracted the data from Oracle into HDFS using Sqoop Using Kafka on publishsubscribe messaging as a distributed commit log have experienced in its fast scalable and durability Test Driven Development TDD process and extensive experience with Agile and SCRUM programming methodology Implemented POC to migrate Map Reduce jobs into Spark RDD transformations using SCALA Scheduled map reduces jobs in production environment using Oozie scheduler Involved in Cluster maintenance Cluster Monitoring and Troubleshooting Manage and review data backups and log files Designed and implemented map reduce jobs to support distributed processing using java Hive and Apache Pig Analyzing Hadoop cluster and different Big Data analytic tools including Pig Hive HBase and Sqoop Improved the Performance by tuning of HIVE and map reduce Research evaluate and utilize modern technologiestoolsframeworks around Hadoop ecosystem Environment HDFS Map Reduce Hive Sqoop Pig Flume Vertica Oozie Scheduler Java Shell Scripts Teradata Oracle HBase MongoDB Cassandra Cloudera AWS JavaScript JSP Kafka Spark Scala and ETL Python Big dataHadoop Developer Sprint Overland Park KS May 2017 to July 2018 Description Sprint offers all the latest musthave smartphones and tablets from top industryleading manufacturers including Apple Samsung LG HTC exclusively offering the new EssentialPhone and much more Sprint is also your onestopshop for the coolest accessories from fashionforward designers and tech innovators Responsibilities Involved in Installing Configuring Hadoop EcoSystem Cloudera Manager using CDH3 CDH4 Distributions Responsible to manage data coming from different sources and involved in HDFS maintenance and loading of structured and unstructured data Imported data using Sqoop from Teradata using Teradata connector Created Data Pipeline of MapReduce programs using Chained Mappers Implemented Optimized join base by joining different data sets to get top claims based on state using Map Reduce Visualize the HDFS data to the customer using BI tool with the help of HiveODBCDriver Worked on POC of Talend integration with Hadoop where Created Talend Jobs to extract data from Hadoop Imported data using Sqoop to load data fromMySQL to HDFS on a regular basis Worked on social media Facebook Twitter etc data crawling using Java and R language and MongoDB for unstructured data storage Integrated Quartz scheduler with Oozieworkflows to get data from multiple data sources parallels using a fork Created Partitions Buckets based on State to further process using Bucket based Hive joins Experienced with different kind of compression techniques like LZO GZip Snappy Created Hive Generic UDFs to process business logic that varies based on policy Imported Relational Database data using Sqoop into Hive Dynamic partition tables using staging tables Worked on custom Pig Loaders and storage classes to work with a variety of data formats such as JSON and XML file formats Used Oozie workflow engine to manage interdependent Hadoop jobs and to automate several types of Hadoop jobs such as Java mapreduce Hive Pig and Sqoop Developed Unit test cases using JUnit Easy MockandMRUnit testing frameworks Experienced in MonitoringCluster using Clouderamanager Environment Hadoop HDFS HBase Spark MapReduce Tera Data MySQL Java Hive Pig Sqoop Flume Oozie SQL Cloudera Manager Hadoop Developer Center Light Health Bronx NY January 2016 to April 2017 Description CenterLight Healthcare a member of CenterLight Health System is a notforprofit New York State Managed Long Term Care organization Founded in 1985 to provide home healthcare services to the elderly disabled and chronically ill in the New York metropolitan area CenterLight Healthcare today is recognized as an innovator in the managed care field Responsibilities Provided application demo to the client by designing and developing a search engine report analysis trends application administration prototype screens using AngularJS and Bootstrap JS Took the ownership of complete application Design of Java part Hadoop integration Apart from the normal requirement gathering participated in a Business meeting with the client to gather security requirements Assisted with the architect to analyze the existing system and future system Prepared design blue pints and application flow documentation Experienced in managing and reviewing Hadoop log files Load and transform large sets of structured semistructured and unstructured data Responsible to manage data coming from different sources and application Supported Map Reduce Programs those are running on the cluster Responsible for working with Message broker system such as Kafka Extracted data from mainframes and feed to KAFKA and ingested to HBase to perform Analytics Written eventdriven link tracking system to capture user events and feed to KAFKA to push it to HBASE Created MapReduce jobs to extracts the contents from HBase and configured in OOZIE workflow to generate analytical reports Developed the JAX RS web services code using Apache CXF framework to fetch data from SOLR when the user performed the search for documents Participated in SOLR schema and ingested data into SOLR for data indexing Written MapReduce programs to organize the data and ingest the data to suitable for analytics in client specified format Hands on experience in writing python scripts to optimize the performance Implemented Storm builder topologies to perform cleansing operations before moving data into Cassandra Extracted files from Cassandra through Sqoop and placed in HDFS and processed Implemented Bloom filters in Cassandra using keyspace creation Involved in writing Cassandra CQL statements God handson experience in developing concurrency using spark and Cassandra together Involved in writing spark applications using Scala Hands on experience in creating RDDs transformationsand Actions while implementing spark applications Good knowledge in creating data frames using Spark SQL Involved in loading data into Cassandra NoSQL Database Implemented record level atomicity on writes using Cassandra Written PIG Scripts to query and process the Datasets to figure out the patterns of trends by applying clientspecific criteria and configured OOZIE workflows to run the jobs along with the MR jobs Stored the derived the results in HBasefrom analysis and make it available to data ingestion for SOLR for indexing data Involved in integration of java search UI SOLR and HDFS Involved in code deployments using continuous integration tool using Jenkins Documented all the challenges issues involved to deal with the security system and Implemented best practices Created Project structures and configurations according to the project architecture and made it available to the junior developer to continue their work Handled onsite coordinator role to deliver work to offshore Involved in core reviews and application lead supported activities Environment Cassandra Spring 32 Restful services using CXF web services framework spring data SOLR 521 PIG HIVE apache AVRO Map Reduce Sqoop Zookeeper SVN Jenkins windows AD windows KDC Hortonworks distribution of Hadoop 23 YARN Ambari Hadoop developer Engage Point Calverton MD March 2014 to December 2015 Description Engage Point chosen as Agile Development Prequalified Vendor by California Health and Human Services Agency Responsibilities Collected and aggregated large amounts of weblog data from different sources such as webservers mobile and network devices using Apache Flume and stored the data into HDFS for analysis Collecting data from various Flume agents that are imported on various servers using Multihop flow Ingest realtime and nearrealtime NRT streaming data into HDFS using Flume Extensively involved in Installation and configuration of Cloudera distribution Hadoop Name node Secondary Name Node Job Tracker Task Trackers and Data Nodes Developed Map Reduce programs in Java and Sqoop the data from ORACLE database Responsible for building Scalable distributed data solutions using Hadoop Written various Hive and Pig scripts Used various HBase commands and generated different Datasets as per requirements and provided access to the data when required using grant and revoke Created HBase tables to store variable data formats of input data coming from different portfolios Worked on HBase for support enterprise production and loading data into HBase using SQOOP Installed Oozie workflow engine to run multiple Hive and Pig jobs which run independently with time and data availability Expertise in understanding Partitions Bucketing concepts in Hive Experience working with Apache SOLR for indexing and querying Created custom SOLR Query segments to optimize ideal search matching Used Oozie Scheduler system to automate the pipeline workflow and orchestrate the Map Reduce Jobs that extract the data in a timely manner Responsible for loading data from the UNIX file system to HDFS Developed suit of Unit Test Cases for Mapper Reducer and Driver classes using MR Testing library Analyzed the weblog data using the HiveQL integrated Oozie with the rest of the Hadoop stack Utilized cluster coordination services through Zookeeper Worked on the Ingestion of Files into HDFS from remote systems using MFT Got good experience with various NoSQL databases and Comprehensive knowledge in process improvement normalizationdenormalization data extraction data cleansing and data manipulation Developed Pig scripts to convert the data from Text file to Avro format Created Partitioned Hive tables and worked on them using HiveQL Developed Shell scripts to automate routine DBA tasks Used Maven extensively for building jar files of Map Reduce programs and deployed to Cluster Responsible for cluster maintenance adding and removing cluster nodes cluster monitoring and troubleshooting managing and reviewing data backups and Hadoop log files Environment HDFS MapReduce Pig Hive Oozie Sqoop Flume HBase Java Maven Avro Cloudera Eclipse and Shell Scripting Java Developer Financial Technologies Limited Chennai Tamil Nadu December 2012 to February 2014 Description Financial Technologies India Ltd is a world leader in providing nextgeneration technology ventures innovations platforms and solutions for creating digital markets and marketplaces that enable price discovery and transaction efficiencies across industry segments Responsibilities Involved in gathering system requirements for the application and worked with the business team to review the requirements and went through the Software Requirement Specification document and Architecture document Involved in intense User Interface UI operations and clientside validations using AJAX toolkit Used SOAP to expose company applications as a Web Service to outside clients Log package is used for the debugging Used Clear Case for version control Ensuring adherence to delivery schedules and quality process on projects Used Web Services for creating rate summary and used WSDL and SOAP messages for getting insurance plans from the different module and used XML parsers for data retrieval Developed business components and integrated those using Spring features such as Dependency Injection Auto wiring components such as DAO layers and service proxy layers Used Spring AOP to implement Distributed declarative transaction throughout the application Wrote Hibernate configuration XML files to manage data persistence Used TOAD to generate SQL queries for the applications and to see the reports from log tables Involved in the migration of Data from Excel Flat file Oracle XML files to SQL Server by using BCP and DTS utility Environment JavaJ2EE MVC Arch with CICS interaction HTML Axis SOAP Servlets Web services Restful Web Services Sybase Spring DB2 RAD Rational Clear case WCF AJAX Toad Java Developer Rockwell Collins Hyderabad Telangana January 2011 to November 2012 Description Rockwell Collins NYSE COL is a leader in aviation and highintegrity solutions for commercial and military customers around the world Every day we help pilots safely and reliably navigate to the far corners of the earth keep warfighters aware and informed in battle deliver millions of messages for airlines and airports and help passengers stay connected and comfortable throughout their journey Responsibilities Responsible for developing various modules frontend and backend components using several design patterns based on the clients business requirements Designed and Developed application modules using spring and Hibernate frameworks Designed and developed the frontend with Swings and Spring MVC framework Tag libraries and Custom Tag Libraries and development of Presentation Tier using JSP pages integrating AJAX Custom Tags JSP Tag Lists HTML JavaScript and JQuery Used Hibernate to develop persistent classes following ORM principles Deployed spring configuration files such as application context application resources and application files Used JavaJ2EE patterns like Model View Controller MVC Business Delegate Session faade Service Locator Data Transfer Objects Data Access Objects Singleton and factory patterns Used JUnit for Testing Java Classes Used Waterfall methodology Worked with Maven for build scripts and Setup the Log4J Logging framework Involved in the Integration of the Application with other services Involved in Units integration bug fixing and testing with test cases Fixed the bugs reported in User Testing and deployed the changes to the server Managing the version control for the deliverables by streamlining and rebasing the development streams of the SVN Environment JavaJDK J2EE Spring 25 Spring MVC Hibernate Eclipse Tomcat XML JSTL JavaScript Maven2 Web Services JQuery SVN JUnit Log4J Windows Oracle Development Methodologies 2010 to 2012 M Oziee Hue SOAP UI Reporting Tools MS Office  VisioOutlook Crystal Reports XI SSRS Cognos 7060 Databases Microsoft SQL Server 12 MySQL 4x5x Oracle 11g 12c DB2 Teradata Netezza Operating Systems All versions of Windows UNIX LINUX Macintosh HD Sun Solaris Education Bachelors Skills Cassandra Hdfs Impala Mapreduce Oozie Sqoop Hbase Kafka Flume Hadoop Mongodb Splunk Tableau server C Hadoop Hbase Hive Json Mapreduce Pig Additional Information TECHNICAL SKILLS Hadoop Technologies Hadoop HDFS YARN MapReduce Hive Pig Impala Sqoop Flume Spark Kafka Storm Drill Zookeeperand Oozie Languages HTML5 DHTML WSDL CSS3 C C XML RR Studio SAS Schemas JSON Ajax Java Scala Python Shell Scripting Big Data Platforms Hortonworks Cloudera NO SQL Databases Cassandra HBase MongoDB MariaDB Business Intelligence Tools Tableau server Tableau Reader Tableau Splunk SAP Business Objects QlikView Amazon Redshift or Azure Data Warehouse Development Tools Microsoft SQL Studio IntelliJ Eclipse NetBeans",
    "entities": [
        "Created Partitioned Hive",
        "Developer Bank of America Avenel NJ",
        "JMS Java Messaging Service Implemented J2EE Design Patterns",
        "JQuery Used Hibernate",
        "Clouderamanager Environment Hadoop HDFS HBase Spark MapReduce Tera Data",
        "Transformation and Actions Created",
        "AJAX",
        "SNS",
        "Test Driven Development TDD",
        "New York",
        "Hadoop Distributions like Apache Hadoop",
        "Cassandra",
        "BI",
        "Present Description Bank of America Merrill Lynch",
        "HDFS",
        "UNIX",
        "SOLR",
        "Partitions Bucketing",
        "Implemented Storm",
        "Text",
        "ETL Python Big dataHadoop",
        "HBASE Created MapReduce",
        "JSON",
        "IBM",
        "Deployed",
        "Hadoop Imported",
        "Used Web Services",
        "Cassandra Written",
        "Hadoop",
        "HTML Axis SOAP Servlets Web services",
        "XML",
        "Structured SemiStructured",
        "HDFS Involved",
        "SOAP",
        "Apple Samsung LG",
        "Created Partitions Buckets",
        "Dependency Injection Auto",
        "CDH3 CDH4 Distributions Responsible",
        "the Hadoop Cluster",
        "Shell",
        "DTS",
        "State",
        "SVN Jenkins",
        "HBase",
        "Apache Spark",
        "JavaJ2EE",
        "YARN Architecture Hands",
        "Hadoop Developer Center Light Health",
        "MonitoringCluster",
        "Description CenterLight Healthcare",
        "Created Project",
        "SQL Server",
        "SparkSQL",
        "Developed",
        "CenterLight Healthcare",
        "DAO",
        "Data Warehouse",
        "CenterLight Health System",
        "Sqoop Improved",
        "JUnit Easy MockandMRUnit",
        "Cassandra CQL",
        "AJAX Custom Tags JSP Tag",
        "JavaScript Maven2",
        "Scala Developed",
        "California Health and Human Services Agency Responsibilities Collected",
        "Data Mart Strong",
        "UML",
        "Analysis Design Development Testing Implementation Maintenance and Enhancements",
        "Waterfall",
        "Pig Hive HBase",
        "ORACLE",
        "Developer Bank of America",
        "Hibernate Expertise",
        "NRT",
        "Java Developer Financial Technologies Limited",
        "Integrated Quartz",
        "Bank of America NA",
        "JSP",
        "Unstructured",
        "Model View Controller MVC",
        "Hadoop MapReduce HDFS Sqoop Pig Hive",
        "DTO Singleton",
        "Spark Streaming",
        "Zookeeper Worked",
        "Hadoop Written",
        "Oracle XML",
        "Java J2EE Servlets JSP",
        "FDIC Responsibilities Responsible",
        "Description Sprint",
        "Driver",
        "BCP",
        "MVC",
        "Oozie scheduler Involved",
        "Spark",
        "Agile",
        "Bank of America Corporation",
        "WSDL Experienced",
        "SPLIT",
        "Oracle 12",
        "Bank of America Corporation Lending",
        "Engage Point",
        "HTML CSS",
        "Database",
        "Sqoop",
        "HIVE",
        "EssentialPhone",
        "Tableau Reader Tableau Splunk SAP Business Objects QlikView Amazon Redshift or Azure Data Warehouse Development Tools",
        "MFT",
        "Created",
        "Spark SQL Involved",
        "AWS",
        "Scala",
        "Created Data Pipeline of MapReduce",
        "Scala Akka Java JavaScript",
        "Oracle",
        "Singleton",
        "Created Talend Jobs",
        "Hive using HBaseHive Integration Streamed AWS log",
        "PIG",
        "Created Hive Generic",
        "HDFS Developed Spark",
        "java",
        "Windows Oracle Development Methodologies",
        "Intelligence Tools Tableau",
        "Oozie",
        "CXF",
        "Description Financial Technologies India Ltd",
        "SQL",
        "Spark RDD",
        "MVC Session Faade DAO",
        "Chained Mappers Implemented Optimized",
        "Restful Web Services Sybase",
        "HDFS Exporting the",
        "the Software Requirement Specification",
        "DOM SAX JAXP JSON",
        "Written MapReduce",
        "Horton Works Distributions HDP",
        "Created Managed",
        "CICS",
        "Big Data",
        "Hive",
        "SQOOP",
        "Ambari Hadoop",
        "Macintosh",
        "Apache Tomcat",
        "Amazon AWS",
        "TestDriven",
        "Sun Solaris",
        "OOZIE",
        "Unit Test Cases for Mapper Reducer",
        "Hive Pig Hbase",
        "RAD Rational Clear",
        "ETL",
        "Reporting Tools MS Office",
        "Custom Tag Libraries",
        "JAVA",
        "Bronx",
        "Responsibilities Provided",
        "Maven",
        "Implemented Service Oriented Architecture SOA",
        "Collecting",
        "Impala",
        "Oozieworkflows",
        "ANT",
        "Microsoft",
        "XSD",
        "User Testing",
        "LZO",
        "SVN",
        "Extraction Transformation",
        "HIVEQL",
        "Created HBase",
        "Cluster Responsible",
        "Actions",
        "REST",
        "NetBeans",
        "a Web Service",
        "NoSQL",
        "Tableau",
        "COL",
        "SOLR Query",
        "TOAD",
        "Sprint",
        "KAFKA",
        "Installation",
        "Cloudera"
    ],
    "experience": "Projects and experience in Big Data in implementing endtoend Hadoop solutions Experience in working in environments using Agile SCRUM RUP and TestDriven development methodologies Good Knowledge in Amazon AWS concepts like EMR and EC2 web services which provides fast and efficient processing Extensive experience in installing configuring and using ecosystem components like Hadoop MapReduce HDFS Sqoop Pig Hive Impala Spark Expertise in using J2EE application servers such as IBM Web Sphere JBoss and web servers like Apache Tomcat Experience in different Hadoop distributions like Cloudera CDH3 CDH4 and Horton Works Distributions HDP Experience in analyzing data using HIVEQL PIG Latin and custom Map Reduce programs in JAVA Extending HIVE and PIG core functionality by using custom UDFs Experienced in configuring and administering the Hadoop Cluster using major Hadoop Distributions like Apache Hadoop and Cloudera Diverse experience utilizing Java tools in business Web and clientserver environments including Java Platform J2EE EJB JSP Java Servlets Struts and Java database Connectivity JDBC technologies Good understanding in integration of various data sources like RDBMS Spreadsheets Text files JSON and XML files Good working experience on using Sqoop to import data into HDFS from RDBMS and viceversa Implemented Service Oriented Architecture SOA using Web Services and JMS Java Messaging Service Implemented J2EE Design Patterns such as MVC Session Faade DAO DTO Singleton Pattern Front Controller and Business Delegate Experienced in developing web services with XML based protocols such as SOAP Axis UDDI and WSDL Experienced in MVC Model View Controller architecture and various J2EE design patterns like singleton and factory design patterns Extensive experience in loading and analyzing large datasets with Hadoop framework Map Reduce HDFS PIG HIVE Flume Sqoop SPARK Impala NoSQL databases like Mongo DB HBase Cassandra Solid understanding of Hadoop MRV1 and Hadoop MRV2 or YARN Architecture Hands on experience in configuring and administering the Hadoop Cluster using major Hadoop Distributions like Apache Hadoop and Cloudera Good knowledge in SQL and PLSQL to write Stored Procedures and Functions and writing unit test cases using JUnit Extensive experience in Extraction Transformation and Loading ETL of data from multiple sources into Data Warehouse and Data Mart Strong knowledge in Object oriented designanalysis UML modeling Classic design patterns and J2EE patterns Hands on experience working with databases like Oracle 12 g SQL Server 2010 and MySQL Hands on experience on the entire latest UI stack including HTML CSS mobile friendly responsive design usercentric design etc Experience in developing webbased enterprise applications using Java J2EE Servlets JSP EJB JDBC Hibernate Spring IOC Spring AOP Spring MVC Spring Web Flow Spring Boot Spring Security Spring Batch Spring Integration Web Services SOAP and REST and ORM frameworks like Hibernate Strong knowledge on Hadoopeco systems including HDFS Hive Oozie HBase Pig Sqoop Zookeeper etc Extensive experience with advanced J2EE Frameworks such as spring Struts JSF and Hibernate Expertise in JavaScript JavaScriptMVC patterns ObjectOrientedJavaScriptDesign Patterns and AJAX calls Installation configuration and administration experience in Big Data platforms Cloudera Manager of Cloudera MCS of MapR Expertise in using XML related technologies such as XML DTD XSD XPATH XSLT DOM SAX JAXP JSON and JAXB Experience in using ANT and Maven for building and deploying the projects in servers and also using Junit and log4j for debugging Work Experience Big dataHadoop Developer Bank of America August 2018 to Present Description Bank of America Merrill Lynch is the marketing name for the global banking and global markets businesses of Bank of America Corporation Lending derivatives and other commercial banking activities are performed globally by banking affiliates of Bank of America Corporation including Bank of America NA and Member FDIC Responsibilities Responsible for installation and configuration of Hive Pig Hbase and Sqoop on the Hadoop cluster and created hive tables to store the processed results in a tabular format Configured Spark Streaming to receive real time data from the Apache Kafka and store the stream data to HDFS using Scala Developed the Sqoop scripts to make the interaction between Hive and vertica Database Processed data into HDFS by developing solutions and analyzed the data using Map Reduce PIG and Hive to produce summary results from Hadoop to downstream systems Build servers using AWS Importing volumes launching EC2 creating security groups autoscaling load balancers Route 53 SES and SNS in the defined virtual private connection Written Map Reduce code to process and parsing the data from various sources and storing parsed data into HBase and Hive using HBaseHive Integration Streamed AWS log group into Lambda function to create service now incident Involved in loading and transforming large sets of Structured SemiStructured and Unstructured data and analyzed them by running Hive queries and Pig scripts Created Managed tables and External tables in Hive and loaded data from HDFS Developed Spark code by using Scala and SparkSQL for faster processing and testing and performed complex HiveQL queries on Hive tables Scheduled several times based Oozie workflow by developing Python scripts Developed Pig Latin scripts using operators such as LOAD STORE DUMP FILTER DISTINCT FOREACH GENERATE GROUP COGROUP ORDER LIMIT UNION SPLIT to extract data from data files to load into HDFS Exporting the data using Sqoop to RDBMS servers and processed that data for ETL operations Worked on S3 buckets on AWS to store Cloud Formation Templates and worked on AWS to create EC2 instances Designing ETL Data Pipeline flow to ingest the data from RDBMS source to Hadoop using shell script sqoop package and MySQL Endtoend architecture and implementation of clientserver systems using Scala Akka Java JavaScript and related Linux Optimized the Hive tables using optimization techniques like partitions and bucketing to provide better Used Oozie workflow engine to manage interdependent Hadoop jobs and to automate several types of Hadoop jobs such as Java mapreduce Hive Pig and Sqoop Implementing Hadoop with the AWS EC2 system using a few instances in gathering and analyzing data log files Involved in Spark and Spark Streaming creating RDDs applying operations Transformation and Actions Created partitioned tables and loaded data using both static partition and dynamic partition method Developed custom Apache Spark programs in Scala to analyze and transform unstructured data Handled importing of data from various data sources performed transformations using Hive MapReduce loaded data into HDFS and Extracted the data from Oracle into HDFS using Sqoop Using Kafka on publishsubscribe messaging as a distributed commit log have experienced in its fast scalable and durability Test Driven Development TDD process and extensive experience with Agile and SCRUM programming methodology Implemented POC to migrate Map Reduce jobs into Spark RDD transformations using SCALA Scheduled map reduces jobs in production environment using Oozie scheduler Involved in Cluster maintenance Cluster Monitoring and Troubleshooting Manage and review data backups and log files Designed and implemented map reduce jobs to support distributed processing using java Hive and Apache Pig Analyzing Hadoop cluster and different Big Data analytic tools including Pig Hive HBase and Sqoop Improved the Performance by tuning of HIVE and map reduce Research evaluate and utilize modern technologiestoolsframeworks around Hadoop ecosystem Environment HDFS Map Reduce Hive Sqoop Pig Flume Vertica Oozie Scheduler Java Shell Scripts Teradata Oracle HBase MongoDB Cassandra Cloudera AWS JavaScript JSP Kafka Spark Scala and ETL Python Big dataHadoop Developer Sprint Overland Park KS May 2017 to July 2018 Description Sprint offers all the latest musthave smartphones and tablets from top industryleading manufacturers including Apple Samsung LG HTC exclusively offering the new EssentialPhone and much more Sprint is also your onestopshop for the coolest accessories from fashionforward designers and tech innovators Responsibilities Involved in Installing Configuring Hadoop EcoSystem Cloudera Manager using CDH3 CDH4 Distributions Responsible to manage data coming from different sources and involved in HDFS maintenance and loading of structured and unstructured data Imported data using Sqoop from Teradata using Teradata connector Created Data Pipeline of MapReduce programs using Chained Mappers Implemented Optimized join base by joining different data sets to get top claims based on state using Map Reduce Visualize the HDFS data to the customer using BI tool with the help of HiveODBCDriver Worked on POC of Talend integration with Hadoop where Created Talend Jobs to extract data from Hadoop Imported data using Sqoop to load data fromMySQL to HDFS on a regular basis Worked on social media Facebook Twitter etc data crawling using Java and R language and MongoDB for unstructured data storage Integrated Quartz scheduler with Oozieworkflows to get data from multiple data sources parallels using a fork Created Partitions Buckets based on State to further process using Bucket based Hive joins Experienced with different kind of compression techniques like LZO GZip Snappy Created Hive Generic UDFs to process business logic that varies based on policy Imported Relational Database data using Sqoop into Hive Dynamic partition tables using staging tables Worked on custom Pig Loaders and storage classes to work with a variety of data formats such as JSON and XML file formats Used Oozie workflow engine to manage interdependent Hadoop jobs and to automate several types of Hadoop jobs such as Java mapreduce Hive Pig and Sqoop Developed Unit test cases using JUnit Easy MockandMRUnit testing frameworks Experienced in MonitoringCluster using Clouderamanager Environment Hadoop HDFS HBase Spark MapReduce Tera Data MySQL Java Hive Pig Sqoop Flume Oozie SQL Cloudera Manager Hadoop Developer Center Light Health Bronx NY January 2016 to April 2017 Description CenterLight Healthcare a member of CenterLight Health System is a notforprofit New York State Managed Long Term Care organization Founded in 1985 to provide home healthcare services to the elderly disabled and chronically ill in the New York metropolitan area CenterLight Healthcare today is recognized as an innovator in the managed care field Responsibilities Provided application demo to the client by designing and developing a search engine report analysis trends application administration prototype screens using AngularJS and Bootstrap JS Took the ownership of complete application Design of Java part Hadoop integration Apart from the normal requirement gathering participated in a Business meeting with the client to gather security requirements Assisted with the architect to analyze the existing system and future system Prepared design blue pints and application flow documentation Experienced in managing and reviewing Hadoop log files Load and transform large sets of structured semistructured and unstructured data Responsible to manage data coming from different sources and application Supported Map Reduce Programs those are running on the cluster Responsible for working with Message broker system such as Kafka Extracted data from mainframes and feed to KAFKA and ingested to HBase to perform Analytics Written eventdriven link tracking system to capture user events and feed to KAFKA to push it to HBASE Created MapReduce jobs to extracts the contents from HBase and configured in OOZIE workflow to generate analytical reports Developed the JAX RS web services code using Apache CXF framework to fetch data from SOLR when the user performed the search for documents Participated in SOLR schema and ingested data into SOLR for data indexing Written MapReduce programs to organize the data and ingest the data to suitable for analytics in client specified format Hands on experience in writing python scripts to optimize the performance Implemented Storm builder topologies to perform cleansing operations before moving data into Cassandra Extracted files from Cassandra through Sqoop and placed in HDFS and processed Implemented Bloom filters in Cassandra using keyspace creation Involved in writing Cassandra CQL statements God handson experience in developing concurrency using spark and Cassandra together Involved in writing spark applications using Scala Hands on experience in creating RDDs transformationsand Actions while implementing spark applications Good knowledge in creating data frames using Spark SQL Involved in loading data into Cassandra NoSQL Database Implemented record level atomicity on writes using Cassandra Written PIG Scripts to query and process the Datasets to figure out the patterns of trends by applying clientspecific criteria and configured OOZIE workflows to run the jobs along with the MR jobs Stored the derived the results in HBasefrom analysis and make it available to data ingestion for SOLR for indexing data Involved in integration of java search UI SOLR and HDFS Involved in code deployments using continuous integration tool using Jenkins Documented all the challenges issues involved to deal with the security system and Implemented best practices Created Project structures and configurations according to the project architecture and made it available to the junior developer to continue their work Handled onsite coordinator role to deliver work to offshore Involved in core reviews and application lead supported activities Environment Cassandra Spring 32 Restful services using CXF web services framework spring data SOLR 521 PIG HIVE apache AVRO Map Reduce Sqoop Zookeeper SVN Jenkins windows AD windows KDC Hortonworks distribution of Hadoop 23 YARN Ambari Hadoop developer Engage Point Calverton MD March 2014 to December 2015 Description Engage Point chosen as Agile Development Prequalified Vendor by California Health and Human Services Agency Responsibilities Collected and aggregated large amounts of weblog data from different sources such as webservers mobile and network devices using Apache Flume and stored the data into HDFS for analysis Collecting data from various Flume agents that are imported on various servers using Multihop flow Ingest realtime and nearrealtime NRT streaming data into HDFS using Flume Extensively involved in Installation and configuration of Cloudera distribution Hadoop Name node Secondary Name Node Job Tracker Task Trackers and Data Nodes Developed Map Reduce programs in Java and Sqoop the data from ORACLE database Responsible for building Scalable distributed data solutions using Hadoop Written various Hive and Pig scripts Used various HBase commands and generated different Datasets as per requirements and provided access to the data when required using grant and revoke Created HBase tables to store variable data formats of input data coming from different portfolios Worked on HBase for support enterprise production and loading data into HBase using SQOOP Installed Oozie workflow engine to run multiple Hive and Pig jobs which run independently with time and data availability Expertise in understanding Partitions Bucketing concepts in Hive Experience working with Apache SOLR for indexing and querying Created custom SOLR Query segments to optimize ideal search matching Used Oozie Scheduler system to automate the pipeline workflow and orchestrate the Map Reduce Jobs that extract the data in a timely manner Responsible for loading data from the UNIX file system to HDFS Developed suit of Unit Test Cases for Mapper Reducer and Driver classes using MR Testing library Analyzed the weblog data using the HiveQL integrated Oozie with the rest of the Hadoop stack Utilized cluster coordination services through Zookeeper Worked on the Ingestion of Files into HDFS from remote systems using MFT Got good experience with various NoSQL databases and Comprehensive knowledge in process improvement normalizationdenormalization data extraction data cleansing and data manipulation Developed Pig scripts to convert the data from Text file to Avro format Created Partitioned Hive tables and worked on them using HiveQL Developed Shell scripts to automate routine DBA tasks Used Maven extensively for building jar files of Map Reduce programs and deployed to Cluster Responsible for cluster maintenance adding and removing cluster nodes cluster monitoring and troubleshooting managing and reviewing data backups and Hadoop log files Environment HDFS MapReduce Pig Hive Oozie Sqoop Flume HBase Java Maven Avro Cloudera Eclipse and Shell Scripting Java Developer Financial Technologies Limited Chennai Tamil Nadu December 2012 to February 2014 Description Financial Technologies India Ltd is a world leader in providing nextgeneration technology ventures innovations platforms and solutions for creating digital markets and marketplaces that enable price discovery and transaction efficiencies across industry segments Responsibilities Involved in gathering system requirements for the application and worked with the business team to review the requirements and went through the Software Requirement Specification document and Architecture document Involved in intense User Interface UI operations and clientside validations using AJAX toolkit Used SOAP to expose company applications as a Web Service to outside clients Log package is used for the debugging Used Clear Case for version control Ensuring adherence to delivery schedules and quality process on projects Used Web Services for creating rate summary and used WSDL and SOAP messages for getting insurance plans from the different module and used XML parsers for data retrieval Developed business components and integrated those using Spring features such as Dependency Injection Auto wiring components such as DAO layers and service proxy layers Used Spring AOP to implement Distributed declarative transaction throughout the application Wrote Hibernate configuration XML files to manage data persistence Used TOAD to generate SQL queries for the applications and to see the reports from log tables Involved in the migration of Data from Excel Flat file Oracle XML files to SQL Server by using BCP and DTS utility Environment JavaJ2EE MVC Arch with CICS interaction HTML Axis SOAP Servlets Web services Restful Web Services Sybase Spring DB2 RAD Rational Clear case WCF AJAX Toad Java Developer Rockwell Collins Hyderabad Telangana January 2011 to November 2012 Description Rockwell Collins NYSE COL is a leader in aviation and highintegrity solutions for commercial and military customers around the world Every day we help pilots safely and reliably navigate to the far corners of the earth keep warfighters aware and informed in battle deliver millions of messages for airlines and airports and help passengers stay connected and comfortable throughout their journey Responsibilities Responsible for developing various modules frontend and backend components using several design patterns based on the clients business requirements Designed and Developed application modules using spring and Hibernate frameworks Designed and developed the frontend with Swings and Spring MVC framework Tag libraries and Custom Tag Libraries and development of Presentation Tier using JSP pages integrating AJAX Custom Tags JSP Tag Lists HTML JavaScript and JQuery Used Hibernate to develop persistent classes following ORM principles Deployed spring configuration files such as application context application resources and application files Used JavaJ2EE patterns like Model View Controller MVC Business Delegate Session faade Service Locator Data Transfer Objects Data Access Objects Singleton and factory patterns Used JUnit for Testing Java Classes Used Waterfall methodology Worked with Maven for build scripts and Setup the Log4J Logging framework Involved in the Integration of the Application with other services Involved in Units integration bug fixing and testing with test cases Fixed the bugs reported in User Testing and deployed the changes to the server Managing the version control for the deliverables by streamlining and rebasing the development streams of the SVN Environment JavaJDK J2EE Spring 25 Spring MVC Hibernate Eclipse Tomcat XML JSTL JavaScript Maven2 Web Services JQuery SVN JUnit Log4J Windows Oracle Development Methodologies 2010 to 2012 M Oziee Hue SOAP UI Reporting Tools MS Office   VisioOutlook Crystal Reports XI SSRS Cognos 7060 Databases Microsoft SQL Server 12 MySQL 4x5x Oracle 11 g 12c DB2 Teradata Netezza Operating Systems All versions of Windows UNIX LINUX Macintosh HD Sun Solaris Education Bachelors Skills Cassandra Hdfs Impala Mapreduce Oozie Sqoop Hbase Kafka Flume Hadoop Mongodb Splunk Tableau server C Hadoop Hbase Hive Json Mapreduce Pig Additional Information TECHNICAL SKILLS Hadoop Technologies Hadoop HDFS YARN MapReduce Hive Pig Impala Sqoop Flume Spark Kafka Storm Drill Zookeeperand Oozie Languages HTML5 DHTML WSDL CSS3 C C XML RR Studio SAS Schemas JSON Ajax Java Scala Python Shell Scripting Big Data Platforms Hortonworks Cloudera NO SQL Databases Cassandra HBase MongoDB MariaDB Business Intelligence Tools Tableau server Tableau Reader Tableau Splunk SAP Business Objects QlikView Amazon Redshift or Azure Data Warehouse Development Tools Microsoft SQL Studio IntelliJ Eclipse NetBeans",
    "extracted_keywords": [
        "dataHadoop",
        "Developer",
        "Big",
        "dataHadoop",
        "span",
        "lDeveloperspan",
        "Big",
        "dataHadoop",
        "Developer",
        "Bank",
        "America",
        "Avenel",
        "NJ",
        "years",
        "experience",
        "Analysis",
        "Design",
        "Development",
        "Testing",
        "Implementation",
        "Maintenance",
        "Enhancements",
        "IT",
        "Projects",
        "experience",
        "Big",
        "Data",
        "endtoend",
        "Hadoop",
        "solutions",
        "Experience",
        "environments",
        "Agile",
        "SCRUM",
        "RUP",
        "TestDriven",
        "development",
        "methodologies",
        "Good",
        "Knowledge",
        "Amazon",
        "AWS",
        "concepts",
        "EMR",
        "EC2",
        "web",
        "services",
        "experience",
        "configuring",
        "ecosystem",
        "components",
        "Hadoop",
        "MapReduce",
        "HDFS",
        "Sqoop",
        "Pig",
        "Hive",
        "Impala",
        "Spark",
        "Expertise",
        "J2EE",
        "application",
        "servers",
        "IBM",
        "Web",
        "Sphere",
        "JBoss",
        "web",
        "servers",
        "Apache",
        "Tomcat",
        "Experience",
        "Hadoop",
        "distributions",
        "Cloudera",
        "CDH3",
        "CDH4",
        "Horton",
        "Distributions",
        "HDP",
        "Experience",
        "data",
        "HIVEQL",
        "PIG",
        "Latin",
        "custom",
        "Map",
        "Reduce",
        "programs",
        "JAVA",
        "HIVE",
        "PIG",
        "core",
        "functionality",
        "custom",
        "UDFs",
        "configuring",
        "Hadoop",
        "Cluster",
        "Hadoop",
        "Distributions",
        "Apache",
        "Hadoop",
        "Cloudera",
        "Diverse",
        "experience",
        "Java",
        "tools",
        "business",
        "Web",
        "environments",
        "Java",
        "Platform",
        "J2EE",
        "EJB",
        "JSP",
        "Java",
        "Servlets",
        "Struts",
        "Java",
        "database",
        "Connectivity",
        "JDBC",
        "technologies",
        "understanding",
        "integration",
        "data",
        "sources",
        "RDBMS",
        "Spreadsheets",
        "Text",
        "XML",
        "working",
        "experience",
        "Sqoop",
        "data",
        "HDFS",
        "RDBMS",
        "viceversa",
        "Service",
        "Oriented",
        "Architecture",
        "SOA",
        "Web",
        "Services",
        "JMS",
        "Java",
        "Messaging",
        "Service",
        "J2EE",
        "Design",
        "Patterns",
        "MVC",
        "Session",
        "Faade",
        "DAO",
        "DTO",
        "Singleton",
        "Pattern",
        "Front",
        "Controller",
        "Business",
        "Delegate",
        "web",
        "services",
        "XML",
        "protocols",
        "SOAP",
        "Axis",
        "UDDI",
        "WSDL",
        "MVC",
        "Model",
        "View",
        "Controller",
        "architecture",
        "J2EE",
        "design",
        "patterns",
        "singleton",
        "factory",
        "design",
        "patterns",
        "experience",
        "loading",
        "datasets",
        "Hadoop",
        "framework",
        "Map",
        "Reduce",
        "HDFS",
        "PIG",
        "HIVE",
        "Flume",
        "Sqoop",
        "SPARK",
        "Impala",
        "NoSQL",
        "Mongo",
        "DB",
        "HBase",
        "Cassandra",
        "understanding",
        "Hadoop",
        "MRV1",
        "Hadoop",
        "MRV2",
        "YARN",
        "Architecture",
        "Hands",
        "experience",
        "Hadoop",
        "Cluster",
        "Hadoop",
        "Distributions",
        "Apache",
        "Hadoop",
        "Cloudera",
        "knowledge",
        "SQL",
        "PLSQL",
        "Stored",
        "Procedures",
        "Functions",
        "unit",
        "test",
        "cases",
        "JUnit",
        "experience",
        "Extraction",
        "Transformation",
        "Loading",
        "ETL",
        "data",
        "sources",
        "Data",
        "Warehouse",
        "Data",
        "Mart",
        "knowledge",
        "Object",
        "designanalysis",
        "UML",
        "Classic",
        "design",
        "patterns",
        "J2EE",
        "Hands",
        "experience",
        "databases",
        "Oracle",
        "g",
        "SQL",
        "Server",
        "MySQL",
        "Hands",
        "experience",
        "UI",
        "stack",
        "HTML",
        "CSS",
        "mobile",
        "design",
        "design",
        "Experience",
        "enterprise",
        "applications",
        "Java",
        "J2EE",
        "Servlets",
        "JSP",
        "EJB",
        "JDBC",
        "Hibernate",
        "Spring",
        "IOC",
        "Spring",
        "AOP",
        "Spring",
        "MVC",
        "Spring",
        "Web",
        "Flow",
        "Spring",
        "Boot",
        "Spring",
        "Security",
        "Spring",
        "Batch",
        "Spring",
        "Integration",
        "Web",
        "Services",
        "SOAP",
        "REST",
        "ORM",
        "frameworks",
        "Hibernate",
        "knowledge",
        "Hadoopeco",
        "systems",
        "HDFS",
        "Hive",
        "Oozie",
        "HBase",
        "Pig",
        "Sqoop",
        "Zookeeper",
        "experience",
        "J2EE",
        "Frameworks",
        "spring",
        "Struts",
        "JSF",
        "Hibernate",
        "Expertise",
        "JavaScript",
        "JavaScriptMVC",
        "ObjectOrientedJavaScriptDesign",
        "Patterns",
        "AJAX",
        "Installation",
        "configuration",
        "administration",
        "experience",
        "Big",
        "Data",
        "Cloudera",
        "Manager",
        "Cloudera",
        "MCS",
        "MapR",
        "Expertise",
        "XML",
        "technologies",
        "XML",
        "DTD",
        "XSD",
        "XPATH",
        "XSLT",
        "DOM",
        "SAX",
        "JAXP",
        "JSON",
        "JAXB",
        "Experience",
        "ANT",
        "Maven",
        "projects",
        "servers",
        "Junit",
        "log4j",
        "Work",
        "Experience",
        "Big",
        "dataHadoop",
        "Developer",
        "Bank",
        "America",
        "August",
        "Present",
        "Description",
        "Bank",
        "America",
        "Merrill",
        "Lynch",
        "marketing",
        "name",
        "banking",
        "markets",
        "businesses",
        "Bank",
        "America",
        "Corporation",
        "Lending",
        "derivatives",
        "banking",
        "activities",
        "banking",
        "affiliates",
        "Bank",
        "America",
        "Corporation",
        "Bank",
        "America",
        "NA",
        "Member",
        "FDIC",
        "Responsibilities",
        "installation",
        "configuration",
        "Hive",
        "Pig",
        "Hbase",
        "Sqoop",
        "Hadoop",
        "cluster",
        "tables",
        "results",
        "format",
        "Configured",
        "Spark",
        "Streaming",
        "time",
        "data",
        "Apache",
        "Kafka",
        "stream",
        "data",
        "HDFS",
        "Scala",
        "Sqoop",
        "scripts",
        "interaction",
        "Hive",
        "vertica",
        "Database",
        "data",
        "HDFS",
        "solutions",
        "data",
        "Map",
        "Reduce",
        "PIG",
        "Hive",
        "summary",
        "results",
        "Hadoop",
        "systems",
        "Build",
        "servers",
        "AWS",
        "volumes",
        "EC2",
        "security",
        "groups",
        "load",
        "balancers",
        "SES",
        "SNS",
        "connection",
        "Written",
        "Map",
        "code",
        "data",
        "sources",
        "data",
        "HBase",
        "Hive",
        "Integration",
        "Streamed",
        "AWS",
        "group",
        "Lambda",
        "function",
        "service",
        "incident",
        "loading",
        "sets",
        "Structured",
        "SemiStructured",
        "Unstructured",
        "data",
        "Hive",
        "queries",
        "Pig",
        "scripts",
        "Managed",
        "tables",
        "tables",
        "Hive",
        "data",
        "HDFS",
        "Developed",
        "Spark",
        "code",
        "Scala",
        "SparkSQL",
        "processing",
        "testing",
        "HiveQL",
        "queries",
        "Hive",
        "tables",
        "times",
        "Oozie",
        "workflow",
        "Python",
        "scripts",
        "Developed",
        "Pig",
        "Latin",
        "scripts",
        "operators",
        "STORE",
        "DUMP",
        "FILTER",
        "DISTINCT",
        "FOREACH",
        "GENERATE",
        "GROUP",
        "COGROUP",
        "ORDER",
        "LIMIT",
        "UNION",
        "SPLIT",
        "data",
        "data",
        "files",
        "HDFS",
        "Exporting",
        "data",
        "Sqoop",
        "servers",
        "data",
        "ETL",
        "operations",
        "S3",
        "buckets",
        "AWS",
        "Cloud",
        "Formation",
        "Templates",
        "AWS",
        "EC2",
        "instances",
        "ETL",
        "Data",
        "Pipeline",
        "flow",
        "data",
        "source",
        "Hadoop",
        "shell",
        "script",
        "sqoop",
        "package",
        "MySQL",
        "Endtoend",
        "architecture",
        "implementation",
        "clientserver",
        "systems",
        "Scala",
        "Akka",
        "Java",
        "JavaScript",
        "Linux",
        "Hive",
        "tables",
        "optimization",
        "techniques",
        "partitions",
        "bucketing",
        "Oozie",
        "workflow",
        "engine",
        "Hadoop",
        "jobs",
        "types",
        "Hadoop",
        "jobs",
        "Java",
        "mapreduce",
        "Hive",
        "Pig",
        "Sqoop",
        "Implementing",
        "Hadoop",
        "AWS",
        "EC2",
        "system",
        "instances",
        "data",
        "log",
        "files",
        "Spark",
        "Spark",
        "Streaming",
        "RDDs",
        "operations",
        "Transformation",
        "Actions",
        "tables",
        "data",
        "partition",
        "partition",
        "method",
        "custom",
        "Apache",
        "Spark",
        "programs",
        "Scala",
        "data",
        "importing",
        "data",
        "data",
        "sources",
        "transformations",
        "Hive",
        "MapReduce",
        "data",
        "HDFS",
        "data",
        "Oracle",
        "HDFS",
        "Sqoop",
        "Kafka",
        "publishsubscribe",
        "commit",
        "log",
        "durability",
        "Test",
        "Driven",
        "Development",
        "TDD",
        "process",
        "experience",
        "SCRUM",
        "programming",
        "methodology",
        "POC",
        "Map",
        "Reduce",
        "jobs",
        "Spark",
        "RDD",
        "transformations",
        "SCALA",
        "map",
        "jobs",
        "production",
        "environment",
        "Oozie",
        "scheduler",
        "Cluster",
        "maintenance",
        "Cluster",
        "Monitoring",
        "Troubleshooting",
        "Manage",
        "data",
        "backups",
        "files",
        "map",
        "jobs",
        "processing",
        "Hive",
        "Apache",
        "Pig",
        "Analyzing",
        "Hadoop",
        "cluster",
        "Big",
        "Data",
        "tools",
        "Pig",
        "Hive",
        "HBase",
        "Sqoop",
        "Performance",
        "HIVE",
        "Research",
        "evaluate",
        "technologiestoolsframeworks",
        "Hadoop",
        "ecosystem",
        "Environment",
        "HDFS",
        "Map",
        "Reduce",
        "Hive",
        "Sqoop",
        "Pig",
        "Flume",
        "Vertica",
        "Oozie",
        "Scheduler",
        "Java",
        "Shell",
        "Scripts",
        "Teradata",
        "Oracle",
        "HBase",
        "MongoDB",
        "Cassandra",
        "Cloudera",
        "JavaScript",
        "JSP",
        "Kafka",
        "Spark",
        "Scala",
        "ETL",
        "Python",
        "Big",
        "dataHadoop",
        "Developer",
        "Sprint",
        "Overland",
        "Park",
        "KS",
        "May",
        "July",
        "Description",
        "Sprint",
        "musthave",
        "smartphones",
        "tablets",
        "manufacturers",
        "Apple",
        "Samsung",
        "LG",
        "HTC",
        "EssentialPhone",
        "Sprint",
        "onestopshop",
        "accessories",
        "designers",
        "tech",
        "innovators",
        "Responsibilities",
        "Configuring",
        "Hadoop",
        "EcoSystem",
        "Cloudera",
        "Manager",
        "CDH3",
        "CDH4",
        "Distributions",
        "data",
        "sources",
        "HDFS",
        "maintenance",
        "loading",
        "data",
        "data",
        "Sqoop",
        "Teradata",
        "Teradata",
        "connector",
        "Created",
        "Data",
        "Pipeline",
        "MapReduce",
        "programs",
        "Mappers",
        "join",
        "base",
        "data",
        "sets",
        "claims",
        "state",
        "Map",
        "Reduce",
        "Visualize",
        "HDFS",
        "data",
        "customer",
        "BI",
        "tool",
        "help",
        "POC",
        "Talend",
        "integration",
        "Hadoop",
        "Created",
        "Talend",
        "Jobs",
        "data",
        "Hadoop",
        "Imported",
        "data",
        "Sqoop",
        "data",
        "HDFS",
        "basis",
        "media",
        "Facebook",
        "Twitter",
        "data",
        "Java",
        "R",
        "language",
        "MongoDB",
        "data",
        "storage",
        "Integrated",
        "Quartz",
        "scheduler",
        "Oozieworkflows",
        "data",
        "data",
        "sources",
        "fork",
        "Created",
        "Partitions",
        "Buckets",
        "State",
        "process",
        "Bucket",
        "Hive",
        "kind",
        "compression",
        "techniques",
        "LZO",
        "GZip",
        "Snappy",
        "Created",
        "Hive",
        "Generic",
        "UDFs",
        "business",
        "logic",
        "policy",
        "Imported",
        "Relational",
        "Database",
        "data",
        "Sqoop",
        "Hive",
        "partition",
        "tables",
        "staging",
        "tables",
        "custom",
        "Pig",
        "Loaders",
        "storage",
        "classes",
        "variety",
        "data",
        "formats",
        "JSON",
        "XML",
        "file",
        "formats",
        "Oozie",
        "workflow",
        "engine",
        "Hadoop",
        "jobs",
        "types",
        "Hadoop",
        "jobs",
        "Java",
        "mapreduce",
        "Hive",
        "Pig",
        "Sqoop",
        "Developed",
        "Unit",
        "test",
        "cases",
        "JUnit",
        "Easy",
        "MockandMRUnit",
        "testing",
        "frameworks",
        "MonitoringCluster",
        "Clouderamanager",
        "Environment",
        "Hadoop",
        "HDFS",
        "HBase",
        "Spark",
        "MapReduce",
        "Tera",
        "Data",
        "MySQL",
        "Java",
        "Hive",
        "Pig",
        "Sqoop",
        "Flume",
        "Oozie",
        "SQL",
        "Cloudera",
        "Manager",
        "Hadoop",
        "Developer",
        "Center",
        "Light",
        "Health",
        "Bronx",
        "NY",
        "January",
        "April",
        "Description",
        "CenterLight",
        "Healthcare",
        "member",
        "CenterLight",
        "Health",
        "System",
        "New",
        "York",
        "State",
        "Managed",
        "Long",
        "Term",
        "Care",
        "organization",
        "home",
        "healthcare",
        "services",
        "New",
        "York",
        "area",
        "CenterLight",
        "Healthcare",
        "today",
        "innovator",
        "care",
        "field",
        "Responsibilities",
        "application",
        "demo",
        "client",
        "search",
        "engine",
        "report",
        "analysis",
        "trends",
        "application",
        "administration",
        "prototype",
        "screens",
        "AngularJS",
        "Bootstrap",
        "JS",
        "ownership",
        "application",
        "Design",
        "Java",
        "part",
        "Hadoop",
        "integration",
        "requirement",
        "gathering",
        "Business",
        "meeting",
        "client",
        "security",
        "requirements",
        "architect",
        "system",
        "system",
        "Prepared",
        "design",
        "pints",
        "application",
        "flow",
        "documentation",
        "Hadoop",
        "log",
        "Load",
        "sets",
        "data",
        "data",
        "sources",
        "application",
        "Map",
        "Reduce",
        "Programs",
        "cluster",
        "Message",
        "broker",
        "system",
        "Kafka",
        "data",
        "mainframes",
        "feed",
        "KAFKA",
        "HBase",
        "Analytics",
        "link",
        "tracking",
        "system",
        "user",
        "events",
        "feed",
        "KAFKA",
        "HBASE",
        "MapReduce",
        "jobs",
        "contents",
        "HBase",
        "OOZIE",
        "reports",
        "JAX",
        "RS",
        "web",
        "services",
        "code",
        "Apache",
        "CXF",
        "framework",
        "data",
        "SOLR",
        "user",
        "search",
        "documents",
        "SOLR",
        "schema",
        "data",
        "SOLR",
        "data",
        "indexing",
        "Written",
        "MapReduce",
        "programs",
        "data",
        "data",
        "analytics",
        "client",
        "format",
        "Hands",
        "experience",
        "scripts",
        "performance",
        "Storm",
        "builder",
        "topologies",
        "cleansing",
        "operations",
        "data",
        "Cassandra",
        "files",
        "Cassandra",
        "Sqoop",
        "HDFS",
        "Implemented",
        "Bloom",
        "filters",
        "Cassandra",
        "creation",
        "Cassandra",
        "CQL",
        "God",
        "handson",
        "experience",
        "concurrency",
        "spark",
        "Cassandra",
        "spark",
        "applications",
        "Scala",
        "Hands",
        "experience",
        "RDDs",
        "transformationsand",
        "Actions",
        "spark",
        "applications",
        "knowledge",
        "data",
        "frames",
        "Spark",
        "SQL",
        "loading",
        "data",
        "Cassandra",
        "NoSQL",
        "Database",
        "record",
        "level",
        "atomicity",
        "writes",
        "Cassandra",
        "Written",
        "PIG",
        "Scripts",
        "query",
        "Datasets",
        "patterns",
        "trends",
        "clientspecific",
        "criteria",
        "OOZIE",
        "workflows",
        "jobs",
        "MR",
        "jobs",
        "results",
        "HBasefrom",
        "analysis",
        "data",
        "ingestion",
        "SOLR",
        "indexing",
        "data",
        "integration",
        "search",
        "UI",
        "SOLR",
        "HDFS",
        "code",
        "deployments",
        "integration",
        "tool",
        "Jenkins",
        "challenges",
        "issues",
        "security",
        "system",
        "practices",
        "Project",
        "structures",
        "configurations",
        "project",
        "architecture",
        "developer",
        "work",
        "coordinator",
        "role",
        "work",
        "core",
        "reviews",
        "application",
        "lead",
        "activities",
        "Environment",
        "Cassandra",
        "Spring",
        "services",
        "CXF",
        "web",
        "services",
        "framework",
        "spring",
        "data",
        "SOLR",
        "PIG",
        "HIVE",
        "apache",
        "AVRO",
        "Map",
        "Reduce",
        "Sqoop",
        "Zookeeper",
        "SVN",
        "Jenkins",
        "AD",
        "KDC",
        "Hortonworks",
        "distribution",
        "Hadoop",
        "YARN",
        "Ambari",
        "Hadoop",
        "developer",
        "Engage",
        "Point",
        "Calverton",
        "MD",
        "March",
        "December",
        "Description",
        "Engage",
        "Point",
        "Agile",
        "Development",
        "Vendor",
        "California",
        "Health",
        "Human",
        "Services",
        "Agency",
        "Responsibilities",
        "amounts",
        "weblog",
        "data",
        "sources",
        "webservers",
        "mobile",
        "network",
        "devices",
        "Apache",
        "Flume",
        "data",
        "HDFS",
        "analysis",
        "data",
        "Flume",
        "agents",
        "servers",
        "Multihop",
        "flow",
        "realtime",
        "nearrealtime",
        "NRT",
        "streaming",
        "data",
        "HDFS",
        "Flume",
        "Installation",
        "configuration",
        "Cloudera",
        "distribution",
        "Hadoop",
        "Name",
        "node",
        "Secondary",
        "Name",
        "Node",
        "Job",
        "Tracker",
        "Task",
        "Trackers",
        "Data",
        "Nodes",
        "Developed",
        "Map",
        "programs",
        "Java",
        "Sqoop",
        "data",
        "database",
        "data",
        "solutions",
        "Hadoop",
        "Written",
        "Hive",
        "Pig",
        "scripts",
        "HBase",
        "commands",
        "Datasets",
        "requirements",
        "access",
        "data",
        "grant",
        "Created",
        "HBase",
        "tables",
        "data",
        "formats",
        "input",
        "data",
        "portfolios",
        "HBase",
        "support",
        "enterprise",
        "production",
        "loading",
        "data",
        "HBase",
        "SQOOP",
        "Installed",
        "Oozie",
        "workflow",
        "engine",
        "Hive",
        "Pig",
        "jobs",
        "time",
        "data",
        "availability",
        "Expertise",
        "Partitions",
        "Bucketing",
        "concepts",
        "Hive",
        "Experience",
        "Apache",
        "SOLR",
        "indexing",
        "custom",
        "SOLR",
        "Query",
        "search",
        "Oozie",
        "Scheduler",
        "system",
        "pipeline",
        "Map",
        "Reduce",
        "Jobs",
        "data",
        "manner",
        "loading",
        "data",
        "UNIX",
        "file",
        "system",
        "suit",
        "Unit",
        "Test",
        "Cases",
        "Mapper",
        "Reducer",
        "Driver",
        "classes",
        "MR",
        "Testing",
        "library",
        "weblog",
        "data",
        "HiveQL",
        "Oozie",
        "rest",
        "Hadoop",
        "stack",
        "cluster",
        "coordination",
        "services",
        "Zookeeper",
        "Worked",
        "Ingestion",
        "Files",
        "HDFS",
        "systems",
        "MFT",
        "experience",
        "NoSQL",
        "databases",
        "knowledge",
        "process",
        "improvement",
        "normalizationdenormalization",
        "data",
        "extraction",
        "data",
        "cleansing",
        "data",
        "manipulation",
        "Developed",
        "Pig",
        "scripts",
        "data",
        "Text",
        "file",
        "Avro",
        "format",
        "Partitioned",
        "Hive",
        "tables",
        "HiveQL",
        "Developed",
        "Shell",
        "scripts",
        "DBA",
        "tasks",
        "Maven",
        "jar",
        "files",
        "Map",
        "Reduce",
        "programs",
        "Cluster",
        "Responsible",
        "cluster",
        "maintenance",
        "cluster",
        "nodes",
        "cluster",
        "monitoring",
        "data",
        "backups",
        "Hadoop",
        "log",
        "Environment",
        "HDFS",
        "MapReduce",
        "Pig",
        "Hive",
        "Oozie",
        "Sqoop",
        "Flume",
        "HBase",
        "Java",
        "Maven",
        "Avro",
        "Cloudera",
        "Eclipse",
        "Shell",
        "Scripting",
        "Java",
        "Developer",
        "Financial",
        "Technologies",
        "Limited",
        "Chennai",
        "Tamil",
        "Nadu",
        "December",
        "February",
        "Description",
        "Financial",
        "Technologies",
        "India",
        "Ltd",
        "world",
        "leader",
        "nextgeneration",
        "technology",
        "ventures",
        "innovations",
        "platforms",
        "solutions",
        "markets",
        "marketplaces",
        "price",
        "discovery",
        "transaction",
        "efficiencies",
        "industry",
        "segments",
        "Responsibilities",
        "system",
        "requirements",
        "application",
        "business",
        "team",
        "requirements",
        "Software",
        "Requirement",
        "Specification",
        "document",
        "Architecture",
        "document",
        "User",
        "Interface",
        "UI",
        "operations",
        "validations",
        "AJAX",
        "toolkit",
        "SOAP",
        "company",
        "applications",
        "Web",
        "Service",
        "clients",
        "Log",
        "package",
        "debugging",
        "Clear",
        "Case",
        "version",
        "control",
        "Ensuring",
        "adherence",
        "delivery",
        "schedules",
        "quality",
        "process",
        "projects",
        "Web",
        "Services",
        "rate",
        "summary",
        "WSDL",
        "messages",
        "insurance",
        "plans",
        "module",
        "XML",
        "parsers",
        "data",
        "retrieval",
        "business",
        "components",
        "Spring",
        "features",
        "Dependency",
        "Injection",
        "Auto",
        "wiring",
        "components",
        "DAO",
        "layers",
        "service",
        "layers",
        "Spring",
        "AOP",
        "transaction",
        "application",
        "Wrote",
        "Hibernate",
        "configuration",
        "XML",
        "files",
        "data",
        "persistence",
        "TOAD",
        "SQL",
        "queries",
        "applications",
        "reports",
        "log",
        "tables",
        "migration",
        "Data",
        "Excel",
        "file",
        "Oracle",
        "XML",
        "files",
        "SQL",
        "Server",
        "BCP",
        "DTS",
        "utility",
        "Environment",
        "JavaJ2EE",
        "MVC",
        "Arch",
        "CICS",
        "interaction",
        "HTML",
        "Axis",
        "SOAP",
        "Servlets",
        "Web",
        "services",
        "Restful",
        "Web",
        "Services",
        "Sybase",
        "Spring",
        "DB2",
        "RAD",
        "Rational",
        "Clear",
        "case",
        "WCF",
        "AJAX",
        "Toad",
        "Java",
        "Developer",
        "Rockwell",
        "Collins",
        "Hyderabad",
        "Telangana",
        "January",
        "November",
        "Description",
        "Rockwell",
        "Collins",
        "NYSE",
        "COL",
        "leader",
        "aviation",
        "highintegrity",
        "solutions",
        "customers",
        "world",
        "day",
        "pilots",
        "corners",
        "earth",
        "warfighters",
        "battle",
        "millions",
        "messages",
        "airlines",
        "airports",
        "passengers",
        "journey",
        "Responsibilities",
        "modules",
        "components",
        "design",
        "patterns",
        "clients",
        "business",
        "requirements",
        "application",
        "modules",
        "spring",
        "Hibernate",
        "frameworks",
        "frontend",
        "Swings",
        "Spring",
        "MVC",
        "framework",
        "Tag",
        "libraries",
        "Custom",
        "Tag",
        "Libraries",
        "development",
        "Presentation",
        "Tier",
        "JSP",
        "pages",
        "AJAX",
        "Custom",
        "Tags",
        "JSP",
        "Tag",
        "Lists",
        "HTML",
        "JavaScript",
        "JQuery",
        "Hibernate",
        "classes",
        "ORM",
        "principles",
        "spring",
        "configuration",
        "files",
        "application",
        "context",
        "application",
        "resources",
        "application",
        "files",
        "JavaJ2EE",
        "patterns",
        "Model",
        "View",
        "Controller",
        "MVC",
        "Business",
        "Delegate",
        "Session",
        "faade",
        "Service",
        "Locator",
        "Data",
        "Transfer",
        "Data",
        "Access",
        "Singleton",
        "factory",
        "patterns",
        "JUnit",
        "Testing",
        "Java",
        "Classes",
        "Waterfall",
        "methodology",
        "Maven",
        "build",
        "scripts",
        "Setup",
        "Logging",
        "framework",
        "Integration",
        "Application",
        "services",
        "Units",
        "integration",
        "bug",
        "fixing",
        "test",
        "cases",
        "bugs",
        "User",
        "Testing",
        "changes",
        "server",
        "version",
        "control",
        "deliverables",
        "development",
        "streams",
        "SVN",
        "Environment",
        "JavaJDK",
        "J2EE",
        "Spring",
        "Spring",
        "MVC",
        "Hibernate",
        "Eclipse",
        "Tomcat",
        "XML",
        "JSTL",
        "JavaScript",
        "Maven2",
        "Web",
        "Services",
        "JQuery",
        "SVN",
        "JUnit",
        "Log4J",
        "Windows",
        "Oracle",
        "Development",
        "Methodologies",
        "M",
        "Oziee",
        "Hue",
        "SOAP",
        "UI",
        "Reporting",
        "Tools",
        "MS",
        "Office",
        "VisioOutlook",
        "Crystal",
        "Reports",
        "XI",
        "SSRS",
        "Cognos",
        "Microsoft",
        "SQL",
        "Server",
        "MySQL",
        "Oracle",
        "g",
        "DB2",
        "Teradata",
        "Netezza",
        "Operating",
        "Systems",
        "versions",
        "Windows",
        "UNIX",
        "LINUX",
        "Macintosh",
        "HD",
        "Sun",
        "Solaris",
        "Education",
        "Bachelors",
        "Skills",
        "Cassandra",
        "Hdfs",
        "Impala",
        "Mapreduce",
        "Oozie",
        "Sqoop",
        "Hbase",
        "Kafka",
        "Flume",
        "Hadoop",
        "Mongodb",
        "Splunk",
        "Tableau",
        "server",
        "C",
        "Hadoop",
        "Hbase",
        "Hive",
        "Json",
        "Mapreduce",
        "Pig",
        "Additional",
        "Information",
        "TECHNICAL",
        "SKILLS",
        "Hadoop",
        "Technologies",
        "Hadoop",
        "HDFS",
        "YARN",
        "MapReduce",
        "Hive",
        "Pig",
        "Impala",
        "Sqoop",
        "Flume",
        "Spark",
        "Kafka",
        "Storm",
        "Drill",
        "Zookeeperand",
        "Oozie",
        "Languages",
        "HTML5",
        "DHTML",
        "WSDL",
        "CSS3",
        "C",
        "C",
        "XML",
        "RR",
        "Studio",
        "SAS",
        "Schemas",
        "JSON",
        "Ajax",
        "Java",
        "Scala",
        "Python",
        "Shell",
        "Scripting",
        "Big",
        "Data",
        "Platforms",
        "Hortonworks",
        "Cloudera",
        "SQL",
        "Databases",
        "Cassandra",
        "HBase",
        "MongoDB",
        "MariaDB",
        "Business",
        "Intelligence",
        "Tools",
        "Tableau",
        "server",
        "Tableau",
        "Reader",
        "Tableau",
        "Splunk",
        "SAP",
        "Business",
        "QlikView",
        "Amazon",
        "Redshift",
        "Azure",
        "Data",
        "Warehouse",
        "Development",
        "Tools",
        "Microsoft",
        "SQL",
        "Studio",
        "Eclipse",
        "NetBeans"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T22:54:46.348165",
    "resume_data": "Big dataHadoop Developer Big dataHadoop span lDeveloperspan Big dataHadoop Developer Bank of America Avenel NJ Above 8 years of experience in Analysis Design Development Testing Implementation Maintenance and Enhancements on various IT Projects and experience in Big Data in implementing endtoend Hadoop solutions Experience in working in environments using Agile SCRUM RUP and TestDriven development methodologies Good Knowledge in Amazon AWS concepts like EMR and EC2 web services which provides fast and efficient processing Extensive experience in installing configuring and using ecosystem components like Hadoop MapReduce HDFS Sqoop Pig Hive Impala Spark Expertise in using J2EE application servers such as IBM Web Sphere JBoss and web servers like Apache Tomcat Experience in different Hadoop distributions like Cloudera CDH3 CDH4 and Horton Works Distributions HDP Experience in analyzing data using HIVEQL PIG Latin and custom Map Reduce programs in JAVA Extending HIVE and PIG core functionality by using custom UDFs Experienced in configuring and administering the Hadoop Cluster using major Hadoop Distributions like Apache Hadoop and Cloudera Diverse experience utilizing Java tools in business Web and clientserver environments including Java Platform J2EE EJB JSP Java Servlets Struts and Java database Connectivity JDBC technologies Good understanding in integration of various data sources like RDBMS Spreadsheets Text files JSON and XML files Good working experience on using Sqoop to import data into HDFS from RDBMS and viceversa Implemented Service Oriented Architecture SOA using Web Services and JMS Java Messaging Service Implemented J2EE Design Patterns such as MVC Session Faade DAO DTO Singleton Pattern Front Controller and Business Delegate Experienced in developing web services with XML based protocols such as SOAP Axis UDDI and WSDL Experienced in MVC Model View Controller architecture and various J2EE design patterns like singleton and factory design patterns Extensive experience in loading and analyzing large datasets with Hadoop framework Map Reduce HDFS PIG HIVE Flume Sqoop SPARK Impala NoSQL databases like Mongo DB HBase Cassandra Solid understanding of Hadoop MRV1 and Hadoop MRV2 or YARN Architecture Hands on experience in configuring and administering the Hadoop Cluster using major Hadoop Distributions like Apache Hadoop and Cloudera Good knowledge in SQL and PLSQL to write Stored Procedures and Functions and writing unit test cases using JUnit Extensive experience in Extraction Transformation and Loading ETL of data from multiple sources into Data Warehouse and Data Mart Strong knowledge in Object oriented designanalysis UML modeling Classic design patterns and J2EE patterns Hands on experience working with databases like Oracle 12g SQL Server 2010 and MySQL Hands on experience on the entire latest UI stack including HTML CSS mobile friendly responsive design usercentric design etc Experience in developing webbased enterprise applications using Java J2EE Servlets JSP EJB JDBC Hibernate Spring IOC Spring AOP Spring MVC Spring Web Flow Spring Boot Spring Security Spring Batch Spring Integration Web Services SOAP and REST and ORM frameworks like Hibernate Strong knowledge on Hadoopeco systems including HDFS Hive Oozie HBase Pig Sqoop Zookeeper etc Extensive experience with advanced J2EE Frameworks such as spring Struts JSF and Hibernate Expertise in JavaScript JavaScriptMVC patterns ObjectOrientedJavaScriptDesign Patterns and AJAX calls Installation configuration and administration experience in Big Data platforms Cloudera Manager of Cloudera MCS of MapR Expertise in using XML related technologies such as XML DTD XSD XPATH XSLT DOM SAX JAXP JSON and JAXB Experience in using ANT and Maven for building and deploying the projects in servers and also using Junit and log4j for debugging Work Experience Big dataHadoop Developer Bank of America August 2018 to Present Description Bank of America Merrill Lynch is the marketing name for the global banking and global markets businesses of Bank of America Corporation Lending derivatives and other commercial banking activities are performed globally by banking affiliates of Bank of America Corporation including Bank of America NA and Member FDIC Responsibilities Responsible for installation and configuration of Hive Pig Hbase and Sqoop on the Hadoop cluster and created hive tables to store the processed results in a tabular format Configured Spark Streaming to receive real time data from the Apache Kafka and store the stream data to HDFS using Scala Developed the Sqoop scripts to make the interaction between Hive and vertica Database Processed data into HDFS by developing solutions and analyzed the data using Map Reduce PIG and Hive to produce summary results from Hadoop to downstream systems Build servers using AWS Importing volumes launching EC2 creating security groups autoscaling load balancers Route 53 SES and SNS in the defined virtual private connection Written Map Reduce code to process and parsing the data from various sources and storing parsed data into HBase and Hive using HBaseHive Integration Streamed AWS log group into Lambda function to create service now incident Involved in loading and transforming large sets of Structured SemiStructured and Unstructured data and analyzed them by running Hive queries and Pig scripts Created Managed tables and External tables in Hive and loaded data from HDFS Developed Spark code by using Scala and SparkSQL for faster processing and testing and performed complex HiveQL queries on Hive tables Scheduled several times based Oozie workflow by developing Python scripts Developed Pig Latin scripts using operators such as LOAD STORE DUMP FILTER DISTINCT FOREACH GENERATE GROUP COGROUP ORDER LIMIT UNION SPLIT to extract data from data files to load into HDFS Exporting the data using Sqoop to RDBMS servers and processed that data for ETL operations Worked on S3 buckets on AWS to store Cloud Formation Templates and worked on AWS to create EC2 instances Designing ETL Data Pipeline flow to ingest the data from RDBMS source to Hadoop using shell script sqoop package and MySQL Endtoend architecture and implementation of clientserver systems using Scala Akka Java JavaScript and related Linux Optimized the Hive tables using optimization techniques like partitions and bucketing to provide better Used Oozie workflow engine to manage interdependent Hadoop jobs and to automate several types of Hadoop jobs such as Java mapreduce Hive Pig and Sqoop Implementing Hadoop with the AWS EC2 system using a few instances in gathering and analyzing data log files Involved in Spark and Spark Streaming creating RDDs applying operations Transformation and Actions Created partitioned tables and loaded data using both static partition and dynamic partition method Developed custom Apache Spark programs in Scala to analyze and transform unstructured data Handled importing of data from various data sources performed transformations using Hive MapReduce loaded data into HDFS and Extracted the data from Oracle into HDFS using Sqoop Using Kafka on publishsubscribe messaging as a distributed commit log have experienced in its fast scalable and durability Test Driven Development TDD process and extensive experience with Agile and SCRUM programming methodology Implemented POC to migrate Map Reduce jobs into Spark RDD transformations using SCALA Scheduled map reduces jobs in production environment using Oozie scheduler Involved in Cluster maintenance Cluster Monitoring and Troubleshooting Manage and review data backups and log files Designed and implemented map reduce jobs to support distributed processing using java Hive and Apache Pig Analyzing Hadoop cluster and different Big Data analytic tools including Pig Hive HBase and Sqoop Improved the Performance by tuning of HIVE and map reduce Research evaluate and utilize modern technologiestoolsframeworks around Hadoop ecosystem Environment HDFS Map Reduce Hive Sqoop Pig Flume Vertica Oozie Scheduler Java Shell Scripts Teradata Oracle HBase MongoDB Cassandra Cloudera AWS JavaScript JSP Kafka Spark Scala and ETL Python Big dataHadoop Developer Sprint Overland Park KS May 2017 to July 2018 Description Sprint offers all the latest musthave smartphones and tablets from top industryleading manufacturers including Apple Samsung LG HTC exclusively offering the new EssentialPhone and much more Sprint is also your onestopshop for the coolest accessories from fashionforward designers and tech innovators Responsibilities Involved in Installing Configuring Hadoop EcoSystem Cloudera Manager using CDH3 CDH4 Distributions Responsible to manage data coming from different sources and involved in HDFS maintenance and loading of structured and unstructured data Imported data using Sqoop from Teradata using Teradata connector Created Data Pipeline of MapReduce programs using Chained Mappers Implemented Optimized join base by joining different data sets to get top claims based on state using Map Reduce Visualize the HDFS data to the customer using BI tool with the help of HiveODBCDriver Worked on POC of Talend integration with Hadoop where Created Talend Jobs to extract data from Hadoop Imported data using Sqoop to load data fromMySQL to HDFS on a regular basis Worked on social media Facebook Twitter etc data crawling using Java and R language and MongoDB for unstructured data storage Integrated Quartz scheduler with Oozieworkflows to get data from multiple data sources parallels using a fork Created Partitions Buckets based on State to further process using Bucket based Hive joins Experienced with different kind of compression techniques like LZO GZip Snappy Created Hive Generic UDFs to process business logic that varies based on policy Imported Relational Database data using Sqoop into Hive Dynamic partition tables using staging tables Worked on custom Pig Loaders and storage classes to work with a variety of data formats such as JSON and XML file formats Used Oozie workflow engine to manage interdependent Hadoop jobs and to automate several types of Hadoop jobs such as Java mapreduce Hive Pig and Sqoop Developed Unit test cases using JUnit Easy MockandMRUnit testing frameworks Experienced in MonitoringCluster using Clouderamanager Environment Hadoop HDFS HBase Spark MapReduce Tera Data MySQL Java Hive Pig Sqoop Flume Oozie SQL Cloudera Manager Hadoop Developer Center Light Health Bronx NY January 2016 to April 2017 Description CenterLight Healthcare a member of CenterLight Health System is a notforprofit New York State Managed Long Term Care organization Founded in 1985 to provide home healthcare services to the elderly disabled and chronically ill in the New York metropolitan area CenterLight Healthcare today is recognized as an innovator in the managed care field Responsibilities Provided application demo to the client by designing and developing a search engine report analysis trends application administration prototype screens using AngularJS and Bootstrap JS Took the ownership of complete application Design of Java part Hadoop integration Apart from the normal requirement gathering participated in a Business meeting with the client to gather security requirements Assisted with the architect to analyze the existing system and future system Prepared design blue pints and application flow documentation Experienced in managing and reviewing Hadoop log files Load and transform large sets of structured semistructured and unstructured data Responsible to manage data coming from different sources and application Supported Map Reduce Programs those are running on the cluster Responsible for working with Message broker system such as Kafka Extracted data from mainframes and feed to KAFKA and ingested to HBase to perform Analytics Written eventdriven link tracking system to capture user events and feed to KAFKA to push it to HBASE Created MapReduce jobs to extracts the contents from HBase and configured in OOZIE workflow to generate analytical reports Developed the JAX RS web services code using Apache CXF framework to fetch data from SOLR when the user performed the search for documents Participated in SOLR schema and ingested data into SOLR for data indexing Written MapReduce programs to organize the data and ingest the data to suitable for analytics in client specified format Hands on experience in writing python scripts to optimize the performance Implemented Storm builder topologies to perform cleansing operations before moving data into Cassandra Extracted files from Cassandra through Sqoop and placed in HDFS and processed Implemented Bloom filters in Cassandra using keyspace creation Involved in writing Cassandra CQL statements God handson experience in developing concurrency using spark and Cassandra together Involved in writing spark applications using Scala Hands on experience in creating RDDs transformationsand Actions while implementing spark applications Good knowledge in creating data frames using Spark SQL Involved in loading data into Cassandra NoSQL Database Implemented record level atomicity on writes using Cassandra Written PIG Scripts to query and process the Datasets to figure out the patterns of trends by applying clientspecific criteria and configured OOZIE workflows to run the jobs along with the MR jobs Stored the derived the results in HBasefrom analysis and make it available to data ingestion for SOLR for indexing data Involved in integration of java search UI SOLR and HDFS Involved in code deployments using continuous integration tool using Jenkins Documented all the challenges issues involved to deal with the security system and Implemented best practices Created Project structures and configurations according to the project architecture and made it available to the junior developer to continue their work Handled onsite coordinator role to deliver work to offshore Involved in core reviews and application lead supported activities Environment Cassandra Spring 32 Restful services using CXF web services framework spring data SOLR 521 PIG HIVE apache AVRO Map Reduce Sqoop Zookeeper SVN Jenkins windows AD windows KDC Hortonworks distribution of Hadoop 23 YARN Ambari Hadoop developer Engage Point Calverton MD March 2014 to December 2015 Description Engage Point chosen as Agile Development Prequalified Vendor by California Health and Human Services Agency Responsibilities Collected and aggregated large amounts of weblog data from different sources such as webservers mobile and network devices using Apache Flume and stored the data into HDFS for analysis Collecting data from various Flume agents that are imported on various servers using Multihop flow Ingest realtime and nearrealtime NRT streaming data into HDFS using Flume Extensively involved in Installation and configuration of Cloudera distribution Hadoop Name node Secondary Name Node Job Tracker Task Trackers and Data Nodes Developed Map Reduce programs in Java and Sqoop the data from ORACLE database Responsible for building Scalable distributed data solutions using Hadoop Written various Hive and Pig scripts Used various HBase commands and generated different Datasets as per requirements and provided access to the data when required using grant and revoke Created HBase tables to store variable data formats of input data coming from different portfolios Worked on HBase for support enterprise production and loading data into HBase using SQOOP Installed Oozie workflow engine to run multiple Hive and Pig jobs which run independently with time and data availability Expertise in understanding Partitions Bucketing concepts in Hive Experience working with Apache SOLR for indexing and querying Created custom SOLR Query segments to optimize ideal search matching Used Oozie Scheduler system to automate the pipeline workflow and orchestrate the Map Reduce Jobs that extract the data in a timely manner Responsible for loading data from the UNIX file system to HDFS Developed suit of Unit Test Cases for Mapper Reducer and Driver classes using MR Testing library Analyzed the weblog data using the HiveQL integrated Oozie with the rest of the Hadoop stack Utilized cluster coordination services through Zookeeper Worked on the Ingestion of Files into HDFS from remote systems using MFT Got good experience with various NoSQL databases and Comprehensive knowledge in process improvement normalizationdenormalization data extraction data cleansing and data manipulation Developed Pig scripts to convert the data from Text file to Avro format Created Partitioned Hive tables and worked on them using HiveQL Developed Shell scripts to automate routine DBA tasks Used Maven extensively for building jar files of Map Reduce programs and deployed to Cluster Responsible for cluster maintenance adding and removing cluster nodes cluster monitoring and troubleshooting managing and reviewing data backups and Hadoop log files Environment HDFS MapReduce Pig Hive Oozie Sqoop Flume HBase Java Maven Avro Cloudera Eclipse and Shell Scripting Java Developer Financial Technologies Limited Chennai Tamil Nadu December 2012 to February 2014 Description Financial Technologies India Ltd is a world leader in providing nextgeneration technology ventures innovations platforms and solutions for creating digital markets and marketplaces that enable price discovery and transaction efficiencies across industry segments Responsibilities Involved in gathering system requirements for the application and worked with the business team to review the requirements and went through the Software Requirement Specification document and Architecture document Involved in intense User Interface UI operations and clientside validations using AJAX toolkit Used SOAP to expose company applications as a Web Service to outside clients Log package is used for the debugging Used Clear Case for version control Ensuring adherence to delivery schedules and quality process on projects Used Web Services for creating rate summary and used WSDL and SOAP messages for getting insurance plans from the different module and used XML parsers for data retrieval Developed business components and integrated those using Spring features such as Dependency Injection Auto wiring components such as DAO layers and service proxy layers Used Spring AOP to implement Distributed declarative transaction throughout the application Wrote Hibernate configuration XML files to manage data persistence Used TOAD to generate SQL queries for the applications and to see the reports from log tables Involved in the migration of Data from Excel Flat file Oracle XML files to SQL Server by using BCP and DTS utility Environment JavaJ2EE MVC Arch with CICS interaction HTML Axis SOAP Servlets Web services Restful Web Services Sybase Spring DB2 RAD Rational Clear case WCF AJAX Toad Java Developer Rockwell Collins Hyderabad Telangana January 2011 to November 2012 Description Rockwell Collins NYSE COL is a leader in aviation and highintegrity solutions for commercial and military customers around the world Every day we help pilots safely and reliably navigate to the far corners of the earth keep warfighters aware and informed in battle deliver millions of messages for airlines and airports and help passengers stay connected and comfortable throughout their journey Responsibilities Responsible for developing various modules frontend and backend components using several design patterns based on the clients business requirements Designed and Developed application modules using spring and Hibernate frameworks Designed and developed the frontend with Swings and Spring MVC framework Tag libraries and Custom Tag Libraries and development of Presentation Tier using JSP pages integrating AJAX Custom Tags JSP Tag Lists HTML JavaScript and JQuery Used Hibernate to develop persistent classes following ORM principles Deployed spring configuration files such as application context application resources and application files Used JavaJ2EE patterns like Model View Controller MVC Business Delegate Session faade Service Locator Data Transfer Objects Data Access Objects Singleton and factory patterns Used JUnit for Testing Java Classes Used Waterfall methodology Worked with Maven for build scripts and Setup the Log4J Logging framework Involved in the Integration of the Application with other services Involved in Units integration bug fixing and testing with test cases Fixed the bugs reported in User Testing and deployed the changes to the server Managing the version control for the deliverables by streamlining and rebasing the development streams of the SVN Environment JavaJDK J2EE Spring 25 Spring MVC Hibernate Eclipse Tomcat XML JSTL JavaScript Maven2 Web Services JQuery SVN JUnit Log4J Windows Oracle Development Methodologies 2010 to 2012 M Oziee Hue SOAP UI Reporting Tools MS Office WordExcelPowerPoint VisioOutlook Crystal Reports XI SSRS Cognos 7060 Databases Microsoft SQL Server 200820102012 MySQL 4x5x Oracle 11g 12c DB2 Teradata Netezza Operating Systems All versions of Windows UNIX LINUX Macintosh HD Sun Solaris Education Bachelors Skills Cassandra Hdfs Impala Mapreduce Oozie Sqoop Hbase Kafka Flume Hadoop Mongodb Splunk Tableau server C Hadoop Hbase Hive Json Mapreduce Pig Additional Information TECHNICAL SKILLS Hadoop Technologies Hadoop HDFS YARN MapReduce Hive Pig Impala Sqoop Flume Spark Kafka Storm Drill Zookeeperand Oozie Languages HTML5 DHTML WSDL CSS3 C C XML RR Studio SAS Schemas JSON Ajax Java Scala Python Shell Scripting Big Data Platforms Hortonworks Cloudera NO SQL Databases Cassandra HBase MongoDB MariaDB Business Intelligence Tools Tableau server Tableau Reader Tableau Splunk SAP Business Objects QlikView Amazon Redshift or Azure Data Warehouse Development Tools Microsoft SQL Studio IntelliJ Eclipse NetBeans",
    "unique_id": "7cacd8f6-90f8-4b21-a3b0-dd20c857e297"
}