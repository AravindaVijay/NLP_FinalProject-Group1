{
    "clean_data": "Database Developer Administrator span lDatabasespan Developerspan lAdministratorspan Database Developer Administrator Santa Clara CA Work Experience Database Developer Administrator ADP June 2013 to June 2016 Worked on live nodes Hadoop cluster running on CDH55 Hands on experience in Hadoop components like HDFS MapReduce Job Tracker Name Node Data Node and Task Tracker Involved in the process of load transform and analyze data from various sources into HDFS Hadoop Distributed File System using Hive Pig and Sqoop Created Hive tables based on the business requirements and Hive queries Pig scripts were used to analyze the large data sets Generated reports based on Hive queries and ingested data using Apache Sqoop Extracted the data from SQL Teradata into HDFS using Sqoop Developed multiple Kafka Producers and Consumers as per the software requirement specifications Developed UDFs in Java and Python to use in PIG and HIVE queries Developed multiple Kafka Producers and Consumers as per the software requirement specifications Developed Spark code using Scala and SparkSQLStreaming for faster processing of data Handled importing of data from various sources performed transformations using Hive Spark and loaded data into HDFS Worked on Payroll and Time Management services Created databases of different versions 10g and 11g RAC based on the clients requirement Performed clones and migrations from one database to another Experience in working with the upgrade and downgrade of the databases of different versions Wrote automation scripts for the database monitoring sanity checks and other routine tasks Worked on different RMAN recovery scenarios using hot backup and point in time recoveries Performed sanity check of the databases which includes backup job alert log file system monitoring and troubleshooting the issues and perform several system admin tasks Effectively handled all Customerend user requests like user creation refreshes other tasks by strictly adhering to defined SLAsSOWs Worked on deployments which involved analysis of complex scripts Additionally wrote sub queries for improving their efficiency Setting up Golden Gate for replication and fixing the issues related to it Tuning the database as the load or user base grows with time Involved in maintenance of RAC environment of multi nodes with ASM file system using clusterware Developed stored procedures Triggers Joins views and synonyms for databases OnCall Support Provided oncall support for P1P2 Prioritized databases and interacting with customers and also providing regular updates to Management on a parallel management bridge Education Bachelor of Technology in Science Jawaharlal Nehru Technological University August 2009 to May 2013 Skills SQL 3 years Hadoop 2 years Oracle 3 years Spark 2 years Java 1 year Python 2 years CertificationsLicenses Oracle Certified Associate11G Additional Information Have 3 years of experience in Database field and seeking opportunities in an organization to leverage my technical knowledge and hone my skills that contribute to the technology industry A selfmotivated responsible reliable team player with strong technical expertise and excellent communication skills Hands on experience with Hadoop ecosystem components like HDFS MapReduce Pig Hive Oozie Sqoop Spark Zookeeper and HBase Knowledge on various Hadoop distributions like Cloudera Hoartonworks MapR Comprehensive understanding of HDFS architecture its components like Job Tracker Task Tracker Name Node and Data Node Hands on experience with the data extraction transformation and load using Hive Pig and HBase Good experience in analyzing data using Hive Query Language Pig Latin and custom MapReduce programs in Java along with using User Defined Functions Setup ingestion systems for data feeds using Apache Kafka into HDFS Hands on experiences on MR Hive Beeline SQLite Pig languages Experience in capturing data and importing it to HDFS using Flume and Kafka for semistructured data and Sqoop for existing relational databases Fair knowledge of NoSQL database management system like HBase MongoDB Experience in designing RDBMS schemas writing SQL queries to maintain and extract data Good knowledge of ETL process and data warehousing concepts Worked on various execution engines like Tez Spark along with MR Implemented Daily Cron jobs that automate parallel tasks of loading the data into HDFS using Oozie cocoordinator jobs Strong passion and expertise in Linux and Windows Operating Systems Experience in fulfilling DBA daily activities including schema management space management monitoring and scheduling jobs Experience with Oracle Enterprise Manager RAC and RMAN Experienced in developing UNIX shell scripts and performing database health checks Experience in design and maintenance of Oracle Data guard and RAC administration Proficiency in Oracle database BackupRecovery RMAN Installation Maintenance ExpImpData pump PLSQL Programming Good knowledge on OEM Grid Control for monitoring and user management Experience in working on Service now Remedy tool CA service desk Incident management Change management Expertise in understanding the ECommerce Database Infrastructure and worked on the Automation scripts for Database Security environment Good knowledge in developing Complex database objects like Stored Procedures Functions Packages and Triggers using SQL and PLSQL",
    "entities": [
        "Hive Query Language",
        "HDFS MapReduce Job Tracker Name",
        "Generated",
        "Time Management services Created",
        "ETL",
        "HDFS MapReduce Pig Hive Oozie Sqoop Spark Zookeeper",
        "Windows Operating Systems",
        "Database",
        "RAC",
        "G Additional Information",
        "Sqoop",
        "HIVE",
        "Node Data",
        "Oracle Data",
        "UNIX",
        "Triggers",
        "Oracle database BackupRecovery RMAN Installation Maintenance",
        "HBase Good",
        "MR Implemented Daily Cron",
        "PIG",
        "Task Tracker Involved",
        "Job Tracker Task Tracker",
        "Consumers",
        "RMAN Experienced",
        "Oozie",
        "Linux",
        "Oracle Enterprise",
        "Customerend",
        "ASM",
        "SQL",
        "Hadoop",
        "Apache Sqoop Extracted",
        "HDFS Worked on Payroll",
        "Nehru Technological University",
        "Database Developer Administrator",
        "Hive Pig",
        "MapReduce",
        "Hive Spark",
        "HDFS Hadoop Distributed File System",
        "NoSQL",
        "desk Incident management Change management Expertise",
        "Automation",
        "Database Security",
        "Fair",
        "Skills SQL",
        "HBase",
        "Hive",
        "Node",
        "Stored Procedures Functions Packages"
    ],
    "experience": "Experience Database Developer Administrator ADP June 2013 to June 2016 Worked on live nodes Hadoop cluster running on CDH55 Hands on experience in Hadoop components like HDFS MapReduce Job Tracker Name Node Data Node and Task Tracker Involved in the process of load transform and analyze data from various sources into HDFS Hadoop Distributed File System using Hive Pig and Sqoop Created Hive tables based on the business requirements and Hive queries Pig scripts were used to analyze the large data sets Generated reports based on Hive queries and ingested data using Apache Sqoop Extracted the data from SQL Teradata into HDFS using Sqoop Developed multiple Kafka Producers and Consumers as per the software requirement specifications Developed UDFs in Java and Python to use in PIG and HIVE queries Developed multiple Kafka Producers and Consumers as per the software requirement specifications Developed Spark code using Scala and SparkSQLStreaming for faster processing of data Handled importing of data from various sources performed transformations using Hive Spark and loaded data into HDFS Worked on Payroll and Time Management services Created databases of different versions 10 g and 11 g RAC based on the clients requirement Performed clones and migrations from one database to another Experience in working with the upgrade and downgrade of the databases of different versions Wrote automation scripts for the database monitoring sanity checks and other routine tasks Worked on different RMAN recovery scenarios using hot backup and point in time recoveries Performed sanity check of the databases which includes backup job alert log file system monitoring and troubleshooting the issues and perform several system admin tasks Effectively handled all Customerend user requests like user creation refreshes other tasks by strictly adhering to defined SLAsSOWs Worked on deployments which involved analysis of complex scripts Additionally wrote sub queries for improving their efficiency Setting up Golden Gate for replication and fixing the issues related to it Tuning the database as the load or user base grows with time Involved in maintenance of RAC environment of multi nodes with ASM file system using clusterware Developed stored procedures Triggers Joins views and synonyms for databases OnCall Support Provided oncall support for P1P2 Prioritized databases and interacting with customers and also providing regular updates to Management on a parallel management bridge Education Bachelor of Technology in Science Jawaharlal Nehru Technological University August 2009 to May 2013 Skills SQL 3 years Hadoop 2 years Oracle 3 years Spark 2 years Java 1 year Python 2 years CertificationsLicenses Oracle Certified Associate11 G Additional Information Have 3 years of experience in Database field and seeking opportunities in an organization to leverage my technical knowledge and hone my skills that contribute to the technology industry A selfmotivated responsible reliable team player with strong technical expertise and excellent communication skills Hands on experience with Hadoop ecosystem components like HDFS MapReduce Pig Hive Oozie Sqoop Spark Zookeeper and HBase Knowledge on various Hadoop distributions like Cloudera Hoartonworks MapR Comprehensive understanding of HDFS architecture its components like Job Tracker Task Tracker Name Node and Data Node Hands on experience with the data extraction transformation and load using Hive Pig and HBase Good experience in analyzing data using Hive Query Language Pig Latin and custom MapReduce programs in Java along with using User Defined Functions Setup ingestion systems for data feeds using Apache Kafka into HDFS Hands on experiences on MR Hive Beeline SQLite Pig languages Experience in capturing data and importing it to HDFS using Flume and Kafka for semistructured data and Sqoop for existing relational databases Fair knowledge of NoSQL database management system like HBase MongoDB Experience in designing RDBMS schemas writing SQL queries to maintain and extract data Good knowledge of ETL process and data warehousing concepts Worked on various execution engines like Tez Spark along with MR Implemented Daily Cron jobs that automate parallel tasks of loading the data into HDFS using Oozie cocoordinator jobs Strong passion and expertise in Linux and Windows Operating Systems Experience in fulfilling DBA daily activities including schema management space management monitoring and scheduling jobs Experience with Oracle Enterprise Manager RAC and RMAN Experienced in developing UNIX shell scripts and performing database health checks Experience in design and maintenance of Oracle Data guard and RAC administration Proficiency in Oracle database BackupRecovery RMAN Installation Maintenance ExpImpData pump PLSQL Programming Good knowledge on OEM Grid Control for monitoring and user management Experience in working on Service now Remedy tool CA service desk Incident management Change management Expertise in understanding the ECommerce Database Infrastructure and worked on the Automation scripts for Database Security environment Good knowledge in developing Complex database objects like Stored Procedures Functions Packages and Triggers using SQL and PLSQL",
    "extracted_keywords": [
        "Database",
        "Developer",
        "Administrator",
        "span",
        "lDatabasespan",
        "Developerspan",
        "lAdministratorspan",
        "Database",
        "Developer",
        "Administrator",
        "Santa",
        "Clara",
        "CA",
        "Work",
        "Experience",
        "Database",
        "Developer",
        "Administrator",
        "ADP",
        "June",
        "June",
        "nodes",
        "Hadoop",
        "cluster",
        "CDH55",
        "Hands",
        "experience",
        "Hadoop",
        "components",
        "MapReduce",
        "Job",
        "Tracker",
        "Name",
        "Node",
        "Data",
        "Node",
        "Task",
        "Tracker",
        "process",
        "load",
        "transform",
        "data",
        "sources",
        "HDFS",
        "Hadoop",
        "Distributed",
        "File",
        "System",
        "Hive",
        "Pig",
        "Sqoop",
        "Created",
        "Hive",
        "tables",
        "business",
        "requirements",
        "Hive",
        "Pig",
        "scripts",
        "data",
        "sets",
        "reports",
        "Hive",
        "queries",
        "data",
        "Apache",
        "Sqoop",
        "data",
        "SQL",
        "Teradata",
        "HDFS",
        "Sqoop",
        "Developed",
        "Kafka",
        "Producers",
        "Consumers",
        "software",
        "requirement",
        "specifications",
        "UDFs",
        "Java",
        "Python",
        "PIG",
        "HIVE",
        "Kafka",
        "Producers",
        "Consumers",
        "software",
        "requirement",
        "specifications",
        "Developed",
        "Spark",
        "code",
        "Scala",
        "SparkSQLStreaming",
        "processing",
        "data",
        "importing",
        "data",
        "sources",
        "transformations",
        "Hive",
        "Spark",
        "data",
        "HDFS",
        "Worked",
        "Payroll",
        "Time",
        "Management",
        "services",
        "databases",
        "versions",
        "g",
        "g",
        "RAC",
        "clients",
        "requirement",
        "Performed",
        "clones",
        "migrations",
        "database",
        "Experience",
        "upgrade",
        "downgrade",
        "databases",
        "versions",
        "automation",
        "scripts",
        "database",
        "sanity",
        "checks",
        "tasks",
        "RMAN",
        "recovery",
        "scenarios",
        "backup",
        "point",
        "time",
        "recoveries",
        "Performed",
        "sanity",
        "check",
        "databases",
        "job",
        "alert",
        "log",
        "file",
        "system",
        "monitoring",
        "issues",
        "system",
        "admin",
        "tasks",
        "Customerend",
        "user",
        "requests",
        "user",
        "creation",
        "tasks",
        "SLAsSOWs",
        "deployments",
        "analysis",
        "scripts",
        "sub",
        "efficiency",
        "Golden",
        "Gate",
        "replication",
        "issues",
        "database",
        "load",
        "user",
        "base",
        "time",
        "maintenance",
        "RAC",
        "environment",
        "multi",
        "nodes",
        "ASM",
        "file",
        "system",
        "clusterware",
        "Developed",
        "procedures",
        "Triggers",
        "Joins",
        "views",
        "synonyms",
        "databases",
        "OnCall",
        "Support",
        "oncall",
        "support",
        "P1P2",
        "Prioritized",
        "databases",
        "customers",
        "updates",
        "Management",
        "management",
        "bridge",
        "Education",
        "Bachelor",
        "Technology",
        "Science",
        "Jawaharlal",
        "Nehru",
        "Technological",
        "University",
        "August",
        "May",
        "Skills",
        "SQL",
        "years",
        "Hadoop",
        "years",
        "Oracle",
        "years",
        "Spark",
        "years",
        "Java",
        "year",
        "Python",
        "years",
        "CertificationsLicenses",
        "Oracle",
        "Certified",
        "Associate11",
        "G",
        "Additional",
        "Information",
        "years",
        "experience",
        "Database",
        "field",
        "opportunities",
        "organization",
        "knowledge",
        "skills",
        "technology",
        "industry",
        "team",
        "player",
        "expertise",
        "communication",
        "Hands",
        "experience",
        "Hadoop",
        "ecosystem",
        "components",
        "MapReduce",
        "Pig",
        "Hive",
        "Oozie",
        "Sqoop",
        "Spark",
        "Zookeeper",
        "HBase",
        "Knowledge",
        "Hadoop",
        "distributions",
        "Cloudera",
        "Hoartonworks",
        "MapR",
        "Comprehensive",
        "understanding",
        "HDFS",
        "architecture",
        "components",
        "Job",
        "Tracker",
        "Task",
        "Tracker",
        "Name",
        "Node",
        "Data",
        "Node",
        "Hands",
        "experience",
        "data",
        "extraction",
        "transformation",
        "load",
        "Hive",
        "Pig",
        "HBase",
        "experience",
        "data",
        "Hive",
        "Query",
        "Language",
        "Pig",
        "Latin",
        "MapReduce",
        "programs",
        "Java",
        "User",
        "Defined",
        "Functions",
        "Setup",
        "ingestion",
        "systems",
        "data",
        "feeds",
        "Apache",
        "Kafka",
        "HDFS",
        "Hands",
        "experiences",
        "MR",
        "Hive",
        "Beeline",
        "SQLite",
        "Pig",
        "Experience",
        "data",
        "HDFS",
        "Flume",
        "Kafka",
        "data",
        "Sqoop",
        "knowledge",
        "NoSQL",
        "database",
        "management",
        "system",
        "HBase",
        "MongoDB",
        "Experience",
        "RDBMS",
        "schemas",
        "SQL",
        "data",
        "knowledge",
        "ETL",
        "process",
        "data",
        "warehousing",
        "concepts",
        "execution",
        "engines",
        "Tez",
        "Spark",
        "MR",
        "Daily",
        "Cron",
        "jobs",
        "tasks",
        "data",
        "HDFS",
        "Oozie",
        "cocoordinator",
        "jobs",
        "passion",
        "expertise",
        "Linux",
        "Windows",
        "Operating",
        "Systems",
        "Experience",
        "DBA",
        "activities",
        "schema",
        "management",
        "space",
        "management",
        "monitoring",
        "scheduling",
        "jobs",
        "Experience",
        "Oracle",
        "Enterprise",
        "Manager",
        "RAC",
        "RMAN",
        "UNIX",
        "shell",
        "scripts",
        "database",
        "health",
        "checks",
        "Experience",
        "design",
        "maintenance",
        "Oracle",
        "Data",
        "guard",
        "RAC",
        "administration",
        "Proficiency",
        "Oracle",
        "database",
        "BackupRecovery",
        "RMAN",
        "Installation",
        "Maintenance",
        "PLSQL",
        "Programming",
        "knowledge",
        "OEM",
        "Grid",
        "Control",
        "monitoring",
        "user",
        "management",
        "Experience",
        "Service",
        "Remedy",
        "tool",
        "CA",
        "service",
        "desk",
        "Incident",
        "management",
        "Change",
        "management",
        "Expertise",
        "ECommerce",
        "Database",
        "Infrastructure",
        "Automation",
        "scripts",
        "Database",
        "Security",
        "environment",
        "knowledge",
        "database",
        "Stored",
        "Procedures",
        "Functions",
        "Packages",
        "Triggers",
        "SQL",
        "PLSQL"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T20:59:14.298135",
    "resume_data": "Database Developer Administrator span lDatabasespan Developerspan lAdministratorspan Database Developer Administrator Santa Clara CA Work Experience Database Developer Administrator ADP June 2013 to June 2016 Worked on live nodes Hadoop cluster running on CDH55 Hands on experience in Hadoop components like HDFS MapReduce Job Tracker Name Node Data Node and Task Tracker Involved in the process of load transform and analyze data from various sources into HDFS Hadoop Distributed File System using Hive Pig and Sqoop Created Hive tables based on the business requirements and Hive queries Pig scripts were used to analyze the large data sets Generated reports based on Hive queries and ingested data using Apache Sqoop Extracted the data from SQL Teradata into HDFS using Sqoop Developed multiple Kafka Producers and Consumers as per the software requirement specifications Developed UDFs in Java and Python to use in PIG and HIVE queries Developed multiple Kafka Producers and Consumers as per the software requirement specifications Developed Spark code using Scala and SparkSQLStreaming for faster processing of data Handled importing of data from various sources performed transformations using Hive Spark and loaded data into HDFS Worked on Payroll and Time Management services Created databases of different versions 10g and 11g RAC based on the clients requirement Performed clones and migrations from one database to another Experience in working with the upgrade and downgrade of the databases of different versions Wrote automation scripts for the database monitoring sanity checks and other routine tasks Worked on different RMAN recovery scenarios using hot backup and point in time recoveries Performed sanity check of the databases which includes backup job alert log file system monitoring and troubleshooting the issues and perform several system admin tasks Effectively handled all Customerend user requests like user creation refreshes other tasks by strictly adhering to defined SLAsSOWs Worked on deployments which involved analysis of complex scripts Additionally wrote sub queries for improving their efficiency Setting up Golden Gate for replication and fixing the issues related to it Tuning the database as the load or user base grows with time Involved in maintenance of RAC environment of multi nodes with ASM file system using clusterware Developed stored procedures Triggers Joins views and synonyms for databases OnCall Support Provided oncall support for P1P2 Prioritized databases and interacting with customers and also providing regular updates to Management on a parallel management bridge Education Bachelor of Technology in Science Jawaharlal Nehru Technological University August 2009 to May 2013 Skills SQL 3 years Hadoop 2 years Oracle 3 years Spark 2 years Java 1 year Python 2 years CertificationsLicenses Oracle Certified Associate11G Additional Information Have 3 years of experience in Database field and seeking opportunities in an organization to leverage my technical knowledge and hone my skills that contribute to the technology industry A selfmotivated responsible reliable team player with strong technical expertise and excellent communication skills Hands on experience with Hadoop ecosystem components like HDFS MapReduce Pig Hive Oozie Sqoop Spark Zookeeper and HBase Knowledge on various Hadoop distributions like Cloudera Hoartonworks MapR Comprehensive understanding of HDFS architecture its components like Job Tracker Task Tracker Name Node and Data Node Hands on experience with the data extraction transformation and load using Hive Pig and HBase Good experience in analyzing data using Hive Query Language Pig Latin and custom MapReduce programs in Java along with using User Defined Functions Setup ingestion systems for data feeds using Apache Kafka into HDFS Hands on experiences on MR Hive Beeline SQLite Pig languages Experience in capturing data and importing it to HDFS using Flume and Kafka for semistructured data and Sqoop for existing relational databases Fair knowledge of NoSQL database management system like HBase MongoDB Experience in designing RDBMS schemas writing SQL queries to maintain and extract data Good knowledge of ETL process and data warehousing concepts Worked on various execution engines like Tez Spark along with MR Implemented Daily Cron jobs that automate parallel tasks of loading the data into HDFS using Oozie cocoordinator jobs Strong passion and expertise in Linux and Windows Operating Systems Experience in fulfilling DBA daily activities including schema management space management monitoring and scheduling jobs Experience with Oracle Enterprise Manager RAC and RMAN Experienced in developing UNIX shell scripts and performing database health checks Experience in design and maintenance of Oracle Data guard and RAC administration Proficiency in Oracle database BackupRecovery RMAN Installation Maintenance ExpImpData pump PLSQL Programming Good knowledge on OEM Grid Control for monitoring and user management Experience in working on Service now Remedy tool CA service desk Incident management Change management Expertise in understanding the ECommerce Database Infrastructure and worked on the Automation scripts for Database Security environment Good knowledge in developing Complex database objects like Stored Procedures Functions Packages and Triggers using SQL and PLSQL",
    "unique_id": "2997a456-f52d-40a7-8aef-429304fe2a54"
}