{
    "clean_data": "Sr Big DataHadoop Developer Sr Big DataHadoop span lDeveloperspan Sr Big DataHadoop Developer NetApp Durham NC Over 9 years of professional IT experience of Big Data Hadoop Ecosystems experience in ingestion storage querying processing and analysis of big data Experience in using Maven for building and deploying J2EE Application archives Jar and War on Web Logic IBM Web Sphere Experience in building Pig scripts to extract transform and load data onto HDFS for processing Experience in importing and exporting data using Sqoop from HDFS to Relational Database Systems and viceversa Experience in Hadoop Shell commands writing MapReduce Programs verifying managing and reviewing Hadoop Log files Strong background in JavaJ2EE environments Well experienced in MVC architecture of Spring framework Experience in Big Data analysis using PIG and HIVE and understanding of SQOOP and Puppet Experience with leveraging Hadoop ecosystem components including Pig and Hive for data analysis Sqoop for data migration Oozie for scheduling and HBase as a NoSQL data store Good Exposure on Apache Hadoop MapReduce programming PIG Scripting and Distribute Application and HDFS Expert in developing applications using all J2EE technologies like Servlets JSP JDBC JNDI JMS Experience in Object Oriented Analysis Design OOAD and development of software using UML Methodology good knowledge of J2EE design patterns and Core Java design patterns Hands on experience in application development using Java RDBMS and Linux shell scripting Strong work ethics with desire to succeed and make significant contributions to the organization Load streaming log data from various web servers into HDFS using Flume Experience in deployment of Hadoop Cluster using Puppet tool Experience in scheduling Cron jobs on EMR Kafka and Spark using Clover Server Proficient in using RDMS concepts with Oracle SQL Server and MySQL Hands on experience with build and deploying tools like Maven and GitHub using Bash scripting Hands on experience with spring tool suit for development of Scala Applications Extensive experience working with structured data using Spark SQL Data frames Hive QL optimizing queries and incorporate complex UDFs in business logic Experience working with Text Sequence files XML Parquet JSON ORC AVRO file formats and Click Stream log files Experience working with Data Frames RDD Spark SQL Spark Streaming APIs System Architecture and Infrastructure Planning Experience with Core Java component Collection Generics Inheritance Exception Handling and Multithreading Developed Java applications using various IDEs like Spring Tool Suite and Eclipse Experience in usage of Hadoop distribution like Cloudera and Hortonworks Strong knowledge in working with UNIXLINUX environments writing shell scripts and PLSQL Stored Procedures Good knowledge in using Hibernate for mapping Java classes with database and using Hibernate Query Language HQL Operated on JavaJ2EE systems with different databases which include Oracle MySQL and DB2 Knowledge on implementing Big Data in Amazon Elastic MapReduce Amazon EMR for processing managing Hadoop framework dynamically scalable Amazon EC2 instances Build AWS secured solutions by creating VPC with private and public subnets Extensive experience in Application servers likes Web logic Web Sphere JBoss Glassfish and Web Servers like Apache Tomcat Authorized to work in the US for any employer Work Experience Sr Big DataHadoop Developer NetApp Durham NC US September 2017 to Present Responsibilities Worked as a Sr Big DataHadoop Developer with Hadoop Ecosystems components like HBase Sqoop Zookeeper Hive and Pig with Cloudera Hadoop distribution Worked with data science team to build statistical model with Spark MLLIB and Pyspark Actively involved in designing Hadoop ecosystem pipeline Developed Spark code using Scala and SparkSQLStreaming for faster testing and processing of data Implemented Security in Web Applications using Azure and deployed Web Applications to Azure Worked in Agile development environment having KANBAN methodology Used Kibana which is an open source based browser analytics and search dashboard for Elastic Search Developed Spark code and SparkSQLStreaming for faster testing and processing of data Used Java Persistence API JPA framework for object relational mapping which is based on POJO Classes Responsible for fetching real time data using Kafka and processing using Spark and Scala Worked on Kafka to import real time weblogs and ingested the data to Spark Streaming Developed business logic using Kafka Direct Stream in Spark Streaming and implemented business transformations Actively involved in daily scrum and other design related meetings Maintained Hadoop Hadoop ecosystems and database with updatesupgrades performance tuning and monitoring Extensively used JQuery to provide dynamic User Interface and for the client side validations Responsible for defining the data flow within Hadoop ecosystem and direct the team in implement them Worked extensively with importing metadata into Hive and migrated existing tables and applications to work on Hive and Spark Build largescale data processing systems in data warehousing solutions and work with unstructured data mining on NoSQL Worked with application teams to install operating system Hadoop updates patches version upgrades as required Specified the cluster size allocating Resource pool Distribution of Hadoop by writing the specification texts in JSON File format Created Hive tables and loading and analyzing data using hive queries Wrote Hive Queries for analyzing data in Hive warehouse using Hive Query Language HQL Developed Hive queries to process the data and generate the data cubes for visualizing Involved in running Hadoop jobs for processing millions of records of text data Created and maintained Technical documentation for launching Hadoop Clusters and for executing Hive queries Used Struts which is an open source MVC framework for creating elegant modern java web applications Developed customized Hive UDFs and UDAFs in Java JDBC connectivity with hive development and execution of Pig scripts and Pig UDFs Used Hadoop YARN to perform analytics on data in Hive Developed and maintained batch data flow using HiveQL and Unix scripting Used Oozie and Zookeeper operational services for coordinating cluster and scheduling workflows Involved in converting HiveSQL queries into Spark transformations using Spark RDD Scala and Python Configured Spark streaming to receive real time data from Kafka and store the stream data to HDFS using Scala Continuous coordination with QA team production support team and deployment team Developed SQL scripts using Spark for handling different data sets and verifying the performance over MapReduce jobs Used J2EE design patterns like Factory pattern Singleton Pattern Involved in converting MapReduce programs into Spark transformations using Spark RDDs using Scala and Python Developed and execute data pipeline testing processes and validate business rules and policies Built code for real time data ingestion using Java MapRStreams Kafka and STORM Environment HBase 14 Sqoop 14 Zookeeper 34 Oozie 43 Hive 23 Hadoop 30 Scala 212 Spark 23 Python 37 MS Azure Agile NoSQL JSON JDBC 43 Java Unix MapR Kafka 20 Storm 105 Sr Hadoop Developer 3M Health Troy NY US May 2016 to August 2017 Responsibilities Involved in all phases of Software Development Life Cycle SDLC and Worked on all activities related to the development implementation and support for Hadoop Installed and Configured Apache Hadoop clusters for application development and Hadoop tools like Hive Pig HBase Zookeeper and Sqoop Involved in Agile methodologies daily scrum meetings spring planning Worked on MongoDB by using CRUD Create Read Update and Delete Indexing Replication and Shading features Developing data pipeline using Flume Sqoop Pig and Java MapReduce to ingest customer behavioral data and financial histories into HDFS for analysis Worked on MongoDB HBase NoSQL databases which differ from classic relational databases Worked on creating data models for Cassandra from Existing Oracle data model Designed Column families in Cassandra and Ingested data from RDBMS performed data transformations and then export the transformed data to Cassandra as per the business requirement Responsible for installation and configuration of Hive HBase and Sqoop on the Hadoop cluster Configured Spark Streaming to receive real time data from the Apache Kafka and store the stream data to HDFS using Scala Implemented usage of Amazon EMR for processing Big Data across a Hadoop Cluster of virtual servers on Amazon Elastic Compute Cloud EC2 and Amazon Simple Storage Service S3 Implemented Apache Nifi flow topologies to perform cleansing operations before moving data into HDFS Worked extensively with importing metadata into Hive and migrated existing tables and applications to work on Hive and AWS cloud Implemented a distributed messaging queue to integrate with Cassandra using Apache Kafka and Zookeeper Involved in loading the realtime data to NoSQL database like Cassandra Worked on connecting Cassandra database to the Amazon EMR File System for storing the database in S3 Developed UDFs using Scala Scripts which used in Data framesSQL and RDD in Spark for Data Aggregation queries and writing data back into RDBMS through Sqoop Deployed the project on Amazon EMR with S3 connectivity for setting a backup storage Imported data from AWS S3 and into spark RDD and performed transformations and actions on RDDs Responsible for developing data pipeline with Amazon AWS to extract the data from weblogs and store in Amazon EMR Implemented multiple MapReduce Jobs in java for data cleansing and preprocessing Analyzed large amounts of data sets to determine optimal way to aggregate and report on it Installed and configured local Hadoop Cluster with 3 nodes and set up 4 nodes cluster on EC2 cloud Designed and implemented MapReduce based largescale parallel relationlearning system Developed the code which will create XML files and Flat files with the data retrieved from Databases and XML files Environment Hadoop 30 Hive 23 Pig 017 HBase 14 Zookeeper 34 Sqoop 14 Agile MongoDB 40 Spark 23 Scala 212 Apache Kafka 20 Nifi 17 HDFS AWS Cassandra 311 NoSQL java XML Sr JavaHadoop Developer TMobile Bellevue WA US January 2015 to April 2016 Responsibilities Designed and Developed application modules using spring and Hibernate frameworks Responsible for building scalable distributed data solutions using Hadoop Experienced in loading and transforming of large sets of structured semi structured and unstructured data Used MAVEN for developing build scripts and deploying the application onto WebLogic Implemented Spark RDD transformations to map business analysis and apply actions on top of transformations Developed Spark jobs and Hive Jobs to summarize and transform data Involved in converting HiveSQL queries into Spark transformations using Spark data frames Scala and Python Implemented MVC architecture using Spring Framework Coding involves writing Action ClassesCustom Tag Libraries JSP Expertise in implementing Spark Scala application using higher order functions for both batch and interactive analysis requirement Creating Hive tables with periodic backups writing complex HiveImpala queries to run on Impala Implemented partitioning bucketing and worked on Hive using file formats and compressions techniques with optimizations Involved in designing and developing modules at both Client and Server Side Worked on JDBC framework encapsulated using DAO pattern to connect to the database Developed the UI Screens using JSP and HTML and did the client side validation with the JavaScript Worked on various SOAP and RESTful services used in various internal applications Developed JSP and Java classes for various transactional nontransactional reports of the system using extensive SQL queries Worked on analyzing Hadoop cluster and different big data analytic tools including MapReduce Hive and Spark Implemented Storm topologies to preprocess data before moving into HDFS system Implemented POC to migrate MapReduce programs into Spark transformations using Spark and Scala Involved in configuring builds using Jenkins with Git and used Jenkins to deploy the applications onto Dev QA environments Involved in unit testing system integration testing and enterprise user testing using JUnit Involved in creating Hive tables loading with data and writing Hive queries which runs internally in MapReduce way Developed Shell Perl and Python scripts to automate and provide Control flow to Pig scripts Environment spring 40 Hibernate 507 Hadoop 265 Spark 11 Hive Python 33 Scala Sqoop Flume 131 Impala MapReduce LINUX Sr JavaJ2EE Developer JPMC New York NY US August 2012 to December 2014 Responsibilities As a JavaJ2ee developer involved in backend and frontend developing team Designed and developed various modules of the application with J2EE design architecture frameworks like Spring MVC architecture and Spring Bean Factory using IOC AOP concepts Implemented JavaJ2EE design patterns such as Factory DAO Session Faade and Singleton Used Hibernate in persistence layer and developed POJOs Data Access Object DAO to handle all database operations Used Maven as the build tool GIT for version control Jenkins for Continuous Integration and JIRA as a defect tracking tool Developed the user interface components using HTML CSS JavaScript AJAX JQuery and also created custom tags Implemented the Project structure based on Spring MVC pattern using spring boot Worked on JavaScript to validate input manipulated HTML elements using JavaScript Developed external JavaScript codes that can be used in several different web pages Developed Web pages using JSP HTML CSS Struts Tag libs and AJAX for the Credit Risk module Used Spring Beans to encapsulate business logic and Implemented Application MVC Architecture using Spring MVC framework Developed XMLs JavaScript and Java classes for dynamic HTML generation to perform the server side processing on the client requests Used JUnit framework for unit testing of application and Maven to build the application and deployed on Jetty server Developed unit test cases in JUnit and documented all the test scenarios as per the user specifications Implemented Spring framework based on the Model View Controller design paradigm Involved in development activities using Core Java J2EE Servlets JSP JSF used for creating web application XML and springs Used Spring Framework for Dependency injection and integrated with the Hibernate Developed RESTful Web Services client to consume JSON messages using Spring JMS configuration Developed services using Spring IOC and Hibernate persistence layer with Oracle Database Implemented build script using ANT for compiling building and deploying the application on WebSphere application server Environment J2EE Java Spring MVC 30 POJO Jenkins HTML JavaScript AJAX JQuery CSS XML Maven JUnit Hibernate 428 POJO ANT Oracle 10g Java Developer Valuelabs September 2009 to July 2012 Responsibilities Applied ModelViewController MVC design pattern and Single Tone class design pattern for designing the application Developed JSP pages with Struts and EJB Enterprise JavaBeans for implementing different search pages for transaction of each module Used JavaScript and struts validation framework for client side validation Implemented Ant and Maven build tools to build jar and war files and deployed war files to target servers Responsible for writing Struts action classes Hibernate POJO classes and integrating Struts and Hibernate with spring for processing business needs Developed web application using Struts JSP Servlets and JavaBeans that uses MVC design pattern Used Maven to build the application and deployed on IBM WebSphere Application Server Created bean XML files and row Mappers to map tables and fields in the database Wrote Apache ANT build scripts for building the application and unit test cases using JUnit for performing the unit testing Designed the user interfaces using JSPs developed custom tags and used JSTL Tag lib Involved in creating the Hibernate POJO Objects and mapped using Hibernate Annotations Created UML diagrams use case class sequence and collaboration based on the business requirements Responsible for designing and developing of Object oriented methodologies using UML Developed Web Services to allow communication between the applications using Rest Web Services Created JUnit test cases for unit testing the code at minute level and used Eclipse IDE Used AJAX frameworks like JQuery JSON to develop rich GUIs and also involved in performance tuning the website Developed ANT scripts to build and deploy the application in the JBOSS Application Server Developed MessageDriven beans in collaboration with Java Messaging Service JMS Implemented server pages using Apache Tomcat as application server and Log4j for application logging and debugging Developed the presentation layer and content management framework using HTML and JavaScript Extensively used MVC architecture and JBoss for deployment purposes Created JSP Form Beans for effective way of implementing Model View Controller architecture Developed Servlets to perform business logic and to interact with the database using JDBC Environment JavaScript Ant Maven Struts JavaBeans MVC XML Apache ANT Hibernate 419 POJO JUnit Eclipse JQuery JSON AJAX JBOSS Apache Tomcat HTML Education Bachelors Skills database 9 years Java 9 years JavaScript 6 years JQuery 6 years JSON 6 years Additional Information Hadoop Big Data Cloud Nodes Azure Elastic Cloud Sqoop Flume Yarn Spark Hive Hue Apache Camel SQL MySQL PostgreSQL MongoDB HBase Cassandra and JQuery Technical Skills HadoopBig Data Technologies Hadoop 30 HDFS MapReduce HBase 14 Apache Pig 017 Hive 23 Sqoop 14 Apache Impala 30 Oozie 43 Yarn Apache Flume 18 Kafka 11 Zookeeper 34 Hadoop Distributions Cloudera Hortonworks MapR Cloud AWS Azure Azure SQL Database Azure SQL Data Warehouse Azure Analysis Services HDInsight Azure Data Lake and Data Factory Programming Language Java Scala 212 Python 36 SQL PLSQL Shell Scripting Storm 10 JSP Servlets Frameworks Spring 505 Hibernate 52 Struts 13 JSF EJB JMS Web Technologies HTML5 CSS JavaScript JQuery 33 Bootstrap 41 XML JSON AJAX Databases Oracle 12c11g SQL Database Tools TOAD SQL PLUS SQL Operating Systems Linux Unix Windows 1087 IDE and Tools Eclipse 47 NetBeans 82 IntelliJ Maven NoSQL Databases HBase 14 Cassandra 311 MongoDB WebApplication Server Apache Tomcat 907 JBoss Web Logic Web Sphere SDLC Methodologies Agile Waterfall Version Control GIT SVN CVS",
    "entities": [
        "JSP Servlets Frameworks",
        "Hadoop Clusters",
        "Oracle SQL Server",
        "MapReduce Hive",
        "AJAX",
        "IOC AOP",
        "Oracle MySQL",
        "New York",
        "Java Messaging Service",
        "Cassandra",
        "HDFS",
        "Implemented Spring",
        "Developed Spark",
        "Spring IOC",
        "Implemented JavaJ2EE",
        "Spark Implemented Storm",
        "Flume Sqoop Pig",
        "Amazon Elastic Compute Cloud EC2",
        "Additional Information Hadoop Big Data",
        "Spark Build",
        "Implemented Security",
        "UDAFs",
        "RDD",
        "Hadoop",
        "Jenkins for Continuous Integration",
        "XML",
        "SOAP",
        "Spark SQL Data",
        "Spark Streaming Developed",
        "CRUD Create Read Update",
        "Rest Web Services Created",
        "Software Development Life Cycle SDLC",
        "JUnit",
        "Hive Developed",
        "EJB Enterprise JavaBeans",
        "Click Stream",
        "HBase",
        "Amazon Simple Storage Service",
        "STORM Environment HBase",
        "JavaJ2EE",
        "Amazon",
        "Hibernate Annotations Created UML",
        "Cloudera Hadoop",
        "Hadoop Ecosystems",
        "WebSphere",
        "Implemented Application MVC Architecture",
        "Scala Implemented",
        "HTML CSS JavaScript AJAX JQuery",
        "SQL Operating",
        "Good Exposure on Apache Hadoop MapReduce",
        "Distribute Application",
        "Kafka Direct Stream",
        "Developed",
        "DAO",
        "AWS S3",
        "RDMS",
        "UNIXLINUX",
        "Spring MVC",
        "POJO Classes Responsible",
        "Hadoop Log",
        "Git",
        "KANBAN",
        "Sr Big DataHadoop Developer Sr Big DataHadoop",
        "Hibernate 507 Hadoop",
        "Spark for Data Aggregation",
        "Data Frames RDD Spark",
        "Singleton Pattern Involved",
        "Troy",
        "Tools",
        "Data Access Object DAO",
        "JSP",
        "WebApplication Server",
        "Pig and Hive",
        "Spark Streaming",
        "Hadoop Cluster",
        "Hadoop Shell",
        "IBM WebSphere Application Server Created",
        "Developed Servlets",
        "Oozie 43",
        "Developed SQL",
        "MapReduce Jobs",
        "MVC",
        "Hive HBase",
        "Hive Pig HBase Zookeeper",
        "Spark",
        "Hadoop Installed and Configured Apache Hadoop",
        "Created Hive",
        "GIT",
        "Amazon EMR",
        "Pyspark Actively",
        "Core Java",
        "Hibernate Query Language HQL Operated",
        "US",
        "Sqoop",
        "Scala Involved",
        "QA",
        "HIVE",
        "Hadoop Experienced",
        "Maintained Hadoop Hadoop",
        "JSON File",
        "JUnit Involved",
        "Spring Bean Factory",
        "Created JSP Form Beans",
        "Created",
        "AWS",
        "Struts JSP Servlets",
        "Spark MLLIB",
        "PIG",
        "Zookeeper 34 Sqoop",
        "HTML",
        "POJO Jenkins",
        "Cron",
        "SQL",
        "GitHub",
        "MAVEN",
        "J2EE Application archives Jar and War on Web Logic IBM Web Sphere Experience",
        "Relational Database Systems",
        "Build AWS",
        "Object Oriented Analysis Design OOAD",
        "JQuery Technical Skills",
        "Big Data",
        "Hive",
        "SQOOP",
        "Amazon AWS",
        "Big Data Hadoop Ecosystems",
        "Python Implemented",
        "MapReduce Programs",
        "Data Technologies Hadoop",
        "WebLogic Implemented Spark",
        "Maven",
        "Zookeeper",
        "Puppet Experience",
        "Impala",
        "the Hibernate Developed RESTful Web Services",
        "PIG Scripting and",
        "JavaScript",
        "ANT",
        "Infrastructure Planning",
        "the Amazon EMR File System",
        "Hibernate frameworks Responsible",
        "Action ClassesCustom Tag Libraries JSP Expertise",
        "HiveImpala",
        "Hive Query Language HQL Developed Hive",
        "Data",
        "Clover Server Proficient",
        "Oracle Database Implemented",
        "MapReduce",
        "NetBeans",
        "UML Methodology",
        "RDBMS",
        "NoSQL",
        "Application",
        "Jetty",
        "UML Developed Web Services",
        "JQuery",
        "Single Tone",
        "Resource pool Distribution of Hadoop"
    ],
    "experience": "Experience in using Maven for building and deploying J2EE Application archives Jar and War on Web Logic IBM Web Sphere Experience in building Pig scripts to extract transform and load data onto HDFS for processing Experience in importing and exporting data using Sqoop from HDFS to Relational Database Systems and viceversa Experience in Hadoop Shell commands writing MapReduce Programs verifying managing and reviewing Hadoop Log files Strong background in JavaJ2EE environments Well experienced in MVC architecture of Spring framework Experience in Big Data analysis using PIG and HIVE and understanding of SQOOP and Puppet Experience with leveraging Hadoop ecosystem components including Pig and Hive for data analysis Sqoop for data migration Oozie for scheduling and HBase as a NoSQL data store Good Exposure on Apache Hadoop MapReduce programming PIG Scripting and Distribute Application and HDFS Expert in developing applications using all J2EE technologies like Servlets JSP JDBC JNDI JMS Experience in Object Oriented Analysis Design OOAD and development of software using UML Methodology good knowledge of J2EE design patterns and Core Java design patterns Hands on experience in application development using Java RDBMS and Linux shell scripting Strong work ethics with desire to succeed and make significant contributions to the organization Load streaming log data from various web servers into HDFS using Flume Experience in deployment of Hadoop Cluster using Puppet tool Experience in scheduling Cron jobs on EMR Kafka and Spark using Clover Server Proficient in using RDMS concepts with Oracle SQL Server and MySQL Hands on experience with build and deploying tools like Maven and GitHub using Bash scripting Hands on experience with spring tool suit for development of Scala Applications Extensive experience working with structured data using Spark SQL Data frames Hive QL optimizing queries and incorporate complex UDFs in business logic Experience working with Text Sequence files XML Parquet JSON ORC AVRO file formats and Click Stream log files Experience working with Data Frames RDD Spark SQL Spark Streaming APIs System Architecture and Infrastructure Planning Experience with Core Java component Collection Generics Inheritance Exception Handling and Multithreading Developed Java applications using various IDEs like Spring Tool Suite and Eclipse Experience in usage of Hadoop distribution like Cloudera and Hortonworks Strong knowledge in working with UNIXLINUX environments writing shell scripts and PLSQL Stored Procedures Good knowledge in using Hibernate for mapping Java classes with database and using Hibernate Query Language HQL Operated on JavaJ2EE systems with different databases which include Oracle MySQL and DB2 Knowledge on implementing Big Data in Amazon Elastic MapReduce Amazon EMR for processing managing Hadoop framework dynamically scalable Amazon EC2 instances Build AWS secured solutions by creating VPC with private and public subnets Extensive experience in Application servers likes Web logic Web Sphere JBoss Glassfish and Web Servers like Apache Tomcat Authorized to work in the US for any employer Work Experience Sr Big DataHadoop Developer NetApp Durham NC US September 2017 to Present Responsibilities Worked as a Sr Big DataHadoop Developer with Hadoop Ecosystems components like HBase Sqoop Zookeeper Hive and Pig with Cloudera Hadoop distribution Worked with data science team to build statistical model with Spark MLLIB and Pyspark Actively involved in designing Hadoop ecosystem pipeline Developed Spark code using Scala and SparkSQLStreaming for faster testing and processing of data Implemented Security in Web Applications using Azure and deployed Web Applications to Azure Worked in Agile development environment having KANBAN methodology Used Kibana which is an open source based browser analytics and search dashboard for Elastic Search Developed Spark code and SparkSQLStreaming for faster testing and processing of data Used Java Persistence API JPA framework for object relational mapping which is based on POJO Classes Responsible for fetching real time data using Kafka and processing using Spark and Scala Worked on Kafka to import real time weblogs and ingested the data to Spark Streaming Developed business logic using Kafka Direct Stream in Spark Streaming and implemented business transformations Actively involved in daily scrum and other design related meetings Maintained Hadoop Hadoop ecosystems and database with updatesupgrades performance tuning and monitoring Extensively used JQuery to provide dynamic User Interface and for the client side validations Responsible for defining the data flow within Hadoop ecosystem and direct the team in implement them Worked extensively with importing metadata into Hive and migrated existing tables and applications to work on Hive and Spark Build largescale data processing systems in data warehousing solutions and work with unstructured data mining on NoSQL Worked with application teams to install operating system Hadoop updates patches version upgrades as required Specified the cluster size allocating Resource pool Distribution of Hadoop by writing the specification texts in JSON File format Created Hive tables and loading and analyzing data using hive queries Wrote Hive Queries for analyzing data in Hive warehouse using Hive Query Language HQL Developed Hive queries to process the data and generate the data cubes for visualizing Involved in running Hadoop jobs for processing millions of records of text data Created and maintained Technical documentation for launching Hadoop Clusters and for executing Hive queries Used Struts which is an open source MVC framework for creating elegant modern java web applications Developed customized Hive UDFs and UDAFs in Java JDBC connectivity with hive development and execution of Pig scripts and Pig UDFs Used Hadoop YARN to perform analytics on data in Hive Developed and maintained batch data flow using HiveQL and Unix scripting Used Oozie and Zookeeper operational services for coordinating cluster and scheduling workflows Involved in converting HiveSQL queries into Spark transformations using Spark RDD Scala and Python Configured Spark streaming to receive real time data from Kafka and store the stream data to HDFS using Scala Continuous coordination with QA team production support team and deployment team Developed SQL scripts using Spark for handling different data sets and verifying the performance over MapReduce jobs Used J2EE design patterns like Factory pattern Singleton Pattern Involved in converting MapReduce programs into Spark transformations using Spark RDDs using Scala and Python Developed and execute data pipeline testing processes and validate business rules and policies Built code for real time data ingestion using Java MapRStreams Kafka and STORM Environment HBase 14 Sqoop 14 Zookeeper 34 Oozie 43 Hive 23 Hadoop 30 Scala 212 Spark 23 Python 37 MS Azure Agile NoSQL JSON JDBC 43 Java Unix MapR Kafka 20 Storm 105 Sr Hadoop Developer 3 M Health Troy NY US May 2016 to August 2017 Responsibilities Involved in all phases of Software Development Life Cycle SDLC and Worked on all activities related to the development implementation and support for Hadoop Installed and Configured Apache Hadoop clusters for application development and Hadoop tools like Hive Pig HBase Zookeeper and Sqoop Involved in Agile methodologies daily scrum meetings spring planning Worked on MongoDB by using CRUD Create Read Update and Delete Indexing Replication and Shading features Developing data pipeline using Flume Sqoop Pig and Java MapReduce to ingest customer behavioral data and financial histories into HDFS for analysis Worked on MongoDB HBase NoSQL databases which differ from classic relational databases Worked on creating data models for Cassandra from Existing Oracle data model Designed Column families in Cassandra and Ingested data from RDBMS performed data transformations and then export the transformed data to Cassandra as per the business requirement Responsible for installation and configuration of Hive HBase and Sqoop on the Hadoop cluster Configured Spark Streaming to receive real time data from the Apache Kafka and store the stream data to HDFS using Scala Implemented usage of Amazon EMR for processing Big Data across a Hadoop Cluster of virtual servers on Amazon Elastic Compute Cloud EC2 and Amazon Simple Storage Service S3 Implemented Apache Nifi flow topologies to perform cleansing operations before moving data into HDFS Worked extensively with importing metadata into Hive and migrated existing tables and applications to work on Hive and AWS cloud Implemented a distributed messaging queue to integrate with Cassandra using Apache Kafka and Zookeeper Involved in loading the realtime data to NoSQL database like Cassandra Worked on connecting Cassandra database to the Amazon EMR File System for storing the database in S3 Developed UDFs using Scala Scripts which used in Data framesSQL and RDD in Spark for Data Aggregation queries and writing data back into RDBMS through Sqoop Deployed the project on Amazon EMR with S3 connectivity for setting a backup storage Imported data from AWS S3 and into spark RDD and performed transformations and actions on RDDs Responsible for developing data pipeline with Amazon AWS to extract the data from weblogs and store in Amazon EMR Implemented multiple MapReduce Jobs in java for data cleansing and preprocessing Analyzed large amounts of data sets to determine optimal way to aggregate and report on it Installed and configured local Hadoop Cluster with 3 nodes and set up 4 nodes cluster on EC2 cloud Designed and implemented MapReduce based largescale parallel relationlearning system Developed the code which will create XML files and Flat files with the data retrieved from Databases and XML files Environment Hadoop 30 Hive 23 Pig 017 HBase 14 Zookeeper 34 Sqoop 14 Agile MongoDB 40 Spark 23 Scala 212 Apache Kafka 20 Nifi 17 HDFS AWS Cassandra 311 NoSQL java XML Sr JavaHadoop Developer TMobile Bellevue WA US January 2015 to April 2016 Responsibilities Designed and Developed application modules using spring and Hibernate frameworks Responsible for building scalable distributed data solutions using Hadoop Experienced in loading and transforming of large sets of structured semi structured and unstructured data Used MAVEN for developing build scripts and deploying the application onto WebLogic Implemented Spark RDD transformations to map business analysis and apply actions on top of transformations Developed Spark jobs and Hive Jobs to summarize and transform data Involved in converting HiveSQL queries into Spark transformations using Spark data frames Scala and Python Implemented MVC architecture using Spring Framework Coding involves writing Action ClassesCustom Tag Libraries JSP Expertise in implementing Spark Scala application using higher order functions for both batch and interactive analysis requirement Creating Hive tables with periodic backups writing complex HiveImpala queries to run on Impala Implemented partitioning bucketing and worked on Hive using file formats and compressions techniques with optimizations Involved in designing and developing modules at both Client and Server Side Worked on JDBC framework encapsulated using DAO pattern to connect to the database Developed the UI Screens using JSP and HTML and did the client side validation with the JavaScript Worked on various SOAP and RESTful services used in various internal applications Developed JSP and Java classes for various transactional nontransactional reports of the system using extensive SQL queries Worked on analyzing Hadoop cluster and different big data analytic tools including MapReduce Hive and Spark Implemented Storm topologies to preprocess data before moving into HDFS system Implemented POC to migrate MapReduce programs into Spark transformations using Spark and Scala Involved in configuring builds using Jenkins with Git and used Jenkins to deploy the applications onto Dev QA environments Involved in unit testing system integration testing and enterprise user testing using JUnit Involved in creating Hive tables loading with data and writing Hive queries which runs internally in MapReduce way Developed Shell Perl and Python scripts to automate and provide Control flow to Pig scripts Environment spring 40 Hibernate 507 Hadoop 265 Spark 11 Hive Python 33 Scala Sqoop Flume 131 Impala MapReduce LINUX Sr JavaJ2EE Developer JPMC New York NY US August 2012 to December 2014 Responsibilities As a JavaJ2ee developer involved in backend and frontend developing team Designed and developed various modules of the application with J2EE design architecture frameworks like Spring MVC architecture and Spring Bean Factory using IOC AOP concepts Implemented JavaJ2EE design patterns such as Factory DAO Session Faade and Singleton Used Hibernate in persistence layer and developed POJOs Data Access Object DAO to handle all database operations Used Maven as the build tool GIT for version control Jenkins for Continuous Integration and JIRA as a defect tracking tool Developed the user interface components using HTML CSS JavaScript AJAX JQuery and also created custom tags Implemented the Project structure based on Spring MVC pattern using spring boot Worked on JavaScript to validate input manipulated HTML elements using JavaScript Developed external JavaScript codes that can be used in several different web pages Developed Web pages using JSP HTML CSS Struts Tag libs and AJAX for the Credit Risk module Used Spring Beans to encapsulate business logic and Implemented Application MVC Architecture using Spring MVC framework Developed XMLs JavaScript and Java classes for dynamic HTML generation to perform the server side processing on the client requests Used JUnit framework for unit testing of application and Maven to build the application and deployed on Jetty server Developed unit test cases in JUnit and documented all the test scenarios as per the user specifications Implemented Spring framework based on the Model View Controller design paradigm Involved in development activities using Core Java J2EE Servlets JSP JSF used for creating web application XML and springs Used Spring Framework for Dependency injection and integrated with the Hibernate Developed RESTful Web Services client to consume JSON messages using Spring JMS configuration Developed services using Spring IOC and Hibernate persistence layer with Oracle Database Implemented build script using ANT for compiling building and deploying the application on WebSphere application server Environment J2EE Java Spring MVC 30 POJO Jenkins HTML JavaScript AJAX JQuery CSS XML Maven JUnit Hibernate 428 POJO ANT Oracle 10 g Java Developer Valuelabs September 2009 to July 2012 Responsibilities Applied ModelViewController MVC design pattern and Single Tone class design pattern for designing the application Developed JSP pages with Struts and EJB Enterprise JavaBeans for implementing different search pages for transaction of each module Used JavaScript and struts validation framework for client side validation Implemented Ant and Maven build tools to build jar and war files and deployed war files to target servers Responsible for writing Struts action classes Hibernate POJO classes and integrating Struts and Hibernate with spring for processing business needs Developed web application using Struts JSP Servlets and JavaBeans that uses MVC design pattern Used Maven to build the application and deployed on IBM WebSphere Application Server Created bean XML files and row Mappers to map tables and fields in the database Wrote Apache ANT build scripts for building the application and unit test cases using JUnit for performing the unit testing Designed the user interfaces using JSPs developed custom tags and used JSTL Tag lib Involved in creating the Hibernate POJO Objects and mapped using Hibernate Annotations Created UML diagrams use case class sequence and collaboration based on the business requirements Responsible for designing and developing of Object oriented methodologies using UML Developed Web Services to allow communication between the applications using Rest Web Services Created JUnit test cases for unit testing the code at minute level and used Eclipse IDE Used AJAX frameworks like JQuery JSON to develop rich GUIs and also involved in performance tuning the website Developed ANT scripts to build and deploy the application in the JBOSS Application Server Developed MessageDriven beans in collaboration with Java Messaging Service JMS Implemented server pages using Apache Tomcat as application server and Log4j for application logging and debugging Developed the presentation layer and content management framework using HTML and JavaScript Extensively used MVC architecture and JBoss for deployment purposes Created JSP Form Beans for effective way of implementing Model View Controller architecture Developed Servlets to perform business logic and to interact with the database using JDBC Environment JavaScript Ant Maven Struts JavaBeans MVC XML Apache ANT Hibernate 419 POJO JUnit Eclipse JQuery JSON AJAX JBOSS Apache Tomcat HTML Education Bachelors Skills database 9 years Java 9 years JavaScript 6 years JQuery 6 years JSON 6 years Additional Information Hadoop Big Data Cloud Nodes Azure Elastic Cloud Sqoop Flume Yarn Spark Hive Hue Apache Camel SQL MySQL PostgreSQL MongoDB HBase Cassandra and JQuery Technical Skills HadoopBig Data Technologies Hadoop 30 HDFS MapReduce HBase 14 Apache Pig 017 Hive 23 Sqoop 14 Apache Impala 30 Oozie 43 Yarn Apache Flume 18 Kafka 11 Zookeeper 34 Hadoop Distributions Cloudera Hortonworks MapR Cloud AWS Azure Azure SQL Database Azure SQL Data Warehouse Azure Analysis Services HDInsight Azure Data Lake and Data Factory Programming Language Java Scala 212 Python 36 SQL PLSQL Shell Scripting Storm 10 JSP Servlets Frameworks Spring 505 Hibernate 52 Struts 13 JSF EJB JMS Web Technologies HTML5 CSS JavaScript JQuery 33 Bootstrap 41 XML JSON AJAX Databases Oracle 12c11 g SQL Database Tools TOAD SQL PLUS SQL Operating Systems Linux Unix Windows 1087 IDE and Tools Eclipse 47 NetBeans 82 IntelliJ Maven NoSQL Databases HBase 14 Cassandra 311 MongoDB WebApplication Server Apache Tomcat 907 JBoss Web Logic Web Sphere SDLC Methodologies Agile Waterfall Version Control GIT SVN CVS",
    "extracted_keywords": [
        "Sr",
        "Big",
        "DataHadoop",
        "Developer",
        "Sr",
        "Big",
        "DataHadoop",
        "span",
        "lDeveloperspan",
        "Sr",
        "Big",
        "DataHadoop",
        "Developer",
        "NetApp",
        "Durham",
        "NC",
        "years",
        "IT",
        "experience",
        "Big",
        "Data",
        "Hadoop",
        "Ecosystems",
        "experience",
        "ingestion",
        "storage",
        "processing",
        "analysis",
        "data",
        "Experience",
        "Maven",
        "J2EE",
        "Application",
        "Jar",
        "War",
        "Web",
        "Logic",
        "IBM",
        "Web",
        "Sphere",
        "Experience",
        "Pig",
        "scripts",
        "transform",
        "data",
        "HDFS",
        "Experience",
        "data",
        "Sqoop",
        "HDFS",
        "Relational",
        "Database",
        "Systems",
        "viceversa",
        "Experience",
        "Hadoop",
        "Shell",
        "MapReduce",
        "Programs",
        "Hadoop",
        "Log",
        "background",
        "JavaJ2EE",
        "environments",
        "MVC",
        "architecture",
        "Spring",
        "framework",
        "Experience",
        "Big",
        "Data",
        "analysis",
        "PIG",
        "HIVE",
        "understanding",
        "SQOOP",
        "Puppet",
        "Experience",
        "Hadoop",
        "ecosystem",
        "components",
        "Pig",
        "Hive",
        "data",
        "analysis",
        "Sqoop",
        "data",
        "migration",
        "Oozie",
        "scheduling",
        "HBase",
        "NoSQL",
        "data",
        "store",
        "Good",
        "Exposure",
        "Apache",
        "Hadoop",
        "MapReduce",
        "programming",
        "PIG",
        "Scripting",
        "Distribute",
        "Application",
        "HDFS",
        "Expert",
        "applications",
        "J2EE",
        "technologies",
        "Servlets",
        "JSP",
        "JDBC",
        "JNDI",
        "JMS",
        "Experience",
        "Object",
        "Oriented",
        "Analysis",
        "Design",
        "OOAD",
        "development",
        "software",
        "UML",
        "Methodology",
        "knowledge",
        "J2EE",
        "design",
        "patterns",
        "Core",
        "Java",
        "design",
        "Hands",
        "experience",
        "application",
        "development",
        "Java",
        "RDBMS",
        "Linux",
        "shell",
        "work",
        "ethics",
        "desire",
        "contributions",
        "organization",
        "Load",
        "streaming",
        "log",
        "data",
        "web",
        "servers",
        "HDFS",
        "Flume",
        "Experience",
        "deployment",
        "Hadoop",
        "Cluster",
        "Puppet",
        "tool",
        "Experience",
        "scheduling",
        "Cron",
        "jobs",
        "EMR",
        "Kafka",
        "Spark",
        "Clover",
        "Server",
        "Proficient",
        "RDMS",
        "concepts",
        "Oracle",
        "SQL",
        "Server",
        "MySQL",
        "Hands",
        "experience",
        "build",
        "tools",
        "Maven",
        "GitHub",
        "Bash",
        "Hands",
        "experience",
        "spring",
        "tool",
        "suit",
        "development",
        "Scala",
        "Applications",
        "experience",
        "data",
        "Spark",
        "SQL",
        "Data",
        "Hive",
        "QL",
        "queries",
        "UDFs",
        "business",
        "logic",
        "Experience",
        "Text",
        "Sequence",
        "XML",
        "Parquet",
        "JSON",
        "ORC",
        "AVRO",
        "file",
        "formats",
        "Click",
        "Stream",
        "log",
        "files",
        "Experience",
        "Data",
        "Frames",
        "RDD",
        "Spark",
        "SQL",
        "Spark",
        "Streaming",
        "APIs",
        "System",
        "Architecture",
        "Infrastructure",
        "Planning",
        "Experience",
        "Core",
        "Java",
        "component",
        "Collection",
        "Generics",
        "Inheritance",
        "Exception",
        "Handling",
        "Multithreading",
        "Developed",
        "Java",
        "applications",
        "IDEs",
        "Spring",
        "Tool",
        "Suite",
        "Eclipse",
        "Experience",
        "usage",
        "Hadoop",
        "distribution",
        "Cloudera",
        "Hortonworks",
        "knowledge",
        "UNIXLINUX",
        "environments",
        "shell",
        "scripts",
        "PLSQL",
        "Stored",
        "Procedures",
        "knowledge",
        "Hibernate",
        "mapping",
        "Java",
        "classes",
        "database",
        "Hibernate",
        "Query",
        "Language",
        "HQL",
        "JavaJ2EE",
        "systems",
        "databases",
        "Oracle",
        "MySQL",
        "DB2",
        "Knowledge",
        "Big",
        "Data",
        "Amazon",
        "Elastic",
        "MapReduce",
        "Amazon",
        "EMR",
        "Hadoop",
        "framework",
        "Amazon",
        "EC2",
        "AWS",
        "solutions",
        "VPC",
        "subnets",
        "experience",
        "Application",
        "servers",
        "Web",
        "logic",
        "Web",
        "Sphere",
        "JBoss",
        "Glassfish",
        "Web",
        "Servers",
        "Apache",
        "Tomcat",
        "US",
        "employer",
        "Work",
        "Experience",
        "Sr",
        "Big",
        "DataHadoop",
        "Developer",
        "NetApp",
        "Durham",
        "NC",
        "US",
        "September",
        "Present",
        "Responsibilities",
        "Sr",
        "Big",
        "DataHadoop",
        "Developer",
        "Hadoop",
        "Ecosystems",
        "components",
        "HBase",
        "Sqoop",
        "Zookeeper",
        "Hive",
        "Pig",
        "Cloudera",
        "Hadoop",
        "distribution",
        "data",
        "science",
        "team",
        "model",
        "Spark",
        "MLLIB",
        "Pyspark",
        "Hadoop",
        "ecosystem",
        "pipeline",
        "Spark",
        "code",
        "Scala",
        "SparkSQLStreaming",
        "testing",
        "processing",
        "data",
        "Security",
        "Web",
        "Applications",
        "Azure",
        "Web",
        "Applications",
        "Azure",
        "Worked",
        "Agile",
        "development",
        "environment",
        "methodology",
        "Kibana",
        "source",
        "browser",
        "analytics",
        "search",
        "dashboard",
        "Elastic",
        "Search",
        "Spark",
        "code",
        "SparkSQLStreaming",
        "testing",
        "processing",
        "data",
        "Java",
        "Persistence",
        "API",
        "JPA",
        "framework",
        "mapping",
        "POJO",
        "Classes",
        "time",
        "data",
        "Kafka",
        "processing",
        "Spark",
        "Scala",
        "Kafka",
        "time",
        "weblogs",
        "data",
        "Spark",
        "Streaming",
        "business",
        "logic",
        "Kafka",
        "Direct",
        "Stream",
        "Spark",
        "Streaming",
        "business",
        "transformations",
        "scrum",
        "design",
        "meetings",
        "Maintained",
        "Hadoop",
        "Hadoop",
        "ecosystems",
        "database",
        "performance",
        "JQuery",
        "User",
        "Interface",
        "client",
        "side",
        "data",
        "flow",
        "Hadoop",
        "ecosystem",
        "team",
        "metadata",
        "Hive",
        "tables",
        "applications",
        "Hive",
        "Spark",
        "Build",
        "largescale",
        "data",
        "processing",
        "systems",
        "data",
        "warehousing",
        "solutions",
        "work",
        "data",
        "mining",
        "NoSQL",
        "application",
        "teams",
        "operating",
        "system",
        "Hadoop",
        "updates",
        "version",
        "upgrades",
        "cluster",
        "size",
        "Resource",
        "pool",
        "Distribution",
        "Hadoop",
        "specification",
        "texts",
        "JSON",
        "File",
        "format",
        "Hive",
        "tables",
        "loading",
        "data",
        "hive",
        "queries",
        "Wrote",
        "Hive",
        "Queries",
        "data",
        "Hive",
        "warehouse",
        "Hive",
        "Query",
        "Language",
        "HQL",
        "Hive",
        "queries",
        "data",
        "data",
        "cubes",
        "Hadoop",
        "jobs",
        "millions",
        "records",
        "text",
        "data",
        "documentation",
        "Hadoop",
        "Clusters",
        "Hive",
        "queries",
        "Struts",
        "source",
        "MVC",
        "framework",
        "java",
        "web",
        "applications",
        "Hive",
        "UDFs",
        "UDAFs",
        "Java",
        "JDBC",
        "connectivity",
        "hive",
        "development",
        "execution",
        "Pig",
        "scripts",
        "Pig",
        "UDFs",
        "Used",
        "Hadoop",
        "YARN",
        "analytics",
        "data",
        "Hive",
        "Developed",
        "batch",
        "data",
        "flow",
        "HiveQL",
        "Unix",
        "Oozie",
        "Zookeeper",
        "services",
        "cluster",
        "scheduling",
        "workflows",
        "HiveSQL",
        "queries",
        "Spark",
        "transformations",
        "Spark",
        "RDD",
        "Scala",
        "Python",
        "Configured",
        "Spark",
        "streaming",
        "time",
        "data",
        "Kafka",
        "stream",
        "data",
        "HDFS",
        "Scala",
        "Continuous",
        "coordination",
        "QA",
        "team",
        "production",
        "support",
        "team",
        "deployment",
        "team",
        "SQL",
        "scripts",
        "Spark",
        "data",
        "sets",
        "performance",
        "MapReduce",
        "jobs",
        "J2EE",
        "design",
        "patterns",
        "Factory",
        "pattern",
        "Singleton",
        "Pattern",
        "MapReduce",
        "programs",
        "Spark",
        "transformations",
        "Spark",
        "RDDs",
        "Scala",
        "Python",
        "data",
        "pipeline",
        "testing",
        "processes",
        "business",
        "rules",
        "policies",
        "code",
        "time",
        "data",
        "ingestion",
        "Java",
        "MapRStreams",
        "Kafka",
        "STORM",
        "Environment",
        "HBase",
        "Sqoop",
        "Zookeeper",
        "Oozie",
        "Hive",
        "Hadoop",
        "Scala",
        "Spark",
        "Python",
        "MS",
        "Azure",
        "Agile",
        "NoSQL",
        "JSON",
        "JDBC",
        "Java",
        "Unix",
        "MapR",
        "Kafka",
        "Storm",
        "Sr",
        "Hadoop",
        "Developer",
        "3",
        "M",
        "Health",
        "Troy",
        "NY",
        "US",
        "May",
        "August",
        "Responsibilities",
        "phases",
        "Software",
        "Development",
        "Life",
        "Cycle",
        "SDLC",
        "activities",
        "development",
        "implementation",
        "support",
        "Hadoop",
        "Installed",
        "Configured",
        "Apache",
        "Hadoop",
        "clusters",
        "application",
        "development",
        "Hadoop",
        "tools",
        "Hive",
        "Pig",
        "HBase",
        "Zookeeper",
        "Sqoop",
        "methodologies",
        "meetings",
        "spring",
        "planning",
        "MongoDB",
        "CRUD",
        "Create",
        "Read",
        "Update",
        "Delete",
        "Indexing",
        "Replication",
        "Shading",
        "data",
        "pipeline",
        "Flume",
        "Sqoop",
        "Pig",
        "Java",
        "MapReduce",
        "customer",
        "data",
        "histories",
        "HDFS",
        "analysis",
        "MongoDB",
        "HBase",
        "NoSQL",
        "databases",
        "data",
        "models",
        "Cassandra",
        "Existing",
        "Oracle",
        "data",
        "model",
        "Column",
        "families",
        "Cassandra",
        "data",
        "data",
        "transformations",
        "data",
        "Cassandra",
        "business",
        "requirement",
        "installation",
        "configuration",
        "Hive",
        "HBase",
        "Sqoop",
        "Hadoop",
        "cluster",
        "Configured",
        "Spark",
        "Streaming",
        "time",
        "data",
        "Apache",
        "Kafka",
        "stream",
        "data",
        "HDFS",
        "Scala",
        "usage",
        "Amazon",
        "EMR",
        "Big",
        "Data",
        "Hadoop",
        "Cluster",
        "servers",
        "Amazon",
        "Elastic",
        "Compute",
        "Cloud",
        "EC2",
        "Amazon",
        "Simple",
        "Storage",
        "Service",
        "S3",
        "Apache",
        "Nifi",
        "flow",
        "topologies",
        "cleansing",
        "operations",
        "data",
        "HDFS",
        "Worked",
        "metadata",
        "Hive",
        "tables",
        "applications",
        "Hive",
        "AWS",
        "cloud",
        "queue",
        "Cassandra",
        "Apache",
        "Kafka",
        "Zookeeper",
        "data",
        "NoSQL",
        "database",
        "Cassandra",
        "Cassandra",
        "database",
        "Amazon",
        "EMR",
        "File",
        "System",
        "database",
        "S3",
        "Developed",
        "UDFs",
        "Scala",
        "Scripts",
        "Data",
        "framesSQL",
        "RDD",
        "Spark",
        "Data",
        "Aggregation",
        "data",
        "RDBMS",
        "Sqoop",
        "project",
        "Amazon",
        "EMR",
        "S3",
        "connectivity",
        "storage",
        "data",
        "AWS",
        "S3",
        "spark",
        "RDD",
        "transformations",
        "actions",
        "RDDs",
        "data",
        "pipeline",
        "Amazon",
        "AWS",
        "data",
        "weblogs",
        "store",
        "Amazon",
        "EMR",
        "MapReduce",
        "Jobs",
        "java",
        "data",
        "cleansing",
        "amounts",
        "data",
        "sets",
        "way",
        "Hadoop",
        "Cluster",
        "nodes",
        "nodes",
        "cluster",
        "EC2",
        "cloud",
        "MapReduce",
        "largescale",
        "system",
        "code",
        "XML",
        "files",
        "files",
        "data",
        "Databases",
        "XML",
        "files",
        "Environment",
        "Hadoop",
        "Hive",
        "Pig",
        "HBase",
        "Zookeeper",
        "Sqoop",
        "Agile",
        "Spark",
        "Scala",
        "Apache",
        "Kafka",
        "Nifi",
        "HDFS",
        "AWS",
        "Cassandra",
        "NoSQL",
        "XML",
        "Sr",
        "JavaHadoop",
        "Developer",
        "TMobile",
        "Bellevue",
        "WA",
        "US",
        "January",
        "April",
        "Responsibilities",
        "application",
        "modules",
        "spring",
        "Hibernate",
        "frameworks",
        "data",
        "solutions",
        "Hadoop",
        "Experienced",
        "loading",
        "transforming",
        "sets",
        "data",
        "MAVEN",
        "build",
        "scripts",
        "application",
        "WebLogic",
        "Implemented",
        "Spark",
        "RDD",
        "transformations",
        "business",
        "analysis",
        "actions",
        "top",
        "transformations",
        "Spark",
        "jobs",
        "Hive",
        "Jobs",
        "data",
        "HiveSQL",
        "queries",
        "Spark",
        "transformations",
        "Spark",
        "data",
        "Scala",
        "Python",
        "MVC",
        "architecture",
        "Spring",
        "Framework",
        "Coding",
        "Action",
        "ClassesCustom",
        "Tag",
        "JSP",
        "Expertise",
        "Spark",
        "Scala",
        "application",
        "order",
        "functions",
        "batch",
        "analysis",
        "requirement",
        "Hive",
        "tables",
        "backups",
        "HiveImpala",
        "Impala",
        "bucketing",
        "Hive",
        "file",
        "formats",
        "compressions",
        "techniques",
        "optimizations",
        "modules",
        "Client",
        "Server",
        "Side",
        "JDBC",
        "framework",
        "DAO",
        "pattern",
        "database",
        "UI",
        "Screens",
        "JSP",
        "HTML",
        "client",
        "side",
        "validation",
        "JavaScript",
        "SOAP",
        "services",
        "applications",
        "JSP",
        "Java",
        "classes",
        "reports",
        "system",
        "SQL",
        "queries",
        "Hadoop",
        "cluster",
        "data",
        "tools",
        "MapReduce",
        "Hive",
        "Spark",
        "Storm",
        "topologies",
        "data",
        "HDFS",
        "system",
        "POC",
        "MapReduce",
        "programs",
        "Spark",
        "transformations",
        "Spark",
        "Scala",
        "builds",
        "Jenkins",
        "Git",
        "Jenkins",
        "applications",
        "Dev",
        "QA",
        "environments",
        "unit",
        "testing",
        "system",
        "integration",
        "testing",
        "enterprise",
        "user",
        "testing",
        "JUnit",
        "Hive",
        "tables",
        "data",
        "Hive",
        "queries",
        "MapReduce",
        "way",
        "Shell",
        "Perl",
        "Python",
        "scripts",
        "Control",
        "flow",
        "Pig",
        "scripts",
        "Environment",
        "spring",
        "Hibernate",
        "Hadoop",
        "Spark",
        "Hive",
        "Python",
        "Scala",
        "Sqoop",
        "Flume",
        "Impala",
        "MapReduce",
        "LINUX",
        "Sr",
        "JavaJ2EE",
        "Developer",
        "JPMC",
        "New",
        "York",
        "NY",
        "US",
        "August",
        "December",
        "Responsibilities",
        "developer",
        "backend",
        "team",
        "modules",
        "application",
        "J2EE",
        "design",
        "architecture",
        "frameworks",
        "Spring",
        "MVC",
        "architecture",
        "Spring",
        "Bean",
        "Factory",
        "IOC",
        "AOP",
        "concepts",
        "JavaJ2EE",
        "design",
        "patterns",
        "Factory",
        "DAO",
        "Session",
        "Faade",
        "Singleton",
        "Hibernate",
        "persistence",
        "layer",
        "POJOs",
        "Data",
        "Access",
        "Object",
        "DAO",
        "database",
        "operations",
        "Maven",
        "build",
        "tool",
        "GIT",
        "version",
        "control",
        "Jenkins",
        "Continuous",
        "Integration",
        "JIRA",
        "tracking",
        "tool",
        "user",
        "interface",
        "components",
        "HTML",
        "CSS",
        "JavaScript",
        "AJAX",
        "JQuery",
        "custom",
        "tags",
        "Project",
        "structure",
        "Spring",
        "MVC",
        "pattern",
        "spring",
        "boot",
        "JavaScript",
        "input",
        "HTML",
        "elements",
        "JavaScript",
        "JavaScript",
        "codes",
        "web",
        "pages",
        "Web",
        "pages",
        "JSP",
        "HTML",
        "CSS",
        "Struts",
        "Tag",
        "libs",
        "AJAX",
        "Credit",
        "Risk",
        "module",
        "Spring",
        "Beans",
        "business",
        "logic",
        "Implemented",
        "Application",
        "MVC",
        "Architecture",
        "Spring",
        "MVC",
        "framework",
        "XMLs",
        "JavaScript",
        "Java",
        "classes",
        "HTML",
        "generation",
        "server",
        "side",
        "processing",
        "client",
        "requests",
        "JUnit",
        "framework",
        "unit",
        "testing",
        "application",
        "Maven",
        "application",
        "Jetty",
        "server",
        "unit",
        "test",
        "cases",
        "JUnit",
        "test",
        "scenarios",
        "user",
        "specifications",
        "Spring",
        "framework",
        "Model",
        "View",
        "Controller",
        "design",
        "paradigm",
        "development",
        "activities",
        "Core",
        "Java",
        "J2EE",
        "Servlets",
        "JSP",
        "JSF",
        "web",
        "application",
        "XML",
        "springs",
        "Spring",
        "Framework",
        "Dependency",
        "injection",
        "Hibernate",
        "Web",
        "Services",
        "client",
        "messages",
        "Spring",
        "JMS",
        "configuration",
        "services",
        "Spring",
        "IOC",
        "Hibernate",
        "persistence",
        "layer",
        "Oracle",
        "Database",
        "build",
        "script",
        "ANT",
        "building",
        "application",
        "WebSphere",
        "application",
        "server",
        "Environment",
        "J2EE",
        "Java",
        "Spring",
        "MVC",
        "POJO",
        "Jenkins",
        "HTML",
        "JavaScript",
        "AJAX",
        "JQuery",
        "CSS",
        "XML",
        "Maven",
        "JUnit",
        "Hibernate",
        "POJO",
        "ANT",
        "Oracle",
        "g",
        "Java",
        "Developer",
        "Valuelabs",
        "September",
        "July",
        "Responsibilities",
        "ModelViewController",
        "MVC",
        "design",
        "pattern",
        "Single",
        "Tone",
        "class",
        "design",
        "pattern",
        "application",
        "JSP",
        "pages",
        "Struts",
        "EJB",
        "Enterprise",
        "JavaBeans",
        "search",
        "pages",
        "transaction",
        "module",
        "JavaScript",
        "validation",
        "framework",
        "client",
        "side",
        "validation",
        "Ant",
        "Maven",
        "tools",
        "jar",
        "war",
        "files",
        "war",
        "files",
        "servers",
        "Struts",
        "action",
        "classes",
        "Hibernate",
        "POJO",
        "classes",
        "Struts",
        "Hibernate",
        "spring",
        "business",
        "web",
        "application",
        "Struts",
        "JSP",
        "Servlets",
        "JavaBeans",
        "MVC",
        "design",
        "pattern",
        "Maven",
        "application",
        "IBM",
        "WebSphere",
        "Application",
        "Server",
        "bean",
        "XML",
        "files",
        "Mappers",
        "tables",
        "fields",
        "database",
        "Wrote",
        "Apache",
        "ANT",
        "scripts",
        "application",
        "unit",
        "test",
        "cases",
        "JUnit",
        "unit",
        "testing",
        "user",
        "interfaces",
        "JSPs",
        "custom",
        "tags",
        "JSTL",
        "Tag",
        "lib",
        "Hibernate",
        "POJO",
        "Objects",
        "Hibernate",
        "Annotations",
        "UML",
        "diagrams",
        "case",
        "class",
        "sequence",
        "collaboration",
        "business",
        "requirements",
        "developing",
        "Object",
        "methodologies",
        "UML",
        "Developed",
        "Web",
        "Services",
        "communication",
        "applications",
        "Rest",
        "Web",
        "Services",
        "JUnit",
        "test",
        "cases",
        "unit",
        "code",
        "minute",
        "level",
        "Eclipse",
        "IDE",
        "AJAX",
        "frameworks",
        "JQuery",
        "JSON",
        "GUIs",
        "performance",
        "website",
        "ANT",
        "scripts",
        "application",
        "JBOSS",
        "Application",
        "Server",
        "Developed",
        "MessageDriven",
        "beans",
        "collaboration",
        "Java",
        "Messaging",
        "Service",
        "JMS",
        "server",
        "pages",
        "Apache",
        "Tomcat",
        "application",
        "server",
        "Log4j",
        "application",
        "presentation",
        "layer",
        "content",
        "management",
        "framework",
        "HTML",
        "JavaScript",
        "MVC",
        "architecture",
        "JBoss",
        "deployment",
        "purposes",
        "JSP",
        "Form",
        "Beans",
        "way",
        "Model",
        "View",
        "Controller",
        "architecture",
        "Developed",
        "Servlets",
        "business",
        "logic",
        "database",
        "JDBC",
        "Environment",
        "JavaScript",
        "Ant",
        "Maven",
        "Struts",
        "JavaBeans",
        "MVC",
        "XML",
        "Apache",
        "ANT",
        "Hibernate",
        "POJO",
        "JUnit",
        "Eclipse",
        "JQuery",
        "JSON",
        "AJAX",
        "JBOSS",
        "Apache",
        "Tomcat",
        "HTML",
        "Education",
        "Bachelors",
        "Skills",
        "database",
        "years",
        "Java",
        "years",
        "JavaScript",
        "years",
        "JQuery",
        "years",
        "JSON",
        "years",
        "Additional",
        "Information",
        "Hadoop",
        "Big",
        "Data",
        "Cloud",
        "Nodes",
        "Azure",
        "Cloud",
        "Sqoop",
        "Flume",
        "Yarn",
        "Spark",
        "Hive",
        "Hue",
        "Apache",
        "Camel",
        "SQL",
        "MySQL",
        "PostgreSQL",
        "MongoDB",
        "HBase",
        "Cassandra",
        "JQuery",
        "Technical",
        "Skills",
        "HadoopBig",
        "Data",
        "Technologies",
        "Hadoop",
        "HDFS",
        "MapReduce",
        "HBase",
        "Apache",
        "Pig",
        "Hive",
        "Sqoop",
        "Apache",
        "Impala",
        "Oozie",
        "Yarn",
        "Apache",
        "Flume",
        "Kafka",
        "Zookeeper",
        "Hadoop",
        "Distributions",
        "Cloudera",
        "Hortonworks",
        "MapR",
        "Cloud",
        "Azure",
        "SQL",
        "Database",
        "Azure",
        "SQL",
        "Data",
        "Warehouse",
        "Azure",
        "Analysis",
        "Services",
        "HDInsight",
        "Azure",
        "Data",
        "Lake",
        "Data",
        "Factory",
        "Programming",
        "Language",
        "Java",
        "Scala",
        "Python",
        "SQL",
        "PLSQL",
        "Shell",
        "Scripting",
        "Storm",
        "JSP",
        "Servlets",
        "Frameworks",
        "Spring",
        "Hibernate",
        "Struts",
        "JSF",
        "EJB",
        "JMS",
        "Web",
        "Technologies",
        "HTML5",
        "CSS",
        "JavaScript",
        "JQuery",
        "Bootstrap",
        "XML",
        "JSON",
        "AJAX",
        "Oracle",
        "g",
        "SQL",
        "Database",
        "Tools",
        "TOAD",
        "SQL",
        "PLUS",
        "SQL",
        "Operating",
        "Systems",
        "Linux",
        "Unix",
        "Windows",
        "IDE",
        "Tools",
        "Eclipse",
        "NetBeans",
        "IntelliJ",
        "Maven",
        "NoSQL",
        "Databases",
        "HBase",
        "Cassandra",
        "WebApplication",
        "Server",
        "Apache",
        "Tomcat",
        "JBoss",
        "Web",
        "Logic",
        "Web",
        "Sphere",
        "SDLC",
        "Methodologies",
        "Agile",
        "Waterfall",
        "Version",
        "Control",
        "GIT",
        "SVN",
        "CVS"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T20:16:17.928511",
    "resume_data": "Sr Big DataHadoop Developer Sr Big DataHadoop span lDeveloperspan Sr Big DataHadoop Developer NetApp Durham NC Over 9 years of professional IT experience of Big Data Hadoop Ecosystems experience in ingestion storage querying processing and analysis of big data Experience in using Maven for building and deploying J2EE Application archives Jar and War on Web Logic IBM Web Sphere Experience in building Pig scripts to extract transform and load data onto HDFS for processing Experience in importing and exporting data using Sqoop from HDFS to Relational Database Systems and viceversa Experience in Hadoop Shell commands writing MapReduce Programs verifying managing and reviewing Hadoop Log files Strong background in JavaJ2EE environments Well experienced in MVC architecture of Spring framework Experience in Big Data analysis using PIG and HIVE and understanding of SQOOP and Puppet Experience with leveraging Hadoop ecosystem components including Pig and Hive for data analysis Sqoop for data migration Oozie for scheduling and HBase as a NoSQL data store Good Exposure on Apache Hadoop MapReduce programming PIG Scripting and Distribute Application and HDFS Expert in developing applications using all J2EE technologies like Servlets JSP JDBC JNDI JMS Experience in Object Oriented Analysis Design OOAD and development of software using UML Methodology good knowledge of J2EE design patterns and Core Java design patterns Hands on experience in application development using Java RDBMS and Linux shell scripting Strong work ethics with desire to succeed and make significant contributions to the organization Load streaming log data from various web servers into HDFS using Flume Experience in deployment of Hadoop Cluster using Puppet tool Experience in scheduling Cron jobs on EMR Kafka and Spark using Clover Server Proficient in using RDMS concepts with Oracle SQL Server and MySQL Hands on experience with build and deploying tools like Maven and GitHub using Bash scripting Hands on experience with spring tool suit for development of Scala Applications Extensive experience working with structured data using Spark SQL Data frames Hive QL optimizing queries and incorporate complex UDFs in business logic Experience working with Text Sequence files XML Parquet JSON ORC AVRO file formats and Click Stream log files Experience working with Data Frames RDD Spark SQL Spark Streaming APIs System Architecture and Infrastructure Planning Experience with Core Java component Collection Generics Inheritance Exception Handling and Multithreading Developed Java applications using various IDEs like Spring Tool Suite and Eclipse Experience in usage of Hadoop distribution like Cloudera and Hortonworks Strong knowledge in working with UNIXLINUX environments writing shell scripts and PLSQL Stored Procedures Good knowledge in using Hibernate for mapping Java classes with database and using Hibernate Query Language HQL Operated on JavaJ2EE systems with different databases which include Oracle MySQL and DB2 Knowledge on implementing Big Data in Amazon Elastic MapReduce Amazon EMR for processing managing Hadoop framework dynamically scalable Amazon EC2 instances Build AWS secured solutions by creating VPC with private and public subnets Extensive experience in Application servers likes Web logic Web Sphere JBoss Glassfish and Web Servers like Apache Tomcat Authorized to work in the US for any employer Work Experience Sr Big DataHadoop Developer NetApp Durham NC US September 2017 to Present Responsibilities Worked as a Sr Big DataHadoop Developer with Hadoop Ecosystems components like HBase Sqoop Zookeeper Hive and Pig with Cloudera Hadoop distribution Worked with data science team to build statistical model with Spark MLLIB and Pyspark Actively involved in designing Hadoop ecosystem pipeline Developed Spark code using Scala and SparkSQLStreaming for faster testing and processing of data Implemented Security in Web Applications using Azure and deployed Web Applications to Azure Worked in Agile development environment having KANBAN methodology Used Kibana which is an open source based browser analytics and search dashboard for Elastic Search Developed Spark code and SparkSQLStreaming for faster testing and processing of data Used Java Persistence API JPA framework for object relational mapping which is based on POJO Classes Responsible for fetching real time data using Kafka and processing using Spark and Scala Worked on Kafka to import real time weblogs and ingested the data to Spark Streaming Developed business logic using Kafka Direct Stream in Spark Streaming and implemented business transformations Actively involved in daily scrum and other design related meetings Maintained Hadoop Hadoop ecosystems and database with updatesupgrades performance tuning and monitoring Extensively used JQuery to provide dynamic User Interface and for the client side validations Responsible for defining the data flow within Hadoop ecosystem and direct the team in implement them Worked extensively with importing metadata into Hive and migrated existing tables and applications to work on Hive and Spark Build largescale data processing systems in data warehousing solutions and work with unstructured data mining on NoSQL Worked with application teams to install operating system Hadoop updates patches version upgrades as required Specified the cluster size allocating Resource pool Distribution of Hadoop by writing the specification texts in JSON File format Created Hive tables and loading and analyzing data using hive queries Wrote Hive Queries for analyzing data in Hive warehouse using Hive Query Language HQL Developed Hive queries to process the data and generate the data cubes for visualizing Involved in running Hadoop jobs for processing millions of records of text data Created and maintained Technical documentation for launching Hadoop Clusters and for executing Hive queries Used Struts which is an open source MVC framework for creating elegant modern java web applications Developed customized Hive UDFs and UDAFs in Java JDBC connectivity with hive development and execution of Pig scripts and Pig UDFs Used Hadoop YARN to perform analytics on data in Hive Developed and maintained batch data flow using HiveQL and Unix scripting Used Oozie and Zookeeper operational services for coordinating cluster and scheduling workflows Involved in converting HiveSQL queries into Spark transformations using Spark RDD Scala and Python Configured Spark streaming to receive real time data from Kafka and store the stream data to HDFS using Scala Continuous coordination with QA team production support team and deployment team Developed SQL scripts using Spark for handling different data sets and verifying the performance over MapReduce jobs Used J2EE design patterns like Factory pattern Singleton Pattern Involved in converting MapReduce programs into Spark transformations using Spark RDDs using Scala and Python Developed and execute data pipeline testing processes and validate business rules and policies Built code for real time data ingestion using Java MapRStreams Kafka and STORM Environment HBase 14 Sqoop 14 Zookeeper 34 Oozie 43 Hive 23 Hadoop 30 Scala 212 Spark 23 Python 37 MS Azure Agile NoSQL JSON JDBC 43 Java Unix MapR Kafka 20 Storm 105 Sr Hadoop Developer 3M Health Troy NY US May 2016 to August 2017 Responsibilities Involved in all phases of Software Development Life Cycle SDLC and Worked on all activities related to the development implementation and support for Hadoop Installed and Configured Apache Hadoop clusters for application development and Hadoop tools like Hive Pig HBase Zookeeper and Sqoop Involved in Agile methodologies daily scrum meetings spring planning Worked on MongoDB by using CRUD Create Read Update and Delete Indexing Replication and Shading features Developing data pipeline using Flume Sqoop Pig and Java MapReduce to ingest customer behavioral data and financial histories into HDFS for analysis Worked on MongoDB HBase NoSQL databases which differ from classic relational databases Worked on creating data models for Cassandra from Existing Oracle data model Designed Column families in Cassandra and Ingested data from RDBMS performed data transformations and then export the transformed data to Cassandra as per the business requirement Responsible for installation and configuration of Hive HBase and Sqoop on the Hadoop cluster Configured Spark Streaming to receive real time data from the Apache Kafka and store the stream data to HDFS using Scala Implemented usage of Amazon EMR for processing Big Data across a Hadoop Cluster of virtual servers on Amazon Elastic Compute Cloud EC2 and Amazon Simple Storage Service S3 Implemented Apache Nifi flow topologies to perform cleansing operations before moving data into HDFS Worked extensively with importing metadata into Hive and migrated existing tables and applications to work on Hive and AWS cloud Implemented a distributed messaging queue to integrate with Cassandra using Apache Kafka and Zookeeper Involved in loading the realtime data to NoSQL database like Cassandra Worked on connecting Cassandra database to the Amazon EMR File System for storing the database in S3 Developed UDFs using Scala Scripts which used in Data framesSQL and RDD in Spark for Data Aggregation queries and writing data back into RDBMS through Sqoop Deployed the project on Amazon EMR with S3 connectivity for setting a backup storage Imported data from AWS S3 and into spark RDD and performed transformations and actions on RDDs Responsible for developing data pipeline with Amazon AWS to extract the data from weblogs and store in Amazon EMR Implemented multiple MapReduce Jobs in java for data cleansing and preprocessing Analyzed large amounts of data sets to determine optimal way to aggregate and report on it Installed and configured local Hadoop Cluster with 3 nodes and set up 4 nodes cluster on EC2 cloud Designed and implemented MapReduce based largescale parallel relationlearning system Developed the code which will create XML files and Flat files with the data retrieved from Databases and XML files Environment Hadoop 30 Hive 23 Pig 017 HBase 14 Zookeeper 34 Sqoop 14 Agile MongoDB 40 Spark 23 Scala 212 Apache Kafka 20 Nifi 17 HDFS AWS Cassandra 311 NoSQL java XML Sr JavaHadoop Developer TMobile Bellevue WA US January 2015 to April 2016 Responsibilities Designed and Developed application modules using spring and Hibernate frameworks Responsible for building scalable distributed data solutions using Hadoop Experienced in loading and transforming of large sets of structured semi structured and unstructured data Used MAVEN for developing build scripts and deploying the application onto WebLogic Implemented Spark RDD transformations to map business analysis and apply actions on top of transformations Developed Spark jobs and Hive Jobs to summarize and transform data Involved in converting HiveSQL queries into Spark transformations using Spark data frames Scala and Python Implemented MVC architecture using Spring Framework Coding involves writing Action ClassesCustom Tag Libraries JSP Expertise in implementing Spark Scala application using higher order functions for both batch and interactive analysis requirement Creating Hive tables with periodic backups writing complex HiveImpala queries to run on Impala Implemented partitioning bucketing and worked on Hive using file formats and compressions techniques with optimizations Involved in designing and developing modules at both Client and Server Side Worked on JDBC framework encapsulated using DAO pattern to connect to the database Developed the UI Screens using JSP and HTML and did the client side validation with the JavaScript Worked on various SOAP and RESTful services used in various internal applications Developed JSP and Java classes for various transactional nontransactional reports of the system using extensive SQL queries Worked on analyzing Hadoop cluster and different big data analytic tools including MapReduce Hive and Spark Implemented Storm topologies to preprocess data before moving into HDFS system Implemented POC to migrate MapReduce programs into Spark transformations using Spark and Scala Involved in configuring builds using Jenkins with Git and used Jenkins to deploy the applications onto Dev QA environments Involved in unit testing system integration testing and enterprise user testing using JUnit Involved in creating Hive tables loading with data and writing Hive queries which runs internally in MapReduce way Developed Shell Perl and Python scripts to automate and provide Control flow to Pig scripts Environment spring 40 Hibernate 507 Hadoop 265 Spark 11 Hive Python 33 Scala Sqoop Flume 131 Impala MapReduce LINUX Sr JavaJ2EE Developer JPMC New York NY US August 2012 to December 2014 Responsibilities As a JavaJ2ee developer involved in backend and frontend developing team Designed and developed various modules of the application with J2EE design architecture frameworks like Spring MVC architecture and Spring Bean Factory using IOC AOP concepts Implemented JavaJ2EE design patterns such as Factory DAO Session Faade and Singleton Used Hibernate in persistence layer and developed POJOs Data Access Object DAO to handle all database operations Used Maven as the build tool GIT for version control Jenkins for Continuous Integration and JIRA as a defect tracking tool Developed the user interface components using HTML CSS JavaScript AJAX JQuery and also created custom tags Implemented the Project structure based on Spring MVC pattern using spring boot Worked on JavaScript to validate input manipulated HTML elements using JavaScript Developed external JavaScript codes that can be used in several different web pages Developed Web pages using JSP HTML CSS Struts Tag libs and AJAX for the Credit Risk module Used Spring Beans to encapsulate business logic and Implemented Application MVC Architecture using Spring MVC framework Developed XMLs JavaScript and Java classes for dynamic HTML generation to perform the server side processing on the client requests Used JUnit framework for unit testing of application and Maven to build the application and deployed on Jetty server Developed unit test cases in JUnit and documented all the test scenarios as per the user specifications Implemented Spring framework based on the Model View Controller design paradigm Involved in development activities using Core Java J2EE Servlets JSP JSF used for creating web application XML and springs Used Spring Framework for Dependency injection and integrated with the Hibernate Developed RESTful Web Services client to consume JSON messages using Spring JMS configuration Developed services using Spring IOC and Hibernate persistence layer with Oracle Database Implemented build script using ANT for compiling building and deploying the application on WebSphere application server Environment J2EE Java Spring MVC 30 POJO Jenkins HTML JavaScript AJAX JQuery CSS XML Maven JUnit Hibernate 428 POJO ANT Oracle 10g Java Developer Valuelabs September 2009 to July 2012 Responsibilities Applied ModelViewController MVC design pattern and Single Tone class design pattern for designing the application Developed JSP pages with Struts and EJB Enterprise JavaBeans for implementing different search pages for transaction of each module Used JavaScript and struts validation framework for client side validation Implemented Ant and Maven build tools to build jar and war files and deployed war files to target servers Responsible for writing Struts action classes Hibernate POJO classes and integrating Struts and Hibernate with spring for processing business needs Developed web application using Struts JSP Servlets and JavaBeans that uses MVC design pattern Used Maven to build the application and deployed on IBM WebSphere Application Server Created bean XML files and row Mappers to map tables and fields in the database Wrote Apache ANT build scripts for building the application and unit test cases using JUnit for performing the unit testing Designed the user interfaces using JSPs developed custom tags and used JSTL Tag lib Involved in creating the Hibernate POJO Objects and mapped using Hibernate Annotations Created UML diagrams use case class sequence and collaboration based on the business requirements Responsible for designing and developing of Object oriented methodologies using UML Developed Web Services to allow communication between the applications using Rest Web Services Created JUnit test cases for unit testing the code at minute level and used Eclipse IDE Used AJAX frameworks like JQuery JSON to develop rich GUIs and also involved in performance tuning the website Developed ANT scripts to build and deploy the application in the JBOSS Application Server Developed MessageDriven beans in collaboration with Java Messaging Service JMS Implemented server pages using Apache Tomcat as application server and Log4j for application logging and debugging Developed the presentation layer and content management framework using HTML and JavaScript Extensively used MVC architecture and JBoss for deployment purposes Created JSP Form Beans for effective way of implementing Model View Controller architecture Developed Servlets to perform business logic and to interact with the database using JDBC Environment JavaScript Ant Maven Struts JavaBeans MVC XML Apache ANT Hibernate 419 POJO JUnit Eclipse JQuery JSON AJAX JBOSS Apache Tomcat HTML Education Bachelors Skills database 9 years Java 9 years JavaScript 6 years JQuery 6 years JSON 6 years Additional Information Hadoop Big Data Cloud Nodes Azure Elastic Cloud Sqoop Flume Yarn Spark Hive Hue Apache Camel SQL MySQL PostgreSQL MongoDB HBase Cassandra and JQuery Technical Skills HadoopBig Data Technologies Hadoop 30 HDFS MapReduce HBase 14 Apache Pig 017 Hive 23 Sqoop 14 Apache Impala 30 Oozie 43 Yarn Apache Flume 18 Kafka 11 Zookeeper 34 Hadoop Distributions Cloudera Hortonworks MapR Cloud AWS Azure Azure SQL Database Azure SQL Data Warehouse Azure Analysis Services HDInsight Azure Data Lake and Data Factory Programming Language Java Scala 212 Python 36 SQL PLSQL Shell Scripting Storm 10 JSP Servlets Frameworks Spring 505 Hibernate 52 Struts 13 JSF EJB JMS Web Technologies HTML5 CSS JavaScript JQuery 33 Bootstrap 41 XML JSON AJAX Databases Oracle 12c11g SQL Database Tools TOAD SQL PLUS SQL Operating Systems Linux Unix Windows 1087 IDE and Tools Eclipse 47 NetBeans 82 IntelliJ Maven NoSQL Databases HBase 14 Cassandra 311 MongoDB WebApplication Server Apache Tomcat 907 JBoss Web Logic Web Sphere SDLC Methodologies Agile Waterfall Version Control GIT SVN CVS",
    "unique_id": "f6edf31b-fc7f-4eb2-9ea5-f949e768e8af"
}