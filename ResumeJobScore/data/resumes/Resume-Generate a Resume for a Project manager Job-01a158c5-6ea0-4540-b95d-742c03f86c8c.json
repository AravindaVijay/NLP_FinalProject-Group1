{
    "clean_data": "Hadoop Developer Hadoop span lDeveloperspan Hadoop Developer Ericsson North Brunswick NJ 5 years of Professional experience in IT Industry in Developing Implementing configuring testing Hadoop ecosystem components and maintenance of various webbased applications using Java J2EE 3 years of Realtime experience in Hadoop Framework and its ecosystem Worked on Multi Clustered environment and setting up Cloudera Hadoop echo system Good understanding of Classic Hadoop and Yarn architecture along with various Hadoop Demons such as Job Tracker Task Tracker Name Node Data Node Secondary Name Node Resource Manager Node Manager Experience in dealing with Apache Hadoop components like HDFS Map Reduce Sqoop Hive Oozie Good knowledge on Spark Inmemory capabilities and its modules Spark Streaming  Skilled in integrating Kafka with Spark streaming for high speed data processing Hands on experience with working on Spark using both Scala and python Performed various actions and transformations on spark RDDs and DataFrames Expertise in writing Hadoop Jobs for processing and analyzing data using MapReduce Hive Pig Experienced in extending Hive and Pig core functionality by writing custom UDFs using Java Handson experience on YARN MapReduce 20 architecture and it components Hands on experience using Core Java UNIX Shell scripting and RDBMS Excellent Java development skills using J2EE J2SE Servlets JUnit JSP JDBC Very good handson technical knowledge of ETL Tools DataStage SQL and PLSQL Vast Experience in Teradata and Involved in Converting Projects from Teradata to Hadoop Hands on experience working on virtualization tools like Tableau Arcadia Data Well versed with Agile working environment using JIRA and code version tools like GIT Knowledge of job workflow scheduling and monitoring tools like oozie and Zookeeper of NoSQL databases such as HBase Cassandra Researchoriented motivated proactive selfstarter with strong technical analytical and interpersonal skills Authorized to work in the US for any employer Work Experience Hadoop Developer Ericsson Piscataway NJ January 2017 to Present Responsibilities Responsible for building scalable distributed data solutions using Hadoop Job duties include design and development of various modules in Hadoop Big Data platform and processing data using MapReduce Hive SQOOP Kafka and Oozie Developed job processing scripts using Oozie workflow Implemented POC to migrate map reduce jobs into Spark RDD transformations using Scala Worked with Apache Hadoop Spark and Scala Used Data Frame API in Scala for converting the distributed collection of data organized into named columns Used different Hadoop components in Talend to design the framework Involved in Hadoop cluster task like commissioning decommissioning Nodes without any effect to running jobs and data Spark streaming collects the data from Kafka in near real time and performs necessary transformations and aggregations on the fly to build the common learner data model and persists the data in Cassandra Wrote Map Reduce jobs to discover trends in data usage by users Worked extensively with Sqoop for importing metadata from Oracle Used Sqoop to import data from SQL server to Cassandra Real streaming the data using Spark with Kafka Designed developed and did maintenance of data integration programs in a Hadoop and RDBMS environment with both traditional and nontraditional source systems as we as RDBMS and NoSQL data stores for data access and analysis Experienced in running Hadoop streaming jobs to process terabytes of xml format data Involved in installing configuring and managing Hadoop Ecosystem components like Hive Pig Sqoop Kafka and Flume Assisted in exporting analyzed data to relational databases using Sqoop Wrote Hive Queries and UDFs Developed Hive queries to process the data and generate the data cubes for visualizing Environment MapReduce Spark HDFS Pig HBase Oozie Zookeeper Sqoop Cassandra Linux Kafka XML Hadoop Maven NoSQL MySQL Hive Java Eclipse Python Hadoop Developer Big Data February 2016 to December 2016 Onblick Iving TX Responsibilities Created Hive tables and working on them using Hive QL Involved in installing Hadoop Ecosystem components Validated Name node Data node status in a HDFS cluster Importing and exporting data from HDFS to RDBMS and viceversa using SQOOP Experienced in developing HIVE Queries on different data formats like Text file CSV file Developed multiple MapReduce jobs in java for data cleaning and preprocessing Collecting and aggregating large amounts of log data using Apache Flume and staging data in HDFS for further analysis Installed and configured Hadoop cluster in Test and Production environments Code review as per the customer coding standards Testing and providing the valid test data to users as per requirement Weekly meetings with technical collaborators and active participation in code review sessions with senior and junior developers Responsible to manage data coming from different sources Supporting Hbase Architecture Design with the Hadoop Architect team to develop a Database Design in HDFS Involved in HDFS maintenance and loading of structured and unstructured data Wrote Hive queries for data analysis to meet the business requirements Installed and configured Pig and also written Pig Latin scripts Developed UDFs for Pig Data Analysis Involved in managing and reviewing Hadoop log files Developed Scripts and Batch Job to schedule various Hadoop Program Environment Java Hadoop MapReduce HDFS Hive Pig Sqoop Zookeeper XML Eclipse Cloudera Java Developer RIG Hyderabad Telangana August 2013 to December 2014 Responsibilities Individually worked on all the stages of a Software Development Life Cycle SDLC Used JavaScript code HTML and CSS style declarations to enrich websites Implemented the application using Spring MVC Framework which is based on MVC design pattern Developed application service components and configured beans using  Spring IOC Designed User Interface and the business logic for customer registration and maintenance Integrating Web services and working with data in different servers Involved in designing and Development of SOA services using Web Services Understanding the requirements from business users and end users Working with XMLXSLT files Experience creating UML class and sequence diagram Experience in Creating Tables Views Triggers Indexes Constraints and functions in SQL Server2005 Worked in content management for versioning and notifications Environment Java J2EE JSP spring Struts Hibernate Eclipse SOA WebLogic Oracle HTML CSS Web Services JUnit SVN Windows UNIX Education Masters in Computer in Computer Texas AM University 2016",
    "entities": [
        "Cloudera Hadoop",
        "Hive Pig Sqoop Kafka",
        "Work Experience Hadoop Developer Ericsson Piscataway",
        "MapReduce Hive",
        "Classic Hadoop",
        "CSV",
        "Hadoop Jobs",
        "US",
        "Present Responsibilities Responsible",
        "Sqoop",
        "node Data",
        "HIVE",
        "Supporting Hbase Architecture Design",
        "Apache Hadoop",
        "Maven",
        "Performed",
        "Hadoop Program",
        "Collecting",
        "Environment MapReduce Spark",
        "UML",
        "Hadoop Developer Hadoop",
        "Hadoop Framework",
        "J2EE J2SE Servlets JUnit",
        "ETL Tools DataStage SQL",
        "Tableau Arcadia Data",
        "SQOOP Experienced",
        "Working",
        "Node Resource",
        "log data",
        "Job Tracker Task Tracker",
        "Spark Inmemory",
        "Hadoop Ecosystem",
        "MapReduce Hive Pig Experienced",
        "Oozie Developed",
        "java",
        "CSS",
        "MVC",
        "a Software Development Life Cycle SDLC",
        "Computer Texas AM University",
        "SQL",
        "Hadoop",
        "Spring MVC Framework",
        "Spark RDD",
        "Development of SOA",
        "Spark Streaming  Skilled",
        "Worked",
        "Creating Tables Views Triggers Indexes Constraints",
        "DataFrames Expertise",
        "MapReduce",
        "Talend",
        "NoSQL",
        "Pig Data Analysis Involved",
        "WebLogic Oracle",
        "Scala Used Data Frame API",
        "Hadoop Big Data",
        "Shell",
        "Apache Hadoop Spark",
        "Oracle Used Sqoop",
        "Teradata",
        "Hadoop Demons",
        "Hadoop Architect",
        "Node",
        "Spark",
        "Hadoop Job",
        "Agile"
    ],
    "experience": "Experience in dealing with Apache Hadoop components like HDFS Map Reduce Sqoop Hive Oozie Good knowledge on Spark Inmemory capabilities and its modules Spark Streaming   Skilled in integrating Kafka with Spark streaming for high speed data processing Hands on experience with working on Spark using both Scala and python Performed various actions and transformations on spark RDDs and DataFrames Expertise in writing Hadoop Jobs for processing and analyzing data using MapReduce Hive Pig Experienced in extending Hive and Pig core functionality by writing custom UDFs using Java Handson experience on YARN MapReduce 20 architecture and it components Hands on experience using Core Java UNIX Shell scripting and RDBMS Excellent Java development skills using J2EE J2SE Servlets JUnit JSP JDBC Very good handson technical knowledge of ETL Tools DataStage SQL and PLSQL Vast Experience in Teradata and Involved in Converting Projects from Teradata to Hadoop Hands on experience working on virtualization tools like Tableau Arcadia Data Well versed with Agile working environment using JIRA and code version tools like GIT Knowledge of job workflow scheduling and monitoring tools like oozie and Zookeeper of NoSQL databases such as HBase Cassandra Researchoriented motivated proactive selfstarter with strong technical analytical and interpersonal skills Authorized to work in the US for any employer Work Experience Hadoop Developer Ericsson Piscataway NJ January 2017 to Present Responsibilities Responsible for building scalable distributed data solutions using Hadoop Job duties include design and development of various modules in Hadoop Big Data platform and processing data using MapReduce Hive SQOOP Kafka and Oozie Developed job processing scripts using Oozie workflow Implemented POC to migrate map reduce jobs into Spark RDD transformations using Scala Worked with Apache Hadoop Spark and Scala Used Data Frame API in Scala for converting the distributed collection of data organized into named columns Used different Hadoop components in Talend to design the framework Involved in Hadoop cluster task like commissioning decommissioning Nodes without any effect to running jobs and data Spark streaming collects the data from Kafka in near real time and performs necessary transformations and aggregations on the fly to build the common learner data model and persists the data in Cassandra Wrote Map Reduce jobs to discover trends in data usage by users Worked extensively with Sqoop for importing metadata from Oracle Used Sqoop to import data from SQL server to Cassandra Real streaming the data using Spark with Kafka Designed developed and did maintenance of data integration programs in a Hadoop and RDBMS environment with both traditional and nontraditional source systems as we as RDBMS and NoSQL data stores for data access and analysis Experienced in running Hadoop streaming jobs to process terabytes of xml format data Involved in installing configuring and managing Hadoop Ecosystem components like Hive Pig Sqoop Kafka and Flume Assisted in exporting analyzed data to relational databases using Sqoop Wrote Hive Queries and UDFs Developed Hive queries to process the data and generate the data cubes for visualizing Environment MapReduce Spark HDFS Pig HBase Oozie Zookeeper Sqoop Cassandra Linux Kafka XML Hadoop Maven NoSQL MySQL Hive Java Eclipse Python Hadoop Developer Big Data February 2016 to December 2016 Onblick Iving TX Responsibilities Created Hive tables and working on them using Hive QL Involved in installing Hadoop Ecosystem components Validated Name node Data node status in a HDFS cluster Importing and exporting data from HDFS to RDBMS and viceversa using SQOOP Experienced in developing HIVE Queries on different data formats like Text file CSV file Developed multiple MapReduce jobs in java for data cleaning and preprocessing Collecting and aggregating large amounts of log data using Apache Flume and staging data in HDFS for further analysis Installed and configured Hadoop cluster in Test and Production environments Code review as per the customer coding standards Testing and providing the valid test data to users as per requirement Weekly meetings with technical collaborators and active participation in code review sessions with senior and junior developers Responsible to manage data coming from different sources Supporting Hbase Architecture Design with the Hadoop Architect team to develop a Database Design in HDFS Involved in HDFS maintenance and loading of structured and unstructured data Wrote Hive queries for data analysis to meet the business requirements Installed and configured Pig and also written Pig Latin scripts Developed UDFs for Pig Data Analysis Involved in managing and reviewing Hadoop log files Developed Scripts and Batch Job to schedule various Hadoop Program Environment Java Hadoop MapReduce HDFS Hive Pig Sqoop Zookeeper XML Eclipse Cloudera Java Developer RIG Hyderabad Telangana August 2013 to December 2014 Responsibilities Individually worked on all the stages of a Software Development Life Cycle SDLC Used JavaScript code HTML and CSS style declarations to enrich websites Implemented the application using Spring MVC Framework which is based on MVC design pattern Developed application service components and configured beans using   Spring IOC Designed User Interface and the business logic for customer registration and maintenance Integrating Web services and working with data in different servers Involved in designing and Development of SOA services using Web Services Understanding the requirements from business users and end users Working with XMLXSLT files Experience creating UML class and sequence diagram Experience in Creating Tables Views Triggers Indexes Constraints and functions in SQL Server2005 Worked in content management for versioning and notifications Environment Java J2EE JSP spring Struts Hibernate Eclipse SOA WebLogic Oracle HTML CSS Web Services JUnit SVN Windows UNIX Education Masters in Computer in Computer Texas AM University 2016",
    "extracted_keywords": [
        "Hadoop",
        "Developer",
        "Hadoop",
        "span",
        "lDeveloperspan",
        "Hadoop",
        "Developer",
        "Ericsson",
        "North",
        "Brunswick",
        "NJ",
        "years",
        "experience",
        "IT",
        "Industry",
        "configuring",
        "testing",
        "Hadoop",
        "ecosystem",
        "components",
        "maintenance",
        "applications",
        "Java",
        "J2EE",
        "years",
        "Realtime",
        "experience",
        "Hadoop",
        "Framework",
        "ecosystem",
        "Multi",
        "Clustered",
        "environment",
        "Cloudera",
        "Hadoop",
        "echo",
        "system",
        "understanding",
        "Classic",
        "Hadoop",
        "Yarn",
        "architecture",
        "Hadoop",
        "Demons",
        "Job",
        "Tracker",
        "Task",
        "Tracker",
        "Name",
        "Node",
        "Data",
        "Node",
        "Secondary",
        "Name",
        "Node",
        "Resource",
        "Manager",
        "Node",
        "Manager",
        "Experience",
        "Apache",
        "Hadoop",
        "components",
        "Map",
        "Reduce",
        "Sqoop",
        "Hive",
        "Oozie",
        "knowledge",
        "Spark",
        "Inmemory",
        "capabilities",
        "modules",
        "Spark",
        "Streaming",
        "Skilled",
        "Kafka",
        "Spark",
        "streaming",
        "speed",
        "data",
        "Hands",
        "experience",
        "Spark",
        "Scala",
        "python",
        "actions",
        "transformations",
        "spark",
        "RDDs",
        "DataFrames",
        "Expertise",
        "Hadoop",
        "Jobs",
        "data",
        "MapReduce",
        "Hive",
        "Pig",
        "Hive",
        "Pig",
        "core",
        "functionality",
        "custom",
        "UDFs",
        "Java",
        "Handson",
        "experience",
        "YARN",
        "MapReduce",
        "architecture",
        "Hands",
        "experience",
        "Core",
        "Java",
        "UNIX",
        "Shell",
        "scripting",
        "Excellent",
        "Java",
        "development",
        "skills",
        "J2EE",
        "J2SE",
        "Servlets",
        "JUnit",
        "JSP",
        "JDBC",
        "handson",
        "knowledge",
        "ETL",
        "Tools",
        "DataStage",
        "SQL",
        "PLSQL",
        "Experience",
        "Teradata",
        "Converting",
        "Projects",
        "Teradata",
        "Hadoop",
        "Hands",
        "experience",
        "virtualization",
        "tools",
        "Tableau",
        "Arcadia",
        "Data",
        "environment",
        "JIRA",
        "version",
        "tools",
        "GIT",
        "Knowledge",
        "job",
        "workflow",
        "scheduling",
        "monitoring",
        "tools",
        "oozie",
        "Zookeeper",
        "NoSQL",
        "HBase",
        "Cassandra",
        "Researchoriented",
        "selfstarter",
        "skills",
        "US",
        "employer",
        "Work",
        "Experience",
        "Hadoop",
        "Developer",
        "Ericsson",
        "Piscataway",
        "NJ",
        "January",
        "Present",
        "Responsibilities",
        "data",
        "solutions",
        "Hadoop",
        "Job",
        "duties",
        "design",
        "development",
        "modules",
        "Hadoop",
        "Big",
        "Data",
        "platform",
        "processing",
        "data",
        "MapReduce",
        "Hive",
        "SQOOP",
        "Kafka",
        "Oozie",
        "job",
        "processing",
        "scripts",
        "Oozie",
        "workflow",
        "Implemented",
        "POC",
        "map",
        "jobs",
        "Spark",
        "RDD",
        "transformations",
        "Scala",
        "Worked",
        "Apache",
        "Hadoop",
        "Spark",
        "Scala",
        "Data",
        "Frame",
        "API",
        "Scala",
        "collection",
        "data",
        "columns",
        "Hadoop",
        "components",
        "Talend",
        "framework",
        "Hadoop",
        "cluster",
        "task",
        "Nodes",
        "effect",
        "jobs",
        "data",
        "Spark",
        "streaming",
        "data",
        "Kafka",
        "time",
        "transformations",
        "aggregations",
        "fly",
        "learner",
        "data",
        "model",
        "data",
        "Cassandra",
        "Wrote",
        "Map",
        "jobs",
        "trends",
        "data",
        "usage",
        "users",
        "Sqoop",
        "metadata",
        "Oracle",
        "Used",
        "Sqoop",
        "data",
        "SQL",
        "server",
        "Cassandra",
        "Real",
        "data",
        "Spark",
        "Kafka",
        "maintenance",
        "data",
        "integration",
        "programs",
        "Hadoop",
        "RDBMS",
        "environment",
        "source",
        "systems",
        "RDBMS",
        "NoSQL",
        "data",
        "stores",
        "data",
        "access",
        "analysis",
        "Hadoop",
        "streaming",
        "jobs",
        "terabytes",
        "xml",
        "format",
        "data",
        "configuring",
        "Hadoop",
        "Ecosystem",
        "components",
        "Hive",
        "Pig",
        "Sqoop",
        "Kafka",
        "Flume",
        "Assisted",
        "data",
        "databases",
        "Sqoop",
        "Wrote",
        "Hive",
        "Queries",
        "UDFs",
        "Developed",
        "Hive",
        "queries",
        "data",
        "data",
        "cubes",
        "Environment",
        "MapReduce",
        "Spark",
        "HDFS",
        "Pig",
        "HBase",
        "Oozie",
        "Zookeeper",
        "Sqoop",
        "Cassandra",
        "Linux",
        "Kafka",
        "XML",
        "Hadoop",
        "Maven",
        "NoSQL",
        "MySQL",
        "Hive",
        "Java",
        "Eclipse",
        "Python",
        "Hadoop",
        "Developer",
        "Big",
        "Data",
        "February",
        "December",
        "Onblick",
        "Iving",
        "TX",
        "Responsibilities",
        "Hive",
        "tables",
        "Hive",
        "QL",
        "Hadoop",
        "Ecosystem",
        "Validated",
        "Name",
        "node",
        "Data",
        "node",
        "status",
        "HDFS",
        "cluster",
        "Importing",
        "data",
        "HDFS",
        "viceversa",
        "SQOOP",
        "HIVE",
        "Queries",
        "data",
        "formats",
        "Text",
        "file",
        "CSV",
        "file",
        "MapReduce",
        "jobs",
        "java",
        "data",
        "Collecting",
        "amounts",
        "log",
        "data",
        "Apache",
        "Flume",
        "data",
        "HDFS",
        "analysis",
        "Hadoop",
        "cluster",
        "Test",
        "Production",
        "Code",
        "review",
        "customer",
        "standards",
        "Testing",
        "test",
        "data",
        "users",
        "requirement",
        "meetings",
        "collaborators",
        "participation",
        "code",
        "review",
        "sessions",
        "developers",
        "data",
        "sources",
        "Hbase",
        "Architecture",
        "Design",
        "Hadoop",
        "Architect",
        "team",
        "Database",
        "Design",
        "HDFS",
        "HDFS",
        "maintenance",
        "loading",
        "data",
        "Wrote",
        "Hive",
        "data",
        "analysis",
        "business",
        "requirements",
        "Pig",
        "Pig",
        "Latin",
        "UDFs",
        "Pig",
        "Data",
        "Analysis",
        "Hadoop",
        "log",
        "Developed",
        "Scripts",
        "Batch",
        "Job",
        "Hadoop",
        "Program",
        "Environment",
        "Java",
        "Hadoop",
        "MapReduce",
        "HDFS",
        "Hive",
        "Pig",
        "Sqoop",
        "Zookeeper",
        "XML",
        "Eclipse",
        "Cloudera",
        "Java",
        "Developer",
        "RIG",
        "Hyderabad",
        "Telangana",
        "August",
        "December",
        "Responsibilities",
        "stages",
        "Software",
        "Development",
        "Life",
        "Cycle",
        "SDLC",
        "JavaScript",
        "code",
        "HTML",
        "CSS",
        "style",
        "declarations",
        "websites",
        "application",
        "Spring",
        "MVC",
        "Framework",
        "MVC",
        "design",
        "pattern",
        "application",
        "service",
        "components",
        "beans",
        "Spring",
        "IOC",
        "Designed",
        "User",
        "Interface",
        "business",
        "logic",
        "customer",
        "registration",
        "maintenance",
        "Integrating",
        "Web",
        "services",
        "data",
        "servers",
        "designing",
        "Development",
        "SOA",
        "services",
        "Web",
        "Services",
        "requirements",
        "business",
        "users",
        "users",
        "files",
        "Experience",
        "UML",
        "class",
        "sequence",
        "diagram",
        "Experience",
        "Tables",
        "Views",
        "Triggers",
        "Indexes",
        "Constraints",
        "functions",
        "SQL",
        "Server2005",
        "content",
        "management",
        "notifications",
        "Environment",
        "Java",
        "J2EE",
        "JSP",
        "spring",
        "Struts",
        "Hibernate",
        "Eclipse",
        "SOA",
        "WebLogic",
        "Oracle",
        "HTML",
        "CSS",
        "Web",
        "Services",
        "JUnit",
        "SVN",
        "Windows",
        "UNIX",
        "Education",
        "Masters",
        "Computer",
        "Computer",
        "Texas",
        "AM",
        "University"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T20:51:59.672084",
    "resume_data": "Hadoop Developer Hadoop span lDeveloperspan Hadoop Developer Ericsson North Brunswick NJ 5 years of Professional experience in IT Industry in Developing Implementing configuring testing Hadoop ecosystem components and maintenance of various webbased applications using Java J2EE 3 years of Realtime experience in Hadoop Framework and its ecosystem Worked on Multi Clustered environment and setting up Cloudera Hadoop echo system Good understanding of Classic Hadoop and Yarn architecture along with various Hadoop Demons such as Job Tracker Task Tracker Name Node Data Node Secondary Name Node Resource Manager Node Manager Experience in dealing with Apache Hadoop components like HDFS Map Reduce Sqoop Hive Oozie Good knowledge on Spark Inmemory capabilities and its modules Spark Streaming SparkSQL Skilled in integrating Kafka with Spark streaming for high speed data processing Hands on experience with working on Spark using both Scala and python Performed various actions and transformations on spark RDDs and DataFrames Expertise in writing Hadoop Jobs for processing and analyzing data using MapReduce Hive Pig Experienced in extending Hive and Pig core functionality by writing custom UDFs using Java Handson experience on YARN MapReduce 20 architecture and it components Hands on experience using Core Java UNIX Shell scripting and RDBMS Excellent Java development skills using J2EE J2SE Servlets JUnit JSP JDBC Very good handson technical knowledge of ETL Tools DataStage SQL and PLSQL Vast Experience in Teradata and Involved in Converting Projects from Teradata to Hadoop Hands on experience working on virtualization tools like Tableau Arcadia Data Well versed with Agile working environment using JIRA and code version tools like GIT Knowledge of job workflow scheduling and monitoring tools like oozie and Zookeeper of NoSQL databases such as HBase Cassandra Researchoriented motivated proactive selfstarter with strong technical analytical and interpersonal skills Authorized to work in the US for any employer Work Experience Hadoop Developer Ericsson Piscataway NJ January 2017 to Present Responsibilities Responsible for building scalable distributed data solutions using Hadoop Job duties include design and development of various modules in Hadoop Big Data platform and processing data using MapReduce Hive SQOOP Kafka and Oozie Developed job processing scripts using Oozie workflow Implemented POC to migrate map reduce jobs into Spark RDD transformations using Scala Worked with Apache Hadoop Spark and Scala Used Data Frame API in Scala for converting the distributed collection of data organized into named columns Used different Hadoop components in Talend to design the framework Involved in Hadoop cluster task like commissioning decommissioning Nodes without any effect to running jobs and data Spark streaming collects the data from Kafka in near real time and performs necessary transformations and aggregations on the fly to build the common learner data model and persists the data in Cassandra Wrote Map Reduce jobs to discover trends in data usage by users Worked extensively with Sqoop for importing metadata from Oracle Used Sqoop to import data from SQL server to Cassandra Real streaming the data using Spark with Kafka Designed developed and did maintenance of data integration programs in a Hadoop and RDBMS environment with both traditional and nontraditional source systems as we as RDBMS and NoSQL data stores for data access and analysis Experienced in running Hadoop streaming jobs to process terabytes of xml format data Involved in installing configuring and managing Hadoop Ecosystem components like Hive Pig Sqoop Kafka and Flume Assisted in exporting analyzed data to relational databases using Sqoop Wrote Hive Queries and UDFs Developed Hive queries to process the data and generate the data cubes for visualizing Environment MapReduce Spark HDFS Pig HBase Oozie Zookeeper Sqoop Cassandra Linux Kafka XML Hadoop Maven NoSQL MySQL Hive Java Eclipse Python Hadoop Developer Big Data February 2016 to December 2016 Onblick Iving TX Responsibilities Created Hive tables and working on them using Hive QL Involved in installing Hadoop Ecosystem components Validated Name node Data node status in a HDFS cluster Importing and exporting data from HDFS to RDBMS and viceversa using SQOOP Experienced in developing HIVE Queries on different data formats like Text file CSV file Developed multiple MapReduce jobs in java for data cleaning and preprocessing Collecting and aggregating large amounts of log data using Apache Flume and staging data in HDFS for further analysis Installed and configured Hadoop cluster in Test and Production environments Code review as per the customer coding standards Testing and providing the valid test data to users as per requirement Weekly meetings with technical collaborators and active participation in code review sessions with senior and junior developers Responsible to manage data coming from different sources Supporting Hbase Architecture Design with the Hadoop Architect team to develop a Database Design in HDFS Involved in HDFS maintenance and loading of structured and unstructured data Wrote Hive queries for data analysis to meet the business requirements Installed and configured Pig and also written Pig Latin scripts Developed UDFs for Pig Data Analysis Involved in managing and reviewing Hadoop log files Developed Scripts and Batch Job to schedule various Hadoop Program Environment Java Hadoop MapReduce HDFS Hive Pig Sqoop Zookeeper XML Eclipse Cloudera Java Developer RIG Hyderabad Telangana August 2013 to December 2014 Responsibilities Individually worked on all the stages of a Software Development Life Cycle SDLC Used JavaScript code HTML and CSS style declarations to enrich websites Implemented the application using Spring MVC Framework which is based on MVC design pattern Developed application service components and configured beans using applicationContextxml Spring IOC Designed User Interface and the business logic for customer registration and maintenance Integrating Web services and working with data in different servers Involved in designing and Development of SOA services using Web Services Understanding the requirements from business users and end users Working with XMLXSLT files Experience creating UML class and sequence diagram Experience in Creating Tables Views Triggers Indexes Constraints and functions in SQL Server2005 Worked in content management for versioning and notifications Environment Java J2EE JSP spring Struts Hibernate Eclipse SOA WebLogic Oracle HTML CSS Web Services JUnit SVN Windows UNIX Education Masters in Computer in Computer Texas AM University 2016",
    "unique_id": "01a158c5-6ea0-4540-b95d-742c03f86c8c"
}