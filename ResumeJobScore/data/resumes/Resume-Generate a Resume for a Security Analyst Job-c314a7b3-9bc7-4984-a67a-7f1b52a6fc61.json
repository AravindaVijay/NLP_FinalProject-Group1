{
    "clean_data": "Job Seeker Edison NJ Work Experience Wells Fargo September 2018 to Present S P Global Ratings Sep2017 Feb2018 Spark Python Developer and Data Modeler Wells Fargo Wholesale Data Summit NJ September 2018 to Present Location Summit NJ Role Spark Python Developer and Data Modeler Project Design holistic view with conceptual and logical data model for CRM 360 for the consolidated view of 20 different lines of business LOB within the wholesale region New initiative in order to bring various LOBs in wholesale under one roof and regulate data through surveillance process for entire wholesale region Developing Spark programs using Python APIs to compare the performance of Spark with Hive and Oracle Designed and created Hive external tables using shared metastore instead of derby with partitioning dynamic partitioning and buckets Used PySparkSQL to load JSON data and create schema RDD Data Frames and loaded it into Hive Tables and handled Structured data using SparkSQL Imported required tables from RDBMS to HDFS using Sqoop and used PySpark RDDs to get real time streaming of data into HBase Analyzed the SQL scripts and designed solutions to implement using PySpark and implemented process using Python and utilizing data frames and temporary table SQL for faster processing of data Performing Business Area Analysis and logical and physical data modeling for Data Warehouse Data Mart applications as well as Operational applications enhancements and new development Develop Technical Metadata and Business Glossary for all wholesale LOBs business systems by partnering with the IT and business systems teams The data is cleaned in Oracle and loaded into a new table which is moved into HDFS using sqoop Working with Data Stewards to establish metadata registry responsibilities Tools Oracle 12C database Teradata TOAD Power Designer ERStudio SPARX Enterprise architect Pyspark Spark SQLHiveSqoop Senior PLSQL developer and Data Architect New York NY September 2017 to February 2018 Location New York NY Role Senior PLSQL developer and Data Architect Project The Surveillance Optimization is project for restructuring existing rating application along with additional surveillance optimization rule enhancement This platform has been introduced in order to accommodate new analytical solutions provided by business Created new DB objects like Tables Procedures Functions Triggers and View using Oracle PLSQL Taking care of DB Performance issues by tuning SQL queries and stored procedures by using SQL Profiler and Oracle execution plan tables Involved in writing Companys Metadata standards and practices including naming standards modeling guidelines and Data Warehouse Strategy Identified capabilities and supporting applications imbalances resulting in revisions to corporate data models and reporting capabilities Analyze data across multiple sources to design and document logical and physical data models and maintain the Enterprise Data Warehouse Data Model Gather and define data requirements based on specific application business requirements Prepare normalized models and dimensional models Define conceptual logical and physical data models Understand crucial areas in existing process and new process for rating surveillance and design the data model which will maintain and support all surveillance processes including new and existing one Communicate all designing requirement to all implementation partners which includes application database and ETL team Data Warehouse architecture data design and implementation Responsible for the Change Control and Release of Enterprise Data Models for Data Warehouse Subject Area Data Marts for all rating applications along with Logical and Physical design for Dimensional data warehouse designs using following tools CAERWIN QuestTOAD Oracle RDBMS Ralph Kimball methodology Tools Oracle 12C database PLSQL Developer TOAD Oracle SQL Developer Erwin ERStudio Perl Deutsche Bank April 2016 to August 2017 Spark Python Developer and Data ModelerAnalyst Deutsche Bank New York NY April 2016 to August 2017 Location New York NY Role Spark Python Developer and Data ModelerAnalyst Projects The Control Room ReEngineering program has been commissioned to deliver strategic technical solution which will support the functions the Global Control Room perform such as conflict and research clearance restricted list watch list management and information barrier maintenance as well LUCID is compliance hub which is designed to source data for Actimize trade surveillance data for different products like FX Equities and FX Along with trade surveillance there is pricing and valuation for the different product of the repository Involved in creating Hive tables loading with data and writing hive queries that will run internally in mapReduce way Importing and exporting data in HDFS and Hive using Sqoop Used Hive to do transformations event joins and some preaggregations before storing the data onto HDFS Used Sqoop efficiently transfer data between databases and HDFS and used PySpark to stream the log data from servers Primary business focus is in the area of trade surveillance for different IB products Pricing and Valuation compliance and control room application understanding along with complete data security and sensitivity for Control Room data Position reporting data analysis for reference security master data alignment and 13F holding fillings Developed SparkMapReduce jobs to parse the JSON files for Oracle data Data Analysis with the help of Python Scripting for AML alert data files and few process implementations to store AML Alert data in DB in Python Data analysis for Watch ListRestricted List for existing Control Room Source data mapping for transaction data for different asset classes in order utilize this data for Actimize trade surveillance modules Data AnalystData Modeler performing Business Area Analysis and logical and physical data modeling for Data Warehouse Data Mart applications as well as Operational applications enhancements and new development Used the Ralph Kimball Methodology for Data WarehouseData Mart designs Used Spark RDD and python for processing and transformation of data and integration with popular NoSQL Oracle database for huge volumes of data ER data modelling and logical database designs Metadata and data taxonomy management Ensuring integrity of backend designs and reporting data marts Initiating data design and reviews of highlevel design requirements Tools Oracle 10g 11g database PLSQL Developer TOAD Oracle SQL Developer Power Designer 165Apache Hadoop23 SqoopSparkPySpark Python Vice President April 2015 to April 2016 Lead Data Modeler and Datawarehouse Developer TD Bank Mount Laurel NJ April 2015 to April 2016 Location Mount Laurel NJ Role Lead Data Modeler and Datawarehouse Developer Projects Collects detailed data on bank holding Companies BHCs for various assets classes capital components small loans corporate loan and preprovision net revenue PPNR on monthlyquarterlyyearly basis This data will be reported to Federal Government in the form of FR Y14 FR Q14 or FR M14 reporting format Working with solution architect business analysts to define implementation design and coding of the assigned modulesresponsibilities with highest quality bug free with the help of Teradata FSLDM Involved in performance tuning of code using execution plan and SQL Profiler Oracle DB implementation and ETL process development Involved in Migration activity for few modules from IBM DB2 to Oracle Implemented Python Scripts to validate source system data Participating with key management resources in the strategic analysis and planning requirements for Data WarehouseData Mart reporting and data mining solutions Manage newly built Enterprise Data Warehouse Analytics Data Mart and the Customer Data Platform Data Quality Management and Data Architecture standardization Managed the metadata for the Subject Area models for both Operational Data WarehouseData Mart applications Loaded data from Oracle into HDFS using sqoop batch jobs for post trade transaction for different reporting applications Developed SPARK programs using Python to load data from Oracle to HDFS and Hive external tables Loaded JSON and XML file data using SparkSQL and created RDD schema to load same data into HIVE tables Implemented performance effective solutions for long running SQL processes by using PySpark Data Frame processing Implemented CSV file load processes in Oracle in PySpark by using Panda libraries Tools Oracle 10g 11g database IBM DB2 PLSQL Developer TOAD Oracle SQL Developer Power designer 165 ERStudioErwin PythonSPARK Datawarehouse Architect Cognizant Technology Solutions October 2005 to April 2015 Spark Python Developer and Data Modeler Credit Suisse Singapore March 2011 to March 2013 from Mar2011 Mar2013 and Credit Suisse New York from Apr 2013 Apr2015 Location Singapore and NY Role Spark Python Developer and Data Modeler Projects Sales Reporting Securities Intelligence Processing is state of the Reporting and Sales intelligence Applications which cater the enterprise wide Reporting and Real time data needs It includes equity trade surveillance along with sale credit calculations for equity trades Data Modeling logical dimensional and physical for various projects work with application teams to analyze data requirements review data model and address the functional and nonfunctional requirements performance geographical separation auditing archiving Core member of the solution team responsible for proposing architecture high level design for Investor Tax platform for multiple geographies US UK Singapore and India FPML data processing implementation for FX products Led data model team on Data Marts Star Schema for Volcker reporting Data model set up and implementation Define best practices for data modelling and database development Built and tuned complex and large data load from various sources to ODS Operational Data System Financial Star Schema and Campaign Analysis Mart ETL Informatica Power Center 17Oracle Analytical SQL Tools Oracle 10g 11g database PLSQL Developer TOAD Oracle SQL Developer Oracle Data Modeler Informatica Erwin Apache Hadoop 22 SqoopSparkPySpark Python Senior Database Developer Wellington Management Company Pune Maharashtra March 2009 to February 2011 Location Pune India Role Senior Database Developer Assess Troubleshoot analyze performance issues during 9i to 11g migration nightly batch processes and advise solutions SQL tuning 10053 trace analysis responsible for improving PLSQL refactoring improving performance from hours to minutes optimize distributed processing reduce database resource usage logical IOs temporary table space usages CPU consumption latch contention etc Played crucial role in golive of the 11g Upgrade project Design database objects including tables indexes views materialized views sequences and referential integrity for a reporting data warehouse Develop and maintain database programs including packages procedures functions and triggers Tools Oracle 10g 11g database PLSQL Developer TOAD Oracle SQL Developer Erwin Database Developer and Data Modeler AOL Pune Maharashtra September 2005 to February 2009 Sep2005 to Feb 2007 for ABN AMRO and from Mar 2007 Feb 2009 for RBS Location Pune India Role Database Developer and Data Modeler Projects This is online corporate reporting application developed in Java Oracle10g which is used to process ABNAMRO corporate client payment data Develop and maintain database programs including packages procedures functions and triggers using PLSQL Develop and support the Oracle PLSQL code that performs the calculations for the all fulfillment marketing programs As requested supportfulfill requests for information and troubleshoot code Tools Oracle 11g database PLSQL Developer TOAD Oracle SQL Developer Data Architect Credit Suisse Boston MA May 2005 to September 2005 Boston US ABN AMRO Bank AOLAccess Reporting Persistent Systems Ltd AMTS May2005 Sep2005 Lead Database Developer and data Modeler Home Builder Association Pune Maharashtra May 2005 to September 2005 Location Pune India Role Lead Database Developer and data Modeler Projects This is online corporate reporting application developed in NET Oracle 9i for local builders Bay Systems Pvt Ltd December 2004 to April 2005 Senior Database Developer MBTBay system Pune Maharashtra December 2004 to April 2005 Projects This is online credit card application developed in NET Oracle 9i for UAE based financial institution DSK Systems Pvt Ltd May 2002 to December 2004 Database Developer DSK Systems Pvt Ltd Pune Maharashtra May 2002 to December 2004 Projects This is online builder application which is used for DSK builders internal all site details The application developed in NET Oracle 9i for UAE based financial institution Education DAC in Advance Computing PUNE University January 2002 Bachelor of Computer Science in BSCComp Dr Babasaheb Ambedkar Marathwada University Aurangabad Maharashtra May 2001 Skills database 10 years Erwin 5 years Oracle 10 years Plsql 10 years Sql 10 years",
    "entities": [
        "Performing Business Area Analysis",
        "New York",
        "Python Scripting for AML",
        "SQL Developer",
        "Informatica Power Center 17Oracle Analytical",
        "HDFS",
        "Oracle PLSQL Taking",
        "CAERWIN QuestTOAD Oracle",
        "UK",
        "Oracle Implemented Python",
        "IBM",
        "LOB",
        "Panda",
        "ER",
        "Data Architect Project The Surveillance Optimization",
        "Database Developer Wellington Management Company",
        "RDD",
        "XML",
        "SQL Profiler Oracle DB",
        "Credit Suisse New York",
        "DAC",
        "HBase Analyzed",
        "Spark with",
        "Deutsche Bank",
        "SparkSQL",
        "ERStudio SPARX Enterprise",
        "Wells Fargo",
        "Data Warehouse",
        "Apr 2013 Apr2015 Location Singapore",
        "Data Warehouse Strategy Identified",
        "Database Developer DSK Systems Pvt Ltd",
        "Laurel NJ",
        "Teradata FSLDM Involved",
        "FX",
        "Core",
        "DSK Systems Pvt Ltd",
        "Develop",
        "Communicate",
        "AOL",
        "Advance Computing PUNE University",
        "NET Oracle",
        "Built",
        "Present S P Global Ratings Sep2017 Feb2018 Spark Python Developer and Data",
        "FX Along",
        "Location New York",
        "Data AnalystData Modeler",
        "CPU",
        "Singapore",
        "Logical and Physical",
        "SparkSQL Imported",
        "the Ralph Kimball Methodology",
        "Data WarehouseData Mart",
        "Developed SPARK",
        "View",
        "Actimize",
        "RBS Location Pune India Role Database Developer",
        "Migration",
        "ODS Operational Data System Financial Star Schema and Campaign Analysis Mart",
        "Hive Tables",
        "US",
        "Sqoop",
        "Release of Enterprise Data Models for Data Warehouse Subject Area Data Marts",
        "Spark Python Developer and Data Modeler Credit Suisse Singapore",
        "HIVE",
        "Bay Systems Pvt Ltd",
        "Created",
        "the Global Control Room",
        "SQL Profiler",
        "Oracle",
        "PySpark",
        "Data Warehouse Data Mart",
        "Oracle Data Modeler Informatica",
        "Modeler Home Builder Association",
        "Data Architect New York",
        "ABN AMRO Bank AOLAccess Reporting Persistent Systems Ltd",
        "SQL",
        "Prepare",
        "PySpark Data Frame",
        "Hive",
        "Datawarehouse Developer Projects Collects",
        "DSK",
        "ETL",
        "DB",
        "ABN AMRO",
        "Location Mount Laurel NJ",
        "India",
        "Datawarehouse Developer TD Bank",
        "Spark Python Developer and Data ModelerAnalyst Deutsche Bank",
        "Oracle Designed",
        "Business Area Analysis",
        "Control Room data Position",
        "Working with Data Stewards",
        "Pyspark Spark",
        "Y14",
        "Bachelor of Computer Science",
        "Operational Data WarehouseData Mart",
        "Data Modeling",
        "Present Location Summit NJ Role Spark Python Developer and Data Modeler Project Design",
        "Data Marts Star Schema for Volcker",
        "Boston",
        "Structured",
        "Federal Government",
        "Tables Procedures Functions Triggers",
        "Oracle data Data Analysis",
        "FX Equities",
        "AML Alert"
    ],
    "experience": "Experience Wells Fargo September 2018 to Present S P Global Ratings Sep2017 Feb2018 Spark Python Developer and Data Modeler Wells Fargo Wholesale Data Summit NJ September 2018 to Present Location Summit NJ Role Spark Python Developer and Data Modeler Project Design holistic view with conceptual and logical data model for CRM 360 for the consolidated view of 20 different lines of business LOB within the wholesale region New initiative in order to bring various LOBs in wholesale under one roof and regulate data through surveillance process for entire wholesale region Developing Spark programs using Python APIs to compare the performance of Spark with Hive and Oracle Designed and created Hive external tables using shared metastore instead of derby with partitioning dynamic partitioning and buckets Used PySparkSQL to load JSON data and create schema RDD Data Frames and loaded it into Hive Tables and handled Structured data using SparkSQL Imported required tables from RDBMS to HDFS using Sqoop and used PySpark RDDs to get real time streaming of data into HBase Analyzed the SQL scripts and designed solutions to implement using PySpark and implemented process using Python and utilizing data frames and temporary table SQL for faster processing of data Performing Business Area Analysis and logical and physical data modeling for Data Warehouse Data Mart applications as well as Operational applications enhancements and new development Develop Technical Metadata and Business Glossary for all wholesale LOBs business systems by partnering with the IT and business systems teams The data is cleaned in Oracle and loaded into a new table which is moved into HDFS using sqoop Working with Data Stewards to establish metadata registry responsibilities Tools Oracle 12C database Teradata TOAD Power Designer ERStudio SPARX Enterprise architect Pyspark Spark SQLHiveSqoop Senior PLSQL developer and Data Architect New York NY September 2017 to February 2018 Location New York NY Role Senior PLSQL developer and Data Architect Project The Surveillance Optimization is project for restructuring existing rating application along with additional surveillance optimization rule enhancement This platform has been introduced in order to accommodate new analytical solutions provided by business Created new DB objects like Tables Procedures Functions Triggers and View using Oracle PLSQL Taking care of DB Performance issues by tuning SQL queries and stored procedures by using SQL Profiler and Oracle execution plan tables Involved in writing Companys Metadata standards and practices including naming standards modeling guidelines and Data Warehouse Strategy Identified capabilities and supporting applications imbalances resulting in revisions to corporate data models and reporting capabilities Analyze data across multiple sources to design and document logical and physical data models and maintain the Enterprise Data Warehouse Data Model Gather and define data requirements based on specific application business requirements Prepare normalized models and dimensional models Define conceptual logical and physical data models Understand crucial areas in existing process and new process for rating surveillance and design the data model which will maintain and support all surveillance processes including new and existing one Communicate all designing requirement to all implementation partners which includes application database and ETL team Data Warehouse architecture data design and implementation Responsible for the Change Control and Release of Enterprise Data Models for Data Warehouse Subject Area Data Marts for all rating applications along with Logical and Physical design for Dimensional data warehouse designs using following tools CAERWIN QuestTOAD Oracle RDBMS Ralph Kimball methodology Tools Oracle 12C database PLSQL Developer TOAD Oracle SQL Developer Erwin ERStudio Perl Deutsche Bank April 2016 to August 2017 Spark Python Developer and Data ModelerAnalyst Deutsche Bank New York NY April 2016 to August 2017 Location New York NY Role Spark Python Developer and Data ModelerAnalyst Projects The Control Room ReEngineering program has been commissioned to deliver strategic technical solution which will support the functions the Global Control Room perform such as conflict and research clearance restricted list watch list management and information barrier maintenance as well LUCID is compliance hub which is designed to source data for Actimize trade surveillance data for different products like FX Equities and FX Along with trade surveillance there is pricing and valuation for the different product of the repository Involved in creating Hive tables loading with data and writing hive queries that will run internally in mapReduce way Importing and exporting data in HDFS and Hive using Sqoop Used Hive to do transformations event joins and some preaggregations before storing the data onto HDFS Used Sqoop efficiently transfer data between databases and HDFS and used PySpark to stream the log data from servers Primary business focus is in the area of trade surveillance for different IB products Pricing and Valuation compliance and control room application understanding along with complete data security and sensitivity for Control Room data Position reporting data analysis for reference security master data alignment and 13F holding fillings Developed SparkMapReduce jobs to parse the JSON files for Oracle data Data Analysis with the help of Python Scripting for AML alert data files and few process implementations to store AML Alert data in DB in Python Data analysis for Watch ListRestricted List for existing Control Room Source data mapping for transaction data for different asset classes in order utilize this data for Actimize trade surveillance modules Data AnalystData Modeler performing Business Area Analysis and logical and physical data modeling for Data Warehouse Data Mart applications as well as Operational applications enhancements and new development Used the Ralph Kimball Methodology for Data WarehouseData Mart designs Used Spark RDD and python for processing and transformation of data and integration with popular NoSQL Oracle database for huge volumes of data ER data modelling and logical database designs Metadata and data taxonomy management Ensuring integrity of backend designs and reporting data marts Initiating data design and reviews of highlevel design requirements Tools Oracle 10 g 11 g database PLSQL Developer TOAD Oracle SQL Developer Power Designer 165Apache Hadoop23 SqoopSparkPySpark Python Vice President April 2015 to April 2016 Lead Data Modeler and Datawarehouse Developer TD Bank Mount Laurel NJ April 2015 to April 2016 Location Mount Laurel NJ Role Lead Data Modeler and Datawarehouse Developer Projects Collects detailed data on bank holding Companies BHCs for various assets classes capital components small loans corporate loan and preprovision net revenue PPNR on monthlyquarterlyyearly basis This data will be reported to Federal Government in the form of FR Y14 FR Q14 or FR M14 reporting format Working with solution architect business analysts to define implementation design and coding of the assigned modulesresponsibilities with highest quality bug free with the help of Teradata FSLDM Involved in performance tuning of code using execution plan and SQL Profiler Oracle DB implementation and ETL process development Involved in Migration activity for few modules from IBM DB2 to Oracle Implemented Python Scripts to validate source system data Participating with key management resources in the strategic analysis and planning requirements for Data WarehouseData Mart reporting and data mining solutions Manage newly built Enterprise Data Warehouse Analytics Data Mart and the Customer Data Platform Data Quality Management and Data Architecture standardization Managed the metadata for the Subject Area models for both Operational Data WarehouseData Mart applications Loaded data from Oracle into HDFS using sqoop batch jobs for post trade transaction for different reporting applications Developed SPARK programs using Python to load data from Oracle to HDFS and Hive external tables Loaded JSON and XML file data using SparkSQL and created RDD schema to load same data into HIVE tables Implemented performance effective solutions for long running SQL processes by using PySpark Data Frame processing Implemented CSV file load processes in Oracle in PySpark by using Panda libraries Tools Oracle 10 g 11 g database IBM DB2 PLSQL Developer TOAD Oracle SQL Developer Power designer 165 ERStudioErwin PythonSPARK Datawarehouse Architect Cognizant Technology Solutions October 2005 to April 2015 Spark Python Developer and Data Modeler Credit Suisse Singapore March 2011 to March 2013 from Mar2011 Mar2013 and Credit Suisse New York from Apr 2013 Apr2015 Location Singapore and NY Role Spark Python Developer and Data Modeler Projects Sales Reporting Securities Intelligence Processing is state of the Reporting and Sales intelligence Applications which cater the enterprise wide Reporting and Real time data needs It includes equity trade surveillance along with sale credit calculations for equity trades Data Modeling logical dimensional and physical for various projects work with application teams to analyze data requirements review data model and address the functional and nonfunctional requirements performance geographical separation auditing archiving Core member of the solution team responsible for proposing architecture high level design for Investor Tax platform for multiple geographies US UK Singapore and India FPML data processing implementation for FX products Led data model team on Data Marts Star Schema for Volcker reporting Data model set up and implementation Define best practices for data modelling and database development Built and tuned complex and large data load from various sources to ODS Operational Data System Financial Star Schema and Campaign Analysis Mart ETL Informatica Power Center 17Oracle Analytical SQL Tools Oracle 10 g 11 g database PLSQL Developer TOAD Oracle SQL Developer Oracle Data Modeler Informatica Erwin Apache Hadoop 22 SqoopSparkPySpark Python Senior Database Developer Wellington Management Company Pune Maharashtra March 2009 to February 2011 Location Pune India Role Senior Database Developer Assess Troubleshoot analyze performance issues during 9i to 11 g migration nightly batch processes and advise solutions SQL tuning 10053 trace analysis responsible for improving PLSQL refactoring improving performance from hours to minutes optimize distributed processing reduce database resource usage logical IOs temporary table space usages CPU consumption latch contention etc Played crucial role in golive of the 11 g Upgrade project Design database objects including tables indexes views materialized views sequences and referential integrity for a reporting data warehouse Develop and maintain database programs including packages procedures functions and triggers Tools Oracle 10 g 11 g database PLSQL Developer TOAD Oracle SQL Developer Erwin Database Developer and Data Modeler AOL Pune Maharashtra September 2005 to February 2009 Sep2005 to Feb 2007 for ABN AMRO and from Mar 2007 Feb 2009 for RBS Location Pune India Role Database Developer and Data Modeler Projects This is online corporate reporting application developed in Java Oracle10 g which is used to process ABNAMRO corporate client payment data Develop and maintain database programs including packages procedures functions and triggers using PLSQL Develop and support the Oracle PLSQL code that performs the calculations for the all fulfillment marketing programs As requested supportfulfill requests for information and troubleshoot code Tools Oracle 11 g database PLSQL Developer TOAD Oracle SQL Developer Data Architect Credit Suisse Boston MA May 2005 to September 2005 Boston US ABN AMRO Bank AOLAccess Reporting Persistent Systems Ltd AMTS May2005 Sep2005 Lead Database Developer and data Modeler Home Builder Association Pune Maharashtra May 2005 to September 2005 Location Pune India Role Lead Database Developer and data Modeler Projects This is online corporate reporting application developed in NET Oracle 9i for local builders Bay Systems Pvt Ltd December 2004 to April 2005 Senior Database Developer MBTBay system Pune Maharashtra December 2004 to April 2005 Projects This is online credit card application developed in NET Oracle 9i for UAE based financial institution DSK Systems Pvt Ltd May 2002 to December 2004 Database Developer DSK Systems Pvt Ltd Pune Maharashtra May 2002 to December 2004 Projects This is online builder application which is used for DSK builders internal all site details The application developed in NET Oracle 9i for UAE based financial institution Education DAC in Advance Computing PUNE University January 2002 Bachelor of Computer Science in BSCComp Dr Babasaheb Ambedkar Marathwada University Aurangabad Maharashtra May 2001 Skills database 10 years Erwin 5 years Oracle 10 years Plsql 10 years Sql 10 years",
    "extracted_keywords": [
        "Job",
        "Seeker",
        "Edison",
        "NJ",
        "Work",
        "Experience",
        "Wells",
        "Fargo",
        "September",
        "Present",
        "S",
        "P",
        "Global",
        "Ratings",
        "Sep2017",
        "Feb2018",
        "Spark",
        "Python",
        "Developer",
        "Data",
        "Modeler",
        "Wells",
        "Fargo",
        "Wholesale",
        "Data",
        "Summit",
        "NJ",
        "September",
        "Present",
        "Location",
        "Summit",
        "NJ",
        "Role",
        "Spark",
        "Python",
        "Developer",
        "Data",
        "Modeler",
        "Project",
        "Design",
        "view",
        "data",
        "model",
        "CRM",
        "view",
        "lines",
        "business",
        "LOB",
        "region",
        "New",
        "initiative",
        "order",
        "LOBs",
        "roof",
        "data",
        "surveillance",
        "process",
        "region",
        "Spark",
        "programs",
        "Python",
        "APIs",
        "performance",
        "Spark",
        "Hive",
        "Oracle",
        "Designed",
        "Hive",
        "tables",
        "metastore",
        "derby",
        "partitioning",
        "buckets",
        "PySparkSQL",
        "data",
        "schema",
        "RDD",
        "Data",
        "Frames",
        "Hive",
        "Tables",
        "data",
        "SparkSQL",
        "Imported",
        "tables",
        "RDBMS",
        "HDFS",
        "Sqoop",
        "PySpark",
        "RDDs",
        "time",
        "streaming",
        "data",
        "HBase",
        "SQL",
        "scripts",
        "solutions",
        "PySpark",
        "process",
        "Python",
        "data",
        "frames",
        "table",
        "SQL",
        "processing",
        "data",
        "Performing",
        "Business",
        "Area",
        "Analysis",
        "data",
        "Data",
        "Warehouse",
        "Data",
        "Mart",
        "applications",
        "applications",
        "enhancements",
        "development",
        "Develop",
        "Technical",
        "Metadata",
        "Business",
        "Glossary",
        "LOBs",
        "business",
        "systems",
        "IT",
        "business",
        "systems",
        "teams",
        "data",
        "Oracle",
        "table",
        "HDFS",
        "sqoop",
        "Working",
        "Data",
        "Stewards",
        "metadata",
        "registry",
        "responsibilities",
        "Tools",
        "Oracle",
        "database",
        "Teradata",
        "TOAD",
        "Power",
        "Designer",
        "ERStudio",
        "SPARX",
        "Enterprise",
        "architect",
        "Pyspark",
        "Spark",
        "SQLHiveSqoop",
        "Senior",
        "PLSQL",
        "developer",
        "Data",
        "Architect",
        "New",
        "York",
        "NY",
        "September",
        "February",
        "Location",
        "New",
        "York",
        "NY",
        "Role",
        "Senior",
        "PLSQL",
        "developer",
        "Data",
        "Architect",
        "Project",
        "Surveillance",
        "Optimization",
        "project",
        "rating",
        "application",
        "surveillance",
        "optimization",
        "rule",
        "enhancement",
        "platform",
        "order",
        "solutions",
        "business",
        "DB",
        "Tables",
        "Procedures",
        "Functions",
        "Triggers",
        "View",
        "Oracle",
        "PLSQL",
        "care",
        "DB",
        "Performance",
        "issues",
        "SQL",
        "queries",
        "procedures",
        "SQL",
        "Profiler",
        "Oracle",
        "execution",
        "plan",
        "tables",
        "Companys",
        "Metadata",
        "standards",
        "practices",
        "naming",
        "standards",
        "guidelines",
        "Data",
        "Warehouse",
        "Strategy",
        "capabilities",
        "applications",
        "imbalances",
        "revisions",
        "data",
        "models",
        "capabilities",
        "Analyze",
        "data",
        "sources",
        "document",
        "data",
        "models",
        "Enterprise",
        "Data",
        "Warehouse",
        "Data",
        "Model",
        "Gather",
        "data",
        "requirements",
        "application",
        "business",
        "requirements",
        "Prepare",
        "models",
        "models",
        "data",
        "models",
        "areas",
        "process",
        "process",
        "surveillance",
        "data",
        "model",
        "surveillance",
        "processes",
        "Communicate",
        "designing",
        "requirement",
        "implementation",
        "partners",
        "application",
        "database",
        "ETL",
        "team",
        "Data",
        "Warehouse",
        "architecture",
        "data",
        "design",
        "implementation",
        "Change",
        "Control",
        "Release",
        "Enterprise",
        "Data",
        "Models",
        "Data",
        "Warehouse",
        "Subject",
        "Area",
        "Data",
        "Marts",
        "rating",
        "applications",
        "design",
        "data",
        "warehouse",
        "tools",
        "CAERWIN",
        "Oracle",
        "RDBMS",
        "Ralph",
        "Kimball",
        "methodology",
        "Tools",
        "Oracle",
        "database",
        "PLSQL",
        "Developer",
        "TOAD",
        "Oracle",
        "SQL",
        "Developer",
        "Erwin",
        "ERStudio",
        "Perl",
        "Deutsche",
        "Bank",
        "April",
        "August",
        "Spark",
        "Python",
        "Developer",
        "Data",
        "ModelerAnalyst",
        "Deutsche",
        "Bank",
        "New",
        "York",
        "NY",
        "April",
        "August",
        "Location",
        "New",
        "York",
        "NY",
        "Role",
        "Spark",
        "Python",
        "Developer",
        "Data",
        "ModelerAnalyst",
        "Projects",
        "Control",
        "Room",
        "ReEngineering",
        "program",
        "solution",
        "functions",
        "Global",
        "Control",
        "Room",
        "conflict",
        "research",
        "clearance",
        "list",
        "watch",
        "list",
        "management",
        "information",
        "barrier",
        "maintenance",
        "LUCID",
        "compliance",
        "hub",
        "data",
        "Actimize",
        "trade",
        "surveillance",
        "data",
        "products",
        "FX",
        "Equities",
        "FX",
        "trade",
        "surveillance",
        "pricing",
        "valuation",
        "product",
        "repository",
        "Hive",
        "tables",
        "data",
        "hive",
        "queries",
        "mapReduce",
        "way",
        "data",
        "HDFS",
        "Hive",
        "Sqoop",
        "Hive",
        "transformations",
        "event",
        "preaggregations",
        "data",
        "HDFS",
        "Sqoop",
        "data",
        "databases",
        "HDFS",
        "PySpark",
        "log",
        "data",
        "servers",
        "Primary",
        "business",
        "focus",
        "area",
        "trade",
        "surveillance",
        "IB",
        "products",
        "Pricing",
        "Valuation",
        "compliance",
        "control",
        "room",
        "application",
        "understanding",
        "data",
        "security",
        "sensitivity",
        "Control",
        "Room",
        "data",
        "Position",
        "data",
        "analysis",
        "reference",
        "security",
        "master",
        "data",
        "alignment",
        "fillings",
        "SparkMapReduce",
        "jobs",
        "files",
        "Oracle",
        "data",
        "Data",
        "Analysis",
        "help",
        "Python",
        "Scripting",
        "AML",
        "alert",
        "data",
        "files",
        "process",
        "implementations",
        "AML",
        "Alert",
        "data",
        "DB",
        "Python",
        "Data",
        "analysis",
        "Watch",
        "ListRestricted",
        "List",
        "Control",
        "Room",
        "Source",
        "data",
        "mapping",
        "transaction",
        "data",
        "asset",
        "classes",
        "order",
        "data",
        "Actimize",
        "trade",
        "surveillance",
        "modules",
        "Data",
        "AnalystData",
        "Modeler",
        "Business",
        "Area",
        "Analysis",
        "data",
        "Data",
        "Warehouse",
        "Data",
        "Mart",
        "applications",
        "applications",
        "enhancements",
        "development",
        "Ralph",
        "Kimball",
        "Methodology",
        "Data",
        "WarehouseData",
        "Mart",
        "Spark",
        "RDD",
        "processing",
        "transformation",
        "data",
        "integration",
        "NoSQL",
        "Oracle",
        "database",
        "volumes",
        "data",
        "ER",
        "data",
        "modelling",
        "database",
        "Metadata",
        "data",
        "taxonomy",
        "management",
        "Ensuring",
        "integrity",
        "designs",
        "data",
        "marts",
        "data",
        "design",
        "reviews",
        "highlevel",
        "design",
        "requirements",
        "Tools",
        "Oracle",
        "g",
        "g",
        "database",
        "PLSQL",
        "Developer",
        "TOAD",
        "Oracle",
        "SQL",
        "Developer",
        "Power",
        "Designer",
        "165Apache",
        "Hadoop23",
        "SqoopSparkPySpark",
        "Python",
        "Vice",
        "President",
        "April",
        "April",
        "Lead",
        "Data",
        "Modeler",
        "Datawarehouse",
        "Developer",
        "TD",
        "Bank",
        "Mount",
        "Laurel",
        "NJ",
        "April",
        "April",
        "Location",
        "Mount",
        "Laurel",
        "NJ",
        "Role",
        "Lead",
        "Data",
        "Modeler",
        "Datawarehouse",
        "Developer",
        "Projects",
        "data",
        "bank",
        "Companies",
        "BHCs",
        "assets",
        "classes",
        "capital",
        "components",
        "loans",
        "loan",
        "preprovision",
        "revenue",
        "PPNR",
        "basis",
        "data",
        "Federal",
        "Government",
        "form",
        "FR",
        "Y14",
        "FR",
        "Q14",
        "FR",
        "M14",
        "reporting",
        "format",
        "solution",
        "architect",
        "business",
        "analysts",
        "implementation",
        "design",
        "coding",
        "modulesresponsibilities",
        "quality",
        "bug",
        "help",
        "Teradata",
        "FSLDM",
        "performance",
        "tuning",
        "code",
        "execution",
        "plan",
        "SQL",
        "Profiler",
        "Oracle",
        "DB",
        "implementation",
        "ETL",
        "process",
        "development",
        "Migration",
        "activity",
        "modules",
        "IBM",
        "DB2",
        "Oracle",
        "Python",
        "Scripts",
        "source",
        "system",
        "data",
        "management",
        "resources",
        "analysis",
        "planning",
        "requirements",
        "Data",
        "WarehouseData",
        "Mart",
        "data",
        "mining",
        "solutions",
        "Manage",
        "Enterprise",
        "Data",
        "Warehouse",
        "Analytics",
        "Data",
        "Mart",
        "Customer",
        "Data",
        "Platform",
        "Data",
        "Quality",
        "Management",
        "Data",
        "Architecture",
        "standardization",
        "metadata",
        "Subject",
        "Area",
        "models",
        "Operational",
        "Data",
        "WarehouseData",
        "Mart",
        "data",
        "Oracle",
        "HDFS",
        "sqoop",
        "batch",
        "jobs",
        "post",
        "trade",
        "transaction",
        "reporting",
        "applications",
        "SPARK",
        "programs",
        "Python",
        "data",
        "Oracle",
        "HDFS",
        "tables",
        "JSON",
        "XML",
        "file",
        "data",
        "SparkSQL",
        "RDD",
        "schema",
        "data",
        "HIVE",
        "tables",
        "performance",
        "solutions",
        "SQL",
        "processes",
        "PySpark",
        "Data",
        "Frame",
        "processing",
        "CSV",
        "file",
        "load",
        "processes",
        "Oracle",
        "PySpark",
        "Panda",
        "libraries",
        "Tools",
        "Oracle",
        "g",
        "g",
        "database",
        "IBM",
        "DB2",
        "PLSQL",
        "Developer",
        "TOAD",
        "Oracle",
        "SQL",
        "Developer",
        "Power",
        "designer",
        "ERStudioErwin",
        "PythonSPARK",
        "Datawarehouse",
        "Architect",
        "Cognizant",
        "Technology",
        "Solutions",
        "October",
        "April",
        "Spark",
        "Python",
        "Developer",
        "Data",
        "Modeler",
        "Credit",
        "Suisse",
        "Singapore",
        "March",
        "March",
        "Mar2011",
        "Mar2013",
        "Credit",
        "Suisse",
        "New",
        "York",
        "Apr",
        "Apr2015",
        "Location",
        "Singapore",
        "NY",
        "Role",
        "Spark",
        "Python",
        "Developer",
        "Data",
        "Modeler",
        "Projects",
        "Sales",
        "Reporting",
        "Securities",
        "Intelligence",
        "Processing",
        "state",
        "Reporting",
        "Sales",
        "intelligence",
        "Applications",
        "enterprise",
        "Reporting",
        "time",
        "data",
        "equity",
        "trade",
        "surveillance",
        "sale",
        "credit",
        "calculations",
        "equity",
        "trades",
        "Data",
        "Modeling",
        "projects",
        "application",
        "teams",
        "data",
        "requirements",
        "data",
        "model",
        "requirements",
        "separation",
        "Core",
        "member",
        "solution",
        "team",
        "architecture",
        "level",
        "design",
        "Investor",
        "Tax",
        "platform",
        "geographies",
        "US",
        "UK",
        "Singapore",
        "India",
        "FPML",
        "data",
        "processing",
        "implementation",
        "FX",
        "products",
        "data",
        "model",
        "team",
        "Data",
        "Marts",
        "Star",
        "Schema",
        "Volcker",
        "Data",
        "model",
        "implementation",
        "Define",
        "practices",
        "data",
        "modelling",
        "database",
        "development",
        "data",
        "load",
        "sources",
        "ODS",
        "Operational",
        "Data",
        "System",
        "Financial",
        "Star",
        "Schema",
        "Campaign",
        "Analysis",
        "Mart",
        "ETL",
        "Informatica",
        "Power",
        "Center",
        "17Oracle",
        "Analytical",
        "SQL",
        "Tools",
        "Oracle",
        "g",
        "g",
        "database",
        "PLSQL",
        "Developer",
        "TOAD",
        "Oracle",
        "SQL",
        "Developer",
        "Oracle",
        "Data",
        "Modeler",
        "Informatica",
        "Erwin",
        "Apache",
        "Hadoop",
        "SqoopSparkPySpark",
        "Python",
        "Senior",
        "Database",
        "Developer",
        "Wellington",
        "Management",
        "Company",
        "Pune",
        "Maharashtra",
        "March",
        "February",
        "Location",
        "Pune",
        "India",
        "Role",
        "Senior",
        "Database",
        "Developer",
        "Assess",
        "Troubleshoot",
        "performance",
        "issues",
        "9i",
        "g",
        "migration",
        "batch",
        "processes",
        "solutions",
        "SQL",
        "trace",
        "analysis",
        "PLSQL",
        "refactoring",
        "performance",
        "hours",
        "minutes",
        "processing",
        "database",
        "resource",
        "usage",
        "table",
        "space",
        "usages",
        "CPU",
        "consumption",
        "latch",
        "contention",
        "role",
        "g",
        "Upgrade",
        "project",
        "Design",
        "database",
        "objects",
        "tables",
        "indexes",
        "views",
        "views",
        "sequences",
        "integrity",
        "data",
        "warehouse",
        "database",
        "programs",
        "packages",
        "procedures",
        "functions",
        "Tools",
        "Oracle",
        "g",
        "g",
        "database",
        "PLSQL",
        "Developer",
        "TOAD",
        "Oracle",
        "SQL",
        "Developer",
        "Erwin",
        "Database",
        "Developer",
        "Data",
        "Modeler",
        "AOL",
        "Pune",
        "Maharashtra",
        "September",
        "February",
        "Sep2005",
        "Feb",
        "ABN",
        "AMRO",
        "Mar",
        "Feb",
        "RBS",
        "Location",
        "Pune",
        "India",
        "Role",
        "Database",
        "Developer",
        "Data",
        "Modeler",
        "Projects",
        "reporting",
        "application",
        "Java",
        "Oracle10",
        "g",
        "ABNAMRO",
        "client",
        "payment",
        "data",
        "Develop",
        "database",
        "programs",
        "packages",
        "procedures",
        "functions",
        "triggers",
        "PLSQL",
        "Develop",
        "Oracle",
        "PLSQL",
        "code",
        "calculations",
        "fulfillment",
        "marketing",
        "programs",
        "requests",
        "information",
        "troubleshoot",
        "code",
        "Tools",
        "Oracle",
        "g",
        "database",
        "PLSQL",
        "Developer",
        "TOAD",
        "Oracle",
        "SQL",
        "Developer",
        "Data",
        "Architect",
        "Credit",
        "Suisse",
        "Boston",
        "MA",
        "May",
        "September",
        "Boston",
        "US",
        "ABN",
        "AMRO",
        "Bank",
        "AOLAccess",
        "Reporting",
        "Persistent",
        "Systems",
        "Ltd",
        "AMTS",
        "May2005",
        "Sep2005",
        "Lead",
        "Database",
        "Developer",
        "data",
        "Modeler",
        "Home",
        "Builder",
        "Association",
        "Pune",
        "Maharashtra",
        "May",
        "September",
        "Location",
        "Pune",
        "India",
        "Role",
        "Lead",
        "Database",
        "Developer",
        "data",
        "Modeler",
        "Projects",
        "reporting",
        "application",
        "NET",
        "Oracle",
        "9i",
        "builders",
        "Bay",
        "Systems",
        "Pvt",
        "Ltd",
        "December",
        "April",
        "Senior",
        "Database",
        "Developer",
        "MBTBay",
        "system",
        "Pune",
        "Maharashtra",
        "December",
        "April",
        "Projects",
        "credit",
        "card",
        "application",
        "NET",
        "Oracle",
        "9i",
        "UAE",
        "institution",
        "DSK",
        "Systems",
        "Pvt",
        "Ltd",
        "May",
        "December",
        "Database",
        "Developer",
        "DSK",
        "Systems",
        "Pvt",
        "Ltd",
        "Pune",
        "Maharashtra",
        "May",
        "December",
        "Projects",
        "builder",
        "application",
        "DSK",
        "builders",
        "site",
        "details",
        "application",
        "NET",
        "Oracle",
        "9i",
        "UAE",
        "institution",
        "Education",
        "DAC",
        "Advance",
        "Computing",
        "PUNE",
        "University",
        "January",
        "Bachelor",
        "Computer",
        "Science",
        "BSCComp",
        "Dr",
        "Babasaheb",
        "Ambedkar",
        "Marathwada",
        "University",
        "Aurangabad",
        "Maharashtra",
        "May",
        "Skills",
        "database",
        "years",
        "Erwin",
        "years",
        "Oracle",
        "years",
        "Plsql",
        "years",
        "Sql",
        "years"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T21:26:27.978741",
    "resume_data": "Job Seeker Edison NJ Work Experience Wells Fargo September 2018 to Present S P Global Ratings Sep2017 Feb2018 Spark Python Developer and Data Modeler Wells Fargo Wholesale Data Summit NJ September 2018 to Present Location Summit NJ Role Spark Python Developer and Data Modeler Project Design holistic view with conceptual and logical data model for CRM 360 for the consolidated view of 20 different lines of business LOB within the wholesale region New initiative in order to bring various LOBs in wholesale under one roof and regulate data through surveillance process for entire wholesale region Developing Spark programs using Python APIs to compare the performance of Spark with Hive and Oracle Designed and created Hive external tables using shared metastore instead of derby with partitioning dynamic partitioning and buckets Used PySparkSQL to load JSON data and create schema RDD Data Frames and loaded it into Hive Tables and handled Structured data using SparkSQL Imported required tables from RDBMS to HDFS using Sqoop and used PySpark RDDs to get real time streaming of data into HBase Analyzed the SQL scripts and designed solutions to implement using PySpark and implemented process using Python and utilizing data frames and temporary table SQL for faster processing of data Performing Business Area Analysis and logical and physical data modeling for Data Warehouse Data Mart applications as well as Operational applications enhancements and new development Develop Technical Metadata and Business Glossary for all wholesale LOBs business systems by partnering with the IT and business systems teams The data is cleaned in Oracle and loaded into a new table which is moved into HDFS using sqoop Working with Data Stewards to establish metadata registry responsibilities Tools Oracle 12C database Teradata TOAD Power Designer ERStudio SPARX Enterprise architect Pyspark Spark SQLHiveSqoop Senior PLSQL developer and Data Architect New York NY September 2017 to February 2018 Location New York NY Role Senior PLSQL developer and Data Architect Project The Surveillance Optimization is project for restructuring existing rating application along with additional surveillance optimization rule enhancement This platform has been introduced in order to accommodate new analytical solutions provided by business Created new DB objects like Tables Procedures Functions Triggers and View using Oracle PLSQL Taking care of DB Performance issues by tuning SQL queries and stored procedures by using SQL Profiler and Oracle execution plan tables Involved in writing Companys Metadata standards and practices including naming standards modeling guidelines and Data Warehouse Strategy Identified capabilities and supporting applications imbalances resulting in revisions to corporate data models and reporting capabilities Analyze data across multiple sources to design and document logical and physical data models and maintain the Enterprise Data Warehouse Data Model Gather and define data requirements based on specific application business requirements Prepare normalized models and dimensional models Define conceptual logical and physical data models Understand crucial areas in existing process and new process for rating surveillance and design the data model which will maintain and support all surveillance processes including new and existing one Communicate all designing requirement to all implementation partners which includes application database and ETL team Data Warehouse architecture data design and implementation Responsible for the Change Control and Release of Enterprise Data Models for Data Warehouse Subject Area Data Marts for all rating applications along with Logical and Physical design for Dimensional data warehouse designs using following tools CAERWIN QuestTOAD Oracle RDBMS Ralph Kimball methodology Tools Oracle 12C database PLSQL Developer TOAD Oracle SQL Developer Erwin ERStudio Perl Deutsche Bank April 2016 to August 2017 Spark Python Developer and Data ModelerAnalyst Deutsche Bank New York NY April 2016 to August 2017 Location New York NY Role Spark Python Developer and Data ModelerAnalyst Projects The Control Room ReEngineering program has been commissioned to deliver strategic technical solution which will support the functions the Global Control Room perform such as conflict and research clearance restricted list watch list management and information barrier maintenance as well LUCID is compliance hub which is designed to source data for Actimize trade surveillance data for different products like FX Equities and FX Along with trade surveillance there is pricing and valuation for the different product of the repository Involved in creating Hive tables loading with data and writing hive queries that will run internally in mapReduce way Importing and exporting data in HDFS and Hive using Sqoop Used Hive to do transformations event joins and some preaggregations before storing the data onto HDFS Used Sqoop efficiently transfer data between databases and HDFS and used PySpark to stream the log data from servers Primary business focus is in the area of trade surveillance for different IB products Pricing and Valuation compliance and control room application understanding along with complete data security and sensitivity for Control Room data Position reporting data analysis for reference security master data alignment and 13F holding fillings Developed SparkMapReduce jobs to parse the JSON files for Oracle data Data Analysis with the help of Python Scripting for AML alert data files and few process implementations to store AML Alert data in DB in Python Data analysis for Watch ListRestricted List for existing Control Room Source data mapping for transaction data for different asset classes in order utilize this data for Actimize trade surveillance modules Data AnalystData Modeler performing Business Area Analysis and logical and physical data modeling for Data Warehouse Data Mart applications as well as Operational applications enhancements and new development Used the Ralph Kimball Methodology for Data WarehouseData Mart designs Used Spark RDD and python for processing and transformation of data and integration with popular NoSQL Oracle database for huge volumes of data ER data modelling and logical database designs Metadata and data taxonomy management Ensuring integrity of backend designs and reporting data marts Initiating data design and reviews of highlevel design requirements Tools Oracle 10g 11g database PLSQL Developer TOAD Oracle SQL Developer Power Designer 165Apache Hadoop23 SqoopSparkPySpark Python Vice President April 2015 to April 2016 Lead Data Modeler and Datawarehouse Developer TD Bank Mount Laurel NJ April 2015 to April 2016 Location Mount Laurel NJ Role Lead Data Modeler and Datawarehouse Developer Projects Collects detailed data on bank holding Companies BHCs for various assets classes capital components small loans corporate loan and preprovision net revenue PPNR on monthlyquarterlyyearly basis This data will be reported to Federal Government in the form of FR Y14 FR Q14 or FR M14 reporting format Working with solution architect business analysts to define implementation design and coding of the assigned modulesresponsibilities with highest quality bug free with the help of Teradata FSLDM Involved in performance tuning of code using execution plan and SQL Profiler Oracle DB implementation and ETL process development Involved in Migration activity for few modules from IBM DB2 to Oracle Implemented Python Scripts to validate source system data Participating with key management resources in the strategic analysis and planning requirements for Data WarehouseData Mart reporting and data mining solutions Manage newly built Enterprise Data Warehouse Analytics Data Mart and the Customer Data Platform Data Quality Management and Data Architecture standardization Managed the metadata for the Subject Area models for both Operational Data WarehouseData Mart applications Loaded data from Oracle into HDFS using sqoop batch jobs for post trade transaction for different reporting applications Developed SPARK programs using Python to load data from Oracle to HDFS and Hive external tables Loaded JSON and XML file data using SparkSQL and created RDD schema to load same data into HIVE tables Implemented performance effective solutions for long running SQL processes by using PySpark Data Frame processing Implemented CSV file load processes in Oracle in PySpark by using Panda libraries Tools Oracle 10g 11g database IBM DB2 PLSQL Developer TOAD Oracle SQL Developer Power designer 165 ERStudioErwin PythonSPARK Datawarehouse Architect Cognizant Technology Solutions October 2005 to April 2015 Spark Python Developer and Data Modeler Credit Suisse Singapore March 2011 to March 2013 from Mar2011 Mar2013 and Credit Suisse New York from Apr 2013 Apr2015 Location Singapore and NY Role Spark Python Developer and Data Modeler Projects Sales Reporting Securities Intelligence Processing is state of the Reporting and Sales intelligence Applications which cater the enterprise wide Reporting and Real time data needs It includes equity trade surveillance along with sale credit calculations for equity trades Data Modeling logical dimensional and physical for various projects work with application teams to analyze data requirements review data model and address the functional and nonfunctional requirements performance geographical separation auditing archiving Core member of the solution team responsible for proposing architecture high level design for Investor Tax platform for multiple geographies US UK Singapore and India FPML data processing implementation for FX products Led data model team on Data Marts Star Schema for Volcker reporting Data model set up and implementation Define best practices for data modelling and database development Built and tuned complex and large data load from various sources to ODS Operational Data System Financial Star Schema and Campaign Analysis Mart ETL Informatica Power Center 17Oracle Analytical SQL Tools Oracle 10g 11g database PLSQL Developer TOAD Oracle SQL Developer Oracle Data Modeler Informatica Erwin Apache Hadoop 22 SqoopSparkPySpark Python Senior Database Developer Wellington Management Company Pune Maharashtra March 2009 to February 2011 Location Pune India Role Senior Database Developer Assess Troubleshoot analyze performance issues during 9i to 11g migration nightly batch processes and advise solutions SQL tuning 10053 trace analysis responsible for improving PLSQL refactoring improving performance from hours to minutes optimize distributed processing reduce database resource usage logical IOs temporary table space usages CPU consumption latch contention etc Played crucial role in golive of the 11g Upgrade project Design database objects including tables indexes views materialized views sequences and referential integrity for a reporting data warehouse Develop and maintain database programs including packages procedures functions and triggers Tools Oracle 10g 11g database PLSQL Developer TOAD Oracle SQL Developer Erwin Database Developer and Data Modeler AOL Pune Maharashtra September 2005 to February 2009 Sep2005 to Feb 2007 for ABN AMRO and from Mar 2007 Feb 2009 for RBS Location Pune India Role Database Developer and Data Modeler Projects This is online corporate reporting application developed in Java Oracle10g which is used to process ABNAMRO corporate client payment data Develop and maintain database programs including packages procedures functions and triggers using PLSQL Develop and support the Oracle PLSQL code that performs the calculations for the all fulfillment marketing programs As requested supportfulfill requests for information and troubleshoot code Tools Oracle 11g database PLSQL Developer TOAD Oracle SQL Developer Data Architect Credit Suisse Boston MA May 2005 to September 2005 Boston US ABN AMRO Bank AOLAccess Reporting Persistent Systems Ltd AMTS May2005 Sep2005 Lead Database Developer and data Modeler Home Builder Association Pune Maharashtra May 2005 to September 2005 Location Pune India Role Lead Database Developer and data Modeler Projects This is online corporate reporting application developed in NET Oracle 9i for local builders Bay Systems Pvt Ltd December 2004 to April 2005 Senior Database Developer MBTBay system Pune Maharashtra December 2004 to April 2005 Projects This is online credit card application developed in NET Oracle 9i for UAE based financial institution DSK Systems Pvt Ltd May 2002 to December 2004 Database Developer DSK Systems Pvt Ltd Pune Maharashtra May 2002 to December 2004 Projects This is online builder application which is used for DSK builders internal all site details The application developed in NET Oracle 9i for UAE based financial institution Education DAC in Advance Computing PUNE University January 2002 Bachelor of Computer Science in BSCComp Dr Babasaheb Ambedkar Marathwada University Aurangabad Maharashtra May 2001 Skills database 10 years Erwin 5 years Oracle 10 years Plsql 10 years Sql 10 years",
    "unique_id": "c314a7b3-9bc7-4984-a67a-7f1b52a6fc61"
}