{
    "clean_data": "Data engineer II Data engineer II Data engineer II Wex Inc 5 years of experience in designing and implementing enterprise level data warehouses and data pipeline architectures using Big Data technologies Apache Hadoop Apache Spark Amazon Web Services and Python Experience in building highly reliable scalable big data solutions on AWS using EMR Glue S3 buckets EC2 instances Redshift RDS and others Strong experience in building Enterprise level Data Warehouse applications using Informatica Power Center 9x8x ETL tools Extensively worked on developing Spark applications using spark SQL Dataframes and RDDs to improve the transformations on huge datasets Solid knowledge of Data Marts Operational Data Stores OLTPOLAP Dimensional Data Modeling with Ralph Kimball Methodology Star Schema Modeling SnowFlake Modeling for FACT and Dimensions Tables Hands on experience with HDFS Map Reduce Hive and Pig Experience in integration of various data sources with Relational Databases like Oracle SQL Server and Worked on integrating data from flat files Good analytical interpersonal communication problem solving skills with ability to quickly master new concepts and capable of working in group as well as independently Extensive Experience in Agile Methodology and waterfall methodologies of SDLC Work Experience Data engineer II Wex Inc July 2018 to Present Responsible for building data pipelines required for data transformation and movement using AWS cloud services Apache Spark framework and python Extensive experience in developing data processing scripts using SQL python to load data into databases Extensively used SQL to build data processing and analytical queries for understanding data behaviour and incoorpating new aggregated features into the data Developed robust data tier to feed the dashboards and customized tiles in TipBoard as per business requirement Responsible to support and monitor data load process feeding a webbased fleet data analytical application used by thousands of Wex customers Built pipelines to load data into Postgres RDS database to optimize query execution time and improve Alexa performance Built an event based pipeline to process huge incoming telematics feeds on daily basis using AWS SNS SQS Lambda Cloud watch S3 and glue services Implemented Spark big data processing using Python and utilizing Data Frames Spark SQL for faster testing and processing of data Built database architectural strategies to implement and support warehouse for analytical requirements Monitor daily batch processing of transactional data into Postgres and AWS Redshift environment Developed framework to process large sets of data using python AWS cloud services and scheduled corn jobs to automate the data processing and ETL jobs Used AWS Athena extensively to analysis and process both structured and unstructured data Built data pipelines to perform extract transform and load using boto3 SDK for python and apache spark framework sql Streamlined raw data from different unconnected structured and unstructured data source to help data scientists for faster analytics Maintain documentation for all data pipelines and AWS glue jobs in both dev and prod environments Worked to stream line data pipeline with various BI Developer InformaticaAWS PythonSQL Indic Solutions Inc September 2017 to June 2018 Efficiently implemented a pipeline to load data into Amazon Redshift DWH using Apache Spark python spark SQL and AWS EMR concepts Imported data from Amazon S3 into Spark RDD and performed business transformations and actions on RDD Configured AWS Data pipeline using JSON to schedule and handle the datadriven workflows Developed script to unload data from AWS Redshift DB to S3 bucket on incremental basis and generated parquet files using Spark python for reporting purpose Extensively used Apache Spark concepts to process huge volumes of data sets Created external tables using Amazon Spectrum concepts to support faster retrieval of large data sets for reporting Strong experience in designing and developing Business Intelligence solutions in Data Warehousing Decision Support Systems using Informatica Power Center Implemented mappings using various Informatica Transformations like Aggregator Expression Filter Sequence Generator Update Strategy Source Qualifier Union Lookup transformations Extensive experience in developing complex mappings in Informatica to load the data from various sources using different transformations like Source Qualifier Lookup Expression Update Strategy etc Responsible for implementing Reusable Mapplets transformations Mappings Email Command tasks and Unix Scripts Building publishing customized interactive reports and dashboards report scheduling using Tableau Utilized Jenkins to automate and schedule AWS data processing jobs Developed and maintained ETL Data Extract Transformation and Loading mappings to extract the Data from multiple source systems like Oracle and Flat Files and loaded into AWS S3 bucket Redshift DB Created presession and post session scripts to validate and push the data to Amazon s3 bucket using UNIX shell scripting Extensively worked with Slowly Changing Dimensions Type1 Type2 and Type3 for Data Loads and Data Transformations Created worksheets reports and converted into interactive dashboards by using Tableau Desktop and provided to Business Users Project Managers and End Users Created different Calculation fields according to requirement various conditions and applied multiple Filters for various analytical reports and dashboards Intern University of South Alabama AL May 2016 to July 2017 Interacted with Data Modellers and Business Analysts to understand the requirements and the impact of the ETL on the business Designed and developed Informatica ETL mappings to extract master and transactional data from heterogeneous data feeds and load it to target Responsible for developing and maintaining ETL jobs including ETL implementation and enhancements testing and quality assurance troubleshooting issues and ETLQuery performance tuning Designed and developed several SQL Server Stored Procedures Triggers and Views Extensive experience in Analysis Design Data Extraction Cleansing Transformation and Loading into Data Marts Monitored sessions using the workflow monitor which were scheduled running completed or failed Debugged mappings for faile1d sessions Involved in Unit Integration System and Performance testing levels Written documentation to describe program development logic coding testing changes and corrections Worked as a fully contributing team member under manager guidance with independent planning execution responsibilities Database Developer Atos Pvt Ltd Pune Maharashtra June 2014 to December 2015 Involved in gathering client requirements and converting them into User Requirement Specifications and Functional Requirement Specifications for the designers and developers to understand them as per their perspective Designed and developed end to end mappings workflows sessions to transform the source based on the business needs to target system Implemented reusable mapplets and tasks transformations to reduce redundant mappings and improve efficiency Developed PLSQL code for migrating financial data managed in SAP in legacy to Amdocs target tables Developed a code for sequence buffer estimation to reduce significant loss in sequence due to adhoc buffer guesses Reduced migration window from 8 hours to 6 hours using SQL joins execution plan query optimization Indexing CR MNP Developed a code for PORTIN and PORTOUT numbers which fall under different category Same instance Inter Instance called Mobile number portability For migration purpose created SQL tables to streamline Enterprise customers legacy finance data which used to be managed in Excel The task required thorough knowledge of BSCS as well as Amdocs finance module Good experience developing scripts to maintain the smooth flow of migration using UNIX shell scripting Responsible for the quality assurance of the migrated data Developed mapping documents to explain end to end business logic of how the data is migrated Used agile methodology for the software development Responsible to monitor the process throughout the migration window Resolve any code break issue during the migration window and restart the process trying to minimize the lost time Performed User AcceptanceBusiness process testing and suggested enhancements to improve the End User functionality Education Master of Science in Computers and Information Sciences in Computers and Information Sciences University of South Alabama Mobile AL July 2017 Bachelor of Technology in Computer Science Engineering in Computer Science Engineering Jawaharlal Nehru Technological University Hyderabad Telangana May 2014 Skills MICROSOFT SQL SERVER SQL SERVER MYSQL ORACLE ORACLE 11 Links httpswwwlinkedincominashritharavula755b2b159",
    "entities": [
        "Implemented Spark",
        "Views Extensive",
        "Inter Instance",
        "Present Responsible",
        "Analysis Design Data Extraction Cleansing Transformation",
        "Oracle SQL Server",
        "Database Developer Atos Pvt Ltd",
        "RDD Configured AWS Data",
        "II Data",
        "ETL",
        "Developed",
        "Informatica Power Center",
        "Data Warehousing Decision Support Systems",
        "Informatica",
        "Performed User AcceptanceBusiness",
        "AWS S3",
        "Data Loads",
        "Computer Science Engineering",
        "Apache Hadoop",
        "TipBoard",
        "Amazon Spectrum",
        "Redshift DB Created",
        "Amdocs",
        "AWS Redshift DB",
        "II Wex Inc",
        "Created",
        "UNIX",
        "AWS",
        "Intern University of South Alabama AL",
        "Relational Databases",
        "Alexa",
        "Tableau Utilized Jenkins",
        "Ralph Kimball Methodology",
        "SDLC Work Experience Data",
        "Data Marts Operational Data Stores",
        "Postgres",
        "Skills MICROSOFT",
        "Business Users Project Managers",
        "Indexing CR MNP Developed",
        "Amazon",
        "Business Intelligence",
        "SAP",
        "Computers and Information Sciences University of South Alabama Mobile AL",
        "Mappings Email Command",
        "Informatica Transformations like",
        "Mobile",
        "AWS EMR",
        "SQL",
        "Imported",
        "Agile Methodology",
        "Tableau Desktop",
        "Amazon Web Services",
        "Worked",
        "Built",
        "End User",
        "User Requirement Specifications and Functional Requirement Specifications",
        "Informatica Power Center Implemented",
        "Nehru Technological University",
        "Telangana",
        "BI Developer InformaticaAWS PythonSQL Indic Solutions Inc",
        "PORTIN",
        "Amazon Redshift DWH",
        "ORACLE ORACLE 11 Links",
        "SQL Dataframes",
        "ETL Data Extract Transformation",
        "Wex",
        "Athena",
        "Big Data",
        "AWS SNS SQS",
        "Apache Spark",
        "Data Transformations Created",
        "Spark",
        "Slowly Changing Dimensions Type1 Type2"
    ],
    "experience": "Experience in building highly reliable scalable big data solutions on AWS using EMR Glue S3 buckets EC2 instances Redshift RDS and others Strong experience in building Enterprise level Data Warehouse applications using Informatica Power Center 9x8x ETL tools Extensively worked on developing Spark applications using spark SQL Dataframes and RDDs to improve the transformations on huge datasets Solid knowledge of Data Marts Operational Data Stores OLTPOLAP Dimensional Data Modeling with Ralph Kimball Methodology Star Schema Modeling SnowFlake Modeling for FACT and Dimensions Tables Hands on experience with HDFS Map Reduce Hive and Pig Experience in integration of various data sources with Relational Databases like Oracle SQL Server and Worked on integrating data from flat files Good analytical interpersonal communication problem solving skills with ability to quickly master new concepts and capable of working in group as well as independently Extensive Experience in Agile Methodology and waterfall methodologies of SDLC Work Experience Data engineer II Wex Inc July 2018 to Present Responsible for building data pipelines required for data transformation and movement using AWS cloud services Apache Spark framework and python Extensive experience in developing data processing scripts using SQL python to load data into databases Extensively used SQL to build data processing and analytical queries for understanding data behaviour and incoorpating new aggregated features into the data Developed robust data tier to feed the dashboards and customized tiles in TipBoard as per business requirement Responsible to support and monitor data load process feeding a webbased fleet data analytical application used by thousands of Wex customers Built pipelines to load data into Postgres RDS database to optimize query execution time and improve Alexa performance Built an event based pipeline to process huge incoming telematics feeds on daily basis using AWS SNS SQS Lambda Cloud watch S3 and glue services Implemented Spark big data processing using Python and utilizing Data Frames Spark SQL for faster testing and processing of data Built database architectural strategies to implement and support warehouse for analytical requirements Monitor daily batch processing of transactional data into Postgres and AWS Redshift environment Developed framework to process large sets of data using python AWS cloud services and scheduled corn jobs to automate the data processing and ETL jobs Used AWS Athena extensively to analysis and process both structured and unstructured data Built data pipelines to perform extract transform and load using boto3 SDK for python and apache spark framework sql Streamlined raw data from different unconnected structured and unstructured data source to help data scientists for faster analytics Maintain documentation for all data pipelines and AWS glue jobs in both dev and prod environments Worked to stream line data pipeline with various BI Developer InformaticaAWS PythonSQL Indic Solutions Inc September 2017 to June 2018 Efficiently implemented a pipeline to load data into Amazon Redshift DWH using Apache Spark python spark SQL and AWS EMR concepts Imported data from Amazon S3 into Spark RDD and performed business transformations and actions on RDD Configured AWS Data pipeline using JSON to schedule and handle the datadriven workflows Developed script to unload data from AWS Redshift DB to S3 bucket on incremental basis and generated parquet files using Spark python for reporting purpose Extensively used Apache Spark concepts to process huge volumes of data sets Created external tables using Amazon Spectrum concepts to support faster retrieval of large data sets for reporting Strong experience in designing and developing Business Intelligence solutions in Data Warehousing Decision Support Systems using Informatica Power Center Implemented mappings using various Informatica Transformations like Aggregator Expression Filter Sequence Generator Update Strategy Source Qualifier Union Lookup transformations Extensive experience in developing complex mappings in Informatica to load the data from various sources using different transformations like Source Qualifier Lookup Expression Update Strategy etc Responsible for implementing Reusable Mapplets transformations Mappings Email Command tasks and Unix Scripts Building publishing customized interactive reports and dashboards report scheduling using Tableau Utilized Jenkins to automate and schedule AWS data processing jobs Developed and maintained ETL Data Extract Transformation and Loading mappings to extract the Data from multiple source systems like Oracle and Flat Files and loaded into AWS S3 bucket Redshift DB Created presession and post session scripts to validate and push the data to Amazon s3 bucket using UNIX shell scripting Extensively worked with Slowly Changing Dimensions Type1 Type2 and Type3 for Data Loads and Data Transformations Created worksheets reports and converted into interactive dashboards by using Tableau Desktop and provided to Business Users Project Managers and End Users Created different Calculation fields according to requirement various conditions and applied multiple Filters for various analytical reports and dashboards Intern University of South Alabama AL May 2016 to July 2017 Interacted with Data Modellers and Business Analysts to understand the requirements and the impact of the ETL on the business Designed and developed Informatica ETL mappings to extract master and transactional data from heterogeneous data feeds and load it to target Responsible for developing and maintaining ETL jobs including ETL implementation and enhancements testing and quality assurance troubleshooting issues and ETLQuery performance tuning Designed and developed several SQL Server Stored Procedures Triggers and Views Extensive experience in Analysis Design Data Extraction Cleansing Transformation and Loading into Data Marts Monitored sessions using the workflow monitor which were scheduled running completed or failed Debugged mappings for faile1d sessions Involved in Unit Integration System and Performance testing levels Written documentation to describe program development logic coding testing changes and corrections Worked as a fully contributing team member under manager guidance with independent planning execution responsibilities Database Developer Atos Pvt Ltd Pune Maharashtra June 2014 to December 2015 Involved in gathering client requirements and converting them into User Requirement Specifications and Functional Requirement Specifications for the designers and developers to understand them as per their perspective Designed and developed end to end mappings workflows sessions to transform the source based on the business needs to target system Implemented reusable mapplets and tasks transformations to reduce redundant mappings and improve efficiency Developed PLSQL code for migrating financial data managed in SAP in legacy to Amdocs target tables Developed a code for sequence buffer estimation to reduce significant loss in sequence due to adhoc buffer guesses Reduced migration window from 8 hours to 6 hours using SQL joins execution plan query optimization Indexing CR MNP Developed a code for PORTIN and PORTOUT numbers which fall under different category Same instance Inter Instance called Mobile number portability For migration purpose created SQL tables to streamline Enterprise customers legacy finance data which used to be managed in Excel The task required thorough knowledge of BSCS as well as Amdocs finance module Good experience developing scripts to maintain the smooth flow of migration using UNIX shell scripting Responsible for the quality assurance of the migrated data Developed mapping documents to explain end to end business logic of how the data is migrated Used agile methodology for the software development Responsible to monitor the process throughout the migration window Resolve any code break issue during the migration window and restart the process trying to minimize the lost time Performed User AcceptanceBusiness process testing and suggested enhancements to improve the End User functionality Education Master of Science in Computers and Information Sciences in Computers and Information Sciences University of South Alabama Mobile AL July 2017 Bachelor of Technology in Computer Science Engineering in Computer Science Engineering Jawaharlal Nehru Technological University Hyderabad Telangana May 2014 Skills MICROSOFT SQL SERVER SQL SERVER MYSQL ORACLE ORACLE 11 Links httpswwwlinkedincominashritharavula755b2b159",
    "extracted_keywords": [
        "Data",
        "engineer",
        "II",
        "Data",
        "engineer",
        "II",
        "Data",
        "engineer",
        "II",
        "Wex",
        "Inc",
        "years",
        "experience",
        "enterprise",
        "level",
        "data",
        "warehouses",
        "data",
        "pipeline",
        "Big",
        "Data",
        "technologies",
        "Apache",
        "Hadoop",
        "Apache",
        "Spark",
        "Amazon",
        "Web",
        "Services",
        "Python",
        "Experience",
        "data",
        "solutions",
        "AWS",
        "EMR",
        "Glue",
        "S3",
        "EC2",
        "instances",
        "RDS",
        "others",
        "experience",
        "Enterprise",
        "level",
        "Data",
        "Warehouse",
        "applications",
        "Informatica",
        "Power",
        "Center",
        "ETL",
        "tools",
        "Spark",
        "applications",
        "spark",
        "SQL",
        "Dataframes",
        "RDDs",
        "transformations",
        "datasets",
        "knowledge",
        "Data",
        "Marts",
        "Operational",
        "Data",
        "Stores",
        "Dimensional",
        "Data",
        "Modeling",
        "Ralph",
        "Kimball",
        "Methodology",
        "Star",
        "Schema",
        "Modeling",
        "SnowFlake",
        "Modeling",
        "FACT",
        "Dimensions",
        "Tables",
        "Hands",
        "experience",
        "HDFS",
        "Map",
        "Reduce",
        "Hive",
        "Pig",
        "Experience",
        "integration",
        "data",
        "sources",
        "Relational",
        "Databases",
        "Oracle",
        "SQL",
        "Server",
        "Worked",
        "data",
        "files",
        "communication",
        "problem",
        "skills",
        "ability",
        "concepts",
        "group",
        "Experience",
        "Agile",
        "Methodology",
        "waterfall",
        "methodologies",
        "SDLC",
        "Work",
        "Experience",
        "Data",
        "engineer",
        "II",
        "Wex",
        "Inc",
        "July",
        "Present",
        "data",
        "pipelines",
        "data",
        "transformation",
        "movement",
        "AWS",
        "cloud",
        "services",
        "Apache",
        "Spark",
        "framework",
        "experience",
        "data",
        "processing",
        "scripts",
        "SQL",
        "python",
        "data",
        "databases",
        "SQL",
        "data",
        "processing",
        "queries",
        "data",
        "behaviour",
        "features",
        "data",
        "data",
        "tier",
        "dashboards",
        "tiles",
        "TipBoard",
        "business",
        "requirement",
        "data",
        "load",
        "process",
        "fleet",
        "data",
        "application",
        "thousands",
        "Wex",
        "customers",
        "pipelines",
        "data",
        "Postgres",
        "RDS",
        "database",
        "query",
        "execution",
        "time",
        "performance",
        "event",
        "pipeline",
        "telematics",
        "basis",
        "AWS",
        "SNS",
        "SQS",
        "Lambda",
        "Cloud",
        "S3",
        "glue",
        "services",
        "Spark",
        "data",
        "processing",
        "Python",
        "Data",
        "Frames",
        "Spark",
        "SQL",
        "testing",
        "processing",
        "data",
        "database",
        "strategies",
        "warehouse",
        "requirements",
        "batch",
        "processing",
        "data",
        "Postgres",
        "AWS",
        "Redshift",
        "environment",
        "framework",
        "sets",
        "data",
        "AWS",
        "cloud",
        "services",
        "corn",
        "jobs",
        "data",
        "processing",
        "ETL",
        "jobs",
        "AWS",
        "Athena",
        "analysis",
        "process",
        "data",
        "data",
        "pipelines",
        "extract",
        "transform",
        "load",
        "SDK",
        "python",
        "apache",
        "spark",
        "framework",
        "data",
        "data",
        "source",
        "scientists",
        "analytics",
        "documentation",
        "data",
        "pipelines",
        "AWS",
        "glue",
        "jobs",
        "dev",
        "prod",
        "environments",
        "line",
        "data",
        "pipeline",
        "BI",
        "Developer",
        "PythonSQL",
        "Indic",
        "Solutions",
        "Inc",
        "September",
        "June",
        "pipeline",
        "data",
        "Amazon",
        "Redshift",
        "DWH",
        "Apache",
        "Spark",
        "python",
        "spark",
        "SQL",
        "AWS",
        "EMR",
        "data",
        "Amazon",
        "S3",
        "Spark",
        "RDD",
        "business",
        "transformations",
        "actions",
        "RDD",
        "AWS",
        "Data",
        "pipeline",
        "JSON",
        "workflows",
        "script",
        "data",
        "AWS",
        "Redshift",
        "DB",
        "S3",
        "bucket",
        "basis",
        "files",
        "Spark",
        "python",
        "purpose",
        "Apache",
        "Spark",
        "concepts",
        "volumes",
        "data",
        "sets",
        "tables",
        "Amazon",
        "Spectrum",
        "concepts",
        "retrieval",
        "data",
        "sets",
        "experience",
        "Business",
        "Intelligence",
        "solutions",
        "Data",
        "Warehousing",
        "Decision",
        "Support",
        "Systems",
        "Informatica",
        "Power",
        "Center",
        "mappings",
        "Informatica",
        "Transformations",
        "Aggregator",
        "Expression",
        "Filter",
        "Sequence",
        "Generator",
        "Update",
        "Strategy",
        "Source",
        "Qualifier",
        "Union",
        "Lookup",
        "experience",
        "mappings",
        "Informatica",
        "data",
        "sources",
        "transformations",
        "Source",
        "Qualifier",
        "Lookup",
        "Expression",
        "Update",
        "Strategy",
        "Reusable",
        "Mapplets",
        "transformations",
        "Mappings",
        "Email",
        "Command",
        "tasks",
        "Unix",
        "Scripts",
        "Building",
        "reports",
        "dashboards",
        "scheduling",
        "Tableau",
        "Utilized",
        "Jenkins",
        "schedule",
        "AWS",
        "data",
        "processing",
        "jobs",
        "ETL",
        "Data",
        "Extract",
        "Transformation",
        "Loading",
        "mappings",
        "Data",
        "source",
        "systems",
        "Oracle",
        "Flat",
        "Files",
        "AWS",
        "S3",
        "bucket",
        "Redshift",
        "DB",
        "presession",
        "post",
        "session",
        "scripts",
        "data",
        "Amazon",
        "s3",
        "bucket",
        "UNIX",
        "shell",
        "scripting",
        "Dimensions",
        "Type1",
        "Type2",
        "Type3",
        "Data",
        "Loads",
        "Data",
        "Transformations",
        "worksheets",
        "reports",
        "dashboards",
        "Tableau",
        "Desktop",
        "Business",
        "Users",
        "Project",
        "Managers",
        "End",
        "Users",
        "Calculation",
        "fields",
        "requirement",
        "conditions",
        "Filters",
        "reports",
        "Intern",
        "University",
        "South",
        "Alabama",
        "AL",
        "May",
        "July",
        "Data",
        "Modellers",
        "Business",
        "Analysts",
        "requirements",
        "impact",
        "ETL",
        "business",
        "Informatica",
        "ETL",
        "mappings",
        "master",
        "data",
        "data",
        "feeds",
        "ETL",
        "jobs",
        "ETL",
        "implementation",
        "enhancements",
        "testing",
        "quality",
        "assurance",
        "troubleshooting",
        "issues",
        "performance",
        "SQL",
        "Server",
        "Stored",
        "Procedures",
        "Triggers",
        "Views",
        "experience",
        "Analysis",
        "Design",
        "Data",
        "Extraction",
        "Cleansing",
        "Transformation",
        "Loading",
        "Data",
        "Marts",
        "sessions",
        "monitor",
        "mappings",
        "sessions",
        "Unit",
        "Integration",
        "System",
        "Performance",
        "testing",
        "levels",
        "documentation",
        "program",
        "development",
        "logic",
        "testing",
        "changes",
        "corrections",
        "team",
        "member",
        "manager",
        "guidance",
        "planning",
        "execution",
        "responsibilities",
        "Database",
        "Developer",
        "Atos",
        "Pvt",
        "Ltd",
        "Pune",
        "Maharashtra",
        "June",
        "December",
        "client",
        "requirements",
        "User",
        "Requirement",
        "Specifications",
        "Functional",
        "Requirement",
        "Specifications",
        "designers",
        "developers",
        "perspective",
        "end",
        "end",
        "mappings",
        "workflows",
        "sessions",
        "source",
        "business",
        "system",
        "mapplets",
        "tasks",
        "transformations",
        "mappings",
        "efficiency",
        "PLSQL",
        "code",
        "data",
        "SAP",
        "legacy",
        "Amdocs",
        "target",
        "tables",
        "code",
        "sequence",
        "buffer",
        "estimation",
        "loss",
        "sequence",
        "buffer",
        "Reduced",
        "migration",
        "window",
        "hours",
        "hours",
        "SQL",
        "execution",
        "plan",
        "query",
        "optimization",
        "Indexing",
        "CR",
        "MNP",
        "code",
        "PORTIN",
        "PORTOUT",
        "numbers",
        "category",
        "instance",
        "Inter",
        "Instance",
        "Mobile",
        "number",
        "portability",
        "migration",
        "purpose",
        "SQL",
        "tables",
        "Enterprise",
        "customers",
        "finance",
        "data",
        "Excel",
        "task",
        "knowledge",
        "BSCS",
        "Amdocs",
        "module",
        "experience",
        "scripts",
        "flow",
        "migration",
        "UNIX",
        "shell",
        "quality",
        "assurance",
        "data",
        "mapping",
        "documents",
        "end",
        "business",
        "logic",
        "data",
        "methodology",
        "software",
        "development",
        "process",
        "migration",
        "window",
        "Resolve",
        "code",
        "break",
        "issue",
        "migration",
        "window",
        "process",
        "time",
        "Performed",
        "User",
        "AcceptanceBusiness",
        "process",
        "testing",
        "enhancements",
        "End",
        "User",
        "functionality",
        "Education",
        "Master",
        "Science",
        "Computers",
        "Information",
        "Sciences",
        "Computers",
        "Information",
        "Sciences",
        "University",
        "South",
        "Alabama",
        "Mobile",
        "AL",
        "July",
        "Bachelor",
        "Technology",
        "Computer",
        "Science",
        "Engineering",
        "Computer",
        "Science",
        "Engineering",
        "Jawaharlal",
        "Nehru",
        "Technological",
        "University",
        "Hyderabad",
        "Telangana",
        "May",
        "Skills",
        "MICROSOFT",
        "SQL",
        "SERVER",
        "SQL",
        "SERVER",
        "MYSQL",
        "ORACLE",
        "ORACLE",
        "Links",
        "httpswwwlinkedincominashritharavula755b2b159"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T20:28:53.267593",
    "resume_data": "Data engineer II Data engineer II Data engineer II Wex Inc 5 years of experience in designing and implementing enterprise level data warehouses and data pipeline architectures using Big Data technologies Apache Hadoop Apache Spark Amazon Web Services and Python Experience in building highly reliable scalable big data solutions on AWS using EMR Glue S3 buckets EC2 instances Redshift RDS and others Strong experience in building Enterprise level Data Warehouse applications using Informatica Power Center 9x8x ETL tools Extensively worked on developing Spark applications using spark SQL Dataframes and RDDs to improve the transformations on huge datasets Solid knowledge of Data Marts Operational Data Stores OLTPOLAP Dimensional Data Modeling with Ralph Kimball Methodology Star Schema Modeling SnowFlake Modeling for FACT and Dimensions Tables Hands on experience with HDFS Map Reduce Hive and Pig Experience in integration of various data sources with Relational Databases like Oracle SQL Server and Worked on integrating data from flat files Good analytical interpersonal communication problem solving skills with ability to quickly master new concepts and capable of working in group as well as independently Extensive Experience in Agile Methodology and waterfall methodologies of SDLC Work Experience Data engineer II Wex Inc July 2018 to Present Responsible for building data pipelines required for data transformation and movement using AWS cloud services Apache Spark framework and python Extensive experience in developing data processing scripts using SQL python to load data into databases Extensively used SQL to build data processing and analytical queries for understanding data behaviour and incoorpating new aggregated features into the data Developed robust data tier to feed the dashboards and customized tiles in TipBoard as per business requirement Responsible to support and monitor data load process feeding a webbased fleet data analytical application used by thousands of Wex customers Built pipelines to load data into Postgres RDS database to optimize query execution time and improve Alexa performance Built an event based pipeline to process huge incoming telematics feeds on daily basis using AWS SNS SQS Lambda Cloud watch S3 and glue services Implemented Spark big data processing using Python and utilizing Data Frames Spark SQL for faster testing and processing of data Built database architectural strategies to implement and support warehouse for analytical requirements Monitor daily batch processing of transactional data into Postgres and AWS Redshift environment Developed framework to process large sets of data using python AWS cloud services and scheduled corn jobs to automate the data processing and ETL jobs Used AWS Athena extensively to analysis and process both structured and unstructured data Built data pipelines to perform extract transform and load using boto3 SDK for python and apache spark framework sql Streamlined raw data from different unconnected structured and unstructured data source to help data scientists for faster analytics Maintain documentation for all data pipelines and AWS glue jobs in both dev and prod environments Worked to stream line data pipeline with various BI Developer InformaticaAWS PythonSQL Indic Solutions Inc September 2017 to June 2018 Efficiently implemented a pipeline to load data into Amazon Redshift DWH using Apache Spark python spark SQL and AWS EMR concepts Imported data from Amazon S3 into Spark RDD and performed business transformations and actions on RDD Configured AWS Data pipeline using JSON to schedule and handle the datadriven workflows Developed script to unload data from AWS Redshift DB to S3 bucket on incremental basis and generated parquet files using Spark python for reporting purpose Extensively used Apache Spark concepts to process huge volumes of data sets Created external tables using Amazon Spectrum concepts to support faster retrieval of large data sets for reporting Strong experience in designing and developing Business Intelligence solutions in Data Warehousing Decision Support Systems using Informatica Power Center Implemented mappings using various Informatica Transformations like Aggregator Expression Filter Sequence Generator Update Strategy Source Qualifier Union Lookup transformations Extensive experience in developing complex mappings in Informatica to load the data from various sources using different transformations like Source Qualifier Lookup Expression Update Strategy etc Responsible for implementing Reusable Mapplets transformations Mappings Email Command tasks and Unix Scripts Building publishing customized interactive reports and dashboards report scheduling using Tableau Utilized Jenkins to automate and schedule AWS data processing jobs Developed and maintained ETL Data Extract Transformation and Loading mappings to extract the Data from multiple source systems like Oracle and Flat Files and loaded into AWS S3 bucket Redshift DB Created presession and post session scripts to validate and push the data to Amazon s3 bucket using UNIX shell scripting Extensively worked with Slowly Changing Dimensions Type1 Type2 and Type3 for Data Loads and Data Transformations Created worksheets reports and converted into interactive dashboards by using Tableau Desktop and provided to Business Users Project Managers and End Users Created different Calculation fields according to requirement various conditions and applied multiple Filters for various analytical reports and dashboards Intern University of South Alabama AL May 2016 to July 2017 Interacted with Data Modellers and Business Analysts to understand the requirements and the impact of the ETL on the business Designed and developed Informatica ETL mappings to extract master and transactional data from heterogeneous data feeds and load it to target Responsible for developing and maintaining ETL jobs including ETL implementation and enhancements testing and quality assurance troubleshooting issues and ETLQuery performance tuning Designed and developed several SQL Server Stored Procedures Triggers and Views Extensive experience in Analysis Design Data Extraction Cleansing Transformation and Loading into Data Marts Monitored sessions using the workflow monitor which were scheduled running completed or failed Debugged mappings for faile1d sessions Involved in Unit Integration System and Performance testing levels Written documentation to describe program development logic coding testing changes and corrections Worked as a fully contributing team member under manager guidance with independent planning execution responsibilities Database Developer Atos Pvt Ltd Pune Maharashtra June 2014 to December 2015 Involved in gathering client requirements and converting them into User Requirement Specifications and Functional Requirement Specifications for the designers and developers to understand them as per their perspective Designed and developed end to end mappings workflows sessions to transform the source based on the business needs to target system Implemented reusable mapplets and tasks transformations to reduce redundant mappings and improve efficiency Developed PLSQL code for migrating financial data managed in SAP in legacy to Amdocs target tables Developed a code for sequence buffer estimation to reduce significant loss in sequence due to adhoc buffer guesses Reduced migration window from 8 hours to 6 hours using SQL joins execution plan query optimization Indexing CR MNP Developed a code for PORTIN and PORTOUT numbers which fall under different category Same instance Inter Instance called Mobile number portability For migration purpose created SQL tables to streamline Enterprise customers legacy finance data which used to be managed in Excel The task required thorough knowledge of BSCS as well as Amdocs finance module Good experience developing scripts to maintain the smooth flow of migration using UNIX shell scripting Responsible for the quality assurance of the migrated data Developed mapping documents to explain end to end business logic of how the data is migrated Used agile methodology for the software development Responsible to monitor the process throughout the migration window Resolve any code break issue during the migration window and restart the process trying to minimize the lost time Performed User AcceptanceBusiness process testing and suggested enhancements to improve the End User functionality Education Master of Science in Computers and Information Sciences in Computers and Information Sciences University of South Alabama Mobile AL July 2017 Bachelor of Technology in Computer Science Engineering in Computer Science Engineering Jawaharlal Nehru Technological University Hyderabad Telangana May 2014 Skills MICROSOFT SQL SERVER SQL SERVER MYSQL ORACLE ORACLE 11 Links httpswwwlinkedincominashritharavula755b2b159",
    "unique_id": "7bba7a23-bc38-4f31-b1bf-91059dcdbdea"
}