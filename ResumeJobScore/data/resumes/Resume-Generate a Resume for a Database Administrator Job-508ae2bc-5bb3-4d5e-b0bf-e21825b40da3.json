{
    "clean_data": "Big Data Project Lead Big Data Project Lead Big Data Project Lead Mphasis Irving TX More than 13 years of experience in building distributed scalable and complex applications using Java J2EE and Big Data Technologies In depth understandingknowledge of Hadoop Architecture and its components More than 6 years of experience as Big Data senior developer and technology lead with good understanding of the Hadoop Eco System Map Reduce Hive Sqoop Oozie Storm Flume HBase Pig Sqoop Cassandra MongoDB DynamoDB Phoenix Impala Experience in setting up Hadoop clusters using open and vendor specific distribution like Horton Works Strong experience working on Apache Hortonworks and Cloudera Hadoop distributions Strong understanding of Map Reduce programming and experience in analyzing data using Map Reduce HiveQL Very good experience with both Map Reduce 1 Job Tracker and Map Reduce 2 YARN setups Good experience in monitoring and managing the Hadoop cluster Very Good knowledge of writing hive UDFs and queries Expertise in ImportingExporting data into HDFS from existing relational databases using Sqoop sparksql Very Good working knowledge of Amazon Web Service components like EC2 SQS SNS Dynamo DB S3 etc Managed data extraction for ETL Data warehouse and applied the transformation rules as necessary for data consistency Experienced with Struts Spring EJB Hibernate JMS SOA SOAP Restful Web services Design Patterns JBoss Apache tomcat Jetty JSP JSF Tiles and Servlets Experienced with Jenkins Hudson SVN git CVS JIRA Bugzilla Python and build tool ANT Experienced with XML related technologies such as XML XSLT XSD DOM and SAX Hands on experience in using IDEs like Eclipse NetBeans and JBuilder Excellent written and verbal communication skills interpersonal skills and selflearning attitude Worked extensively in Linux environment and writing shell scripts Excellent debugging skills and strong troubleshooting skills Extensive experience in all phases of Software Development Life Cycle SDLC including identification of business needs and constraints collection of requirements detailed design implementation testing deployment and maintenance Lead the team of 7 and highly Involved in recruiting team members for more than 6 years Work Experience Big Data Project Lead Mphasis Lewisville TX March 2019 to Present Client Information JP Morgan Chase Co is an American multinational investment bank and financial services company headquartered in New York City It is major provider of various investment banking and financial services Business Problem Global Identity Access Management has their internal system to generate reports on the requests coming from internal and external audit teams Since the data is located in different sources it takes good amount of time to generate a single report as a response to their query Also the reports manually generated are not accurate and provides missleading information Hence as a solution to this problem all the data needs to be centrally located in Hadoop ecosystem and a UI portal needs to be built on top of it to generate reports for internal and external audit systems This process of data ingestion is termed as Historical Access Archive and the reporting system is termed as Self Service Audit Portal Responsibilities Responsible for Project delivery Understand the business requirements and Involved in the evaluating the right technology stacks for the project Created Conceptual and detailed design diagrams in Visio for Historical Access Archive Working on implementation in Cloudera cluster owned by JPMCs CSOrion team Working closely with Product Managers Data Modeling teams Business Analysts and testing teams Created the low level and high level design documents in Confluence Working on Sqoop scripts to fetch the data from RDBMS source to store in CSOrion Datalake in Parquet file formats Loading the data from RDBMS Oracle source to HDFS and then load from HDFS to Hive tables Participate in daily Scrum and Sprint PlanningRetrospective meetings Develop the application in Linux development environment Test environment Linux and move the application to Production As a technical lead took the responsibility for all the releases and post validations Documenting the changes done during the release Brought entire team up to the speed by ramping up the team with skillset and ongoing tasks Took the leadership of the completing processes from start to end and integrating with multiple teams ToolsTechnologies Cloudera Hadoop ecosystem Sqoop Shell script Hive Autosys Scheduling tool Oracle Hadoop Technical Lead Infosys Richardson TX May 2015 to March 2019 Client Information BCBS is Health Care Service Corporation insurance company where in nearly one in three Americans rely on Blue Cross Blue Shield companies for access to safe quality and affordable healthcare Business Problem Government programs are receiving huge data from different vendors from different sources With the increasing data it is difficult to analyze which providervendor has sent the files MedicareMediciad claims data on timely basis Also the incoming data from providers needs to be processed and send to the vendors in the form of extracts with specific information Working on a GPD Government Program Data and ICC Projects HCSC Health Care Service Corporation receives different types of membership and claims files Medicare and Medicaid from different vendors from different sources HCSC will process these files for analytics teams and stores the data in Hive and HBase tables The processed data will be stored in history current snapshot and CDC tables to retain the latest and greatest information of each Subscriber claim details and send to the vendors in JSON format ICC will receive claims data from bluechip and TMG vendors and process the data and generates the professional and institutional claims These generated files will be validated against and Edifecs Responsibilities Responsible for Project delivery Understand the business requirements and Involved in the design and development of project Working on implementation in Horton Works 26 cluster Working closely with Product Managers Data Modeling teams Business Analysts and testing teams Involved in the low level and high level design Working on Pig scripts in Java to process the event data Xml Text and CSV formats and create and store in HBaseHive table in ORC Sequence and XML formats Wrote the shell scripts to process the jobs and schedule the jobs using ASK Zena scheduler tool Configured and created jobs in event based and time based Loading the data from Teradata to HDFS and then load from HDFS to Hive tables Participate in daily Scrum and Sprint PlanningRetrospective meetings Develop the application in Linux development environment Test environment Linux and move the application to Production As a technical lead took the responsibility for all the releases and post validations Documenting the changes done during the release Brought entire team up to the speed by taking sessions on using GIT and ramping up the team with skillset Took the leadership of the completing processes from start to end and integrating with multiple teams ToolsTechnologies Horton works 26 Pig Script Phoenix 47 Pig Hive HBase Zena Scheduling tool Shell Script and Python Merck Billerica Masachussets US onsite Hadoop Developer Infosys April 2016 to November 2016 Client Information is a German multinational chemical pharmaceutical and life sciences company headquartered in Darmstadt Business Problem Merck Data scientists wants to analyze the products chemicals their usage their impact on variation of proportion aggregated information This problem statement was acknowledged with the visualizations which was built to work on Hadoop environment on hive tables Responsibilities Worked on a live 50 nodes Hadoop cluster running on Horton works Hadoop Developed visualizations using Tableau and hive tables which is used for data analysis by data scientists Created custom UDAFs using java and used in MRD process ToolsTechnologies Hive unix scripts sqoop Tableau Horton works Hadoop Hadoop Lead Developer Infosys Durham NC March 2015 to December 2015 Client Information Fidelity Investments is an American multinational financial services corporation Business Problem Fidelitys business users wanted to have aggregated information over a period of time on millions of IPs at one go This problem statement was acknowledged with the Model Ready Data MRD process which was built to work on Hadoop environment on hive tables Responsibilities Worked on a live 87 nodes Hadoop cluster running on cloudera Hadoop Developed Model Ready Data Process using hive queries Java and Unix Scripts which is used for modeling and scoring purpose by business Created custom UDAFs using java and used in MRD process Unit and Integration testing of the MRD process using hive queries Did unit and integration testing of Monthly MID to IP and ABT process which are basically monthly and daily aggregations of data using ControlM jobs ToolsTechnologies Hive unix scripts ControlM cloudera Hadoop 54 Hadoop Lead Developer Infosys Durham NC December 2014 to March 2015 Client Information Fidelity Investments is an American multinational financial services corporation Business Problem To produce meaningful and consumable key insights into our clients actions and behaviors in order to have a more comprehensive and holistic understanding therefore increasing productivity of the FFAS business and ultimately driving sales Responsibilities Requirements gathering and clarification from clients Coordinating with offshore team to explain and crossverify the requirements Solving technical challenges faced by offshore team in deployingrunning the use cases Developed Reports using Tableau and hiveimpala Worked on establishing the environment by installing hadoop components needed for use cases Code Review and deployment Showcasing the use cases in business meetings to the clients ToolsTechnologies Map Reduce Hive Impala Tableau NLTK Oozie sqoop Client TMobile Washington US Hadoop Developer Infosys July 2013 to October 2014 Business Problem TMobile client was interested in developing new features functionalities and enhancements in current eservices using Hadoop architecture in cloud environment The objective is to enhance TMobiles customer experience performance scalability and provide improved eService Platform agility Responsibilities Worked on architecture design to replace 23 components built in java by storm component Designed and developed restful web services to load and fetch data tofrom HBase Wrote Map Reduce programs to read huge log files and push the data into Hive Explored few components like Apache Kafka Apache Flume and Nagios to check if it suffices the requirements Administered the cluster of 10 nodes and made changes to addremove nodes and HDFS directory Configured added in extra node zoo keeper to check the consistency ToolsTechnologies Map Reduce Hive HBase Restful Web Services Apache Storm Apache kafka Apache Flume Nagios Zookeeper Big Data Developer Infosys Oxford July 2012 to June 2013 Business Problem The project represents the first enterprise set of foundation capabilities to enable unified access to any digital information in any format structured and unstructured in any information store The program makes the set of federated content stores look and act like a single system The user need not be aware of where the data is stored Responsibilities Involved in sprint planning and estimation Involved in development of all the modules using Spring Framework and amazon web services to createupdatedelete the data in DynamoDB Developed unit and integrated test cases using junit and grizzly Created sample test cases for the testing team using Selenium testing Developed JSON XML data adapter for transforming data sets Developed data adapter for Dynamo Db and S3 file system Experience on loading and transforming of large sets of structured semi structured and Unstructured data Responsible to manage data coming from different sources and application Design develop UI screen for clients to interact with system ToolsTechnologies Java Java Script JQuery CSS3 HTML AWS S3 file system Dynamo DB EC2 and Spring Framework Technology Lead Language CoreAdvanced Java April 2012 to June 2012 ICA and AH are among northern Europes leading retail companies They both use common java based application which is deployed on WAS61 server They are planning to upgrade this server to IBM WAS8 We are doing POC on two modules namely EMS buying and DCR for this upgrade Worked on building Java based tool to find incompatibilities in java based application Worked on building UNIX scripts for profiling of WAS server deployed on unix environment deploying EAR application on WAS etc Mentored a team of 6 members Research work on finding incompatibilities among different versions of J2EE technologies Technology Analyst Infosys Diageo UK China July 2011 to April 2012 Diageo is worlds leading premium drinks business Project included building webservices to support CRUD operations on the live Data Prepared High level and low level documents Developed web services to support CRUD operations on live data Unit tested the web services using JUnit Integration testing using JMeter Technology Analyst Infosys Istanbul TR January 2011 to June 2011 AKBank was an integration project where client wanted to replace their legacy systems with Finacle Infosys product using OSB oracle service bus as middleware Did detailed design of the services Developed services Resolved technical issues in integration testing Did unit and integration testing with multiple teams Coordination with other teams in SIT Associate cognizant Credit Suisse Zurich CH December 2009 to December 2010 Involved in Technical analysis of all the use cases of the project CRM for RMIC Customer Relationship Management for Relationship Managers of Institutional Clients Developed use cases using OBP and MCP frameworks defined by Credit Suisse Responsible for unit and integration testing Senior Developer Argusoft MedicAlert Foundation Turlock April 2009 to November 2009 Part of Analysis and designing of UI component development using JSF framework Involved in tools and technology selection for the UI development Development of all JSF pages Developing ANT scripts for WAR generation Integration responsibility with other layers in architecture Worked on Web Methods layer in the project Involved in the integration and unit testing of the modules Technologies used are JDK 1516 J2EE JSF XML SchemaXSDXSLT WSDL Net Beans as an IDE Web Methods SOA Suite Developer Designer Blaze J2ee Patterns TILES JSTL Oracle as a Database Toad Sql Developer Konakart MSVisio Senior Developer Argusoft District Health Information System Government of Gujarat November 2008 to April 2009 Involved in each and every phase of Spring MVC Web Architecture ie Hibernate Managed Bean Creation Domain Model Layer DAO Layer Service Layer Web Layer UI Layer Responsible for integrating web layer with business layers Involved in Report Creations and its designing using Ireport Technologies involved JAVAJ2EE JSP Spring WS XML SchemaXSDXSLT XBRL Net Beans as an IDE Oracle as a Database Glassfish App Server Toad MSVisio Windows XP Windows 6 mobile Java Developer Argusoft Enterprise class Investment Financial management system KW June 2006 to October 2008 Involved in Report creations and its designing using Business Intelligence Reporting Tool BIRT Hands on creating many services in Financial Accounting and Asset Management modules Developed JSPs for almost all modules Involved in integration and unit testing of modules Education BE in Computer Engineering Atmiya Institute of Technology Science Saurashtra University 2006 Skills Ejb J2ee Java Hibernate Spring Jboss jquery Jsp Struts Git Hadoop Hbase Hive Javascript Jenkins Orm Pig Python Reporting tools Svn",
    "entities": [
        "ToolsTechnologies Hive",
        "Spring Framework",
        "CSOrion Datalake",
        "Horton",
        "Edifecs Responsibilities Responsible for Project",
        "Developer Argusoft MedicAlert Foundation",
        "HDFS",
        "UNIX",
        "MRD",
        "UK",
        "Working",
        "JSON",
        "Oracle Hadoop Technical Lead Infosys Richardson",
        "IBM",
        "IP",
        "Credit Suisse Zurich",
        "providervendor",
        "UDAFs",
        "HCSC",
        "Hadoop",
        "XML",
        "EAR",
        "ASK Zena",
        "Morgan Chase Co",
        "Financial Accounting",
        "ANT Experienced",
        "GPD Government Program Data",
        "HBase",
        "Code Review",
        "eService",
        "Medicaid",
        "Cloudera Hadoop",
        "SAX Hands",
        "Hadoop Developer Infosys",
        "Developed Reports",
        "Developed",
        "US Hadoop Developer Infosys",
        "Historical Access Archive",
        "RDBMS Oracle",
        "Responsibilities Involved",
        "Education BE",
        "Ireport Technologies",
        "Technologies",
        "Developed JSON XML",
        "Horton Works",
        "Gujarat",
        "Big Data Project Lead Big Data",
        "Phoenix",
        "Linux",
        "Unstructured",
        "MedicareMediciad",
        "Investment Financial",
        "Parquet",
        "Credit Suisse Responsible",
        "TMG",
        "JBuilder Excellent",
        "Developer Argusoft District Health Information System Government",
        "OBP",
        "Business Intelligence Reporting Tool BIRT Hands",
        "Created Conceptual",
        "GIT",
        "CSV",
        "US",
        "Sqoop",
        "Sprint PlanningRetrospective",
        "Java Developer Argusoft Enterprise",
        "EMS",
        "CSOrion",
        "Created",
        "China",
        "Darmstadt Business Problem Merck Data",
        "Hadoop Architecture",
        "RMIC Customer Relationship Management for Relationship Managers of Institutional Clients Developed",
        "Present Client Information",
        "JSF",
        "ControlM",
        "Business Problem Government",
        "Servlets Experienced",
        "java",
        "AKBank",
        "Computer Engineering Atmiya Institute of Technology Science Saurashtra University 2006 Skills Ejb",
        "JUnit Integration",
        "Visio for Historical Access Archive Working",
        "ImportingExporting",
        "Asset Management",
        "OSB",
        "Finacle Infosys",
        "Self Service Audit Portal Responsibilities Responsible for Project",
        "Health Care Service Corporation",
        "JMeter Technology Analyst",
        "Big Data",
        "Hive",
        "Responsibilities Requirements",
        "XML XSLT",
        "J2EE technologies Technology Analyst",
        "AH",
        "Research",
        "SQS",
        "CRUD",
        "Medicare",
        "Blue Cross Blue Shield",
        "Impala",
        "the Model Ready Data",
        "CDC",
        "New York City",
        "UI",
        "XSD",
        "DCR",
        "ToolsTechnologies Horton",
        "FFAS",
        "IDE Oracle",
        "ABT",
        "Amazon Web Service",
        "Shell Script and Python Merck Billerica Masachussets",
        "Tableau",
        "Europes",
        "Software Development Life Cycle",
        "Fidelity Investments",
        "MCP",
        "ToolsTechnologies Cloudera Hadoop",
        "ETL Data",
        "Cloudera",
        "Showcasing",
        "Confluence Working"
    ],
    "experience": "Experience in setting up Hadoop clusters using open and vendor specific distribution like Horton Works Strong experience working on Apache Hortonworks and Cloudera Hadoop distributions Strong understanding of Map Reduce programming and experience in analyzing data using Map Reduce HiveQL Very good experience with both Map Reduce 1 Job Tracker and Map Reduce 2 YARN setups Good experience in monitoring and managing the Hadoop cluster Very Good knowledge of writing hive UDFs and queries Expertise in ImportingExporting data into HDFS from existing relational databases using Sqoop sparksql Very Good working knowledge of Amazon Web Service components like EC2 SQS SNS Dynamo DB S3 etc Managed data extraction for ETL Data warehouse and applied the transformation rules as necessary for data consistency Experienced with Struts Spring EJB Hibernate JMS SOA SOAP Restful Web services Design Patterns JBoss Apache tomcat Jetty JSP JSF Tiles and Servlets Experienced with Jenkins Hudson SVN git CVS JIRA Bugzilla Python and build tool ANT Experienced with XML related technologies such as XML XSLT XSD DOM and SAX Hands on experience in using IDEs like Eclipse NetBeans and JBuilder Excellent written and verbal communication skills interpersonal skills and selflearning attitude Worked extensively in Linux environment and writing shell scripts Excellent debugging skills and strong troubleshooting skills Extensive experience in all phases of Software Development Life Cycle SDLC including identification of business needs and constraints collection of requirements detailed design implementation testing deployment and maintenance Lead the team of 7 and highly Involved in recruiting team members for more than 6 years Work Experience Big Data Project Lead Mphasis Lewisville TX March 2019 to Present Client Information JP Morgan Chase Co is an American multinational investment bank and financial services company headquartered in New York City It is major provider of various investment banking and financial services Business Problem Global Identity Access Management has their internal system to generate reports on the requests coming from internal and external audit teams Since the data is located in different sources it takes good amount of time to generate a single report as a response to their query Also the reports manually generated are not accurate and provides missleading information Hence as a solution to this problem all the data needs to be centrally located in Hadoop ecosystem and a UI portal needs to be built on top of it to generate reports for internal and external audit systems This process of data ingestion is termed as Historical Access Archive and the reporting system is termed as Self Service Audit Portal Responsibilities Responsible for Project delivery Understand the business requirements and Involved in the evaluating the right technology stacks for the project Created Conceptual and detailed design diagrams in Visio for Historical Access Archive Working on implementation in Cloudera cluster owned by JPMCs CSOrion team Working closely with Product Managers Data Modeling teams Business Analysts and testing teams Created the low level and high level design documents in Confluence Working on Sqoop scripts to fetch the data from RDBMS source to store in CSOrion Datalake in Parquet file formats Loading the data from RDBMS Oracle source to HDFS and then load from HDFS to Hive tables Participate in daily Scrum and Sprint PlanningRetrospective meetings Develop the application in Linux development environment Test environment Linux and move the application to Production As a technical lead took the responsibility for all the releases and post validations Documenting the changes done during the release Brought entire team up to the speed by ramping up the team with skillset and ongoing tasks Took the leadership of the completing processes from start to end and integrating with multiple teams ToolsTechnologies Cloudera Hadoop ecosystem Sqoop Shell script Hive Autosys Scheduling tool Oracle Hadoop Technical Lead Infosys Richardson TX May 2015 to March 2019 Client Information BCBS is Health Care Service Corporation insurance company where in nearly one in three Americans rely on Blue Cross Blue Shield companies for access to safe quality and affordable healthcare Business Problem Government programs are receiving huge data from different vendors from different sources With the increasing data it is difficult to analyze which providervendor has sent the files MedicareMediciad claims data on timely basis Also the incoming data from providers needs to be processed and send to the vendors in the form of extracts with specific information Working on a GPD Government Program Data and ICC Projects HCSC Health Care Service Corporation receives different types of membership and claims files Medicare and Medicaid from different vendors from different sources HCSC will process these files for analytics teams and stores the data in Hive and HBase tables The processed data will be stored in history current snapshot and CDC tables to retain the latest and greatest information of each Subscriber claim details and send to the vendors in JSON format ICC will receive claims data from bluechip and TMG vendors and process the data and generates the professional and institutional claims These generated files will be validated against and Edifecs Responsibilities Responsible for Project delivery Understand the business requirements and Involved in the design and development of project Working on implementation in Horton Works 26 cluster Working closely with Product Managers Data Modeling teams Business Analysts and testing teams Involved in the low level and high level design Working on Pig scripts in Java to process the event data Xml Text and CSV formats and create and store in HBaseHive table in ORC Sequence and XML formats Wrote the shell scripts to process the jobs and schedule the jobs using ASK Zena scheduler tool Configured and created jobs in event based and time based Loading the data from Teradata to HDFS and then load from HDFS to Hive tables Participate in daily Scrum and Sprint PlanningRetrospective meetings Develop the application in Linux development environment Test environment Linux and move the application to Production As a technical lead took the responsibility for all the releases and post validations Documenting the changes done during the release Brought entire team up to the speed by taking sessions on using GIT and ramping up the team with skillset Took the leadership of the completing processes from start to end and integrating with multiple teams ToolsTechnologies Horton works 26 Pig Script Phoenix 47 Pig Hive HBase Zena Scheduling tool Shell Script and Python Merck Billerica Masachussets US onsite Hadoop Developer Infosys April 2016 to November 2016 Client Information is a German multinational chemical pharmaceutical and life sciences company headquartered in Darmstadt Business Problem Merck Data scientists wants to analyze the products chemicals their usage their impact on variation of proportion aggregated information This problem statement was acknowledged with the visualizations which was built to work on Hadoop environment on hive tables Responsibilities Worked on a live 50 nodes Hadoop cluster running on Horton works Hadoop Developed visualizations using Tableau and hive tables which is used for data analysis by data scientists Created custom UDAFs using java and used in MRD process ToolsTechnologies Hive unix scripts sqoop Tableau Horton works Hadoop Hadoop Lead Developer Infosys Durham NC March 2015 to December 2015 Client Information Fidelity Investments is an American multinational financial services corporation Business Problem Fidelitys business users wanted to have aggregated information over a period of time on millions of IPs at one go This problem statement was acknowledged with the Model Ready Data MRD process which was built to work on Hadoop environment on hive tables Responsibilities Worked on a live 87 nodes Hadoop cluster running on cloudera Hadoop Developed Model Ready Data Process using hive queries Java and Unix Scripts which is used for modeling and scoring purpose by business Created custom UDAFs using java and used in MRD process Unit and Integration testing of the MRD process using hive queries Did unit and integration testing of Monthly MID to IP and ABT process which are basically monthly and daily aggregations of data using ControlM jobs ToolsTechnologies Hive unix scripts ControlM cloudera Hadoop 54 Hadoop Lead Developer Infosys Durham NC December 2014 to March 2015 Client Information Fidelity Investments is an American multinational financial services corporation Business Problem To produce meaningful and consumable key insights into our clients actions and behaviors in order to have a more comprehensive and holistic understanding therefore increasing productivity of the FFAS business and ultimately driving sales Responsibilities Requirements gathering and clarification from clients Coordinating with offshore team to explain and crossverify the requirements Solving technical challenges faced by offshore team in deployingrunning the use cases Developed Reports using Tableau and hiveimpala Worked on establishing the environment by installing hadoop components needed for use cases Code Review and deployment Showcasing the use cases in business meetings to the clients ToolsTechnologies Map Reduce Hive Impala Tableau NLTK Oozie sqoop Client TMobile Washington US Hadoop Developer Infosys July 2013 to October 2014 Business Problem TMobile client was interested in developing new features functionalities and enhancements in current eservices using Hadoop architecture in cloud environment The objective is to enhance TMobiles customer experience performance scalability and provide improved eService Platform agility Responsibilities Worked on architecture design to replace 23 components built in java by storm component Designed and developed restful web services to load and fetch data tofrom HBase Wrote Map Reduce programs to read huge log files and push the data into Hive Explored few components like Apache Kafka Apache Flume and Nagios to check if it suffices the requirements Administered the cluster of 10 nodes and made changes to addremove nodes and HDFS directory Configured added in extra node zoo keeper to check the consistency ToolsTechnologies Map Reduce Hive HBase Restful Web Services Apache Storm Apache kafka Apache Flume Nagios Zookeeper Big Data Developer Infosys Oxford July 2012 to June 2013 Business Problem The project represents the first enterprise set of foundation capabilities to enable unified access to any digital information in any format structured and unstructured in any information store The program makes the set of federated content stores look and act like a single system The user need not be aware of where the data is stored Responsibilities Involved in sprint planning and estimation Involved in development of all the modules using Spring Framework and amazon web services to createupdatedelete the data in DynamoDB Developed unit and integrated test cases using junit and grizzly Created sample test cases for the testing team using Selenium testing Developed JSON XML data adapter for transforming data sets Developed data adapter for Dynamo Db and S3 file system Experience on loading and transforming of large sets of structured semi structured and Unstructured data Responsible to manage data coming from different sources and application Design develop UI screen for clients to interact with system ToolsTechnologies Java Java Script JQuery CSS3 HTML AWS S3 file system Dynamo DB EC2 and Spring Framework Technology Lead Language CoreAdvanced Java April 2012 to June 2012 ICA and AH are among northern Europes leading retail companies They both use common java based application which is deployed on WAS61 server They are planning to upgrade this server to IBM WAS8 We are doing POC on two modules namely EMS buying and DCR for this upgrade Worked on building Java based tool to find incompatibilities in java based application Worked on building UNIX scripts for profiling of WAS server deployed on unix environment deploying EAR application on WAS etc Mentored a team of 6 members Research work on finding incompatibilities among different versions of J2EE technologies Technology Analyst Infosys Diageo UK China July 2011 to April 2012 Diageo is worlds leading premium drinks business Project included building webservices to support CRUD operations on the live Data Prepared High level and low level documents Developed web services to support CRUD operations on live data Unit tested the web services using JUnit Integration testing using JMeter Technology Analyst Infosys Istanbul TR January 2011 to June 2011 AKBank was an integration project where client wanted to replace their legacy systems with Finacle Infosys product using OSB oracle service bus as middleware Did detailed design of the services Developed services Resolved technical issues in integration testing Did unit and integration testing with multiple teams Coordination with other teams in SIT Associate cognizant Credit Suisse Zurich CH December 2009 to December 2010 Involved in Technical analysis of all the use cases of the project CRM for RMIC Customer Relationship Management for Relationship Managers of Institutional Clients Developed use cases using OBP and MCP frameworks defined by Credit Suisse Responsible for unit and integration testing Senior Developer Argusoft MedicAlert Foundation Turlock April 2009 to November 2009 Part of Analysis and designing of UI component development using JSF framework Involved in tools and technology selection for the UI development Development of all JSF pages Developing ANT scripts for WAR generation Integration responsibility with other layers in architecture Worked on Web Methods layer in the project Involved in the integration and unit testing of the modules Technologies used are JDK 1516 J2EE JSF XML SchemaXSDXSLT WSDL Net Beans as an IDE Web Methods SOA Suite Developer Designer Blaze J2ee Patterns TILES JSTL Oracle as a Database Toad Sql Developer Konakart MSVisio Senior Developer Argusoft District Health Information System Government of Gujarat November 2008 to April 2009 Involved in each and every phase of Spring MVC Web Architecture ie Hibernate Managed Bean Creation Domain Model Layer DAO Layer Service Layer Web Layer UI Layer Responsible for integrating web layer with business layers Involved in Report Creations and its designing using Ireport Technologies involved JAVAJ2EE JSP Spring WS XML SchemaXSDXSLT XBRL Net Beans as an IDE Oracle as a Database Glassfish App Server Toad MSVisio Windows XP Windows 6 mobile Java Developer Argusoft Enterprise class Investment Financial management system KW June 2006 to October 2008 Involved in Report creations and its designing using Business Intelligence Reporting Tool BIRT Hands on creating many services in Financial Accounting and Asset Management modules Developed JSPs for almost all modules Involved in integration and unit testing of modules Education BE in Computer Engineering Atmiya Institute of Technology Science Saurashtra University 2006 Skills Ejb J2ee Java Hibernate Spring Jboss jquery Jsp Struts Git Hadoop Hbase Hive Javascript Jenkins Orm Pig Python Reporting tools Svn",
    "extracted_keywords": [
        "Big",
        "Data",
        "Project",
        "Lead",
        "Big",
        "Data",
        "Project",
        "Lead",
        "Big",
        "Data",
        "Project",
        "Lead",
        "Mphasis",
        "Irving",
        "TX",
        "years",
        "experience",
        "building",
        "applications",
        "Java",
        "J2EE",
        "Big",
        "Data",
        "Technologies",
        "depth",
        "understandingknowledge",
        "Hadoop",
        "Architecture",
        "components",
        "years",
        "experience",
        "Big",
        "Data",
        "developer",
        "technology",
        "lead",
        "understanding",
        "Hadoop",
        "Eco",
        "System",
        "Map",
        "Reduce",
        "Hive",
        "Sqoop",
        "Oozie",
        "Storm",
        "Flume",
        "HBase",
        "Pig",
        "Sqoop",
        "Cassandra",
        "MongoDB",
        "Phoenix",
        "Impala",
        "Experience",
        "Hadoop",
        "clusters",
        "vendor",
        "distribution",
        "Horton",
        "experience",
        "Apache",
        "Hortonworks",
        "Cloudera",
        "Hadoop",
        "understanding",
        "Map",
        "programming",
        "experience",
        "data",
        "Map",
        "Reduce",
        "HiveQL",
        "experience",
        "Map",
        "Reduce",
        "Job",
        "Tracker",
        "Map",
        "Reduce",
        "YARN",
        "setups",
        "experience",
        "monitoring",
        "Hadoop",
        "cluster",
        "knowledge",
        "hive",
        "UDFs",
        "Expertise",
        "ImportingExporting",
        "data",
        "HDFS",
        "databases",
        "Sqoop",
        "sparksql",
        "knowledge",
        "Amazon",
        "Web",
        "Service",
        "components",
        "EC2",
        "SQS",
        "SNS",
        "Dynamo",
        "DB",
        "S3",
        "data",
        "extraction",
        "ETL",
        "Data",
        "warehouse",
        "transformation",
        "rules",
        "data",
        "consistency",
        "Struts",
        "Spring",
        "EJB",
        "Hibernate",
        "JMS",
        "SOA",
        "SOAP",
        "Restful",
        "Web",
        "services",
        "Design",
        "Patterns",
        "JBoss",
        "Apache",
        "tomcat",
        "Jetty",
        "JSP",
        "JSF",
        "Tiles",
        "Servlets",
        "Jenkins",
        "Hudson",
        "SVN",
        "git",
        "CVS",
        "JIRA",
        "Bugzilla",
        "Python",
        "tool",
        "ANT",
        "XML",
        "technologies",
        "XML",
        "XSLT",
        "XSD",
        "DOM",
        "SAX",
        "Hands",
        "experience",
        "IDEs",
        "Eclipse",
        "NetBeans",
        "JBuilder",
        "Excellent",
        "communication",
        "skills",
        "skills",
        "attitude",
        "Linux",
        "environment",
        "shell",
        "scripts",
        "debugging",
        "skills",
        "troubleshooting",
        "skills",
        "experience",
        "phases",
        "Software",
        "Development",
        "Life",
        "Cycle",
        "SDLC",
        "identification",
        "business",
        "needs",
        "collection",
        "requirements",
        "design",
        "implementation",
        "testing",
        "deployment",
        "maintenance",
        "Lead",
        "team",
        "recruiting",
        "team",
        "members",
        "years",
        "Work",
        "Experience",
        "Big",
        "Data",
        "Project",
        "Lead",
        "Mphasis",
        "Lewisville",
        "TX",
        "March",
        "Present",
        "Client",
        "Information",
        "JP",
        "Morgan",
        "Chase",
        "Co",
        "investment",
        "bank",
        "services",
        "company",
        "New",
        "York",
        "City",
        "provider",
        "investment",
        "banking",
        "services",
        "Business",
        "Problem",
        "Global",
        "Identity",
        "Access",
        "Management",
        "system",
        "reports",
        "requests",
        "audit",
        "teams",
        "data",
        "sources",
        "amount",
        "time",
        "report",
        "response",
        "query",
        "reports",
        "information",
        "solution",
        "problem",
        "data",
        "Hadoop",
        "ecosystem",
        "UI",
        "needs",
        "top",
        "reports",
        "audit",
        "systems",
        "process",
        "data",
        "ingestion",
        "Historical",
        "Access",
        "Archive",
        "reporting",
        "system",
        "Self",
        "Service",
        "Audit",
        "Portal",
        "Responsibilities",
        "Project",
        "delivery",
        "business",
        "requirements",
        "technology",
        "stacks",
        "project",
        "Conceptual",
        "design",
        "diagrams",
        "Visio",
        "Historical",
        "Access",
        "Archive",
        "Working",
        "implementation",
        "Cloudera",
        "cluster",
        "JPMCs",
        "CSOrion",
        "team",
        "Product",
        "Managers",
        "Data",
        "Modeling",
        "teams",
        "Business",
        "Analysts",
        "testing",
        "teams",
        "level",
        "level",
        "design",
        "documents",
        "Confluence",
        "Working",
        "Sqoop",
        "scripts",
        "data",
        "source",
        "CSOrion",
        "Datalake",
        "Parquet",
        "file",
        "formats",
        "data",
        "RDBMS",
        "Oracle",
        "source",
        "HDFS",
        "HDFS",
        "Hive",
        "tables",
        "Participate",
        "Scrum",
        "Sprint",
        "PlanningRetrospective",
        "meetings",
        "application",
        "Linux",
        "development",
        "environment",
        "Test",
        "environment",
        "Linux",
        "application",
        "Production",
        "lead",
        "responsibility",
        "releases",
        "post",
        "validations",
        "changes",
        "release",
        "team",
        "speed",
        "team",
        "skillset",
        "tasks",
        "leadership",
        "processes",
        "start",
        "end",
        "teams",
        "ToolsTechnologies",
        "Cloudera",
        "Hadoop",
        "ecosystem",
        "Sqoop",
        "Shell",
        "script",
        "Hive",
        "Autosys",
        "Scheduling",
        "tool",
        "Oracle",
        "Hadoop",
        "Technical",
        "Lead",
        "Infosys",
        "Richardson",
        "TX",
        "May",
        "March",
        "Client",
        "Information",
        "BCBS",
        "Health",
        "Care",
        "Service",
        "Corporation",
        "insurance",
        "company",
        "Americans",
        "Blue",
        "Cross",
        "Blue",
        "Shield",
        "companies",
        "access",
        "quality",
        "healthcare",
        "Business",
        "Problem",
        "Government",
        "programs",
        "data",
        "vendors",
        "sources",
        "data",
        "providervendor",
        "files",
        "MedicareMediciad",
        "data",
        "basis",
        "data",
        "providers",
        "vendors",
        "form",
        "extracts",
        "information",
        "GPD",
        "Government",
        "Program",
        "Data",
        "ICC",
        "Projects",
        "HCSC",
        "Health",
        "Care",
        "Service",
        "Corporation",
        "types",
        "membership",
        "claims",
        "Medicare",
        "Medicaid",
        "vendors",
        "sources",
        "HCSC",
        "files",
        "analytics",
        "teams",
        "stores",
        "data",
        "Hive",
        "HBase",
        "data",
        "history",
        "snapshot",
        "CDC",
        "tables",
        "information",
        "Subscriber",
        "details",
        "vendors",
        "format",
        "ICC",
        "claims",
        "data",
        "bluechip",
        "TMG",
        "vendors",
        "data",
        "claims",
        "files",
        "Edifecs",
        "Responsibilities",
        "Project",
        "delivery",
        "business",
        "requirements",
        "design",
        "development",
        "project",
        "implementation",
        "Horton",
        "cluster",
        "Working",
        "Product",
        "Managers",
        "Data",
        "Modeling",
        "teams",
        "Business",
        "Analysts",
        "testing",
        "teams",
        "level",
        "level",
        "design",
        "Working",
        "Pig",
        "scripts",
        "Java",
        "event",
        "data",
        "Xml",
        "Text",
        "CSV",
        "formats",
        "table",
        "ORC",
        "Sequence",
        "XML",
        "formats",
        "shell",
        "scripts",
        "jobs",
        "jobs",
        "ASK",
        "Zena",
        "scheduler",
        "tool",
        "Configured",
        "jobs",
        "event",
        "time",
        "Loading",
        "data",
        "Teradata",
        "HDFS",
        "HDFS",
        "Hive",
        "tables",
        "Participate",
        "Scrum",
        "Sprint",
        "PlanningRetrospective",
        "meetings",
        "application",
        "Linux",
        "development",
        "environment",
        "Test",
        "environment",
        "Linux",
        "application",
        "Production",
        "lead",
        "responsibility",
        "releases",
        "post",
        "validations",
        "changes",
        "release",
        "team",
        "speed",
        "sessions",
        "GIT",
        "team",
        "skillset",
        "leadership",
        "processes",
        "start",
        "end",
        "teams",
        "ToolsTechnologies",
        "Horton",
        "Pig",
        "Script",
        "Phoenix",
        "Pig",
        "Hive",
        "HBase",
        "Zena",
        "Scheduling",
        "tool",
        "Shell",
        "Script",
        "Python",
        "Merck",
        "Billerica",
        "Masachussets",
        "US",
        "Hadoop",
        "Developer",
        "Infosys",
        "April",
        "November",
        "Client",
        "Information",
        "chemical",
        "pharmaceutical",
        "life",
        "sciences",
        "company",
        "Darmstadt",
        "Business",
        "Problem",
        "Merck",
        "Data",
        "scientists",
        "products",
        "usage",
        "impact",
        "variation",
        "proportion",
        "information",
        "problem",
        "statement",
        "visualizations",
        "Hadoop",
        "environment",
        "tables",
        "Responsibilities",
        "nodes",
        "Hadoop",
        "cluster",
        "Horton",
        "Hadoop",
        "visualizations",
        "Tableau",
        "hive",
        "tables",
        "data",
        "analysis",
        "data",
        "scientists",
        "custom",
        "UDAFs",
        "MRD",
        "process",
        "ToolsTechnologies",
        "Hive",
        "unix",
        "scripts",
        "sqoop",
        "Tableau",
        "Horton",
        "Hadoop",
        "Hadoop",
        "Lead",
        "Developer",
        "Infosys",
        "Durham",
        "NC",
        "March",
        "December",
        "Client",
        "Information",
        "Fidelity",
        "Investments",
        "services",
        "corporation",
        "Business",
        "Problem",
        "Fidelitys",
        "business",
        "users",
        "information",
        "period",
        "time",
        "millions",
        "IPs",
        "problem",
        "statement",
        "Model",
        "Ready",
        "Data",
        "MRD",
        "process",
        "Hadoop",
        "environment",
        "tables",
        "Responsibilities",
        "nodes",
        "Hadoop",
        "cluster",
        "cloudera",
        "Hadoop",
        "Developed",
        "Model",
        "Ready",
        "Data",
        "Process",
        "hive",
        "queries",
        "Java",
        "Unix",
        "Scripts",
        "modeling",
        "scoring",
        "purpose",
        "business",
        "custom",
        "UDAFs",
        "MRD",
        "process",
        "Unit",
        "Integration",
        "testing",
        "MRD",
        "process",
        "hive",
        "queries",
        "unit",
        "integration",
        "testing",
        "MID",
        "IP",
        "ABT",
        "process",
        "aggregations",
        "data",
        "ControlM",
        "jobs",
        "ToolsTechnologies",
        "Hive",
        "unix",
        "scripts",
        "ControlM",
        "cloudera",
        "Hadoop",
        "Hadoop",
        "Lead",
        "Developer",
        "Infosys",
        "Durham",
        "NC",
        "December",
        "March",
        "Client",
        "Information",
        "Fidelity",
        "Investments",
        "services",
        "corporation",
        "Business",
        "Problem",
        "insights",
        "clients",
        "actions",
        "behaviors",
        "order",
        "understanding",
        "productivity",
        "FFAS",
        "business",
        "sales",
        "Responsibilities",
        "Requirements",
        "gathering",
        "clarification",
        "clients",
        "team",
        "requirements",
        "challenges",
        "team",
        "use",
        "cases",
        "Reports",
        "Tableau",
        "hiveimpala",
        "environment",
        "hadoop",
        "components",
        "use",
        "cases",
        "Code",
        "Review",
        "deployment",
        "Showcasing",
        "use",
        "cases",
        "business",
        "meetings",
        "clients",
        "ToolsTechnologies",
        "Map",
        "Reduce",
        "Hive",
        "Impala",
        "Tableau",
        "NLTK",
        "Oozie",
        "sqoop",
        "Client",
        "TMobile",
        "Washington",
        "US",
        "Hadoop",
        "Developer",
        "Infosys",
        "July",
        "October",
        "Business",
        "Problem",
        "TMobile",
        "client",
        "features",
        "functionalities",
        "enhancements",
        "eservices",
        "Hadoop",
        "architecture",
        "cloud",
        "environment",
        "objective",
        "TMobiles",
        "customer",
        "experience",
        "performance",
        "scalability",
        "eService",
        "Platform",
        "agility",
        "Responsibilities",
        "architecture",
        "design",
        "components",
        "java",
        "storm",
        "component",
        "web",
        "services",
        "data",
        "HBase",
        "Wrote",
        "Map",
        "programs",
        "log",
        "files",
        "data",
        "Hive",
        "components",
        "Apache",
        "Kafka",
        "Apache",
        "Flume",
        "Nagios",
        "requirements",
        "cluster",
        "nodes",
        "changes",
        "nodes",
        "HDFS",
        "directory",
        "Configured",
        "node",
        "zoo",
        "keeper",
        "consistency",
        "ToolsTechnologies",
        "Map",
        "Reduce",
        "Hive",
        "HBase",
        "Restful",
        "Web",
        "Services",
        "Apache",
        "Storm",
        "Apache",
        "Apache",
        "Flume",
        "Nagios",
        "Zookeeper",
        "Big",
        "Data",
        "Developer",
        "Infosys",
        "Oxford",
        "July",
        "June",
        "Business",
        "Problem",
        "project",
        "enterprise",
        "set",
        "foundation",
        "capabilities",
        "access",
        "information",
        "format",
        "information",
        "store",
        "program",
        "set",
        "content",
        "stores",
        "system",
        "user",
        "data",
        "Responsibilities",
        "sprint",
        "planning",
        "estimation",
        "development",
        "modules",
        "Spring",
        "Framework",
        "amazon",
        "web",
        "services",
        "data",
        "unit",
        "test",
        "cases",
        "junit",
        "grizzly",
        "Created",
        "sample",
        "test",
        "cases",
        "testing",
        "team",
        "Selenium",
        "testing",
        "XML",
        "data",
        "data",
        "data",
        "adapter",
        "Dynamo",
        "Db",
        "S3",
        "file",
        "system",
        "Experience",
        "loading",
        "transforming",
        "sets",
        "data",
        "data",
        "sources",
        "application",
        "Design",
        "UI",
        "screen",
        "clients",
        "system",
        "ToolsTechnologies",
        "Java",
        "Java",
        "Script",
        "JQuery",
        "CSS3",
        "HTML",
        "AWS",
        "S3",
        "file",
        "system",
        "Dynamo",
        "DB",
        "EC2",
        "Spring",
        "Framework",
        "Technology",
        "Lead",
        "Language",
        "CoreAdvanced",
        "Java",
        "April",
        "June",
        "ICA",
        "AH",
        "Europes",
        "companies",
        "java",
        "application",
        "WAS61",
        "server",
        "server",
        "IBM",
        "WAS8",
        "POC",
        "modules",
        "EMS",
        "buying",
        "DCR",
        "upgrade",
        "Java",
        "tool",
        "incompatibilities",
        "java",
        "application",
        "UNIX",
        "scripts",
        "profiling",
        "WAS",
        "server",
        "unix",
        "environment",
        "EAR",
        "application",
        "team",
        "members",
        "Research",
        "work",
        "incompatibilities",
        "versions",
        "J2EE",
        "technologies",
        "Technology",
        "Analyst",
        "Infosys",
        "Diageo",
        "UK",
        "China",
        "July",
        "April",
        "Diageo",
        "worlds",
        "premium",
        "drinks",
        "business",
        "Project",
        "building",
        "webservices",
        "CRUD",
        "operations",
        "Data",
        "Prepared",
        "level",
        "level",
        "documents",
        "web",
        "services",
        "CRUD",
        "operations",
        "data",
        "Unit",
        "web",
        "services",
        "JUnit",
        "Integration",
        "testing",
        "JMeter",
        "Technology",
        "Analyst",
        "Infosys",
        "Istanbul",
        "TR",
        "January",
        "June",
        "AKBank",
        "integration",
        "project",
        "client",
        "legacy",
        "systems",
        "Finacle",
        "Infosys",
        "product",
        "OSB",
        "oracle",
        "service",
        "bus",
        "middleware",
        "design",
        "services",
        "services",
        "issues",
        "integration",
        "testing",
        "unit",
        "integration",
        "testing",
        "teams",
        "Coordination",
        "teams",
        "SIT",
        "cognizant",
        "Credit",
        "Suisse",
        "Zurich",
        "CH",
        "December",
        "December",
        "analysis",
        "use",
        "cases",
        "project",
        "CRM",
        "RMIC",
        "Customer",
        "Relationship",
        "Management",
        "Relationship",
        "Managers",
        "Institutional",
        "Clients",
        "use",
        "cases",
        "OBP",
        "MCP",
        "frameworks",
        "Credit",
        "Suisse",
        "Responsible",
        "unit",
        "integration",
        "Senior",
        "Developer",
        "Argusoft",
        "MedicAlert",
        "Foundation",
        "Turlock",
        "April",
        "November",
        "Part",
        "Analysis",
        "designing",
        "UI",
        "component",
        "development",
        "JSF",
        "framework",
        "tools",
        "technology",
        "selection",
        "UI",
        "development",
        "Development",
        "JSF",
        "pages",
        "ANT",
        "scripts",
        "WAR",
        "generation",
        "Integration",
        "responsibility",
        "layers",
        "architecture",
        "Web",
        "Methods",
        "layer",
        "project",
        "integration",
        "unit",
        "testing",
        "modules",
        "Technologies",
        "JDK",
        "J2EE",
        "JSF",
        "XML",
        "SchemaXSDXSLT",
        "WSDL",
        "Net",
        "Beans",
        "IDE",
        "Web",
        "Methods",
        "SOA",
        "Suite",
        "Developer",
        "Designer",
        "Blaze",
        "J2ee",
        "Patterns",
        "TILES",
        "JSTL",
        "Oracle",
        "Database",
        "Toad",
        "Sql",
        "Developer",
        "Konakart",
        "MSVisio",
        "Senior",
        "Developer",
        "Argusoft",
        "District",
        "Health",
        "Information",
        "System",
        "Government",
        "Gujarat",
        "November",
        "April",
        "phase",
        "Spring",
        "MVC",
        "Web",
        "Architecture",
        "Hibernate",
        "Managed",
        "Bean",
        "Creation",
        "Domain",
        "Model",
        "Layer",
        "DAO",
        "Layer",
        "Service",
        "Layer",
        "Web",
        "Layer",
        "UI",
        "Layer",
        "Responsible",
        "web",
        "layer",
        "business",
        "layers",
        "Report",
        "Creations",
        "designing",
        "Ireport",
        "Technologies",
        "JAVAJ2EE",
        "JSP",
        "Spring",
        "WS",
        "XML",
        "XBRL",
        "Net",
        "Beans",
        "IDE",
        "Oracle",
        "Database",
        "Glassfish",
        "App",
        "Server",
        "Toad",
        "MSVisio",
        "Windows",
        "XP",
        "Windows",
        "mobile",
        "Java",
        "Developer",
        "Argusoft",
        "Enterprise",
        "class",
        "Investment",
        "Financial",
        "management",
        "system",
        "KW",
        "June",
        "October",
        "Report",
        "creations",
        "designing",
        "Business",
        "Intelligence",
        "Reporting",
        "Tool",
        "BIRT",
        "Hands",
        "services",
        "Financial",
        "Accounting",
        "Asset",
        "Management",
        "modules",
        "JSPs",
        "modules",
        "integration",
        "unit",
        "testing",
        "modules",
        "Education",
        "Computer",
        "Engineering",
        "Atmiya",
        "Institute",
        "Technology",
        "Science",
        "Saurashtra",
        "University",
        "Skills",
        "Ejb",
        "J2ee",
        "Java",
        "Hibernate",
        "Spring",
        "Jboss",
        "jquery",
        "Jsp",
        "Struts",
        "Git",
        "Hadoop",
        "Hbase",
        "Hive",
        "Javascript",
        "Jenkins",
        "Orm",
        "Pig",
        "Python",
        "Reporting",
        "tools",
        "Svn"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T19:37:45.108335",
    "resume_data": "Big Data Project Lead Big Data Project Lead Big Data Project Lead Mphasis Irving TX More than 13 years of experience in building distributed scalable and complex applications using Java J2EE and Big Data Technologies In depth understandingknowledge of Hadoop Architecture and its components More than 6 years of experience as Big Data senior developer and technology lead with good understanding of the Hadoop Eco System Map Reduce Hive Sqoop Oozie Storm Flume HBase Pig Sqoop Cassandra MongoDB DynamoDB Phoenix Impala Experience in setting up Hadoop clusters using open and vendor specific distribution like Horton Works Strong experience working on Apache Hortonworks and Cloudera Hadoop distributions Strong understanding of Map Reduce programming and experience in analyzing data using Map Reduce HiveQL Very good experience with both Map Reduce 1 Job Tracker and Map Reduce 2 YARN setups Good experience in monitoring and managing the Hadoop cluster Very Good knowledge of writing hive UDFs and queries Expertise in ImportingExporting data into HDFS from existing relational databases using Sqoop sparksql Very Good working knowledge of Amazon Web Service components like EC2 SQS SNS Dynamo DB S3 etc Managed data extraction for ETL Data warehouse and applied the transformation rules as necessary for data consistency Experienced with Struts Spring EJB Hibernate JMS SOA SOAP Restful Web services Design Patterns JBoss Apache tomcat Jetty JSP JSF Tiles and Servlets Experienced with Jenkins Hudson SVN git CVS JIRA Bugzilla Python and build tool ANT Experienced with XML related technologies such as XML XSLT XSD DOM and SAX Hands on experience in using IDEs like Eclipse NetBeans and JBuilder Excellent written and verbal communication skills interpersonal skills and selflearning attitude Worked extensively in Linux environment and writing shell scripts Excellent debugging skills and strong troubleshooting skills Extensive experience in all phases of Software Development Life Cycle SDLC including identification of business needs and constraints collection of requirements detailed design implementation testing deployment and maintenance Lead the team of 7 and highly Involved in recruiting team members for more than 6 years Work Experience Big Data Project Lead Mphasis Lewisville TX March 2019 to Present Client Information JP Morgan Chase Co is an American multinational investment bank and financial services company headquartered in New York City It is major provider of various investment banking and financial services Business Problem Global Identity Access Management has their internal system to generate reports on the requests coming from internal and external audit teams Since the data is located in different sources it takes good amount of time to generate a single report as a response to their query Also the reports manually generated are not accurate and provides missleading information Hence as a solution to this problem all the data needs to be centrally located in Hadoop ecosystem and a UI portal needs to be built on top of it to generate reports for internal and external audit systems This process of data ingestion is termed as Historical Access Archive and the reporting system is termed as Self Service Audit Portal Responsibilities Responsible for Project delivery Understand the business requirements and Involved in the evaluating the right technology stacks for the project Created Conceptual and detailed design diagrams in Visio for Historical Access Archive Working on implementation in Cloudera cluster owned by JPMCs CSOrion team Working closely with Product Managers Data Modeling teams Business Analysts and testing teams Created the low level and high level design documents in Confluence Working on Sqoop scripts to fetch the data from RDBMS source to store in CSOrion Datalake in Parquet file formats Loading the data from RDBMS Oracle source to HDFS and then load from HDFS to Hive tables Participate in daily Scrum and Sprint PlanningRetrospective meetings Develop the application in Linux development environment Test environment Linux and move the application to Production As a technical lead took the responsibility for all the releases and post validations Documenting the changes done during the release Brought entire team up to the speed by ramping up the team with skillset and ongoing tasks Took the leadership of the completing processes from start to end and integrating with multiple teams ToolsTechnologies Cloudera Hadoop ecosystem Sqoop Shell script Hive Autosys Scheduling tool Oracle Hadoop Technical Lead Infosys Richardson TX May 2015 to March 2019 Client Information BCBS is Health Care Service Corporation insurance company where in nearly one in three Americans rely on Blue Cross Blue Shield companies for access to safe quality and affordable healthcare Business Problem Government programs are receiving huge data from different vendors from different sources With the increasing data it is difficult to analyze which providervendor has sent the files MedicareMediciad claims data on timely basis Also the incoming data from providers needs to be processed and send to the vendors in the form of extracts with specific information Working on a GPD Government Program Data and ICC Projects HCSC Health Care Service Corporation receives different types of membership and claims files Medicare and Medicaid from different vendors from different sources HCSC will process these files for analytics teams and stores the data in Hive and HBase tables The processed data will be stored in history current snapshot and CDC tables to retain the latest and greatest information of each Subscriber claim details and send to the vendors in JSON format ICC will receive claims data from bluechip and TMG vendors and process the data and generates the professional and institutional claims These generated files will be validated against and Edifecs Responsibilities Responsible for Project delivery Understand the business requirements and Involved in the design and development of project Working on implementation in Horton Works 26 cluster Working closely with Product Managers Data Modeling teams Business Analysts and testing teams Involved in the low level and high level design Working on Pig scripts in Java to process the event data Xml Text and CSV formats and create and store in HBaseHive table in ORC Sequence and XML formats Wrote the shell scripts to process the jobs and schedule the jobs using ASK Zena scheduler tool Configured and created jobs in event based and time based Loading the data from Teradata to HDFS and then load from HDFS to Hive tables Participate in daily Scrum and Sprint PlanningRetrospective meetings Develop the application in Linux development environment Test environment Linux and move the application to Production As a technical lead took the responsibility for all the releases and post validations Documenting the changes done during the release Brought entire team up to the speed by taking sessions on using GIT and ramping up the team with skillset Took the leadership of the completing processes from start to end and integrating with multiple teams ToolsTechnologies Horton works 26 Pig Script Phoenix 47 Pig Hive HBase Zena Scheduling tool Shell Script and Python Merck Billerica Masachussets US onsite Hadoop Developer Infosys April 2016 to November 2016 Client Information is a German multinational chemical pharmaceutical and life sciences company headquartered in Darmstadt Business Problem Merck Data scientists wants to analyze the products chemicals their usage their impact on variation of proportion aggregated information This problem statement was acknowledged with the visualizations which was built to work on Hadoop environment on hive tables Responsibilities Worked on a live 50 nodes Hadoop cluster running on Horton works Hadoop Developed visualizations using Tableau and hive tables which is used for data analysis by data scientists Created custom UDAFs using java and used in MRD process ToolsTechnologies Hive unix scripts sqoop Tableau Horton works Hadoop Hadoop Lead Developer Infosys Durham NC March 2015 to December 2015 Client Information Fidelity Investments is an American multinational financial services corporation Business Problem Fidelitys business users wanted to have aggregated information over a period of time on millions of IPs at one go This problem statement was acknowledged with the Model Ready Data MRD process which was built to work on Hadoop environment on hive tables Responsibilities Worked on a live 87 nodes Hadoop cluster running on cloudera Hadoop Developed Model Ready Data Process using hive queries Java and Unix Scripts which is used for modeling and scoring purpose by business Created custom UDAFs using java and used in MRD process Unit and Integration testing of the MRD process using hive queries Did unit and integration testing of Monthly MID to IP and ABT process which are basically monthly and daily aggregations of data using ControlM jobs ToolsTechnologies Hive unix scripts ControlM cloudera Hadoop 54 Hadoop Lead Developer Infosys Durham NC December 2014 to March 2015 Client Information Fidelity Investments is an American multinational financial services corporation Business Problem To produce meaningful and consumable key insights into our clients actions and behaviors in order to have a more comprehensive and holistic understanding therefore increasing productivity of the FFAS business and ultimately driving sales Responsibilities Requirements gathering and clarification from clients Coordinating with offshore team to explain and crossverify the requirements Solving technical challenges faced by offshore team in deployingrunning the use cases Developed Reports using Tableau and hiveimpala Worked on establishing the environment by installing hadoop components needed for use cases Code Review and deployment Showcasing the use cases in business meetings to the clients ToolsTechnologies Map Reduce Hive Impala Tableau NLTK Oozie sqoop Client TMobile Washington US Hadoop Developer Infosys July 2013 to October 2014 Business Problem TMobile client was interested in developing new features functionalities and enhancements in current eservices using Hadoop architecture in cloud environment The objective is to enhance TMobiles customer experience performance scalability and provide improved eService Platform agility Responsibilities Worked on architecture design to replace 23 components built in java by storm component Designed and developed restful web services to load and fetch data tofrom HBase Wrote Map Reduce programs to read huge log files and push the data into Hive Explored few components like Apache Kafka Apache Flume and Nagios to check if it suffices the requirements Administered the cluster of 10 nodes and made changes to addremove nodes and HDFS directory Configured added in extra node zoo keeper to check the consistency ToolsTechnologies Map Reduce Hive HBase Restful Web Services Apache Storm Apache kafka Apache Flume Nagios Zookeeper Big Data Developer Infosys Oxford July 2012 to June 2013 Business Problem The project represents the first enterprise set of foundation capabilities to enable unified access to any digital information in any format structured and unstructured in any information store The program makes the set of federated content stores look and act like a single system The user need not be aware of where the data is stored Responsibilities Involved in sprint planning and estimation Involved in development of all the modules using Spring Framework and amazon web services to createupdatedelete the data in DynamoDB Developed unit and integrated test cases using junit and grizzly Created sample test cases for the testing team using Selenium testing Developed JSON XML data adapter for transforming data sets Developed data adapter for Dynamo Db and S3 file system Experience on loading and transforming of large sets of structured semi structured and Unstructured data Responsible to manage data coming from different sources and application Design develop UI screen for clients to interact with system ToolsTechnologies Java Java Script JQuery CSS3 HTML AWS S3 file system Dynamo DB EC2 and Spring Framework Technology Lead Language CoreAdvanced Java April 2012 to June 2012 ICA and AH are among northern Europes leading retail companies They both use common java based application which is deployed on WAS61 server They are planning to upgrade this server to IBM WAS8 We are doing POC on two modules namely EMS buying and DCR for this upgrade Worked on building Java based tool to find incompatibilities in java based application Worked on building UNIX scripts for profiling of WAS server deployed on unix environment deploying EAR application on WAS etc Mentored a team of 6 members Research work on finding incompatibilities among different versions of J2EE technologies Technology Analyst Infosys Diageo UK China July 2011 to April 2012 Diageo is worlds leading premium drinks business Project included building webservices to support CRUD operations on the live Data Prepared High level and low level documents Developed web services to support CRUD operations on live data Unit tested the web services using JUnit Integration testing using JMeter Technology Analyst Infosys Istanbul TR January 2011 to June 2011 AKBank was an integration project where client wanted to replace their legacy systems with Finacle Infosys product using OSB oracle service bus as middleware Did detailed design of the services Developed services Resolved technical issues in integration testing Did unit and integration testing with multiple teams Coordination with other teams in SIT Associate cognizant Credit Suisse Zurich CH December 2009 to December 2010 Involved in Technical analysis of all the use cases of the project CRM for RMIC Customer Relationship Management for Relationship Managers of Institutional Clients Developed use cases using OBP and MCP frameworks defined by Credit Suisse Responsible for unit and integration testing Senior Developer Argusoft MedicAlert Foundation Turlock April 2009 to November 2009 Part of Analysis and designing of UI component development using JSF framework Involved in tools and technology selection for the UI development Development of all JSF pages Developing ANT scripts for WAR generation Integration responsibility with other layers in architecture Worked on Web Methods layer in the project Involved in the integration and unit testing of the modules Technologies used are JDK 1516 J2EE JSF XML SchemaXSDXSLT WSDL Net Beans as an IDE Web Methods SOA Suite Developer Designer Blaze J2ee Patterns TILES JSTL Oracle as a Database Toad Sql Developer Konakart MSVisio Senior Developer Argusoft District Health Information System Government of Gujarat November 2008 to April 2009 Involved in each and every phase of Spring MVC Web Architecture ie Hibernate Managed Bean Creation Domain Model Layer DAO Layer Service Layer Web Layer UI Layer Responsible for integrating web layer with business layers Involved in Report Creations and its designing using Ireport Technologies involved JAVAJ2EE JSP Spring WS XML SchemaXSDXSLT XBRL Net Beans as an IDE Oracle as a Database Glassfish App Server Toad MSVisio Windows XP Windows 6 mobile Java Developer Argusoft Enterprise class Investment Financial management system KW June 2006 to October 2008 Involved in Report creations and its designing using Business Intelligence Reporting Tool BIRT Hands on creating many services in Financial Accounting and Asset Management modules Developed JSPs for almost all modules Involved in integration and unit testing of modules Education BE in Computer Engineering Atmiya Institute of Technology Science Saurashtra University 2006 Skills Ejb J2ee Java Hibernate Spring Jboss jquery Jsp Struts Git Hadoop Hbase Hive Javascript Jenkins Orm Pig Python Reporting tools Svn",
    "unique_id": "508ae2bc-5bb3-4d5e-b0bf-e21825b40da3"
}