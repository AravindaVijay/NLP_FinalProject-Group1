{
    "clean_data": "ETLApplication Developer ETLApplication Developer business intelligence specialistApplication Developer Remarkable Technology Group Washington DC 11 years of IT experience with 6 years in the ETL developing testing deployment maintenance and production support and database administration Proficiency utilizing ETL tool Informatica Power Center 918x7x for developing the Datawarehouse loads with work experience focused in Data Integration as per client requirement Expertise in designing confirmed and traditional ETL Architecture involving Source databases Oracle Flat Files fixed width delimited DB2 SQL server and Target databases Oracle Teradata and Flat Files fixed width delimited MySQL Expertise in Extraction Transformation and Loading ETL process Dimensional Data Modeling experience using Data modeling Star schemaSnowflake modeling Fact and Dimensions tables dimensional multidimensional modeling and Denormalization techniques Thorough understanding of Kimball and Inmon methodologies Created ETL mappings Mapplets reusable transformations sessions workflows worked on performance tuning of ETL mappings and debugged Experience in creating pivot tables for reporting Expert in data visualization using tools such as tableau ssrs microstratgegy and excel Knowledge of medical coding systems ie ICD9 CPT HCPCS ACGs is desirable Expertise in using Tableau to develop reports and data visualization Expertise in using DB DESIGNER and ERWIN in creating database tables with entity relationships Expertise in loading data into operating data storage Experience in using Sql Server integration and visual studio tools Signifiant exprience with data quality data profiling metadata management and reporting Exprience in working with customer development team to ingest approximately 175 datasets of varying types Exprience in Performing extraction transformation and load of data sets in differing formats Exprience in the enhancement of reports for dissemination or offline viewing Exprience design enhancements to the data base schema dictionary and normalize data into a consistent schema to accept new data types Experience in writing SQL queriesscripts for both source and target databases Experience in Production Validation Testing and Source to Target Testing Validation Testingusing DVOINFORFMATICA VALIDATION OPTION Experience in developing database schemas and data marts Experience in using RESTSOAP API Experience in using TFS for versioning Experience in creating Test cases Experience in loading and defining data from and into Amazon Redshift and S3 buckets Experience in ingesting data from API sources to ETL process Experience in using ETL in SLDC environment Experience in loading data into Datawarehouse and modeling Work Experience ETLApplication Developer Remarkable Technology Group May 2014 to Present Collected and analyzed data obtained from subscribers audit the data to ensure compliance with the required Data Requirements Standards to write a comparison analysis report on the extracted data for both internal and external use Coordinated data gathering analysis for marketing teams from multiple internal and external sources to improve marketing strategies Designed queries for monthly and quarterly reports for marketing and senior management Extensively used Excel VBA functions in development Focusing on readwrite integration to databases Extracted transformed and analyzed data from data warehouse generated reports identified trends gaps in data quality and provided vital information to address root causes Supported Clients Services Teams and improved sales by producing adhoc analysis of key trends in the financial services sector Created and documented findings from analyzed data in report and presentation formats for proper data visualization Manipulated cleansed processed data using Excel Access and SQL while maintaining quality control of data at the point of collection and entry Worked closely with the clients clarifying the data requirements to ensure that data captured by the clients are accurate and to minimize data discrepancies Processed generated and produced regular and adhoc reports with Data unit head Performed Detailed Data Analysis DDA and Data Quality Analysis DQA on source data Used Tableau to get reports and populate on live portal to check for data compliance Define connections for Talend metadata DATABASE IN USE IS MYSQL Create Mappings in Talend open studio using tMaptJoin to fulfill business requirements Explore and access source files to know what am trying to migrate Talend is a code generation tool define dtPrejob to both connection component Amazon redshift and DB mysql Using tMysqlTablelist to specify what tables to migrate Use tMysqlglobal variable Configure tMysqlglobal variable so that I can pull columns from tables being gathered by MysqlTablelist Add a tFixedFlowInput to generate the tableName and columnName columns Adding a tLogRow after the fixed flow allows seeing the names of the tables and columns that the job is pulling from by displaying the information on the run console Set schema to be dynamic by using  component of Talend open studio to set the schema based on the value Colunmname to avoid having to move tables individually After above processes have been completed because the data is on premises need an input component that reads from databaseMYSQLtMysqlInput to make sure data is being pulled Write out data into CSV file by using tFileOutputeliminited Files are then moved out to Amazon S3 using tFileList and TS3Put assuming S3 instance this name into specific folder Using tPostJob tMysqlClose and tS3Close I can control what happens after the loop is complied Schedule or run the job Once data is moved to Amazon S3 importcopy directly to Amazon Redshift ETL Engineer Remarkable Technology Group Buffalo NY April 2018 to March 2019 Experience in using Informatica Client Tools and Talend open studio Developed various mappings using different transformations like Source Qualifier Expression Filter Joiner Router Union Unconnected Connected Lookups and Aggregator Closely worked with other IT team members business partners data stewards stakeholders steering committee members and executive sponsors for all MDM and Data governance related activities Experience in creating High Level Design and Detailed Design in the Design phase Experience in integration of various data sources like Oracle DB2 MS SQL Server and Flat Files with different delineation Experience in packages to extract transform and load data ETL using SSIS designed packages which are utilized for tasks and transformations Data Conversion and Pivot tables Extracted Data from multiple operational sources of loading staging area Data Warehouse and data marts using CDC SCD Type1Type2 loads Experienced in creating Reusable Tasks Sessions Command Email and NonReusable Tasks Decision Event Wait Event Raise Timer Assignment Worklet Control Worked with multiple Informatica repositories version control deployment groups mapping and folder migrations Fixed the invalid mappings and troubleshoot the technical problems of the power center Coordinated with various team members across the globe ie Application teams Business Analysts Users DBA and Infrastructure team to resolve any technical and functional issues in UAT and PROD Created various technical documents required for the knowledge transition of the application which includes reusable objects Informatica UNIX Responsible for gathering requirements for developing mapping documents for OFSSA application and also populating required fields for OFSSA Responsible for mapping MT mortgage data on a daily and weekly basis Define sources and target using Talend open studio for Talend projects Responsible for updating dimensional table from data warehouse environment using Talend created mappings in Talend using tMap tJoin tReplicate tParallelize Developed shell scripts in Unix environment to support scheduling of the Talend jobs Worked on Talend components like tReplace tmap tsort and tFilterColumn tFilterRow Experienced in Exporting and importing Talend jobs Performed data migration from Mysql to Amazon RedshiftS3 bucket using Talend Open Studio Performed unit testing on populated fields Responsible for analyzing mapping document and understanding business requirements Experienced in creating data in staging layer cleansing landing and enterprise Proficient in documenting mapping documents and updating them Responsible for data modelling using DB DESIGNER 4 Responsible for exporting newly created database table design into the database ETL Developer Communicare Healthcare services Washington DC January 2013 to April 2018 Converted the business requirements into technical specifications for ETL process Design source definitions and target definitions based on requirements Creating Informatica Mappings and workflows to migrate data from one environment to another and data enhancements Involved in optimization of sql queries Created New Staging tables in Staging DB to store data from client files CMS MD Medicaid Files etc Parsing highlevel design specs to simple ETL coding and mapping standards AnalysisPreparation of High Level Design HLD and preparation of Low level design LLD Populated the Fact and Dim Table of data warehouse from various sources like Oracle Ms Access and Flat files Created staging tables to do validations against data before loading data into original fact and dim Tables Extensively used Update Strategy and Lookup transformations for updating the target tables and also extensively used the Debugger to check the data flow and the mappings were modified depending on the requirements Handled alerting mechanisms system utilization issues performance statistics capacity planning population and maintenance Designed and developed complex Aggregator Joiner Router Lookup and Update strategy transformation rules Created sessions database connections and worklets using Informatica Power center Involved in performance tuning by identifying bottlenecks eliminating them and tuning the PLSQL used in Transformations Used Presession and Postsession procedures to Drop and recreate indexes while the session runs to improve performance while loading to the target Created session partitions to increase the performance of Informatica Server Manager Responsible for data and table modelling using data modeler tools such as VBA and ERWIN Involved in writing UNIX shell scripts for Informatica ETL tool to run the Sessions Automated the daily scheduling of ETL process for populating the data warehouse Scheduled the ETL jobs daily weekly and monthly based on the business requirement Performing ReviewChecks for components Design Codes and Quality as per Informatica Standards Performs data cleansing and Data quality Environment Informatica Power Center 911 PLSQL and SQL Developer InformaticaAutosys MYSQL Database Administrator Communicare Health Care Services January 2011 to December 2013 Pioneered the development and implementation of MySQL databases Provided Development test and production databases DBA support and everyday maintenance Proficient in Standard query languages DML DDL DCL Stored Procedures triggers Experienced installing and configuring MySQL databases with proven best practice configurations Comfortable with mycnf parameter file customizations Expertise in InnoDB storage engines Comfortable with InnoDB storage engines and migration from one to the other as best needed Performed 55 to 56 upgrade and patching of MySQL database software Knowledge in Bash shell and VB scripting for task automation and very comfortable navigating and configuring UNIX environment Comfortable with Performance tuning Ability to identify performance issues Query optimization Indexing Partitioning Explain plan and rectifying performance issues in the most efficient way to maximum performance Utilizing Performance Schema views slowquery log Percona tools pt query digest ptosc pttablechecksum etc Table maintenance Fragmentation error check table repair table Strategized Backup for maximum uptime and efficiency using tools like xtrabackup mydumpyer mysqldump Tracked Issues and provided troubleshooting as needed in a timely manner Using BMC ticketing system and automated tracking system like MONyog alerts Zabbix Grafana Replication Recovery organizations best practices MasterSlave MasterMaster Cloned and set up new environments Provided Security management UsersGrants privileges database remediation based on NIST standards Education Bachelor of Science Ogun State University Nigeria",
    "entities": [
        "Data Quality Analysis",
        "Performed 55 to",
        "Informatica",
        "Informatica Power Center 918x7x",
        "BMC",
        "UNIX",
        "Fact and Dimensions",
        "Exprience",
        "MysqlTablelist Add",
        "Provided Development",
        "Define",
        "Query",
        "Provided Security",
        "ERWIN Involved",
        "Datawarehouse",
        "Inmon",
        "Amazon S3",
        "Amazon",
        "UAT",
        "Mysql",
        "Target",
        "Standard",
        "SSIS",
        "Data Warehouse",
        "Bash",
        "Amazon Redshift",
        "tMap",
        "SQL Developer InformaticaAutosys MYSQL Database Administrator Communicare Health Care Services",
        "Data modeling Star",
        "Processed",
        "Update",
        "ETL Architecture involving Source",
        "Washington DC",
        "Talend",
        "Kimball",
        "tMysqlglobal",
        "Percona",
        "Informatica Server Manager Responsible",
        "ERWIN",
        "Sql Server",
        "NonReusable Tasks Decision Event Wait Event Raise",
        "Strategized Backup",
        "UsersGrants",
        "CSV",
        "API",
        "Created",
        "Reusable Tasks Sessions Command",
        "Coordinated",
        "Created New Staging",
        "Created ETL",
        "Data Integration",
        "Informatica Power center Involved",
        "PROD Created",
        "SQL",
        "Design Codes and Quality",
        "Data Requirements Standards",
        "Amazon Redshift ETL Engineer Remarkable Technology Group Buffalo",
        "tReplace",
        "Oracle Flat Files",
        "Update Strategy and Lookup",
        "Informatica Client Tools",
        "MDM",
        "Oracle Ms Access and Flat",
        "ETL",
        "Dimensional Data Modeling",
        "Performed Detailed Data Analysis DDA",
        "MONyog",
        "Performed",
        "Informatica UNIX Responsible",
        "ETLApplication Developer ETLApplication Developer",
        "CDC",
        "Oracle DB2 MS SQL Server",
        "Expertise",
        "CMS",
        "Data Conversion",
        "VBA",
        "Supported Clients Services Teams",
        "Data",
        "the Sessions Automated",
        "Drop",
        "Work Experience ETLApplication Developer Remarkable Technology Group",
        "Tableau",
        "Denormalization",
        "ETL Developer Communicare Healthcare",
        "Files"
    ],
    "experience": "Experience in creating pivot tables for reporting Expert in data visualization using tools such as tableau ssrs microstratgegy and excel Knowledge of medical coding systems ie ICD9 CPT HCPCS ACGs is desirable Expertise in using Tableau to develop reports and data visualization Expertise in using DB DESIGNER and ERWIN in creating database tables with entity relationships Expertise in loading data into operating data storage Experience in using Sql Server integration and visual studio tools Signifiant exprience with data quality data profiling metadata management and reporting Exprience in working with customer development team to ingest approximately 175 datasets of varying types Exprience in Performing extraction transformation and load of data sets in differing formats Exprience in the enhancement of reports for dissemination or offline viewing Exprience design enhancements to the data base schema dictionary and normalize data into a consistent schema to accept new data types Experience in writing SQL queriesscripts for both source and target databases Experience in Production Validation Testing and Source to Target Testing Validation Testingusing DVOINFORFMATICA VALIDATION OPTION Experience in developing database schemas and data marts Experience in using RESTSOAP API Experience in using TFS for versioning Experience in creating Test cases Experience in loading and defining data from and into Amazon Redshift and S3 buckets Experience in ingesting data from API sources to ETL process Experience in using ETL in SLDC environment Experience in loading data into Datawarehouse and modeling Work Experience ETLApplication Developer Remarkable Technology Group May 2014 to Present Collected and analyzed data obtained from subscribers audit the data to ensure compliance with the required Data Requirements Standards to write a comparison analysis report on the extracted data for both internal and external use Coordinated data gathering analysis for marketing teams from multiple internal and external sources to improve marketing strategies Designed queries for monthly and quarterly reports for marketing and senior management Extensively used Excel VBA functions in development Focusing on readwrite integration to databases Extracted transformed and analyzed data from data warehouse generated reports identified trends gaps in data quality and provided vital information to address root causes Supported Clients Services Teams and improved sales by producing adhoc analysis of key trends in the financial services sector Created and documented findings from analyzed data in report and presentation formats for proper data visualization Manipulated cleansed processed data using Excel Access and SQL while maintaining quality control of data at the point of collection and entry Worked closely with the clients clarifying the data requirements to ensure that data captured by the clients are accurate and to minimize data discrepancies Processed generated and produced regular and adhoc reports with Data unit head Performed Detailed Data Analysis DDA and Data Quality Analysis DQA on source data Used Tableau to get reports and populate on live portal to check for data compliance Define connections for Talend metadata DATABASE IN USE IS MYSQL Create Mappings in Talend open studio using tMaptJoin to fulfill business requirements Explore and access source files to know what am trying to migrate Talend is a code generation tool define dtPrejob to both connection component Amazon redshift and DB mysql Using tMysqlTablelist to specify what tables to migrate Use tMysqlglobal variable Configure tMysqlglobal variable so that I can pull columns from tables being gathered by MysqlTablelist Add a tFixedFlowInput to generate the tableName and columnName columns Adding a tLogRow after the fixed flow allows seeing the names of the tables and columns that the job is pulling from by displaying the information on the run console Set schema to be dynamic by using   component of Talend open studio to set the schema based on the value Colunmname to avoid having to move tables individually After above processes have been completed because the data is on premises need an input component that reads from databaseMYSQLtMysqlInput to make sure data is being pulled Write out data into CSV file by using tFileOutputeliminited Files are then moved out to Amazon S3 using tFileList and TS3Put assuming S3 instance this name into specific folder Using tPostJob tMysqlClose and tS3Close I can control what happens after the loop is complied Schedule or run the job Once data is moved to Amazon S3 importcopy directly to Amazon Redshift ETL Engineer Remarkable Technology Group Buffalo NY April 2018 to March 2019 Experience in using Informatica Client Tools and Talend open studio Developed various mappings using different transformations like Source Qualifier Expression Filter Joiner Router Union Unconnected Connected Lookups and Aggregator Closely worked with other IT team members business partners data stewards stakeholders steering committee members and executive sponsors for all MDM and Data governance related activities Experience in creating High Level Design and Detailed Design in the Design phase Experience in integration of various data sources like Oracle DB2 MS SQL Server and Flat Files with different delineation Experience in packages to extract transform and load data ETL using SSIS designed packages which are utilized for tasks and transformations Data Conversion and Pivot tables Extracted Data from multiple operational sources of loading staging area Data Warehouse and data marts using CDC SCD Type1Type2 loads Experienced in creating Reusable Tasks Sessions Command Email and NonReusable Tasks Decision Event Wait Event Raise Timer Assignment Worklet Control Worked with multiple Informatica repositories version control deployment groups mapping and folder migrations Fixed the invalid mappings and troubleshoot the technical problems of the power center Coordinated with various team members across the globe ie Application teams Business Analysts Users DBA and Infrastructure team to resolve any technical and functional issues in UAT and PROD Created various technical documents required for the knowledge transition of the application which includes reusable objects Informatica UNIX Responsible for gathering requirements for developing mapping documents for OFSSA application and also populating required fields for OFSSA Responsible for mapping MT mortgage data on a daily and weekly basis Define sources and target using Talend open studio for Talend projects Responsible for updating dimensional table from data warehouse environment using Talend created mappings in Talend using tMap tJoin tReplicate tParallelize Developed shell scripts in Unix environment to support scheduling of the Talend jobs Worked on Talend components like tReplace tmap tsort and tFilterColumn tFilterRow Experienced in Exporting and importing Talend jobs Performed data migration from Mysql to Amazon RedshiftS3 bucket using Talend Open Studio Performed unit testing on populated fields Responsible for analyzing mapping document and understanding business requirements Experienced in creating data in staging layer cleansing landing and enterprise Proficient in documenting mapping documents and updating them Responsible for data modelling using DB DESIGNER 4 Responsible for exporting newly created database table design into the database ETL Developer Communicare Healthcare services Washington DC January 2013 to April 2018 Converted the business requirements into technical specifications for ETL process Design source definitions and target definitions based on requirements Creating Informatica Mappings and workflows to migrate data from one environment to another and data enhancements Involved in optimization of sql queries Created New Staging tables in Staging DB to store data from client files CMS MD Medicaid Files etc Parsing highlevel design specs to simple ETL coding and mapping standards AnalysisPreparation of High Level Design HLD and preparation of Low level design LLD Populated the Fact and Dim Table of data warehouse from various sources like Oracle Ms Access and Flat files Created staging tables to do validations against data before loading data into original fact and dim Tables Extensively used Update Strategy and Lookup transformations for updating the target tables and also extensively used the Debugger to check the data flow and the mappings were modified depending on the requirements Handled alerting mechanisms system utilization issues performance statistics capacity planning population and maintenance Designed and developed complex Aggregator Joiner Router Lookup and Update strategy transformation rules Created sessions database connections and worklets using Informatica Power center Involved in performance tuning by identifying bottlenecks eliminating them and tuning the PLSQL used in Transformations Used Presession and Postsession procedures to Drop and recreate indexes while the session runs to improve performance while loading to the target Created session partitions to increase the performance of Informatica Server Manager Responsible for data and table modelling using data modeler tools such as VBA and ERWIN Involved in writing UNIX shell scripts for Informatica ETL tool to run the Sessions Automated the daily scheduling of ETL process for populating the data warehouse Scheduled the ETL jobs daily weekly and monthly based on the business requirement Performing ReviewChecks for components Design Codes and Quality as per Informatica Standards Performs data cleansing and Data quality Environment Informatica Power Center 911 PLSQL and SQL Developer InformaticaAutosys MYSQL Database Administrator Communicare Health Care Services January 2011 to December 2013 Pioneered the development and implementation of MySQL databases Provided Development test and production databases DBA support and everyday maintenance Proficient in Standard query languages DML DDL DCL Stored Procedures triggers Experienced installing and configuring MySQL databases with proven best practice configurations Comfortable with mycnf parameter file customizations Expertise in InnoDB storage engines Comfortable with InnoDB storage engines and migration from one to the other as best needed Performed 55 to 56 upgrade and patching of MySQL database software Knowledge in Bash shell and VB scripting for task automation and very comfortable navigating and configuring UNIX environment Comfortable with Performance tuning Ability to identify performance issues Query optimization Indexing Partitioning Explain plan and rectifying performance issues in the most efficient way to maximum performance Utilizing Performance Schema views slowquery log Percona tools pt query digest ptosc pttablechecksum etc Table maintenance Fragmentation error check table repair table Strategized Backup for maximum uptime and efficiency using tools like xtrabackup mydumpyer mysqldump Tracked Issues and provided troubleshooting as needed in a timely manner Using BMC ticketing system and automated tracking system like MONyog alerts Zabbix Grafana Replication Recovery organizations best practices MasterSlave MasterMaster Cloned and set up new environments Provided Security management UsersGrants privileges database remediation based on NIST standards Education Bachelor of Science Ogun State University Nigeria",
    "extracted_keywords": [
        "ETLApplication",
        "Developer",
        "ETLApplication",
        "Developer",
        "business",
        "intelligence",
        "specialistApplication",
        "Developer",
        "Remarkable",
        "Technology",
        "Group",
        "Washington",
        "DC",
        "years",
        "IT",
        "experience",
        "years",
        "ETL",
        "testing",
        "deployment",
        "maintenance",
        "production",
        "support",
        "database",
        "administration",
        "Proficiency",
        "ETL",
        "tool",
        "Informatica",
        "Power",
        "Center",
        "918x7x",
        "Datawarehouse",
        "loads",
        "work",
        "experience",
        "Data",
        "Integration",
        "client",
        "requirement",
        "Expertise",
        "ETL",
        "Architecture",
        "Source",
        "Oracle",
        "Flat",
        "Files",
        "width",
        "DB2",
        "SQL",
        "server",
        "Target",
        "Oracle",
        "Teradata",
        "Flat",
        "Files",
        "width",
        "MySQL",
        "Expertise",
        "Extraction",
        "Transformation",
        "Loading",
        "ETL",
        "process",
        "Dimensional",
        "Data",
        "Modeling",
        "experience",
        "Data",
        "Star",
        "schemaSnowflake",
        "Fact",
        "Dimensions",
        "modeling",
        "Denormalization",
        "understanding",
        "Kimball",
        "Inmon",
        "methodologies",
        "Created",
        "ETL",
        "mappings",
        "Mapplets",
        "transformations",
        "sessions",
        "workflows",
        "performance",
        "tuning",
        "ETL",
        "mappings",
        "Experience",
        "tables",
        "Expert",
        "data",
        "visualization",
        "tools",
        "tableau",
        "ssrs",
        "microstratgegy",
        "Knowledge",
        "systems",
        "ICD9",
        "CPT",
        "HCPCS",
        "ACGs",
        "Expertise",
        "Tableau",
        "reports",
        "data",
        "visualization",
        "Expertise",
        "DB",
        "DESIGNER",
        "ERWIN",
        "database",
        "tables",
        "entity",
        "relationships",
        "Expertise",
        "loading",
        "data",
        "data",
        "storage",
        "Experience",
        "Sql",
        "Server",
        "integration",
        "studio",
        "tools",
        "Signifiant",
        "exprience",
        "data",
        "quality",
        "data",
        "metadata",
        "management",
        "Exprience",
        "customer",
        "development",
        "team",
        "datasets",
        "types",
        "Exprience",
        "extraction",
        "transformation",
        "load",
        "data",
        "sets",
        "formats",
        "Exprience",
        "enhancement",
        "reports",
        "dissemination",
        "Exprience",
        "design",
        "enhancements",
        "data",
        "base",
        "schema",
        "dictionary",
        "data",
        "schema",
        "data",
        "types",
        "Experience",
        "SQL",
        "queriesscripts",
        "source",
        "target",
        "Experience",
        "Production",
        "Validation",
        "Testing",
        "Source",
        "Target",
        "Testing",
        "Validation",
        "Testingusing",
        "VALIDATION",
        "OPTION",
        "Experience",
        "database",
        "schemas",
        "data",
        "Experience",
        "RESTSOAP",
        "API",
        "Experience",
        "TFS",
        "Experience",
        "Test",
        "cases",
        "Experience",
        "loading",
        "data",
        "Amazon",
        "Redshift",
        "S3",
        "data",
        "API",
        "sources",
        "ETL",
        "process",
        "Experience",
        "ETL",
        "SLDC",
        "environment",
        "Experience",
        "loading",
        "data",
        "Datawarehouse",
        "Work",
        "Experience",
        "ETLApplication",
        "Developer",
        "Remarkable",
        "Technology",
        "Group",
        "May",
        "Present",
        "Collected",
        "data",
        "subscribers",
        "data",
        "compliance",
        "Data",
        "Requirements",
        "Standards",
        "comparison",
        "analysis",
        "report",
        "data",
        "data",
        "analysis",
        "marketing",
        "teams",
        "sources",
        "marketing",
        "strategies",
        "queries",
        "reports",
        "marketing",
        "management",
        "Excel",
        "VBA",
        "functions",
        "development",
        "integration",
        "databases",
        "data",
        "data",
        "warehouse",
        "reports",
        "trends",
        "gaps",
        "data",
        "quality",
        "information",
        "root",
        "Supported",
        "Clients",
        "Services",
        "Teams",
        "sales",
        "analysis",
        "trends",
        "services",
        "sector",
        "findings",
        "data",
        "report",
        "presentation",
        "formats",
        "data",
        "visualization",
        "Manipulated",
        "data",
        "Excel",
        "Access",
        "SQL",
        "quality",
        "control",
        "data",
        "point",
        "collection",
        "entry",
        "clients",
        "data",
        "requirements",
        "data",
        "clients",
        "data",
        "discrepancies",
        "reports",
        "Data",
        "unit",
        "head",
        "Performed",
        "Detailed",
        "Data",
        "Analysis",
        "DDA",
        "Data",
        "Quality",
        "Analysis",
        "DQA",
        "source",
        "data",
        "Tableau",
        "reports",
        "portal",
        "data",
        "compliance",
        "Define",
        "connections",
        "Talend",
        "metadata",
        "DATABASE",
        "USE",
        "MYSQL",
        "Create",
        "Mappings",
        "Talend",
        "studio",
        "tMaptJoin",
        "business",
        "requirements",
        "access",
        "source",
        "files",
        "Talend",
        "code",
        "generation",
        "tool",
        "dtPrejob",
        "connection",
        "component",
        "Amazon",
        "redshift",
        "DB",
        "mysql",
        "tMysqlTablelist",
        "tables",
        "Use",
        "tMysqlglobal",
        "variable",
        "Configure",
        "tMysqlglobal",
        "variable",
        "columns",
        "tables",
        "MysqlTablelist",
        "Add",
        "tFixedFlowInput",
        "columns",
        "tLogRow",
        "flow",
        "names",
        "tables",
        "columns",
        "job",
        "information",
        "console",
        "Set",
        "schema",
        "component",
        "studio",
        "schema",
        "value",
        "Colunmname",
        "tables",
        "processes",
        "data",
        "premises",
        "input",
        "component",
        "databaseMYSQLtMysqlInput",
        "data",
        "Write",
        "data",
        "CSV",
        "file",
        "Files",
        "Amazon",
        "S3",
        "tFileList",
        "TS3Put",
        "S3",
        "instance",
        "name",
        "folder",
        "tPostJob",
        "loop",
        "Schedule",
        "job",
        "data",
        "Amazon",
        "S3",
        "importcopy",
        "Amazon",
        "Redshift",
        "ETL",
        "Engineer",
        "Remarkable",
        "Technology",
        "Group",
        "Buffalo",
        "NY",
        "April",
        "March",
        "Experience",
        "Informatica",
        "Client",
        "Tools",
        "studio",
        "mappings",
        "transformations",
        "Source",
        "Qualifier",
        "Expression",
        "Filter",
        "Joiner",
        "Router",
        "Union",
        "Unconnected",
        "Connected",
        "Lookups",
        "Aggregator",
        "IT",
        "team",
        "members",
        "business",
        "partners",
        "data",
        "stewards",
        "committee",
        "members",
        "executive",
        "sponsors",
        "MDM",
        "Data",
        "governance",
        "activities",
        "Experience",
        "High",
        "Level",
        "Design",
        "Detailed",
        "Design",
        "Design",
        "phase",
        "Experience",
        "integration",
        "data",
        "sources",
        "Oracle",
        "DB2",
        "MS",
        "SQL",
        "Server",
        "Flat",
        "Files",
        "delineation",
        "Experience",
        "packages",
        "transform",
        "data",
        "ETL",
        "SSIS",
        "packages",
        "tasks",
        "transformations",
        "Data",
        "Conversion",
        "Pivot",
        "Data",
        "sources",
        "loading",
        "staging",
        "area",
        "Data",
        "Warehouse",
        "data",
        "marts",
        "CDC",
        "SCD",
        "Type1Type2",
        "Reusable",
        "Tasks",
        "Sessions",
        "Command",
        "Email",
        "NonReusable",
        "Tasks",
        "Decision",
        "Event",
        "Wait",
        "Event",
        "Raise",
        "Timer",
        "Assignment",
        "Worklet",
        "Control",
        "Informatica",
        "repositories",
        "version",
        "control",
        "deployment",
        "groups",
        "mapping",
        "folder",
        "migrations",
        "mappings",
        "problems",
        "power",
        "center",
        "team",
        "members",
        "globe",
        "Application",
        "teams",
        "Business",
        "Analysts",
        "Users",
        "DBA",
        "Infrastructure",
        "team",
        "issues",
        "UAT",
        "PROD",
        "documents",
        "knowledge",
        "transition",
        "application",
        "objects",
        "Informatica",
        "UNIX",
        "Responsible",
        "requirements",
        "mapping",
        "documents",
        "application",
        "fields",
        "OFSSA",
        "Responsible",
        "mapping",
        "MT",
        "mortgage",
        "data",
        "basis",
        "Define",
        "sources",
        "target",
        "Talend",
        "studio",
        "Talend",
        "projects",
        "table",
        "data",
        "warehouse",
        "environment",
        "Talend",
        "mappings",
        "Talend",
        "tMap",
        "tJoin",
        "tReplicate",
        "tParallelize",
        "shell",
        "scripts",
        "Unix",
        "environment",
        "scheduling",
        "Talend",
        "jobs",
        "Talend",
        "components",
        "tReplace",
        "tmap",
        "tsort",
        "tFilterColumn",
        "tFilterRow",
        "Exporting",
        "Talend",
        "jobs",
        "data",
        "migration",
        "Mysql",
        "Amazon",
        "RedshiftS3",
        "bucket",
        "Talend",
        "Open",
        "Studio",
        "Performed",
        "unit",
        "testing",
        "fields",
        "mapping",
        "document",
        "business",
        "requirements",
        "data",
        "layer",
        "cleansing",
        "landing",
        "enterprise",
        "Proficient",
        "mapping",
        "documents",
        "data",
        "modelling",
        "DB",
        "DESIGNER",
        "database",
        "table",
        "design",
        "database",
        "ETL",
        "Developer",
        "Communicare",
        "Healthcare",
        "Washington",
        "DC",
        "January",
        "April",
        "business",
        "requirements",
        "specifications",
        "ETL",
        "process",
        "Design",
        "source",
        "definitions",
        "target",
        "definitions",
        "requirements",
        "Informatica",
        "Mappings",
        "workflows",
        "data",
        "environment",
        "data",
        "enhancements",
        "optimization",
        "sql",
        "queries",
        "New",
        "Staging",
        "tables",
        "Staging",
        "DB",
        "data",
        "client",
        "files",
        "CMS",
        "MD",
        "Medicaid",
        "Files",
        "highlevel",
        "design",
        "specs",
        "ETL",
        "mapping",
        "standards",
        "AnalysisPreparation",
        "High",
        "Level",
        "Design",
        "HLD",
        "preparation",
        "level",
        "design",
        "LLD",
        "Fact",
        "Dim",
        "Table",
        "data",
        "warehouse",
        "sources",
        "Oracle",
        "Ms",
        "Access",
        "files",
        "staging",
        "tables",
        "validations",
        "data",
        "data",
        "fact",
        "Tables",
        "Update",
        "Strategy",
        "Lookup",
        "transformations",
        "target",
        "tables",
        "Debugger",
        "data",
        "flow",
        "mappings",
        "requirements",
        "mechanisms",
        "system",
        "utilization",
        "issues",
        "performance",
        "statistics",
        "capacity",
        "population",
        "maintenance",
        "Aggregator",
        "Joiner",
        "Router",
        "Lookup",
        "Update",
        "strategy",
        "transformation",
        "sessions",
        "database",
        "connections",
        "worklets",
        "Informatica",
        "Power",
        "center",
        "performance",
        "bottlenecks",
        "PLSQL",
        "Transformations",
        "Presession",
        "Postsession",
        "procedures",
        "recreate",
        "indexes",
        "session",
        "performance",
        "target",
        "session",
        "partitions",
        "performance",
        "Informatica",
        "Server",
        "Manager",
        "Responsible",
        "data",
        "table",
        "modelling",
        "data",
        "modeler",
        "tools",
        "VBA",
        "ERWIN",
        "UNIX",
        "shell",
        "scripts",
        "Informatica",
        "ETL",
        "tool",
        "Sessions",
        "Automated",
        "scheduling",
        "ETL",
        "process",
        "data",
        "warehouse",
        "ETL",
        "jobs",
        "business",
        "requirement",
        "ReviewChecks",
        "components",
        "Design",
        "Codes",
        "Quality",
        "Informatica",
        "Standards",
        "data",
        "cleansing",
        "Data",
        "quality",
        "Environment",
        "Informatica",
        "Power",
        "Center",
        "PLSQL",
        "SQL",
        "Developer",
        "InformaticaAutosys",
        "MYSQL",
        "Database",
        "Administrator",
        "Communicare",
        "Health",
        "Care",
        "Services",
        "January",
        "December",
        "development",
        "implementation",
        "MySQL",
        "Development",
        "test",
        "production",
        "DBA",
        "support",
        "maintenance",
        "Proficient",
        "query",
        "DML",
        "DDL",
        "DCL",
        "Stored",
        "Procedures",
        "MySQL",
        "databases",
        "practice",
        "configurations",
        "parameter",
        "file",
        "customizations",
        "Expertise",
        "InnoDB",
        "storage",
        "engines",
        "InnoDB",
        "storage",
        "engines",
        "migration",
        "Performed",
        "upgrade",
        "patching",
        "MySQL",
        "database",
        "software",
        "Knowledge",
        "Bash",
        "shell",
        "VB",
        "task",
        "automation",
        "UNIX",
        "environment",
        "Performance",
        "Ability",
        "performance",
        "issues",
        "Query",
        "optimization",
        "Indexing",
        "Partitioning",
        "Explain",
        "plan",
        "performance",
        "issues",
        "way",
        "performance",
        "Performance",
        "Schema",
        "slowquery",
        "log",
        "Percona",
        "tools",
        "pt",
        "query",
        "digest",
        "ptosc",
        "Table",
        "maintenance",
        "Fragmentation",
        "error",
        "check",
        "table",
        "repair",
        "table",
        "Backup",
        "uptime",
        "efficiency",
        "tools",
        "xtrabackup",
        "mydumpyer",
        "mysqldump",
        "Issues",
        "troubleshooting",
        "manner",
        "BMC",
        "ticketing",
        "system",
        "tracking",
        "system",
        "MONyog",
        "Zabbix",
        "Grafana",
        "Replication",
        "Recovery",
        "organizations",
        "practices",
        "MasterSlave",
        "MasterMaster",
        "Cloned",
        "environments",
        "Security",
        "management",
        "UsersGrants",
        "database",
        "remediation",
        "NIST",
        "standards",
        "Education",
        "Bachelor",
        "Science",
        "Ogun",
        "State",
        "University",
        "Nigeria"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T22:54:50.226146",
    "resume_data": "ETLApplication Developer ETLApplication Developer business intelligence specialistApplication Developer Remarkable Technology Group Washington DC 11 years of IT experience with 6 years in the ETL developing testing deployment maintenance and production support and database administration Proficiency utilizing ETL tool Informatica Power Center 918x7x for developing the Datawarehouse loads with work experience focused in Data Integration as per client requirement Expertise in designing confirmed and traditional ETL Architecture involving Source databases Oracle Flat Files fixed width delimited DB2 SQL server and Target databases Oracle Teradata and Flat Files fixed width delimited MySQL Expertise in Extraction Transformation and Loading ETL process Dimensional Data Modeling experience using Data modeling Star schemaSnowflake modeling Fact and Dimensions tables dimensional multidimensional modeling and Denormalization techniques Thorough understanding of Kimball and Inmon methodologies Created ETL mappings Mapplets reusable transformations sessions workflows worked on performance tuning of ETL mappings and debugged Experience in creating pivot tables for reporting Expert in data visualization using tools such as tableau ssrs microstratgegy and excel Knowledge of medical coding systems ie ICD9 CPT HCPCS ACGs is desirable Expertise in using Tableau to develop reports and data visualization Expertise in using DB DESIGNER and ERWIN in creating database tables with entity relationships Expertise in loading data into operating data storage Experience in using Sql Server integration and visual studio tools Signifiant exprience with data quality data profiling metadata management and reporting Exprience in working with customer development team to ingest approximately 175 datasets of varying types Exprience in Performing extraction transformation and load of data sets in differing formats Exprience in the enhancement of reports for dissemination or offline viewing Exprience design enhancements to the data base schema dictionary and normalize data into a consistent schema to accept new data types Experience in writing SQL queriesscripts for both source and target databases Experience in Production Validation Testing and Source to Target Testing Validation Testingusing DVOINFORFMATICA VALIDATION OPTION Experience in developing database schemas and data marts Experience in using RESTSOAP API Experience in using TFS for versioning Experience in creating Test cases Experience in loading and defining data from and into Amazon Redshift and S3 buckets Experience in ingesting data from API sources to ETL process Experience in using ETL in SLDC environment Experience in loading data into Datawarehouse and modeling Work Experience ETLApplication Developer Remarkable Technology Group May 2014 to Present Collected and analyzed data obtained from subscribers audit the data to ensure compliance with the required Data Requirements Standards to write a comparison analysis report on the extracted data for both internal and external use Coordinated data gathering analysis for marketing teams from multiple internal and external sources to improve marketing strategies Designed queries for monthly and quarterly reports for marketing and senior management Extensively used Excel VBA functions in development Focusing on readwrite integration to databases Extracted transformed and analyzed data from data warehouse generated reports identified trends gaps in data quality and provided vital information to address root causes Supported Clients Services Teams and improved sales by producing adhoc analysis of key trends in the financial services sector Created and documented findings from analyzed data in report and presentation formats for proper data visualization Manipulated cleansed processed data using Excel Access and SQL while maintaining quality control of data at the point of collection and entry Worked closely with the clients clarifying the data requirements to ensure that data captured by the clients are accurate and to minimize data discrepancies Processed generated and produced regular and adhoc reports with Data unit head Performed Detailed Data Analysis DDA and Data Quality Analysis DQA on source data Used Tableau to get reports and populate on live portal to check for data compliance Define connections for Talend metadata DATABASE IN USE IS MYSQL Create Mappings in Talend open studio using tMaptJoin to fulfill business requirements Explore and access source files to know what am trying to migrate Talend is a code generation tool define dtPrejob to both connection component Amazon redshift and DB mysql Using tMysqlTablelist to specify what tables to migrate Use tMysqlglobal variable Configure tMysqlglobal variable so that I can pull columns from tables being gathered by MysqlTablelist Add a tFixedFlowInput to generate the tableName and columnName columns Adding a tLogRow after the fixed flow allows seeing the names of the tables and columns that the job is pulling from by displaying the information on the run console Set schema to be dynamic by using tSetDynamicSchema component of Talend open studio to set the schema based on the value Colunmname to avoid having to move tables individually After above processes have been completed because the data is on premises need an input component that reads from databaseMYSQLtMysqlInput to make sure data is being pulled Write out data into CSV file by using tFileOutputeliminited Files are then moved out to Amazon S3 using tFileList and TS3Put assuming S3 instance this name into specific folder Using tPostJob tMysqlClose and tS3Close I can control what happens after the loop is complied Schedule or run the job Once data is moved to Amazon S3 importcopy directly to Amazon Redshift ETL Engineer Remarkable Technology Group Buffalo NY April 2018 to March 2019 Experience in using Informatica Client Tools and Talend open studio Developed various mappings using different transformations like Source Qualifier Expression Filter Joiner Router Union Unconnected Connected Lookups and Aggregator Closely worked with other IT team members business partners data stewards stakeholders steering committee members and executive sponsors for all MDM and Data governance related activities Experience in creating High Level Design and Detailed Design in the Design phase Experience in integration of various data sources like Oracle DB2 MS SQL Server and Flat Files with different delineation Experience in packages to extract transform and load data ETL using SSIS designed packages which are utilized for tasks and transformations Data Conversion and Pivot tables Extracted Data from multiple operational sources of loading staging area Data Warehouse and data marts using CDC SCD Type1Type2 loads Experienced in creating Reusable Tasks Sessions Command Email and NonReusable Tasks Decision Event Wait Event Raise Timer Assignment Worklet Control Worked with multiple Informatica repositories version control deployment groups mapping and folder migrations Fixed the invalid mappings and troubleshoot the technical problems of the power center Coordinated with various team members across the globe ie Application teams Business Analysts Users DBA and Infrastructure team to resolve any technical and functional issues in UAT and PROD Created various technical documents required for the knowledge transition of the application which includes reusable objects Informatica UNIX Responsible for gathering requirements for developing mapping documents for OFSSA application and also populating required fields for OFSSA Responsible for mapping MT mortgage data on a daily and weekly basis Define sources and target using Talend open studio for Talend projects Responsible for updating dimensional table from data warehouse environment using Talend created mappings in Talend using tMap tJoin tReplicate tParallelize Developed shell scripts in Unix environment to support scheduling of the Talend jobs Worked on Talend components like tReplace tmap tsort and tFilterColumn tFilterRow Experienced in Exporting and importing Talend jobs Performed data migration from Mysql to Amazon RedshiftS3 bucket using Talend Open Studio Performed unit testing on populated fields Responsible for analyzing mapping document and understanding business requirements Experienced in creating data in staging layer cleansing landing and enterprise Proficient in documenting mapping documents and updating them Responsible for data modelling using DB DESIGNER 4 Responsible for exporting newly created database table design into the database ETL Developer Communicare Healthcare services Washington DC January 2013 to April 2018 Converted the business requirements into technical specifications for ETL process Design source definitions and target definitions based on requirements Creating Informatica Mappings and workflows to migrate data from one environment to another and data enhancements Involved in optimization of sql queries Created New Staging tables in Staging DB to store data from client files CMS MD Medicaid Files etc Parsing highlevel design specs to simple ETL coding and mapping standards AnalysisPreparation of High Level Design HLD and preparation of Low level design LLD Populated the Fact and Dim Table of data warehouse from various sources like Oracle Ms Access and Flat files Created staging tables to do validations against data before loading data into original fact and dim Tables Extensively used Update Strategy and Lookup transformations for updating the target tables and also extensively used the Debugger to check the data flow and the mappings were modified depending on the requirements Handled alerting mechanisms system utilization issues performance statistics capacity planning population and maintenance Designed and developed complex Aggregator Joiner Router Lookup and Update strategy transformation rules Created sessions database connections and worklets using Informatica Power center Involved in performance tuning by identifying bottlenecks eliminating them and tuning the PLSQL used in Transformations Used Presession and Postsession procedures to Drop and recreate indexes while the session runs to improve performance while loading to the target Created session partitions to increase the performance of Informatica Server Manager Responsible for data and table modelling using data modeler tools such as VBA and ERWIN Involved in writing UNIX shell scripts for Informatica ETL tool to run the Sessions Automated the daily scheduling of ETL process for populating the data warehouse Scheduled the ETL jobs daily weekly and monthly based on the business requirement Performing ReviewChecks for components Design Codes and Quality as per Informatica Standards Performs data cleansing and Data quality Environment Informatica Power Center 911 PLSQL and SQL Developer InformaticaAutosys MYSQL Database Administrator Communicare Health Care Services January 2011 to December 2013 Pioneered the development and implementation of MySQL databases Provided Development test and production databases DBA support and everyday maintenance Proficient in Standard query languages DML DDL DCL Stored Procedures triggers Experienced installing and configuring MySQL databases with proven best practice configurations Comfortable with mycnf parameter file customizations Expertise in InnoDB storage engines Comfortable with InnoDB storage engines and migration from one to the other as best needed Performed 55 to 56 upgrade and patching of MySQL database software Knowledge in Bash shell and VB scripting for task automation and very comfortable navigating and configuring UNIX environment Comfortable with Performance tuning Ability to identify performance issues Query optimization Indexing Partitioning Explain plan and rectifying performance issues in the most efficient way to maximum performance Utilizing Performance Schema views slowquery log Percona tools pt query digest ptosc pttablechecksum etc Table maintenance Fragmentation error check table repair table Strategized Backup for maximum uptime and efficiency using tools like xtrabackup mydumpyer mysqldump Tracked Issues and provided troubleshooting as needed in a timely manner Using BMC ticketing system and automated tracking system like MONyog alerts Zabbix Grafana Replication Recovery organizations best practices MasterSlave MasterMaster Cloned and set up new environments Provided Security management UsersGrants privileges database remediation based on NIST standards Education Bachelor of Science Ogun State University Nigeria",
    "unique_id": "988026df-9a99-47f4-b345-ab623dbda8dd"
}