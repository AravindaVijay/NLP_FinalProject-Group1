{
    "clean_data": "Hadoop Developer Hadoop span lDeveloperspan Hadoop Developer Vaizva Inc Alpharetta GA Around 4 years of IT experience in Big Data Analysis Design Development Implementation and Testing of various standalone and clientserver architecture based enterprise application software in Python on various domains Proven organizational time management and multitasking skills and ability to work independently and quickly learn new technology and adopt to new environment Expert knowledge experience in Object Oriented Design and Programming concepts Strong experience in using Pythons application development frameworks such as Django Experience in realtime configuration and management of MySQL MongoDB and PostgreSQL databases on a large scale Experience on several python packages like Numpy Beautiful Soup Pickle PySide Scipy and PyTables Experience in developing Web Services with Python programming language Familiar with JSON based REST Web services and Amazon Web services Good experience in writing SQL queries and implementing stored procedures functions packages tables views cursors triggers Significant application development experience in Ubuntu RedHat and Windows environments Extensive experience on developmental tools such as Eclipse PyCharm and Sublime Text version control tools like Git and Apache SVN Experience in testing and debugging applications using PyUnit PyTest and JUnit frameworks Experience in working with AWS Amazon S3 Amazon EC2 and Relational Database Services Have experience in Objectoriented programming multithreading algorithms data structures and system programming Proficient in Python experience building and product ionizing endtoend systems Knowledge of Information Extraction NLP algorithms coupled with Deep Learning Experience with file systems server architectures databases SQL and data movement ETL Excellent understanding of Hadoop architecture and various components such as HDFS Job Tracker Task Tracker Name Node Data Node YARN Spark and MapReduce programming paradigm Experience in importing and exporting data using Sqoop from Relational Database to HDFS and from HDFS to Relational Database Experience in continuous build and version control systems like Jira Git and SVN Setting up python REST API Frame work using Django Hands on experience in data quality data organization metadata and data profiling and large Data set sizes Ability to move data between production systems and work on crossmultiple platforms Experience in writing Sub Queries Stored Procedures Triggers Cursors and Functions on MySQL and PostgreSQL database Good experience in handling errorsexceptions and debugging the issues in large scale applications Good Analytical and ProblemSolving skills and ability to work on own besides being a valuable and contributing team player Exceptional problem solving and decisionmaking capabilities recognized by associates for quality of data in alternative solutions Wellversed with Agile Development process tools like Jira Practical experience with working on MultipleEnvironments like Development Testing Production Excellent Interpersonal and Communication skills Efficient Time Management and Organization Skills ability to handle MultipleTasks and work well in a TeamEnvironment Work Experience Hadoop Developer Vaizva Inc Alpharetta GA January 2018 to Present PTT serves at the onestop solution for logistics business It is a coherently designed integrated solution with planning scheduling realtime tracking and key performance indicators with rich builtin analytics And as there is no middleware software involved users can significantly reduce their IT costs Responsibilities Experience in writing Sqoop scripts to import and export data from RDBMS into HDFS HIVE and handled incremental loading on the customer and transaction information data dynamically Involved in creating Hive tables loading with data and writing hive queries that will run internally in map reduce way Developed Spark code using Scala and Spark SQLStreaming for faster testing and processing of data Skilled experience in installing configuring and using Apache Hadoop ecosystems such as Pig and Spark Estimated the Software Hardware requirements for the Name Node and Data Node in the cluster Extracted the needed data from the server into HDFS and Bulk Loaded the cleaned data into HBase using MapReduce Develop HIVE queries for the analysts Created an email notification service upon completion of job for the particular team which requested for the data Migrate on inhouse database to AWS Cloud and also designed built and deployed a multitude of applications utilizing the AWS stack Including EC2 RDS by focusing on highavailability and autoscaling Involved in support for Amazon AWS and RDS to host staticmedia files and the database into Amazon Cloud Microservice architecture development using Python and Docker on an Ubuntu Linux platform using HTTPREST interfaces with deployment into a multinode Kubernetes environment Performed Test Driven Development TDD and continuous integration to keep in line with Agile Software Methodology Worked in development of applications especially in LINUX environment and familiar with the commands All projects adhered to agile principles to deliver commitments meet acceptance criteria and ensure fast releases Environment Python Hortonworks Spark Oozie Hive HBase Linux MapReduce Scala Sqoop GitHub AWS Linux Shell Scripting RPA Developer The Vanguard Group Charlotte NC August 2017 to December 2017 Retails Systems focuses on automating the manual process of creating Report by extracting data from Web Database and Main Frame Application and preparing the report and sending a mail with attaching these reports and exceptions At VANGUARD the focus was on creating Robots are designed to identify existing huge time taking repetitive manual processes and provide innovative automated solutions Responsibilities Developed Blue Prism frame work Visual Business Objects Process studio to update the Provider directory from excel XML Website data through screen scraping and using OCR Optical character recognition from printed material for some of the Financial Advisor and Client facing Applications Worked on Integrating applications like Web Services Mainframe MS Office GUI Outlook etc using workflow automation tools Blue Prism Worked with BA Business Analysts in developing and creating the estimations for Processes Extensively used Blue Prism Inbuilt objects functions in Expression Builder for String Operations Collection Manipulation to Compare and eliminate duplicates from the data Interacting with different based applications and buildings various action stages within the business object and interacted all the objects in process studio Implemented Blue Prism User authentication by defining user roles creating users and setting password policies Efficiently handled monitoring and troubleshooting the Blue Prism environment through Control room Created access control interfaces via the Application Modeler within Object Studio Experience in using Blue Prisms Credential Manager for maintaining securing and retrieving the user credentials Experience using Excel VBO XML VBO object to perform operations while taking data as inputs Efficient use of stages blocks data types session and environmental variables for processing of Business Process diagrams and Process flow charts using RPA tools Creating and documenting test procedures and scenarios for PreUAT phases supporting the operational teams during UAT and rollout phases Actively involved in vigorous unit testing endtoend testing and Production support to accelerate Business processes Provided Exception handling at every possible scenario in order to diffuse an exception during Process development Managed a team of developers in an Agile environment Extensively used different action stages to perform logical operations in Blue Prism Responsible to develop services using NET and Web API technology Performed DEV SAT and POAT testing before sending to production Worked closely with QA Quality Analyst and involved in creating Elevation checklists and Elevating processes Proven organizational time management and multitasking skills and ability to work independently and quickly learn new technology and adopt to new environment Professional communication skills coupled with very positive user interaction team spirit Environment Blue Prism V50 V42 OCR CNET VBA HTML XML MS Excel SQL Server Visual Studio Hadoop Developer Vaizva Inc Alpharetta GA February 2017 to August 2017 VaizTrack intelligent software along with a physical device installed in vehicles can allow organizations to surveil and track their vehicles round the clock monitor battery status and remotely disable battery in the event of theft It can also help manage maintain and retrieve vehicle and vehicle parts information efficiently and manage inventories with automated alerts Responsibilities Development of Map Reduce jobs in cascading for data cleansing and data processing of flat files Responsible for importing the flat files from external environments to ACC in Hadoop Design developed and implemented main flow component for end to end data flow process within the platform Responsible for creating the SOAP clients for consuming the web service Requests Involved in Analysis and design for setting up edge node as per the client requirement Created Pig Latin scripts to sort group join and filter the enterprise wise data Expertise in writing the hive scripts for large data sets comparison Expertise in performance optimization and memory tuning of mapreduce applications Experience with MySQL for utilizing it for auditing purposes on the cluster Responsible for MapR upgrade in both production and nonproduction environment Written shell scripts for data extraction and data cleansing for performing member specific analytics Coordinate with Administrator team to analyze Map Reduce Jobs performance for resolving any cluster related issues Expertise in platform related Hadoop Production support tasks by analyzing the job logs Transferred data from external sources from MySQL to Hadoop using Sqoop Coordinate with different teams to determine the root cause and taking steps to resolve them Responsible for continuous Integration with Jenkins and deploying the applications into production using XL Deploy Performed POC in installation configuration of Apache Hadoop on Amazon AWS EC2 system Managed and reviewed Hadoop log files to identify issues when job fails and finding out the root cause Utilizing service now to provide application support for the existing clients Environment Python Hortonworks Hadoop YARN MapReduce Hive Pig Shell Scripting SOAP web service MySQL Sqoop MapR Git Jenkins Python Developer Sutherland Global Services Chennai Tamil Nadu November 2013 to May 2015 Responsibilities Wrote application views which triggered by URLs and open respected templates Developed different REST APIs in Jinja and flask framework with using python scripting Used MySQL as backend database and MySQL dB of Python as database connector to interact with MySQL server Used Restful APIs to access data from different suppliers Support the scripts configuration testing execution deployment and run monitoring and metering Used Restful APIs to gather network traffic data from Servers Developed and executed User Acceptance Testing portion of test plan Involved in complete SDLC Requirement Analysis Development System and Integration Testing Followed MVC Structure to develop Application Responsible for search engine optimization to improve the visibility of the website Developed Merge jobs in Python to extract and load data into MySQL database Performed Unit and system testing Handled cross browserplatform compatibility issues IE Firefox and Chrome on both Windows Managed application state using server and clientbased State Management options Environment Python 27 Django MySQL HTML XHTML CSS JavaScript Apache Web Server Git Linux Education Masters Skills HTML 1 year Linux 1 year MySQL 2 years Python 2 years Scripting 2 years Additional Information Technical skills Languages Python C NET JAVAHTML CSS Shell Scripting Tools BluePrism V42 V50 UiPath Selenium web driver Mainframes SAP Ui5 Platforms LinuxUnix Redhat 56 7 Ubuntu 12041404 CentOS Windows NT20032008 Windows 10 Databases MySQL Oracle 1011g PostgreSQL SQL Developer SQL Server Hadoop Tools HortonworksHDFS Hive PIG Sqoop Flume MapReduceOozieSpark Ambari HDP Hortonworks Data Flow HDF NiFi Other tools SAP Support PortalHPSM JIRA Service NowMS Office applications Outlook Visual Studio",
    "entities": [
        "Big Data Analysis Design Development Implementation and Testing",
        "Knowledge of Information Extraction NLP",
        "Sublime Text",
        "Hadoop Developer Hadoop",
        "Developed Spark",
        "The Vanguard Group",
        "Amazon AWS EC2",
        "MultipleEnvironments",
        "Business Process",
        "Hadoop",
        "TeamEnvironment Work Experience Hadoop Developer Vaizva Inc",
        "SOAP",
        "JUnit",
        "REST API Frame",
        "QA Quality Analyst",
        "HBase",
        "Amazon",
        "Control room Created",
        "Web Database",
        "Python",
        "SDLC Requirement Analysis Development System",
        "Spark SQLStreaming",
        "Provider",
        "NC",
        "Efficient Time Management and Organization Skills",
        "Django Hands",
        "Retails Systems",
        "Performed Test Driven Development TDD",
        "Linux",
        "Visual Studio Hadoop Developer Vaizva Inc",
        "the Financial Advisor",
        "RDS",
        "Agile",
        "IE Firefox",
        "Agile Development",
        "NET",
        "Mainframes SAP Ui5 Platforms",
        "Provided Exception",
        "Languages Python C",
        "API",
        "Object Oriented Design",
        "Windows NT20032008 Windows 10",
        "Sqoop",
        "sort group join",
        "LINUX",
        "ACC",
        "Created",
        "POAT",
        "State Management options Environment Python",
        "AWS",
        "Sub Queries Stored Procedures Triggers Cursors and Functions",
        "Ubuntu RedHat",
        "Main Frame Application",
        "ETL Excellent",
        "Relational Database Services",
        "HDFS Job Tracker Task Tracker",
        "Performed Unit",
        "PyUnit PyTest",
        "Servers Developed",
        "SQL",
        "BA Business Analysts",
        "Responsibilities Development of Map Reduce",
        "Agile Software Methodology Worked",
        "Jira Practical",
        "RPA",
        "Application Responsible",
        "Amazon AWS",
        "MapReduce Develop",
        "Visual Business Objects Process",
        "HTTPREST",
        "Hadoop Design",
        "Apache Hadoop",
        "Excel VBO XML VBO",
        "Outlook Visual Studio",
        "Blue Prisms Credential",
        "Deep Learning",
        "SVN",
        "Expertise",
        "Additional Information Technical",
        "Good Analytical",
        "REST",
        "Data",
        "Relational Database",
        "MapReduce",
        "OCR Optical",
        "MultipleTasks",
        "Implemented Blue Prism User",
        "Blue Prism Responsible",
        "Node"
    ],
    "experience": "Experience in realtime configuration and management of MySQL MongoDB and PostgreSQL databases on a large scale Experience on several python packages like Numpy Beautiful Soup Pickle PySide Scipy and PyTables Experience in developing Web Services with Python programming language Familiar with JSON based REST Web services and Amazon Web services Good experience in writing SQL queries and implementing stored procedures functions packages tables views cursors triggers Significant application development experience in Ubuntu RedHat and Windows environments Extensive experience on developmental tools such as Eclipse PyCharm and Sublime Text version control tools like Git and Apache SVN Experience in testing and debugging applications using PyUnit PyTest and JUnit frameworks Experience in working with AWS Amazon S3 Amazon EC2 and Relational Database Services Have experience in Objectoriented programming multithreading algorithms data structures and system programming Proficient in Python experience building and product ionizing endtoend systems Knowledge of Information Extraction NLP algorithms coupled with Deep Learning Experience with file systems server architectures databases SQL and data movement ETL Excellent understanding of Hadoop architecture and various components such as HDFS Job Tracker Task Tracker Name Node Data Node YARN Spark and MapReduce programming paradigm Experience in importing and exporting data using Sqoop from Relational Database to HDFS and from HDFS to Relational Database Experience in continuous build and version control systems like Jira Git and SVN Setting up python REST API Frame work using Django Hands on experience in data quality data organization metadata and data profiling and large Data set sizes Ability to move data between production systems and work on crossmultiple platforms Experience in writing Sub Queries Stored Procedures Triggers Cursors and Functions on MySQL and PostgreSQL database Good experience in handling errorsexceptions and debugging the issues in large scale applications Good Analytical and ProblemSolving skills and ability to work on own besides being a valuable and contributing team player Exceptional problem solving and decisionmaking capabilities recognized by associates for quality of data in alternative solutions Wellversed with Agile Development process tools like Jira Practical experience with working on MultipleEnvironments like Development Testing Production Excellent Interpersonal and Communication skills Efficient Time Management and Organization Skills ability to handle MultipleTasks and work well in a TeamEnvironment Work Experience Hadoop Developer Vaizva Inc Alpharetta GA January 2018 to Present PTT serves at the onestop solution for logistics business It is a coherently designed integrated solution with planning scheduling realtime tracking and key performance indicators with rich builtin analytics And as there is no middleware software involved users can significantly reduce their IT costs Responsibilities Experience in writing Sqoop scripts to import and export data from RDBMS into HDFS HIVE and handled incremental loading on the customer and transaction information data dynamically Involved in creating Hive tables loading with data and writing hive queries that will run internally in map reduce way Developed Spark code using Scala and Spark SQLStreaming for faster testing and processing of data Skilled experience in installing configuring and using Apache Hadoop ecosystems such as Pig and Spark Estimated the Software Hardware requirements for the Name Node and Data Node in the cluster Extracted the needed data from the server into HDFS and Bulk Loaded the cleaned data into HBase using MapReduce Develop HIVE queries for the analysts Created an email notification service upon completion of job for the particular team which requested for the data Migrate on inhouse database to AWS Cloud and also designed built and deployed a multitude of applications utilizing the AWS stack Including EC2 RDS by focusing on highavailability and autoscaling Involved in support for Amazon AWS and RDS to host staticmedia files and the database into Amazon Cloud Microservice architecture development using Python and Docker on an Ubuntu Linux platform using HTTPREST interfaces with deployment into a multinode Kubernetes environment Performed Test Driven Development TDD and continuous integration to keep in line with Agile Software Methodology Worked in development of applications especially in LINUX environment and familiar with the commands All projects adhered to agile principles to deliver commitments meet acceptance criteria and ensure fast releases Environment Python Hortonworks Spark Oozie Hive HBase Linux MapReduce Scala Sqoop GitHub AWS Linux Shell Scripting RPA Developer The Vanguard Group Charlotte NC August 2017 to December 2017 Retails Systems focuses on automating the manual process of creating Report by extracting data from Web Database and Main Frame Application and preparing the report and sending a mail with attaching these reports and exceptions At VANGUARD the focus was on creating Robots are designed to identify existing huge time taking repetitive manual processes and provide innovative automated solutions Responsibilities Developed Blue Prism frame work Visual Business Objects Process studio to update the Provider directory from excel XML Website data through screen scraping and using OCR Optical character recognition from printed material for some of the Financial Advisor and Client facing Applications Worked on Integrating applications like Web Services Mainframe MS Office GUI Outlook etc using workflow automation tools Blue Prism Worked with BA Business Analysts in developing and creating the estimations for Processes Extensively used Blue Prism Inbuilt objects functions in Expression Builder for String Operations Collection Manipulation to Compare and eliminate duplicates from the data Interacting with different based applications and buildings various action stages within the business object and interacted all the objects in process studio Implemented Blue Prism User authentication by defining user roles creating users and setting password policies Efficiently handled monitoring and troubleshooting the Blue Prism environment through Control room Created access control interfaces via the Application Modeler within Object Studio Experience in using Blue Prisms Credential Manager for maintaining securing and retrieving the user credentials Experience using Excel VBO XML VBO object to perform operations while taking data as inputs Efficient use of stages blocks data types session and environmental variables for processing of Business Process diagrams and Process flow charts using RPA tools Creating and documenting test procedures and scenarios for PreUAT phases supporting the operational teams during UAT and rollout phases Actively involved in vigorous unit testing endtoend testing and Production support to accelerate Business processes Provided Exception handling at every possible scenario in order to diffuse an exception during Process development Managed a team of developers in an Agile environment Extensively used different action stages to perform logical operations in Blue Prism Responsible to develop services using NET and Web API technology Performed DEV SAT and POAT testing before sending to production Worked closely with QA Quality Analyst and involved in creating Elevation checklists and Elevating processes Proven organizational time management and multitasking skills and ability to work independently and quickly learn new technology and adopt to new environment Professional communication skills coupled with very positive user interaction team spirit Environment Blue Prism V50 V42 OCR CNET VBA HTML XML MS Excel SQL Server Visual Studio Hadoop Developer Vaizva Inc Alpharetta GA February 2017 to August 2017 VaizTrack intelligent software along with a physical device installed in vehicles can allow organizations to surveil and track their vehicles round the clock monitor battery status and remotely disable battery in the event of theft It can also help manage maintain and retrieve vehicle and vehicle parts information efficiently and manage inventories with automated alerts Responsibilities Development of Map Reduce jobs in cascading for data cleansing and data processing of flat files Responsible for importing the flat files from external environments to ACC in Hadoop Design developed and implemented main flow component for end to end data flow process within the platform Responsible for creating the SOAP clients for consuming the web service Requests Involved in Analysis and design for setting up edge node as per the client requirement Created Pig Latin scripts to sort group join and filter the enterprise wise data Expertise in writing the hive scripts for large data sets comparison Expertise in performance optimization and memory tuning of mapreduce applications Experience with MySQL for utilizing it for auditing purposes on the cluster Responsible for MapR upgrade in both production and nonproduction environment Written shell scripts for data extraction and data cleansing for performing member specific analytics Coordinate with Administrator team to analyze Map Reduce Jobs performance for resolving any cluster related issues Expertise in platform related Hadoop Production support tasks by analyzing the job logs Transferred data from external sources from MySQL to Hadoop using Sqoop Coordinate with different teams to determine the root cause and taking steps to resolve them Responsible for continuous Integration with Jenkins and deploying the applications into production using XL Deploy Performed POC in installation configuration of Apache Hadoop on Amazon AWS EC2 system Managed and reviewed Hadoop log files to identify issues when job fails and finding out the root cause Utilizing service now to provide application support for the existing clients Environment Python Hortonworks Hadoop YARN MapReduce Hive Pig Shell Scripting SOAP web service MySQL Sqoop MapR Git Jenkins Python Developer Sutherland Global Services Chennai Tamil Nadu November 2013 to May 2015 Responsibilities Wrote application views which triggered by URLs and open respected templates Developed different REST APIs in Jinja and flask framework with using python scripting Used MySQL as backend database and MySQL dB of Python as database connector to interact with MySQL server Used Restful APIs to access data from different suppliers Support the scripts configuration testing execution deployment and run monitoring and metering Used Restful APIs to gather network traffic data from Servers Developed and executed User Acceptance Testing portion of test plan Involved in complete SDLC Requirement Analysis Development System and Integration Testing Followed MVC Structure to develop Application Responsible for search engine optimization to improve the visibility of the website Developed Merge jobs in Python to extract and load data into MySQL database Performed Unit and system testing Handled cross browserplatform compatibility issues IE Firefox and Chrome on both Windows Managed application state using server and clientbased State Management options Environment Python 27 Django MySQL HTML XHTML CSS JavaScript Apache Web Server Git Linux Education Masters Skills HTML 1 year Linux 1 year MySQL 2 years Python 2 years Scripting 2 years Additional Information Technical skills Languages Python C NET JAVAHTML CSS Shell Scripting Tools BluePrism V42 V50 UiPath Selenium web driver Mainframes SAP Ui5 Platforms LinuxUnix Redhat 56 7 Ubuntu 12041404 CentOS Windows NT20032008 Windows 10 Databases MySQL Oracle 1011 g PostgreSQL SQL Developer SQL Server Hadoop Tools HortonworksHDFS Hive PIG Sqoop Flume MapReduceOozieSpark Ambari HDP Hortonworks Data Flow HDF NiFi Other tools SAP Support PortalHPSM JIRA Service NowMS Office applications Outlook Visual Studio",
    "extracted_keywords": [
        "Hadoop",
        "Developer",
        "Hadoop",
        "span",
        "lDeveloperspan",
        "Hadoop",
        "Developer",
        "Vaizva",
        "Inc",
        "Alpharetta",
        "GA",
        "years",
        "IT",
        "experience",
        "Big",
        "Data",
        "Analysis",
        "Design",
        "Development",
        "Implementation",
        "Testing",
        "standalone",
        "architecture",
        "enterprise",
        "application",
        "software",
        "Python",
        "domains",
        "time",
        "management",
        "skills",
        "ability",
        "technology",
        "environment",
        "Expert",
        "knowledge",
        "experience",
        "Object",
        "Oriented",
        "Design",
        "Programming",
        "experience",
        "Pythons",
        "application",
        "development",
        "frameworks",
        "Django",
        "Experience",
        "configuration",
        "management",
        "MySQL",
        "MongoDB",
        "PostgreSQL",
        "databases",
        "scale",
        "Experience",
        "python",
        "packages",
        "Numpy",
        "Beautiful",
        "Soup",
        "Pickle",
        "PySide",
        "Scipy",
        "PyTables",
        "Experience",
        "Web",
        "Services",
        "Python",
        "programming",
        "language",
        "JSON",
        "REST",
        "Web",
        "services",
        "Amazon",
        "Web",
        "services",
        "experience",
        "SQL",
        "queries",
        "procedures",
        "functions",
        "packages",
        "views",
        "cursors",
        "application",
        "development",
        "experience",
        "Ubuntu",
        "RedHat",
        "Windows",
        "experience",
        "tools",
        "Eclipse",
        "PyCharm",
        "Sublime",
        "Text",
        "version",
        "control",
        "tools",
        "Git",
        "Apache",
        "SVN",
        "Experience",
        "testing",
        "debugging",
        "applications",
        "PyUnit",
        "PyTest",
        "JUnit",
        "frameworks",
        "Experience",
        "AWS",
        "Amazon",
        "S3",
        "Amazon",
        "EC2",
        "Relational",
        "Database",
        "Services",
        "experience",
        "programming",
        "multithreading",
        "algorithms",
        "data",
        "structures",
        "system",
        "programming",
        "Proficient",
        "Python",
        "experience",
        "building",
        "product",
        "endtoend",
        "systems",
        "Knowledge",
        "Information",
        "Extraction",
        "NLP",
        "algorithms",
        "Deep",
        "Learning",
        "Experience",
        "file",
        "systems",
        "server",
        "SQL",
        "data",
        "movement",
        "ETL",
        "understanding",
        "Hadoop",
        "architecture",
        "components",
        "HDFS",
        "Job",
        "Tracker",
        "Task",
        "Tracker",
        "Name",
        "Node",
        "Data",
        "Node",
        "YARN",
        "Spark",
        "MapReduce",
        "programming",
        "paradigm",
        "Experience",
        "data",
        "Sqoop",
        "Relational",
        "Database",
        "HDFS",
        "HDFS",
        "Relational",
        "Database",
        "Experience",
        "build",
        "version",
        "control",
        "systems",
        "Jira",
        "Git",
        "SVN",
        "python",
        "REST",
        "API",
        "Frame",
        "work",
        "Django",
        "Hands",
        "experience",
        "data",
        "quality",
        "data",
        "organization",
        "metadata",
        "data",
        "profiling",
        "Data",
        "set",
        "Ability",
        "data",
        "production",
        "systems",
        "work",
        "platforms",
        "Experience",
        "Sub",
        "Queries",
        "Stored",
        "Procedures",
        "Triggers",
        "Cursors",
        "Functions",
        "MySQL",
        "PostgreSQL",
        "database",
        "experience",
        "errorsexceptions",
        "issues",
        "scale",
        "applications",
        "Good",
        "Analytical",
        "ProblemSolving",
        "skills",
        "ability",
        "team",
        "player",
        "Exceptional",
        "problem",
        "capabilities",
        "associates",
        "quality",
        "data",
        "solutions",
        "Agile",
        "Development",
        "process",
        "tools",
        "Jira",
        "Practical",
        "experience",
        "MultipleEnvironments",
        "Development",
        "Testing",
        "Production",
        "Excellent",
        "Interpersonal",
        "Communication",
        "Efficient",
        "Time",
        "Management",
        "Organization",
        "Skills",
        "ability",
        "MultipleTasks",
        "TeamEnvironment",
        "Work",
        "Experience",
        "Hadoop",
        "Developer",
        "Vaizva",
        "Inc",
        "Alpharetta",
        "GA",
        "January",
        "Present",
        "PTT",
        "solution",
        "logistics",
        "business",
        "solution",
        "planning",
        "scheduling",
        "realtime",
        "tracking",
        "performance",
        "indicators",
        "builtin",
        "analytics",
        "middleware",
        "software",
        "users",
        "IT",
        "Responsibilities",
        "Experience",
        "Sqoop",
        "scripts",
        "export",
        "data",
        "RDBMS",
        "HDFS",
        "HIVE",
        "loading",
        "customer",
        "transaction",
        "information",
        "data",
        "Hive",
        "tables",
        "data",
        "hive",
        "queries",
        "map",
        "way",
        "Developed",
        "Spark",
        "code",
        "Scala",
        "Spark",
        "testing",
        "processing",
        "data",
        "experience",
        "configuring",
        "Apache",
        "Hadoop",
        "ecosystems",
        "Pig",
        "Spark",
        "Software",
        "Hardware",
        "requirements",
        "Name",
        "Node",
        "Data",
        "Node",
        "cluster",
        "data",
        "server",
        "HDFS",
        "Bulk",
        "Loaded",
        "data",
        "HBase",
        "MapReduce",
        "Develop",
        "HIVE",
        "analysts",
        "email",
        "notification",
        "service",
        "completion",
        "job",
        "team",
        "data",
        "Migrate",
        "inhouse",
        "database",
        "AWS",
        "Cloud",
        "multitude",
        "applications",
        "AWS",
        "stack",
        "EC2",
        "RDS",
        "highavailability",
        "support",
        "Amazon",
        "AWS",
        "RDS",
        "files",
        "database",
        "Amazon",
        "Cloud",
        "Microservice",
        "architecture",
        "development",
        "Python",
        "Docker",
        "Ubuntu",
        "Linux",
        "platform",
        "HTTPREST",
        "interfaces",
        "deployment",
        "multinode",
        "Kubernetes",
        "environment",
        "Performed",
        "Test",
        "Driven",
        "Development",
        "TDD",
        "integration",
        "line",
        "Agile",
        "Software",
        "Methodology",
        "development",
        "applications",
        "LINUX",
        "environment",
        "commands",
        "projects",
        "principles",
        "commitments",
        "acceptance",
        "criteria",
        "releases",
        "Environment",
        "Python",
        "Hortonworks",
        "Spark",
        "Oozie",
        "Hive",
        "HBase",
        "Linux",
        "MapReduce",
        "Scala",
        "Sqoop",
        "GitHub",
        "Linux",
        "Shell",
        "Scripting",
        "RPA",
        "Developer",
        "Vanguard",
        "Group",
        "Charlotte",
        "NC",
        "August",
        "December",
        "Retails",
        "Systems",
        "process",
        "Report",
        "data",
        "Web",
        "Database",
        "Main",
        "Frame",
        "Application",
        "report",
        "mail",
        "reports",
        "exceptions",
        "VANGUARD",
        "focus",
        "Robots",
        "time",
        "processes",
        "solutions",
        "Responsibilities",
        "Blue",
        "Prism",
        "frame",
        "work",
        "Visual",
        "Business",
        "Objects",
        "Process",
        "studio",
        "Provider",
        "directory",
        "XML",
        "Website",
        "data",
        "screen",
        "OCR",
        "character",
        "recognition",
        "material",
        "Financial",
        "Advisor",
        "Client",
        "Applications",
        "applications",
        "Web",
        "Services",
        "Mainframe",
        "MS",
        "Office",
        "GUI",
        "Outlook",
        "workflow",
        "automation",
        "tools",
        "Blue",
        "Prism",
        "BA",
        "Business",
        "Analysts",
        "estimations",
        "Processes",
        "Blue",
        "Prism",
        "Inbuilt",
        "functions",
        "Expression",
        "Builder",
        "String",
        "Operations",
        "Collection",
        "Manipulation",
        "duplicates",
        "data",
        "applications",
        "buildings",
        "action",
        "stages",
        "business",
        "object",
        "objects",
        "process",
        "studio",
        "Blue",
        "Prism",
        "User",
        "authentication",
        "user",
        "roles",
        "users",
        "password",
        "policies",
        "monitoring",
        "Blue",
        "Prism",
        "environment",
        "Control",
        "room",
        "access",
        "control",
        "interfaces",
        "Application",
        "Modeler",
        "Object",
        "Studio",
        "Experience",
        "Blue",
        "Prisms",
        "Credential",
        "Manager",
        "user",
        "credentials",
        "Experience",
        "Excel",
        "VBO",
        "XML",
        "VBO",
        "object",
        "operations",
        "data",
        "use",
        "stages",
        "blocks",
        "data",
        "types",
        "session",
        "variables",
        "processing",
        "Business",
        "Process",
        "diagrams",
        "Process",
        "flow",
        "charts",
        "RPA",
        "tools",
        "test",
        "procedures",
        "scenarios",
        "PreUAT",
        "phases",
        "teams",
        "UAT",
        "phases",
        "unit",
        "testing",
        "testing",
        "Production",
        "support",
        "Business",
        "processes",
        "Exception",
        "scenario",
        "order",
        "exception",
        "Process",
        "development",
        "team",
        "developers",
        "environment",
        "action",
        "stages",
        "operations",
        "Blue",
        "Prism",
        "services",
        "NET",
        "Web",
        "API",
        "technology",
        "Performed",
        "DEV",
        "SAT",
        "POAT",
        "testing",
        "production",
        "QA",
        "Quality",
        "Analyst",
        "Elevation",
        "checklists",
        "Elevating",
        "time",
        "management",
        "skills",
        "ability",
        "technology",
        "environment",
        "communication",
        "skills",
        "user",
        "interaction",
        "team",
        "spirit",
        "Environment",
        "Blue",
        "Prism",
        "V50",
        "V42",
        "OCR",
        "CNET",
        "VBA",
        "HTML",
        "XML",
        "MS",
        "Excel",
        "SQL",
        "Server",
        "Visual",
        "Studio",
        "Hadoop",
        "Developer",
        "Vaizva",
        "Inc",
        "Alpharetta",
        "GA",
        "February",
        "August",
        "VaizTrack",
        "software",
        "device",
        "vehicles",
        "organizations",
        "vehicles",
        "clock",
        "monitor",
        "battery",
        "status",
        "battery",
        "event",
        "theft",
        "retrieve",
        "vehicle",
        "vehicle",
        "parts",
        "information",
        "inventories",
        "alerts",
        "Responsibilities",
        "Development",
        "Map",
        "Reduce",
        "jobs",
        "data",
        "cleansing",
        "data",
        "processing",
        "files",
        "files",
        "environments",
        "ACC",
        "Hadoop",
        "Design",
        "flow",
        "component",
        "end",
        "data",
        "flow",
        "process",
        "platform",
        "clients",
        "web",
        "service",
        "Requests",
        "Analysis",
        "design",
        "edge",
        "node",
        "client",
        "requirement",
        "Created",
        "Pig",
        "Latin",
        "scripts",
        "group",
        "join",
        "enterprise",
        "data",
        "Expertise",
        "hive",
        "scripts",
        "data",
        "comparison",
        "Expertise",
        "performance",
        "optimization",
        "memory",
        "tuning",
        "mapreduce",
        "applications",
        "Experience",
        "MySQL",
        "auditing",
        "purposes",
        "cluster",
        "MapR",
        "production",
        "nonproduction",
        "environment",
        "Written",
        "shell",
        "scripts",
        "data",
        "extraction",
        "data",
        "member",
        "analytics",
        "Coordinate",
        "Administrator",
        "team",
        "Map",
        "Reduce",
        "Jobs",
        "performance",
        "cluster",
        "issues",
        "Expertise",
        "platform",
        "Hadoop",
        "Production",
        "support",
        "tasks",
        "job",
        "logs",
        "data",
        "sources",
        "MySQL",
        "Hadoop",
        "Sqoop",
        "Coordinate",
        "teams",
        "root",
        "cause",
        "steps",
        "Integration",
        "Jenkins",
        "applications",
        "production",
        "XL",
        "Deploy",
        "Performed",
        "POC",
        "installation",
        "configuration",
        "Apache",
        "Hadoop",
        "Amazon",
        "AWS",
        "EC2",
        "system",
        "Hadoop",
        "log",
        "files",
        "issues",
        "job",
        "root",
        "Utilizing",
        "service",
        "application",
        "support",
        "clients",
        "Environment",
        "Python",
        "Hortonworks",
        "Hadoop",
        "YARN",
        "MapReduce",
        "Hive",
        "Pig",
        "Shell",
        "Scripting",
        "SOAP",
        "web",
        "service",
        "MySQL",
        "Sqoop",
        "MapR",
        "Git",
        "Jenkins",
        "Python",
        "Developer",
        "Sutherland",
        "Global",
        "Services",
        "Chennai",
        "Tamil",
        "Nadu",
        "November",
        "May",
        "Responsibilities",
        "application",
        "views",
        "URLs",
        "templates",
        "REST",
        "APIs",
        "Jinja",
        "flask",
        "framework",
        "python",
        "scripting",
        "MySQL",
        "database",
        "MySQL",
        "Python",
        "database",
        "connector",
        "MySQL",
        "server",
        "APIs",
        "data",
        "suppliers",
        "scripts",
        "configuration",
        "testing",
        "execution",
        "deployment",
        "monitoring",
        "APIs",
        "network",
        "traffic",
        "data",
        "Servers",
        "Developed",
        "User",
        "Acceptance",
        "Testing",
        "portion",
        "test",
        "plan",
        "SDLC",
        "Requirement",
        "Analysis",
        "Development",
        "System",
        "Integration",
        "Testing",
        "MVC",
        "Structure",
        "Application",
        "Responsible",
        "search",
        "engine",
        "optimization",
        "visibility",
        "website",
        "Merge",
        "jobs",
        "Python",
        "data",
        "MySQL",
        "database",
        "Performed",
        "Unit",
        "system",
        "testing",
        "cross",
        "browserplatform",
        "compatibility",
        "issues",
        "IE",
        "Firefox",
        "Chrome",
        "Windows",
        "application",
        "state",
        "server",
        "State",
        "Management",
        "options",
        "Environment",
        "Python",
        "Django",
        "MySQL",
        "HTML",
        "XHTML",
        "CSS",
        "JavaScript",
        "Apache",
        "Web",
        "Server",
        "Git",
        "Linux",
        "Education",
        "Masters",
        "Skills",
        "HTML",
        "year",
        "Linux",
        "year",
        "MySQL",
        "years",
        "Python",
        "years",
        "Scripting",
        "years",
        "Additional",
        "Information",
        "Technical",
        "skills",
        "Languages",
        "Python",
        "C",
        "NET",
        "JAVAHTML",
        "CSS",
        "Shell",
        "Scripting",
        "Tools",
        "BluePrism",
        "V42",
        "V50",
        "UiPath",
        "Selenium",
        "web",
        "driver",
        "Mainframes",
        "SAP",
        "Ui5",
        "Platforms",
        "LinuxUnix",
        "Redhat",
        "Ubuntu",
        "CentOS",
        "Windows",
        "NT20032008",
        "Windows",
        "Databases",
        "MySQL",
        "Oracle",
        "g",
        "PostgreSQL",
        "SQL",
        "Developer",
        "SQL",
        "Server",
        "Hadoop",
        "Tools",
        "Hive",
        "PIG",
        "Sqoop",
        "Flume",
        "MapReduceOozieSpark",
        "Ambari",
        "HDP",
        "Hortonworks",
        "Data",
        "Flow",
        "HDF",
        "NiFi",
        "tools",
        "SAP",
        "Support",
        "PortalHPSM",
        "JIRA",
        "Service",
        "NowMS",
        "Office",
        "applications",
        "Outlook",
        "Visual",
        "Studio"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T21:37:41.729268",
    "resume_data": "Hadoop Developer Hadoop span lDeveloperspan Hadoop Developer Vaizva Inc Alpharetta GA Around 4 years of IT experience in Big Data Analysis Design Development Implementation and Testing of various standalone and clientserver architecture based enterprise application software in Python on various domains Proven organizational time management and multitasking skills and ability to work independently and quickly learn new technology and adopt to new environment Expert knowledge experience in Object Oriented Design and Programming concepts Strong experience in using Pythons application development frameworks such as Django Experience in realtime configuration and management of MySQL MongoDB and PostgreSQL databases on a large scale Experience on several python packages like Numpy Beautiful Soup Pickle PySide Scipy and PyTables Experience in developing Web Services with Python programming language Familiar with JSON based REST Web services and Amazon Web services Good experience in writing SQL queries and implementing stored procedures functions packages tables views cursors triggers Significant application development experience in Ubuntu RedHat and Windows environments Extensive experience on developmental tools such as Eclipse PyCharm and Sublime Text version control tools like Git and Apache SVN Experience in testing and debugging applications using PyUnit PyTest and JUnit frameworks Experience in working with AWS Amazon S3 Amazon EC2 and Relational Database Services Have experience in Objectoriented programming multithreading algorithms data structures and system programming Proficient in Python experience building and product ionizing endtoend systems Knowledge of Information Extraction NLP algorithms coupled with Deep Learning Experience with file systems server architectures databases SQL and data movement ETL Excellent understanding of Hadoop architecture and various components such as HDFS Job Tracker Task Tracker Name Node Data Node YARN Spark and MapReduce programming paradigm Experience in importing and exporting data using Sqoop from Relational Database to HDFS and from HDFS to Relational Database Experience in continuous build and version control systems like Jira Git and SVN Setting up python REST API Frame work using Django Hands on experience in data quality data organization metadata and data profiling and large Data set sizes Ability to move data between production systems and work on crossmultiple platforms Experience in writing Sub Queries Stored Procedures Triggers Cursors and Functions on MySQL and PostgreSQL database Good experience in handling errorsexceptions and debugging the issues in large scale applications Good Analytical and ProblemSolving skills and ability to work on own besides being a valuable and contributing team player Exceptional problem solving and decisionmaking capabilities recognized by associates for quality of data in alternative solutions Wellversed with Agile Development process tools like Jira Practical experience with working on MultipleEnvironments like Development Testing Production Excellent Interpersonal and Communication skills Efficient Time Management and Organization Skills ability to handle MultipleTasks and work well in a TeamEnvironment Work Experience Hadoop Developer Vaizva Inc Alpharetta GA January 2018 to Present PTT serves at the onestop solution for logistics business It is a coherently designed integrated solution with planning scheduling realtime tracking and key performance indicators with rich builtin analytics And as there is no middleware software involved users can significantly reduce their IT costs Responsibilities Experience in writing Sqoop scripts to import and export data from RDBMS into HDFS HIVE and handled incremental loading on the customer and transaction information data dynamically Involved in creating Hive tables loading with data and writing hive queries that will run internally in map reduce way Developed Spark code using Scala and Spark SQLStreaming for faster testing and processing of data Skilled experience in installing configuring and using Apache Hadoop ecosystems such as Pig and Spark Estimated the Software Hardware requirements for the Name Node and Data Node in the cluster Extracted the needed data from the server into HDFS and Bulk Loaded the cleaned data into HBase using MapReduce Develop HIVE queries for the analysts Created an email notification service upon completion of job for the particular team which requested for the data Migrate on inhouse database to AWS Cloud and also designed built and deployed a multitude of applications utilizing the AWS stack Including EC2 RDS by focusing on highavailability and autoscaling Involved in support for Amazon AWS and RDS to host staticmedia files and the database into Amazon Cloud Microservice architecture development using Python and Docker on an Ubuntu Linux platform using HTTPREST interfaces with deployment into a multinode Kubernetes environment Performed Test Driven Development TDD and continuous integration to keep in line with Agile Software Methodology Worked in development of applications especially in LINUX environment and familiar with the commands All projects adhered to agile principles to deliver commitments meet acceptance criteria and ensure fast releases Environment Python Hortonworks Spark Oozie Hive HBase Linux MapReduce Scala Sqoop GitHub AWS Linux Shell Scripting RPA Developer The Vanguard Group Charlotte NC August 2017 to December 2017 Retails Systems focuses on automating the manual process of creating Report by extracting data from Web Database and Main Frame Application and preparing the report and sending a mail with attaching these reports and exceptions At VANGUARD the focus was on creating Robots are designed to identify existing huge time taking repetitive manual processes and provide innovative automated solutions Responsibilities Developed Blue Prism frame work Visual Business Objects Process studio to update the Provider directory from excel XML Website data through screen scraping and using OCR Optical character recognition from printed material for some of the Financial Advisor and Client facing Applications Worked on Integrating applications like Web Services Mainframe MS Office GUI Outlook etc using workflow automation tools Blue Prism Worked with BA Business Analysts in developing and creating the estimations for Processes Extensively used Blue Prism Inbuilt objects functions in Expression Builder for String Operations Collection Manipulation to Compare and eliminate duplicates from the data Interacting with different based applications and buildings various action stages within the business object and interacted all the objects in process studio Implemented Blue Prism User authentication by defining user roles creating users and setting password policies Efficiently handled monitoring and troubleshooting the Blue Prism environment through Control room Created access control interfaces via the Application Modeler within Object Studio Experience in using Blue Prisms Credential Manager for maintaining securing and retrieving the user credentials Experience using Excel VBO XML VBO object to perform operations while taking data as inputs Efficient use of stages blocks data types session and environmental variables for processing of Business Process diagrams and Process flow charts using RPA tools Creating and documenting test procedures and scenarios for PreUAT phases supporting the operational teams during UAT and rollout phases Actively involved in vigorous unit testing endtoend testing and Production support to accelerate Business processes Provided Exception handling at every possible scenario in order to diffuse an exception during Process development Managed a team of developers in an Agile environment Extensively used different action stages to perform logical operations in Blue Prism Responsible to develop services using NET and Web API technology Performed DEV SAT and POAT testing before sending to production Worked closely with QA Quality Analyst and involved in creating Elevation checklists and Elevating processes Proven organizational time management and multitasking skills and ability to work independently and quickly learn new technology and adopt to new environment Professional communication skills coupled with very positive user interaction team spirit Environment Blue Prism V50 V42 OCR CNET VBA HTML XML MS Excel SQL Server Visual Studio Hadoop Developer Vaizva Inc Alpharetta GA February 2017 to August 2017 VaizTrack intelligent software along with a physical device installed in vehicles can allow organizations to surveil and track their vehicles round the clock monitor battery status and remotely disable battery in the event of theft It can also help manage maintain and retrieve vehicle and vehicle parts information efficiently and manage inventories with automated alerts Responsibilities Development of Map Reduce jobs in cascading for data cleansing and data processing of flat files Responsible for importing the flat files from external environments to ACC in Hadoop Design developed and implemented main flow component for end to end data flow process within the platform Responsible for creating the SOAP clients for consuming the web service Requests Involved in Analysis and design for setting up edge node as per the client requirement Created Pig Latin scripts to sort group join and filter the enterprise wise data Expertise in writing the hive scripts for large data sets comparison Expertise in performance optimization and memory tuning of mapreduce applications Experience with MySQL for utilizing it for auditing purposes on the cluster Responsible for MapR upgrade in both production and nonproduction environment Written shell scripts for data extraction and data cleansing for performing member specific analytics Coordinate with Administrator team to analyze Map Reduce Jobs performance for resolving any cluster related issues Expertise in platform related Hadoop Production support tasks by analyzing the job logs Transferred data from external sources from MySQL to Hadoop using Sqoop Coordinate with different teams to determine the root cause and taking steps to resolve them Responsible for continuous Integration with Jenkins and deploying the applications into production using XL Deploy Performed POC in installation configuration of Apache Hadoop on Amazon AWS EC2 system Managed and reviewed Hadoop log files to identify issues when job fails and finding out the root cause Utilizing service now to provide application support for the existing clients Environment Python Hortonworks Hadoop YARN MapReduce Hive Pig Shell Scripting SOAP web service MySQL Sqoop MapR Git Jenkins Python Developer Sutherland Global Services Chennai Tamil Nadu November 2013 to May 2015 Responsibilities Wrote application views which triggered by URLs and open respected templates Developed different REST APIs in Jinja and flask framework with using python scripting Used MySQL as backend database and MySQL dB of Python as database connector to interact with MySQL server Used Restful APIs to access data from different suppliers Support the scripts configuration testing execution deployment and run monitoring and metering Used Restful APIs to gather network traffic data from Servers Developed and executed User Acceptance Testing portion of test plan Involved in complete SDLC Requirement Analysis Development System and Integration Testing Followed MVC Structure to develop Application Responsible for search engine optimization to improve the visibility of the website Developed Merge jobs in Python to extract and load data into MySQL database Performed Unit and system testing Handled cross browserplatform compatibility issues IE Firefox and Chrome on both Windows Managed application state using server and clientbased State Management options Environment Python 27 Django MySQL HTML XHTML CSS JavaScript Apache Web Server Git Linux Education Masters Skills HTML 1 year Linux 1 year MySQL 2 years Python 2 years Scripting 2 years Additional Information Technical skills Languages Python C NET JAVAHTML CSS Shell Scripting Tools BluePrism V42 V50 UiPath Selenium web driver Mainframes SAP Ui5 Platforms LinuxUnix Redhat 56 7 Ubuntu 12041404 CentOS Windows NT20032008 Windows 10 Databases MySQL Oracle 1011g PostgreSQL SQL Developer SQL Server Hadoop Tools HortonworksHDFS Hive PIG Sqoop Flume MapReduceOozieSpark Ambari HDP Hortonworks Data Flow HDF NiFi Other tools SAP Support PortalHPSM JIRA Service NowMS Office applications Outlook Visual Studio",
    "unique_id": "cb23d850-3c4b-4cd6-9c07-0569e7098ff2"
}