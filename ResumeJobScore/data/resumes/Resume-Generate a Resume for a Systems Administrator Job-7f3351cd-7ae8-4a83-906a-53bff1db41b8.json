{
    "clean_data": "Data Analyst Data Analyst Data Analyst EPATH USA IOWA Richmond VA Overall 6years of experience in Data Analysis Data Conversion Data Validation Data Profiling UAT Testing and Report Creation and working experience in Tableau Teradata AWS Redshift AWS S3 Python Unix and Oracle Experience in Teradata SQL and Utilities like Tpump Multiload and Fast load Good experience in Developing Teradata SQL queries and using Utilities such As BTEQ Strong experience in using Excel and MS Access to dump the data and analyze based on business needs Experience in Creating Teradata SQL scripts using OLAP functions like rank and rank Over to improve the query performance while pulling the data from large tables Experience in data analysis using Python Pandas NumPy Worked on standard Python libraries include boto3 to connect to AWS Experience in moving data to cloud platform like AWS S3 and manipulate data using Redshift Worked on performance tuning and optimization to improve the efficiency in script executions Good working experience loading Data Files in AWS Environment and Performed SQL Testing on AWS redshift databases Exceptional ability to research analyze and convey complex technical information to diverse endusers at all levels Solutionsdriven strategist who consistently improves efficiency productivity and the bottom line Recognized for partnering with business leaders and technical teams to plan integrate document and execute complex project plans on time and on budget Work Experience Data Analyst EPATH USA IOWA April 2017 to Present Responsibilities Involved in analysis design and documenting business reports such as Executive summaries Scorecards and drilldown reports Interacted with Business analysts to understand data requirements to ensure high quality data is provided to the customers Developed scripts using Teradata advanced techniques like Row Number and Rank Functions Worked on performance tuning and Query Optimization for increasing the efficiency of the scripts Extracted data from existing data source and performed adhoc queries by using SQL Using advanced Excel features like Pivot tables and Charts for generating Graphs Worked on completing the metadata documentation for the projects Created monthly and quarterly business monitoring reports Developed BTEQ scripts in UNIX using Putty and used crontab to automate the batch scripts and execute scheduled jobs in UNIX Developed Python programs for manipulating the data reading from various Teradata Tables and convert them as one CSV Files update the Content in the database tables Used Python modules like Pandas and NumPy and date time to perform extensive data analysis Used python to save and retrieve data files from Amazon S3 buckets Performed transformations on loaded datasets using Python over the spark engine using both batch and streaming data Leverage Python development environment for data analysis and report building Moved data from AWS S3 buckets to AWS Redshift cluster by using CLI commands Performed verification and validation for accuracy of data in the monthlyquarterly reports Good knowledge on Json format data and performed the source target validations using aggregations and null validity functions Created multiset tables and volatile tables using existing tables and collected statistics on table to improve the performance Supported the users of application with using multiple SQL and PLSQL techniques Designed stunning visualizations using tableau software and publishing and presenting dashboards on web and desktop platforms Performed data visualization and developed presentation material using Tableau Identified and fixed causes of the reported issues by checking batch loading and python scripts scheduled as cron jobs Implemented point of view security to Tableau dashboards to facilitate visibility across various levels of the Organization Drew upon full range of Tableau platform technologies to design and implement proof of concept solutions and create advanced BI visualizations Created Tableau scorecards dashboards using stack bars bar graphs scattered plots geographical maps and Gantt charts Developed and reviewed SQL queries with use of joins clauses inner left right in Tableau Desktop to validate static and dynamic data for data validation Technical Skills Teradata SQL Assistant Teradata Loading utilities BTEQ Fast Load MultiLoad Spark PLSQL Python AWS Redshift AWS S3 Tableau MS Excel MS Power Point Python DeveloperData Specialist PPDI NC January 2016 to March 2017 Responsibilities Responsible for gathering requirements from business analysts and operational analysts Identified the data sources required for the reports needed to the customers Used Python programs automated the process of combining the large datasets and data files and then converting as Teradata tables for Data Analysis Created an Automated Python Programs to Archive the database tables which large in size and not in use into Mainframes folders Responsible for running scripts in SAS Enterprise Guide and generate Daily Weekly and Monthly reports Developed programs with manipulate arrays using libraries like Numpy and Python Did performance tuning and optimization for increasing the efficiency of the scripts by creating indexes adding constrains and query optimization Analyzed data from AWS Redshift database in Python using Pandas and NumPy module Experience analyzing huge datasets in AWS Redshift Generated graphs using MS Excel Pivot tables and creating presentations using Power Point Replace VBA macros with python using PyXll a python addin for Microsoft Excel Communicated with business users and analysts on business requirements Gathered and documented technical and business Meta data about the data Created numerous processes and flow charts to meet the business needs and interacted with business users to understand their data needs Experience in Teradata SQL and Utilities like Tpump Multiload and Fastload Experience in writing korn shell scripts for automating the jobs Automated reports by connecting Teradata from MS Excel using ODBC Flat Files brought it through Transformation and model it into humanreadable form using the visualization Software tools like Tableau Designed rich data visualizations to communicate complex ideas to customers or company leaders using Tableau Software Performed numerous data pulling requests manipulated and prepared data for business analyst using SQL Unix PLSQL Oracle Excel and Access Documented scripts specifications other processes and preparation of Technical Design Documents Technical Skills Teradata Teradata utilities SQL Assistant BTEQ Fast Load PLSQL SAS AWS Python Tableau Agile Excel Macros Data Analyst ATT Atlanta GA September 2013 to December 2015 Responsibilities Analysis of functional and nonfunctional categorized data elements for data profiling and mapping from source to target data environment Developed working documents to support findings and assign specific tasks Created data masking mappings to mask the sensitive data between production and test environment Worked on claims data and extracted data from various sources such as flat files Oracle and Mainframes Worked with data investigation discovery and mapping tools to scan every single data record from many sources to ensure data quality Performed data analysis and data profiling using complex SQL on various sources systems including Oracle and Teradata Written several shell scripts using UNIX Korn shell for file transfers error logging data archiving checking the log files and cleanup process Created Webi reports and modified existing Universes in SAP Business Objects Performing data management projects and fulfilling adhoc requests according to user specifications by utilizing data management software programs and tools like Perl Toad MS Access Excel and SQL Written SQL scripts to test the mappings and Developed Traceability Matrix of Business Requirements mapped to Test Scripts to ensure any Change Control in requirements leads to test case update Worked on loading data from flat files to Teradata tables using SAS Proc Import and Fast Load Techniques Designed scripts in SAS to be compatible with Teradata to load and access data from the Teradata tables Connected to Mainframe and Teradata from SAS using Macros Used Proc SQL to connect to Teradata Loaded files from SAS to Teradata and Created files from Teradata tables Using SAS Developed web mail and email campaigns for marketing to impress the customer with new business features provided Involved in extensive DATA validation by writing several complex SQL queries and Involved in backend testing and worked with data quality issues Assisted in defining business requirements for the IT team and created BRD and functional specifications documents along with mapping documents to assist the developers in their coding Designed and developed database models for the operational data store data warehouse and federated databases to support client enterprise Information Management Strategy Was flexible to work late hours to coordinate with offshore team Technical Skills MS SQL Server 2008 Oracle 10g MS office SAS Clear Quest Clear Case Teradata R13 Data Analyst Inforica Hyderabad Telangana October 2008 to June 2010 Roles and Responsiblities Generated a streamline process to understand the various steps in the Digital web design lifecycle and desired functionality of the new system by interacting with users consultants stakeholders and subject matter experts Organized the Initial KickOff meeting with the involved Business Stakeholders to discuss the initiatives and the business intent understand the stake holders needs and expectations in that scope using various Elicitation Techniques Identified researched investigated analyzed defined opportunities for business process improvement documented business processes and initiated efforts to make improvements Conducted and participated in JAD sessions with stakeholders and system users to collect the system Requirement specificationsSRSanalyzed the feasibility of their needs by coordinating with the project manager and technical lead Worked in Agile environment playing an active role in Iteration planning Sprint Review Lesson Learned and resolving Impediments Used Agile methodology to analyze and translate business requirements into system specifications Every request is well documented with the complete approval and stored in the Knowledge Link a data storage repository location for the future reference Assisted the Project Manager in setting realistic project expectations and in evaluating the impact of changes on the organization and plans accordingly and conducted project related presentations Manage Scope and change throughout the life cycle of the product Transferring file across various servers using secured FTP and uploading flat files into Teradata database tables using SAS and FastLoad utilities Responsible for creating requirements data profiling and data mapping of customer one attributes across multiple LOBs Coordinated with different LOBs in identifying the data and logic as part of the requirements gathering efforts Identify the project dependencies and escalate impediments Data analysis of customer data for creating custom reports in support of the LOBs Technical Skills AgileEnhancement Rational Requisite Pro MS Excel MS WORD UML Flow Charts Activity State Diagram MS VisioUnix Terra Data SAS HTML Java Script Skills Data modeling Ms access Ms sql server Sql server Oracle Plsql Sql Teradata Tableau Shell scripting Unix Unix shell Gui Python Reporting tools Scripting Pivot tables Rdbms Erwin Rational Additional Information Skill Set GUI Reporting Tools Business Objects12 Tableau Data Modeling StarSchema Modeling SnowflakeSchema Modeling FACT and dimension tables Pivot Tables Erwin Testing Tools Mercury Quality Center Rational Clear Quest RDBMS Oracle 11g10g9i8i7x MS SQL Server Teradata V2R6R12R13 R14 MS Access 70 Programming SQL PLSQL UNIX Shell Scripting VB Script Python Environment Windows 95 98 2000 NT XP 7 UNIX Other Tools TOAD AWS Spark MSOffice suite Word Excel Project and Outlook BTEQ Teradata V2R6R12R13 SQL Assistant SQL Workbench",
    "entities": [
        "Oracle Plsql Sql Teradata Tableau Shell",
        "Data Files",
        "Tableau Software Performed",
        "BI",
        "SAP Business Objects Performing",
        "UNIX",
        "MS Power Point Python",
        "Redshift Worked",
        "Macros Used Proc SQL",
        "Teradata Loaded",
        "MS Access",
        "Present Responsibilities Involved",
        "Row Number",
        "Technical Skills MS",
        "Atlanta",
        "Telangana",
        "JAD",
        "SQL Using",
        "Automated",
        "Python Pandas NumPy Worked",
        "Amazon",
        "ODBC Flat Files",
        "Assisted",
        "Created Tableau",
        "Developed",
        "AWS Redshift Generated",
        "Oracle Excel",
        "Conducted",
        "AWS S3",
        "UNIX Korn",
        "NC",
        "MS Excel",
        "SQL Unix",
        "Meta",
        "Data Analysis Data Conversion Data Validation Data Profiling UAT Testing and Report Creation",
        "Utilities",
        "Developed Traceability Matrix of Business Requirements",
        "SAS Clear Quest Clear Case Teradata",
        "R14 MS",
        "Tableau Desktop",
        "SQL Written",
        "Content",
        "CLI",
        "Information Management Strategy Was",
        "Tpump Multiload",
        "Initial KickOff",
        "Mainframes folders Responsible",
        "Created",
        "Analyzed",
        "Oracle",
        "Oracle Experience",
        "VisioUnix Terra Data",
        "Sql",
        "SAS Enterprise Guide",
        "SAS",
        "SQL",
        "BRD",
        "Transformation",
        "Daily Weekly",
        "Created Webi",
        "Tableau Identified",
        "Query Optimization",
        "Technical Skills AgileEnhancement Rational",
        "Tableau Teradata AWS Redshift AWS S3 Python",
        "Data Analysis Created",
        "FTP",
        "SAS Proc Import",
        "Technical Design Documents Technical Skills Teradata Teradata",
        "Pandas",
        "Performed",
        "OLAP",
        "Perl Toad MS Access Excel",
        "AWS Redshift",
        "ATT",
        "Macros Data",
        "FastLoad utilities Responsible",
        "Digital",
        "Tableau",
        "Microsoft Excel Communicated",
        "Teradata",
        "Change Control",
        "Iteration planning Sprint",
        "Putty",
        "Word Excel Project"
    ],
    "experience": "Experience in Teradata SQL and Utilities like Tpump Multiload and Fast load Good experience in Developing Teradata SQL queries and using Utilities such As BTEQ Strong experience in using Excel and MS Access to dump the data and analyze based on business needs Experience in Creating Teradata SQL scripts using OLAP functions like rank and rank Over to improve the query performance while pulling the data from large tables Experience in data analysis using Python Pandas NumPy Worked on standard Python libraries include boto3 to connect to AWS Experience in moving data to cloud platform like AWS S3 and manipulate data using Redshift Worked on performance tuning and optimization to improve the efficiency in script executions Good working experience loading Data Files in AWS Environment and Performed SQL Testing on AWS redshift databases Exceptional ability to research analyze and convey complex technical information to diverse endusers at all levels Solutionsdriven strategist who consistently improves efficiency productivity and the bottom line Recognized for partnering with business leaders and technical teams to plan integrate document and execute complex project plans on time and on budget Work Experience Data Analyst EPATH USA IOWA April 2017 to Present Responsibilities Involved in analysis design and documenting business reports such as Executive summaries Scorecards and drilldown reports Interacted with Business analysts to understand data requirements to ensure high quality data is provided to the customers Developed scripts using Teradata advanced techniques like Row Number and Rank Functions Worked on performance tuning and Query Optimization for increasing the efficiency of the scripts Extracted data from existing data source and performed adhoc queries by using SQL Using advanced Excel features like Pivot tables and Charts for generating Graphs Worked on completing the metadata documentation for the projects Created monthly and quarterly business monitoring reports Developed BTEQ scripts in UNIX using Putty and used crontab to automate the batch scripts and execute scheduled jobs in UNIX Developed Python programs for manipulating the data reading from various Teradata Tables and convert them as one CSV Files update the Content in the database tables Used Python modules like Pandas and NumPy and date time to perform extensive data analysis Used python to save and retrieve data files from Amazon S3 buckets Performed transformations on loaded datasets using Python over the spark engine using both batch and streaming data Leverage Python development environment for data analysis and report building Moved data from AWS S3 buckets to AWS Redshift cluster by using CLI commands Performed verification and validation for accuracy of data in the monthlyquarterly reports Good knowledge on Json format data and performed the source target validations using aggregations and null validity functions Created multiset tables and volatile tables using existing tables and collected statistics on table to improve the performance Supported the users of application with using multiple SQL and PLSQL techniques Designed stunning visualizations using tableau software and publishing and presenting dashboards on web and desktop platforms Performed data visualization and developed presentation material using Tableau Identified and fixed causes of the reported issues by checking batch loading and python scripts scheduled as cron jobs Implemented point of view security to Tableau dashboards to facilitate visibility across various levels of the Organization Drew upon full range of Tableau platform technologies to design and implement proof of concept solutions and create advanced BI visualizations Created Tableau scorecards dashboards using stack bars bar graphs scattered plots geographical maps and Gantt charts Developed and reviewed SQL queries with use of joins clauses inner left right in Tableau Desktop to validate static and dynamic data for data validation Technical Skills Teradata SQL Assistant Teradata Loading utilities BTEQ Fast Load MultiLoad Spark PLSQL Python AWS Redshift AWS S3 Tableau MS Excel MS Power Point Python DeveloperData Specialist PPDI NC January 2016 to March 2017 Responsibilities Responsible for gathering requirements from business analysts and operational analysts Identified the data sources required for the reports needed to the customers Used Python programs automated the process of combining the large datasets and data files and then converting as Teradata tables for Data Analysis Created an Automated Python Programs to Archive the database tables which large in size and not in use into Mainframes folders Responsible for running scripts in SAS Enterprise Guide and generate Daily Weekly and Monthly reports Developed programs with manipulate arrays using libraries like Numpy and Python Did performance tuning and optimization for increasing the efficiency of the scripts by creating indexes adding constrains and query optimization Analyzed data from AWS Redshift database in Python using Pandas and NumPy module Experience analyzing huge datasets in AWS Redshift Generated graphs using MS Excel Pivot tables and creating presentations using Power Point Replace VBA macros with python using PyXll a python addin for Microsoft Excel Communicated with business users and analysts on business requirements Gathered and documented technical and business Meta data about the data Created numerous processes and flow charts to meet the business needs and interacted with business users to understand their data needs Experience in Teradata SQL and Utilities like Tpump Multiload and Fastload Experience in writing korn shell scripts for automating the jobs Automated reports by connecting Teradata from MS Excel using ODBC Flat Files brought it through Transformation and model it into humanreadable form using the visualization Software tools like Tableau Designed rich data visualizations to communicate complex ideas to customers or company leaders using Tableau Software Performed numerous data pulling requests manipulated and prepared data for business analyst using SQL Unix PLSQL Oracle Excel and Access Documented scripts specifications other processes and preparation of Technical Design Documents Technical Skills Teradata Teradata utilities SQL Assistant BTEQ Fast Load PLSQL SAS AWS Python Tableau Agile Excel Macros Data Analyst ATT Atlanta GA September 2013 to December 2015 Responsibilities Analysis of functional and nonfunctional categorized data elements for data profiling and mapping from source to target data environment Developed working documents to support findings and assign specific tasks Created data masking mappings to mask the sensitive data between production and test environment Worked on claims data and extracted data from various sources such as flat files Oracle and Mainframes Worked with data investigation discovery and mapping tools to scan every single data record from many sources to ensure data quality Performed data analysis and data profiling using complex SQL on various sources systems including Oracle and Teradata Written several shell scripts using UNIX Korn shell for file transfers error logging data archiving checking the log files and cleanup process Created Webi reports and modified existing Universes in SAP Business Objects Performing data management projects and fulfilling adhoc requests according to user specifications by utilizing data management software programs and tools like Perl Toad MS Access Excel and SQL Written SQL scripts to test the mappings and Developed Traceability Matrix of Business Requirements mapped to Test Scripts to ensure any Change Control in requirements leads to test case update Worked on loading data from flat files to Teradata tables using SAS Proc Import and Fast Load Techniques Designed scripts in SAS to be compatible with Teradata to load and access data from the Teradata tables Connected to Mainframe and Teradata from SAS using Macros Used Proc SQL to connect to Teradata Loaded files from SAS to Teradata and Created files from Teradata tables Using SAS Developed web mail and email campaigns for marketing to impress the customer with new business features provided Involved in extensive DATA validation by writing several complex SQL queries and Involved in backend testing and worked with data quality issues Assisted in defining business requirements for the IT team and created BRD and functional specifications documents along with mapping documents to assist the developers in their coding Designed and developed database models for the operational data store data warehouse and federated databases to support client enterprise Information Management Strategy Was flexible to work late hours to coordinate with offshore team Technical Skills MS SQL Server 2008 Oracle 10 g MS office SAS Clear Quest Clear Case Teradata R13 Data Analyst Inforica Hyderabad Telangana October 2008 to June 2010 Roles and Responsiblities Generated a streamline process to understand the various steps in the Digital web design lifecycle and desired functionality of the new system by interacting with users consultants stakeholders and subject matter experts Organized the Initial KickOff meeting with the involved Business Stakeholders to discuss the initiatives and the business intent understand the stake holders needs and expectations in that scope using various Elicitation Techniques Identified researched investigated analyzed defined opportunities for business process improvement documented business processes and initiated efforts to make improvements Conducted and participated in JAD sessions with stakeholders and system users to collect the system Requirement specificationsSRSanalyzed the feasibility of their needs by coordinating with the project manager and technical lead Worked in Agile environment playing an active role in Iteration planning Sprint Review Lesson Learned and resolving Impediments Used Agile methodology to analyze and translate business requirements into system specifications Every request is well documented with the complete approval and stored in the Knowledge Link a data storage repository location for the future reference Assisted the Project Manager in setting realistic project expectations and in evaluating the impact of changes on the organization and plans accordingly and conducted project related presentations Manage Scope and change throughout the life cycle of the product Transferring file across various servers using secured FTP and uploading flat files into Teradata database tables using SAS and FastLoad utilities Responsible for creating requirements data profiling and data mapping of customer one attributes across multiple LOBs Coordinated with different LOBs in identifying the data and logic as part of the requirements gathering efforts Identify the project dependencies and escalate impediments Data analysis of customer data for creating custom reports in support of the LOBs Technical Skills AgileEnhancement Rational Requisite Pro MS Excel MS WORD UML Flow Charts Activity State Diagram MS VisioUnix Terra Data SAS HTML Java Script Skills Data modeling Ms access Ms sql server Sql server Oracle Plsql Sql Teradata Tableau Shell scripting Unix Unix shell Gui Python Reporting tools Scripting Pivot tables Rdbms Erwin Rational Additional Information Skill Set GUI Reporting Tools Business Objects12 Tableau Data Modeling StarSchema Modeling SnowflakeSchema Modeling FACT and dimension tables Pivot Tables Erwin Testing Tools Mercury Quality Center Rational Clear Quest RDBMS Oracle 11g10g9i8i7x MS SQL Server Teradata V2R6R12R13 R14 MS Access 70 Programming SQL PLSQL UNIX Shell Scripting VB Script Python Environment Windows 95 98 2000 NT XP 7 UNIX Other Tools TOAD AWS Spark MSOffice suite Word Excel Project and Outlook BTEQ Teradata V2R6R12R13 SQL Assistant SQL Workbench",
    "extracted_keywords": [
        "Data",
        "Analyst",
        "Data",
        "Analyst",
        "Data",
        "Analyst",
        "EPATH",
        "USA",
        "IOWA",
        "Richmond",
        "VA",
        "Overall",
        "experience",
        "Data",
        "Analysis",
        "Data",
        "Conversion",
        "Data",
        "Validation",
        "Data",
        "Profiling",
        "UAT",
        "Testing",
        "Report",
        "Creation",
        "working",
        "experience",
        "Tableau",
        "Teradata",
        "AWS",
        "Redshift",
        "AWS",
        "S3",
        "Python",
        "Unix",
        "Oracle",
        "Experience",
        "Teradata",
        "SQL",
        "Utilities",
        "Tpump",
        "Multiload",
        "load",
        "experience",
        "Teradata",
        "SQL",
        "Utilities",
        "BTEQ",
        "experience",
        "Excel",
        "MS",
        "Access",
        "data",
        "analyze",
        "business",
        "Experience",
        "Teradata",
        "SQL",
        "scripts",
        "OLAP",
        "functions",
        "rank",
        "query",
        "performance",
        "data",
        "tables",
        "Experience",
        "data",
        "analysis",
        "Python",
        "Pandas",
        "NumPy",
        "Python",
        "libraries",
        "AWS",
        "Experience",
        "data",
        "cloud",
        "platform",
        "AWS",
        "S3",
        "manipulate",
        "data",
        "Redshift",
        "Worked",
        "performance",
        "tuning",
        "optimization",
        "efficiency",
        "script",
        "executions",
        "working",
        "experience",
        "Data",
        "Files",
        "AWS",
        "Environment",
        "Performed",
        "SQL",
        "Testing",
        "AWS",
        "redshift",
        "ability",
        "information",
        "endusers",
        "levels",
        "Solutionsdriven",
        "strategist",
        "efficiency",
        "productivity",
        "line",
        "partnering",
        "business",
        "leaders",
        "teams",
        "document",
        "project",
        "plans",
        "time",
        "budget",
        "Work",
        "Experience",
        "Data",
        "Analyst",
        "EPATH",
        "USA",
        "IOWA",
        "April",
        "Present",
        "Responsibilities",
        "analysis",
        "design",
        "business",
        "reports",
        "Executive",
        "summaries",
        "Scorecards",
        "drilldown",
        "reports",
        "Business",
        "analysts",
        "data",
        "requirements",
        "quality",
        "data",
        "customers",
        "scripts",
        "Teradata",
        "techniques",
        "Row",
        "Number",
        "Rank",
        "Functions",
        "performance",
        "tuning",
        "Query",
        "Optimization",
        "efficiency",
        "scripts",
        "data",
        "data",
        "source",
        "queries",
        "SQL",
        "Excel",
        "features",
        "Pivot",
        "tables",
        "Charts",
        "Graphs",
        "metadata",
        "documentation",
        "projects",
        "business",
        "monitoring",
        "BTEQ",
        "scripts",
        "UNIX",
        "Putty",
        "crontab",
        "batch",
        "scripts",
        "jobs",
        "UNIX",
        "Developed",
        "Python",
        "programs",
        "data",
        "Teradata",
        "Tables",
        "CSV",
        "Files",
        "Content",
        "database",
        "tables",
        "Python",
        "modules",
        "Pandas",
        "NumPy",
        "date",
        "time",
        "data",
        "analysis",
        "python",
        "data",
        "files",
        "Amazon",
        "S3",
        "transformations",
        "datasets",
        "Python",
        "spark",
        "engine",
        "batch",
        "streaming",
        "data",
        "Leverage",
        "Python",
        "development",
        "environment",
        "data",
        "analysis",
        "report",
        "Moved",
        "data",
        "AWS",
        "S3",
        "buckets",
        "AWS",
        "cluster",
        "CLI",
        "commands",
        "Performed",
        "verification",
        "validation",
        "accuracy",
        "data",
        "knowledge",
        "Json",
        "format",
        "data",
        "source",
        "target",
        "validations",
        "aggregations",
        "validity",
        "functions",
        "multiset",
        "tables",
        "tables",
        "tables",
        "statistics",
        "table",
        "performance",
        "users",
        "application",
        "SQL",
        "PLSQL",
        "techniques",
        "visualizations",
        "tableau",
        "software",
        "publishing",
        "dashboards",
        "web",
        "desktop",
        "platforms",
        "data",
        "visualization",
        "presentation",
        "material",
        "Tableau",
        "causes",
        "issues",
        "batch",
        "loading",
        "scripts",
        "cron",
        "jobs",
        "point",
        "view",
        "security",
        "Tableau",
        "dashboards",
        "visibility",
        "levels",
        "Organization",
        "Drew",
        "range",
        "Tableau",
        "platform",
        "technologies",
        "proof",
        "concept",
        "solutions",
        "BI",
        "visualizations",
        "Tableau",
        "dashboards",
        "stack",
        "bars",
        "bar",
        "graphs",
        "plots",
        "maps",
        "Gantt",
        "charts",
        "SQL",
        "queries",
        "use",
        "joins",
        "clauses",
        "Tableau",
        "Desktop",
        "data",
        "data",
        "validation",
        "Technical",
        "Skills",
        "Teradata",
        "SQL",
        "Assistant",
        "Teradata",
        "Loading",
        "utilities",
        "BTEQ",
        "Fast",
        "Load",
        "MultiLoad",
        "Spark",
        "PLSQL",
        "Python",
        "AWS",
        "Redshift",
        "AWS",
        "S3",
        "Tableau",
        "MS",
        "Excel",
        "MS",
        "Power",
        "Point",
        "Python",
        "DeveloperData",
        "Specialist",
        "PPDI",
        "NC",
        "January",
        "March",
        "Responsibilities",
        "requirements",
        "business",
        "analysts",
        "analysts",
        "data",
        "sources",
        "reports",
        "customers",
        "Python",
        "programs",
        "process",
        "datasets",
        "data",
        "files",
        "Teradata",
        "tables",
        "Data",
        "Analysis",
        "Automated",
        "Python",
        "Programs",
        "Archive",
        "database",
        "tables",
        "size",
        "use",
        "Mainframes",
        "folders",
        "scripts",
        "SAS",
        "Enterprise",
        "Guide",
        "Daily",
        "Weekly",
        "reports",
        "programs",
        "manipulate",
        "arrays",
        "libraries",
        "Numpy",
        "Python",
        "performance",
        "tuning",
        "optimization",
        "efficiency",
        "scripts",
        "indexes",
        "constrains",
        "query",
        "optimization",
        "data",
        "AWS",
        "Redshift",
        "database",
        "Python",
        "Pandas",
        "NumPy",
        "module",
        "Experience",
        "datasets",
        "AWS",
        "Redshift",
        "graphs",
        "MS",
        "Excel",
        "Pivot",
        "presentations",
        "Power",
        "Point",
        "Replace",
        "VBA",
        "macros",
        "python",
        "PyXll",
        "python",
        "addin",
        "Microsoft",
        "Excel",
        "Communicated",
        "business",
        "users",
        "analysts",
        "business",
        "requirements",
        "business",
        "Meta",
        "data",
        "data",
        "processes",
        "charts",
        "business",
        "needs",
        "business",
        "users",
        "data",
        "Experience",
        "Teradata",
        "SQL",
        "Utilities",
        "Tpump",
        "Multiload",
        "Fastload",
        "Experience",
        "shell",
        "scripts",
        "jobs",
        "reports",
        "Teradata",
        "MS",
        "Excel",
        "ODBC",
        "Flat",
        "Files",
        "Transformation",
        "model",
        "form",
        "visualization",
        "Software",
        "tools",
        "Tableau",
        "data",
        "visualizations",
        "ideas",
        "customers",
        "company",
        "leaders",
        "Tableau",
        "Software",
        "data",
        "requests",
        "data",
        "business",
        "analyst",
        "SQL",
        "Unix",
        "PLSQL",
        "Oracle",
        "Excel",
        "Access",
        "scripts",
        "specifications",
        "processes",
        "preparation",
        "Technical",
        "Design",
        "Documents",
        "Technical",
        "Skills",
        "Teradata",
        "Teradata",
        "utilities",
        "SQL",
        "Assistant",
        "BTEQ",
        "Fast",
        "Load",
        "PLSQL",
        "SAS",
        "AWS",
        "Python",
        "Tableau",
        "Agile",
        "Excel",
        "Macros",
        "Data",
        "Analyst",
        "ATT",
        "Atlanta",
        "GA",
        "September",
        "December",
        "Responsibilities",
        "Analysis",
        "data",
        "elements",
        "data",
        "profiling",
        "mapping",
        "source",
        "data",
        "environment",
        "documents",
        "findings",
        "tasks",
        "data",
        "mappings",
        "data",
        "production",
        "test",
        "environment",
        "claims",
        "data",
        "data",
        "sources",
        "files",
        "Oracle",
        "Mainframes",
        "data",
        "investigation",
        "discovery",
        "mapping",
        "tools",
        "data",
        "record",
        "sources",
        "data",
        "quality",
        "Performed",
        "data",
        "analysis",
        "data",
        "profiling",
        "SQL",
        "sources",
        "systems",
        "Oracle",
        "Teradata",
        "shell",
        "scripts",
        "UNIX",
        "Korn",
        "shell",
        "file",
        "transfers",
        "data",
        "archiving",
        "log",
        "files",
        "cleanup",
        "process",
        "Created",
        "Webi",
        "reports",
        "Universes",
        "SAP",
        "Business",
        "Objects",
        "data",
        "management",
        "projects",
        "adhoc",
        "requests",
        "user",
        "specifications",
        "data",
        "management",
        "software",
        "programs",
        "tools",
        "Perl",
        "Toad",
        "MS",
        "Access",
        "Excel",
        "SQL",
        "Written",
        "SQL",
        "scripts",
        "mappings",
        "Traceability",
        "Matrix",
        "Business",
        "Requirements",
        "Scripts",
        "Change",
        "Control",
        "requirements",
        "test",
        "case",
        "loading",
        "data",
        "files",
        "tables",
        "SAS",
        "Proc",
        "Import",
        "Fast",
        "Load",
        "Techniques",
        "scripts",
        "SAS",
        "Teradata",
        "load",
        "access",
        "data",
        "Teradata",
        "tables",
        "Mainframe",
        "Teradata",
        "SAS",
        "Macros",
        "Used",
        "Proc",
        "SQL",
        "Teradata",
        "files",
        "SAS",
        "Teradata",
        "files",
        "Teradata",
        "tables",
        "SAS",
        "Developed",
        "web",
        "mail",
        "email",
        "campaigns",
        "marketing",
        "customer",
        "business",
        "features",
        "DATA",
        "validation",
        "SQL",
        "queries",
        "testing",
        "data",
        "quality",
        "issues",
        "business",
        "requirements",
        "IT",
        "team",
        "BRD",
        "specifications",
        "documents",
        "mapping",
        "documents",
        "developers",
        "database",
        "models",
        "data",
        "store",
        "data",
        "warehouse",
        "databases",
        "client",
        "enterprise",
        "Information",
        "Management",
        "Strategy",
        "hours",
        "team",
        "Technical",
        "Skills",
        "MS",
        "SQL",
        "Server",
        "Oracle",
        "g",
        "MS",
        "office",
        "SAS",
        "Clear",
        "Quest",
        "Clear",
        "Case",
        "Teradata",
        "R13",
        "Data",
        "Analyst",
        "Inforica",
        "Hyderabad",
        "Telangana",
        "October",
        "June",
        "Roles",
        "Responsiblities",
        "process",
        "steps",
        "Digital",
        "web",
        "design",
        "lifecycle",
        "functionality",
        "system",
        "users",
        "consultants",
        "stakeholders",
        "matter",
        "experts",
        "Initial",
        "KickOff",
        "meeting",
        "Business",
        "Stakeholders",
        "initiatives",
        "business",
        "intent",
        "stake",
        "holders",
        "needs",
        "expectations",
        "scope",
        "Elicitation",
        "Techniques",
        "opportunities",
        "business",
        "process",
        "improvement",
        "business",
        "processes",
        "efforts",
        "improvements",
        "JAD",
        "sessions",
        "stakeholders",
        "system",
        "users",
        "system",
        "Requirement",
        "feasibility",
        "needs",
        "project",
        "manager",
        "lead",
        "Agile",
        "environment",
        "role",
        "Iteration",
        "planning",
        "Sprint",
        "Review",
        "Lesson",
        "Learned",
        "Impediments",
        "methodology",
        "business",
        "requirements",
        "system",
        "specifications",
        "request",
        "approval",
        "Knowledge",
        "Link",
        "data",
        "storage",
        "repository",
        "location",
        "reference",
        "Assisted",
        "Project",
        "Manager",
        "project",
        "expectations",
        "impact",
        "changes",
        "organization",
        "plans",
        "project",
        "presentations",
        "Manage",
        "Scope",
        "life",
        "cycle",
        "product",
        "file",
        "servers",
        "FTP",
        "files",
        "Teradata",
        "database",
        "tables",
        "SAS",
        "FastLoad",
        "utilities",
        "requirements",
        "data",
        "profiling",
        "data",
        "mapping",
        "customer",
        "LOBs",
        "LOBs",
        "data",
        "logic",
        "part",
        "requirements",
        "efforts",
        "project",
        "dependencies",
        "impediments",
        "Data",
        "analysis",
        "customer",
        "data",
        "custom",
        "reports",
        "support",
        "LOBs",
        "Technical",
        "Skills",
        "AgileEnhancement",
        "Rational",
        "Requisite",
        "Pro",
        "MS",
        "Excel",
        "MS",
        "WORD",
        "UML",
        "Flow",
        "Charts",
        "Activity",
        "State",
        "Diagram",
        "MS",
        "VisioUnix",
        "Terra",
        "Data",
        "SAS",
        "HTML",
        "Java",
        "Script",
        "Skills",
        "Data",
        "Ms",
        "access",
        "Ms",
        "server",
        "Sql",
        "server",
        "Oracle",
        "Plsql",
        "Sql",
        "Teradata",
        "Tableau",
        "Shell",
        "Unix",
        "Unix",
        "shell",
        "Gui",
        "Python",
        "Reporting",
        "tools",
        "Scripting",
        "Pivot",
        "Rdbms",
        "Erwin",
        "Rational",
        "Additional",
        "Information",
        "Skill",
        "Set",
        "GUI",
        "Reporting",
        "Tools",
        "Business",
        "Objects12",
        "Tableau",
        "Data",
        "Modeling",
        "StarSchema",
        "SnowflakeSchema",
        "FACT",
        "dimension",
        "Pivot",
        "Tables",
        "Erwin",
        "Testing",
        "Tools",
        "Mercury",
        "Quality",
        "Center",
        "Rational",
        "Clear",
        "Quest",
        "RDBMS",
        "Oracle",
        "11g10g9i8i7x",
        "MS",
        "SQL",
        "Server",
        "Teradata",
        "V2R6R12R13",
        "R14",
        "MS",
        "Access",
        "Programming",
        "SQL",
        "PLSQL",
        "UNIX",
        "Shell",
        "Scripting",
        "VB",
        "Script",
        "Python",
        "Environment",
        "Windows",
        "NT",
        "XP",
        "UNIX",
        "Tools",
        "TOAD",
        "AWS",
        "Spark",
        "MSOffice",
        "suite",
        "Word",
        "Excel",
        "Project",
        "Outlook",
        "BTEQ",
        "Teradata",
        "V2R6R12R13",
        "SQL",
        "Assistant",
        "SQL",
        "Workbench"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T22:44:27.719828",
    "resume_data": "Data Analyst Data Analyst Data Analyst EPATH USA IOWA Richmond VA Overall 6years of experience in Data Analysis Data Conversion Data Validation Data Profiling UAT Testing and Report Creation and working experience in Tableau Teradata AWS Redshift AWS S3 Python Unix and Oracle Experience in Teradata SQL and Utilities like Tpump Multiload and Fast load Good experience in Developing Teradata SQL queries and using Utilities such As BTEQ Strong experience in using Excel and MS Access to dump the data and analyze based on business needs Experience in Creating Teradata SQL scripts using OLAP functions like rank and rank Over to improve the query performance while pulling the data from large tables Experience in data analysis using Python Pandas NumPy Worked on standard Python libraries include boto3 to connect to AWS Experience in moving data to cloud platform like AWS S3 and manipulate data using Redshift Worked on performance tuning and optimization to improve the efficiency in script executions Good working experience loading Data Files in AWS Environment and Performed SQL Testing on AWS redshift databases Exceptional ability to research analyze and convey complex technical information to diverse endusers at all levels Solutionsdriven strategist who consistently improves efficiency productivity and the bottom line Recognized for partnering with business leaders and technical teams to plan integrate document and execute complex project plans on time and on budget Work Experience Data Analyst EPATH USA IOWA April 2017 to Present Responsibilities Involved in analysis design and documenting business reports such as Executive summaries Scorecards and drilldown reports Interacted with Business analysts to understand data requirements to ensure high quality data is provided to the customers Developed scripts using Teradata advanced techniques like Row Number and Rank Functions Worked on performance tuning and Query Optimization for increasing the efficiency of the scripts Extracted data from existing data source and performed adhoc queries by using SQL Using advanced Excel features like Pivot tables and Charts for generating Graphs Worked on completing the metadata documentation for the projects Created monthly and quarterly business monitoring reports Developed BTEQ scripts in UNIX using Putty and used crontab to automate the batch scripts and execute scheduled jobs in UNIX Developed Python programs for manipulating the data reading from various Teradata Tables and convert them as one CSV Files update the Content in the database tables Used Python modules like Pandas and NumPy and date time to perform extensive data analysis Used python to save and retrieve data files from Amazon S3 buckets Performed transformations on loaded datasets using Python over the spark engine using both batch and streaming data Leverage Python development environment for data analysis and report building Moved data from AWS S3 buckets to AWS Redshift cluster by using CLI commands Performed verification and validation for accuracy of data in the monthlyquarterly reports Good knowledge on Json format data and performed the source target validations using aggregations and null validity functions Created multiset tables and volatile tables using existing tables and collected statistics on table to improve the performance Supported the users of application with using multiple SQL and PLSQL techniques Designed stunning visualizations using tableau software and publishing and presenting dashboards on web and desktop platforms Performed data visualization and developed presentation material using Tableau Identified and fixed causes of the reported issues by checking batch loading and python scripts scheduled as cron jobs Implemented point of view security to Tableau dashboards to facilitate visibility across various levels of the Organization Drew upon full range of Tableau platform technologies to design and implement proof of concept solutions and create advanced BI visualizations Created Tableau scorecards dashboards using stack bars bar graphs scattered plots geographical maps and Gantt charts Developed and reviewed SQL queries with use of joins clauses inner left right in Tableau Desktop to validate static and dynamic data for data validation Technical Skills Teradata SQL Assistant Teradata Loading utilities BTEQ Fast Load MultiLoad Spark PLSQL Python AWS Redshift AWS S3 Tableau MS Excel MS Power Point Python DeveloperData Specialist PPDI NC January 2016 to March 2017 Responsibilities Responsible for gathering requirements from business analysts and operational analysts Identified the data sources required for the reports needed to the customers Used Python programs automated the process of combining the large datasets and data files and then converting as Teradata tables for Data Analysis Created an Automated Python Programs to Archive the database tables which large in size and not in use into Mainframes folders Responsible for running scripts in SAS Enterprise Guide and generate Daily Weekly and Monthly reports Developed programs with manipulate arrays using libraries like Numpy and Python Did performance tuning and optimization for increasing the efficiency of the scripts by creating indexes adding constrains and query optimization Analyzed data from AWS Redshift database in Python using Pandas and NumPy module Experience analyzing huge datasets in AWS Redshift Generated graphs using MS Excel Pivot tables and creating presentations using Power Point Replace VBA macros with python using PyXll a python addin for Microsoft Excel Communicated with business users and analysts on business requirements Gathered and documented technical and business Meta data about the data Created numerous processes and flow charts to meet the business needs and interacted with business users to understand their data needs Experience in Teradata SQL and Utilities like Tpump Multiload and Fastload Experience in writing korn shell scripts for automating the jobs Automated reports by connecting Teradata from MS Excel using ODBC Flat Files brought it through Transformation and model it into humanreadable form using the visualization Software tools like Tableau Designed rich data visualizations to communicate complex ideas to customers or company leaders using Tableau Software Performed numerous data pulling requests manipulated and prepared data for business analyst using SQL Unix PLSQL Oracle Excel and Access Documented scripts specifications other processes and preparation of Technical Design Documents Technical Skills Teradata Teradata utilities SQL Assistant BTEQ Fast Load PLSQL SAS AWS Python Tableau Agile Excel Macros Data Analyst ATT Atlanta GA September 2013 to December 2015 Responsibilities Analysis of functional and nonfunctional categorized data elements for data profiling and mapping from source to target data environment Developed working documents to support findings and assign specific tasks Created data masking mappings to mask the sensitive data between production and test environment Worked on claims data and extracted data from various sources such as flat files Oracle and Mainframes Worked with data investigation discovery and mapping tools to scan every single data record from many sources to ensure data quality Performed data analysis and data profiling using complex SQL on various sources systems including Oracle and Teradata Written several shell scripts using UNIX Korn shell for file transfers error logging data archiving checking the log files and cleanup process Created Webi reports and modified existing Universes in SAP Business Objects Performing data management projects and fulfilling adhoc requests according to user specifications by utilizing data management software programs and tools like Perl Toad MS Access Excel and SQL Written SQL scripts to test the mappings and Developed Traceability Matrix of Business Requirements mapped to Test Scripts to ensure any Change Control in requirements leads to test case update Worked on loading data from flat files to Teradata tables using SAS Proc Import and Fast Load Techniques Designed scripts in SAS to be compatible with Teradata to load and access data from the Teradata tables Connected to Mainframe and Teradata from SAS using Macros Used Proc SQL to connect to Teradata Loaded files from SAS to Teradata and Created files from Teradata tables Using SAS Developed web mail and email campaigns for marketing to impress the customer with new business features provided Involved in extensive DATA validation by writing several complex SQL queries and Involved in backend testing and worked with data quality issues Assisted in defining business requirements for the IT team and created BRD and functional specifications documents along with mapping documents to assist the developers in their coding Designed and developed database models for the operational data store data warehouse and federated databases to support client enterprise Information Management Strategy Was flexible to work late hours to coordinate with offshore team Technical Skills MS SQL Server 2008 Oracle 10g MS office SAS Clear Quest Clear Case Teradata R13 Data Analyst Inforica Hyderabad Telangana October 2008 to June 2010 Roles and Responsiblities Generated a streamline process to understand the various steps in the Digital web design lifecycle and desired functionality of the new system by interacting with users consultants stakeholders and subject matter experts Organized the Initial KickOff meeting with the involved Business Stakeholders to discuss the initiatives and the business intent understand the stake holders needs and expectations in that scope using various Elicitation Techniques Identified researched investigated analyzed defined opportunities for business process improvement documented business processes and initiated efforts to make improvements Conducted and participated in JAD sessions with stakeholders and system users to collect the system Requirement specificationsSRSanalyzed the feasibility of their needs by coordinating with the project manager and technical lead Worked in Agile environment playing an active role in Iteration planning Sprint Review Lesson Learned and resolving Impediments Used Agile methodology to analyze and translate business requirements into system specifications Every request is well documented with the complete approval and stored in the Knowledge Link a data storage repository location for the future reference Assisted the Project Manager in setting realistic project expectations and in evaluating the impact of changes on the organization and plans accordingly and conducted project related presentations Manage Scope and change throughout the life cycle of the product Transferring file across various servers using secured FTP and uploading flat files into Teradata database tables using SAS and FastLoad utilities Responsible for creating requirements data profiling and data mapping of customer one attributes across multiple LOBs Coordinated with different LOBs in identifying the data and logic as part of the requirements gathering efforts Identify the project dependencies and escalate impediments Data analysis of customer data for creating custom reports in support of the LOBs Technical Skills AgileEnhancement Rational Requisite Pro MS Excel MS WORD UML Flow Charts Activity State Diagram MS VisioUnix Terra Data SAS HTML Java Script Skills Data modeling Ms access Ms sql server Sql server Oracle Plsql Sql Teradata Tableau Shell scripting Unix Unix shell Gui Python Reporting tools Scripting Pivot tables Rdbms Erwin Rational Additional Information Skill Set GUI Reporting Tools Business Objects12 Tableau Data Modeling StarSchema Modeling SnowflakeSchema Modeling FACT and dimension tables Pivot Tables Erwin Testing Tools Mercury Quality Center Rational Clear Quest RDBMS Oracle 11g10g9i8i7x MS SQL Server Teradata V2R6R12R13 R14 MS Access 70 Programming SQL PLSQL UNIX Shell Scripting VB Script Python Environment Windows 95 98 2000 NT XP 7 UNIX Other Tools TOAD AWS Spark MSOffice suite Word Excel Project and Outlook BTEQ Teradata V2R6R12R13 SQL Assistant SQL Workbench",
    "unique_id": "7f3351cd-7ae8-4a83-906a-53bff1db41b8"
}