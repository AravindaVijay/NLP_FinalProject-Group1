{
    "clean_data": "Sr ETL Talend Developer Sr ETL Talend span lDeveloperspan Sr ETL Talend Developer Northern Trust Richmond VA 8 years of experience in IT Industry involving Software Analysis Design ImplementationCoding Development Testing and Maintenancewith focus on Data warehousing applicationsusing ETL tools like Talend63 andIBM InfoSphere DataStage 91 3 years of experience using Talend Integration Suite 6x5x Talend Open Studio 6x5x and 2 years of experience with Talend Admin Console TAC Experience working with Data Warehousing Concepts like KimballInmon methodologiesOLAP OLTP Star Schema Snow Flake Schema LogicalPhysical Dimensional Data Modeling Experienced in working with Horton works distribution of Hadoop HDFS MapReduce Hive Impala Sqoop Flume Pig HBase and MongoDB Experience in dealing with structured and semistructured data in HDFS In depth understanding of the Gap Analysis ie AsIs and ToBe business processes andexperience in converting these requirements into Technical Specifications andTest Plans Highly Proficient in Agile Test Driven Iterative Scrum and Waterfall software development lifecycle Extensively used ETL methodology for performing Data Profiling Data Migration ExtractionTransformation and Loading using Talend and designed data conversions from wide variety ofsource systems including Netezza Oracle DB2 SQL server Teradata Hive nonrelational sources like flat files XML and Mainframe Files Experience in analyzing data using HiveQL and Pig Latin in HDFS Extracted data from multiple operational sources for loading staging area Data warehouse DataMarts using SCDs Type 1Type 2 Type 3 loads Extensively created mappings in TALEND using tMap tJoin tReplicate tParallelizetConvertType  tAggregate tSortRowtFlowMetertLogCatchertRowGenerator tNormalize tDenormalize tSetGlobalVar tHashInput tHashOutput tJavatJavarow tAggregateRow tWarn tLogCatcher tMysqlScd tFilter tGlobalmap tDie etc Extensive experience in using Talend features such as context variables triggers connectorsfor Database and flat files like tMySqlInput tMySqlConnection tOracle tMSSqlInputTMSSqlOutput tMSSqlrow tFileCopy tFileInputDelimited tFileExists Experience in using cloud components and connectors to make API calls for accessing data fromcloud storage Google Drive Salesforce Amazon S3 DropBox inTalend Open Studio Experience in creatingJoblets in TALEND for the processes which can be used in most of the jobsin a project like to Start job and Commit job Experience in monitoring and scheduling using AutoSys Control M Job Conductor TalendAdmin Console and using UNIX Korn Bourn Shell Scripting Expertise in creating sub jobs in parallel to maximize the performance and reduce overall jobexecution time with the use of parallelize component of Talend in TIS and using the MultithreadedExecutions in TOS Experienced in creating Triggers on TAC server to schedule Talend jobs to run on server Strong experience in Extraction Transformation loading ETL data from various sources into Data Warehouses and Data Marts using DataStageversions 91 87 81 Designed jobs in DataStage 9187 using stages like Transformer sequential Aggregator Data Set File Set Remove Duplicates Sort Join Merge Lookup Funnel Copy Modify Filter Change Data Capture Surrogate Key External Source External Target Compare and Schedule jobs through UNIX shell scripts Strong hands on experience using Teradata utilities like SQL Assistant BTEQ FastLoadMultiLoad Fast Export and TPUMP Authorized to work in the US for any employer Work Experience Sr ETL Talend Developer Northern Trust Chicago IL October 2016 to Present Responsibilities Worked with Data mapping team to understand the source to target mapping rules Analyzed the requirements and framed the business logic and implemented it using Talend Involved in ETL design and documentation Analyzed and performed data integration using Talend open integration suite Worked on the design development and testing of Talend mappings Created ETL job infrastructure using Talend Open Studio Load and transform data into HDFS from large set of structured data OracleSql server using Talend Big data studio Used Big Data components Hive components for extracting data from hive sources Wrote HiveQL queries using joins and implemented in tHiveInput component Utilized Big Data components like tHiveInput tHiveOutput tHDFSOutput tHiveRow tHiveLoad tHiveConnection tOracleInput tOracleOutput tPreJob tPostJob tLogRow Worked on Talend components like tReplace tmap tsort and tFilterColumn tFilterRow tJava tjavarow tConvertType etc Used Database components like tMSSQLInput tMsSqlRow tMsSqlOutput tOracleOutput tOracleInput etc Worked with various File components like tFileCopy tFileCompare tFileExist tFileDelete tFileRename Worked on improving the performance of Talend jobs Created triggers for a Talend job to run automatically on server Worked on Exporting and Importing of Talend jobs Created jobs to pass parameters from child job to parent job Exported jobs to Nexus and SVN repository Implemented update strategy on tables and used tJava tJavarow components to read data from tables to pull only newly inserted data from source tables Observed statistics of Talend jobs in AMC to improve the performance and in what scenarios errors are causing Created Generic and Repository schemas Developed project specific Deployment job responsible to deploy Talend jar files on to the windowsenvironment as a zip file later this zip file is unzipped and the files are again deployed to the UNIX box Also this deployment job is responsible to maintain versioning of the Talend jobs that are deployed in the UNIX environment Developed shell scripts in UNIX environment to support scheduling of the Talend jobs Monitored the daily runs weekly runs and adhoc runs to load data into the target systems Environment Talend 6361 Oracle IBM DB2 Teradata HDFS Hive Impala SQL PLSQL HP ALM JIRA Sr ETLTalend Developer IHG Atlanta GA June 2015 to September 2016 Responsibilities Worked in the Data Integration Team to perform data and application integration with a goal of moving more data more effectively efficiently and with high performance to assist in businesscritical projects coming up with huge data extraction Perform technical analysis ETL design development testing and deployment of IT solutions as needed by business or IT Participate in designing the overall logical physical Data warehouseDatamart data model and data architectures to support business requirements Explore prebuilt ETL metadata mappings and DAC metadata and Develop and maintain SQL code as needed for SQL Server database Performed data manipulations using various Talend components like tMap tJavarow tjava tOracleRow tOracleInput tOracleOutput tMSSQLInput and many more Analyzing the source data to know the quality of data by using Talend Data Quality Troubleshoot data integration issues and bugs analyze reasons for failure implement optimal solutions and revise procedures and documentation as needed Worked on Migration projects to migrate data from data warehouses on OracleDB2 and migrated those toNetezza Used SQL queries and other data analysis methods as well as Talend Enterprise Data Quality Platform for profiling and comparison of data which will be used to make decisions regarding how to measure business rules and quality of the data Writing Netezza SQL queries to join or any modifications in the table Used Talend reusable components like routines context variable and globalMap variables Responsible to tune ETL mappings Workflows and underlying data model to optimize load and Query performance Developed Talend ESB services and deployed them on ESB servers on different instances Implementing fast and efficient data acquisition using Big Data processing techniques and tools Monitored and supported the Talend jobs scheduled through Talend Admin Center TAC Developed Oracle PLSQL DDLs and Stored Procedures and worked on performance and fine Tuning of SQL Environment Talend 61 552 UNIX Shell script SQL Server Oracle Business Objects ERwin SVN DataStageTalendETL Developer Navistar Inc Chicago IL February 2013 to May 2015 Responsibilities Work with the Business team to better understand the requirements and determine the appropriate data source and conversion approach Developed DataStage Jobs define job parameters range lookups Extensively used the CDC Change Data Capture stage to implement the slowly changing Dimensional and Fact tables Used stages like Transformer sequential Aggregator Data Set File Set Remove Duplicates Sort Join Merge Lookup Funnel Copy Modify Filter Change Data Capture Surrogate Key External Source External Target Compare and Schedule jobs through UNIX shell scripts Assist the functional teams by insuring good functional requirements exist converting resulting functional specs into technical specs for data integration Created the DataStage jobs to load data from ECOM database to ODS to Business Intelligence layer Performed performance tuning of the jobs by interpreting performance statistics Create DataStage components efficiently and made them reusable Designing XML schema definitions XSDs extract and load to huge XML files Involved in loading data from Oracle database to Teradata tables using DataStage Design complex DataStage jobs with Java DB2 extractions SCD functionality and bulk loads Created Jobs using different stages like Aggregators Joins Merge Lookup Source dataset Row generator Change Capture Peak stages Column generator Oracle Teradata connectors Involved in design and development of complex enterprise parallel jobs to extract data into Oracle Teradata and flat files EnvironmentIBM InfoSphere DataStage 1 XML Oracle Teradata SharePoint UNIX HDFS JavaJ2EE Programmer State Farm Insurance Bloomington IL July 2011 to January 2013 State Farms Existing plus is the platform for Data Access Web Services Production Solutions Data team providessupports web services for Agreement related tables The data inserted updated deleted or retrieved in to the tables from these services resides on Existing Plus such as DB2 Z This data can be moved to Technical Platform through our data movement processes but onplatform will only be available for reading These services are consumed by State Farms Clients like Auto Fire Health Life and Bank etc Responsibilities Developing J2EE web services involving all the stages of Software Development Life Cycle Involve in the requirements gathering Design Development Unit testing and Bug fixing Use Design Patterns like MVC Business Delegate Service Locator Session Facade and DAO Developed the functionalities using Agile Methodology Involved in writing Thread Safe blocks for multithread access to make valid transactions Developed and implemented responsive web pages using JSF Primefaces while maintaining high usability standards Created and injected Spring services Spring boot Spring controllers and DAOs to achieve dependency injection and to wire objects of business classes Used Spring Inheritance to develop beans from already developed parent beans Develop and Implement interface for SOAP Web Services using JAXWS framework involving Global Transactions Used AJAX extensively to implement front end user interface features in the application Developed Web Services clients to consume those Web Services as well other enterprise wide Web Services Use IBM DB2 for creating databases and performing DB2 operations on the tables Used IBM Pure Query for generating JDBC connection beans that can be used for connecting DB2 database Database development required creation of new tables inserting data into the tables updating data in the tables and deleting data from the tables and required SQL tuning to reduce the response time in the application Deployed the application on IBM Web Sphere Application Server 85 Worked closely with QA team and fixed QA bugs as well as production issues with a quick turnaround time Used Splunk to get the Testing Preproductions and Production logs Focused on Test Driven Development thereby creating detailed JUnit tests for every single piece of functionality before writing the functionality Used Apache Maven for project management and building the application SVN being used for project management and version management Environment J2EE Java 17 Spring framework Hibernate JSP 20 JSR303 JDBC AJAX JAXWS Web services SOAP XML Java Beans Apache Axis2 JQuery JavaScript AngularJS IBM DB2 IBM Pure Query Java Developer SilverCloud Technologies Hyderabad Telangana May 2009 to June 2011 SilverCloud Technologieshas been one of the pioneering members of the book2net fraternity An endeavor to integrate services and products giving clients complete turnkey solutions for their digitization needs Responsibilities Worked on both WebLogic Portal 92 for Portal development and WebLogic 81 for Data Services Programming Developed the presentation layer using JSP HTML CSS and client validations using JavaScript Used GWT to send AJAX request to the server and updating data in the UI dynamically Developed Hibernate 30 in Data Access Layer to access and update information in the database Used JDBC SQL and PLSQL programming for storing retrieving manipulating the data Involved in designing and development of the ecommerce site using JSP Servlet EJBs JavaScript and JDBC Used Eclipse 60 as IDE for application development Configured Struts framework to implement MVC design patterns Validated all forms using Struts validation framework and implemented Tiles framework in the presentation layer Designed and developed GUI using JSP HTML DHTML and CSS Worked with JMS for messaging interface Used Hibernate for handling database transactions and persisting objects deployed the entire project on WebLogic application server Used AJAX for interactive user operations and client side validations Used XSL transforms on certain XML data Used XML for ORM mapping relations with the java classes and the database Developed ANT script for compiling and deployment Performed unit testing using Junit Environment JavaJ2EE Oracle 10g SQL PLSQL JSP EJB Struts Hibernate WebLogic 80 HTML AJAX Java Script JDBC XML JMS XSLT UML JUnit Log4j Eclipse 60 Education Bachelors Skills DB2 6 years SQL 6 years XML 6 years DATA INTEGRATION 5 years INTEGRATION 5 years Additional Information TECHNICAL SKILLS BigData HDFS Hive Pig Spark HBase Data Warehousing Talend Open Studio TOS for Data Integration 63 InfoSphere DataStage 91 87 Databases Netezza Teradata utilities BTEQ FASTLOAD FASTEXPORT MULTILOAD TRUMPOracle12c11x10g DB2 Microsoft SQL Server Hive Impala Sybase Programming TSQL PLSQL HTML XML Environment Windows UNIX  Linux Scripting Korn shell script Windows batch scripting JavaScript Languages SQL Developer AginityWorkBench Teradata SQL Assistant SQLPlus Toad OtherTools SQL Navigator Putty MSOffice VMWare Workstation",
    "entities": [
        "Used Hibernate",
        "AJAX",
        "GUI",
        "Business Intelligence layer Performed",
        "UNIX",
        "InfoSphere",
        "TALEND",
        "the Testing Preproductions",
        "Created the DataStage",
        "DataMarts",
        "Software Development Life Cycle Involve",
        "Query",
        "IBM",
        "Netezza Oracle DB2",
        "tOracle",
        "Hadoop HDFS MapReduce Hive",
        "ERwin",
        "XML",
        "AMC",
        "DHTML",
        "Atlanta",
        "Telangana",
        "Developer Navistar Inc",
        "Create DataStage",
        "DAC",
        "JAXWS",
        "WebLogic",
        "JUnit",
        "Developed DataStage Jobs",
        "DataStage",
        "Shell",
        "Used IBM Pure Query",
        "Created Generic",
        "Data Access Web Services Production Solutions Data",
        "Amazon",
        "Junit Environment JavaJ2EE Oracle",
        "Work Experience Sr ETL Talend Developer Northern Trust",
        "Data Services Programming Developed",
        "SQL Server",
        "Utilized Big Data",
        "Plans Highly Proficient",
        "Agile Methodology Involved",
        "Talend Enterprise Data Quality Platform",
        "Maintenancewith",
        "Worked on Migration",
        "tMap",
        "Talend Big",
        "Used Spring Inheritance",
        "Windows",
        "Waterfall",
        "the Data Integration Team",
        "Monitored",
        "DataStage Design",
        "Develop",
        "Worked on Exporting and Importing of",
        "Developed Web Services",
        "Data Profiling Data Migration ExtractionTransformation",
        "TOS Experienced",
        "JSP",
        "Repository",
        "ToBe",
        "Additional Information TECHNICAL SKILLS BigData HDFS Hive Pig Spark HBase Data",
        "Teradata Hive",
        "Talend",
        "Nexus",
        "MVC",
        "SCD",
        "TAC",
        "Developed Talend ESB",
        "SOAP Web Services",
        "API",
        "US",
        "Database",
        "Perform",
        "Oracle Teradata",
        "QA",
        "OracleSql",
        "Created",
        "Talend Admin Center TAC Developed Oracle",
        "MVC Business Delegate Service",
        "Analyzed",
        "Oracle",
        "Talend Involved",
        "Oracle Teradata connectors Involved",
        "Data Integration",
        "MultithreadedExecutions",
        "Workflows",
        "Design Development Unit",
        "DropBox",
        "Created Jobs",
        "Gap Analysis",
        "Putty MSOffice VMWare Workstation",
        "SQL",
        "Technical Platform",
        "tReplace",
        "AutoSys Control M Job Conductor TalendAdmin Console",
        "Transformer",
        "Chicago",
        "tJava",
        "Big Data",
        "Hive",
        "Data Access Layer",
        "Talend Admin Console TAC",
        "Technical Specifications",
        "ECOM",
        "SQL Server Oracle Business Objects",
        "ETL",
        "JavaScript Languages SQL Developer",
        "Dimensional and Fact",
        "Performed",
        "Talend Data Quality Troubleshoot",
        "IT Industry",
        "XSLT UML JUnit",
        "tAggregate",
        "the CDC Change Data Capture",
        "Impala",
        "Global Transactions",
        "GWT",
        "UI",
        "Microsoft",
        "SVN",
        "Present Responsibilities Worked",
        "Extraction Transformation",
        "JSF Primefaces",
        "Sr ETL Talend Developer Sr ETL Talend",
        "Data",
        "EnvironmentIBM",
        "State Farms Clients like Auto Fire Health Life and Bank etc Responsibilities Developing J2EE",
        "WebLogic Portal",
        "Teradata",
        "ESB",
        "ODS",
        "AsIs",
        "Data Warehousing Concepts",
        "Data Warehouses"
    ],
    "experience": "Experience working with Data Warehousing Concepts like KimballInmon methodologiesOLAP OLTP Star Schema Snow Flake Schema LogicalPhysical Dimensional Data Modeling Experienced in working with Horton works distribution of Hadoop HDFS MapReduce Hive Impala Sqoop Flume Pig HBase and MongoDB Experience in dealing with structured and semistructured data in HDFS In depth understanding of the Gap Analysis ie AsIs and ToBe business processes andexperience in converting these requirements into Technical Specifications andTest Plans Highly Proficient in Agile Test Driven Iterative Scrum and Waterfall software development lifecycle Extensively used ETL methodology for performing Data Profiling Data Migration ExtractionTransformation and Loading using Talend and designed data conversions from wide variety ofsource systems including Netezza Oracle DB2 SQL server Teradata Hive nonrelational sources like flat files XML and Mainframe Files Experience in analyzing data using HiveQL and Pig Latin in HDFS Extracted data from multiple operational sources for loading staging area Data warehouse DataMarts using SCDs Type 1Type 2 Type 3 loads Extensively created mappings in TALEND using tMap tJoin tReplicate tParallelizetConvertType   tAggregate tSortRowtFlowMetertLogCatchertRowGenerator tNormalize tDenormalize tSetGlobalVar tHashInput tHashOutput tJavatJavarow tAggregateRow tWarn tLogCatcher tMysqlScd tFilter tGlobalmap tDie etc Extensive experience in using Talend features such as context variables triggers connectorsfor Database and flat files like tMySqlInput tMySqlConnection tOracle tMSSqlInputTMSSqlOutput tMSSqlrow tFileCopy tFileInputDelimited tFileExists Experience in using cloud components and connectors to make API calls for accessing data fromcloud storage Google Drive Salesforce Amazon S3 DropBox inTalend Open Studio Experience in creatingJoblets in TALEND for the processes which can be used in most of the jobsin a project like to Start job and Commit job Experience in monitoring and scheduling using AutoSys Control M Job Conductor TalendAdmin Console and using UNIX Korn Bourn Shell Scripting Expertise in creating sub jobs in parallel to maximize the performance and reduce overall jobexecution time with the use of parallelize component of Talend in TIS and using the MultithreadedExecutions in TOS Experienced in creating Triggers on TAC server to schedule Talend jobs to run on server Strong experience in Extraction Transformation loading ETL data from various sources into Data Warehouses and Data Marts using DataStageversions 91 87 81 Designed jobs in DataStage 9187 using stages like Transformer sequential Aggregator Data Set File Set Remove Duplicates Sort Join Merge Lookup Funnel Copy Modify Filter Change Data Capture Surrogate Key External Source External Target Compare and Schedule jobs through UNIX shell scripts Strong hands on experience using Teradata utilities like SQL Assistant BTEQ FastLoadMultiLoad Fast Export and TPUMP Authorized to work in the US for any employer Work Experience Sr ETL Talend Developer Northern Trust Chicago IL October 2016 to Present Responsibilities Worked with Data mapping team to understand the source to target mapping rules Analyzed the requirements and framed the business logic and implemented it using Talend Involved in ETL design and documentation Analyzed and performed data integration using Talend open integration suite Worked on the design development and testing of Talend mappings Created ETL job infrastructure using Talend Open Studio Load and transform data into HDFS from large set of structured data OracleSql server using Talend Big data studio Used Big Data components Hive components for extracting data from hive sources Wrote HiveQL queries using joins and implemented in tHiveInput component Utilized Big Data components like tHiveInput tHiveOutput tHDFSOutput tHiveRow tHiveLoad tHiveConnection tOracleInput tOracleOutput tPreJob tPostJob tLogRow Worked on Talend components like tReplace tmap tsort and tFilterColumn tFilterRow tJava tjavarow tConvertType etc Used Database components like tMSSQLInput tMsSqlRow tMsSqlOutput tOracleOutput tOracleInput etc Worked with various File components like tFileCopy tFileCompare tFileExist tFileDelete tFileRename Worked on improving the performance of Talend jobs Created triggers for a Talend job to run automatically on server Worked on Exporting and Importing of Talend jobs Created jobs to pass parameters from child job to parent job Exported jobs to Nexus and SVN repository Implemented update strategy on tables and used tJava tJavarow components to read data from tables to pull only newly inserted data from source tables Observed statistics of Talend jobs in AMC to improve the performance and in what scenarios errors are causing Created Generic and Repository schemas Developed project specific Deployment job responsible to deploy Talend jar files on to the windowsenvironment as a zip file later this zip file is unzipped and the files are again deployed to the UNIX box Also this deployment job is responsible to maintain versioning of the Talend jobs that are deployed in the UNIX environment Developed shell scripts in UNIX environment to support scheduling of the Talend jobs Monitored the daily runs weekly runs and adhoc runs to load data into the target systems Environment Talend 6361 Oracle IBM DB2 Teradata HDFS Hive Impala SQL PLSQL HP ALM JIRA Sr ETLTalend Developer IHG Atlanta GA June 2015 to September 2016 Responsibilities Worked in the Data Integration Team to perform data and application integration with a goal of moving more data more effectively efficiently and with high performance to assist in businesscritical projects coming up with huge data extraction Perform technical analysis ETL design development testing and deployment of IT solutions as needed by business or IT Participate in designing the overall logical physical Data warehouseDatamart data model and data architectures to support business requirements Explore prebuilt ETL metadata mappings and DAC metadata and Develop and maintain SQL code as needed for SQL Server database Performed data manipulations using various Talend components like tMap tJavarow tjava tOracleRow tOracleInput tOracleOutput tMSSQLInput and many more Analyzing the source data to know the quality of data by using Talend Data Quality Troubleshoot data integration issues and bugs analyze reasons for failure implement optimal solutions and revise procedures and documentation as needed Worked on Migration projects to migrate data from data warehouses on OracleDB2 and migrated those toNetezza Used SQL queries and other data analysis methods as well as Talend Enterprise Data Quality Platform for profiling and comparison of data which will be used to make decisions regarding how to measure business rules and quality of the data Writing Netezza SQL queries to join or any modifications in the table Used Talend reusable components like routines context variable and globalMap variables Responsible to tune ETL mappings Workflows and underlying data model to optimize load and Query performance Developed Talend ESB services and deployed them on ESB servers on different instances Implementing fast and efficient data acquisition using Big Data processing techniques and tools Monitored and supported the Talend jobs scheduled through Talend Admin Center TAC Developed Oracle PLSQL DDLs and Stored Procedures and worked on performance and fine Tuning of SQL Environment Talend 61 552 UNIX Shell script SQL Server Oracle Business Objects ERwin SVN DataStageTalendETL Developer Navistar Inc Chicago IL February 2013 to May 2015 Responsibilities Work with the Business team to better understand the requirements and determine the appropriate data source and conversion approach Developed DataStage Jobs define job parameters range lookups Extensively used the CDC Change Data Capture stage to implement the slowly changing Dimensional and Fact tables Used stages like Transformer sequential Aggregator Data Set File Set Remove Duplicates Sort Join Merge Lookup Funnel Copy Modify Filter Change Data Capture Surrogate Key External Source External Target Compare and Schedule jobs through UNIX shell scripts Assist the functional teams by insuring good functional requirements exist converting resulting functional specs into technical specs for data integration Created the DataStage jobs to load data from ECOM database to ODS to Business Intelligence layer Performed performance tuning of the jobs by interpreting performance statistics Create DataStage components efficiently and made them reusable Designing XML schema definitions XSDs extract and load to huge XML files Involved in loading data from Oracle database to Teradata tables using DataStage Design complex DataStage jobs with Java DB2 extractions SCD functionality and bulk loads Created Jobs using different stages like Aggregators Joins Merge Lookup Source dataset Row generator Change Capture Peak stages Column generator Oracle Teradata connectors Involved in design and development of complex enterprise parallel jobs to extract data into Oracle Teradata and flat files EnvironmentIBM InfoSphere DataStage 1 XML Oracle Teradata SharePoint UNIX HDFS JavaJ2EE Programmer State Farm Insurance Bloomington IL July 2011 to January 2013 State Farms Existing plus is the platform for Data Access Web Services Production Solutions Data team providessupports web services for Agreement related tables The data inserted updated deleted or retrieved in to the tables from these services resides on Existing Plus such as DB2 Z This data can be moved to Technical Platform through our data movement processes but onplatform will only be available for reading These services are consumed by State Farms Clients like Auto Fire Health Life and Bank etc Responsibilities Developing J2EE web services involving all the stages of Software Development Life Cycle Involve in the requirements gathering Design Development Unit testing and Bug fixing Use Design Patterns like MVC Business Delegate Service Locator Session Facade and DAO Developed the functionalities using Agile Methodology Involved in writing Thread Safe blocks for multithread access to make valid transactions Developed and implemented responsive web pages using JSF Primefaces while maintaining high usability standards Created and injected Spring services Spring boot Spring controllers and DAOs to achieve dependency injection and to wire objects of business classes Used Spring Inheritance to develop beans from already developed parent beans Develop and Implement interface for SOAP Web Services using JAXWS framework involving Global Transactions Used AJAX extensively to implement front end user interface features in the application Developed Web Services clients to consume those Web Services as well other enterprise wide Web Services Use IBM DB2 for creating databases and performing DB2 operations on the tables Used IBM Pure Query for generating JDBC connection beans that can be used for connecting DB2 database Database development required creation of new tables inserting data into the tables updating data in the tables and deleting data from the tables and required SQL tuning to reduce the response time in the application Deployed the application on IBM Web Sphere Application Server 85 Worked closely with QA team and fixed QA bugs as well as production issues with a quick turnaround time Used Splunk to get the Testing Preproductions and Production logs Focused on Test Driven Development thereby creating detailed JUnit tests for every single piece of functionality before writing the functionality Used Apache Maven for project management and building the application SVN being used for project management and version management Environment J2EE Java 17 Spring framework Hibernate JSP 20 JSR303 JDBC AJAX JAXWS Web services SOAP XML Java Beans Apache Axis2 JQuery JavaScript AngularJS IBM DB2 IBM Pure Query Java Developer SilverCloud Technologies Hyderabad Telangana May 2009 to June 2011 SilverCloud Technologieshas been one of the pioneering members of the book2net fraternity An endeavor to integrate services and products giving clients complete turnkey solutions for their digitization needs Responsibilities Worked on both WebLogic Portal 92 for Portal development and WebLogic 81 for Data Services Programming Developed the presentation layer using JSP HTML CSS and client validations using JavaScript Used GWT to send AJAX request to the server and updating data in the UI dynamically Developed Hibernate 30 in Data Access Layer to access and update information in the database Used JDBC SQL and PLSQL programming for storing retrieving manipulating the data Involved in designing and development of the ecommerce site using JSP Servlet EJBs JavaScript and JDBC Used Eclipse 60 as IDE for application development Configured Struts framework to implement MVC design patterns Validated all forms using Struts validation framework and implemented Tiles framework in the presentation layer Designed and developed GUI using JSP HTML DHTML and CSS Worked with JMS for messaging interface Used Hibernate for handling database transactions and persisting objects deployed the entire project on WebLogic application server Used AJAX for interactive user operations and client side validations Used XSL transforms on certain XML data Used XML for ORM mapping relations with the java classes and the database Developed ANT script for compiling and deployment Performed unit testing using Junit Environment JavaJ2EE Oracle 10 g SQL PLSQL JSP EJB Struts Hibernate WebLogic 80 HTML AJAX Java Script JDBC XML JMS XSLT UML JUnit Log4j Eclipse 60 Education Bachelors Skills DB2 6 years SQL 6 years XML 6 years DATA INTEGRATION 5 years INTEGRATION 5 years Additional Information TECHNICAL SKILLS BigData HDFS Hive Pig Spark HBase Data Warehousing Talend Open Studio TOS for Data Integration 63 InfoSphere DataStage 91 87 Databases Netezza Teradata utilities BTEQ FASTLOAD FASTEXPORT MULTILOAD TRUMPOracle12c11x10 g DB2 Microsoft SQL Server Hive Impala Sybase Programming TSQL PLSQL HTML XML Environment Windows UNIX   Linux Scripting Korn shell script Windows batch scripting JavaScript Languages SQL Developer AginityWorkBench Teradata SQL Assistant SQLPlus Toad OtherTools SQL Navigator Putty MSOffice VMWare Workstation",
    "extracted_keywords": [
        "Sr",
        "ETL",
        "Talend",
        "Developer",
        "Sr",
        "ETL",
        "Talend",
        "span",
        "lDeveloperspan",
        "Sr",
        "ETL",
        "Talend",
        "Developer",
        "Northern",
        "Trust",
        "Richmond",
        "VA",
        "years",
        "experience",
        "IT",
        "Industry",
        "Software",
        "Analysis",
        "Design",
        "ImplementationCoding",
        "Development",
        "Testing",
        "Maintenancewith",
        "Data",
        "warehousing",
        "ETL",
        "tools",
        "Talend63",
        "andIBM",
        "InfoSphere",
        "DataStage",
        "years",
        "experience",
        "Talend",
        "Integration",
        "Suite",
        "6x5x",
        "Talend",
        "Open",
        "Studio",
        "6x5x",
        "years",
        "experience",
        "Talend",
        "Admin",
        "Console",
        "TAC",
        "Experience",
        "Data",
        "Warehousing",
        "Concepts",
        "KimballInmon",
        "OLTP",
        "Star",
        "Schema",
        "Snow",
        "Flake",
        "Schema",
        "LogicalPhysical",
        "Dimensional",
        "Data",
        "Modeling",
        "Horton",
        "distribution",
        "Hadoop",
        "HDFS",
        "MapReduce",
        "Hive",
        "Impala",
        "Sqoop",
        "Flume",
        "Pig",
        "HBase",
        "Experience",
        "data",
        "HDFS",
        "depth",
        "understanding",
        "Gap",
        "Analysis",
        "AsIs",
        "ToBe",
        "business",
        "processes",
        "requirements",
        "Technical",
        "Specifications",
        "Plans",
        "Proficient",
        "Agile",
        "Test",
        "Iterative",
        "Scrum",
        "Waterfall",
        "software",
        "development",
        "lifecycle",
        "ETL",
        "methodology",
        "Data",
        "Profiling",
        "Data",
        "Migration",
        "ExtractionTransformation",
        "Loading",
        "Talend",
        "data",
        "conversions",
        "variety",
        "ofsource",
        "systems",
        "Netezza",
        "Oracle",
        "DB2",
        "SQL",
        "server",
        "Teradata",
        "Hive",
        "sources",
        "files",
        "XML",
        "Mainframe",
        "Files",
        "Experience",
        "data",
        "HiveQL",
        "Pig",
        "Latin",
        "HDFS",
        "data",
        "sources",
        "staging",
        "area",
        "Data",
        "warehouse",
        "DataMarts",
        "SCDs",
        "Type",
        "1Type",
        "Type",
        "loads",
        "mappings",
        "TALEND",
        "tMap",
        "tJoin",
        "tReplicate",
        "tParallelizetConvertType",
        "tAggregate",
        "tSortRowtFlowMetertLogCatchertRowGenerator",
        "tNormalize",
        "tDenormalize",
        "tSetGlobalVar",
        "tHashInput",
        "tHashOutput",
        "tAggregateRow",
        "tWarn",
        "tLogCatcher",
        "tMysqlScd",
        "tFilter",
        "tGlobalmap",
        "tDie",
        "experience",
        "Talend",
        "features",
        "context",
        "variables",
        "Database",
        "files",
        "tMySqlConnection",
        "tOracle",
        "tMSSqlInputTMSSqlOutput",
        "tMSSqlrow",
        "tFileCopy",
        "tFileExists",
        "Experience",
        "cloud",
        "components",
        "connectors",
        "API",
        "data",
        "fromcloud",
        "storage",
        "Google",
        "Drive",
        "Salesforce",
        "Amazon",
        "S3",
        "DropBox",
        "Open",
        "Studio",
        "Experience",
        "creatingJoblets",
        "TALEND",
        "processes",
        "jobsin",
        "project",
        "job",
        "job",
        "Experience",
        "monitoring",
        "scheduling",
        "AutoSys",
        "Control",
        "M",
        "Job",
        "Conductor",
        "TalendAdmin",
        "Console",
        "UNIX",
        "Korn",
        "Bourn",
        "Shell",
        "Scripting",
        "Expertise",
        "sub",
        "jobs",
        "parallel",
        "performance",
        "jobexecution",
        "time",
        "use",
        "parallelize",
        "component",
        "Talend",
        "TIS",
        "MultithreadedExecutions",
        "TOS",
        "Triggers",
        "TAC",
        "server",
        "Talend",
        "jobs",
        "server",
        "experience",
        "Extraction",
        "Transformation",
        "loading",
        "ETL",
        "data",
        "sources",
        "Data",
        "Warehouses",
        "Data",
        "Marts",
        "DataStageversions",
        "jobs",
        "DataStage",
        "stages",
        "Transformer",
        "Aggregator",
        "Data",
        "Set",
        "File",
        "Set",
        "Remove",
        "Duplicates",
        "Sort",
        "Join",
        "Merge",
        "Lookup",
        "Funnel",
        "Copy",
        "Modify",
        "Filter",
        "Change",
        "Data",
        "Capture",
        "Surrogate",
        "Key",
        "External",
        "Source",
        "External",
        "Target",
        "Compare",
        "Schedule",
        "jobs",
        "UNIX",
        "shell",
        "hands",
        "experience",
        "Teradata",
        "utilities",
        "SQL",
        "Assistant",
        "BTEQ",
        "FastLoadMultiLoad",
        "Fast",
        "Export",
        "TPUMP",
        "Authorized",
        "US",
        "employer",
        "Work",
        "Experience",
        "Sr",
        "ETL",
        "Talend",
        "Developer",
        "Northern",
        "Trust",
        "Chicago",
        "IL",
        "October",
        "Present",
        "Responsibilities",
        "Data",
        "mapping",
        "team",
        "source",
        "mapping",
        "rules",
        "requirements",
        "business",
        "logic",
        "Talend",
        "ETL",
        "design",
        "documentation",
        "data",
        "integration",
        "Talend",
        "integration",
        "suite",
        "design",
        "development",
        "testing",
        "mappings",
        "ETL",
        "job",
        "infrastructure",
        "Talend",
        "Open",
        "Studio",
        "Load",
        "data",
        "HDFS",
        "set",
        "data",
        "OracleSql",
        "server",
        "data",
        "studio",
        "Big",
        "Data",
        "components",
        "Hive",
        "components",
        "data",
        "hive",
        "sources",
        "Wrote",
        "HiveQL",
        "queries",
        "joins",
        "tHiveInput",
        "component",
        "Big",
        "Data",
        "components",
        "tHiveInput",
        "tHiveOutput",
        "tHDFSOutput",
        "tHiveRow",
        "tHiveLoad",
        "tHiveConnection",
        "tOracleInput",
        "tOracleOutput",
        "tPreJob",
        "tPostJob",
        "tLogRow",
        "Talend",
        "components",
        "tReplace",
        "tmap",
        "tsort",
        "tFilterColumn",
        "tJava",
        "tjavarow",
        "tConvertType",
        "Database",
        "components",
        "tMSSQLInput",
        "tMsSqlRow",
        "tOracleOutput",
        "tOracleInput",
        "File",
        "components",
        "tFileCopy",
        "tFileCompare",
        "tFileExist",
        "tFileDelete",
        "tFileRename",
        "performance",
        "jobs",
        "triggers",
        "Talend",
        "job",
        "server",
        "Exporting",
        "Importing",
        "Talend",
        "jobs",
        "jobs",
        "parameters",
        "child",
        "job",
        "parent",
        "job",
        "jobs",
        "Nexus",
        "SVN",
        "repository",
        "update",
        "strategy",
        "tables",
        "tJava",
        "tJavarow",
        "components",
        "data",
        "tables",
        "data",
        "source",
        "statistics",
        "jobs",
        "AMC",
        "performance",
        "scenarios",
        "errors",
        "Generic",
        "Repository",
        "schemas",
        "project",
        "Deployment",
        "job",
        "jar",
        "files",
        "windowsenvironment",
        "zip",
        "file",
        "zip",
        "file",
        "files",
        "UNIX",
        "box",
        "deployment",
        "job",
        "versioning",
        "Talend",
        "jobs",
        "UNIX",
        "environment",
        "shell",
        "scripts",
        "UNIX",
        "environment",
        "scheduling",
        "Talend",
        "jobs",
        "runs",
        "adhoc",
        "runs",
        "data",
        "target",
        "systems",
        "Environment",
        "Talend",
        "Oracle",
        "IBM",
        "DB2",
        "Teradata",
        "HDFS",
        "Hive",
        "Impala",
        "SQL",
        "PLSQL",
        "HP",
        "ALM",
        "JIRA",
        "Sr",
        "ETLTalend",
        "Developer",
        "IHG",
        "Atlanta",
        "GA",
        "June",
        "September",
        "Responsibilities",
        "Data",
        "Integration",
        "Team",
        "data",
        "application",
        "integration",
        "goal",
        "data",
        "performance",
        "projects",
        "data",
        "extraction",
        "Perform",
        "analysis",
        "ETL",
        "design",
        "development",
        "testing",
        "deployment",
        "IT",
        "solutions",
        "business",
        "IT",
        "Data",
        "warehouseDatamart",
        "data",
        "model",
        "data",
        "business",
        "requirements",
        "prebuilt",
        "ETL",
        "metadata",
        "mappings",
        "DAC",
        "metadata",
        "SQL",
        "code",
        "SQL",
        "Server",
        "database",
        "data",
        "manipulations",
        "Talend",
        "components",
        "tMap",
        "tJavarow",
        "tjava",
        "tOracleRow",
        "tOracleInput",
        "tOracleOutput",
        "tMSSQLInput",
        "source",
        "data",
        "quality",
        "data",
        "Talend",
        "Data",
        "Quality",
        "Troubleshoot",
        "data",
        "integration",
        "issues",
        "bugs",
        "reasons",
        "failure",
        "solutions",
        "procedures",
        "documentation",
        "Worked",
        "Migration",
        "projects",
        "data",
        "data",
        "warehouses",
        "OracleDB2",
        "SQL",
        "queries",
        "data",
        "analysis",
        "methods",
        "Talend",
        "Enterprise",
        "Data",
        "Quality",
        "Platform",
        "profiling",
        "comparison",
        "data",
        "decisions",
        "business",
        "rules",
        "quality",
        "data",
        "Netezza",
        "SQL",
        "modifications",
        "table",
        "Talend",
        "components",
        "routines",
        "globalMap",
        "variables",
        "tune",
        "ETL",
        "mappings",
        "Workflows",
        "data",
        "model",
        "load",
        "Query",
        "performance",
        "Developed",
        "Talend",
        "ESB",
        "services",
        "ESB",
        "servers",
        "instances",
        "data",
        "acquisition",
        "Big",
        "Data",
        "processing",
        "techniques",
        "tools",
        "Talend",
        "jobs",
        "Talend",
        "Admin",
        "Center",
        "TAC",
        "Developed",
        "Oracle",
        "PLSQL",
        "DDLs",
        "Procedures",
        "performance",
        "Tuning",
        "SQL",
        "Environment",
        "Talend",
        "UNIX",
        "Shell",
        "script",
        "SQL",
        "Server",
        "Oracle",
        "Business",
        "ERwin",
        "SVN",
        "Developer",
        "Navistar",
        "Inc",
        "Chicago",
        "IL",
        "February",
        "May",
        "Responsibilities",
        "Business",
        "team",
        "requirements",
        "data",
        "source",
        "conversion",
        "approach",
        "DataStage",
        "Jobs",
        "job",
        "parameters",
        "lookups",
        "CDC",
        "Change",
        "Data",
        "Capture",
        "stage",
        "Fact",
        "stages",
        "Transformer",
        "Aggregator",
        "Data",
        "Set",
        "File",
        "Set",
        "Remove",
        "Duplicates",
        "Sort",
        "Join",
        "Merge",
        "Lookup",
        "Funnel",
        "Copy",
        "Modify",
        "Filter",
        "Change",
        "Data",
        "Capture",
        "Surrogate",
        "Key",
        "External",
        "Source",
        "External",
        "Target",
        "Compare",
        "Schedule",
        "jobs",
        "UNIX",
        "shell",
        "scripts",
        "teams",
        "requirements",
        "specs",
        "specs",
        "data",
        "integration",
        "DataStage",
        "jobs",
        "data",
        "ECOM",
        "database",
        "ODS",
        "Business",
        "Intelligence",
        "layer",
        "performance",
        "tuning",
        "jobs",
        "performance",
        "statistics",
        "DataStage",
        "components",
        "Designing",
        "XML",
        "schema",
        "definitions",
        "XSDs",
        "load",
        "XML",
        "files",
        "loading",
        "data",
        "Oracle",
        "database",
        "Teradata",
        "tables",
        "DataStage",
        "Design",
        "DataStage",
        "jobs",
        "Java",
        "DB2",
        "SCD",
        "functionality",
        "loads",
        "Created",
        "Jobs",
        "stages",
        "Aggregators",
        "Joins",
        "Merge",
        "Lookup",
        "Source",
        "Row",
        "generator",
        "Change",
        "Capture",
        "Peak",
        "Column",
        "generator",
        "Oracle",
        "Teradata",
        "connectors",
        "design",
        "development",
        "enterprise",
        "jobs",
        "data",
        "Oracle",
        "Teradata",
        "files",
        "EnvironmentIBM",
        "InfoSphere",
        "DataStage",
        "XML",
        "Oracle",
        "Teradata",
        "SharePoint",
        "UNIX",
        "HDFS",
        "JavaJ2EE",
        "Programmer",
        "State",
        "Farm",
        "Insurance",
        "Bloomington",
        "IL",
        "July",
        "January",
        "State",
        "Farms",
        "platform",
        "Data",
        "Access",
        "Web",
        "Services",
        "Production",
        "Solutions",
        "Data",
        "team",
        "web",
        "services",
        "Agreement",
        "tables",
        "data",
        "tables",
        "services",
        "Existing",
        "DB2",
        "Z",
        "data",
        "Technical",
        "Platform",
        "data",
        "movement",
        "processes",
        "onplatform",
        "services",
        "State",
        "Farms",
        "Clients",
        "Auto",
        "Fire",
        "Health",
        "Life",
        "Bank",
        "Responsibilities",
        "J2EE",
        "web",
        "services",
        "stages",
        "Software",
        "Development",
        "Life",
        "Cycle",
        "Involve",
        "requirements",
        "Design",
        "Development",
        "Unit",
        "testing",
        "Bug",
        "Use",
        "Design",
        "Patterns",
        "MVC",
        "Business",
        "Delegate",
        "Service",
        "Locator",
        "Session",
        "Facade",
        "DAO",
        "functionalities",
        "Agile",
        "Methodology",
        "Thread",
        "Safe",
        "blocks",
        "multithread",
        "access",
        "transactions",
        "web",
        "pages",
        "JSF",
        "Primefaces",
        "usability",
        "standards",
        "Spring",
        "services",
        "Spring",
        "boot",
        "Spring",
        "controllers",
        "DAOs",
        "dependency",
        "injection",
        "wire",
        "objects",
        "business",
        "classes",
        "Spring",
        "Inheritance",
        "beans",
        "parent",
        "beans",
        "Develop",
        "Implement",
        "interface",
        "SOAP",
        "Web",
        "Services",
        "JAXWS",
        "framework",
        "Global",
        "Transactions",
        "AJAX",
        "end",
        "user",
        "interface",
        "features",
        "application",
        "Developed",
        "Web",
        "Services",
        "clients",
        "Web",
        "Services",
        "enterprise",
        "Web",
        "Services",
        "Use",
        "IBM",
        "DB2",
        "databases",
        "DB2",
        "operations",
        "tables",
        "IBM",
        "Pure",
        "Query",
        "JDBC",
        "connection",
        "beans",
        "DB2",
        "database",
        "Database",
        "development",
        "creation",
        "tables",
        "data",
        "tables",
        "data",
        "tables",
        "data",
        "tables",
        "SQL",
        "response",
        "time",
        "application",
        "application",
        "IBM",
        "Web",
        "Sphere",
        "Application",
        "Server",
        "QA",
        "team",
        "QA",
        "bugs",
        "production",
        "issues",
        "turnaround",
        "time",
        "Splunk",
        "Testing",
        "Preproductions",
        "Production",
        "logs",
        "Test",
        "Driven",
        "Development",
        "JUnit",
        "tests",
        "piece",
        "functionality",
        "functionality",
        "Apache",
        "Maven",
        "project",
        "management",
        "application",
        "SVN",
        "project",
        "management",
        "version",
        "management",
        "Environment",
        "J2EE",
        "Java",
        "Spring",
        "framework",
        "Hibernate",
        "JSP",
        "JSR303",
        "JDBC",
        "AJAX",
        "JAXWS",
        "Web",
        "services",
        "XML",
        "Java",
        "Beans",
        "Apache",
        "JQuery",
        "JavaScript",
        "IBM",
        "DB2",
        "IBM",
        "Pure",
        "Query",
        "Java",
        "Developer",
        "SilverCloud",
        "Technologies",
        "Hyderabad",
        "Telangana",
        "May",
        "June",
        "SilverCloud",
        "Technologieshas",
        "members",
        "book2net",
        "fraternity",
        "endeavor",
        "services",
        "products",
        "clients",
        "turnkey",
        "solutions",
        "digitization",
        "Responsibilities",
        "WebLogic",
        "Portal",
        "Portal",
        "development",
        "WebLogic",
        "Data",
        "Services",
        "Programming",
        "presentation",
        "layer",
        "JSP",
        "HTML",
        "CSS",
        "client",
        "validations",
        "JavaScript",
        "GWT",
        "AJAX",
        "request",
        "server",
        "data",
        "UI",
        "Hibernate",
        "Data",
        "Access",
        "Layer",
        "information",
        "database",
        "JDBC",
        "SQL",
        "PLSQL",
        "programming",
        "retrieving",
        "data",
        "designing",
        "development",
        "ecommerce",
        "site",
        "JSP",
        "Servlet",
        "EJBs",
        "JavaScript",
        "JDBC",
        "Eclipse",
        "IDE",
        "application",
        "development",
        "Configured",
        "Struts",
        "framework",
        "MVC",
        "design",
        "patterns",
        "forms",
        "Struts",
        "validation",
        "framework",
        "Tiles",
        "framework",
        "presentation",
        "layer",
        "GUI",
        "JSP",
        "HTML",
        "DHTML",
        "CSS",
        "JMS",
        "interface",
        "Hibernate",
        "database",
        "transactions",
        "objects",
        "project",
        "WebLogic",
        "application",
        "server",
        "AJAX",
        "user",
        "operations",
        "client",
        "side",
        "XSL",
        "XML",
        "data",
        "XML",
        "ORM",
        "mapping",
        "relations",
        "classes",
        "database",
        "ANT",
        "script",
        "deployment",
        "Performed",
        "unit",
        "testing",
        "Junit",
        "Environment",
        "JavaJ2EE",
        "Oracle",
        "g",
        "SQL",
        "PLSQL",
        "JSP",
        "EJB",
        "Struts",
        "Hibernate",
        "WebLogic",
        "HTML",
        "AJAX",
        "Java",
        "Script",
        "JDBC",
        "XML",
        "JMS",
        "UML",
        "JUnit",
        "Log4j",
        "Eclipse",
        "Education",
        "Bachelors",
        "Skills",
        "DB2",
        "years",
        "SQL",
        "years",
        "XML",
        "years",
        "DATA",
        "INTEGRATION",
        "years",
        "INTEGRATION",
        "years",
        "Information",
        "TECHNICAL",
        "SKILLS",
        "BigData",
        "HDFS",
        "Hive",
        "Pig",
        "Spark",
        "HBase",
        "Data",
        "Warehousing",
        "Talend",
        "Open",
        "Studio",
        "TOS",
        "Data",
        "Integration",
        "InfoSphere",
        "DataStage",
        "Netezza",
        "Teradata",
        "utilities",
        "BTEQ",
        "FASTLOAD",
        "FASTEXPORT",
        "MULTILOAD",
        "TRUMPOracle12c11x10",
        "g",
        "DB2",
        "Microsoft",
        "SQL",
        "Server",
        "Hive",
        "Impala",
        "Sybase",
        "Programming",
        "TSQL",
        "PLSQL",
        "HTML",
        "XML",
        "Environment",
        "UNIX",
        "Linux",
        "Scripting",
        "Korn",
        "shell",
        "script",
        "Windows",
        "batch",
        "JavaScript",
        "Languages",
        "SQL",
        "Developer",
        "AginityWorkBench",
        "Teradata",
        "SQL",
        "Assistant",
        "SQLPlus",
        "Toad",
        "OtherTools",
        "SQL",
        "Navigator",
        "Putty",
        "MSOffice",
        "VMWare",
        "Workstation"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T22:20:18.244810",
    "resume_data": "Sr ETL Talend Developer Sr ETL Talend span lDeveloperspan Sr ETL Talend Developer Northern Trust Richmond VA 8 years of experience in IT Industry involving Software Analysis Design ImplementationCoding Development Testing and Maintenancewith focus on Data warehousing applicationsusing ETL tools like Talend63 andIBM InfoSphere DataStage 91 3 years of experience using Talend Integration Suite 6x5x Talend Open Studio 6x5x and 2 years of experience with Talend Admin Console TAC Experience working with Data Warehousing Concepts like KimballInmon methodologiesOLAP OLTP Star Schema Snow Flake Schema LogicalPhysical Dimensional Data Modeling Experienced in working with Horton works distribution of Hadoop HDFS MapReduce Hive Impala Sqoop Flume Pig HBase and MongoDB Experience in dealing with structured and semistructured data in HDFS In depth understanding of the Gap Analysis ie AsIs and ToBe business processes andexperience in converting these requirements into Technical Specifications andTest Plans Highly Proficient in Agile Test Driven Iterative Scrum and Waterfall software development lifecycle Extensively used ETL methodology for performing Data Profiling Data Migration ExtractionTransformation and Loading using Talend and designed data conversions from wide variety ofsource systems including Netezza Oracle DB2 SQL server Teradata Hive nonrelational sources like flat files XML and Mainframe Files Experience in analyzing data using HiveQL and Pig Latin in HDFS Extracted data from multiple operational sources for loading staging area Data warehouse DataMarts using SCDs Type 1Type 2 Type 3 loads Extensively created mappings in TALEND using tMap tJoin tReplicate tParallelizetConvertType tflowtoIterate tAggregate tSortRowtFlowMetertLogCatchertRowGenerator tNormalize tDenormalize tSetGlobalVar tHashInput tHashOutput tJavatJavarow tAggregateRow tWarn tLogCatcher tMysqlScd tFilter tGlobalmap tDie etc Extensive experience in using Talend features such as context variables triggers connectorsfor Database and flat files like tMySqlInput tMySqlConnection tOracle tMSSqlInputTMSSqlOutput tMSSqlrow tFileCopy tFileInputDelimited tFileExists Experience in using cloud components and connectors to make API calls for accessing data fromcloud storage Google Drive Salesforce Amazon S3 DropBox inTalend Open Studio Experience in creatingJoblets in TALEND for the processes which can be used in most of the jobsin a project like to Start job and Commit job Experience in monitoring and scheduling using AutoSys Control M Job Conductor TalendAdmin Console and using UNIX Korn Bourn Shell Scripting Expertise in creating sub jobs in parallel to maximize the performance and reduce overall jobexecution time with the use of parallelize component of Talend in TIS and using the MultithreadedExecutions in TOS Experienced in creating Triggers on TAC server to schedule Talend jobs to run on server Strong experience in Extraction Transformation loading ETL data from various sources into Data Warehouses and Data Marts using DataStageversions 91 87 81 Designed jobs in DataStage 9187 using stages like Transformer sequential Aggregator Data Set File Set Remove Duplicates Sort Join Merge Lookup Funnel Copy Modify Filter Change Data Capture Surrogate Key External Source External Target Compare and Schedule jobs through UNIX shell scripts Strong hands on experience using Teradata utilities like SQL Assistant BTEQ FastLoadMultiLoad Fast Export and TPUMP Authorized to work in the US for any employer Work Experience Sr ETL Talend Developer Northern Trust Chicago IL October 2016 to Present Responsibilities Worked with Data mapping team to understand the source to target mapping rules Analyzed the requirements and framed the business logic and implemented it using Talend Involved in ETL design and documentation Analyzed and performed data integration using Talend open integration suite Worked on the design development and testing of Talend mappings Created ETL job infrastructure using Talend Open Studio Load and transform data into HDFS from large set of structured data OracleSql server using Talend Big data studio Used Big Data components Hive components for extracting data from hive sources Wrote HiveQL queries using joins and implemented in tHiveInput component Utilized Big Data components like tHiveInput tHiveOutput tHDFSOutput tHiveRow tHiveLoad tHiveConnection tOracleInput tOracleOutput tPreJob tPostJob tLogRow Worked on Talend components like tReplace tmap tsort and tFilterColumn tFilterRow tJava tjavarow tConvertType etc Used Database components like tMSSQLInput tMsSqlRow tMsSqlOutput tOracleOutput tOracleInput etc Worked with various File components like tFileCopy tFileCompare tFileExist tFileDelete tFileRename Worked on improving the performance of Talend jobs Created triggers for a Talend job to run automatically on server Worked on Exporting and Importing of Talend jobs Created jobs to pass parameters from child job to parent job Exported jobs to Nexus and SVN repository Implemented update strategy on tables and used tJava tJavarow components to read data from tables to pull only newly inserted data from source tables Observed statistics of Talend jobs in AMC to improve the performance and in what scenarios errors are causing Created Generic and Repository schemas Developed project specific Deployment job responsible to deploy Talend jar files on to the windowsenvironment as a zip file later this zip file is unzipped and the files are again deployed to the UNIX box Also this deployment job is responsible to maintain versioning of the Talend jobs that are deployed in the UNIX environment Developed shell scripts in UNIX environment to support scheduling of the Talend jobs Monitored the daily runs weekly runs and adhoc runs to load data into the target systems Environment Talend 6361 Oracle IBM DB2 Teradata HDFS Hive Impala SQL PLSQL HP ALM JIRA Sr ETLTalend Developer IHG Atlanta GA June 2015 to September 2016 Responsibilities Worked in the Data Integration Team to perform data and application integration with a goal of moving more data more effectively efficiently and with high performance to assist in businesscritical projects coming up with huge data extraction Perform technical analysis ETL design development testing and deployment of IT solutions as needed by business or IT Participate in designing the overall logical physical Data warehouseDatamart data model and data architectures to support business requirements Explore prebuilt ETL metadata mappings and DAC metadata and Develop and maintain SQL code as needed for SQL Server database Performed data manipulations using various Talend components like tMap tJavarow tjava tOracleRow tOracleInput tOracleOutput tMSSQLInput and many more Analyzing the source data to know the quality of data by using Talend Data Quality Troubleshoot data integration issues and bugs analyze reasons for failure implement optimal solutions and revise procedures and documentation as needed Worked on Migration projects to migrate data from data warehouses on OracleDB2 and migrated those toNetezza Used SQL queries and other data analysis methods as well as Talend Enterprise Data Quality Platform for profiling and comparison of data which will be used to make decisions regarding how to measure business rules and quality of the data Writing Netezza SQL queries to join or any modifications in the table Used Talend reusable components like routines context variable and globalMap variables Responsible to tune ETL mappings Workflows and underlying data model to optimize load and Query performance Developed Talend ESB services and deployed them on ESB servers on different instances Implementing fast and efficient data acquisition using Big Data processing techniques and tools Monitored and supported the Talend jobs scheduled through Talend Admin Center TAC Developed Oracle PLSQL DDLs and Stored Procedures and worked on performance and fine Tuning of SQL Environment Talend 61 552 UNIX Shell script SQL Server Oracle Business Objects ERwin SVN DataStageTalendETL Developer Navistar Inc Chicago IL February 2013 to May 2015 Responsibilities Work with the Business team to better understand the requirements and determine the appropriate data source and conversion approach Developed DataStage Jobs define job parameters range lookups Extensively used the CDC Change Data Capture stage to implement the slowly changing Dimensional and Fact tables Used stages like Transformer sequential Aggregator Data Set File Set Remove Duplicates Sort Join Merge Lookup Funnel Copy Modify Filter Change Data Capture Surrogate Key External Source External Target Compare and Schedule jobs through UNIX shell scripts Assist the functional teams by insuring good functional requirements exist converting resulting functional specs into technical specs for data integration Created the DataStage jobs to load data from ECOM database to ODS to Business Intelligence layer Performed performance tuning of the jobs by interpreting performance statistics Create DataStage components efficiently and made them reusable Designing XML schema definitions XSDs extract and load to huge XML files Involved in loading data from Oracle database to Teradata tables using DataStage Design complex DataStage jobs with Java DB2 extractions SCD functionality and bulk loads Created Jobs using different stages like Aggregators Joins Merge Lookup Source dataset Row generator Change Capture Peak stages Column generator Oracle Teradata connectors Involved in design and development of complex enterprise parallel jobs to extract data into Oracle Teradata and flat files EnvironmentIBM InfoSphere DataStage 11391878171 XML Oracle Teradata SharePoint UNIX HDFS JavaJ2EE Programmer State Farm Insurance Bloomington IL July 2011 to January 2013 State Farms Existing plus is the platform for Data Access Web Services Production Solutions Data team providessupports web services for Agreement related tables The data inserted updated deleted or retrieved in to the tables from these services resides on Existing Plus such as DB2 Z This data can be moved to Technical Platform through our data movement processes but onplatform will only be available for reading These services are consumed by State Farms Clients like Auto Fire Health Life and Bank etc Responsibilities Developing J2EE web services involving all the stages of Software Development Life Cycle Involve in the requirements gathering Design Development Unit testing and Bug fixing Use Design Patterns like MVC Business Delegate Service Locator Session Facade and DAO Developed the functionalities using Agile Methodology Involved in writing Thread Safe blocks for multithread access to make valid transactions Developed and implemented responsive web pages using JSF Primefaces while maintaining high usability standards Created and injected Spring services Spring boot Spring controllers and DAOs to achieve dependency injection and to wire objects of business classes Used Spring Inheritance to develop beans from already developed parent beans Develop and Implement interface for SOAP Web Services using JAXWS framework involving Global Transactions Used AJAX extensively to implement front end user interface features in the application Developed Web Services clients to consume those Web Services as well other enterprise wide Web Services Use IBM DB2 for creating databases and performing DB2 operations on the tables Used IBM Pure Query for generating JDBC connection beans that can be used for connecting DB2 database Database development required creation of new tables inserting data into the tables updating data in the tables and deleting data from the tables and required SQL tuning to reduce the response time in the application Deployed the application on IBM Web Sphere Application Server 85 Worked closely with QA team and fixed QA bugs as well as production issues with a quick turnaround time Used Splunk to get the Testing Preproductions and Production logs Focused on Test Driven Development thereby creating detailed JUnit tests for every single piece of functionality before writing the functionality Used Apache Maven for project management and building the application SVN being used for project management and version management Environment J2EE Java 17 Spring framework Hibernate JSP 20 JSR303 JDBC AJAX JAXWS Web services SOAP XML Java Beans Apache Axis2 JQuery JavaScript AngularJS IBM DB2 IBM Pure Query Java Developer SilverCloud Technologies Hyderabad Telangana May 2009 to June 2011 SilverCloud Technologieshas been one of the pioneering members of the book2net fraternity An endeavor to integrate services and products giving clients complete turnkey solutions for their digitization needs Responsibilities Worked on both WebLogic Portal 92 for Portal development and WebLogic 81 for Data Services Programming Developed the presentation layer using JSP HTML CSS and client validations using JavaScript Used GWT to send AJAX request to the server and updating data in the UI dynamically Developed Hibernate 30 in Data Access Layer to access and update information in the database Used JDBC SQL and PLSQL programming for storing retrieving manipulating the data Involved in designing and development of the ecommerce site using JSP Servlet EJBs JavaScript and JDBC Used Eclipse 60 as IDE for application development Configured Struts framework to implement MVC design patterns Validated all forms using Struts validation framework and implemented Tiles framework in the presentation layer Designed and developed GUI using JSP HTML DHTML and CSS Worked with JMS for messaging interface Used Hibernate for handling database transactions and persisting objects deployed the entire project on WebLogic application server Used AJAX for interactive user operations and client side validations Used XSL transforms on certain XML data Used XML for ORM mapping relations with the java classes and the database Developed ANT script for compiling and deployment Performed unit testing using Junit Environment JavaJ2EE Oracle 10g SQL PLSQL JSP EJB Struts Hibernate WebLogic 80 HTML AJAX Java Script JDBC XML JMS XSLT UML JUnit Log4j Eclipse 60 Education Bachelors Skills DB2 6 years SQL 6 years XML 6 years DATA INTEGRATION 5 years INTEGRATION 5 years Additional Information TECHNICAL SKILLS BigData HDFS Hive Pig Spark HBase Data Warehousing Talend Open Studio TOS for Data Integration 63 InfoSphere DataStage 91 87 Databases Netezza Teradata utilities BTEQ FASTLOAD FASTEXPORT MULTILOAD TRUMPOracle12c11x10g DB2 Microsoft SQL Server Hive Impala Sybase Programming TSQL PLSQL HTML XML Environment Windows UNIX SunSolaris10HPAIX Linux Scripting Korn shell script Windows batch scripting JavaScript Languages SQL Developer AginityWorkBench Teradata SQL Assistant SQLPlus Toad OtherTools SQL Navigator Putty MSOffice VMWare Workstation",
    "unique_id": "1c42da1c-bc8f-49cb-b4c7-67f9f7aa62e5"
}