{
    "clean_data": "Hadoop Developer Hadoop span lDeveloperspan Hadoop Developer Johnson Johnson Irvine CA Cloudera certified Spark and Hadoop Developer with around 10 years of experience in software development deployment and maintenance of various web based applications using Java and Big Data Ecosystems on Windows and Linux environments Expertise in designing Hadoop applications and recommending the right solutions and technologies for the applications Expertise in major components of Hadoop ecosystems like HDFS MapReduce YARN Hive Pig HBase Zookeeper Sqoop Spark Kafka Cassandra and Impala Good knowledge and handson experience on installing configuring and maintaining multinode clusters on various environments and distributions of Hadoop Experience working in different Hadoop distributions like Cloudera 55 CDH4 CDH5 and Hortonworks distributions HDP Through knowledge in ETL Data Integration and Migration extensively used ETL methodology for supporting Data Extraction transformations and loading using Informatica Good knowledge on Statistical and quantitative analysis using tools like R Studio Handson Experience in using version control tools like CVS GITand SVN Build tools like SBT Ant and Maven Working experience on NoSQL databases like HBase MongoDB and Cassandra with functionality and implementation Involved in developing Impalascripts for extraction transformation loading of data into data warehouse Working experience on Spark ecosystems using spark components like Spark Core Spark SQL Spark Streaming MLliband GraphX Experienced in collecting metrics for Hadoop clusters using Ambari and ClouderaManager Extensive experience working with real time streaming applications and batch style large scale distributed computing applications worked on integrating Kafka with NiFi and Spark Developed reusable and configurable components as part of project requirements in Java Scala and Python Good knowledge of Scalas functional style programming techniques like Anonymous Functions Closures Currying Higher Order Functions and Pattern Matching Handson experience in training evaluating and predicting the data as a part of Machine Learning using SparkMLlib TensorFlow and a regular contributor to Machine Learning projects on GitHub Good experience in working with cloud environment like Amazon Web ServicesEC2 and S3 Hands on experience on working with Amazon EMR framework transferring data to EC2server Strong Problem Solving and Analytical skills and abilities to make Balanced Independent Decisions Good Team Player Strong Interpersonal Organizational and Communication skills combined with SelfMotivation Initiative and Project Management Attributes Holds strong ability to handle multiple priorities and work load and has ability to understand and adapt to new technologies and environments faster Work Experience Hadoop Developer Johnson Johnson Irvine CA July 2018 to Present Responsibilities Worked on analyzing Hadoop cluster using different big data analytic tools including Pig Hive and MapReduce Managing fully distributed Hadoop cluster is an additional responsibility assigned to me I was trained to overtake the responsibilities of A Hadoop Administrator which includes managing the cluster Upgrades and installation of tools that uses Hadoop ecosystem Worked on Installation and configuring of Zoo Keeper to coordinate and monitor the cluster resources Implemented test scripts to support testdriven development and continuous integration Worked on POCs with Apache Spark using Scala to implement spark in project Consumed the data from Kafka using Apache spark Load and transform large sets of structured semi structured and unstructured data Involved in loading data from LINUX filesystem to HDFS Importing and exporting data into HDFS and Hive using Sqoop Implemented Partitioning Dynamic Partitions Buckets in Hive Worked in creating HBase tables to load large sets of semistructured data coming from various sources Extending HIVE and PIG core functionality by using custom User Defined Functions UDF User Defined TableGenerating Functions UDTF and User DefinedAggregating Functions UDAF for Hive and Pig using python Experienced in running Hadoop streaming jobs to process terabytes of xml format data Involved in scheduling Oozie workflow engine to run multiple Hive and pig jobs Experienced with performing CURD operations in HBase Developed workflow in Oozie to automate the tasks of loading the data into HDFS and preprocessing with Pig Responsible for loading data les from various external sources like ORACLE MySQL into staging area in MySQL databases Executed Hive queries on Parquet tables stored in Hive to perform data analysis to meet the business requirements Actively involved in code review and bug fixing for improving the performance Good experience in handling data manipulation using python Scripts Involved in development building testing and deploy to Hadoop cluster in distributed mode Created Linux shell Scripts to automate the daily ingestion of IVR data Executed Hive queries on Parquet tables stored in Hive to perform data analysis to meet the business requirements Helped the Analytics team with Aster queries using HCatlog Automated the History and Purge Process Created HBase tables to store various data formats of incoming data from different portfolios Created Pig Latin scripts to sort group join and filter the enterprise wise data Developed the verification and control process for daily load Experience in Daily production support to monitor and trouble shoots HadoopHive jobs Environment Hive SQL Pig Flume Kafka Map reduce SQOOP Spark Python Java Shell Scripting Teradata Oracle Oozie Cassandra HadoopSpark Developer Citi Bank Tampa FL January 2016 to June 2018 This project is to capture the large currency transactions made on the various channels of the current system All cash transactions made by nonexempt customers above the threshold amount are reported to The Financial Crimes Enforcement Network FinCEN by submitting the Cash Transaction Report CTR ECV gets all the transactions details from various upstream data source systems into Hadoop as a Service HAAS environment and filters only the cash transactions These transactions are further curated with ATM and Teller OSI data and enriched with user profile data This data will be stored in HBase for auto generating CTRs Responsibilities Evaluated business requirements and prepared Detailed Design documents that follows Project guidelines and SLAs required procuring data from all the upstream data sources and developing written programs Data files are retrieved by various data transmission protocols like Sqoop NDM SFTP DMS etc these data files are then validated by various Spark Control jobs written in Scala Spark RDDs are created for all the data files and then transformed to cash only transaction RDDs The filtered cash only RDDs are aggregated and curated based on the business rules and CTR requirements converted into data frames and saved as temporary hive tables for intermediate processing The RDDs and data frames undergo various transformations and actions and are stored in HDFS as parquet Files and in HBase for auto generating CTRs Developed Spark scripts by using Scala and Python shell commands as per the requirement Maintained and administrated HDFS through Hadoop Java API shell scripting Python Used Python for writing script to move the data across clusters Expertise in designing Python scripts to interact with middlewareback end services Worked on python scripts to analyze the data of the customer Involved in converting CassandraHiveSQL queries into Spark transformations using Spark RDDs and Scala Python Developed monitoring and notification tools using Python Wrote Python routines to log into the websites and fetch data for selected options Used Collections in Python for manipulating and looping through different user defined objects Wrote and tested Python scripts to create new data files for Linux sever configuration using a Python templet tool Wrote shell scripts to automate the jobs in UNIX Used log4j API to write log files Understood the existing Oozie workflows and modified them as per new requirements Environment Cloudera Distribution 55 Hadoop Map Reduce Spark 16 HDFS Python Hive HBase HiveQL SQOOP Java Scala 2104 Unix IntelliJ Maven Hadoop Developer Sears Holding Corporation Hoffman Estate IL February 2014 to December 2015 With over 100 million customers and annual revenue of 30 billion Sears generates a huge amount of data on the transactions made by the customers The scope of the project is to use the customer transaction data in store and online over a period to recommend items to customer that they will find engaging Another concept is a recommendation engine that introduces new products to a customer which they might have not came across before To achieve these goals we perform data exploration to learn about user behavior We perform future engineering to create new features from existing features that truly reflects the signals in the data and avoid noise Responsibilities Worked with the source team to understand the format delimiters of the data files Responsible for generating actionable insights from complex data to drive significant business results for various application teams Developed and implemented API services using Python in spark Troubleshoot and resolve data quality issues and maintain important level of data accuracy in the data being reported Extensively implemented POCs on migrating to SparkStreaming to process the live data Ingested data from RDBMS and performed data transformations and then export the transformed data to Cassandra as per the business requirement Rewriting existing mapreduce jobs to use new features and improvements for achieving faster results Analyzes large amount of data sets to determine optimal way to aggregate and report on it Performance tuned slow running resource intensive jobs Worked on Data serialization formats for converting complex objects into sequence bits by using Avro Parquet JSON CSV formats Hands on experience working on inmemory based Apache Spark application for ETL transformations Involved in converting HiveSQL queries into Spark transformations using Spark RDDs Python Developed multiple POCs using PySpark and deployed on the Yarn cluster compared the performance of Spark with Hive and SQLTeradata Developed Flume configuration to extract log data from different resources and transfer data with different file formats JSON XML and Parquet to hive tables using different SerDes Setup Oozie workflow sub workflow jobs for HiveSQOOPHDFS actions Experience in accessing Kafka cluster to consume data into Hadoop Involved in importing the realtime data to Hadoop using Kafka and implemented the Oozie job for daily imports Worked with business and functional requirement gathering team updated user comments in JIRA and documented in confluence Handled tasks like maintaining accurate roadmap for project or certain product Monitoring the sprints burndown charts and completing the monthly reports Environment Hive SQL Pig Flume Kafka Map reduce SQOOP Spark Python Java Shell Scripting Teradata Oracle Oozie Cassandra Hadoop Developer Asurion Inc Nashville TN September 2012 to December 2013 Asurion is a privately held company that provides insurance for smartphones tablets consumer electronics appliances satellite receivers My team was responsible for generating a Hadoop based implementation of integrating data from the Warehouse and Database as input to the algorithm used by the Business Intelligence team Responsibilities Setting up the cluster configuration and maintenance install components of the Hadoop ecosystem Exported the analyzed data to the relational databases using Sqoop and process the data for visualization and to generate reports for the BI team Stored data from HDFS to respective Hive tables for business analysts to conduct further analysis in identifying data trends Developed Hive adhoc queries and filtered data in order to increase the effectiveness of the process execution by using functions like Joins Group By and Having Increased the time efficiency of the Hive QL using partitioning of data and reduced the time difference of executing the sets of data by applying the compression techniques like SNAPPY for MapReduce Jobs Created Hive Partitions for storing data for different trends under different partitions Connected the Hive tables to data analysis tools like Tableau for graphical representation of the trends Assisted project manager in problem shooting relevant to Hadoop technologies for data integration between different platforms like SqoopSqoop HiveSqoop and SqoopHive Environment Hortonworks Java 7 HBase HDFS MapReduce Hadoop 20 Hive Pig Eclipse Linux Sqoop MySQL Agile Kafka Java Developer Automatic Data Processing July 2009 to August 2012 Automatic Data Processing is a provider of business outsourcing solutions They provide Services such as Human Capital Management Tax Compliance Electronic Payment Solutions Dealer Services and Medical Practice Services This project is focused on Human Capital Management and particularly on Payroll Services Human Resource Management Talent Management Benefits Administration Time Attendance and International Solutions This frontend application has interface with various Java applications which are used to send and receive information to third party vendors Responsibilities Involved in Full Life Cycle Development in Distributed Environment Using Java and J2EE framework Responsible for developing and modifying the existing service layer based on the business requirements Involved in designing developing webservices using SOAP and WSDL Involved in database design Created tables views triggers stored procedures in SQL for data manipulation and retrieval Developed Web Services for Payment Transaction and Payment Release Involved in Requirement Analysis Development and Documentation Developed frontend using JSP HTML CSSand JavaScript Coding for DAO Objects using JDBC using DAO pattern XML and XSDs are used to define data formats Implemented J2EE design patterns such as singleton DAO for the presentation tier business tier and Integration Tier layers of the project Involved in Bug fixing and functionality enhancements Followed coding and documentation standards and best practices Participated in project planning discussions and worked with team members to analyze the requirements and translate them into working software modules Environment Java J2EE JSP SOAP WSDL SQL PLSQL XML JDBC Eclipse Windows XP Oracle Skills CASSANDRA AMBARI HDFS MAPREDUCE OOZIE SQOOP HBASE FLUME MONGODB TERADATA CC C Git HBase Hive HTML JAVASCRIPT MapReduce PHP Pig Additional Information TECHNICAL SKILLS BigData Ecosystem HDFS MapReduce YARN Hive Pig Flume Sqoop HBase Oozie Zookeeper Ambari Spark also Spark Core Spark SQL Spark Streaming Spark MLlib GraphX Databases Microsoft SQL Server Oracle 12c PLSQL MySQL MongoDB Cassandra Teradata Programming Languages CC Java Scala Python Shell Scripting R Programming Python Web HTML CSS PHP JavaScript AngularJS NodeJS Version Control Git Ant Maven Operating System UNIX RedHat Linux CentOS Ubuntu Microsoft Windows Amazon AWS Amazon S3 Amazon EC2 Amazon RDS Tools IDE Eclipse IntelliJ NetBeans Maven Jenkin SBT",
    "entities": [
        "IVR",
        "Joins Group",
        "Connected the Hive",
        "Sears Holding Corporation Hoffman Estate",
        "MapReduce Hadoop",
        "CTR",
        "SparkStreaming",
        "SNAPPY for MapReduce Jobs Created Hive Partitions",
        "ClouderaManager Extensive",
        "BI",
        "HDFS",
        "UNIX",
        "Maven Jenkin SBT",
        "Hadoop Developer Hadoop",
        "Medical Practice Services",
        "Developed Spark",
        "A Hadoop Administrator",
        "Working",
        "NiFi",
        "CVS",
        "Ambari",
        "Sears",
        "Hadoop",
        "SOAP",
        "XML",
        "Oozie Cassandra Hadoop Developer Asurion Inc Nashville",
        "JIRA",
        "Maintained",
        "the Hive QL",
        "HBase",
        "Apache Spark",
        "Amazon",
        "NodeJS Version Control",
        "Informatica Good",
        "Assisted",
        "DAO",
        "SqoopSqoop HiveSqoop",
        "Responsibilities Involved",
        "Balanced Independent Decisions Good Team Player Strong Interpersonal Organizational and Communication",
        "ORACLE",
        "Developed Hive",
        "Hadoop Developer",
        "Linux",
        "Hadoop Involved",
        "JSP",
        "CassandraHiveSQL",
        "the Business Intelligence team Responsibilities Setting",
        "HDP",
        "Developed Web Services for Payment Transaction",
        "Created Pig Latin",
        "Spark",
        "Data Extraction",
        "Amazon EMR",
        "The Financial Crimes Enforcement Network",
        "Spark Developed",
        "API",
        "Sqoop",
        "sort group join",
        "HIVE",
        "DAO Objects",
        "Executed Hive",
        "LINUX",
        "Spark Core Spark",
        "Human Capital Management",
        "Spark Control",
        "HDFS Importing",
        "multinode",
        "Studio Handson",
        "PIG",
        "ETL Data Integration",
        "SBT Ant",
        "Oozie",
        "SQL",
        "Amazon RDS Tools IDE Eclipse",
        "Citi Bank",
        "Hive",
        "CDH5",
        "JDBC",
        "ATM",
        "Maven Working",
        "Wrote",
        "HDFS MapReduce YARN Hive Pig HBase Zookeeper",
        "Human Capital Management Tax Compliance Electronic Payment Solutions Dealer Services",
        "ETL",
        "SelfMotivation Initiative and Project Management Attributes",
        "Maven Hadoop Developer",
        "Microsoft",
        "Work Experience Hadoop Developer",
        "Payroll Services Human Resource Management Talent Management Benefits Administration Time Attendance and International Solutions",
        "Responsibilities Evaluated",
        "Present Responsibilities Worked",
        "User DefinedAggregating Functions UDAF",
        "Data",
        "NetBeans",
        "Impalascripts",
        "RDBMS",
        "NoSQL",
        "Tableau",
        "Machine Learning",
        "HBase Developed",
        "Troubleshoot"
    ],
    "experience": "Experience working in different Hadoop distributions like Cloudera 55 CDH4 CDH5 and Hortonworks distributions HDP Through knowledge in ETL Data Integration and Migration extensively used ETL methodology for supporting Data Extraction transformations and loading using Informatica Good knowledge on Statistical and quantitative analysis using tools like R Studio Handson Experience in using version control tools like CVS GITand SVN Build tools like SBT Ant and Maven Working experience on NoSQL databases like HBase MongoDB and Cassandra with functionality and implementation Involved in developing Impalascripts for extraction transformation loading of data into data warehouse Working experience on Spark ecosystems using spark components like Spark Core Spark SQL Spark Streaming MLliband GraphX Experienced in collecting metrics for Hadoop clusters using Ambari and ClouderaManager Extensive experience working with real time streaming applications and batch style large scale distributed computing applications worked on integrating Kafka with NiFi and Spark Developed reusable and configurable components as part of project requirements in Java Scala and Python Good knowledge of Scalas functional style programming techniques like Anonymous Functions Closures Currying Higher Order Functions and Pattern Matching Handson experience in training evaluating and predicting the data as a part of Machine Learning using SparkMLlib TensorFlow and a regular contributor to Machine Learning projects on GitHub Good experience in working with cloud environment like Amazon Web ServicesEC2 and S3 Hands on experience on working with Amazon EMR framework transferring data to EC2server Strong Problem Solving and Analytical skills and abilities to make Balanced Independent Decisions Good Team Player Strong Interpersonal Organizational and Communication skills combined with SelfMotivation Initiative and Project Management Attributes Holds strong ability to handle multiple priorities and work load and has ability to understand and adapt to new technologies and environments faster Work Experience Hadoop Developer Johnson Johnson Irvine CA July 2018 to Present Responsibilities Worked on analyzing Hadoop cluster using different big data analytic tools including Pig Hive and MapReduce Managing fully distributed Hadoop cluster is an additional responsibility assigned to me I was trained to overtake the responsibilities of A Hadoop Administrator which includes managing the cluster Upgrades and installation of tools that uses Hadoop ecosystem Worked on Installation and configuring of Zoo Keeper to coordinate and monitor the cluster resources Implemented test scripts to support testdriven development and continuous integration Worked on POCs with Apache Spark using Scala to implement spark in project Consumed the data from Kafka using Apache spark Load and transform large sets of structured semi structured and unstructured data Involved in loading data from LINUX filesystem to HDFS Importing and exporting data into HDFS and Hive using Sqoop Implemented Partitioning Dynamic Partitions Buckets in Hive Worked in creating HBase tables to load large sets of semistructured data coming from various sources Extending HIVE and PIG core functionality by using custom User Defined Functions UDF User Defined TableGenerating Functions UDTF and User DefinedAggregating Functions UDAF for Hive and Pig using python Experienced in running Hadoop streaming jobs to process terabytes of xml format data Involved in scheduling Oozie workflow engine to run multiple Hive and pig jobs Experienced with performing CURD operations in HBase Developed workflow in Oozie to automate the tasks of loading the data into HDFS and preprocessing with Pig Responsible for loading data les from various external sources like ORACLE MySQL into staging area in MySQL databases Executed Hive queries on Parquet tables stored in Hive to perform data analysis to meet the business requirements Actively involved in code review and bug fixing for improving the performance Good experience in handling data manipulation using python Scripts Involved in development building testing and deploy to Hadoop cluster in distributed mode Created Linux shell Scripts to automate the daily ingestion of IVR data Executed Hive queries on Parquet tables stored in Hive to perform data analysis to meet the business requirements Helped the Analytics team with Aster queries using HCatlog Automated the History and Purge Process Created HBase tables to store various data formats of incoming data from different portfolios Created Pig Latin scripts to sort group join and filter the enterprise wise data Developed the verification and control process for daily load Experience in Daily production support to monitor and trouble shoots HadoopHive jobs Environment Hive SQL Pig Flume Kafka Map reduce SQOOP Spark Python Java Shell Scripting Teradata Oracle Oozie Cassandra HadoopSpark Developer Citi Bank Tampa FL January 2016 to June 2018 This project is to capture the large currency transactions made on the various channels of the current system All cash transactions made by nonexempt customers above the threshold amount are reported to The Financial Crimes Enforcement Network FinCEN by submitting the Cash Transaction Report CTR ECV gets all the transactions details from various upstream data source systems into Hadoop as a Service HAAS environment and filters only the cash transactions These transactions are further curated with ATM and Teller OSI data and enriched with user profile data This data will be stored in HBase for auto generating CTRs Responsibilities Evaluated business requirements and prepared Detailed Design documents that follows Project guidelines and SLAs required procuring data from all the upstream data sources and developing written programs Data files are retrieved by various data transmission protocols like Sqoop NDM SFTP DMS etc these data files are then validated by various Spark Control jobs written in Scala Spark RDDs are created for all the data files and then transformed to cash only transaction RDDs The filtered cash only RDDs are aggregated and curated based on the business rules and CTR requirements converted into data frames and saved as temporary hive tables for intermediate processing The RDDs and data frames undergo various transformations and actions and are stored in HDFS as parquet Files and in HBase for auto generating CTRs Developed Spark scripts by using Scala and Python shell commands as per the requirement Maintained and administrated HDFS through Hadoop Java API shell scripting Python Used Python for writing script to move the data across clusters Expertise in designing Python scripts to interact with middlewareback end services Worked on python scripts to analyze the data of the customer Involved in converting CassandraHiveSQL queries into Spark transformations using Spark RDDs and Scala Python Developed monitoring and notification tools using Python Wrote Python routines to log into the websites and fetch data for selected options Used Collections in Python for manipulating and looping through different user defined objects Wrote and tested Python scripts to create new data files for Linux sever configuration using a Python templet tool Wrote shell scripts to automate the jobs in UNIX Used log4j API to write log files Understood the existing Oozie workflows and modified them as per new requirements Environment Cloudera Distribution 55 Hadoop Map Reduce Spark 16 HDFS Python Hive HBase HiveQL SQOOP Java Scala 2104 Unix IntelliJ Maven Hadoop Developer Sears Holding Corporation Hoffman Estate IL February 2014 to December 2015 With over 100 million customers and annual revenue of 30 billion Sears generates a huge amount of data on the transactions made by the customers The scope of the project is to use the customer transaction data in store and online over a period to recommend items to customer that they will find engaging Another concept is a recommendation engine that introduces new products to a customer which they might have not came across before To achieve these goals we perform data exploration to learn about user behavior We perform future engineering to create new features from existing features that truly reflects the signals in the data and avoid noise Responsibilities Worked with the source team to understand the format delimiters of the data files Responsible for generating actionable insights from complex data to drive significant business results for various application teams Developed and implemented API services using Python in spark Troubleshoot and resolve data quality issues and maintain important level of data accuracy in the data being reported Extensively implemented POCs on migrating to SparkStreaming to process the live data Ingested data from RDBMS and performed data transformations and then export the transformed data to Cassandra as per the business requirement Rewriting existing mapreduce jobs to use new features and improvements for achieving faster results Analyzes large amount of data sets to determine optimal way to aggregate and report on it Performance tuned slow running resource intensive jobs Worked on Data serialization formats for converting complex objects into sequence bits by using Avro Parquet JSON CSV formats Hands on experience working on inmemory based Apache Spark application for ETL transformations Involved in converting HiveSQL queries into Spark transformations using Spark RDDs Python Developed multiple POCs using PySpark and deployed on the Yarn cluster compared the performance of Spark with Hive and SQLTeradata Developed Flume configuration to extract log data from different resources and transfer data with different file formats JSON XML and Parquet to hive tables using different SerDes Setup Oozie workflow sub workflow jobs for HiveSQOOPHDFS actions Experience in accessing Kafka cluster to consume data into Hadoop Involved in importing the realtime data to Hadoop using Kafka and implemented the Oozie job for daily imports Worked with business and functional requirement gathering team updated user comments in JIRA and documented in confluence Handled tasks like maintaining accurate roadmap for project or certain product Monitoring the sprints burndown charts and completing the monthly reports Environment Hive SQL Pig Flume Kafka Map reduce SQOOP Spark Python Java Shell Scripting Teradata Oracle Oozie Cassandra Hadoop Developer Asurion Inc Nashville TN September 2012 to December 2013 Asurion is a privately held company that provides insurance for smartphones tablets consumer electronics appliances satellite receivers My team was responsible for generating a Hadoop based implementation of integrating data from the Warehouse and Database as input to the algorithm used by the Business Intelligence team Responsibilities Setting up the cluster configuration and maintenance install components of the Hadoop ecosystem Exported the analyzed data to the relational databases using Sqoop and process the data for visualization and to generate reports for the BI team Stored data from HDFS to respective Hive tables for business analysts to conduct further analysis in identifying data trends Developed Hive adhoc queries and filtered data in order to increase the effectiveness of the process execution by using functions like Joins Group By and Having Increased the time efficiency of the Hive QL using partitioning of data and reduced the time difference of executing the sets of data by applying the compression techniques like SNAPPY for MapReduce Jobs Created Hive Partitions for storing data for different trends under different partitions Connected the Hive tables to data analysis tools like Tableau for graphical representation of the trends Assisted project manager in problem shooting relevant to Hadoop technologies for data integration between different platforms like SqoopSqoop HiveSqoop and SqoopHive Environment Hortonworks Java 7 HBase HDFS MapReduce Hadoop 20 Hive Pig Eclipse Linux Sqoop MySQL Agile Kafka Java Developer Automatic Data Processing July 2009 to August 2012 Automatic Data Processing is a provider of business outsourcing solutions They provide Services such as Human Capital Management Tax Compliance Electronic Payment Solutions Dealer Services and Medical Practice Services This project is focused on Human Capital Management and particularly on Payroll Services Human Resource Management Talent Management Benefits Administration Time Attendance and International Solutions This frontend application has interface with various Java applications which are used to send and receive information to third party vendors Responsibilities Involved in Full Life Cycle Development in Distributed Environment Using Java and J2EE framework Responsible for developing and modifying the existing service layer based on the business requirements Involved in designing developing webservices using SOAP and WSDL Involved in database design Created tables views triggers stored procedures in SQL for data manipulation and retrieval Developed Web Services for Payment Transaction and Payment Release Involved in Requirement Analysis Development and Documentation Developed frontend using JSP HTML CSSand JavaScript Coding for DAO Objects using JDBC using DAO pattern XML and XSDs are used to define data formats Implemented J2EE design patterns such as singleton DAO for the presentation tier business tier and Integration Tier layers of the project Involved in Bug fixing and functionality enhancements Followed coding and documentation standards and best practices Participated in project planning discussions and worked with team members to analyze the requirements and translate them into working software modules Environment Java J2EE JSP SOAP WSDL SQL PLSQL XML JDBC Eclipse Windows XP Oracle Skills CASSANDRA AMBARI HDFS MAPREDUCE OOZIE SQOOP HBASE FLUME MONGODB TERADATA CC C Git HBase Hive HTML JAVASCRIPT MapReduce PHP Pig Additional Information TECHNICAL SKILLS BigData Ecosystem HDFS MapReduce YARN Hive Pig Flume Sqoop HBase Oozie Zookeeper Ambari Spark also Spark Core Spark SQL Spark Streaming Spark MLlib GraphX Databases Microsoft SQL Server Oracle 12c PLSQL MySQL MongoDB Cassandra Teradata Programming Languages CC Java Scala Python Shell Scripting R Programming Python Web HTML CSS PHP JavaScript AngularJS NodeJS Version Control Git Ant Maven Operating System UNIX RedHat Linux CentOS Ubuntu Microsoft Windows Amazon AWS Amazon S3 Amazon EC2 Amazon RDS Tools IDE Eclipse IntelliJ NetBeans Maven Jenkin SBT",
    "extracted_keywords": [
        "Hadoop",
        "Developer",
        "Hadoop",
        "span",
        "lDeveloperspan",
        "Hadoop",
        "Developer",
        "Johnson",
        "Johnson",
        "Irvine",
        "CA",
        "Cloudera",
        "Spark",
        "Hadoop",
        "Developer",
        "years",
        "experience",
        "software",
        "development",
        "deployment",
        "maintenance",
        "web",
        "applications",
        "Java",
        "Big",
        "Data",
        "Ecosystems",
        "Windows",
        "Linux",
        "Expertise",
        "Hadoop",
        "applications",
        "solutions",
        "technologies",
        "applications",
        "Expertise",
        "components",
        "Hadoop",
        "ecosystems",
        "HDFS",
        "MapReduce",
        "YARN",
        "Hive",
        "Pig",
        "HBase",
        "Zookeeper",
        "Sqoop",
        "Spark",
        "Kafka",
        "Cassandra",
        "Impala",
        "knowledge",
        "handson",
        "experience",
        "configuring",
        "multinode",
        "clusters",
        "environments",
        "distributions",
        "Hadoop",
        "Experience",
        "Hadoop",
        "distributions",
        "Cloudera",
        "CDH4",
        "CDH5",
        "Hortonworks",
        "distributions",
        "HDP",
        "knowledge",
        "ETL",
        "Data",
        "Integration",
        "Migration",
        "ETL",
        "methodology",
        "Data",
        "Extraction",
        "transformations",
        "loading",
        "Informatica",
        "knowledge",
        "analysis",
        "tools",
        "R",
        "Studio",
        "Handson",
        "Experience",
        "version",
        "control",
        "tools",
        "CVS",
        "GITand",
        "SVN",
        "Build",
        "tools",
        "SBT",
        "Ant",
        "Maven",
        "Working",
        "experience",
        "NoSQL",
        "databases",
        "HBase",
        "MongoDB",
        "Cassandra",
        "functionality",
        "implementation",
        "Impalascripts",
        "extraction",
        "transformation",
        "loading",
        "data",
        "data",
        "warehouse",
        "Working",
        "experience",
        "Spark",
        "ecosystems",
        "spark",
        "components",
        "Spark",
        "Core",
        "Spark",
        "SQL",
        "Spark",
        "Streaming",
        "MLliband",
        "GraphX",
        "metrics",
        "Hadoop",
        "clusters",
        "Ambari",
        "ClouderaManager",
        "experience",
        "time",
        "streaming",
        "applications",
        "batch",
        "style",
        "scale",
        "computing",
        "applications",
        "Kafka",
        "NiFi",
        "Spark",
        "components",
        "part",
        "project",
        "requirements",
        "Java",
        "Scala",
        "Python",
        "knowledge",
        "Scalas",
        "style",
        "programming",
        "techniques",
        "Anonymous",
        "Functions",
        "Closures",
        "Order",
        "Functions",
        "Pattern",
        "Matching",
        "Handson",
        "experience",
        "training",
        "data",
        "part",
        "Machine",
        "Learning",
        "SparkMLlib",
        "TensorFlow",
        "contributor",
        "Machine",
        "Learning",
        "projects",
        "GitHub",
        "experience",
        "cloud",
        "environment",
        "Amazon",
        "Web",
        "ServicesEC2",
        "S3",
        "Hands",
        "experience",
        "Amazon",
        "EMR",
        "framework",
        "data",
        "Strong",
        "Problem",
        "Solving",
        "Analytical",
        "skills",
        "abilities",
        "Balanced",
        "Independent",
        "Decisions",
        "Good",
        "Team",
        "Player",
        "Strong",
        "Interpersonal",
        "Organizational",
        "Communication",
        "skills",
        "SelfMotivation",
        "Initiative",
        "Project",
        "Management",
        "Attributes",
        "ability",
        "priorities",
        "work",
        "load",
        "ability",
        "technologies",
        "environments",
        "Work",
        "Experience",
        "Hadoop",
        "Developer",
        "Johnson",
        "Johnson",
        "Irvine",
        "CA",
        "July",
        "Present",
        "Responsibilities",
        "Hadoop",
        "cluster",
        "data",
        "tools",
        "Pig",
        "Hive",
        "MapReduce",
        "Managing",
        "Hadoop",
        "cluster",
        "responsibility",
        "responsibilities",
        "Hadoop",
        "Administrator",
        "cluster",
        "Upgrades",
        "installation",
        "tools",
        "Hadoop",
        "ecosystem",
        "Installation",
        "configuring",
        "Zoo",
        "Keeper",
        "cluster",
        "resources",
        "test",
        "scripts",
        "testdriven",
        "development",
        "integration",
        "POCs",
        "Apache",
        "Spark",
        "Scala",
        "spark",
        "project",
        "data",
        "Kafka",
        "Apache",
        "spark",
        "Load",
        "sets",
        "data",
        "loading",
        "data",
        "LINUX",
        "filesystem",
        "HDFS",
        "Importing",
        "data",
        "HDFS",
        "Hive",
        "Sqoop",
        "Partitioning",
        "Dynamic",
        "Partitions",
        "Buckets",
        "Hive",
        "Worked",
        "HBase",
        "tables",
        "sets",
        "data",
        "sources",
        "HIVE",
        "PIG",
        "core",
        "functionality",
        "custom",
        "User",
        "Defined",
        "Functions",
        "UDF",
        "User",
        "Defined",
        "TableGenerating",
        "Functions",
        "UDTF",
        "User",
        "DefinedAggregating",
        "Functions",
        "UDAF",
        "Hive",
        "Pig",
        "python",
        "Hadoop",
        "streaming",
        "jobs",
        "terabytes",
        "xml",
        "format",
        "data",
        "Oozie",
        "workflow",
        "engine",
        "Hive",
        "pig",
        "jobs",
        "CURD",
        "operations",
        "HBase",
        "workflow",
        "Oozie",
        "tasks",
        "data",
        "HDFS",
        "Pig",
        "Responsible",
        "loading",
        "data",
        "sources",
        "ORACLE",
        "MySQL",
        "staging",
        "area",
        "MySQL",
        "Executed",
        "Hive",
        "queries",
        "Parquet",
        "tables",
        "Hive",
        "data",
        "analysis",
        "business",
        "requirements",
        "code",
        "review",
        "bug",
        "performance",
        "experience",
        "data",
        "manipulation",
        "python",
        "Scripts",
        "development",
        "building",
        "testing",
        "Hadoop",
        "cluster",
        "mode",
        "Linux",
        "Scripts",
        "ingestion",
        "IVR",
        "data",
        "Hive",
        "queries",
        "Parquet",
        "tables",
        "Hive",
        "data",
        "analysis",
        "business",
        "requirements",
        "Analytics",
        "team",
        "Aster",
        "queries",
        "HCatlog",
        "Automated",
        "History",
        "Purge",
        "Process",
        "Created",
        "HBase",
        "data",
        "formats",
        "data",
        "portfolios",
        "Pig",
        "Latin",
        "scripts",
        "group",
        "join",
        "enterprise",
        "data",
        "verification",
        "control",
        "process",
        "load",
        "Experience",
        "Daily",
        "production",
        "support",
        "trouble",
        "HadoopHive",
        "jobs",
        "Environment",
        "Hive",
        "SQL",
        "Pig",
        "Flume",
        "Kafka",
        "Map",
        "SQOOP",
        "Spark",
        "Python",
        "Java",
        "Shell",
        "Scripting",
        "Teradata",
        "Oracle",
        "Oozie",
        "Cassandra",
        "HadoopSpark",
        "Developer",
        "Citi",
        "Bank",
        "Tampa",
        "FL",
        "January",
        "June",
        "project",
        "currency",
        "transactions",
        "channels",
        "system",
        "cash",
        "transactions",
        "customers",
        "threshold",
        "amount",
        "Financial",
        "Crimes",
        "Enforcement",
        "Network",
        "FinCEN",
        "Cash",
        "Transaction",
        "Report",
        "CTR",
        "ECV",
        "transactions",
        "details",
        "data",
        "source",
        "systems",
        "Hadoop",
        "Service",
        "HAAS",
        "environment",
        "cash",
        "transactions",
        "transactions",
        "ATM",
        "Teller",
        "OSI",
        "data",
        "user",
        "profile",
        "data",
        "data",
        "HBase",
        "auto",
        "generating",
        "CTRs",
        "Responsibilities",
        "business",
        "requirements",
        "Detailed",
        "Design",
        "documents",
        "Project",
        "guidelines",
        "SLAs",
        "procuring",
        "data",
        "data",
        "sources",
        "programs",
        "Data",
        "files",
        "data",
        "transmission",
        "protocols",
        "Sqoop",
        "NDM",
        "DMS",
        "data",
        "files",
        "Spark",
        "Control",
        "jobs",
        "Scala",
        "Spark",
        "RDDs",
        "data",
        "files",
        "transaction",
        "RDDs",
        "cash",
        "RDDs",
        "business",
        "rules",
        "CTR",
        "requirements",
        "data",
        "frames",
        "hive",
        "tables",
        "processing",
        "RDDs",
        "data",
        "frames",
        "transformations",
        "actions",
        "HDFS",
        "Files",
        "HBase",
        "auto",
        "generating",
        "CTRs",
        "Spark",
        "scripts",
        "Scala",
        "Python",
        "shell",
        "commands",
        "requirement",
        "HDFS",
        "Hadoop",
        "Java",
        "API",
        "shell",
        "Python",
        "Python",
        "script",
        "data",
        "clusters",
        "Expertise",
        "Python",
        "scripts",
        "end",
        "services",
        "scripts",
        "data",
        "customer",
        "CassandraHiveSQL",
        "queries",
        "Spark",
        "transformations",
        "Spark",
        "RDDs",
        "Scala",
        "Python",
        "monitoring",
        "notification",
        "tools",
        "Python",
        "Wrote",
        "Python",
        "websites",
        "data",
        "options",
        "Collections",
        "Python",
        "user",
        "objects",
        "Wrote",
        "Python",
        "scripts",
        "data",
        "files",
        "Linux",
        "configuration",
        "Python",
        "templet",
        "tool",
        "Wrote",
        "shell",
        "scripts",
        "jobs",
        "UNIX",
        "log4j",
        "API",
        "log",
        "files",
        "Understood",
        "Oozie",
        "workflows",
        "requirements",
        "Environment",
        "Cloudera",
        "Distribution",
        "Hadoop",
        "Map",
        "Spark",
        "HDFS",
        "Python",
        "Hive",
        "HBase",
        "HiveQL",
        "SQOOP",
        "Java",
        "Scala",
        "Unix",
        "IntelliJ",
        "Maven",
        "Hadoop",
        "Developer",
        "Sears",
        "Holding",
        "Corporation",
        "Hoffman",
        "Estate",
        "IL",
        "February",
        "December",
        "customers",
        "revenue",
        "Sears",
        "amount",
        "data",
        "transactions",
        "customers",
        "scope",
        "project",
        "customer",
        "transaction",
        "data",
        "store",
        "period",
        "items",
        "customer",
        "concept",
        "recommendation",
        "engine",
        "products",
        "customer",
        "goals",
        "data",
        "exploration",
        "user",
        "behavior",
        "engineering",
        "features",
        "features",
        "signals",
        "data",
        "noise",
        "Responsibilities",
        "source",
        "team",
        "format",
        "delimiters",
        "data",
        "files",
        "insights",
        "data",
        "business",
        "results",
        "application",
        "teams",
        "API",
        "services",
        "Python",
        "spark",
        "Troubleshoot",
        "data",
        "quality",
        "issues",
        "level",
        "data",
        "accuracy",
        "data",
        "POCs",
        "SparkStreaming",
        "data",
        "data",
        "RDBMS",
        "data",
        "transformations",
        "data",
        "Cassandra",
        "business",
        "requirement",
        "mapreduce",
        "jobs",
        "features",
        "improvements",
        "results",
        "Analyzes",
        "amount",
        "data",
        "sets",
        "way",
        "Performance",
        "resource",
        "jobs",
        "Data",
        "serialization",
        "formats",
        "objects",
        "sequence",
        "bits",
        "Avro",
        "Parquet",
        "CSV",
        "Hands",
        "experience",
        "inmemory",
        "Apache",
        "Spark",
        "application",
        "ETL",
        "transformations",
        "HiveSQL",
        "queries",
        "Spark",
        "transformations",
        "Spark",
        "RDDs",
        "Python",
        "POCs",
        "PySpark",
        "Yarn",
        "cluster",
        "performance",
        "Spark",
        "Hive",
        "SQLTeradata",
        "Developed",
        "Flume",
        "configuration",
        "data",
        "resources",
        "transfer",
        "data",
        "file",
        "formats",
        "JSON",
        "XML",
        "Parquet",
        "tables",
        "SerDes",
        "Setup",
        "Oozie",
        "workflow",
        "sub",
        "jobs",
        "HiveSQOOPHDFS",
        "actions",
        "Experience",
        "Kafka",
        "cluster",
        "data",
        "Hadoop",
        "data",
        "Hadoop",
        "Kafka",
        "Oozie",
        "job",
        "imports",
        "business",
        "requirement",
        "gathering",
        "team",
        "user",
        "comments",
        "JIRA",
        "confluence",
        "tasks",
        "roadmap",
        "project",
        "product",
        "sprints",
        "charts",
        "reports",
        "Environment",
        "Hive",
        "SQL",
        "Pig",
        "Flume",
        "Kafka",
        "Map",
        "SQOOP",
        "Spark",
        "Python",
        "Java",
        "Shell",
        "Scripting",
        "Teradata",
        "Oracle",
        "Oozie",
        "Cassandra",
        "Hadoop",
        "Developer",
        "Asurion",
        "Inc",
        "Nashville",
        "TN",
        "September",
        "December",
        "Asurion",
        "company",
        "insurance",
        "smartphones",
        "consumer",
        "electronics",
        "appliances",
        "satellite",
        "receivers",
        "team",
        "Hadoop",
        "implementation",
        "data",
        "Warehouse",
        "Database",
        "input",
        "algorithm",
        "Business",
        "Intelligence",
        "team",
        "Responsibilities",
        "cluster",
        "configuration",
        "maintenance",
        "install",
        "components",
        "Hadoop",
        "ecosystem",
        "data",
        "databases",
        "Sqoop",
        "data",
        "visualization",
        "reports",
        "BI",
        "team",
        "data",
        "HDFS",
        "Hive",
        "tables",
        "business",
        "analysts",
        "analysis",
        "data",
        "trends",
        "Hive",
        "queries",
        "data",
        "order",
        "effectiveness",
        "process",
        "execution",
        "functions",
        "Joins",
        "Group",
        "By",
        "time",
        "efficiency",
        "Hive",
        "QL",
        "partitioning",
        "data",
        "time",
        "difference",
        "sets",
        "data",
        "compression",
        "techniques",
        "SNAPPY",
        "MapReduce",
        "Jobs",
        "Hive",
        "Partitions",
        "data",
        "trends",
        "partitions",
        "Hive",
        "tables",
        "data",
        "analysis",
        "tools",
        "Tableau",
        "representation",
        "trends",
        "project",
        "manager",
        "problem",
        "Hadoop",
        "technologies",
        "data",
        "integration",
        "platforms",
        "SqoopSqoop",
        "HiveSqoop",
        "SqoopHive",
        "Environment",
        "Hortonworks",
        "Java",
        "HBase",
        "HDFS",
        "MapReduce",
        "Hadoop",
        "Hive",
        "Pig",
        "Eclipse",
        "Linux",
        "Sqoop",
        "MySQL",
        "Agile",
        "Kafka",
        "Java",
        "Developer",
        "Automatic",
        "Data",
        "Processing",
        "July",
        "August",
        "Automatic",
        "Data",
        "Processing",
        "provider",
        "business",
        "outsourcing",
        "solutions",
        "Services",
        "Human",
        "Capital",
        "Management",
        "Tax",
        "Compliance",
        "Electronic",
        "Payment",
        "Solutions",
        "Dealer",
        "Services",
        "Medical",
        "Practice",
        "Services",
        "project",
        "Human",
        "Capital",
        "Management",
        "Payroll",
        "Services",
        "Human",
        "Resource",
        "Management",
        "Talent",
        "Management",
        "Benefits",
        "Administration",
        "Time",
        "Attendance",
        "International",
        "Solutions",
        "frontend",
        "application",
        "interface",
        "Java",
        "applications",
        "information",
        "party",
        "vendors",
        "Responsibilities",
        "Full",
        "Life",
        "Cycle",
        "Development",
        "Distributed",
        "Environment",
        "Java",
        "J2EE",
        "framework",
        "service",
        "layer",
        "business",
        "requirements",
        "webservices",
        "SOAP",
        "WSDL",
        "database",
        "design",
        "tables",
        "views",
        "triggers",
        "procedures",
        "SQL",
        "data",
        "manipulation",
        "retrieval",
        "Developed",
        "Web",
        "Services",
        "Payment",
        "Transaction",
        "Payment",
        "Release",
        "Requirement",
        "Analysis",
        "Development",
        "Documentation",
        "frontend",
        "JSP",
        "HTML",
        "CSSand",
        "JavaScript",
        "Coding",
        "DAO",
        "Objects",
        "JDBC",
        "DAO",
        "pattern",
        "XML",
        "XSDs",
        "data",
        "formats",
        "J2EE",
        "design",
        "patterns",
        "singleton",
        "DAO",
        "presentation",
        "tier",
        "business",
        "tier",
        "Integration",
        "Tier",
        "layers",
        "project",
        "Bug",
        "fixing",
        "functionality",
        "enhancements",
        "documentation",
        "standards",
        "practices",
        "project",
        "planning",
        "discussions",
        "team",
        "members",
        "requirements",
        "software",
        "modules",
        "Environment",
        "Java",
        "J2EE",
        "JSP",
        "SOAP",
        "WSDL",
        "SQL",
        "PLSQL",
        "XML",
        "JDBC",
        "Eclipse",
        "Windows",
        "XP",
        "Oracle",
        "Skills",
        "CASSANDRA",
        "HDFS",
        "MAPREDUCE",
        "OOZIE",
        "SQOOP",
        "HBASE",
        "MONGODB",
        "TERADATA",
        "CC",
        "C",
        "Git",
        "HBase",
        "Hive",
        "HTML",
        "JAVASCRIPT",
        "MapReduce",
        "PHP",
        "Pig",
        "Additional",
        "Information",
        "TECHNICAL",
        "SKILLS",
        "BigData",
        "Ecosystem",
        "HDFS",
        "MapReduce",
        "YARN",
        "Hive",
        "Pig",
        "Flume",
        "Sqoop",
        "HBase",
        "Oozie",
        "Zookeeper",
        "Ambari",
        "Spark",
        "Spark",
        "Core",
        "Spark",
        "SQL",
        "Spark",
        "Streaming",
        "Spark",
        "MLlib",
        "GraphX",
        "Microsoft",
        "SQL",
        "Server",
        "Oracle",
        "12c",
        "PLSQL",
        "MySQL",
        "MongoDB",
        "Cassandra",
        "Teradata",
        "Programming",
        "Languages",
        "CC",
        "Java",
        "Scala",
        "Python",
        "Shell",
        "Scripting",
        "R",
        "Programming",
        "Python",
        "Web",
        "HTML",
        "CSS",
        "PHP",
        "JavaScript",
        "NodeJS",
        "Version",
        "Control",
        "Git",
        "Ant",
        "Maven",
        "Operating",
        "System",
        "UNIX",
        "RedHat",
        "Linux",
        "CentOS",
        "Ubuntu",
        "Microsoft",
        "Windows",
        "Amazon",
        "AWS",
        "Amazon",
        "S3",
        "Amazon",
        "EC2",
        "Amazon",
        "RDS",
        "Tools",
        "IDE",
        "Eclipse",
        "IntelliJ",
        "NetBeans",
        "Maven",
        "Jenkin",
        "SBT"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T23:13:33.437458",
    "resume_data": "Hadoop Developer Hadoop span lDeveloperspan Hadoop Developer Johnson Johnson Irvine CA Cloudera certified Spark and Hadoop Developer with around 10 years of experience in software development deployment and maintenance of various web based applications using Java and Big Data Ecosystems on Windows and Linux environments Expertise in designing Hadoop applications and recommending the right solutions and technologies for the applications Expertise in major components of Hadoop ecosystems like HDFS MapReduce YARN Hive Pig HBase Zookeeper Sqoop Spark Kafka Cassandra and Impala Good knowledge and handson experience on installing configuring and maintaining multinode clusters on various environments and distributions of Hadoop Experience working in different Hadoop distributions like Cloudera 55 CDH4 CDH5 and Hortonworks distributions HDP Through knowledge in ETL Data Integration and Migration extensively used ETL methodology for supporting Data Extraction transformations and loading using Informatica Good knowledge on Statistical and quantitative analysis using tools like R Studio Handson Experience in using version control tools like CVS GITand SVN Build tools like SBT Ant and Maven Working experience on NoSQL databases like HBase MongoDB and Cassandra with functionality and implementation Involved in developing Impalascripts for extraction transformation loading of data into data warehouse Working experience on Spark ecosystems using spark components like Spark Core Spark SQL Spark Streaming MLliband GraphX Experienced in collecting metrics for Hadoop clusters using Ambari and ClouderaManager Extensive experience working with real time streaming applications and batch style large scale distributed computing applications worked on integrating Kafka with NiFi and Spark Developed reusable and configurable components as part of project requirements in Java Scala and Python Good knowledge of Scalas functional style programming techniques like Anonymous Functions Closures Currying Higher Order Functions and Pattern Matching Handson experience in training evaluating and predicting the data as a part of Machine Learning using SparkMLlib TensorFlow and a regular contributor to Machine Learning projects on GitHub Good experience in working with cloud environment like Amazon Web ServicesEC2 and S3 Hands on experience on working with Amazon EMR framework transferring data to EC2server Strong Problem Solving and Analytical skills and abilities to make Balanced Independent Decisions Good Team Player Strong Interpersonal Organizational and Communication skills combined with SelfMotivation Initiative and Project Management Attributes Holds strong ability to handle multiple priorities and work load and has ability to understand and adapt to new technologies and environments faster Work Experience Hadoop Developer Johnson Johnson Irvine CA July 2018 to Present Responsibilities Worked on analyzing Hadoop cluster using different big data analytic tools including Pig Hive and MapReduce Managing fully distributed Hadoop cluster is an additional responsibility assigned to me I was trained to overtake the responsibilities of A Hadoop Administrator which includes managing the cluster Upgrades and installation of tools that uses Hadoop ecosystem Worked on Installation and configuring of Zoo Keeper to coordinate and monitor the cluster resources Implemented test scripts to support testdriven development and continuous integration Worked on POCs with Apache Spark using Scala to implement spark in project Consumed the data from Kafka using Apache spark Load and transform large sets of structured semi structured and unstructured data Involved in loading data from LINUX filesystem to HDFS Importing and exporting data into HDFS and Hive using Sqoop Implemented Partitioning Dynamic Partitions Buckets in Hive Worked in creating HBase tables to load large sets of semistructured data coming from various sources Extending HIVE and PIG core functionality by using custom User Defined Functions UDF User Defined TableGenerating Functions UDTF and User DefinedAggregating Functions UDAF for Hive and Pig using python Experienced in running Hadoop streaming jobs to process terabytes of xml format data Involved in scheduling Oozie workflow engine to run multiple Hive and pig jobs Experienced with performing CURD operations in HBase Developed workflow in Oozie to automate the tasks of loading the data into HDFS and preprocessing with Pig Responsible for loading data les from various external sources like ORACLE MySQL into staging area in MySQL databases Executed Hive queries on Parquet tables stored in Hive to perform data analysis to meet the business requirements Actively involved in code review and bug fixing for improving the performance Good experience in handling data manipulation using python Scripts Involved in development building testing and deploy to Hadoop cluster in distributed mode Created Linux shell Scripts to automate the daily ingestion of IVR data Executed Hive queries on Parquet tables stored in Hive to perform data analysis to meet the business requirements Helped the Analytics team with Aster queries using HCatlog Automated the History and Purge Process Created HBase tables to store various data formats of incoming data from different portfolios Created Pig Latin scripts to sort group join and filter the enterprise wise data Developed the verification and control process for daily load Experience in Daily production support to monitor and trouble shoots HadoopHive jobs Environment Hive SQL Pig Flume Kafka Map reduce SQOOP Spark Python Java Shell Scripting Teradata Oracle Oozie Cassandra HadoopSpark Developer Citi Bank Tampa FL January 2016 to June 2018 This project is to capture the large currency transactions made on the various channels of the current system All cash transactions made by nonexempt customers above the threshold amount are reported to The Financial Crimes Enforcement Network FinCEN by submitting the Cash Transaction Report CTR ECV gets all the transactions details from various upstream data source systems into Hadoop as a Service HAAS environment and filters only the cash transactions These transactions are further curated with ATM and Teller OSI data and enriched with user profile data This data will be stored in HBase for auto generating CTRs Responsibilities Evaluated business requirements and prepared Detailed Design documents that follows Project guidelines and SLAs required procuring data from all the upstream data sources and developing written programs Data files are retrieved by various data transmission protocols like Sqoop NDM SFTP DMS etc these data files are then validated by various Spark Control jobs written in Scala Spark RDDs are created for all the data files and then transformed to cash only transaction RDDs The filtered cash only RDDs are aggregated and curated based on the business rules and CTR requirements converted into data frames and saved as temporary hive tables for intermediate processing The RDDs and data frames undergo various transformations and actions and are stored in HDFS as parquet Files and in HBase for auto generating CTRs Developed Spark scripts by using Scala and Python shell commands as per the requirement Maintained and administrated HDFS through Hadoop Java API shell scripting Python Used Python for writing script to move the data across clusters Expertise in designing Python scripts to interact with middlewareback end services Worked on python scripts to analyze the data of the customer Involved in converting CassandraHiveSQL queries into Spark transformations using Spark RDDs and Scala Python Developed monitoring and notification tools using Python Wrote Python routines to log into the websites and fetch data for selected options Used Collections in Python for manipulating and looping through different user defined objects Wrote and tested Python scripts to create new data files for Linux sever configuration using a Python templet tool Wrote shell scripts to automate the jobs in UNIX Used log4j API to write log files Understood the existing Oozie workflows and modified them as per new requirements Environment Cloudera Distribution 55 Hadoop Map Reduce Spark 16 HDFS Python Hive HBase HiveQL SQOOP Java Scala 2104 Unix IntelliJ Maven Hadoop Developer Sears Holding Corporation Hoffman Estate IL February 2014 to December 2015 With over 100 million customers and annual revenue of 30 billion Sears generates a huge amount of data on the transactions made by the customers The scope of the project is to use the customer transaction data in store and online over a period to recommend items to customer that they will find engaging Another concept is a recommendation engine that introduces new products to a customer which they might have not came across before To achieve these goals we perform data exploration to learn about user behavior We perform future engineering to create new features from existing features that truly reflects the signals in the data and avoid noise Responsibilities Worked with the source team to understand the format delimiters of the data files Responsible for generating actionable insights from complex data to drive significant business results for various application teams Developed and implemented API services using Python in spark Troubleshoot and resolve data quality issues and maintain important level of data accuracy in the data being reported Extensively implemented POCs on migrating to SparkStreaming to process the live data Ingested data from RDBMS and performed data transformations and then export the transformed data to Cassandra as per the business requirement Rewriting existing mapreduce jobs to use new features and improvements for achieving faster results Analyzes large amount of data sets to determine optimal way to aggregate and report on it Performance tuned slow running resource intensive jobs Worked on Data serialization formats for converting complex objects into sequence bits by using Avro Parquet JSON CSV formats Hands on experience working on inmemory based Apache Spark application for ETL transformations Involved in converting HiveSQL queries into Spark transformations using Spark RDDs Python Developed multiple POCs using PySpark and deployed on the Yarn cluster compared the performance of Spark with Hive and SQLTeradata Developed Flume configuration to extract log data from different resources and transfer data with different file formats JSON XML and Parquet to hive tables using different SerDes Setup Oozie workflow sub workflow jobs for HiveSQOOPHDFS actions Experience in accessing Kafka cluster to consume data into Hadoop Involved in importing the realtime data to Hadoop using Kafka and implemented the Oozie job for daily imports Worked with business and functional requirement gathering team updated user comments in JIRA and documented in confluence Handled tasks like maintaining accurate roadmap for project or certain product Monitoring the sprints burndown charts and completing the monthly reports Environment Hive SQL Pig Flume Kafka Map reduce SQOOP Spark Python Java Shell Scripting Teradata Oracle Oozie Cassandra Hadoop Developer Asurion Inc Nashville TN September 2012 to December 2013 Asurion is a privately held company that provides insurance for smartphones tablets consumer electronics appliances satellite receivers My team was responsible for generating a Hadoop based implementation of integrating data from the Warehouse and Database as input to the algorithm used by the Business Intelligence team Responsibilities Setting up the cluster configuration and maintenance install components of the Hadoop ecosystem Exported the analyzed data to the relational databases using Sqoop and process the data for visualization and to generate reports for the BI team Stored data from HDFS to respective Hive tables for business analysts to conduct further analysis in identifying data trends Developed Hive adhoc queries and filtered data in order to increase the effectiveness of the process execution by using functions like Joins Group By and Having Increased the time efficiency of the Hive QL using partitioning of data and reduced the time difference of executing the sets of data by applying the compression techniques like SNAPPY for MapReduce Jobs Created Hive Partitions for storing data for different trends under different partitions Connected the Hive tables to data analysis tools like Tableau for graphical representation of the trends Assisted project manager in problem shooting relevant to Hadoop technologies for data integration between different platforms like SqoopSqoop HiveSqoop and SqoopHive Environment Hortonworks Java 7 HBase HDFS MapReduce Hadoop 20 Hive Pig Eclipse Linux Sqoop MySQL Agile Kafka Java Developer Automatic Data Processing July 2009 to August 2012 Automatic Data Processing is a provider of business outsourcing solutions They provide Services such as Human Capital Management Tax Compliance Electronic Payment Solutions Dealer Services and Medical Practice Services This project is focused on Human Capital Management and particularly on Payroll Services Human Resource Management Talent Management Benefits Administration Time Attendance and International Solutions This frontend application has interface with various Java applications which are used to send and receive information to third party vendors Responsibilities Involved in Full Life Cycle Development in Distributed Environment Using Java and J2EE framework Responsible for developing and modifying the existing service layer based on the business requirements Involved in designing developing webservices using SOAP and WSDL Involved in database design Created tables views triggers stored procedures in SQL for data manipulation and retrieval Developed Web Services for Payment Transaction and Payment Release Involved in Requirement Analysis Development and Documentation Developed frontend using JSP HTML CSSand JavaScript Coding for DAO Objects using JDBC using DAO pattern XML and XSDs are used to define data formats Implemented J2EE design patterns such as singleton DAO for the presentation tier business tier and Integration Tier layers of the project Involved in Bug fixing and functionality enhancements Followed coding and documentation standards and best practices Participated in project planning discussions and worked with team members to analyze the requirements and translate them into working software modules Environment Java J2EE JSP SOAP WSDL SQL PLSQL XML JDBC Eclipse Windows XP Oracle Skills CASSANDRA AMBARI HDFS MAPREDUCE OOZIE SQOOP HBASE FLUME MONGODB TERADATA CC C Git HBase Hive HTML JAVASCRIPT MapReduce PHP Pig Additional Information TECHNICAL SKILLS BigData Ecosystem HDFS MapReduce YARN Hive Pig Flume Sqoop HBase Oozie Zookeeper Ambari Spark also Spark Core Spark SQL Spark Streaming Spark MLlib GraphX Databases Microsoft SQL Server Oracle 12c PLSQL MySQL MongoDB Cassandra Teradata Programming Languages CC Java Scala Python Shell Scripting R Programming Python Web HTML CSS PHP JavaScript AngularJS NodeJS Version Control Git Ant Maven Operating System UNIX RedHat Linux CentOS Ubuntu Microsoft Windows Amazon AWS Amazon S3 Amazon EC2 Amazon RDS Tools IDE Eclipse IntelliJ NetBeans Maven Jenkin SBT",
    "unique_id": "410be9b8-3982-4520-80c5-990e6802e0d8"
}