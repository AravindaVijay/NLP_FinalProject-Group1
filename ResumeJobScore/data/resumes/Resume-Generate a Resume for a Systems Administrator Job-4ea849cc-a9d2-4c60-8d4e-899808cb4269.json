{
    "clean_data": "Data Analyst Python Developer Data Analystspan lPythonspan span lDeveloperspan Data Analyst Python Developer Capital Group Highly efficient Data ScientistData Analyst with 6 years of experience in Data Analysis Machine Learning Data mining with large data sets of Structured and Unstructured data Data Acquisition Data Validation Predictive modeling Data Visualization Web Scraping Adept in statistical programming languages like R and Python including Big Data technologies like Hadoop Hive Proficient in managing entire data science project life cycle and actively involved in all the phases of project life cycle including data acquisition data cleaning data engineering features scaling features engineering statistical modeling decision trees regression modelsclustering dimensionality reduction using Principal Component Analysis and Factor Analysis testing and validation using ROC plot K fold crossvalidation and data visualization Adept and deep understanding of Statistical modeling Multivariate Analysis model testing problem analysis model comparison and validation Expertise in transforming business requirements into analytical models designing algorithms building models developing data mining and reporting solutions that scale across a massive volume of structured and unstructured data Skilled in performing data parsing data manipulation and data preparation with methods including describe data contents compute descriptive statistics of data regex split and combine Remap merge subset reindex melt and reshape Experience in using various packages in R and pythonlike ggplot2 caret dplyr Rweka gmodels twitter NLP Reshape2 rjson plyr pandas NumPy Seaborn SciPy Matplotlib scikitlearn Beautiful Soup Extensive experience in Text Analytics generating data visualizations using R Python and creating dashboards using tools like Tableau Hands on experience with big data tools like Hadoop Spark Hive Pig PySpark Spark SQLPySpark Hands on experience in implementing LDA Naive Bayes and skilled in Random Forests Decision Trees Linear and Logistic Regression SVM Clustering neural networks Principle Component Analysis Good Knowledge in Proof of Concepts PoCs gap analysis and gathered necessary data for analysis from different sources prepared data for data exploration using data munging Good industry knowledge analytical problemsolving skills and ability to work well within a team as well as an individual Expertise in transforming business requirements into analytical models designing algorithms building models developing data mining and reporting solutions that scale across a massive volume of structured and unstructured data Experience in designing stunning visualizations using Tableau software and publishing and presenting dashboards Storyline on web and desktop platforms Experience and Technical proficiency in Designing Data Modeling Online Applications Solution Lead for Architecting Data WarehouseBusiness Intelligence Applications Experience with Data Analytics Data Reporting Adhoc Reporting Graphs Scales PivotTables and OLAP reporting Highly skilled in using Hadoop pig and Hive for basic analysis and extraction of data in the infrastructure to provide data summarization Highly skilled in using visualization tools like Tableau ggplot2 dash flask for creating dashboards Worked and extracted data from various database sources like Oracle SQL Server DB2 regularly accessing JIRA tool and other internal issue trackers for the Project development Highly creative innovative committed intellectually curious business savvy with good communication and interpersonal skills Extensive experience in Data Visualization including producing tables graphs listings using various procedures and tools such as Tableau Work Experience Data Analyst Python Developer Capital Group Los Angeles CA February 2018 to Present Capital Group is an American financial services company Capital offers a range of products more than 40 mutual funds through its subsidiary American Funds as well as separately managed accounts private equity investment services for high net worth investors in the US and a range of other offerings for institutional clients and individual investors globally Worked with a Fixed Income front office client to build a model for the Investment managers Portfolio Managers Traders to help make better investment decisions Created an aggregated report daily for the client to make investment decisions and help analyze market trends Built an internal visualization platform for the clients to view historic data make comparisons between various issuers analytics for different bonds and market The model collects merges daily data from market providers and applies different cleaning techniques to eliminate bad data points The model merges the daily data with the historical data and applies various quantitative algorithms to check the best fit for the day Captures the changes for each market to create a daily email alert to the client to help make better investment decisions Built the model on Azure platform using Python and Spark for the model development and Dash by plotly for visualizations Built REST APIs to easily add new analytics or issuers into the model Automate different workflows which are initiated manually with Python scripts and Unix shell scripting Create activate and program in Anaconda environment Worked on predictive analytics usecases using Python language Clean data and processed third party spending data into maneuverable deliverables within specific format with Excel macros and python libraries such as NumPy SQLAlchemy and matplotlib Used Pandas as API to put the data as time series and tabular format for manipulation and retrieval of data Helped with the migration from the old server to Jira database Matching Fields with Python scripts for transferring and verifying the information Analyze Format data using Machine Learning algorithm by Python ScikitLearn Experience in python Jupyter Scientific computing stack numpy scipy pandasand matplotlib Perform troubleshooting fixed and deployed many Python bug fixes of the two main applications that were a main source of data for both customers and internal customer service team Write Python scripts to parse JSON documents and load the data in database Generating various capacity planning reports graphical using Python packages like Numpy matplotlib Analyzing various logs that are been generating and predictingforecasting next occurrence of event with various Python libraries Created Autosys batch processes to fully automate the model to pick the latest as well as the best bond that fits best for that market Created a framework using plotly dash and flask for visualizing the trends and understanding patterns for each market using the history data Used python APIs for extracting daily data from multiple vendors Used Spark and SparkSQL for data integrations manipulationsWorked on a POC for creating a docker image on azure to run the model Environment Python Pyspark Spark SQL Plotly Dash Flask Post Man Microsoft Azure Autosys Docker Data ScientistData Analyst Python Anthem Richmond VA March 2017 to January 2018 Anthem Inc is an American health insurance company founded in the 1940s prior to 2014 known as WellPoint Inc It is the largest forprofit managed health care company in the Blue Cross and Blue Shield Association Data Lake Creation Data is collected from different sources as flat files and exported to cloud storage using AWS and Hadoop technologies Data analyzed reported and presented using Tableau dashboards Worked on creation of Orchestration engine that was controlling the complete flow of data ingestion using Sqoop pipelines dumping data into Hive and DQM layers were enabled Data processed in Hive and end results were reported using Tableau dashboards Customer Segmentation Developed 11 customer segments using unsupervised learning techniques like KMeans and Gaussian mixture models The clusters helped business simplify complex patterns to manageable set of 11 patterns that helped set strategic and tactical objectives pertaining to customer retention acquisition spend and loyalty Responsibilities Implemented Data Exploration to analyze patterns and to select features using Python SciPy Built Factor Analysis and Cluster Analysis models using Python SciPy to classify customers into different target groups Designed an AB experiment for testing the business performance of the new recommendation system Supported MapReduce Programs running on the cluster Evaluated business requirements and prepared detailed specifications that follow project guidelines required to develop written programs Participated in Data Acquisition with Data Engineer team to extract historical and realtime data by using Hadoop MapReduce and HDFS Communicated and presented default customers profiles along with reports using Python and Tableau analytical results and strategic implications to senior management for strategic decision making Developed scripts in Python to automate the customer query addressable system using python which decreased the time for solving the query of the customer by 45 Collaborated with other functional teams across the Risk and NonRisk groups to use standard methodologies and ensure a positive customer experience throughout the customer journey Performed Data Enrichment jobs to deal missing value to normalize data and to select features Developed multiple MapReduce jobs in java for data cleaning and preprocessing Analyzed the partitioned and bucketed data and compute various metrics for reporting Extracted data from Twitter using Java and Twitter API Parsed JSON formatted twitter data and uploaded to database Developed Hive queries for analysis and exported the result set from Hive to MySQL using Sqoop after processing the data Created HBase tables to store various data formats of data coming from different portfolios Worked on improving performance of existing Pig and Hive Queries Created reports and dashboards by using D3js and Tableau 9x to explain and communicate data insights significant features models scores and performance of new recommendation system to both technical and business teams Utilize SQL Excel and several MarketingWeb Analytics tools Google Analytics Bing Ads AdWords AdSense Criteo Smartly SurveyMonkey and Mailchimp in order to complete business marketing analysis and assessment Used Git 2x for version control with Data Engineer team and Data Scientists colleagues Used Agile methodology and SCRUM process for project developing KT with the client to understand their various Data Management systems and understanding the data Creating metadata and data dictionary for the future data use data refresh of the same client Structuring the Data Marts to store and organize the customers data Running SQL scripts creating indexes stored procedures for data analysis Data Lineage methodology for data mapping and maintaining data quality Prepared Scripts in Python and Shell for Automation of administration tasks Maintained PLSQL objects like packages triggers procedures etc Mapping flow of trade cycle data from source to target and documenting the same Performing QA on the data extracted transformed and exported to excel Participated in all phases of data mining data collection data cleaning developing models validation visualization and performed Gap analysis Extracted data from HDFS and prepared data for exploratory analysis using data munging Built models using Statistical techniques like Bayesian HMM and Machine Learning classification models like XG Boost SVM and Random Forest A highly immersive Data Science program involving Data Manipulation Visualization Web Scraping Machine Learning Python programming SQL GIT Unix Commands NoSQL MongoDB Hadoop Used pandas numpy seaborn scipy matplotlib scikitlearn NLTK in Python for developing various machine learning algorithms Worked on different data formats such as JSON XML and performed machine learning algorithms in Python Environment ER Studio 97 MDM GIT Unix Python SciPy NumPy Pandas StatsModel Plotly MySQL Excel Google Cloud Platform Tableau 9x D3js SVM Random Forests Nave Bayes Classifier AB experiment Git 2x AgileSCRUM MLLib SAS regression logistic regression Hadoop NoSQL Teradata OLTP random forest OLAP HDFS ODS NLTK SVM JSON XML MapReduce Data Scientist Yum Brands Louisville KY March 2016 to February 2017 Yum Brands Inc or Yum is an American fast food company A Fortune 500 corporation Yum operates the brands Taco Bell KFC Pizza Hut and WingStreet worldwide Based in Louisville Kentucky it is one of the worlds largest fast food restaurant companies in terms of system units with 43617 restaurants around the world in over 135 countries and territories Project Description The project involves data extraction and applying data integrity and analytical techniques for story telling from the data The major key performance indicators such as In store behavior price optimization and distribution and logistics optimization and improved order handling times etc using supervised and unsupervised machine learning techniques Responsibilities Applied Lean Six Sigma process improvement in plant and developed Capacity Calculation systems using purchase order tracking system and improvement inbound efficiency by 2356 Worked with Machine learning algorithms like Linear Regressions linear logistic etc SVMs Decision trees for classification of groups and analyzing most significant variables such as FTE Waiting times of purchase orders and Capacities available and applied process improvement techniques And calculated Process Cycle efficiency of 332 and identified value added and nonvalue added activities And utilized SAS for developing Pareto Chart for identifying highly impacting categories in modules to find the work force distribution and created various data visualization charts Performed univariate bivariate and multivariate analysis of approx 4890 tuples using bar charts box plots and histograms Participated in features engineering such as feature creating feature scaling and OneHot encoding with Scikitlearn Converted raw data to processed data by merging finding outliers errors trends missing values and distributions in the data Generated detailed report after validating the graphs using R and adjusting the variables to fit the model Worked on Clustering and factor analysis for classification of data using machine learning algorithms Developed Descriptive statistics and inferential statistics for Logistics optimization Average hours per job Value throughput data to at 95 confidence interval Written MapReduce code to process and parsing the data from various sources and storing parsed data into HBase and Hive using HBase Hive Integration Created SQL tables with referential integrity and developed advanced queries using stored procedures and functions using SQL server management studio Used Pandas NumPy seaborn SciPy Matplotlib Scikitlearn NLTK in Python for developing various machine learning algorithms and utilized machine learning algorithms such as linear regression multivariate regression naive Bayes Random Forests Kmeans KNN for data analysis Used packages like dplyr tidyr ggplot2 in R Studio for data visualization and generated scatter plot and high low graph to identify relation between different variables Worked on Business forecasting segmentation analysis and Data mining and prepared management reports defining the problem documenting the analysis and recommending courses of action to determine the best outcomes Worked on various Statistical models like DOE hypothesis testing Survey testing and queuing theory Experience with risk analysis root cause analysis cluster analysis correlation and optimization and Kmeans algorithm for clustering data into groups Coordinate with data scientists and senior technical staff to identify clients needs and document assumptions Environment SQL Server 2012 Jupyter R 312 Python MATLAB SSRS SSIS SSAS MongoDB HBase HDFS Hive Pig Microsoft office SQL Server Management Studio Business Intelligence Development Studio MS Access Data Analyst JP Morgan New York NY February 2014 to December 2015 Credit Card Fraud The purpose of this project was to fight against credit card fraud My team mainly focused on rebuilding credit card fraud detection model monitoring the model in production taking action if model performance degrades and working closely with business team to onboard new model Monthly Dashboard Project is to develop monthly dashboard that will help the business to see the usage of the cards Credit Debit on monthly basis with respect to their card categories like Signature Platinum Gold and Silver etc This dashboard also provides the activation status of newly issued cards With this dashboard business can easily track the history over latest one year Responsibilities Built scalable and deployable machine learning models Utilized Sqoop to ingest realtime data Used analytics libraries SciKit Learn MLLIB and MLxtend Extensively used Pythons multiple data science packages like Pandas NumPy matplotlib Seaborn SciPy Scikitlearn and NLTK Performed Exploratory Data Analysis trying to find trends and clusters Built models using techniques like Regression Tree based ensemble methods Time Series forecasting KNN Clustering and Isolation Forest methods Worked on data that was a combination of unstructured and structured data from multiple sources and automated the cleaning using Python scripts Extensively performed large data readwrites to and from csv and excel files using pandas Tasked with maintaining RDDs using SparkSQL Communicated and coordinated with other departments to collection business requirement Tackled highly imbalanced Fraud dataset using undersampling with ensemble methods oversampling and cost sensitive algorithms Improved fraud prediction performance by using random forest and gradient boosting for feature selection with Python Scikitlearn Implemented machine learning model logistic regression XGboost with Python Scikit learn Optimized algorithm with stochastic gradient descent algorithm Finetuned the algorithm parameter with manual tuning and automated tuning such as Bayesian Optimization Developed a technical brief based on the business brief This contains detailed steps and stages of developing and delivering the project including timelines After signoff from the client on technical brief started developing the SAS codes Wrote the data validation SAS codes with the help of Univariate Frequency procedures Summarising the data at customer level by joining the datasets of customer transaction dimension and from 3rd party sources Separately calculated the KPIs for Target and Mass campaigns at prepromopost periods with respective to their transactions spend and visits Also measured the KPIs at MoM Month on Month QoQ Quarter on Quarter and YoY Year on Year with respect to prepromopost Measured the ROI based on the differences prepromopost KPIs Extensively used SAS procedures like IMPORT EXPORT SORT FREQ MEANS FORMAT APPEND UNIVARIATE DATASETS and REPORT Standardised the data with the help of PROC STANDARD Implemented cluster analysis PROC CLUSTER and PROC FASTCLUS iteratively Worked extensively with data governance team to maintain data models Metadata and dictionaries Used Python to preprocess data and attempt to find insights Iteratively rebuild models dealing with changes in data and refining them over time Created and published multiple dashboards and reports using Tableau server Extensively used SQL queries for legacy data retrieval jobs Tasked with migrating the django database from MySQL to PostgreSQL Gained expertise in Data Visualization using matplotlib Bokeh and Plotly Responsible for maintaining and analyzing large datasets used to analyze risk by domain experts Developed Hive queries that compared new incoming data against historic data Built tables in Hive to store large volumes of data Used big data tools Spark Sparksql Mllib to conduct the real time analysis of credit card fraud based on AWS Performed Data audit QA of SAS codeprojects and sense check of results Accomplishments Accomplished 75 reduction in cycle time for automation of gathering and reporting of performance issues in Hadoop applications used by the company for ETL in Enterprise Data Management reducing team effort resulting in a net savings of 80000 per annum for the companys budget Won the Harbinger award for Value Engineering from the clients for lead the Standards and Compliance team in overseeing programming standards debugging and authorized migration to production environment 15 largescale data management software applications in Hadoop ecosystem Built Hadoop based data warehouse streamlined data ingestion distributed data storage data lake HDFS standardized and provided scalable data processing to monetize data effectively for a Bank of America Migrated raw data from Mainframes and extracted it to HDFS and Hive using sqoop to for preprocessing and structuring Collaborated with team directly interacting with clients in large scale warehousing of sensitive data using Hadoop ecosystem UNIX scripting Map Reduce and Hive and Extraction Transformation Loading Environment Spark Hadoop AWS SAS Enterprise Guide SASMACROS SASACCESS SASSTAT SASSQL ORACLE MSOFFICE Python scikitlearn pandas Numpy Machine Learning logistic regression XGboost Gradient Descent algorithm Bayesian optimization Tableau Data Scientist Travelers Insurance Pune Maharashtra August 2012 to August 2013 The Travelers Companies is an American insurance company It is the second largest writer of US commercial property casualty insurance and the third largest writer of US personal insurance through independent agents Project Predicting Customer Churn Project Description To predict the attrition in personal insurance products The Churn prediction model predicts a customers propensity to churn by using information about the customer such as household and financial data transactional data and behavioral data The inputs for the Churn prediction model are customer demographic data insurance policies premiums tenure claims complaints and the sentiment score from past surveys Responsibilities Aggregate all available information about the customer The data that is obtained for predicting the churn is classified in the following categories Demographic data such as age gender education marital status employment status income home ownership status and retirement plan Policyrelated data such as insurance lines number of policies in the household household tenure premium disposable income and insured cars Claims such as claim settlement duration number of claims that are filed and denied Complaints such as number of open and closed complaints Survey sentiment data Sentiment scores from past surveys are captured in the latest and average note attitude score fields The note attitude score is derived from customer negative feedback only If the note attitude is zero the customer is more satisfied while as the number increases satisfaction level decreases Responsible for building data analysis infrastructure to collect analyze and visualize data Data elements validation using exploratory data analysis univariate bivariate multivariate analysis Missing value treatment outlier capping and anomalies treatment using statistical methods Variable selection was done by making use of Rsquare and VIF values Deployed Machine Learning Logistic Regression and PCA to predict customer churn Environment Statistical tools R 330 Python 30 SQL Server MSExcel MSPowerPoint Data Analyst Travelers Insurance Pune Maharashtra June 2011 to August 2012 Intern Responsibilities Developed and implemented predictive models using Natural Language Processing Techniques and machinelearning algorithms such as linear regression classification multivariate regression Naive Bayes RandomForests Kmeans clustering KNN PCA and regularization for data analysis Designed and developed Natural Language Processing models for sentiment analysis Applied clustering algorithms ie Hierarchical Kmeans with help of Scikit and Scipy Developed visualizations and dashboards using ggplot Tableau Worked on development of data warehouse Data Lake and ETL systems using relational and non relationaltools like SQL No SQL Built and analyzed datasets using R SAS Matlab and Python in decreasing order of usage Participated in all phases of datamining datacollection datacleaning developingmodels validation visualization and performed Gapanalysis DataManipulation and Aggregation from different source using Nexus Toad BusinessObjects PowerBI and SmartView Implemented Agile Methodology for building an internal application Good knowledge of HadoopArchitecture and various components such as HDFS Job Tracker Task Tracker Name Node Data Node Secondary Name Node and MapReduce concepts As Architect delivered various complex OLAPdatabasescubes scorecards dashboards and reports Programmed a utility in Python that used multiple packages scipy numpy pandas Implemented Classification using supervised algorithms like LogisticRegression Decisiontrees KNN NaiveBayes Used Teradata15 utilities such as FastExport MLOAD for handling various tasks data migrationETL from OLTP Source Systems to OLAP Target Systems Maintenance in the testing team for System  Involved in preparation design of technical documents like Bus Matrix Document PPDM Model and LDM PDM Understanding the client business problems and analyzing the data by using appropriate Statistical models to generate insights Environment R 30 Erwin 95 Tableau 80 MDM QlikView ML Lib PLSQL HDFS Teradata 141 JSON HADOOP HDFS MapReduce PIG Spark R Studio MAHOUT JAVA HIVE AWS Education Bachelor of Technology in Information Technology Jawaharlal Nehru Technological University Hyderabad Telangana Skills APPLICATION DEVELOPMENT Hadoop HBase HDFS Hive MapReduce Pig PYTHON FLASK GGPLOT2 NUMPY REPORTING TOOLS VISIO XML JDBC MS ACCESS SQL CASSANDRA IMPALA MAPREDUCE Additional Information SKILLS Languages Python R Java 8 Packages ggplot2 caret dplyr Rweka gmodels RCurl C50 twitter NLP Reshape2 rjson plyr pandas numPy Seaborn sciPy matplot lib scikitlearn Beautiful Soup Rpy2 Web Technologies JDBC HTML5 DHTML and XML CSS3 Web Services WSDL Data Modelling Tools Erwin r 96 95 91 8x Rational Rose ERStudio MS Visio SAP Power designer Big Data Technologies Hadoop Hive HDFS MapReduce Pig Databases SQL Hive Impala Pig Spark SQL Databases SQLServer My SQL MS Access HDFS HBase Teradata Netezza MongoDB Cassandra Reporting Tools MS Office WordExcelPower Point Visio Tableau Crystal reports XI Business Intelligence SSRS Business Objects 5x 6x Cognos7060 Flask Dash ETL Tools Informatica Power Centre SSIS Version Control Tools SVM GitHub Project Execution Methodologies Ralph Kimball and Bill Inmon data warehousing methodology Rational Unified Process RUP Rapid Application Development RAD Joint Application Development JAD BI Tools Tableau Tableau Server Tableau Reader SAP Business Objects OBIEE QlikView SAP Business Intelligence Amazon Redshift or Azure Data Warehouse Operating System Windows Linux Unix Macintosh HD Red Hat",
    "entities": [
        "HDFS Communicated",
        "Univariate",
        "Oracle SQL Server",
        "Created Autosys",
        "SurveyMonkey",
        "a Bank of America Migrated",
        "OLAP Target Systems Maintenance",
        "Enterprise Data Management",
        "HDFS",
        "UNIX",
        "My SQL MS Access HDFS HBase Teradata Netezza",
        "Shell for Automation",
        "Scikitlearn Converted",
        "Data Science",
        "Data Lake",
        "Los Angeles",
        "XG Boost SVM",
        "Portfolio Managers Traders",
        "DQM",
        "Informatica Power Centre SSIS Version",
        "Louisville",
        "Python and Spark",
        "Data Analysis Machine Learning Data",
        "Hadoop",
        "Nehru Technological University",
        "Project Description",
        "Maintained",
        "Principal Component Analysis",
        "HBase",
        "Target",
        "Data Scientists",
        "Project Predicting Customer Churn Project Description",
        "Python",
        "Data Acquisition",
        "the Project development Highly",
        "Create",
        "SparkSQL",
        "Developed",
        "Developer Capital Group",
        "Tableau Data Scientist Travelers Insurance",
        "Implemented Classification",
        "Bayesian Optimization Developed",
        "Data Lineage",
        "XGboost Gradient Descent",
        "Hadoop MapReduce",
        "Developed Descriptive",
        "Performed Data Enrichment",
        "Anthem Inc",
        "Natural Language Processing Techniques",
        "Taco Bell",
        "SQL No SQL Built",
        "SciKit Learn",
        "LDA Naive Bayes",
        "Control Tools",
        "Unstructured",
        "Yum Brands Inc",
        "Responsibilities Implemented Data Exploration",
        "Collaborated",
        "Built",
        "Automate",
        "VIF",
        "KNN Clustering",
        "Isolation Forest",
        "XI Business Intelligence SSRS Business Objects",
        "STANDARD",
        "ROC",
        "Cluster Analysis",
        "NonRisk",
        "Capacity Calculation",
        "Spark",
        "Credit Debit",
        "Blue Shield Association Data Lake Creation Data",
        "Data Visualization",
        "linear",
        "Scikit",
        "The Travelers Companies",
        "Spark Sparksql Mllib",
        "Tableau Work",
        "API",
        "Complaints",
        "Multivariate Analysis",
        "OLTP Source Systems",
        "US",
        "Sqoop",
        "Gapanalysis DataManipulation",
        "Kentucky",
        "Data Analytics Data Reporting Adhoc Reporting Graphs Scales PivotTables",
        "Hadoop Hive Proficient",
        "Created",
        "KT",
        "Statistical",
        "AWS",
        "KNN",
        "the Blue Cross",
        "Text Analytics",
        "Time Series",
        "Utilized Sqoop",
        "Tableau Hands",
        "AWS Performed Data",
        "Regression Tree",
        "Random Forest",
        "HDFS Job Tracker Task Tracker",
        "java",
        "Python SciPy Built Factor Analysis",
        "SAS",
        "K",
        "Morgan New York",
        "Anthem Richmond VA",
        "Data Acquisition Data Validation Predictive",
        "Environment Python Pyspark Spark",
        "SQL",
        "WellPoint Inc",
        "OLTP",
        "Mainframes",
        "NLP",
        "Developer Capital Group Highly",
        "Hadoop Spark Hive Pig PySpark Spark SQLPySpark Hands",
        "GitHub Project Execution Methodologies",
        "Pandas NumPy",
        "Big Data Technologies Hadoop Hive HDFS MapReduce Pig",
        "Data Management",
        "Bayes Random Forests Kmeans",
        "OneHot",
        "Principle Component Analysis Good Knowledge in Proof of Concepts",
        "HadoopArchitecture",
        "Anaconda",
        "Big Data",
        "Hive",
        "Macintosh",
        "Erwin",
        "SAP Power",
        "Rsquare",
        "Factor Analysis",
        "Pandas",
        "Linear Regressions",
        "Supported MapReduce Programs",
        "MDM",
        "Python Environment ER Studio",
        "ETL",
        "Utilize SQL Excel",
        "SmartView Implemented Agile Methodology",
        "Python Scikitlearn Implemented",
        "Monthly Dashboard Project",
        "Performed",
        "OLAP",
        "Impala",
        "Nexus Toad BusinessObjects",
        "Built Hadoop",
        "Numpy Machine Learning",
        "Microsoft",
        "Random Forests Decision Trees Linear and Logistic Regression SVM",
        "DOE",
        "SparkSQL Communicated",
        "Present Capital Group",
        "Created HBase",
        "Pizza Hut",
        "Data",
        "Structured",
        "MapReduce",
        "Data Analyst Travelers",
        "Data Engineer",
        "Data ScientistData Analyst",
        "MLxtend",
        "Tableau",
        "Machine Learning",
        "HMM",
        "HBase Hive Integration Created",
        "SQL Server Management Studio Business Intelligence Development Studio MS Access Data",
        "Skills APPLICATION DEVELOPMENT Hadoop HBase",
        "Value",
        "FastExport",
        "MarketingWeb Analytics",
        "Node",
        "Python ScikitLearn Experience",
        "JSON XML"
    ],
    "experience": "Experience in using various packages in R and pythonlike ggplot2 caret dplyr Rweka gmodels twitter NLP Reshape2 rjson plyr pandas NumPy Seaborn SciPy Matplotlib scikitlearn Beautiful Soup Extensive experience in Text Analytics generating data visualizations using R Python and creating dashboards using tools like Tableau Hands on experience with big data tools like Hadoop Spark Hive Pig PySpark Spark SQLPySpark Hands on experience in implementing LDA Naive Bayes and skilled in Random Forests Decision Trees Linear and Logistic Regression SVM Clustering neural networks Principle Component Analysis Good Knowledge in Proof of Concepts PoCs gap analysis and gathered necessary data for analysis from different sources prepared data for data exploration using data munging Good industry knowledge analytical problemsolving skills and ability to work well within a team as well as an individual Expertise in transforming business requirements into analytical models designing algorithms building models developing data mining and reporting solutions that scale across a massive volume of structured and unstructured data Experience in designing stunning visualizations using Tableau software and publishing and presenting dashboards Storyline on web and desktop platforms Experience and Technical proficiency in Designing Data Modeling Online Applications Solution Lead for Architecting Data WarehouseBusiness Intelligence Applications Experience with Data Analytics Data Reporting Adhoc Reporting Graphs Scales PivotTables and OLAP reporting Highly skilled in using Hadoop pig and Hive for basic analysis and extraction of data in the infrastructure to provide data summarization Highly skilled in using visualization tools like Tableau ggplot2 dash flask for creating dashboards Worked and extracted data from various database sources like Oracle SQL Server DB2 regularly accessing JIRA tool and other internal issue trackers for the Project development Highly creative innovative committed intellectually curious business savvy with good communication and interpersonal skills Extensive experience in Data Visualization including producing tables graphs listings using various procedures and tools such as Tableau Work Experience Data Analyst Python Developer Capital Group Los Angeles CA February 2018 to Present Capital Group is an American financial services company Capital offers a range of products more than 40 mutual funds through its subsidiary American Funds as well as separately managed accounts private equity investment services for high net worth investors in the US and a range of other offerings for institutional clients and individual investors globally Worked with a Fixed Income front office client to build a model for the Investment managers Portfolio Managers Traders to help make better investment decisions Created an aggregated report daily for the client to make investment decisions and help analyze market trends Built an internal visualization platform for the clients to view historic data make comparisons between various issuers analytics for different bonds and market The model collects merges daily data from market providers and applies different cleaning techniques to eliminate bad data points The model merges the daily data with the historical data and applies various quantitative algorithms to check the best fit for the day Captures the changes for each market to create a daily email alert to the client to help make better investment decisions Built the model on Azure platform using Python and Spark for the model development and Dash by plotly for visualizations Built REST APIs to easily add new analytics or issuers into the model Automate different workflows which are initiated manually with Python scripts and Unix shell scripting Create activate and program in Anaconda environment Worked on predictive analytics usecases using Python language Clean data and processed third party spending data into maneuverable deliverables within specific format with Excel macros and python libraries such as NumPy SQLAlchemy and matplotlib Used Pandas as API to put the data as time series and tabular format for manipulation and retrieval of data Helped with the migration from the old server to Jira database Matching Fields with Python scripts for transferring and verifying the information Analyze Format data using Machine Learning algorithm by Python ScikitLearn Experience in python Jupyter Scientific computing stack numpy scipy pandasand matplotlib Perform troubleshooting fixed and deployed many Python bug fixes of the two main applications that were a main source of data for both customers and internal customer service team Write Python scripts to parse JSON documents and load the data in database Generating various capacity planning reports graphical using Python packages like Numpy matplotlib Analyzing various logs that are been generating and predictingforecasting next occurrence of event with various Python libraries Created Autosys batch processes to fully automate the model to pick the latest as well as the best bond that fits best for that market Created a framework using plotly dash and flask for visualizing the trends and understanding patterns for each market using the history data Used python APIs for extracting daily data from multiple vendors Used Spark and SparkSQL for data integrations manipulationsWorked on a POC for creating a docker image on azure to run the model Environment Python Pyspark Spark SQL Plotly Dash Flask Post Man Microsoft Azure Autosys Docker Data ScientistData Analyst Python Anthem Richmond VA March 2017 to January 2018 Anthem Inc is an American health insurance company founded in the 1940s prior to 2014 known as WellPoint Inc It is the largest forprofit managed health care company in the Blue Cross and Blue Shield Association Data Lake Creation Data is collected from different sources as flat files and exported to cloud storage using AWS and Hadoop technologies Data analyzed reported and presented using Tableau dashboards Worked on creation of Orchestration engine that was controlling the complete flow of data ingestion using Sqoop pipelines dumping data into Hive and DQM layers were enabled Data processed in Hive and end results were reported using Tableau dashboards Customer Segmentation Developed 11 customer segments using unsupervised learning techniques like KMeans and Gaussian mixture models The clusters helped business simplify complex patterns to manageable set of 11 patterns that helped set strategic and tactical objectives pertaining to customer retention acquisition spend and loyalty Responsibilities Implemented Data Exploration to analyze patterns and to select features using Python SciPy Built Factor Analysis and Cluster Analysis models using Python SciPy to classify customers into different target groups Designed an AB experiment for testing the business performance of the new recommendation system Supported MapReduce Programs running on the cluster Evaluated business requirements and prepared detailed specifications that follow project guidelines required to develop written programs Participated in Data Acquisition with Data Engineer team to extract historical and realtime data by using Hadoop MapReduce and HDFS Communicated and presented default customers profiles along with reports using Python and Tableau analytical results and strategic implications to senior management for strategic decision making Developed scripts in Python to automate the customer query addressable system using python which decreased the time for solving the query of the customer by 45 Collaborated with other functional teams across the Risk and NonRisk groups to use standard methodologies and ensure a positive customer experience throughout the customer journey Performed Data Enrichment jobs to deal missing value to normalize data and to select features Developed multiple MapReduce jobs in java for data cleaning and preprocessing Analyzed the partitioned and bucketed data and compute various metrics for reporting Extracted data from Twitter using Java and Twitter API Parsed JSON formatted twitter data and uploaded to database Developed Hive queries for analysis and exported the result set from Hive to MySQL using Sqoop after processing the data Created HBase tables to store various data formats of data coming from different portfolios Worked on improving performance of existing Pig and Hive Queries Created reports and dashboards by using D3js and Tableau 9x to explain and communicate data insights significant features models scores and performance of new recommendation system to both technical and business teams Utilize SQL Excel and several MarketingWeb Analytics tools Google Analytics Bing Ads AdWords AdSense Criteo Smartly SurveyMonkey and Mailchimp in order to complete business marketing analysis and assessment Used Git 2x for version control with Data Engineer team and Data Scientists colleagues Used Agile methodology and SCRUM process for project developing KT with the client to understand their various Data Management systems and understanding the data Creating metadata and data dictionary for the future data use data refresh of the same client Structuring the Data Marts to store and organize the customers data Running SQL scripts creating indexes stored procedures for data analysis Data Lineage methodology for data mapping and maintaining data quality Prepared Scripts in Python and Shell for Automation of administration tasks Maintained PLSQL objects like packages triggers procedures etc Mapping flow of trade cycle data from source to target and documenting the same Performing QA on the data extracted transformed and exported to excel Participated in all phases of data mining data collection data cleaning developing models validation visualization and performed Gap analysis Extracted data from HDFS and prepared data for exploratory analysis using data munging Built models using Statistical techniques like Bayesian HMM and Machine Learning classification models like XG Boost SVM and Random Forest A highly immersive Data Science program involving Data Manipulation Visualization Web Scraping Machine Learning Python programming SQL GIT Unix Commands NoSQL MongoDB Hadoop Used pandas numpy seaborn scipy matplotlib scikitlearn NLTK in Python for developing various machine learning algorithms Worked on different data formats such as JSON XML and performed machine learning algorithms in Python Environment ER Studio 97 MDM GIT Unix Python SciPy NumPy Pandas StatsModel Plotly MySQL Excel Google Cloud Platform Tableau 9x D3js SVM Random Forests Nave Bayes Classifier AB experiment Git 2x AgileSCRUM MLLib SAS regression logistic regression Hadoop NoSQL Teradata OLTP random forest OLAP HDFS ODS NLTK SVM JSON XML MapReduce Data Scientist Yum Brands Louisville KY March 2016 to February 2017 Yum Brands Inc or Yum is an American fast food company A Fortune 500 corporation Yum operates the brands Taco Bell KFC Pizza Hut and WingStreet worldwide Based in Louisville Kentucky it is one of the worlds largest fast food restaurant companies in terms of system units with 43617 restaurants around the world in over 135 countries and territories Project Description The project involves data extraction and applying data integrity and analytical techniques for story telling from the data The major key performance indicators such as In store behavior price optimization and distribution and logistics optimization and improved order handling times etc using supervised and unsupervised machine learning techniques Responsibilities Applied Lean Six Sigma process improvement in plant and developed Capacity Calculation systems using purchase order tracking system and improvement inbound efficiency by 2356 Worked with Machine learning algorithms like Linear Regressions linear logistic etc SVMs Decision trees for classification of groups and analyzing most significant variables such as FTE Waiting times of purchase orders and Capacities available and applied process improvement techniques And calculated Process Cycle efficiency of 332 and identified value added and nonvalue added activities And utilized SAS for developing Pareto Chart for identifying highly impacting categories in modules to find the work force distribution and created various data visualization charts Performed univariate bivariate and multivariate analysis of approx 4890 tuples using bar charts box plots and histograms Participated in features engineering such as feature creating feature scaling and OneHot encoding with Scikitlearn Converted raw data to processed data by merging finding outliers errors trends missing values and distributions in the data Generated detailed report after validating the graphs using R and adjusting the variables to fit the model Worked on Clustering and factor analysis for classification of data using machine learning algorithms Developed Descriptive statistics and inferential statistics for Logistics optimization Average hours per job Value throughput data to at 95 confidence interval Written MapReduce code to process and parsing the data from various sources and storing parsed data into HBase and Hive using HBase Hive Integration Created SQL tables with referential integrity and developed advanced queries using stored procedures and functions using SQL server management studio Used Pandas NumPy seaborn SciPy Matplotlib Scikitlearn NLTK in Python for developing various machine learning algorithms and utilized machine learning algorithms such as linear regression multivariate regression naive Bayes Random Forests Kmeans KNN for data analysis Used packages like dplyr tidyr ggplot2 in R Studio for data visualization and generated scatter plot and high low graph to identify relation between different variables Worked on Business forecasting segmentation analysis and Data mining and prepared management reports defining the problem documenting the analysis and recommending courses of action to determine the best outcomes Worked on various Statistical models like DOE hypothesis testing Survey testing and queuing theory Experience with risk analysis root cause analysis cluster analysis correlation and optimization and Kmeans algorithm for clustering data into groups Coordinate with data scientists and senior technical staff to identify clients needs and document assumptions Environment SQL Server 2012 Jupyter R 312 Python MATLAB SSRS SSIS SSAS MongoDB HBase HDFS Hive Pig Microsoft office SQL Server Management Studio Business Intelligence Development Studio MS Access Data Analyst JP Morgan New York NY February 2014 to December 2015 Credit Card Fraud The purpose of this project was to fight against credit card fraud My team mainly focused on rebuilding credit card fraud detection model monitoring the model in production taking action if model performance degrades and working closely with business team to onboard new model Monthly Dashboard Project is to develop monthly dashboard that will help the business to see the usage of the cards Credit Debit on monthly basis with respect to their card categories like Signature Platinum Gold and Silver etc This dashboard also provides the activation status of newly issued cards With this dashboard business can easily track the history over latest one year Responsibilities Built scalable and deployable machine learning models Utilized Sqoop to ingest realtime data Used analytics libraries SciKit Learn MLLIB and MLxtend Extensively used Pythons multiple data science packages like Pandas NumPy matplotlib Seaborn SciPy Scikitlearn and NLTK Performed Exploratory Data Analysis trying to find trends and clusters Built models using techniques like Regression Tree based ensemble methods Time Series forecasting KNN Clustering and Isolation Forest methods Worked on data that was a combination of unstructured and structured data from multiple sources and automated the cleaning using Python scripts Extensively performed large data readwrites to and from csv and excel files using pandas Tasked with maintaining RDDs using SparkSQL Communicated and coordinated with other departments to collection business requirement Tackled highly imbalanced Fraud dataset using undersampling with ensemble methods oversampling and cost sensitive algorithms Improved fraud prediction performance by using random forest and gradient boosting for feature selection with Python Scikitlearn Implemented machine learning model logistic regression XGboost with Python Scikit learn Optimized algorithm with stochastic gradient descent algorithm Finetuned the algorithm parameter with manual tuning and automated tuning such as Bayesian Optimization Developed a technical brief based on the business brief This contains detailed steps and stages of developing and delivering the project including timelines After signoff from the client on technical brief started developing the SAS codes Wrote the data validation SAS codes with the help of Univariate Frequency procedures Summarising the data at customer level by joining the datasets of customer transaction dimension and from 3rd party sources Separately calculated the KPIs for Target and Mass campaigns at prepromopost periods with respective to their transactions spend and visits Also measured the KPIs at MoM Month on Month QoQ Quarter on Quarter and YoY Year on Year with respect to prepromopost Measured the ROI based on the differences prepromopost KPIs Extensively used SAS procedures like IMPORT EXPORT SORT FREQ MEANS FORMAT APPEND UNIVARIATE DATASETS and REPORT Standardised the data with the help of PROC STANDARD Implemented cluster analysis PROC CLUSTER and PROC FASTCLUS iteratively Worked extensively with data governance team to maintain data models Metadata and dictionaries Used Python to preprocess data and attempt to find insights Iteratively rebuild models dealing with changes in data and refining them over time Created and published multiple dashboards and reports using Tableau server Extensively used SQL queries for legacy data retrieval jobs Tasked with migrating the django database from MySQL to PostgreSQL Gained expertise in Data Visualization using matplotlib Bokeh and Plotly Responsible for maintaining and analyzing large datasets used to analyze risk by domain experts Developed Hive queries that compared new incoming data against historic data Built tables in Hive to store large volumes of data Used big data tools Spark Sparksql Mllib to conduct the real time analysis of credit card fraud based on AWS Performed Data audit QA of SAS codeprojects and sense check of results Accomplishments Accomplished 75 reduction in cycle time for automation of gathering and reporting of performance issues in Hadoop applications used by the company for ETL in Enterprise Data Management reducing team effort resulting in a net savings of 80000 per annum for the companys budget Won the Harbinger award for Value Engineering from the clients for lead the Standards and Compliance team in overseeing programming standards debugging and authorized migration to production environment 15 largescale data management software applications in Hadoop ecosystem Built Hadoop based data warehouse streamlined data ingestion distributed data storage data lake HDFS standardized and provided scalable data processing to monetize data effectively for a Bank of America Migrated raw data from Mainframes and extracted it to HDFS and Hive using sqoop to for preprocessing and structuring Collaborated with team directly interacting with clients in large scale warehousing of sensitive data using Hadoop ecosystem UNIX scripting Map Reduce and Hive and Extraction Transformation Loading Environment Spark Hadoop AWS SAS Enterprise Guide SASMACROS SASACCESS SASSTAT SASSQL ORACLE MSOFFICE Python scikitlearn pandas Numpy Machine Learning logistic regression XGboost Gradient Descent algorithm Bayesian optimization Tableau Data Scientist Travelers Insurance Pune Maharashtra August 2012 to August 2013 The Travelers Companies is an American insurance company It is the second largest writer of US commercial property casualty insurance and the third largest writer of US personal insurance through independent agents Project Predicting Customer Churn Project Description To predict the attrition in personal insurance products The Churn prediction model predicts a customers propensity to churn by using information about the customer such as household and financial data transactional data and behavioral data The inputs for the Churn prediction model are customer demographic data insurance policies premiums tenure claims complaints and the sentiment score from past surveys Responsibilities Aggregate all available information about the customer The data that is obtained for predicting the churn is classified in the following categories Demographic data such as age gender education marital status employment status income home ownership status and retirement plan Policyrelated data such as insurance lines number of policies in the household household tenure premium disposable income and insured cars Claims such as claim settlement duration number of claims that are filed and denied Complaints such as number of open and closed complaints Survey sentiment data Sentiment scores from past surveys are captured in the latest and average note attitude score fields The note attitude score is derived from customer negative feedback only If the note attitude is zero the customer is more satisfied while as the number increases satisfaction level decreases Responsible for building data analysis infrastructure to collect analyze and visualize data Data elements validation using exploratory data analysis univariate bivariate multivariate analysis Missing value treatment outlier capping and anomalies treatment using statistical methods Variable selection was done by making use of Rsquare and VIF values Deployed Machine Learning Logistic Regression and PCA to predict customer churn Environment Statistical tools R 330 Python 30 SQL Server MSExcel MSPowerPoint Data Analyst Travelers Insurance Pune Maharashtra June 2011 to August 2012 Intern Responsibilities Developed and implemented predictive models using Natural Language Processing Techniques and machinelearning algorithms such as linear regression classification multivariate regression Naive Bayes RandomForests Kmeans clustering KNN PCA and regularization for data analysis Designed and developed Natural Language Processing models for sentiment analysis Applied clustering algorithms ie Hierarchical Kmeans with help of Scikit and Scipy Developed visualizations and dashboards using ggplot Tableau Worked on development of data warehouse Data Lake and ETL systems using relational and non relationaltools like SQL No SQL Built and analyzed datasets using R SAS Matlab and Python in decreasing order of usage Participated in all phases of datamining datacollection datacleaning developingmodels validation visualization and performed Gapanalysis DataManipulation and Aggregation from different source using Nexus Toad BusinessObjects PowerBI and SmartView Implemented Agile Methodology for building an internal application Good knowledge of HadoopArchitecture and various components such as HDFS Job Tracker Task Tracker Name Node Data Node Secondary Name Node and MapReduce concepts As Architect delivered various complex OLAPdatabasescubes scorecards dashboards and reports Programmed a utility in Python that used multiple packages scipy numpy pandas Implemented Classification using supervised algorithms like LogisticRegression Decisiontrees KNN NaiveBayes Used Teradata15 utilities such as FastExport MLOAD for handling various tasks data migrationETL from OLTP Source Systems to OLAP Target Systems Maintenance in the testing team for System   Involved in preparation design of technical documents like Bus Matrix Document PPDM Model and LDM PDM Understanding the client business problems and analyzing the data by using appropriate Statistical models to generate insights Environment R 30 Erwin 95 Tableau 80 MDM QlikView ML Lib PLSQL HDFS Teradata 141 JSON HADOOP HDFS MapReduce PIG Spark R Studio MAHOUT JAVA HIVE AWS Education Bachelor of Technology in Information Technology Jawaharlal Nehru Technological University Hyderabad Telangana Skills APPLICATION DEVELOPMENT Hadoop HBase HDFS Hive MapReduce Pig PYTHON FLASK GGPLOT2 NUMPY REPORTING TOOLS VISIO XML JDBC MS ACCESS SQL CASSANDRA IMPALA MAPREDUCE Additional Information SKILLS Languages Python R Java 8 Packages ggplot2 caret dplyr Rweka gmodels RCurl C50 twitter NLP Reshape2 rjson plyr pandas numPy Seaborn sciPy matplot lib scikitlearn Beautiful Soup Rpy2 Web Technologies JDBC HTML5 DHTML and XML CSS3 Web Services WSDL Data Modelling Tools Erwin r 96 95 91 8x Rational Rose ERStudio MS Visio SAP Power designer Big Data Technologies Hadoop Hive HDFS MapReduce Pig Databases SQL Hive Impala Pig Spark SQL Databases SQLServer My SQL MS Access HDFS HBase Teradata Netezza MongoDB Cassandra Reporting Tools MS Office WordExcelPower Point Visio Tableau Crystal reports XI Business Intelligence SSRS Business Objects 5x 6x Cognos7060 Flask Dash ETL Tools Informatica Power Centre SSIS Version Control Tools SVM GitHub Project Execution Methodologies Ralph Kimball and Bill Inmon data warehousing methodology Rational Unified Process RUP Rapid Application Development RAD Joint Application Development JAD BI Tools Tableau Tableau Server Tableau Reader SAP Business Objects OBIEE QlikView SAP Business Intelligence Amazon Redshift or Azure Data Warehouse Operating System Windows Linux Unix Macintosh HD Red Hat",
    "extracted_keywords": [
        "Data",
        "Analyst",
        "Python",
        "Developer",
        "Data",
        "Analystspan",
        "lPythonspan",
        "span",
        "lDeveloperspan",
        "Data",
        "Analyst",
        "Python",
        "Developer",
        "Capital",
        "Group",
        "Data",
        "ScientistData",
        "Analyst",
        "years",
        "experience",
        "Data",
        "Analysis",
        "Machine",
        "Learning",
        "Data",
        "mining",
        "data",
        "sets",
        "Structured",
        "Unstructured",
        "data",
        "Data",
        "Acquisition",
        "Data",
        "Validation",
        "Predictive",
        "Data",
        "Visualization",
        "Web",
        "Adept",
        "programming",
        "languages",
        "R",
        "Python",
        "Big",
        "Data",
        "technologies",
        "Hadoop",
        "Hive",
        "Proficient",
        "data",
        "science",
        "project",
        "life",
        "cycle",
        "phases",
        "project",
        "life",
        "cycle",
        "data",
        "acquisition",
        "data",
        "data",
        "engineering",
        "features",
        "features",
        "engineering",
        "modeling",
        "decision",
        "trees",
        "dimensionality",
        "reduction",
        "Principal",
        "Component",
        "Analysis",
        "Factor",
        "Analysis",
        "testing",
        "validation",
        "ROC",
        "plot",
        "K",
        "fold",
        "crossvalidation",
        "data",
        "visualization",
        "Adept",
        "understanding",
        "modeling",
        "Multivariate",
        "Analysis",
        "model",
        "testing",
        "problem",
        "analysis",
        "model",
        "comparison",
        "validation",
        "Expertise",
        "business",
        "requirements",
        "models",
        "algorithms",
        "building",
        "models",
        "data",
        "mining",
        "reporting",
        "solutions",
        "volume",
        "data",
        "data",
        "data",
        "manipulation",
        "data",
        "preparation",
        "methods",
        "describe",
        "data",
        "contents",
        "statistics",
        "data",
        "regex",
        "Remap",
        "merge",
        "subset",
        "reindex",
        "melt",
        "Experience",
        "packages",
        "R",
        "ggplot2",
        "caret",
        "dplyr",
        "Rweka",
        "twitter",
        "NLP",
        "Reshape2",
        "rjson",
        "plyr",
        "NumPy",
        "Seaborn",
        "SciPy",
        "Matplotlib",
        "Beautiful",
        "Soup",
        "experience",
        "Text",
        "Analytics",
        "data",
        "visualizations",
        "R",
        "Python",
        "dashboards",
        "tools",
        "Tableau",
        "Hands",
        "experience",
        "data",
        "tools",
        "Hadoop",
        "Spark",
        "Hive",
        "Pig",
        "PySpark",
        "Spark",
        "SQLPySpark",
        "Hands",
        "experience",
        "LDA",
        "Naive",
        "Bayes",
        "Random",
        "Forests",
        "Decision",
        "Trees",
        "Linear",
        "Logistic",
        "Regression",
        "SVM",
        "networks",
        "Principle",
        "Component",
        "Analysis",
        "Good",
        "Knowledge",
        "Proof",
        "Concepts",
        "PoCs",
        "gap",
        "analysis",
        "data",
        "analysis",
        "sources",
        "data",
        "data",
        "exploration",
        "data",
        "industry",
        "knowledge",
        "skills",
        "ability",
        "team",
        "Expertise",
        "business",
        "requirements",
        "models",
        "algorithms",
        "building",
        "models",
        "data",
        "mining",
        "reporting",
        "solutions",
        "volume",
        "data",
        "Experience",
        "visualizations",
        "Tableau",
        "software",
        "publishing",
        "dashboards",
        "Storyline",
        "web",
        "desktop",
        "platforms",
        "Experience",
        "proficiency",
        "Designing",
        "Data",
        "Modeling",
        "Online",
        "Applications",
        "Solution",
        "Lead",
        "Data",
        "WarehouseBusiness",
        "Intelligence",
        "Applications",
        "Experience",
        "Data",
        "Analytics",
        "Data",
        "Adhoc",
        "Reporting",
        "Graphs",
        "Scales",
        "PivotTables",
        "OLAP",
        "Hadoop",
        "pig",
        "Hive",
        "analysis",
        "extraction",
        "data",
        "infrastructure",
        "data",
        "summarization",
        "visualization",
        "tools",
        "Tableau",
        "ggplot2",
        "dash",
        "flask",
        "dashboards",
        "data",
        "database",
        "sources",
        "Oracle",
        "SQL",
        "Server",
        "DB2",
        "JIRA",
        "tool",
        "issue",
        "trackers",
        "Project",
        "development",
        "business",
        "communication",
        "skills",
        "experience",
        "Data",
        "Visualization",
        "tables",
        "graphs",
        "listings",
        "procedures",
        "tools",
        "Tableau",
        "Work",
        "Experience",
        "Data",
        "Analyst",
        "Python",
        "Developer",
        "Capital",
        "Group",
        "Los",
        "Angeles",
        "CA",
        "February",
        "Present",
        "Capital",
        "Group",
        "services",
        "company",
        "Capital",
        "range",
        "products",
        "funds",
        "American",
        "Funds",
        "equity",
        "investment",
        "services",
        "investors",
        "US",
        "range",
        "offerings",
        "clients",
        "investors",
        "Fixed",
        "Income",
        "office",
        "client",
        "model",
        "Investment",
        "managers",
        "Portfolio",
        "Managers",
        "Traders",
        "investment",
        "decisions",
        "report",
        "client",
        "investment",
        "decisions",
        "market",
        "trends",
        "visualization",
        "platform",
        "clients",
        "data",
        "comparisons",
        "issuers",
        "analytics",
        "bonds",
        "market",
        "model",
        "data",
        "market",
        "providers",
        "cleaning",
        "techniques",
        "data",
        "points",
        "model",
        "data",
        "data",
        "algorithms",
        "fit",
        "day",
        "changes",
        "market",
        "email",
        "alert",
        "client",
        "investment",
        "decisions",
        "model",
        "platform",
        "Python",
        "Spark",
        "model",
        "development",
        "Dash",
        "visualizations",
        "REST",
        "APIs",
        "analytics",
        "issuers",
        "model",
        "Automate",
        "workflows",
        "Python",
        "scripts",
        "Unix",
        "shell",
        "scripting",
        "Create",
        "activate",
        "program",
        "Anaconda",
        "environment",
        "analytics",
        "usecases",
        "Python",
        "language",
        "data",
        "party",
        "spending",
        "data",
        "deliverables",
        "format",
        "Excel",
        "macros",
        "python",
        "libraries",
        "NumPy",
        "SQLAlchemy",
        "Pandas",
        "API",
        "data",
        "time",
        "series",
        "format",
        "manipulation",
        "retrieval",
        "data",
        "migration",
        "server",
        "Jira",
        "database",
        "Matching",
        "Fields",
        "Python",
        "scripts",
        "information",
        "Analyze",
        "Format",
        "data",
        "Machine",
        "Learning",
        "algorithm",
        "Python",
        "ScikitLearn",
        "Experience",
        "python",
        "Jupyter",
        "Scientific",
        "computing",
        "stack",
        "numpy",
        "pandasand",
        "Perform",
        "troubleshooting",
        "Python",
        "bug",
        "fixes",
        "applications",
        "source",
        "data",
        "customers",
        "customer",
        "service",
        "team",
        "Python",
        "scripts",
        "documents",
        "data",
        "database",
        "capacity",
        "planning",
        "reports",
        "Python",
        "packages",
        "Numpy",
        "matplotlib",
        "logs",
        "occurrence",
        "event",
        "Python",
        "libraries",
        "Autosys",
        "batch",
        "processes",
        "model",
        "bond",
        "market",
        "framework",
        "dash",
        "flask",
        "trends",
        "patterns",
        "market",
        "history",
        "data",
        "python",
        "APIs",
        "data",
        "vendors",
        "Spark",
        "SparkSQL",
        "data",
        "integrations",
        "POC",
        "docker",
        "image",
        "azure",
        "model",
        "Environment",
        "Python",
        "Pyspark",
        "Spark",
        "SQL",
        "Dash",
        "Flask",
        "Post",
        "Man",
        "Microsoft",
        "Azure",
        "Autosys",
        "Docker",
        "Data",
        "ScientistData",
        "Analyst",
        "Python",
        "Anthem",
        "Richmond",
        "VA",
        "March",
        "January",
        "Anthem",
        "Inc",
        "health",
        "insurance",
        "company",
        "1940s",
        "WellPoint",
        "Inc",
        "forprofit",
        "health",
        "care",
        "company",
        "Blue",
        "Cross",
        "Blue",
        "Shield",
        "Association",
        "Data",
        "Lake",
        "Creation",
        "Data",
        "sources",
        "files",
        "cloud",
        "storage",
        "AWS",
        "Hadoop",
        "technologies",
        "Data",
        "Tableau",
        "dashboards",
        "creation",
        "Orchestration",
        "engine",
        "flow",
        "data",
        "ingestion",
        "Sqoop",
        "pipelines",
        "data",
        "Hive",
        "DQM",
        "layers",
        "Data",
        "Hive",
        "end",
        "results",
        "Tableau",
        "dashboards",
        "Customer",
        "Segmentation",
        "customer",
        "segments",
        "techniques",
        "KMeans",
        "mixture",
        "models",
        "clusters",
        "business",
        "patterns",
        "set",
        "patterns",
        "objectives",
        "customer",
        "retention",
        "acquisition",
        "spend",
        "loyalty",
        "Responsibilities",
        "Data",
        "Exploration",
        "patterns",
        "features",
        "Python",
        "SciPy",
        "Built",
        "Factor",
        "Analysis",
        "Cluster",
        "Analysis",
        "models",
        "Python",
        "SciPy",
        "customers",
        "target",
        "groups",
        "AB",
        "experiment",
        "business",
        "performance",
        "recommendation",
        "system",
        "MapReduce",
        "Programs",
        "cluster",
        "business",
        "requirements",
        "specifications",
        "project",
        "guidelines",
        "programs",
        "Data",
        "Acquisition",
        "Data",
        "Engineer",
        "team",
        "data",
        "Hadoop",
        "MapReduce",
        "HDFS",
        "Communicated",
        "default",
        "customers",
        "profiles",
        "reports",
        "Python",
        "Tableau",
        "results",
        "implications",
        "management",
        "decision",
        "scripts",
        "Python",
        "customer",
        "query",
        "system",
        "python",
        "time",
        "query",
        "customer",
        "Collaborated",
        "teams",
        "Risk",
        "NonRisk",
        "groups",
        "methodologies",
        "customer",
        "experience",
        "customer",
        "journey",
        "Performed",
        "Data",
        "Enrichment",
        "jobs",
        "value",
        "data",
        "features",
        "MapReduce",
        "jobs",
        "java",
        "data",
        "data",
        "metrics",
        "data",
        "Twitter",
        "Java",
        "Twitter",
        "API",
        "twitter",
        "data",
        "Developed",
        "Hive",
        "analysis",
        "result",
        "Hive",
        "MySQL",
        "Sqoop",
        "data",
        "Created",
        "HBase",
        "data",
        "formats",
        "data",
        "portfolios",
        "performance",
        "Pig",
        "Hive",
        "Queries",
        "reports",
        "dashboards",
        "D3js",
        "Tableau",
        "data",
        "insights",
        "features",
        "models",
        "scores",
        "performance",
        "recommendation",
        "system",
        "business",
        "teams",
        "Utilize",
        "SQL",
        "Excel",
        "MarketingWeb",
        "Analytics",
        "tools",
        "Google",
        "Analytics",
        "Bing",
        "Ads",
        "AdWords",
        "AdSense",
        "Criteo",
        "Smartly",
        "SurveyMonkey",
        "Mailchimp",
        "order",
        "business",
        "marketing",
        "analysis",
        "assessment",
        "Git",
        "version",
        "control",
        "Data",
        "Engineer",
        "team",
        "Data",
        "Scientists",
        "colleagues",
        "methodology",
        "SCRUM",
        "process",
        "project",
        "KT",
        "client",
        "Data",
        "Management",
        "systems",
        "data",
        "metadata",
        "data",
        "data",
        "data",
        "client",
        "Data",
        "Marts",
        "customers",
        "data",
        "SQL",
        "scripts",
        "indexes",
        "procedures",
        "data",
        "analysis",
        "Data",
        "Lineage",
        "methodology",
        "data",
        "mapping",
        "data",
        "quality",
        "Prepared",
        "Scripts",
        "Python",
        "Shell",
        "Automation",
        "administration",
        "tasks",
        "PLSQL",
        "objects",
        "packages",
        "procedures",
        "Mapping",
        "flow",
        "trade",
        "cycle",
        "data",
        "source",
        "Performing",
        "QA",
        "data",
        "phases",
        "data",
        "mining",
        "data",
        "collection",
        "data",
        "models",
        "validation",
        "visualization",
        "analysis",
        "data",
        "HDFS",
        "data",
        "analysis",
        "data",
        "models",
        "techniques",
        "Bayesian",
        "HMM",
        "Machine",
        "Learning",
        "classification",
        "models",
        "XG",
        "Boost",
        "SVM",
        "Random",
        "Forest",
        "Data",
        "Science",
        "program",
        "Data",
        "Manipulation",
        "Visualization",
        "Web",
        "Machine",
        "Learning",
        "Python",
        "programming",
        "SQL",
        "GIT",
        "Unix",
        "Commands",
        "NoSQL",
        "MongoDB",
        "Hadoop",
        "pandas",
        "numpy",
        "matplotlib",
        "scikitlearn",
        "NLTK",
        "Python",
        "machine",
        "algorithms",
        "data",
        "formats",
        "JSON",
        "XML",
        "machine",
        "algorithms",
        "Python",
        "Environment",
        "ER",
        "Studio",
        "MDM",
        "GIT",
        "Unix",
        "Python",
        "SciPy",
        "NumPy",
        "Pandas",
        "StatsModel",
        "Plotly",
        "Excel",
        "Google",
        "Cloud",
        "Platform",
        "Tableau",
        "9x",
        "SVM",
        "Random",
        "Forests",
        "Nave",
        "Bayes",
        "Classifier",
        "AB",
        "experiment",
        "Git",
        "AgileSCRUM",
        "MLLib",
        "SAS",
        "regression",
        "regression",
        "Hadoop",
        "NoSQL",
        "Teradata",
        "OLTP",
        "forest",
        "OLAP",
        "HDFS",
        "ODS",
        "NLTK",
        "SVM",
        "JSON",
        "XML",
        "MapReduce",
        "Data",
        "Scientist",
        "Yum",
        "Brands",
        "Louisville",
        "KY",
        "March",
        "February",
        "Yum",
        "Brands",
        "Inc",
        "Yum",
        "food",
        "company",
        "Fortune",
        "corporation",
        "Yum",
        "brands",
        "Taco",
        "Bell",
        "KFC",
        "Pizza",
        "Hut",
        "WingStreet",
        "Louisville",
        "Kentucky",
        "worlds",
        "food",
        "restaurant",
        "companies",
        "terms",
        "system",
        "units",
        "restaurants",
        "world",
        "countries",
        "territories",
        "Project",
        "Description",
        "project",
        "data",
        "extraction",
        "data",
        "integrity",
        "techniques",
        "story",
        "data",
        "performance",
        "indicators",
        "store",
        "behavior",
        "price",
        "optimization",
        "distribution",
        "logistics",
        "optimization",
        "order",
        "times",
        "machine",
        "techniques",
        "Responsibilities",
        "Lean",
        "Sigma",
        "process",
        "improvement",
        "plant",
        "Capacity",
        "Calculation",
        "systems",
        "purchase",
        "order",
        "tracking",
        "system",
        "improvement",
        "efficiency",
        "Machine",
        "algorithms",
        "Linear",
        "Regressions",
        "Decision",
        "trees",
        "classification",
        "groups",
        "variables",
        "FTE",
        "Waiting",
        "times",
        "purchase",
        "orders",
        "Capacities",
        "process",
        "improvement",
        "techniques",
        "Process",
        "Cycle",
        "efficiency",
        "value",
        "activities",
        "SAS",
        "Pareto",
        "Chart",
        "categories",
        "modules",
        "work",
        "force",
        "distribution",
        "data",
        "visualization",
        "charts",
        "bivariate",
        "multivariate",
        "analysis",
        "approx",
        "tuples",
        "bar",
        "charts",
        "box",
        "plots",
        "histograms",
        "features",
        "engineering",
        "feature",
        "feature",
        "scaling",
        "OneHot",
        "encoding",
        "Scikitlearn",
        "data",
        "data",
        "outliers",
        "errors",
        "trends",
        "values",
        "distributions",
        "data",
        "report",
        "graphs",
        "R",
        "variables",
        "model",
        "Clustering",
        "factor",
        "analysis",
        "classification",
        "data",
        "machine",
        "learning",
        "statistics",
        "statistics",
        "Logistics",
        "optimization",
        "hours",
        "job",
        "Value",
        "data",
        "confidence",
        "interval",
        "Written",
        "MapReduce",
        "code",
        "data",
        "sources",
        "data",
        "HBase",
        "Hive",
        "HBase",
        "Hive",
        "Integration",
        "SQL",
        "tables",
        "integrity",
        "queries",
        "procedures",
        "functions",
        "SQL",
        "server",
        "management",
        "studio",
        "Pandas",
        "NumPy",
        "SciPy",
        "Matplotlib",
        "Scikitlearn",
        "NLTK",
        "Python",
        "machine",
        "algorithms",
        "machine",
        "learning",
        "algorithms",
        "linear",
        "regression",
        "multivariate",
        "regression",
        "Bayes",
        "Random",
        "Forests",
        "Kmeans",
        "KNN",
        "data",
        "analysis",
        "packages",
        "tidyr",
        "ggplot2",
        "R",
        "Studio",
        "data",
        "visualization",
        "scatter",
        "plot",
        "graph",
        "relation",
        "variables",
        "Business",
        "forecasting",
        "segmentation",
        "analysis",
        "Data",
        "mining",
        "management",
        "reports",
        "problem",
        "analysis",
        "courses",
        "action",
        "outcomes",
        "models",
        "DOE",
        "hypothesis",
        "testing",
        "Survey",
        "testing",
        "theory",
        "Experience",
        "risk",
        "analysis",
        "root",
        "analysis",
        "cluster",
        "analysis",
        "correlation",
        "optimization",
        "Kmeans",
        "algorithm",
        "data",
        "groups",
        "Coordinate",
        "data",
        "scientists",
        "staff",
        "clients",
        "needs",
        "document",
        "assumptions",
        "Environment",
        "SQL",
        "Server",
        "Jupyter",
        "R",
        "Python",
        "MATLAB",
        "SSRS",
        "SSIS",
        "SSAS",
        "MongoDB",
        "HBase",
        "HDFS",
        "Hive",
        "Pig",
        "Microsoft",
        "office",
        "SQL",
        "Server",
        "Management",
        "Studio",
        "Business",
        "Intelligence",
        "Development",
        "Studio",
        "MS",
        "Access",
        "Data",
        "Analyst",
        "JP",
        "Morgan",
        "New",
        "York",
        "NY",
        "February",
        "December",
        "Credit",
        "Card",
        "Fraud",
        "purpose",
        "project",
        "credit",
        "card",
        "fraud",
        "team",
        "credit",
        "card",
        "fraud",
        "detection",
        "model",
        "model",
        "production",
        "action",
        "model",
        "performance",
        "degrades",
        "business",
        "team",
        "model",
        "Monthly",
        "Dashboard",
        "Project",
        "dashboard",
        "business",
        "usage",
        "cards",
        "Credit",
        "Debit",
        "basis",
        "respect",
        "card",
        "categories",
        "Signature",
        "Platinum",
        "Gold",
        "Silver",
        "dashboard",
        "activation",
        "status",
        "cards",
        "dashboard",
        "business",
        "history",
        "year",
        "Responsibilities",
        "machine",
        "learning",
        "models",
        "Sqoop",
        "data",
        "analytics",
        "SciKit",
        "MLLIB",
        "MLxtend",
        "Pythons",
        "data",
        "science",
        "packages",
        "Pandas",
        "NumPy",
        "matplotlib",
        "Seaborn",
        "SciPy",
        "Scikitlearn",
        "NLTK",
        "Performed",
        "Exploratory",
        "Data",
        "Analysis",
        "trends",
        "clusters",
        "models",
        "techniques",
        "Regression",
        "Tree",
        "methods",
        "Time",
        "Series",
        "KNN",
        "Clustering",
        "Isolation",
        "Forest",
        "methods",
        "data",
        "combination",
        "data",
        "sources",
        "cleaning",
        "Python",
        "scripts",
        "data",
        "csv",
        "files",
        "pandas",
        "RDDs",
        "SparkSQL",
        "Communicated",
        "departments",
        "collection",
        "business",
        "requirement",
        "Tackled",
        "Fraud",
        "dataset",
        "methods",
        "algorithms",
        "fraud",
        "prediction",
        "performance",
        "forest",
        "gradient",
        "feature",
        "selection",
        "Python",
        "Scikitlearn",
        "machine",
        "model",
        "regression",
        "XGboost",
        "Python",
        "Scikit",
        "algorithm",
        "descent",
        "algorithm",
        "algorithm",
        "parameter",
        "tuning",
        "tuning",
        "Bayesian",
        "Optimization",
        "brief",
        "business",
        "brief",
        "steps",
        "stages",
        "project",
        "timelines",
        "signoff",
        "client",
        "brief",
        "SAS",
        "codes",
        "data",
        "validation",
        "SAS",
        "codes",
        "help",
        "Univariate",
        "Frequency",
        "procedures",
        "data",
        "customer",
        "level",
        "datasets",
        "customer",
        "transaction",
        "dimension",
        "party",
        "sources",
        "KPIs",
        "Target",
        "Mass",
        "campaigns",
        "periods",
        "transactions",
        "visits",
        "KPIs",
        "MoM",
        "Month",
        "Month",
        "QoQ",
        "Quarter",
        "Quarter",
        "YoY",
        "Year",
        "Year",
        "respect",
        "ROI",
        "differences",
        "KPIs",
        "SAS",
        "procedures",
        "IMPORT",
        "EXPORT",
        "SORT",
        "FREQ",
        "MEANS",
        "FORMAT",
        "APPEND",
        "UNIVARIATE",
        "DATASETS",
        "REPORT",
        "data",
        "help",
        "PROC",
        "cluster",
        "analysis",
        "PROC",
        "CLUSTER",
        "PROC",
        "data",
        "governance",
        "team",
        "data",
        "models",
        "Metadata",
        "dictionaries",
        "Python",
        "data",
        "attempt",
        "insights",
        "models",
        "changes",
        "data",
        "time",
        "dashboards",
        "reports",
        "Tableau",
        "server",
        "SQL",
        "queries",
        "legacy",
        "data",
        "retrieval",
        "jobs",
        "django",
        "database",
        "MySQL",
        "PostgreSQL",
        "expertise",
        "Data",
        "Visualization",
        "matplotlib",
        "Bokeh",
        "datasets",
        "risk",
        "domain",
        "experts",
        "Developed",
        "Hive",
        "data",
        "data",
        "tables",
        "Hive",
        "volumes",
        "data",
        "data",
        "tools",
        "Spark",
        "Sparksql",
        "Mllib",
        "time",
        "analysis",
        "credit",
        "card",
        "fraud",
        "AWS",
        "Performed",
        "Data",
        "audit",
        "QA",
        "SAS",
        "codeprojects",
        "sense",
        "check",
        "results",
        "Accomplishments",
        "reduction",
        "cycle",
        "time",
        "automation",
        "gathering",
        "reporting",
        "performance",
        "issues",
        "Hadoop",
        "applications",
        "company",
        "ETL",
        "Enterprise",
        "Data",
        "Management",
        "team",
        "effort",
        "savings",
        "companys",
        "budget",
        "Won",
        "Harbinger",
        "award",
        "Value",
        "Engineering",
        "clients",
        "lead",
        "Standards",
        "Compliance",
        "team",
        "programming",
        "standards",
        "debugging",
        "migration",
        "production",
        "environment",
        "largescale",
        "data",
        "management",
        "software",
        "applications",
        "Hadoop",
        "ecosystem",
        "Built",
        "Hadoop",
        "data",
        "warehouse",
        "data",
        "ingestion",
        "data",
        "storage",
        "data",
        "lake",
        "HDFS",
        "data",
        "processing",
        "data",
        "Bank",
        "America",
        "data",
        "Mainframes",
        "HDFS",
        "Hive",
        "sqoop",
        "Collaborated",
        "team",
        "clients",
        "scale",
        "warehousing",
        "data",
        "Hadoop",
        "ecosystem",
        "UNIX",
        "Map",
        "Reduce",
        "Hive",
        "Extraction",
        "Transformation",
        "Loading",
        "Environment",
        "Spark",
        "Hadoop",
        "AWS",
        "SAS",
        "Enterprise",
        "Guide",
        "SASMACROS",
        "SASACCESS",
        "SASSQL",
        "MSOFFICE",
        "Python",
        "Numpy",
        "Machine",
        "Learning",
        "regression",
        "XGboost",
        "Gradient",
        "Descent",
        "algorithm",
        "Bayesian",
        "optimization",
        "Tableau",
        "Data",
        "Scientist",
        "Travelers",
        "Insurance",
        "Pune",
        "Maharashtra",
        "August",
        "August",
        "Travelers",
        "Companies",
        "insurance",
        "company",
        "writer",
        "US",
        "property",
        "casualty",
        "insurance",
        "writer",
        "US",
        "insurance",
        "agents",
        "Project",
        "Predicting",
        "Customer",
        "Churn",
        "Project",
        "Description",
        "attrition",
        "insurance",
        "products",
        "Churn",
        "prediction",
        "model",
        "customers",
        "propensity",
        "information",
        "customer",
        "household",
        "data",
        "data",
        "data",
        "inputs",
        "Churn",
        "prediction",
        "model",
        "customer",
        "data",
        "insurance",
        "policies",
        "premiums",
        "tenure",
        "complaints",
        "sentiment",
        "score",
        "surveys",
        "Responsibilities",
        "information",
        "customer",
        "data",
        "churn",
        "categories",
        "data",
        "age",
        "gender",
        "education",
        "status",
        "employment",
        "status",
        "income",
        "home",
        "ownership",
        "status",
        "retirement",
        "plan",
        "Policyrelated",
        "data",
        "insurance",
        "lines",
        "number",
        "policies",
        "household",
        "household",
        "tenure",
        "premium",
        "income",
        "cars",
        "Claims",
        "claim",
        "settlement",
        "duration",
        "number",
        "claims",
        "Complaints",
        "number",
        "complaints",
        "Survey",
        "sentiment",
        "data",
        "Sentiment",
        "scores",
        "surveys",
        "note",
        "attitude",
        "score",
        "note",
        "attitude",
        "score",
        "customer",
        "feedback",
        "note",
        "attitude",
        "customer",
        "number",
        "satisfaction",
        "level",
        "data",
        "analysis",
        "infrastructure",
        "data",
        "Data",
        "elements",
        "validation",
        "data",
        "analysis",
        "bivariate",
        "multivariate",
        "analysis",
        "value",
        "treatment",
        "outlier",
        "capping",
        "anomalies",
        "treatment",
        "methods",
        "selection",
        "use",
        "Rsquare",
        "VIF",
        "values",
        "Machine",
        "Learning",
        "Logistic",
        "Regression",
        "PCA",
        "customer",
        "churn",
        "Environment",
        "tools",
        "R",
        "Python",
        "SQL",
        "Server",
        "MSExcel",
        "MSPowerPoint",
        "Data",
        "Analyst",
        "Travelers",
        "Insurance",
        "Pune",
        "Maharashtra",
        "June",
        "August",
        "Intern",
        "Responsibilities",
        "models",
        "Natural",
        "Language",
        "Processing",
        "Techniques",
        "machinelearning",
        "algorithms",
        "linear",
        "regression",
        "classification",
        "multivariate",
        "regression",
        "Naive",
        "Bayes",
        "RandomForests",
        "Kmeans",
        "KNN",
        "PCA",
        "regularization",
        "data",
        "analysis",
        "Natural",
        "Language",
        "Processing",
        "models",
        "sentiment",
        "analysis",
        "algorithms",
        "Kmeans",
        "help",
        "Scikit",
        "Scipy",
        "visualizations",
        "dashboards",
        "ggplot",
        "Tableau",
        "development",
        "data",
        "warehouse",
        "Data",
        "Lake",
        "ETL",
        "systems",
        "non",
        "relationaltools",
        "SQL",
        "SQL",
        "datasets",
        "R",
        "SAS",
        "Matlab",
        "Python",
        "order",
        "usage",
        "phases",
        "datacollection",
        "datacleaning",
        "validation",
        "visualization",
        "Gapanalysis",
        "DataManipulation",
        "Aggregation",
        "source",
        "Nexus",
        "Toad",
        "BusinessObjects",
        "PowerBI",
        "SmartView",
        "Agile",
        "Methodology",
        "application",
        "knowledge",
        "HadoopArchitecture",
        "components",
        "HDFS",
        "Job",
        "Tracker",
        "Task",
        "Tracker",
        "Name",
        "Node",
        "Data",
        "Node",
        "Secondary",
        "Name",
        "Node",
        "MapReduce",
        "concepts",
        "OLAPdatabasescubes",
        "scorecards",
        "dashboards",
        "reports",
        "utility",
        "Python",
        "packages",
        "pandas",
        "Classification",
        "algorithms",
        "LogisticRegression",
        "Decisiontrees",
        "KNN",
        "NaiveBayes",
        "Teradata15",
        "utilities",
        "FastExport",
        "MLOAD",
        "tasks",
        "data",
        "migrationETL",
        "OLTP",
        "Source",
        "Systems",
        "OLAP",
        "Target",
        "Systems",
        "Maintenance",
        "testing",
        "team",
        "System",
        "preparation",
        "design",
        "documents",
        "Bus",
        "Matrix",
        "Document",
        "PPDM",
        "Model",
        "LDM",
        "PDM",
        "client",
        "business",
        "problems",
        "data",
        "models",
        "insights",
        "Environment",
        "R",
        "Erwin",
        "Tableau",
        "MDM",
        "QlikView",
        "ML",
        "Lib",
        "PLSQL",
        "Teradata",
        "JSON",
        "HADOOP",
        "HDFS",
        "MapReduce",
        "PIG",
        "Spark",
        "R",
        "Studio",
        "MAHOUT",
        "HIVE",
        "AWS",
        "Education",
        "Bachelor",
        "Technology",
        "Information",
        "Technology",
        "Jawaharlal",
        "Nehru",
        "Technological",
        "University",
        "Hyderabad",
        "Telangana",
        "Skills",
        "APPLICATION",
        "DEVELOPMENT",
        "Hadoop",
        "HBase",
        "HDFS",
        "Hive",
        "MapReduce",
        "Pig",
        "PYTHON",
        "FLASK",
        "GGPLOT2",
        "NUMPY",
        "REPORTING",
        "VISIO",
        "XML",
        "JDBC",
        "MS",
        "ACCESS",
        "SQL",
        "CASSANDRA",
        "IMPALA",
        "Additional",
        "Information",
        "SKILLS",
        "Languages",
        "Python",
        "R",
        "Java",
        "Packages",
        "ggplot2",
        "dplyr",
        "Rweka",
        "RCurl",
        "C50",
        "twitter",
        "NLP",
        "Reshape2",
        "rjson",
        "plyr",
        "numPy",
        "Seaborn",
        "sciPy",
        "matplot",
        "lib",
        "Beautiful",
        "Soup",
        "Rpy2",
        "Web",
        "Technologies",
        "JDBC",
        "HTML5",
        "DHTML",
        "XML",
        "CSS3",
        "Web",
        "Services",
        "WSDL",
        "Data",
        "Modelling",
        "Tools",
        "Erwin",
        "r",
        "8x",
        "Rational",
        "Rose",
        "ERStudio",
        "MS",
        "Visio",
        "SAP",
        "Power",
        "designer",
        "Big",
        "Data",
        "Technologies",
        "Hadoop",
        "Hive",
        "HDFS",
        "MapReduce",
        "Pig",
        "SQL",
        "Hive",
        "Impala",
        "Pig",
        "Spark",
        "SQL",
        "Databases",
        "SQLServer",
        "SQL",
        "MS",
        "Access",
        "HDFS",
        "HBase",
        "Teradata",
        "Netezza",
        "MongoDB",
        "Cassandra",
        "Reporting",
        "Tools",
        "MS",
        "Office",
        "WordExcelPower",
        "Point",
        "Visio",
        "Tableau",
        "Crystal",
        "XI",
        "Business",
        "Intelligence",
        "SSRS",
        "Business",
        "5x",
        "Cognos7060",
        "Flask",
        "Dash",
        "ETL",
        "Tools",
        "Informatica",
        "Power",
        "Centre",
        "SSIS",
        "Version",
        "Control",
        "Tools",
        "SVM",
        "GitHub",
        "Project",
        "Execution",
        "Methodologies",
        "Ralph",
        "Kimball",
        "Bill",
        "Inmon",
        "warehousing",
        "methodology",
        "Rational",
        "Unified",
        "Process",
        "RUP",
        "Rapid",
        "Application",
        "Development",
        "RAD",
        "Joint",
        "Application",
        "Development",
        "JAD",
        "BI",
        "Tools",
        "Tableau",
        "Tableau",
        "Server",
        "Tableau",
        "Reader",
        "SAP",
        "Business",
        "OBIEE",
        "QlikView",
        "SAP",
        "Business",
        "Intelligence",
        "Amazon",
        "Redshift",
        "Azure",
        "Data",
        "Warehouse",
        "Operating",
        "System",
        "Windows",
        "Linux",
        "Unix",
        "Macintosh",
        "HD",
        "Red",
        "Hat"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T22:28:38.183153",
    "resume_data": "Data Analyst Python Developer Data Analystspan lPythonspan span lDeveloperspan Data Analyst Python Developer Capital Group Highly efficient Data ScientistData Analyst with 6 years of experience in Data Analysis Machine Learning Data mining with large data sets of Structured and Unstructured data Data Acquisition Data Validation Predictive modeling Data Visualization Web Scraping Adept in statistical programming languages like R and Python including Big Data technologies like Hadoop Hive Proficient in managing entire data science project life cycle and actively involved in all the phases of project life cycle including data acquisition data cleaning data engineering features scaling features engineering statistical modeling decision trees regression modelsclustering dimensionality reduction using Principal Component Analysis and Factor Analysis testing and validation using ROC plot K fold crossvalidation and data visualization Adept and deep understanding of Statistical modeling Multivariate Analysis model testing problem analysis model comparison and validation Expertise in transforming business requirements into analytical models designing algorithms building models developing data mining and reporting solutions that scale across a massive volume of structured and unstructured data Skilled in performing data parsing data manipulation and data preparation with methods including describe data contents compute descriptive statistics of data regex split and combine Remap merge subset reindex melt and reshape Experience in using various packages in R and pythonlike ggplot2 caret dplyr Rweka gmodels twitter NLP Reshape2 rjson plyr pandas NumPy Seaborn SciPy Matplotlib scikitlearn Beautiful Soup Extensive experience in Text Analytics generating data visualizations using R Python and creating dashboards using tools like Tableau Hands on experience with big data tools like Hadoop Spark Hive Pig PySpark Spark SQLPySpark Hands on experience in implementing LDA Naive Bayes and skilled in Random Forests Decision Trees Linear and Logistic Regression SVM Clustering neural networks Principle Component Analysis Good Knowledge in Proof of Concepts PoCs gap analysis and gathered necessary data for analysis from different sources prepared data for data exploration using data munging Good industry knowledge analytical problemsolving skills and ability to work well within a team as well as an individual Expertise in transforming business requirements into analytical models designing algorithms building models developing data mining and reporting solutions that scale across a massive volume of structured and unstructured data Experience in designing stunning visualizations using Tableau software and publishing and presenting dashboards Storyline on web and desktop platforms Experience and Technical proficiency in Designing Data Modeling Online Applications Solution Lead for Architecting Data WarehouseBusiness Intelligence Applications Experience with Data Analytics Data Reporting Adhoc Reporting Graphs Scales PivotTables and OLAP reporting Highly skilled in using Hadoop pig and Hive for basic analysis and extraction of data in the infrastructure to provide data summarization Highly skilled in using visualization tools like Tableau ggplot2 dash flask for creating dashboards Worked and extracted data from various database sources like Oracle SQL Server DB2 regularly accessing JIRA tool and other internal issue trackers for the Project development Highly creative innovative committed intellectually curious business savvy with good communication and interpersonal skills Extensive experience in Data Visualization including producing tables graphs listings using various procedures and tools such as Tableau Work Experience Data Analyst Python Developer Capital Group Los Angeles CA February 2018 to Present Capital Group is an American financial services company Capital offers a range of products more than 40 mutual funds through its subsidiary American Funds as well as separately managed accounts private equity investment services for high net worth investors in the US and a range of other offerings for institutional clients and individual investors globally Worked with a Fixed Income front office client to build a model for the Investment managers Portfolio Managers Traders to help make better investment decisions Created an aggregated report daily for the client to make investment decisions and help analyze market trends Built an internal visualization platform for the clients to view historic data make comparisons between various issuers analytics for different bonds and market The model collects merges daily data from market providers and applies different cleaning techniques to eliminate bad data points The model merges the daily data with the historical data and applies various quantitative algorithms to check the best fit for the day Captures the changes for each market to create a daily email alert to the client to help make better investment decisions Built the model on Azure platform using Python and Spark for the model development and Dash by plotly for visualizations Built REST APIs to easily add new analytics or issuers into the model Automate different workflows which are initiated manually with Python scripts and Unix shell scripting Create activate and program in Anaconda environment Worked on predictive analytics usecases using Python language Clean data and processed third party spending data into maneuverable deliverables within specific format with Excel macros and python libraries such as NumPy SQLAlchemy and matplotlib Used Pandas as API to put the data as time series and tabular format for manipulation and retrieval of data Helped with the migration from the old server to Jira database Matching Fields with Python scripts for transferring and verifying the information Analyze Format data using Machine Learning algorithm by Python ScikitLearn Experience in python Jupyter Scientific computing stack numpy scipy pandasand matplotlib Perform troubleshooting fixed and deployed many Python bug fixes of the two main applications that were a main source of data for both customers and internal customer service team Write Python scripts to parse JSON documents and load the data in database Generating various capacity planning reports graphical using Python packages like Numpy matplotlib Analyzing various logs that are been generating and predictingforecasting next occurrence of event with various Python libraries Created Autosys batch processes to fully automate the model to pick the latest as well as the best bond that fits best for that market Created a framework using plotly dash and flask for visualizing the trends and understanding patterns for each market using the history data Used python APIs for extracting daily data from multiple vendors Used Spark and SparkSQL for data integrations manipulationsWorked on a POC for creating a docker image on azure to run the model Environment Python Pyspark Spark SQL Plotly Dash Flask Post Man Microsoft Azure Autosys Docker Data ScientistData Analyst Python Anthem Richmond VA March 2017 to January 2018 Anthem Inc is an American health insurance company founded in the 1940s prior to 2014 known as WellPoint Inc It is the largest forprofit managed health care company in the Blue Cross and Blue Shield Association Data Lake Creation Data is collected from different sources as flat files and exported to cloud storage using AWS and Hadoop technologies Data analyzed reported and presented using Tableau dashboards Worked on creation of Orchestration engine that was controlling the complete flow of data ingestion using Sqoop pipelines dumping data into Hive and DQM layers were enabled Data processed in Hive and end results were reported using Tableau dashboards Customer Segmentation Developed 11 customer segments using unsupervised learning techniques like KMeans and Gaussian mixture models The clusters helped business simplify complex patterns to manageable set of 11 patterns that helped set strategic and tactical objectives pertaining to customer retention acquisition spend and loyalty Responsibilities Implemented Data Exploration to analyze patterns and to select features using Python SciPy Built Factor Analysis and Cluster Analysis models using Python SciPy to classify customers into different target groups Designed an AB experiment for testing the business performance of the new recommendation system Supported MapReduce Programs running on the cluster Evaluated business requirements and prepared detailed specifications that follow project guidelines required to develop written programs Participated in Data Acquisition with Data Engineer team to extract historical and realtime data by using Hadoop MapReduce and HDFS Communicated and presented default customers profiles along with reports using Python and Tableau analytical results and strategic implications to senior management for strategic decision making Developed scripts in Python to automate the customer query addressable system using python which decreased the time for solving the query of the customer by 45 Collaborated with other functional teams across the Risk and NonRisk groups to use standard methodologies and ensure a positive customer experience throughout the customer journey Performed Data Enrichment jobs to deal missing value to normalize data and to select features Developed multiple MapReduce jobs in java for data cleaning and preprocessing Analyzed the partitioned and bucketed data and compute various metrics for reporting Extracted data from Twitter using Java and Twitter API Parsed JSON formatted twitter data and uploaded to database Developed Hive queries for analysis and exported the result set from Hive to MySQL using Sqoop after processing the data Created HBase tables to store various data formats of data coming from different portfolios Worked on improving performance of existing Pig and Hive Queries Created reports and dashboards by using D3js and Tableau 9x to explain and communicate data insights significant features models scores and performance of new recommendation system to both technical and business teams Utilize SQL Excel and several MarketingWeb Analytics tools Google Analytics Bing Ads AdWords AdSense Criteo Smartly SurveyMonkey and Mailchimp in order to complete business marketing analysis and assessment Used Git 2x for version control with Data Engineer team and Data Scientists colleagues Used Agile methodology and SCRUM process for project developing KT with the client to understand their various Data Management systems and understanding the data Creating metadata and data dictionary for the future data use data refresh of the same client Structuring the Data Marts to store and organize the customers data Running SQL scripts creating indexes stored procedures for data analysis Data Lineage methodology for data mapping and maintaining data quality Prepared Scripts in Python and Shell for Automation of administration tasks Maintained PLSQL objects like packages triggers procedures etc Mapping flow of trade cycle data from source to target and documenting the same Performing QA on the data extracted transformed and exported to excel Participated in all phases of data mining data collection data cleaning developing models validation visualization and performed Gap analysis Extracted data from HDFS and prepared data for exploratory analysis using data munging Built models using Statistical techniques like Bayesian HMM and Machine Learning classification models like XG Boost SVM and Random Forest A highly immersive Data Science program involving Data Manipulation Visualization Web Scraping Machine Learning Python programming SQL GIT Unix Commands NoSQL MongoDB Hadoop Used pandas numpy seaborn scipy matplotlib scikitlearn NLTK in Python for developing various machine learning algorithms Worked on different data formats such as JSON XML and performed machine learning algorithms in Python Environment ER Studio 97 MDM GIT Unix Python SciPy NumPy Pandas StatsModel Plotly MySQL Excel Google Cloud Platform Tableau 9x D3js SVM Random Forests Nave Bayes Classifier AB experiment Git 2x AgileSCRUM MLLib SAS regression logistic regression Hadoop NoSQL Teradata OLTP random forest OLAP HDFS ODS NLTK SVM JSON XML MapReduce Data Scientist Yum Brands Louisville KY March 2016 to February 2017 Yum Brands Inc or Yum is an American fast food company A Fortune 500 corporation Yum operates the brands Taco Bell KFC Pizza Hut and WingStreet worldwide Based in Louisville Kentucky it is one of the worlds largest fast food restaurant companies in terms of system units with 43617 restaurants around the world in over 135 countries and territories Project Description The project involves data extraction and applying data integrity and analytical techniques for story telling from the data The major key performance indicators such as In store behavior price optimization and distribution and logistics optimization and improved order handling times etc using supervised and unsupervised machine learning techniques Responsibilities Applied Lean Six Sigma process improvement in plant and developed Capacity Calculation systems using purchase order tracking system and improvement inbound efficiency by 2356 Worked with Machine learning algorithms like Linear Regressions linear logistic etc SVMs Decision trees for classification of groups and analyzing most significant variables such as FTE Waiting times of purchase orders and Capacities available and applied process improvement techniques And calculated Process Cycle efficiency of 332 and identified value added and nonvalue added activities And utilized SAS for developing Pareto Chart for identifying highly impacting categories in modules to find the work force distribution and created various data visualization charts Performed univariate bivariate and multivariate analysis of approx 4890 tuples using bar charts box plots and histograms Participated in features engineering such as feature creating feature scaling and OneHot encoding with Scikitlearn Converted raw data to processed data by merging finding outliers errors trends missing values and distributions in the data Generated detailed report after validating the graphs using R and adjusting the variables to fit the model Worked on Clustering and factor analysis for classification of data using machine learning algorithms Developed Descriptive statistics and inferential statistics for Logistics optimization Average hours per job Value throughput data to at 95 confidence interval Written MapReduce code to process and parsing the data from various sources and storing parsed data into HBase and Hive using HBase Hive Integration Created SQL tables with referential integrity and developed advanced queries using stored procedures and functions using SQL server management studio Used Pandas NumPy seaborn SciPy Matplotlib Scikitlearn NLTK in Python for developing various machine learning algorithms and utilized machine learning algorithms such as linear regression multivariate regression naive Bayes Random Forests Kmeans KNN for data analysis Used packages like dplyr tidyr ggplot2 in R Studio for data visualization and generated scatter plot and high low graph to identify relation between different variables Worked on Business forecasting segmentation analysis and Data mining and prepared management reports defining the problem documenting the analysis and recommending courses of action to determine the best outcomes Worked on various Statistical models like DOE hypothesis testing Survey testing and queuing theory Experience with risk analysis root cause analysis cluster analysis correlation and optimization and Kmeans algorithm for clustering data into groups Coordinate with data scientists and senior technical staff to identify clients needs and document assumptions Environment SQL Server 2012 Jupyter R 312 Python MATLAB SSRS SSIS SSAS MongoDB HBase HDFS Hive Pig Microsoft office SQL Server Management Studio Business Intelligence Development Studio MS Access Data Analyst JP Morgan New York NY February 2014 to December 2015 Credit Card Fraud The purpose of this project was to fight against credit card fraud My team mainly focused on rebuilding credit card fraud detection model monitoring the model in production taking action if model performance degrades and working closely with business team to onboard new model Monthly Dashboard Project is to develop monthly dashboard that will help the business to see the usage of the cards Credit Debit on monthly basis with respect to their card categories like Signature Platinum Gold and Silver etc This dashboard also provides the activation status of newly issued cards With this dashboard business can easily track the history over latest one year Responsibilities Built scalable and deployable machine learning models Utilized Sqoop to ingest realtime data Used analytics libraries SciKit Learn MLLIB and MLxtend Extensively used Pythons multiple data science packages like Pandas NumPy matplotlib Seaborn SciPy Scikitlearn and NLTK Performed Exploratory Data Analysis trying to find trends and clusters Built models using techniques like Regression Tree based ensemble methods Time Series forecasting KNN Clustering and Isolation Forest methods Worked on data that was a combination of unstructured and structured data from multiple sources and automated the cleaning using Python scripts Extensively performed large data readwrites to and from csv and excel files using pandas Tasked with maintaining RDDs using SparkSQL Communicated and coordinated with other departments to collection business requirement Tackled highly imbalanced Fraud dataset using undersampling with ensemble methods oversampling and cost sensitive algorithms Improved fraud prediction performance by using random forest and gradient boosting for feature selection with Python Scikitlearn Implemented machine learning model logistic regression XGboost with Python Scikit learn Optimized algorithm with stochastic gradient descent algorithm Finetuned the algorithm parameter with manual tuning and automated tuning such as Bayesian Optimization Developed a technical brief based on the business brief This contains detailed steps and stages of developing and delivering the project including timelines After signoff from the client on technical brief started developing the SAS codes Wrote the data validation SAS codes with the help of Univariate Frequency procedures Summarising the data at customer level by joining the datasets of customer transaction dimension and from 3rd party sources Separately calculated the KPIs for Target and Mass campaigns at prepromopost periods with respective to their transactions spend and visits Also measured the KPIs at MoM Month on Month QoQ Quarter on Quarter and YoY Year on Year with respect to prepromopost Measured the ROI based on the differences prepromopost KPIs Extensively used SAS procedures like IMPORT EXPORT SORT FREQ MEANS FORMAT APPEND UNIVARIATE DATASETS and REPORT Standardised the data with the help of PROC STANDARD Implemented cluster analysis PROC CLUSTER and PROC FASTCLUS iteratively Worked extensively with data governance team to maintain data models Metadata and dictionaries Used Python to preprocess data and attempt to find insights Iteratively rebuild models dealing with changes in data and refining them over time Created and published multiple dashboards and reports using Tableau server Extensively used SQL queries for legacy data retrieval jobs Tasked with migrating the django database from MySQL to PostgreSQL Gained expertise in Data Visualization using matplotlib Bokeh and Plotly Responsible for maintaining and analyzing large datasets used to analyze risk by domain experts Developed Hive queries that compared new incoming data against historic data Built tables in Hive to store large volumes of data Used big data tools Spark Sparksql Mllib to conduct the real time analysis of credit card fraud based on AWS Performed Data audit QA of SAS codeprojects and sense check of results Accomplishments Accomplished 75 reduction in cycle time for automation of gathering and reporting of performance issues in Hadoop applications used by the company for ETL in Enterprise Data Management reducing team effort resulting in a net savings of 80000 per annum for the companys budget Won the Harbinger award for Value Engineering from the clients for lead the Standards and Compliance team in overseeing programming standards debugging and authorized migration to production environment 15 largescale data management software applications in Hadoop ecosystem Built Hadoop based data warehouse streamlined data ingestion distributed data storage data lake HDFS standardized and provided scalable data processing to monetize data effectively for a Bank of America Migrated raw data from Mainframes and extracted it to HDFS and Hive using sqoop to for preprocessing and structuring Collaborated with team directly interacting with clients in large scale warehousing of sensitive data using Hadoop ecosystem UNIX scripting Map Reduce and Hive and Extraction Transformation Loading Environment Spark Hadoop AWS SAS Enterprise Guide SASMACROS SASACCESS SASSTAT SASSQL ORACLE MSOFFICE Python scikitlearn pandas Numpy Machine Learning logistic regression XGboost Gradient Descent algorithm Bayesian optimization Tableau Data Scientist Travelers Insurance Pune Maharashtra August 2012 to August 2013 The Travelers Companies is an American insurance company It is the second largest writer of US commercial property casualty insurance and the third largest writer of US personal insurance through independent agents Project Predicting Customer Churn Project Description To predict the attrition in personal insurance products The Churn prediction model predicts a customers propensity to churn by using information about the customer such as household and financial data transactional data and behavioral data The inputs for the Churn prediction model are customer demographic data insurance policies premiums tenure claims complaints and the sentiment score from past surveys Responsibilities Aggregate all available information about the customer The data that is obtained for predicting the churn is classified in the following categories Demographic data such as age gender education marital status employment status income home ownership status and retirement plan Policyrelated data such as insurance lines number of policies in the household household tenure premium disposable income and insured cars Claims such as claim settlement duration number of claims that are filed and denied Complaints such as number of open and closed complaints Survey sentiment data Sentiment scores from past surveys are captured in the latest and average note attitude score fields The note attitude score is derived from customer negative feedback only If the note attitude is zero the customer is more satisfied while as the number increases satisfaction level decreases Responsible for building data analysis infrastructure to collect analyze and visualize data Data elements validation using exploratory data analysis univariate bivariate multivariate analysis Missing value treatment outlier capping and anomalies treatment using statistical methods Variable selection was done by making use of Rsquare and VIF values Deployed Machine Learning Logistic Regression and PCA to predict customer churn Environment Statistical tools R 330 Python 30 SQL Server MSExcel MSPowerPoint Data Analyst Travelers Insurance Pune Maharashtra June 2011 to August 2012 Intern Responsibilities Developed and implemented predictive models using Natural Language Processing Techniques and machinelearning algorithms such as linear regression classification multivariate regression Naive Bayes RandomForests Kmeans clustering KNN PCA and regularization for data analysis Designed and developed Natural Language Processing models for sentiment analysis Applied clustering algorithms ie Hierarchical Kmeans with help of Scikit and Scipy Developed visualizations and dashboards using ggplot Tableau Worked on development of data warehouse Data Lake and ETL systems using relational and non relationaltools like SQL No SQL Built and analyzed datasets using R SAS Matlab and Python in decreasing order of usage Participated in all phases of datamining datacollection datacleaning developingmodels validation visualization and performed Gapanalysis DataManipulation and Aggregation from different source using Nexus Toad BusinessObjects PowerBI and SmartView Implemented Agile Methodology for building an internal application Good knowledge of HadoopArchitecture and various components such as HDFS Job Tracker Task Tracker Name Node Data Node Secondary Name Node and MapReduce concepts As Architect delivered various complex OLAPdatabasescubes scorecards dashboards and reports Programmed a utility in Python that used multiple packages scipy numpy pandas Implemented Classification using supervised algorithms like LogisticRegression Decisiontrees KNN NaiveBayes Used Teradata15 utilities such as FastExport MLOAD for handling various tasks data migrationETL from OLTP Source Systems to OLAP Target Systems Maintenance in the testing team for System testingIntegrationUAT Involved in preparation design of technical documents like Bus Matrix Document PPDM Model and LDM PDM Understanding the client business problems and analyzing the data by using appropriate Statistical models to generate insights Environment R 30 Erwin 95 Tableau 80 MDM QlikView ML Lib PLSQL HDFS Teradata 141 JSON HADOOP HDFS MapReduce PIG Spark R Studio MAHOUT JAVA HIVE AWS Education Bachelor of Technology in Information Technology Jawaharlal Nehru Technological University Hyderabad Telangana Skills APPLICATION DEVELOPMENT Hadoop HBase HDFS Hive MapReduce Pig PYTHON FLASK GGPLOT2 NUMPY REPORTING TOOLS VISIO XML JDBC MS ACCESS SQL CASSANDRA IMPALA MAPREDUCE Additional Information SKILLS Languages Python R Java 8 Packages ggplot2 caret dplyr Rweka gmodels RCurl C50 twitter NLP Reshape2 rjson plyr pandas numPy Seaborn sciPy matplot lib scikitlearn Beautiful Soup Rpy2 Web Technologies JDBC HTML5 DHTML and XML CSS3 Web Services WSDL Data Modelling Tools Erwin r 96 95 91 8x Rational Rose ERStudio MS Visio SAP Power designer Big Data Technologies Hadoop Hive HDFS MapReduce Pig Databases SQL Hive Impala Pig Spark SQL Databases SQLServer My SQL MS Access HDFS HBase Teradata Netezza MongoDB Cassandra Reporting Tools MS Office WordExcelPower Point Visio Tableau Crystal reports XI Business Intelligence SSRS Business Objects 5x 6x Cognos7060 Flask Dash ETL Tools Informatica Power Centre SSIS Version Control Tools SVM GitHub Project Execution Methodologies Ralph Kimball and Bill Inmon data warehousing methodology Rational Unified Process RUP Rapid Application Development RAD Joint Application Development JAD BI Tools Tableau Tableau Server Tableau Reader SAP Business Objects OBIEE QlikView SAP Business Intelligence Amazon Redshift or Azure Data Warehouse Operating System Windows Linux Unix Macintosh HD Red Hat",
    "unique_id": "4ea849cc-a9d2-4c60-8d4e-899808cb4269"
}