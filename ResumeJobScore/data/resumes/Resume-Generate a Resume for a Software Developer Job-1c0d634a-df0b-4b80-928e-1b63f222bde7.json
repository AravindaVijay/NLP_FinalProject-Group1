{
    "clean_data": "Sr Hadoop Developer Sr Hadoop span lDeveloperspan Sr Hadoop Developer ATT Inc Middletown NJ Around 9 years of programming experience involved in all phases of Software Development Life Cycle SDLC Over 5 Years of Big Data experience in building highly scalable data analytics applications Strong experience working with Hadoop ecosystem components like HDFS Map Reduce Spark HBase Oozie Hive Sqoop Pig Flume and Kafka Good handson experiencing working with various Hadoop distributions mainly Cloudera CDH Hortonworks HDP and Amazon EMR Good understanding of Distributed Systems architecture and design principles behind Parallel Computing Expertise in developing production ready Spark applications utilizing SparkCore Dataframes SparkSQL SparkML and SparkStreaming APIs SciKitLearn SparkMLMLlib and Tensorflow Strong experience troubleshooting failures in spark applications and finetuning spark applications and hive queries for better performance Worked extensively on Hive for building complex data analytical applications Strong experience writing complex mapreduce jobs including development of custom Input Formats and custom Record Readers Sound Knowledge in map side join reduce side join shuffle sort distributed cache compression techniques multiple hadoop Input output formats Worked with Apache NiFi to automate the data flow between the systems and managed flow of information between system Good experience working with AWS Cloud services like S3 EMR Redshift Athena Dynamo DB etc Deep understanding of performance tuning partitioning for optimizing spark applications Worked on building real time data workflows using Kafka Spark streaming and HBase Extensive knowledge on NoSQL databases like HBase Cassandra and Mongo DB Solid experience in working with csv text sequential avro parquet orc json formats of data Extensive experience in performing ETL on structured semistructured data using Pig Latin Scripts Designed and implemented Hive and Pig UDFs using Java for evaluation filtering loading and storing of data Experience in using Hadoop ecosystem and processing data using Tableau Experience with Apache Phoenix to access the data stored in HBase Good knowledge in the core concepts of programming such as algorithms data structures collections Developed core modules in large crossplatform applications using JAVA JSP Servlets Hibernate RESTful JDBC JavaScript XML and HTML Extensive experience in developing and deploying applications using Web Logic Apache Tomcat and JBOSS Development experience with RDBMS including writing SQL queries views stored procedure triggers etc Strong understanding of Software Development Lifecycle SDLC and various methodologies Waterfall Agile Authorized to work in the US for any employer Work Experience Sr Hadoop Developer ATT Inc Middletown NJ July 2018 to Present Roles Responsibilities Developed Spark applications using PySpark utilizing Data frames and Spark SQL API for faster processing of data Developed highly optimized Spark applications to perform various data cleansing validation transformation and summarization activities according to the requirement Data pipeline consists Spark Hive and Sqoop and custom build Input Adapters to ingest transform and analyze operational data Developed Spark jobs and Hive Jobs to summarize and transform data Used Spark for interactive queries processing of streaming data and integration with popular NoSQL database for huge volume of data Involved in converting HiveSQL queries into Spark transformations using Spark DataFrames and Scala Used different tools for data integration with different databases and Hadoop Analyzed the SQL scripts and designed the solution to implement using Pyspark Involved in installation of Tez and improved the query performance Used Spark for interactive queries processing of streaming data and integration with popular NoSQL database for huge volume of data Built real time data pipelines by developing kafka producers and spark streaming applications for consuming Ingested syslog messages parses them and streams the data to Kafka Handled importing data from different data sources into HDFS using Sqoop and performing transformations using Hive Map Reduce and then loading data into HDFS Exported the analyzed data to the relational databases using Sqoop to further visualize and generate reports for the BI team Collecting and aggregating large amounts of log data using Flume and staging data in HDFS for further analysis Analyzed the data by performing Hive queries Hive QL to study customer behavior Helped Dev ops Engineers for deploying code and debug issues Used Hive to analyze the partitioned and bucketed data and compute various metrics for reporting Developed Hive scripts in Hive QL to denormalize and aggregate the data Scheduled and executed workflows in Oozie to run various jobs Experience in using Hadoop ecosystem and processing data using Amazon AWS Environment Hadoop HDFS HBase Spark Scala Hive MapReduce Sqoop ETL Java PLSQL Oracle 11g UnixLinux Sr Hadoop Developer ON SOLVE Daytona Beach FL April 2017 to July 2018 Roles Responsibilities Build a framework Spark with Scala and migrated existing PySpark applications to improve the runtime and performance Developed highly optimized Spark applications to perform various data cleansing validation transformation and summarization activities according to the requirement Performed Transformations like Denormalizing Cleansing of data sets Date Transformations parsing some complex columns Worked with different compression codecs like GZIP SNAPPY and BZIP2 in MapReduce Pig and Hive for better performance Worked with Apache NiFi to automate the data flow between the systems and managed flow of information between systems Have used Ansible for automation of frameworks Handled Avro JSON and Apache Log data in Hive using custom Hive SerDes Worked on batch processing and scheduled workflows using Oozie Implemented installation and configuration of multinode cluster on the cloud using Amazon Web Services AWS on EC2 Worked in agile sprint methodology environment Have used the Knox gateway for having Hadoop security between the users and operators Used cloud computing on the multinode cluster and deployed Hadoop application on cloud S3 and used Elastic Map Reduce EMR to run Mapreduce Used HiveQL to create partitioned RC ORC tables used compression techniques to optimize data process and faster retrieval Implemented Partitioning Dynamic Partitioning and Buckets in Hive for efficient data access Environment Apache Hadoop HDFS Cloudera Manager Java MapReduce Eclipse Indigo Hive HBASE PIG Sqoop Oozie SQL Spring Hadoop Developer Maximus Inc Reston VA September 2013 to March 2016 Roles Responsibilities Involved in requirement analysis design coding and implementation phases of the project Used Sqoop to load structured data from relational databases into HDFS Loaded transactional data from Teradata using Sqoop and created Hive Tables Worked on automation of delta feeds from Teradata using Sqoop and from FTP Servers to Hive Performed Transformations like Denormalizing Cleansing of data sets Date Transformations parsing some complex columns Worked with different compression codecs like GZIP SNAPPY and BZIP2 in MapReduce Pig and Hive for better performance Worked with Apache NiFi to automate the data flow between the systems and managed flow of information between systems Have used Ansible for automation of frameworks Handled Avro JSON and Apache Log data in Hive using custom Hive SerDes Worked on batch processing and scheduled workflows using Oozie Implemented installation and configuration of multinode cluster on the cloud using Amazon Web Services AWS on EC2 Worked in agile sprint methodology environment Have used the Knox gateway for having Hadoop security between the users and operators Used cloud computing on the multinode cluster and deployed Hadoop application on cloud S3 and used Elastic Map Reduce EMR to run Mapreduce Used HiveQL to create partitioned RC ORC tables used compression techniques to optimize data process and faster retrieval Implemented Partitioning Dynamic Partitioning and Buckets in Hive for efficient data access Environment Apache Hadoop HDFS Cloudera Manager Java MapReduce Eclipse Indigo Hive HBASE PIG Sqoop Oozie SQL Spring Java Developer Dataguise Inc Fremont CA November 2011 to August 2013 Roles Responsibilities Involved in all the phases of the project development requirements gathering analysis design development coding testing and debugging Implemented MVC architecture by using Struts to send and receive the data from frontend to business layer Integrated the Struts and Hibernate to achieve Object relational mapping Used apache struts to develop the webbased components and implemented DAO Implemented Struts framework in the presentation tier for all the essential control flow business level validations and for communicating with the business layer Integrated the Struts application with Hibernate for queryinginserting data management for SQL server database Responsible for design and development of Web Application in J2EE using Struts MVC Framework Involved in creating consuming SOAP based Restful web services Used Web Services for communication between the different internal applications Used SOAP for the communication between the different internal applications Used GitHub for version control management and consistently produced high quality code through disciplined and rigorous unit testing Used Maven script for building and deploying the application Developed the XML schema and Web Services for the data maintenance and structures Involved in designing test plans test cases and overall Unit testing of the system Object Oriented Analysis and Design using UML include development of class diagrams Sequence diagrams and state diagrams and implemented these diagrams in Microsoft Visio Worked in agile sprint methodology environment Implemented MVC DAOJ2EE design patterns as a part of application development Used Spring IOC and MVC for enhanced modules Developed the Persistence Layer using Hibernate Used DB2 as the database and wrote SQL PLSQL Designed and developed message driven beans that consumed the messages from the Java message queue Design and development of Web pages using HTML CSS including Ajax controls and XML Written controllers based on Spring MVC and made calls to JSP pages Environment Struts Spring HTML CSS Java J2ee JSP XML Eclipse WebLogic JavaScript Java Mail API Hibernate SQL Server JBoss GitHub Maven Agile Junit Java Developer Media3 Pvt Ltd Hyderabad Telangana June 2010 to October 2011 Roles Responsibilities Implemented the presentation layer with HTML CSS and JavaScript Developed web components using JSP Servlets and JDBC Implemented secured cookies using Servlets Wrote complex SQL queries and stored procedures Implemented Persistent layer using Hibernate API Implemented Search queries using Hibernate Criteria interface Provided support for loans reports for CBT Designed and developed Loans reports for Evans bank using Jasper and iReport Involved in fixing bugs and unit testing with test cases using Junit Object Oriented Analysis and Design using UML include development of class diagrams Sequence diagrams and state diagrams and implemented these diagrams in Microsoft Visio Maintained Jasper server on client server and resolved issues Actively involved in system testing Fine tuning SQL queries for maximum efficiency to improve the performance Designed Tables and indexes by following normalizations Involved in Unit testing Integration testing and User Acceptance testing Utilizes Java and SQL day to day to debug and fix issues with client processes Environment Java Servlets HTML Java Script JSP Hibernate Junit Testing Oracle DB SQL Jasper Reports iReport Maven Jenkins Education Bachelors in Computer Science Engineering in Computer Science Engineering Indian Institute of Technology IIT Skills ECLIPSE EJB J2EE JAVA SPRING JBOSS JSP SERVLETS STRUTS DB2 TERADATA JDBC MYSQL SQL HTML JAVASCRIPT PHP PYTHON WEB UI XML Additional Information TECHNICAL SKILLS Programming Skills JavaJ2EE JSP Servlets AJAX EJB Struts Spring JDBC JavaScript PHP and Python Databases MYSQL SQL DB2 and Teradata Web services REST AWS SOAP WSDL Servers Apache Tomcat WebSphere JBoss Operating Systems Unix Linux Windows Solaris IDE tools My Eclipse Eclipse NetBeans QA Tools Crashlytics or Fabrics Web UI HTML JavaScript XML SOAP WSDL",
    "entities": [
        "QA Tools Crashlytics or Fabrics Web UI",
        "SparkStreaming",
        "BI",
        "Jasper",
        "Software Development Lifecycle SDLC",
        "SPRING JBOSS",
        "CBT",
        "Java Script JSP Hibernate Junit Testing",
        "User Acceptance",
        "Amazon Web Services AWS",
        "Maven Jenkins Education Bachelors",
        "Distributed Systems",
        "Hadoop",
        "XML",
        "Input Adapters",
        "WebLogic",
        "Software Development Life Cycle SDLC",
        "HBase Cassandra",
        "SparkCore Dataframes",
        "Sr Hadoop Developer Sr Hadoop",
        "Skills JavaJ2EE JSP Servlets",
        "Knox",
        "SparkSQL",
        "Developed",
        "Date Transformations",
        "UML",
        "PySpark utilizing Data",
        "Restful",
        "Oozie SQL Spring Hadoop",
        "Sequence",
        "AWS Cloud",
        "iReport",
        "WebSphere JBoss Operating Systems",
        "JSP",
        "Waterfall Agile Authorized",
        "Spark SQL API",
        "HDP",
        "Object Oriented Analysis and Design",
        "MVC",
        "Spark",
        "Pyspark Involved",
        "UnixLinux Sr Hadoop Developer",
        "Implemented Persistent",
        "HBase Extensive",
        "Computer Science Engineering Indian Institute of Technology",
        "HTML CSS",
        "US",
        "Sqoop",
        "Computer Science Engineering",
        "Spark DataFrames",
        "Microsoft Visio Worked",
        "Microsoft Visio",
        "Servlets Wrote",
        "log data",
        "JAVASCRIPT PHP PYTHON",
        "Oozie",
        "Hadoop Analyzed",
        "SQL",
        "GitHub",
        "Big Data",
        "Hive",
        "Amazon EMR Good",
        "Environment Apache Hadoop",
        "Spark Hive",
        "ETL",
        "FL",
        "HTML Extensive",
        "Maven",
        "Collecting",
        "JSP Servlets",
        "Work Experience Sr Hadoop Developer ATT Inc",
        "Hibernate Criteria",
        "Oozie Implemented",
        "Input Formats",
        "Amazon AWS Environment Hadoop HDFS HBase Spark",
        "Parallel Computing Expertise",
        "iReport Involved",
        "NetBeans",
        "NoSQL",
        "Tableau",
        "Java Developer Dataguise Inc",
        "Integration",
        "Teradata",
        "JAVA JSP Servlets Hibernate",
        "GZIP SNAPPY"
    ],
    "experience": "Experience in using Hadoop ecosystem and processing data using Tableau Experience with Apache Phoenix to access the data stored in HBase Good knowledge in the core concepts of programming such as algorithms data structures collections Developed core modules in large crossplatform applications using JAVA JSP Servlets Hibernate RESTful JDBC JavaScript XML and HTML Extensive experience in developing and deploying applications using Web Logic Apache Tomcat and JBOSS Development experience with RDBMS including writing SQL queries views stored procedure triggers etc Strong understanding of Software Development Lifecycle SDLC and various methodologies Waterfall Agile Authorized to work in the US for any employer Work Experience Sr Hadoop Developer ATT Inc Middletown NJ July 2018 to Present Roles Responsibilities Developed Spark applications using PySpark utilizing Data frames and Spark SQL API for faster processing of data Developed highly optimized Spark applications to perform various data cleansing validation transformation and summarization activities according to the requirement Data pipeline consists Spark Hive and Sqoop and custom build Input Adapters to ingest transform and analyze operational data Developed Spark jobs and Hive Jobs to summarize and transform data Used Spark for interactive queries processing of streaming data and integration with popular NoSQL database for huge volume of data Involved in converting HiveSQL queries into Spark transformations using Spark DataFrames and Scala Used different tools for data integration with different databases and Hadoop Analyzed the SQL scripts and designed the solution to implement using Pyspark Involved in installation of Tez and improved the query performance Used Spark for interactive queries processing of streaming data and integration with popular NoSQL database for huge volume of data Built real time data pipelines by developing kafka producers and spark streaming applications for consuming Ingested syslog messages parses them and streams the data to Kafka Handled importing data from different data sources into HDFS using Sqoop and performing transformations using Hive Map Reduce and then loading data into HDFS Exported the analyzed data to the relational databases using Sqoop to further visualize and generate reports for the BI team Collecting and aggregating large amounts of log data using Flume and staging data in HDFS for further analysis Analyzed the data by performing Hive queries Hive QL to study customer behavior Helped Dev ops Engineers for deploying code and debug issues Used Hive to analyze the partitioned and bucketed data and compute various metrics for reporting Developed Hive scripts in Hive QL to denormalize and aggregate the data Scheduled and executed workflows in Oozie to run various jobs Experience in using Hadoop ecosystem and processing data using Amazon AWS Environment Hadoop HDFS HBase Spark Scala Hive MapReduce Sqoop ETL Java PLSQL Oracle 11 g UnixLinux Sr Hadoop Developer ON SOLVE Daytona Beach FL April 2017 to July 2018 Roles Responsibilities Build a framework Spark with Scala and migrated existing PySpark applications to improve the runtime and performance Developed highly optimized Spark applications to perform various data cleansing validation transformation and summarization activities according to the requirement Performed Transformations like Denormalizing Cleansing of data sets Date Transformations parsing some complex columns Worked with different compression codecs like GZIP SNAPPY and BZIP2 in MapReduce Pig and Hive for better performance Worked with Apache NiFi to automate the data flow between the systems and managed flow of information between systems Have used Ansible for automation of frameworks Handled Avro JSON and Apache Log data in Hive using custom Hive SerDes Worked on batch processing and scheduled workflows using Oozie Implemented installation and configuration of multinode cluster on the cloud using Amazon Web Services AWS on EC2 Worked in agile sprint methodology environment Have used the Knox gateway for having Hadoop security between the users and operators Used cloud computing on the multinode cluster and deployed Hadoop application on cloud S3 and used Elastic Map Reduce EMR to run Mapreduce Used HiveQL to create partitioned RC ORC tables used compression techniques to optimize data process and faster retrieval Implemented Partitioning Dynamic Partitioning and Buckets in Hive for efficient data access Environment Apache Hadoop HDFS Cloudera Manager Java MapReduce Eclipse Indigo Hive HBASE PIG Sqoop Oozie SQL Spring Hadoop Developer Maximus Inc Reston VA September 2013 to March 2016 Roles Responsibilities Involved in requirement analysis design coding and implementation phases of the project Used Sqoop to load structured data from relational databases into HDFS Loaded transactional data from Teradata using Sqoop and created Hive Tables Worked on automation of delta feeds from Teradata using Sqoop and from FTP Servers to Hive Performed Transformations like Denormalizing Cleansing of data sets Date Transformations parsing some complex columns Worked with different compression codecs like GZIP SNAPPY and BZIP2 in MapReduce Pig and Hive for better performance Worked with Apache NiFi to automate the data flow between the systems and managed flow of information between systems Have used Ansible for automation of frameworks Handled Avro JSON and Apache Log data in Hive using custom Hive SerDes Worked on batch processing and scheduled workflows using Oozie Implemented installation and configuration of multinode cluster on the cloud using Amazon Web Services AWS on EC2 Worked in agile sprint methodology environment Have used the Knox gateway for having Hadoop security between the users and operators Used cloud computing on the multinode cluster and deployed Hadoop application on cloud S3 and used Elastic Map Reduce EMR to run Mapreduce Used HiveQL to create partitioned RC ORC tables used compression techniques to optimize data process and faster retrieval Implemented Partitioning Dynamic Partitioning and Buckets in Hive for efficient data access Environment Apache Hadoop HDFS Cloudera Manager Java MapReduce Eclipse Indigo Hive HBASE PIG Sqoop Oozie SQL Spring Java Developer Dataguise Inc Fremont CA November 2011 to August 2013 Roles Responsibilities Involved in all the phases of the project development requirements gathering analysis design development coding testing and debugging Implemented MVC architecture by using Struts to send and receive the data from frontend to business layer Integrated the Struts and Hibernate to achieve Object relational mapping Used apache struts to develop the webbased components and implemented DAO Implemented Struts framework in the presentation tier for all the essential control flow business level validations and for communicating with the business layer Integrated the Struts application with Hibernate for queryinginserting data management for SQL server database Responsible for design and development of Web Application in J2EE using Struts MVC Framework Involved in creating consuming SOAP based Restful web services Used Web Services for communication between the different internal applications Used SOAP for the communication between the different internal applications Used GitHub for version control management and consistently produced high quality code through disciplined and rigorous unit testing Used Maven script for building and deploying the application Developed the XML schema and Web Services for the data maintenance and structures Involved in designing test plans test cases and overall Unit testing of the system Object Oriented Analysis and Design using UML include development of class diagrams Sequence diagrams and state diagrams and implemented these diagrams in Microsoft Visio Worked in agile sprint methodology environment Implemented MVC DAOJ2EE design patterns as a part of application development Used Spring IOC and MVC for enhanced modules Developed the Persistence Layer using Hibernate Used DB2 as the database and wrote SQL PLSQL Designed and developed message driven beans that consumed the messages from the Java message queue Design and development of Web pages using HTML CSS including Ajax controls and XML Written controllers based on Spring MVC and made calls to JSP pages Environment Struts Spring HTML CSS Java J2ee JSP XML Eclipse WebLogic JavaScript Java Mail API Hibernate SQL Server JBoss GitHub Maven Agile Junit Java Developer Media3 Pvt Ltd Hyderabad Telangana June 2010 to October 2011 Roles Responsibilities Implemented the presentation layer with HTML CSS and JavaScript Developed web components using JSP Servlets and JDBC Implemented secured cookies using Servlets Wrote complex SQL queries and stored procedures Implemented Persistent layer using Hibernate API Implemented Search queries using Hibernate Criteria interface Provided support for loans reports for CBT Designed and developed Loans reports for Evans bank using Jasper and iReport Involved in fixing bugs and unit testing with test cases using Junit Object Oriented Analysis and Design using UML include development of class diagrams Sequence diagrams and state diagrams and implemented these diagrams in Microsoft Visio Maintained Jasper server on client server and resolved issues Actively involved in system testing Fine tuning SQL queries for maximum efficiency to improve the performance Designed Tables and indexes by following normalizations Involved in Unit testing Integration testing and User Acceptance testing Utilizes Java and SQL day to day to debug and fix issues with client processes Environment Java Servlets HTML Java Script JSP Hibernate Junit Testing Oracle DB SQL Jasper Reports iReport Maven Jenkins Education Bachelors in Computer Science Engineering in Computer Science Engineering Indian Institute of Technology IIT Skills ECLIPSE EJB J2EE JAVA SPRING JBOSS JSP SERVLETS STRUTS DB2 TERADATA JDBC MYSQL SQL HTML JAVASCRIPT PHP PYTHON WEB UI XML Additional Information TECHNICAL SKILLS Programming Skills JavaJ2EE JSP Servlets AJAX EJB Struts Spring JDBC JavaScript PHP and Python Databases MYSQL SQL DB2 and Teradata Web services REST AWS SOAP WSDL Servers Apache Tomcat WebSphere JBoss Operating Systems Unix Linux Windows Solaris IDE tools My Eclipse Eclipse NetBeans QA Tools Crashlytics or Fabrics Web UI HTML JavaScript XML SOAP WSDL",
    "extracted_keywords": [
        "Sr",
        "Hadoop",
        "Developer",
        "Sr",
        "Hadoop",
        "span",
        "lDeveloperspan",
        "Sr",
        "Hadoop",
        "Developer",
        "ATT",
        "Inc",
        "Middletown",
        "NJ",
        "years",
        "programming",
        "experience",
        "phases",
        "Software",
        "Development",
        "Life",
        "Cycle",
        "SDLC",
        "Years",
        "Big",
        "Data",
        "experience",
        "data",
        "analytics",
        "applications",
        "experience",
        "Hadoop",
        "ecosystem",
        "components",
        "Map",
        "Reduce",
        "Spark",
        "HBase",
        "Oozie",
        "Hive",
        "Sqoop",
        "Pig",
        "Flume",
        "Kafka",
        "Good",
        "handson",
        "Hadoop",
        "distributions",
        "Cloudera",
        "CDH",
        "Hortonworks",
        "HDP",
        "Amazon",
        "EMR",
        "understanding",
        "Distributed",
        "Systems",
        "architecture",
        "design",
        "principles",
        "Parallel",
        "Computing",
        "Expertise",
        "production",
        "Spark",
        "applications",
        "SparkCore",
        "Dataframes",
        "SparkSQL",
        "SparkML",
        "SparkStreaming",
        "APIs",
        "SciKitLearn",
        "SparkMLMLlib",
        "Tensorflow",
        "experience",
        "troubleshooting",
        "failures",
        "spark",
        "applications",
        "spark",
        "applications",
        "hive",
        "queries",
        "performance",
        "Hive",
        "data",
        "applications",
        "experience",
        "mapreduce",
        "jobs",
        "development",
        "custom",
        "Input",
        "Formats",
        "custom",
        "Record",
        "Readers",
        "Sound",
        "Knowledge",
        "map",
        "side",
        "join",
        "side",
        "join",
        "shuffle",
        "cache",
        "compression",
        "hadoop",
        "Input",
        "output",
        "formats",
        "Apache",
        "NiFi",
        "data",
        "flow",
        "systems",
        "flow",
        "information",
        "system",
        "experience",
        "AWS",
        "Cloud",
        "services",
        "S3",
        "EMR",
        "Redshift",
        "Athena",
        "Dynamo",
        "DB",
        "understanding",
        "performance",
        "spark",
        "applications",
        "time",
        "data",
        "workflows",
        "Kafka",
        "Spark",
        "streaming",
        "HBase",
        "knowledge",
        "NoSQL",
        "databases",
        "HBase",
        "Cassandra",
        "Mongo",
        "DB",
        "experience",
        "csv",
        "text",
        "avro",
        "parquet",
        "orc",
        "formats",
        "data",
        "experience",
        "ETL",
        "data",
        "Pig",
        "Latin",
        "Scripts",
        "Hive",
        "Pig",
        "UDFs",
        "Java",
        "evaluation",
        "loading",
        "storing",
        "data",
        "Experience",
        "Hadoop",
        "ecosystem",
        "processing",
        "data",
        "Tableau",
        "Experience",
        "Apache",
        "Phoenix",
        "data",
        "HBase",
        "knowledge",
        "core",
        "concepts",
        "programming",
        "algorithms",
        "data",
        "structures",
        "collections",
        "core",
        "modules",
        "crossplatform",
        "applications",
        "JSP",
        "Servlets",
        "Hibernate",
        "JDBC",
        "JavaScript",
        "XML",
        "HTML",
        "experience",
        "applications",
        "Web",
        "Logic",
        "Apache",
        "Tomcat",
        "JBOSS",
        "Development",
        "experience",
        "RDBMS",
        "SQL",
        "queries",
        "procedure",
        "triggers",
        "understanding",
        "Software",
        "Development",
        "Lifecycle",
        "SDLC",
        "methodologies",
        "Waterfall",
        "Agile",
        "Authorized",
        "US",
        "employer",
        "Work",
        "Experience",
        "Sr",
        "Hadoop",
        "Developer",
        "ATT",
        "Inc",
        "Middletown",
        "NJ",
        "July",
        "Present",
        "Roles",
        "Responsibilities",
        "Spark",
        "applications",
        "PySpark",
        "Data",
        "frames",
        "Spark",
        "SQL",
        "API",
        "processing",
        "data",
        "Spark",
        "applications",
        "data",
        "cleansing",
        "validation",
        "transformation",
        "summarization",
        "activities",
        "requirement",
        "Data",
        "pipeline",
        "Spark",
        "Hive",
        "Sqoop",
        "custom",
        "Input",
        "Adapters",
        "transform",
        "data",
        "Spark",
        "jobs",
        "Hive",
        "Jobs",
        "data",
        "Spark",
        "queries",
        "processing",
        "data",
        "integration",
        "NoSQL",
        "database",
        "volume",
        "data",
        "HiveSQL",
        "queries",
        "Spark",
        "transformations",
        "Spark",
        "DataFrames",
        "Scala",
        "tools",
        "data",
        "integration",
        "databases",
        "Hadoop",
        "SQL",
        "scripts",
        "solution",
        "Pyspark",
        "installation",
        "Tez",
        "query",
        "performance",
        "Spark",
        "queries",
        "processing",
        "data",
        "integration",
        "NoSQL",
        "database",
        "volume",
        "data",
        "time",
        "data",
        "pipelines",
        "producers",
        "streaming",
        "applications",
        "syslog",
        "messages",
        "data",
        "Kafka",
        "data",
        "data",
        "sources",
        "HDFS",
        "Sqoop",
        "transformations",
        "Hive",
        "Map",
        "Reduce",
        "data",
        "HDFS",
        "data",
        "databases",
        "Sqoop",
        "reports",
        "BI",
        "team",
        "amounts",
        "log",
        "data",
        "Flume",
        "data",
        "HDFS",
        "analysis",
        "data",
        "Hive",
        "queries",
        "Hive",
        "QL",
        "customer",
        "behavior",
        "Dev",
        "ops",
        "Engineers",
        "code",
        "issues",
        "Hive",
        "data",
        "metrics",
        "Developed",
        "Hive",
        "scripts",
        "Hive",
        "QL",
        "data",
        "workflows",
        "Oozie",
        "jobs",
        "Experience",
        "Hadoop",
        "ecosystem",
        "processing",
        "data",
        "Amazon",
        "AWS",
        "Environment",
        "Hadoop",
        "HDFS",
        "HBase",
        "Spark",
        "Scala",
        "Hive",
        "MapReduce",
        "Sqoop",
        "ETL",
        "Java",
        "PLSQL",
        "Oracle",
        "g",
        "UnixLinux",
        "Sr",
        "Hadoop",
        "Developer",
        "ON",
        "SOLVE",
        "Daytona",
        "Beach",
        "FL",
        "April",
        "July",
        "Roles",
        "Responsibilities",
        "framework",
        "Spark",
        "Scala",
        "PySpark",
        "applications",
        "runtime",
        "performance",
        "Spark",
        "applications",
        "data",
        "cleansing",
        "validation",
        "transformation",
        "summarization",
        "activities",
        "requirement",
        "Performed",
        "Transformations",
        "Denormalizing",
        "Cleansing",
        "data",
        "Date",
        "Transformations",
        "columns",
        "compression",
        "codecs",
        "GZIP",
        "BZIP2",
        "MapReduce",
        "Pig",
        "Hive",
        "performance",
        "Apache",
        "NiFi",
        "data",
        "flow",
        "systems",
        "flow",
        "information",
        "systems",
        "Ansible",
        "automation",
        "frameworks",
        "Avro",
        "JSON",
        "Apache",
        "Log",
        "data",
        "Hive",
        "custom",
        "Hive",
        "SerDes",
        "batch",
        "processing",
        "workflows",
        "Oozie",
        "installation",
        "configuration",
        "multinode",
        "cluster",
        "cloud",
        "Amazon",
        "Web",
        "Services",
        "AWS",
        "EC2",
        "Worked",
        "sprint",
        "methodology",
        "environment",
        "Knox",
        "gateway",
        "Hadoop",
        "security",
        "users",
        "operators",
        "cloud",
        "computing",
        "multinode",
        "cluster",
        "Hadoop",
        "application",
        "cloud",
        "S3",
        "Elastic",
        "Map",
        "EMR",
        "Mapreduce",
        "HiveQL",
        "RC",
        "ORC",
        "tables",
        "compression",
        "techniques",
        "data",
        "process",
        "retrieval",
        "Partitioning",
        "Dynamic",
        "Partitioning",
        "Buckets",
        "Hive",
        "data",
        "access",
        "Environment",
        "Apache",
        "Hadoop",
        "HDFS",
        "Cloudera",
        "Manager",
        "Java",
        "MapReduce",
        "Eclipse",
        "Indigo",
        "Hive",
        "HBASE",
        "PIG",
        "Sqoop",
        "Oozie",
        "SQL",
        "Spring",
        "Hadoop",
        "Developer",
        "Maximus",
        "Inc",
        "Reston",
        "VA",
        "September",
        "March",
        "Roles",
        "Responsibilities",
        "requirement",
        "analysis",
        "design",
        "coding",
        "implementation",
        "phases",
        "project",
        "Sqoop",
        "data",
        "databases",
        "data",
        "Teradata",
        "Sqoop",
        "Hive",
        "Tables",
        "automation",
        "delta",
        "Teradata",
        "Sqoop",
        "FTP",
        "Servers",
        "Hive",
        "Performed",
        "Transformations",
        "Denormalizing",
        "Cleansing",
        "data",
        "Date",
        "Transformations",
        "columns",
        "compression",
        "codecs",
        "GZIP",
        "BZIP2",
        "MapReduce",
        "Pig",
        "Hive",
        "performance",
        "Apache",
        "NiFi",
        "data",
        "flow",
        "systems",
        "flow",
        "information",
        "systems",
        "Ansible",
        "automation",
        "frameworks",
        "Avro",
        "JSON",
        "Apache",
        "Log",
        "data",
        "Hive",
        "custom",
        "Hive",
        "SerDes",
        "batch",
        "processing",
        "workflows",
        "Oozie",
        "installation",
        "configuration",
        "multinode",
        "cluster",
        "cloud",
        "Amazon",
        "Web",
        "Services",
        "AWS",
        "EC2",
        "Worked",
        "sprint",
        "methodology",
        "environment",
        "Knox",
        "gateway",
        "Hadoop",
        "security",
        "users",
        "operators",
        "cloud",
        "computing",
        "multinode",
        "cluster",
        "Hadoop",
        "application",
        "cloud",
        "S3",
        "Elastic",
        "Map",
        "EMR",
        "Mapreduce",
        "HiveQL",
        "RC",
        "ORC",
        "tables",
        "compression",
        "techniques",
        "data",
        "process",
        "retrieval",
        "Partitioning",
        "Dynamic",
        "Partitioning",
        "Buckets",
        "Hive",
        "data",
        "access",
        "Environment",
        "Apache",
        "Hadoop",
        "HDFS",
        "Cloudera",
        "Manager",
        "Java",
        "MapReduce",
        "Eclipse",
        "Indigo",
        "Hive",
        "HBASE",
        "PIG",
        "Sqoop",
        "Oozie",
        "SQL",
        "Spring",
        "Java",
        "Developer",
        "Dataguise",
        "Inc",
        "Fremont",
        "CA",
        "November",
        "August",
        "Roles",
        "Responsibilities",
        "phases",
        "project",
        "development",
        "requirements",
        "analysis",
        "design",
        "development",
        "testing",
        "MVC",
        "architecture",
        "Struts",
        "data",
        "frontend",
        "business",
        "layer",
        "Integrated",
        "Struts",
        "Hibernate",
        "Object",
        "mapping",
        "apache",
        "struts",
        "components",
        "DAO",
        "Struts",
        "framework",
        "presentation",
        "tier",
        "control",
        "flow",
        "business",
        "level",
        "validations",
        "business",
        "layer",
        "Integrated",
        "Struts",
        "application",
        "Hibernate",
        "data",
        "management",
        "SQL",
        "server",
        "database",
        "design",
        "development",
        "Web",
        "Application",
        "J2EE",
        "Struts",
        "MVC",
        "Framework",
        "SOAP",
        "Restful",
        "web",
        "services",
        "Web",
        "Services",
        "communication",
        "applications",
        "SOAP",
        "communication",
        "applications",
        "GitHub",
        "version",
        "control",
        "management",
        "quality",
        "code",
        "unit",
        "testing",
        "Maven",
        "script",
        "building",
        "application",
        "XML",
        "schema",
        "Web",
        "Services",
        "data",
        "maintenance",
        "structures",
        "test",
        "plans",
        "test",
        "cases",
        "Unit",
        "testing",
        "system",
        "Object",
        "Oriented",
        "Analysis",
        "Design",
        "UML",
        "development",
        "class",
        "diagrams",
        "Sequence",
        "diagrams",
        "state",
        "diagrams",
        "diagrams",
        "Microsoft",
        "Visio",
        "methodology",
        "environment",
        "MVC",
        "DAOJ2EE",
        "design",
        "patterns",
        "part",
        "application",
        "development",
        "Spring",
        "IOC",
        "MVC",
        "modules",
        "Persistence",
        "Layer",
        "Hibernate",
        "DB2",
        "database",
        "SQL",
        "PLSQL",
        "message",
        "beans",
        "messages",
        "Java",
        "message",
        "queue",
        "Design",
        "development",
        "Web",
        "pages",
        "HTML",
        "CSS",
        "controls",
        "XML",
        "controllers",
        "Spring",
        "MVC",
        "calls",
        "JSP",
        "pages",
        "Environment",
        "Struts",
        "Spring",
        "HTML",
        "CSS",
        "Java",
        "J2ee",
        "JSP",
        "XML",
        "Eclipse",
        "WebLogic",
        "JavaScript",
        "Java",
        "Mail",
        "API",
        "Hibernate",
        "SQL",
        "Server",
        "JBoss",
        "GitHub",
        "Maven",
        "Agile",
        "Junit",
        "Java",
        "Developer",
        "Media3",
        "Pvt",
        "Ltd",
        "Hyderabad",
        "Telangana",
        "June",
        "October",
        "Roles",
        "Responsibilities",
        "presentation",
        "layer",
        "HTML",
        "CSS",
        "JavaScript",
        "Developed",
        "web",
        "components",
        "JSP",
        "Servlets",
        "JDBC",
        "cookies",
        "Servlets",
        "Wrote",
        "SQL",
        "queries",
        "procedures",
        "layer",
        "Hibernate",
        "API",
        "Search",
        "queries",
        "Hibernate",
        "Criteria",
        "interface",
        "support",
        "loans",
        "reports",
        "CBT",
        "Loans",
        "reports",
        "Evans",
        "bank",
        "Jasper",
        "iReport",
        "bugs",
        "unit",
        "testing",
        "test",
        "cases",
        "Junit",
        "Object",
        "Oriented",
        "Analysis",
        "Design",
        "UML",
        "development",
        "class",
        "diagrams",
        "Sequence",
        "diagrams",
        "state",
        "diagrams",
        "diagrams",
        "Microsoft",
        "Visio",
        "Jasper",
        "server",
        "client",
        "server",
        "issues",
        "system",
        "tuning",
        "SQL",
        "efficiency",
        "performance",
        "Tables",
        "indexes",
        "normalizations",
        "Unit",
        "testing",
        "Integration",
        "testing",
        "User",
        "Acceptance",
        "testing",
        "Utilizes",
        "Java",
        "SQL",
        "day",
        "day",
        "issues",
        "client",
        "processes",
        "Environment",
        "Java",
        "Servlets",
        "HTML",
        "Java",
        "Script",
        "JSP",
        "Hibernate",
        "Junit",
        "Testing",
        "Oracle",
        "DB",
        "SQL",
        "Jasper",
        "Reports",
        "iReport",
        "Maven",
        "Jenkins",
        "Education",
        "Bachelors",
        "Computer",
        "Science",
        "Engineering",
        "Computer",
        "Science",
        "Engineering",
        "Indian",
        "Institute",
        "Technology",
        "IIT",
        "Skills",
        "ECLIPSE",
        "EJB",
        "J2EE",
        "JAVA",
        "SPRING",
        "JBOSS",
        "JSP",
        "SERVLETS",
        "STRUTS",
        "DB2",
        "TERADATA",
        "JDBC",
        "MYSQL",
        "SQL",
        "HTML",
        "JAVASCRIPT",
        "PHP",
        "PYTHON",
        "WEB",
        "UI",
        "XML",
        "Information",
        "TECHNICAL",
        "SKILLS",
        "Programming",
        "Skills",
        "JavaJ2EE",
        "JSP",
        "Servlets",
        "AJAX",
        "EJB",
        "Struts",
        "Spring",
        "JDBC",
        "JavaScript",
        "PHP",
        "Python",
        "MYSQL",
        "SQL",
        "DB2",
        "Teradata",
        "Web",
        "services",
        "REST",
        "AWS",
        "WSDL",
        "Servers",
        "Apache",
        "Tomcat",
        "WebSphere",
        "JBoss",
        "Operating",
        "Systems",
        "Unix",
        "Linux",
        "Windows",
        "Solaris",
        "IDE",
        "tools",
        "Eclipse",
        "Eclipse",
        "NetBeans",
        "QA",
        "Tools",
        "Crashlytics",
        "Fabrics",
        "Web",
        "UI",
        "HTML",
        "JavaScript",
        "XML",
        "SOAP",
        "WSDL"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T22:06:50.329217",
    "resume_data": "Sr Hadoop Developer Sr Hadoop span lDeveloperspan Sr Hadoop Developer ATT Inc Middletown NJ Around 9 years of programming experience involved in all phases of Software Development Life Cycle SDLC Over 5 Years of Big Data experience in building highly scalable data analytics applications Strong experience working with Hadoop ecosystem components like HDFS Map Reduce Spark HBase Oozie Hive Sqoop Pig Flume and Kafka Good handson experiencing working with various Hadoop distributions mainly Cloudera CDH Hortonworks HDP and Amazon EMR Good understanding of Distributed Systems architecture and design principles behind Parallel Computing Expertise in developing production ready Spark applications utilizing SparkCore Dataframes SparkSQL SparkML and SparkStreaming APIs SciKitLearn SparkMLMLlib and Tensorflow Strong experience troubleshooting failures in spark applications and finetuning spark applications and hive queries for better performance Worked extensively on Hive for building complex data analytical applications Strong experience writing complex mapreduce jobs including development of custom Input Formats and custom Record Readers Sound Knowledge in map side join reduce side join shuffle sort distributed cache compression techniques multiple hadoop Input output formats Worked with Apache NiFi to automate the data flow between the systems and managed flow of information between system Good experience working with AWS Cloud services like S3 EMR Redshift Athena Dynamo DB etc Deep understanding of performance tuning partitioning for optimizing spark applications Worked on building real time data workflows using Kafka Spark streaming and HBase Extensive knowledge on NoSQL databases like HBase Cassandra and Mongo DB Solid experience in working with csv text sequential avro parquet orc json formats of data Extensive experience in performing ETL on structured semistructured data using Pig Latin Scripts Designed and implemented Hive and Pig UDFs using Java for evaluation filtering loading and storing of data Experience in using Hadoop ecosystem and processing data using Tableau Experience with Apache Phoenix to access the data stored in HBase Good knowledge in the core concepts of programming such as algorithms data structures collections Developed core modules in large crossplatform applications using JAVA JSP Servlets Hibernate RESTful JDBC JavaScript XML and HTML Extensive experience in developing and deploying applications using Web Logic Apache Tomcat and JBOSS Development experience with RDBMS including writing SQL queries views stored procedure triggers etc Strong understanding of Software Development Lifecycle SDLC and various methodologies Waterfall Agile Authorized to work in the US for any employer Work Experience Sr Hadoop Developer ATT Inc Middletown NJ July 2018 to Present Roles Responsibilities Developed Spark applications using PySpark utilizing Data frames and Spark SQL API for faster processing of data Developed highly optimized Spark applications to perform various data cleansing validation transformation and summarization activities according to the requirement Data pipeline consists Spark Hive and Sqoop and custom build Input Adapters to ingest transform and analyze operational data Developed Spark jobs and Hive Jobs to summarize and transform data Used Spark for interactive queries processing of streaming data and integration with popular NoSQL database for huge volume of data Involved in converting HiveSQL queries into Spark transformations using Spark DataFrames and Scala Used different tools for data integration with different databases and Hadoop Analyzed the SQL scripts and designed the solution to implement using Pyspark Involved in installation of Tez and improved the query performance Used Spark for interactive queries processing of streaming data and integration with popular NoSQL database for huge volume of data Built real time data pipelines by developing kafka producers and spark streaming applications for consuming Ingested syslog messages parses them and streams the data to Kafka Handled importing data from different data sources into HDFS using Sqoop and performing transformations using Hive Map Reduce and then loading data into HDFS Exported the analyzed data to the relational databases using Sqoop to further visualize and generate reports for the BI team Collecting and aggregating large amounts of log data using Flume and staging data in HDFS for further analysis Analyzed the data by performing Hive queries Hive QL to study customer behavior Helped Dev ops Engineers for deploying code and debug issues Used Hive to analyze the partitioned and bucketed data and compute various metrics for reporting Developed Hive scripts in Hive QL to denormalize and aggregate the data Scheduled and executed workflows in Oozie to run various jobs Experience in using Hadoop ecosystem and processing data using Amazon AWS Environment Hadoop HDFS HBase Spark Scala Hive MapReduce Sqoop ETL Java PLSQL Oracle 11g UnixLinux Sr Hadoop Developer ON SOLVE Daytona Beach FL April 2017 to July 2018 Roles Responsibilities Build a framework Spark with Scala and migrated existing PySpark applications to improve the runtime and performance Developed highly optimized Spark applications to perform various data cleansing validation transformation and summarization activities according to the requirement Performed Transformations like Denormalizing Cleansing of data sets Date Transformations parsing some complex columns Worked with different compression codecs like GZIP SNAPPY and BZIP2 in MapReduce Pig and Hive for better performance Worked with Apache NiFi to automate the data flow between the systems and managed flow of information between systems Have used Ansible for automation of frameworks Handled Avro JSON and Apache Log data in Hive using custom Hive SerDes Worked on batch processing and scheduled workflows using Oozie Implemented installation and configuration of multinode cluster on the cloud using Amazon Web Services AWS on EC2 Worked in agile sprint methodology environment Have used the Knox gateway for having Hadoop security between the users and operators Used cloud computing on the multinode cluster and deployed Hadoop application on cloud S3 and used Elastic Map Reduce EMR to run Mapreduce Used HiveQL to create partitioned RC ORC tables used compression techniques to optimize data process and faster retrieval Implemented Partitioning Dynamic Partitioning and Buckets in Hive for efficient data access Environment Apache Hadoop HDFS Cloudera Manager Java MapReduce Eclipse Indigo Hive HBASE PIG Sqoop Oozie SQL Spring Hadoop Developer Maximus Inc Reston VA September 2013 to March 2016 Roles Responsibilities Involved in requirement analysis design coding and implementation phases of the project Used Sqoop to load structured data from relational databases into HDFS Loaded transactional data from Teradata using Sqoop and created Hive Tables Worked on automation of delta feeds from Teradata using Sqoop and from FTP Servers to Hive Performed Transformations like Denormalizing Cleansing of data sets Date Transformations parsing some complex columns Worked with different compression codecs like GZIP SNAPPY and BZIP2 in MapReduce Pig and Hive for better performance Worked with Apache NiFi to automate the data flow between the systems and managed flow of information between systems Have used Ansible for automation of frameworks Handled Avro JSON and Apache Log data in Hive using custom Hive SerDes Worked on batch processing and scheduled workflows using Oozie Implemented installation and configuration of multinode cluster on the cloud using Amazon Web Services AWS on EC2 Worked in agile sprint methodology environment Have used the Knox gateway for having Hadoop security between the users and operators Used cloud computing on the multinode cluster and deployed Hadoop application on cloud S3 and used Elastic Map Reduce EMR to run Mapreduce Used HiveQL to create partitioned RC ORC tables used compression techniques to optimize data process and faster retrieval Implemented Partitioning Dynamic Partitioning and Buckets in Hive for efficient data access Environment Apache Hadoop HDFS Cloudera Manager Java MapReduce Eclipse Indigo Hive HBASE PIG Sqoop Oozie SQL Spring Java Developer Dataguise Inc Fremont CA November 2011 to August 2013 Roles Responsibilities Involved in all the phases of the project development requirements gathering analysis design development coding testing and debugging Implemented MVC architecture by using Struts to send and receive the data from frontend to business layer Integrated the Struts and Hibernate to achieve Object relational mapping Used apache struts to develop the webbased components and implemented DAO Implemented Struts framework in the presentation tier for all the essential control flow business level validations and for communicating with the business layer Integrated the Struts application with Hibernate for queryinginserting data management for SQL server database Responsible for design and development of Web Application in J2EE using Struts MVC Framework Involved in creating consuming SOAP based Restful web services Used Web Services for communication between the different internal applications Used SOAP for the communication between the different internal applications Used GitHub for version control management and consistently produced high quality code through disciplined and rigorous unit testing Used Maven script for building and deploying the application Developed the XML schema and Web Services for the data maintenance and structures Involved in designing test plans test cases and overall Unit testing of the system Object Oriented Analysis and Design using UML include development of class diagrams Sequence diagrams and state diagrams and implemented these diagrams in Microsoft Visio Worked in agile sprint methodology environment Implemented MVC DAOJ2EE design patterns as a part of application development Used Spring IOC and MVC for enhanced modules Developed the Persistence Layer using Hibernate Used DB2 as the database and wrote SQL PLSQL Designed and developed message driven beans that consumed the messages from the Java message queue Design and development of Web pages using HTML CSS including Ajax controls and XML Written controllers based on Spring MVC and made calls to JSP pages Environment Struts Spring HTML CSS Java J2ee JSP XML Eclipse WebLogic JavaScript Java Mail API Hibernate SQL Server JBoss GitHub Maven Agile Junit Java Developer Media3 Pvt Ltd Hyderabad Telangana June 2010 to October 2011 Roles Responsibilities Implemented the presentation layer with HTML CSS and JavaScript Developed web components using JSP Servlets and JDBC Implemented secured cookies using Servlets Wrote complex SQL queries and stored procedures Implemented Persistent layer using Hibernate API Implemented Search queries using Hibernate Criteria interface Provided support for loans reports for CBT Designed and developed Loans reports for Evans bank using Jasper and iReport Involved in fixing bugs and unit testing with test cases using Junit Object Oriented Analysis and Design using UML include development of class diagrams Sequence diagrams and state diagrams and implemented these diagrams in Microsoft Visio Maintained Jasper server on client server and resolved issues Actively involved in system testing Fine tuning SQL queries for maximum efficiency to improve the performance Designed Tables and indexes by following normalizations Involved in Unit testing Integration testing and User Acceptance testing Utilizes Java and SQL day to day to debug and fix issues with client processes Environment Java Servlets HTML Java Script JSP Hibernate Junit Testing Oracle DB SQL Jasper Reports iReport Maven Jenkins Education Bachelors in Computer Science Engineering in Computer Science Engineering Indian Institute of Technology IIT Skills ECLIPSE EJB J2EE JAVA SPRING JBOSS JSP SERVLETS STRUTS DB2 TERADATA JDBC MYSQL SQL HTML JAVASCRIPT PHP PYTHON WEB UI XML Additional Information TECHNICAL SKILLS Programming Skills JavaJ2EE JSP Servlets AJAX EJB Struts Spring JDBC JavaScript PHP and Python Databases MYSQL SQL DB2 and Teradata Web services REST AWS SOAP WSDL Servers Apache Tomcat WebSphere JBoss Operating Systems Unix Linux Windows Solaris IDE tools My Eclipse Eclipse NetBeans QA Tools Crashlytics or Fabrics Web UI HTML JavaScript XML SOAP WSDL",
    "unique_id": "1c0d634a-df0b-4b80-928e-1b63f222bde7"
}