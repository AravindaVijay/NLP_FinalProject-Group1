{
    "clean_data": "Data Scientist Python Developer Data Scientist span lPythonspan span lDeveloperspan Data Scientist Python Developer McLean VA Work Experience Data Scientist Python Developer PenFed Credit UnioN McLean VA October 2018 to Present Implemented full lifecycle in Data Modeler Data Analyst Data warehouses and DataMarts with Star Schemas Snowflake Schemas and SCD Dimensional Modeling Erwin Gathering all the data that is required from multiple data sources and creating datasets that will be used in analysis Update Python scripts to match training data with our database stored in AWS Cloud Search so that we would be able to assign each document a response label for further classification Analyze data from AWS Redshift database in Python using psycopg2 Pandas and Numpy module Use AWS Python SDK to download data from DynamoDB Use Python MySQL client to download data from AWS RDS MySQL database Wrote SQL queries to analyze transactional and customer data from MySQL database Use Sparkcontext to analyze dataset and explore data using lambda function Develop MapReduceSpark Python modules for machine learning predictive analytics in Hadoop on AWS Implement endtoend systems for Data Analytics Data Automation and integrated with custom visualization tools using R Python and Hadoop Create Data Quality Scripts using SQL and Hive to validate successful data load and quality of the data Create several types of data visualizations using Python and Tableau Perform data analysis by using Hive to retrieve the data from the Hadoop cluster SQL to retrieve data from Oracle database Wrangle data worked on large datasets acquired data and cleaned the data analyzed trends by making visualizations using mat plot lib and python Oversees the mapping of the integrating process for data into Power BI tool and Reporting Platforms from the data warehouse Create stored procedures in various complexities for SSRS report datasets and Power BI dashboards Created subreports drilldownreports summary reports and parameterized reports in SSRS Develop data mapping documentation to establish relationships between source and target tables including transformation processes using SQL Tableau is used for analyzing the data to show the trends variations and density of the data in form of graphs and charts Tableau was connected to files relational and big data sources to acquire and process data Build and maintain SQL scripts indexes and complex queries for data analysis and extraction Environment Python DynamoDB MySQL SQL AWS RDS AWS Python SDK Spark Lambda function MapReduce Jupyter Notebook PowerBI SSRS Tableau Data Scientist Morgan Stanley Baltimore MD January 2017 to September 2018 Created 3NF business area data modeling with denormalized physical implementation data and information requirements analysis using ERWIN tool Participated in all phases of data mining data collection data cleaning developing models validation visualization and performed Gap analysis Built analytical data pipelines to port data in and out of HadoopHDFS from structured and unstructured sources and designed and implemented system architecture for Amazon EC2 based cloudhosted solution for client Performed data analysis by retrieving the data from the Hadoop cluster Created Logical and Physical data models with Star and Snowflake schema techniques using Erwin in Datawarehouse as well as in Data Mart Involved in Data Analysis Data Validation Data Cleansing Data Verification and identifying data mismatch Resolved the data related issues such as assessing data quality data consolidation evaluating existing data sources Participated in data modeling discussion and provided inputs on both logical and physical data modelling Data transformation from various resources data organization features extraction from raw and stored Collaborated with data engineers and operation team to implement the ETL process wrote and optimized SQL queries to perform data extraction to fit the analytical requirements Involved in creating various reports like Drill through drill down and Adhoc according to the user requirement using SQL Server Reporting Services SSRS Tools Hadoop MapReduce Tableau Python Education Masters Skills Mapreduce Clustering Hadoop Data analysis Mysql Sql Git Hadoop Json Mapreduce Python Ggplot2 Matplotlib Numpy Pandas Shiny Anova Boosting Decision trees Dimensionality reduction",
    "entities": [
        "SQL Server Reporting Services",
        "Python",
        "Data Analysis Data Validation Data Cleansing Data Verification",
        "Data Scientist Python Developer Data Scientist",
        "Create",
        "ETL",
        "Tableau Perform",
        "Oversees",
        "Star and Snowflake",
        "Performed",
        "Power BI",
        "Star Schemas Snowflake",
        "Data Mart Involved",
        "Data Modeler Data Analyst Data",
        "Oracle",
        "Reporting Platforms",
        "Use AWS Python SDK",
        "lDeveloperspan Data Scientist Python Developer McLean VA Work",
        "Data Analytics Data Automation",
        "Mysql Sql Git Hadoop Json Mapreduce Python",
        "DataMarts",
        "Wrangle",
        "Present Implemented",
        "AWS Redshift",
        "SCD Dimensional Modeling Erwin Gathering",
        "Amazon",
        "SQL Tableau",
        "SSRS",
        "SQL",
        "Hadoop",
        "Data",
        "Collaborated",
        "MD",
        "lPythonspan",
        "SSRS Develop",
        "Tableau",
        "ERWIN",
        "Update Python",
        "Datawarehouse",
        "Created Logical and Physical",
        "Hive"
    ],
    "experience": "Experience Data Scientist Python Developer PenFed Credit UnioN McLean VA October 2018 to Present Implemented full lifecycle in Data Modeler Data Analyst Data warehouses and DataMarts with Star Schemas Snowflake Schemas and SCD Dimensional Modeling Erwin Gathering all the data that is required from multiple data sources and creating datasets that will be used in analysis Update Python scripts to match training data with our database stored in AWS Cloud Search so that we would be able to assign each document a response label for further classification Analyze data from AWS Redshift database in Python using psycopg2 Pandas and Numpy module Use AWS Python SDK to download data from DynamoDB Use Python MySQL client to download data from AWS RDS MySQL database Wrote SQL queries to analyze transactional and customer data from MySQL database Use Sparkcontext to analyze dataset and explore data using lambda function Develop MapReduceSpark Python modules for machine learning predictive analytics in Hadoop on AWS Implement endtoend systems for Data Analytics Data Automation and integrated with custom visualization tools using R Python and Hadoop Create Data Quality Scripts using SQL and Hive to validate successful data load and quality of the data Create several types of data visualizations using Python and Tableau Perform data analysis by using Hive to retrieve the data from the Hadoop cluster SQL to retrieve data from Oracle database Wrangle data worked on large datasets acquired data and cleaned the data analyzed trends by making visualizations using mat plot lib and python Oversees the mapping of the integrating process for data into Power BI tool and Reporting Platforms from the data warehouse Create stored procedures in various complexities for SSRS report datasets and Power BI dashboards Created subreports drilldownreports summary reports and parameterized reports in SSRS Develop data mapping documentation to establish relationships between source and target tables including transformation processes using SQL Tableau is used for analyzing the data to show the trends variations and density of the data in form of graphs and charts Tableau was connected to files relational and big data sources to acquire and process data Build and maintain SQL scripts indexes and complex queries for data analysis and extraction Environment Python DynamoDB MySQL SQL AWS RDS AWS Python SDK Spark Lambda function MapReduce Jupyter Notebook PowerBI SSRS Tableau Data Scientist Morgan Stanley Baltimore MD January 2017 to September 2018 Created 3NF business area data modeling with denormalized physical implementation data and information requirements analysis using ERWIN tool Participated in all phases of data mining data collection data cleaning developing models validation visualization and performed Gap analysis Built analytical data pipelines to port data in and out of HadoopHDFS from structured and unstructured sources and designed and implemented system architecture for Amazon EC2 based cloudhosted solution for client Performed data analysis by retrieving the data from the Hadoop cluster Created Logical and Physical data models with Star and Snowflake schema techniques using Erwin in Datawarehouse as well as in Data Mart Involved in Data Analysis Data Validation Data Cleansing Data Verification and identifying data mismatch Resolved the data related issues such as assessing data quality data consolidation evaluating existing data sources Participated in data modeling discussion and provided inputs on both logical and physical data modelling Data transformation from various resources data organization features extraction from raw and stored Collaborated with data engineers and operation team to implement the ETL process wrote and optimized SQL queries to perform data extraction to fit the analytical requirements Involved in creating various reports like Drill through drill down and Adhoc according to the user requirement using SQL Server Reporting Services SSRS Tools Hadoop MapReduce Tableau Python Education Masters Skills Mapreduce Clustering Hadoop Data analysis Mysql Sql Git Hadoop Json Mapreduce Python Ggplot2 Matplotlib Numpy Pandas Shiny Anova Boosting Decision trees Dimensionality reduction",
    "extracted_keywords": [
        "Data",
        "Scientist",
        "Python",
        "Developer",
        "Data",
        "Scientist",
        "span",
        "lPythonspan",
        "span",
        "lDeveloperspan",
        "Data",
        "Scientist",
        "Python",
        "Developer",
        "McLean",
        "VA",
        "Work",
        "Experience",
        "Data",
        "Scientist",
        "Python",
        "Developer",
        "PenFed",
        "Credit",
        "UnioN",
        "McLean",
        "VA",
        "October",
        "Present",
        "lifecycle",
        "Data",
        "Modeler",
        "Data",
        "Analyst",
        "Data",
        "warehouses",
        "DataMarts",
        "Star",
        "Schemas",
        "Snowflake",
        "Schemas",
        "SCD",
        "Dimensional",
        "Modeling",
        "Erwin",
        "data",
        "data",
        "sources",
        "datasets",
        "analysis",
        "Update",
        "Python",
        "scripts",
        "training",
        "data",
        "database",
        "AWS",
        "Cloud",
        "Search",
        "document",
        "response",
        "label",
        "classification",
        "Analyze",
        "data",
        "AWS",
        "Redshift",
        "database",
        "Python",
        "Pandas",
        "Numpy",
        "module",
        "Use",
        "AWS",
        "Python",
        "SDK",
        "data",
        "Use",
        "Python",
        "MySQL",
        "client",
        "data",
        "AWS",
        "RDS",
        "MySQL",
        "database",
        "Wrote",
        "SQL",
        "customer",
        "data",
        "MySQL",
        "database",
        "Use",
        "Sparkcontext",
        "data",
        "lambda",
        "function",
        "Develop",
        "MapReduceSpark",
        "Python",
        "modules",
        "machine",
        "analytics",
        "Hadoop",
        "AWS",
        "Implement",
        "systems",
        "Data",
        "Analytics",
        "Data",
        "Automation",
        "custom",
        "visualization",
        "tools",
        "R",
        "Python",
        "Hadoop",
        "Create",
        "Data",
        "Quality",
        "Scripts",
        "SQL",
        "Hive",
        "data",
        "load",
        "quality",
        "data",
        "types",
        "data",
        "visualizations",
        "Python",
        "Tableau",
        "Perform",
        "data",
        "analysis",
        "Hive",
        "data",
        "Hadoop",
        "cluster",
        "SQL",
        "data",
        "Oracle",
        "database",
        "Wrangle",
        "data",
        "datasets",
        "data",
        "data",
        "trends",
        "visualizations",
        "mat",
        "plot",
        "lib",
        "python",
        "Oversees",
        "mapping",
        "process",
        "data",
        "Power",
        "BI",
        "tool",
        "Reporting",
        "Platforms",
        "data",
        "warehouse",
        "procedures",
        "complexities",
        "SSRS",
        "report",
        "datasets",
        "Power",
        "BI",
        "dashboards",
        "subreports",
        "summary",
        "reports",
        "reports",
        "SSRS",
        "Develop",
        "data",
        "mapping",
        "documentation",
        "relationships",
        "source",
        "target",
        "tables",
        "transformation",
        "processes",
        "SQL",
        "Tableau",
        "data",
        "trends",
        "variations",
        "density",
        "data",
        "form",
        "graphs",
        "charts",
        "Tableau",
        "files",
        "data",
        "sources",
        "process",
        "data",
        "Build",
        "SQL",
        "scripts",
        "indexes",
        "queries",
        "data",
        "analysis",
        "extraction",
        "Environment",
        "Python",
        "DynamoDB",
        "MySQL",
        "SQL",
        "AWS",
        "RDS",
        "Python",
        "SDK",
        "Spark",
        "Lambda",
        "MapReduce",
        "Jupyter",
        "Notebook",
        "PowerBI",
        "SSRS",
        "Tableau",
        "Data",
        "Scientist",
        "Morgan",
        "Stanley",
        "Baltimore",
        "MD",
        "January",
        "September",
        "business",
        "area",
        "data",
        "implementation",
        "data",
        "information",
        "requirements",
        "analysis",
        "ERWIN",
        "tool",
        "phases",
        "data",
        "mining",
        "data",
        "collection",
        "data",
        "models",
        "validation",
        "visualization",
        "analysis",
        "data",
        "pipelines",
        "port",
        "data",
        "sources",
        "system",
        "architecture",
        "Amazon",
        "EC2",
        "solution",
        "client",
        "Performed",
        "data",
        "analysis",
        "data",
        "Hadoop",
        "cluster",
        "Created",
        "Logical",
        "data",
        "models",
        "Star",
        "Snowflake",
        "schema",
        "techniques",
        "Erwin",
        "Datawarehouse",
        "Data",
        "Mart",
        "Data",
        "Analysis",
        "Data",
        "Validation",
        "Data",
        "Cleansing",
        "Data",
        "Verification",
        "data",
        "mismatch",
        "data",
        "issues",
        "data",
        "quality",
        "data",
        "consolidation",
        "data",
        "sources",
        "data",
        "modeling",
        "discussion",
        "inputs",
        "data",
        "Data",
        "transformation",
        "resources",
        "data",
        "organization",
        "extraction",
        "Collaborated",
        "data",
        "engineers",
        "operation",
        "team",
        "ETL",
        "process",
        "SQL",
        "queries",
        "data",
        "extraction",
        "requirements",
        "reports",
        "Drill",
        "Adhoc",
        "user",
        "requirement",
        "SQL",
        "Server",
        "Reporting",
        "Services",
        "SSRS",
        "Tools",
        "Hadoop",
        "MapReduce",
        "Tableau",
        "Python",
        "Education",
        "Masters",
        "Skills",
        "Mapreduce",
        "Clustering",
        "Hadoop",
        "Data",
        "analysis",
        "Mysql",
        "Sql",
        "Git",
        "Hadoop",
        "Json",
        "Mapreduce",
        "Python",
        "Ggplot2",
        "Matplotlib",
        "Numpy",
        "Pandas",
        "Shiny",
        "Anova",
        "Boosting",
        "Decision",
        "trees",
        "Dimensionality",
        "reduction"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T21:34:18.460921",
    "resume_data": "Data Scientist Python Developer Data Scientist span lPythonspan span lDeveloperspan Data Scientist Python Developer McLean VA Work Experience Data Scientist Python Developer PenFed Credit UnioN McLean VA October 2018 to Present Implemented full lifecycle in Data Modeler Data Analyst Data warehouses and DataMarts with Star Schemas Snowflake Schemas and SCD Dimensional Modeling Erwin Gathering all the data that is required from multiple data sources and creating datasets that will be used in analysis Update Python scripts to match training data with our database stored in AWS Cloud Search so that we would be able to assign each document a response label for further classification Analyze data from AWS Redshift database in Python using psycopg2 Pandas and Numpy module Use AWS Python SDK to download data from DynamoDB Use Python MySQL client to download data from AWS RDS MySQL database Wrote SQL queries to analyze transactional and customer data from MySQL database Use Sparkcontext to analyze dataset and explore data using lambda function Develop MapReduceSpark Python modules for machine learning predictive analytics in Hadoop on AWS Implement endtoend systems for Data Analytics Data Automation and integrated with custom visualization tools using R Python and Hadoop Create Data Quality Scripts using SQL and Hive to validate successful data load and quality of the data Create several types of data visualizations using Python and Tableau Perform data analysis by using Hive to retrieve the data from the Hadoop cluster SQL to retrieve data from Oracle database Wrangle data worked on large datasets acquired data and cleaned the data analyzed trends by making visualizations using mat plot lib and python Oversees the mapping of the integrating process for data into Power BI tool and Reporting Platforms from the data warehouse Create stored procedures in various complexities for SSRS report datasets and Power BI dashboards Created subreports drilldownreports summary reports and parameterized reports in SSRS Develop data mapping documentation to establish relationships between source and target tables including transformation processes using SQL Tableau is used for analyzing the data to show the trends variations and density of the data in form of graphs and charts Tableau was connected to files relational and big data sources to acquire and process data Build and maintain SQL scripts indexes and complex queries for data analysis and extraction Environment Python DynamoDB MySQL SQL AWS RDS AWS Python SDK Spark Lambda function MapReduce Jupyter Notebook PowerBI SSRS Tableau Data Scientist Morgan Stanley Baltimore MD January 2017 to September 2018 Created 3NF business area data modeling with denormalized physical implementation data and information requirements analysis using ERWIN tool Participated in all phases of data mining data collection data cleaning developing models validation visualization and performed Gap analysis Built analytical data pipelines to port data in and out of HadoopHDFS from structured and unstructured sources and designed and implemented system architecture for Amazon EC2 based cloudhosted solution for client Performed data analysis by retrieving the data from the Hadoop cluster Created Logical and Physical data models with Star and Snowflake schema techniques using Erwin in Datawarehouse as well as in Data Mart Involved in Data Analysis Data Validation Data Cleansing Data Verification and identifying data mismatch Resolved the data related issues such as assessing data quality data consolidation evaluating existing data sources Participated in data modeling discussion and provided inputs on both logical and physical data modelling Data transformation from various resources data organization features extraction from raw and stored Collaborated with data engineers and operation team to implement the ETL process wrote and optimized SQL queries to perform data extraction to fit the analytical requirements Involved in creating various reports like Drill through drill down and Adhoc according to the user requirement using SQL Server Reporting Services SSRS Tools Hadoop MapReduce Tableau Python Education Masters Skills Mapreduce Clustering Hadoop Data analysis Mysql Sql Git Hadoop Json Mapreduce Python Ggplot2 Matplotlib Numpy Pandas Shiny Anova Boosting Decision trees Dimensionality reduction",
    "unique_id": "c6a2ce0c-8215-4667-9031-0325ffe9c71b"
}