{
    "clean_data": "Site Reliability Engineer Site Reliability Engineer Software Developer Experience in Analysis Design Development Testing Customization Bug fixes Enhancement Support and Implementation of various standalone clientserver enterprise applications Extensive experience in PYTHON PROGRAMMING Achived extensive experience in Business Intelligence BI tool MSBI Working experience in Microsoft SQL SERVER Solid experince in creating ETL Packages using SSIS Real Time experience in ROBOTIC AUTOMATION PROCESSRPA used KAPOW and BLUE PRISM tools for automation process Skillful experience in Data analysis concepts like Machine learning Stasitical methods Scrapy Pandas Beautiful soup numpy Scipy Pickle SQLITE NLTK JSON PYQT4 Experince in data Visulization and hands on Matlablib Tableau SSRS Knowledge on data analysis tools like R Microsoft Azure SAS Extensive Automation Testing done using PYTEST Unit Test for GUIs and analytics Working experience in Unix Ubuntu for morethan 15 years and excellent shell scripting programming skills Familiar with data architecture including data ingestion pipeline design Hadoop information architecture data modeling and data mining machine learning and advanced data processing Hands on experience in AWS Git JIRA Working knowledge on IOT stack tools like AWS Green Grass Raspberry Pi etc Experience in writing Sub Queries Stored Procedures Triggers Cursors and Functions on MySQL and PostgreSQL database Familiar with JSON based REST Web services Having experienced in Agile Methodologies Scrum stories and sprints experience in a Python based environment along with data analytics data wrangling and Excel data extracts Excellent Interpersonal and communication skills efficient time management and organization skills ability to handle multiple tasks and work well in a team environment Excellent written and oral communication skills with resultsoriented attitude Experience in working with different operating systems WINDOWS LINUX Work Experience Site Reliability Engineer Bank Of America December 2017 to Present Software Developer Union Bank San Diego CA June 2017 to August 2017 Web Scraper Description The Project involves data retrieving from multiple websites formatting them and saving back to FileNet The process automation is the main objective of the Responsibilities The main languages used for the project are Python SQL JS Finished a project that deals with 100s of gigabytes of data and used ETL processes for the whole project The ROBOTIC PROCESS AUTOMATION was done using KAPOW tool Used KAPOW Tool to extract data from websites Implementing database usage for time taking tasks and reduced time consumption Java Script was used a lot for many tasks and proper outcomes The data visulazation was done using tableau Used SQL to build tables and run queries Converted data into JSON CSV XML files according to the requirements Fixed bugs for first two months and made several changes in the existing codes Automated several tasks for easy usage of business users Testing is done extensively and SQL reporting service is used to report the activities Finished a project from start point to end point by my own and got a good feedback from the manager and client Have experience in working multiple projects a time Created ETL packages in SSIS for process quality betterment Environment Python Kapow Tableau ETL SQL JAVA SCRPIT XML JSON HIVE Shell Scripting HTTP web services Windows California University November 2016 to March 2017 Data analysis Finished the whole project using PYTHON and R Used a lot of OOPs concepts for valid output The modules Like PANDAS was handy for all statistical analysis The WEB SCRAPING part was done using SCRAPY modules and tools like Blue prism MACHINE LEARNING for forecasting purpose and modules like SCISKIT NLTK Learning and used regression Models for process improvement By using SQL express 2014 many tables were created and reports were also made Used APIs for ML or data analytics Data Was stored using JSON CSV formats The Data visualization was implemented using libraries like SeaBorn Matlablib tableau Used AWS Elastic Search and S3 storage for data storage Coding was done on Windows OS IPYTHON editor California University March 2016 to August 2016 of management and Sciences March 2016 Aug 2016 DJANGO Web Development The whole project was developed using PYTHON and DJANGO Framework Used a lot of OOPs concepts for a valid output The Frame work DJANGO was used vastly and the apps like HTML The WEB SCRAPING part was done using SCRAPY modules and tools like Blue prism Came across of REST web services in developing process The Developing process was done in IPYTHON editor Testing was done using PYTEST module to check the output Used AWS Elastic Search and S3 storage for data storage Coding was done on Windows OS California University March 2015 to August 2015 Python Programming The multiple projects using were finished using PYTHON 3X version The usage of OOPs concepts was handy for many task Over 600 s programs were written using Sublime text editor Mastered many intermediate level Python concepts like Decorators Generators Modules Worked on both the versions of python 2X and 3X later changed to 3X Coding was done on LINUX OS Python developer WeSecureapp Hyderabad Telangana January 2014 to November 2014 Description The assignment involves in the data collection from Clients and storing them and available for user friendly Responsibilities Developed entire backend modules using Python on Web2py framework Extensively worked in backend development using Python Retrieved data using ELASTICSEARCH and used all other modules of Elastics Developed API modularizing existing python module with the help of pyYAML libraries Used several python libraries like wxPython numpy and matPlotLib Responsible for user validations on client side as well as server side Automated the existing scripts for performance calculations Worked with JSON based REST Web services and AWS Used ML extensively for analyzing and forecasting Built development environment with JIRA StashGit Followed AGILE development methodology to develop the application Placed data into JSON files using Python Participated in the complete SDLC process Testing was done using PYTEST module to check the output Involved in Python OOP code for quality logging monitoring and debugging code optimization Used Django Database APIs to access database objects Environment Python SQL SCIKit learn XML JSON Eclipse Shell Scripting HTTP web services Windows Linux InternshipJAVA Intugine Technologies Pvt Ltd Bengaluru Karnataka June 2013 to December 2013 developing Contributed to servlet based application development Assisted in maintaining and updating existing applications and modules Helped design form validation programs using HTML Provided assistance and support to programming team members as require Internship ING Hyderabad Telangana January 2013 to April 2013 Conducting market research and finding local vendors in the city Getting vendors on board by pitching the benefits of partnering with company Brainstorming and coming up with ideas and suggestions for the benefit of the company Education Master of Science in Computer Information System California University of Management and Sciences January 2015 to March 2017 Skills AWS 1 year JSON 1 year SQL 1 year DJANGO 1 year REST 1 year Additional Information Skills Languages PYHON SQL JAVA C Java script Shell Scriptingbasics Framework KAPOW MSBI Blue Prism HTML DJANGO AWS Tableau Software development skills Agile methodologies Water fall Methods Iterative process Project management Operations and managements Knowledge on Concepts HTTP HTTPs AJAX REST APIS APIs JSON R AZURE SAS Platforms WindowsVistaXP UNIX Linux Hadoop MapReduce ETL Hive SparkBasics",
    "entities": [
        "Elastics Developed API",
        "Linux Hadoop MapReduce ETL",
        "Fixed",
        "ETL",
        "SCISKIT NLTK Learning",
        "Sublime",
        "Kapow Tableau",
        "AWS Git JIRA Working",
        "IOT",
        "UNIX",
        "AWS",
        "HTML Provided",
        "Internship ING Hyderabad",
        "Coding",
        "Sub Queries Stored Procedures Triggers Cursors and Functions",
        "Microsoft",
        "Windows OS California University",
        "Project management Operations",
        "Created ETL",
        "Business Intelligence BI",
        "Windows California University",
        "Present Software Developer Union Bank",
        "SCRAPY",
        "Frame",
        "Mastered",
        "IPYTHON",
        "Shell Scriptingbasics Framework",
        "ML",
        "AGILE",
        "Used AWS Elastic Search",
        "Contributed",
        "Matlablib Tableau",
        "Web Scraper Description",
        "SQL",
        "Machine learning Stasitical",
        "Data",
        "Hadoop",
        "REST",
        "Testing",
        "SeaBorn Matlablib",
        "California University",
        "Science in Computer Information System California University of Management and Sciences",
        "Site Reliability Engineer Site Reliability Engineer Software Developer Experience in Analysis Design Development Testing Customization Bug",
        "Windows OS IPYTHON",
        "Python SQL JS",
        "Blue prism MACHINE LEARNING",
        "Worked with JSON",
        "Automated",
        "matPlotLib Responsible",
        "FileNet",
        "SSIS"
    ],
    "experience": "Experience in Analysis Design Development Testing Customization Bug fixes Enhancement Support and Implementation of various standalone clientserver enterprise applications Extensive experience in PYTHON PROGRAMMING Achived extensive experience in Business Intelligence BI tool MSBI Working experience in Microsoft SQL SERVER Solid experince in creating ETL Packages using SSIS Real Time experience in ROBOTIC AUTOMATION PROCESSRPA used KAPOW and BLUE PRISM tools for automation process Skillful experience in Data analysis concepts like Machine learning Stasitical methods Scrapy Pandas Beautiful soup numpy Scipy Pickle SQLITE NLTK JSON PYQT4 Experince in data Visulization and hands on Matlablib Tableau SSRS Knowledge on data analysis tools like R Microsoft Azure SAS Extensive Automation Testing done using PYTEST Unit Test for GUIs and analytics Working experience in Unix Ubuntu for morethan 15 years and excellent shell scripting programming skills Familiar with data architecture including data ingestion pipeline design Hadoop information architecture data modeling and data mining machine learning and advanced data processing Hands on experience in AWS Git JIRA Working knowledge on IOT stack tools like AWS Green Grass Raspberry Pi etc Experience in writing Sub Queries Stored Procedures Triggers Cursors and Functions on MySQL and PostgreSQL database Familiar with JSON based REST Web services Having experienced in Agile Methodologies Scrum stories and sprints experience in a Python based environment along with data analytics data wrangling and Excel data extracts Excellent Interpersonal and communication skills efficient time management and organization skills ability to handle multiple tasks and work well in a team environment Excellent written and oral communication skills with resultsoriented attitude Experience in working with different operating systems WINDOWS LINUX Work Experience Site Reliability Engineer Bank Of America December 2017 to Present Software Developer Union Bank San Diego CA June 2017 to August 2017 Web Scraper Description The Project involves data retrieving from multiple websites formatting them and saving back to FileNet The process automation is the main objective of the Responsibilities The main languages used for the project are Python SQL JS Finished a project that deals with 100s of gigabytes of data and used ETL processes for the whole project The ROBOTIC PROCESS AUTOMATION was done using KAPOW tool Used KAPOW Tool to extract data from websites Implementing database usage for time taking tasks and reduced time consumption Java Script was used a lot for many tasks and proper outcomes The data visulazation was done using tableau Used SQL to build tables and run queries Converted data into JSON CSV XML files according to the requirements Fixed bugs for first two months and made several changes in the existing codes Automated several tasks for easy usage of business users Testing is done extensively and SQL reporting service is used to report the activities Finished a project from start point to end point by my own and got a good feedback from the manager and client Have experience in working multiple projects a time Created ETL packages in SSIS for process quality betterment Environment Python Kapow Tableau ETL SQL JAVA SCRPIT XML JSON HIVE Shell Scripting HTTP web services Windows California University November 2016 to March 2017 Data analysis Finished the whole project using PYTHON and R Used a lot of OOPs concepts for valid output The modules Like PANDAS was handy for all statistical analysis The WEB SCRAPING part was done using SCRAPY modules and tools like Blue prism MACHINE LEARNING for forecasting purpose and modules like SCISKIT NLTK Learning and used regression Models for process improvement By using SQL express 2014 many tables were created and reports were also made Used APIs for ML or data analytics Data Was stored using JSON CSV formats The Data visualization was implemented using libraries like SeaBorn Matlablib tableau Used AWS Elastic Search and S3 storage for data storage Coding was done on Windows OS IPYTHON editor California University March 2016 to August 2016 of management and Sciences March 2016 Aug 2016 DJANGO Web Development The whole project was developed using PYTHON and DJANGO Framework Used a lot of OOPs concepts for a valid output The Frame work DJANGO was used vastly and the apps like HTML The WEB SCRAPING part was done using SCRAPY modules and tools like Blue prism Came across of REST web services in developing process The Developing process was done in IPYTHON editor Testing was done using PYTEST module to check the output Used AWS Elastic Search and S3 storage for data storage Coding was done on Windows OS California University March 2015 to August 2015 Python Programming The multiple projects using were finished using PYTHON 3X version The usage of OOPs concepts was handy for many task Over 600 s programs were written using Sublime text editor Mastered many intermediate level Python concepts like Decorators Generators Modules Worked on both the versions of python 2X and 3X later changed to 3X Coding was done on LINUX OS Python developer WeSecureapp Hyderabad Telangana January 2014 to November 2014 Description The assignment involves in the data collection from Clients and storing them and available for user friendly Responsibilities Developed entire backend modules using Python on Web2py framework Extensively worked in backend development using Python Retrieved data using ELASTICSEARCH and used all other modules of Elastics Developed API modularizing existing python module with the help of pyYAML libraries Used several python libraries like wxPython numpy and matPlotLib Responsible for user validations on client side as well as server side Automated the existing scripts for performance calculations Worked with JSON based REST Web services and AWS Used ML extensively for analyzing and forecasting Built development environment with JIRA StashGit Followed AGILE development methodology to develop the application Placed data into JSON files using Python Participated in the complete SDLC process Testing was done using PYTEST module to check the output Involved in Python OOP code for quality logging monitoring and debugging code optimization Used Django Database APIs to access database objects Environment Python SQL SCIKit learn XML JSON Eclipse Shell Scripting HTTP web services Windows Linux InternshipJAVA Intugine Technologies Pvt Ltd Bengaluru Karnataka June 2013 to December 2013 developing Contributed to servlet based application development Assisted in maintaining and updating existing applications and modules Helped design form validation programs using HTML Provided assistance and support to programming team members as require Internship ING Hyderabad Telangana January 2013 to April 2013 Conducting market research and finding local vendors in the city Getting vendors on board by pitching the benefits of partnering with company Brainstorming and coming up with ideas and suggestions for the benefit of the company Education Master of Science in Computer Information System California University of Management and Sciences January 2015 to March 2017 Skills AWS 1 year JSON 1 year SQL 1 year DJANGO 1 year REST 1 year Additional Information Skills Languages PYHON SQL JAVA C Java script Shell Scriptingbasics Framework KAPOW MSBI Blue Prism HTML DJANGO AWS Tableau Software development skills Agile methodologies Water fall Methods Iterative process Project management Operations and managements Knowledge on Concepts HTTP HTTPs AJAX REST APIS APIs JSON R AZURE SAS Platforms WindowsVistaXP UNIX Linux Hadoop MapReduce ETL Hive SparkBasics",
    "extracted_keywords": [
        "Site",
        "Reliability",
        "Engineer",
        "Site",
        "Reliability",
        "Engineer",
        "Software",
        "Developer",
        "Experience",
        "Analysis",
        "Design",
        "Development",
        "Testing",
        "Customization",
        "Bug",
        "fixes",
        "Enhancement",
        "Support",
        "Implementation",
        "enterprise",
        "applications",
        "experience",
        "PYTHON",
        "PROGRAMMING",
        "experience",
        "Business",
        "Intelligence",
        "BI",
        "tool",
        "MSBI",
        "Working",
        "experience",
        "Microsoft",
        "SQL",
        "SERVER",
        "experince",
        "ETL",
        "Packages",
        "SSIS",
        "Real",
        "Time",
        "experience",
        "ROBOTIC",
        "AUTOMATION",
        "PROCESSRPA",
        "KAPOW",
        "BLUE",
        "tools",
        "automation",
        "process",
        "experience",
        "Data",
        "analysis",
        "concepts",
        "Machine",
        "Stasitical",
        "methods",
        "Scrapy",
        "Pandas",
        "soup",
        "numpy",
        "Scipy",
        "Pickle",
        "SQLITE",
        "NLTK",
        "JSON",
        "PYQT4",
        "Experince",
        "data",
        "Visulization",
        "hands",
        "Matlablib",
        "Tableau",
        "SSRS",
        "Knowledge",
        "data",
        "analysis",
        "tools",
        "R",
        "Microsoft",
        "Azure",
        "SAS",
        "Extensive",
        "Automation",
        "Testing",
        "PYTEST",
        "Unit",
        "Test",
        "GUIs",
        "Working",
        "experience",
        "Unix",
        "Ubuntu",
        "morethan",
        "years",
        "shell",
        "scripting",
        "programming",
        "skills",
        "data",
        "architecture",
        "data",
        "ingestion",
        "pipeline",
        "design",
        "Hadoop",
        "information",
        "architecture",
        "data",
        "modeling",
        "data",
        "mining",
        "machine",
        "learning",
        "data",
        "Hands",
        "experience",
        "AWS",
        "Git",
        "JIRA",
        "Working",
        "knowledge",
        "IOT",
        "stack",
        "tools",
        "AWS",
        "Green",
        "Grass",
        "Raspberry",
        "Pi",
        "Experience",
        "Sub",
        "Queries",
        "Stored",
        "Procedures",
        "Triggers",
        "Cursors",
        "Functions",
        "MySQL",
        "PostgreSQL",
        "database",
        "JSON",
        "REST",
        "Web",
        "services",
        "Agile",
        "Methodologies",
        "Scrum",
        "stories",
        "sprints",
        "experience",
        "Python",
        "environment",
        "data",
        "analytics",
        "data",
        "Excel",
        "data",
        "Excellent",
        "Interpersonal",
        "communication",
        "skills",
        "time",
        "management",
        "organization",
        "ability",
        "tasks",
        "team",
        "environment",
        "Excellent",
        "communication",
        "skills",
        "attitude",
        "Experience",
        "operating",
        "systems",
        "WINDOWS",
        "LINUX",
        "Work",
        "Experience",
        "Site",
        "Reliability",
        "Engineer",
        "Bank",
        "America",
        "December",
        "Present",
        "Software",
        "Developer",
        "Union",
        "Bank",
        "San",
        "Diego",
        "CA",
        "June",
        "August",
        "Web",
        "Scraper",
        "Description",
        "Project",
        "data",
        "websites",
        "FileNet",
        "process",
        "automation",
        "objective",
        "Responsibilities",
        "languages",
        "project",
        "Python",
        "SQL",
        "JS",
        "project",
        "100s",
        "gigabytes",
        "data",
        "ETL",
        "processes",
        "project",
        "ROBOTIC",
        "PROCESS",
        "AUTOMATION",
        "KAPOW",
        "tool",
        "KAPOW",
        "Tool",
        "data",
        "websites",
        "database",
        "usage",
        "time",
        "tasks",
        "time",
        "consumption",
        "Java",
        "Script",
        "lot",
        "tasks",
        "outcomes",
        "data",
        "visulazation",
        "tableau",
        "SQL",
        "tables",
        "run",
        "queries",
        "data",
        "CSV",
        "XML",
        "files",
        "requirements",
        "bugs",
        "months",
        "changes",
        "codes",
        "tasks",
        "usage",
        "business",
        "users",
        "Testing",
        "SQL",
        "reporting",
        "service",
        "activities",
        "project",
        "start",
        "point",
        "end",
        "point",
        "feedback",
        "manager",
        "client",
        "experience",
        "projects",
        "time",
        "ETL",
        "packages",
        "SSIS",
        "process",
        "quality",
        "betterment",
        "Environment",
        "Python",
        "Kapow",
        "Tableau",
        "ETL",
        "SQL",
        "SCRPIT",
        "XML",
        "JSON",
        "HIVE",
        "Shell",
        "Scripting",
        "HTTP",
        "web",
        "services",
        "Windows",
        "California",
        "University",
        "November",
        "March",
        "Data",
        "analysis",
        "project",
        "PYTHON",
        "R",
        "lot",
        "OOPs",
        "concepts",
        "output",
        "modules",
        "PANDAS",
        "analysis",
        "WEB",
        "SCRAPING",
        "part",
        "SCRAPY",
        "modules",
        "tools",
        "prism",
        "MACHINE",
        "LEARNING",
        "forecasting",
        "purpose",
        "modules",
        "SCISKIT",
        "NLTK",
        "Learning",
        "regression",
        "Models",
        "process",
        "improvement",
        "SQL",
        "tables",
        "reports",
        "APIs",
        "ML",
        "data",
        "analytics",
        "Data",
        "JSON",
        "CSV",
        "formats",
        "Data",
        "visualization",
        "libraries",
        "SeaBorn",
        "Matlablib",
        "tableau",
        "AWS",
        "Elastic",
        "Search",
        "S3",
        "storage",
        "data",
        "storage",
        "Coding",
        "Windows",
        "OS",
        "IPYTHON",
        "editor",
        "California",
        "University",
        "March",
        "August",
        "management",
        "Sciences",
        "March",
        "Aug",
        "DJANGO",
        "Web",
        "Development",
        "project",
        "PYTHON",
        "DJANGO",
        "Framework",
        "lot",
        "OOPs",
        "concepts",
        "output",
        "Frame",
        "work",
        "DJANGO",
        "apps",
        "HTML",
        "WEB",
        "SCRAPING",
        "part",
        "SCRAPY",
        "modules",
        "tools",
        "prism",
        "REST",
        "web",
        "services",
        "process",
        "process",
        "IPYTHON",
        "editor",
        "Testing",
        "PYTEST",
        "module",
        "output",
        "AWS",
        "Elastic",
        "Search",
        "S3",
        "storage",
        "data",
        "storage",
        "Coding",
        "Windows",
        "OS",
        "California",
        "University",
        "March",
        "August",
        "Python",
        "Programming",
        "projects",
        "PYTHON",
        "3X",
        "version",
        "usage",
        "OOPs",
        "concepts",
        "task",
        "s",
        "programs",
        "Sublime",
        "text",
        "editor",
        "level",
        "Python",
        "concepts",
        "Decorators",
        "Generators",
        "Modules",
        "versions",
        "python",
        "2X",
        "3X",
        "3X",
        "Coding",
        "LINUX",
        "OS",
        "Python",
        "developer",
        "WeSecureapp",
        "Hyderabad",
        "Telangana",
        "January",
        "November",
        "Description",
        "assignment",
        "data",
        "collection",
        "Clients",
        "user",
        "Responsibilities",
        "modules",
        "Python",
        "Web2py",
        "framework",
        "development",
        "Python",
        "Retrieved",
        "data",
        "ELASTICSEARCH",
        "modules",
        "Elastics",
        "API",
        "module",
        "help",
        "pyYAML",
        "libraries",
        "python",
        "libraries",
        "wxPython",
        "numpy",
        "matPlotLib",
        "Responsible",
        "user",
        "validations",
        "client",
        "side",
        "server",
        "side",
        "scripts",
        "performance",
        "calculations",
        "JSON",
        "REST",
        "Web",
        "services",
        "AWS",
        "ML",
        "development",
        "environment",
        "JIRA",
        "StashGit",
        "AGILE",
        "development",
        "methodology",
        "application",
        "data",
        "files",
        "Python",
        "Participated",
        "SDLC",
        "process",
        "Testing",
        "PYTEST",
        "module",
        "output",
        "Python",
        "OOP",
        "code",
        "quality",
        "monitoring",
        "code",
        "optimization",
        "Django",
        "Database",
        "APIs",
        "database",
        "Environment",
        "Python",
        "SQL",
        "SCIKit",
        "XML",
        "JSON",
        "Eclipse",
        "Shell",
        "Scripting",
        "HTTP",
        "web",
        "services",
        "Windows",
        "Linux",
        "InternshipJAVA",
        "Intugine",
        "Technologies",
        "Pvt",
        "Ltd",
        "Bengaluru",
        "Karnataka",
        "June",
        "December",
        "Contributed",
        "application",
        "development",
        "applications",
        "modules",
        "form",
        "validation",
        "programs",
        "HTML",
        "assistance",
        "support",
        "team",
        "members",
        "Internship",
        "ING",
        "Hyderabad",
        "Telangana",
        "January",
        "April",
        "Conducting",
        "market",
        "research",
        "vendors",
        "city",
        "vendors",
        "board",
        "benefits",
        "partnering",
        "company",
        "ideas",
        "suggestions",
        "benefit",
        "company",
        "Education",
        "Master",
        "Science",
        "Computer",
        "Information",
        "System",
        "California",
        "University",
        "Management",
        "Sciences",
        "January",
        "March",
        "Skills",
        "AWS",
        "year",
        "JSON",
        "year",
        "SQL",
        "year",
        "DJANGO",
        "year",
        "REST",
        "year",
        "Additional",
        "Information",
        "Skills",
        "Languages",
        "PYHON",
        "SQL",
        "JAVA",
        "C",
        "Java",
        "script",
        "Shell",
        "Scriptingbasics",
        "Framework",
        "MSBI",
        "Blue",
        "Prism",
        "HTML",
        "DJANGO",
        "Tableau",
        "Software",
        "development",
        "methodologies",
        "Water",
        "fall",
        "Methods",
        "Iterative",
        "process",
        "Project",
        "management",
        "Operations",
        "Knowledge",
        "Concepts",
        "HTTP",
        "HTTPs",
        "AJAX",
        "REST",
        "APIS",
        "APIs",
        "JSON",
        "R",
        "AZURE",
        "SAS",
        "Platforms",
        "UNIX",
        "Linux",
        "Hadoop",
        "MapReduce",
        "ETL",
        "Hive",
        "SparkBasics"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T21:12:04.803960",
    "resume_data": "Site Reliability Engineer Site Reliability Engineer Software Developer Experience in Analysis Design Development Testing Customization Bug fixes Enhancement Support and Implementation of various standalone clientserver enterprise applications Extensive experience in PYTHON PROGRAMMING Achived extensive experience in Business Intelligence BI tool MSBI Working experience in Microsoft SQL SERVER Solid experince in creating ETL Packages using SSIS Real Time experience in ROBOTIC AUTOMATION PROCESSRPA used KAPOW and BLUE PRISM tools for automation process Skillful experience in Data analysis concepts like Machine learning Stasitical methods Scrapy Pandas Beautiful soup numpy Scipy Pickle SQLITE NLTK JSON PYQT4 Experince in data Visulization and hands on Matlablib Tableau SSRS Knowledge on data analysis tools like R Microsoft Azure SAS Extensive Automation Testing done using PYTEST Unit Test for GUIs and analytics Working experience in Unix Ubuntu for morethan 15 years and excellent shell scripting programming skills Familiar with data architecture including data ingestion pipeline design Hadoop information architecture data modeling and data mining machine learning and advanced data processing Hands on experience in AWS Git JIRA Working knowledge on IOT stack tools like AWS Green Grass Raspberry Pi etc Experience in writing Sub Queries Stored Procedures Triggers Cursors and Functions on MySQL and PostgreSQL database Familiar with JSON based REST Web services Having experienced in Agile Methodologies Scrum stories and sprints experience in a Python based environment along with data analytics data wrangling and Excel data extracts Excellent Interpersonal and communication skills efficient time management and organization skills ability to handle multiple tasks and work well in a team environment Excellent written and oral communication skills with resultsoriented attitude Experience in working with different operating systems WINDOWS LINUX Work Experience Site Reliability Engineer Bank Of America December 2017 to Present Software Developer Union Bank San Diego CA June 2017 to August 2017 Web Scraper Description The Project involves data retrieving from multiple websites formatting them and saving back to FileNet The process automation is the main objective of the Responsibilities The main languages used for the project are Python SQL JS Finished a project that deals with 100s of gigabytes of data and used ETL processes for the whole project The ROBOTIC PROCESS AUTOMATION was done using KAPOW tool Used KAPOW Tool to extract data from websites Implementing database usage for time taking tasks and reduced time consumption Java Script was used a lot for many tasks and proper outcomes The data visulazation was done using tableau Used SQL to build tables and run queries Converted data into JSON CSV XML files according to the requirements Fixed bugs for first two months and made several changes in the existing codes Automated several tasks for easy usage of business users Testing is done extensively and SQL reporting service is used to report the activities Finished a project from start point to end point by my own and got a good feedback from the manager and client Have experience in working multiple projects a time Created ETL packages in SSIS for process quality betterment Environment Python Kapow Tableau ETL SQL JAVA SCRPIT XML JSON HIVE Shell Scripting HTTP web services Windows California University November 2016 to March 2017 Data analysis Finished the whole project using PYTHON and R Used a lot of OOPs concepts for valid output The modules Like PANDAS was handy for all statistical analysis The WEB SCRAPING part was done using SCRAPY modules and tools like Blue prism MACHINE LEARNING for forecasting purpose and modules like SCISKIT NLTK Learning and used regression Models for process improvement By using SQL express 2014 many tables were created and reports were also made Used APIs for ML or data analytics Data Was stored using JSON CSV formats The Data visualization was implemented using libraries like SeaBorn Matlablib tableau Used AWS Elastic Search and S3 storage for data storage Coding was done on Windows OS IPYTHON editor California University March 2016 to August 2016 of management and Sciences March 2016 Aug 2016 DJANGO Web Development The whole project was developed using PYTHON and DJANGO Framework Used a lot of OOPs concepts for a valid output The Frame work DJANGO was used vastly and the apps like HTML The WEB SCRAPING part was done using SCRAPY modules and tools like Blue prism Came across of REST web services in developing process The Developing process was done in IPYTHON editor Testing was done using PYTEST module to check the output Used AWS Elastic Search and S3 storage for data storage Coding was done on Windows OS California University March 2015 to August 2015 Python Programming The multiple projects using were finished using PYTHON 3X version The usage of OOPs concepts was handy for many task Over 600 s programs were written using Sublime text editor Mastered many intermediate level Python concepts like Decorators Generators Modules Worked on both the versions of python 2X and 3X later changed to 3X Coding was done on LINUX OS Python developer WeSecureapp Hyderabad Telangana January 2014 to November 2014 Description The assignment involves in the data collection from Clients and storing them and available for user friendly Responsibilities Developed entire backend modules using Python on Web2py framework Extensively worked in backend development using Python Retrieved data using ELASTICSEARCH and used all other modules of Elastics Developed API modularizing existing python module with the help of pyYAML libraries Used several python libraries like wxPython numpy and matPlotLib Responsible for user validations on client side as well as server side Automated the existing scripts for performance calculations Worked with JSON based REST Web services and AWS Used ML extensively for analyzing and forecasting Built development environment with JIRA StashGit Followed AGILE development methodology to develop the application Placed data into JSON files using Python Participated in the complete SDLC process Testing was done using PYTEST module to check the output Involved in Python OOP code for quality logging monitoring and debugging code optimization Used Django Database APIs to access database objects Environment Python SQL SCIKit learn XML JSON Eclipse Shell Scripting HTTP web services Windows Linux InternshipJAVA Intugine Technologies Pvt Ltd Bengaluru Karnataka June 2013 to December 2013 developing Contributed to servlet based application development Assisted in maintaining and updating existing applications and modules Helped design form validation programs using HTML Provided assistance and support to programming team members as require Internship ING Hyderabad Telangana January 2013 to April 2013 Conducting market research and finding local vendors in the city Getting vendors on board by pitching the benefits of partnering with company Brainstorming and coming up with ideas and suggestions for the benefit of the company Education Master of Science in Computer Information System California University of Management and Sciences January 2015 to March 2017 Skills AWS 1 year JSON 1 year SQL 1 year DJANGO 1 year REST 1 year Additional Information Skills Languages PYHON SQL JAVA C Java script Shell Scriptingbasics Framework KAPOW MSBI Blue Prism HTML DJANGO AWS Tableau Software development skills Agile methodologies Water fall Methods Iterative process Project management Operations and managements Knowledge on Concepts HTTP HTTPs AJAX REST APIS APIs JSON R AZURE SAS Platforms WindowsVistaXP UNIX Linux Hadoop MapReduce ETL Hive SparkBasics",
    "unique_id": "947d2a64-5de6-40fc-ac0c-5fcc74b3da8f"
}