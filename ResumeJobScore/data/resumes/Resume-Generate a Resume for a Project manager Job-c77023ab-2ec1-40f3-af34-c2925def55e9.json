{
    "clean_data": "PythonBig Data Engineer span lPythonspanBig Data Engineer PythonBig Data Engineer Aldie VA Authorized to work in the US for any employer Work Experience PythonBig Data Engineer Next Shift Health LLC Bethlehem PA October 2017 to May 2018 Worked on parsing of multiple log files segregating the data based on certain attributes and uploading the data into DynamoDB table on Amazon Web Services Wrote regular expression to parse files of different formats Managed Hive tables in an Big Data environment while facilitating transfer of data between HDFS to RDBMS and vice versa Processed and stored the files in Avro Parquet ORC and JSON formats as per the requirement for further processing Created Hive Staging tables as a temporary directory for transfer of data between Hive Database and RDBMS Worked with Big Data distributions such as Cloudera and Hortonworks Wrote Python Spark application to join two disparate datasets and utilized Spark APIs to perform transformations and actions on the subset data Used Unit Testing on python code Spark RDDs DataFrames were used for processing the data from Data Lake Data transformed from python collections textfiles and converted to KeyValue pairs for Spark API processing Utilized Apache Solr for tabular and text search on Files and Data stored in Hadoop Distributed File System Used Flume and Kafka for streaming analytics processing of web server logs and store the data collected into HDFS for analysis Application Programmer Analyst Intern Best Docs Live Inc Plano TX December 2015 to May 2017 Worked as a PythonDjango Developer Built a backend API with Django Rest Framework to handle user accounts and registrations Collaborated with Senior Developer to handle complicated issues related with deployment of Django based applications Resolved ongoing problems and documented progress of Python project Developed pages for cross browser and cross platform compatibility Utilized Browser Stack to test for compatibility Wrote Python program to parse and upload csv files into PostgreSQL Database HTTP Request Library was used for Web API call Maintained both Dev and Production Databases in PostgreSQL environment PostgreSQL DB was setup in Amazon Web Services Data was Stored in HIPPA complaint servers Deployed the application in Amazon Web Services Have experience in Amazon EC2 RDS S3 Lambda Elastic Beanstalk and Route 53 AWS Cloudfront was used as CDN to transfer data with low latency and high transfer speeds SQL Developer Mahindra and Mahindra Financial Services Delhi Delhi August 2011 to May 2014 Designed Database using different forms of relationships Documented and maintained database system specifications diagram and connectivity charts Work with application developers to identify business needs and discuss solution options Designed and configured database and backend applications and programs Build and maintain SQL scripts indexes and complex queries for data analysis and extraction Ran SQL Queries to back up the data from Handheld devices and perform maintenance Worked closely and effectively with vendors to replacerepair defective hardware and software Education Master of Science in Computer Information Science THE UNIVERSITY OF TEXAS AT DALLAS Dallas TX May 2017 Bachelor of Engineering in Electronics and Communications ANNA UNIVERSITY May 2011 Skills PYTHON 2 years WEB SERVICES 2 years AWS 1 year DATABASES 1 year DJANGO 1 year Links httpsgithubcomrjshekar90 httpswwwlinkedincominrajashekar90 Additional Information Technical Skills Languages Python Scala Java Restful Web Services Frameworks Spark Hadoop Django Flask ElasticSearch AWS Elastic Map Reduce Databases MySQL PostgreSQL Oracle MongoDB Hive AWS Redshift Tools PyCharm IntelliJ IDEA DataGrip Apache Sqoop NiFi GIT JIRA",
    "entities": [
        "SQL Developer Mahindra and Mahindra Financial Services",
        "Hive Database",
        "httpsgithubcomrjshekar90 httpswwwlinkedincominrajashekar90 Additional Information Technical Skills Languages",
        "Created Hive Staging",
        "Cloudfront",
        "US",
        "Dallas",
        "Build",
        "Delhi",
        "Data Lake Data",
        "Work Experience",
        "IDEA DataGrip Apache Sqoop NiFi GIT",
        "Amazon Web Services Data",
        "Ran SQL Queries",
        "Processed",
        "Application Programmer",
        "Amazon",
        "CDN",
        "KeyValue",
        "SQL",
        "DataFrames",
        "Data",
        "DALLAS",
        "Amazon Web Services",
        "Science in Computer Information Science THE UNIVERSITY OF TEXAS",
        "Data Engineer PythonBig",
        "Hadoop Distributed File System",
        "Handheld",
        "Spark API",
        "Files",
        "PythonBig Data Engineer",
        "Big Data",
        "Wrote Python",
        "TX",
        "Amazon Web Services Wrote",
        "Spark"
    ],
    "experience": "Experience PythonBig Data Engineer Next Shift Health LLC Bethlehem PA October 2017 to May 2018 Worked on parsing of multiple log files segregating the data based on certain attributes and uploading the data into DynamoDB table on Amazon Web Services Wrote regular expression to parse files of different formats Managed Hive tables in an Big Data environment while facilitating transfer of data between HDFS to RDBMS and vice versa Processed and stored the files in Avro Parquet ORC and JSON formats as per the requirement for further processing Created Hive Staging tables as a temporary directory for transfer of data between Hive Database and RDBMS Worked with Big Data distributions such as Cloudera and Hortonworks Wrote Python Spark application to join two disparate datasets and utilized Spark APIs to perform transformations and actions on the subset data Used Unit Testing on python code Spark RDDs DataFrames were used for processing the data from Data Lake Data transformed from python collections textfiles and converted to KeyValue pairs for Spark API processing Utilized Apache Solr for tabular and text search on Files and Data stored in Hadoop Distributed File System Used Flume and Kafka for streaming analytics processing of web server logs and store the data collected into HDFS for analysis Application Programmer Analyst Intern Best Docs Live Inc Plano TX December 2015 to May 2017 Worked as a PythonDjango Developer Built a backend API with Django Rest Framework to handle user accounts and registrations Collaborated with Senior Developer to handle complicated issues related with deployment of Django based applications Resolved ongoing problems and documented progress of Python project Developed pages for cross browser and cross platform compatibility Utilized Browser Stack to test for compatibility Wrote Python program to parse and upload csv files into PostgreSQL Database HTTP Request Library was used for Web API call Maintained both Dev and Production Databases in PostgreSQL environment PostgreSQL DB was setup in Amazon Web Services Data was Stored in HIPPA complaint servers Deployed the application in Amazon Web Services Have experience in Amazon EC2 RDS S3 Lambda Elastic Beanstalk and Route 53 AWS Cloudfront was used as CDN to transfer data with low latency and high transfer speeds SQL Developer Mahindra and Mahindra Financial Services Delhi Delhi August 2011 to May 2014 Designed Database using different forms of relationships Documented and maintained database system specifications diagram and connectivity charts Work with application developers to identify business needs and discuss solution options Designed and configured database and backend applications and programs Build and maintain SQL scripts indexes and complex queries for data analysis and extraction Ran SQL Queries to back up the data from Handheld devices and perform maintenance Worked closely and effectively with vendors to replacerepair defective hardware and software Education Master of Science in Computer Information Science THE UNIVERSITY OF TEXAS AT DALLAS Dallas TX May 2017 Bachelor of Engineering in Electronics and Communications ANNA UNIVERSITY May 2011 Skills PYTHON 2 years WEB SERVICES 2 years AWS 1 year DATABASES 1 year DJANGO 1 year Links httpsgithubcomrjshekar90 httpswwwlinkedincominrajashekar90 Additional Information Technical Skills Languages Python Scala Java Restful Web Services Frameworks Spark Hadoop Django Flask ElasticSearch AWS Elastic Map Reduce Databases MySQL PostgreSQL Oracle MongoDB Hive AWS Redshift Tools PyCharm IntelliJ IDEA DataGrip Apache Sqoop NiFi GIT JIRA",
    "extracted_keywords": [
        "PythonBig",
        "Data",
        "Engineer",
        "span",
        "lPythonspanBig",
        "Data",
        "Engineer",
        "PythonBig",
        "Data",
        "Engineer",
        "Aldie",
        "VA",
        "US",
        "employer",
        "Work",
        "Experience",
        "PythonBig",
        "Data",
        "Engineer",
        "Next",
        "Shift",
        "Health",
        "LLC",
        "Bethlehem",
        "PA",
        "October",
        "May",
        "log",
        "files",
        "data",
        "attributes",
        "data",
        "DynamoDB",
        "table",
        "Amazon",
        "Web",
        "Services",
        "expression",
        "files",
        "formats",
        "Managed",
        "Hive",
        "tables",
        "Big",
        "Data",
        "environment",
        "transfer",
        "data",
        "HDFS",
        "vice",
        "versa",
        "files",
        "Avro",
        "Parquet",
        "ORC",
        "formats",
        "requirement",
        "Hive",
        "Staging",
        "tables",
        "directory",
        "transfer",
        "data",
        "Hive",
        "Database",
        "Data",
        "distributions",
        "Cloudera",
        "Hortonworks",
        "Wrote",
        "Python",
        "Spark",
        "application",
        "datasets",
        "Spark",
        "APIs",
        "transformations",
        "actions",
        "subset",
        "data",
        "Unit",
        "Testing",
        "python",
        "code",
        "Spark",
        "RDDs",
        "DataFrames",
        "data",
        "Data",
        "Lake",
        "Data",
        "python",
        "collections",
        "textfiles",
        "KeyValue",
        "pairs",
        "Spark",
        "API",
        "processing",
        "Apache",
        "Solr",
        "text",
        "search",
        "Files",
        "Data",
        "Hadoop",
        "Distributed",
        "File",
        "System",
        "Flume",
        "Kafka",
        "analytics",
        "processing",
        "web",
        "server",
        "logs",
        "data",
        "HDFS",
        "analysis",
        "Application",
        "Programmer",
        "Analyst",
        "Intern",
        "Best",
        "Docs",
        "Live",
        "Inc",
        "Plano",
        "TX",
        "December",
        "May",
        "PythonDjango",
        "Developer",
        "API",
        "Django",
        "Rest",
        "Framework",
        "user",
        "accounts",
        "registrations",
        "Senior",
        "Developer",
        "issues",
        "deployment",
        "Django",
        "applications",
        "problems",
        "progress",
        "Python",
        "project",
        "pages",
        "cross",
        "browser",
        "platform",
        "compatibility",
        "Browser",
        "Stack",
        "compatibility",
        "Wrote",
        "Python",
        "program",
        "files",
        "PostgreSQL",
        "Database",
        "HTTP",
        "Request",
        "Library",
        "Web",
        "API",
        "call",
        "Dev",
        "Production",
        "Databases",
        "PostgreSQL",
        "environment",
        "PostgreSQL",
        "DB",
        "setup",
        "Amazon",
        "Web",
        "Services",
        "Data",
        "HIPPA",
        "complaint",
        "servers",
        "application",
        "Amazon",
        "Web",
        "Services",
        "experience",
        "Amazon",
        "EC2",
        "RDS",
        "S3",
        "Lambda",
        "Elastic",
        "Beanstalk",
        "Route",
        "AWS",
        "Cloudfront",
        "CDN",
        "data",
        "latency",
        "transfer",
        "SQL",
        "Developer",
        "Mahindra",
        "Mahindra",
        "Financial",
        "Services",
        "Delhi",
        "Delhi",
        "August",
        "May",
        "Designed",
        "Database",
        "forms",
        "relationships",
        "database",
        "system",
        "specifications",
        "diagram",
        "connectivity",
        "charts",
        "Work",
        "application",
        "developers",
        "business",
        "needs",
        "solution",
        "options",
        "database",
        "applications",
        "programs",
        "SQL",
        "scripts",
        "indexes",
        "queries",
        "data",
        "analysis",
        "extraction",
        "Ran",
        "SQL",
        "Queries",
        "data",
        "Handheld",
        "devices",
        "maintenance",
        "vendors",
        "hardware",
        "software",
        "Education",
        "Master",
        "Science",
        "Computer",
        "Information",
        "Science",
        "UNIVERSITY",
        "TEXAS",
        "DALLAS",
        "Dallas",
        "TX",
        "May",
        "Bachelor",
        "Engineering",
        "Electronics",
        "Communications",
        "ANNA",
        "UNIVERSITY",
        "May",
        "Skills",
        "PYTHON",
        "years",
        "WEB",
        "SERVICES",
        "years",
        "AWS",
        "year",
        "DATABASES",
        "year",
        "DJANGO",
        "year",
        "Links",
        "Additional",
        "Information",
        "Technical",
        "Skills",
        "Languages",
        "Python",
        "Scala",
        "Java",
        "Restful",
        "Web",
        "Services",
        "Frameworks",
        "Spark",
        "Hadoop",
        "Django",
        "Flask",
        "ElasticSearch",
        "AWS",
        "Elastic",
        "Map",
        "MySQL",
        "PostgreSQL",
        "Oracle",
        "MongoDB",
        "Hive",
        "AWS",
        "Redshift",
        "Tools",
        "PyCharm",
        "IntelliJ",
        "IDEA",
        "DataGrip",
        "Apache",
        "Sqoop",
        "NiFi",
        "GIT",
        "JIRA"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T20:41:58.447102",
    "resume_data": "PythonBig Data Engineer span lPythonspanBig Data Engineer PythonBig Data Engineer Aldie VA Authorized to work in the US for any employer Work Experience PythonBig Data Engineer Next Shift Health LLC Bethlehem PA October 2017 to May 2018 Worked on parsing of multiple log files segregating the data based on certain attributes and uploading the data into DynamoDB table on Amazon Web Services Wrote regular expression to parse files of different formats Managed Hive tables in an Big Data environment while facilitating transfer of data between HDFS to RDBMS and vice versa Processed and stored the files in Avro Parquet ORC and JSON formats as per the requirement for further processing Created Hive Staging tables as a temporary directory for transfer of data between Hive Database and RDBMS Worked with Big Data distributions such as Cloudera and Hortonworks Wrote Python Spark application to join two disparate datasets and utilized Spark APIs to perform transformations and actions on the subset data Used Unit Testing on python code Spark RDDs DataFrames were used for processing the data from Data Lake Data transformed from python collections textfiles and converted to KeyValue pairs for Spark API processing Utilized Apache Solr for tabular and text search on Files and Data stored in Hadoop Distributed File System Used Flume and Kafka for streaming analytics processing of web server logs and store the data collected into HDFS for analysis Application Programmer Analyst Intern Best Docs Live Inc Plano TX December 2015 to May 2017 Worked as a PythonDjango Developer Built a backend API with Django Rest Framework to handle user accounts and registrations Collaborated with Senior Developer to handle complicated issues related with deployment of Django based applications Resolved ongoing problems and documented progress of Python project Developed pages for cross browser and cross platform compatibility Utilized Browser Stack to test for compatibility Wrote Python program to parse and upload csv files into PostgreSQL Database HTTP Request Library was used for Web API call Maintained both Dev and Production Databases in PostgreSQL environment PostgreSQL DB was setup in Amazon Web Services Data was Stored in HIPPA complaint servers Deployed the application in Amazon Web Services Have experience in Amazon EC2 RDS S3 Lambda Elastic Beanstalk and Route 53 AWS Cloudfront was used as CDN to transfer data with low latency and high transfer speeds SQL Developer Mahindra and Mahindra Financial Services Delhi Delhi August 2011 to May 2014 Designed Database using different forms of relationships Documented and maintained database system specifications diagram and connectivity charts Work with application developers to identify business needs and discuss solution options Designed and configured database and backend applications and programs Build and maintain SQL scripts indexes and complex queries for data analysis and extraction Ran SQL Queries to back up the data from Handheld devices and perform maintenance Worked closely and effectively with vendors to replacerepair defective hardware and software Education Master of Science in Computer Information Science THE UNIVERSITY OF TEXAS AT DALLAS Dallas TX May 2017 Bachelor of Engineering in Electronics and Communications ANNA UNIVERSITY May 2011 Skills PYTHON 2 years WEB SERVICES 2 years AWS 1 year DATABASES 1 year DJANGO 1 year Links httpsgithubcomrjshekar90 httpswwwlinkedincominrajashekar90 Additional Information Technical Skills Languages Python Scala Java Restful Web Services Frameworks Spark Hadoop Django Flask ElasticSearch AWS Elastic Map Reduce Databases MySQL PostgreSQL Oracle MongoDB Hive AWS Redshift Tools PyCharm IntelliJ IDEA DataGrip Apache Sqoop NiFi GIT JIRA",
    "unique_id": "c77023ab-2ec1-40f3-af34-c2925def55e9"
}