{
    "clean_data": "Data ScientistData Science Manager Data ScientistData Science Manager Data ScientistData Science Manager American ExpressHome Depot Atlanta GA Professional qualified Data ScientistSoftware Engineer with over 9 years of experience in Data Science and Analytics including Artificial IntelligenceDeep LearningMachine Learning Big Data Data Mining and Statistical Analysis and demonstrated Leadership Involved in the entire data science project life cycle and actively involved in all the phases including data extraction data cleaning statistical modeling and data visualization with large data sets of structured and unstructured data created ER diagrams and schema Experienced with machine learning algorithm such as logistic regression random forest XGboost KNN SVM neural network linear regression lasso regression and kmeans Implemented Bagging and Boosting to enhance the model performance Strong skills in statistical methodologies such as AB test experiment design hypothesis test ANOVA Extensively worked on Python 3527 Numpy Pandas Matplotlib NLTK and Scikitlearn and Scala Experience in implementing data analysis with various analytic tools such as Anaconda 40 Jupiter Notebook 4X R 30 ggplot2 Caret dplyr and Excel Solid ability to write and optimize diverse SQL queries working knowledge of RDBMS like SQL Server 2008 NoSql databases like AWS DynamoDB MongoDB Developed API libraries and coded business logic using C XML and designed web pages using NET framework C Python Django HTML AJAX Strong experience in Big Data technologies like Spark Sparksql pySpark Hadoop HDFS Hive Experience in visualization tools like Tableau 9X 10X for creating dashboards Excellent understanding Agile and Scrum development methodology Used the version control tools like Git 2X and build tools like Apache MavenAnt Passionate about gleaning insightful information from massive data assets and developing a culture of sound datadriven decision making Worked on containerization using Docker and managed the containers in Kubernetes Ability to maintain a fun casual professional and productive team atmosphere Experienced the full software life cycle in SDLC Agile Devops and Scrum methodologies including creating requirements test plans Worked on message queues such as Amazon SNS SQS Google Pub Sub and integrated with other services like AWS Lambda Storage using Bash scripts Skilled in Advanced Regression Modeling Correlation Multivariate Analysis Model Building Business Intelligence tools and application of Statistical Concepts Proficient in Predictive Modeling Data Mining Methods Factor Analysis ANOVA Hypothetical testing normal distribution and other advanced statistical and econometric techniques Developed predictive models using Decision Tree Random Forest Nave Bayes Logistic Regression Social Network Analysis Cluster Analysis and Neural Networks Experienced in Machine Learning and Statistical Analysis with Python ScikitLearn Experienced in Python to manipulate data for data loading and extraction and worked with python libraries like Matplotlib Numpy Scipy and Pandas for data analysis Worked with complex applications such as R SAS Matlab and SPSS to develop neural network cluster analysis Strong SQL and T SQL programming skills with experience in working with functions packages and triggers Expertise in transforming business requirements into analytical models designing algorithms building models developing data mining and reporting solutions that scales across massive volume of structured and unstructured data Skilled in performing data parsing data ingestion data manipulationdata architecture data modelling and data preparation with methods including describe data contents compute descriptive statistics of data regex split and combine Remap merge subset reindex melt and reshape Experienced in Visual Basic for Applications and VB programming languages C NET framework to work with developing applications Experienced in Big Data with Hadoop HDFS MapReduce and Spark Proficient in Tableau and RShiny data visualization tools to analyze and obtain insights into large datasets create visually powerful and actionable interactive reports and dashboards Automated recurring reports using SQL and Python and visualized them on BI platform like Tableau Worked in development environment like Git and VM Excellent communication skills Successfully working in fastpaced multitasking environment both independently and in collaborative team a selfmotivated enthusiastic learner Authorized to work in the US for any employer Work Experience Data ScientistData Science Manager American ExpressHome Depot July 2017 to Present Description American Express delivers predictive analytics as a service and offers hosted cloudbased systems for specific business problems eg predicting the behavior of individual consumers stopping revenue leakage in hospitals warning of threats to corporate security or brand health etc Home Depot is an American home improvement supplies retailing company that sells tools construction products and services Home Depot is the largest home improvement retailer in the United States ahead of rival Lowes Responsibilities Demonstrated leadership skills by successfully managing mentoring and delivering 3 data science projects at Srishti Biz Evaluated models using Cross Validation Log loss function ROC curves and used AUC for feature selection and elastic technologies like ElasticSearch Kibana etc Addressed overfitting by implementing of the algorithm regularization methods like L2 and L1 Implemented statistical modeling with XGBoost machine learning software package using Python to determine the predicted probabilities of each model Created master data for modelling by combining various tables and derived fields from client data and students LORs essays and various performance metrics Hands on Experience on Spark MLLib using both Python and Scala Worked on containerization of different modules of the project like Python APIs Web framework written in Javascript using Docker and managed it using Kubernetes Formulated a basis for variable selection and GridSearch KFold for optimal hyperparameters Utilized Boosting algorithms to build a model for predictive analysis of students behaviour who took USMLE exam apply for residency Used numpy scipy pandas nltkNatural Language Processing Toolkitmatplotlib to build the model Formulated several graphs to show the performance of the students by demographics and their mean score in different USMLE exams Application of various Artificial IntelligenceAImachine learning algorithms and statistical modeling like decision treestext analytics natural language processingNLP supervised and unsupervised regression models Used Principal Component Analysis in feature engineering to analyze high dimensional data Performed Data Cleaning features scaling features engineering using pandas and numpy packages in python and build models using deep learning frameworks Scheduled jobs to run SQL statements to transfer data from Google Bigquery to Mysql using Airflow Created deep learning models using Tensorflow and keras by combining all tests as a single normalized score and predict residency attainment of students Used XGB classifier if the feature is a categorical variable and XGB regressor for continuous variables and combined it using FeatureUnion and FunctionTransfomer methods of Natural Language Processing Used OnevsRest Classifier to fit each classifier against all other classifiers and used it on multiclass classification problems Implemented application of various machine learning algorithms and statistical modeling like Decision Tree Text Analytics Sentiment Analysis Naive Bayes Logistic Regression and Linear Regression using Python to determine the accuracy rate of each model Created and designed reports that will use gathered metrics to infer and draw logical conclusions of past and future behavior Generated various models by using different machine learning and deep learning frameworks and tuned the best performance model using Signal Hub Created data layers as signals to Signal Hub to predict new unseen data with performance not less than the static model build using deep learning framework Environment Python 2x3x Hive AWS Linux Tableau Desktop Microsoft Excel NLP Deep learning frameworks such as TensorFLow Keras Boosting algorithms etc Data Scientist January 2016 to November 2016 Description The goal of this project is short text summarization on the comment stream of a message from social network services The users of the social sites always desire to get a brief understanding of a comment stream without reading the whole comment list Responsibilities Performed Data Profiling to learn about behavior with various features such as traffic pattern location time Date and Time etc Application of various Artificial IntelligenceAImachine learning algorithms and statistical modeling like decision treestext analytics natural language processingNLP supervised and unsupervised regression models social network analysis neural networks deep learning SVM clustering to identify Volume using scikitlearn package in python Matlab Categorised comments into positive and negative clusters from different social networking sites using Sentiment Analysis and Text Analytics Analyze traffic patterns by calculating autocorrelation with different time lags Sets of comment streams are processed by NLP module in stages like stemming punctuation removal etc to categorize into vector model and input into Incremental Short Text Summarization ISTS algorithm Clusters of comments with same pattern will be grouped together and presented visually to the end user Ensured that the model has low False Positive Rate and Text classification and sentiment analysis for unstructured and semistructured data Addressed overfitting by implementing of the algorithm regularization methods like L2 and L1 Used Principal Component Analysis in feature engineering to analyze high dimensional data Performed Data Cleaning features scaling features engineering using pandas and numpy packages in python and build models using SAP Predictive Analytics Created Data Quality Scripts using SQL and Hive to validate successful data load and quality of the data Created various types of data visualizations using Python and Tableau Data Scientist First DataAccenture July 2011 to July 2015 Description First Data Corporation is a global payment processing company headquartered in Atlanta Georgia United States The companys portfolio includes merchant transaction processing services credit debit privatelabel gift payroll and other prepaid card offerings fraud protection and authentication solutions Responsibilities Provided Configuration Management and Build support for more than 5 different applications built and deployed to the production and lower environments Explored and Extracted data from source XML in HDFS used ETL for preparing data for exploratory analysis using data munging Responsible for different Data mapping activities from Source systems to Teradata Text mining and building models using topic analysis sentiment analysis for both semistructured and unstructured data Handled importing data from various data sources performed transformations using Hive Map Reduce and loaded data into HDFS Write adhoc queries using T SQL for data analysis on Microsoft SQL Server Used R and python for Exploratory Data Analysis AB testing HQL VQL Data Lake AWS Redshift oozie pySpark Anova test and Hypothesis test to compare and identify the effectiveness of Creative Campaigns Computing AB testing frameworks clickstream Created clusters to Control and test groups and conducted group campaigns using Text Analytics Created positive and negative clusters from merchants transaction using Sentiment Analysis to test the authenticity of transactions and resolve any chargebacks Analyzed and calculated the lifetime cost of everyone in the welfare system using 20 years of historical data Created and developed classes and web page elements using C and AJAX JSP was used for validating client side responses and connected C to database to retrieve SQL data Developed LINUXShell scripts by using NZSQLNZLOAD utilities to load data from flat files to Netezza database Developed triggers stored procedures functions and packages using cursors and ref cursor concepts associated with the project using PlSQL Created various types of data visualizations using R C python and TableauSpotfire also connected Pipeline Pilot with Spotfire to create more interactive business driven layouts Used Python R SQL to create Statistical algorithms involving Multivariate Regression Linear Regression Logistic Regression PCA Random forest models Decision trees Support Vector Machine for estimating the risks of welfare dependency Identified and targeted welfare highrisk groups with Machine learningdeep learning algorithms Conducted campaigns and run realtime trials to determine what works fast and track the impact of different initiatives Developed Tableau visualizations and dashboards using Tableau Desktop Used Graphical EntityRelationship Diagramming to create new database design via easy to use graphical interface Created multiple custom SQL queries in Teradata SQL Workbench to prepare the right data sets for Tableau dashboards Perform analyses such as regression analysis logistic regression discriminant analysis cluster analysis using SAS programming Environment R 3x HDFS C Hadoop 23 Pig Hive Linux RStudio Tableau 10 SQL Server MS Excel PySpark Python Developer Cenvien Technologies Hyderabad Telangana January 2011 to June 2011 Description Cenvien technologies gather the requirements by listening and understanding to the clients business requirement to deliver quality products It is highly qualified and strongly dedicated developing team that produces unique solutions Responsibilities Developed entire frontend and backend modules using Python on Django Web Framework Implemented the presentation layer with HTML CSS and JavaScript Involved in writing stored procedures using Oracle Optimized the database queries to improve the performance Designed and developed data management system using Oracle Environment MySQL ORACLE HTML5 CSS3 JavaScript Shell Linux Windows Django Python Data Analyst Pennar Industries Limited Hyderabad Telangana March 2009 to December 2010 Description As a backend developer of web applications and data science infrastructure The main area of focus is to come up with comprehensive solutions that need massive capacity and throughput Responsibilities Effectively communicated with the stakeholders to gather requirements for different projects Used MySQL db package and PythonMySQL connector for writing and executing several MYSQL database queries from Python Implemented ClientServer applications using C JSP and SQL Created functions triggers views and stored procedures using My SQL Worked closely with backend developer to find ways to push the limits of existing Web technology Involved in the code review meetings Environment Python MySQL C Education Master of Computer Science in Computer Science Lamar University Beaumont TX 2015 to 2017 Skills Business Intelligence SQL access testing Excel Data Science 9 years Python 9 years Links httpswwwlinkedincominpavithrakumar22b753173 CertificationsLicenses Acadgild Masters in Data Science February 2019 to Present Assessments Data Analysis Proficient August 2019 Measures a candidates skill in interpreting and producing graphs identifying trends and drawing justifiable conclusions from data Full results httpsshareindeedassessmentscomshare_assignment5p0ejlgzweim6ro Problem Solving Expert August 2019 Measures a candidates ability to analyze relevant information when solving problems Full results httpsshareindeedassessmentscomshare_assignmentdso7visv0vjm2cfc Indeed Assessments provides skills tests that are not indicative of a license or certification or continued development in any professional field",
    "entities": [
        "Exploratory Data Analysis AB",
        "AUC",
        "BI",
        "HDFS",
        "Pipeline Pilot",
        "Georgia",
        "Mysql using Airflow Created",
        "Responsibilities",
        "Teradata SQL Workbench",
        "XGB",
        "Control",
        "Responsibilities Performed Data Profiling",
        "XGBoost",
        "Apache MavenAnt Passionate",
        "ER",
        "XML",
        "Oracle Optimized",
        "Atlanta",
        "Data Science and Analytics",
        "kmeans Implemented Bagging and Boosting",
        "SPSS",
        "Principal Component Analysis",
        "Formulated",
        "Incremental Short Text Summarization ISTS",
        "Amazon",
        "Beaumont",
        "Python R SQL",
        "Python",
        "Scikitlearn",
        "SQL Server",
        "Performed Data Cleaning",
        "Responsibilities Provided Configuration Management",
        "Conducted",
        "Bash",
        "Present Assessments",
        "VM Excellent",
        "Tableau Worked",
        "SQL Created",
        "Kubernetes Ability",
        "Data ScientistSoftware Engineer",
        "JavaScript Involved",
        "Tableau Desktop Used Graphical EntityRelationship Diagramming",
        "JSP",
        "Data Scientist",
        "FunctionTransfomer",
        "Spark Sparksql pySpark Hadoop HDFS Hive Experience",
        "Teradata Text",
        "Docker",
        "ROC",
        "NZSQLNZLOAD",
        "Statistical Concepts Proficient",
        "RShiny",
        "NET",
        "linear",
        "Kubernetes Formulated",
        "HTML CSS",
        "Python Implemented ClientServer",
        "Text Analytics Created",
        "US",
        "Created",
        "Hypothesis",
        "Analyzed",
        "Home Depot",
        "Oracle Environment MySQL ORACLE HTML5",
        "Developed Tableau",
        "Signal Hub",
        "SDLC Agile Devops",
        "SAS",
        "Pennar Industries Limited Hyderabad",
        "Spark MLLib",
        "Tableau Data",
        "SQL",
        "NLP",
        "ANOVA",
        "the United States",
        "Anaconda",
        "Big Data",
        "Hive",
        "Pandas",
        "Anova",
        "ETL",
        "FeatureUnion",
        "Multivariate Regression Linear Regression Logistic Regression PCA Random",
        "Build",
        "Utilized Boosting",
        "Skills Business Intelligence SQL",
        "Language Processing Toolkitmatplotlib",
        "Google Bigquery",
        "Python ScikitLearn Experienced",
        "Microsoft",
        "Present Description American Express",
        "Description First Data Corporation",
        "Signal Hub Created",
        "SAP Predictive Analytics Created Data Quality Scripts",
        "Spark Proficient",
        "ref",
        "Tableau",
        "Machine Learning",
        "Predictive Modeling Data Mining Methods Factor Analysis ANOVA Hypothetical",
        "Decision Tree Random Forest Nave Bayes Logistic Regression Social Network Analysis Cluster Analysis",
        "Creative Campaigns Computing AB",
        "SVM",
        "Advanced Regression Modeling Correlation Multivariate Analysis Model Building Business Intelligence",
        "Cross Validation Log"
    ],
    "experience": "Experience in implementing data analysis with various analytic tools such as Anaconda 40 Jupiter Notebook 4X R 30 ggplot2 Caret dplyr and Excel Solid ability to write and optimize diverse SQL queries working knowledge of RDBMS like SQL Server 2008 NoSql databases like AWS DynamoDB MongoDB Developed API libraries and coded business logic using C XML and designed web pages using NET framework C Python Django HTML AJAX Strong experience in Big Data technologies like Spark Sparksql pySpark Hadoop HDFS Hive Experience in visualization tools like Tableau 9X 10X for creating dashboards Excellent understanding Agile and Scrum development methodology Used the version control tools like Git 2X and build tools like Apache MavenAnt Passionate about gleaning insightful information from massive data assets and developing a culture of sound datadriven decision making Worked on containerization using Docker and managed the containers in Kubernetes Ability to maintain a fun casual professional and productive team atmosphere Experienced the full software life cycle in SDLC Agile Devops and Scrum methodologies including creating requirements test plans Worked on message queues such as Amazon SNS SQS Google Pub Sub and integrated with other services like AWS Lambda Storage using Bash scripts Skilled in Advanced Regression Modeling Correlation Multivariate Analysis Model Building Business Intelligence tools and application of Statistical Concepts Proficient in Predictive Modeling Data Mining Methods Factor Analysis ANOVA Hypothetical testing normal distribution and other advanced statistical and econometric techniques Developed predictive models using Decision Tree Random Forest Nave Bayes Logistic Regression Social Network Analysis Cluster Analysis and Neural Networks Experienced in Machine Learning and Statistical Analysis with Python ScikitLearn Experienced in Python to manipulate data for data loading and extraction and worked with python libraries like Matplotlib Numpy Scipy and Pandas for data analysis Worked with complex applications such as R SAS Matlab and SPSS to develop neural network cluster analysis Strong SQL and T SQL programming skills with experience in working with functions packages and triggers Expertise in transforming business requirements into analytical models designing algorithms building models developing data mining and reporting solutions that scales across massive volume of structured and unstructured data Skilled in performing data parsing data ingestion data manipulationdata architecture data modelling and data preparation with methods including describe data contents compute descriptive statistics of data regex split and combine Remap merge subset reindex melt and reshape Experienced in Visual Basic for Applications and VB programming languages C NET framework to work with developing applications Experienced in Big Data with Hadoop HDFS MapReduce and Spark Proficient in Tableau and RShiny data visualization tools to analyze and obtain insights into large datasets create visually powerful and actionable interactive reports and dashboards Automated recurring reports using SQL and Python and visualized them on BI platform like Tableau Worked in development environment like Git and VM Excellent communication skills Successfully working in fastpaced multitasking environment both independently and in collaborative team a selfmotivated enthusiastic learner Authorized to work in the US for any employer Work Experience Data ScientistData Science Manager American ExpressHome Depot July 2017 to Present Description American Express delivers predictive analytics as a service and offers hosted cloudbased systems for specific business problems eg predicting the behavior of individual consumers stopping revenue leakage in hospitals warning of threats to corporate security or brand health etc Home Depot is an American home improvement supplies retailing company that sells tools construction products and services Home Depot is the largest home improvement retailer in the United States ahead of rival Lowes Responsibilities Demonstrated leadership skills by successfully managing mentoring and delivering 3 data science projects at Srishti Biz Evaluated models using Cross Validation Log loss function ROC curves and used AUC for feature selection and elastic technologies like ElasticSearch Kibana etc Addressed overfitting by implementing of the algorithm regularization methods like L2 and L1 Implemented statistical modeling with XGBoost machine learning software package using Python to determine the predicted probabilities of each model Created master data for modelling by combining various tables and derived fields from client data and students LORs essays and various performance metrics Hands on Experience on Spark MLLib using both Python and Scala Worked on containerization of different modules of the project like Python APIs Web framework written in Javascript using Docker and managed it using Kubernetes Formulated a basis for variable selection and GridSearch KFold for optimal hyperparameters Utilized Boosting algorithms to build a model for predictive analysis of students behaviour who took USMLE exam apply for residency Used numpy scipy pandas nltkNatural Language Processing Toolkitmatplotlib to build the model Formulated several graphs to show the performance of the students by demographics and their mean score in different USMLE exams Application of various Artificial IntelligenceAImachine learning algorithms and statistical modeling like decision treestext analytics natural language processingNLP supervised and unsupervised regression models Used Principal Component Analysis in feature engineering to analyze high dimensional data Performed Data Cleaning features scaling features engineering using pandas and numpy packages in python and build models using deep learning frameworks Scheduled jobs to run SQL statements to transfer data from Google Bigquery to Mysql using Airflow Created deep learning models using Tensorflow and keras by combining all tests as a single normalized score and predict residency attainment of students Used XGB classifier if the feature is a categorical variable and XGB regressor for continuous variables and combined it using FeatureUnion and FunctionTransfomer methods of Natural Language Processing Used OnevsRest Classifier to fit each classifier against all other classifiers and used it on multiclass classification problems Implemented application of various machine learning algorithms and statistical modeling like Decision Tree Text Analytics Sentiment Analysis Naive Bayes Logistic Regression and Linear Regression using Python to determine the accuracy rate of each model Created and designed reports that will use gathered metrics to infer and draw logical conclusions of past and future behavior Generated various models by using different machine learning and deep learning frameworks and tuned the best performance model using Signal Hub Created data layers as signals to Signal Hub to predict new unseen data with performance not less than the static model build using deep learning framework Environment Python 2x3x Hive AWS Linux Tableau Desktop Microsoft Excel NLP Deep learning frameworks such as TensorFLow Keras Boosting algorithms etc Data Scientist January 2016 to November 2016 Description The goal of this project is short text summarization on the comment stream of a message from social network services The users of the social sites always desire to get a brief understanding of a comment stream without reading the whole comment list Responsibilities Performed Data Profiling to learn about behavior with various features such as traffic pattern location time Date and Time etc Application of various Artificial IntelligenceAImachine learning algorithms and statistical modeling like decision treestext analytics natural language processingNLP supervised and unsupervised regression models social network analysis neural networks deep learning SVM clustering to identify Volume using scikitlearn package in python Matlab Categorised comments into positive and negative clusters from different social networking sites using Sentiment Analysis and Text Analytics Analyze traffic patterns by calculating autocorrelation with different time lags Sets of comment streams are processed by NLP module in stages like stemming punctuation removal etc to categorize into vector model and input into Incremental Short Text Summarization ISTS algorithm Clusters of comments with same pattern will be grouped together and presented visually to the end user Ensured that the model has low False Positive Rate and Text classification and sentiment analysis for unstructured and semistructured data Addressed overfitting by implementing of the algorithm regularization methods like L2 and L1 Used Principal Component Analysis in feature engineering to analyze high dimensional data Performed Data Cleaning features scaling features engineering using pandas and numpy packages in python and build models using SAP Predictive Analytics Created Data Quality Scripts using SQL and Hive to validate successful data load and quality of the data Created various types of data visualizations using Python and Tableau Data Scientist First DataAccenture July 2011 to July 2015 Description First Data Corporation is a global payment processing company headquartered in Atlanta Georgia United States The companys portfolio includes merchant transaction processing services credit debit privatelabel gift payroll and other prepaid card offerings fraud protection and authentication solutions Responsibilities Provided Configuration Management and Build support for more than 5 different applications built and deployed to the production and lower environments Explored and Extracted data from source XML in HDFS used ETL for preparing data for exploratory analysis using data munging Responsible for different Data mapping activities from Source systems to Teradata Text mining and building models using topic analysis sentiment analysis for both semistructured and unstructured data Handled importing data from various data sources performed transformations using Hive Map Reduce and loaded data into HDFS Write adhoc queries using T SQL for data analysis on Microsoft SQL Server Used R and python for Exploratory Data Analysis AB testing HQL VQL Data Lake AWS Redshift oozie pySpark Anova test and Hypothesis test to compare and identify the effectiveness of Creative Campaigns Computing AB testing frameworks clickstream Created clusters to Control and test groups and conducted group campaigns using Text Analytics Created positive and negative clusters from merchants transaction using Sentiment Analysis to test the authenticity of transactions and resolve any chargebacks Analyzed and calculated the lifetime cost of everyone in the welfare system using 20 years of historical data Created and developed classes and web page elements using C and AJAX JSP was used for validating client side responses and connected C to database to retrieve SQL data Developed LINUXShell scripts by using NZSQLNZLOAD utilities to load data from flat files to Netezza database Developed triggers stored procedures functions and packages using cursors and ref cursor concepts associated with the project using PlSQL Created various types of data visualizations using R C python and TableauSpotfire also connected Pipeline Pilot with Spotfire to create more interactive business driven layouts Used Python R SQL to create Statistical algorithms involving Multivariate Regression Linear Regression Logistic Regression PCA Random forest models Decision trees Support Vector Machine for estimating the risks of welfare dependency Identified and targeted welfare highrisk groups with Machine learningdeep learning algorithms Conducted campaigns and run realtime trials to determine what works fast and track the impact of different initiatives Developed Tableau visualizations and dashboards using Tableau Desktop Used Graphical EntityRelationship Diagramming to create new database design via easy to use graphical interface Created multiple custom SQL queries in Teradata SQL Workbench to prepare the right data sets for Tableau dashboards Perform analyses such as regression analysis logistic regression discriminant analysis cluster analysis using SAS programming Environment R 3x HDFS C Hadoop 23 Pig Hive Linux RStudio Tableau 10 SQL Server MS Excel PySpark Python Developer Cenvien Technologies Hyderabad Telangana January 2011 to June 2011 Description Cenvien technologies gather the requirements by listening and understanding to the clients business requirement to deliver quality products It is highly qualified and strongly dedicated developing team that produces unique solutions Responsibilities Developed entire frontend and backend modules using Python on Django Web Framework Implemented the presentation layer with HTML CSS and JavaScript Involved in writing stored procedures using Oracle Optimized the database queries to improve the performance Designed and developed data management system using Oracle Environment MySQL ORACLE HTML5 CSS3 JavaScript Shell Linux Windows Django Python Data Analyst Pennar Industries Limited Hyderabad Telangana March 2009 to December 2010 Description As a backend developer of web applications and data science infrastructure The main area of focus is to come up with comprehensive solutions that need massive capacity and throughput Responsibilities Effectively communicated with the stakeholders to gather requirements for different projects Used MySQL db package and PythonMySQL connector for writing and executing several MYSQL database queries from Python Implemented ClientServer applications using C JSP and SQL Created functions triggers views and stored procedures using My SQL Worked closely with backend developer to find ways to push the limits of existing Web technology Involved in the code review meetings Environment Python MySQL C Education Master of Computer Science in Computer Science Lamar University Beaumont TX 2015 to 2017 Skills Business Intelligence SQL access testing Excel Data Science 9 years Python 9 years Links httpswwwlinkedincominpavithrakumar22b753173 CertificationsLicenses Acadgild Masters in Data Science February 2019 to Present Assessments Data Analysis Proficient August 2019 Measures a candidates skill in interpreting and producing graphs identifying trends and drawing justifiable conclusions from data Full results httpsshareindeedassessmentscomshare_assignment5p0ejlgzweim6ro Problem Solving Expert August 2019 Measures a candidates ability to analyze relevant information when solving problems Full results httpsshareindeedassessmentscomshare_assignmentdso7visv0vjm2cfc Indeed Assessments provides skills tests that are not indicative of a license or certification or continued development in any professional field",
    "extracted_keywords": [
        "Data",
        "ScientistData",
        "Science",
        "Manager",
        "Data",
        "ScientistData",
        "Science",
        "Manager",
        "Data",
        "ScientistData",
        "Science",
        "Manager",
        "American",
        "ExpressHome",
        "Depot",
        "Atlanta",
        "GA",
        "Professional",
        "Data",
        "ScientistSoftware",
        "Engineer",
        "years",
        "experience",
        "Data",
        "Science",
        "Analytics",
        "Artificial",
        "IntelligenceDeep",
        "LearningMachine",
        "Learning",
        "Big",
        "Data",
        "Data",
        "Mining",
        "Statistical",
        "Analysis",
        "Leadership",
        "data",
        "science",
        "project",
        "life",
        "cycle",
        "phases",
        "data",
        "extraction",
        "data",
        "modeling",
        "data",
        "visualization",
        "data",
        "sets",
        "data",
        "ER",
        "diagrams",
        "schema",
        "machine",
        "learning",
        "algorithm",
        "regression",
        "forest",
        "XGboost",
        "KNN",
        "SVM",
        "network",
        "linear",
        "regression",
        "lasso",
        "regression",
        "kmeans",
        "Bagging",
        "model",
        "performance",
        "skills",
        "methodologies",
        "AB",
        "test",
        "experiment",
        "design",
        "hypothesis",
        "test",
        "ANOVA",
        "Python",
        "Numpy",
        "Pandas",
        "Matplotlib",
        "NLTK",
        "Scikitlearn",
        "Scala",
        "Experience",
        "data",
        "analysis",
        "tools",
        "Anaconda",
        "Jupiter",
        "Notebook",
        "4X",
        "ggplot2",
        "Caret",
        "dplyr",
        "Excel",
        "ability",
        "SQL",
        "knowledge",
        "RDBMS",
        "SQL",
        "Server",
        "NoSql",
        "AWS",
        "DynamoDB",
        "API",
        "libraries",
        "business",
        "logic",
        "C",
        "XML",
        "web",
        "pages",
        "NET",
        "framework",
        "C",
        "Python",
        "Django",
        "HTML",
        "experience",
        "Big",
        "Data",
        "technologies",
        "Spark",
        "Sparksql",
        "pySpark",
        "Hadoop",
        "HDFS",
        "Hive",
        "Experience",
        "visualization",
        "tools",
        "Tableau",
        "9X",
        "dashboards",
        "understanding",
        "Agile",
        "Scrum",
        "development",
        "methodology",
        "version",
        "control",
        "tools",
        "Git",
        "2X",
        "tools",
        "Apache",
        "MavenAnt",
        "Passionate",
        "information",
        "data",
        "assets",
        "culture",
        "decision",
        "containerization",
        "Docker",
        "containers",
        "Kubernetes",
        "Ability",
        "team",
        "atmosphere",
        "software",
        "life",
        "cycle",
        "SDLC",
        "Agile",
        "Devops",
        "Scrum",
        "methodologies",
        "requirements",
        "test",
        "plans",
        "message",
        "queues",
        "Amazon",
        "SNS",
        "SQS",
        "Google",
        "Pub",
        "Sub",
        "services",
        "AWS",
        "Lambda",
        "Storage",
        "Bash",
        "scripts",
        "Regression",
        "Modeling",
        "Correlation",
        "Multivariate",
        "Analysis",
        "Model",
        "Building",
        "Business",
        "Intelligence",
        "tools",
        "application",
        "Statistical",
        "Concepts",
        "Proficient",
        "Predictive",
        "Modeling",
        "Data",
        "Mining",
        "Methods",
        "Factor",
        "Analysis",
        "ANOVA",
        "Hypothetical",
        "distribution",
        "techniques",
        "models",
        "Decision",
        "Tree",
        "Random",
        "Forest",
        "Nave",
        "Bayes",
        "Logistic",
        "Regression",
        "Social",
        "Network",
        "Analysis",
        "Cluster",
        "Analysis",
        "Neural",
        "Networks",
        "Machine",
        "Learning",
        "Statistical",
        "Analysis",
        "Python",
        "ScikitLearn",
        "Python",
        "data",
        "data",
        "loading",
        "extraction",
        "python",
        "libraries",
        "Matplotlib",
        "Numpy",
        "Scipy",
        "Pandas",
        "data",
        "analysis",
        "applications",
        "R",
        "SAS",
        "Matlab",
        "SPSS",
        "network",
        "cluster",
        "analysis",
        "Strong",
        "SQL",
        "T",
        "SQL",
        "programming",
        "skills",
        "experience",
        "functions",
        "packages",
        "Expertise",
        "business",
        "requirements",
        "models",
        "algorithms",
        "building",
        "models",
        "data",
        "mining",
        "reporting",
        "solutions",
        "volume",
        "data",
        "data",
        "data",
        "ingestion",
        "data",
        "manipulationdata",
        "architecture",
        "data",
        "modelling",
        "data",
        "preparation",
        "methods",
        "describe",
        "data",
        "contents",
        "statistics",
        "data",
        "regex",
        "Remap",
        "merge",
        "subset",
        "reindex",
        "melt",
        "reshape",
        "Visual",
        "Basic",
        "Applications",
        "VB",
        "programming",
        "languages",
        "C",
        "NET",
        "framework",
        "applications",
        "Big",
        "Data",
        "Hadoop",
        "HDFS",
        "MapReduce",
        "Spark",
        "Proficient",
        "Tableau",
        "RShiny",
        "data",
        "visualization",
        "tools",
        "insights",
        "datasets",
        "reports",
        "dashboards",
        "reports",
        "SQL",
        "Python",
        "BI",
        "platform",
        "Tableau",
        "development",
        "environment",
        "Git",
        "VM",
        "Excellent",
        "communication",
        "skills",
        "environment",
        "team",
        "learner",
        "US",
        "employer",
        "Work",
        "Experience",
        "Data",
        "ScientistData",
        "Science",
        "Manager",
        "American",
        "ExpressHome",
        "Depot",
        "July",
        "Present",
        "Description",
        "American",
        "Express",
        "analytics",
        "service",
        "systems",
        "business",
        "problems",
        "eg",
        "behavior",
        "consumers",
        "revenue",
        "leakage",
        "hospitals",
        "threats",
        "security",
        "brand",
        "health",
        "Home",
        "Depot",
        "home",
        "improvement",
        "supplies",
        "retailing",
        "company",
        "tools",
        "construction",
        "products",
        "services",
        "Home",
        "Depot",
        "home",
        "improvement",
        "retailer",
        "United",
        "States",
        "Lowes",
        "Responsibilities",
        "leadership",
        "skills",
        "mentoring",
        "data",
        "science",
        "projects",
        "Srishti",
        "Biz",
        "Evaluated",
        "models",
        "Cross",
        "Validation",
        "Log",
        "loss",
        "function",
        "ROC",
        "curves",
        "AUC",
        "feature",
        "selection",
        "technologies",
        "ElasticSearch",
        "Kibana",
        "algorithm",
        "regularization",
        "methods",
        "L2",
        "L1",
        "modeling",
        "XGBoost",
        "machine",
        "software",
        "package",
        "Python",
        "probabilities",
        "model",
        "master",
        "data",
        "modelling",
        "tables",
        "fields",
        "client",
        "data",
        "students",
        "LORs",
        "essays",
        "performance",
        "metrics",
        "Hands",
        "Experience",
        "Spark",
        "MLLib",
        "Python",
        "Scala",
        "containerization",
        "modules",
        "project",
        "Python",
        "APIs",
        "Web",
        "framework",
        "Javascript",
        "Docker",
        "Kubernetes",
        "Formulated",
        "basis",
        "selection",
        "GridSearch",
        "KFold",
        "hyperparameters",
        "algorithms",
        "model",
        "analysis",
        "students",
        "behaviour",
        "USMLE",
        "exam",
        "residency",
        "scipy",
        "nltkNatural",
        "Language",
        "Processing",
        "Toolkitmatplotlib",
        "model",
        "graphs",
        "performance",
        "students",
        "demographics",
        "score",
        "USMLE",
        "exams",
        "Application",
        "Artificial",
        "IntelligenceAImachine",
        "learning",
        "algorithms",
        "modeling",
        "decision",
        "treestext",
        "analytics",
        "language",
        "regression",
        "models",
        "Principal",
        "Component",
        "Analysis",
        "feature",
        "engineering",
        "data",
        "Performed",
        "Data",
        "Cleaning",
        "features",
        "engineering",
        "pandas",
        "packages",
        "python",
        "models",
        "frameworks",
        "jobs",
        "SQL",
        "statements",
        "data",
        "Google",
        "Bigquery",
        "Mysql",
        "Airflow",
        "learning",
        "models",
        "Tensorflow",
        "keras",
        "tests",
        "score",
        "residency",
        "students",
        "XGB",
        "classifier",
        "feature",
        "variable",
        "XGB",
        "regressor",
        "variables",
        "FeatureUnion",
        "FunctionTransfomer",
        "methods",
        "Natural",
        "Language",
        "Processing",
        "OnevsRest",
        "Classifier",
        "classifier",
        "classifiers",
        "classification",
        "problems",
        "application",
        "machine",
        "algorithms",
        "modeling",
        "Decision",
        "Tree",
        "Text",
        "Analytics",
        "Sentiment",
        "Analysis",
        "Naive",
        "Bayes",
        "Logistic",
        "Regression",
        "Linear",
        "Regression",
        "Python",
        "accuracy",
        "rate",
        "model",
        "reports",
        "metrics",
        "conclusions",
        "behavior",
        "models",
        "machine",
        "learning",
        "frameworks",
        "performance",
        "model",
        "Signal",
        "Hub",
        "data",
        "layers",
        "signals",
        "Signal",
        "Hub",
        "data",
        "performance",
        "model",
        "build",
        "learning",
        "framework",
        "Environment",
        "Python",
        "2x3x",
        "Hive",
        "AWS",
        "Linux",
        "Tableau",
        "Desktop",
        "Microsoft",
        "Excel",
        "NLP",
        "Deep",
        "frameworks",
        "TensorFLow",
        "Keras",
        "algorithms",
        "Data",
        "Scientist",
        "January",
        "November",
        "Description",
        "goal",
        "project",
        "text",
        "summarization",
        "comment",
        "stream",
        "message",
        "network",
        "services",
        "users",
        "sites",
        "understanding",
        "comment",
        "stream",
        "comment",
        "list",
        "Performed",
        "Data",
        "Profiling",
        "behavior",
        "features",
        "traffic",
        "pattern",
        "location",
        "time",
        "Date",
        "Time",
        "Application",
        "Artificial",
        "IntelligenceAImachine",
        "learning",
        "algorithms",
        "modeling",
        "decision",
        "treestext",
        "analytics",
        "language",
        "regression",
        "models",
        "network",
        "analysis",
        "networks",
        "SVM",
        "Volume",
        "package",
        "python",
        "Matlab",
        "comments",
        "clusters",
        "networking",
        "sites",
        "Sentiment",
        "Analysis",
        "Text",
        "Analytics",
        "Analyze",
        "traffic",
        "patterns",
        "autocorrelation",
        "time",
        "Sets",
        "comment",
        "streams",
        "NLP",
        "module",
        "stages",
        "punctuation",
        "removal",
        "vector",
        "model",
        "input",
        "Incremental",
        "Short",
        "Text",
        "Summarization",
        "ISTS",
        "algorithm",
        "Clusters",
        "comments",
        "pattern",
        "end",
        "user",
        "model",
        "False",
        "Positive",
        "Rate",
        "Text",
        "classification",
        "sentiment",
        "analysis",
        "data",
        "algorithm",
        "regularization",
        "methods",
        "L2",
        "L1",
        "Principal",
        "Component",
        "Analysis",
        "feature",
        "engineering",
        "data",
        "Performed",
        "Data",
        "Cleaning",
        "features",
        "engineering",
        "pandas",
        "packages",
        "python",
        "models",
        "SAP",
        "Predictive",
        "Analytics",
        "Created",
        "Data",
        "Quality",
        "Scripts",
        "SQL",
        "Hive",
        "data",
        "load",
        "quality",
        "data",
        "types",
        "data",
        "visualizations",
        "Python",
        "Tableau",
        "Data",
        "Scientist",
        "First",
        "DataAccenture",
        "July",
        "July",
        "Description",
        "First",
        "Data",
        "Corporation",
        "payment",
        "processing",
        "company",
        "Atlanta",
        "Georgia",
        "United",
        "States",
        "companys",
        "portfolio",
        "merchant",
        "transaction",
        "processing",
        "services",
        "credit",
        "debit",
        "privatelabel",
        "gift",
        "payroll",
        "card",
        "offerings",
        "fraud",
        "protection",
        "authentication",
        "solutions",
        "Responsibilities",
        "Configuration",
        "Management",
        "Build",
        "support",
        "applications",
        "production",
        "environments",
        "data",
        "source",
        "XML",
        "HDFS",
        "ETL",
        "data",
        "analysis",
        "data",
        "Data",
        "mapping",
        "activities",
        "Source",
        "systems",
        "Teradata",
        "Text",
        "mining",
        "building",
        "models",
        "topic",
        "analysis",
        "sentiment",
        "analysis",
        "data",
        "data",
        "data",
        "sources",
        "transformations",
        "Hive",
        "Map",
        "Reduce",
        "data",
        "HDFS",
        "adhoc",
        "queries",
        "T",
        "SQL",
        "data",
        "analysis",
        "Microsoft",
        "SQL",
        "Server",
        "Used",
        "R",
        "python",
        "Exploratory",
        "Data",
        "Analysis",
        "AB",
        "HQL",
        "VQL",
        "Data",
        "Lake",
        "AWS",
        "oozie",
        "pySpark",
        "Anova",
        "test",
        "Hypothesis",
        "test",
        "effectiveness",
        "Campaigns",
        "Computing",
        "AB",
        "testing",
        "frameworks",
        "clusters",
        "Control",
        "test",
        "groups",
        "group",
        "campaigns",
        "Text",
        "Analytics",
        "clusters",
        "merchants",
        "transaction",
        "Sentiment",
        "Analysis",
        "authenticity",
        "transactions",
        "chargebacks",
        "lifetime",
        "cost",
        "welfare",
        "system",
        "years",
        "data",
        "classes",
        "web",
        "page",
        "elements",
        "C",
        "AJAX",
        "JSP",
        "client",
        "side",
        "responses",
        "C",
        "SQL",
        "data",
        "Developed",
        "LINUXShell",
        "scripts",
        "NZSQLNZLOAD",
        "utilities",
        "data",
        "files",
        "Netezza",
        "database",
        "triggers",
        "procedures",
        "functions",
        "packages",
        "cursors",
        "ref",
        "cursor",
        "concepts",
        "project",
        "types",
        "data",
        "visualizations",
        "R",
        "C",
        "python",
        "TableauSpotfire",
        "Pipeline",
        "Pilot",
        "Spotfire",
        "business",
        "layouts",
        "Python",
        "R",
        "SQL",
        "algorithms",
        "Multivariate",
        "Regression",
        "Linear",
        "Regression",
        "Logistic",
        "Regression",
        "PCA",
        "Random",
        "forest",
        "models",
        "Decision",
        "trees",
        "Support",
        "Vector",
        "Machine",
        "risks",
        "welfare",
        "dependency",
        "welfare",
        "highrisk",
        "groups",
        "Machine",
        "learningdeep",
        "campaigns",
        "trials",
        "impact",
        "initiatives",
        "Tableau",
        "visualizations",
        "dashboards",
        "Tableau",
        "Desktop",
        "Used",
        "Graphical",
        "EntityRelationship",
        "database",
        "design",
        "interface",
        "custom",
        "SQL",
        "Teradata",
        "SQL",
        "Workbench",
        "data",
        "sets",
        "Tableau",
        "dashboards",
        "Perform",
        "analyses",
        "regression",
        "analysis",
        "regression",
        "discriminant",
        "analysis",
        "cluster",
        "analysis",
        "SAS",
        "programming",
        "Environment",
        "R",
        "3x",
        "HDFS",
        "C",
        "Hadoop",
        "Pig",
        "Hive",
        "Linux",
        "RStudio",
        "Tableau",
        "SQL",
        "Server",
        "MS",
        "Excel",
        "PySpark",
        "Python",
        "Developer",
        "Cenvien",
        "Technologies",
        "Hyderabad",
        "Telangana",
        "January",
        "June",
        "Description",
        "Cenvien",
        "technologies",
        "requirements",
        "understanding",
        "clients",
        "business",
        "requirement",
        "quality",
        "products",
        "team",
        "solutions",
        "Responsibilities",
        "frontend",
        "modules",
        "Python",
        "Django",
        "Web",
        "Framework",
        "presentation",
        "layer",
        "HTML",
        "CSS",
        "JavaScript",
        "procedures",
        "Oracle",
        "database",
        "performance",
        "data",
        "management",
        "system",
        "Oracle",
        "Environment",
        "MySQL",
        "ORACLE",
        "HTML5",
        "CSS3",
        "JavaScript",
        "Shell",
        "Linux",
        "Windows",
        "Django",
        "Python",
        "Data",
        "Analyst",
        "Pennar",
        "Industries",
        "Limited",
        "Hyderabad",
        "Telangana",
        "March",
        "December",
        "Description",
        "developer",
        "web",
        "applications",
        "data",
        "science",
        "infrastructure",
        "area",
        "focus",
        "solutions",
        "capacity",
        "Responsibilities",
        "stakeholders",
        "requirements",
        "projects",
        "MySQL",
        "package",
        "connector",
        "writing",
        "MYSQL",
        "database",
        "Python",
        "ClientServer",
        "applications",
        "C",
        "JSP",
        "SQL",
        "Created",
        "functions",
        "views",
        "procedures",
        "SQL",
        "developer",
        "ways",
        "limits",
        "Web",
        "technology",
        "code",
        "review",
        "meetings",
        "Environment",
        "Python",
        "MySQL",
        "C",
        "Education",
        "Master",
        "Computer",
        "Science",
        "Computer",
        "Science",
        "Lamar",
        "University",
        "Beaumont",
        "TX",
        "Skills",
        "Business",
        "Intelligence",
        "SQL",
        "access",
        "Excel",
        "Data",
        "Science",
        "years",
        "Python",
        "years",
        "Links",
        "CertificationsLicenses",
        "Acadgild",
        "Masters",
        "Data",
        "Science",
        "February",
        "Present",
        "Assessments",
        "Data",
        "Analysis",
        "Proficient",
        "August",
        "Measures",
        "candidates",
        "skill",
        "graphs",
        "trends",
        "conclusions",
        "data",
        "results",
        "Problem",
        "Solving",
        "Expert",
        "August",
        "Measures",
        "candidates",
        "ability",
        "information",
        "problems",
        "results",
        "httpsshareindeedassessmentscomshare_assignmentdso7visv0vjm2cfc",
        "Indeed",
        "Assessments",
        "skills",
        "tests",
        "license",
        "certification",
        "development",
        "field"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T19:56:00.278340",
    "resume_data": "Data ScientistData Science Manager Data ScientistData Science Manager Data ScientistData Science Manager American ExpressHome Depot Atlanta GA Professional qualified Data ScientistSoftware Engineer with over 9 years of experience in Data Science and Analytics including Artificial IntelligenceDeep LearningMachine Learning Big Data Data Mining and Statistical Analysis and demonstrated Leadership Involved in the entire data science project life cycle and actively involved in all the phases including data extraction data cleaning statistical modeling and data visualization with large data sets of structured and unstructured data created ER diagrams and schema Experienced with machine learning algorithm such as logistic regression random forest XGboost KNN SVM neural network linear regression lasso regression and kmeans Implemented Bagging and Boosting to enhance the model performance Strong skills in statistical methodologies such as AB test experiment design hypothesis test ANOVA Extensively worked on Python 3527 Numpy Pandas Matplotlib NLTK and Scikitlearn and Scala Experience in implementing data analysis with various analytic tools such as Anaconda 40 Jupiter Notebook 4X R 30 ggplot2 Caret dplyr and Excel Solid ability to write and optimize diverse SQL queries working knowledge of RDBMS like SQL Server 2008 NoSql databases like AWS DynamoDB MongoDB Developed API libraries and coded business logic using C XML and designed web pages using NET framework C Python Django HTML AJAX Strong experience in Big Data technologies like Spark Sparksql pySpark Hadoop HDFS Hive Experience in visualization tools like Tableau 9X 10X for creating dashboards Excellent understanding Agile and Scrum development methodology Used the version control tools like Git 2X and build tools like Apache MavenAnt Passionate about gleaning insightful information from massive data assets and developing a culture of sound datadriven decision making Worked on containerization using Docker and managed the containers in Kubernetes Ability to maintain a fun casual professional and productive team atmosphere Experienced the full software life cycle in SDLC Agile Devops and Scrum methodologies including creating requirements test plans Worked on message queues such as Amazon SNS SQS Google Pub Sub and integrated with other services like AWS Lambda Storage using Bash scripts Skilled in Advanced Regression Modeling Correlation Multivariate Analysis Model Building Business Intelligence tools and application of Statistical Concepts Proficient in Predictive Modeling Data Mining Methods Factor Analysis ANOVA Hypothetical testing normal distribution and other advanced statistical and econometric techniques Developed predictive models using Decision Tree Random Forest Nave Bayes Logistic Regression Social Network Analysis Cluster Analysis and Neural Networks Experienced in Machine Learning and Statistical Analysis with Python ScikitLearn Experienced in Python to manipulate data for data loading and extraction and worked with python libraries like Matplotlib Numpy Scipy and Pandas for data analysis Worked with complex applications such as R SAS Matlab and SPSS to develop neural network cluster analysis Strong SQL and T SQL programming skills with experience in working with functions packages and triggers Expertise in transforming business requirements into analytical models designing algorithms building models developing data mining and reporting solutions that scales across massive volume of structured and unstructured data Skilled in performing data parsing data ingestion data manipulationdata architecture data modelling and data preparation with methods including describe data contents compute descriptive statistics of data regex split and combine Remap merge subset reindex melt and reshape Experienced in Visual Basic for Applications and VB programming languages C NET framework to work with developing applications Experienced in Big Data with Hadoop HDFS MapReduce and Spark Proficient in Tableau and RShiny data visualization tools to analyze and obtain insights into large datasets create visually powerful and actionable interactive reports and dashboards Automated recurring reports using SQL and Python and visualized them on BI platform like Tableau Worked in development environment like Git and VM Excellent communication skills Successfully working in fastpaced multitasking environment both independently and in collaborative team a selfmotivated enthusiastic learner Authorized to work in the US for any employer Work Experience Data ScientistData Science Manager American ExpressHome Depot July 2017 to Present Description American Express delivers predictive analytics as a service and offers hosted cloudbased systems for specific business problems eg predicting the behavior of individual consumers stopping revenue leakage in hospitals warning of threats to corporate security or brand health etc Home Depot is an American home improvement supplies retailing company that sells tools construction products and services Home Depot is the largest home improvement retailer in the United States ahead of rival Lowes Responsibilities Demonstrated leadership skills by successfully managing mentoring and delivering 3 data science projects at Srishti Biz Evaluated models using Cross Validation Log loss function ROC curves and used AUC for feature selection and elastic technologies like ElasticSearch Kibana etc Addressed overfitting by implementing of the algorithm regularization methods like L2 and L1 Implemented statistical modeling with XGBoost machine learning software package using Python to determine the predicted probabilities of each model Created master data for modelling by combining various tables and derived fields from client data and students LORs essays and various performance metrics Hands on Experience on Spark MLLib using both Python and Scala Worked on containerization of different modules of the project like Python APIs Web framework written in Javascript using Docker and managed it using Kubernetes Formulated a basis for variable selection and GridSearch KFold for optimal hyperparameters Utilized Boosting algorithms to build a model for predictive analysis of students behaviour who took USMLE exam apply for residency Used numpy scipy pandas nltkNatural Language Processing Toolkitmatplotlib to build the model Formulated several graphs to show the performance of the students by demographics and their mean score in different USMLE exams Application of various Artificial IntelligenceAImachine learning algorithms and statistical modeling like decision treestext analytics natural language processingNLP supervised and unsupervised regression models Used Principal Component Analysis in feature engineering to analyze high dimensional data Performed Data Cleaning features scaling features engineering using pandas and numpy packages in python and build models using deep learning frameworks Scheduled jobs to run SQL statements to transfer data from Google Bigquery to Mysql using Airflow Created deep learning models using Tensorflow and keras by combining all tests as a single normalized score and predict residency attainment of students Used XGB classifier if the feature is a categorical variable and XGB regressor for continuous variables and combined it using FeatureUnion and FunctionTransfomer methods of Natural Language Processing Used OnevsRest Classifier to fit each classifier against all other classifiers and used it on multiclass classification problems Implemented application of various machine learning algorithms and statistical modeling like Decision Tree Text Analytics Sentiment Analysis Naive Bayes Logistic Regression and Linear Regression using Python to determine the accuracy rate of each model Created and designed reports that will use gathered metrics to infer and draw logical conclusions of past and future behavior Generated various models by using different machine learning and deep learning frameworks and tuned the best performance model using Signal Hub Created data layers as signals to Signal Hub to predict new unseen data with performance not less than the static model build using deep learning framework Environment Python 2x3x Hive AWS Linux Tableau Desktop Microsoft Excel NLP Deep learning frameworks such as TensorFLow Keras Boosting algorithms etc Data Scientist January 2016 to November 2016 Description The goal of this project is short text summarization on the comment stream of a message from social network services The users of the social sites always desire to get a brief understanding of a comment stream without reading the whole comment list Responsibilities Performed Data Profiling to learn about behavior with various features such as traffic pattern location time Date and Time etc Application of various Artificial IntelligenceAImachine learning algorithms and statistical modeling like decision treestext analytics natural language processingNLP supervised and unsupervised regression models social network analysis neural networks deep learning SVM clustering to identify Volume using scikitlearn package in python Matlab Categorised comments into positive and negative clusters from different social networking sites using Sentiment Analysis and Text Analytics Analyze traffic patterns by calculating autocorrelation with different time lags Sets of comment streams are processed by NLP module in stages like stemming punctuation removal etc to categorize into vector model and input into Incremental Short Text Summarization ISTS algorithm Clusters of comments with same pattern will be grouped together and presented visually to the end user Ensured that the model has low False Positive Rate and Text classification and sentiment analysis for unstructured and semistructured data Addressed overfitting by implementing of the algorithm regularization methods like L2 and L1 Used Principal Component Analysis in feature engineering to analyze high dimensional data Performed Data Cleaning features scaling features engineering using pandas and numpy packages in python and build models using SAP Predictive Analytics Created Data Quality Scripts using SQL and Hive to validate successful data load and quality of the data Created various types of data visualizations using Python and Tableau Data Scientist First DataAccenture July 2011 to July 2015 Description First Data Corporation is a global payment processing company headquartered in Atlanta Georgia United States The companys portfolio includes merchant transaction processing services credit debit privatelabel gift payroll and other prepaid card offerings fraud protection and authentication solutions Responsibilities Provided Configuration Management and Build support for more than 5 different applications built and deployed to the production and lower environments Explored and Extracted data from source XML in HDFS used ETL for preparing data for exploratory analysis using data munging Responsible for different Data mapping activities from Source systems to Teradata Text mining and building models using topic analysis sentiment analysis for both semistructured and unstructured data Handled importing data from various data sources performed transformations using Hive Map Reduce and loaded data into HDFS Write adhoc queries using T SQL for data analysis on Microsoft SQL Server Used R and python for Exploratory Data Analysis AB testing HQL VQL Data Lake AWS Redshift oozie pySpark Anova test and Hypothesis test to compare and identify the effectiveness of Creative Campaigns Computing AB testing frameworks clickstream Created clusters to Control and test groups and conducted group campaigns using Text Analytics Created positive and negative clusters from merchants transaction using Sentiment Analysis to test the authenticity of transactions and resolve any chargebacks Analyzed and calculated the lifetime cost of everyone in the welfare system using 20 years of historical data Created and developed classes and web page elements using C and AJAX JSP was used for validating client side responses and connected C to database to retrieve SQL data Developed LINUXShell scripts by using NZSQLNZLOAD utilities to load data from flat files to Netezza database Developed triggers stored procedures functions and packages using cursors and ref cursor concepts associated with the project using PlSQL Created various types of data visualizations using R C python and TableauSpotfire also connected Pipeline Pilot with Spotfire to create more interactive business driven layouts Used Python R SQL to create Statistical algorithms involving Multivariate Regression Linear Regression Logistic Regression PCA Random forest models Decision trees Support Vector Machine for estimating the risks of welfare dependency Identified and targeted welfare highrisk groups with Machine learningdeep learning algorithms Conducted campaigns and run realtime trials to determine what works fast and track the impact of different initiatives Developed Tableau visualizations and dashboards using Tableau Desktop Used Graphical EntityRelationship Diagramming to create new database design via easy to use graphical interface Created multiple custom SQL queries in Teradata SQL Workbench to prepare the right data sets for Tableau dashboards Perform analyses such as regression analysis logistic regression discriminant analysis cluster analysis using SAS programming Environment R 3x HDFS C Hadoop 23 Pig Hive Linux RStudio Tableau 10 SQL Server MS Excel PySpark Python Developer Cenvien Technologies Hyderabad Telangana January 2011 to June 2011 Description Cenvien technologies gather the requirements by listening and understanding to the clients business requirement to deliver quality products It is highly qualified and strongly dedicated developing team that produces unique solutions Responsibilities Developed entire frontend and backend modules using Python on Django Web Framework Implemented the presentation layer with HTML CSS and JavaScript Involved in writing stored procedures using Oracle Optimized the database queries to improve the performance Designed and developed data management system using Oracle Environment MySQL ORACLE HTML5 CSS3 JavaScript Shell Linux Windows Django Python Data Analyst Pennar Industries Limited Hyderabad Telangana March 2009 to December 2010 Description As a backend developer of web applications and data science infrastructure The main area of focus is to come up with comprehensive solutions that need massive capacity and throughput Responsibilities Effectively communicated with the stakeholders to gather requirements for different projects Used MySQL db package and PythonMySQL connector for writing and executing several MYSQL database queries from Python Implemented ClientServer applications using C JSP and SQL Created functions triggers views and stored procedures using My SQL Worked closely with backend developer to find ways to push the limits of existing Web technology Involved in the code review meetings Environment Python MySQL C Education Master of Computer Science in Computer Science Lamar University Beaumont TX 2015 to 2017 Skills Business Intelligence SQL access testing Excel Data Science 9 years Python 9 years Links httpswwwlinkedincominpavithrakumar22b753173 CertificationsLicenses Acadgild Masters in Data Science February 2019 to Present Assessments Data Analysis Proficient August 2019 Measures a candidates skill in interpreting and producing graphs identifying trends and drawing justifiable conclusions from data Full results httpsshareindeedassessmentscomshare_assignment5p0ejlgzweim6ro Problem Solving Expert August 2019 Measures a candidates ability to analyze relevant information when solving problems Full results httpsshareindeedassessmentscomshare_assignmentdso7visv0vjm2cfc Indeed Assessments provides skills tests that are not indicative of a license or certification or continued development in any professional field",
    "unique_id": "a53bca8c-95bb-4aad-b010-a24428a395a4"
}