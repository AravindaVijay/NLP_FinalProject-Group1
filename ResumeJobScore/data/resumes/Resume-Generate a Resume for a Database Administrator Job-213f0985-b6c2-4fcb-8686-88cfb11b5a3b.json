{
    "clean_data": "Sr Big Data Developer Sr Big Data span lDeveloperspan Sr Big Data Developer Charlotte NC Above 9 years of working experience as a Sr Big DataHadoop Developer in designed and developed various applications like big data Hadoop JavaJ2EE opensource technologies Work Extensively in Core Java Struts2 JSF spring Hibernate Servlets JSP and Handson experience with PLSQL XML and SOAP Well verse working with Relational Database Management Systems as Oracle MS SQL MySQL Server Hands on experience on Hadoop Big Data related technology experience in Storage Querying Processing and analysis of data Hands on experience in working on XML suite of technologies like XML XSL XSLT DTD XML Schema SAX DOM JAXB Experience in working with Web Servers like Apache Tomcat and Application Servers like IBM Web Sphere and JBOSS Hands on experience in advanced BigData technologies like Spark Ecosystem Spark SQL MLlib Spark R and Spark Streaming Kafka and Predictive analytics Knowledge of the software Development Life Cycle SDLC Agile and Waterfall Methodologies Experience on applications using Java python and UNIX shell scripting Experience in consuming Web services with Apache Axis using JAXRSREST APIs Experienced in building tool Maven ANT and logging tool Log4J Experience in Programming and Development of java modules for an existing web portal based in Java using technologies like JSP Servlets JavaScript and HTML SOA with MVC architecture Expertise in ingesting real timenear real time data using Flume Kafka Storm Good knowledge of NO SQL databases like Mongo DB Cassandra and HBase Strong development skills in Hadoop HDFS Map Reduce Hive Sqoop HBase with solid understanding of Hadoop internals Excellent knowledge of Hadoop Architecture and various components such as HDFS Job Tracker Task Tracker Name Node Data Node and MRA and MRv2 YARN Good knowledge of NoSQL databases such as HBase MongoDB and Cassandra Experience in working with Eclipse IDE Net Beans and Rational Application Developer Experience in using PLSQL to write Stored Procedures Functions and Triggers Expertise in developing a simple web based application using J2EE technologies like JSP Servlets and JDBC Experience working on EC2 Elastic Compute Cloud cluster instances setup data buckets on S3 Simple Storage Service set EMR Elastic MapReduce Hands on experience in installing configuring and using Apache Hadoop ecosystem components like Hadoop Distributed File System HDFS MapReduce Pig Hive HBase Apache Crunch Zookeeper Scoop Hue Scala AVRO Strong Programming Skills in designing and implementing of multitier applications using Java J2EE JDBC JSP JSTL HTML CSS JSF Struts JavaScript JAXB Extensive experience in SOAbased solutions Web Services Web API WCF SOAP including Restful APIs services Good Knowledge in Amazon Web Service AWS concepts like EMR and EC2 web services which provides fast and efficient processing of Teradata Big Data Analytics Experience in collection of Log Data and JSON data into HDFS using Flume and processed the data using HivePig Work Experience Sr Big Data Developer Wells Fargo Charlotte NC January 2018 to Present Responsibilities As a Big Data Developer I worked on Hadoop ecosystems including Hive HBase Oozie Pig Zookeeper Spark Streaming MCS MapR Control System and so on with MapR distribution Installed and configured Hadoop MapReduce HDFS Developed multiple MapReduce jobs in Java for data cleaning and Preprocessing Primarily involved in Data Migration process using Azure by integrating with Github repository and Jenkins Built code for real time data ingestion using Java Map RStreams Kafka and STORM Involved in various phases of development analysed and developed the system going through Agile Scrum methodology Worked on Apache Solr which is used as indexing and search engine Involved in development of Hadoop System and improving multinode Hadoop Cluster performance Worked on analysing Hadoop stack and different Big data tools including Pig and Hive HBase database and Sqoop Developed data pipeline using flume Sqoop and pig to extract the data from weblogs and store in HDFS Used Hive to analyze the partitioned and bucketed data and compute various metrics for reporting Worked with different data sources like Avro data files XML files JSON files SQL server and Oracle to load data into Hive tables Used J2EE design patterns like Factory pattern Singleton Pattern Used Spark to create the structured data from large amount of unstructured data from various sources Implemented usage of Amazon EMR for processing Big Data across Hadoop Cluster of virtual servers on Amazon Elastic Compute Cloud EC2 and Amazon Simple Storage Service S3 Performed transformations cleaning and filtering on imported data using Hive MapReduce Impala and loaded final data into HDFS Developed Python scripts to find vulnerabilities with SQL Queries by doing SQL injection Experienced in designing and developing POCs in Spark using Scala to compare the performance of Spark with Hive and SQL Responsible for coding MapReduce program Hive queries testing and debugging the MapReduce programs Extracted Real time feed using Spark streaming and convert it to RDD and process data into Data Frame and load the data into Cassandra Involved in the process of data acquisition data preprocessing and data exploration of telecommunication project in Scala Implemented a distributed messaging queue to integrate with Cassandra using Apache Kafka and Zookeeper Specified the cluster size allocating Resource pool Distribution of Hadoop by writing the specification texts in JSON File format Imported weblogs unstructured data using the Apache Flume and stores the data in Flume channel Exported event weblogs to HDFS by creating a HDFS sink which directly deposits the weblogs in HDFS Used RESTful web services with MVC for parsing and processing XML data Utilized XML and XSL Transformation for dynamic webcontent and database connectivity Involved in loading data from UNIX file system to HDFS Involved in designing schema writing CQLs and loading data using Cassandra Built the automated build and deployment framework using Jenkins Maven etc Environment Hadoop 30 Hive 23 HBase 12 Oozie Pig 017 Zookeeper Spark MapReduce Azure Java Agile J2EE Cassandra 311 Jenkins Maven Big DataHadoop Developer Caterpillar Peoria IL March 2015 to December 2017 Responsibilities Responsible for installation and configuration of Hive Pig HBase and Sqoop on the Hadoop cluster and created hive tables to store the processed results in a tabular format Involved in Agile methodologies daily scrum meetings spring planning Integrate visualizations into a Spark application using Data bricks and popular visualization libraries ggplot matplotlib Involved in all phases of Software Development Life Cycle SDLC and Worked on all activities related to the development implementation and support for Hadoop Configured Spark Streaming to receive real time data from the Apache Kafka and store the stream data to HDFS using Scala Developed the Sqoop scripts to make the interaction between Hive and vertica Database Processed data into HDFS by developing solutions and analyzed the data using MapReduce PIG and Hive to produce summary results from Hadoop to downstream systems Streamed AWS log group into Lambda function to create service now incident Involved in loading and transforming large sets of Structured SemiStructured and Unstructured data and analyzed them by running Hive queries and Pig scripts Created Managed tables and External tables in Hive and loaded data from HDFS Developed Spark code by using Scala and SparkSQL for faster processing and testing and performed complex HiveQL queries on Hive tables Scheduled several times based Oozie workflow by developing Python scripts Exporting the data using Sqoop to RDBMS servers and processed that data for ETL operations Worked on S3 buckets on AWS to store Cloud Formation Templates and worked on AWS to create EC2 instances Designing ETL Data Pipeline flow to ingest the data from RDBMS source to Hadoop using shell script sqoop package and MySQL Endtoend architecture and implementation of clientserver systems using Scala Akka Java JavaScript and related Linux Optimized the Hive tables using optimization techniques like partitions and bucketing to provide better Used Oozie workflow engine to manage interdependent Hadoop jobs and to automate several types of Hadoop jobs such as Java mapreduce Hive Pig and Sqoop Implementing Hadoop with the AWS EC2 system using a few instances in gathering and analyzing data log files Involved in Spark and Spark Streaming creating RDDs applying operations Transformation and Actions Created partitioned tables and loaded data using both static partition and dynamic partition method Developed custom Apache Spark programs in Scala to analyze and transform unstructured data Using Kafka on publishsubscribe messaging as a distributed commit log have experienced in its fast scalable and durability Scheduled map reduces jobs in production environment using Oozie scheduler Involved in Cluster maintenance Cluster Monitoring and Troubleshooting Manage and review data backups and log files Designed and implemented MapReduce jobs to support distributed processing using java Hive and Apache Pig Analyzing Hadoop cluster and different Big Data analytic tools including Pig Hive HBase and Sqoop Improved the Performance by tuning of Hive and MapReduce Implemented POC to migrate MapReduce jobs into Spark RDD transformations using Scala Environment Hive 23 Pig 017 HBase 12 Sqoop Hadoop 30 Agile Spark Scala AWS JavaScript Hadoop Developer Facebook Menlo Park CA October 2013 to February 2015 Responsibilities Developed Spark code and SparkSQLStreaming for faster testing and processing of data using Lambda Architecture Experience in deploying data from various sources into HDFS and building reports using Tableau Developed a data pipeline using Kafka and Strom to store data into HDFS Developed REST APIs using Scala and Play framework to retrieve processed data from Cassandra database Reengineered ntiered architecture involving technologies like EJB XML and Java into distributed applications Explored the possibilities of using technologies like JMX for better monitoring of the system Configured deployed and maintained multinode Dev and Test Kafka Clusters Performed transformations cleaning and filtering on imported data using Hive MapReduce and loaded final data into HDFS Load the data into Spark RDD and performed inmemory data computation to generate the output response Responsible for Cluster Maintenance Monitoring Managing Commissioning and decommissioning Data nodes Troubleshooting and review data backups Manage review log files for Hortonworks Installed Oozie workflow engine to run multiple Hive and Pig jobs Building and maintaining scalable data pipelines using the Hadoop ecosystem and other open source components like Hive and HBase Installed and configured HA of Hue to point Hadoop Cluster in cloudera Manager Installed and configured MapReduce HDFS and developed multiple MapReduce jobs in java for data cleaning and preprocessing Working with applications teams to install operating system Hadoop updates patches version upgrades as required Responsible for developing data pipeline using HDInsight Flume Sqoop and Pig to extract the data from weblogs and store in HDFS Performed transformations cleaning and filtering on imported data using Hive MapReduce and loaded final data into HDFS Developed and maintained the continuous integration and deployment systems using Jenkins ANT Akka and MAVEN Effectively used GIT version control to collaborate with the Akka team members Developed HDFS with huge amounts of data using Apache Kafka Collected the log data from web servers and integrated into HDFS using Flume Environment Spark HDFS Kafka Scala Cassandra Java Hive MapReduce Hortonworks Oozie Hadoop HBase Flume Sqoop Pig MAVEN ANT JavaJ2ee Developer Key Bank San Francisco CA June 2012 to September 2013 Responsibilities Worked closely with the Requirements team and analyzed the Use cases Elaborated on the Use cases based on business requirements and was responsible for creation of class diagrams sequence diagrams Adopted J2EE best Practices using Core J2EE patterns Developed in Eclipse environment using Struts based MVC framework Designed and developed presentation layer using JSP HTML and JavaScript Created JSPs using JSTL and Spring tag libraries Developed Struts Action and Action Form classes Deployed J2EE components EJB Servlets in Tomcat Application server Involved in understanding business requirements and provide technical designs and necessary documentation Implemented Microservices architecture to make application smaller and independent Developed new Spring Boot application with microservices and added functionality to existing applications using Java J2EE technologies Used Hibernate as Object relational mapping tool for mapping Java Objects to database tables Used Hibernate Query Language HQL annotations and Criteria for access and updating data Implemented REST Web Services and performed the HTTP operations Implemented multithreading to process multiple tasks concurrently in order to perform the readwrite operations Worked extensively with Core Java concepts like Collections Exception Handling Java IO and Generics to implement business logic Developed web pages using with HTML CSS JavaScript jQuery and Ajax Used JavaScript for clientside validations in the JSP and HTML pages Created SQL queries and stored procedures to create retrieve and update data from database Developed Maven Scripts to build and deploy EAR files GitHub was used for the version control and source code management Followed Test Driven Development TDD responsible for testing debugging and bug fixing of the application Used log4j to capture the logs that included runtime exceptions and debug information The application was deployed in LINUX environment Environment J2EE MVC HTML JavaScript Java Hibernate jQuery Ajax Maven Java Developer Commvault Hyderabad Telangana March 2010 to May 2012 Responsibilities Involved in the complete Software Development Life Cycle including Requirement Analysis Design Implementation Testing and Maintenance Designed the front end using JSP jQuery CSS and HTML as per the requirements that are provided Developed Responsive web application for the backend system using AngularJS with HTML and CSS Used Spring AOP to enable the log interfaces and crosscutting concerns Developed payment flow using AJAX partial page refresh validation and dynamic dropdown list Responsible for use case diagrams class diagrams and sequence diagrams using Rational Rose in the Design phase Used Spring Core for dependency injectionInversion of Control IOC to have loosecoupling Implemented application using MVC architecture integrating Hibernate and spring frameworks Implemented the Enterprise JavaBeans EJBs to handle various transactions and incorporated the validation framework for the project Designed and developed all UI Screens using Java Server Pages JSP Static Content HTML CSS and JavaScript Worked on serverside web applications using Nodejs and involved in Construction of UI using jQuery Bootstrap and JavaScript Developed Database access components using Spring DAO integrated with Hibernate for accessing the data Integrate the dynamic pages with AngularJS to make the pages dynamic Developed Custom Tags and JSTL to support custom user interfaces Used CSS style sheets for presenting data from XML documents and data from databases to render on HTML web pages Used Spring Framework for integrating Hibernate for dependency injection Extensively used Eclipse for writing code Used Maven as a build tool and deployed on WebSphere Application Server Developed test cases on JUnit Used Log4J for logging and tracing the messages Environment HTML CSS JavaScript jQuery AngularJS AJAX Bootstrap XML Maven Eclipse JUnit Education Bachelors in Computer Science Engineering Jawaharlal Nehru Technological University Skills Cassandra Hdfs Impala Mapreduce Oozie Sqoop Hbase Kafka Flume Hadoop Jboss Jms Mongodb Nosql Analysis services Application server Git Hadoop Hbase Hive Additional Information Technical Skills HadoopBig Data Technologies Hadoop 30 HDFS MapReduce HBase 14 Apache Pig 017 Hive 23 Sqoop 14 Apache Impala 21 Oozie 43 Yarn Apache Flume 18 Kafka 11 Zookeeper Cloud Platform Amazon AWS EC2 EC3 MS Azure Azure SQL Database Azure SQL Data Warehouse Azure Analysis Services HDInsight Azure Data Lake Data Factory Hadoop Distributions Cloudera Hortonworks MapR Programming Language Java Scala Python 36 SQL PLSQL Shell Scripting Storm 10 JSP Servlets Frameworks Spring 505 Hibernate 52 Struts 13 JSF EJB JMS Web Technologies HTML CSS JavaScript JQuery 33 Bootstrap 41 XML JSON AJAX Operating Systems Linux Unix Windows 1087 IDE and Tools Eclipse 47 NetBeans 82 IntelliJ Maven NoSQL Databases HBase 14 Cassandra 311 MongoDB WebApplication Server Apache Tomcat 907 JBoss Web Logic Web Sphere SDLC Methodologies Agile Waterfall Version Control GIT SVN CVS",
    "entities": [
        "Flume Environment Spark",
        "REST Web Services",
        "JSP Servlets Frameworks",
        "Transformation and Actions Created",
        "AJAX",
        "Nodejs",
        "UNIX",
        "Building",
        "Criteria",
        "HTTP",
        "JAXRSREST",
        "S3 Simple Storage Service",
        "IBM",
        "HBase Strong",
        "Amazon Elastic Compute Cloud EC2",
        "MapReduce PIG",
        "Oracle MS",
        "MapReduce Implemented",
        "Tomcat Application",
        "Requirements",
        "Hortonworks Installed Oozie",
        "EJB Servlets",
        "RDD",
        "Hadoop",
        "XML",
        "HDFS Involved",
        "Structured SemiStructured",
        "Tableau Developed",
        "Spark Ecosystem Spark",
        "EAR",
        "Telangana",
        "Nehru Technological University",
        "Software Development Life Cycle SDLC",
        "JUnit",
        "Present Responsibilities",
        "Control IOC",
        "HBase",
        "Amazon Simple Storage Service",
        "Avro",
        "Apache Spark",
        "Used Hibernate Query Language",
        "Spark with",
        "SparkSQL",
        "Developed",
        "Hibernate Servlets JSP",
        "Sqoop Improved",
        "Node Data",
        "Streamed AWS log",
        "SOAbased",
        "Scala Developed",
        "HadoopBig Data Technologies Hadoop",
        "JSTL",
        "Developed Responsive",
        "Pig Hive HBase",
        "Tools",
        "HivePig Work Experience Sr Big Data",
        "Teradata Big Data Analytics",
        "Hive Pig HBase",
        "JSP",
        "Unstructured",
        "WebApplication Server",
        "Rational Rose",
        "Sr Big Data Developer Sr Big Data",
        "SQL Queries",
        "Apache Tomcat and Application Servers",
        "Spark Streaming",
        "Hadoop Cluster",
        "Hadoop Big Data",
        "Hadoop System",
        "Lambda Architecture",
        "Implemented Microservices",
        "BigData",
        "Maven ANT",
        "MVC",
        "Oozie scheduler Involved",
        "Spark",
        "EJB",
        "GIT",
        "Log Data",
        "Amazon EMR",
        "Hadoop Distributed File System HDFS MapReduce Pig Hive HBase",
        "Database",
        "Sqoop",
        "Computer Science Engineering",
        "JSON File",
        "Java Server Pages JSP Static Content HTML CSS",
        "AWS",
        "Scala",
        "Created SQL",
        "Hadoop Architecture",
        "Oracle",
        "Scala Akka Java JavaScript",
        "Rational Application Developer Experience",
        "Singleton",
        "Construction of UI",
        "HDFS Job Tracker Task Tracker",
        "HDFS Developed Spark",
        "java",
        "HTML",
        "Oozie",
        "Hive HBase Oozie",
        "SQL",
        "Spark RDD",
        "GitHub",
        "jQuery Bootstrap",
        "Git Hadoop Hbase Hive Additional Information Technical Skills",
        "MAVEN",
        "Development Life Cycle",
        "Github",
        "Created Managed",
        "Data Frame",
        "Big Data",
        "Hive",
        "Handson",
        "ETL",
        "Utilized XML",
        "Apache Hadoop",
        "JavaScript Hadoop",
        "JavaScript Created",
        "Maven",
        "XSLT",
        "Data Migration",
        "Impala",
        "Hadoop JavaJ2EE",
        "JavaScript",
        "SQL Responsible",
        "HBase Installed",
        "Followed Test Driven Development TDD",
        "UI",
        "HTML CSS JavaScript jQuery",
        "JSP Servlets",
        "Amazon Web Service AWS",
        "Programming and Development",
        "MRA",
        "Expertise",
        "CSS",
        "SAX DOM JAXB",
        "Data",
        "MapReduce",
        "Spring DAO",
        "NetBeans",
        "WebSphere Application Server Developed",
        "NoSQL",
        "Application",
        "Software Development Life Cycle",
        "Relational Database Management Systems",
        "Storage Querying Processing",
        "Developed Custom Tags",
        "Resource pool Distribution of Hadoop",
        "Sqoop Developed",
        "JMX",
        "Hadoop MapReduce HDFS Developed"
    ],
    "experience": "Experience in working with Web Servers like Apache Tomcat and Application Servers like IBM Web Sphere and JBOSS Hands on experience in advanced BigData technologies like Spark Ecosystem Spark SQL MLlib Spark R and Spark Streaming Kafka and Predictive analytics Knowledge of the software Development Life Cycle SDLC Agile and Waterfall Methodologies Experience on applications using Java python and UNIX shell scripting Experience in consuming Web services with Apache Axis using JAXRSREST APIs Experienced in building tool Maven ANT and logging tool Log4J Experience in Programming and Development of java modules for an existing web portal based in Java using technologies like JSP Servlets JavaScript and HTML SOA with MVC architecture Expertise in ingesting real timenear real time data using Flume Kafka Storm Good knowledge of NO SQL databases like Mongo DB Cassandra and HBase Strong development skills in Hadoop HDFS Map Reduce Hive Sqoop HBase with solid understanding of Hadoop internals Excellent knowledge of Hadoop Architecture and various components such as HDFS Job Tracker Task Tracker Name Node Data Node and MRA and MRv2 YARN Good knowledge of NoSQL databases such as HBase MongoDB and Cassandra Experience in working with Eclipse IDE Net Beans and Rational Application Developer Experience in using PLSQL to write Stored Procedures Functions and Triggers Expertise in developing a simple web based application using J2EE technologies like JSP Servlets and JDBC Experience working on EC2 Elastic Compute Cloud cluster instances setup data buckets on S3 Simple Storage Service set EMR Elastic MapReduce Hands on experience in installing configuring and using Apache Hadoop ecosystem components like Hadoop Distributed File System HDFS MapReduce Pig Hive HBase Apache Crunch Zookeeper Scoop Hue Scala AVRO Strong Programming Skills in designing and implementing of multitier applications using Java J2EE JDBC JSP JSTL HTML CSS JSF Struts JavaScript JAXB Extensive experience in SOAbased solutions Web Services Web API WCF SOAP including Restful APIs services Good Knowledge in Amazon Web Service AWS concepts like EMR and EC2 web services which provides fast and efficient processing of Teradata Big Data Analytics Experience in collection of Log Data and JSON data into HDFS using Flume and processed the data using HivePig Work Experience Sr Big Data Developer Wells Fargo Charlotte NC January 2018 to Present Responsibilities As a Big Data Developer I worked on Hadoop ecosystems including Hive HBase Oozie Pig Zookeeper Spark Streaming MCS MapR Control System and so on with MapR distribution Installed and configured Hadoop MapReduce HDFS Developed multiple MapReduce jobs in Java for data cleaning and Preprocessing Primarily involved in Data Migration process using Azure by integrating with Github repository and Jenkins Built code for real time data ingestion using Java Map RStreams Kafka and STORM Involved in various phases of development analysed and developed the system going through Agile Scrum methodology Worked on Apache Solr which is used as indexing and search engine Involved in development of Hadoop System and improving multinode Hadoop Cluster performance Worked on analysing Hadoop stack and different Big data tools including Pig and Hive HBase database and Sqoop Developed data pipeline using flume Sqoop and pig to extract the data from weblogs and store in HDFS Used Hive to analyze the partitioned and bucketed data and compute various metrics for reporting Worked with different data sources like Avro data files XML files JSON files SQL server and Oracle to load data into Hive tables Used J2EE design patterns like Factory pattern Singleton Pattern Used Spark to create the structured data from large amount of unstructured data from various sources Implemented usage of Amazon EMR for processing Big Data across Hadoop Cluster of virtual servers on Amazon Elastic Compute Cloud EC2 and Amazon Simple Storage Service S3 Performed transformations cleaning and filtering on imported data using Hive MapReduce Impala and loaded final data into HDFS Developed Python scripts to find vulnerabilities with SQL Queries by doing SQL injection Experienced in designing and developing POCs in Spark using Scala to compare the performance of Spark with Hive and SQL Responsible for coding MapReduce program Hive queries testing and debugging the MapReduce programs Extracted Real time feed using Spark streaming and convert it to RDD and process data into Data Frame and load the data into Cassandra Involved in the process of data acquisition data preprocessing and data exploration of telecommunication project in Scala Implemented a distributed messaging queue to integrate with Cassandra using Apache Kafka and Zookeeper Specified the cluster size allocating Resource pool Distribution of Hadoop by writing the specification texts in JSON File format Imported weblogs unstructured data using the Apache Flume and stores the data in Flume channel Exported event weblogs to HDFS by creating a HDFS sink which directly deposits the weblogs in HDFS Used RESTful web services with MVC for parsing and processing XML data Utilized XML and XSL Transformation for dynamic webcontent and database connectivity Involved in loading data from UNIX file system to HDFS Involved in designing schema writing CQLs and loading data using Cassandra Built the automated build and deployment framework using Jenkins Maven etc Environment Hadoop 30 Hive 23 HBase 12 Oozie Pig 017 Zookeeper Spark MapReduce Azure Java Agile J2EE Cassandra 311 Jenkins Maven Big DataHadoop Developer Caterpillar Peoria IL March 2015 to December 2017 Responsibilities Responsible for installation and configuration of Hive Pig HBase and Sqoop on the Hadoop cluster and created hive tables to store the processed results in a tabular format Involved in Agile methodologies daily scrum meetings spring planning Integrate visualizations into a Spark application using Data bricks and popular visualization libraries ggplot matplotlib Involved in all phases of Software Development Life Cycle SDLC and Worked on all activities related to the development implementation and support for Hadoop Configured Spark Streaming to receive real time data from the Apache Kafka and store the stream data to HDFS using Scala Developed the Sqoop scripts to make the interaction between Hive and vertica Database Processed data into HDFS by developing solutions and analyzed the data using MapReduce PIG and Hive to produce summary results from Hadoop to downstream systems Streamed AWS log group into Lambda function to create service now incident Involved in loading and transforming large sets of Structured SemiStructured and Unstructured data and analyzed them by running Hive queries and Pig scripts Created Managed tables and External tables in Hive and loaded data from HDFS Developed Spark code by using Scala and SparkSQL for faster processing and testing and performed complex HiveQL queries on Hive tables Scheduled several times based Oozie workflow by developing Python scripts Exporting the data using Sqoop to RDBMS servers and processed that data for ETL operations Worked on S3 buckets on AWS to store Cloud Formation Templates and worked on AWS to create EC2 instances Designing ETL Data Pipeline flow to ingest the data from RDBMS source to Hadoop using shell script sqoop package and MySQL Endtoend architecture and implementation of clientserver systems using Scala Akka Java JavaScript and related Linux Optimized the Hive tables using optimization techniques like partitions and bucketing to provide better Used Oozie workflow engine to manage interdependent Hadoop jobs and to automate several types of Hadoop jobs such as Java mapreduce Hive Pig and Sqoop Implementing Hadoop with the AWS EC2 system using a few instances in gathering and analyzing data log files Involved in Spark and Spark Streaming creating RDDs applying operations Transformation and Actions Created partitioned tables and loaded data using both static partition and dynamic partition method Developed custom Apache Spark programs in Scala to analyze and transform unstructured data Using Kafka on publishsubscribe messaging as a distributed commit log have experienced in its fast scalable and durability Scheduled map reduces jobs in production environment using Oozie scheduler Involved in Cluster maintenance Cluster Monitoring and Troubleshooting Manage and review data backups and log files Designed and implemented MapReduce jobs to support distributed processing using java Hive and Apache Pig Analyzing Hadoop cluster and different Big Data analytic tools including Pig Hive HBase and Sqoop Improved the Performance by tuning of Hive and MapReduce Implemented POC to migrate MapReduce jobs into Spark RDD transformations using Scala Environment Hive 23 Pig 017 HBase 12 Sqoop Hadoop 30 Agile Spark Scala AWS JavaScript Hadoop Developer Facebook Menlo Park CA October 2013 to February 2015 Responsibilities Developed Spark code and SparkSQLStreaming for faster testing and processing of data using Lambda Architecture Experience in deploying data from various sources into HDFS and building reports using Tableau Developed a data pipeline using Kafka and Strom to store data into HDFS Developed REST APIs using Scala and Play framework to retrieve processed data from Cassandra database Reengineered ntiered architecture involving technologies like EJB XML and Java into distributed applications Explored the possibilities of using technologies like JMX for better monitoring of the system Configured deployed and maintained multinode Dev and Test Kafka Clusters Performed transformations cleaning and filtering on imported data using Hive MapReduce and loaded final data into HDFS Load the data into Spark RDD and performed inmemory data computation to generate the output response Responsible for Cluster Maintenance Monitoring Managing Commissioning and decommissioning Data nodes Troubleshooting and review data backups Manage review log files for Hortonworks Installed Oozie workflow engine to run multiple Hive and Pig jobs Building and maintaining scalable data pipelines using the Hadoop ecosystem and other open source components like Hive and HBase Installed and configured HA of Hue to point Hadoop Cluster in cloudera Manager Installed and configured MapReduce HDFS and developed multiple MapReduce jobs in java for data cleaning and preprocessing Working with applications teams to install operating system Hadoop updates patches version upgrades as required Responsible for developing data pipeline using HDInsight Flume Sqoop and Pig to extract the data from weblogs and store in HDFS Performed transformations cleaning and filtering on imported data using Hive MapReduce and loaded final data into HDFS Developed and maintained the continuous integration and deployment systems using Jenkins ANT Akka and MAVEN Effectively used GIT version control to collaborate with the Akka team members Developed HDFS with huge amounts of data using Apache Kafka Collected the log data from web servers and integrated into HDFS using Flume Environment Spark HDFS Kafka Scala Cassandra Java Hive MapReduce Hortonworks Oozie Hadoop HBase Flume Sqoop Pig MAVEN ANT JavaJ2ee Developer Key Bank San Francisco CA June 2012 to September 2013 Responsibilities Worked closely with the Requirements team and analyzed the Use cases Elaborated on the Use cases based on business requirements and was responsible for creation of class diagrams sequence diagrams Adopted J2EE best Practices using Core J2EE patterns Developed in Eclipse environment using Struts based MVC framework Designed and developed presentation layer using JSP HTML and JavaScript Created JSPs using JSTL and Spring tag libraries Developed Struts Action and Action Form classes Deployed J2EE components EJB Servlets in Tomcat Application server Involved in understanding business requirements and provide technical designs and necessary documentation Implemented Microservices architecture to make application smaller and independent Developed new Spring Boot application with microservices and added functionality to existing applications using Java J2EE technologies Used Hibernate as Object relational mapping tool for mapping Java Objects to database tables Used Hibernate Query Language HQL annotations and Criteria for access and updating data Implemented REST Web Services and performed the HTTP operations Implemented multithreading to process multiple tasks concurrently in order to perform the readwrite operations Worked extensively with Core Java concepts like Collections Exception Handling Java IO and Generics to implement business logic Developed web pages using with HTML CSS JavaScript jQuery and Ajax Used JavaScript for clientside validations in the JSP and HTML pages Created SQL queries and stored procedures to create retrieve and update data from database Developed Maven Scripts to build and deploy EAR files GitHub was used for the version control and source code management Followed Test Driven Development TDD responsible for testing debugging and bug fixing of the application Used log4j to capture the logs that included runtime exceptions and debug information The application was deployed in LINUX environment Environment J2EE MVC HTML JavaScript Java Hibernate jQuery Ajax Maven Java Developer Commvault Hyderabad Telangana March 2010 to May 2012 Responsibilities Involved in the complete Software Development Life Cycle including Requirement Analysis Design Implementation Testing and Maintenance Designed the front end using JSP jQuery CSS and HTML as per the requirements that are provided Developed Responsive web application for the backend system using AngularJS with HTML and CSS Used Spring AOP to enable the log interfaces and crosscutting concerns Developed payment flow using AJAX partial page refresh validation and dynamic dropdown list Responsible for use case diagrams class diagrams and sequence diagrams using Rational Rose in the Design phase Used Spring Core for dependency injectionInversion of Control IOC to have loosecoupling Implemented application using MVC architecture integrating Hibernate and spring frameworks Implemented the Enterprise JavaBeans EJBs to handle various transactions and incorporated the validation framework for the project Designed and developed all UI Screens using Java Server Pages JSP Static Content HTML CSS and JavaScript Worked on serverside web applications using Nodejs and involved in Construction of UI using jQuery Bootstrap and JavaScript Developed Database access components using Spring DAO integrated with Hibernate for accessing the data Integrate the dynamic pages with AngularJS to make the pages dynamic Developed Custom Tags and JSTL to support custom user interfaces Used CSS style sheets for presenting data from XML documents and data from databases to render on HTML web pages Used Spring Framework for integrating Hibernate for dependency injection Extensively used Eclipse for writing code Used Maven as a build tool and deployed on WebSphere Application Server Developed test cases on JUnit Used Log4J for logging and tracing the messages Environment HTML CSS JavaScript jQuery AngularJS AJAX Bootstrap XML Maven Eclipse JUnit Education Bachelors in Computer Science Engineering Jawaharlal Nehru Technological University Skills Cassandra Hdfs Impala Mapreduce Oozie Sqoop Hbase Kafka Flume Hadoop Jboss Jms Mongodb Nosql Analysis services Application server Git Hadoop Hbase Hive Additional Information Technical Skills HadoopBig Data Technologies Hadoop 30 HDFS MapReduce HBase 14 Apache Pig 017 Hive 23 Sqoop 14 Apache Impala 21 Oozie 43 Yarn Apache Flume 18 Kafka 11 Zookeeper Cloud Platform Amazon AWS EC2 EC3 MS Azure Azure SQL Database Azure SQL Data Warehouse Azure Analysis Services HDInsight Azure Data Lake Data Factory Hadoop Distributions Cloudera Hortonworks MapR Programming Language Java Scala Python 36 SQL PLSQL Shell Scripting Storm 10 JSP Servlets Frameworks Spring 505 Hibernate 52 Struts 13 JSF EJB JMS Web Technologies HTML CSS JavaScript JQuery 33 Bootstrap 41 XML JSON AJAX Operating Systems Linux Unix Windows 1087 IDE and Tools Eclipse 47 NetBeans 82 IntelliJ Maven NoSQL Databases HBase 14 Cassandra 311 MongoDB WebApplication Server Apache Tomcat 907 JBoss Web Logic Web Sphere SDLC Methodologies Agile Waterfall Version Control GIT SVN CVS",
    "extracted_keywords": [
        "Sr",
        "Big",
        "Data",
        "Developer",
        "Sr",
        "Big",
        "Data",
        "span",
        "lDeveloperspan",
        "Sr",
        "Big",
        "Data",
        "Developer",
        "Charlotte",
        "NC",
        "years",
        "working",
        "experience",
        "Sr",
        "Big",
        "DataHadoop",
        "Developer",
        "applications",
        "data",
        "Hadoop",
        "JavaJ2EE",
        "opensource",
        "technologies",
        "Core",
        "Java",
        "Struts2",
        "JSF",
        "Hibernate",
        "Servlets",
        "JSP",
        "Handson",
        "experience",
        "PLSQL",
        "XML",
        "verse",
        "Relational",
        "Database",
        "Management",
        "Systems",
        "Oracle",
        "MS",
        "SQL",
        "MySQL",
        "Server",
        "Hands",
        "experience",
        "Hadoop",
        "Big",
        "Data",
        "technology",
        "experience",
        "Storage",
        "Querying",
        "Processing",
        "analysis",
        "data",
        "Hands",
        "experience",
        "XML",
        "suite",
        "technologies",
        "XML",
        "XSL",
        "XSLT",
        "DTD",
        "XML",
        "Schema",
        "SAX",
        "DOM",
        "JAXB",
        "Experience",
        "Web",
        "Servers",
        "Apache",
        "Tomcat",
        "Application",
        "Servers",
        "IBM",
        "Web",
        "Sphere",
        "JBOSS",
        "Hands",
        "experience",
        "BigData",
        "technologies",
        "Spark",
        "Ecosystem",
        "Spark",
        "SQL",
        "MLlib",
        "Spark",
        "R",
        "Spark",
        "Streaming",
        "Kafka",
        "Predictive",
        "analytics",
        "Knowledge",
        "software",
        "Development",
        "Life",
        "Cycle",
        "SDLC",
        "Agile",
        "Waterfall",
        "Methodologies",
        "Experience",
        "applications",
        "Java",
        "python",
        "UNIX",
        "shell",
        "scripting",
        "Experience",
        "Web",
        "services",
        "Apache",
        "Axis",
        "JAXRSREST",
        "APIs",
        "building",
        "tool",
        "Maven",
        "ANT",
        "tool",
        "Log4J",
        "Experience",
        "Programming",
        "Development",
        "modules",
        "web",
        "portal",
        "Java",
        "technologies",
        "JSP",
        "Servlets",
        "JavaScript",
        "HTML",
        "SOA",
        "MVC",
        "architecture",
        "Expertise",
        "time",
        "data",
        "Flume",
        "Kafka",
        "Storm",
        "knowledge",
        "SQL",
        "Mongo",
        "DB",
        "Cassandra",
        "HBase",
        "Strong",
        "development",
        "skills",
        "Hadoop",
        "HDFS",
        "Map",
        "Reduce",
        "Hive",
        "Sqoop",
        "HBase",
        "understanding",
        "Hadoop",
        "internals",
        "knowledge",
        "Hadoop",
        "Architecture",
        "components",
        "HDFS",
        "Job",
        "Tracker",
        "Task",
        "Tracker",
        "Name",
        "Node",
        "Data",
        "Node",
        "MRA",
        "YARN",
        "knowledge",
        "NoSQL",
        "HBase",
        "MongoDB",
        "Cassandra",
        "Experience",
        "Eclipse",
        "IDE",
        "Net",
        "Beans",
        "Rational",
        "Application",
        "Developer",
        "Experience",
        "PLSQL",
        "Stored",
        "Procedures",
        "Functions",
        "Triggers",
        "Expertise",
        "web",
        "application",
        "J2EE",
        "technologies",
        "JSP",
        "Servlets",
        "JDBC",
        "Experience",
        "EC2",
        "Elastic",
        "Compute",
        "Cloud",
        "cluster",
        "instances",
        "data",
        "buckets",
        "S3",
        "Simple",
        "Storage",
        "Service",
        "EMR",
        "Elastic",
        "MapReduce",
        "Hands",
        "experience",
        "configuring",
        "Apache",
        "Hadoop",
        "ecosystem",
        "components",
        "Hadoop",
        "File",
        "System",
        "HDFS",
        "MapReduce",
        "Pig",
        "Hive",
        "HBase",
        "Apache",
        "Crunch",
        "Zookeeper",
        "Scoop",
        "Hue",
        "Scala",
        "AVRO",
        "Strong",
        "Programming",
        "Skills",
        "applications",
        "Java",
        "J2EE",
        "JDBC",
        "JSP",
        "JSTL",
        "HTML",
        "CSS",
        "JSF",
        "Struts",
        "JavaScript",
        "experience",
        "solutions",
        "Web",
        "Services",
        "Web",
        "API",
        "WCF",
        "SOAP",
        "APIs",
        "services",
        "Good",
        "Knowledge",
        "Amazon",
        "Web",
        "Service",
        "AWS",
        "concepts",
        "EMR",
        "EC2",
        "web",
        "services",
        "processing",
        "Teradata",
        "Big",
        "Data",
        "Analytics",
        "Experience",
        "collection",
        "Log",
        "Data",
        "data",
        "HDFS",
        "Flume",
        "data",
        "HivePig",
        "Work",
        "Experience",
        "Sr",
        "Big",
        "Data",
        "Developer",
        "Wells",
        "Fargo",
        "Charlotte",
        "NC",
        "January",
        "Present",
        "Responsibilities",
        "Big",
        "Data",
        "Developer",
        "Hadoop",
        "ecosystems",
        "Hive",
        "HBase",
        "Oozie",
        "Pig",
        "Zookeeper",
        "Spark",
        "Streaming",
        "MCS",
        "MapR",
        "Control",
        "System",
        "MapR",
        "distribution",
        "Hadoop",
        "MapReduce",
        "MapReduce",
        "jobs",
        "Java",
        "data",
        "cleaning",
        "Preprocessing",
        "Data",
        "Migration",
        "process",
        "Azure",
        "Github",
        "repository",
        "Jenkins",
        "code",
        "time",
        "data",
        "ingestion",
        "Java",
        "Map",
        "RStreams",
        "Kafka",
        "STORM",
        "phases",
        "development",
        "system",
        "Agile",
        "Scrum",
        "methodology",
        "Apache",
        "Solr",
        "indexing",
        "search",
        "engine",
        "development",
        "Hadoop",
        "System",
        "multinode",
        "Hadoop",
        "Cluster",
        "performance",
        "Hadoop",
        "stack",
        "data",
        "tools",
        "Pig",
        "Hive",
        "HBase",
        "database",
        "Sqoop",
        "Developed",
        "data",
        "pipeline",
        "flume",
        "Sqoop",
        "pig",
        "data",
        "weblogs",
        "HDFS",
        "Hive",
        "data",
        "metrics",
        "data",
        "sources",
        "Avro",
        "data",
        "XML",
        "files",
        "JSON",
        "files",
        "SQL",
        "server",
        "Oracle",
        "data",
        "Hive",
        "tables",
        "J2EE",
        "design",
        "patterns",
        "Factory",
        "pattern",
        "Singleton",
        "Pattern",
        "Spark",
        "data",
        "amount",
        "data",
        "sources",
        "usage",
        "Amazon",
        "EMR",
        "Big",
        "Data",
        "Hadoop",
        "Cluster",
        "servers",
        "Amazon",
        "Elastic",
        "Compute",
        "Cloud",
        "EC2",
        "Amazon",
        "Simple",
        "Storage",
        "Service",
        "S3",
        "Performed",
        "transformations",
        "filtering",
        "data",
        "Hive",
        "MapReduce",
        "Impala",
        "data",
        "HDFS",
        "Python",
        "scripts",
        "vulnerabilities",
        "SQL",
        "Queries",
        "SQL",
        "injection",
        "POCs",
        "Spark",
        "Scala",
        "performance",
        "Spark",
        "Hive",
        "SQL",
        "Responsible",
        "MapReduce",
        "program",
        "Hive",
        "MapReduce",
        "programs",
        "time",
        "feed",
        "Spark",
        "streaming",
        "process",
        "data",
        "Data",
        "Frame",
        "data",
        "Cassandra",
        "process",
        "data",
        "acquisition",
        "data",
        "data",
        "exploration",
        "telecommunication",
        "project",
        "Scala",
        "queue",
        "Cassandra",
        "Apache",
        "Kafka",
        "Zookeeper",
        "cluster",
        "size",
        "Resource",
        "pool",
        "Distribution",
        "Hadoop",
        "specification",
        "texts",
        "JSON",
        "File",
        "format",
        "Imported",
        "data",
        "Apache",
        "Flume",
        "data",
        "Flume",
        "channel",
        "event",
        "HDFS",
        "HDFS",
        "sink",
        "weblogs",
        "HDFS",
        "web",
        "services",
        "MVC",
        "XML",
        "data",
        "XML",
        "XSL",
        "Transformation",
        "webcontent",
        "database",
        "connectivity",
        "loading",
        "data",
        "UNIX",
        "file",
        "system",
        "HDFS",
        "schema",
        "CQLs",
        "loading",
        "data",
        "Cassandra",
        "build",
        "deployment",
        "framework",
        "Jenkins",
        "Maven",
        "Environment",
        "Hadoop",
        "Hive",
        "HBase",
        "Oozie",
        "Pig",
        "Zookeeper",
        "Spark",
        "MapReduce",
        "Azure",
        "Java",
        "Agile",
        "J2EE",
        "Cassandra",
        "Jenkins",
        "Maven",
        "Big",
        "DataHadoop",
        "Developer",
        "Caterpillar",
        "Peoria",
        "IL",
        "March",
        "December",
        "Responsibilities",
        "installation",
        "configuration",
        "Hive",
        "Pig",
        "HBase",
        "Sqoop",
        "Hadoop",
        "cluster",
        "tables",
        "results",
        "format",
        "methodologies",
        "meetings",
        "spring",
        "Integrate",
        "visualizations",
        "Spark",
        "application",
        "Data",
        "bricks",
        "visualization",
        "libraries",
        "ggplot",
        "matplotlib",
        "phases",
        "Software",
        "Development",
        "Life",
        "Cycle",
        "SDLC",
        "activities",
        "development",
        "implementation",
        "support",
        "Hadoop",
        "Configured",
        "Spark",
        "Streaming",
        "time",
        "data",
        "Apache",
        "Kafka",
        "stream",
        "data",
        "HDFS",
        "Scala",
        "Sqoop",
        "scripts",
        "interaction",
        "Hive",
        "vertica",
        "Database",
        "data",
        "HDFS",
        "solutions",
        "data",
        "MapReduce",
        "PIG",
        "Hive",
        "summary",
        "results",
        "Hadoop",
        "systems",
        "Streamed",
        "AWS",
        "group",
        "Lambda",
        "function",
        "service",
        "incident",
        "loading",
        "sets",
        "Structured",
        "SemiStructured",
        "Unstructured",
        "data",
        "Hive",
        "queries",
        "Pig",
        "scripts",
        "Managed",
        "tables",
        "tables",
        "Hive",
        "data",
        "HDFS",
        "Developed",
        "Spark",
        "code",
        "Scala",
        "SparkSQL",
        "processing",
        "testing",
        "HiveQL",
        "queries",
        "Hive",
        "tables",
        "times",
        "Oozie",
        "workflow",
        "Python",
        "scripts",
        "data",
        "Sqoop",
        "servers",
        "data",
        "ETL",
        "operations",
        "S3",
        "buckets",
        "AWS",
        "Cloud",
        "Formation",
        "Templates",
        "AWS",
        "EC2",
        "instances",
        "ETL",
        "Data",
        "Pipeline",
        "flow",
        "data",
        "source",
        "Hadoop",
        "shell",
        "script",
        "sqoop",
        "package",
        "MySQL",
        "Endtoend",
        "architecture",
        "implementation",
        "clientserver",
        "systems",
        "Scala",
        "Akka",
        "Java",
        "JavaScript",
        "Linux",
        "Hive",
        "tables",
        "optimization",
        "techniques",
        "partitions",
        "bucketing",
        "Oozie",
        "workflow",
        "engine",
        "Hadoop",
        "jobs",
        "types",
        "Hadoop",
        "jobs",
        "Java",
        "mapreduce",
        "Hive",
        "Pig",
        "Sqoop",
        "Implementing",
        "Hadoop",
        "AWS",
        "EC2",
        "system",
        "instances",
        "data",
        "log",
        "files",
        "Spark",
        "Spark",
        "Streaming",
        "RDDs",
        "operations",
        "Transformation",
        "Actions",
        "tables",
        "data",
        "partition",
        "partition",
        "method",
        "custom",
        "Apache",
        "Spark",
        "programs",
        "Scala",
        "data",
        "Kafka",
        "publishsubscribe",
        "commit",
        "log",
        "durability",
        "map",
        "jobs",
        "production",
        "environment",
        "Oozie",
        "scheduler",
        "Cluster",
        "maintenance",
        "Cluster",
        "Monitoring",
        "Troubleshooting",
        "Manage",
        "data",
        "backups",
        "files",
        "MapReduce",
        "jobs",
        "processing",
        "Hive",
        "Apache",
        "Pig",
        "Analyzing",
        "Hadoop",
        "cluster",
        "Big",
        "Data",
        "tools",
        "Pig",
        "Hive",
        "HBase",
        "Sqoop",
        "Performance",
        "Hive",
        "MapReduce",
        "POC",
        "MapReduce",
        "jobs",
        "Spark",
        "RDD",
        "transformations",
        "Scala",
        "Environment",
        "Hive",
        "Pig",
        "HBase",
        "Sqoop",
        "Hadoop",
        "Agile",
        "Spark",
        "Scala",
        "JavaScript",
        "Hadoop",
        "Developer",
        "Facebook",
        "Menlo",
        "Park",
        "CA",
        "October",
        "February",
        "Responsibilities",
        "Spark",
        "code",
        "SparkSQLStreaming",
        "testing",
        "processing",
        "data",
        "Lambda",
        "Architecture",
        "Experience",
        "data",
        "sources",
        "HDFS",
        "building",
        "reports",
        "Tableau",
        "data",
        "pipeline",
        "Kafka",
        "Strom",
        "data",
        "HDFS",
        "REST",
        "APIs",
        "Scala",
        "framework",
        "data",
        "Cassandra",
        "database",
        "architecture",
        "technologies",
        "EJB",
        "XML",
        "Java",
        "applications",
        "possibilities",
        "technologies",
        "JMX",
        "monitoring",
        "system",
        "multinode",
        "Dev",
        "Test",
        "Kafka",
        "Clusters",
        "transformations",
        "filtering",
        "data",
        "Hive",
        "MapReduce",
        "data",
        "HDFS",
        "Load",
        "data",
        "Spark",
        "RDD",
        "data",
        "computation",
        "output",
        "response",
        "Cluster",
        "Maintenance",
        "Monitoring",
        "Managing",
        "Commissioning",
        "Data",
        "nodes",
        "Troubleshooting",
        "data",
        "backups",
        "Manage",
        "review",
        "files",
        "Hortonworks",
        "Installed",
        "Oozie",
        "workflow",
        "engine",
        "Hive",
        "Pig",
        "jobs",
        "Building",
        "data",
        "pipelines",
        "Hadoop",
        "ecosystem",
        "source",
        "components",
        "Hive",
        "HBase",
        "Installed",
        "HA",
        "Hue",
        "Hadoop",
        "Cluster",
        "cloudera",
        "Manager",
        "Installed",
        "MapReduce",
        "HDFS",
        "MapReduce",
        "jobs",
        "java",
        "data",
        "Working",
        "applications",
        "teams",
        "operating",
        "system",
        "Hadoop",
        "updates",
        "version",
        "upgrades",
        "data",
        "pipeline",
        "HDInsight",
        "Flume",
        "Sqoop",
        "Pig",
        "data",
        "weblogs",
        "HDFS",
        "Performed",
        "transformations",
        "filtering",
        "data",
        "Hive",
        "MapReduce",
        "data",
        "HDFS",
        "Developed",
        "integration",
        "deployment",
        "systems",
        "Jenkins",
        "ANT",
        "Akka",
        "MAVEN",
        "GIT",
        "version",
        "control",
        "Akka",
        "team",
        "members",
        "amounts",
        "data",
        "Apache",
        "Kafka",
        "log",
        "data",
        "web",
        "servers",
        "HDFS",
        "Flume",
        "Environment",
        "Spark",
        "HDFS",
        "Kafka",
        "Scala",
        "Cassandra",
        "Java",
        "Hive",
        "MapReduce",
        "Hortonworks",
        "Oozie",
        "Hadoop",
        "HBase",
        "Flume",
        "Sqoop",
        "Pig",
        "MAVEN",
        "ANT",
        "JavaJ2ee",
        "Developer",
        "Key",
        "Bank",
        "San",
        "Francisco",
        "CA",
        "June",
        "September",
        "Responsibilities",
        "Requirements",
        "team",
        "Use",
        "cases",
        "Use",
        "cases",
        "business",
        "requirements",
        "creation",
        "class",
        "diagrams",
        "sequence",
        "diagrams",
        "J2EE",
        "Practices",
        "Core",
        "J2EE",
        "patterns",
        "Eclipse",
        "environment",
        "Struts",
        "MVC",
        "framework",
        "presentation",
        "layer",
        "JSP",
        "HTML",
        "JavaScript",
        "JSPs",
        "JSTL",
        "Spring",
        "tag",
        "Struts",
        "Action",
        "Action",
        "Form",
        "classes",
        "J2EE",
        "components",
        "EJB",
        "Servlets",
        "Tomcat",
        "Application",
        "server",
        "business",
        "requirements",
        "designs",
        "documentation",
        "Microservices",
        "architecture",
        "application",
        "Spring",
        "Boot",
        "application",
        "microservices",
        "functionality",
        "applications",
        "Java",
        "J2EE",
        "technologies",
        "Hibernate",
        "Object",
        "mapping",
        "tool",
        "mapping",
        "Java",
        "Objects",
        "database",
        "tables",
        "Hibernate",
        "Query",
        "Language",
        "HQL",
        "annotations",
        "Criteria",
        "access",
        "data",
        "REST",
        "Web",
        "Services",
        "HTTP",
        "operations",
        "tasks",
        "order",
        "operations",
        "Core",
        "Java",
        "concepts",
        "Collections",
        "Exception",
        "Handling",
        "Java",
        "IO",
        "Generics",
        "business",
        "logic",
        "web",
        "pages",
        "HTML",
        "CSS",
        "JavaScript",
        "jQuery",
        "Ajax",
        "JavaScript",
        "validations",
        "JSP",
        "HTML",
        "pages",
        "SQL",
        "queries",
        "procedures",
        "retrieve",
        "data",
        "database",
        "Developed",
        "Maven",
        "Scripts",
        "EAR",
        "files",
        "GitHub",
        "version",
        "control",
        "source",
        "code",
        "management",
        "Test",
        "Driven",
        "Development",
        "TDD",
        "testing",
        "debugging",
        "bug",
        "fixing",
        "application",
        "log4j",
        "logs",
        "runtime",
        "exceptions",
        "information",
        "application",
        "LINUX",
        "environment",
        "Environment",
        "J2EE",
        "MVC",
        "HTML",
        "JavaScript",
        "Java",
        "Hibernate",
        "jQuery",
        "Ajax",
        "Maven",
        "Java",
        "Developer",
        "Commvault",
        "Hyderabad",
        "Telangana",
        "March",
        "May",
        "Responsibilities",
        "Software",
        "Development",
        "Life",
        "Cycle",
        "Requirement",
        "Analysis",
        "Design",
        "Implementation",
        "Testing",
        "Maintenance",
        "end",
        "JSP",
        "jQuery",
        "CSS",
        "HTML",
        "requirements",
        "Developed",
        "Responsive",
        "web",
        "application",
        "system",
        "HTML",
        "CSS",
        "Spring",
        "AOP",
        "log",
        "interfaces",
        "concerns",
        "payment",
        "flow",
        "AJAX",
        "page",
        "validation",
        "list",
        "use",
        "case",
        "diagrams",
        "class",
        "diagrams",
        "sequence",
        "diagrams",
        "Rational",
        "Rose",
        "Design",
        "phase",
        "Spring",
        "Core",
        "dependency",
        "injectionInversion",
        "Control",
        "IOC",
        "application",
        "MVC",
        "architecture",
        "Hibernate",
        "spring",
        "frameworks",
        "Enterprise",
        "JavaBeans",
        "EJBs",
        "transactions",
        "validation",
        "framework",
        "project",
        "UI",
        "Screens",
        "Java",
        "Server",
        "Pages",
        "JSP",
        "Static",
        "Content",
        "HTML",
        "CSS",
        "JavaScript",
        "serverside",
        "web",
        "applications",
        "Nodejs",
        "Construction",
        "UI",
        "jQuery",
        "Bootstrap",
        "JavaScript",
        "Developed",
        "Database",
        "access",
        "components",
        "Spring",
        "DAO",
        "Hibernate",
        "data",
        "pages",
        "pages",
        "Custom",
        "Tags",
        "JSTL",
        "custom",
        "user",
        "CSS",
        "style",
        "sheets",
        "data",
        "XML",
        "documents",
        "data",
        "databases",
        "HTML",
        "web",
        "pages",
        "Spring",
        "Framework",
        "Hibernate",
        "dependency",
        "injection",
        "Eclipse",
        "code",
        "Maven",
        "build",
        "tool",
        "WebSphere",
        "Application",
        "Server",
        "test",
        "cases",
        "JUnit",
        "Log4J",
        "messages",
        "Environment",
        "HTML",
        "CSS",
        "JavaScript",
        "jQuery",
        "AngularJS",
        "AJAX",
        "Bootstrap",
        "XML",
        "Maven",
        "Eclipse",
        "JUnit",
        "Education",
        "Bachelors",
        "Computer",
        "Science",
        "Engineering",
        "Jawaharlal",
        "Nehru",
        "Technological",
        "University",
        "Skills",
        "Cassandra",
        "Hdfs",
        "Impala",
        "Mapreduce",
        "Oozie",
        "Sqoop",
        "Hbase",
        "Kafka",
        "Flume",
        "Hadoop",
        "Jboss",
        "Jms",
        "Mongodb",
        "Nosql",
        "Analysis",
        "services",
        "Application",
        "server",
        "Git",
        "Hadoop",
        "Hbase",
        "Hive",
        "Additional",
        "Information",
        "Technical",
        "Skills",
        "HadoopBig",
        "Data",
        "Technologies",
        "Hadoop",
        "HDFS",
        "MapReduce",
        "HBase",
        "Apache",
        "Pig",
        "Hive",
        "Sqoop",
        "Apache",
        "Impala",
        "Oozie",
        "Yarn",
        "Apache",
        "Flume",
        "Kafka",
        "Zookeeper",
        "Cloud",
        "Platform",
        "Amazon",
        "EC2",
        "EC3",
        "MS",
        "Azure",
        "Azure",
        "SQL",
        "Database",
        "Azure",
        "SQL",
        "Data",
        "Warehouse",
        "Azure",
        "Analysis",
        "Services",
        "HDInsight",
        "Azure",
        "Data",
        "Lake",
        "Data",
        "Factory",
        "Hadoop",
        "Distributions",
        "Cloudera",
        "Hortonworks",
        "MapR",
        "Programming",
        "Language",
        "Java",
        "Scala",
        "Python",
        "SQL",
        "PLSQL",
        "Shell",
        "Scripting",
        "Storm",
        "JSP",
        "Servlets",
        "Frameworks",
        "Spring",
        "Hibernate",
        "Struts",
        "JSF",
        "EJB",
        "JMS",
        "Web",
        "Technologies",
        "HTML",
        "CSS",
        "JavaScript",
        "JQuery",
        "Bootstrap",
        "XML",
        "JSON",
        "AJAX",
        "Operating",
        "Systems",
        "Linux",
        "Unix",
        "Windows",
        "IDE",
        "Tools",
        "Eclipse",
        "NetBeans",
        "IntelliJ",
        "Maven",
        "NoSQL",
        "Databases",
        "HBase",
        "Cassandra",
        "WebApplication",
        "Server",
        "Apache",
        "Tomcat",
        "JBoss",
        "Web",
        "Logic",
        "Web",
        "Sphere",
        "SDLC",
        "Methodologies",
        "Agile",
        "Waterfall",
        "Version",
        "Control",
        "GIT",
        "SVN",
        "CVS"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T19:37:36.424924",
    "resume_data": "Sr Big Data Developer Sr Big Data span lDeveloperspan Sr Big Data Developer Charlotte NC Above 9 years of working experience as a Sr Big DataHadoop Developer in designed and developed various applications like big data Hadoop JavaJ2EE opensource technologies Work Extensively in Core Java Struts2 JSF spring Hibernate Servlets JSP and Handson experience with PLSQL XML and SOAP Well verse working with Relational Database Management Systems as Oracle MS SQL MySQL Server Hands on experience on Hadoop Big Data related technology experience in Storage Querying Processing and analysis of data Hands on experience in working on XML suite of technologies like XML XSL XSLT DTD XML Schema SAX DOM JAXB Experience in working with Web Servers like Apache Tomcat and Application Servers like IBM Web Sphere and JBOSS Hands on experience in advanced BigData technologies like Spark Ecosystem Spark SQL MLlib Spark R and Spark Streaming Kafka and Predictive analytics Knowledge of the software Development Life Cycle SDLC Agile and Waterfall Methodologies Experience on applications using Java python and UNIX shell scripting Experience in consuming Web services with Apache Axis using JAXRSREST APIs Experienced in building tool Maven ANT and logging tool Log4J Experience in Programming and Development of java modules for an existing web portal based in Java using technologies like JSP Servlets JavaScript and HTML SOA with MVC architecture Expertise in ingesting real timenear real time data using Flume Kafka Storm Good knowledge of NO SQL databases like Mongo DB Cassandra and HBase Strong development skills in Hadoop HDFS Map Reduce Hive Sqoop HBase with solid understanding of Hadoop internals Excellent knowledge of Hadoop Architecture and various components such as HDFS Job Tracker Task Tracker Name Node Data Node and MRA and MRv2 YARN Good knowledge of NoSQL databases such as HBase MongoDB and Cassandra Experience in working with Eclipse IDE Net Beans and Rational Application Developer Experience in using PLSQL to write Stored Procedures Functions and Triggers Expertise in developing a simple web based application using J2EE technologies like JSP Servlets and JDBC Experience working on EC2 Elastic Compute Cloud cluster instances setup data buckets on S3 Simple Storage Service set EMR Elastic MapReduce Hands on experience in installing configuring and using Apache Hadoop ecosystem components like Hadoop Distributed File System HDFS MapReduce Pig Hive HBase Apache Crunch Zookeeper Scoop Hue Scala AVRO Strong Programming Skills in designing and implementing of multitier applications using Java J2EE JDBC JSP JSTL HTML CSS JSF Struts JavaScript JAXB Extensive experience in SOAbased solutions Web Services Web API WCF SOAP including Restful APIs services Good Knowledge in Amazon Web Service AWS concepts like EMR and EC2 web services which provides fast and efficient processing of Teradata Big Data Analytics Experience in collection of Log Data and JSON data into HDFS using Flume and processed the data using HivePig Work Experience Sr Big Data Developer Wells Fargo Charlotte NC January 2018 to Present Responsibilities As a Big Data Developer I worked on Hadoop ecosystems including Hive HBase Oozie Pig Zookeeper Spark Streaming MCS MapR Control System and so on with MapR distribution Installed and configured Hadoop MapReduce HDFS Developed multiple MapReduce jobs in Java for data cleaning and Preprocessing Primarily involved in Data Migration process using Azure by integrating with Github repository and Jenkins Built code for real time data ingestion using Java Map RStreams Kafka and STORM Involved in various phases of development analysed and developed the system going through Agile Scrum methodology Worked on Apache Solr which is used as indexing and search engine Involved in development of Hadoop System and improving multinode Hadoop Cluster performance Worked on analysing Hadoop stack and different Big data tools including Pig and Hive HBase database and Sqoop Developed data pipeline using flume Sqoop and pig to extract the data from weblogs and store in HDFS Used Hive to analyze the partitioned and bucketed data and compute various metrics for reporting Worked with different data sources like Avro data files XML files JSON files SQL server and Oracle to load data into Hive tables Used J2EE design patterns like Factory pattern Singleton Pattern Used Spark to create the structured data from large amount of unstructured data from various sources Implemented usage of Amazon EMR for processing Big Data across Hadoop Cluster of virtual servers on Amazon Elastic Compute Cloud EC2 and Amazon Simple Storage Service S3 Performed transformations cleaning and filtering on imported data using Hive MapReduce Impala and loaded final data into HDFS Developed Python scripts to find vulnerabilities with SQL Queries by doing SQL injection Experienced in designing and developing POCs in Spark using Scala to compare the performance of Spark with Hive and SQL Responsible for coding MapReduce program Hive queries testing and debugging the MapReduce programs Extracted Real time feed using Spark streaming and convert it to RDD and process data into Data Frame and load the data into Cassandra Involved in the process of data acquisition data preprocessing and data exploration of telecommunication project in Scala Implemented a distributed messaging queue to integrate with Cassandra using Apache Kafka and Zookeeper Specified the cluster size allocating Resource pool Distribution of Hadoop by writing the specification texts in JSON File format Imported weblogs unstructured data using the Apache Flume and stores the data in Flume channel Exported event weblogs to HDFS by creating a HDFS sink which directly deposits the weblogs in HDFS Used RESTful web services with MVC for parsing and processing XML data Utilized XML and XSL Transformation for dynamic webcontent and database connectivity Involved in loading data from UNIX file system to HDFS Involved in designing schema writing CQLs and loading data using Cassandra Built the automated build and deployment framework using Jenkins Maven etc Environment Hadoop 30 Hive 23 HBase 12 Oozie Pig 017 Zookeeper Spark MapReduce Azure Java Agile J2EE Cassandra 311 Jenkins Maven Big DataHadoop Developer Caterpillar Peoria IL March 2015 to December 2017 Responsibilities Responsible for installation and configuration of Hive Pig HBase and Sqoop on the Hadoop cluster and created hive tables to store the processed results in a tabular format Involved in Agile methodologies daily scrum meetings spring planning Integrate visualizations into a Spark application using Data bricks and popular visualization libraries ggplot matplotlib Involved in all phases of Software Development Life Cycle SDLC and Worked on all activities related to the development implementation and support for Hadoop Configured Spark Streaming to receive real time data from the Apache Kafka and store the stream data to HDFS using Scala Developed the Sqoop scripts to make the interaction between Hive and vertica Database Processed data into HDFS by developing solutions and analyzed the data using MapReduce PIG and Hive to produce summary results from Hadoop to downstream systems Streamed AWS log group into Lambda function to create service now incident Involved in loading and transforming large sets of Structured SemiStructured and Unstructured data and analyzed them by running Hive queries and Pig scripts Created Managed tables and External tables in Hive and loaded data from HDFS Developed Spark code by using Scala and SparkSQL for faster processing and testing and performed complex HiveQL queries on Hive tables Scheduled several times based Oozie workflow by developing Python scripts Exporting the data using Sqoop to RDBMS servers and processed that data for ETL operations Worked on S3 buckets on AWS to store Cloud Formation Templates and worked on AWS to create EC2 instances Designing ETL Data Pipeline flow to ingest the data from RDBMS source to Hadoop using shell script sqoop package and MySQL Endtoend architecture and implementation of clientserver systems using Scala Akka Java JavaScript and related Linux Optimized the Hive tables using optimization techniques like partitions and bucketing to provide better Used Oozie workflow engine to manage interdependent Hadoop jobs and to automate several types of Hadoop jobs such as Java mapreduce Hive Pig and Sqoop Implementing Hadoop with the AWS EC2 system using a few instances in gathering and analyzing data log files Involved in Spark and Spark Streaming creating RDDs applying operations Transformation and Actions Created partitioned tables and loaded data using both static partition and dynamic partition method Developed custom Apache Spark programs in Scala to analyze and transform unstructured data Using Kafka on publishsubscribe messaging as a distributed commit log have experienced in its fast scalable and durability Scheduled map reduces jobs in production environment using Oozie scheduler Involved in Cluster maintenance Cluster Monitoring and Troubleshooting Manage and review data backups and log files Designed and implemented MapReduce jobs to support distributed processing using java Hive and Apache Pig Analyzing Hadoop cluster and different Big Data analytic tools including Pig Hive HBase and Sqoop Improved the Performance by tuning of Hive and MapReduce Implemented POC to migrate MapReduce jobs into Spark RDD transformations using Scala Environment Hive 23 Pig 017 HBase 12 Sqoop Hadoop 30 Agile Spark Scala AWS JavaScript Hadoop Developer Facebook Menlo Park CA October 2013 to February 2015 Responsibilities Developed Spark code and SparkSQLStreaming for faster testing and processing of data using Lambda Architecture Experience in deploying data from various sources into HDFS and building reports using Tableau Developed a data pipeline using Kafka and Strom to store data into HDFS Developed REST APIs using Scala and Play framework to retrieve processed data from Cassandra database Reengineered ntiered architecture involving technologies like EJB XML and Java into distributed applications Explored the possibilities of using technologies like JMX for better monitoring of the system Configured deployed and maintained multinode Dev and Test Kafka Clusters Performed transformations cleaning and filtering on imported data using Hive MapReduce and loaded final data into HDFS Load the data into Spark RDD and performed inmemory data computation to generate the output response Responsible for Cluster Maintenance Monitoring Managing Commissioning and decommissioning Data nodes Troubleshooting and review data backups Manage review log files for Hortonworks Installed Oozie workflow engine to run multiple Hive and Pig jobs Building and maintaining scalable data pipelines using the Hadoop ecosystem and other open source components like Hive and HBase Installed and configured HA of Hue to point Hadoop Cluster in cloudera Manager Installed and configured MapReduce HDFS and developed multiple MapReduce jobs in java for data cleaning and preprocessing Working with applications teams to install operating system Hadoop updates patches version upgrades as required Responsible for developing data pipeline using HDInsight Flume Sqoop and Pig to extract the data from weblogs and store in HDFS Performed transformations cleaning and filtering on imported data using Hive MapReduce and loaded final data into HDFS Developed and maintained the continuous integration and deployment systems using Jenkins ANT Akka and MAVEN Effectively used GIT version control to collaborate with the Akka team members Developed HDFS with huge amounts of data using Apache Kafka Collected the log data from web servers and integrated into HDFS using Flume Environment Spark HDFS Kafka Scala Cassandra Java Hive MapReduce Hortonworks Oozie Hadoop HBase Flume Sqoop Pig MAVEN ANT JavaJ2ee Developer Key Bank San Francisco CA June 2012 to September 2013 Responsibilities Worked closely with the Requirements team and analyzed the Use cases Elaborated on the Use cases based on business requirements and was responsible for creation of class diagrams sequence diagrams Adopted J2EE best Practices using Core J2EE patterns Developed in Eclipse environment using Struts based MVC framework Designed and developed presentation layer using JSP HTML and JavaScript Created JSPs using JSTL and Spring tag libraries Developed Struts Action and Action Form classes Deployed J2EE components EJB Servlets in Tomcat Application server Involved in understanding business requirements and provide technical designs and necessary documentation Implemented Microservices architecture to make application smaller and independent Developed new Spring Boot application with microservices and added functionality to existing applications using Java J2EE technologies Used Hibernate as Object relational mapping tool for mapping Java Objects to database tables Used Hibernate Query Language HQL annotations and Criteria for access and updating data Implemented REST Web Services and performed the HTTP operations Implemented multithreading to process multiple tasks concurrently in order to perform the readwrite operations Worked extensively with Core Java concepts like Collections Exception Handling Java IO and Generics to implement business logic Developed web pages using with HTML CSS JavaScript jQuery and Ajax Used JavaScript for clientside validations in the JSP and HTML pages Created SQL queries and stored procedures to create retrieve and update data from database Developed Maven Scripts to build and deploy EAR files GitHub was used for the version control and source code management Followed Test Driven Development TDD responsible for testing debugging and bug fixing of the application Used log4j to capture the logs that included runtime exceptions and debug information The application was deployed in LINUX environment Environment J2EE MVC HTML JavaScript Java Hibernate jQuery Ajax Maven Java Developer Commvault Hyderabad Telangana March 2010 to May 2012 Responsibilities Involved in the complete Software Development Life Cycle including Requirement Analysis Design Implementation Testing and Maintenance Designed the front end using JSP jQuery CSS and HTML as per the requirements that are provided Developed Responsive web application for the backend system using AngularJS with HTML and CSS Used Spring AOP to enable the log interfaces and crosscutting concerns Developed payment flow using AJAX partial page refresh validation and dynamic dropdown list Responsible for use case diagrams class diagrams and sequence diagrams using Rational Rose in the Design phase Used Spring Core for dependency injectionInversion of Control IOC to have loosecoupling Implemented application using MVC architecture integrating Hibernate and spring frameworks Implemented the Enterprise JavaBeans EJBs to handle various transactions and incorporated the validation framework for the project Designed and developed all UI Screens using Java Server Pages JSP Static Content HTML CSS and JavaScript Worked on serverside web applications using Nodejs and involved in Construction of UI using jQuery Bootstrap and JavaScript Developed Database access components using Spring DAO integrated with Hibernate for accessing the data Integrate the dynamic pages with AngularJS to make the pages dynamic Developed Custom Tags and JSTL to support custom user interfaces Used CSS style sheets for presenting data from XML documents and data from databases to render on HTML web pages Used Spring Framework for integrating Hibernate for dependency injection Extensively used Eclipse for writing code Used Maven as a build tool and deployed on WebSphere Application Server Developed test cases on JUnit Used Log4J for logging and tracing the messages Environment HTML CSS JavaScript jQuery AngularJS AJAX Bootstrap XML Maven Eclipse JUnit Education Bachelors in Computer Science Engineering Jawaharlal Nehru Technological University Skills Cassandra Hdfs Impala Mapreduce Oozie Sqoop Hbase Kafka Flume Hadoop Jboss Jms Mongodb Nosql Analysis services Application server Git Hadoop Hbase Hive Additional Information Technical Skills HadoopBig Data Technologies Hadoop 30 HDFS MapReduce HBase 14 Apache Pig 017 Hive 23 Sqoop 14 Apache Impala 21 Oozie 43 Yarn Apache Flume 18 Kafka 11 Zookeeper Cloud Platform Amazon AWS EC2 EC3 MS Azure Azure SQL Database Azure SQL Data Warehouse Azure Analysis Services HDInsight Azure Data Lake Data Factory Hadoop Distributions Cloudera Hortonworks MapR Programming Language Java Scala Python 36 SQL PLSQL Shell Scripting Storm 10 JSP Servlets Frameworks Spring 505 Hibernate 52 Struts 13 JSF EJB JMS Web Technologies HTML CSS JavaScript JQuery 33 Bootstrap 41 XML JSON AJAX Operating Systems Linux Unix Windows 1087 IDE and Tools Eclipse 47 NetBeans 82 IntelliJ Maven NoSQL Databases HBase 14 Cassandra 311 MongoDB WebApplication Server Apache Tomcat 907 JBoss Web Logic Web Sphere SDLC Methodologies Agile Waterfall Version Control GIT SVN CVS",
    "unique_id": "213f0985-b6c2-4fcb-8686-88cfb11b5a3b"
}