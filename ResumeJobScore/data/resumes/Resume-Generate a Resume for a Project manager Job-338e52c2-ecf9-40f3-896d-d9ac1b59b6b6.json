{
    "clean_data": "Data Scientist Big Data Data Scientist Big Data Data Scientist Big Data Cigna Health Insurance Bloomfield CT Professional qualified Data ScientistData Analyst with over 9 years of experience in Data Science and Analytics including Artificial IntelligenceDeep LearningMachine Learning Data Mining and Statistical Analysis Involved in the entire data science project life cycle and actively involved in all the phases including data extraction data cleaning statistical modeling and data visualization with large data sets of structured and unstructured data created ER diagrams and schema Experienced with machine learning algorithm such as logistic regression random forest XGboost KNN SVM neural network linear regression lasso regression and kmeans Implemented Bagging and Boosting to enhance the model performance Strong skills in statistical methodologies such as AB test experiment design hypothesis test ANOVA Experience in implementing data analysis with various analytic tools such as Anaconda 40 Jupiter Notebook 4X R 30 ggplot2 Caret dplyr and Excel Solid ability to write and optimize diverse SQL queries working knowledge of RDBMS like SQL Server 2008 NoSql databases like MongoDB 32 Developed API libraries and coded business logic using C XML and designed web pages using NET framework C Python Django HTML AJAX Strong experience for over 5 years in Image Recognition and Big Data technologies like Spark 16 Sparksql pySpark Hadoop 2X HDFS Hive 1X Experience in visualization tools like Tableau 9X 10X for creating dashboards Excellent understanding Agile and Scrum development methodology Used the version control tools like Git 2X and build tools like Apache MavenAnt Passionate about gleaning insightful information from massive data assets and developing a culture of sound datadriven decision making Ability to maintain a fun casual professional and productive team atmosphere Experienced the full software life cycle in SDLC Agile Devops and Scrum methodologies including creating requirements test plans Skilled in Advanced Regression Modeling Correlation Multivariate Analysis Model Building Business Intelligence tools and application of Statistical Concepts Proficient in Predictive Modeling Data Mining Methods Factor Analysis ANOVA Hypothetical testing normal distribution and other advanced statistical and econometric techniques Developed predictive models using Decision Tree Random Forest Nave Bayes Logistic Regression Social Network Analysis Cluster Analysis and Neural Networks Experienced in Machine Learning and Statistical Analysis with Python ScikitLearn Experienced in Python to manipulate data for data loading and extraction and worked with python libraries like Matplotlib Numpy Scipy and Pandas for data analysis Worked with complex applications such as R Python Theano H20 SAS Matlab and SPSS to develop neural network cluster analysis Expertise in transforming business requirements into analytical models designing algorithms building models developing data mining and reporting solutions that scales across massive volume of structured and unstructured data Skilled in performing data parsing data ingestion data manipulationdata architecture data modelling and data preparation with methods including describe data contents compute descriptive statistics of data regex split and combine Remap merge subset reindex melt and reshape Strong C SQL programming skills with experience in working with functions packages and triggers Extensively worked on Python 3527 Numpy Pandas Matplotlib NLTK and Scikitlearn Experienced in Visual Basic for Applications and VB programming languages C NET framework to work with developing applications Worked with NoSQL Database including Hbase Cassandra and MongoDB Experienced in Big Data with Hadoop HDFS MapReduce and Spark Experienced in Data Integration Validation and Data Quality controls for ETL process and Data Warehousing using MS Visual Studio SSIS SSAS SSRS Proficient in TableauAdobe Analytics and RShiny data visualization tools to analyze and obtain insights into large datasets create visually powerful and actionable interactive reports and dashboards Automated recurring reports using SQL and Python and visualized them on BI platform like Tableau Worked in development environment like Git and VM Excellent communication skills Successfully working in fastpaced multitasking environment both independently and in collaborative team a selfmotivated enthusiastic learner TOOLS AND TECHNOLOGIES Languages Java 8 Python R C Powershell Packages ggplot2 caret dplyr Rweka gmodels Edward RCurl tm C50 twitteR NLP Reshape2 rjson plyr pandas numPy TensorFlow seaborn sciPy matplot lib scikitlearn Beautiful Soup Rpy2 Web Technologies JDBC HTML5 DHTML and XML CSS3 Web Services WSDL Django AWS Data Modelling Tools Erwin r 96 95 91 8x Rational Rose ERStudio MS Visio SAP Power designer Big Data Technologies Hadoop Hive HDFS Presto MapReduce Pig Kafka oozie Databases SQL Hive Impala Pig Spark SQL HQL VQL Databases SQLServer My SQL MS SQLMS Access HDFS HBase Teradata Netezza MongoDB Cassandra Reporting Tools MS Office WordExcelPower Point Visio Tableau Spotfire Crystal reports XI Business Intelligence SSRS Business Objects 5x 6x Cognos7060 ETL Tools Informatica Power Centre SSIS Version Control Tools SVM GitHub Project Execution Work Experience Data Scientist Big Data Cigna Health Insurance Bloomfield CT August 2017 to Present Description The Cigna Information Management Analytics CIMA unit offers solutions that provide actionable insights to internal and external business partners and customers that help reduce health costs improve outcomes provide financial security and measure and forecast business performance Responsibilities Several visualizations density plots forest plots leverage plots network plots covariant adjustment plots etc were made using packages such as GGPLOT2 GGMCMC Successfully delivered multiple NLP projects like building a chatbot that assists a customer to trouble shoot claim issues and recommend actionsFurther the bot could handle questions asked in natural language related to common issues with the customer eg when is my premium due what is my plan deductible what is my copay for a sick reject Extracted data from multiple sources like Medicare Medicaid ACA claims Performed data preprocessing like data cleaning text preprocessing noise removal lexicon normalization and object standardization Perform featuring engineering like Word Embedding using word2vec models Build seq2seqmodels using structured data word embedding Seq2Seq model take an input and returns as desired output for eg it can take a question as an input and returns an answer The benefit is it can take any arbitrary length question and returns and answers in natural language It uses a recurrent neural network LSTMMemory Network at the backend Performing Map Reduce jobs in Hadoop and implemented Spark analysis using Python for performing machine learning predictive analytics on AWS platform Analyzed administrative claims data Medicare and ACA Marketplace to answer health services research questions on costs utilization or outcomes using advanced statistical and econometric methods Used Tensorflow packages to train machine learning models Developed Oozie workflows to ingestparse the raw data populate staging tables and store the refined data in partitioned tables in the Hive Handon experience with data ingestion into Big Data platform from disparate data sources using Sqoop Hive Pig Flume and Spark Created an EndtoEnd data analytical solutions and models by manipulating large data sets and integrating diverse data sources Worked with team of developers to design develop and implement BI solutions in Tableau to measure Point of Sale KPIs at micro and macro level Environment Tableau Python PyCharm Statistics Machine Learning Tensorflow Alteryx Hadoop Hive Pig No SQL PLSQL Excel AWS Data Scientist Opera Solutions New Jersey January 2016 to July 2017 Description Opera Solutions LLC is a technology and analytics company mainly focused on big data The firm uses a combination of machine learning science advanced predictive analytics technology largescale data management and human expertise Opera Solutions delivers predictive analytics as a service and offers hosted cloudbased systems for specific business problems eg predicting the behavior of individual consumers stopping revenue leakage in hospitals warning of threats to corporate security or brand health etc Responsibilities Performed Data Profiling to learn about behavior with various features of USMLE examinations of various student patterns using Tableau Adobe Analytics and Python Matplotlib Evaluated models using Cross Validation Log loss function ROC curves and used AUC for feature selection and elastic technologies like ElasticSearch Kibana etc Addressed overfitting by implementing of the algorithm regularization methods like L2 and L1 Implemented statistical modeling with XGBoost machine learning software package using Python to determine the predicted probabilities of each model Created master data for modelling by combining various tables and derived fields from client data and students LORs essays and various performance metrics Formulated a basis for variable selection and GridSearch KFold for optimal hyperparameters Utilized Boosting algorithms to build a model for predictive analysis of students behaviour who took USMLE exam apply for residency Used numpy scipy pandas nltkNatural Language Processing Toolkitmatplotlib to build the model Formulated several graphs to show the performance of the students by demographics and their mean score in different USMLE exams Extracted data from HDFS using Hive Presto and performed data analysis using Spark with Scala pySpark Redshift and feature selection and created nonparametric models in Spark Application of various Artificial IntelligenceAImachine learning algorithms and statistical modeling like decision treestext analytics Image and Text Recognition using OCR tools like Abbyy natural language processingNLP supervised and unsupervised regression models Used Principal Component Analysis in feature engineering to analyze high dimensional data Performed Data Cleaning features scaling features engineering using pandas and numpy packages in python and build models using deep learning frameworks Created deep learning models using Tensorflow and keras by combining all tests as a single normalized score and predict residency attainment of students Used XGB classifier if the feature is an categorical variable and XGB regressor for continuous variables and combined it using FeatureUnion and FunctionTransfomer methods of Natural Language Processing Used OnevsRest Classifier to fit each classifier against all other classifiers and used it on multiclass classification problems Implemented application of various machine learning algorithms and statistical modeling like Decision Tree Text Analytics Sentiment Analysis Naive Bayes Logistic Regression and Linear Regression using Python to determine the accuracy rate of each model Created and designed reports that will use gathered metrics to infer and draw logical conclusions of past and future behavior with cloud based products like Azure ML Studio and Dataiku Generated various models by using different machine learning and deep learning frameworks and tuned the best performance model using Signal Hub and AWS SagemakerAzure Databricks Created data layers as signals to Signal Hub to predict new unseen data with performance not less than the static model build using deep learning framework Environment Python 2x3x Hive AWS Linux Tableau Desktop Microsoft Excel NLP Deep learning frameworks such as TensorFLow Keras Boosting algorithms etc Data Scientist Mercedes Benz Financial Services Michigan July 2014 to December 2015 Description MercedesBenz Financial Services is a leading captive financial services provider and the global financial services company of Daimler AG Doing business as MercedesBenz Financial Services and Daimler Truck Financial we provide financing for automotive and commercial vehicle dealers and their retail consumers in the United States Canada Mexico Brazil and Argentina Responsibilities Performed Data Profiling to learn about behavior with various features such as traffic pattern location time Date and Time etc using Adode Analytics Application of various Artificial IntelligenceAImachine learning algorithms and statistical modeling like decision treestext analytics natural language processingNLP supervised and unsupervised regression models social network analysis neural networks deep learning SVM clustering to identify Volume using scikitlearn package in python Matlab Utilized Spark Snowflake Presto Scala Hadoop HQL VQL oozie pySpark Data Lake TensorFlow HBase Cassandra Athena Redshift MongoDB Kafka Kinesis Spark Streaming Edward CUDA MLLib AWS Python a broad variety of machine learning methods including classifications regressions dimensionally reduction etc and Utilized the engine to increase user lifetime by 45 and triple user conversations for target categories Created and connected SQL engine through C to connect database developed API libraries and business logic using C XML and Python Exploring DAGs their dependencies and logs using AirFlow pipelines for automation Performed data cleaning and feature selection using MLlib package in PySpark and working with deep learning frameworks such as Caffe Neon etc Developed SparkScala PythonR for regular expression regex project in the HadoopHive environment with LinuxWindows for big data resources Used clustering technique KMeans to identify outliers and to classify unlabeled data Utilized AWS Lambda in created userfriendly interface for quick view of reports by using C JSP XML and developed expandable menu that show drilldown data on graph click Evaluated models using Cross Validation Log loss function ROC curves and used AUC for feature selection and elastic technologies like ElasticSearch Kibana etc Categorised comments into positive and negative clusters from different social networking sites using Sentiment Analysis and Text Analytics and have done Image Recognition Tracking operations using sensors until certain criteria is met using AirFlow technology Responsible for different Data mapping activities from Source systems to Teradata using utilities like TPump FEXP MLOAD BTEQ FLOAD etc Analyze traffic patterns by calculating autocorrelation with different time lags Ensured that the model has low False Positive Rate and Text classification and sentiment analysis for unstructured and semistructured data Addressed overfitting by implementing of the algorithm regularization methods like L2 and L1 Used Principal Component Analysis in feature engineering to analyze high dimensional data Created and designed reports that will use gathered metrics to infer and draw logical conclusions of past and future behavior Performed Multinomial Logistic Regression Random forest Decision Tree SVM to classify package is going to deliver on time for the new route Performed data analysis by using Hive to retrieve the data from Hadoop cluster Sql to retrieve data from Oracle database and used ETL for data transformation Used MLlib Sparks Machine learning library to build and evaluate different models and used AWS Rekognition for image analysis Implemented rule based expertise system from the results of exploratory analysis and information gathered from the people from different departments Performed Data Cleaning features scaling features engineering using pandas and numpy packages in python and build models using SAP Predictive Analytics Developed MapReduce pipeline for feature extraction using Hive and Pig Created Data Quality Scripts using SQL and Hive to validate successful data load and quality of the data Created various types of data visualizations using Python and TableauSpotfire Communicated the results with operations team for taking best decisions Collected data needs and requirements by Interacting with the other departments Environment Python 2x CDH5 HDFS C Hadoop 23 Hive Impala AWS Linux Spark Tableau Desktop SQL Server 2012 Microsoft Excel Matlab Spark SQL Pyspark Data Scientist First Data Atlanta GA January 2013 to June 2014 Description First Data Corporation is a global payment processing company headquartered in Atlanta Georgia United States The companys portfolio includes merchant transaction processing services credit debit privatelabel gift payroll and other prepaid card offerings fraud protection and authentication solutions Responsibilities Provided Configuration Management and Build support for more than 5 different applications built and deployed to the production and lower environments Implemented public segmentation using unsupervised machine learning algorithms by implementing kmeans algorithm using Pyspark Using AirFlow to keep track of job statuses in repositories like MySQl and Postgre databases Explored and Extracted data from source XML in HDFS used ETL for preparing data for exploratory analysis using data munging Responsible for different Data mapping activities from Source systems to Teradata Text mining and building models using topic analysis sentiment analysis for both semistructured and unstructured data Handled importing data from various data sources performed transformations using Hive Map Reduce and loaded data into HDFS Used R and python for Exploratory Data Analysis AB testing HQL VQL Data Lake AWS Redshift oozie pySpark Anova test and Hypothesis test to compare and identify the effectiveness of Creative Campaigns Computing AB testing frameworks clickstream and time spent databases using Airflow Created clusters to Control and test groups and conducted group campaigns using Text Analytics Created positive and negative clusters from merchants transaction using Sentiment Analysis to test the authenticity of transactions and resolve any chargebacks Analyzed and calculated the lifetime cost of everyone in the welfare system using 20 years of historical data Created and developed classes and web page elements using C and AJAX JSP was used for validating client side responses and connected C to database to retrieve SQL data Developed LINUXShell scripts by using NZSQLNZLOAD utilities to load data from flat files to Netezza database Developed triggers stored procedures functions and packages using cursors and ref cursor concepts associated with the project using PlSQL Created various types of data visualizations using R C python and TableauSpotfire also connected Pipeline Pilot with Spotfire to create more interactive business driven layouts Used Python R SQL Tensorflow to create Statistical algorithms involving Multivariate Regression Linear Regression Logistic Regression PCA Image Recognition Random forest models Decision trees Support Vector Machine for estimating the risks of welfare dependency Identified and targeted welfare highrisk groups with Machine learningdeep learning algorithms Conducted campaigns and run realtime trials to determine what works fast and track the impact of different initiatives Developed Tableau visualizations and dashboards using Tableau Desktop Used Graphical EntityRelationship Diagramming to create new database design via easy to use graphical interface Created multiple custom SQL queries in Teradata SQL Workbench to prepare the right data sets for Tableau dashboards Perform analyses such as regression analysis logistic regression discriminant analysis cluster analysis using SAS programming Environment R 3x HDFS CHadoop 23 Pig Hive Linux RStudio Tableau 10 SQL Server Ms Excel Pypark Data Scientist TripAdvisor New York NY November 2011 to December 2012 Description TripAdvisor Inc is an American travel website company providing reviews of travelrelated content It also includes interactive travel forums TripAdvisor was an early adopter of usergenerated content The website services are free to users who provide most of the content and the website is supported by an advertising business model Responsibilities Involved in Design Development and Support phases of Software Development Life Cycle SDLC Performed data ETL by collecting exporting merging and massaging data from multiple sources and platforms including SSRSSSIS SQL Server Integration Services in SQL Server Programming experience with NET framework C Visual Studio 20052008 to build web based clientserver architecture and to produce reports with C and JSP Worked with crossfunctional teams including data engineer team to extract data and rapidly execute from MongoDB through MongDB connector for Hadoop Performed data cleaning and feature selection using MLlib package in PySpark Performed partitional clustering into 100 by kmeans clustering using Scikitlearn package in Python where similar hotels for a search are grouped together and Image Recognition Used Python to perform ANOVA test to analyze the differences among hotel clusters Implemented application of various machine learning algorithms and statistical modeling like Decision Tree Text Analytics Sentiment Analysis Naive Bayes Logistic Regression and Linear Regression using Python to determine the accuracy rate of each model Determined the most accurately prediction model based on the accuracy rate Used textmining process of reviews to determine customers concentrations Delivered analysis support to hotel recommendation and providing an online AB test Designed Tableau bar graphs scattered plots and geographical maps to create detailed level summary reports and dashboards Developed hybrid model to improve the accuracy rate Environment Python PySpark C Tableau MongoDB Hadoop SQL Server SDLC ETL SSIS recommendation systems Machine Learning Algorithms textmining process AB test Data Scientist Bank of America Wilmington DE October 2010 to October 2011 Description Bank of America is a multinational banking and financial services corporation It is ranked 2nd on the list of largest banks in the United States by assets As of 2016 Bank of America was the 26th largest company in the United States by total revenue Responsibilities Participated in all phases of research including data collection data cleaning data mining developing models and visualizations Collaborated with data engineers and operation team to collect data from internal system to fit the analytical requirements Redefined many attributes and relationships and cleansed unwanted tablescolumns using SQL queries Utilized Spark SQL API in PySpark to extract and load data and perform SQL queries and also used C connector to perform SQL queries by creating and connecting to SQL engine Performed data imputation using Scikitlearn package in Python Performed data processing using Python libraries like Numpy and Pandas Worked with data analysis using ggplot2 library in R to do data visualizations for better understanding of customers behaviors Implemented statistical modeling with XGBoost machine learning software package using R to determine the predicted probabilities of each model Delivered the results with operation team for better decisions Environment Python R SQL Tableau Spark Machine Learning Software Package recommendation systems Python Developer Cenvien Technologies Hyderabad Telangana June 2009 to August 2010 Description Cenvien technologies gather the requirements by listening and understanding to the clients business requirement to deliver quality products It is highly qualified and strongly dedicated developing team that produces unique solutions Responsibilities Developed entire frontend and backend modules using Python on Django Web Framework Implemented the presentation layer with HTML CSS and JavaScript Involved in writing stored procedures using Oracle Optimized the database queries to improve the performance Designed and developed data management system using Oracle Environment MySQL ORACLE HTML5 CSS3 JavaScript Shell Linux Windows Django Python Programmer Analyst Pennar Industries Limited Hyderabad Telangana April 2008 to May 2009 Description As a backend developer of web applications and data science infrastructure The main area of focus is to come up with comprehensive solutions that need massive capacity and throughput Responsibilities Effectively communicated with the stakeholders to gather requirements for different projects Used MySQL db package and PythonMySQL connector for writing and executing several MYSQL database queries from Python Implemented ClientServer applications using C C JSP and SQL Created functions triggers views and stored procedures using My SQL Worked closely with backend developer to find ways to push the limits of existing Web technology Involved in the code review meetings Environment Python MySQL C Education Bachelor of Computer Science in Computer Science JNTU Anantapur Anantapur Andhra Pradesh Skills APPLICATION DEVELOPMENT TABLEAU 8 years LINUX 5 years SAP 1 year BI 1 year Python R Hadoop Machine Learning Spark Additional Information Methodologies Ralph Kimball and Bill Inmon data warehousing methodology Rational Unified Process RUP Rapid Application Development RAD Joint Application Development JAD BI Tools Tableau Tableau server Tableau Reader SAP Business Objects OBIEE QlikView SAP Business Intelligence Amazon Redshift or Azure Data Warehouse Operating System Windows Linux Unix Macintosh HD Red HatAndroid",
    "entities": [
        "Python Performed",
        "Spark Created",
        "MLlib",
        "Adode Analytics Application",
        "TableauAdobe Analytics",
        "Exploratory Data Analysis AB",
        "C50",
        "Canada",
        "New York",
        "Utilized Spark SQL API",
        "AUC",
        "The Cigna Information Management Analytics CIMA",
        "BI",
        "HDFS",
        "Ability",
        "Pipeline Pilot",
        "Georgia",
        "GitHub Project Execution Work",
        "Responsibilities",
        "Teradata SQL Workbench",
        "Opera Solutions",
        "XGB",
        "Control",
        "Abbyy",
        "Data Scientist Bank of America",
        "Responsibilities Performed Data Profiling",
        "XGBoost",
        "Apache MavenAnt Passionate",
        "ER",
        "Informatica Power Centre SSIS Version",
        "Dataiku Generated",
        "Data Scientist Mercedes Benz Financial Services",
        "Hadoop",
        "Tableau Adobe Analytics",
        "XML",
        "Utilized AWS Lambda",
        "Oracle Optimized",
        "Brazil",
        "Atlanta",
        "Python R Hadoop Machine Learning Spark Additional Information Methodologies Ralph Kimball",
        "Data Science and Analytics",
        "SAP Predictive Analytics Developed MapReduce",
        "kmeans Implemented Bagging and Boosting",
        "SPSS",
        "Hadoop Performed",
        "PySpark Performed",
        "Principal Component Analysis",
        "Sparksql",
        "Formulated",
        "AWS SagemakerAzure Databricks Created",
        "Performed Data Cleaning",
        "Responsibilities Provided Configuration Management",
        "Conducted",
        "2016 Bank of America",
        "VM Excellent",
        "My SQL MS",
        "Utilized",
        "Description TripAdvisor Inc",
        "Image Recognition",
        "New Jersey",
        "Tableau Worked",
        "SQL Server Programming",
        "Redefined",
        "Pyspark Using AirFlow",
        "Spark Application",
        "Mexico",
        "Responsibilities Involved in Design Development",
        "SQL Created",
        "C JSP XML",
        "Software Development Life Cycle SDLC Performed",
        "AirFlow",
        "Data Scientist Big Data Data Scientist Big Data Data Scientist Big Data Cigna Health Insurance Bloomfield CT Professional",
        "Control Tools",
        "JavaScript Involved",
        "Tableau Desktop Used Graphical EntityRelationship Diagramming",
        "JSP",
        "Collaborated",
        "Description MercedesBenz Financial Services",
        "FunctionTransfomer",
        "Description Opera Solutions",
        "Teradata Text",
        "XI Business Intelligence SSRS Business Objects",
        "ROC",
        "NZSQLNZLOAD",
        "Argentina Responsibilities Performed Data Profiling",
        "ANOVA Experience",
        "Medicare Medicaid ACA",
        "Statistical Concepts Proficient",
        "Spark Experienced",
        "Spark",
        "EndtoEnd",
        "Michigan",
        "NET",
        "linear",
        "HTML CSS",
        "Python Implemented ClientServer",
        "API",
        "Text Analytics Created",
        "LinuxWindows",
        "Created",
        "Hypothesis",
        "AWS",
        "Analyzed",
        "Hive and Pig Created Data Quality Scripts",
        "Oracle Environment MySQL ORACLE HTML5",
        "Oracle",
        "PySpark",
        "Daimler Truck Financial",
        "GGPLOT2 GGMCMC Successfully",
        "Developed Tableau",
        "C C JSP",
        "LLC",
        "Signal Hub",
        "SDLC Agile Devops",
        "Sql",
        "Performed Multinomial Logistic Regression Random",
        "SAS",
        "Data Integration Validation",
        "Pennar Industries Limited Hyderabad",
        "SQL",
        "Matlab Utilized Spark Snowflake",
        "Multivariate Regression Linear Regression Logistic Regression PCA Image Recognition Random",
        "TripAdvisor",
        "NLP",
        "Data Quality",
        "Tableau Reader SAP Business Objects OBIEE QlikView SAP Business Intelligence Amazon Redshift or Azure Data Warehouse Operating System",
        "Airflow Created",
        "ANOVA",
        "the United States",
        "OCR",
        "Python R SQL Tensorflow",
        "Anaconda",
        "Big Data",
        "Hive",
        "Macintosh",
        "SAP Power",
        "Scala pySpark Redshift",
        "Pandas",
        "Anova",
        "ETL",
        "Medicare",
        "FeatureUnion",
        "Build",
        "Performed",
        "Utilized Boosting",
        "LSTMMemory Network",
        "Impala",
        "Skills APPLICATION DEVELOPMENT TABLEAU",
        "TPump",
        "Language Processing Toolkitmatplotlib",
        "Daimler AG Doing",
        "Python ScikitLearn Experienced",
        "Microsoft",
        "SSRSSSIS SQL Server Integration Services",
        "C Visual Studio",
        "Big Data Technologies Hadoop Hive HDFS Presto MapReduce Pig Kafka",
        "Python Developer Cenvien Technologies Hyderabad",
        "MercedesBenz Financial Services",
        "ref",
        "Data ScientistData Analyst",
        "Data Warehousing",
        "Tableau",
        "ACA",
        "Machine Learning",
        "Predictive Modeling Data Mining Methods Factor Analysis ANOVA Hypothetical",
        "Decision Tree Random Forest Nave Bayes Logistic Regression Social Network Analysis Cluster Analysis",
        "Creative Campaigns Computing AB",
        "NoSQL Database",
        "SVM",
        "Advanced Regression Modeling Correlation Multivariate Analysis Model Building Business Intelligence",
        "Cross Validation Log"
    ],
    "experience": "Experience in implementing data analysis with various analytic tools such as Anaconda 40 Jupiter Notebook 4X R 30 ggplot2 Caret dplyr and Excel Solid ability to write and optimize diverse SQL queries working knowledge of RDBMS like SQL Server 2008 NoSql databases like MongoDB 32 Developed API libraries and coded business logic using C XML and designed web pages using NET framework C Python Django HTML AJAX Strong experience for over 5 years in Image Recognition and Big Data technologies like Spark 16 Sparksql pySpark Hadoop 2X HDFS Hive 1X Experience in visualization tools like Tableau 9X 10X for creating dashboards Excellent understanding Agile and Scrum development methodology Used the version control tools like Git 2X and build tools like Apache MavenAnt Passionate about gleaning insightful information from massive data assets and developing a culture of sound datadriven decision making Ability to maintain a fun casual professional and productive team atmosphere Experienced the full software life cycle in SDLC Agile Devops and Scrum methodologies including creating requirements test plans Skilled in Advanced Regression Modeling Correlation Multivariate Analysis Model Building Business Intelligence tools and application of Statistical Concepts Proficient in Predictive Modeling Data Mining Methods Factor Analysis ANOVA Hypothetical testing normal distribution and other advanced statistical and econometric techniques Developed predictive models using Decision Tree Random Forest Nave Bayes Logistic Regression Social Network Analysis Cluster Analysis and Neural Networks Experienced in Machine Learning and Statistical Analysis with Python ScikitLearn Experienced in Python to manipulate data for data loading and extraction and worked with python libraries like Matplotlib Numpy Scipy and Pandas for data analysis Worked with complex applications such as R Python Theano H20 SAS Matlab and SPSS to develop neural network cluster analysis Expertise in transforming business requirements into analytical models designing algorithms building models developing data mining and reporting solutions that scales across massive volume of structured and unstructured data Skilled in performing data parsing data ingestion data manipulationdata architecture data modelling and data preparation with methods including describe data contents compute descriptive statistics of data regex split and combine Remap merge subset reindex melt and reshape Strong C SQL programming skills with experience in working with functions packages and triggers Extensively worked on Python 3527 Numpy Pandas Matplotlib NLTK and Scikitlearn Experienced in Visual Basic for Applications and VB programming languages C NET framework to work with developing applications Worked with NoSQL Database including Hbase Cassandra and MongoDB Experienced in Big Data with Hadoop HDFS MapReduce and Spark Experienced in Data Integration Validation and Data Quality controls for ETL process and Data Warehousing using MS Visual Studio SSIS SSAS SSRS Proficient in TableauAdobe Analytics and RShiny data visualization tools to analyze and obtain insights into large datasets create visually powerful and actionable interactive reports and dashboards Automated recurring reports using SQL and Python and visualized them on BI platform like Tableau Worked in development environment like Git and VM Excellent communication skills Successfully working in fastpaced multitasking environment both independently and in collaborative team a selfmotivated enthusiastic learner TOOLS AND TECHNOLOGIES Languages Java 8 Python R C Powershell Packages ggplot2 caret dplyr Rweka gmodels Edward RCurl tm C50 twitteR NLP Reshape2 rjson plyr pandas numPy TensorFlow seaborn sciPy matplot lib scikitlearn Beautiful Soup Rpy2 Web Technologies JDBC HTML5 DHTML and XML CSS3 Web Services WSDL Django AWS Data Modelling Tools Erwin r 96 95 91 8x Rational Rose ERStudio MS Visio SAP Power designer Big Data Technologies Hadoop Hive HDFS Presto MapReduce Pig Kafka oozie Databases SQL Hive Impala Pig Spark SQL HQL VQL Databases SQLServer My SQL MS SQLMS Access HDFS HBase Teradata Netezza MongoDB Cassandra Reporting Tools MS Office WordExcelPower Point Visio Tableau Spotfire Crystal reports XI Business Intelligence SSRS Business Objects 5x 6x Cognos7060 ETL Tools Informatica Power Centre SSIS Version Control Tools SVM GitHub Project Execution Work Experience Data Scientist Big Data Cigna Health Insurance Bloomfield CT August 2017 to Present Description The Cigna Information Management Analytics CIMA unit offers solutions that provide actionable insights to internal and external business partners and customers that help reduce health costs improve outcomes provide financial security and measure and forecast business performance Responsibilities Several visualizations density plots forest plots leverage plots network plots covariant adjustment plots etc were made using packages such as GGPLOT2 GGMCMC Successfully delivered multiple NLP projects like building a chatbot that assists a customer to trouble shoot claim issues and recommend actionsFurther the bot could handle questions asked in natural language related to common issues with the customer eg when is my premium due what is my plan deductible what is my copay for a sick reject Extracted data from multiple sources like Medicare Medicaid ACA claims Performed data preprocessing like data cleaning text preprocessing noise removal lexicon normalization and object standardization Perform featuring engineering like Word Embedding using word2vec models Build seq2seqmodels using structured data word embedding Seq2Seq model take an input and returns as desired output for eg it can take a question as an input and returns an answer The benefit is it can take any arbitrary length question and returns and answers in natural language It uses a recurrent neural network LSTMMemory Network at the backend Performing Map Reduce jobs in Hadoop and implemented Spark analysis using Python for performing machine learning predictive analytics on AWS platform Analyzed administrative claims data Medicare and ACA Marketplace to answer health services research questions on costs utilization or outcomes using advanced statistical and econometric methods Used Tensorflow packages to train machine learning models Developed Oozie workflows to ingestparse the raw data populate staging tables and store the refined data in partitioned tables in the Hive Handon experience with data ingestion into Big Data platform from disparate data sources using Sqoop Hive Pig Flume and Spark Created an EndtoEnd data analytical solutions and models by manipulating large data sets and integrating diverse data sources Worked with team of developers to design develop and implement BI solutions in Tableau to measure Point of Sale KPIs at micro and macro level Environment Tableau Python PyCharm Statistics Machine Learning Tensorflow Alteryx Hadoop Hive Pig No SQL PLSQL Excel AWS Data Scientist Opera Solutions New Jersey January 2016 to July 2017 Description Opera Solutions LLC is a technology and analytics company mainly focused on big data The firm uses a combination of machine learning science advanced predictive analytics technology largescale data management and human expertise Opera Solutions delivers predictive analytics as a service and offers hosted cloudbased systems for specific business problems eg predicting the behavior of individual consumers stopping revenue leakage in hospitals warning of threats to corporate security or brand health etc Responsibilities Performed Data Profiling to learn about behavior with various features of USMLE examinations of various student patterns using Tableau Adobe Analytics and Python Matplotlib Evaluated models using Cross Validation Log loss function ROC curves and used AUC for feature selection and elastic technologies like ElasticSearch Kibana etc Addressed overfitting by implementing of the algorithm regularization methods like L2 and L1 Implemented statistical modeling with XGBoost machine learning software package using Python to determine the predicted probabilities of each model Created master data for modelling by combining various tables and derived fields from client data and students LORs essays and various performance metrics Formulated a basis for variable selection and GridSearch KFold for optimal hyperparameters Utilized Boosting algorithms to build a model for predictive analysis of students behaviour who took USMLE exam apply for residency Used numpy scipy pandas nltkNatural Language Processing Toolkitmatplotlib to build the model Formulated several graphs to show the performance of the students by demographics and their mean score in different USMLE exams Extracted data from HDFS using Hive Presto and performed data analysis using Spark with Scala pySpark Redshift and feature selection and created nonparametric models in Spark Application of various Artificial IntelligenceAImachine learning algorithms and statistical modeling like decision treestext analytics Image and Text Recognition using OCR tools like Abbyy natural language processingNLP supervised and unsupervised regression models Used Principal Component Analysis in feature engineering to analyze high dimensional data Performed Data Cleaning features scaling features engineering using pandas and numpy packages in python and build models using deep learning frameworks Created deep learning models using Tensorflow and keras by combining all tests as a single normalized score and predict residency attainment of students Used XGB classifier if the feature is an categorical variable and XGB regressor for continuous variables and combined it using FeatureUnion and FunctionTransfomer methods of Natural Language Processing Used OnevsRest Classifier to fit each classifier against all other classifiers and used it on multiclass classification problems Implemented application of various machine learning algorithms and statistical modeling like Decision Tree Text Analytics Sentiment Analysis Naive Bayes Logistic Regression and Linear Regression using Python to determine the accuracy rate of each model Created and designed reports that will use gathered metrics to infer and draw logical conclusions of past and future behavior with cloud based products like Azure ML Studio and Dataiku Generated various models by using different machine learning and deep learning frameworks and tuned the best performance model using Signal Hub and AWS SagemakerAzure Databricks Created data layers as signals to Signal Hub to predict new unseen data with performance not less than the static model build using deep learning framework Environment Python 2x3x Hive AWS Linux Tableau Desktop Microsoft Excel NLP Deep learning frameworks such as TensorFLow Keras Boosting algorithms etc Data Scientist Mercedes Benz Financial Services Michigan July 2014 to December 2015 Description MercedesBenz Financial Services is a leading captive financial services provider and the global financial services company of Daimler AG Doing business as MercedesBenz Financial Services and Daimler Truck Financial we provide financing for automotive and commercial vehicle dealers and their retail consumers in the United States Canada Mexico Brazil and Argentina Responsibilities Performed Data Profiling to learn about behavior with various features such as traffic pattern location time Date and Time etc using Adode Analytics Application of various Artificial IntelligenceAImachine learning algorithms and statistical modeling like decision treestext analytics natural language processingNLP supervised and unsupervised regression models social network analysis neural networks deep learning SVM clustering to identify Volume using scikitlearn package in python Matlab Utilized Spark Snowflake Presto Scala Hadoop HQL VQL oozie pySpark Data Lake TensorFlow HBase Cassandra Athena Redshift MongoDB Kafka Kinesis Spark Streaming Edward CUDA MLLib AWS Python a broad variety of machine learning methods including classifications regressions dimensionally reduction etc and Utilized the engine to increase user lifetime by 45 and triple user conversations for target categories Created and connected SQL engine through C to connect database developed API libraries and business logic using C XML and Python Exploring DAGs their dependencies and logs using AirFlow pipelines for automation Performed data cleaning and feature selection using MLlib package in PySpark and working with deep learning frameworks such as Caffe Neon etc Developed SparkScala PythonR for regular expression regex project in the HadoopHive environment with LinuxWindows for big data resources Used clustering technique KMeans to identify outliers and to classify unlabeled data Utilized AWS Lambda in created userfriendly interface for quick view of reports by using C JSP XML and developed expandable menu that show drilldown data on graph click Evaluated models using Cross Validation Log loss function ROC curves and used AUC for feature selection and elastic technologies like ElasticSearch Kibana etc Categorised comments into positive and negative clusters from different social networking sites using Sentiment Analysis and Text Analytics and have done Image Recognition Tracking operations using sensors until certain criteria is met using AirFlow technology Responsible for different Data mapping activities from Source systems to Teradata using utilities like TPump FEXP MLOAD BTEQ FLOAD etc Analyze traffic patterns by calculating autocorrelation with different time lags Ensured that the model has low False Positive Rate and Text classification and sentiment analysis for unstructured and semistructured data Addressed overfitting by implementing of the algorithm regularization methods like L2 and L1 Used Principal Component Analysis in feature engineering to analyze high dimensional data Created and designed reports that will use gathered metrics to infer and draw logical conclusions of past and future behavior Performed Multinomial Logistic Regression Random forest Decision Tree SVM to classify package is going to deliver on time for the new route Performed data analysis by using Hive to retrieve the data from Hadoop cluster Sql to retrieve data from Oracle database and used ETL for data transformation Used MLlib Sparks Machine learning library to build and evaluate different models and used AWS Rekognition for image analysis Implemented rule based expertise system from the results of exploratory analysis and information gathered from the people from different departments Performed Data Cleaning features scaling features engineering using pandas and numpy packages in python and build models using SAP Predictive Analytics Developed MapReduce pipeline for feature extraction using Hive and Pig Created Data Quality Scripts using SQL and Hive to validate successful data load and quality of the data Created various types of data visualizations using Python and TableauSpotfire Communicated the results with operations team for taking best decisions Collected data needs and requirements by Interacting with the other departments Environment Python 2x CDH5 HDFS C Hadoop 23 Hive Impala AWS Linux Spark Tableau Desktop SQL Server 2012 Microsoft Excel Matlab Spark SQL Pyspark Data Scientist First Data Atlanta GA January 2013 to June 2014 Description First Data Corporation is a global payment processing company headquartered in Atlanta Georgia United States The companys portfolio includes merchant transaction processing services credit debit privatelabel gift payroll and other prepaid card offerings fraud protection and authentication solutions Responsibilities Provided Configuration Management and Build support for more than 5 different applications built and deployed to the production and lower environments Implemented public segmentation using unsupervised machine learning algorithms by implementing kmeans algorithm using Pyspark Using AirFlow to keep track of job statuses in repositories like MySQl and Postgre databases Explored and Extracted data from source XML in HDFS used ETL for preparing data for exploratory analysis using data munging Responsible for different Data mapping activities from Source systems to Teradata Text mining and building models using topic analysis sentiment analysis for both semistructured and unstructured data Handled importing data from various data sources performed transformations using Hive Map Reduce and loaded data into HDFS Used R and python for Exploratory Data Analysis AB testing HQL VQL Data Lake AWS Redshift oozie pySpark Anova test and Hypothesis test to compare and identify the effectiveness of Creative Campaigns Computing AB testing frameworks clickstream and time spent databases using Airflow Created clusters to Control and test groups and conducted group campaigns using Text Analytics Created positive and negative clusters from merchants transaction using Sentiment Analysis to test the authenticity of transactions and resolve any chargebacks Analyzed and calculated the lifetime cost of everyone in the welfare system using 20 years of historical data Created and developed classes and web page elements using C and AJAX JSP was used for validating client side responses and connected C to database to retrieve SQL data Developed LINUXShell scripts by using NZSQLNZLOAD utilities to load data from flat files to Netezza database Developed triggers stored procedures functions and packages using cursors and ref cursor concepts associated with the project using PlSQL Created various types of data visualizations using R C python and TableauSpotfire also connected Pipeline Pilot with Spotfire to create more interactive business driven layouts Used Python R SQL Tensorflow to create Statistical algorithms involving Multivariate Regression Linear Regression Logistic Regression PCA Image Recognition Random forest models Decision trees Support Vector Machine for estimating the risks of welfare dependency Identified and targeted welfare highrisk groups with Machine learningdeep learning algorithms Conducted campaigns and run realtime trials to determine what works fast and track the impact of different initiatives Developed Tableau visualizations and dashboards using Tableau Desktop Used Graphical EntityRelationship Diagramming to create new database design via easy to use graphical interface Created multiple custom SQL queries in Teradata SQL Workbench to prepare the right data sets for Tableau dashboards Perform analyses such as regression analysis logistic regression discriminant analysis cluster analysis using SAS programming Environment R 3x HDFS CHadoop 23 Pig Hive Linux RStudio Tableau 10 SQL Server Ms Excel Pypark Data Scientist TripAdvisor New York NY November 2011 to December 2012 Description TripAdvisor Inc is an American travel website company providing reviews of travelrelated content It also includes interactive travel forums TripAdvisor was an early adopter of usergenerated content The website services are free to users who provide most of the content and the website is supported by an advertising business model Responsibilities Involved in Design Development and Support phases of Software Development Life Cycle SDLC Performed data ETL by collecting exporting merging and massaging data from multiple sources and platforms including SSRSSSIS SQL Server Integration Services in SQL Server Programming experience with NET framework C Visual Studio 20052008 to build web based clientserver architecture and to produce reports with C and JSP Worked with crossfunctional teams including data engineer team to extract data and rapidly execute from MongoDB through MongDB connector for Hadoop Performed data cleaning and feature selection using MLlib package in PySpark Performed partitional clustering into 100 by kmeans clustering using Scikitlearn package in Python where similar hotels for a search are grouped together and Image Recognition Used Python to perform ANOVA test to analyze the differences among hotel clusters Implemented application of various machine learning algorithms and statistical modeling like Decision Tree Text Analytics Sentiment Analysis Naive Bayes Logistic Regression and Linear Regression using Python to determine the accuracy rate of each model Determined the most accurately prediction model based on the accuracy rate Used textmining process of reviews to determine customers concentrations Delivered analysis support to hotel recommendation and providing an online AB test Designed Tableau bar graphs scattered plots and geographical maps to create detailed level summary reports and dashboards Developed hybrid model to improve the accuracy rate Environment Python PySpark C Tableau MongoDB Hadoop SQL Server SDLC ETL SSIS recommendation systems Machine Learning Algorithms textmining process AB test Data Scientist Bank of America Wilmington DE October 2010 to October 2011 Description Bank of America is a multinational banking and financial services corporation It is ranked 2nd on the list of largest banks in the United States by assets As of 2016 Bank of America was the 26th largest company in the United States by total revenue Responsibilities Participated in all phases of research including data collection data cleaning data mining developing models and visualizations Collaborated with data engineers and operation team to collect data from internal system to fit the analytical requirements Redefined many attributes and relationships and cleansed unwanted tablescolumns using SQL queries Utilized Spark SQL API in PySpark to extract and load data and perform SQL queries and also used C connector to perform SQL queries by creating and connecting to SQL engine Performed data imputation using Scikitlearn package in Python Performed data processing using Python libraries like Numpy and Pandas Worked with data analysis using ggplot2 library in R to do data visualizations for better understanding of customers behaviors Implemented statistical modeling with XGBoost machine learning software package using R to determine the predicted probabilities of each model Delivered the results with operation team for better decisions Environment Python R SQL Tableau Spark Machine Learning Software Package recommendation systems Python Developer Cenvien Technologies Hyderabad Telangana June 2009 to August 2010 Description Cenvien technologies gather the requirements by listening and understanding to the clients business requirement to deliver quality products It is highly qualified and strongly dedicated developing team that produces unique solutions Responsibilities Developed entire frontend and backend modules using Python on Django Web Framework Implemented the presentation layer with HTML CSS and JavaScript Involved in writing stored procedures using Oracle Optimized the database queries to improve the performance Designed and developed data management system using Oracle Environment MySQL ORACLE HTML5 CSS3 JavaScript Shell Linux Windows Django Python Programmer Analyst Pennar Industries Limited Hyderabad Telangana April 2008 to May 2009 Description As a backend developer of web applications and data science infrastructure The main area of focus is to come up with comprehensive solutions that need massive capacity and throughput Responsibilities Effectively communicated with the stakeholders to gather requirements for different projects Used MySQL db package and PythonMySQL connector for writing and executing several MYSQL database queries from Python Implemented ClientServer applications using C C JSP and SQL Created functions triggers views and stored procedures using My SQL Worked closely with backend developer to find ways to push the limits of existing Web technology Involved in the code review meetings Environment Python MySQL C Education Bachelor of Computer Science in Computer Science JNTU Anantapur Anantapur Andhra Pradesh Skills APPLICATION DEVELOPMENT TABLEAU 8 years LINUX 5 years SAP 1 year BI 1 year Python R Hadoop Machine Learning Spark Additional Information Methodologies Ralph Kimball and Bill Inmon data warehousing methodology Rational Unified Process RUP Rapid Application Development RAD Joint Application Development JAD BI Tools Tableau Tableau server Tableau Reader SAP Business Objects OBIEE QlikView SAP Business Intelligence Amazon Redshift or Azure Data Warehouse Operating System Windows Linux Unix Macintosh HD Red HatAndroid",
    "extracted_keywords": [
        "Data",
        "Scientist",
        "Big",
        "Data",
        "Data",
        "Scientist",
        "Big",
        "Data",
        "Data",
        "Scientist",
        "Big",
        "Data",
        "Cigna",
        "Health",
        "Insurance",
        "Bloomfield",
        "CT",
        "Professional",
        "Data",
        "ScientistData",
        "Analyst",
        "years",
        "experience",
        "Data",
        "Science",
        "Analytics",
        "Artificial",
        "IntelligenceDeep",
        "LearningMachine",
        "Learning",
        "Data",
        "Mining",
        "Statistical",
        "Analysis",
        "data",
        "science",
        "project",
        "life",
        "cycle",
        "phases",
        "data",
        "extraction",
        "data",
        "modeling",
        "data",
        "visualization",
        "data",
        "sets",
        "data",
        "ER",
        "diagrams",
        "schema",
        "machine",
        "learning",
        "algorithm",
        "regression",
        "forest",
        "XGboost",
        "KNN",
        "SVM",
        "network",
        "linear",
        "regression",
        "lasso",
        "regression",
        "kmeans",
        "Bagging",
        "model",
        "performance",
        "skills",
        "methodologies",
        "AB",
        "test",
        "experiment",
        "design",
        "hypothesis",
        "test",
        "ANOVA",
        "Experience",
        "data",
        "analysis",
        "tools",
        "Anaconda",
        "Jupiter",
        "Notebook",
        "4X",
        "ggplot2",
        "Caret",
        "dplyr",
        "Excel",
        "ability",
        "SQL",
        "knowledge",
        "RDBMS",
        "SQL",
        "Server",
        "NoSql",
        "API",
        "libraries",
        "business",
        "logic",
        "C",
        "XML",
        "web",
        "pages",
        "NET",
        "framework",
        "C",
        "Python",
        "Django",
        "HTML",
        "experience",
        "years",
        "Image",
        "Recognition",
        "Big",
        "Data",
        "technologies",
        "Spark",
        "Sparksql",
        "pySpark",
        "Hadoop",
        "2X",
        "HDFS",
        "Hive",
        "1X",
        "Experience",
        "visualization",
        "tools",
        "Tableau",
        "9X",
        "dashboards",
        "understanding",
        "Agile",
        "Scrum",
        "development",
        "methodology",
        "version",
        "control",
        "tools",
        "Git",
        "2X",
        "tools",
        "Apache",
        "MavenAnt",
        "Passionate",
        "information",
        "data",
        "assets",
        "culture",
        "decision",
        "Ability",
        "team",
        "atmosphere",
        "software",
        "life",
        "cycle",
        "SDLC",
        "Agile",
        "Devops",
        "Scrum",
        "methodologies",
        "requirements",
        "test",
        "plans",
        "Regression",
        "Modeling",
        "Correlation",
        "Multivariate",
        "Analysis",
        "Model",
        "Building",
        "Business",
        "Intelligence",
        "tools",
        "application",
        "Statistical",
        "Concepts",
        "Proficient",
        "Predictive",
        "Modeling",
        "Data",
        "Mining",
        "Methods",
        "Factor",
        "Analysis",
        "ANOVA",
        "Hypothetical",
        "distribution",
        "techniques",
        "models",
        "Decision",
        "Tree",
        "Random",
        "Forest",
        "Nave",
        "Bayes",
        "Logistic",
        "Regression",
        "Social",
        "Network",
        "Analysis",
        "Cluster",
        "Analysis",
        "Neural",
        "Networks",
        "Machine",
        "Learning",
        "Statistical",
        "Analysis",
        "Python",
        "ScikitLearn",
        "Python",
        "data",
        "data",
        "loading",
        "extraction",
        "python",
        "libraries",
        "Matplotlib",
        "Numpy",
        "Scipy",
        "Pandas",
        "data",
        "analysis",
        "applications",
        "R",
        "Python",
        "Theano",
        "H20",
        "SAS",
        "Matlab",
        "SPSS",
        "network",
        "cluster",
        "analysis",
        "Expertise",
        "business",
        "requirements",
        "models",
        "algorithms",
        "building",
        "models",
        "data",
        "mining",
        "reporting",
        "solutions",
        "volume",
        "data",
        "data",
        "data",
        "ingestion",
        "data",
        "manipulationdata",
        "architecture",
        "data",
        "modelling",
        "data",
        "preparation",
        "methods",
        "describe",
        "data",
        "contents",
        "statistics",
        "data",
        "regex",
        "Remap",
        "merge",
        "subset",
        "reindex",
        "Strong",
        "C",
        "SQL",
        "programming",
        "skills",
        "experience",
        "functions",
        "packages",
        "triggers",
        "Python",
        "Numpy",
        "Pandas",
        "Matplotlib",
        "NLTK",
        "Scikitlearn",
        "Experienced",
        "Visual",
        "Basic",
        "Applications",
        "VB",
        "programming",
        "languages",
        "C",
        "NET",
        "framework",
        "applications",
        "NoSQL",
        "Database",
        "Hbase",
        "Cassandra",
        "MongoDB",
        "Big",
        "Data",
        "Hadoop",
        "HDFS",
        "MapReduce",
        "Spark",
        "Data",
        "Integration",
        "Validation",
        "Data",
        "Quality",
        "ETL",
        "process",
        "Data",
        "Warehousing",
        "MS",
        "Visual",
        "Studio",
        "SSIS",
        "SSAS",
        "SSRS",
        "Proficient",
        "TableauAdobe",
        "Analytics",
        "RShiny",
        "data",
        "visualization",
        "tools",
        "insights",
        "datasets",
        "reports",
        "dashboards",
        "reports",
        "SQL",
        "Python",
        "BI",
        "platform",
        "Tableau",
        "development",
        "environment",
        "Git",
        "VM",
        "Excellent",
        "communication",
        "skills",
        "environment",
        "team",
        "learner",
        "TOOLS",
        "TECHNOLOGIES",
        "Languages",
        "Java",
        "Python",
        "R",
        "C",
        "Powershell",
        "Packages",
        "ggplot2",
        "dplyr",
        "Rweka",
        "Edward",
        "RCurl",
        "tm",
        "C50",
        "twitteR",
        "NLP",
        "Reshape2",
        "rjson",
        "plyr",
        "numPy",
        "TensorFlow",
        "matplot",
        "lib",
        "Beautiful",
        "Soup",
        "Rpy2",
        "Web",
        "Technologies",
        "JDBC",
        "HTML5",
        "DHTML",
        "XML",
        "CSS3",
        "Web",
        "Services",
        "WSDL",
        "Django",
        "Data",
        "Modelling",
        "Tools",
        "Erwin",
        "r",
        "8x",
        "Rational",
        "Rose",
        "ERStudio",
        "MS",
        "Visio",
        "SAP",
        "Power",
        "designer",
        "Big",
        "Data",
        "Technologies",
        "Hadoop",
        "Hive",
        "HDFS",
        "Presto",
        "MapReduce",
        "Pig",
        "Kafka",
        "oozie",
        "SQL",
        "Hive",
        "Impala",
        "Pig",
        "Spark",
        "SQL",
        "HQL",
        "VQL",
        "Databases",
        "SQLServer",
        "SQL",
        "MS",
        "SQLMS",
        "Access",
        "HDFS",
        "HBase",
        "Teradata",
        "Netezza",
        "MongoDB",
        "Cassandra",
        "Reporting",
        "Tools",
        "MS",
        "Office",
        "WordExcelPower",
        "Point",
        "Visio",
        "Tableau",
        "Spotfire",
        "Crystal",
        "XI",
        "Business",
        "Intelligence",
        "SSRS",
        "Business",
        "5x",
        "Cognos7060",
        "ETL",
        "Tools",
        "Informatica",
        "Power",
        "Centre",
        "SSIS",
        "Version",
        "Control",
        "Tools",
        "SVM",
        "GitHub",
        "Project",
        "Execution",
        "Work",
        "Experience",
        "Data",
        "Scientist",
        "Big",
        "Data",
        "Cigna",
        "Health",
        "Insurance",
        "Bloomfield",
        "CT",
        "August",
        "Present",
        "Description",
        "Cigna",
        "Information",
        "Management",
        "Analytics",
        "CIMA",
        "unit",
        "solutions",
        "insights",
        "business",
        "partners",
        "customers",
        "health",
        "costs",
        "outcomes",
        "security",
        "measure",
        "forecast",
        "business",
        "performance",
        "visualizations",
        "density",
        "forest",
        "plots",
        "leverage",
        "plots",
        "network",
        "covariant",
        "adjustment",
        "plots",
        "packages",
        "GGPLOT2",
        "GGMCMC",
        "NLP",
        "projects",
        "chatbot",
        "customer",
        "trouble",
        "shoot",
        "claim",
        "issues",
        "actionsFurther",
        "bot",
        "questions",
        "language",
        "issues",
        "customer",
        "eg",
        "premium",
        "plan",
        "copay",
        "reject",
        "data",
        "sources",
        "Medicare",
        "Medicaid",
        "ACA",
        "Performed",
        "data",
        "data",
        "text",
        "noise",
        "removal",
        "lexicon",
        "normalization",
        "object",
        "standardization",
        "Perform",
        "engineering",
        "Word",
        "models",
        "Build",
        "seq2seqmodels",
        "data",
        "word",
        "model",
        "input",
        "returns",
        "output",
        "eg",
        "question",
        "input",
        "answer",
        "benefit",
        "length",
        "question",
        "returns",
        "answers",
        "language",
        "network",
        "LSTMMemory",
        "Network",
        "Performing",
        "Map",
        "Reduce",
        "jobs",
        "Hadoop",
        "Spark",
        "analysis",
        "Python",
        "machine",
        "analytics",
        "AWS",
        "platform",
        "claims",
        "data",
        "Medicare",
        "ACA",
        "Marketplace",
        "health",
        "services",
        "research",
        "questions",
        "costs",
        "utilization",
        "outcomes",
        "methods",
        "Tensorflow",
        "packages",
        "machine",
        "learning",
        "models",
        "Oozie",
        "workflows",
        "data",
        "staging",
        "tables",
        "data",
        "tables",
        "Hive",
        "Handon",
        "experience",
        "data",
        "ingestion",
        "Data",
        "platform",
        "data",
        "sources",
        "Sqoop",
        "Hive",
        "Pig",
        "Flume",
        "Spark",
        "EndtoEnd",
        "data",
        "solutions",
        "models",
        "data",
        "sets",
        "data",
        "sources",
        "team",
        "developers",
        "BI",
        "solutions",
        "Tableau",
        "Point",
        "Sale",
        "KPIs",
        "micro",
        "level",
        "Environment",
        "Tableau",
        "Python",
        "PyCharm",
        "Statistics",
        "Machine",
        "Learning",
        "Tensorflow",
        "Alteryx",
        "Hadoop",
        "Hive",
        "Pig",
        "SQL",
        "PLSQL",
        "Excel",
        "AWS",
        "Data",
        "Scientist",
        "Opera",
        "Solutions",
        "New",
        "Jersey",
        "January",
        "July",
        "Description",
        "Opera",
        "Solutions",
        "LLC",
        "technology",
        "analytics",
        "company",
        "data",
        "firm",
        "combination",
        "machine",
        "science",
        "analytics",
        "technology",
        "largescale",
        "data",
        "management",
        "expertise",
        "Opera",
        "Solutions",
        "analytics",
        "service",
        "systems",
        "business",
        "problems",
        "eg",
        "behavior",
        "consumers",
        "revenue",
        "leakage",
        "hospitals",
        "threats",
        "security",
        "brand",
        "health",
        "Responsibilities",
        "Performed",
        "Data",
        "Profiling",
        "behavior",
        "features",
        "USMLE",
        "examinations",
        "student",
        "patterns",
        "Tableau",
        "Adobe",
        "Analytics",
        "Python",
        "Matplotlib",
        "models",
        "Cross",
        "Validation",
        "Log",
        "loss",
        "function",
        "ROC",
        "curves",
        "AUC",
        "feature",
        "selection",
        "technologies",
        "ElasticSearch",
        "Kibana",
        "algorithm",
        "regularization",
        "methods",
        "L2",
        "L1",
        "modeling",
        "XGBoost",
        "machine",
        "software",
        "package",
        "Python",
        "probabilities",
        "model",
        "master",
        "data",
        "modelling",
        "tables",
        "fields",
        "client",
        "data",
        "students",
        "LORs",
        "essays",
        "performance",
        "metrics",
        "basis",
        "selection",
        "GridSearch",
        "KFold",
        "hyperparameters",
        "algorithms",
        "model",
        "analysis",
        "students",
        "behaviour",
        "USMLE",
        "exam",
        "residency",
        "scipy",
        "nltkNatural",
        "Language",
        "Processing",
        "Toolkitmatplotlib",
        "model",
        "graphs",
        "performance",
        "students",
        "demographics",
        "score",
        "USMLE",
        "exams",
        "data",
        "HDFS",
        "Hive",
        "Presto",
        "data",
        "analysis",
        "Spark",
        "Scala",
        "pySpark",
        "Redshift",
        "feature",
        "selection",
        "nonparametric",
        "models",
        "Spark",
        "Application",
        "Artificial",
        "IntelligenceAImachine",
        "learning",
        "algorithms",
        "modeling",
        "decision",
        "treestext",
        "analytics",
        "Image",
        "Text",
        "Recognition",
        "OCR",
        "tools",
        "Abbyy",
        "language",
        "regression",
        "models",
        "Principal",
        "Component",
        "Analysis",
        "feature",
        "engineering",
        "data",
        "Performed",
        "Data",
        "Cleaning",
        "features",
        "engineering",
        "pandas",
        "packages",
        "python",
        "models",
        "learning",
        "frameworks",
        "learning",
        "models",
        "Tensorflow",
        "keras",
        "tests",
        "score",
        "residency",
        "students",
        "XGB",
        "classifier",
        "feature",
        "variable",
        "XGB",
        "regressor",
        "variables",
        "FeatureUnion",
        "FunctionTransfomer",
        "methods",
        "Natural",
        "Language",
        "Processing",
        "OnevsRest",
        "Classifier",
        "classifier",
        "classifiers",
        "classification",
        "problems",
        "application",
        "machine",
        "algorithms",
        "modeling",
        "Decision",
        "Tree",
        "Text",
        "Analytics",
        "Sentiment",
        "Analysis",
        "Naive",
        "Bayes",
        "Logistic",
        "Regression",
        "Linear",
        "Regression",
        "Python",
        "accuracy",
        "rate",
        "model",
        "reports",
        "metrics",
        "conclusions",
        "behavior",
        "cloud",
        "products",
        "ML",
        "Studio",
        "Dataiku",
        "models",
        "machine",
        "learning",
        "frameworks",
        "performance",
        "model",
        "Signal",
        "Hub",
        "AWS",
        "SagemakerAzure",
        "Databricks",
        "data",
        "layers",
        "signals",
        "Signal",
        "Hub",
        "data",
        "performance",
        "model",
        "build",
        "learning",
        "framework",
        "Environment",
        "Python",
        "2x3x",
        "Hive",
        "AWS",
        "Linux",
        "Tableau",
        "Desktop",
        "Microsoft",
        "Excel",
        "NLP",
        "Deep",
        "frameworks",
        "TensorFLow",
        "Keras",
        "algorithms",
        "Data",
        "Scientist",
        "Mercedes",
        "Benz",
        "Financial",
        "Services",
        "Michigan",
        "July",
        "December",
        "Description",
        "MercedesBenz",
        "Financial",
        "Services",
        "services",
        "provider",
        "services",
        "company",
        "Daimler",
        "AG",
        "business",
        "MercedesBenz",
        "Financial",
        "Services",
        "Daimler",
        "Truck",
        "Financial",
        "financing",
        "vehicle",
        "dealers",
        "consumers",
        "United",
        "States",
        "Canada",
        "Mexico",
        "Brazil",
        "Argentina",
        "Responsibilities",
        "Performed",
        "Data",
        "Profiling",
        "behavior",
        "features",
        "traffic",
        "pattern",
        "location",
        "time",
        "Date",
        "Time",
        "Adode",
        "Analytics",
        "Application",
        "Artificial",
        "IntelligenceAImachine",
        "learning",
        "algorithms",
        "modeling",
        "decision",
        "treestext",
        "analytics",
        "language",
        "regression",
        "models",
        "network",
        "analysis",
        "networks",
        "SVM",
        "Volume",
        "package",
        "python",
        "Matlab",
        "Utilized",
        "Spark",
        "Snowflake",
        "Presto",
        "Scala",
        "Hadoop",
        "HQL",
        "VQL",
        "oozie",
        "pySpark",
        "Data",
        "Lake",
        "TensorFlow",
        "HBase",
        "Cassandra",
        "Athena",
        "Redshift",
        "MongoDB",
        "Kafka",
        "Kinesis",
        "Spark",
        "Streaming",
        "Edward",
        "CUDA",
        "AWS",
        "Python",
        "variety",
        "machine",
        "learning",
        "methods",
        "classifications",
        "regressions",
        "engine",
        "user",
        "lifetime",
        "user",
        "conversations",
        "target",
        "categories",
        "SQL",
        "engine",
        "C",
        "database",
        "API",
        "libraries",
        "business",
        "logic",
        "C",
        "XML",
        "Python",
        "DAGs",
        "dependencies",
        "logs",
        "AirFlow",
        "pipelines",
        "automation",
        "Performed",
        "data",
        "cleaning",
        "feature",
        "selection",
        "MLlib",
        "package",
        "PySpark",
        "frameworks",
        "Caffe",
        "Neon",
        "Developed",
        "SparkScala",
        "PythonR",
        "expression",
        "regex",
        "project",
        "environment",
        "LinuxWindows",
        "data",
        "resources",
        "clustering",
        "technique",
        "KMeans",
        "outliers",
        "data",
        "AWS",
        "Lambda",
        "interface",
        "view",
        "reports",
        "C",
        "JSP",
        "XML",
        "menu",
        "drilldown",
        "data",
        "graph",
        "models",
        "Cross",
        "Validation",
        "Log",
        "loss",
        "function",
        "ROC",
        "curves",
        "AUC",
        "feature",
        "selection",
        "technologies",
        "ElasticSearch",
        "Kibana",
        "comments",
        "clusters",
        "networking",
        "sites",
        "Sentiment",
        "Analysis",
        "Text",
        "Analytics",
        "Image",
        "Recognition",
        "Tracking",
        "operations",
        "sensors",
        "criteria",
        "AirFlow",
        "technology",
        "Data",
        "mapping",
        "activities",
        "Source",
        "systems",
        "Teradata",
        "utilities",
        "TPump",
        "FEXP",
        "MLOAD",
        "BTEQ",
        "FLOAD",
        "traffic",
        "patterns",
        "autocorrelation",
        "time",
        "lags",
        "model",
        "False",
        "Positive",
        "Rate",
        "Text",
        "classification",
        "sentiment",
        "analysis",
        "data",
        "algorithm",
        "regularization",
        "methods",
        "L2",
        "L1",
        "Principal",
        "Component",
        "Analysis",
        "feature",
        "engineering",
        "data",
        "reports",
        "metrics",
        "conclusions",
        "behavior",
        "Performed",
        "Multinomial",
        "Logistic",
        "Regression",
        "Random",
        "forest",
        "Decision",
        "Tree",
        "SVM",
        "package",
        "time",
        "route",
        "Performed",
        "data",
        "analysis",
        "Hive",
        "data",
        "Hadoop",
        "cluster",
        "Sql",
        "data",
        "Oracle",
        "database",
        "ETL",
        "data",
        "transformation",
        "MLlib",
        "Sparks",
        "Machine",
        "library",
        "models",
        "AWS",
        "Rekognition",
        "image",
        "analysis",
        "rule",
        "expertise",
        "system",
        "results",
        "analysis",
        "information",
        "people",
        "departments",
        "Performed",
        "Data",
        "Cleaning",
        "features",
        "engineering",
        "pandas",
        "packages",
        "python",
        "models",
        "SAP",
        "Predictive",
        "Analytics",
        "MapReduce",
        "pipeline",
        "feature",
        "extraction",
        "Hive",
        "Pig",
        "Created",
        "Data",
        "Quality",
        "Scripts",
        "SQL",
        "Hive",
        "data",
        "load",
        "quality",
        "data",
        "types",
        "data",
        "visualizations",
        "Python",
        "TableauSpotfire",
        "results",
        "operations",
        "team",
        "decisions",
        "data",
        "needs",
        "requirements",
        "departments",
        "Environment",
        "Python",
        "CDH5",
        "HDFS",
        "C",
        "Hadoop",
        "Hive",
        "Impala",
        "Linux",
        "Spark",
        "Tableau",
        "Desktop",
        "SQL",
        "Server",
        "Microsoft",
        "Excel",
        "Matlab",
        "Spark",
        "SQL",
        "Pyspark",
        "Data",
        "Scientist",
        "First",
        "Data",
        "Atlanta",
        "GA",
        "January",
        "June",
        "Description",
        "First",
        "Data",
        "Corporation",
        "payment",
        "processing",
        "company",
        "Atlanta",
        "Georgia",
        "United",
        "States",
        "companys",
        "portfolio",
        "merchant",
        "transaction",
        "processing",
        "services",
        "credit",
        "debit",
        "privatelabel",
        "gift",
        "payroll",
        "card",
        "offerings",
        "fraud",
        "protection",
        "authentication",
        "solutions",
        "Responsibilities",
        "Configuration",
        "Management",
        "Build",
        "support",
        "applications",
        "production",
        "environments",
        "segmentation",
        "machine",
        "algorithms",
        "kmeans",
        "algorithm",
        "Pyspark",
        "AirFlow",
        "track",
        "job",
        "statuses",
        "repositories",
        "Postgre",
        "data",
        "source",
        "XML",
        "HDFS",
        "ETL",
        "data",
        "analysis",
        "data",
        "Data",
        "mapping",
        "activities",
        "Source",
        "systems",
        "Teradata",
        "Text",
        "mining",
        "building",
        "models",
        "topic",
        "analysis",
        "sentiment",
        "analysis",
        "data",
        "data",
        "data",
        "sources",
        "transformations",
        "Hive",
        "Map",
        "Reduce",
        "data",
        "HDFS",
        "R",
        "python",
        "Exploratory",
        "Data",
        "Analysis",
        "AB",
        "HQL",
        "VQL",
        "Data",
        "Lake",
        "AWS",
        "oozie",
        "pySpark",
        "Anova",
        "test",
        "Hypothesis",
        "test",
        "effectiveness",
        "Campaigns",
        "Computing",
        "AB",
        "testing",
        "frameworks",
        "clickstream",
        "time",
        "databases",
        "Airflow",
        "Created",
        "clusters",
        "Control",
        "test",
        "groups",
        "group",
        "campaigns",
        "Text",
        "Analytics",
        "clusters",
        "merchants",
        "transaction",
        "Sentiment",
        "Analysis",
        "authenticity",
        "transactions",
        "chargebacks",
        "lifetime",
        "cost",
        "welfare",
        "system",
        "years",
        "data",
        "classes",
        "web",
        "page",
        "elements",
        "C",
        "AJAX",
        "JSP",
        "client",
        "side",
        "responses",
        "C",
        "SQL",
        "data",
        "Developed",
        "LINUXShell",
        "scripts",
        "NZSQLNZLOAD",
        "utilities",
        "data",
        "files",
        "Netezza",
        "database",
        "triggers",
        "procedures",
        "functions",
        "packages",
        "cursors",
        "ref",
        "cursor",
        "concepts",
        "project",
        "types",
        "data",
        "visualizations",
        "R",
        "C",
        "python",
        "TableauSpotfire",
        "Pipeline",
        "Pilot",
        "Spotfire",
        "business",
        "layouts",
        "Python",
        "R",
        "SQL",
        "Tensorflow",
        "algorithms",
        "Multivariate",
        "Regression",
        "Linear",
        "Regression",
        "Logistic",
        "Regression",
        "PCA",
        "Image",
        "Recognition",
        "Random",
        "forest",
        "models",
        "Decision",
        "trees",
        "Support",
        "Vector",
        "Machine",
        "risks",
        "welfare",
        "dependency",
        "welfare",
        "highrisk",
        "groups",
        "Machine",
        "learningdeep",
        "campaigns",
        "trials",
        "impact",
        "initiatives",
        "Tableau",
        "visualizations",
        "dashboards",
        "Tableau",
        "Desktop",
        "Used",
        "Graphical",
        "EntityRelationship",
        "database",
        "design",
        "interface",
        "custom",
        "SQL",
        "Teradata",
        "SQL",
        "Workbench",
        "data",
        "sets",
        "Tableau",
        "dashboards",
        "Perform",
        "analyses",
        "regression",
        "analysis",
        "regression",
        "discriminant",
        "analysis",
        "cluster",
        "analysis",
        "SAS",
        "programming",
        "Environment",
        "R",
        "3x",
        "HDFS",
        "CHadoop",
        "Pig",
        "Hive",
        "Linux",
        "RStudio",
        "Tableau",
        "SQL",
        "Server",
        "Ms",
        "Excel",
        "Pypark",
        "Data",
        "Scientist",
        "TripAdvisor",
        "New",
        "York",
        "NY",
        "November",
        "December",
        "Description",
        "TripAdvisor",
        "Inc",
        "travel",
        "website",
        "company",
        "reviews",
        "content",
        "travel",
        "forums",
        "TripAdvisor",
        "adopter",
        "content",
        "website",
        "services",
        "users",
        "content",
        "website",
        "advertising",
        "business",
        "model",
        "Responsibilities",
        "Design",
        "Development",
        "Support",
        "phases",
        "Software",
        "Development",
        "Life",
        "Cycle",
        "SDLC",
        "Performed",
        "data",
        "ETL",
        "merging",
        "data",
        "sources",
        "platforms",
        "SSRSSSIS",
        "SQL",
        "Server",
        "Integration",
        "Services",
        "SQL",
        "Server",
        "Programming",
        "experience",
        "NET",
        "framework",
        "C",
        "Visual",
        "Studio",
        "web",
        "clientserver",
        "architecture",
        "reports",
        "C",
        "JSP",
        "teams",
        "data",
        "engineer",
        "team",
        "data",
        "MongDB",
        "connector",
        "Hadoop",
        "Performed",
        "data",
        "cleaning",
        "feature",
        "selection",
        "MLlib",
        "package",
        "PySpark",
        "Performed",
        "clustering",
        "kmeans",
        "Scikitlearn",
        "package",
        "Python",
        "hotels",
        "search",
        "Image",
        "Recognition",
        "Python",
        "ANOVA",
        "test",
        "differences",
        "hotel",
        "clusters",
        "application",
        "machine",
        "algorithms",
        "modeling",
        "Decision",
        "Tree",
        "Text",
        "Analytics",
        "Sentiment",
        "Analysis",
        "Naive",
        "Bayes",
        "Logistic",
        "Regression",
        "Linear",
        "Regression",
        "Python",
        "accuracy",
        "rate",
        "model",
        "prediction",
        "model",
        "accuracy",
        "rate",
        "textmining",
        "process",
        "reviews",
        "customers",
        "concentrations",
        "analysis",
        "support",
        "hotel",
        "recommendation",
        "AB",
        "test",
        "Tableau",
        "bar",
        "graphs",
        "plots",
        "maps",
        "level",
        "summary",
        "reports",
        "dashboards",
        "model",
        "accuracy",
        "rate",
        "Environment",
        "Python",
        "PySpark",
        "C",
        "Tableau",
        "MongoDB",
        "Hadoop",
        "SQL",
        "Server",
        "SDLC",
        "ETL",
        "SSIS",
        "recommendation",
        "systems",
        "Machine",
        "Learning",
        "Algorithms",
        "textmining",
        "process",
        "AB",
        "test",
        "Data",
        "Scientist",
        "Bank",
        "America",
        "Wilmington",
        "DE",
        "October",
        "October",
        "Description",
        "Bank",
        "America",
        "banking",
        "services",
        "corporation",
        "list",
        "banks",
        "United",
        "States",
        "assets",
        "Bank",
        "America",
        "company",
        "United",
        "States",
        "revenue",
        "Responsibilities",
        "phases",
        "research",
        "data",
        "collection",
        "data",
        "data",
        "mining",
        "models",
        "visualizations",
        "data",
        "engineers",
        "operation",
        "team",
        "data",
        "system",
        "requirements",
        "attributes",
        "relationships",
        "tablescolumns",
        "SQL",
        "queries",
        "Spark",
        "SQL",
        "API",
        "PySpark",
        "data",
        "SQL",
        "queries",
        "C",
        "connector",
        "SQL",
        "queries",
        "SQL",
        "engine",
        "data",
        "imputation",
        "Scikitlearn",
        "package",
        "Python",
        "Performed",
        "data",
        "processing",
        "Python",
        "libraries",
        "Numpy",
        "Pandas",
        "data",
        "analysis",
        "library",
        "R",
        "data",
        "visualizations",
        "understanding",
        "customers",
        "behaviors",
        "modeling",
        "XGBoost",
        "machine",
        "software",
        "package",
        "R",
        "probabilities",
        "model",
        "results",
        "operation",
        "team",
        "decisions",
        "Environment",
        "Python",
        "R",
        "SQL",
        "Tableau",
        "Spark",
        "Machine",
        "Learning",
        "Software",
        "Package",
        "recommendation",
        "systems",
        "Python",
        "Developer",
        "Cenvien",
        "Technologies",
        "Hyderabad",
        "Telangana",
        "June",
        "August",
        "Description",
        "Cenvien",
        "technologies",
        "requirements",
        "understanding",
        "clients",
        "business",
        "requirement",
        "quality",
        "products",
        "team",
        "solutions",
        "Responsibilities",
        "frontend",
        "modules",
        "Python",
        "Django",
        "Web",
        "Framework",
        "presentation",
        "layer",
        "HTML",
        "CSS",
        "JavaScript",
        "procedures",
        "Oracle",
        "database",
        "performance",
        "data",
        "management",
        "system",
        "Oracle",
        "Environment",
        "MySQL",
        "ORACLE",
        "HTML5",
        "CSS3",
        "JavaScript",
        "Shell",
        "Linux",
        "Windows",
        "Django",
        "Python",
        "Programmer",
        "Analyst",
        "Pennar",
        "Industries",
        "Limited",
        "Hyderabad",
        "Telangana",
        "April",
        "May",
        "Description",
        "developer",
        "web",
        "applications",
        "data",
        "science",
        "infrastructure",
        "area",
        "focus",
        "solutions",
        "capacity",
        "Responsibilities",
        "stakeholders",
        "requirements",
        "projects",
        "MySQL",
        "package",
        "connector",
        "writing",
        "MYSQL",
        "database",
        "Python",
        "ClientServer",
        "applications",
        "C",
        "C",
        "JSP",
        "SQL",
        "Created",
        "functions",
        "views",
        "procedures",
        "SQL",
        "developer",
        "ways",
        "limits",
        "Web",
        "technology",
        "code",
        "review",
        "meetings",
        "Environment",
        "Python",
        "MySQL",
        "C",
        "Education",
        "Bachelor",
        "Computer",
        "Science",
        "Computer",
        "Science",
        "JNTU",
        "Anantapur",
        "Anantapur",
        "Andhra",
        "Pradesh",
        "Skills",
        "APPLICATION",
        "DEVELOPMENT",
        "TABLEAU",
        "years",
        "LINUX",
        "years",
        "SAP",
        "year",
        "BI",
        "year",
        "Python",
        "R",
        "Hadoop",
        "Machine",
        "Learning",
        "Spark",
        "Additional",
        "Information",
        "Methodologies",
        "Ralph",
        "Kimball",
        "Bill",
        "Inmon",
        "warehousing",
        "methodology",
        "Rational",
        "Unified",
        "Process",
        "RUP",
        "Rapid",
        "Application",
        "Development",
        "RAD",
        "Joint",
        "Application",
        "Development",
        "JAD",
        "BI",
        "Tools",
        "Tableau",
        "Tableau",
        "server",
        "Tableau",
        "Reader",
        "SAP",
        "Business",
        "OBIEE",
        "QlikView",
        "SAP",
        "Business",
        "Intelligence",
        "Amazon",
        "Redshift",
        "Azure",
        "Data",
        "Warehouse",
        "Operating",
        "System",
        "Windows",
        "Linux",
        "Unix",
        "Macintosh",
        "HD",
        "Red",
        "HatAndroid"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T20:43:03.914258",
    "resume_data": "Data Scientist Big Data Data Scientist Big Data Data Scientist Big Data Cigna Health Insurance Bloomfield CT Professional qualified Data ScientistData Analyst with over 9 years of experience in Data Science and Analytics including Artificial IntelligenceDeep LearningMachine Learning Data Mining and Statistical Analysis Involved in the entire data science project life cycle and actively involved in all the phases including data extraction data cleaning statistical modeling and data visualization with large data sets of structured and unstructured data created ER diagrams and schema Experienced with machine learning algorithm such as logistic regression random forest XGboost KNN SVM neural network linear regression lasso regression and kmeans Implemented Bagging and Boosting to enhance the model performance Strong skills in statistical methodologies such as AB test experiment design hypothesis test ANOVA Experience in implementing data analysis with various analytic tools such as Anaconda 40 Jupiter Notebook 4X R 30 ggplot2 Caret dplyr and Excel Solid ability to write and optimize diverse SQL queries working knowledge of RDBMS like SQL Server 2008 NoSql databases like MongoDB 32 Developed API libraries and coded business logic using C XML and designed web pages using NET framework C Python Django HTML AJAX Strong experience for over 5 years in Image Recognition and Big Data technologies like Spark 16 Sparksql pySpark Hadoop 2X HDFS Hive 1X Experience in visualization tools like Tableau 9X 10X for creating dashboards Excellent understanding Agile and Scrum development methodology Used the version control tools like Git 2X and build tools like Apache MavenAnt Passionate about gleaning insightful information from massive data assets and developing a culture of sound datadriven decision making Ability to maintain a fun casual professional and productive team atmosphere Experienced the full software life cycle in SDLC Agile Devops and Scrum methodologies including creating requirements test plans Skilled in Advanced Regression Modeling Correlation Multivariate Analysis Model Building Business Intelligence tools and application of Statistical Concepts Proficient in Predictive Modeling Data Mining Methods Factor Analysis ANOVA Hypothetical testing normal distribution and other advanced statistical and econometric techniques Developed predictive models using Decision Tree Random Forest Nave Bayes Logistic Regression Social Network Analysis Cluster Analysis and Neural Networks Experienced in Machine Learning and Statistical Analysis with Python ScikitLearn Experienced in Python to manipulate data for data loading and extraction and worked with python libraries like Matplotlib Numpy Scipy and Pandas for data analysis Worked with complex applications such as R Python Theano H20 SAS Matlab and SPSS to develop neural network cluster analysis Expertise in transforming business requirements into analytical models designing algorithms building models developing data mining and reporting solutions that scales across massive volume of structured and unstructured data Skilled in performing data parsing data ingestion data manipulationdata architecture data modelling and data preparation with methods including describe data contents compute descriptive statistics of data regex split and combine Remap merge subset reindex melt and reshape Strong C SQL programming skills with experience in working with functions packages and triggers Extensively worked on Python 3527 Numpy Pandas Matplotlib NLTK and Scikitlearn Experienced in Visual Basic for Applications and VB programming languages C NET framework to work with developing applications Worked with NoSQL Database including Hbase Cassandra and MongoDB Experienced in Big Data with Hadoop HDFS MapReduce and Spark Experienced in Data Integration Validation and Data Quality controls for ETL process and Data Warehousing using MS Visual Studio SSIS SSAS SSRS Proficient in TableauAdobe Analytics and RShiny data visualization tools to analyze and obtain insights into large datasets create visually powerful and actionable interactive reports and dashboards Automated recurring reports using SQL and Python and visualized them on BI platform like Tableau Worked in development environment like Git and VM Excellent communication skills Successfully working in fastpaced multitasking environment both independently and in collaborative team a selfmotivated enthusiastic learner TOOLS AND TECHNOLOGIES Languages Java 8 Python R C Powershell Packages ggplot2 caret dplyr Rweka gmodels Edward RCurl tm C50 twitteR NLP Reshape2 rjson plyr pandas numPy TensorFlow seaborn sciPy matplot lib scikitlearn Beautiful Soup Rpy2 Web Technologies JDBC HTML5 DHTML and XML CSS3 Web Services WSDL Django AWS Data Modelling Tools Erwin r 96 95 91 8x Rational Rose ERStudio MS Visio SAP Power designer Big Data Technologies Hadoop Hive HDFS Presto MapReduce Pig Kafka oozie Databases SQL Hive Impala Pig Spark SQL HQL VQL Databases SQLServer My SQL MS SQLMS Access HDFS HBase Teradata Netezza MongoDB Cassandra Reporting Tools MS Office WordExcelPower Point Visio Tableau Spotfire Crystal reports XI Business Intelligence SSRS Business Objects 5x 6x Cognos7060 ETL Tools Informatica Power Centre SSIS Version Control Tools SVM GitHub Project Execution Work Experience Data Scientist Big Data Cigna Health Insurance Bloomfield CT August 2017 to Present Description The Cigna Information Management Analytics CIMA unit offers solutions that provide actionable insights to internal and external business partners and customers that help reduce health costs improve outcomes provide financial security and measure and forecast business performance Responsibilities Several visualizations density plots forest plots leverage plots network plots covariant adjustment plots etc were made using packages such as GGPLOT2 GGMCMC Successfully delivered multiple NLP projects like building a chatbot that assists a customer to trouble shoot claim issues and recommend actionsFurther the bot could handle questions asked in natural language related to common issues with the customer eg when is my premium due what is my plan deductible what is my copay for a sick reject Extracted data from multiple sources like Medicare Medicaid ACA claims Performed data preprocessing like data cleaning text preprocessing noise removal lexicon normalization and object standardization Perform featuring engineering like Word Embedding using word2vec models Build seq2seqmodels using structured data word embedding Seq2Seq model take an input and returns as desired output for eg it can take a question as an input and returns an answer The benefit is it can take any arbitrary length question and returns and answers in natural language It uses a recurrent neural network LSTMMemory Network at the backend Performing Map Reduce jobs in Hadoop and implemented Spark analysis using Python for performing machine learning predictive analytics on AWS platform Analyzed administrative claims data Medicare and ACA Marketplace to answer health services research questions on costs utilization or outcomes using advanced statistical and econometric methods Used Tensorflow packages to train machine learning models Developed Oozie workflows to ingestparse the raw data populate staging tables and store the refined data in partitioned tables in the Hive Handon experience with data ingestion into Big Data platform from disparate data sources using Sqoop Hive Pig Flume and Spark Created an EndtoEnd data analytical solutions and models by manipulating large data sets and integrating diverse data sources Worked with team of developers to design develop and implement BI solutions in Tableau to measure Point of Sale KPIs at micro and macro level Environment Tableau Python PyCharm Statistics Machine Learning Tensorflow Alteryx Hadoop Hive Pig No SQL PLSQL Excel AWS Data Scientist Opera Solutions New Jersey January 2016 to July 2017 Description Opera Solutions LLC is a technology and analytics company mainly focused on big data The firm uses a combination of machine learning science advanced predictive analytics technology largescale data management and human expertise Opera Solutions delivers predictive analytics as a service and offers hosted cloudbased systems for specific business problems eg predicting the behavior of individual consumers stopping revenue leakage in hospitals warning of threats to corporate security or brand health etc Responsibilities Performed Data Profiling to learn about behavior with various features of USMLE examinations of various student patterns using Tableau Adobe Analytics and Python Matplotlib Evaluated models using Cross Validation Log loss function ROC curves and used AUC for feature selection and elastic technologies like ElasticSearch Kibana etc Addressed overfitting by implementing of the algorithm regularization methods like L2 and L1 Implemented statistical modeling with XGBoost machine learning software package using Python to determine the predicted probabilities of each model Created master data for modelling by combining various tables and derived fields from client data and students LORs essays and various performance metrics Formulated a basis for variable selection and GridSearch KFold for optimal hyperparameters Utilized Boosting algorithms to build a model for predictive analysis of students behaviour who took USMLE exam apply for residency Used numpy scipy pandas nltkNatural Language Processing Toolkitmatplotlib to build the model Formulated several graphs to show the performance of the students by demographics and their mean score in different USMLE exams Extracted data from HDFS using Hive Presto and performed data analysis using Spark with Scala pySpark Redshift and feature selection and created nonparametric models in Spark Application of various Artificial IntelligenceAImachine learning algorithms and statistical modeling like decision treestext analytics Image and Text Recognition using OCR tools like Abbyy natural language processingNLP supervised and unsupervised regression models Used Principal Component Analysis in feature engineering to analyze high dimensional data Performed Data Cleaning features scaling features engineering using pandas and numpy packages in python and build models using deep learning frameworks Created deep learning models using Tensorflow and keras by combining all tests as a single normalized score and predict residency attainment of students Used XGB classifier if the feature is an categorical variable and XGB regressor for continuous variables and combined it using FeatureUnion and FunctionTransfomer methods of Natural Language Processing Used OnevsRest Classifier to fit each classifier against all other classifiers and used it on multiclass classification problems Implemented application of various machine learning algorithms and statistical modeling like Decision Tree Text Analytics Sentiment Analysis Naive Bayes Logistic Regression and Linear Regression using Python to determine the accuracy rate of each model Created and designed reports that will use gathered metrics to infer and draw logical conclusions of past and future behavior with cloud based products like Azure ML Studio and Dataiku Generated various models by using different machine learning and deep learning frameworks and tuned the best performance model using Signal Hub and AWS SagemakerAzure Databricks Created data layers as signals to Signal Hub to predict new unseen data with performance not less than the static model build using deep learning framework Environment Python 2x3x Hive AWS Linux Tableau Desktop Microsoft Excel NLP Deep learning frameworks such as TensorFLow Keras Boosting algorithms etc Data Scientist Mercedes Benz Financial Services Michigan July 2014 to December 2015 Description MercedesBenz Financial Services is a leading captive financial services provider and the global financial services company of Daimler AG Doing business as MercedesBenz Financial Services and Daimler Truck Financial we provide financing for automotive and commercial vehicle dealers and their retail consumers in the United States Canada Mexico Brazil and Argentina Responsibilities Performed Data Profiling to learn about behavior with various features such as traffic pattern location time Date and Time etc using Adode Analytics Application of various Artificial IntelligenceAImachine learning algorithms and statistical modeling like decision treestext analytics natural language processingNLP supervised and unsupervised regression models social network analysis neural networks deep learning SVM clustering to identify Volume using scikitlearn package in python Matlab Utilized Spark Snowflake Presto Scala Hadoop HQL VQL oozie pySpark Data Lake TensorFlow HBase Cassandra Athena Redshift MongoDB Kafka Kinesis Spark Streaming Edward CUDA MLLib AWS Python a broad variety of machine learning methods including classifications regressions dimensionally reduction etc and Utilized the engine to increase user lifetime by 45 and triple user conversations for target categories Created and connected SQL engine through C to connect database developed API libraries and business logic using C XML and Python Exploring DAGs their dependencies and logs using AirFlow pipelines for automation Performed data cleaning and feature selection using MLlib package in PySpark and working with deep learning frameworks such as Caffe Neon etc Developed SparkScala PythonR for regular expression regex project in the HadoopHive environment with LinuxWindows for big data resources Used clustering technique KMeans to identify outliers and to classify unlabeled data Utilized AWS Lambda in created userfriendly interface for quick view of reports by using C JSP XML and developed expandable menu that show drilldown data on graph click Evaluated models using Cross Validation Log loss function ROC curves and used AUC for feature selection and elastic technologies like ElasticSearch Kibana etc Categorised comments into positive and negative clusters from different social networking sites using Sentiment Analysis and Text Analytics and have done Image Recognition Tracking operations using sensors until certain criteria is met using AirFlow technology Responsible for different Data mapping activities from Source systems to Teradata using utilities like TPump FEXP MLOAD BTEQ FLOAD etc Analyze traffic patterns by calculating autocorrelation with different time lags Ensured that the model has low False Positive Rate and Text classification and sentiment analysis for unstructured and semistructured data Addressed overfitting by implementing of the algorithm regularization methods like L2 and L1 Used Principal Component Analysis in feature engineering to analyze high dimensional data Created and designed reports that will use gathered metrics to infer and draw logical conclusions of past and future behavior Performed Multinomial Logistic Regression Random forest Decision Tree SVM to classify package is going to deliver on time for the new route Performed data analysis by using Hive to retrieve the data from Hadoop cluster Sql to retrieve data from Oracle database and used ETL for data transformation Used MLlib Sparks Machine learning library to build and evaluate different models and used AWS Rekognition for image analysis Implemented rule based expertise system from the results of exploratory analysis and information gathered from the people from different departments Performed Data Cleaning features scaling features engineering using pandas and numpy packages in python and build models using SAP Predictive Analytics Developed MapReduce pipeline for feature extraction using Hive and Pig Created Data Quality Scripts using SQL and Hive to validate successful data load and quality of the data Created various types of data visualizations using Python and TableauSpotfire Communicated the results with operations team for taking best decisions Collected data needs and requirements by Interacting with the other departments Environment Python 2x CDH5 HDFS C Hadoop 23 Hive Impala AWS Linux Spark Tableau Desktop SQL Server 2012 Microsoft Excel Matlab Spark SQL Pyspark Data Scientist First Data Atlanta GA January 2013 to June 2014 Description First Data Corporation is a global payment processing company headquartered in Atlanta Georgia United States The companys portfolio includes merchant transaction processing services credit debit privatelabel gift payroll and other prepaid card offerings fraud protection and authentication solutions Responsibilities Provided Configuration Management and Build support for more than 5 different applications built and deployed to the production and lower environments Implemented public segmentation using unsupervised machine learning algorithms by implementing kmeans algorithm using Pyspark Using AirFlow to keep track of job statuses in repositories like MySQl and Postgre databases Explored and Extracted data from source XML in HDFS used ETL for preparing data for exploratory analysis using data munging Responsible for different Data mapping activities from Source systems to Teradata Text mining and building models using topic analysis sentiment analysis for both semistructured and unstructured data Handled importing data from various data sources performed transformations using Hive Map Reduce and loaded data into HDFS Used R and python for Exploratory Data Analysis AB testing HQL VQL Data Lake AWS Redshift oozie pySpark Anova test and Hypothesis test to compare and identify the effectiveness of Creative Campaigns Computing AB testing frameworks clickstream and time spent databases using Airflow Created clusters to Control and test groups and conducted group campaigns using Text Analytics Created positive and negative clusters from merchants transaction using Sentiment Analysis to test the authenticity of transactions and resolve any chargebacks Analyzed and calculated the lifetime cost of everyone in the welfare system using 20 years of historical data Created and developed classes and web page elements using C and AJAX JSP was used for validating client side responses and connected C to database to retrieve SQL data Developed LINUXShell scripts by using NZSQLNZLOAD utilities to load data from flat files to Netezza database Developed triggers stored procedures functions and packages using cursors and ref cursor concepts associated with the project using PlSQL Created various types of data visualizations using R C python and TableauSpotfire also connected Pipeline Pilot with Spotfire to create more interactive business driven layouts Used Python R SQL Tensorflow to create Statistical algorithms involving Multivariate Regression Linear Regression Logistic Regression PCA Image Recognition Random forest models Decision trees Support Vector Machine for estimating the risks of welfare dependency Identified and targeted welfare highrisk groups with Machine learningdeep learning algorithms Conducted campaigns and run realtime trials to determine what works fast and track the impact of different initiatives Developed Tableau visualizations and dashboards using Tableau Desktop Used Graphical EntityRelationship Diagramming to create new database design via easy to use graphical interface Created multiple custom SQL queries in Teradata SQL Workbench to prepare the right data sets for Tableau dashboards Perform analyses such as regression analysis logistic regression discriminant analysis cluster analysis using SAS programming Environment R 3x HDFS CHadoop 23 Pig Hive Linux RStudio Tableau 10 SQL Server Ms Excel Pypark Data Scientist TripAdvisor New York NY November 2011 to December 2012 Description TripAdvisor Inc is an American travel website company providing reviews of travelrelated content It also includes interactive travel forums TripAdvisor was an early adopter of usergenerated content The website services are free to users who provide most of the content and the website is supported by an advertising business model Responsibilities Involved in Design Development and Support phases of Software Development Life Cycle SDLC Performed data ETL by collecting exporting merging and massaging data from multiple sources and platforms including SSRSSSIS SQL Server Integration Services in SQL Server Programming experience with NET framework C Visual Studio 20052008 to build web based clientserver architecture and to produce reports with C and JSP Worked with crossfunctional teams including data engineer team to extract data and rapidly execute from MongoDB through MongDB connector for Hadoop Performed data cleaning and feature selection using MLlib package in PySpark Performed partitional clustering into 100 by kmeans clustering using Scikitlearn package in Python where similar hotels for a search are grouped together and Image Recognition Used Python to perform ANOVA test to analyze the differences among hotel clusters Implemented application of various machine learning algorithms and statistical modeling like Decision Tree Text Analytics Sentiment Analysis Naive Bayes Logistic Regression and Linear Regression using Python to determine the accuracy rate of each model Determined the most accurately prediction model based on the accuracy rate Used textmining process of reviews to determine customers concentrations Delivered analysis support to hotel recommendation and providing an online AB test Designed Tableau bar graphs scattered plots and geographical maps to create detailed level summary reports and dashboards Developed hybrid model to improve the accuracy rate Environment Python PySpark C Tableau MongoDB Hadoop SQL Server SDLC ETL SSIS recommendation systems Machine Learning Algorithms textmining process AB test Data Scientist Bank of America Wilmington DE October 2010 to October 2011 Description Bank of America is a multinational banking and financial services corporation It is ranked 2nd on the list of largest banks in the United States by assets As of 2016 Bank of America was the 26th largest company in the United States by total revenue Responsibilities Participated in all phases of research including data collection data cleaning data mining developing models and visualizations Collaborated with data engineers and operation team to collect data from internal system to fit the analytical requirements Redefined many attributes and relationships and cleansed unwanted tablescolumns using SQL queries Utilized Spark SQL API in PySpark to extract and load data and perform SQL queries and also used C connector to perform SQL queries by creating and connecting to SQL engine Performed data imputation using Scikitlearn package in Python Performed data processing using Python libraries like Numpy and Pandas Worked with data analysis using ggplot2 library in R to do data visualizations for better understanding of customers behaviors Implemented statistical modeling with XGBoost machine learning software package using R to determine the predicted probabilities of each model Delivered the results with operation team for better decisions Environment Python R SQL Tableau Spark Machine Learning Software Package recommendation systems Python Developer Cenvien Technologies Hyderabad Telangana June 2009 to August 2010 Description Cenvien technologies gather the requirements by listening and understanding to the clients business requirement to deliver quality products It is highly qualified and strongly dedicated developing team that produces unique solutions Responsibilities Developed entire frontend and backend modules using Python on Django Web Framework Implemented the presentation layer with HTML CSS and JavaScript Involved in writing stored procedures using Oracle Optimized the database queries to improve the performance Designed and developed data management system using Oracle Environment MySQL ORACLE HTML5 CSS3 JavaScript Shell Linux Windows Django Python Programmer Analyst Pennar Industries Limited Hyderabad Telangana April 2008 to May 2009 Description As a backend developer of web applications and data science infrastructure The main area of focus is to come up with comprehensive solutions that need massive capacity and throughput Responsibilities Effectively communicated with the stakeholders to gather requirements for different projects Used MySQL db package and PythonMySQL connector for writing and executing several MYSQL database queries from Python Implemented ClientServer applications using C C JSP and SQL Created functions triggers views and stored procedures using My SQL Worked closely with backend developer to find ways to push the limits of existing Web technology Involved in the code review meetings Environment Python MySQL C Education Bachelor of Computer Science in Computer Science JNTU Anantapur Anantapur Andhra Pradesh Skills APPLICATION DEVELOPMENT TABLEAU 8 years LINUX 5 years SAP 1 year BI 1 year Python R Hadoop Machine Learning Spark Additional Information Methodologies Ralph Kimball and Bill Inmon data warehousing methodology Rational Unified Process RUP Rapid Application Development RAD Joint Application Development JAD BI Tools Tableau Tableau server Tableau Reader SAP Business Objects OBIEE QlikView SAP Business Intelligence Amazon Redshift or Azure Data Warehouse Operating System Windows Linux Unix Macintosh HD Red HatAndroid",
    "unique_id": "338e52c2-ecf9-40f3-896d-d9ac1b59b6b6"
}