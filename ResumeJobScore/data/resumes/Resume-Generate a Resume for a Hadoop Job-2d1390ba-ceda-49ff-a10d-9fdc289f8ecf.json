{
    "clean_data": "Operating systems Linux Ubuntu Windows 2007 08 Other tools Tableau SVN Beyond Compare Education Details January 2016 Bachelors of Engineering Engineering Gujarat Technological University Systems Engineer Hadoop Developer Systems Engineer Hadoop Developer Tata Consultancy Services Skill Details Hadoop Spark Sqoop Hive Flume Pig Exprience 24 monthsCompany Details company Tata Consultancy Services description Roles and responsibility Working for a American pharmaceutical company one of the world s premier biopharmaceutical who develops and produces medicines and vaccines for a wide range of medical disciplines including immunology oncology cardiology endocrinology and neurology To handle large amount of United Healthcare data big data analytics is used Data from all possible data sources like records of all Patients Old and New records of medicines Treatment Pathways Patient Journey for Health Outcomes Patient Finder or Rare Disease Patient Finder etc being gathered stored and processed at one place Worked on cluster with specs as o Cluster Architecture Fully Distributed Package Used CDH3 o Cluster Capacity 20 TB o No of Nodes 10 Data Nodes 3 Masters NFS Backup For NN Developed proof of concepts for enterprise adoption of Hadoop Used SparkAPI over Cloudera Hadoop YARN to perform analytics on the Healthcare data in Cloudera distribution Responsible for cluster maintenance adding and removing cluster nodes cluster monitoring and trouble shooting manage and review data backups and reviewing Hadoop log files Imported exported large data sets of data into HDFS and vice versa using sqoop Involved developing the Pig scripts and Hive Reports Worked on Hive partition and bucketing concepts and created hive external and Internal tables with Hive partition Monitoring Hadoop scripts which take the input from HDFS and load the data into Hive Developed Spark scripts by using Scala shell commands as per the requirement and worked with both Data frames SQL Data sets and RDD MapReduce in Spark Optimizing of existing algorithms in Hadoop using SparkContext Spark SQL Data Frames and RDD s Collaborated with infrastructure network database application and BI to ensure data quality and availability Developed reports using TABLEAU and exported data to HDFS and hive using Sqoop Used ORC Parquet file formats for serialization of data and Snappy for the compression of the data Achievements Appreciation for showing articulate leadership qualities in doing work with the team Completed the internal certification of TCS Certified Hadoop Developer Ongoing Learning Preparing and scheduled the Cloudera Certified Spark Developer CCA 175",
    "entities": [
        "Cloudera Hadoop",
        "TABLEAU",
        "Monitoring Hadoop",
        "Patients Old",
        "Developed",
        "Tata Consultancy Services",
        "Sqoop",
        "SQL Data",
        "BI",
        "HDFS",
        "TCS Certified Hadoop Developer Ongoing Learning Preparing",
        "Healthcare",
        "Exprience",
        "Internal",
        "Spark Optimizing",
        "Hive Developed Spark",
        "RDD MapReduce",
        "Completed",
        "Engineering Engineering Gujarat Technological University Systems Engineer Hadoop Developer Systems Engineer Hadoop",
        "SparkContext Spark SQL Data Frames",
        "TB",
        "RDD",
        "Hadoop",
        "Data",
        "Cluster Architecture Fully Distributed Package",
        "United Healthcare",
        "Cloudera",
        "CDH3"
    ],
    "experience": "Education Details January 2016 Bachelors of Engineering Engineering Gujarat Technological University Systems Engineer Hadoop Developer Systems Engineer Hadoop Developer Tata Consultancy Services Skill Details Hadoop Spark Sqoop Hive Flume Pig Exprience 24 monthsCompany Details company Tata Consultancy Services description Roles and responsibility Working for a American pharmaceutical company one of the world s premier biopharmaceutical who develops and produces medicines and vaccines for a wide range of medical disciplines including immunology oncology cardiology endocrinology and neurology To handle large amount of United Healthcare data big data analytics is used Data from all possible data sources like records of all Patients Old and New records of medicines Treatment Pathways Patient Journey for Health Outcomes Patient Finder or Rare Disease Patient Finder etc being gathered stored and processed at one place Worked on cluster with specs as o Cluster Architecture Fully Distributed Package Used CDH3 o Cluster Capacity 20 TB o No of Nodes 10 Data Nodes 3 Masters NFS Backup For NN Developed proof of concepts for enterprise adoption of Hadoop Used SparkAPI over Cloudera Hadoop YARN to perform analytics on the Healthcare data in Cloudera distribution Responsible for cluster maintenance adding and removing cluster nodes cluster monitoring and trouble shooting manage and review data backups and reviewing Hadoop log files Imported exported large data sets of data into HDFS and vice versa using sqoop Involved developing the Pig scripts and Hive Reports Worked on Hive partition and bucketing concepts and created hive external and Internal tables with Hive partition Monitoring Hadoop scripts which take the input from HDFS and load the data into Hive Developed Spark scripts by using Scala shell commands as per the requirement and worked with both Data frames SQL Data sets and RDD MapReduce in Spark Optimizing of existing algorithms in Hadoop using SparkContext Spark SQL Data Frames and RDD s Collaborated with infrastructure network database application and BI to ensure data quality and availability Developed reports using TABLEAU and exported data to HDFS and hive using Sqoop Used ORC Parquet file formats for serialization of data and Snappy for the compression of the data Achievements Appreciation for showing articulate leadership qualities in doing work with the team Completed the internal certification of TCS Certified Hadoop Developer Ongoing Learning Preparing and scheduled the Cloudera Certified Spark Developer CCA 175",
    "extracted_keywords": [
        "systems",
        "Linux",
        "Ubuntu",
        "Windows",
        "tools",
        "Tableau",
        "SVN",
        "Compare",
        "Education",
        "Details",
        "January",
        "Bachelors",
        "Engineering",
        "Engineering",
        "Gujarat",
        "Technological",
        "University",
        "Systems",
        "Engineer",
        "Hadoop",
        "Developer",
        "Systems",
        "Engineer",
        "Hadoop",
        "Developer",
        "Tata",
        "Consultancy",
        "Services",
        "Skill",
        "Details",
        "Hadoop",
        "Spark",
        "Sqoop",
        "Hive",
        "Flume",
        "Pig",
        "Exprience",
        "Details",
        "company",
        "Tata",
        "Consultancy",
        "Services",
        "description",
        "Roles",
        "responsibility",
        "company",
        "world",
        "premier",
        "biopharmaceutical",
        "medicines",
        "vaccines",
        "range",
        "disciplines",
        "immunology",
        "oncology",
        "cardiology",
        "endocrinology",
        "neurology",
        "amount",
        "United",
        "Healthcare",
        "data",
        "data",
        "analytics",
        "Data",
        "data",
        "sources",
        "records",
        "Patients",
        "Old",
        "records",
        "medicines",
        "Treatment",
        "Pathways",
        "Patient",
        "Journey",
        "Health",
        "Outcomes",
        "Patient",
        "Finder",
        "Rare",
        "Disease",
        "Patient",
        "Finder",
        "etc",
        "place",
        "cluster",
        "specs",
        "o",
        "Cluster",
        "Architecture",
        "Package",
        "CDH3",
        "o",
        "Cluster",
        "Capacity",
        "TB",
        "o",
        "No",
        "Nodes",
        "Data",
        "Nodes",
        "Masters",
        "NFS",
        "Backup",
        "NN",
        "proof",
        "concepts",
        "enterprise",
        "adoption",
        "Hadoop",
        "SparkAPI",
        "Cloudera",
        "Hadoop",
        "YARN",
        "analytics",
        "Healthcare",
        "data",
        "Cloudera",
        "distribution",
        "cluster",
        "maintenance",
        "cluster",
        "nodes",
        "cluster",
        "monitoring",
        "trouble",
        "manage",
        "data",
        "backups",
        "Hadoop",
        "log",
        "files",
        "data",
        "sets",
        "data",
        "HDFS",
        "vice",
        "sqoop",
        "Pig",
        "scripts",
        "Hive",
        "Reports",
        "Hive",
        "partition",
        "bucketing",
        "concepts",
        "hive",
        "tables",
        "Hive",
        "partition",
        "Monitoring",
        "Hadoop",
        "scripts",
        "input",
        "HDFS",
        "data",
        "Hive",
        "Developed",
        "Spark",
        "scripts",
        "Scala",
        "shell",
        "commands",
        "requirement",
        "Data",
        "SQL",
        "Data",
        "sets",
        "MapReduce",
        "Spark",
        "Optimizing",
        "algorithms",
        "Hadoop",
        "SparkContext",
        "Spark",
        "SQL",
        "Data",
        "Frames",
        "RDD",
        "infrastructure",
        "network",
        "database",
        "application",
        "BI",
        "data",
        "quality",
        "availability",
        "reports",
        "TABLEAU",
        "data",
        "HDFS",
        "hive",
        "Sqoop",
        "Used",
        "ORC",
        "Parquet",
        "file",
        "formats",
        "serialization",
        "data",
        "Snappy",
        "compression",
        "data",
        "Achievements",
        "Appreciation",
        "leadership",
        "qualities",
        "work",
        "team",
        "certification",
        "TCS",
        "Certified",
        "Hadoop",
        "Developer",
        "Ongoing",
        "Learning",
        "Preparing",
        "Cloudera",
        "Certified",
        "Spark",
        "Developer",
        "CCA"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T20:01:39.184479",
    "resume_data": "Operating systems Linux Ubuntu Windows 2007 08 Other tools Tableau SVN Beyond Compare Education Details January 2016 Bachelors of Engineering Engineering Gujarat Technological University Systems Engineer Hadoop Developer Systems Engineer Hadoop Developer Tata Consultancy Services Skill Details Hadoop Spark Sqoop Hive Flume Pig Exprience 24 monthsCompany Details company Tata Consultancy Services description Roles and responsibility Working for a American pharmaceutical company one of the world s premier biopharmaceutical who develops and produces medicines and vaccines for a wide range of medical disciplines including immunology oncology cardiology endocrinology and neurology To handle large amount of United Healthcare data big data analytics is used Data from all possible data sources like records of all Patients Old and New records of medicines Treatment Pathways Patient Journey for Health Outcomes Patient Finder or Rare Disease Patient Finder etc being gathered stored and processed at one place Worked on cluster with specs as o Cluster Architecture Fully Distributed Package Used CDH3 o Cluster Capacity 20 TB o No of Nodes 10 Data Nodes 3 Masters NFS Backup For NN Developed proof of concepts for enterprise adoption of Hadoop Used SparkAPI over Cloudera Hadoop YARN to perform analytics on the Healthcare data in Cloudera distribution Responsible for cluster maintenance adding and removing cluster nodes cluster monitoring and trouble shooting manage and review data backups and reviewing Hadoop log files Imported exported large data sets of data into HDFS and vice versa using sqoop Involved developing the Pig scripts and Hive Reports Worked on Hive partition and bucketing concepts and created hive external and Internal tables with Hive partition Monitoring Hadoop scripts which take the input from HDFS and load the data into Hive Developed Spark scripts by using Scala shell commands as per the requirement and worked with both Data frames SQL Data sets and RDD MapReduce in Spark Optimizing of existing algorithms in Hadoop using SparkContext Spark SQL Data Frames and RDD s Collaborated with infrastructure network database application and BI to ensure data quality and availability Developed reports using TABLEAU and exported data to HDFS and hive using Sqoop Used ORC Parquet file formats for serialization of data and Snappy for the compression of the data Achievements Appreciation for showing articulate leadership qualities in doing work with the team Completed the internal certification of TCS Certified Hadoop Developer Ongoing Learning Preparing and scheduled the Cloudera Certified Spark Developer CCA 175",
    "unique_id": "2d1390ba-ceda-49ff-a10d-9fdc289f8ecf"
}