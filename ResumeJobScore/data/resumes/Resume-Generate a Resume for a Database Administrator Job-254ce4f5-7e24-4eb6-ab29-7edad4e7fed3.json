{
    "clean_data": "Big Data Developer Big Data span lDeveloperspan Big Data Developer Wipro LTD Wheeling IL 11 years of experience in software design development maintenance testing and troubleshooting of enterprise applications Over 4 years of experience in development design maintenance and support of Big Data Analytics using Hadoop Ecosystem components like HDFS Hive Pig HBase Sqoop Flume Zookeeper Map Reduce and Oozie Good experience in distributed programming through Spark specifically in Scala Expertise in Java Spring Hibernate JDBC and proficient in using Java APIs for application development Good knowledge on AWS and Microsoft Azure Good working experience with ingestion storage querying processing and analysis of Big Data Good knowledge on Streaming Analytics using Spark Streaming Apache Flume distributed streaming using Kafka Extensive Experience on working with Hadoop Architecture and the components of Hadoop MapReduce HDFS YARN Name Node and Data Node Expertise in writing Hadoop Jobs for analyzing data using Hive Impala Hue and Pig Good Experience on writing Map Reduce programs using Java Good understanding of NoSQL databases like HBase and MongoDB Ingesting data to HDFS from Oracle SQL Server Teradata using SQOOP Experience with SQL PLSQL and database concepts Good Experience with job workflow scheduling like Oozie Airflow Experience in Tableau in generating reports dashboards Experience with performance tuning on map reduce and Hive jobs Load and transform large sets of structured semistructured and unstructured data using Hadoop ecosystem components Knowledge in installation configuration supporting and managing Hadoop clusters Experience in working with different data sources like Flat files XML Parquet and Databases Experience in various phases of Software Development Life Cycle Analysis Requirements gathering Designing with expertise in documenting various requirement specifications functional specifications Test Plans Source to Target mappings SQL Joins Worked on different operating systems platform UNIXLinux Windows Quick learner team player and proficient in handling multiple projects simultaneously Sponsorship required to work in the US Work Experience Big Data Developer Wipro LTD Chicago IL May 2018 to Present This project involves ingesting and processing of legacy application data reports extracts to Big Data Platform from different sources Data Cleansing partitioning transforming data to create Data Lakes for faster analytics Create Spark programs to query big data lakes to generate data reports for meaningful data insights generate dashboards in Tableau Responsibilities Distributed programming through Apache Spark using Scala Created Scala programs to develop the big data queries for business use cases Understanding the existing data warehouse set up and provided design and architecture suggestion converting to Hadoop using MapReduce Hive Spark Sqoop Creating data lakes from Unstructured Semi Structured data in HDFS Transformation and Analysis in Hive Parsing the raw data using Spark Create Hadoop workflow jobs using Oozie Use Spark Streaming and Kafka to analyze real time data coming from different sources Worked on capturing transactional changes in the data using Spark and saving the data in HDFS S3 HBASE Using AWS services like EC2 S3 Route 53 OffshoreOnshore coordination of tasks Following Change and Incidents management for application deployments Environment Spark Scala Hive Kafka Cloudera Unix scripting HBase Hadoop Developer Wipro LTD January 2017 to April 2018 This Project is developing ETL workflow to support and perform big data analytics and AWS Cloud development for the enterprise by customizing AWS Products and creating Big Data Pipeline using AWS Kinesis and Lambda functions Responsibilities Design Develop ETL workflow using Oozie which includes automating the extraction of data from different database into HDFS using Sqoop scripts Transformation and Analysis in Hive Developed Map reduce programs for data analytics Create AWS S3 Data Lakes and create a AWS data pipeline to extract data using Kinesis Streams and analyze the logs using Kinesis Analytics and AWS Lambda functions Customizing AWS Products using Cloud Formation Templates Elastic Load Balancers Creating roles for access to services of AWS VPC Transferring data from S3 Redshift DynamoDB for data analysis Creating Web Application to request AWS EC2 machines Integration with Ticketing tools Worked with different file formats and compression techniques in Hadoop Performance tuning of Hive queries Map reduce programs for different applications Environment CDH Hive AWS Kinesis AWS Lambda Oozie Flume Sqoop Cloudera manager Tableau Hadoop Developer Wipro LTD January 2016 to November 2016 This project is for processing data from disparate data sources media base etc to develop ETL work flow and performing data analytics using the Hadoop ecosystem Log aggregation and analysis of logs generated from different projects to show the trends and help them to provide better service Responsibilities Created Map Reduce Programs to analyze the log and click stream data Data preprocessing and querying tables using HIVE Worked on importing and exporting data into HDFS and Hive using Sqoop Automated all the jobs from pulling data from databases to loading data into SQL server using shell scripts Utilized Flume to import the log data from servers into Hadoop cluster Worked on setting up the Hadoop cluster for the dev test and prod Environment Environment CDH Hadoop HDFS MapReduce Hive Pig Flume Sqoop Tableau Java Developer Wipro LTD January 2012 to July 2015 EPWF is a work flow application that processes electronic payment transactions and keeps track of various stages of the payment transaction flow The work flow will track payments from initiation through process to through life cycle process of payment The work flow will have configurable business rules for each type of customer payment method and payment type Each payment taking system within the customers system will enter an initiated payment process with the work flow The work flow engine will listen for or the payment status changes actively trigger events to work a payment through the system and after settlement of payment transactions EPWF posted to Different types of billing Systems Responsibilities Used Spring Framework for Dependency injection Distributed Component and integrated with web services API Involved in code enhancement and defect fixing Actively participated in Object Oriented Analysis Design sessions of the Project which is based on MVC Architecture using J2EE and Spring Designed and developed presentation layer using HTML JSP and JavaScript Used XML as data communication format between different modules of the application Created mapping files and POJOS using Hibernate Implemented DAO classes and Service classes for business requirement Conducted code reviews against coding standards and made sure the best practices are maintained in development process Environment Java JSP Spring J2EE Ajax JavaScript JDBC Java Developer Infosys LTD June 2008 to September 2011 This project involved in maintaining a website to handle the application packaging and deployment workflow automating the deployment process new web apps to servers Create scripts to gather the health of the servers and creating dashboards to visualize Responsibilities Develop and maintain Web Application for the Process flow store automation scripts and KB articles Creating scripts to monitor the health of the Web and DB Servers and creating JSPs for visualization Developing XML documents for deploying the web application files to Dev QA UAT Environments Automating installation of prerequisites for Web and DB Servers Creation of various scripts to assist with tasks for various teams Knowledge on tools like Microsoft SCCM Microsoft WIX Microsoft Orca Wise studio Environment Java JSP and JavaScript VB Scripting Power shell scripting Unix Shell Scripting Education Bachelors Skills Hdfs Oozie Sqoop Hbase Kafka Big Data ETL Tableau Hadoop Python Redshift CertificationsLicenses CCA Spark and Hadoop Developer May 2019 to May 2021 Oracle Certified Associate June 2019 to Present AWS Certified Solutions Architect September 2018 to September 2020 Additional Information Technical Skills Core Competencies Big Data Hadoop Application Development Core Java SE J2EE Spring Framework Object Oriented Programming OOP Full Lifecycle SDLC Client Server Development Application Integration Programming Languages C C Java J2EE Scala Python JavaScript Big Data Apache Spark Map Reduce HDFS PIG Hive YARN Sqoop Oozie Kafka Flume Zookeeper NoSQL Database HBASE MongoDB Relational Database Oracle 9i 10g 11g MySQL Reporting Tableau 10 Operating Systems Windows UNIX Linux RHEL CENTOS Solaris Scripting Shell PowerShell Training and Achievements",
    "entities": [
        "Create Spark",
        "Create AWS S3 Data Lakes",
        "Kinesis Streams",
        "J2EE Spring Framework Object Oriented Programming OOP Full Lifecycle SDLC",
        "Oozie Airflow",
        "J2EE and Spring Designed",
        "UNIXLinux Windows Quick",
        "Hadoop Jobs",
        "ETL",
        "US",
        "Server Development Application Integration Programming Languages C C",
        "Sqoop",
        "HDFS S3 HBASE Using AWS",
        "Operating Systems Windows UNIX Linux",
        "Oozie Use Spark Streaming",
        "Hibernate Implemented",
        "HDFS",
        "Created",
        "Transformation and Analysis",
        "Kinesis Analytics",
        "AWS",
        "Spark Create Hadoop",
        "Hadoop Architecture",
        "HBase Hadoop Developer Wipro LTD",
        "API Involved",
        "Tableau Hadoop Developer Wipro LTD",
        "Hadoop MapReduce HDFS YARN Name",
        "MapReduce Hive Spark",
        "Java Spring Hibernate",
        "WIX",
        "Microsoft",
        "Object Oriented Analysis Design",
        "Developer Infosys LTD",
        "Big Data Analytics",
        "HDFS Transformation and Analysis",
        "Hadoop Ecosystem",
        "Unstructured Semi Structured",
        "JSP",
        "Present AWS Certified Solutions",
        "Oozie Good",
        "SQL",
        "Responsibilities Develop",
        "HDFS Hive Pig HBase Sqoop",
        "Hadoop",
        "XML",
        "EPWF",
        "Microsoft Orca Wise",
        "MVC Architecture",
        "NoSQL",
        "Tableau",
        "AWS VPC Transferring",
        "AWS Products",
        "Tableau Responsibilities Distributed",
        "Responsibilities Created",
        "Chicago",
        "HBase",
        "Big Data Pipeline",
        "POJOS",
        "Big Data Developer Big Data",
        "Apache Spark",
        "Big Data",
        "Hive",
        "SQOOP",
        "Spark",
        "Target",
        "Software Development Life Cycle Analysis Requirements gathering Designing",
        "Oracle SQL Server Teradata"
    ],
    "experience": "Experience on working with Hadoop Architecture and the components of Hadoop MapReduce HDFS YARN Name Node and Data Node Expertise in writing Hadoop Jobs for analyzing data using Hive Impala Hue and Pig Good Experience on writing Map Reduce programs using Java Good understanding of NoSQL databases like HBase and MongoDB Ingesting data to HDFS from Oracle SQL Server Teradata using SQOOP Experience with SQL PLSQL and database concepts Good Experience with job workflow scheduling like Oozie Airflow Experience in Tableau in generating reports dashboards Experience with performance tuning on map reduce and Hive jobs Load and transform large sets of structured semistructured and unstructured data using Hadoop ecosystem components Knowledge in installation configuration supporting and managing Hadoop clusters Experience in working with different data sources like Flat files XML Parquet and Databases Experience in various phases of Software Development Life Cycle Analysis Requirements gathering Designing with expertise in documenting various requirement specifications functional specifications Test Plans Source to Target mappings SQL Joins Worked on different operating systems platform UNIXLinux Windows Quick learner team player and proficient in handling multiple projects simultaneously Sponsorship required to work in the US Work Experience Big Data Developer Wipro LTD Chicago IL May 2018 to Present This project involves ingesting and processing of legacy application data reports extracts to Big Data Platform from different sources Data Cleansing partitioning transforming data to create Data Lakes for faster analytics Create Spark programs to query big data lakes to generate data reports for meaningful data insights generate dashboards in Tableau Responsibilities Distributed programming through Apache Spark using Scala Created Scala programs to develop the big data queries for business use cases Understanding the existing data warehouse set up and provided design and architecture suggestion converting to Hadoop using MapReduce Hive Spark Sqoop Creating data lakes from Unstructured Semi Structured data in HDFS Transformation and Analysis in Hive Parsing the raw data using Spark Create Hadoop workflow jobs using Oozie Use Spark Streaming and Kafka to analyze real time data coming from different sources Worked on capturing transactional changes in the data using Spark and saving the data in HDFS S3 HBASE Using AWS services like EC2 S3 Route 53 OffshoreOnshore coordination of tasks Following Change and Incidents management for application deployments Environment Spark Scala Hive Kafka Cloudera Unix scripting HBase Hadoop Developer Wipro LTD January 2017 to April 2018 This Project is developing ETL workflow to support and perform big data analytics and AWS Cloud development for the enterprise by customizing AWS Products and creating Big Data Pipeline using AWS Kinesis and Lambda functions Responsibilities Design Develop ETL workflow using Oozie which includes automating the extraction of data from different database into HDFS using Sqoop scripts Transformation and Analysis in Hive Developed Map reduce programs for data analytics Create AWS S3 Data Lakes and create a AWS data pipeline to extract data using Kinesis Streams and analyze the logs using Kinesis Analytics and AWS Lambda functions Customizing AWS Products using Cloud Formation Templates Elastic Load Balancers Creating roles for access to services of AWS VPC Transferring data from S3 Redshift DynamoDB for data analysis Creating Web Application to request AWS EC2 machines Integration with Ticketing tools Worked with different file formats and compression techniques in Hadoop Performance tuning of Hive queries Map reduce programs for different applications Environment CDH Hive AWS Kinesis AWS Lambda Oozie Flume Sqoop Cloudera manager Tableau Hadoop Developer Wipro LTD January 2016 to November 2016 This project is for processing data from disparate data sources media base etc to develop ETL work flow and performing data analytics using the Hadoop ecosystem Log aggregation and analysis of logs generated from different projects to show the trends and help them to provide better service Responsibilities Created Map Reduce Programs to analyze the log and click stream data Data preprocessing and querying tables using HIVE Worked on importing and exporting data into HDFS and Hive using Sqoop Automated all the jobs from pulling data from databases to loading data into SQL server using shell scripts Utilized Flume to import the log data from servers into Hadoop cluster Worked on setting up the Hadoop cluster for the dev test and prod Environment Environment CDH Hadoop HDFS MapReduce Hive Pig Flume Sqoop Tableau Java Developer Wipro LTD January 2012 to July 2015 EPWF is a work flow application that processes electronic payment transactions and keeps track of various stages of the payment transaction flow The work flow will track payments from initiation through process to through life cycle process of payment The work flow will have configurable business rules for each type of customer payment method and payment type Each payment taking system within the customers system will enter an initiated payment process with the work flow The work flow engine will listen for or the payment status changes actively trigger events to work a payment through the system and after settlement of payment transactions EPWF posted to Different types of billing Systems Responsibilities Used Spring Framework for Dependency injection Distributed Component and integrated with web services API Involved in code enhancement and defect fixing Actively participated in Object Oriented Analysis Design sessions of the Project which is based on MVC Architecture using J2EE and Spring Designed and developed presentation layer using HTML JSP and JavaScript Used XML as data communication format between different modules of the application Created mapping files and POJOS using Hibernate Implemented DAO classes and Service classes for business requirement Conducted code reviews against coding standards and made sure the best practices are maintained in development process Environment Java JSP Spring J2EE Ajax JavaScript JDBC Java Developer Infosys LTD June 2008 to September 2011 This project involved in maintaining a website to handle the application packaging and deployment workflow automating the deployment process new web apps to servers Create scripts to gather the health of the servers and creating dashboards to visualize Responsibilities Develop and maintain Web Application for the Process flow store automation scripts and KB articles Creating scripts to monitor the health of the Web and DB Servers and creating JSPs for visualization Developing XML documents for deploying the web application files to Dev QA UAT Environments Automating installation of prerequisites for Web and DB Servers Creation of various scripts to assist with tasks for various teams Knowledge on tools like Microsoft SCCM Microsoft WIX Microsoft Orca Wise studio Environment Java JSP and JavaScript VB Scripting Power shell scripting Unix Shell Scripting Education Bachelors Skills Hdfs Oozie Sqoop Hbase Kafka Big Data ETL Tableau Hadoop Python Redshift CertificationsLicenses CCA Spark and Hadoop Developer May 2019 to May 2021 Oracle Certified Associate June 2019 to Present AWS Certified Solutions Architect September 2018 to September 2020 Additional Information Technical Skills Core Competencies Big Data Hadoop Application Development Core Java SE J2EE Spring Framework Object Oriented Programming OOP Full Lifecycle SDLC Client Server Development Application Integration Programming Languages C C Java J2EE Scala Python JavaScript Big Data Apache Spark Map Reduce HDFS PIG Hive YARN Sqoop Oozie Kafka Flume Zookeeper NoSQL Database HBASE MongoDB Relational Database Oracle 9i 10 g 11 g MySQL Reporting Tableau 10 Operating Systems Windows UNIX Linux RHEL CENTOS Solaris Scripting Shell PowerShell Training and Achievements",
    "extracted_keywords": [
        "Data",
        "Developer",
        "Big",
        "Data",
        "span",
        "lDeveloperspan",
        "Big",
        "Data",
        "Developer",
        "Wipro",
        "LTD",
        "Wheeling",
        "IL",
        "years",
        "experience",
        "software",
        "design",
        "development",
        "maintenance",
        "testing",
        "troubleshooting",
        "enterprise",
        "applications",
        "years",
        "experience",
        "development",
        "design",
        "maintenance",
        "support",
        "Big",
        "Data",
        "Analytics",
        "Hadoop",
        "Ecosystem",
        "components",
        "Hive",
        "Pig",
        "HBase",
        "Sqoop",
        "Flume",
        "Zookeeper",
        "Map",
        "Reduce",
        "Oozie",
        "Good",
        "experience",
        "programming",
        "Spark",
        "Scala",
        "Expertise",
        "Java",
        "Spring",
        "Hibernate",
        "JDBC",
        "Java",
        "APIs",
        "application",
        "development",
        "knowledge",
        "AWS",
        "Microsoft",
        "Azure",
        "working",
        "experience",
        "ingestion",
        "storage",
        "processing",
        "analysis",
        "Data",
        "knowledge",
        "Streaming",
        "Analytics",
        "Spark",
        "Streaming",
        "Apache",
        "Flume",
        "streaming",
        "Kafka",
        "Extensive",
        "Experience",
        "Hadoop",
        "Architecture",
        "components",
        "Hadoop",
        "MapReduce",
        "HDFS",
        "YARN",
        "Name",
        "Node",
        "Data",
        "Node",
        "Expertise",
        "Hadoop",
        "Jobs",
        "data",
        "Hive",
        "Impala",
        "Hue",
        "Pig",
        "Good",
        "Experience",
        "Map",
        "Reduce",
        "programs",
        "Java",
        "understanding",
        "HBase",
        "MongoDB",
        "data",
        "HDFS",
        "Oracle",
        "SQL",
        "Server",
        "Teradata",
        "Experience",
        "SQL",
        "PLSQL",
        "database",
        "Experience",
        "job",
        "workflow",
        "scheduling",
        "Oozie",
        "Airflow",
        "Experience",
        "Tableau",
        "reports",
        "Experience",
        "performance",
        "map",
        "Hive",
        "jobs",
        "Load",
        "sets",
        "data",
        "Hadoop",
        "ecosystem",
        "Knowledge",
        "installation",
        "configuration",
        "Hadoop",
        "clusters",
        "Experience",
        "data",
        "sources",
        "files",
        "XML",
        "Parquet",
        "Databases",
        "Experience",
        "phases",
        "Software",
        "Development",
        "Life",
        "Cycle",
        "Analysis",
        "Requirements",
        "Designing",
        "expertise",
        "requirement",
        "specifications",
        "specifications",
        "Test",
        "Plans",
        "Source",
        "Target",
        "mappings",
        "SQL",
        "Joins",
        "operating",
        "systems",
        "platform",
        "UNIXLinux",
        "Windows",
        "Quick",
        "learner",
        "team",
        "player",
        "projects",
        "Sponsorship",
        "US",
        "Work",
        "Experience",
        "Big",
        "Data",
        "Developer",
        "Wipro",
        "LTD",
        "Chicago",
        "IL",
        "May",
        "Present",
        "project",
        "processing",
        "legacy",
        "application",
        "data",
        "reports",
        "Big",
        "Data",
        "Platform",
        "sources",
        "Data",
        "Cleansing",
        "data",
        "Data",
        "Lakes",
        "analytics",
        "Spark",
        "programs",
        "data",
        "lakes",
        "data",
        "reports",
        "data",
        "insights",
        "dashboards",
        "Tableau",
        "Responsibilities",
        "programming",
        "Apache",
        "Spark",
        "Scala",
        "Created",
        "Scala",
        "programs",
        "data",
        "queries",
        "business",
        "use",
        "cases",
        "data",
        "warehouse",
        "design",
        "architecture",
        "suggestion",
        "Hadoop",
        "MapReduce",
        "Hive",
        "Spark",
        "Sqoop",
        "data",
        "lakes",
        "Unstructured",
        "Semi",
        "data",
        "HDFS",
        "Transformation",
        "Analysis",
        "Hive",
        "data",
        "Spark",
        "Create",
        "Hadoop",
        "workflow",
        "jobs",
        "Oozie",
        "Use",
        "Spark",
        "Streaming",
        "Kafka",
        "time",
        "data",
        "sources",
        "changes",
        "data",
        "Spark",
        "data",
        "HDFS",
        "S3",
        "HBASE",
        "AWS",
        "services",
        "EC2",
        "S3",
        "Route",
        "OffshoreOnshore",
        "coordination",
        "tasks",
        "Change",
        "Incidents",
        "management",
        "application",
        "deployments",
        "Environment",
        "Spark",
        "Scala",
        "Hive",
        "Kafka",
        "Cloudera",
        "Unix",
        "HBase",
        "Hadoop",
        "Developer",
        "Wipro",
        "LTD",
        "January",
        "April",
        "Project",
        "ETL",
        "workflow",
        "data",
        "analytics",
        "AWS",
        "Cloud",
        "development",
        "enterprise",
        "AWS",
        "Products",
        "Big",
        "Data",
        "Pipeline",
        "AWS",
        "Kinesis",
        "Lambda",
        "functions",
        "Responsibilities",
        "Design",
        "Develop",
        "ETL",
        "workflow",
        "Oozie",
        "extraction",
        "data",
        "database",
        "HDFS",
        "Sqoop",
        "scripts",
        "Transformation",
        "Analysis",
        "Hive",
        "Developed",
        "Map",
        "programs",
        "data",
        "analytics",
        "AWS",
        "S3",
        "Data",
        "Lakes",
        "AWS",
        "data",
        "pipeline",
        "data",
        "Kinesis",
        "Streams",
        "logs",
        "Kinesis",
        "Analytics",
        "AWS",
        "Lambda",
        "functions",
        "Customizing",
        "AWS",
        "Products",
        "Cloud",
        "Formation",
        "Templates",
        "Elastic",
        "Load",
        "Balancers",
        "roles",
        "access",
        "services",
        "AWS",
        "VPC",
        "data",
        "S3",
        "Redshift",
        "DynamoDB",
        "data",
        "analysis",
        "Web",
        "Application",
        "AWS",
        "EC2",
        "machines",
        "Integration",
        "Ticketing",
        "tools",
        "file",
        "formats",
        "compression",
        "techniques",
        "Hadoop",
        "Performance",
        "tuning",
        "Hive",
        "Map",
        "programs",
        "applications",
        "Environment",
        "CDH",
        "Hive",
        "AWS",
        "Kinesis",
        "AWS",
        "Lambda",
        "Oozie",
        "Flume",
        "Sqoop",
        "Cloudera",
        "manager",
        "Tableau",
        "Hadoop",
        "Developer",
        "Wipro",
        "LTD",
        "January",
        "November",
        "project",
        "data",
        "data",
        "sources",
        "media",
        "base",
        "ETL",
        "work",
        "flow",
        "data",
        "analytics",
        "Hadoop",
        "ecosystem",
        "Log",
        "aggregation",
        "analysis",
        "logs",
        "projects",
        "trends",
        "service",
        "Responsibilities",
        "Map",
        "Reduce",
        "Programs",
        "log",
        "stream",
        "data",
        "Data",
        "tables",
        "HIVE",
        "Worked",
        "data",
        "HDFS",
        "Hive",
        "Sqoop",
        "Automated",
        "jobs",
        "data",
        "databases",
        "data",
        "SQL",
        "server",
        "scripts",
        "Flume",
        "log",
        "data",
        "servers",
        "Hadoop",
        "cluster",
        "Hadoop",
        "cluster",
        "dev",
        "test",
        "prod",
        "Environment",
        "Environment",
        "CDH",
        "Hadoop",
        "HDFS",
        "MapReduce",
        "Hive",
        "Pig",
        "Flume",
        "Sqoop",
        "Tableau",
        "Java",
        "Developer",
        "Wipro",
        "LTD",
        "January",
        "July",
        "EPWF",
        "work",
        "flow",
        "application",
        "payment",
        "transactions",
        "track",
        "stages",
        "payment",
        "transaction",
        "work",
        "flow",
        "payments",
        "initiation",
        "process",
        "life",
        "cycle",
        "process",
        "payment",
        "work",
        "flow",
        "business",
        "rules",
        "type",
        "customer",
        "payment",
        "method",
        "payment",
        "type",
        "payment",
        "system",
        "customers",
        "system",
        "payment",
        "process",
        "work",
        "flow",
        "work",
        "flow",
        "engine",
        "payment",
        "status",
        "changes",
        "events",
        "payment",
        "system",
        "settlement",
        "payment",
        "transactions",
        "EPWF",
        "types",
        "billing",
        "Systems",
        "Responsibilities",
        "Spring",
        "Framework",
        "Dependency",
        "injection",
        "Component",
        "web",
        "services",
        "API",
        "code",
        "enhancement",
        "defect",
        "fixing",
        "Object",
        "Oriented",
        "Analysis",
        "Design",
        "sessions",
        "Project",
        "MVC",
        "Architecture",
        "J2EE",
        "Spring",
        "presentation",
        "layer",
        "HTML",
        "JSP",
        "JavaScript",
        "XML",
        "data",
        "communication",
        "format",
        "modules",
        "application",
        "mapping",
        "files",
        "POJOS",
        "Hibernate",
        "Implemented",
        "DAO",
        "classes",
        "Service",
        "classes",
        "business",
        "requirement",
        "Conducted",
        "code",
        "reviews",
        "standards",
        "practices",
        "development",
        "process",
        "Environment",
        "Java",
        "JSP",
        "Spring",
        "J2EE",
        "Ajax",
        "JavaScript",
        "JDBC",
        "Java",
        "Developer",
        "Infosys",
        "LTD",
        "June",
        "September",
        "project",
        "website",
        "application",
        "packaging",
        "deployment",
        "workflow",
        "deployment",
        "process",
        "web",
        "apps",
        "servers",
        "scripts",
        "health",
        "servers",
        "dashboards",
        "Responsibilities",
        "Web",
        "Application",
        "Process",
        "flow",
        "store",
        "automation",
        "scripts",
        "KB",
        "scripts",
        "health",
        "Web",
        "DB",
        "Servers",
        "JSPs",
        "visualization",
        "XML",
        "documents",
        "web",
        "application",
        "files",
        "Dev",
        "QA",
        "UAT",
        "Environments",
        "Automating",
        "installation",
        "prerequisites",
        "Web",
        "DB",
        "Servers",
        "Creation",
        "scripts",
        "tasks",
        "teams",
        "Knowledge",
        "tools",
        "Microsoft",
        "Microsoft",
        "WIX",
        "Microsoft",
        "Orca",
        "Wise",
        "studio",
        "Environment",
        "Java",
        "JSP",
        "JavaScript",
        "VB",
        "Scripting",
        "Power",
        "shell",
        "Unix",
        "Shell",
        "Scripting",
        "Education",
        "Bachelors",
        "Skills",
        "Hdfs",
        "Oozie",
        "Sqoop",
        "Hbase",
        "Kafka",
        "Big",
        "Data",
        "ETL",
        "Tableau",
        "Hadoop",
        "Python",
        "Redshift",
        "CertificationsLicenses",
        "CCA",
        "Spark",
        "Hadoop",
        "Developer",
        "May",
        "May",
        "Oracle",
        "Certified",
        "Associate",
        "June",
        "AWS",
        "Certified",
        "Solutions",
        "Architect",
        "September",
        "September",
        "Additional",
        "Information",
        "Technical",
        "Skills",
        "Core",
        "Competencies",
        "Big",
        "Data",
        "Hadoop",
        "Application",
        "Development",
        "Core",
        "Java",
        "SE",
        "J2EE",
        "Spring",
        "Framework",
        "Object",
        "Programming",
        "OOP",
        "Lifecycle",
        "SDLC",
        "Client",
        "Server",
        "Development",
        "Application",
        "Integration",
        "Programming",
        "Languages",
        "C",
        "C",
        "Java",
        "J2EE",
        "Scala",
        "Python",
        "JavaScript",
        "Big",
        "Data",
        "Apache",
        "Spark",
        "Map",
        "Reduce",
        "HDFS",
        "PIG",
        "Hive",
        "YARN",
        "Sqoop",
        "Oozie",
        "Kafka",
        "Flume",
        "Zookeeper",
        "NoSQL",
        "Database",
        "HBASE",
        "MongoDB",
        "Relational",
        "Database",
        "Oracle",
        "9i",
        "g",
        "g",
        "MySQL",
        "Tableau",
        "Operating",
        "Systems",
        "Windows",
        "UNIX",
        "Linux",
        "RHEL",
        "CENTOS",
        "Solaris",
        "Scripting",
        "Shell",
        "PowerShell",
        "Training",
        "Achievements"
    ],
    "input_field": null,
    "instruction": "",
    "processed_at": "2024-11-24T19:43:19.699306",
    "resume_data": "Big Data Developer Big Data span lDeveloperspan Big Data Developer Wipro LTD Wheeling IL 11 years of experience in software design development maintenance testing and troubleshooting of enterprise applications Over 4 years of experience in development design maintenance and support of Big Data Analytics using Hadoop Ecosystem components like HDFS Hive Pig HBase Sqoop Flume Zookeeper Map Reduce and Oozie Good experience in distributed programming through Spark specifically in Scala Expertise in Java Spring Hibernate JDBC and proficient in using Java APIs for application development Good knowledge on AWS and Microsoft Azure Good working experience with ingestion storage querying processing and analysis of Big Data Good knowledge on Streaming Analytics using Spark Streaming Apache Flume distributed streaming using Kafka Extensive Experience on working with Hadoop Architecture and the components of Hadoop MapReduce HDFS YARN Name Node and Data Node Expertise in writing Hadoop Jobs for analyzing data using Hive Impala Hue and Pig Good Experience on writing Map Reduce programs using Java Good understanding of NoSQL databases like HBase and MongoDB Ingesting data to HDFS from Oracle SQL Server Teradata using SQOOP Experience with SQL PLSQL and database concepts Good Experience with job workflow scheduling like Oozie Airflow Experience in Tableau in generating reports dashboards Experience with performance tuning on map reduce and Hive jobs Load and transform large sets of structured semistructured and unstructured data using Hadoop ecosystem components Knowledge in installation configuration supporting and managing Hadoop clusters Experience in working with different data sources like Flat files XML Parquet and Databases Experience in various phases of Software Development Life Cycle Analysis Requirements gathering Designing with expertise in documenting various requirement specifications functional specifications Test Plans Source to Target mappings SQL Joins Worked on different operating systems platform UNIXLinux Windows Quick learner team player and proficient in handling multiple projects simultaneously Sponsorship required to work in the US Work Experience Big Data Developer Wipro LTD Chicago IL May 2018 to Present This project involves ingesting and processing of legacy application data reports extracts to Big Data Platform from different sources Data Cleansing partitioning transforming data to create Data Lakes for faster analytics Create Spark programs to query big data lakes to generate data reports for meaningful data insights generate dashboards in Tableau Responsibilities Distributed programming through Apache Spark using Scala Created Scala programs to develop the big data queries for business use cases Understanding the existing data warehouse set up and provided design and architecture suggestion converting to Hadoop using MapReduce Hive Spark Sqoop Creating data lakes from Unstructured Semi Structured data in HDFS Transformation and Analysis in Hive Parsing the raw data using Spark Create Hadoop workflow jobs using Oozie Use Spark Streaming and Kafka to analyze real time data coming from different sources Worked on capturing transactional changes in the data using Spark and saving the data in HDFS S3 HBASE Using AWS services like EC2 S3 Route 53 OffshoreOnshore coordination of tasks Following Change and Incidents management for application deployments Environment Spark Scala Hive Kafka Cloudera Unix scripting HBase Hadoop Developer Wipro LTD January 2017 to April 2018 This Project is developing ETL workflow to support and perform big data analytics and AWS Cloud development for the enterprise by customizing AWS Products and creating Big Data Pipeline using AWS Kinesis and Lambda functions Responsibilities Design Develop ETL workflow using Oozie which includes automating the extraction of data from different database into HDFS using Sqoop scripts Transformation and Analysis in Hive Developed Map reduce programs for data analytics Create AWS S3 Data Lakes and create a AWS data pipeline to extract data using Kinesis Streams and analyze the logs using Kinesis Analytics and AWS Lambda functions Customizing AWS Products using Cloud Formation Templates Elastic Load Balancers Creating roles for access to services of AWS VPC Transferring data from S3 Redshift DynamoDB for data analysis Creating Web Application to request AWS EC2 machines Integration with Ticketing tools Worked with different file formats and compression techniques in Hadoop Performance tuning of Hive queries Map reduce programs for different applications Environment CDH Hive AWS Kinesis AWS Lambda Oozie Flume Sqoop Cloudera manager Tableau Hadoop Developer Wipro LTD January 2016 to November 2016 This project is for processing data from disparate data sources media base etc to develop ETL work flow and performing data analytics using the Hadoop ecosystem Log aggregation and analysis of logs generated from different projects to show the trends and help them to provide better service Responsibilities Created Map Reduce Programs to analyze the log and click stream data Data preprocessing and querying tables using HIVE Worked on importing and exporting data into HDFS and Hive using Sqoop Automated all the jobs from pulling data from databases to loading data into SQL server using shell scripts Utilized Flume to import the log data from servers into Hadoop cluster Worked on setting up the Hadoop cluster for the dev test and prod Environment Environment CDH Hadoop HDFS MapReduce Hive Pig Flume Sqoop Tableau Java Developer Wipro LTD January 2012 to July 2015 EPWF is a work flow application that processes electronic payment transactions and keeps track of various stages of the payment transaction flow The work flow will track payments from initiation through process to through life cycle process of payment The work flow will have configurable business rules for each type of customer payment method and payment type Each payment taking system within the customers system will enter an initiated payment process with the work flow The work flow engine will listen for or the payment status changes actively trigger events to work a payment through the system and after settlement of payment transactions EPWF posted to Different types of billing Systems Responsibilities Used Spring Framework for Dependency injection Distributed Component and integrated with web services API Involved in code enhancement and defect fixing Actively participated in Object Oriented Analysis Design sessions of the Project which is based on MVC Architecture using J2EE and Spring Designed and developed presentation layer using HTML JSP and JavaScript Used XML as data communication format between different modules of the application Created mapping files and POJOS using Hibernate Implemented DAO classes and Service classes for business requirement Conducted code reviews against coding standards and made sure the best practices are maintained in development process Environment Java JSP Spring J2EE Ajax JavaScript JDBC Java Developer Infosys LTD June 2008 to September 2011 This project involved in maintaining a website to handle the application packaging and deployment workflow automating the deployment process new web apps to servers Create scripts to gather the health of the servers and creating dashboards to visualize Responsibilities Develop and maintain Web Application for the Process flow store automation scripts and KB articles Creating scripts to monitor the health of the Web and DB Servers and creating JSPs for visualization Developing XML documents for deploying the web application files to Dev QA UAT Environments Automating installation of prerequisites for Web and DB Servers Creation of various scripts to assist with tasks for various teams Knowledge on tools like Microsoft SCCM Microsoft WIX Microsoft Orca Wise studio Environment Java JSP and JavaScript VB Scripting Power shell scripting Unix Shell Scripting Education Bachelors Skills Hdfs Oozie Sqoop Hbase Kafka Big Data ETL Tableau Hadoop Python Redshift CertificationsLicenses CCA Spark and Hadoop Developer May 2019 to May 2021 Oracle Certified Associate June 2019 to Present AWS Certified Solutions Architect September 2018 to September 2020 Additional Information Technical Skills Core Competencies Big Data Hadoop Application Development Core Java SE J2EE Spring Framework Object Oriented Programming OOP Full Lifecycle SDLC Client Server Development Application Integration Programming Languages C C Java J2EE Scala Python JavaScript Big Data Apache Spark Map Reduce HDFS PIG Hive YARN Sqoop Oozie Kafka Flume Zookeeper NoSQL Database HBASE MongoDB Relational Database Oracle 9i 10g 11g MySQL Reporting Tableau 10 Operating Systems Windows UNIX Linux RHEL CENTOS Solaris Scripting Shell PowerShell Training and Achievements",
    "unique_id": "254ce4f5-7e24-4eb6-ab29-7edad4e7fed3"
}